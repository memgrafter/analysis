---
ver: rpa2
title: Empirical Study of Symmetrical Reasoning in Conversational Chatbots
arxiv_id: '2407.05734'
source_url: https://arxiv.org/abs/2407.05734
tags:
- chatbots
- symmetry
- gemini
- sentences
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates whether conversational chatbots powered\
  \ by large language models (LLMs) can understand and evaluate predicate symmetry\u2014\
  a cognitive linguistic function. Using in-context learning, five chatbots (ChatGPT\
  \ 4, HuggingChat, Copilot, LLaMA via Perplexity, and Gemini Advanced) were prompted\
  \ with the Symmetry Inference Sentence (SIS) dataset to rate how alike in meaning\
  \ are pairs of sentences in which the order of entities is swapped."
---

# Empirical Study of Symmetrical Reasoning in Conversational Chatbots

## Quick Facts
- **arXiv ID**: 2407.05734
- **Source URL**: https://arxiv.org/abs/2407.05734
- **Reference count**: 31
- **Primary result**: Conversational chatbots using in-context learning can evaluate predicate symmetry, with Gemini Advanced achieving 0.85 correlation with human judgments

## Executive Summary
This study investigates whether conversational chatbots powered by large language models can understand and evaluate predicate symmetry—a cognitive linguistic function. Using in-context learning, five chatbots (ChatGPT 4, HuggingChat, Copilot, LLaMA via Perplexity, and Gemini Advanced) were prompted with the Symmetry Inference Sentence (SIS) dataset to rate how alike in meaning are pairs of sentences where the order of entities is swapped. Results show varied performance, with Gemini Advanced achieving the highest correlation (0.85) with human evaluations and providing clear justifications for each rating. Some chatbots, like Gemini and HuggingChat, demonstrated reasoning capabilities comparable to fine-tuned models such as BERT. The findings suggest that LLMs can mirror complex cognitive processes like symmetrical reasoning, though with inconsistent success across models.

## Method Summary
The study employed in-context learning (ICL) to evaluate five conversational chatbots on predicate symmetry tasks without fine-tuning. Researchers used the SIS dataset containing 400 sentence pairs and prompted chatbots to rate their semantic similarity on a scale of 1-5. Human evaluators had previously rated these pairs, providing ground truth. The chatbots were presented with prompts explaining symmetry and the rating task. Performance was measured by computing correlation coefficients between chatbot scores and averaged human ratings, with Gemini Advanced tested seven times to assess consistency. Qualitative analysis examined the reasoning provided by models in their justifications.

## Key Results
- Gemini Advanced achieved the highest correlation (0.85) with human symmetry judgments
- Some chatbots (Gemini, HuggingChat) performed comparably to fine-tuned BERT models on symmetry tasks
- Model performance varied significantly, with ChatGPT 4, HuggingChat, and Copilot showing lower correlations than Gemini Advanced

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conversational chatbots can evaluate predicate symmetry through in-context learning without fine-tuning.
- Mechanism: The models leverage their pre-trained language understanding to generalize patterns of symmetrical relationships from few-shot prompts, capturing linguistic invariance under word order swaps.
- Core assumption: LLMs have internalized structural linguistic patterns that enable symmetry inference across diverse contexts.
- Evidence anchors:
  - [abstract] "Using in-context learning (ICL), a paradigm shift enabling chatbots to learn new tasks from prompts without re-training"
  - [section] "The essence of ICL lies in the ability of LLMs to capture language patterns from the vast training data, and thus being able to adapt and generalize this knowledge to novel tasks"
  - [corpus] Weak: Corpus lacks direct evidence of symmetry task generalization.
- Break condition: Prompts fail to convey symmetry concept clearly, or model lacks sufficient exposure to symmetrical/reciprocal constructions in training data.

### Mechanism 2
- Claim: Gemini Advanced achieves high correlation with human symmetry judgments due to deeper reasoning.
- Mechanism: Gemini provides detailed justifications for each score, indicating it performs step-by-step semantic analysis rather than surface-level token matching.
- Core assumption: Detailed reasoning outputs correlate with accurate symmetry evaluation.
- Evidence anchors:
  - [abstract] "Gemini, for example, reaches a correlation of 0.85 with human scores, while providing a sounding justification for each symmetry evaluation"
  - [section] "We also provide an example of a dissimilar score of 4 given by Gemini... performs step-by-step semantic analysis"
  - [corpus] Weak: No corpus evidence of reasoning quality correlation.
- Break condition: Model begins hallucinating justifications or produces inconsistent scores across repeated trials.

### Mechanism 3
- Claim: Some chatbots perform comparably to fine-tuned BERT models on symmetry tasks.
- Mechanism: ICL enables zero-shot transfer of linguistic knowledge to symmetry inference without task-specific training.
- Core assumption: Pre-training on diverse linguistic data equips models with symmetry-relevant features.
- Evidence anchors:
  - [abstract] "some approaching human-like reasoning capabilities"
  - [section] "HuggingChat and Gemini scores show evidence of ICL... performed comparably to fine-tuning BERT"
  - [corpus] Weak: Corpus lacks fine-tuned model comparison data.
- Break condition: Model fails to understand prompt format or misinterprets symmetry definition.

## Foundational Learning

- Concept: Predicate symmetry in linguistics
  - Why needed here: Understanding what symmetry means linguistically is prerequisite to evaluating model performance on symmetry tasks.
  - Quick check question: Can you explain why "John married Mary" and "Mary married John" are symmetrical, but "John kissed Mary" and "Mary kissed John" may not be?
- Concept: In-context learning (ICL)
  - Why needed here: The entire evaluation methodology depends on models learning from prompts without retraining.
  - Quick check question: How does ICL differ from traditional fine-tuning, and why is this significant for this study?
- Concept: Natural Language Inference (NLI)
  - Why needed here: Symmetry evaluation is related to NLI tasks, and understanding this connection helps contextualize results.
  - Quick check question: How might symmetry reasoning enhance performance on NLI tasks?

## Architecture Onboarding

- Component map: Dataset (SIS) -> Prompt Design -> Conversational Chatbots (LLM-powered) -> Score Extraction -> Correlation Computation -> Analysis
- Critical path: Prompt → LLM processing → Score extraction → Correlation computation → Analysis
- Design tradeoffs: Using ICL avoids fine-tuning costs but may yield less consistent results than specialized models; choosing public chatbots enables reproducibility but limits control over model versions
- Failure signatures: Poor understanding of symmetry definition, inconsistent scoring across trials, inability to provide justifications, low correlation with human judgments
- First 3 experiments:
  1. Test prompt comprehension with simple symmetrical and non-symmetrical sentence pairs
  2. Run a small subset of SIS dataset to check scoring consistency across multiple trials
  3. Compare model scores against human majority vote on a validation set before full-scale evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different architectures of LLMs affect their ability to perform symmetrical reasoning tasks?
- Basis in paper: [inferred] The paper mentions that the success of LLMs is based on a combination of training data quality, architectures, optimization techniques, and computational resources, but does not explicitly compare different architectures.
- Why unresolved: The study does not provide a comparative analysis of LLM architectures regarding their symmetrical reasoning capabilities.
- What evidence would resolve it: A systematic study comparing the performance of various LLM architectures on the same symmetrical reasoning tasks would provide insights into the impact of architecture on performance.

### Open Question 2
- Question: Can the performance of LLMs on symmetrical reasoning tasks be improved through fine-tuning?
- Basis in paper: [explicit] The paper discusses in-context learning (ICL) but does not explore the effects of fine-tuning on symmetrical reasoning tasks.
- Why unresolved: The study focuses on ICL without exploring whether fine-tuning could enhance performance.
- What evidence would resolve it: Conducting experiments where LLMs are fine-tuned specifically on symmetrical reasoning tasks and comparing their performance with ICL results would clarify the impact of fine-tuning.

### Open Question 3
- Question: What are the implications of the stochasticity observed in chatbot responses for their reliability in real-world applications?
- Basis in paper: [explicit] The paper notes the stochastic nature of conversational chatbots and observes variability in their responses.
- Why unresolved: The study does not investigate the broader implications of this variability for practical applications.
- What evidence would resolve it: A detailed analysis of the consistency and reliability of chatbot responses across multiple trials and applications would provide insights into their real-world usability.

## Limitations

- The study relies on a single dataset (SIS) without systematic error analysis of individual symmetry cases
- Human evaluation process and dataset availability are not fully specified, limiting reproducibility
- The mechanisms underlying why certain models succeed or fail remain largely unexplained

## Confidence

- **High confidence**: Gemini Advanced's superior performance and ability to provide detailed justifications for symmetry judgments
- **Medium confidence**: General claim that some chatbots can perform symmetry reasoning, given observed variability across models
- **Low confidence**: Claims about reasoning mechanisms being "comparable to fine-tuned models" without direct benchmarking

## Next Checks

1. Conduct systematic error analysis on symmetry cases where models disagree with human judgments to identify systematic failure patterns
2. Test model performance across multiple versions and temperature settings to establish stability of symmetry judgments
3. Benchmark against explicitly fine-tuned symmetry models on the same dataset to validate comparative claims about ICL performance