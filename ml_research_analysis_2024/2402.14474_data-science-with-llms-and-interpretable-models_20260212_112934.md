---
ver: rpa2
title: Data Science with LLMs and Interpretable Models
arxiv_id: '2402.14474'
source_url: https://arxiv.org/abs/2402.14474
tags:
- graphs
- graph
- gpt-4
- interpretable
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using large language models (LLMs) to work
  with interpretable machine learning models, specifically Generalized Additive Models
  (GAMs). The authors show that GPT-4 can effectively describe, interpret, and debug
  GAMs by analyzing individual feature graphs converted to JSON format.
---

# Data Science with LLMs and Interpretable Models

## Quick Facts
- arXiv ID: 2402.14474
- Source URL: https://arxiv.org/abs/2402.14474
- Authors: Sebastian Bordt; Ben Lengerich; Harsha Nori; Rich Caruana
- Reference count: 17
- Key outcome: GPT-4 can effectively describe, interpret, and debug GAMs by analyzing individual feature graphs converted to JSON format

## Executive Summary
This paper investigates using large language models (LLMs) to work with interpretable machine learning models, specifically Generalized Additive Models (GAMs). The authors demonstrate that GPT-4 can effectively describe, interpret, and debug GAMs by analyzing individual feature graphs converted to JSON format. Key experiments show GPT-4's ability to read values from graphs (64/75 success rate), identify monotonicity (11/12 correct), and find largest jumps (27/31 correct). The approach enables dataset summarization, question answering, and model critique while staying within context window limits. The authors release TalkToEBM as an open-source LLM-GAM interface and discuss the potential for improving domain expert interaction with interpretable models.

## Method Summary
The authors convert GAM graphs to JSON format and provide them to GPT-4 with chain-of-thought prompting. They train GAMs on various datasets using interpretml, convert individual feature graphs to JSON with key-value pairs, and query GPT-4 with structured prompts to perform interpretation tasks. The approach analyzes one graph at a time to stay within context window limits while enabling analysis of large models with many features.

## Key Results
- GPT-4 reads values from GAM graphs with 64/75 accuracy (85.3%)
- GPT-4 correctly identifies monotonicity in 11/12 cases (91.7%)
- GPT-4 finds largest jumps in 27/31 graphs (87.1%)
- The LLM can generate accurate model summaries and detect anomalies
- The TalkToEBM interface enables question answering and model critique

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can accurately interpret GAM graphs because the JSON representation preserves essential structure of univariate functions in a format LLMs are trained to parse
- Mechanism: JSON format converts each graph into discrete intervals with contribution values, creating structured textual representation that mirrors how LLMs process tabular data during pretraining
- Core assumption: LLMs can maintain semantic understanding when statistical patterns are represented as structured key-value pairs
- Evidence anchors:
  - [abstract] "Combining the flexibility of LLMs with the breadth of statistical patterns accurately described by GAMs enables dataset summarization, question answering, and model critique"
  - [section] "The individual graphs are much more compact: the maximum description length of a single graph in the pneumonia GAM is 2,345 GPT-4 tokens"
- Break condition: Mechanism breaks when graphs become too complex or JSON structure becomes too nested for effective parsing

### Mechanism 2
- Claim: Chain-of-thought prompting enables multi-step reasoning on statistical patterns by first describing general patterns before answering specific questions
- Mechanism: Separating graph description from analysis tasks allows LLM to establish mental model of relationship before performing complex reasoning
- Core assumption: LLMs can maintain context about graph structure across multiple prompt-response exchanges
- Evidence anchors:
  - [section] "We engage in chain-of-thought (CoT) prompting (Wei et al. 2022), first asking the LLM to describe the general pattern of the graph, then asking more specific questions"
- Break condition: Mechanism breaks when conversation history exceeds context window limits or LLM loses track of graph structure

### Mechanism 3
- Claim: Additivity of GAMs allows LLMs to analyze complex models feature-by-feature without exceeding context window limits
- Mechanism: GAMs decompose complex relationships into additive univariate components, allowing LLMs to analyze each component separately and aggregate insights
- Core assumption: Individual feature interpretations can be meaningfully combined to form accurate global model summaries
- Evidence anchors:
  - [section] "By analyzing the graphs of a GAM one at a time, the LLM can analyze large models with many different features, without ever having the entire model fit into the context window"
- Break condition: Mechanism breaks when feature interactions are important and cannot be adequately captured through additive decomposition

## Foundational Learning

- Concept: Generalized Additive Models (GAMs)
  - Why needed here: Understanding that GAMs represent complex outcomes as sums of univariate functions is essential for grasping why LLMs can process them feature-by-feature
  - Quick check question: What makes GAMs particularly suitable for LLM interpretation compared to other interpretable models?

- Concept: Chain-of-thought prompting
  - Why needed here: This technique enables multi-step reasoning that's crucial for tasks like anomaly detection and monotonicity assessment
  - Quick check question: How does separating graph description from analysis improve the LLM's ability to detect surprising patterns?

- Concept: JSON data representation
  - Why needed here: The structured format makes statistical patterns accessible to LLMs, bridging gap between mathematical models and language model processing
  - Quick check question: Why might JSON be more effective than raw mathematical notation for LLM interpretation of statistical models?

## Architecture Onboarding

- Component map: Data source → GAM training → JSON conversion → LLM API (GPT-4) → Prompt template → Response analysis → User interface
- Critical path:
  1. Train GAM on dataset
  2. Convert each feature's graph to JSON format
  3. Apply chain-of-thought prompting sequence
  4. Aggregate individual graph interpretations
  5. Generate model-level summary
- Design tradeoffs:
  - Context window vs. model completeness: Analyzing one graph at a time enables large model interpretation but may miss interactions
  - Prompt complexity vs. response quality: More detailed prompts improve accuracy but increase token usage
  - JSON precision vs. readability: Higher decimal precision improves accuracy but makes patterns harder for LLM to parse
- Failure signatures:
  - Incorrect value reading (especially with high decimal precision)
  - Misclassification of monotonicity in nearly-monotone graphs
  - Hallucination of patterns not present in actual graph
  - Loss of context when processing many graphs sequentially
- First 3 experiments:
  1. Value reading test: Provide JSON graph with known values at specific points and verify LLM can read them correctly
  2. Monotonicity assessment: Test LLM's ability to identify monotone vs. non-monotone graphs across different feature types
  3. Pattern description: Evaluate LLM's ability to describe general patterns before asking specific analytical questions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LLM performance on graph interpretation tasks scale with graph complexity (number of intervals, decimal precision, non-monotonic patterns)?
- Basis in paper: [inferred] Paper shows performance on 31 graphs but doesn't systematically vary complexity
- Why unresolved: Paper only tested on fixed set of 31 graphs without varying complexity parameters
- What evidence would resolve it: Systematic experiments varying graph complexity parameters while measuring accuracy

### Open Question 2
- Question: Under what specific conditions do LLM responses become hallucinated rather than grounded in the provided model?
- Basis in paper: [explicit] "In summary, while the evidence from our simple qualitative experiments is far from conclusive, they do demonstrate that the responses of GPT-4 are frequently grounded in the provided interpretable model"
- Why unresolved: Paper only provides preliminary qualitative evidence about grounding vs hallucination
- What evidence would resolve it: Controlled experiments varying prompt structure, graph complexity, and domain knowledge

### Open Question 3
- Question: How does prompt tuning affect LLM performance on interpretable model tasks, and what is optimal prompt structure?
- Basis in paper: [explicit] "Although we gave only limited effort to prompt tuning, the results already are reasonably strong and probably could be further improved with more advanced prompting strategies"
- Why unresolved: Paper used basic chain-of-thought prompting but didn't explore advanced prompt engineering techniques
- What evidence would resolve it: Systematic comparison of different prompt structures and tuning strategies while measuring performance improvements

## Limitations
- The JSON representation may not scale well to extremely high-dimensional or complex feature relationships
- The approach relies heavily on careful prompt engineering that may not generalize across different statistical models or LLM architectures
- Evaluation focuses primarily on accuracy metrics without deeper analysis of potential hallucination risks

## Confidence
- **High Confidence**: GPT-4 can read values from GAM graphs and identify monotonicity with reasonable accuracy (64/75 and 11/12 success rates)
- **Medium Confidence**: Approach enables meaningful model critique and anomaly detection, but requires more extensive validation
- **Low Confidence**: JSON representation preserves all essential statistical patterns for LLM interpretation needs further testing, particularly for edge cases

## Next Checks
1. **Robustness Testing**: Test the JSON-to-LLM pipeline with deliberately challenging graphs including high-precision values, sharp discontinuities, and nearly-monotone features to identify breaking points
2. **Cross-LLM Validation**: Compare results across multiple LLM architectures (GPT-4, Claude, LLaMA) to assess whether interpretability gains are specific to GPT-4's training or represent more general capability
3. **Interaction Detection**: Design experiments specifically targeting feature interaction detection to validate whether additive decomposition approach can identify when simple univariate analysis is insufficient