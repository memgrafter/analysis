---
ver: rpa2
title: 'Cyclic Sparse Training: Is it Enough?'
arxiv_id: '2406.02773'
source_url: https://arxiv.org/abs/2406.02773
tags:
- training
- cyclic
- mask
- pruning
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates why iterative pruning methods like LRR achieve
  strong performance on sparse networks and whether these benefits can be transferred
  to pruning at initialization (PaI) methods. The key finding is that the repeated
  cyclic training schedule, rather than improved mask identification or sparsity regularization,
  is the dominant factor enabling LRR's success.
---

# Cyclic Sparse Training: Is it Enough?

## Quick Facts
- arXiv ID: 2406.02773
- Source URL: https://arxiv.org/abs/2406.02773
- Reference count: 40
- This work investigates why iterative pruning methods like LRR achieve strong performance on sparse networks and whether these benefits can be transferred to pruning at initialization (PaI) methods.

## Executive Summary
This work challenges the prevailing hypothesis that iterative pruning methods like LRR succeed due to superior mask identification or sparsity regularization. Instead, the authors demonstrate that the repeated cyclic training schedule is the dominant factor enabling LRR's success. They show that cyclic training significantly boosts PaI methods' performance and even enables random masks to outperform LRR at low sparsity. However, at high sparsity, cyclic training alone is insufficient, and proper coupling between parameter initialization and sparse mask becomes critical for performance.

## Method Summary
The authors investigate sparse neural network training by comparing iterative pruning methods (LRR, WR, IMP) with pruning at initialization methods (SNIP, Synflow, random pruning) under cyclic training schedules. They propose SCULPT-ing as an alternative approach: cyclic training of any sparse mask followed by one-shot magnitude pruning to couple parameters and mask, then retraining. The method is evaluated across CIFAR10, CIFAR100, and ImageNet datasets using ResNet architectures at various sparsity levels (59% to 95%).

## Key Results
- Cyclic training is the dominant factor enabling LRR's success, not mask identification or sparsity regularization
- Cyclic training significantly boosts PaI methods' performance, with random masks outperforming LRR at low sparsity
- At high sparsity, proper coupling between parameter initialization and sparse mask is critical for performance
- SCULPT-ing matches LRR's high-sparsity performance while requiring fewer training cycles and less memory

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Repeated cyclic training schedule, not mask identification or sparsity regularization, is the dominant factor enabling LRR's success.
- Mechanism: Cyclic training enables better exploration of the loss landscape, allowing the optimization process to jump between local optima and find flatter minima associated with better generalization.
- Core assumption: The standard training schedule in LRR is more critical than its pruning operations for achieving state-of-the-art performance.
- Evidence anchors:
  - [abstract] "we challenge this hypothesis and instead posit that their repeated cyclic training schedules enable improved optimization"
  - [section 3] "we argue that cyclic training has a strong influence on LRR and also boosts dense training"
  - [corpus] Weak evidence - no direct citations about cyclic training mechanisms in related works

### Mechanism 2
- Claim: At high sparsity, proper coupling between parameter initialization and sparse mask is critical for performance.
- Mechanism: The initialization contains task-specific information that must align with the sparse mask structure for effective optimization.
- Core assumption: Lottery tickets exist not just as masks but as coupled pairs of masks and initializations that contain task-relevant information.
- Evidence anchors:
  - [section 4] "we find that cyclically training a supposedly superior sparse LRR mask with a random initialization does not surpass a cyclically trained random mask"
  - [section 5] "it is crucial to have an appropriate initialization for the sparse mask to improve performance at high sparsity"
  - [corpus] No direct evidence - this coupling concept is primarily supported by internal experiments

### Mechanism 3
- Claim: Magnitude pruning after cyclic training effectively couples parameters and mask for sparse training.
- Mechanism: The magnitude-based pruning step serves to align the learned parameters with the sparse mask structure, creating a coupled pair that can be effectively trained with a single cycle.
- Core assumption: Magnitude pruning minimally changes the neural network function while effectively selecting important parameters.
- Evidence anchors:
  - [section 5] "magnitude seems to be best suitable for realizing a good coupling between mask and its parameters"
  - [section 5] "magnitude based pruning minimally changes the neural network function"
  - [corpus] Weak evidence - the effectiveness of magnitude pruning for coupling is primarily demonstrated through internal experiments

## Foundational Learning

- Concept: Learning rate schedules and their impact on optimization dynamics
  - Why needed here: Understanding how cyclic training differs from standard training is crucial for grasping why it improves sparse network optimization
  - Quick check question: How does increasing the learning rate during training help escape local minima?

- Concept: Lottery Ticket Hypothesis and parameter initialization importance
  - Why needed here: The work challenges the assumption that masks alone contain task-specific information, emphasizing the importance of coupled initialization-mask pairs
  - Quick check question: Why might the same mask perform differently with different initializations?

- Concept: Linear mode connectivity analysis
  - Why needed here: This tool is used to understand whether consecutive training cycles find parameters in the same loss basin, indicating whether the optimization is making progress
  - Quick check question: What does it mean if two sets of parameters have linearly connected test loss but separated train loss?

## Architecture Onboarding

- Component map:
  Sparse mask generation (PaI methods: SNIP, Synflow, random) -> Cyclic training engine (multiple cycles with step-warmup learning rate schedule) -> Coupling mechanism (magnitude pruning step) -> Retraining phase (single cycle optimization) -> Evaluation framework (linear mode connectivity, sign flip analysis, generalization performance)

- Critical path:
  1. Generate sparse mask at initialization
  2. Apply cyclic training for optimal number of cycles
  3. Perform one-shot magnitude pruning
  4. Retrain with single cycle
  5. Evaluate performance and coupling quality

- Design tradeoffs:
  - Number of cyclic training cycles vs. computational cost
  - Sparsity level at which to start SCULPT-ing vs. final performance
  - Mask quality vs. initialization coupling strength
  - Memory footprint (sparse training from start vs. dense-then-prune)

- Failure signatures:
  - Poor generalization despite high training accuracy (likely insufficient cyclic training)
  - No performance improvement after coupling step (likely mask-initialization misalignment)
  - High sensitivity to random seed (likely unstable optimization dynamics)
  - Linear mode connectivity shows error barriers between cycles (likely stuck in local minima)

- First 3 experiments:
  1. Compare cyclic vs. standard training for random sparse mask on CIFAR10 to verify optimization benefits
  2. Test LRR mask with random initialization vs. warmup initialization to demonstrate coupling importance
  3. Apply SCULPT-ing to random mask starting from 70% sparsity on CIFAR10 to validate the full pipeline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does cyclic sparse training improve generalization primarily through better exploration of the loss landscape or through improved sign recovery?
- Basis in paper: [explicit] The paper identifies two potential mechanisms: escaping local optima to find flatter optima and improved sign recovery during training.
- Why unresolved: The paper provides evidence for both mechanisms but doesn't definitively determine which is more dominant.
- What evidence would resolve it: Comparative experiments isolating the effects of sign flips versus landscape exploration (e.g., using sign-frozen networks) would clarify which mechanism drives the majority of the generalization improvement.

### Open Question 2
- Question: Are lottery tickets truly universal across tasks at high sparsity levels, or is their performance task-dependent?
- Basis in paper: [explicit] The paper suggests that coupling between mask and initialization is crucial at high sparsity, but doesn't conclusively determine if LRR masks are inherently better than PaI masks.
- Why unresolved: While the paper demonstrates that proper coupling can recover LRR performance, it doesn't establish whether the LRR mask structure itself provides an advantage over other masks.
- What evidence would resolve it: Direct comparisons of LRR masks with optimal initializations versus other high-quality masks with optimal initializations across multiple tasks would reveal if mask structure or coupling is more important.

### Open Question 3
- Question: What is the optimal number of training cycles for cyclic sparse training at different sparsity levels?
- Basis in paper: [explicit] The paper shows that the number of cycles affects performance but doesn't provide a systematic method for determining the optimal cycle count.
- Why unresolved: The paper suggests that fewer cycles might be needed at higher sparsity but doesn't explore this relationship comprehensively.
- What evidence would resolve it: A detailed study mapping cycle count to performance across various sparsity levels and tasks would establish optimal training schedules.

## Limitations

- Limited external validation of the cyclic training mechanism through citations
- The coupling concept, while well-supported internally, lacks broader theoretical grounding
- SCULPT-ing's effectiveness at extreme sparsities (>95%) remains untested

## Confidence

- **High Confidence**: The cyclic training mechanism and its benefits for dense and sparse network optimization
- **Medium Confidence**: The coupling hypothesis and its importance for high-sparsity performance
- **Medium Confidence**: The SCULPT-ing approach as an effective alternative to iterative pruning

## Next Checks

1. Test SCULPT-ing on additional architectures (e.g., Vision Transformers) to verify its generalizability beyond ResNets
2. Compare linear mode connectivity between cyclic training cycles for LRR vs. SCULPT-ing to quantify the optimization benefits
3. Investigate whether the coupling mechanism extends to other initialization schemes beyond Kaiming Normal, such as Xavier or orthogonal initialization