---
ver: rpa2
title: 'TE-SSL: Time and Event-aware Self Supervised Learning for Alzheimer''s Disease
  Progression Analysis'
arxiv_id: '2407.06852'
source_url: https://arxiv.org/abs/2407.06852
tags:
- learning
- event
- disease
- alzheimer
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Time and Event-aware Self Supervised Learning
  (TE-SSL), a framework for Alzheimer's disease progression analysis that leverages
  both time-to-event and event labels as supervisory signals in self-supervised learning.
  The authors develop a novel contrastive learning approach that weighs samples based
  on their temporal proximity to enhance feature representation.
---

# TE-SSL: Time and Event-aware Self Supervised Learning for Alzheimer's Disease Progression Analysis

## Quick Facts
- arXiv ID: 2407.06852
- Source URL: https://arxiv.org/abs/2407.06852
- Reference count: 23
- C-td scores of 0.7873 and IBS scores of 0.1889 on ADNI dataset

## Executive Summary
This paper introduces Time and Event-aware Self Supervised Learning (TE-SSL), a framework for Alzheimer's disease progression analysis that leverages both time-to-event and event labels as supervisory signals in self-supervised learning. The authors develop a novel contrastive learning approach that weighs samples based on their temporal proximity to enhance feature representation. When evaluated on the ADNI dataset for time-to-event prediction, TE-SSL outperforms standard SSL and event-only approaches, achieving C-td scores of 0.7873 and IBS scores of 0.1889. The method demonstrates the value of incorporating temporal information in self-supervised learning for disease progression analysis.

## Method Summary
TE-SSL is a self-supervised learning framework that integrates time-to-event and event data as supervisory signals for contrastive learning. The method first pretrains a 3D CNN encoder using a modified contrastive loss that weighs positive pairs based on their temporal proximity (smaller time differences receive higher weights). After pretraining, the encoder is fine-tuned for downstream time-to-event prediction using the DeepHit framework, which outputs a discrete distribution over time points. The framework is evaluated on the ADNI dataset containing 493 patients with 2007 total visits, using 3D T1-weighted MRI scans.

## Key Results
- TE-SSL achieves C-td scores of 0.7873 on ADNI dataset, outperforming standard SSL and event-only approaches
- Integrated Brier Scores (IBS) of 0.1889 demonstrate superior calibration compared to baseline methods
- Time-aware weighting significantly improves representation learning by pulling samples closer together based on temporal proximity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Time-aware weighting improves representation learning by pulling samples closer together based on temporal proximity.
- Mechanism: The weight term ωi,j in Equation 4 assigns higher weights to pairs with smaller time differences (∆i,j), making their feature representations more similar during contrastive learning.
- Core assumption: Samples at similar stages of disease progression have more similar features than those at different stages.
- Evidence anchors:
  - [abstract]: "develops a novel contrastive learning approach that weighs samples based on their temporal proximity to enhance feature representation"
  - [section]: "We therefore hypothesize that utilizing both types of labels in SSL training enhances the learning process, leading to nuanced models that can accurately predict the event timing and occurrence"
- Break condition: If disease progression features do not correlate with time-to-event information, or if temporal ordering is non-monotonic in feature space.

### Mechanism 2
- Claim: Event labels provide supervisory signals that guide contrastive learning toward separating event and censored samples.
- Mechanism: The supervisory signal framework (Equation 2) modifies the standard SSL objective to pull positive samples (same event status) closer and push negative samples (different event status) apart.
- Core assumption: Event occurrence creates distinct feature distributions that can be learned through contrastive objectives.
- Evidence anchors:
  - [abstract]: "integrates time-to-event and event data as supervisory signals to refine the learning process"
  - [section]: "the SSL objective (in Eqn. 1) can be generalized as follows: LE-SSL" with P(i) representing sets of samples with matching event labels
- Break condition: If event labels do not create separable feature distributions, or if the event signal is too weak to guide learning.

### Mechanism 3
- Claim: Combining time-aware weighting with event supervision creates synergistic effects that outperform either approach alone.
- Mechanism: TE-SSL integrates both mechanisms (Equations 3 and 4), where time-based weights modulate the strength of event-based contrastive learning.
- Core assumption: Temporal proximity and event status provide complementary information that reinforces each other during representation learning.
- Evidence anchors:
  - [abstract]: "incorporates both event and time-to-event labels as additional supervisory signals" and "demonstrated an improvement in the downstream performance"
  - [section]: "Our frameworks outperform others in both C-td (higher is better) and IBS (lower is better) metrics"
- Break condition: If the combination creates conflicting optimization pressures or if one signal dominates and negates the benefit of the other.

## Foundational Learning

- Concept: Survival analysis with time-to-event data
  - Why needed here: The framework operates on data where each sample has both an event indicator (δi) and time-to-event (Ti), requiring understanding of censored data and survival functions
  - Quick check question: What is the difference between an event occurring and data being censored in survival analysis?

- Concept: Contrastive self-supervised learning
  - Why needed here: The core pretraining method uses contrastive learning to pull augmented views of the same sample together while pushing different samples apart
  - Quick check question: How does the standard InfoNCE loss work in contrastive learning frameworks?

- Concept: Representation learning and transfer learning
  - Why needed here: The framework first pretrains on SSL objectives then fine-tunes for downstream survival analysis, requiring understanding of how learned representations transfer
  - Quick check question: Why do we discard the projection head after pretraining and replace it with a task-specific head?

## Architecture Onboarding

- Component map: Input MRI → 3D CNN encoder → Projection head (pretraining) → Task head (fine-tuning) → Output distribution over discrete time points
- Critical path: Data preprocessing → Pretraining with TE-SSL loss → Fine-tuning with DeepHit loss → Evaluation
- Design tradeoffs: Large batch sizes (128) simulated through gradient accumulation vs. memory constraints; temperature parameter τ = 0.07 vs. sensitivity to scaling
- Failure signatures: Poor downstream performance may indicate insufficient pretraining (too few epochs), suboptimal temperature, or inadequate batch size; time-weighting may fail if temporal differences are not meaningful
- First 3 experiments:
  1. Baseline: Run No Pretraining model to establish performance floor
  2. Ablation: Run standard SSL (without time or event supervision) to measure benefit of supervisory signals
  3. Component isolation: Run E-SSL (event-only supervision) to isolate effect of temporal weighting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal weighting function between time-to-event differences for TE-SSL?
- Basis in paper: [explicit] The authors acknowledge that α and β parameters define the intensity of weight values and conduct ablation studies exploring different configurations, but do not establish optimal values.
- Why unresolved: The ablation analysis only tests a limited range of α and β values (1 ≤ α ≤ 1.5, 0.5 ≤ β ≤ 1) and shows relative stability within this range, but does not identify optimal values or provide theoretical justification for the chosen ranges.
- What evidence would resolve it: A comprehensive grid search or optimization procedure across a broader parameter space, potentially coupled with theoretical analysis of the weighting function's properties.

### Open Question 2
- Question: How does TE-SSL perform on other neurodegenerative diseases beyond Alzheimer's?
- Basis in paper: [inferred] The authors demonstrate TE-SSL's effectiveness on Alzheimer's progression analysis but do not evaluate its generalizability to other diseases with time-to-event labels.
- Why unresolved: The study is limited to the ADNI dataset and Alzheimer's disease, leaving open whether the method's benefits extend to other conditions like Parkinson's disease, Huntington's disease, or other neurodegenerative disorders.
- What evidence would resolve it: Application and evaluation of TE-SSL on datasets for other neurodegenerative diseases with similar time-to-event labels, comparing performance against both standard SSL and disease-specific baselines.

### Open Question 3
- Question: What is the minimum required dataset size for TE-SSL to outperform standard SSL?
- Basis in paper: [explicit] The authors note that contrastive SSL techniques require large batch sizes and simulate a batch size of 128 through gradient accumulation, but do not analyze performance as a function of dataset size.
- Why unresolved: While the authors demonstrate TE-SSL's superiority on their dataset of 493 patients with 2007 data points, they do not investigate how performance scales with smaller or larger datasets, or establish a threshold where TE-SSL's advantages become apparent.
- What evidence would resolve it: Systematic evaluation of TE-SSL across datasets of varying sizes, identifying the point at which the additional complexity of time and event-aware weighting provides meaningful improvements over simpler SSL approaches.

## Limitations
- Temporal weighting sensitivity depends critically on appropriate choice of hyperparameters (α, β) and the assumption that temporal proximity correlates with feature similarity in disease progression space
- Domain specificity to Alzheimer's disease progression may not generalize to other neurodegenerative diseases or conditions with different progression patterns
- Implementation complexity introduces additional computational overhead and hyperparameter tuning requirements compared to standard SSL approaches

## Confidence
- High confidence: The core hypothesis that combining time-to-event and event labels provides supervisory signals for contrastive learning
- Medium confidence: The claim that time-aware weighting significantly improves representation learning beyond event-only supervision
- Medium confidence: The synergistic effect claim that combining both mechanisms outperforms either alone

## Next Checks
1. **Ablation study extension**: Systematically vary α and β parameters in the weighting scheme across a wider range (e.g., 0.1 to 10) to establish robustness and identify optimal values for different disease stages

2. **Cross-dataset generalization**: Evaluate TE-SSL on an independent Alzheimer's dataset (e.g., AIBL or OASIS) to verify that improvements generalize beyond the training domain and are not artifacts of dataset-specific characteristics

3. **Temporal sensitivity analysis**: Conduct experiments where temporal information is intentionally corrupted (e.g., shuffled time-to-event values) to quantify how much performance degradation occurs, validating the importance of accurate temporal supervision