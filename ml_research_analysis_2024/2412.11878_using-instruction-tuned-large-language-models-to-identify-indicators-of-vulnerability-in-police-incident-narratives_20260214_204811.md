---
ver: rpa2
title: Using Instruction-Tuned Large Language Models to Identify Indicators of Vulnerability
  in Police Incident Narratives
arxiv_id: '2412.11878'
source_url: https://arxiv.org/abs/2412.11878
tags:
- labels
- https
- classi
- evidence
- positive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that instruction-tuned large language models
  (IT-LLMs) can effectively augment human qualitative coding of police incident narratives
  to identify vulnerability indicators. Custom prompts significantly improved model
  performance, with an 8-billion parameter model achieving F1 scores comparable to
  GPT-4o.
---

# Using Instruction-Tuned Large Language Models to Identify Indicators of Vulnerability in Police Incident Narratives

## Quick Facts
- arXiv ID: 2412.11878
- Source URL: https://arxiv.org/abs/2412.11878
- Authors: Sam Relins; Daniel Birks; Charlie Lloyd
- Reference count: 40
- Key outcome: Instruction-tuned LLMs achieved F1 scores comparable to GPT-4o for vulnerability screening in police incident narratives

## Executive Summary
This study evaluates the effectiveness of instruction-tuned large language models (IT-LLMs) for identifying vulnerability indicators in police incident narratives. The researchers tested various model sizes and custom prompts on a dataset of 6,616 real police incident narratives, comparing model performance against human-coded labels. The findings demonstrate that IT-LLMs can achieve high accuracy in screening narratives without vulnerability indicators (>95% alignment), while showing comparable performance to GPT-4o on positive classifications. Custom prompts significantly improved model performance, with an 8-billion parameter model performing particularly well. The study also conducted counterfactual analyses to examine potential demographic biases, finding minimal effects across most protected characteristics.

## Method Summary
The researchers utilized a dataset of 6,616 police incident narratives from 2021-2022, each labeled for 17 different vulnerability indicators by trained coders. They tested six different IT-LLMs ranging from 3 to 70 billion parameters, comparing their performance against GPT-4o as a baseline. Custom prompts were developed and tested against zero-shot approaches, with the best-performing prompt combining structured fields with free-text description. Models were evaluated using F1 scores, alignment with human labels, and confusion matrices. A counterfactual analysis was conducted by systematically modifying narratives to include demographic attributes (race, age, gender, disability, homelessness, Indigenous status) and re-evaluating model outputs to assess potential bias.

## Key Results
- Instruction-tuned LLMs achieved F1 scores of 0.72-0.74 for "any indicator present" classification, comparable to GPT-4o
- Models excelled at negative screening, achieving >95% alignment with human labels for unanimous negative classifications
- Custom prompts improved performance by 10-30% compared to zero-shot approaches
- Counterfactual analyses revealed minimal demographic bias, with effects generally <5% and few statistically significant after correction

## Why This Works (Mechanism)
Instruction-tuned LLMs leverage their pre-trained understanding of language patterns combined with fine-tuning on specific tasks to recognize nuanced indicators of vulnerability in text. The instruction tuning process enables models to better interpret complex prompts and follow structured output formats, which is crucial for extracting specific vulnerability indicators from unstructured police narratives. The models' ability to process long sequences allows them to consider contextual information across entire incident descriptions, improving accuracy in identifying subtle vulnerability indicators that may not be explicitly stated.

## Foundational Learning
- Vulnerability indicator identification: Understanding the 17 specific indicators used (mental health, domestic violence, etc.) is essential for interpreting model performance metrics and limitations
- Instruction tuning: This process adapts pre-trained LLMs to follow specific instructions and output formats, critical for task-specific performance
- Counterfactual analysis: Systematic modification of input data to test for bias, necessary for evaluating fairness in automated decision systems
- F1 score and alignment metrics: These performance measures provide standardized ways to compare model accuracy against human coding
- Prompt engineering: The design and optimization of input prompts significantly impacts model performance on specific tasks

## Architecture Onboarding

**Component Map:**
Police Incident Narratives -> Preprocessing (OCR/Cleaning) -> IT-LLM Model (3B-70B) -> Structured Output (Vulnerability Indicators) -> Performance Evaluation (F1, Alignment) -> Counterfactual Analysis (Bias Testing)

**Critical Path:**
Raw text narratives → Prompt formatting → Model inference → Vulnerability indicator extraction → Performance comparison with human labels → Bias analysis

**Design Tradeoffs:**
- Model size vs. computational cost: Larger models show better performance but require more resources
- Prompt complexity vs. interpretability: Structured prompts improve accuracy but may limit flexibility
- Binary classification vs. multi-label: Simplifies evaluation but may miss nuanced vulnerability patterns

**Failure Signatures:**
- High false negative rates in ambiguous cases
- Systematic misclassification of certain vulnerability types
- Performance degradation on narratives with complex language or cultural references

**First Experiments to Run:**
1. Test model performance on a holdout set of narratives with high inter-rater disagreement
2. Evaluate prompt sensitivity by varying prompt structure while keeping model and data constant
3. Conduct ablation studies removing individual vulnerability indicators to assess model dependency

## Open Questions the Paper Calls Out
None

## Limitations
- The study relied on a single human-coded dataset from a specific jurisdiction, limiting generalizability to other contexts
- No inter-rater reliability metrics were reported for the original human coding, creating uncertainty about ground truth quality
- Counterfactual demographic analysis used small sample sizes (n=40) per attribute combination, potentially limiting power to detect subtle biases
- The study focused on binary classification without examining potential false negatives that could have serious real-world consequences

## Confidence

**Model Performance Claims**: High
- Well-supported by reported F1 scores, alignment metrics, and comparative analysis

**Demographic Bias Findings**: Medium
- Methodology is rigorous but small sample sizes and single-attribute modifications may miss complex intersectional effects

**Practical Implementation Recommendations**: Low
- Assumes stable performance across different datasets and operational contexts not tested in this study

## Next Checks

1. Conduct inter-rater reliability analysis on the human-coded dataset to establish ground truth quality and quantify inherent coding uncertainty.

2. Test model performance on incident narratives from multiple police departments across different geographic regions and demographic compositions to assess generalizability.

3. Implement a formal error analysis framework examining false negative rates specifically, including their potential real-world consequences and comparison with human error patterns in similar screening tasks.