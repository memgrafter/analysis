---
ver: rpa2
title: Dissecting Query-Key Interaction in Vision Transformers
arxiv_id: '2405.14880'
source_url: https://arxiv.org/abs/2405.14880
tags:
- singular
- attention
- tokens
- layers
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how self-attention in vision transformers
  (ViTs) processes tokens and contextual information. The authors use singular value
  decomposition (SVD) of the query-key interaction matrix to analyze how tokens attend
  to similar vs.
---

# Dissecting Query-Key Interaction in Vision Transformers

## Quick Facts
- arXiv ID: 2405.14880
- Source URL: https://arxiv.org/abs/2405.14880
- Authors: Xu Pan; Aaron Philip; Ziqian Xie; Odelia Schwartz
- Reference count: 40
- Key outcome: SVD of query-key interaction matrices reveals interpretable semantic patterns in vision transformer attention

## Executive Summary
This paper investigates how self-attention mechanisms in vision transformers process tokens and contextual information by analyzing the query-key interaction matrix using singular value decomposition (SVD). The authors find that early layers tend to attend to similar tokens (grouping), while later layers attend more to dissimilar tokens (contextualization). The SVD approach decomposes attention into interpretable feature directions, revealing semantic interactions like part-to-part, object-to-object, and foreground-to-background attention. These patterns vary by model architecture and training objective, with self-supervised models sometimes showing different trends.

## Method Summary
The method analyzes pre-trained vision transformer models by computing the SVD of the query-key interaction matrix (QK⊤) for each attention layer. The singular value decomposition factorizes the bilinear form Wq⊤Wk into UΣV⊤, where each singular mode {un, σn, vn} represents a pair of feature directions - one for queries and one for keys - that interact in attention. The authors compute cosine similarity between left and right singular vectors to measure alignment preferences, visualize optimal attention images to interpret semantic patterns, and analyze how these interactions evolve across layers and vary across different training objectives.

## Key Results
- Early layers show higher cosine similarity between singular vector pairs (grouping), while later layers show decreased similarity (contextualization)
- Many singular modes reveal interpretable semantic interactions including part-to-part, object-to-object, and foreground-to-background attention
- Self-supervised models like SimMIM and DINO exhibit different patterns, with some increasing attention to similar features in later layers
- The shift from grouping to contextualization varies by model architecture and training objective

## Why This Works (Mechanism)

### Mechanism 1
SVD of the query-key interaction matrix decomposes attention into interpretable feature directions. The bilinear form Wq⊤Wk is factorized into UΣV⊤, where each singular mode {un, σn, vn} represents a pair of feature directions - one for queries (un) and one for keys (vn) - that interact in attention. Left and right singular vectors capture meaningful semantic features rather than arbitrary basis vectors.

### Mechanism 2
Early layers show higher cosine similarity between singular vector pairs, indicating grouping behavior. When left and right singular vectors are aligned (high cosine similarity), tokens attend to similar features; early layers have higher alignment. Cosine similarity between singular vectors indicates similarity preference in attention.

### Mechanism 3
Singular modes reveal semantic interactions like part-to-part, object-to-object, and foreground-to-background attention. Optimal attention images (images maximizing attention score for each mode) show query and key maps highlighting semantically related regions. Projection of embeddings onto singular vectors reveals interpretable visual features.

## Foundational Learning

- Concept: Singular Value Decomposition
  - Why needed here: SVD decomposes the query-key interaction matrix to reveal interpretable feature directions and their interactions
  - Quick check question: What does each singular mode {un, σn, vn} represent in the context of attention?

- Concept: Cosine similarity
  - Why needed here: Used to measure alignment between left and right singular vectors to determine if attention prefers similar or dissimilar tokens
  - Quick check question: What does high vs. low cosine similarity between singular vector pairs indicate about attention behavior?

- Concept: Attention score formulation
  - Why needed here: Understanding x⊤i Wq⊤Wkxj = Σ σn x⊤i un v⊤n xj shows how singular modes contribute to total attention
  - Quick check question: How does the attention score decompose across singular modes?

## Architecture Onboarding

- Component map: Input → LayerNorm → Query/Key/Value projection → SVD analysis of QK⊤ → Attention computation → Output
- Critical path: Token embedding → Self-attention layers → SVD decomposition of query-key interaction → Semantic interpretation of singular modes
- Design tradeoffs: SVD provides interpretability but adds computational overhead; singular vectors may be less stable than raw embeddings
- Failure signatures: Nonsensical singular modes, inconsistent semantic patterns, high anisotropy masking true similarity preferences
- First 3 experiments:
  1. Compute cosine similarity between singular vector pairs across layers for a base ViT model
  2. Visualize optimal attention images for top singular modes to check for semantic patterns
  3. Compare grouping vs. contextualization trends across different training objectives (supervised vs. self-supervised)

## Open Questions the Paper Calls Out

### Open Question 1
How do different training objectives (self-supervised vs. supervised) fundamentally alter the query-key interaction patterns in vision transformers? The paper shows that self-supervised models like SimMIM and DINO exhibit different patterns in their attention to similar vs. dissimilar tokens compared to supervised classification models, particularly in later layers. While the paper observes these differences, it doesn't provide a mechanistic explanation for why training objectives lead to such distinct interaction patterns.

### Open Question 2
Can the singular value decomposition approach be extended to analyze token interactions in multi-modal transformers or transformers trained on non-visual data? The paper suggests that the SVD method "can be easily adapted to study token interactions in transformer networks trained on other modalities like language" but doesn't demonstrate this extension. Different modalities may have fundamentally different interaction patterns that the current approach doesn't capture.

### Open Question 3
What is the relationship between the semantic interpretability of singular modes and their contribution to model performance on downstream tasks? The paper shows that many singular modes are semantically interpretable but doesn't investigate whether these interpretable modes are actually important for task performance or if they're merely correlated with the model's internal representations.

## Limitations
- The interpretability of singular vectors is not empirically validated across diverse model architectures
- Observed patterns may be influenced by dataset-specific biases rather than fundamental architectural principles
- The semantic interpretation of singular modes is qualitative and subjective, lacking rigorous quantitative validation

## Confidence

- High confidence: The mathematical framework of using SVD to decompose query-key interactions is sound and well-established
- Medium confidence: The observed layer-wise patterns of grouping-to-contextualization shifts are reproducible across models
- Medium confidence: The qualitative interpretation of singular modes as representing semantic interactions
- Low confidence: The generalizability of these patterns across different architectures and training objectives

## Next Checks

1. Perform ablation studies on token selection methodology to verify that observed patterns are not artifacts of specific token sampling strategies
2. Test the robustness of SVD-based interpretability across different random seeds and initialization schemes to establish statistical significance
3. Compare SVD-derived attention patterns with ground-truth semantic annotations on benchmark datasets to quantitatively validate the interpretability claims