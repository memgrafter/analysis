---
ver: rpa2
title: Is All Learning (Natural) Gradient Descent?
arxiv_id: '2409.16422'
source_url: https://arxiv.org/abs/2409.16422
tags:
- gradient
- learning
- loss
- descent
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes that effective learning rules, defined as
  those improving a scalar performance measure over time, can be universally expressed
  as natural gradient descent with a suitably defined loss function and metric. The
  key method involves proving that parameter updates in such learning rules can always
  be decomposed into the product of a symmetric positive definite matrix (the metric)
  and the negative gradient of a loss function.
---

# Is All Learning (Natural) Gradient Descent?

## Quick Facts
- arXiv ID: 2409.16422
- Source URL: https://arxiv.org/abs/2409.16422
- Authors: Lucas Shoji; Kenta Suzuki; Leo Kozachkov
- Reference count: 32
- Primary result: Any effective learning rule can be universally expressed as natural gradient descent with a suitably defined loss function and metric.

## Executive Summary
This paper proves that effective learning rules—those that improve a scalar performance measure over time—can always be expressed as natural gradient descent with an appropriately constructed loss function and metric. The authors show that parameter updates in such learning rules can be decomposed into the product of a symmetric positive definite matrix (the metric) and the negative gradient of a loss function. They derive a canonical form for these metrics and identify optimal variants, including one that achieves the minimum possible condition number. The work generalizes previous results to include discrete-time and stochastic systems, time-varying losses, and learning rules that don't require monotonic improvement.

## Method Summary
The authors use elementary linear algebra and calculus to prove their main results. They construct a symmetric positive definite matrix M such that the parameter update g equals -M⁻¹ times the negative gradient of a loss function. This is achieved by defining M in a canonical form using the update vector and its orthogonal complement. For discrete-time learning rules, they use Taylor's theorem to show that sufficiently small updates can be approximated using a discrete gradient definition. The proofs rely on the assumption that learning rules are "effective" (decreasing loss over some time window) and that updates are sufficiently small for Taylor approximations to hold.

## Key Results
- Effective learning rules can be universally expressed as natural gradient descent with a suitably defined loss function and metric
- The metric M has a canonical form: M = (1/y⊤g)yy⊤ + sum of ui u⊤i where ui span the subspace orthogonal to g
- An optimal metric exists that achieves the minimum possible condition number, with α = y⊤y/g⊤y
- Both linear time-invariant dynamics and feedback alignment can be reformulated as natural gradient descent

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Any effective learning rule can be rewritten as natural gradient descent with a suitably defined loss function and metric.
- Mechanism: The proof constructs a symmetric positive definite matrix M such that the parameter update g equals -M⁻¹ times the negative gradient of a loss function. This is done by defining M = (1/y⊤g)yy⊤ + sum of ui u⊤i where ui span the subspace orthogonal to g, ensuring M maps g to y and is positive definite.
- Core assumption: The learning rule is "effective" meaning L(t+m) < L(t) for some time window m, and updates are sufficiently small.
- Evidence anchors: [abstract] "parameter updates within this class of learning rules can be expressed as the product of a symmetric positive definite matrix (i.e., a metric) and the negative gradient of a loss function."
- Break condition: If the learning rule does not decrease the loss function over any time window, or if updates are too large for the Taylor approximation to hold.

### Mechanism 2
- Claim: For discrete-time learning rules with sufficiently small learning rates, the updates can be written in natural gradient form using a discrete gradient definition.
- Mechanism: The proof uses Taylor's theorem to show that for small η, the discrete gradient can be approximated by the continuous gradient plus a Hessian term. This allows construction of a metric M that satisfies the natural gradient form.
- Core assumption: The loss function is twice continuously differentiable and the learning rate η is sufficiently small.
- Evidence anchors: [section] "for smooth loss functions L and sufficiently small η, it is possible to construct at every time t a symmetric positive definite matrix M... such that g(θt) = -M⁻¹∇θL(θt)"
- Break condition: If the learning rate is too large, or if the loss function is not sufficiently smooth for the Taylor approximation to be valid.

### Mechanism 3
- Claim: The metric M has a canonical form and optimal variants exist, including one that achieves the minimum possible condition number.
- Mechanism: The proof shows that any valid metric must be of the form M = (1/y⊤g)yy⊤ + sum of ui u⊤i. It then identifies optimal metrics within this family, particularly when the coefficient α is chosen to minimize the condition number.
- Core assumption: The metric must be symmetric positive definite and map the update vector g to the negative gradient y.
- Evidence anchors: [section] "we show that any symmetric, positive definite matrix M such that Mg = y, with g⊤y > 0, is of the form given in (6)"
- Break condition: If the angle between y and g approaches π/2, the metric becomes ill-conditioned as the smallest eigenvalue approaches zero.

## Foundational Learning

- Concept: Gradient descent and natural gradient descent
  - Why needed here: The paper builds on these optimization methods to show that any effective learning rule can be expressed in natural gradient form
  - Quick check question: What is the key difference between gradient descent and natural gradient descent in terms of the geometry they optimize over?

- Concept: Positive definite matrices and metrics
  - Why needed here: The proof relies on constructing symmetric positive definite matrices that serve as metrics for the natural gradient descent
  - Quick check question: Why must the matrix M be positive definite for the natural gradient descent to work correctly?

- Concept: Taylor's theorem and discrete gradients
  - Why needed here: The proof for discrete-time learning rules uses Taylor's theorem to relate discrete gradients to continuous gradients
  - Quick check question: How does Taylor's theorem allow us to approximate the difference between L(x+p) and L(x) when p is small?

## Architecture Onboarding

- Component map:
  Learning rule analyzer -> Metric constructor -> Natural gradient formatter -> Condition number optimizer

- Critical path:
  1. Verify the learning rule decreases loss over some time window
  2. Construct the metric M using the canonical form
  3. Verify M is positive definite and maps g to y
  4. (Optional) Optimize M to minimize condition number
  5. Express the learning rule as -M⁻¹∇L

- Design tradeoffs:
  - Exact vs. approximate metrics: Exact metrics require computing the full eigendecomposition, while approximate metrics may be faster but less accurate
  - Global vs. local optimality: The minimum condition number metric is globally optimal but may be more expensive to compute than other choices

- Failure signatures:
  - Non-positive definite M: Indicates the learning rule is not effective or updates are too large
  - Large condition number: May lead to numerical instability in the natural gradient computation
  - Poor approximation of discrete gradients: Suggests the learning rate is too large for the Taylor approximation to hold

- First 3 experiments:
  1. Test with a simple gradient descent rule on a quadratic loss function
  2. Test with a non-gradient learning rule like feedback alignment on a small neural network
  3. Test with a time-varying loss function to verify the extended parameter space approach works

## Open Questions the Paper Calls Out
None

## Limitations
- The framework assumes learning rules are "effective" (decreasing loss over some time window), excluding non-monotonic exploration methods
- The proofs rely on Taylor approximations that require sufficiently small updates, limiting applicability to learning rules with large step sizes
- The work doesn't fully explore how common optimization techniques like momentum can be expressed in the natural gradient framework

## Confidence
- Main claims: High - The mathematical proofs are rigorous and follow from well-established results in linear algebra and calculus
- Extension to discrete-time learning rules: Medium - The theoretical framework is sound but relies on Taylor approximations with practical limitations
- Biological plausibility claims: Low - The paper mentions biological plausibility but doesn't provide extensive evidence or discussion

## Next Checks
1. Test with momentum-based learning rules to verify whether common optimization techniques can be expressed in the natural gradient framework
2. Characterize the approximation error by quantifying how the condition number of the optimal metric affects convergence rate
3. Apply to non-convex optimization on neural network training to see if natural gradient reformulation provides computational benefits beyond the quadratic and linear examples presented