---
ver: rpa2
title: '1-800-SHARED-TASKS @ NLU of Devanagari Script Languages: Detection of Language,
  Hate Speech, and Targets using LLMs'
arxiv_id: '2411.06850'
source_url: https://arxiv.org/abs/2411.06850
tags:
- detection
- hate
- speech
- gemma-2
- confusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a system for detecting language, hate speech,
  and hate speech targets in Devanagari script languages. It fine-tunes multilingual
  LLMs (MuRIL, IndicBERT, Gemma-2) with focal loss and ensembles for improved performance.
---

# 1-800-SHARED-TASKS @ NLU of Devanagari Script Languages: Detection of Language, Hate Speech, and Targets using LLMs

## Quick Facts
- arXiv ID: 2411.06850
- Source URL: https://arxiv.org/abs/2411.06850
- Reference count: 10
- Key outcome: Achieved F1 scores of 0.9980, 0.7652, and 0.6804 on language, hate speech, and target detection tasks respectively

## Executive Summary
This work presents a system for detecting language, hate speech, and hate speech targets in Devanagari script languages. The approach fine-tunes multilingual LLMs (MuRIL, IndicBERT, Gemma-2) with focal loss to handle class imbalance, and employs ensemble methods for improved performance. The system achieved competitive results across all tasks in the CHiPSAL 2025 competition, demonstrating the effectiveness of combining focal loss, LoRA-based fine-tuning, and ensemble voting for multilingual hate speech detection.

## Method Summary
The system fine-tunes multilingual LLMs using focal loss to address class imbalance in hate speech detection, with LoRA enabling efficient parameter updates. Models are evaluated and selected based on performance, then retrained on combined train+dev data for final testing. An ensemble approach with majority voting and fallback models is used to combine predictions from multiple architectures (MuRIL, IndicBERT, Gemma-2). The system handles three tasks: language identification, hate speech detection, and hate speech target identification across five Devanagari script languages.

## Key Results
- F1 score of 0.9980 for language identification task
- F1 score of 0.7652 for hate speech detection task
- F1 score of 0.6804 for hate speech target detection task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning multilingual LLMs with focal loss improves hate speech detection performance in imbalanced datasets.
- Mechanism: Focal loss reduces the relative loss for well-classified examples and focuses more on hard, misclassified examples, making it effective for handling class imbalance in hate speech detection tasks.
- Core assumption: The class imbalance in hate speech detection (fewer hate speech examples than non-hate speech) significantly impacts model performance.
- Evidence anchors:
  - [abstract] "leveraged unique techniques like focal loss to address challenges in the natural understanding of Devanagari languages, such as multilingual processing and class imbalance"
  - [section] "To handle the class imbalance in sub-task B, focal loss (Lin et al., 2018) was used for BERT-based models"
- Break condition: If the class distribution becomes more balanced or if the dataset size increases significantly, the benefits of focal loss may diminish.

### Mechanism 2
- Claim: Ensemble methods improve overall classification performance by combining predictions from multiple models.
- Mechanism: By using majority voting with fallback models, the ensemble leverages the strengths of individual models while compensating for their weaknesses, resulting in more robust predictions.
- Core assumption: Different models capture different aspects of the data, and combining their predictions can lead to better overall performance than any single model.
- Evidence anchors:
  - [abstract] "achieved competitive results across all tasks: F1 of 0.9980, 0.7652, and 0.6804 for Sub-tasks A, B, and C respectively"
  - [section] "For the testing phase, we retrained the top-selected models from the Evaluation phase by incorporating both the train and dev sets to create a more generalized model for final testing"
- Break condition: If all models in the ensemble make similar errors or if computational resources become severely limited.

### Mechanism 3
- Claim: LoRA (Low-Rank Adaptation) enables efficient fine-tuning of large language models by reducing the number of trainable parameters.
- Mechanism: LoRA decomposes weight updates into low-rank matrices, significantly reducing the number of parameters that need to be trained while maintaining model performance.
- Core assumption: The original model parameters contain sufficient knowledge that can be adapted with a small number of additional parameters for specific tasks.
- Evidence anchors:
  - [section] "All the fine-tuning of decoder-only models was carried out using Unsloth with Low-Rank Adaptation of Large Language Models (LoRA) (Hu et al., 2021)"
- Break condition: If the task requires significant modifications to the base model architecture or if the dataset is extremely large.

## Foundational Learning

- Concept: Multilingual model pre-training
  - Why needed here: The system needs to handle multiple languages (Nepali, Marathi, Sanskrit, Bhojpuri, Hindi) that share the Devanagari script, requiring models pre-trained on diverse language data.
  - Quick check question: Why is it important to use multilingual models rather than training separate models for each language?

- Concept: Class imbalance handling techniques
  - Why needed here: Hate speech detection tasks typically have imbalanced datasets with far fewer positive examples than negative ones, requiring specialized techniques like focal loss.
  - Quick check question: What are the potential consequences of not addressing class imbalance in hate speech detection?

- Concept: Ensemble learning principles
  - Why needed here: Combining multiple models through ensembling can improve overall performance by leveraging the strengths of different architectures and reducing individual model biases.
  - Quick check question: How does majority voting work in ensemble methods, and what happens when there's no clear majority?

## Architecture Onboarding

- Component map: Data → Preprocessing → Fine-tuning (with LoRA) → Focal loss application → Ensemble voting → Evaluation
- Critical path: Data → Preprocessing → Fine-tuning (with LoRA) → Focal loss application → Ensemble voting → Evaluation
- Design tradeoffs:
  - Model size vs. computational efficiency (using LoRA for efficient fine-tuning)
  - Precision (4-bit vs. full precision) vs. performance
  - Single model vs. ensemble approach for different tasks
  - BERT-based vs. decoder-only architectures for different tasks
- Failure signatures:
  - Poor performance on minority classes (indicates focal loss parameters need adjustment)
  - Inconsistent predictions across ensemble members (indicates model diversity issues)
  - Memory errors during fine-tuning (indicates LoRA parameters need adjustment)
- First 3 experiments:
  1. Baseline experiment: Fine-tune each model individually without focal loss or LoRA
  2. Focal loss optimization: Experiment with different alpha and gamma values for focal loss
  3. LoRA parameter tuning: Test different rank and alpha values for LoRA to find optimal configuration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would increasing the dataset size for hate speech and target detection tasks impact model performance?
- Basis in paper: explicit - "The datasets used for training and evaluation in hate speech and target detection are relatively small, which may impact the generalizability of the models"
- Why unresolved: The paper acknowledges dataset limitations but doesn't experiment with larger datasets to quantify the performance impact
- What evidence would resolve it: Comparative experiments training models on progressively larger datasets with performance metrics

### Open Question 2
- Question: How would full-precision training of decoder-only models affect their performance compared to 4-bit precision?
- Basis in paper: explicit - "The decoder-only models were trained in 4-bit precision due to computational limitations, and they may perform better in full-precision mode"
- Why unresolved: The authors note potential performance gains from full-precision training but don't provide experimental validation
- What evidence would resolve it: Performance comparison of decoder-only models trained in 4-bit vs full-precision on identical tasks

### Open Question 3
- Question: What specific challenges arise when extending these models to handle code-mixed languages within the Devanagari script?
- Basis in paper: explicit - "issues with code-mixed languages... remain significant hurdles in the accurate detection of hate speech"
- Why unresolved: The paper mentions code-mixing as a challenge but doesn't investigate how models perform on or could be adapted for code-mixed text
- What evidence would resolve it: Experiments testing model performance on code-mixed Devanagari text and analysis of failure modes

## Limitations
- Dataset limitations: The paper lacks detailed information about dataset splits, class distributions, and exact hyperparameter configurations for grid searches
- Method generalizability: The reliance on specific model architectures and the Unsloth framework may limit generalizability to other models or frameworks
- Single dataset validation: The effectiveness of focal loss and ensemble methods is primarily supported by results on this single competition dataset

## Confidence

**High Confidence:**
- The effectiveness of fine-tuning multilingual LLMs for Devanagari script language tasks is well-established through the reported F1 scores

**Medium Confidence:**
- The focal loss mechanism's effectiveness in handling class imbalance for hate speech detection is demonstrated through results but lacks extensive validation beyond the competition dataset
- The ensemble approach's contribution to overall performance is evident from the results but the specific voting mechanism details are not fully transparent

**Low Confidence:**
- The generalizability of the approach to other Devanagari script languages not included in the dataset
- The optimal hyperparameter configurations for focal loss and LoRA are not fully explored or validated across different datasets

## Next Checks

1. **Dataset Validation Check:**
   - Validate the class distribution and balance in the CHiPSAL 2025 dataset by analyzing the training, development, and test splits. Confirm that the reported class imbalance aligns with the dataset characteristics and that focal loss is appropriately applied.

2. **Focal Loss Hyperparameter Sensitivity:**
   - Conduct a grid search over different α and γ values for focal loss to determine the sensitivity of hate speech detection performance to these parameters. Compare results across different ranges to identify optimal configurations.

3. **Ensemble Robustness Analysis:**
   - Perform ablation studies by removing individual models from the ensemble to assess the contribution of each model to overall performance. Analyze the consistency of predictions across different ensemble configurations to identify potential model diversity issues.