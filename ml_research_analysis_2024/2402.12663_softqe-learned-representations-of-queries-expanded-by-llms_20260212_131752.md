---
ver: rpa2
title: 'SoftQE: Learned Representations of Queries Expanded by LLMs'
arxiv_id: '2402.12663'
source_url: https://arxiv.org/abs/2402.12663
tags:
- softqe
- https
- query
- queries
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces SoftQE, a method to improve dense retrieval
  without incurring the cost of Large Language Models (LLMs) at inference time. SoftQE
  learns to align query representations with those of LLM-expanded queries by mapping
  embeddings of input queries to those of the LLM-generated expanded queries during
  training.
---

# SoftQE: Learned Representations of Queries Expanded by LLMs

## Quick Facts
- arXiv ID: 2402.12663
- Source URL: https://arxiv.org/abs/2402.12663
- Authors: Varad Pimpalkhute; John Heyer; Xusen Yin; Sameer Gupta
- Reference count: 39
- One-line primary result: SoftQE improves dense retrieval without LLM inference costs by aligning query representations with LLM-expanded queries during training

## Executive Summary
SoftQE is a novel approach that improves dense retrieval by learning to align query representations with those of LLM-expanded queries during training, eliminating the need for expensive LLM inference at query time. The method uses a combination of contrastive loss and distance loss to map original query embeddings to expanded query embeddings, effectively transferring the semantic enrichment from LLMs into a compact, efficient form. Experiments demonstrate significant zero-shot improvements on BEIR tasks while maintaining competitive in-domain performance on MS MARCO.

## Method Summary
SoftQE learns query representations by aligning them with LLM-expanded query representations during training. The model uses a dual-encoder architecture where one encoder (Q2D) generates expanded query representations, and the main retriever learns to map original queries to these expanded representations. Training employs a combined loss function with contrastive loss (Lcont) for relevance matching and distance loss (Ldist) for alignment between original and expanded query embeddings. A stepwise warm-up strategy gradually reduces the weight of the alignment objective during training.

## Key Results
- SoftQE significantly improves zero-shot retrieval performance on BEIR tasks, achieving 2.83 absolute percentage points improvement on average
- The method maintains competitive performance on in-domain MS MARCO tasks while outperforming strong baseline dense retrievers
- SoftQE shows complementary benefits when combined with cross-encoder distillation, suggesting different types of information are captured by each approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SoftQE improves dense retrieval by aligning query representations with LLM-expanded query representations during training.
- Mechanism: The model learns to map original query embeddings to those of expanded queries, effectively transferring the semantic enrichment performed by LLMs into a form that can be used at inference without LLM cost.
- Core assumption: The expanded query representations from the Q2D teacher encoder are well-aligned with relevant documents and can serve as effective targets for distillation.
- Evidence anchors:
  - [abstract] "SoftQE incorporates knowledge from LLMs by mapping embeddings of input queries to those of the LLM-expanded queries."
  - [section] "SoftQE performs at least as well as baseline dense retrievers... and significantly improves upon these baselines for a majority of out-of-domain BEIR tasks."
  - [corpus] Weak - only 1 related paper mentions LLM-based routing, no direct evidence for this specific mechanism.
- Break condition: If the expanded query representations from Q2D are not well-aligned with relevant documents, or if the alignment learned during training doesn't generalize to unseen queries.

### Mechanism 2
- Claim: The complementary benefit of SoftQE with cross-encoder distillation comes from different types of information being captured.
- Mechanism: Cross-encoder distillation captures fine-grained semantic relationships through re-scoring, while SoftQE captures broader semantic context through query expansion. Combining both provides more complete supervision.
- Core assumption: The information distilled through cross-encoder scores and expanded query representations is complementary and non-overlapping.
- Evidence anchors:
  - [abstract] "SoftQE significantly improves upon these baselines for a majority of out-of-domain BEIR tasks" and "we observe measurable improvements in the zero-shot setting, suggesting that information learned through the SoftQE objective is complementary to other forms of distillation, such as distillation from a cross-encoder."
  - [section] "we find the information distilled through cross-encoder scores and expanded query representations"

## Foundational Learning

1. **Dense Retrieval** (why needed: Core retrieval paradigm being improved; quick check: Can implement DPR-style dual-encoder with contrastive loss)
2. **Contrastive Learning** (why needed: Primary objective for learning relevant document-query relationships; quick check: Can implement InfoNCE-style contrastive loss)
3. **Knowledge Distillation** (why needed: Mechanism for transferring LLM expansion knowledge to compact model; quick check: Can implement teacher-student distillation setup)
4. **Query Expansion** (why needed: LLM technique being simulated; quick check: Can implement basic query expansion using templates)
5. **BEIR Benchmark** (why needed: Standard zero-shot evaluation; quick check: Can run existing BEIR evaluation scripts)
6. **Pseudo-document Generation** (why needed: LLM-generated training data; quick check: Can implement few-shot prompting for query expansion)

## Architecture Onboarding

**Component Map**: Input Query -> Dense Retriever -> Document Scores; Input Query -> Q2D Encoder -> Expanded Query -> Distance Loss; Input Query + Documents -> Contrastive Loss

**Critical Path**: Query embedding → Q2D encoder → expanded query embedding → distance loss → gradient update (alignment path)

**Design Tradeoffs**: Single-stage retrieval vs. re-ranking trade-off; computational efficiency vs. semantic richness; frozen Q2D encoder vs. joint fine-tuning

**Failure Signatures**: 
- Poor zero-shot performance indicates domain shift issues
- Underperformance on in-domain tasks suggests over-regularization
- Training instability indicates loss weight scheduling problems

**First Experiments**:
1. Baseline DPR training on MS MARCO to establish performance floor
2. Q2D encoder evaluation on query expansion quality metrics
3. SoftQE with only Lcont to isolate contrastive learning contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of SoftQE vary across different LLM expansion strategies beyond the text-davinci-003 approach used in this study?
- Basis in paper: [explicit] The paper mentions using pseudo-documents generated by text-davinci-003 with few-shot and chain of thought techniques, but does not explore other LLM models or prompting strategies
- Why unresolved: The study is limited to one specific LLM and prompting approach, leaving open the question of whether other LLMs or prompting techniques might yield different alignment results
- What evidence would resolve it: Systematic experiments comparing SoftQE performance using different LLM models (GPT-4, Claude, etc.) and prompting strategies, with ablation studies showing which factors contribute most to alignment quality

### Open Question 2
- Question: What is the optimal balance between contrastive learning and knowledge distillation objectives during training, and how does this vary across different retrieval tasks?
- Basis in paper: [explicit] The authors explore different α values but settle on a warmup strategy, acknowledging this is a hyperparameter choice without exploring the full space
- Why unresolved: The paper only explores a few fixed values of α and one warmup strategy, not investigating the optimal trajectory or whether it should be task-specific
- What evidence would resolve it: Comprehensive hyperparameter search across multiple retrieval tasks, potentially using meta-learning or automated hyperparameter optimization to find task-specific optimal schedules

### Open Question 3
- Question: How does SoftQE performance degrade as we move further from the training domain, and what are the theoretical limits of this approach for truly zero-shot retrieval?
- Basis in paper: [inferred] The paper shows improvements on BEIR tasks but doesn't systematically analyze performance degradation across varying domain distances or characterize the approach's zero-shot limits
- Why unresolved: The BEIR evaluation provides some out-of-domain evidence but doesn't quantify how performance changes with domain shift magnitude or establish theoretical bounds on zero-shot effectiveness
- What evidence would resolve it: Controlled experiments varying domain similarity between training and test data, potentially using embedding space similarity metrics to predict performance degradation, and theoretical analysis of the approach's generalization capabilities

## Limitations

- Scalability concerns for document-level retrieval and larger collections beyond passage retrieval
- Limited ablation studies on the distance loss component and warm-up schedule justification
- Reliance on pseudo-documents from a single LLM (text-davinci-003) without exploring alternative expansion strategies

## Confidence

**High Confidence**: The core claim that SoftQE enables LLM-expanded query benefits without inference-time costs is well-supported by experimental results, showing consistent improvements on BEIR tasks while maintaining competitive in-domain performance.

**Medium Confidence**: The claim of complementary benefits with cross-encoder distillation is supported but requires further validation, as the analysis of what specific information each method captures is limited.

**Low Confidence**: The generalizability of SoftQE to different domains and query types remains uncertain, with evaluation limited to academic and scientific retrieval tasks without testing conversational or ambiguous queries.

## Next Checks

1. **Ablation Study on Loss Components**: Run experiments isolating Lcont and Ldist to determine their individual contributions to performance gains, and test alternative warm-up schedules for α to optimize the training process.

2. **Scalability Evaluation**: Test SoftQE on document-level retrieval tasks and larger collections (e.g., full Wikipedia or web-scale datasets) to assess whether the learned alignment maintains effectiveness as the retrieval space grows.

3. **Robustness to Expansion Variations**: Evaluate the method using different LLM models (e.g., GPT-4, Claude, open-source alternatives) and expansion strategies to determine if the benefits are specific to text-davinci-003 or generalize across expansion approaches.