---
ver: rpa2
title: On the Information Redundancy in Non-Autoregressive Translation
arxiv_id: '2405.02673'
source_url: https://arxiv.org/abs/2405.02673
tags:
- redundancy
- oaxe
- which
- translation
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study identifies two new types of information redundancy errors\u2014\
  continuous and discontinuous redundancy\u2014in non-autoregressive translation (NAT)\
  \ models that go beyond conventional token repetition metrics. These errors correspond\
  \ to lexical and reordering multi-modality problems."
---

# On the Information Redundancy in Non-Autoregressive Translation

## Quick Facts
- arXiv ID: 2405.02673
- Source URL: https://arxiv.org/abs/2405.02673
- Authors: Zhihao Wang; Longyue Wang; Jinsong Su; Junfeng Yao; Zhaopeng Tu
- Reference count: 11
- Primary result: Advanced NAT models like DAT reduce continuous repetition but still suffer from high discontinuous redundancy in lexical and reordering errors.

## Executive Summary
This study identifies two new types of information redundancy errors in non-autoregressive translation (NAT) models: continuous redundancy (adjacent repeated tokens) and discontinuous redundancy (non-adjacent repeated concepts or synonyms). These errors correspond to lexical and reordering multi-modality problems in translation. The authors propose automatic metrics based on word embeddings and synonym detection to evaluate these redundancy errors. Manual annotation of 100 sentences from WMT20 English→Chinese testset validates the automatic metrics, showing that while advanced NAT models like DAT significantly reduce continuous repetition, they still struggle with discontinuous redundancy, particularly in lexical and reordering errors.

## Method Summary
The study re-implements CMLM, OAXE, GLAT, and DAT models on Fairseq framework with Transformer-Base configuration. Models are trained on large-scale WMT20 En↔Zh (21.8M) and En↔De (45.1M) translation tasks. The authors propose automatic metrics for information redundancy: Continuous Redundancy Ratio (CRR) and Discontinuous Redundancy Ratio (DRR), which use mBART multilingual word embeddings for synonym detection. Manual annotation of 100 sentences from WMT20 English→Chinese testset validates the automatic metrics' correlation with human judgment.

## Key Results
- DAT model significantly reduces continuous redundancy (CRR: 1.4% vs 2.7% in CMLM) but shows high discontinuous redundancy (DRR: 2.3% vs 3.7%)
- Automatic metrics (CRR and DRR) show good correlation with manual annotations
- Discontinuous redundancy is more challenging than continuous redundancy across all tested NAT models
- Redundancy patterns vary across language pairs, with reordering errors more prominent in certain directions

## Why This Works (Mechanism)

### Mechanism 1: Continuous Redundancy from Parallel Prediction
- Claim: Continuous redundancy stems from NAT models independently predicting neighboring tokens that should be different but end up being the same
- Mechanism: Parallel token prediction without conditioning on previous predictions causes identical outputs for multiple positions when tokens could plausibly be the same
- Core assumption: Token positions are treated independently in NAT models
- Evidence anchors: [abstract] "Token repetition is a typical form of multi-modal problem in fully non-autoregressive translation (NAT)." [section] "independent predictions... leads to multi-modal outputs in the form of token repetitions"

### Mechanism 2: Discontinuous Redundancy from Multi-Modality
- Claim: Discontinuous redundancy arises from lexical and reordering multi-modality where the same semantic concept appears in multiple positions
- Mechanism: NAT models can place the same concept in multiple positions when it should appear only once, manifesting as repeated content words and synonyms
- Core assumption: Multiple valid translations exist with different word orders or lexical choices
- Evidence anchors: [abstract] "two types of information redundancy errors that correspond well to lexical and reordering multi-modality problems." [section] "word reordering... leads to discontinuous repetition"

### Mechanism 3: Synonym Redundancy from Independent Prediction
- Claim: Synonym redundancy occurs when NAT models independently predict different words with the same meaning in multiple positions
- Mechanism: Lack of semantic context causes models to assign high probability to multiple synonyms across different positions
- Core assumption: Model treats semantically similar words as independent choices
- Evidence anchors: [section] "Continuous Synonym: While the conventional metric only considers repetition of the same token... this category includes synonyms"

## Foundational Learning

- Concept: Multi-modality in translation
  - Why needed here: Understanding multiple valid translations for the same source is fundamental to grasping why NAT models produce redundancy errors
  - Quick check question: If a source sentence can be translated as both "I ate pizza" and "I had pizza," what type of redundancy might occur in NAT outputs?

- Concept: Token-level vs. sequence-level prediction
  - Why needed here: The architectural difference between autoregressive and non-autoregressive models drives the redundancy problem
  - Quick check question: How does predicting tokens in parallel (NAT) versus sequentially (AT) affect the model's ability to handle multi-modal translation options?

- Concept: Semantic similarity and synonym detection
  - Why needed here: The paper's automatic metrics rely on detecting synonyms, requiring understanding semantic equivalence beyond surface form
  - Quick check question: What distance metric in embedding space would indicate two words are synonyms rather than just related concepts?

## Architecture Onboarding

- Component map:
  Encoder -> Parallel decoder predictions -> Output distributions -> Sample/greedy decode -> Output sentence -> Redundancy analysis

- Critical path:
  1. Source sentence → Encoder
  2. Encoder output → Parallel decoder predictions
  3. Predictions → Output distributions
  4. Sample/greedy decode → Output sentence
  5. Output sentence → Redundancy analysis

- Design tradeoffs:
  - Speed vs. quality: Parallel prediction enables speed but sacrifices quality due to multi-modality
  - Complexity vs. effectiveness: Adding dependency modeling (like DAT's transition matrix) improves quality but adds complexity
  - Metric sophistication vs. computational cost: Automatic metrics are more comprehensive but require embedding computations

- Failure signatures:
  - High continuous redundancy ratio: Model fails to capture local dependencies
  - High discontinuous redundancy ratio: Model struggles with lexical choices and word order
  - High synonym redundancy: Model lacks semantic understanding
  - BLEU improvement without redundancy reduction: Model may be learning superficial patterns

- First 3 experiments:
  1. Compare CMLM baseline with DAT on both BLEU and redundancy metrics to verify the paper's findings about DAT's performance
  2. Test the automatic metric's correlation with human annotation on a small sample to validate the approach
  3. Apply the automatic metrics to a new NAT model (e.g., Levenshtein Transformer) to benchmark its redundancy characteristics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the automatic metric for discontinuous redundancy to better handle cases where the same token appears multiple times in both the reference and source but should not be considered redundant?
- Basis in paper: [inferred] The paper discusses the challenge of identifying discontinuous redundancy, particularly in cases where a token or its synonym occurs multiple times in the reference or source sentence. The authors mention maintaining a dynamic list to exempt redundant tokens, but acknowledge this may not be sufficient.
- Why unresolved: The current approach relies on a heuristic method of exempting tokens that appear multiple times, which may not always accurately capture the intended meaning. The paper suggests this is a limitation of the current metric.
- What evidence would resolve it: A more robust method for handling multiple occurrences of tokens in the reference and source, such as using semantic similarity or context-based analysis, could improve the accuracy of the automatic metric.

### Open Question 2
- Question: How can we extend the analysis of information redundancy to other language pairs beyond the four tested in the paper (English→Chinese, Chinese→English, English→German, German→English)?
- Basis in paper: [explicit] The paper mentions that the findings are based on several representative NAT models on large-scale datasets and may not be well-suited for other NAT models and datasets. The authors also state that the synonym list is derived from mBART embedding, which may not be accurate for all languages.
- Why unresolved: The current study is limited to a specific set of language pairs, and the authors acknowledge that the conclusions may not generalize to other languages. The accuracy of the synonym detection method may also vary across languages.
- What evidence would resolve it: Conducting a similar analysis on a wider range of language pairs, including low-resource languages, would provide insights into the generalizability of the findings. Additionally, developing language-specific synonym detection methods could improve the accuracy of the automatic metric.

### Open Question 3
- Question: How can we address the issue of information redundancy in non-autoregressive translation models beyond the four types identified in the paper (continuous repetition, discontinuous repetition, continuous synonym, and discontinuous synonym)?
- Basis in paper: [explicit] The paper identifies four types of information redundancy errors in NAT models and proposes automatic metrics to evaluate them. However, the authors acknowledge that there may be other types of redundancy errors that are not captured by the current metrics.
- Why unresolved: The current study focuses on a specific set of redundancy errors, but there may be other types of redundancy that are not explicitly addressed. The authors suggest that future research should explore additional types of redundancy and develop corresponding evaluation metrics.
- What evidence would resolve it: Conducting a comprehensive analysis of NAT outputs to identify new types of redundancy errors, and developing corresponding automatic metrics to evaluate them, would provide a more complete understanding of the information redundancy problem in NAT models.

## Limitations

- Manual annotation sample size is limited to 100 sentences, which may not fully represent translation diversity across language pairs
- Synonym detection relies on mBART embeddings with unspecified threshold values, affecting reproducibility
- Findings may be language-pair specific and may not generalize to languages with significantly different syntactic structures

## Confidence

- **High Confidence**: Identification of continuous redundancy as distinct from token repetition is well-supported by both automatic metrics and manual annotation
- **Medium Confidence**: Automatic metrics show good correlation with human annotation, but confidence intervals are not provided
- **Low Confidence**: Exact threshold values and implementation details for automatic metrics are not fully specified

## Next Checks

1. Expand manual annotation scale to 500+ sentences across multiple language pairs to test generalizability and establish confidence intervals for metric correlation

2. Apply automatic metrics to language pairs with significantly different syntactic structures (e.g., English→Japanese, English→Arabic) to determine if redundancy patterns are universal or language-specific

3. Systematically vary synonym detection threshold values and embedding similarity cutoffs to determine sensitivity to hyperparameters and establish optimal values through ablation studies