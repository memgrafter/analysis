---
ver: rpa2
title: Robustness of Deep Neural Networks for Micro-Doppler Radar Classification
arxiv_id: '2402.13651'
source_url: https://arxiv.org/abs/2402.13651
tags:
- temporal
- adversarial
- examples
- training
- shifts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work investigates the robustness of deep neural networks
  for micro-Doppler radar classification, focusing on two common issues: sensitivity
  to temporal shifts and susceptibility to adversarial examples. The authors evaluate
  two convolutional architectures trained on human activity recognition data, finding
  that standard training leads to models that are highly sensitive to both small temporal
  shifts and adversarial perturbations.'
---

# Robustness of Deep Neural Networks for Micro-Doppler Radar Classification

## Quick Facts
- arXiv ID: 2402.13651
- Source URL: https://arxiv.org/abs/2402.13651
- Authors: Mikolaj Czerkawski; Carmine Clemente; Craig Michie; Christos Tachtatzis
- Reference count: 19
- Primary result: CVD-based models show significantly improved robustness to adversarial examples and temporal shifts while maintaining high accuracy

## Executive Summary
This work investigates the robustness of deep neural networks for micro-Doppler radar classification, focusing on sensitivity to temporal shifts and susceptibility to adversarial examples. The authors evaluate two convolutional architectures trained on human activity recognition data, finding that standard training leads to models that are highly sensitive to both small temporal shifts and adversarial perturbations. Temporal augmentation and adversarial training improve robustness, but models still exhibit some sensitivity. Notably, models using cadence-velocity diagram (CVD) input instead of Doppler-time representation show significantly improved robustness to adversarial examples and temporal shifts. The study demonstrates that CVD-based models achieve a good balance between accuracy and robustness, with accuracy rarely dropping below 0.80 under adversarial attacks. This suggests that incorporating domain-specific input representations can be an effective strategy for improving the robustness of deep learning models in radar applications.

## Method Summary
The study uses a human activity recognition dataset containing 1,752 FMCW radar signatures of 6 activities, preprocessed into both Doppler-time (128x128) and CVD representations. Two CNN architectures (Model A and Model B) are trained under four schemes: standard, adversarial (PGD), temporal augmentation, and both. Gaussian noise is added for regularization. Models are evaluated on test datasets with original samples, adversarial examples (PGD, 20 steps, ±0.1 clipping), and temporally shifted samples (range 0-20). Robustness is measured through classification accuracy and mean label activation variance for temporal and Doppler shifts.

## Key Results
- Standard-trained models show accuracy dropping to 0.00 under PGD adversarial attacks and are highly sensitive to small temporal shifts
- Temporal augmentation and adversarial training improve robustness but models still show some sensitivity (accuracy 0.20-0.40 under PGD)
- CVD-based models achieve accuracy rarely below 0.80 under adversarial attacks and show significantly improved robustness to temporal shifts
- CVD input removes phase information, enforcing temporal invariance and reducing reliance on non-robust features

## Why This Works (Mechanism)

### Mechanism 1
Models trained on raw Doppler-time representations learn non-robust features that are sensitive to small temporal and Doppler shifts, causing misclassification. The deep convolutional models extract features from specific time and frequency bins that are not invariant to small translations, so shifts in the input representation alter the feature maps and thus the prediction. Core assumption: Small temporal and Doppler shifts carry minimal semantic content and should not affect classification.

### Mechanism 2
Adversarial examples exploit non-robust, training-data-specific features, causing misclassification even with imperceptible perturbations. PGD attacks find small perturbations in the input space that maximize the loss, leveraging the fact that the model has learned features highly specific to the training data distribution rather than generalizable features. Core assumption: The perturbations are constrained to be imperceptible (e.g., within ℓ∞ norm) but still cause misclassification due to model overfitting on dataset-specific patterns.

### Mechanism 3
Converting Doppler-time input to Cadence-Velocity Diagram (CVD) removes phase information, enforcing temporal invariance and reducing reliance on non-robust features. CVD is invariant to temporal shifts because it is the magnitude of the Fourier transform along the time axis, so small shifts in the original Doppler-time map do not affect the CVD input. This forces the network to learn features based on the overall shape rather than precise timing. Core assumption: The crucial discriminative information for the classification task is preserved in the magnitude spectrum even without phase.

## Foundational Learning

- Concept: Temporal and Doppler shift invariance in signal representations
  - Why needed here: Understanding why models should ideally be insensitive to small shifts in time or frequency is key to interpreting the robustness issues observed.
  - Quick check question: If a walking signal is shifted slightly in time, should the predicted activity change? Why or why not?

- Concept: Adversarial examples and Projected Gradient Descent (PGD)
  - Why needed here: The paper evaluates model robustness against PGD attacks, so knowing how these attacks work and why they succeed is essential.
  - Quick check question: What constraint is placed on PGD perturbations, and why does this make them hard to detect visually?

- Concept: Input representation choices and their impact on learned features
  - Why needed here: The switch from Doppler-time to CVD representation is central to the robustness improvement; understanding how different representations affect what the model learns is crucial.
  - Quick check question: How does removing phase information from a signal representation affect the network's ability to detect timing-based features?

## Architecture Onboarding

- Component map: Input → STFT preprocessing → optional CVD transform → CNN feature extraction → classifier head → output probabilities
- Critical path: Doppler-time or CVD input → CNN layers with 9x9 kernels and Leaky ReLU → shared classifier head → activity classification
- Design tradeoffs: Raw Doppler-time preserves full information (phase + magnitude) but is sensitive to shifts; CVD is robust but loses phase. Larger models may generalize better but risk overfitting dataset-specific features.
- Failure signatures: High accuracy on clean test set but dramatic drops under small temporal/Doppler shifts or adversarial attacks indicates overfitting to non-robust features.
- First 3 experiments:
  1. Train Model A on Doppler-time with standard training; evaluate accuracy under temporal shifts and PGD attacks.
  2. Convert same dataset to CVD; train Model A on CVD; compare robustness metrics to experiment 1.
  3. Train Model A with temporal augmentation and/or adversarial training on Doppler-time; compare robustness to experiments 1 and 2.

## Open Questions the Paper Calls Out

### Open Question 1
How do different input representations (Doppler-time vs. Cadence-Velocity Diagram) affect the robustness of deep neural networks to adversarial examples and temporal shifts in micro-Doppler radar classification? The paper provides empirical results but does not explore the underlying reasons for the improved robustness of CVD-based models or investigate other potential input representations that might further enhance robustness.

### Open Question 2
What is the relationship between model capacity and robustness to adversarial examples and temporal shifts in micro-Doppler radar classification? The paper mentions that Model A, a larger model, performs better but doesn't systematically explore how model size affects robustness.

## Limitations
- Unspecified architectural details of Model A and Model B beyond kernel size and activation function
- Incomplete PGD/temporal augmentation implementation parameters
- Limited generalizability with robustness improvements demonstrated only on a single dataset with six activities

## Confidence

- **High Confidence:** The observation that standard-trained models are highly sensitive to both temporal shifts and adversarial examples is well-supported by the reported accuracy drops to 0.00 under PGD attacks.
- **Medium Confidence:** The claim that CVD representation improves robustness is supported by accuracy rarely dropping below 0.80 under adversarial attacks, but the underlying mechanism (phase removal) lacks direct validation.
- **Low Confidence:** The assertion that adversarial training only achieves 0.20-0.40 accuracy under PGD attacks is based on limited experimental details, making it difficult to assess whether this reflects fundamental limitations or implementation specifics.

## Next Checks

1. **Phase Ablation Study:** Systematically evaluate whether removing phase information (converting to CVD) maintains accuracy across all activity classes, particularly those that might rely on timing-specific features.

2. **Cross-Dataset Transferability:** Test the robustness of CVD-based models on a different micro-Doppler radar dataset to assess whether the observed improvements generalize beyond the original six-activity dataset.

3. **PGD Parameter Sensitivity:** Vary PGD attack parameters (step size, number of iterations, perturbation budget) to determine whether the reported 0.00 accuracy under standard training is consistent across different attack strengths.