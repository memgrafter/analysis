---
ver: rpa2
title: Iterative thresholding for non-linear learning in the strong $\varepsilon$-contamination
  model
arxiv_id: '2409.03703'
source_url: https://arxiv.org/abs/2409.03703
tags:
- then
- have
- proof
- lemma
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies robust learning of single neuron models under\
  \ the strong \u03B5-contamination model, where both covariates and labels can be\
  \ adversarially corrupted. The authors propose iterative thresholding algorithms\
  \ for gradient-based learning of linear and nonlinear neurons (sigmoid, leaky-ReLU,\
  \ ReLU) and prove approximation bounds."
---

# Iterative thresholding for non-linear learning in the strong $\varepsilon$-contamination model

## Quick Facts
- arXiv ID: 2409.03703
- Source URL: https://arxiv.org/abs/2409.03703
- Reference count: 40
- Achieves O(ν ε log(1/ε)) error for linear regression with O(N d² log(‖w*‖/ε)) runtime

## Executive Summary
This paper studies robust learning of single neuron models under the strong ε-contamination model where both covariates and labels can be adversarially corrupted. The authors propose iterative thresholding algorithms for gradient-based learning of linear and nonlinear neurons (sigmoid, leaky-ReLU, ReLU) and prove approximation bounds. The key technical insight is that iterative thresholding converges linearly to the optimal estimator over uncorrupted data, with additional error terms from adversarial corruption and stochastic noise. The algorithm's runtime scales logarithmically with the target norm, improving upon prior work with polynomial runtime dependence on 1/ε.

## Method Summary
The proposed iterative thresholding algorithm alternates between gradient descent updates and hard thresholding based on loss values. At each iteration, the algorithm computes the loss for all samples, selects the (1-ε)N points with smallest ℓ₂ errors, and updates the parameter estimate using gradient descent on this subset. For linear regression, the algorithm uses a fixed learning rate η = 0.1κ⁻²(Σ) and runs for T = O(κ²(Σ) log(‖w*‖/ε)) iterations. For nonlinear neurons, the method extends naturally by using appropriate loss functions and activation functions. The algorithm requires only access to a gradient oracle and does not need preprocessing filtering steps to handle covariate corruption.

## Key Results
- Achieves O(ν ε log(1/ε)) error for linear regression, improving upon prior O(polylog(N,d)/ε²) runtime
- For nonlinear neurons (sigmoid, leaky-ReLU, ReLU), obtains O(√ε log(1/ε)) error bounds with sample complexity O(d/ε)
- Proves linear convergence to the optimal estimator over uncorrupted data with additive error terms from corruption and noise
- Handles covariate corruption without requiring preprocessing filtering steps

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Iterative thresholding algorithms converge linearly to the optimal estimator over uncorrupted data, with additive error terms from adversarial corruption and stochastic noise.
- **Mechanism**: At each iteration, the algorithm removes samples with large errors (above a threshold), ensuring that the remaining samples are dominated by inliers. The gradient descent update then moves the parameter estimate toward the optimal solution on the inliers. The key insight is that the error in the estimate contracts by a constant factor each iteration (linear convergence) until it reaches a steady-state error determined by the noise and corruption levels.
- **Core assumption**: The uncorrupted data follows a sub-Gaussian distribution with a well-conditioned second-moment matrix (bounded condition number), and the adversary can corrupt at most an ε-fraction of the samples.
- **Evidence anchors**:
  - [abstract]: "The key technical insight is that iterative thresholding converges linearly to the optimal estimator over uncorrupted data, with additional error terms from adversarial corruption and stochastic noise."
  - [section 3.1]: Proof of linear convergence in the linear regression case, showing the estimate contracts by a factor less than 1 each iteration until reaching a steady-state error.
  - [corpus]: No direct evidence, but related works on iterative thresholding for robust learning support the mechanism.
- **Break condition**: If the corruption fraction ε is too large, or if the sub-Gaussian assumption on the uncorrupted data is violated, the algorithm may fail to converge or may converge to a highly inaccurate estimate.

### Mechanism 2
- **Claim**: The runtime of iterative thresholding scales logarithmically with the target norm, rather than with 1/ε² as in prior work.
- **Mechanism**: By using a gradient descent-based approach rather than a full-solve at each iteration, the runtime is dominated by the number of iterations (which scales logarithmically with the target error) and the cost of each iteration (matrix-vector products). This is more efficient than full-solve approaches which require solving a large linear system at each iteration.
- **Core assumption**: The gradient of the loss function can be computed efficiently, and the Hessian has a bounded condition number.
- **Evidence anchors**:
  - [abstract]: "Our algorithm has a O(polylog(N, d) log(R/ε)) runtime complexity when ||w*||_2 ≤ R, improving upon the O(polylog(N, d)/ε²) runtime complexity of Awasthi et al. (NeurIPS 2022)."
  - [section 3.1]: Runtime analysis showing the algorithm runs in O(N d² log(||w*||/ε)) time.
  - [corpus]: No direct evidence, but the claim is supported by the runtime analysis in the paper.
- **Break condition**: If the Hessian has a very large condition number, or if the gradient computation is very expensive, the runtime may not scale as favorably.

### Mechanism 3
- **Claim**: Iterative thresholding can handle covariate corruption without requiring a preprocessing filtering step.
- **Mechanism**: By directly thresholding on the loss values, the algorithm can identify and remove corrupted samples regardless of whether the corruption is in the covariates or the labels. This is in contrast to prior work which required a separate filtering step to handle covariate corruption.
- **Core assumption**: The loss function is sensitive to both covariate and label corruption, and the uncorrupted samples have lower loss than the corrupted samples on average.
- **Evidence anchors**:
  - [abstract]: "Our main contribution consists of algorithms and corresponding approximation bounds for gradient-based iterative thresholding for several non-linear learning problems and for multitarget linear regression. Our proof techniques extend [Bhatia et al., 2015, Shen and Sanghavi, 2019, Awasthi et al., 2022], as we suppose the adversary also corrupts the covariates."
  - [section 1.1]: Comparison with prior work showing that the proposed algorithm handles covariate corruption without preprocessing.
  - [corpus]: No direct evidence, but the claim is supported by the theoretical analysis in the paper.
- **Break condition**: If the loss function is not sensitive to covariate corruption, or if the uncorrupted samples do not have lower loss on average, the algorithm may fail to identify and remove the corrupted samples.

## Foundational Learning

- **Concept**: Sub-Gaussian distributions and concentration inequalities
  - **Why needed here**: The analysis relies heavily on concentration inequalities for sub-Gaussian random variables to bound the error in the estimate at each iteration.
  - **Quick check question**: What is the sub-Gaussian norm of a Gaussian random variable with variance ν²?
    - **Answer**: The sub-Gaussian norm of a Gaussian random variable with variance ν² is O(ν).

- **Concept**: Linear convergence of gradient descent
  - **Why needed here**: The main mechanism of the algorithm is the linear convergence of the estimate to the optimal solution on the uncorrupted data.
  - **Quick check question**: What is the condition for linear convergence of gradient descent on a strongly convex function?
    - **Answer**: The step size must be less than 2/β, where β is the smoothness parameter of the function.

- **Concept**: Strong convexity and smoothness of loss functions
  - **Why needed here**: These properties are used to bound the error in the estimate and to prove linear convergence.
  - **Quick check question**: What is the definition of strong convexity for a function f?
    - **Answer**: A function f is α-strongly convex if f(y) ≥ f(x) + ∇f(x)ᵀ(y-x) + (α/2)||y-x||² for all x, y in the domain of f.

## Architecture Onboarding

- **Component map**: Data → Loss computation → Thresholding → Gradient computation → Parameter update
- **Critical path**: Data → Loss computation → Thresholding → Gradient computation → Parameter update
- **Design tradeoffs**: Linear convergence vs. steady-state error, runtime vs. accuracy, robustness to covariate corruption vs. requirement of sub-Gaussian data
- **Failure signatures**: Non-convergence, slow convergence, high steady-state error, sensitivity to hyperparameters
- **First 3 experiments**:
  1. Linear regression with synthetic data: Generate sub-Gaussian data with a known ground truth, add label corruption, and run the algorithm to see if it recovers the ground truth within the theoretical error bounds.
  2. Nonlinear neuron learning with synthetic data: Generate data from a known nonlinear neuron model, add both covariate and label corruption, and run the algorithm to see if it learns the correct parameters.
  3. Runtime scaling: Generate data with varying dimensions and corruption levels, and measure the runtime of the algorithm to verify the logarithmic scaling with the target norm.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical approximation bounds for iterative thresholding applied to one-hidden-layer neural networks?
- Basis in paper: [explicit] The authors mention "Our paper has established upper bounds on the approximation error of activation functions, an interesting next step is on upper bounds for the sum of activation functions, i.e. one hidden-layer neural networks."
- Why unresolved: The paper only analyzes single neuron models and doesn't extend the analysis to neural networks with multiple neurons.
- What evidence would resolve it: A formal theorem proving approximation bounds for iterative thresholding on one-hidden-layer networks with various activation functions.

### Open Question 2
- Question: Can the current iterative thresholding techniques handle binary classification problems with ±1 labels?
- Basis in paper: [explicit] "In ±1 classification, considering y = sign(w·x + ξ), the sign function adds an interesting complication. A study on if our current techniques can also handle the sign function would be interesting."
- Why unresolved: The paper focuses on regression problems and doesn't analyze classification settings.
- What evidence would resolve it: A proof showing convergence guarantees for iterative thresholding with the sign activation function or experimental results demonstrating success on classification tasks.

### Open Question 3
- Question: Are the derived approximation bounds for sigmoidal, leaky-ReLU, and ReLU functions minimax optimal?
- Basis in paper: [explicit] "In the linear regression case, Gao [2020] derived the minimax optimal error of O(σǫ). Establishing this result for sigmoidal, leaky-ReLU, and ReLU functions would be helpful in the discussing the strength of our bounds."
- Why unresolved: The paper provides upper bounds but doesn't establish matching lower bounds.
- What evidence would resolve it: A proof showing that no algorithm can achieve better than O(√ε log(1/ε)) error for these activation functions under the strong contamination model.

## Limitations
- Analysis relies heavily on sub-Gaussian assumptions about uncorrupted data distribution, which may not hold in practical settings
- Logarithmic runtime improvement assumes bounded condition number of the Hessian
- Linear convergence guarantee requires careful parameter tuning, particularly for learning rate and iteration count

## Confidence
- **High confidence**: Runtime complexity claims (O(N d² log(||w*||/ε))) and basic algorithmic structure
- **Medium confidence**: Error bounds for linear regression (O(νε log(1/ε))) under stated assumptions
- **Low confidence**: Error bounds for nonlinear neurons (O(√ε log(1/ε))) as they depend on more complex concentration arguments

## Next Checks
1. **Sensitivity analysis**: Test algorithm performance across varying corruption fractions ε ∈ [0.01, 0.3] and verify the predicted error scaling O(√ε log(1/ε)) holds empirically

2. **Distribution robustness**: Evaluate performance on non-sub-Gaussian data distributions (e.g., heavy-tailed) to assess practical limitations of the theoretical assumptions

3. **Hyperparameter dependence**: Systematically study the impact of learning rate η and iteration count T on convergence speed and final accuracy to validate the claimed linear convergence rate