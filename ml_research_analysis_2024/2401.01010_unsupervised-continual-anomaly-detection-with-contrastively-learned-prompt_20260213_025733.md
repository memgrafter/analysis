---
ver: rpa2
title: Unsupervised Continual Anomaly Detection with Contrastively-learned Prompt
arxiv_id: '2401.01010'
source_url: https://arxiv.org/abs/2401.01010
tags:
- anomaly
- detection
- continual
- image
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes UCAD, a novel unsupervised continual anomaly
  detection framework that enables a single model to sequentially learn to detect
  anomalies across different classes without catastrophic forgetting. The key idea
  is to use a key-prompt-knowledge memory bank to automatically retrieve task-specific
  prompts for guiding anomaly detection, combined with structure-based contrastive
  learning to enhance feature representations across tasks using SAM segmentation
  masks.
---

# Unsupervised Continual Anomaly Detection with Contrastively-learned Prompt

## Quick Facts
- arXiv ID: 2401.01010
- Source URL: https://arxiv.org/abs/2401.01010
- Authors: Jiaqi Liu; Kai Wu; Qiang Nie; Ying Chen; Bin-Bin Gao; Yong Liu; Jinbao Wang; Chengjie Wang; Feng Zheng
- Reference count: 25
- Key outcome: UCAD achieves 15.6% higher image AUROC and 26.6% higher pixel AUPR than competing methods on MVTec AD and VisA datasets

## Executive Summary
This paper introduces UCAD, an unsupervised continual anomaly detection framework that enables a single model to sequentially learn to detect anomalies across different classes without catastrophic forgetting. The key innovation is a Continual Prompting Module (CPM) with a key-prompt-knowledge memory bank that automatically identifies tasks and retrieves task-specific prompts for guiding anomaly detection. Combined with Structure-based Contrastive Learning (SCL) using SAM segmentation masks, UCAD significantly outperforms state-of-the-art methods, even those using rehearsal training.

## Method Summary
UCAD addresses unsupervised continual anomaly detection through two main components: a Continual Prompting Module (CPM) that uses a key-prompt-knowledge memory bank to automatically identify tasks and retrieve task-specific prompts, and Structure-based Contrastive Learning (SCL) that leverages SAM segmentation masks to enhance feature representations. The method trains on sequential tasks without task labels, using a frozen ViT encoder to extract keys for task identification and features for anomaly detection. CPM stores learned prompts and knowledge representations instead of raw samples, while SCL applies contrastive learning to improve feature compactness and discriminativeness across tasks.

## Key Results
- UCAD achieves 15.6% higher image AUROC and 26.6% higher pixel AUPR than competing methods on MVTec AD and VisA datasets
- Outperforms state-of-the-art methods including those using rehearsal training
- Demonstrates effective continual learning without catastrophic forgetting across multiple object classes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: UCAD's Continual Prompting Module (CPM) enables task-agnostic continual learning by automatically identifying tasks and retrieving task-specific prompts and knowledge without supervision.
- Mechanism: The CPM uses a key-prompt-knowledge memory bank where task identities (keys) are extracted from a frozen ViT encoder, learnable prompts adapt features for the current task, and knowledge stores representative normal features. During inference, the system automatically selects the most similar key to retrieve the corresponding prompt and knowledge, enabling adaptation to new tasks without task labels.
- Core assumption: Task identities can be effectively captured by features from an intermediate layer of a frozen ViT encoder, and the similarity between these features reliably indicates task membership.
- Evidence anchors:
  - [abstract]: "In the proposed UCAD, we design a Continual Prompting Module (CPM) by utilizing a concise key-prompt-knowledge memory bank to guide task-invariant 'anomaly' model predictions using task-specific 'normal' knowledge."
  - [section]: "In the task identification phase, images x ∈ RH×W ×C will go through a frozen pretrained vision transformer f (ViT) to extract keys k ∈ K e, also known as task identities."
  - [corpus]: Weak - no direct evidence found in related papers for this specific key-prompt-knowledge architecture approach.
- Break condition: If the feature representation from the frozen ViT layer does not capture sufficient discriminative information about task identity, the automatic task selection will fail and the wrong prompts/knowledge will be retrieved.

### Mechanism 2
- Claim: Structure-based Contrastive Learning (SCL) with SAM segmentation masks enhances feature representations across tasks by pulling features within the same structure closer and pushing different structures apart.
- Mechanism: SAM generates segmentation masks that identify structures within images. Features from the same masked regions are pulled together while features from different regions are pushed apart through contrastive loss. This creates more compact and discriminative feature representations that generalize better across different tasks.
- Core assumption: SAM's segmentation masks provide meaningful structural information that can be used to create effective contrastive learning signals, and that these structural groupings are consistent enough across different tasks to create useful feature representations.
- Evidence anchors:
  - [abstract]: "Moreover, Structure-based Contrastive Learning (SCL) is designed with the Segment Anything Model (SAM) to improve prompt learning and anomaly segmentation results. Specifically, by treating SAM's masks as structure, we draw features within the same mask closer and push others apart for general feature representations."
  - [section]: "By incorporating contrastive learning, the knowledge generality in Kn is achieved by pulling the features of the same region closer and pushing the features of different regions further apart."
  - [corpus]: Weak - no direct evidence found in related papers for this specific approach of using SAM masks for contrastive learning in anomaly detection.
- Break condition: If SAM's segmentation masks are inconsistent or not meaningful for the types of industrial images used, the contrastive learning signals will be noisy and may degrade rather than improve feature representations.

### Mechanism 3
- Claim: The combination of CPM and SCL allows UCAD to outperform methods using rehearsal training by maintaining discriminative features without requiring stored training samples.
- Mechanism: CPM provides task-specific adaptation through learned prompts and knowledge without storing raw samples, while SCL enhances feature compactness and discriminativeness. Together, they enable the model to retain knowledge of previous tasks and detect anomalies effectively without the memory overhead of rehearsal methods.
- Core assumption: The learned prompts and contrastive features can capture and maintain sufficient information about normal patterns across tasks without needing to store actual training samples, and this compressed representation is more effective than simple replay buffers.
- Evidence anchors:
  - [abstract]: "Experiments on MVTec AD and VisA datasets show UCAD significantly outperforms state-of-the-art methods, achieving 15.6% higher image AUROC and 26.6% higher pixel AUPR than competing methods, even those using rehearsal training."
  - [section]: "In these experiments, we provided them with a buffer capable of storing 100 training samples." (comparing against replay-based methods)
  - [corpus]: Weak - no direct evidence found in related papers comparing against rehearsal methods specifically in unsupervised continual anomaly detection.
- Break condition: If the compressed representation in the key-prompt-knowledge memory bank loses critical information about normal patterns, the model will fail to maintain performance on previous tasks compared to rehearsal methods that store actual samples.

## Foundational Learning

- Concept: Continual Learning and Catastrophic Forgetting
  - Why needed here: UCAD addresses the challenge of learning new tasks sequentially without forgetting previously learned knowledge. Understanding how catastrophic forgetting occurs and the mechanisms to prevent it (like rehearsal, regularization, and parameter isolation) is fundamental to appreciating UCAD's approach.
  - Quick check question: What is catastrophic forgetting and why does it pose a particular challenge for unsupervised anomaly detection compared to supervised learning?

- Concept: Contrastive Learning
  - Why needed here: SCL uses contrastive learning principles to create compact feature representations. Understanding how contrastive learning works (pulling together similar samples while pushing apart dissimilar ones) and its benefits for representation learning is essential to grasp how SCL improves UCAD's performance.
  - Quick check question: How does contrastive learning differ from traditional supervised classification, and what advantages does it offer for unsupervised anomaly detection?

- Concept: Vision Transformers and Feature Extraction
  - Why needed here: UCAD uses a frozen ViT encoder to extract keys for task identification and features for anomaly detection. Understanding how ViTs work, how features are extracted at different layers, and why intermediate layers might be preferred for this task is crucial for implementing and modifying UCAD.
  - Quick check question: Why might an intermediate layer of a ViT be more suitable than the final layer for extracting task-identifying features in UCAD's CPM?

## Architecture Onboarding

- Component map: Frozen ViT Encoder -> Continual Prompting Module (CPM) with key-prompt-knowledge memory bank -> Structure-based Contrastive Learning (SCL) with SAM masks -> Anomaly Detection Module

- Critical path:
  1. Extract keys from frozen ViT (task identification)
  2. Select most similar key to retrieve prompt and knowledge
  3. Apply prompt to current image features
  4. Generate SAM segmentation masks
  5. Apply SCL to enhance features
  6. Compare enhanced features with knowledge to detect anomalies

- Design tradeoffs:
  - Using a frozen ViT backbone provides stable feature extraction but limits adaptability compared to fine-tuning
  - Storing prompts and knowledge instead of raw samples saves memory but may lose some information
  - Relying on SAM for segmentation provides general structure knowledge without training but may not be optimal for specific industrial domains
  - Task-agnostic approach increases flexibility but may be less precise than task-aware methods when task identities are available

- Failure signatures:
  - Poor task identification: Low performance on previously learned tasks when new tasks are added
  - Noisy segmentation: SAM masks don't align well with actual structures in industrial images
  - Degraded features: SCL contrastive loss doesn't improve feature compactness and may even harm performance
  - Memory inefficiency: Key-prompt-knowledge structure becomes too large as tasks accumulate

- First 3 experiments:
  1. Validate task identification: Test CPM's ability to correctly identify tasks on a held-out validation set with known task labels
  2. Test SCL effectiveness: Compare feature representations with and without SCL using a metric like feature compactness or downstream anomaly detection performance
  3. Measure forgetting: Evaluate performance degradation on previous tasks after training on new tasks to quantify catastrophic forgetting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of UCAD scale with the number of tasks/classes in continual learning scenarios?
- Basis in paper: [inferred] The paper demonstrates UCAD's effectiveness on MVTec AD and VisA datasets but doesn't explore performance degradation as the number of tasks increases.
- Why unresolved: The experiments only cover a limited number of tasks, and there's no analysis of how UCAD handles an increasing number of classes over time.
- What evidence would resolve it: Systematic experiments varying the number of tasks while measuring image AUROC, pixel AUPR, and forgetting measure (FM) to identify performance trends and limitations.

### Open Question 2
- Question: How sensitive is UCAD's performance to the choice of SAM segmentation mask parameters and quality?
- Basis in paper: [explicit] The paper states that SAM's masks are used as structure for contrastive learning but doesn't explore how different segmentation qualities or parameters affect performance.
- Why unresolved: The experiments use SAM's default settings without investigating the impact of segmentation quality on feature representation and anomaly detection accuracy.
- What evidence would resolve it: Controlled experiments varying SAM parameters (e.g., confidence thresholds, number of masks) and comparing the resulting UCAD performance metrics.

### Open Question 3
- Question: What is the computational overhead of UCAD compared to non-continual methods during inference?
- Basis in paper: [inferred] While the paper discusses memory requirements of the key-prompt-knowledge structure, it doesn't provide detailed inference time comparisons with baseline methods.
- Why unresolved: The paper focuses on memory efficiency and detection accuracy but doesn't quantify the real-time inference performance trade-offs compared to traditional non-continual approaches.
- What evidence would resolve it: Comprehensive benchmarking of inference times across different methods including UCAD, PatchCore, and UniAD on the same hardware platforms.

## Limitations
- Memory Bank Scalability: The key-prompt-knowledge memory bank may become increasingly inefficient as the number of tasks grows, with no analysis of memory complexity or retrieval performance degradation with scale.
- SAM Dependency: Heavy reliance on SAM for segmentation masks introduces a critical external dependency. SAM's segmentation quality may vary significantly across different industrial domains, and performance could degrade if SAM's mask quality is poor.
- Task-Identity Assumption: The approach assumes task identities can be reliably captured by frozen ViT features, but this may fail for tasks with similar visual characteristics or when the ViT encoder isn't sufficiently pretrained on relevant data.

## Confidence
- High Confidence: The overall experimental results and performance claims (AUROC, AUPR improvements over baselines) are well-supported by the MVTec AD and VisA dataset evaluations.
- Medium Confidence: The mechanism of using SAM masks for contrastive learning is plausible but lacks strong validation that this specific approach is superior to other contrastive learning strategies.
- Low Confidence: The claim that UCAD outperforms rehearsal-based methods without providing comparative memory usage or scalability analysis leaves uncertainty about the practical advantages.

## Next Checks
1. **Memory Efficiency Analysis**: Quantify the growth of the key-prompt-knowledge memory bank across tasks and measure retrieval time/complexity as task count increases to assess scalability limits.
2. **SAM Robustness Testing**: Systematically evaluate performance degradation when SAM mask quality varies by testing on datasets with different segmentation characteristics or by intentionally degrading SAM's output.
3. **Task-Identity Ablation Study**: Compare UCAD's performance against a variant that uses known task identities to quantify the cost of the task-agnostic approach and identify scenarios where task awareness would be beneficial.