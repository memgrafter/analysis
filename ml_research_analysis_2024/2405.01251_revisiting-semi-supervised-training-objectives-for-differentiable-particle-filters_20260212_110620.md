---
ver: rpa2
title: Revisiting semi-supervised training objectives for differentiable particle
  filters
arxiv_id: '2405.01251'
source_url: https://arxiv.org/abs/2405.01251
tags:
- particle
- training
- filters
- data
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper revisits semi-supervised training objectives for differentiable
  particle filters, focusing on two key methods: pseudo-likelihood loss and evidence
  lower bound (ELBO) loss. The authors evaluate these methods in two simulated environments
  where labeled data is scarce.'
---

# Revisiting semi-supervised training objectives for differentiable particle filters

## Quick Facts
- arXiv ID: 2405.01251
- Source URL: https://arxiv.org/abs/2405.01251
- Authors: Jiaxi Li; John-Joseph Brady; Xiongjie Chen; Yunpeng Li
- Reference count: 30
- Key outcome: ELBO loss improves DPF performance with limited labeled data in one simulation setup, while pseudo-likelihood loss shows no benefit in either experimental setup

## Executive Summary
This paper investigates semi-supervised training methods for differentiable particle filters, specifically evaluating pseudo-likelihood loss and evidence lower bound (ELBO) loss. The authors test these approaches in two simulated environments where labeled data is scarce, finding that ELBO loss provides measurable improvement in a linear Gaussian setup but pseudo-likelihood loss fails to enhance performance in either environment. These results challenge previous findings and suggest that the effectiveness of semi-supervised methods depends heavily on the specific problem context. The work highlights the need for careful consideration when applying semi-supervised techniques to particle filter training.

## Method Summary
The authors implement differentiable particle filters with two semi-supervised training objectives: pseudo-likelihood loss and ELBO loss. Both methods combine supervised MSE loss on labeled data with unsupervised losses on unlabeled data. The ELBO loss uses variational inference principles to optimize the particle filter's ability to explain observed data, while pseudo-likelihood loss estimates a stationary distribution over latent states. Models are trained across varying percentages of labeled data (0.1% to 100%) and evaluated using RMSE in both linear Gaussian and simulated maze environments.

## Key Results
- ELBO loss shows improvement in linear Gaussian simulation with limited labeled data (0.1-10%)
- Pseudo-likelihood loss fails to enhance performance in either experimental setup
- Results contrast with previous work suggesting broader effectiveness of these semi-supervised methods
- Performance improvements are context-dependent rather than universal

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ELBO loss improves DPF performance when labeled data is scarce because it provides a variational lower bound on the marginal likelihood, guiding learning even without ground truth states.
- Mechanism: The ELBO loss (LELBO) acts as a surrogate objective that optimizes the particle filter to better explain observed data, indirectly improving state estimation when labels are limited.
- Core assumption: The system dynamics and measurement model are sufficiently learnable from observations alone to make the ELBO a useful signal.
- Evidence anchors:
  - [abstract] "While ELBO loss shows some improvement in one linear Gaussian simulation setup with limited labeled data"
  - [section] "Using the LELBO(θ) to optimise the proposal is theoretically justified for smoothing trajectories in [17], [19]. It has also been successfully applied to filtering tasks in [24], [28], and [11]"
  - [corpus] Weak evidence - related papers discuss ELBO in DPF context but don't specifically validate semi-supervised settings with limited labels
- Break condition: If the model becomes too complex or non-identifiable from observations alone, the ELBO may not provide meaningful gradients.

### Mechanism 2
- Claim: Pseudo-likelihood loss fails to improve performance because the assumed stationary distribution assumption is violated or the block-based estimation is unstable.
- Mechanism: PL loss tries to estimate a pseudo-likelihood by dividing trajectories into blocks and assuming a stationary distribution, but this breaks down when this assumption doesn't hold or when the estimation becomes numerically unstable.
- Core assumption: A stationary distribution µθ(xt) exists and can be reasonably approximated as uniform over some subset of latent space
- Evidence anchors:
  - [abstract] "the pseudo-likelihood loss does not enhance performance in either experimental setup"
  - [section] "This unsupervised training objective can be expressed as..." followed by the block-based formula with stationary distribution assumption
  - [corpus] Weak evidence - no direct discussion of why pseudo-likelihood fails in semi-supervised DPF settings
- Break condition: When the true system dynamics are far from stationary or when the latent space has complex structure that cannot be captured by a simple uniform assumption.

### Mechanism 3
- Claim: Semi-supervised training works by combining limited labeled data with unsupervised objectives to mitigate the non-identifiability problem in observation-only settings.
- Mechanism: The supervised MSE loss provides direct guidance where labels exist, while unsupervised losses (ELBO or PL) regularize the model behavior on unlabeled data, helping to constrain the otherwise non-identifiable model parameters.
- Core assumption: The combination of supervised and unsupervised losses can effectively regularize model learning even when most data lacks labels
- Evidence anchors:
  - [abstract] "We hypothesise that augmenting conventional supervised learning over a limited dataset with unsupervised learning techniques over a larger dataset can lead to improved performance"
  - [section] "Semi-supervised losses are constructed as a linear combination of a supervised loss and an unsupervised loss"
  - [corpus] Weak evidence - related papers discuss semi-supervised learning but not specifically in the context of DPF non-identifiability
- Break condition: If the unsupervised loss conflicts with the supervised loss or if the unlabeled data is too different from the labeled data distribution.

## Foundational Learning

- Concept: Sequential Monte Carlo (Particle Filtering)
  - Why needed here: DPFs are built on particle filtering foundations - understanding importance sampling, weight updates, and resampling is crucial
  - Quick check question: What happens to particle weights if the proposal distribution doesn't match the target distribution well?

- Concept: Normalizing Flows
  - Why needed here: NF-DPFs use normalizing flows to parameterize complex distributions for dynamics and proposals - understanding change of variables and invertible mappings is essential
  - Quick check question: How does the Jacobian determinant appear in the probability density transformation when using normalizing flows?

- Concept: Evidence Lower Bound (ELBO) and Variational Inference
  - Why needed here: ELBO loss is derived from variational inference principles and provides the unsupervised training signal
  - Quick check question: Why is the ELBO a lower bound to the log marginal likelihood, and what does Jensen's inequality have to do with it?

## Architecture Onboarding

- Component map:
  - SSM components: dynamic model pθ(xt|xt-1), measurement model pθ(yt|xt), proposal qθ(xt|yt, xt-1)
  - Normalizing flow layers for each component (Tθ, Fθ, Gθ)
  - Particle filter implementation with resampling
  - Loss computation modules (MSE, ELBO, pseudo-likelihood)
  - Training loop with optimizer

- Critical path:
  1. Sample particles from proposal
  2. Compute weights based on likelihood
  3. Optional resampling
  4. Compute loss (combination of supervised and unsupervised)
  5. Backpropagate through particle filter operations
  6. Update model parameters

- Design tradeoffs:
  - Number of particles vs. computational cost and gradient variance
  - Proposal flexibility vs. stability of weight updates
  - Strength of unsupervised loss vs. potential for conflicting gradients
  - Resampling frequency vs. particle diversity

- Failure signatures:
  - Degenerate weights (all weight concentrated on few particles) - check proposal quality
  - Unstable training with exploding/vanishing gradients - check ELBO implementation and particle count
  - No improvement from semi-supervised losses - verify unlabeled data distribution matches labeled data

- First 3 experiments:
  1. Test basic DPF with MSE loss only on fully labeled data to verify particle filter implementation
  2. Add ELBO loss to DPF and compare performance with varying amounts of labeled data
  3. Implement pseudo-likelihood loss and verify it doesn't improve performance (replicating paper's findings)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions does the ELBO loss outperform the pseudo-likelihood loss in semi-supervised training of differentiable particle filters?
- Basis in paper: [explicit] The paper demonstrates that ELBO loss shows improvement in a linear Gaussian simulation setup with limited labeled data, while pseudo-likelihood loss does not enhance performance in either experimental setup.
- Why unresolved: The paper only tests these methods in two specific environments (multivariate linear Gaussian and simulated maze). The conditions under which each loss function excels are not fully characterized.
- What evidence would resolve it: Systematic testing across a broader range of environments with varying characteristics (e.g., different levels of non-linearity, dimensionality, and identifiability) would clarify the conditions favoring each loss function.

### Open Question 2
- Question: How does the choice of proposal distribution affect the performance of semi-supervised differentiable particle filters?
- Basis in paper: [inferred] The paper uses both bootstrap particle filters and normalising flow-based particle filters, suggesting that the choice of proposal distribution may impact performance.
- Why unresolved: The paper does not directly compare different proposal distributions within the same experimental setup to isolate their effects on semi-supervised learning performance.
- What evidence would resolve it: Conducting experiments where the only variable is the proposal distribution type (e.g., bootstrap vs. learned proposals) while keeping other components constant would reveal the impact of this choice.

### Open Question 3
- Question: What is the optimal balance between supervised and unsupervised loss components in semi-supervised training of differentiable particle filters?
- Basis in paper: [explicit] The paper uses a linear combination of supervised and unsupervised losses but does not provide a systematic method for determining the optimal weighting coefficients (λ1 and λ2).
- Why unresolved: The paper relies on grid search for coefficient selection, which may not find the global optimum and does not provide insight into how this balance should be adjusted for different problem characteristics.
- What evidence would resolve it: Developing a principled method for setting these coefficients based on problem characteristics (e.g., amount of labeled data, model complexity) and validating it across diverse scenarios would address this question.

## Limitations
- Experimental scope limited to two simulation environments (linear Gaussian and maze)
- No validation on real-world data or complex non-linear systems
- Lack of systematic analysis for optimal hyperparameter settings
- Computational overhead differences between supervised and semi-supervised approaches not addressed

## Confidence
- ELBO loss effectiveness: Medium confidence - demonstrated in one experimental setup but limited generalizability
- Pseudo-likelihood failure: Low confidence - insufficient mechanistic explanation for poor performance
- Overall results: Medium confidence - findings are well-documented but may be context-specific

## Next Checks
1. Replicate experiments across additional state-space model types (non-linear, non-Gaussian) to test generalizability of ELBO benefits
2. Implement alternative unsupervised objectives (e.g., contrastive losses) to determine if ELBO's performance is specific to its variational properties
3. Conduct computational efficiency analysis comparing training time and memory usage between supervised and semi-supervised approaches across different particle counts