---
ver: rpa2
title: An Empirical Investigation of Value-Based Multi-objective Reinforcement Learning
  for Stochastic Environments
arxiv_id: '2401.03163'
source_url: https://arxiv.org/abs/2401.03163
tags:
- policy
- learning
- state
- reward
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper examines the ability of multi-objective Q-learning
  algorithms to find the SER-optimal policy in stochastic environments. It tests three
  approaches: modified reward structures, global statistics, and policy options, across
  various Space Traders environments.'
---

# An Empirical Investigation of Value-Based Multi-objective Reinforcement Learning for Stochastic Environments

## Quick Facts
- arXiv ID: 2401.03163
- Source URL: https://arxiv.org/abs/2401.03163
- Reference count: 24
- Key outcome: Noisy Q-value estimates from environmental stochasticity significantly impact stability and convergence of multi-objective Q-learning algorithms, preventing reliable SER-optimal policy learning.

## Executive Summary
This paper investigates the challenges of finding SER-optimal policies in stochastic environments using multi-objective Q-learning algorithms. Through systematic experiments across multiple Space Traders variants, the authors demonstrate that both the local decision-making nature of Q-learning and noisy Q-value estimates arising from environmental stochasticity must be addressed together to achieve reliable SER-optimal policy learning. While policy options can eliminate local decision-making issues, they fail without addressing the noise in estimates, highlighting the fundamental challenge of optimizing for scalarised expected returns in uncertain environments.

## Method Summary
The study tests four multi-objective Q-learning approaches - baseline, reward engineering, global statistics (MOSS), and policy options - across three Space Traders environments with varying complexity. Each algorithm is evaluated for its ability to converge to the SER-optimal policy over 20 independent trials with 20,000 episodes per trial. The environments feature two states with three actions each and stochastic state transitions, with rewards structured to create a trade-off between immediate and delayed returns. Performance is measured by the frequency of convergence to the SER-optimal policy, with additional analysis of reward collection patterns and policy evolution during training.

## Key Results
- Noisy Q-value estimates from environmental stochasticity significantly disrupt SER-optimal policy convergence
- Policy options eliminate local decision-making issues but still fail without addressing noisy estimates
- Neither reward engineering nor global statistics approaches reliably achieve SER-optimal policies in stochastic environments
- The SER-optimal policy (DI) was found in less than 20% of trials across all tested approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Noisy Q-value estimates, arising from environmental stochasticity, significantly disrupt the stability and convergence of multi-objective Q-learning algorithms.
- **Mechanism:** In stochastic environments, Q-values are estimated based on sampled trajectories, introducing variance. This noise causes frequent policy changes as actions with similar scalarised values may switch due to small reward vector differences, particularly problematic for non-linear utility functions like TLO where threshold changes cause incorrect action selection.
- **Core assumption:** Environmental stochasticity leads to variations in returns across episodes.
- **Evidence anchors:**
  - [abstract]: "noisy Q-value estimates, arising from environmental stochasticity, significantly impact the stability and convergence of these algorithms."
  - [section]: "A second issue, identified by both [21] and [22], is the problem of noisy Q-value estimates..."
- **Break condition:** If the environment becomes deterministic, noise in Q-value estimates would disappear.

### Mechanism 2
- **Claim:** Local decision-making in Q-learning is incompatible with SER optimization in stochastic environments.
- **Mechanism:** SER optimization requires maximizing scalarised expected return over multiple executions, but Q-learning selects actions based on local information (current state Q-values and accumulated episode rewards). This local approach fails to consider expected returns in episodes where the current state isn't reached, which is necessary for SER optimization.
- **Core assumption:** Stochastic state transitions create different trajectories and rewards in each episode.
- **Evidence anchors:**
  - [abstract]: "issues can arise with this approach in the context of stochastic environments, particularly when optimising for the Scalarised Expected Reward (SER) criterion."
  - [section]: "In order to find the SER-optimal policy for problems with stochastic rewards and a non-linear utility function..."
- **Break condition:** If the environment becomes deterministic, global trajectory information would become unnecessary.

### Mechanism 3
- **Claim:** Policy options can eliminate local decision-making but still fail without addressing noisy estimates.
- **Mechanism:** By selecting complete policy options at episode start and committing to them, agents avoid local decisions and consider full-episode returns compatible with SER optimization. However, this approach is limited by exponential growth in options with state/action space size, and still suffers from noisy Q-value estimates during option selection.
- **Core assumption:** Environment complexity allows manageable definition of policy options.
- **Evidence anchors:**
  - [abstract]: "While methods like policy options can address the core issue of local decision-making, they still fail without addressing the noise in estimates."
  - [section]: "The use we make of options here differs from their usual application..."
- **Break condition:** If environment becomes too complex, option space becomes unmanageable.

## Foundational Learning

- **Concept:** Scalarisation functions in multi-objective reinforcement learning.
  - **Why needed here:** Scalarisation functions convert vector rewards to scalars for action selection and comparison. Understanding different types (linear vs non-linear) is crucial for understanding algorithm behavior in stochastic environments.
  - **Quick check question:** What is the difference between linear and non-linear scalarisation functions, and why is this difference important in stochastic environments?

- **Concept:** The Scalarised Expected Return (SER) criterion.
  - **Why needed here:** SER aims to maximize scalarised expected return over multiple executions. Understanding SER is essential for grasping the optimization goal and challenges faced by algorithms in stochastic environments.
  - **Quick check question:** How does SER differ from Expected Scalarised Return (ESR), and why is this difference important in stochastic environments?

- **Concept:** The thresholded lexicographic ordering (TLO) utility function.
  - **Why needed here:** TLO prioritizes one objective while meeting thresholds on others. Understanding TLO is crucial for recognizing why multi-objective Q-learning algorithms are particularly sensitive to noisy Q-value estimates in stochastic environments.
  - **Quick check question:** How does TLO work, and why is it particularly sensitive to noisy Q-value estimates in stochastic environments?

## Architecture Onboarding

- **Component map:**
  - Space Traders environment: Two states (A, B), three actions per state (direct, indirect, teleport), stochastic transitions
  - Multi-objective Q-learning algorithms: Baseline, reward engineering, global statistics, policy options
  - Utility function: Thresholded lexicographic ordering (TLO) with success threshold
  - Performance metric: Frequency of convergence to SER-optimal policy

- **Critical path:**
  1. Initialize Q-values for each state-action pair
  2. For each episode, select action based on current Q-values and utility function
  3. Execute action, observe next state and reward, update Q-values using Q-learning rule
  4. Repeat until episode end
  5. After training, evaluate final greedy policy against SER-optimal policy

- **Design tradeoffs:**
  - Baseline approach: Simple but fails to find SER-optimal policy in most cases
  - Reward engineering: Provides additional information but may not be feasible for all environments
  - Global statistics: Incorporates global information but insufficient to overcome noisy estimates
  - Policy options: Eliminates local decision-making but suffers curse of dimensionality

- **Failure signatures:**
  - Inconsistent greedy policy: Agent frequently switches between different policies during learning
  - Suboptimal final policy: Agent converges to non-SER-optimal policy
  - High variance in Q-value estimates: Q-values fluctuate significantly across episodes

- **First 3 experiments:**
  1. **Baseline MOQ-learning:** Run baseline algorithm on Space Traders with constant learning rate to confirm findings and establish baseline
  2. **Reward engineering:** Modify reward structure to provide additional information about terminal transitions, evaluate baseline with modified reward
  3. **Policy options:** Implement policy options approach on Space Traders, compare performance to baseline and reward engineering

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical findings limited to simple discrete MOMDPs with small state and action spaces
- Analysis assumes primary variance source is environmental stochasticity, not function approximation or exploration noise
- Focus on thresholded lexicographic ordering utility functions leaves generalization to other non-linear functions unverified
- Policy options face exponential scalability barriers not tested beyond 3-state variant

## Confidence
- **High confidence**: Core finding that noisy Q-value estimates disrupt SER-optimal policy convergence in stochastic environments
- **Medium confidence**: Conclusion that both local decision-making and noisy estimates must be addressed together
- **Low confidence**: Generalization claims about performance in larger or continuous state spaces

## Next Checks
1. **Replicate with alternative non-linear utility functions** (e.g., epsilon-constraint method) to verify sensitivity to thresholded lexicographic ordering is not a special case
2. **Implement variance reduction techniques** (e.g., temporal difference errors clipping, ensemble Q-learning) to test whether noise mitigation alone can achieve SER-optimal convergence
3. **Scale policy options to larger state spaces** using approximate policy representation (e.g., neural networks) to assess practical viability beyond curse of dimensionality