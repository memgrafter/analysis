---
ver: rpa2
title: Generally-Occurring Model Change for Robust Counterfactual Explanations
arxiv_id: '2407.11426'
source_url: https://arxiv.org/abs/2407.11426
tags:
- change
- counterfactual
- explanations
- generally-occurring
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of robustness of counterfactual
  explanations to model changes. The authors generalize the concept of Naturally-Occurring
  Model Change to a more general concept called Generally-Occurring Model Change,
  which has a wider range of applicability.
---

# Generally-Occurring Model Change for Robust Counterfactual Explanations

## Quick Facts
- arXiv ID: 2407.11426
- Source URL: https://arxiv.org/abs/2407.11426
- Reference count: 28
- Key outcome: Generalizes Naturally-Occurring Model Change to Generally-Occurring Model Change with probabilistic guarantees for counterfactual robustness to model changes

## Executive Summary
This paper addresses the robustness of counterfactual explanations to model changes by introducing a more general framework called Generally-Occurring Model Change (GOMC). The authors prove probabilistic guarantees that bound the difference between predictions of original and changed models for counterfactual instances, based on Lipschitz continuity and data distribution properties. They also demonstrate how dataset perturbations can be modeled as GOMC, providing theoretical results on counterfactual robustness to training data changes.

## Method Summary
The paper develops a theoretical framework for analyzing counterfactual explanation robustness by generalizing the concept of Naturally-Occurring Model Change to Generally-Occurring Model Change. This involves defining conditions under which model parameter changes can be characterized by expectation bounds, subgaussian properties, and Lipschitz continuity. The authors then prove a main theorem providing probability guarantees for counterfactual robustness under GOMC, and apply this to analyze the specific case of dataset perturbations using optimization theory.

## Key Results
- Theorem 2 provides a probability bound on prediction differences between original and changed models for counterfactual instances, based on Lipschitz constants and data distribution
- Theorem 3 shows that dataset perturbations can be modeled as GOMC under certain optimization assumptions, enabling analysis of counterfactual robustness to training data changes
- The generalized GOMC framework removes restrictive requirements from the original NOMC concept while maintaining probabilistic guarantees

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generally-Occurring Model Change (GOMC) generalizes Naturally-Occurring Model Change (NOMC) by removing the restrictive requirement that the model change distribution is centered at the original model.
- Mechanism: By allowing the distribution of M(·) to be not centered at m(·), GOMC expands the range of model changes that can be analyzed while still maintaining probabilistic guarantees through subgaussian conditions and Lipschitz continuity.
- Core assumption: The Lipschitz condition holds for both the original and changed models, and the L2 norm difference between models is subgaussian.
- Evidence anchors:
  - [abstract] "We first further generalize the concept of Naturally-Occurring Model Change, proposing a more general concept of model parameter changes, Generally-Occurring Model Change, which has a wider range of applicability."
  - [section] "Definition 6 (Generally-Occurring Model Change, Main Concept)" which explicitly states the expectation condition is an inequality rather than an equality.
- Break condition: If the Lipschitz condition fails or the model difference is not subgaussian, the probabilistic guarantees no longer hold.

### Mechanism 2
- Claim: The probabilistic guarantee for GOMC (Theorem 2) extends NOMC by not requiring Gaussian sampling distributions near counterfactual instances.
- Mechanism: By using general distributions with bounded likelihood ratio κ(˜µ, µ), the theorem can handle more realistic data distributions that deviate from Gaussian assumptions.
- Core assumption: The sampling distribution near counterfactual instances can be characterized by its likelihood ratio to the data distribution.
- Evidence anchors:
  - [abstract] "We also prove the corresponding probabilistic guarantees."
  - [section] "Theorem 2 (Probability Guarantees under Gernerally-Occurring Model Change, Main Theorem)" which explicitly uses κ(˜µ, µ) instead of a Gaussian parameter.
- Break condition: If the likelihood ratio κ(˜µ, µ) is unbounded or undefined, the guarantee becomes vacuous.

### Mechanism 3
- Claim: Dataset perturbation can be modeled as a GOMC, allowing the use of Theorem 2 to analyze counterfactual robustness to training data changes.
- Mechanism: By showing that the parameter difference between models trained on perturbed datasets is bounded by a function of the step sizes and the number of changed examples, the model change fits the GOMC framework.
- Core assumption: The loss function satisfies Lipschitz, smoothness, and left-admissibility conditions, and gradient descent is used for training.
- Evidence anchors:
  - [abstract] "we consider the specific problem of dataset perturbation, and use the probabilistic guarantees for generally-occurring model change to give relevant theoretical results based on optimization theory."
  - [section] "Theorem 3 (Convex Optimization)" which explicitly derives bounds on model parameter differences for dataset perturbation.
- Break condition: If the optimization algorithm deviates significantly from gradient descent assumptions or the loss function doesn't satisfy the required conditions, the bounds no longer apply.

## Foundational Learning

- Concept: Lipschitz continuity
  - Why needed here: Ensures that small changes in input lead to proportionally small changes in model output, which is crucial for bounding the difference between original and changed model predictions.
  - Quick check question: If a model is γ-Lipschitz, what is the maximum possible difference in output between two inputs that are distance d apart?

- Concept: Subgaussian random variables
  - Why needed here: Provides tail bounds that decay exponentially, which is essential for the probabilistic guarantees in Theorem 2.
  - Quick check question: What is the relationship between the subgaussian parameter and the decay rate of tail probabilities?

- Concept: Gradient descent dynamics
  - Why needed here: Understanding how model parameters evolve during training is crucial for analyzing the impact of dataset perturbations on model changes.
  - Quick check question: In gradient descent, how does the parameter update rule relate to the loss function gradient and learning rate?

## Architecture Onboarding

- Component map: Data distribution (μ, ˜μ) -> Model training (m(·), M(·)) -> Counterfactual generation -> Robustness analysis (Theorem 2) -> Output explanation
- Critical path: Data → Model training → Counterfactual generation → Robustness analysis → Output explanation
- Design tradeoffs: Balancing between generality of GOMC (wider applicability) and tightness of probabilistic bounds (stronger guarantees require stronger assumptions)
- Failure signatures: 
  - Unbounded Lipschitz constants → degraded bound quality
  - Non-subgaussian model differences → failed probability guarantees
  - Large likelihood ratio κ(˜μ, μ) → weakened robustness guarantees

- First 3 experiments:
  1. Verify Lipschitz continuity of a simple logistic regression model on synthetic data.
  2. Implement Theorem 2 for a Gaussian data distribution and compare theoretical bounds with empirical results.
  3. Analyze counterfactual robustness to dataset perturbation using Theorem 3 on a convex loss function.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we better understand or improve the parameter κ(˜µ, µ) in Theorem 2 in a more natural way, potentially from the perspective of information geometry?
- Basis in paper: Explicit - mentioned as future work in the conclusion section
- Why unresolved: The paper acknowledges this as an open problem but doesn't provide a concrete approach or solution
- What evidence would resolve it: A rigorous mathematical framework connecting the density ratio κ(˜µ, µ) to information geometric concepts, along with empirical results demonstrating improved bounds or practical applications

### Open Question 2
- Question: How can we extend Theorem 3 to more popular optimization algorithms such as Stochastic Gradient Descent (SGD), instead of just Gradient Descent (GD)?
- Basis in paper: Explicit - mentioned as future work in the conclusion section
- Why unresolved: The paper only considers the case of Gradient Descent, leaving open the question of how other optimization methods might affect the robustness guarantees
- What evidence would resolve it: Theoretical analysis showing how SGD (or other optimization algorithms) affects the bound on model changes, along with experimental validation comparing the robustness of counterfactuals across different optimization methods

### Open Question 3
- Question: What are the practical implications of the subgaussian condition in Definition 6 for real-world machine learning models and datasets?
- Basis in paper: Inferred - the paper introduces this condition but doesn't discuss its practical implications or how to verify it for common ML models
- Why unresolved: While the paper provides theoretical guarantees, it doesn't address how to assess or enforce the subgaussian condition in practical scenarios
- What evidence would resolve it: Empirical studies on various machine learning models and datasets, demonstrating when and how the subgaussian condition is satisfied, and the impact on counterfactual explanation robustness

## Limitations
- The probabilistic guarantees depend on the boundedness of the likelihood ratio κ(˜µ, µ), which could become large in real-world scenarios with significant distribution shifts
- The analysis assumes Lipschitz continuity holds for both original and changed models, which may not always be satisfied in practice
- The dataset perturbation analysis relies on specific assumptions about the optimization landscape that may not hold for all loss functions or training algorithms

## Confidence
- High Confidence: The generalization from Naturally-Occurring Model Change to Generally-Occurring Model Change is mathematically sound and the probabilistic framework is well-established.
- Medium Confidence: The application of these theoretical results to counterfactual explanation robustness is reasonable but requires careful implementation and may not hold in all practical scenarios.
- Low Confidence: The specific case study on dataset perturbation may have limited practical applicability due to restrictive assumptions on the optimization problem and loss function properties.

## Next Checks
1. **Empirical Validation of Lipschitz Conditions**: Test the Lipschitz continuity assumption on various model architectures (logistic regression, neural networks) using real-world datasets. Measure how often the condition holds and how violations impact the theoretical bounds.

2. **Robustness to Distribution Shift**: Implement a simulation study where the sampling distribution ˜µ deviates from the data distribution µ in controlled ways. Quantify how the likelihood ratio κ(˜µ, µ) affects the probability guarantees for counterfactual robustness.

3. **Dataset Perturbation Experiment**: Create a controlled experiment where a small fraction of training data is modified and retrain models using different optimization algorithms. Compare the actual model changes with the theoretical bounds from Theorem 3 to assess practical applicability.