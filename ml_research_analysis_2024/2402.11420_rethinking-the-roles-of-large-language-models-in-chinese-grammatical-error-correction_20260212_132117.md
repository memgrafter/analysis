---
ver: rpa2
title: Rethinking the Roles of Large Language Models in Chinese Grammatical Error
  Correction
arxiv_id: '2402.11420'
source_url: https://arxiv.org/abs/2402.11420
tags:
- llms
- grammatical
- error
- cgec
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of applying large language
  models (LLMs) to Chinese grammatical error correction (CGEC), where direct use of
  LLMs as correctors has been unsatisfactory due to the minimum change principle required
  in CGEC. The authors propose two frameworks: EXAM, which uses LLMs as explainers
  to provide error types, references, and explanations to enhance small model training,
  and SEE, which employs LLMs as evaluators to assess corrections based on grammar
  and semantics.'
---

# Rethinking the Roles of Large Language Models in Chinese Grammatical Error Correction

## Quick Facts
- arXiv ID: 2402.11420
- Source URL: https://arxiv.org/abs/2402.11420
- Authors: Yinghui Li; Shang Qin; Haojing Huang; Yangning Li; Libo Qin; Xuming Hu; Wenhao Jiang; Hai-Tao Zheng; Philip S. Yu
- Reference count: 38
- Key outcome: EXAM framework improves small model CGEC performance by 2-5% F0.5 points with limited data, while SEE evaluation shows 0.92 human correlation

## Executive Summary
This paper addresses the challenge of applying large language models (LLMs) to Chinese grammatical error correction (CGEC), where direct use of LLMs as correctors has been unsatisfactory due to the minimum change principle required in CGEC. The authors propose two frameworks: EXAM, which uses LLMs as explainers to provide error types, references, and explanations to enhance small model training, and SEE, which employs LLMs as evaluators to assess corrections based on grammar and semantics. Experiments on widely used datasets show that EXAM significantly improves small model performance, even with limited training data, while SEE provides more reasonable and human-aligned evaluations compared to traditional metrics. The work demonstrates effective collaboration between LLMs and small models in downstream tasks.

## Method Summary
The paper proposes EXAM (Explainer-based CGEC) and SEE (Semantic-incorporated Evaluation) frameworks. EXAM leverages LLMs to generate error types, reference corrections, and detailed explanations that are concatenated with input sentences to enhance small model training. SEE uses LLMs to evaluate corrections by classifying them as Correct Edit, Wrong Edit, or Reasonable Edit based on grammatical analysis and semantic preservation. The authors conduct experiments on HSK and NLPCC datasets using baseline models (BART-Large, mT5-Base, GECToR-Chinese) and enhanced versions with EXAM, comparing traditional evaluation metrics with the proposed SEE framework.

## Key Results
- EXAM improves small model performance by 2-5% F0.5 points, with greater gains on limited data (15K vs 156K samples)
- Small models trained with EXAM achieve competitive performance with LLMs while being more cost-effective and faster
- SEE evaluation shows higher correlation with human judgments (0.92) compared to traditional metrics
- Traditional metrics (CharacTER, GLEU) show inconsistent rankings compared to SEE, which better captures semantic preservation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EXAM leverages LLMs' grammatical knowledge to generate auxiliary information (error types, references, explanations) that improves small model training performance.
- Mechanism: LLMs analyze grammatically incorrect sentences and produce structured explanations including error type classification, reference corrections, and detailed grammatical error descriptions. This information is concatenated to input sentences and used during small model training.
- Core assumption: The grammatical knowledge stored in LLMs is sufficient to provide high-quality explanations that small models can learn from, and this explanation process is not constrained by the minimum change principle.
- Evidence anchors:
  - [abstract]: "Considering the rich grammatical knowledge stored in LLMs and their powerful semantic understanding capabilities, we utilize LLMs as explainers to provide explanation information to the CGEC small models during error correction to enhance performance."
  - [section 3.2]: "Based on our understanding of the CGEC task, we divide the explanation information (note that the 'explanation' we consider here is the LLMs analysis of incorrect sentences in a broad sense) we want to obtain from LLMs into three categories: Error Types, References, Explanations"
  - [corpus]: Weak - the corpus provides related work but doesn't directly validate this specific mechanism of using LLM explanations to enhance training
- Break condition: If the LLM explanations are not accurate or do not capture the grammatical errors correctly, or if the small models cannot effectively learn from the concatenated explanation information.

### Mechanism 2
- Claim: SEE uses LLMs to provide more flexible and semantically-aware evaluation by considering both grammatical corrections and semantic preservation.
- Mechanism: LLMs evaluate predicted edits by classifying them as Correct Edit, Wrong Edit, or Reasonable Edit based on grammatical analysis and semantic understanding of error sentences, golden sentences, and predicted sentences.
- Core assumption: LLMs have sufficient semantic understanding capabilities to judge whether edits preserve original meaning while correcting grammatical errors, and this evaluation is more aligned with human judgment than traditional metrics.
- Evidence anchors:
  - [abstract]: "We also use LLMs as evaluators to bring more reasonable CGEC evaluations, thus alleviating the troubles caused by the subjectivity of the CGEC task."
  - [section 3.3]: "SEE requires LLMs to balance the edits annotated in the golden data with the evaluated model's edits, ensuring they do not alter the original semantics of the input sentence."
  - [section 4.3.3]: "From these experimental results, it is evident that even with the enhancement provided by few-shot learning, there remains a significant gap in the correction capabilities of LLMs... our SEE method maintains a high level of alignment with human judgment."
- Break condition: If LLM evaluators are inconsistent in their judgments, or if they cannot properly balance grammatical correctness with semantic preservation.

### Mechanism 3
- Claim: The collaborative framework between LLMs and small models enables better utilization of LLM strengths while maintaining the efficiency of small models.
- Mechanism: LLMs handle the explanation and evaluation tasks that require rich grammatical knowledge and semantic understanding, while small models handle the actual correction task with guidance from LLM-generated information.
- Core assumption: LLMs and small models can effectively collaborate in downstream tasks, with each performing roles suited to their strengths, and this collaboration leads to better overall performance than either working alone.
- Evidence anchors:
  - [abstract]: "In particular, our work is also an active exploration of how LLMs and small models better collaborate in downstream tasks."
  - [section 3.1]: "We argue that the ideal evaluation that can truly reflect the CGEC performance should consider the correction results given by the model as comprehensively as possible. As long as the model provides a sentence that is consistent with the original meaning of the incorrect sentence and has no grammatical errors, its correction should be considered successful."
  - [section 4.2]: "Small models trained with EXAM can exhibit competitive performance with LLMs. This could be more meaningful in real-world scenarios, as deploying a small model is more cost-effective and offers faster response times."
- Break condition: If the collaboration framework creates inefficiencies or if the small models become overly dependent on LLM guidance rather than learning effective correction strategies.

## Foundational Learning

- Concept: Grammatical error types and correction strategies
  - Why needed here: Understanding different types of grammatical errors (punctuation, spelling, word choice, syntax)
  - Quick check: Can identify and classify common Chinese grammatical errors in sample sentences

- Concept: LLM-based explanation generation
  - Why needed here: EXAM framework requires LLMs to generate structured explanations (error types, references, explanations) for grammatically incorrect sentences
  - Quick check: Can generate accurate error type classifications and reference corrections for sample grammatical errors

- Concept: Semantic preservation in error correction
  - Why needed here: SEE evaluation requires balancing grammatical correctness with semantic preservation of the original sentence
  - Quick check: Can evaluate whether a correction preserves the original meaning while fixing grammatical errors

## Architecture Onboarding

### Component Map
HSK Dataset -> EXAM Framework (LLM Explainer) -> Enhanced Training Data -> Small Model Training -> NLPCC/NaCGEC Test -> SEE Evaluation (LLM Evaluator)

### Critical Path
HSK Dataset (156,870 samples) -> EXAM Framework -> Small Model Training -> NLPCC Test Set Evaluation -> SEE Framework

### Design Tradeoffs
- LLM-based explanation generation vs. rule-based/grammar-based approaches
- Semantic preservation vs. grammatical correctness in evaluation
- Small model efficiency vs. LLM performance in CGEC

### Failure Signatures
- EXAM performance degrades if LLM explanations are inaccurate or misaligned with training data
- SEE evaluation diverges from human judgments if LLM evaluator lacks consistency
- Small models fail to learn effective correction strategies if over-reliant on LLM guidance

### Three First Experiments
1. Generate LLM explanations for a subset of HSK training data and evaluate their accuracy against human annotations
2. Compare small model performance with and without EXAM enhancement on a validation set
3. Validate SEE evaluation by comparing its rankings with human evaluations on a sample of corrections

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLM-based evaluation methods be made more robust and generalizable across different types of grammatical error correction tasks and datasets?
- Basis in paper: [explicit] The authors discuss the subjective nature of CGEC and the limitations of traditional evaluation metrics, proposing SEE as a more flexible and comprehensive evaluation method using LLMs.
- Why unresolved: While SEE shows promise in addressing the subjectivity of CGEC evaluation, its performance may vary across different datasets and error types. Further research is needed to ensure the robustness and generalizability of LLM-based evaluation methods.
- What evidence would resolve it: Extensive experiments comparing SEE with traditional metrics on a wide range of CGEC datasets and error types, demonstrating consistent improvements in evaluation accuracy and alignment with human judgments.

### Open Question 2
- Question: How can the quality and reliability of explanation information provided by LLMs be further improved for enhancing small model training in CGEC?
- Basis in paper: [explicit] The authors propose EXAM, which utilizes LLM-generated explanation information (error types, references, and explanations) to enhance small model training. However, they acknowledge that the explanations provided by LLMs may have minor flaws and room for improvement.
- Why unresolved: The quality and reliability of LLM-generated explanations directly impact the effectiveness of EXAM in enhancing small model performance. Further research is needed to develop methods for improving the accuracy and comprehensiveness of LLM-generated explanations.
- What evidence would resolve it: Comparative studies evaluating the impact of different explanation generation techniques on small model performance, demonstrating significant improvements in accuracy and error correction capabilities.

### Open Question 3
- Question: How can the collaboration between LLMs and small models be optimized to achieve better performance in CGEC and other NLP tasks?
- Basis in paper: [explicit] The authors explore the potential of utilizing LLMs as explainers and evaluators to enhance small model performance in CGEC. They highlight the importance of leveraging the strengths of both LLMs and small models for effective collaboration.
- Why unresolved: While the paper demonstrates the benefits of LLM-small model collaboration in CGEC, the optimal strategies for leveraging their respective strengths and achieving seamless integration remain an open question. Further research is needed to develop more effective collaboration frameworks.
- What evidence would resolve it: Comparative studies evaluating different LLM-small model collaboration approaches, demonstrating significant improvements in task performance and efficiency compared to standalone models.

## Limitations
- EXAM performance depends heavily on LLM explanation quality, which lacks explicit validation against human annotations
- SEE evaluation introduces its own subjectivity through LLM-based criteria without extensive human evaluation comparison
- Study focuses specifically on Chinese grammatical error correction, limiting generalizability to other languages or tasks

## Confidence
- High confidence: EXAM improves small model performance by 2-5% F0.5 points with consistent results across multiple model architectures
- High confidence: SEE evaluation shows 0.92 human correlation, outperforming traditional metrics
- Medium confidence: The generalizability of EXAM and SEE frameworks to other languages and grammatical error correction tasks

## Next Checks
1. Generate LLM explanations for a validation set and conduct human evaluation to assess explanation accuracy and alignment with human annotations
2. Perform ablation studies on EXAM framework to determine the individual contributions of error types, references, and explanations to performance improvements
3. Conduct extensive experiments comparing SEE with traditional metrics and human evaluations across multiple CGEC datasets to validate its robustness and generalizability