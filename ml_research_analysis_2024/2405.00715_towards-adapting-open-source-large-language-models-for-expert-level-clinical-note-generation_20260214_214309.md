---
ver: rpa2
title: Towards Adapting Open-Source Large Language Models for Expert-Level Clinical
  Note Generation
arxiv_id: '2405.00715'
source_url: https://arxiv.org/abs/2405.00715
tags:
- training
- notes
- note
- clinical
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a comprehensive domain- and task-specific adaptation
  process for the LLaMA-2 13B model to generate high-quality clinical notes from outpatient
  patient-doctor dialogues. The process incorporates continued pretraining, supervised
  fine-tuning, and reinforcement learning from both AI and human feedback.
---

# Towards Adapting Open-Source Large Language Models for Expert-Level Clinical Note Generation

## Quick Facts
- arXiv ID: 2405.00715
- Source URL: https://arxiv.org/abs/2405.00715
- Reference count: 40
- LLama-Clinic achieves 90.4% of individual evaluations rated as "acceptable" or higher across three criteria

## Executive Summary
This study presents a comprehensive domain- and task-specific adaptation process for the LLaMA-2 13B model to generate high-quality clinical notes from outpatient patient-doctor dialogues. The process incorporates continued pretraining, supervised fine-tuning, and reinforcement learning from both AI and human feedback. A new approach, DistillDirect, is introduced for performing on-policy reinforcement learning with Gemini 1.0 Pro as the teacher model. The resulting model, LLaMA-Clinic, achieves performance comparable to physician-authored notes, with 90.4% of individual evaluations rated as "acceptable" or higher across three criteria: real-world readiness, completeness, and accuracy. Notably, in the "Assessment and Plan" section, LLaMA-Clinic scores higher in real-world readiness compared to physician-authored notes.

## Method Summary
The study employed a multi-stage adaptation pipeline for the LLaMA-2 13B model. First, continued pretraining was performed using 8.6M clinical sentences from de-identified electronic health records. This was followed by supervised fine-tuning on 30K dialogue-note pairs to teach the model to extract key information from patient-doctor conversations. Finally, reinforcement learning was applied using a novel DistillDirect approach with Gemini 1.0 Pro as the teacher model. The evaluation framework included both AI-based assessments and human evaluations across three criteria: real-world readiness, completeness, and accuracy.

## Key Results
- LLaMA-Clinic achieved 90.4% of individual evaluations rated as "acceptable" or higher across three criteria
- The model's performance is comparable to physician-authored notes
- In the "Assessment and Plan" section, LLaMA-Clinic scored higher in real-world readiness compared to physician-authored notes

## Why This Works (Mechanism)
The success of this approach stems from the comprehensive multi-stage adaptation pipeline that progressively refines the model for clinical note generation. Continued pretraining on clinical text establishes domain-specific language patterns and medical terminology. Supervised fine-tuning teaches the model to map dialogue content to structured note formats. The reinforcement learning stage with AI feedback optimizes for both completeness and accuracy while maintaining clinical coherence. The DistillDirect approach enables effective on-policy learning without requiring extensive human feedback at each iteration.

## Foundational Learning
- Clinical language patterns: Why needed - Medical terminology and clinical discourse differ significantly from general language. Quick check - Model can correctly identify and use medical terms in context.
- Dialogue-to-note mapping: Why needed - Converting unstructured conversations to structured clinical notes requires understanding both content and format requirements. Quick check - Generated notes follow standard medical documentation templates.
- Reinforcement learning from AI feedback: Why needed - Human feedback is expensive and slow, while AI can provide immediate, scalable evaluation. Quick check - AI feedback correlates with human expert judgments.

## Architecture Onboarding
**Component Map:** Pretraining -> Supervised Fine-tuning -> Reinforcement Learning (DistillDirect) -> Evaluation
**Critical Path:** Dialogue input -> Information extraction -> Structured note generation -> AI/human evaluation
**Design Tradeoffs:** Open-source model vs. proprietary alternatives, AI feedback vs. human feedback, outpatient focus vs. broader clinical applications
**Failure Signatures:** Incorrect medical terminology usage, missing critical clinical information, format violations in note structure
**First Experiments:** 1) Test model on unseen dialogue types to assess generalization. 2) Compare AI vs. human evaluation consistency. 3) Evaluate performance on complex clinical cases with multiple comorbidities.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies heavily on AI-based assessments which may not capture nuanced quality judgments
- Sample size of 40 human evaluations may not be representative of broader clinical contexts
- Focus exclusively on outpatient encounters limits generalizability to other clinical settings

## Confidence
- **High Confidence**: Technical implementation of the multi-stage adaptation pipeline is sound and well-documented
- **Medium Confidence**: Evaluation framework using both AI and human feedback provides reasonable assessment
- **Low Confidence**: External validity across different healthcare settings and patient populations remains untested

## Next Checks
1. Conduct a larger-scale human evaluation study with diverse physician reviewers across multiple healthcare institutions
2. Test the model's performance on different types of clinical encounters (inpatient, emergency, specialty consultations)
3. Implement and evaluate the model in a controlled clinical environment to measure real-world impact