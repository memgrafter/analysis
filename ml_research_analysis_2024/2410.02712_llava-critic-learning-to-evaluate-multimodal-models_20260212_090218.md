---
ver: rpa2
title: 'LLaVA-Critic: Learning to Evaluate Multimodal Models'
arxiv_id: '2410.02712'
source_url: https://arxiv.org/abs/2410.02712
tags:
- llav
- a-critic
- evaluation
- arxiv
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLaVA-Critic is an open-source large multimodal model designed
  to evaluate the performance of other multimodal models. It is trained on a high-quality
  dataset of 113k instruction-following examples across 46k images, covering pointwise
  scoring and pairwise ranking tasks.
---

# LLaVA-Critic: Learning to Evaluate Multimodal Models

## Quick Facts
- **arXiv ID**: 2410.02712
- **Source URL**: https://arxiv.org/abs/2410.02712
- **Reference count**: 40
- **Primary result**: LLaVA-Critic is an open-source large multimodal model for evaluating other multimodal models, trained on 113k examples and achieving strong correlation with human and GPT-4o preferences.

## Executive Summary
LLaVA-Critic is an open-source large multimodal model designed to evaluate the performance of other multimodal models. It is trained on a high-quality dataset of 113k instruction-following examples across 46k images, covering pointwise scoring and pairwise ranking tasks. The model demonstrates strong alignment with human and GPT-4o preferences in both pointwise scoring (Pearson correlation up to 0.754) and pairwise ranking (accuracy up to 73.6%), making it a cost-effective alternative to commercial models for LMM evaluation. Additionally, LLaVA-Critic serves as an effective reward model for preference learning, improving visual chat capabilities of LMMs through iterative DPO training, surpassing baselines trained with human feedback.

## Method Summary
LLaVA-Critic is a fine-tuned version of LLaVA-OneVision that learns to evaluate multimodal responses through instruction tuning on a dataset of 113k examples spanning 46k images. The model is trained on two types of tasks: pointwise scoring (rating responses on a scale) and pairwise ranking (comparing two responses). The training dataset, MMLU-Critic, was annotated by GPT-4o and includes diverse evaluation criteria such as reasoning, relevance, and creativity. The model is available in 7B and 72B variants, with the larger version showing superior performance across benchmarks.

## Key Results
- LLaVA-Critic achieves Pearson correlation up to 0.754 with GPT-4o preferences in pointwise scoring tasks
- The 72B variant achieves 73.6% accuracy in pairwise ranking without ties, outperforming GPT-4o and GPT-4V
- LLaVA-Critic improves visual chat capabilities of LLaVA-OneVision through iterative DPO training, surpassing baselines trained with human feedback

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Critic training improves pointwise scoring alignment with GPT-4o
- Mechanism: The model learns to predict scores and justifications by observing GPT-4o's evaluation patterns across diverse tasks, building a mapping from image-response pairs to calibrated scores.
- Core assumption: GPT-4o's evaluation patterns are consistent and learnable by a smaller model when trained on sufficient examples.
- Evidence anchors:
  - [abstract] "performing on par with or surpassing GPT models on multiple evaluation benchmarks"
  - [section] "LLaVA-Critic variants significantly outperform their corresponding baseline models across all models and benchmarks"
- Break condition: When evaluation criteria vary significantly across domains in ways that cannot be captured by the training data distribution.

### Mechanism 2
- Claim: Critic training improves pairwise ranking performance
- Mechanism: The model learns preference relationships by observing pairwise comparisons, developing a scoring function that can rank responses based on learned quality dimensions.
- Core assumption: Pairwise preference data provides sufficient signal for learning a generalizable ranking function.
- Evidence anchors:
  - [abstract] "performing on par with or surpassing GPT models on multiple evaluation benchmarks"
  - [section] "LLaVA-Critic-72B achieves an average accuracy of 73.6% in pairwise comparisons without tie, outperforming both GPT-4o and GPT-4V"
- Break condition: When response quality differences are subtle and require deeper semantic understanding not captured in pairwise comparisons.

### Mechanism 3
- Claim: Critic-generated rewards improve preference learning in DPO
- Mechanism: The critic model provides a stable, cost-effective reward signal that can be used iteratively to align LMMs with human-like preferences.
- Core assumption: Self-generated preferences from a well-trained critic are sufficiently aligned with human preferences to guide effective DPO.
- Evidence anchors:
  - [abstract] "improves visual chat capabilities of LMMs through iterative DPO training, surpassing baselines trained with human feedback"
  - [section] "preferences provided by LLaVA-Critic significantly improve LLaVA-OneVision's visual chat capacities and reduce hallucination"
- Break condition: When the critic's preferences diverge significantly from human preferences or when the reward signal becomes too noisy for effective optimization.

## Foundational Learning

- **Concept**: Multimodal instruction tuning
  - Why needed here: LLaVA-Critic builds upon existing multimodal instruction capabilities to add evaluation-specific skills
  - Quick check question: Can the base model follow diverse visual instructions before adding critic capabilities?

- **Concept**: Preference optimization and reward modeling
  - Why needed here: The critic serves as a reward model for preference learning, requiring understanding of DPO and RLHF concepts
  - Quick check question: How does direct preference optimization differ from traditional RLHF in terms of reward signal usage?

- **Concept**: Evaluation methodology and correlation metrics
  - Why needed here: Assessing the critic's performance requires understanding of Pearson correlation, Kendall's Tau, and accuracy metrics
  - Quick check question: What's the difference between instance-level and model-level consistency in evaluation tasks?

## Architecture Onboarding

- **Component map**: Input → Prompt formatting → Model inference → Score/ranking prediction → Justification generation
- **Critical path**: The model processes image-response pairs through its multimodal encoder, applies task-specific prompt formatting (pointwise vs pairwise), generates predictions, and outputs scores/rankings with justifications
- **Design tradeoffs**: Larger models (72B) achieve better performance but at higher computational cost; the 7B variant offers a practical balance
- **Failure signatures**: Fixed score outputs (e.g., always "6" or "Tie"), misaligned rankings compared to GPT-4o, or inconsistent justifications
- **First 3 experiments**:
  1. Test pointwise scoring on a small held-out set from the training distribution
  2. Evaluate pairwise ranking accuracy on the battle dataset
  3. Assess correlation with GPT-4o on a sample from MLLM-as-a-Judge

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LLaVA-Critic's performance scale with increasing instruction-following data diversity beyond the current 113k samples?
- Basis in paper: [explicit] The paper discusses data scaling effects and mentions that LLaVA-Critic-7B outperforms v0.5 on 5 out of 6 benchmarks, highlighting the importance of stronger reward models trained on more diverse critic instructions.
- Why unresolved: The paper only tests two model variants (v0.5 with 53k samples and full dataset with 113k samples). There's no analysis of performance gains from additional data beyond this range or testing with significantly larger datasets.
- What evidence would resolve it: Systematic experiments training LLaVA-Critic with incrementally larger datasets (e.g., 200k, 500k, 1M samples) while measuring performance on MLLM-as-a-Judge and visual chat benchmarks would establish the scaling relationship.

### Open Question 2
- Question: Can LLaVA-Critic maintain its evaluation consistency across tasks involving highly specialized domains (e.g., advanced scientific imaging, artistic critique) that weren't represented in the training data?
- Basis in paper: [inferred] The training data covers general visual conversation, reasoning, OCR, and several specific domains, but the paper doesn't explicitly test LLaVA-Critic's ability to evaluate responses in highly specialized technical or artistic domains.
- Why unresolved: While the paper demonstrates strong performance across diverse benchmarks, these benchmarks themselves may not fully capture the complexity and nuance required for specialized domain evaluation. The model's generalization to truly novel evaluation contexts remains untested.
- What evidence would resolve it: Testing LLaVA-Critic's evaluation consistency on expert-curated datasets from specialized fields (e.g., medical imaging interpretation, fine art criticism, scientific visualization) compared against domain experts would reveal its domain transfer capabilities.

### Open Question 3
- Question: What are the failure modes and limitations of using LLaVA-Critic as a reward model in preference learning, particularly regarding potential reward hacking or preference misalignment?
- Basis in paper: [inferred] The paper shows that LLaVA-Critic improves visual chat capabilities through iterative DPO, but doesn't investigate potential pathological behaviors that could emerge from optimizing against its reward signals.
- Why unresolved: The paper focuses on performance improvements but doesn't examine whether the model could learn to exploit LLaVA-Critic's evaluation criteria in unintended ways, potentially producing responses that score highly according to LLaVA-Critic but are undesirable from a human perspective.
- What evidence would resolve it: Analyzing the qualitative characteristics of responses generated through multiple rounds of DPO with LLaVA-Critic as the reward model, looking for signs of over-optimization (e.g., repetitive patterns, excessive verbosity, avoidance of genuine uncertainty) would reveal potential failure modes.

## Limitations

- Data Quality Dependence: The model's performance critically depends on the quality and diversity of the training dataset, with potential bias from GPT-4o annotations
- Evaluation Metric Gaps: Current evaluation focuses primarily on correlation with GPT-4o and human preferences, lacking comprehensive assessment of specific failure mode detection
- Computational Tradeoffs: While the 72B variant achieves highest performance, the significant computational overhead raises questions about practical deployment

## Confidence

- **High Confidence**: The claim that LLaVA-Critic achieves strong correlation with GPT-4o preferences (up to 0.754 Pearson correlation) is well-supported by quantitative results across multiple benchmarks
- **Medium Confidence**: The assertion that critic-generated rewards improve preference learning in DPO training is supported by empirical results showing improved visual chat capabilities, but long-term stability requires further investigation
- **Low Confidence**: The claim of surpassing baselines trained with human feedback should be interpreted cautiously, as the comparison may depend heavily on specific datasets and training protocols

## Next Checks

1. **Cross-Domain Generalization Test**: Evaluate LLaVA-Critic's performance on evaluation tasks from domains not represented in the training data (e.g., medical imaging, satellite imagery) to assess whether the model can generalize its evaluation criteria beyond the training distribution

2. **Temporal Consistency Analysis**: Assess the model's stability over time by testing whether its evaluations of the same image-response pairs remain consistent across multiple inference runs and whether its preferences align with evolving human standards in multimodal evaluation

3. **Error Attribution Study**: Conduct a detailed analysis of cases where LLaVA-Critic disagrees with GPT-4o or human evaluators to determine whether these disagreements reveal genuine blind spots in the critic model or represent valid alternative evaluation perspectives