---
ver: rpa2
title: 'Relying on the Unreliable: The Impact of Language Models'' Reluctance to Express
  Uncertainty'
arxiv_id: '2401.06730'
source_url: https://arxiv.org/abs/2401.06730
tags:
- epistemic
- markers
- language
- users
- your
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how language models (LMs) express uncertainty
  and how users interpret these expressions. The authors find that LMs are reluctant
  to express uncertainty, preferring to use certainty expressions even when their
  answers are incorrect.
---

# Relying on the Unreliable: The Impact of Language Models' Reluctance to Express Uncertainty

## Quick Facts
- arXiv ID: 2401.06730
- Source URL: https://arxiv.org/abs/2401.06730
- Authors: Kaitlyn Zhou; Jena D. Hwang; Xiang Ren; Maarten Sap
- Reference count: 25
- Key outcome: Language models are reluctant to express uncertainty, preferring certainty expressions even when incorrect, leading to overreliance by users and potential long-term harms to human performance

## Executive Summary
This paper investigates how language models express uncertainty and how users interpret these expressions. The authors find that LMs consistently prefer certainty expressions over uncertainty expressions, even when their answers are incorrect. Through human experiments, they demonstrate that users heavily rely on LM-generated responses regardless of whether they're marked with certainty or not. The study traces this overconfidence back to the RLHF (Reinforcement Learning from Human Feedback) process, identifying a human bias against uncertainty in the annotated datasets used for training. These findings highlight a critical calibration issue in LMs that can lead to long-term harms in human decision-making and knowledge development.

## Method Summary
The authors conducted a multi-pronged investigation combining computational analysis with human experiments. They analyzed uncertainty expression patterns across three major LM families (GPT-3.5, Claude-2, Llama-2) using a structured uncertainty taxonomy. Human experiments were conducted with crowdsourced workers to measure how users interpret and rely on LM responses with different certainty markers. The researchers also traced the origin of model overconfidence by analyzing the RLHF training process and examining human biases in the annotated datasets used for fine-tuning. The study focused specifically on English-language uncertainty expressions and examined how different types of uncertainty markers affect user behavior and decision-making.

## Key Results
- Language models consistently prefer certainty expressions over uncertainty expressions, even when their answers are incorrect
- Users rely heavily on LM-generated responses regardless of whether certainty or uncertainty markers are present
- Miscalibration in LM uncertainty expression can lead to long-term harms in human performance and knowledge development
- The overconfidence issue originates from RLHF training, where human bias against uncertainty in annotated datasets creates a preference for certainty expressions

## Why This Works (Mechanism)
The mechanism behind LM overconfidence stems from the reinforcement learning from human feedback (RLHF) process, where human annotators systematically prefer confident-sounding responses over uncertain ones. This creates a training signal that rewards certainty expression regardless of actual knowledge state. The models learn to associate higher reward scores with confident language patterns, even when those patterns don't accurately reflect the model's true uncertainty. This misalignment between expressed confidence and actual knowledge state leads users to over-rely on potentially incorrect information, creating a cascade effect where initial overreliance compounds over time, degrading human decision-making capabilities and independent knowledge verification skills.

## Foundational Learning
**Reinforcement Learning from Human Feedback (RLHF)**: A training method where models are fine-tuned based on human preferences for certain responses over others. Why needed: RLHF is the dominant alignment technique for modern LMs and directly shapes how models express uncertainty. Quick check: Does the model's behavior align with actual human preferences or just surface-level patterns in the training data?

**Uncertainty Expression Taxonomy**: A structured classification system for different types of uncertainty markers in language (hedges, modals, evidentials, etc.). Why needed: Provides a systematic framework for analyzing how LMs express or suppress uncertainty. Quick check: Are all relevant uncertainty markers captured, or are there systematic blind spots?

**Human-AI Interaction Dynamics**: The psychological and behavioral patterns that emerge when humans interact with AI systems. Why needed: Understanding these dynamics is crucial for predicting how users will respond to different types of LM outputs. Quick check: Do experimental findings generalize to real-world usage patterns?

**Calibration in Machine Learning**: The alignment between a model's expressed confidence and its actual probability of being correct. Why needed: Poor calibration can lead to dangerous overreliance on incorrect information. Quick check: Does the model's confidence accurately reflect its error rates across different types of queries?

## Architecture Onboarding

**Component Map**: Data Collection -> Uncertainty Analysis -> Human Experiments -> RLHF Tracing -> Impact Assessment

**Critical Path**: The chain from model training methodology (RLHF) through uncertainty expression patterns to user behavior and long-term impacts represents the core causal pathway that needs to be understood and potentially intervened upon.

**Design Tradeoffs**: The study balances computational analysis of model behavior with human experimental validation, choosing to focus on three major LM families rather than broader coverage. This provides depth but may limit generalizability.

**Failure Signatures**: Overconfident LM responses lacking appropriate uncertainty markers, user overreliance on LM outputs regardless of certainty markers, and the absence of appropriate hedging language even when models should express uncertainty.

**First Experiments**:
1. Analyze uncertainty expression patterns across additional LM families and architectures
2. Conduct controlled experiments manipulating uncertainty expression to measure direct effects on user decision quality
3. Test the impact of different uncertainty expression strategies on user learning outcomes and knowledge development

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Human experiments rely on crowdsourced workers who may not represent the full spectrum of real-world LM users
- Analysis focuses on three specific LM families and English-language uncertainty expressions, limiting generalizability
- The causal link between RLHF and overconfidence is correlational rather than definitively proven
- Long-term harm predictions are based on relatively short experimental timeframes requiring longitudinal validation

## Confidence

**High confidence**: LMs prefer certainty expressions over uncertainty expressions when incorrect (observed across all tested models)

**Medium confidence**: Causal link between RLHF training and increased model overconfidence (supported by training data analysis but not definitively proven)

**Medium confidence**: Human reliance on LM responses regardless of certainty markers (robust in experimental settings but may not generalize to all user populations)

## Next Checks
1. Replicate the uncertainty expression analysis across additional model families and multiple languages to test generalizability
2. Conduct longitudinal studies with longer timeframes to validate the predicted long-term harms to human performance and knowledge development
3. Design intervention studies that systematically manipulate uncertainty expression in LMs to measure direct effects on human decision quality and learning outcomes across diverse user populations