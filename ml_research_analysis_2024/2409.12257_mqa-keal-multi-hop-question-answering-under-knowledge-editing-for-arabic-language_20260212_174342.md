---
ver: rpa2
title: 'MQA-KEAL: Multi-hop Question Answering under Knowledge Editing for Arabic
  Language'
arxiv_id: '2409.12257'
source_url: https://arxiv.org/abs/2409.12257
tags:
- mqa-k
- knowledge
- language
- multi-hop
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of updating large language models
  with new information for non-English languages, specifically Arabic. It proposes
  MQA-KEAL, a method that stores knowledge edits as structured relational triplets
  in external memory and uses task decomposition to break down multi-hop questions
  into smaller sub-problems.
---

# MQA-KEAL: Multi-hop Question Answering under Knowledge Editing for Arabic Language

## Quick Facts
- arXiv ID: 2409.12257
- Source URL: https://arxiv.org/abs/2409.12257
- Reference count: 12
- Multi-hop accuracy improvement up to 49.3% over best baseline on Arabic benchmark

## Executive Summary
This paper introduces MQA-KEAL, a novel approach for multi-hop question answering under knowledge editing for Arabic language. The system addresses the challenge of updating large language models with new information by storing knowledge edits as structured relational triplets in external memory. Through task decomposition and iterative traversal, MQA-KEAL breaks down complex questions into manageable sub-problems, achieving significant performance improvements on two newly created Arabic evaluation benchmarks.

## Method Summary
MQA-KEAL combines structured knowledge retrieval with iterative traversal to handle multi-hop questions under knowledge editing. The system stores edits as (subject, relation, object) triplets and uses task decomposition to break questions into sequential reasoning paths. For each sub-problem, it retrieves candidate answers using embedding similarity, applies implication and compositional filtering rules, and queries the target LLM when needed. The approach iteratively processes each reasoning step, assembling final responses from filtered candidates or LLM-generated answers.

## Key Results
- MQA-KEAL outperforms baseline methods by up to 49.3% in multi-hop accuracy on MQUAKE-AR benchmark
- Significant improvements achieved using GPT-3.5-Turbo-Instruct as target LLM
- Consistent performance gains across both MQUAKE-AR and MQA-AEVAL Arabic evaluation benchmarks
- Demonstrates effectiveness of structured relational triplet storage vs unstructured embeddings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structured relational triplets enable precise retrieval compared to unstructured embeddings
- **Mechanism:** Relation-specific filtering during candidate retrieval allows more precise matching of sub-questions to relevant edits
- **Core assumption:** Multi-hop questions can be decomposed into sub-questions mapping to relational components
- **Evidence anchors:** Abstract mentions structured knowledge units in external memory; Section 4.1 describes embedding learning over structured triplets
- **Break condition:** When sub-questions cannot be decomposed into clean relational triplets

### Mechanism 2
- **Claim:** LLM-based task decomposition creates traversable reasoning paths
- **Mechanism:** LLM converts questions into (starting entity, reasoning path) tuples for sequential processing
- **Core assumption:** LLMs can accurately parse multi-hop questions into sequential relational reasoning steps
- **Evidence anchors:** Abstract states task-decomposition is used; Section 4.2 describes decomposition process
- **Break condition:** When LLM fails to decompose complex questions correctly

### Mechanism 3
- **Claim:** Implication and compositional rules prune irrelevant retrieved facts
- **Mechanism:** Filters candidates through relational implication, composition rules, and similarity thresholds
- **Core assumption:** Logical rules capture semantic relationships between relations for effective filtering
- **Evidence anchors:** Section 4.3.1 describes pruning heuristics and filtering logic
- **Break condition:** When logical rules are incomplete or incorrect

## Foundational Learning

- **Concept: Knowledge Graph Representation**
  - Why needed here: System represents facts as (s, r, o) triplets requiring knowledge graph understanding
  - Quick check question: Can you explain how (subject, relation, object) triplets capture factual knowledge differently from free text?

- **Concept: Multi-hop Reasoning**
  - Why needed here: System decomposes questions requiring multiple reasoning steps into sequential sub-problems
  - Quick check question: How would you represent a 3-hop question like "Who is the mayor of the capital of France?" as a chain of reasoning steps?

- **Concept: Information Retrieval with Embeddings**
  - Why needed here: System uses embedding similarity to retrieve relevant fact edits from structured memory
  - Quick check question: What's the difference between using dot product vs cosine similarity for embedding comparison in retrieval?

## Architecture Onboarding

- **Component map:** Structured Knowledge Retrieval -> Task Decomposition -> Iterative Traversal -> Candidate Filtering -> LLM Query (if needed) -> Response Assembly

- **Critical path:** Question → Task Decomposition → For each sub-problem: Structured Retrieval → Candidate Filtering → LLM Query (if needed) → Response Assembly

- **Design tradeoffs:**
  - Structured vs unstructured storage: Better precision vs higher storage/computation cost
  - Top-k retrieval with filtering vs single best match: More robust but potentially slower
  - LLM-based decomposition vs rule-based: More flexible but depends on LLM quality

- **Failure signatures:**
  - Incorrect decomposition: LLM fails to break down complex questions correctly
  - Retrieval misses: Relevant edits not found due to embedding mismatch or relation naming
  - Filtering over-pruning: Valid candidates eliminated by strict rule application
  - LLM generation errors: Target LLM produces incorrect responses even with correct inputs

- **First 3 experiments:**
  1. **Decomposition validation:** Test with simple multi-hop questions to verify LLM correctly decomposes into expected reasoning paths
  2. **Retrieval accuracy:** Measure retrieval precision/recall for known fact edits with varying sub-question formulations
  3. **End-to-end with controlled edits:** Create test cases with known edits and verify system retrieves and applies them correctly in multi-hop questions

## Open Questions the Paper Calls Out

1. How effective is MQA-KEAL in performing knowledge editing and multi-hop question answering for Arabic language compared to existing methods?
- Basis in paper: The paper explicitly states that MQA-KEAL outperforms baseline models by a significant margin across both MQUAKE-AR and MQA-AEVAL benchmarks.
- Why unresolved: While the paper shows MQA-KEAL outperforms baselines, it doesn't provide a detailed analysis of why this improvement occurs or under what specific conditions the improvement is most pronounced.

2. How does the structured knowledge retrieval component of MQA-KEAL compare to existing memory-based methods that store edits as unstructured text embeddings?
- Basis in paper: The paper explicitly states that existing memory-based methods store edits as unstructured text embeddings, which makes it challenging to disambiguate among different semantically relevant edits. MQA-KEAL stores edits as structured relational triplets to overcome this limitation.
- Why unresolved: The paper doesn't provide a direct comparison between MQA-KEAL's structured retrieval and the performance of existing methods under varying conditions.

3. What are the limitations of MQA-KEAL and how can they be addressed?
- Basis in paper: The paper explicitly mentions some limitations of MQA-KEAL, including the lack of an effective mechanism for recovery from errors in the intermediate path, reliance on the target LLM for cases where there is no fact edit correlated with a particular entity, and the impact of the target LLM's knowledge decomposition abilities on the end-result.
- Why unresolved: The paper doesn't provide concrete solutions or future work directions to address these limitations.

## Limitations
- Performance claims rely heavily on two newly created Arabic benchmarks without independent validation
- Improvement margins may reflect specific evaluation setup rather than generalizable performance gains
- System's dependency on target LLM quality for task decomposition introduces variability

## Confidence

- **High confidence**: Core architecture combining structured knowledge retrieval with iterative traversal is technically sound and builds on established methods
- **Medium confidence**: Reported performance improvements on Arabic benchmarks are methodologically valid but may have limited generalizability
- **Low confidence**: Effectiveness of specific implication and compositional rules for Arabic language filtering hasn't been independently verified

## Next Checks
1. **Cross-linguistic validation**: Test MQA-KEAL on established English multi-hop QA benchmarks (e.g., HotpotQA) to verify performance gains extend beyond Arabic
2. **Ablation study**: Systematically remove the implication and compositional filtering rules to quantify their individual contribution to performance
3. **Error analysis on decomposition**: Manually examine cases where task decomposition fails to identify patterns in LLM misinterpretation of Arabic multi-hop questions