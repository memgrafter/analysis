---
ver: rpa2
title: 'Sign is Not a Remedy: Multiset-to-Multiset Message Passing for Learning on
  Heterophilic Graphs'
arxiv_id: '2405.20652'
source_url: https://arxiv.org/abs/2405.20652
tags:
- node
- message
- passing
- where
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies two limitations of signed message passing
  (SMP) in Graph Neural Networks for heterophilic graphs: undesirable embedding updates
  for multi-hop neighbors and vulnerability to oversmoothing. To address these issues,
  the authors propose Multiset to Multiset GNN (M2M-GNN), a novel message passing
  scheme that partitions neighbor embeddings into chunks based on classes and concatenates
  the results, avoiding the mixing of heterophilic node features.'
---

# Sign is Not a Remedy: Multiset-to-Multiset Message Passing for Learning on Heterophilic Graphs

## Quick Facts
- arXiv ID: 2405.20652
- Source URL: https://arxiv.org/abs/2405.20652
- Reference count: 40
- Outperforms 12 baselines on 11 datasets with average ranking of 1.7

## Executive Summary
This paper identifies two key limitations of Signed Message Passing (SMP) for heterophilic graphs: undesirable embedding updates for multi-hop neighbors and vulnerability to oversmoothing. To address these issues, the authors propose Multiset-to-Multiset GNN (M2M-GNN), a novel message passing scheme that partitions neighbor embeddings into chunks based on classes and concatenates the results, avoiding the mixing of heterophilic node features. Theoretical analyses prove that M2M-GNN maintains desirable properties across multiple layers and is more robust to oversmoothing compared to SMP. Extensive experiments on 11 benchmark datasets demonstrate that M2M-GNN achieves state-of-the-art results, with an average ranking of 1.7 across all datasets and stable performance across varying model depths.

## Method Summary
M2M-GNN addresses the limitations of SMP in heterophilic graphs by partitioning neighbor embeddings into class-based chunks and concatenating them. The method uses an attention mechanism to assign neighbor embeddings to chunks based on class similarity, then aggregates within each chunk before concatenation. This approach prevents intermixing of heterophilic node features while maintaining the desirable properties of SMP across multiple layers. The architecture includes an MLP projection, attention-based chunk assignment, chunk-wise aggregation, and a concatenation operation, followed by an update rule that mixes initial and message features. A regularization term encourages attention scores to be diversified across chunks.

## Key Results
- M2M-GNN outperforms 12 baseline methods on 11 benchmark datasets
- Achieves state-of-the-art results with average ranking of 1.7 across all datasets
- Shows stable performance across varying model depths, indicating better robustness to oversmoothing
- Maintains desirable properties across multiple layers according to theoretical analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiset-to-Multiset message passing partitions neighbor embeddings by class and concatenates them, preventing intermixing of heterophilic node features.
- Mechanism: Each neighbor embedding is projected to a lower dimension, then assigned to one of C chunks using attention scores. Within each chunk, only embeddings from nodes of the same class are aggregated, and the results are concatenated.
- Core assumption: The attention scores can learn to separate nodes by class, so each chunk contains only nodes of one class.
- Evidence anchors:
  - [abstract] "partitions neighbor embeddings into chunks based on classes and concatenates the results, avoiding the mixing of heterophilic node features"
  - [section] "we propose to utilize soft labels that are obtained via an attention function... these scores can be interpreted as a soft label of vj, and in the ideal case, M2M-GNN assigns a high score to the chunk that corresponds to the class of vj"
  - [corpus] No direct evidence; assumption relies on learned attention.
- Break condition: If attention fails to separate classes, chunks will contain mixed embeddings and the benefit disappears.

### Mechanism 2
- Claim: M2M-GNN maintains the desirable property across multiple layers, unlike Signed Message Passing (SMP) which can become undesirable for multi-hop neighbors.
- Mechanism: By ensuring each chunk contains only embeddings from nodes of the same class, the multi-hop aggregation naturally preserves class separation, so the desirable property is maintained.
- Core assumption: One-hop desirable message passing, when applied repeatedly, yields a desirable multi-hop operation if class separation is preserved.
- Evidence anchors:
  - [abstract] "Theoretical analyses prove that M2M-GNN maintains desirable properties across multiple layers and is more robust to oversmoothing compared to SMP"
  - [section] "Lemma 3.5 (Maintenance of desirable property)... we claim that stacking one-hop desirable m-2-m message passing operations always gives us a desirable (multi-hop) message passing"
  - [corpus] No direct evidence; based on theoretical lemma.
- Break condition: If chunks become mixed (e.g., due to poor attention), multi-hop propagation will mix classes again.

### Mechanism 3
- Claim: M2M-GNN is more robust to oversmoothing because concatenation of per-class chunks preserves inter-class distance better than simple weighted summation.
- Mechanism: Instead of mixing all neighbor embeddings into one vector (as in SMP), M2M-GNN keeps class-specific chunks separate and concatenates them, which increases the distance between message vectors of different classes.
- Core assumption: The distance between concatenated per-class chunks is greater than or equal to the distance after mixing all embeddings.
- Evidence anchors:
  - [abstract] "is more robust to oversmoothing compared to SMP"
  - [section] "Lemma 3.6 (m-2-m is always no worse than m-2-e)... the distance between the two resulting message vectors of m-2-m is greater than or equal to the distance between those of m-2-e"
  - [corpus] No direct evidence; based on theoretical lemma.
- Break condition: If per-class chunks collapse to similar values, concatenation will not help.

## Foundational Learning

- Concept: Graph Neural Networks and message passing
  - Why needed here: The entire method is a novel message passing scheme for GNNs; understanding standard GNN operations is essential.
  - Quick check question: In a standard GCN layer, how are neighbor embeddings aggregated into a node's new representation?

- Concept: Signed Message Passing (SMP) and heterophily
  - Why needed here: SMP is the baseline being improved; knowing how it uses positive/negative weights to handle heterophily is key.
  - Quick check question: In SMP, what do positive and negative propagation coefficients aim to achieve for homophilic vs heterophilic edges?

- Concept: Multi-class classification on graphs
  - Why needed here: The method specifically addresses multi-class cases where SMP fails; understanding why multi-class is harder is important.
  - Quick check question: Why does SMP become undesirable in multi-class graphs even if all per-layer propagation matrices are desirable?

## Architecture Onboarding

- Component map:
  - Input node features X -> MLP projection -> Initial embeddings h(0) -> For each layer: Projection W(k) -> Attention computation -> Chunk aggregation -> Concatenation -> Update with β -> Final node embeddings

- Critical path:
  - Attention computation and chunk aggregation dominate runtime; ensure efficient sparse matrix operations.
  - Regularization loss must be balanced with task loss via λ.

- Design tradeoffs:
  - Number of chunks C vs number of classes C: Setting C = C is best but not required; more chunks may over-separate.
  - Temperature τ in attention: Controls sharpness of attention; too low may collapse to one chunk.
  - β in update: Controls mix of initial features and message; too high may cause oversmoothing.

- Failure signatures:
  - If attention scores collapse to one chunk per node, M2M-GNN behaves like standard message passing.
  - If regularization is too weak, attention may not separate classes well.
  - If C is much larger than actual number of classes, chunks may become noisy.

- First 3 experiments:
  1. Run on a small heterophilic dataset (e.g., Cornell) with C = number of classes; check if attention scores align with true labels.
  2. Compare node classification accuracy with/without Lreg; verify regularization helps.
  3. Vary K (number of layers) and plot accuracy; confirm robustness to oversmoothing compared to SMP baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of M2M-GNN compare to other methods on graphs with more than 5 classes?
- Basis in paper: [explicit] The paper evaluates M2M-GNN on 11 datasets with up to 5 classes, but does not test on graphs with more classes.
- Why unresolved: The paper does not provide experimental results on graphs with more than 5 classes.
- What evidence would resolve it: Experimental results on benchmark datasets with more than 5 classes.

### Open Question 2
- Question: What is the impact of the regularization term Lreg on the performance of M2M-GNN in different types of graphs (e.g., strongly heterophilic vs. strongly homophilic)?
- Basis in paper: [explicit] The paper includes a regularization term Lreg in the loss function and mentions that it helps diversify attention scores, but does not provide a detailed analysis of its impact on different graph types.
- Why unresolved: The paper does not analyze the impact of Lreg on graphs with varying levels of homophily/heterophily.
- What evidence would resolve it: A comprehensive analysis of M2M-GNN's performance with and without Lreg on graphs with different homophily/heterophily levels.

### Open Question 3
- Question: How does the choice of the number of chunks C affect the performance of M2M-GNN, especially when C is not equal to the number of classes?
- Basis in paper: [explicit] The paper mentions that setting the number of chunks equal to the number of classes leads to the best performance, but does not explore the effects of choosing C ≠ C in detail.
- Why unresolved: The paper does not provide an in-depth analysis of the impact of varying C when it is not equal to the number of classes.
- What evidence would resolve it: An extensive ablation study varying C across different values and analyzing the resulting performance.

## Limitations

- The method relies heavily on learned attention to separate nodes by class, but provides limited empirical evidence that attention actually achieves this separation in practice.
- Theoretical proofs assume ideal attention behavior without validating it experimentally, which is a critical assumption for the method to work.
- While showing improved robustness to oversmoothing, the paper does not analyze what happens when attention fails to properly separate classes - a critical failure mode.

## Confidence

- **High confidence**: The experimental results showing M2M-GNN outperforms 12 baseline methods on 11 datasets, with an average ranking of 1.7
- **Medium confidence**: The theoretical claims about maintaining desirable properties across layers and robustness to oversmoothing, as these are mathematically proven but rely on ideal assumptions about attention
- **Medium confidence**: The mechanism explanation for why chunking prevents feature mixing, as it is logically sound but not empirically validated

## Next Checks

1. Analyze attention weight distributions across nodes and classes on a sample dataset to verify that attention actually learns to separate nodes by class as claimed
2. Test M2M-GNN on datasets with varying numbers of classes (not just binary or multi-class) to evaluate generalizability beyond the studied cases
3. Perform ablation studies removing the regularization term Lreg to quantify its impact on both attention quality and final performance