---
ver: rpa2
title: 'CoCA: Regaining Safety-awareness of Multimodal Large Language Models with
  Constitutional Calibration'
arxiv_id: '2409.11365'
source_url: https://arxiv.org/abs/2409.11365
tags:
- safety
- arxiv
- language
- image
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the safety vulnerabilities of multimodal large
  language models (MLLMs) to malicious visual inputs. The authors observe that MLLMs
  possess inherent safety-awareness inherited from text-based safety training, but
  this awareness is weakened by the modality gap between images and text.
---

# CoCA: Regaining Safety-awareness of Multimodal Large Language Models with Constitutional Calibration

## Quick Facts
- **arXiv ID**: 2409.11365
- **Source URL**: https://arxiv.org/abs/2409.11365
- **Reference count**: 26
- **Primary result**: Training-free safety enhancement method that reduces attack success rates by 69% while preserving model capabilities

## Executive Summary
CoCA addresses the safety vulnerabilities of multimodal large language models (MLLMs) to malicious visual inputs. The method recognizes that MLLMs inherit safety-awareness from text-based safety training, but this awareness is weakened by the modality gap between images and text. CoCA amplifies this inherited safety-awareness through inference-time calibration, calculating the disparity between model outputs with and without safety principles, then scaling and incorporating this difference into the original logits during generation. Experiments demonstrate significant safety improvements across multiple MLLMs while maintaining general capabilities on visual understanding tasks.

## Method Summary
CoCA is a training-free inference-time method that enhances MLLM safety by amplifying inherited safety-awareness through logit calibration. The approach calculates a "safety delta" by comparing model outputs with and without safety principles appended to the input prompt. This delta is scaled by a factor (α) and added to the original logits during decoding, effectively boosting the model's safety response when needed. The method preserves original capabilities because for non-safety-related queries, the safety delta is negligible, resulting in minimal impact on normal visual understanding tasks.

## Key Results
- Reduces LLaVA-7B attack success rate on MM-SafetyBench by 69% on average across different image types
- Maintains similar performance on general visual understanding tasks (MM-Vet, GQA, VQA)
- Demonstrates effectiveness across multiple MLLM architectures including LLaVA, CogVLM, and mPLUG-Owl2
- Shows improved performance with task-specific safety principles versus generic ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MLLMs inherit safety-awareness from text-based safety training, but this awareness is weakened by the modality gap between images and text.
- Mechanism: Safety principles learned during text-only training are transferable to image inputs, but the continuous nature of images compared to discrete text tokens reduces effectiveness.
- Core assumption: Safety awareness is a learned property that can transfer across modalities even when input format changes significantly.
- Evidence anchors:
  - [abstract] "the model's safety awareness becomes boosted" when adding safety principles to input
  - [section] "the safety awareness inherited from the textual safety training is transferable to image inputs, it is only weakened due to the modality gap"
  - [corpus] Weak - most related papers focus on attacking MLLMs rather than explaining why they have residual safety awareness
- Break condition: If the underlying LLM has not been trained with safety principles, this mechanism would not apply since there would be no inherited safety awareness to transfer.

### Mechanism 2
- Claim: CoCA amplifies the safety-awareness of MLLMs by calibrating its output distribution through logit adjustment.
- Mechanism: By calculating the disparity between logits with and without safety principles, scaling this difference, and incorporating it into original logits during inference, CoCA explicitly controls the strength of safety prompts without requiring additional training.
- Core assumption: The difference in model outputs with and without safety prompts contains actionable information about the model's safety preferences that can be amplified.
- Evidence anchors:
  - [abstract] "amplifies the safety-awareness of the MLLM by calibrating its output distribution"
  - [section] "we calculate the disparity between the logits of the model's prediction with and without the supplementary safety prompt"
  - [corpus] Weak - related work focuses on attacking MLLMs rather than calibration-based defense mechanisms
- Break condition: If the model's logits do not meaningfully differ between safe and unsafe prompts, the safety delta would be near zero and CoCA would have minimal effect.

### Mechanism 3
- Claim: The safety delta approach preserves original capabilities while enhancing safety because for non-safety-related queries, the delta between safe and unsafe prompts is negligible.
- Mechanism: Since the logit difference only becomes significant for safety-related queries, the calibration primarily affects harmful outputs while leaving normal visual understanding and reasoning tasks largely unchanged.
- Core assumption: The model naturally produces similar outputs for safety-neutral queries regardless of safety prompts, resulting in minimal calibration impact on general capabilities.
- Evidence anchors:
  - [abstract] "CoCA significantly improves safety performance while preserving the models' original capabilities"
  - [section] "for queries unrelated to safety concerns, responses from both sets of prompts are expected to be similar, resulting in a negligible safety delta"
  - [corpus] Weak - related papers don't discuss capability preservation during safety enhancement
- Break condition: If the model's logits differ significantly between safe and unsafe prompts even for neutral queries, CoCA could unnecessarily alter normal outputs and degrade performance.

## Foundational Learning

- Concept: Modality gap and its impact on model behavior
  - Why needed here: Understanding why MLLMs are vulnerable despite inheriting safety from LLMs requires grasping how continuous image representations differ from discrete text tokens
  - Quick check question: Why would an image of a bomb cause a safety-trained model to generate harmful content when text asking about bombs would not?

- Concept: Contrastive decoding and logit calibration
  - Why needed here: CoCA builds on contrastive decoding techniques by using logit differences to guide generation, requiring understanding of how probability distributions are derived from logits
  - Quick check question: How does adjusting logits before softmax affect the probability distribution of generated tokens?

- Concept: Inference-time alignment techniques
  - Why needed here: CoCA operates during inference rather than training, representing a shift from traditional alignment approaches that require fine-tuning
  - Quick check question: What are the advantages and disadvantages of adjusting model behavior during inference versus during training?

## Architecture Onboarding

- Component map:
  Vision encoder (ViT or similar) → Image embeddings → Projector function → Maps image embeddings to vocabulary space → LLM backbone → Processes concatenated text and image embeddings → CoCA module → Calculates safety delta and adjusts logits during decoding → Safety principle prompt → Additional text indicating safety requirements

- Critical path:
  1. User provides image and query
  2. Image encoded → Projected to token space
  3. Combined with text prompt → Input to LLM
  4. CoCA calculates logits difference (with vs without safety principle)
  5. Adjusts logits using scaled safety delta
  6. Softmax → Token probability distribution
  7. Generate response

- Design tradeoffs:
  - CoCA vs. training-based safety: Training-free but may be less robust than fine-tuned approaches
  - Strength of safety delta: Higher scaling improves safety but risks degrading general capabilities
  - Prompt specificity: Task-specific safety principles work better than generic ones but require more customization

- Failure signatures:
  - If ASR doesn't improve: Safety delta calculation may be ineffective or scaling factor too low
  - If general performance drops significantly: Safety delta may be too large or model's logits differ too much even for neutral queries
  - If method only works on some models: Implementation may depend on specific LLM architecture or training history

- First 3 experiments:
  1. Verify safety delta exists: Compare LLM outputs with and without safety principles on harmful vs. neutral queries
  2. Test calibration impact: Apply CoCA with varying scaling factors on known safety benchmarks
  3. Check capability preservation: Run standard vision-language tasks (VQA, GQA) with CoCA enabled

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mechanism by which the safety awareness inherited from textual safety training transfers to image inputs in MLLMs?
- Basis in paper: [explicit] The paper states that safety-awareness inherited from safety training with textual data is transferable to image inputs, but it is only weakened due to the modality gap.
- Why unresolved: The paper does not provide a detailed explanation of how the safety awareness is transferred from text to images. It only mentions that the modality gap weakens this awareness.
- What evidence would resolve it: Experimental results showing the exact process of how textual safety principles are applied to image inputs in MLLMs, and how the modality gap affects this process.

### Open Question 2
- Question: How does the effectiveness of CoCA vary with different types of safety principles and their complexity?
- Basis in paper: [explicit] The paper mentions that CoCA demonstrates more effectiveness given principles customized for the specific task, and that if the principles are too complex, the derived delta may confuse the MLLM during the decoding phase.
- Why unresolved: The paper does not provide a comprehensive analysis of how different types and complexities of safety principles affect the performance of CoCA.
- What evidence would resolve it: Comparative experiments showing the performance of CoCA with various types and complexities of safety principles, and the impact of these variations on the model's safety awareness and capabilities.

### Open Question 3
- Question: What is the impact of CoCA on the long-term behavior and safety of MLLMs in real-world applications?
- Basis in paper: [inferred] The paper focuses on the immediate effects of CoCA on safety performance and capabilities, but does not discuss its long-term implications.
- Why unresolved: The paper does not address the potential long-term effects of using CoCA in real-world scenarios, such as changes in model behavior over time or the model's adaptability to new safety challenges.
- What evidence would resolve it: Longitudinal studies or simulations of MLLMs using CoCA in various real-world applications, monitoring changes in safety performance and model behavior over extended periods.

## Limitations

- CoCA's effectiveness depends on the existence of meaningful safety delta values, which sophisticated attacks could minimize
- The method may be less robust than training-based safety approaches against advanced adversarial examples
- The optimal scaling factor (α) varies across models and tasks, requiring model-specific tuning

## Confidence

**High Confidence**: The core observation that MLLMs inherit safety-awareness from text-based training is well-supported by experimental results showing significant ASR reduction across multiple models.

**Medium Confidence**: The claim that capability preservation is maintained requires more scrutiny, as aggregate metrics may mask task-specific impacts or edge cases.

**Low Confidence**: The generalizability of CoCA to different types of attacks beyond TYPO, SD, and SD+TYPO remains uncertain.

## Next Checks

1. **Cross-model calibration sensitivity analysis**: Systematically vary the scaling factor α across LLaVA, CogVLM, and mPLUG-Owl2 models to identify optimal values for each and map the full safety-capability tradeoff space.

2. **Adversarial attack robustness testing**: Design and test more sophisticated attack patterns that specifically target the safety delta mechanism, such as crafting inputs that minimize logit differences regardless of safety prompts.

3. **Long-context safety consistency evaluation**: Test CoCA's effectiveness on extended conversations or documents where safety principles might need to be maintained across multiple turns.