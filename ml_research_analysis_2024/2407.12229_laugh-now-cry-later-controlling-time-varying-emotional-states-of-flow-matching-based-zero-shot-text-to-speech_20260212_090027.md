---
ver: rpa2
title: 'Laugh Now Cry Later: Controlling Time-Varying Emotional States of Flow-Matching-Based
  Zero-Shot Text-to-Speech'
arxiv_id: '2407.12229'
source_url: https://arxiv.org/abs/2407.12229
tags:
- speech
- emotion
- data
- emotional
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EmoCtrl-TTS is an emotion-controllable zero-shot text-to-speech
  (TTS) system that can generate highly emotional speech with nonverbal vocalizations
  (NVs) like laughter and crying for any speaker. The system leverages arousal and
  valence values, as well as laughter embeddings, to condition a flow-matching-based
  zero-shot TTS model.
---

# Laugh Now Cry Later: Controlling Time-Varying Emotional States of Flow-Matching-Based Zero-Shot Text-to-Speech

## Quick Facts
- arXiv ID: 2407.12229
- Source URL: https://arxiv.org/abs/2407.12229
- Reference count: 0
- EmoCtrl-TTS achieves superior performance in speaker similarity, naturalness, and emotion similarity compared to baseline models.

## Executive Summary
EmoCtrl-TTS introduces an emotion-controllable zero-shot TTS system that generates highly emotional speech with nonverbal vocalizations like laughter and crying for any speaker. The system conditions a flow-matching-based TTS model with arousal, valence, and laughter embeddings to produce speech that mimics audio prompt emotions while incorporating appropriate NVIs. Trained on over 27,000 hours of expressive real-world data curated through pseudo-labeling, EmoCtrl-TTS demonstrates strong performance in speech-to-speech translation scenarios requiring emotional expression.

## Method Summary
EmoCtrl-TTS leverages a flow-matching-based TTS architecture conditioned on arousal-valence values and laughter embeddings extracted from audio prompts. The system is trained on a massive dataset combining Libri-light, LAUGH, and IH-EMO datasets (totaling over 27,000 hours of expressive speech). The flow-matching model learns to transform a simple prior distribution into complex speech distributions while incorporating emotional embeddings. During inference, the model extracts emotion and laughter embeddings from reference audio, combines them with text input, and generates mel-spectrograms that are converted to speech using a vocoder.

## Key Results
- EmoCtrl-TTS excels at capturing emotion changes and generating various nonverbal vocalizations in zero-shot TTS scenarios
- The model achieves superior performance in speaker similarity, naturalness, and emotion similarity compared to baseline models
- Comprehensive evaluations demonstrate strong performance in speech-to-speech translation contexts with emotional audio prompts

## Why This Works (Mechanism)

### Mechanism 1
Integrating arousal and valence embeddings with laughter embeddings allows EmoCtrl-TTS to generate speech with time-varying emotional states and various nonverbal vocalizations. The arousal and valence values provide a continuous representation of emotional intensity and positivity/negativity, while the laughter embeddings capture the spectral characteristics of nonverbal vocalizations. By conditioning the flow-matching-based TTS model with these embeddings, the model can generate speech that not only mimics the emotional content of the audio prompt but also includes appropriate nonverbal vocalizations.

### Mechanism 2
Using a large-scale dataset (27,000 hours) of expressive real-world data curated through pseudo-labeling enhances the robustness and quality of EmoCtrl-TTS. The large-scale dataset provides the model with a diverse range of emotional expressions and speaker characteristics. By leveraging pseudo-labeling techniques, the dataset can be curated efficiently, ensuring that the model is exposed to a wide variety of emotional states and speaker voices. This diversity in training data enables the model to generate more natural and expressive speech for any speaker.

### Mechanism 3
The flow-matching-based architecture of EmoCtrl-TTS enables fine-grained control over the emotional content of generated speech. Flow-matching models learn to transform a simple prior distribution into a complex data distribution through a series of invertible transformations. By conditioning the flow-matching model on arousal, valence, and laughter embeddings, EmoCtrl-TTS can generate speech that closely mimics the emotional content of the audio prompt at a fine-grained level. This allows for the generation of speech with subtle emotional variations and nonverbal vocalizations.

## Foundational Learning

- Concept: Flow-matching-based generative models
  - Why needed here: EmoCtrl-TTS leverages a flow-matching-based architecture to generate speech conditioned on emotional embeddings.
  - Quick check question: What is the key difference between flow-matching models and traditional generative models like GANs or VAEs?

- Concept: Arousal and valence representation of emotions
  - Why needed here: EmoCtrl-TTS uses arousal and valence values to represent the emotional content of speech, enabling fine-grained control over emotional states.
  - Quick check question: How do arousal and valence values differ from categorical emotion labels in representing emotional states?

- Concept: Pseudo-labeling for data curation
  - Why needed here: EmoCtrl-TTS leverages pseudo-labeling techniques to curate a large-scale dataset of expressive real-world speech for training.
  - Quick check question: What are the potential benefits and limitations of using pseudo-labeling for data curation in speech synthesis tasks?

## Architecture Onboarding

- Component map: Text -> Embedding Extractor -> Flow-Matching Model -> Mel-Spectrogram -> Vocoder -> Speech

- Critical path:
  1. Extract arousal, valence, and laughter embeddings from audio prompt.
  2. Condition flow-matching model on extracted embeddings and text prompt.
  3. Generate mel-spectrogram using flow-matching model.
  4. Convert mel-spectrogram to speech using vocoder.

- Design tradeoffs:
  - Using a large-scale dataset of expressive real-world speech improves model robustness but requires significant computational resources for training.
  - Conditioning on both arousal and valence embeddings enables fine-grained control over emotional states but increases model complexity.
  - Leveraging pseudo-labeling for data curation is efficient but may introduce noise or errors in the labels.

- Failure signatures:
  - Inability to generate appropriate nonverbal vocalizations (e.g., laughter or crying) when conditioning on corresponding embeddings.
  - Poor performance in mimicking the emotional content of audio prompts, as measured by objective metrics like Emo SIM and Aro-Val SIM.
  - Degradation in speaker similarity or naturalness of generated speech compared to baseline models.

- First 3 experiments:
  1. Evaluate the impact of different data ratios (Libri-light, LAUGH, and IH-EMO) on model performance using objective metrics like SIM-o, WER, AutoPCP, Emo SIM, and Aro-Val SIM.
  2. Assess the contribution of each embedding type (arousal, valence, and laughter) to the model's ability to generate emotional speech with nonverbal vocalizations by training and evaluating models with different combinations of embeddings.
  3. Compare the performance of EmoCtrl-TTS with baseline models on real-world datasets containing laughter and crying (e.g., Laughter-test and Crying-test) to validate the model's robustness and generalization capabilities.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of EmoCtrl-TTS scale with larger emotional training datasets beyond 27,000 hours?
- Basis in paper: [explicit] The paper notes that most prior emotional TTS systems used less than 100 hours of training data, while EmoCtrl-TTS used 27,000 hours. It suggests this is the first investigation of the impact of using large-scale emotional data for TTS training.
- Why unresolved: The paper only explores one scale of emotional training data (27,000 hours). There's no analysis of whether performance plateaus, continues to improve, or degrades with even larger datasets.
- What evidence would resolve it: Experiments comparing EmoCtrl-TTS performance across multiple dataset sizes (e.g., 100, 1,000, 10,000, 27,000, 50,000+ hours) on consistent evaluation metrics.

### Open Question 2
- Question: Can the laughter detector-based NV embedding effectively capture and generate other nonverbal vocalizations beyond laughter and crying?
- Basis in paper: [explicit] The paper states that "we found that this laughter detector-based embedding actually captures a broader range of NV types than just laughter" and successfully generated various NVs including crying. However, it only explicitly tests laughter and crying.
- Why unresolved: The paper only demonstrates effectiveness for laughter and crying. Other NVs like sighs, gasps, throat clearing, or humming are not evaluated.
- What evidence would resolve it: Comprehensive testing of EmoCtrl-TTS with diverse NV types (e.g., 10+ different vocalizations) across multiple speakers and evaluation metrics for each NV type.

### Open Question 3
- Question: What is the impact of including both arousal and valence embeddings versus using either one alone for emotion controllability in TTS?
- Basis in paper: [inferred] The paper uses both arousal and valence values from a pre-trained extractor, noting that "this representation allows for capturing more nuanced emotional variations." However, it doesn't explore whether one component is more important than the other.
- Why unresolved: The ablation study only tests with and without both embeddings together, not individually. The paper also notes that adding dominance values "hurt the audio quality" but doesn't explore whether removing arousal or valence alone would have similar effects.
- What evidence would resolve it: Ablation studies comparing EmoCtrl-TTS performance with only arousal, only valence, and both embeddings across the same evaluation metrics used in the paper.

## Limitations
- Dataset representativeness concerns due to limited validation of pseudo-labeled data accuracy
- Evaluation scope constraints with focus on controlled translation scenarios rather than diverse real-world applications
- Technical specification gaps including missing implementation details for emotion and laughter detectors

## Confidence

**High Confidence**: The fundamental claim that arousal-valence representations combined with laughter embeddings can control emotional speech generation is well-supported by experimental results showing improvements over baselines in emotion similarity metrics.

**Medium Confidence**: The claim that EmoCtrl-TTS excels at capturing emotion changes and generating various NVs is supported by specific test cases (Laughter-test, Crying-test) but lacks broader validation across diverse emotional expressions and speaker demographics.

**Low Confidence**: The assertion that the model achieves "superior performance in terms of speaker similarity, naturalness, and emotion similarity compared to baseline models" is difficult to fully verify given missing technical details and limited scope of comparative evaluations.

## Next Checks

1. **Dataset Quality Validation**: Conduct a systematic audit of the pseudo-labeled dataset by comparing a sample of pseudo-labels against human annotations to establish the accuracy and reliability of the emotion and laughter embeddings.

2. **Cross-Demographic Testing**: Evaluate EmoCtrl-TTS performance across diverse speaker populations including different age groups, genders, and accents to assess generalization beyond the primarily English-speaking training data.

3. **Real-World Application Testing**: Test the model in practical scenarios such as conversational AI systems or accessibility applications to validate whether the laboratory-level performance translates to useful real-world emotional expression capabilities.