---
ver: rpa2
title: Aligning Language Models to Explicitly Handle Ambiguity
arxiv_id: '2404.11972'
source_url: https://arxiv.org/abs/2404.11972
tags:
- ambiguous
- ambiguity
- question
- samples
- unambiguous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces APA, a method to align LLMs to explicitly\
  \ handle ambiguous queries by leveraging the model\u2019s own perception of ambiguity.\
  \ The core idea is to use the model\u2019s self-disambiguation to estimate its perceived\
  \ ambiguity, and train it to request clarification when needed."
---

# Aligning Language Models to Explicitly Handle Ambiguity

## Quick Facts
- arXiv ID: 2404.11972
- Source URL: https://arxiv.org/abs/2404.11972
- Reference count: 28
- Key outcome: APA improves ambiguity detection F1 by up to 6 points compared to baselines

## Executive Summary
This paper introduces APA (Ambiguity Perception-based Alignment), a method to align LLMs to explicitly handle ambiguous queries by leveraging the model's own perception of ambiguity. The core innovation is using the model's self-disambiguation to estimate its perceived ambiguity, then training it to request clarification when needed. Experiments on multiple QA datasets demonstrate that APA improves both ambiguity detection and correct handling of unambiguous queries compared to existing baselines.

## Method Summary
APA is a four-stage pipeline that aligns language models to handle ambiguity. First, it performs initial prediction assessment using the model to generate responses and identify potential ambiguity. Second, it detects perceived ambiguity by having the model self-disambiguate queries and calculating information gain (INFOGAIN) based on entropy reduction. Third, it constructs responses using either fixed templates or generated clarification requests. Finally, it performs supervised fine-tuning on a balanced dataset of ambiguous and unambiguous samples selected based on INFOGAIN, while preserving the model's ability to answer unambiguous queries correctly.

## Key Results
- APA achieves up to 6 point gains in ambiguity detection F1 compared to baselines
- The method maintains the model's ability to answer unambiguous queries correctly
- INFOGAIN-based training improves generalization compared to ground-truth ambiguity labels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-disambiguation generates additional context that reduces prediction uncertainty, quantified as information gain
- Core assumption: Model's average entropy reliably proxies uncertainty, and entropy reduction indicates perceived ambiguity
- Evidence: INFOGAIN calculation based on entropy difference between original and disambiguated queries
- Break condition: Insufficient model knowledge may prevent significant entropy reduction

### Mechanism 2
- Claim: Training on perceived ambiguous samples improves ambiguity handling compared to ground-truth labels
- Core assumption: Model's perceived ambiguity correlates with actual ambiguity
- Evidence: INFOGAIN-based data selection shows improved generalization
- Break condition: Model perception diverges from human judgment of ambiguity

### Mechanism 3
- Claim: Alignment preserves ability to answer unambiguous queries while adding ambiguity handling
- Core assumption: Including correctly handled unambiguous samples prevents catastrophic forgetting
- Evidence: MCR metrics show maintained performance on unambiguous queries
- Break condition: Training imbalance or overfitting to ambiguity detection

## Foundational Learning

- Concept: Entropy as uncertainty measure
  - Why needed here: Entropy quantifies model uncertainty for detecting ambiguity
  - Quick check question: How does average entropy relate to model confidence?

- Concept: Information gain calculation
  - Why needed here: Information gain quantifies additional context needed to resolve ambiguity
  - Quick check question: What does high INFOGAIN indicate about model's query perception?

- Concept: Catastrophic forgetting prevention
  - Why needed here: Maintaining existing capabilities requires preventing knowledge loss during fine-tuning
  - Quick check question: Why include Dcorrect in training alongside Dambig?

## Architecture Onboarding

- Component map: Initial Prediction Assessment -> Perceived Ambiguity Detection -> Response Construction -> Supervised Fine-Tuning
- Critical path: Disambiguation template → INFOGAIN calculation → ambiguous sample selection
- Design tradeoffs: INFOGAIN allows training on perceived ambiguity vs. ground-truth labels, with threshold controlling precision-recall balance
- Failure signatures: Low F1u indicates loss of unambiguous query handling; low F1a suggests poor ambiguity detection; high MCR indicates unnecessary clarification requests
- First 3 experiments:
  1. Test INFOGAIN calculation on small query set to verify ambiguity capture
  2. Compare F1u and F1a with different threshold values for optimal balance
  3. Measure MCR to ensure maintained unambiguous query handling after alignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does APA's performance scale with larger LLMs (GPT-4, Gemini) compared to smaller models?
- Basis: Paper primarily evaluates on LLaMA 2 7B/13B and Mistral 7B, acknowledging larger models may exhibit different tendencies
- Why unresolved: No experimental results for larger models; identified as future research direction
- What evidence would resolve it: Experiments with larger LLMs comparing ambiguity detection and handling performance

### Open Question 2
- Question: Can APA be extended to handle ambiguity in conversational QA with accumulating context?
- Basis: Paper focuses on single-query scenarios, identifies conversational QA as limitation
- Why unresolved: Current framework designed for single queries; authors explicitly suggest this as future work
- What evidence would resolve it: Implementation in conversational QA setting with multi-turn interactions

### Open Question 3
- Question: How does APA compare when using RLHF or DPO instead of SFT?
- Basis: Paper uses SFT and acknowledges other methods could offer distinct advantages
- Why unresolved: Only SFT evaluated; other methods not experimentally compared
- What evidence would resolve it: Implementation using RLHF and DPO with performance comparison

## Limitations

- Ground-truth ambiguity labeling reliability: Labels may not perfectly align with model-perceived ambiguity
- Threshold sensitivity: INFOGAIN threshold significantly impacts training sample selection without comprehensive sensitivity analysis
- Zero-shot performance gaps: Performance degrades on out-of-domain datasets with different ambiguity characteristics

## Confidence

**High Confidence (8/10)**
- Self-disambiguation as uncertainty proxy (Mechanism 1)
- Preserving unambiguous query handling (Mechanism 3)

**Medium Confidence (6/10)**
- Perceived vs. ground-truth ambiguity training (Mechanism 2)
- INFOGAIN effectiveness as training signal

**Low Confidence (4/10)**
- Performance with different model architectures beyond tested variants
- Computational overhead and practical deployment considerations

## Next Checks

1. Conduct systematic threshold sensitivity analysis across INFOGAIN values to identify optimal precision-recall tradeoffs

2. Perform human evaluations comparing model-perceived ambiguity with human judgments to validate correlation

3. Test APA on additional model architectures (LLaMA, Mistral) to assess generalization beyond original models