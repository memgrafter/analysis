---
ver: rpa2
title: Skipping Computations in Multimodal LLMs
arxiv_id: '2410.09454'
source_url: https://arxiv.org/abs/2410.09454
tags:
- arxiv
- llms
- skipping
- training
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the computation redundancy in Multimodal
  Large Language Models (MLLMs) and proposes methods to skip computations such as
  entire blocks, FFN layers, or self-attention layers. The research demonstrates that
  significant computations can be avoided during inference, especially for Visual
  Question Answering tasks, with up to 33% of blocks skippable while retaining over
  90% of performance.
---

# Skipping Computations in Multimodal LLMs

## Quick Facts
- arXiv ID: 2410.09454
- Source URL: https://arxiv.org/abs/2410.09454
- Authors: Mustafa Shukor; Matthieu Cord
- Reference count: 40
- Key outcome: Significant computations can be avoided during inference for VQA tasks, with up to 33% of blocks skippable while retaining over 90% performance, and training with compressed LLMs can achieve 97% of original performance even with 70% parameter removal.

## Executive Summary
This study investigates computation redundancy in Multimodal Large Language Models (MLLMs) and demonstrates that significant computations can be skipped during inference without major performance loss, particularly for Visual Question Answering tasks. The research shows that entire blocks, FFN layers, or self-attention layers can be skipped while maintaining over 90% of baseline performance. Additionally, the study reveals that training with compressed LLMs or smaller models can achieve comparable performance to larger models, with up to 97% performance retention even when skipping 50% of blocks or removing 70% of parameters. These findings are validated across various datasets and extended to larger-scale models like LLaVA-1.5.

## Method Summary
The study employs frozen LLMs with trainable mapping modules and frozen perceptual encoders to investigate computation skipping strategies. The approach involves static skipping of entire blocks, FFN layers, or self-attention layers during inference, as well as parallelizing certain layers to reduce wall-clock time. Training is conducted using AdamW optimizer with cosine annealing, and various compression techniques (Wanda, α-SubNet) are applied to create compressed LLMs. The mapping module, a lightweight transformer with learnable queries, is trained to compensate for reduced capacity in compressed models. Experiments are conducted across multimodal datasets including images, videos, and audio, with evaluation on VQA and captioning tasks.

## Key Results
- Up to 33% of blocks can be skipped during inference for VQA tasks while retaining over 90% performance
- Training with compressed LLMs can retain 97% of original performance even when 70% of parameters are removed or 50% of blocks are skipped
- Parallelizing FFN and SA layers leads to significantly better results on all tested datasets
- The approach is validated on LLaVA-1.5 with consistent performance retention across multiple benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Skipping entire blocks or FFN/SA layers during inference reduces computational cost without major performance loss for VQA tasks.
- Mechanism: Multimodal LLMs contain redundant blocks where later layers contribute minimally to token refinement, especially for question-answering tasks where generated text tokens can be processed through a reduced model path.
- Core assumption: Embeddings change slowly across layers in MLLMs, making later blocks increasingly redundant for certain tasks.
- Evidence anchors:
  - [abstract] "significant amount of computations can be avoided at inference time, especially for tasks such as Visual Question Answering (VQA)"
  - [section] "Skipping computations during training can recover 97% of the original performance, even when skipping half of the blocks or removing 70% of the weights"
  - [corpus] Weak evidence - corpus papers focus on similar pruning/skip methods but don't directly validate the VQA-specific redundancy claim
- Break condition: Performance degradation exceeds acceptable thresholds (e.g., <90% of baseline) or task complexity increases beyond VQA to require more complex reasoning.

### Mechanism 2
- Claim: Training with compressed LLMs or smaller models achieves comparable performance to larger models.
- Mechanism: The mapping module learns to compensate for the reduced capacity of compressed LLMs through efficient adaptation, leveraging the frozen LLM's general capabilities while training a lightweight adapter.
- Core assumption: The frozen LLM contains sufficient general knowledge, and the mapping module can effectively bridge the gap created by compressed or smaller LLMs.
- Evidence anchors:
  - [abstract] "properly training with smaller LLMs can yield comparable performance to LLMs 2 or 3 times larger"
  - [section] "Training with smaller versions of OPT models leads to comparable performance to larger ones"
  - [corpus] Weak evidence - corpus focuses on inference acceleration rather than training with compressed models
- Break condition: When tasks require complex reasoning or when the compressed model loses critical architectural components that cannot be compensated by the mapping module.

### Mechanism 3
- Claim: Parallelizing FFN and SA layers or entire blocks can reduce inference time without sacrificing performance.
- Mechanism: Sequential dependencies between FFN and SA layers can be relaxed for certain inputs, allowing parallel execution on GPU hardware to improve wall-clock time.
- Core assumption: The mathematical operations in FFN and SA layers can be computed independently for many inputs without affecting the final output significantly.
- Evidence anchors:
  - [section] "parallelizing FFN and SA layers leads to significantly better results on all datasets"
  - [abstract] "we explore parallelizing certain layers, such as FFN and SA layers"
  - [corpus] Weak evidence - corpus papers mention dynamic computation but don't specifically validate FFN/SA parallelization
- Break condition: When parallel execution introduces numerical instability or when specific input patterns require strict sequential processing.

## Foundational Learning

- Concept: Multimodal embeddings and their evolution through transformer layers
  - Why needed here: Understanding how visual and textual tokens interact and change through layers is crucial for determining which computations can be skipped
  - Quick check question: How do multimodal embeddings differ from unimodal embeddings in their layer-wise evolution?

- Concept: Sparse training and pruning techniques
  - Why needed here: The paper builds on existing compression methods like Wanda and α-SubNet, requiring understanding of how weight pruning affects model performance
  - Quick check question: What's the difference between unstructured and structured pruning, and which is more relevant for inference acceleration?

- Concept: Knowledge distillation and adapter-based fine-tuning
  - Why needed here: The approach of training with smaller LLMs relies on understanding how lightweight adapters can effectively transfer knowledge
  - Quick check question: How do adapter-based methods compare to full fine-tuning in terms of parameter efficiency and performance retention?

## Architecture Onboarding

- Component map: Input encoding → Mapping module processing → LLM block execution (with potential skips) → Output generation
- Critical path: The mapping module is the most critical component as it bridges the perceptual encoders to the frozen LLM
- Design tradeoffs:
  - Static vs dynamic computation skipping (static chosen for hardware agnosticism)
  - Layer granularity (blocks vs individual layers vs neurons)
  - Training time vs inference efficiency (training with compressed models reduces both)
  - Task specificity vs generalization (VQA benefits more than captioning from skipping)
- Failure signatures:
  - Performance degradation beyond acceptable thresholds (monitor per-task accuracy)
  - Increased training instability when using highly compressed LLMs
  - Unexpected GPU utilization patterns when parallelizing layers
  - Input-dependent performance variations that weren't captured in static evaluation
- First 3 experiments:
  1. Baseline evaluation: Run standard inference on VQAv2 dataset to establish performance baseline and identify which layers contribute most to accuracy
  2. Static block skipping: Implement block skipping starting from different layers with varying intervals to find optimal skip configuration for VQA tasks
  3. Compressed training evaluation: Train mapping module with 50% and 70% compressed LLMs to verify performance retention claims and identify compression limits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop effective dynamic computation skipping strategies for MLLMs that adapt to input complexity and task requirements?
- Basis in paper: [explicit] The paper mentions that an alternative direction involves exploring more adaptive compute strategies that ideally allocate varying amounts of computations based on the task's complexity. While similar approaches have been proposed for LLMs, the authors believe there is still significant room for improvement in this area.
- Why unresolved: The paper focuses on static computation skipping techniques and acknowledges that dynamic compute strategies could be beneficial but are not explored in depth.
- What evidence would resolve it: Developing and testing dynamic computation skipping methods that show improved efficiency and performance compared to static methods, along with theoretical justification for their effectiveness.

### Open Question 2
- Question: Can the findings on computation redundancy in MLLMs be generalized to more complex multimodal tasks that require reasoning capabilities?
- Basis in paper: [explicit] The paper acknowledges that it primarily focuses on parameter-efficient MLLMs and did not delve into more complex multimodal tasks that necessitate reasoning capabilities. The authors recognize this as an important avenue for future research.
- Why unresolved: The current study is limited to parameter-efficient models and does not explore complex reasoning tasks.
- What evidence would resolve it: Extending the study to complex multimodal reasoning tasks and demonstrating that the observed computation redundancy and skipping techniques still apply effectively.

### Open Question 3
- Question: What are the specific factors that affect the reduction in actual device performance when applying computation skipping techniques to MLLMs?
- Basis in paper: [explicit] The paper mentions that while the study focuses on investigating redundancy that could lead to theoretically high efficient training and inference, the reduction is expected to be smaller at actual devices due to many other affecting factors.
- Why unresolved: The paper identifies that there are other factors affecting actual device performance but does not explore these factors in detail.
- What evidence would resolve it: Conducting empirical studies to identify and quantify the specific factors that influence actual device performance, and developing strategies to mitigate their impact on computation skipping techniques.

## Limitations
- The static skipping approach may miss dynamic optimization opportunities that could be captured with conditional skipping based on input characteristics
- The generalization of skipping strategies to complex reasoning tasks beyond VQA and captioning is uncertain
- The training with compressed LLMs approach relies heavily on the mapping module's ability to compensate for reduced capacity, with unknown limits for more complex tasks

## Confidence
- **High Confidence**: The claim that significant computation can be skipped during inference while maintaining >90% performance for VQA tasks is well-supported by empirical results across multiple datasets
- **Medium Confidence**: The claim about training with compressed LLMs achieving comparable performance to larger models has moderate support, primarily demonstrated on smaller-scale models
- **Low Confidence**: The generalization of these skipping strategies to complex reasoning tasks beyond VQA and captioning is uncertain

## Next Checks
1. **Task Generalization Test**: Evaluate the skipping strategies on complex reasoning benchmarks (e.g., ScienceQA, MME) that require multi-step reasoning beyond simple question answering
2. **Dynamic vs Static Comparison**: Implement a dynamic skipping mechanism that conditions skip decisions on input embeddings or intermediate outputs, then compare performance and efficiency gains against the static approach
3. **Scaling Behavior Analysis**: Apply the compressed training approach to LLaVA-1.5 and larger models, measuring the relationship between model scale, compression ratio, and performance retention