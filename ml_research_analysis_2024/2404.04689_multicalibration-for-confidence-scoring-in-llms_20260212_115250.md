---
ver: rpa2
title: Multicalibration for Confidence Scoring in LLMs
arxiv_id: '2404.04689'
source_url: https://arxiv.org/abs/2404.04689
tags:
- arxiv
- multicalibration
- calibration
- preprint
- scoring
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies multicalibration techniques to generate reliable
  confidence scores for detecting hallucinations in LLM outputs. The key challenge
  addressed is the lack of explicit features to group prompts with similar hallucination
  probabilities.
---

# Multicalibration for Confidence Scoring in LLMs

## Quick Facts
- arXiv ID: 2404.04689
- Source URL: https://arxiv.org/abs/2404.04689
- Reference count: 33
- Key outcome: Multicalibration techniques significantly improve confidence scoring for hallucination detection in LLMs

## Executive Summary
This paper addresses the challenge of detecting hallucinations in LLM outputs by generating reliable confidence scores. The authors propose using multicalibration techniques that ensure calibration within subgroups of prompts, rather than just overall. Since hallucination likelihood varies across prompt types, standard calibration is insufficient. The key innovation is creating groups of prompts with similar hallucination probabilities using either clustering in embedding space or LLM self-annotation with yes-or-no questions. They develop new multicalibration algorithms (IGLB and IGHB) that reduce overfitting through linear scaling updates and early stopping. Extensive experiments on multiple datasets and LLMs demonstrate significant improvements in calibration error and accuracy compared to uncalibrated scores and standard calibration techniques.

## Method Summary
The method involves three main components: initial scoring functions (True/False softmax, inverse perplexity, or multiple-choice softmax), grouping strategies (clustering prompts in embedding space or having the LLM self-annotate with binary features), and multicalibration algorithms (IGHB and IGLB with enhancements). For grouping, clustering uses embeddings reduced to 20 dimensions via UMAP and Gaussian mixture models, while self-annotation involves prompting the LLM to categorize questions into predefined categories. The multicalibration algorithms iteratively update the confidence scores to ensure group-conditional unbiasedness, with IGLB incorporating linear scaling and early stopping to mitigate overfitting. The calibrated scores are then evaluated using metrics like gASCE, MSE, and accuracy.

## Key Results
- IGLB with clustering groups reduces gASCE by 0.097 on MMLU compared to uncalibrated scores
- IGLB with self-annotation groups improves accuracy by 0.015 on BigBench compared to histogram binning
- IGHB and IGLB consistently outperform uncalibrated scores and standard calibration techniques across multiple datasets and LLMs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Clustering prompts in embedding space creates groups correlated with hallucination likelihood, improving calibration.
- **Mechanism:** Semantically similar prompts are likely to have similar difficulty or ambiguity levels, which correlate with hallucination risk. Clustering groups these together.
- **Core assumption:** Embedding similarity reflects semantic similarity, which in turn reflects hallucination probability.
- **Evidence anchors:**
  - [abstract] "clustering prompts in embedding space"
  - [section] "A natural strategy is to find semantically meaningful clusters of prompts within some embedding space"
  - [corpus] Weak/no direct evidence in neighbors about clustering; most focus on calibration post-processing.
- **Break condition:** If embeddings do not capture semantic properties that correlate with hallucination likelihood (e.g., if embeddings are too generic or if hallucination risk is unrelated to semantics).

### Mechanism 2
- **Claim:** LLM self-annotation with yes-or-no questions provides interpretable, correlated groups for multicalibration.
- **Mechanism:** Asking the LLM whether a prompt relates to certain topics or difficulty levels creates binary features. These features correlate with hallucination likelihood because certain topics or complexities may induce more errors.
- **Core assumption:** LLM's own assessment of prompt characteristics (e.g., difficulty, topic) correlates with actual hallucination probability.
- **Evidence anchors:**
  - [abstract] "self-annotation — querying the LLM by asking it various yes-or-no questions about the prompt"
  - [section] "Another approach to forming groups involves using the LLM to annotate prompt/completion pairs"
  - [corpus] No direct evidence; neighbors focus on verbal confidence expression but not on self-annotation for grouping.
- **Break condition:** If the LLM's self-assessments are systematically wrong or uncorrelated with hallucination probability.

### Mechanism 3
- **Claim:** Using upper/lower level set bins (S≤ and S≥) instead of exact value bins reduces overfitting in multicalibration.
- **Mechanism:** Larger conditioning sets (based on CDF rather than density) provide more data points per bin, leading to more stable label mean estimates and less overfitting.
- **Core assumption:** Conditioning on intervals (≤ or ≥) rather than exact values increases bin size without breaking the multicalibration guarantee.
- **Evidence anchors:**
  - [section] "Rather than conditioning on the exact value of the model f(x) = p, we can condition on f(x) ≤ p... the definition of (exact) multicalibration remains unchanged"
  - [section] "updates on large sets are less prone to overfitting because we have many samples to use to estimate their label mean"
  - [corpus] No direct evidence; neighbors do not discuss bin size or overfitting in multicalibration.
- **Break condition:** If the label mean varies significantly within the larger conditioning sets, making the patch ineffective.

## Foundational Learning

- **Concept:** Multicalibration
  - **Why needed here:** Standard calibration averages over all data, missing heterogeneous performance across prompt types. Multicalibration ensures calibration within subgroups, crucial for detecting hallucinations that vary by prompt characteristics.
  - **Quick check question:** What is the difference between calibration and multicalibration in terms of conditioning events?

- **Concept:** Group-conditional unbiasedness
  - **Why needed here:** Groups may overlap, so independent patching per group isn't possible. Group-conditional unbiasedness provides a tractable way to ensure unbiasedness across intersecting groups.
  - **Quick check question:** Why can't we simply apply histogram binning independently to each group?

- **Concept:** Overfitting mitigation in iterative calibration
  - **Why needed here:** Iterative multicalibration algorithms build increasingly complex models, risking overfitting. Techniques like larger bins and early stopping are essential for practical performance.
  - **Quick check question:** How does using S≤ and S≥ bins help reduce overfitting compared to exact value bins?

## Architecture Onboarding

- **Component map:** Initial scoring model -> Grouping strategy (clustering/self-annotation) -> Multicalibration algorithm (IGHB/IGLB) -> Evaluation pipeline

- **Critical path:**
  1. Generate or obtain labeled calibration data (prompts, completions, binary hallucination labels).
  2. Compute initial confidence scores using one of the three scoring methods.
  3. Apply grouping strategy to create groups correlated with hallucination likelihood.
  4. Run multicalibration algorithm (IGLB recommended) to produce calibrated confidence scores.
  5. Evaluate performance using MSE, accuracy, and gASCE metrics.

- **Design tradeoffs:**
  - Clustering vs. self-annotation: Clustering is cheaper at deployment but may miss nuanced features; self-annotation is more interpretable and flexible but adds latency.
  - IGHB vs. IGLB: IGHB is simpler but prone to overfitting; IGLB includes linear scaling and early stopping to mitigate overfitting but is more complex.
  - Bin size: Smaller bins give finer calibration but risk overfitting; larger bins (S≤/S≥) reduce overfitting but may be less precise.

- **Failure signatures:**
  - High gASCE within groups despite low marginal calibration error → groups not well-correlated with hallucination likelihood.
  - Degradation in accuracy after calibration → overfitting or poor grouping.
  - No improvement over uncalibrated scores → initial scoring model already well-calibrated or grouping ineffective.

- **First 3 experiments:**
  1. Run IGLB with clustering groups on MMLU using True/False softmax scores; compare gASCE to uncalibrated scores.
  2. Run IGLB with self-annotation groups on BigBench; compare MSE and accuracy to HB and LS.
  3. Compare IGHB vs. IGLB on OpenBookQA; measure overfitting via validation MSE trend.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of LLM-generated self-annotations affect the performance of multicalibration methods?
- Basis in paper: The paper discusses using LLMs to annotate prompts with binary features, but doesn't systematically study how annotation quality impacts calibration performance.
- Why unresolved: The paper only mentions that annotation quality "may vary with the LLM" but doesn't explore the relationship between annotation accuracy and calibration effectiveness.
- What evidence would resolve it: Experiments comparing multicalibration performance using annotations from LLMs of varying quality, or using noisy/perturbed annotations.

### Open Question 2
- Question: Can multicalibration techniques be extended to handle continuous or multi-valued group features rather than binary annotations?
- Basis in paper: The paper focuses on binary group features from yes-or-no questions, but notes that "any collection of questions can be used to induce a collection of groups."
- Why unresolved: The current framework is limited to binary groups, and it's unclear how well it would generalize to more complex feature types.
- What evidence would resolve it: Extensions of the multicalibration algorithms to handle continuous or categorical group features, with experimental validation.

### Open Question 3
- Question: How do multicalibration methods perform on adversarially crafted prompts designed to exploit the calibration?
- Basis in paper: The impact statement notes that "calibration methods should be used only with an understanding of their limitations" and that "provable calibration guarantees are designed to hold on prompts that are distributed as those in our calibration set are, and do not hold for adversarially generated prompts."
- Why unresolved: The paper doesn't test the robustness of multicalibration against adversarial examples.
- What evidence would resolve it: Experiments evaluating calibration performance on prompts specifically designed to break the calibration (e.g., by exploiting patterns learned from the calibration data).

## Limitations

- The effectiveness of grouping strategies depends on correlation between group characteristics and actual hallucination likelihood, which is not rigorously established
- Long-term robustness across diverse prompt distributions and LLM architectures remains uncertain
- The framework is limited to binary group features, with unclear generalization to continuous or multi-valued features

## Confidence

- **High confidence:** The empirical improvements in calibration error and accuracy using IGLB with both grouping strategies are well-supported by the experimental results.
- **Medium confidence:** The theoretical justification for using upper/lower level set bins (S≤ and S≥) to reduce overfitting is sound, but the practical impact depends on dataset characteristics and implementation details.
- **Low confidence:** The long-term robustness of the grouping strategies across diverse prompt distributions and LLM architectures remains uncertain.

## Next Checks

1. **Group Quality Analysis:** For each grouping strategy, compute the correlation between group-averaged initial confidence scores and group-averaged actual hallucination rates. This will validate whether the groups are genuinely correlated with hallucination likelihood.

2. **Cross-Dataset Generalization:** Apply the IGLB method with clustering groups to a held-out dataset (e.g., a subset of TruthfulQA) not seen during calibration. Compare performance to ensure the method generalizes beyond the training distribution.

3. **Robustness to Group Size:** Systematically vary the number of clusters (for clustering) or the number of self-annotation categories. Measure how gASCE changes with group granularity to identify optimal group sizes and ensure the method is not overly sensitive to hyperparameter choices.