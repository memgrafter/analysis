---
ver: rpa2
title: Shared Control with Black Box Agents using Oracle Queries
arxiv_id: '2410.19612'
source_url: https://arxiv.org/abs/2410.19612
tags:
- control
- oracle
- shared
- action
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a new formalization for shared control systems
  augmented with query capabilities, allowing autonomous agents to request guidance
  from cooperating agents (e.g., humans). Two oracle types are proposed: a teacher
  oracle with full system knowledge and an expert oracle with limited knowledge of
  the black box agent.'
---

# Shared Control with Black Box Agents using Oracle Queries

## Quick Facts
- arXiv ID: 2410.19612
- Source URL: https://arxiv.org/abs/2410.19612
- Reference count: 32
- One-line primary result: Query-based shared control with entropy, utility, and RL heuristics significantly improves learning outcomes while reducing oracle query usage.

## Executive Summary
This paper introduces a new formalization for shared control systems augmented with query capabilities, allowing autonomous agents to request guidance from cooperating agents (e.g., humans). The authors propose two oracle types - teacher oracle with full system knowledge and expert oracle with limited knowledge of the black box agent - and three heuristics for deciding when to query: entropy-based, utility-based, and reinforcement learning-based approaches. The heuristics aim to minimize query usage while maintaining high performance through different mechanisms: entropy reduction, confidence assessment, and reward maximization respectively.

## Method Summary
The method models shared control as a Multi-Agent Markov Decision Process (MA-MDP) where states are joint states of the control and black box agents, and actions are joint actions. The control agent learns a policy using a Recurrent Neural Network (RNN) and can query oracles for action suggestions. Three heuristics determine when to query: entropy-based (queries when expected information gain exceeds threshold), utility-based (queries when RNN confidence falls below threshold), and RL-based (learns to query through Q-learning maximizing reward). The system is evaluated on automata-based shared control scenarios and a lunar lander simulation, comparing performance against baselines with no oracle access.

## Key Results
- Query-based shared control significantly improves learning outcomes compared to no-query baselines
- The utility heuristic achieves the best query efficiency while maintaining high performance
- The RL heuristic reaches the lowest failure rates but uses a large and inconsistent number of queries
- Expert oracles with limited knowledge can actually harm performance in certain scenarios when always queried

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The entropy heuristic improves query efficiency by reducing uncertainty in the control policy before querying the oracle.
- Mechanism: The heuristic computes the information gain from querying an oracle versus relying on the learned policy. It queries only when the expected reduction in entropy (uncertainty) is above a threshold βent, thus avoiding unnecessary queries.
- Core assumption: The learned policy's output probabilities accurately reflect the control's uncertainty about the correct action.
- Evidence anchors:
  - [abstract]: "The Entropy-based heuristic compares the Entropy from the learned policy's suggestion and the expected Entropy from using the oracle"
  - [section]: "Entropy is defined as the randomness or measuring the disorder of the information in our process [25]. It is also a machine learning metric that measures the unpredictability or impurity in the system."
  - [corpus]: Weak evidence - corpus doesn't directly address entropy-based querying heuristics.
- Break condition: If the learned policy is confidently wrong (high probability on incorrect action), the entropy heuristic may avoid querying when it should, leading to persistent errors.

### Mechanism 2
- Claim: The utility heuristic prevents querying when the learned policy is already confident in its action, improving learning efficiency.
- Mechanism: Before considering a query, the heuristic checks if the learned policy's probability for the chosen action exceeds threshold βutil. If confident, it executes the learned action without querying, saving oracle calls.
- Core assumption: The learned policy's confidence level correlates with action correctness, especially in states with clear optimal actions.
- Evidence anchors:
  - [abstract]: "The Utility heuristic reasons about the probability of getting a successful interaction with a query, conditioned by our prior knowledge about successful transitions that state"
  - [section]: "When reaching a state s, it first checks the actions that are available for the control in s, and will choose to use the suggestion of the RNN if it gives the action a probability higher than some chosen threshold βutil."
  - [corpus]: No direct evidence in corpus for utility-based query heuristics.
- Break condition: In states requiring non-myopic reasoning (delayed rewards), the utility heuristic may avoid querying even when oracle guidance would prevent costly mistakes.

### Mechanism 3
- Claim: The reinforcement learning heuristic learns when to query by maximizing long-term reward, including both task success and query efficiency.
- Mechanism: An RL agent learns a Q-function over states and two actions (query or not query). It queries when the expected reward from querying exceeds not querying, considering both immediate task success and cumulative query costs.
- Core assumption: The reward structure (positive for success, negative for failure and queries) properly incentivizes the right balance between querying and task performance.
- Evidence anchors:
  - [abstract]: "The RL-based heuristic takes a Q-learning approach and chooses to query if the expected reward from querying at a state is higher than not querying."
  - [section]: "During training, we run an ϵ-greedy version of Q-learning, where the learner selects the action with the highest expected return or a random action at a ratio of (1 − ϵ) : ϵ."
  - [corpus]: No direct evidence in corpus for RL-based query heuristics in shared control.
- Break condition: If the reward shaping doesn't properly account for the true cost of queries versus task failures, the RL agent may query too frequently or too rarely.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and Multi-Agent MDPs (MA-MDPs)
  - Why needed here: The paper models the shared control problem as an MA-MDP where states are joint states of the control and black box, and actions are joint actions.
  - Quick check question: What are the key differences between a standard MDP and an MA-MDP, and why does shared control require an MA-MDP formulation?

- Concept: Information Gain and Entropy in Machine Learning
  - Why needed here: The entropy heuristic uses information gain to measure the expected reduction in uncertainty from querying an oracle.
  - Quick check question: How does entropy relate to uncertainty in a probability distribution, and why would querying an oracle reduce this entropy?

- Concept: Reinforcement Learning and Q-learning
  - Why needed here: The RL heuristic uses Q-learning to learn when to query by estimating the value of querying versus not querying in each state.
  - Quick check question: In Q-learning, what does the Q-function represent, and how does an agent use it to make decisions about actions?

## Architecture Onboarding

- Component map: Black box agent -> Control agent (RNN policy) -> Oracle (teacher/expert) -> Query heuristics (Entropy/Utility/RL) -> Operation protocol -> Environment

- Critical path: 1. Control observes state (partial visibility) 2. Heuristic decides whether to query oracle 3. If querying, oracle provides action suggestion 4. Control selects action (learned or oracle-suggested) 5. Joint action executed according to operation protocol 6. State transitions, reward observed 7. RNN policy updated based on experience 8. Q-function updated if using RL heuristic

- Design tradeoffs:
  - Query frequency vs. performance: More queries generally improve accuracy but increase cost
  - Oracle type: Teacher oracles (full system knowledge) vs. Expert oracles (limited knowledge)
  - Heuristic choice: Entropy (uncertainty-based), Utility (confidence-based), RL (reward-based)
  - State visibility: Partial vs. full state observability affects learning difficulty

- Failure signatures:
  - High failure rate despite many queries: Oracle may be misleading or heuristic poorly calibrated
  - Low failure rate but many queries: Heuristic not efficiently selecting when to query
  - High failure rate with few queries: Heuristic too conservative, missing opportunities for oracle guidance
  - Oscillation between querying and not querying: Q-function values too close, need better exploration/exploitation balance

- First 3 experiments:
  1. Implement the automata-based shared control environment with the "Cases" scenario and run with no oracle to establish baseline performance.
  2. Add the teacher oracle and implement the utility heuristic, comparing query count and failure rate against the baseline.
  3. Implement the RL heuristic with Q-learning, tuning the reward structure to balance task success and query efficiency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of the oracle affect the learning efficiency and final performance of shared control systems, particularly when the oracle provides misleading or incorrect guidance?
- Basis in paper: [explicit] The paper discusses two oracle types (teacher and expert) and demonstrates that an expert oracle with limited knowledge can actually hinder performance in certain scenarios, such as the Strategy use case where it led to high failure rates when always querying during training and testing.
- Why unresolved: The study only compares two oracle types with predefined knowledge levels. The paper doesn't explore how varying oracle quality (partially knowledgeable, intentionally misleading, or time-varying reliability) affects learning outcomes or how the system could detect and adapt to oracle quality.
- What evidence would resolve it: Empirical results showing performance metrics across multiple oracle types with varying knowledge levels, reliability scores, and potential for intentional deception, including adaptive querying strategies that adjust based on observed oracle accuracy.

### Open Question 2
- Question: What is the optimal balance between query frequency during training versus testing phases in shared control systems, and how does this balance vary across different environments and oracle types?
- Basis in paper: [explicit] The paper shows that some heuristics (Entropy and Utility) do not leverage the ability to query during test time even when it could improve performance, while the RL heuristic learned to query at specific times. The paper mentions that querying can be costly but doesn't specify how to optimize this trade-off.
- Why unresolved: The study evaluates fixed query strategies (always train, always train+test, heuristics) but doesn't systematically explore the optimal query frequency or phase-specific strategies. The cost-benefit analysis of querying in different phases remains unexplored.
- What evidence would resolve it: Comparative studies showing performance, query efficiency, and overall system cost across varying query frequencies and phase-specific strategies, including cost models that quantify the trade-offs between query usage and performance improvements.

### Open Question 3
- Question: How can reinforcement learning techniques be better integrated into the querying heuristics to achieve both low failure rates and query efficiency simultaneously?
- Basis in paper: [explicit] The paper notes that the RL heuristic achieves the lowest failure rates but at the cost of a large and inconsistent number of queries. The authors suggest that future work could investigate using RL techniques to make querying more efficient and consistent.
- Why unresolved: The current RL heuristic uses a separate Q-learning process for deciding whether to query, but this doesn't directly optimize the main control policy. The paper suggests replacing the RNN with an RL algorithm but doesn't implement or evaluate this approach.
- What evidence would resolve it: Empirical results comparing different RL-based approaches where the querying decision is integrated into the main control policy learning, showing both failure rates and query counts across multiple domains and complexity levels.

## Limitations

- The entropy and utility heuristics depend critically on accurate probability estimates from the learned RNN policy, but the paper doesn't address what happens when these estimates are overconfident or underconfident.
- The RL heuristic's performance is highly sensitive to reward shaping, yet the specific reward values for the automata domain are not fully detailed.
- The expert oracle's limited knowledge creates scenarios where querying can actually harm performance (as seen in the Strategy use case), but the paper doesn't provide clear guidance on when this tradeoff becomes problematic.

## Confidence

- **High confidence**: The basic mechanism of using query heuristics to improve shared control performance is well-supported by the empirical results across both domains.
- **Medium confidence**: The relative performance ranking of the three heuristics (utility > entropy > RL in most cases) is supported, but the extreme variance in Combination Lock results suggests sensitivity to initialization.
- **Low confidence**: The theoretical justification for the information gain calculations in the entropy heuristic and the long-term convergence properties of the RL heuristic are not fully established.

## Next Checks

1. **Cross-validate oracle reliability**: Run ablation studies where oracles provide intentionally misleading guidance in specific state regions to test whether the heuristics can learn to avoid querying in those contexts.

2. **Test threshold sensitivity**: Systematically vary βent, βutil, and RL hyperparameters across multiple orders of magnitude to identify whether the reported performance is robust to parameter choices.

3. **Evaluate transfer learning**: Train heuristics on one use case (e.g., Cases) and test on another (e.g., Strategy) to assess whether the learned query policies generalize beyond their training distribution.