---
ver: rpa2
title: 'Rising Rested Bandits: Lower Bounds and Efficient Algorithms'
arxiv_id: '2411.14446'
source_url: https://arxiv.org/abs/2411.14446
tags:
- regret
- r-ed
- expected
- rested
- bandits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the stochastic Multi-Armed Bandit problem
  in the rising rested setting, where arm expected rewards are monotonically non-decreasing
  and concave. The authors derive regret lower bounds showing that non-decreasing
  and concavity assumptions are necessary but not sufficient for learnability.
---

# Rising Rested Bandits: Lower Bounds and Efficient Algorithms

## Quick Facts
- arXiv ID: 2411.14446
- Source URL: https://arxiv.org/abs/2411.14446
- Reference count: 40
- Key outcome: R-ed-UCB achieves O(T^{2/3}) regret in rising rested bandits by leveraging concavity of expected rewards

## Executive Summary
This paper addresses the stochastic Multi-Armed Bandit problem in the rising rested setting, where arm expected rewards are monotonically non-decreasing and concave. The authors derive regret lower bounds showing that non-decreasing and concavity assumptions are necessary but not sufficient for learnability. They propose R-ed-UCB, an optimistic algorithm that uses a carefully designed estimator for expected rewards. R-ed-UCB achieves a regret bound of O(T^{2/3}) under certain conditions and depends on the cumulative increment function Υ_μ. The algorithm outperforms state-of-the-art methods for non-stationary bandits in both synthetic and real-world experiments, including an online model selection task on the IMDB dataset. The work provides the first no-regret algorithm for stochastic rising bandits in the rested setting.

## Method Summary
The paper introduces R-ed-UCB, an optimistic algorithm for rising rested bandits that constructs an estimator using the most recent expected reward and projects future increments based on concavity. For the deterministic case, it uses exact expected rewards with an optimistic estimator, while the stochastic case employs a sliding window estimator to balance bias and variance. The algorithm achieves O(T^{2/3}) regret bounds that depend on the cumulative increment function Υ_μ, with an additional T^{2/3} term in the stochastic setting due to noise. The method is evaluated against state-of-the-art non-stationary bandit algorithms on synthetic data and a real-world IMDB dataset for online model selection.

## Key Results
- R-ed-UCB achieves O(T^{2/3}) regret in deterministic rising bandits
- The algorithm outperforms SW-TS, SW-UCB, and other baselines on synthetic and IMDB datasets
- Linear regret lower bounds are proven for deterministic rising bandits under certain conditions
- The regret bounds depend on the cumulative increment function Υ_μ(T, q)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The regret lower bound for deterministic rising bandits depends on the parameter β controlling the indistinguishability horizon of two bandit instances.
- Mechanism: The proof constructs two bandit instances that are identical up to tT^βu pulls, forcing any policy to pull both arms a number of times proportional to T^β to distinguish them. This creates a linear regret lower bound in T scaled by β.
- Core assumption: The two instances are indistinguishable until tT^βu, and the policy cannot statistically distinguish them before this point.
- Evidence anchors:
  - [abstract] The paper derives regret lower bounds showing that non-decreasing and concavity assumptions are necessary but not sufficient for learnability.
  - [section 4.2] Theorem 4.3 provides a linear regret lower bound of Ω(T) for deterministic rising bandits.
  - [corpus] The related work on rising bandits (e.g., "Rising Rested MAB with Linear Drift") supports the importance of the β parameter in characterizing problem complexity.
- Break condition: If the policy can identify the optimal arm before tT^βu, the lower bound fails. Also, if the concavity assumption is violated, the bound may not hold.

### Mechanism 2
- Claim: R-ed-UCB achieves O(T^{2/3}) regret under certain conditions by using an optimistic estimator that leverages the concavity of expected rewards.
- Mechanism: The algorithm constructs an optimistic estimator μ^R-ed_i(t) that uses the most recent expected reward and projects the most recent increment forward. This exploits concavity to upper bound future rewards.
- Core assumption: The expected rewards are concave and non-decreasing (Assumptions 3.1 and 3.2).
- Evidence anchors:
  - [abstract] R-ed-UCB achieves a regret bound of O(T^{2/3}) under certain conditions and depends on the cumulative increment function Υ_μ.
  - [section 5.1] Theorem 5.1 provides the regret bound for the deterministic case using the estimator μ^R-ed_i(t).
  - [corpus] The related work on Thompson Sampling-like algorithms for rising bandits supports the use of optimistic estimators in this setting.
- Break condition: If the expected rewards are not concave, the estimator loses its optimism and the regret bound breaks. Also, if the noise level σ is too high, the stochastic bound may dominate.

### Mechanism 3
- Claim: The stochastic regret bound includes an additional T^{2/3} term due to noise, creating a bias-variance tradeoff in the choice of window size.
- Mechanism: The algorithm uses a sliding window estimator pμ^R-ed,h_i(t) that averages recent samples. The window size h is chosen to balance bias (few samples) and variance (many samples), with the regret bound showing O(T^{2/3}) from the noise term.
- Core assumption: The rewards are σ²-subgaussian, and the window size is chosen appropriately (e.g., h_i,t = ⌊εN_i,t-1⌋ for ε ∈ (0, 1/2)).
- Evidence anchors:
  - [abstract] The regret bound depends on the cumulative increment function Υ_μ and includes an additional T^{2/3} term due to noise.
  - [section 5.2] Theorem 5.2 provides the regret bound for the stochastic case, showing the T^{2/3} term and the tradeoff with ε.
  - [corpus] The related work on heavy-tailed linear bandits supports the importance of handling noise in bandit algorithms.
- Break condition: If the noise level σ is too high, the T^{2/3} term dominates, making the algorithm inefficient. Also, if the window size is not chosen properly, the bias-variance tradeoff may not be optimal.

## Foundational Learning

- Concept: Regret decomposition
  - Why needed here: To relate the regret in the rising bandit setting to a standard bandit problem with arms having expected rewards μ_i(T), simplifying the lower bound construction.
  - Quick check question: Can you derive the regret decomposition in Lemma 4.1 using the definitions of optimal arm and average expected reward?

- Concept: Concavity of expected rewards
  - Why needed here: To construct an optimistic estimator that upper bounds future rewards, enabling efficient learning in the rising bandit setting.
  - Quick check question: Why does concavity allow us to bound the sum of future increments by projecting the most recent increment?

- Concept: Cumulative increment function Υ_μ
  - Why needed here: To characterize the instance-dependent complexity of the rising bandit problem, appearing in the regret bounds of both deterministic and stochastic cases.
  - Quick check question: How does Υ_μ(T, q) change when the increments γ_i(n) decrease at different rates (e.g., polynomial vs exponential)?

## Architecture Onboarding

- Component map:
  - R-ed-UCB algorithm -> Optimistic estimator μ^R-ed_i(t) -> Arm selection and reward observation -> Estimator update
  - Stochastic case: Sliding window estimator pμ^R-ed,h_i(t) -> Confidence bounds β^R-ed,h_i(t, δ_t)

- Critical path:
  1. Initialize N_i = 0 for all arms.
  2. For each round t:
     a. Compute optimistic index B_i(t) using estimator and confidence bound.
     b. Select arm I_t = arg max_i B_i(t).
     c. Observe reward R_t ~ ν_I_t(N_I_t, t).
     d. Update N_I_t and estimator.

- Design tradeoffs:
  - Deterministic vs stochastic case: The deterministic case uses exact expected rewards, while the stochastic case uses a sliding window estimator, introducing a bias-variance tradeoff.
  - Window size h_i,t: Larger windows reduce variance but increase bias, with the regret bound showing O(T^{2/3}) from the noise term.

- Failure signatures:
  - Linear regret: Indicates the algorithm failed to identify the optimal arm, possibly due to insufficient exploration or violation of concavity assumption.
  - High variance in cumulative regret: Suggests the noise level σ is too high, making the stochastic bound dominant.

- First 3 experiments:
  1. Test R-ed-UCB on a simple deterministic rising bandit with known concavity to verify O(T^{2/3}) regret.
  2. Compare R-ed-UCB with SW-TS on a stochastic rising bandit with varying noise levels to study the impact of σ on performance.
  3. Vary the window size h_i,t in the stochastic case to find the optimal tradeoff between bias and variance for a given problem instance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact dependence of the regret bound on the cumulative increment function Υμ(·, q) for different values of q?
- Basis in paper: [explicit] The paper provides regret bounds that depend on Υμ(·, q) but does not specify the optimal value of q for different function profiles.
- Why unresolved: The optimal choice of q depends on the specific characteristics of the expected reward functions, which are not known a priori.
- What evidence would resolve it: Empirical studies showing the performance of R-ed-UCB with different values of q for various types of expected reward functions.

### Open Question 2
- Question: How does the choice of the window size h in the stochastic setting affect the regret bound?
- Basis in paper: [explicit] The paper mentions a trade-off in choosing the window size h but does not provide specific guidelines for its selection.
- Why unresolved: The optimal window size depends on the trade-off between bias and variance, which is problem-dependent.
- What evidence would resolve it: Empirical studies showing the performance of R-ed-UCB with different window sizes for various types of expected reward functions and noise levels.

### Open Question 3
- Question: What is the impact of the non-stationarity of the expected reward functions on the regret bounds?
- Basis in paper: [inferred] The paper focuses on rising bandits, but the regret bounds may be affected by the non-stationarity of the expected reward functions.
- Why unresolved: The paper does not provide regret bounds that explicitly account for the non-stationarity of the expected reward functions.
- What evidence would resolve it: Extensions of the regret bounds to account for different types of non-stationarity in the expected reward functions.

## Limitations
- The paper doesn't provide clear guidelines for selecting the window size parameter ε in the stochastic case
- The impact of noise levels on the regret bounds and algorithm performance is not fully characterized
- The regret bounds depend on problem-specific parameters (Υ_μ) that may be difficult to estimate in practice

## Confidence
- Theoretical regret bounds (High): The proofs for both deterministic (O(T^{2/3})) and stochastic (O(T^{2/3}) + T^{2/3}) regret bounds appear rigorous and well-structured
- Algorithmic design (Medium): The estimator construction is sound, but the practical impact of the window size parameter is not fully characterized
- Experimental results (Medium): Strong empirical performance shown, but limited exploration of parameter sensitivity and problem variations

## Next Checks
1. **Parameter Sensitivity Analysis**: Systematically vary the window size parameter ε in the stochastic case across a range of problem instances to characterize the optimal bias-variance tradeoff and identify conditions where the T^{2/3} noise term dominates.

2. **Scaling Behavior Verification**: Test R-ed-UCB on synthetic rising bandit instances with different growth rates for the cumulative increment function Υ_μ (e.g., polynomial vs exponential) to verify the instance-dependent nature of the regret bounds and identify problem characteristics that affect performance.

3. **Real-World Robustness Check**: Extend the IMDB experiment to include varying noise levels and different reward structures to test the algorithm's robustness to violations of the concavity assumption and assess whether the theoretical guarantees hold in practice.