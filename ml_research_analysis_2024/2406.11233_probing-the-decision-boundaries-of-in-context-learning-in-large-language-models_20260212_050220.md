---
ver: rpa2
title: Probing the Decision Boundaries of In-context Learning in Large Language Models
arxiv_id: '2406.11233'
source_url: https://arxiv.org/abs/2406.11233
tags:
- feature
- examples
- in-context
- decision
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the decision boundaries of large language
  models (LLMs) during in-context learning by visualizing their behavior in binary
  classification tasks. The authors find that state-of-the-art LLMs produce irregular,
  non-smooth decision boundaries even in simple linearly separable tasks, unlike classical
  machine learning models.
---

# Probing the Decision Boundaries of In-context Learning in Large Language Models

## Quick Facts
- arXiv ID: 2406.11233
- Source URL: https://arxiv.org/abs/2406.11233
- Authors: Siyan Zhao; Tung Nguyen; Aditya Grover
- Reference count: 40
- Key outcome: LLMs produce irregular, non-smooth decision boundaries even in simple linearly separable binary classification tasks

## Executive Summary
This paper investigates the decision boundaries of large language models (LLMs) during in-context learning by visualizing their behavior in binary classification tasks. The authors find that state-of-the-art LLMs produce irregular, non-smooth decision boundaries even in simple linearly separable tasks, unlike classical machine learning models. Through extensive experiments, they examine how factors like model size, quantization levels, label semantics, and prompt ordering affect boundary smoothness. They demonstrate that fine-tuning earlier layers and using uncertainty-aware active learning can improve decision boundary smoothness, while fine-tuning only on in-context examples does not. The findings provide new insights into in-context learning mechanics and suggest methods to enhance robustness and generalization.

## Method Summary
The authors generate binary classification datasets using scikit-learn (linear, circle, moon tasks) and NLP datasets for multi-class experiments. They implement an in-context classification framework that prompts LLMs with in-context examples and grid points, obtaining predictions for each point. Decision boundaries are visualized by plotting predicted labels over the grid and comparing LLM boundaries with classical ML models (SVM, MLP, Decision Tree, K-NN). The study experiments with factors affecting decision boundaries including model size, quantization levels, label semantics, and example ordering, while also exploring methods to improve smoothness through fine-tuning (attention layers, embedding layers) and uncertainty-aware active learning.

## Key Results
- LLMs exhibit irregular, non-smooth decision boundaries in binary classification tasks even when the underlying task is linearly separable
- Model size alone does not guarantee smoother decision boundaries
- Fine-tuning only on in-context examples does not produce smoother decision boundaries
- Fine-tuning earlier layers and using uncertainty-aware active learning can improve decision boundary smoothness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit non-smooth decision boundaries in binary classification tasks even when the underlying task is linearly separable.
- Mechanism: The decision boundary smoothness is influenced by how LLMs process in-context examples. The model's token-based prediction system may not capture smooth functional relationships in the feature space, leading to fragmented decision regions.
- Core assumption: The LLM's prediction mechanism (choosing the most likely class token) can be interpreted as a decision boundary in the input feature space.
- Evidence anchors:
  - [abstract] "To our surprise, we find that the decision boundaries learned by current LLMs in simple binary classification tasks are often irregular and non-smooth, regardless of linear separability in the underlying task."
  - [section] "Even on simple linearly separable classification problems, all of these models exhibit non-smooth decision boundaries."
  - [corpus] Weak - related papers focus on generative classification and prompting, not decision boundary smoothness.

### Mechanism 2
- Claim: Model size alone does not guarantee smoother decision boundaries.
- Mechanism: Increasing model size may improve test accuracy but does not necessarily improve the smoothness of the decision boundary. This suggests that larger models may be better at fitting the data but not at learning smooth functional relationships.
- Core assumption: Model size is independent of decision boundary smoothness.
- Evidence anchors:
  - [section] "From Figure 2, model sizes increase from left to right, yet there is no clear correlation between model size and the smoothness of the decision boundary."
  - [section] "Even the most powerful model, GPT-4o, demonstrates fragmented decision regions."
  - [corpus] Weak - related papers discuss model scaling and in-context learning but not decision boundary smoothness.

### Mechanism 3
- Claim: Fine-tuning only on in-context examples does not produce smoother decision boundaries.
- Mechanism: When LLMs are fine-tuned on the same in-context examples used for testing, they do not learn to generalize the decision boundary smoothly. This suggests that the fine-tuning process on limited examples does not capture the underlying smooth functional relationship.
- Core assumption: Fine-tuning on limited in-context examples does not provide enough information to learn smooth decision boundaries.
- Evidence anchors:
  - [section] "Our experiments indicate that finetuning LLMs on in-context examples does not result in smoother decision boundaries."
  - [section] "Specifically, we finetuned Llama3-8B on 128 in-context learning examples and found that the resulting decision boundaries remained non-smooth."
  - [corpus] Weak - related papers focus on fine-tuning for in-context learning but not specifically on decision boundary smoothness.

## Foundational Learning

- Concept: In-context learning
  - Why needed here: Understanding how LLMs learn from examples without parameter updates is crucial to interpreting their decision boundaries.
  - Quick check question: How does in-context learning differ from traditional fine-tuning in terms of parameter updates?

- Concept: Decision boundaries in machine learning
  - Why needed here: Decision boundaries are the primary tool used to visualize and analyze the LLM's behavior in classification tasks.
  - Quick check question: What does a smooth decision boundary indicate about a model's generalization ability?

- Concept: Transformer architecture
  - Why needed here: Understanding the LLM's architecture helps explain why it might produce non-smooth decision boundaries.
  - Quick check question: How does the self-attention mechanism in transformers contribute to the model's ability to learn from in-context examples?

## Architecture Onboarding

- Component map: Input features -> Text conversion -> Prompt construction -> LLM processing -> Token prediction -> Decision boundary visualization
- Critical path:
  1. Generate classification dataset
  2. Construct in-context prompt with examples
  3. Query LLM on grid points
  4. Visualize decision boundary from predictions
- Design tradeoffs:
  - Grid resolution vs. computational cost
  - Number of in-context examples vs. decision boundary smoothness
  - Model size vs. accuracy and boundary smoothness
  - Quantization level vs. prediction consistency
- Failure signatures:
  - Highly fragmented decision regions
  - Sensitivity to prompt format and example ordering
  - Inconsistent predictions near decision boundaries
  - Lack of improvement with increased model size
- First 3 experiments:
  1. Visualize decision boundaries for different LLM sizes on a simple linear classification task
  2. Test sensitivity to in-context example ordering and label semantics
  3. Compare decision boundaries before and after fine-tuning on classification datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific quantization levels beyond 4-bit and 8-bit affect the smoothness of decision boundaries in LLMs?
- Basis in paper: [explicit] The paper discusses the impact of 4-bit and 8-bit quantization on decision boundaries but does not explore other levels.
- Why unresolved: The study only examines two quantization levels, leaving a gap in understanding how other levels might influence boundary smoothness.
- What evidence would resolve it: Experiments testing various quantization levels (e.g., 2-bit, 6-bit) and their effects on decision boundary smoothness.

### Open Question 2
- Question: What is the impact of fine-tuning only the embedding layers on decision boundary smoothness compared to fine-tuning the entire model?
- Basis in paper: [explicit] The paper mentions fine-tuning embedding layers but does not compare it directly to fine-tuning the entire model.
- Why unresolved: The study focuses on specific fine-tuning strategies but does not provide a comprehensive comparison with full-model fine-tuning.
- What evidence would resolve it: A controlled experiment comparing decision boundary smoothness when fine-tuning only embeddings versus the entire model.

### Open Question 3
- Question: How do different types of noise (e.g., Gaussian, adversarial) affect the decision boundaries of LLMs in in-context learning?
- Basis in paper: [inferred] The paper discusses the sensitivity of decision boundaries to quantization and label semantics, suggesting potential sensitivity to other perturbations.
- Why unresolved: The study does not explicitly test the robustness of decision boundaries to various noise types.
- What evidence would resolve it: Experiments introducing different noise types to the input data and analyzing their effects on decision boundary smoothness.

## Limitations
- The study focuses primarily on binary classification tasks, which may not generalize to more complex multi-class or regression scenarios
- The visualization approach relies on grid sampling, which may miss important boundary features at different resolutions
- The analysis is limited to specific LLM architectures and quantization levels, potentially missing broader patterns across the LLM landscape

## Confidence

High confidence: The finding that LLMs produce non-smooth decision boundaries compared to classical ML models is well-supported by the experimental evidence and visualization results.

Medium confidence: The claim that model size alone doesn't improve boundary smoothness is supported by the experiments shown, but could benefit from testing on a broader range of model families and tasks.

Medium confidence: The assertion that fine-tuning on in-context examples doesn't produce smoother boundaries is based on limited experiments and may not generalize to different fine-tuning approaches or larger datasets.

## Next Checks
1. **Cross-task generalization**: Replicate the decision boundary analysis on more complex multi-class classification tasks and real-world datasets beyond the synthetic binary tasks used in this study.

2. **Alternative fine-tuning methods**: Test whether different fine-tuning strategies (e.g., supervised fine-tuning on larger classification datasets, parameter-efficient fine-tuning methods) can produce smoother decision boundaries.

3. **Resolution sensitivity analysis**: Systematically vary the grid resolution used for boundary visualization to determine whether apparent boundary smoothness is an artifact of sampling density.