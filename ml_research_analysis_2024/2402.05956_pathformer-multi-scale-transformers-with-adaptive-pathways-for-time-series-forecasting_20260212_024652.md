---
ver: rpa2
title: 'Pathformer: Multi-scale Transformers with Adaptive Pathways for Time Series
  Forecasting'
arxiv_id: '2402.05956'
source_url: https://arxiv.org/abs/2402.05956
tags:
- uni00000013
- series
- time
- multi-scale
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Pathformer, a multi-scale Transformer with
  adaptive pathways for time series forecasting. The core idea is to integrate both
  temporal resolution and temporal distance for multi-scale modeling, enabling the
  model to capture diverse variations and fluctuations at different temporal scales.
---

# Pathformer: Multi-scale Transformers with Adaptive Pathways for Time Series Forecasting

## Quick Facts
- arXiv ID: 2402.05956
- Source URL: https://arxiv.org/abs/2402.05956
- Authors: Peng Chen; Yingying Zhang; Yunyao Cheng; Yang Shu; Yihang Wang; Qingsong Wen; Bin Yang; Chenjuan Guo
- Reference count: 40
- Key outcome: Pathformer achieves state-of-the-art performance on eleven real-world datasets with strong generalization under transfer scenarios.

## Executive Summary
Pathformer introduces a multi-scale Transformer architecture with adaptive pathways for time series forecasting. The model integrates temporal resolution and temporal distance modeling through patch division with multiple sizes and dual attention mechanisms. By combining global correlations and local details, Pathformer captures diverse temporal variations at different scales. The adaptive pathways dynamically adjust the multi-scale modeling process based on input characteristics, improving both accuracy and generalization. Experimental results demonstrate superior performance across multiple real-world datasets.

## Method Summary
Pathformer uses multi-scale patch division with varying patch sizes to create different temporal resolutions of the input time series. For each scale, dual attention (inter-patch for global correlations and intra-patch for local details) captures temporal dependencies. An adaptive pathway mechanism consisting of a multi-scale router and aggregator dynamically selects relevant scales for each input based on temporal decomposition (trend and seasonality extraction). The router generates pathway weights that determine which patch sizes and attention mechanisms to activate, while the aggregator combines selected scale outputs through weighted aggregation. This approach enables the model to adaptively capture multi-scale characteristics without being limited to fixed scales.

## Key Results
- Achieves state-of-the-art performance on eleven real-world datasets including ETT, Weather, Electricity, Traffic, ILI, and Cloud Cluster
- Demonstrates strong generalization abilities under various transfer scenarios
- Shows significant improvements over fixed multi-scale approaches by adaptively selecting relevant temporal scales

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-scale division combined with dual attention enables Pathformer to capture both global correlations and local details across different temporal scales.
- Mechanism: The model divides time series into patches of varying sizes (temporal resolution) and applies intra-patch attention for local details within each patch and inter-patch attention for global correlations across patches (temporal distance).
- Core assumption: Temporal dependencies exist at multiple scales and can be effectively captured through patch-based attention mechanisms.
- Evidence anchors:
  - [abstract] "Multi-scale division divides the time series into different temporal resolutions using patches of various sizes. Based on the division of each scale, dual attention is performed over these patches to capture global correlations and local details as temporal dependencies."
  - [section 3.1] "Based on each size of divided patches, dual attention encompassing inter-patch and intra-patch attention is proposed to capture temporal dependencies, with inter-patch attention capturing global correlations across patches and intra-patch attention capturing local details within individual patches."

### Mechanism 2
- Claim: Adaptive pathways enable the model to dynamically select the most relevant scales for each input time series, improving both accuracy and generalization.
- Mechanism: A multi-scale router with temporal decomposition (trend and seasonality extraction) generates pathway weights that determine which patch sizes and attention mechanisms to activate for each input. The aggregator then combines these selected scales through weighted aggregation.
- Core assumption: Different time series have different optimal scales for modeling, and these can be identified through temporal decomposition patterns.
- Evidence anchors:
  - [abstract] "Adaptive pathways are further introduced to adaptively adjust the multi-scale modeling process based on the varying temporal dynamics of the input, improving the accuracy and generalization of the model."
  - [section 3.2] "The multi-scale router selects specific sizes of patch division based on the input data, which activates specific parts in the Transformer and controls the extraction of multi-scale characteristics."

### Mechanism 3
- Claim: The combination of temporal resolution and temporal distance modeling provides a more complete multi-scale understanding than methods using only one perspective.
- Mechanism: Multi-scale division provides different views of temporal resolution (fine-grained vs coarse-grained patches), while dual attention explicitly models temporal dependencies at different distances (local vs global), creating a comprehensive multi-scale modeling framework.
- Core assumption: Effective multi-scale modeling requires both different temporal resolutions and explicit modeling of dependencies at different temporal distances.
- Evidence anchors:
  - [abstract] "It integrates both temporal resolution and temporal distance for multi-scale modeling."
  - [section 2] "Viewing the data from different temporal resolutions implicitly influences the scale of the subsequent modeling process... On the contrary, considering different temporal distances enables modeling dependencies from different ranges, such as global and local correlations."

## Foundational Learning

- Concept: Temporal decomposition (trend and seasonality extraction)
  - Why needed here: The router uses trend and seasonality decomposition to understand the temporal dynamics of each input, which guides the selection of appropriate scales.
  - Quick check question: What are the two main components of temporal decomposition used in the multi-scale router?

- Concept: Patch-based attention mechanisms
  - Why needed here: Pathformer extends patching from CV/NLP to time series by using patches of different sizes and applying dual attention to capture both local and global dependencies.
  - Quick check question: How does intra-patch attention differ from inter-patch attention in Pathformer's architecture?

- Concept: Weighted aggregation of multi-scale features
  - Why needed here: After the router selects relevant scales, the aggregator combines these multi-scale outputs through weighted aggregation to produce the final representation.
  - Quick check question: Why is weighted aggregation preferred over simple averaging when combining outputs from different patch sizes?

## Architecture Onboarding

- Component map: Instance Norm → Adaptive Multi-Scale Blocks (AMS Blocks) → Predictor. Each AMS Block contains: Multi-scale Transformer Block (patch division + dual attention) + Adaptive Pathways (router + aggregator).
- Critical path: Input → Patch division → Dual attention → Router → Aggregator → Output. The router and aggregator form the adaptive pathways that distinguish Pathformer from fixed multi-scale models.
- Design tradeoffs: Fixed multi-scale models (like Scaleformer) are simpler but less adaptive; Pathformer adds complexity through the router and aggregator but gains adaptability and potentially better performance on diverse time series.
- Failure signatures: If the router consistently selects only a few patch sizes, the model loses multi-scale benefits; if dual attention fails to capture relevant patterns, accuracy degrades despite multi-scale architecture.
- First 3 experiments:
  1. Test dual attention on fixed patch sizes to verify local vs global dependency capture
  2. Evaluate router performance by visualizing selected patch sizes across different time series
  3. Compare fixed vs adaptive multi-scale modeling on datasets with known multi-scale characteristics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different patch sizes in the multi-scale division affect the performance of Pathformer on various types of time series data (e.g., seasonal vs. trend-dominated)?
- Basis in paper: [explicit] The paper discusses multi-scale division with patches of various sizes and dual attention, but does not detail the impact of different patch sizes on different types of time series data.
- Why unresolved: The paper does not provide a detailed analysis of how different patch sizes influence the model's performance across diverse time series characteristics.
- What evidence would resolve it: Comparative experiments showing Pathformer's performance with varying patch sizes on datasets with distinct temporal patterns (e.g., seasonal, trend-dominated) would clarify this.

### Open Question 2
- Question: Can the adaptive pathways mechanism be extended to dynamically adjust the number of AMS blocks or the complexity of the model based on the input data's characteristics?
- Basis in paper: [inferred] The paper introduces adaptive pathways for selecting patch sizes but does not explore extending this adaptability to the model's depth or complexity.
- Why unresolved: The focus is on patch size selection, leaving the potential for adaptive model depth unexplored.
- What evidence would resolve it: Experiments demonstrating Pathformer's performance with adaptive model depth based on input data characteristics would provide insights into this extension.

### Open Question 3
- Question: How does the inclusion of noise terms in the routing function impact the stability and reliability of the adaptive pathways in Pathformer?
- Basis in paper: [explicit] The paper mentions the use of noise terms in the routing function to introduce randomness and avoid consistently selecting a few patch sizes.
- Why unresolved: The paper does not analyze the effect of noise terms on the stability and reliability of the adaptive pathways.
- What evidence would resolve it: A study comparing Pathformer's performance with and without noise terms in the routing function would reveal their impact on stability and reliability.

### Open Question 4
- Question: What are the computational trade-offs when using adaptive pathways compared to fixed multi-scale modeling approaches?
- Basis in paper: [inferred] The paper highlights the benefits of adaptive pathways but does not discuss the computational costs involved.
- Why unresolved: The efficiency and computational trade-offs of adaptive pathways are not addressed in the paper.
- What evidence would resolve it: A comparative analysis of computational resources required by adaptive pathways versus fixed multi-scale approaches would clarify the trade-offs.

## Limitations
- The paper lacks detailed specifications for the time series decomposition module in the multi-scale router, making it difficult to assess how effectively the adaptive pathways identify relevant scales.
- Specific patch sizes used for different datasets are not disclosed, which is critical for reproducing the multi-scale modeling performance.
- Claims about superior performance under various transfer scenarios lack supporting analysis showing which specific time series characteristics benefit from adaptive pathways versus fixed multi-scale approaches.

## Confidence
**High Confidence**: The core architectural design combining multi-scale patch division with dual attention is clearly specified and theoretically sound for capturing both local and global temporal dependencies.

**Medium Confidence**: The adaptive pathway mechanism is well-motivated, but the lack of detail about the router's implementation and the decomposition module creates uncertainty about practical effectiveness and reproducibility.

**Low Confidence**: Claims about superior performance under various transfer scenarios lack supporting analysis showing which specific time series characteristics benefit from adaptive pathways versus fixed multi-scale approaches.

## Next Checks
1. Implement the multi-scale router with different temporal decomposition methods (STL, wavelet, etc.) to determine which decomposition approach most effectively guides patch size selection across diverse time series.

2. Conduct ablation studies comparing fixed multi-scale models with adaptive pathways disabled versus the full Pathformer, specifically measuring the benefit of adaptation across time series with varying multi-scale characteristics.

3. Perform visualization analysis of router-selected patch sizes across the eleven datasets to identify patterns in which time series types consistently select which scales, validating the claim that the router adapts to temporal dynamics.