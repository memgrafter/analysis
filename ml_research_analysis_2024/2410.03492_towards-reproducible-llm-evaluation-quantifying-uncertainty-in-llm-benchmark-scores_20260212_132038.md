---
ver: rpa2
title: 'Towards Reproducible LLM Evaluation: Quantifying Uncertainty in LLM Benchmark
  Scores'
arxiv_id: '2410.03492'
source_url: https://arxiv.org/abs/2410.03492
tags:
- large
- benchmark
- temperature
- mean
- seed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of quantifying uncertainty in
  LLM benchmark scores due to the stochastic nature of LLMs. The authors propose a
  method using prediction intervals to measure uncertainty and recommend cost-effective
  sampling strategies for reproducible evaluation.
---

# Towards Reproducible LLM Evaluation: Quantifying Uncertainty in LLM Benchmark Scores

## Quick Facts
- **arXiv ID**: 2410.03492
- **Source URL**: https://arxiv.org/abs/2410.03492
- **Reference count**: 29
- **Key outcome**: Setting temperature to 0 with a fixed seed significantly reduces variability in LLM benchmark scores, and typically only 2-3 experimental repeats are needed to achieve prediction interval width below 0.01.

## Executive Summary
This paper addresses the challenge of quantifying uncertainty in LLM benchmark scores due to the stochastic nature of LLMs. The authors propose using prediction intervals to measure uncertainty and recommend cost-effective sampling strategies for reproducible evaluation. Using spatial reasoning benchmarks, they demonstrate that setting temperature to 0 with a fixed seed significantly reduces variability, and typically only 2-3 experimental repeats are needed to achieve acceptable prediction interval widths. The study also reveals statistically significant differences in scores when using the same model via different APIs, emphasizing the importance of precise documentation of experimental conditions.

## Method Summary
The authors conducted experiments using two spatial reasoning benchmarks (Small: 100 questions, Large: 5760 templated questions) across six LLMs via different APIs. They varied the number of experimental repeats from 1-30 and compared default settings against temperature=0 with fixed seed=123. For each configuration, they computed mean benchmark scores and prediction intervals. Two-sample t-tests were used to compare scores across different APIs for the same model. The primary metric was prediction interval width, with a target width of ≤0.01.

## Key Results
- Setting temperature to 0 with a fixed seed significantly reduces variability in benchmark scores
- Typically only 2-3 experimental repeats are needed to achieve prediction interval width below 0.01
- Statistically significant differences in scores observed when using the same model via different APIs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Setting temperature to 0 with a fixed seed reduces variability in LLM benchmark scores.
- **Mechanism:** LLMs generate outputs probabilistically. Temperature controls the randomness of sampling from the probability distribution over tokens. At temperature=0, the model picks the most likely next token deterministically. Fixing the random seed ensures reproducibility by starting from a consistent state in the random number generator.
- **Core assumption:** Temperature=0 plus fixed seed eliminates all sources of randomness in the model's inference process.
- **Evidence anchors:** Abstract states this significantly reduces variability; section confirms this as expected; related work on uncertainty quantification supports the mechanism.
- **Break condition:** The mechanism breaks if hardware or software introduces non-deterministic behavior (e.g., parallel execution, floating point arithmetic differences), or if the model's implementation ignores the seed parameter.

### Mechanism 2
- **Claim:** Using prediction intervals provides a principled way to quantify uncertainty in benchmark scores.
- **Mechanism:** Prediction intervals account for both sampling variability of the mean score and inherent variability of future observations. They provide a range where future mean scores are likely to fall, rather than just estimating where the true population parameter lies (confidence interval).
- **Core assumption:** The distribution of mean scores across repeats is approximately normal or t-distributed, justifying the use of t-based prediction intervals.
- **Evidence anchors:** Section explicitly uses prediction intervals where n′ = n; prediction intervals estimate the range where future observations or means will likely fall; related work on Bayesian evaluation suggests alternative uncertainty quantification.
- **Break condition:** The mechanism breaks if the underlying distribution of scores is highly non-normal or if the sample size is too small for the t-distribution approximation to hold.

### Mechanism 3
- **Claim:** Larger benchmarks with more questions reduce the variability of the mean score.
- **Mechanism:** The standard error of the mean decreases as the number of questions increases (standard error ∝ 1/√q). More questions provide more data points, averaging out individual answer variability and leading to a more stable mean score.
- **Core assumption:** Individual question scores are independent and identically distributed, so averaging reduces variance.
- **Evidence anchors:** Section states larger benchmarks reduce variability; all models tested showed smaller prediction interval width with fewer repeats for Large vs Small benchmark; basic statistical principles support this.
- **Break condition:** The mechanism breaks if questions are correlated or if there's systematic bias affecting many questions similarly.

## Foundational Learning

- **Concept:** Prediction intervals vs confidence intervals
  - **Why needed here:** The paper uses prediction intervals to quantify uncertainty in benchmark scores, which is different from confidence intervals. Understanding this distinction is crucial for interpreting the results correctly.
  - **Quick check question:** What's the key difference between a prediction interval and a confidence interval, and why is a prediction interval more appropriate for quantifying uncertainty in future benchmark scores?

- **Concept:** Stochastic vs deterministic systems
  - **Why needed here:** LLMs are inherently stochastic, meaning they can produce different outputs for the same input. Understanding what makes a system stochastic versus deterministic is essential for grasping why reproducibility is challenging and how techniques like temperature=0 and fixed seeds help.
  - **Quick check question:** What are the main sources of non-determinism in LLMs, and how do techniques like temperature=0 and fixed seeds address these sources?

- **Concept:** Statistical significance and t-tests
  - **Why needed here:** The paper uses two-sample t-tests to determine if differences in scores between different APIs are statistically significant. Understanding hypothesis testing and p-values is necessary to interpret these results.
  - **Quick check question:** What does a p-value of 0.013 mean in the context of comparing GPT-3.5T scores from Azure OpenAI API versus OpenAI API, and how does this relate to the concept of statistical significance?

## Architecture Onboarding

- **Component map:** Benchmarks (Small/Large) -> LLM APIs (OpenAI, Azure OpenAI, Vertex, Ollama) -> Control parameters (temperature, seed, repeats) -> Statistical analysis (mean, prediction intervals, t-tests)
- **Critical path:** The critical path for reproducibility is: select benchmark → configure LLM API with temperature=0 and fixed seed → run multiple repeats → calculate mean score and prediction interval → document all parameters and conditions.
- **Design tradeoffs:** The main tradeoff is between cost/time (fewer repeats) and uncertainty reduction (more repeats). The paper suggests that typically 2-3 repeats are sufficient when using temperature=0 and fixed seed, which significantly reduces cost while maintaining acceptable uncertainty levels.
- **Failure signatures:** Common failure modes include: (1) prediction interval width doesn't decrease with more repeats (suggesting non-normal distribution or correlated errors), (2) different APIs give statistically significantly different results for the same model (suggesting implementation differences), (3) temperature=0 and fixed seed don't reduce variability (suggesting other sources of randomness).
- **First 3 experiments:**
  1. Run the Small benchmark with default settings (no temperature/seed control) for n=10 repeats to establish baseline variability.
  2. Run the same benchmark with temperature=0 and seed=123 for n=10 repeats to measure reduction in variability.
  3. Run a t-test comparing mean scores from the two different APIs (Azure OpenAI vs OpenAI) for the same model to verify the claim about API differences.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the variability in LLM benchmark scores change across different benchmark types and complexities?
  - **Basis in paper:** The paper mentions that different benchmarks are designed to test different knowledge and reasoning tasks, and it is possible that some benchmarks may require more experimental repeats for the prediction interval width to reduce acceptably.
  - **Why unresolved:** The study only tested two qualitative spatial reasoning benchmarks and did not explore how different types or complexities of benchmarks might affect score variability.
  - **What evidence would resolve it:** Conducting experiments with a diverse range of benchmarks testing various knowledge and reasoning tasks, comparing the number of experimental repeats needed to achieve acceptable prediction interval widths across these benchmarks.

- **Open Question 2:** What is the impact of prompt engineering on the variability of LLM benchmark scores?
  - **Basis in paper:** The paper acknowledges that prompt engineering could potentially affect answer variability but did not explore this aspect in their experiments.
  - **Why unresolved:** The study focused on the impact of temperature and seed settings on score variability, leaving the potential effects of different prompt engineering techniques unexplored.
  - **What evidence would resolve it:** Designing experiments that test various prompt engineering techniques across different benchmarks and models, measuring their impact on the prediction interval width and mean scores.

- **Open Question 3:** How do different sampling strategies, such as nucleus (top_p) sampling, affect the reproducibility of LLM benchmark scores?
  - **Basis in paper:** The paper mentions that preliminary work by the authors suggests nucleus sampling can also be used to reduce the variability of benchmark scores, but this was not explored in the main study.
  - **Why unresolved:** The study primarily focused on temperature-based sampling and did not investigate the effects of other sampling strategies like nucleus sampling on score variability and reproducibility.
  - **What evidence would resolve it:** Conducting experiments comparing temperature-based sampling with nucleus sampling across various benchmarks and models, analyzing their effects on prediction interval widths and mean scores.

## Limitations
- The mechanism's universality across all LLM implementations remains uncertain due to potential non-deterministic behaviors from hardware-level optimizations or floating-point arithmetic differences.
- The generalizability of statistically significant API differences is limited by the small sample size of cross-API comparisons (only one model tested).
- The assumption of normality in score distributions, which underpins the t-distribution approximation, is not explicitly validated across all experimental conditions.

## Confidence
- **Confidence: Medium** - The core claim that temperature=0 with fixed seed reduces variability is supported by experimental results, but the mechanism's universality across all LLM implementations remains uncertain.
- **Confidence: Medium** - While the paper demonstrates statistically significant API differences for GPT-3.5T, the generalizability of this finding across all models and APIs is limited.
- **Confidence: High** - The statistical methodology (prediction intervals) is sound for quantifying uncertainty in benchmark scores, though the normality assumption is not explicitly validated.

## Next Checks
1. **Reproducibility across different benchmark types:** Validate whether the 2-3 repeat recommendation holds for non-spatial reasoning benchmarks (e.g., mathematical reasoning, code generation) that may have different output distributions and variability characteristics.

2. **Cross-platform consistency verification:** Systematically test whether the same LLM model produces statistically indistinguishable results when accessed through different API providers, controlling for model version, temperature, and seed parameters.

3. **Distribution validation study:** Analyze the actual distribution of benchmark scores (e.g., via Q-Q plots or normality tests) for each model-benchmark combination to verify the assumption underlying the prediction interval methodology.