---
ver: rpa2
title: 'Elephants Never Forget: Memorization and Learning of Tabular Data in Large
  Language Models'
arxiv_id: '2404.06209'
source_url: https://arxiv.org/abs/2404.06209
tags:
- data
- dataset
- datasets
- learning
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates training data contamination and memorization
  in large language models (LLMs) when applied to tabular datasets. The authors develop
  four memorization tests and demonstrate that GPT-3.5 and GPT-4 have memorized many
  popular tabular datasets verbatim, including Iris, Wine, and Titanic datasets.
---

# Elephants Never Forget: Memorization and Learning of Tabular Data in Large Language Models

## Quick Facts
- **arXiv ID:** 2404.06209
- **Source URL:** https://arxiv.org/abs/2404.06209
- **Reference count:** 40
- **Primary result:** GPT-3.5 and GPT-4 have memorized popular tabular datasets verbatim, leading to overfitting on memorized data and degraded performance on novel datasets

## Executive Summary
This paper investigates training data contamination and memorization in large language models when applied to tabular datasets. The authors develop four memorization tests and demonstrate that GPT-3.5 and GPT-4 have memorized many popular tabular datasets verbatim, including Iris, Wine, and Titanic datasets. They then compare few-shot learning performance on memorized versus novel datasets, finding that LLMs perform better on memorized data with accuracy drops of up to 6 percentage points when data is slightly perturbed. The study also reveals that LLMs have limited ability to act as statistical predictors, with performance degrading as problem dimensionality increases.

## Method Summary
The authors created four memorization tests to detect training data contamination: detecting verbatim copies of complete datasets, partial dataset matches, dataset subsets, and datasets with slightly different variable formats. They applied these tests to GPT-3.5 and GPT-4 across popular tabular datasets from the UCI repository. For learning capability evaluation, they tested few-shot learning performance on both memorized and novel datasets, including slightly perturbed versions of memorized data to assess overfitting. Statistical prediction capabilities were evaluated using regression tasks across varying dimensions, and random sampling abilities were tested by asking models to generate samples matching dataset summary statistics.

## Key Results
- GPT-3.5 and GPT-4 memorized popular tabular datasets verbatim, including Iris, Wine, and Titanic datasets
- LLMs perform better on memorized datasets, with accuracy drops of up to 6 percentage points when data is slightly perturbed
- On novel datasets, LLM performance depends heavily on feature names and variable formats, with significantly lower performance
- LLMs have limited statistical prediction capabilities that degrade with increasing problem dimensionality
- Models can generate random samples matching summary statistics without copying training data verbatim

## Why This Works (Mechanism)
The paper demonstrates that large language models have effectively memorized popular tabular datasets during training, which manifests as improved performance on these datasets compared to novel ones. This memorization occurs because tabular data from repositories like UCI has been widely used in training corpora, leading to verbatim or near-verbatim copying into model weights. The models leverage this memorized knowledge when encountering familiar dataset structures, feature names, and variable formats, resulting in overfitting behavior. However, this memorized knowledge doesn't translate to strong statistical prediction capabilities, as the models struggle with novel datasets and higher-dimensional problems.

## Foundational Learning

**Tabular Data Representation** - Understanding how tabular data is structured with rows, columns, and variable formats is essential for evaluating LLM performance on structured data tasks. Quick check: Can you identify the difference between categorical and numerical features in a dataset?

**Memorization vs. Generalization** - Distinguishing between model memorization of training data versus true learning of underlying patterns is crucial for understanding LLM behavior. Quick check: Can you explain why a model might perform well on training data but poorly on slightly perturbed versions?

**Few-shot Learning** - Understanding how LLMs learn from limited examples is key to evaluating their tabular data capabilities. Quick check: Can you describe how few-shot learning differs from traditional supervised learning approaches?

## Architecture Onboarding

**Component Map:** Memorization detection tests -> Memorization evaluation -> Few-shot learning assessment -> Statistical prediction testing -> Random sampling evaluation

**Critical Path:** Dataset contamination detection → Performance comparison (memorized vs. novel) → Dimensionality analysis → Statistical capability assessment

**Design Tradeoffs:** The study prioritizes detection of memorization over understanding its underlying mechanisms, and focuses on popular datasets rather than domain-specific ones, potentially limiting generalizability.

**Failure Signatures:** Overfitting on memorized data, performance degradation with perturbations, heavy dependence on feature names and formats, and declining performance with increasing dimensionality.

**First Experiments:**
1. Apply memorization tests to additional tabular datasets beyond UCI repository
2. Test same memorization patterns across multiple model sizes and architectures
3. Evaluate performance on domain-specific tabular datasets not commonly found in training data

## Open Questions the Paper Calls Out
None

## Limitations
- Relatively small number of datasets used in memorization tests, primarily focusing on popular UCI datasets
- Potential for test artifacts in the evaluation methodology
- Lack of analysis regarding how model size affects memorization patterns
- Doesn't explore whether different fine-tuning approaches could mitigate memorization effects

## Confidence
- **Memorization detection:** High confidence in core findings about memorization of popular tabular datasets
- **Statistical prediction capabilities:** Medium confidence due to limited scope of regression problems tested
- **Generalizability:** Medium confidence due to focus on specific dataset types and model architectures

## Next Checks
1. Replicate the memorization tests across a broader range of tabular datasets, including domain-specific and less commonly used datasets, to establish the generalizability of findings
2. Conduct ablation studies varying dataset size, feature dimensionality, and training set composition to better understand the factors driving memorization
3. Test the same memorization and learning capabilities across multiple model sizes and architectures to determine whether observed patterns are consistent across the LLM landscape