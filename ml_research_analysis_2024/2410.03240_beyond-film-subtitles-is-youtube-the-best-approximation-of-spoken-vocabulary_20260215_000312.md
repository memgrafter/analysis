---
ver: rpa2
title: 'Beyond Film Subtitles: Is YouTube the Best Approximation of Spoken Vocabulary?'
arxiv_id: '2410.03240'
source_url: https://arxiv.org/abs/2410.03240
tags:
- language
- corpus
- word
- english
- japanese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of approximating everyday spoken
  language exposure for languages with limited or low-quality subtitle and speech
  corpora. The authors propose using carefully processed YouTube subtitles to construct
  frequency norms for Chinese, English, Indonesian, Japanese, and Spanish, and evaluate
  their correlation with lexical decision time, word familiarity, and lexical complexity.
---

# Beyond Film Subtitles: Is YouTube the Best Approximation of Spoken Vocabulary?

## Quick Facts
- arXiv ID: 2410.03240
- Source URL: https://arxiv.org/abs/2410.03240
- Reference count: 36
- Primary result: YouTube-derived frequencies outperform existing subtitle and speech corpora in lexical complexity prediction tasks, achieving state-of-the-art results in English and Japanese.

## Executive Summary
This paper addresses the challenge of approximating everyday spoken language exposure for languages with limited or low-quality subtitle and speech corpora. The authors propose using carefully processed YouTube subtitles to construct frequency norms for Chinese, English, Indonesian, Japanese, and Spanish. By evaluating their correlation with lexical decision time, word familiarity, and lexical complexity, the study demonstrates that YouTube-derived frequencies provide a strong approximation of spoken vocabulary, often outperforming existing resources. The approach is particularly valuable for languages lacking high-quality subtitle or speech corpora.

## Method Summary
The method involves scraping YouTube videos, cleaning and processing subtitles, and constructing frequency lists using various tokenization and lemmatization techniques. The authors generate frequency norms from YouTube subtitles in five languages and evaluate their performance against psycholinguistic variables and lexical complexity prediction tasks. The process includes downloading subtitles with manual language labels, cleaning the data, applying language-specific tokenization, and generating frequency lists for evaluation.

## Key Results
- YouTube-derived frequencies achieve strong correlations with psycholinguistic variables including lexical decision time and word familiarity.
- TUBELEX frequencies outperform existing subtitle and speech corpora in lexical complexity prediction tasks.
- Simple linear regression on YouTube frequencies achieves state-of-the-art results in English and Japanese lexical complexity prediction.

## Why This Works (Mechanism)

### Mechanism 1
YouTube subtitle frequencies approximate spoken language exposure better than subtitle corpora based on translated film dialogue. YouTube videos contain more original, unscripted speech and less translated dialogue, providing a more authentic representation of everyday spoken vocabulary. This assumes a significant portion of YouTube content is not translated and reflects natural spoken language.

### Mechanism 2
YouTube-derived frequencies achieve strong correlations with psycholinguistic variables and outperform existing subtitle and speech corpora in lexical complexity prediction tasks. The frequency of words in YouTube subtitles correlates well with how familiar native speakers are with those words, making it a good predictor of lexical complexity. This assumes the distribution of word frequencies in YouTube subtitles reflects the actual frequency of word usage in everyday spoken language.

### Mechanism 3
YouTube subtitle corpora are available for languages for which a high-quality subtitle or speech corpus does not exist. YouTube provides a large and diverse source of video content in many languages, including those with limited linguistic resources. This assumes YouTube has a significant presence in the languages targeted by this research.

## Foundational Learning

- **Psycholinguistics**: Understanding how word frequency affects language processing and comprehension is crucial for evaluating the effectiveness of YouTube subtitle frequencies. *Quick check: What is the relationship between word frequency and lexical decision time in psycholinguistics?*

- **Corpus linguistics**: Knowledge of how to construct and process large language corpora is essential for understanding the methodology used to create the YouTube subtitle corpus. *Quick check: What are the key steps involved in cleaning and processing a large corpus of text data?*

- **Natural language processing (NLP)**: Familiarity with NLP techniques such as tokenization, lemmatization, and language modeling is necessary to understand how YouTube subtitle frequencies were derived and used. *Quick check: What is the difference between tokenization and lemmatization in NLP?*

## Architecture Onboarding

- **Component map**: Corpus construction pipeline (scraping → cleaning → processing subtitles) → frequency list generation → evaluation components (correlation with psycholinguistic variables → lexical complexity prediction)
- **Critical path**: Scraping YouTube subtitles → cleaning and processing them → generating frequency lists → evaluating frequencies against psycholinguistic data
- **Design tradeoffs**: The choice to use YouTube subtitles involves tradeoffs between the authenticity of spoken language and potential issues with synthesized speech or translated content
- **Failure signatures**: Potential failures include copyright issues, language identification errors, and poor correlation with psycholinguistic variables if the corpus is not representative of everyday spoken language
- **First 3 experiments**:
  1. Evaluate the correlation between YouTube-derived frequencies and lexical decision times in a new language not covered in the original study
  2. Compare the performance of different tokenization and lemmatization techniques on the YouTube subtitle corpus
  3. Assess the impact of corpus size on the quality of frequency estimates and their correlation with psycholinguistic variables

## Open Questions the Paper Calls Out

### Open Question 1
How do TUBELEX frequency norms perform on languages with limited YouTube presence or small speaker populations? The authors specifically mention they only evaluated five diverse languages, leaving open the generalizability to other languages with varying YouTube presence.

### Open Question 2
What is the impact of including synthesized speech and audio descriptions on the representativeness of TUBELEX as a spoken language corpus? The authors acknowledge that synthesized speech and audio descriptions are present in TUBELEX and discuss their potential impact on representativeness.

### Open Question 3
How do TUBELEX embeddings perform on tasks beyond word similarity and analogy, such as named entity recognition or sentiment analysis? The authors only evaluate TUBELEX embeddings on word similarity and analogy tasks, leaving their performance on other NLP tasks unknown.

## Limitations
- Generalizability across languages may vary significantly for languages with very different linguistic structures or those with less YouTube content
- Copyright and legal restrictions could affect the availability of YouTube subtitle corpora
- The effectiveness depends on the quality and representativeness of YouTube content, which may shift over time

## Confidence
- **Mechanism 1**: Medium confidence - The evidence shows YouTube frequencies correlate well with psycholinguistic variables, but the assumption about YouTube content being less translated is not directly validated
- **Mechanism 2**: High confidence - The paper provides strong statistical evidence for correlations with lexical decision time, word familiarity, and lexical complexity
- **Mechanism 3**: Medium confidence - While the paper targets languages with diverse resource availability, the long-term sustainability of YouTube as a resource is uncertain

## Next Checks
1. Test the YouTube-derived frequencies on a new language not covered in the original study to assess generalizability
2. Monitor the quality and representativeness of YouTube content over time to ensure continued validity
3. Conduct a thorough review of copyright and ethical considerations for using YouTube subtitles in linguistic research