---
ver: rpa2
title: Query-Efficient Planning with Language Models
arxiv_id: '2412.06162'
source_url: https://arxiv.org/abs/2412.06162
tags:
- block
- state
- goal
- planning
- default
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes and compares two frameworks for query-efficient\
  \ planning using LLMs: using an LLM as a heuristic within a classical planner versus\
  \ using it as a generative planner that proposes complete action sequences and adapts\
  \ based on world model feedback. Two new algorithms are introduced\u2014Tree of\
  \ Interaction (ToI) that uses LLMs as heuristics and Boomerang that uses LLMs as\
  \ generative planners\u2014and evaluated on PDDL planning domains and a robotics\
  \ simulator."
---

# Query-Efficient Planning with Language Models

## Quick Facts
- **arXiv ID**: 2412.06162
- **Source URL**: https://arxiv.org/abs/2412.06162
- **Reference count**: 32
- **Primary result**: Boomerang (LLM as generative planner) achieves 78% success on Blocksworld vs 63% for classical planners with fewer world model queries

## Executive Summary
This paper investigates query-efficient planning using Large Language Models (LLMs) as either heuristics within classical planners or as generative planners that propose complete action sequences. The authors introduce two new algorithms: Tree of Interaction (ToI) that uses LLMs as heuristics in BFS/DFS search, and Boomerang that uses LLMs as generative planners with world model feedback. Experiments on PDDL planning domains and a robotics simulator show that the generative planner approach (Boomerang) is more query-efficient, achieving higher success rates with fewer world model queries. The key insight is that generative planners can rapidly adapt their entire plan based on immediate feedback, while heuristic-based planners are constrained by the search algorithm's choices.

## Method Summary
The paper proposes two frameworks for query-efficient planning: using LLMs as heuristics within classical planners (ToI) versus using them as generative planners (Boomerang). ToI uses BFS or DFS search algorithms augmented with an LLM that proposes actions and evaluates states based on their potential to reach the goal. Boomerang generates complete action sequences from start to goal, queries the world model once for the entire sequence, and if the plan fails, it resets to the start state and incorporates the feedback into its next plan generation. Both methods are evaluated on PDDL planning domains (Blocksworld, Logistics, Grippers) and a robotics simulator (Robotouille) with a fixed world model query budget.

## Key Results
- Boomerang achieves 78% success rate on Blocksworld vs 63% for classical planners
- Boomerang requires fewer world model queries (average 3.3 vs 7.2 for ToI-BFS)
- Boomerang outperforms ToI and classical planners on all tested domains
- The generative planner approach shows particular advantage in domains with local minima where classical planners get stuck

## Why This Works (Mechanism)

### Mechanism 1
Using LLMs as generative planners (Boomerang) is more query-efficient than using them as heuristics (ToI) because generative planners can adapt entire plans based on world model feedback, while heuristic-based planners are constrained by the search algorithm's choices. Boomerang generates complete action sequences from start to goal, queries the world model once for the entire sequence, and if the plan fails, it resets to the start state and incorporates the feedback into its next plan generation. This allows it to completely change direction based on the feedback. ToI, on the other hand, uses the LLM to select promising states and actions within a classical search framework (BFS/DFS). The LLM's choices are limited to the states and actions offered by the planner, so it cannot easily change the overall search direction even if it receives feedback about a cul-de-sac.

### Mechanism 2
The query efficiency of Boomerang is connected to the concept of "lazy search" in classical planning, where edges are only evaluated when they belong to promising paths to the goal. Boomerang implicitly builds an internal world model based on the problem description and history of interactions. It generates plans based on this internal model, queries the world model to validate the plan, and updates its internal model with the feedback. This is similar to posterior sampling in reinforcement learning, where the agent samples a model from the posterior and acts optimally in that model. The Bayesian regret bound for posterior sampling implies that Boomerang will make a sub-linear number of queries before finding a feasible solution.

### Mechanism 3
ToI's lower query efficiency compared to Boomerang is partly due to the LLM's inability to effectively rank states independently of each other, leading to an inadmissible and inconsistent heuristic. ToI uses the LLM to evaluate states based on their potential to progress towards the goal (Impossible, Maybe, Certain). However, the LLM ranks states independently, which can lead to suboptimal choices. For example, the LLM might rank a state as "Maybe" even if it's actually on a suboptimal path, leading the planner to explore that state. Additionally, the heuristic is inadmissible (doesn't guarantee optimal paths) and inconsistent (might select visited nodes), further reducing query efficiency.

## Foundational Learning

- **Search algorithms (BFS, DFS, A*)**: ToI uses BFS and DFS as the underlying search algorithms, and understanding their properties (completeness, optimality, time/space complexity) is crucial for understanding ToI's behavior and limitations. *Quick check*: What is the main difference between BFS and DFS in terms of the order in which they explore nodes, and how does this affect their memory usage?

- **Heuristics in search**: ToI uses an LLM as a heuristic to guide the search, so understanding what heuristics are, how they work, and what properties they should have (admissibility, consistency) is important for understanding ToI's design and limitations. *Quick check*: What is the difference between an admissible and a consistent heuristic, and why are these properties important for search algorithms like A*?

- **Reinforcement learning and posterior sampling**: Boomerang's connection to posterior sampling provides a theoretical justification for its query efficiency, so understanding the basics of RL and posterior sampling is helpful for understanding this connection. *Quick check*: In the context of RL, what is the difference between exploration and exploitation, and how does posterior sampling help balance these two?

## Architecture Onboarding

- **Component map**: World Model -> LLM (ToI heuristic or Boomerang planner) -> Planner (ToI only) -> Feedback Mechanism
- **Critical path**: Boomerang: Start state -> LLM generates plan -> Query world model for entire plan -> If plan fails, incorporate feedback and generate new plan -> Repeat until goal reached or query budget exhausted. ToI-BFS: Start state -> LLM proposes actions -> Query world model for next states -> LLM evaluates states -> Keep best states and repeat -> If goal reached, backtrack path.
- **Design tradeoffs**: ToI pros - Leverages existing search algorithms, LLM only needs to rank states and propose actions. ToI cons - LLM's choices are limited by the planner, heuristic might be inadmissible/inconsistent, requires maintaining a search tree. Boomerang pros - Can adapt entire plan based on feedback, no need for external planner, simpler architecture. Boomerang cons - LLM needs to generate complete, coherent plans, might struggle with long horizons or complex tasks, requires resetting to start state after each failed plan.
- **Failure signatures**: Boomerang - LLM generates invalid or suboptimal plans, LLM fails to incorporate feedback effectively, query budget exhausted before finding a solution. ToI - LLM ranks states poorly, leading to exploration of suboptimal paths, heuristic is inadmissible/inconsistent, search gets stuck in local optima.
- **First 3 experiments**: 1) Implement a simple version of Boomerang on a small planning domain (e.g., a 3-block Blocksworld problem) and compare its query efficiency to a classical planner like A*. 2) Implement ToI-BFS and ToI-DFS on the same small planning domain and compare their query efficiency to Boomerang and the classical planner. 3) Analyze the LLM's state evaluations in ToI and check if they are admissible and consistent. If not, try to improve the LLM's evaluation by providing more context or using a different prompting strategy.

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The theoretical connection between Boomerang and posterior sampling remains a conjecture without formal proof
- The comparison with classical planners is limited to PDDL domains with known optimal solutions
- The computational cost of LLM inference versus classical planning algorithms is not quantified

## Confidence

**High confidence**: The empirical finding that Boomerang achieves higher success rates with fewer queries than ToI and classical planners on the tested domains. The experimental setup and results are clearly presented.

**Medium confidence**: The mechanism explanation that generative planners can adapt entire plans based on feedback while heuristic-based planners are constrained by search algorithm choices. This is supported by examples but could benefit from more systematic analysis.

**Low confidence**: The theoretical connection between Boomerang and posterior sampling, which remains a conjecture without formal proof or empirical validation beyond the success rates.

## Next Checks

1. **Implement formal analysis of Boomerang as posterior sampling**: Design experiments that test whether Boomerang's behavior matches posterior sampling properties - does it explore sub-optimal paths proportionally to their probability under the LLM's internal model? Compare its exploration-exploitation balance to known posterior sampling algorithms.

2. **Cost-benefit analysis of query vs computation efficiency**: Measure the total computational cost (LLM inference time + world model query time) for Boomerang vs classical planners. Does the reduction in world model queries translate to overall time savings when accounting for expensive LLM generations?

3. **Plan quality assessment across domains**: Extend evaluation to measure not just success rates but also plan optimality gaps. Compare the length and quality of LLM-generated plans against optimal plans from classical planners, and test on domains with known optimal solutions to quantify suboptimality.