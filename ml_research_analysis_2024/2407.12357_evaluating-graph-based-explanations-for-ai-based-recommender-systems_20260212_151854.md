---
ver: rpa2
title: Evaluating graph-based explanations for AI-based recommender systems
arxiv_id: '2407.12357'
source_url: https://arxiv.org/abs/2407.12357
tags:
- explanation
- design
- explanations
- participants
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates stakeholders' desiderata regarding graph-based
  explanations for AI-based recommender systems. A qualitative study involving 12
  participants reveals a strong link between context knowledge and recommendation
  understanding, with users preferring item-based designs over user-based ones.
---

# Evaluating graph-based explanations for AI-based recommender systems

## Quick Facts
- arXiv ID: 2407.12357
- Source URL: https://arxiv.org/abs/2407.12357
- Authors: Simon Delarue; Astrid Bertrand; Tiphaine Viard
- Reference count: 40
- Primary result: Graph-based explanations are perceived as more usable than SHAP-based designs but achieve lower objective understanding than textual explanations

## Executive Summary
This work investigates stakeholders' preferences and effectiveness of graph-based explanations for AI-based recommender systems. Through qualitative and quantitative studies involving 78 participants total, the research reveals that while users express preference for graph-based designs, textual explanations actually lead to higher objective understanding. The findings highlight a critical disconnect between user preferences and actual performance, suggesting that meeting expressed preferences may not guarantee effective explanations.

The study develops an enhanced graph-based explanation based on insights from qualitative research, then systematically compares it against textual and SHAP-based explanations across four key constructs. Results show that graph-based explanations are perceived as more usable than SHAP-based designs, but textual explanations outperform them in objective understanding metrics. This contrast between expressed preferences and actual ratings presents a significant challenge for designing effective explainable AI systems that balance user expectations with functional performance.

## Method Summary
The research employed a mixed-methods approach with two main studies. First, a qualitative study with 12 participants explored stakeholders' desiderata for graph-based explanations, revealing strong connections between context knowledge and recommendation understanding, with preferences for item-based over user-based designs. Based on these insights, an enhanced graph-based explanation was developed. Second, a quantitative study with 66 participants compared this enhanced graph-based design against textual and SHAP-based explanations across four constructs: objective understanding, subjective understanding, curiosity, and usability. The study used movie recommendations as the content domain for all experiments.

## Key Results
- Textual explanations lead to higher objective understanding than graph-based designs
- Graph-based explanations are perceived as more usable than SHAP-based designs
- Strong contrast exists between participants' expressed preferences for graph design and their actual lower ratings compared to textual design

## Why This Works (Mechanism)
The effectiveness of different explanation types depends on how well they align with users' cognitive processes for understanding recommendations. Textual explanations provide direct, linear information that maps well to how people naturally process sequential reasoning, leading to higher objective understanding. Graph-based explanations, while potentially offering richer contextual information, require additional cognitive effort to parse relationships and patterns, which may explain their lower objective understanding scores despite higher usability ratings. The preference-performance gap suggests that users may be drawn to visually appealing or familiar interface elements without necessarily achieving better comprehension outcomes.

## Foundational Learning

1. **Cognitive load theory** - Why needed: Understanding how different explanation formats affect mental processing effort when interpreting recommendations. Quick check: Measure task completion time and error rates across explanation types.

2. **Social desirability bias** - Why needed: Explains why users might express preferences for certain design elements that don't actually lead to better performance. Quick check: Compare stated preferences with actual behavioral metrics and performance data.

3. **Explanation satisfaction vs. effectiveness** - Why needed: Distinguishes between user satisfaction with explanation presentation and actual understanding achieved. Quick check: Correlate subjective satisfaction ratings with objective comprehension tests.

4. **Graph literacy** - Why needed: Determines users' ability to effectively interpret and extract meaning from graph-based visualizations. Quick check: Assess participants' prior experience with graph-based interfaces and correlate with performance.

5. **Multimodal explanation design** - Why needed: Understanding how to combine different explanation formats to leverage their respective strengths. Quick check: Test hybrid explanations that integrate textual and graph elements.

## Architecture Onboarding

**Component map:** User Interface -> Explanation Engine -> Recommendation Algorithm -> Data Sources

**Critical path:** User requests explanation -> System selects appropriate explanation format (textual/graph/SHAP) -> Explanation engine generates content based on recommendation algorithm output -> UI presents explanation to user

**Design tradeoffs:** Balance between visual appeal (graph-based) and comprehension effectiveness (textual), versus implementation complexity and computational overhead of generating different explanation types

**Failure signatures:** Users express preference for graph design but show poor comprehension; high usability ratings don't translate to understanding; certain user demographics struggle with graph-based explanations

**First experiments:** 1) A/B test textual vs graph explanations with objective comprehension assessments; 2) Measure time-on-task and cognitive load across explanation types; 3) Conduct think-aloud protocols while users interact with different explanation formats

## Open Questions the Paper Calls Out

The paper highlights several open questions, particularly around the generalizability of findings beyond movie recommendations to other domains like e-commerce or news. It questions whether the preference-performance gap persists across different user demographics and use contexts. The research also calls for exploration of hybrid explanation designs that might combine the usability benefits of graph-based explanations with the comprehension advantages of textual explanations.

## Limitations

- Limited sample sizes (12 for qualitative, 66 for quantitative) may not adequately represent broader user populations across different demographics and use contexts
- Study focuses specifically on movie recommendations, raising questions about generalizability to other recommendation domains
- Potential social desirability bias in expressed preferences that wasn't fully explored
- Specific metrics used for objective understanding could influence results

## Confidence

- Textual explanations leading to higher objective understanding: Medium confidence (could be influenced by specific metrics and content domain)
- Usability advantage of graph-based over SHAP-based explanations: High confidence (more directly measured through standardized usability scales)
- Recommendation for hybrid designs balancing social expectations with performance: Medium confidence (extrapolates from current findings without testing specific hybrid approaches)

## Next Checks

1. Replicate the study with larger, more diverse samples across multiple recommendation domains to test generalizability
2. Conduct A/B testing of specific hybrid explanation designs that combine textual and graph-based elements to validate the proposed approach
3. Implement longitudinal studies to assess whether initial preference-performance gaps persist over time as users become more familiar with different explanation types