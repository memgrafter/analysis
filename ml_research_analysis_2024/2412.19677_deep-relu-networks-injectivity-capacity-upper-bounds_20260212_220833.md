---
ver: rpa2
title: Deep ReLU networks -- injectivity capacity upper bounds
arxiv_id: '2412.19677'
source_url: https://arxiv.org/abs/2412.19677
tags:
- injectivity
- layer
- relu
- capacity
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies injectivity capacities of deep ReLU neural\
  \ networks, a critical property for inverse problems and compressed sensing. It\
  \ connects injectivity to \u2113\u2080 spherical perceptrons and uses Random Duality\
  \ Theory (RDT) to analyze deep networks."
---

# Deep ReLU networks -- injectivity capacity upper bounds

## Quick Facts
- arXiv ID: 2412.19677
- Source URL: https://arxiv.org/abs/2412.19677
- Reference count: 40
- Primary result: Analyzes injectivity capacities of deep ReLU networks using Random Duality Theory, showing rapid expansion saturation effect

## Executive Summary
This paper establishes theoretical upper bounds on the injectivity capacity of deep ReLU neural networks, a critical property for inverse problems and compressed sensing applications. The work connects network injectivity to ℓ₀ spherical perceptrons and leverages Random Duality Theory (RDT) to analyze networks with 1-4 layers. The results demonstrate a rapid saturation effect where deeper networks require minimal additional expansion beyond what is needed for shallower architectures.

## Method Summary
The paper develops a framework connecting deep l-layer network injectivity to l-extended ℓ₀ spherical perceptrons, enabling application of Random Duality Theory. The approach involves establishing the ℓ₀ spherical perceptron reduction, applying plain RDT to obtain upper bounds, and optionally using partially lifted RDT for tighter bounds. The method relies on concentration of measure arguments for random Gaussian matrices and requires solving complex optimization problems numerically. The framework systematically analyzes networks with increasing depth to demonstrate expansion saturation effects.

## Key Results
- Weak injectivity capacity upper bounds: 6.7004 (1-layer), 8.267 (2-layer), 9.49 (3-layer), 10.124 (4-layer)
- Layer expansions decrease rapidly: 6.7004 → 1.2338 → 1.1479 → 1.0668 from 1 to 4 layers
- Partially lifted RDT reduces 2-layer bounds from 8.267 to 8.264
- Strong injectivity shows similar bounds but larger values
- Only 4 layers needed to approach no-expansion regime

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The injectivity capacity of l-layer ReLU networks maps directly to the capacity of l-extended ℓ₀ spherical perceptrons.
- Mechanism: The paper establishes a reduction where determining if a deep ReLU network is injective is equivalent to solving a feasibility problem for an l-extended ℓ₀ spherical perceptron. This allows application of random duality theory (RDT) machinery to analyze deep networks.
- Core assumption: The ReLU network weights follow iid standard normal distributions, enabling concentration of measure arguments.
- Evidence anchors:
  - [abstract] "we develop a program that connects deep l-layer net injectivity to an l-extension of the ℓ0 spherical perceptrons"
  - [section 3] "The network is typically injective if frp(A1:3) > 0" where frp is derived from the ℓ₀ spherical perceptron formulation
  - [corpus] Weak evidence - only 1 neighbor paper directly discusses injectivity capacity of ReLU gates
- Break condition: If weight distributions deviate significantly from standard normal, the concentration of measure property may fail.

### Mechanism 2
- Claim: Partially lifted random duality theory (pl RDT) can further reduce injectivity capacity upper bounds.
- Mechanism: By partially lifting the optimization variables in the random dual formulation, the pl RDT variant introduces additional slack that tightens the upper bounds compared to plain RDT.
- Core assumption: The partially lifted formulation maintains sufficient concentration properties for the specific optimization structure.
- Evidence anchors:
  - [section 5.1] "We observe a lowering of the plain RDT upper bound" with specific numerical examples showing reduction from 8.267 to 8.264 for 2-layer weak injectivity
  - [section 5.2] "We did not find substantial improvements beyond the second layer" indicating diminishing returns
  - [corpus] No direct corpus evidence for pl RDT applications to injectivity
- Break condition: If the optimization landscape becomes too complex, the partially lifted formulation may not provide meaningful tightening.

### Mechanism 3
- Claim: Expansion saturation effect - deeper networks require minimal additional expansion.
- Mechanism: The injectivity capacity grows sublinearly with depth, with layer expansions decreasing rapidly (6.7004 → 1.2338 → 1.1479 → 1.0668 from 1 to 4 layers). This occurs because earlier layers already provide sufficient representational capacity.
- Core assumption: The statistical properties of random Gaussian matrices maintain injectivity relationships across layers.
- Evidence anchors:
  - [abstract] "rapid expansion saturation effect" and "Only 4 layers of depth are sufficient to closely approach level of no needed expansion"
  - [section 4.2] Table 7 showing systematic decrease in layer expansions as depth increases
  - [corpus] Weak evidence - only 1 neighbor paper discusses Lipschitz bounds which relate to injectivity but not expansion saturation
- Break condition: If network architecture deviates from standard feed-forward structure or activation functions differ significantly from ReLU.

## Foundational Learning

- Concept: Random Duality Theory (RDT) and its partially lifted variant
  - Why needed here: RDT provides the mathematical machinery to convert the injectivity feasibility problem into a tractable random optimization problem with concentration properties
  - Quick check question: What is the key theorem that enables the concentration of measure argument in RDT?

- Concept: ℓ₀ spherical perceptrons and their capacity
  - Why needed here: The injectivity capacity of ReLU networks is isomorphic to the capacity of ℓ₀ spherical perceptrons, allowing transfer of known results
  - Quick check question: How does the ℓ₀ constraint relate to the ReLU non-linearity in the perceptron formulation?

- Concept: Random feasibility problems (rfps) and their connection to injectivity
  - Why needed here: Injectivity is characterized by the infeasibility of certain recovery problems, which can be analyzed using rfps
  - Quick check question: What condition on the feasibility problem ensures that the network is typically injective?

## Architecture Onboarding

- Component map: Input layer (n nodes) → Hidden layers (m₁, m₂, ..., mₗ nodes) → Output layer
- Critical path:
  1. Define network architecture and expansion coefficients
  2. Establish ℓ₀ spherical perceptron equivalence
  3. Apply plain RDT to obtain upper bounds
  4. (Optional) Apply pl RDT for tighter bounds
  5. Analyze expansion saturation across layers

- Design tradeoffs:
  - Deeper networks provide better expansion saturation but increase computational complexity
  - pl RDT provides tighter bounds but requires significantly more numerical computation
  - Standard normal weights enable clean analysis but may not reflect practical weight distributions

- Failure signatures:
  - If capacity bounds don't decrease with depth, the reduction to ℓ₀ perceptrons may be incorrect
  - If pl RDT doesn't improve plain RDT bounds, the partial lifting may not be appropriate for the problem structure
  - If numerical evaluations are unstable, concentration of measure assumptions may be violated

- First 3 experiments:
  1. Verify the ℓ₀ spherical perceptron reduction for a 2-layer network with small dimensions (n=10, m₁=20, m₂=30)
  2. Implement plain RDT calculation for the same network and compare with theoretical predictions
  3. Test expansion saturation by computing capacity bounds for 1, 2, 3, and 4 layer networks with fixed input size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact injectivity capacity for 4-layer ReLU networks, and how does it compare to the upper bounds provided?
- Basis in paper: [explicit] The paper provides upper bounds for 1-4 layer networks (6.7004, 8.267, 9.49, 10.124) but acknowledges these are upper bounds, not exact values.
- Why unresolved: The paper uses Random Duality Theory (RDT) which provides upper bounds, and while Partially Lifted RDT can lower these bounds further, the exact capacity remains unknown due to the absence of strong random duality.
- What evidence would resolve it: A mathematical proof establishing the exact injectivity capacity for 4-layer ReLU networks, or a computational method that can precisely determine this value.

### Open Question 2
- Question: How do the injectivity capacities change for ReLU networks with more than 4 layers, and does the saturation effect continue?
- Basis in paper: [explicit] The paper observes a rapid saturation effect where the expansion needed per layer decreases quickly, reaching near 1 after 4 layers.
- Why unresolved: The paper only provides numerical results up to 4 layers, and while the trend suggests saturation, the behavior for deeper networks remains unexplored.
- What evidence would resolve it: Analytical or numerical results for 5+ layer ReLU networks showing whether the saturation effect continues or plateaus.

### Open Question 3
- Question: How does the injectivity capacity change under different activation functions (e.g., leaky ReLU, sigmoid) compared to standard ReLU?
- Basis in paper: [inferred] The paper focuses on ReLU networks but mentions that results could generalize to other activation functions, suggesting this is an unexplored area.
- Why unresolved: The paper's methodology is specific to ReLU, and extending it to other activation functions would require new theoretical frameworks and numerical evaluations.
- What evidence would resolve it: Analytical or numerical comparisons of injectivity capacities across different activation functions, showing how the capacity varies with function choice.

## Limitations

- The concentration of measure arguments rely heavily on standard normal weight distributions, which may not reflect practical initialization schemes used in deep learning applications.
- Partially lifted RDT provides only marginal improvements beyond 2-layer networks, suggesting potential limitations in this approach for very deep architectures.
- The numerical stability of RDT calculations becomes increasingly challenging for deeper networks, potentially affecting the accuracy of capacity bounds.

## Confidence

- High confidence: The basic reduction from injectivity to ℓ₀ spherical perceptrons - supported by explicit mathematical derivations and multiple cross-references within the paper
- Medium confidence: The expansion saturation effect - supported by systematic numerical experiments but dependent on the RDT framework accuracy
- Medium confidence: The partially lifted RDT improvements - supported by specific numerical examples but with limited exploration of deeper networks

## Next Checks

1. **Cross-validate the ℓ₀ spherical perceptron reduction**: Implement a small-scale verification using a 2-layer network with n=5, m₁=10, m₂=15 and compare the injectivity results with direct enumeration of all possible sign patterns.

2. **Test concentration of measure assumptions**: Run Monte Carlo simulations with different weight initialization schemes (Gaussian, uniform, Xavier) to verify if the standard normal assumption significantly impacts capacity bounds.

3. **Validate expansion saturation prediction**: Compute capacity bounds for networks with 5 and 6 layers to determine if the saturation effect continues or if additional depth provides meaningful improvements beyond the 4-layer analysis presented.