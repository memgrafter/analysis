---
ver: rpa2
title: The Benefit of Being Bayesian in Online Conformal Prediction
arxiv_id: '2410.02561'
source_url: https://arxiv.org/abs/2410.02561
tags:
- algorithm
- bayesian
- distribution
- confidence
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Bayesian approach to online conformal prediction
  that combines strengths of both direct (iid-based) and indirect (adversarial) methods.
  The key idea is to maintain a Bayesian belief by mixing a prior distribution with
  the empirical distribution of observed scores, which enables answering multiple
  confidence level queries online while avoiding validity issues suffered by first-order
  optimization baselines.
---

# The Benefit of Being Bayesian in Online Conformal Prediction

## Quick Facts
- arXiv ID: 2410.02561
- Source URL: https://arxiv.org/abs/2410.02561
- Authors: Zhiyu Zhang; Zhou Lu; Heng Yang
- Reference count: 40
- Primary result: Achieves optimal O(R√T) regret for online conformal prediction while supporting multiple confidence levels simultaneously

## Executive Summary
This paper introduces a Bayesian approach to online conformal prediction that addresses limitations of existing direct (iid-based) and indirect (adversarial) methods. By maintaining a Bayesian belief through mixing a prior distribution with the empirical distribution of observed scores, the algorithm achieves optimal O(R√T) regret without requiring knowledge of confidence levels beforehand. The method supports answering multiple confidence level queries online while avoiding validity issues that plague first-order optimization baselines.

## Method Summary
The method combines Bayesian inference with online learning concepts, specifically Follow the Regularized Leader (FTRL), to predict quantiles in an online conformal prediction setting. The algorithm maintains a belief distribution Pt by mixing a prior P0 with the empirical distribution of observed scores. For each confidence level α queried by downstream users, the algorithm predicts a threshold rt(α) that best responds to the current belief Pt. This approach achieves low regret while simultaneously guaranteeing valid coverage across all confidence levels α ∈ [0,1].

## Key Results
- Achieves optimal O(R√T) regret simultaneously for all confidence levels α ∈ [0,1]
- Matches coverage probability and excess quantile risk of ERM-based approaches under iid assumption
- Demonstrated on synthetic data and real stock price volatility prediction with competitive performance
- Supports multiple confidence level queries online without requiring knowledge of confidence levels beforehand

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bayesian regularization of the empirical distribution through prior mixing creates a non-linearized Follow the Regularized Leader (FTRL) algorithm.
- Mechanism: The algorithmic belief Pt is updated by mixing a Bayesian prior P0 with the empirical distribution of observed scores. This mixing induces downstream regularization on the predicted threshold rt(α), which best-responds to Pt.
- Core assumption: The quantile loss function is used as the loss function for the FTRL algorithm.
- Evidence anchors:
  - [abstract] "The key idea is to maintain a Bayesian belief by mixing a prior distribution with the empirical distribution of observed scores"
  - [section] "Theorem 1 shows that despite not knowing α at the beginning of the CP game, Algorithm 1 generates the same output rt(α) as a non-linearized Follow the Regularized Leader (FTRL) algorithm on the quantile loss lα"
- Break condition: If the prior P0 has zero density in certain regions, the regularization becomes ineffective and the regret bound degrades.

### Mechanism 2
- Claim: The Bayesian approach overcomes monotonicity issues in confidence set predictions that plague first-order optimization baselines.
- Mechanism: By construction, rt(α) is invariant to permutations of r1:t−1, and for any α1 < α2 we always have rt(α1) ≤ rt(α2). This ensures that higher confidence levels produce larger (not smaller) confidence sets.
- Core assumption: The quantile loss function is used as the loss function for the FTRL algorithm.
- Evidence anchors:
  - [abstract] "it does not suffer from the aforementioned validity issues, due to being 'data-centric' rather than 'iterate-centric'"
  - [section] "Consistent with the classical online learning theory, ERM becomes brittle when α matches the long run average of r1:T (i.e, 0.5), suffering linear regret with respect to T"
- Break condition: If the prior P0 is chosen poorly (e.g., has very low density in regions where true scores concentrate), the algorithm may still exhibit monotonicity violations.

### Mechanism 3
- Claim: The Bayesian approach achieves optimal O(R√T) regret simultaneously for all confidence levels α ∈ [0,1].
- Mechanism: The non-linearized FTRL algorithm with Bayesian regularization achieves low regret without requiring knowledge of confidence levels beforehand.
- Core assumption: The quantile loss function is used as the loss function for the FTRL algorithm.
- Evidence anchors:
  - [abstract] "The algorithm achieves optimal O(R√T) regret without requiring knowledge of confidence levels beforehand, simultaneously for all α ∈ [0,1]"
  - [section] "Theorem 2. Let µt,α := inf{p0(r) : rt(α) ∧ rt ≤ r ≤ rt(α) ∨ rt}. With the step size λt = 1/√t, Algorithm 1 guarantees RegretT (α) = O(R√T)"
- Break condition: If the prior P0 has zero density in certain regions, the regret bound degrades due to division by µt,α.

## Foundational Learning

- Concept: Conformal Prediction (CP)
  - Why needed here: The paper builds upon the CP framework to construct confidence sets for online prediction
  - Quick check question: What is the main difference between direct and indirect approaches in CP?

- Concept: Online Learning and Regret Minimization
  - Why needed here: The paper uses online learning concepts like FTRL to achieve low regret in adversarial settings
  - Quick check question: What is the difference between Follow the Leader (FTL) and Follow the Regularized Leader (FTRL)?

- Concept: Bayesian Inference and Conjugate Priors
  - Why needed here: The paper uses Bayesian distribution estimation with Dirichlet process priors
  - Quick check question: What is the relationship between the Dirichlet process prior and the belief update equation Pt = λtP0 + (1-λt)P(r1:t-1)?

## Architecture Onboarding

- Component map:
  Nature -> Base model -> CP algorithm -> Downstream users

- Critical path:
  1. Receive xt from nature
  2. Receive st from base model
  3. Compute Pt using Eq.(5)
  4. For each queried α, compute rt(α) = qα(Pt)
  5. Receive yt and r*t from nature
  6. Update belief Pt using new observation

- Design tradeoffs:
  - Memory vs accuracy: Storing full empirical distribution vs using quantized version
  - Adaptivity vs stability: Using constant vs decreasing step sizes
  - Prior choice: Uniform prior vs more sophisticated priors

- Failure signatures:
  - Monotonicity violations: Confidence sets with higher α are smaller than those with lower α
  - High regret: Total quantile loss is much larger than optimal O(R√T)
  - Poor coverage: Empirical coverage rate is far from target 1-α

- First 3 experiments:
  1. Test monotonicity of threshold predictions on synthetic iid data
  2. Compare regret on switching data sequence between ERM, OGD, and Bayesian approach
  3. Evaluate coverage frequency on stock price volatility prediction task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How sensitive is the algorithm's performance to the choice of prior distribution P0?
- Basis in paper: [explicit] The paper discusses the role of good priors in Theorem 2, showing that the regret bound depends on the local strong convexity coefficient of ψ at the predicted thresholds.
- Why unresolved: The paper only provides theoretical bounds and does not empirically test different prior distributions beyond the uniform prior.
- What evidence would resolve it: Experiments comparing performance with various prior distributions (e.g., Gaussian, Beta) on synthetic and real data would clarify the sensitivity.

### Open Question 2
- Question: Can the algorithm maintain its performance guarantees when the confidence level queries At change dynamically over time?
- Basis in paper: [inferred] The paper mentions answering multiple arbitrary confidence level queries online but doesn't specifically address how the algorithm would handle dynamically changing query sets.
- Why unresolved: The current analysis assumes At is fixed or at least doesn't change in a way that affects the regret analysis.
- What evidence would resolve it: Extending the theoretical analysis to account for time-varying At, or experimental results showing performance under dynamic query sets.

### Open Question 3
- Question: What additional assumptions or modifications would be needed to extend the algorithm to non-stationary data distributions beyond the simple switching example?

## Limitations

- Strong assumption that base score functions are "near-optimal" for the quantile prediction task, with uncertain practical performance when using off-the-shelf ML models
- Computational complexity of maintaining the Bayesian belief distribution Pt, especially for high-dimensional score spaces, is not analyzed
- Prior distribution assumption of absolute continuity with respect to Lebesgue measure may not hold for discrete score spaces common in many applications

## Confidence

**High Confidence (4/5):**
- The theoretical regret bound of O(R√T) is well-established through the FTRL analysis
- The monotonicity property of threshold predictions is guaranteed by the quantile loss formulation
- The algorithm's ability to handle multiple confidence levels simultaneously is clearly demonstrated

**Medium Confidence (3/5):**
- The practical performance on real stock data, given the limited evaluation scope
- The impact of prior choice on algorithm performance, which is mentioned but not thoroughly explored
- The validity of the near-optimal base model assumption in real-world scenarios

**Low Confidence (2/5):**
- The scalability of the approach to high-dimensional problems
- The sensitivity to hyperparameters like the prior distribution choice
- The performance on non-iid data sequences beyond the simple switching example

## Next Checks

1. **Baseline Comparison:** Implement and compare against simple conformal prediction baselines (split conformal, jackknife+) using the same base models to establish whether the Bayesian approach provides meaningful advantages.

2. **Scalability Test:** Evaluate the algorithm's runtime and memory usage on increasingly large datasets and high-dimensional score spaces to identify practical bottlenecks.

3. **Prior Sensitivity Analysis:** Systematically test the algorithm with different prior distributions (uniform, informative, adversarial) to understand how prior choice affects coverage, regret, and computational efficiency.