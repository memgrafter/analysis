---
ver: rpa2
title: 'Augment before You Try: Knowledge-Enhanced Table Question Answering via Table
  Expansion'
arxiv_id: '2401.15555'
source_url: https://arxiv.org/abs/2401.15555
tags:
- table
- question
- information
- step
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of table question answering
  when the given table lacks sufficient information, necessitating the integration
  of external knowledge. The authors propose a simple yet effective method that first
  constructs an augmenting table containing the missing information and then generates
  a SQL query over the two tables to answer the question.
---

# Augment before You Try: Knowledge-Enhanced Table Question Answering via Table Expansion

## Quick Facts
- arXiv ID: 2401.15555
- Source URL: https://arxiv.org/abs/2401.15555
- Reference count: 40
- Exact match rates: 55.80% on WIKI TQ, 63.12% on TATQA, 53.80% on FINQA

## Executive Summary
This paper addresses the challenge of table question answering when the given table lacks sufficient information, necessitating the integration of external knowledge. The authors propose a simple yet effective method that first constructs an augmenting table containing the missing information and then generates a SQL query over the two tables to answer the question. Experiments on three table QA benchmarks show that this approach outperforms strong baselines, particularly on questions requiring large tables or complex tabular operations.

## Method Summary
The method addresses knowledge-enhanced table QA by first analyzing the question to identify missing information, then constructing an augmenting table with external knowledge, and finally generating a SQL query over both the original and augmenting tables. This three-step pipeline uses GPT-3.5-turbo-1106 with greedy decoding and evaluates performance using exact match rate between predicted and ground truth answers. The approach is tested on WIKITQ, TATQA, and FINQA datasets with both text document and LLM knowledge sources.

## Key Results
- Achieves exact match rates of 55.80% on WIKI TQ, 63.12% on TATQA, and 53.80% on FINQA
- Outperforms Program-of-Thought and Binder baselines, particularly on questions requiring large tables or complex tabular operations
- Shows significant improvements over existing approaches on questions requiring large tables or complex tabular operations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Augmenting the table with missing information before SQL generation improves accuracy compared to embedding queries in SQL.
- Mechanism: The method creates an additional table containing the missing knowledge, which is then joined with the original table. This allows the model to generate standard SQL queries without embedding LLM queries.
- Core assumption: LLMs can generate accurate SQL when provided with complete information in a structured format.
- Evidence anchors:
  - [abstract] "Our method first constructs an augmenting table containing the missing information and then generates a SQL query over the two tables to answer the question."
  - [section] "Our method retains the original table content and augments it with required information."
- Break condition: If the LLM fails to correctly identify missing information or construct the augmenting table.

### Mechanism 2
- Claim: The augment-then-generate pipeline reduces execution errors compared to methods that combine LLM queries with SQL.
- Mechanism: By separating the knowledge augmentation step from SQL generation, the method avoids the complexity of embedding LLM queries in SQL statements.
- Core assumption: Standard SQL is easier for LLMs to generate than SQL with embedded LLM queries.
- Evidence anchors:
  - [section] "it requires the model to learn to embed LLM queries in the standard SQL language, which differs substantially from the SQL statements the model has been trained on."
  - [section] "Our method demonstrates a more pronounced improvement...it exhibits fewer execution errors"
- Break condition: If the augmentation step introduces errors or the joined table becomes too complex for SQL generation.

### Mechanism 3
- Claim: The method scales better to large tables compared to approaches that linearize tables into text.
- Mechanism: By maintaining the structured format of the table and using SQL for interaction, the method avoids the performance degradation seen when tables are linearized into text.
- Core assumption: SQL-based interaction with structured data is more efficient than text-based interaction for large tables.
- Evidence anchors:
  - [section] "Figure 2 plots the performance breakdown by the number of tokens in the table...our method and Binder+CoT are the only methods that maintain performance on large tables"
  - [section] "methods that rely on LLMs to extract information from linearized tables...suffer significant performance degradation on large tables"
- Break condition: If the augmenting table becomes too large or complex to join efficiently with the original table.

## Foundational Learning

- Concept: SQL query generation
  - Why needed here: The method relies on generating accurate SQL queries over augmented tables to answer questions.
  - Quick check question: Can you generate a SQL query to count rows in a table where a column equals a specific value?

- Concept: Table augmentation with external knowledge
  - Why needed here: The method requires constructing an additional table containing missing information from external sources.
  - Quick check question: How would you create a new table with information extracted from a text document?

- Concept: Structured data processing
  - Why needed here: The method maintains structured format throughout the process, avoiding linearization into text.
  - Quick check question: What are the advantages of keeping data in structured format versus linearizing it to text?

## Architecture Onboarding

- Component map: Question -> Analyze -> Augment -> Generate SQL -> Answer
- Critical path: Question → Analyze → Augment → Generate SQL → Answer
- Design tradeoffs:
  - Accuracy vs. complexity: More augmentation improves accuracy but increases SQL complexity
  - External knowledge sources: Flexibility to use different sources (documents, LLMs) but requires source-specific handling
  - SQL generation vs. direct answer: Using SQL provides interpretability but adds an execution step
- Failure signatures:
  - Execution errors: SQL syntax errors or missing information in augmenting table
  - Incorrect answers: Wrong identification of missing information or errors in augmentation step
  - Performance degradation: When tables become too large or complex for SQL generation
- First 3 experiments:
  1. Test on a simple table with one missing piece of information from a text document
  2. Test on a table requiring multiple augmentations from an LLM
  3. Test on a large table to verify scalability compared to linearization approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the method's performance scale when integrating knowledge from multiple external sources simultaneously?
- Basis in paper: [inferred] The paper evaluates integration from two source types (text documents and LLMs) but doesn't explore multi-source scenarios.
- Why unresolved: The current framework appears capable of handling multiple sources, but no experiments test this capability or examine potential conflicts between sources.
- What evidence would resolve it: Experiments comparing single-source vs multi-source augmentation on benchmark datasets, measuring accuracy and error rates.

### Open Question 2
- Question: What is the impact of augmentation quality on final performance, and how can low-quality augmentations be detected and filtered?
- Basis in paper: [inferred] The method relies on LLM-generated augmentations without discussing quality control or error propagation from faulty augmentations.
- Why unresolved: No analysis of augmentation reliability, no mechanism for validating generated table content, and no discussion of how errors in augmentation affect SQL generation.
- What evidence would resolve it: Empirical study showing performance degradation with corrupted augmentations, development of validation metrics for augmentation quality.

### Open Question 3
- Question: How does the method compare to approaches that modify table structure during reasoning versus after reasoning?
- Basis in paper: [explicit] The paper contrasts with concurrent work (Wang et al., 2024) that uses sequential table operations, but only provides preliminary comparison.
- Why unresolved: The comparison uses different backbone LLMs and limited evaluation, preventing definitive conclusions about the trade-offs between pre-augmentation and dynamic table modification.
- What evidence would resolve it: Head-to-head comparison using identical LLMs and comprehensive error analysis on same benchmark splits.

## Limitations
- The method relies heavily on the LLM's ability to accurately identify missing information and construct appropriate augmenting tables, which may fail on complex questions or ambiguous knowledge sources.
- The approach assumes that all required knowledge can be extracted and structured into a table format, potentially limiting its effectiveness for questions requiring temporal reasoning, spatial relationships, or abstract concepts.
- The method's performance depends on the quality of external knowledge sources, and errors in these sources will propagate through the augmentation step.

## Confidence
- **High confidence** in the core claim that augmenting tables before SQL generation improves accuracy compared to embedding queries in SQL, supported by consistent improvements across all three benchmarks and multiple ablation studies.
- **Medium confidence** in the claim about scalability to large tables, as while the method shows better performance than linearization approaches, the comparison is based on token count rather than actual table complexity measures.
- **Medium confidence** in the generalizability claim, as the method was tested on three specific domains (general, airline, financial) and may not perform as well on domains requiring different types of external knowledge.

## Next Checks
1. Test the method on tables requiring temporal reasoning or spatial relationships to evaluate its limitations with non-tabular knowledge types.
2. Evaluate performance when external knowledge sources contain errors or contradictions to assess robustness to knowledge quality.
3. Compare execution error rates between the proposed method and Program-of-Thought approach on the same set of questions to quantify the claimed reduction in execution errors.