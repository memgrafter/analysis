---
ver: rpa2
title: 'Hammer: Robust Function-Calling for On-Device Language Models via Function
  Masking'
arxiv_id: '2410.04587'
source_url: https://arxiv.org/abs/2410.04587
tags:
- function
- performance
- prompt
- function-calling
- hammer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Hammer, a family of on-device language models
  specifically engineered for robust function calling. The key innovation is a function
  masking technique that shifts model attention from function and parameter names
  to their descriptions, reducing reliance on potentially misleading naming conventions.
---

# Hammer: Robust Function-Calling for On-Device Language Models via Function Masking

## Quick Facts
- arXiv ID: 2410.04587
- Source URL: https://arxiv.org/abs/2410.04587
- Reference count: 40
- Hammer-7B achieves 83.92% overall accuracy on Berkeley Function Calling Leaderboard, ranking second only to GPT-4

## Executive Summary
Hammer is a family of on-device language models specifically engineered for robust function calling. The key innovation is a function masking technique that shifts model attention from function and parameter names to their descriptions, reducing reliance on potentially misleading naming conventions. The authors also create an augmented dataset with 7,500 irrelevance detection examples to improve the model's ability to decline tasks when no suitable function exists. Hammer-7B achieves state-of-the-art performance across multiple benchmarks, outperforming larger models and demonstrating strong generalization capabilities across different function-calling tasks and naming conventions.

## Method Summary
Hammer employs function masking techniques during fine-tuning, where function names and parameter names are replaced with random strings while retaining their descriptions. This forces the model to rely on semantic understanding rather than memorized naming patterns. The training dataset is augmented with 7,500 irrelevance detection examples where suitable functions are deliberately excluded, teaching the model when to decline tasks. The approach is evaluated on base models like Qwen and Deepseek-Coder, fine-tuned with the masked and augmented datasets to create the Hammer family of models.

## Key Results
- Hammer-7B achieves 83.92% overall accuracy on Berkeley Function Calling Leaderboard, second only to GPT-4
- State-of-the-art performance across multiple benchmarks including BFCL, API-Bank, Tool-Alpaca, Seal-Tools, and Nexus Raven
- Function masking provides increasing advantages for more complex tasks (Parallel Multiple) while maintaining strong performance on simpler tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Function masking reduces model dependence on potentially misleading function and parameter names by shifting attention to descriptions.
- Mechanism: During training, function names and parameter names are replaced with random strings, forcing the model to rely on descriptions for understanding. This prevents overfitting to specific naming conventions and improves generalization across different naming styles.
- Core assumption: Function and parameter descriptions contain sufficient information for the model to understand functionality, making name-based shortcuts unnecessary.
- Evidence anchors:
  - [abstract]: "function masking techniques to minimize misleading" and "shifts model attention from function and parameter names to their descriptions"
  - [section 4.1]: "minimize the interference from function names and parameter names, while enforcing the model to comprehend the functionality and usage of candidates based on their descriptions"
  - [corpus]: Weak - no direct corpus evidence provided for this specific mechanism

### Mechanism 2
- Claim: Irrelevance-augmented dataset improves the model's ability to detect when no suitable function exists for a given user intent.
- Mechanism: The dataset is augmented with 7,500 examples where the correct functions are deliberately excluded and labels replaced with empty lists. This teaches the model to recognize irrelevance rather than forcing inappropriate function calls.
- Core assumption: Models can learn the concept of "no suitable function exists" through exposure to explicit negative examples during training.
- Evidence anchors:
  - [abstract]: "augmented dataset with 7,500 irrelevance detection examples to improve the model's ability to decline tasks when no suitable function exists"
  - [section 4.2]: "We propose an irrelevance-augmented dataset... incorporates 7,500 examples... where the correct functions are deliberately excluded from the candidate list"
  - [corpus]: Weak - no direct corpus evidence provided for this specific mechanism

### Mechanism 3
- Claim: The function masking approach becomes increasingly advantageous as task complexity rises.
- Mechanism: Complex tasks require deeper understanding of functions, which the description-focused approach facilitates better than name-based approaches. The masking technique forces the model to understand function semantics rather than relying on memorized patterns.
- Core assumption: More complex function-calling tasks (like parallel multiple) inherently require semantic understanding that descriptions provide better than names.
- Evidence anchors:
  - [section 5.3]: "we observe that Hammer-7B achieved state-of-the-art results in both AST Evaluation and Executable Function Evaluation for the most complex Parallel Multiple task"
  - [section 5.3]: "This aligns with our insight that more complex tasks typically demand a deeper understanding of functions, necessitating models to focus more on function descriptions"
  - [corpus]: Weak - no direct corpus evidence provided for this specific mechanism

## Foundational Learning

- Concept: Function calling and API interaction
  - Why needed here: The entire paper focuses on improving language models' ability to call external functions/APIs based on user intent
  - Quick check question: What is the difference between function calling and API calling in the context of language models?

- Concept: Model generalization and overfitting
  - Why needed here: The paper addresses performance inconsistencies across benchmarks due to overfitting to specific naming conventions
  - Quick check question: How does overfitting to training data naming conventions affect a model's performance on new benchmarks with different naming styles?

- Concept: Supervised fine-tuning and dataset augmentation
  - Why needed here: The methodology involves fine-tuning existing models with augmented datasets (irrelevance examples and masking techniques)
  - Quick check question: What is the purpose of augmenting a training dataset with negative examples (like irrelevance detection)?

## Architecture Onboarding

- Component map: User query -> Function selection -> Parameter filling -> Code generation -> Execution
- Critical path: The Hammer model must accurately perform function selection and parameter filling while deciding whether to output function calls or an empty list
- Design tradeoffs: Masking reduces reliance on names but may slow learning speed on same-task scenarios; irrelevance augmentation improves detection but creates an inverse relationship with function-calling accuracy requiring careful balancing
- Failure signatures: Performance degradation on benchmarks with naming conventions matching training data (indicating over-reliance on descriptions), inability to detect irrelevance even with augmented data, inconsistent results across different task types
- First 3 experiments:
  1. Test Hammer performance with different masking ratios (0%, 33%, 66%, 100%) to find optimal balance between same-task and cross-task performance
  2. Evaluate Hammer's irrelevance detection capability by creating test cases where no suitable function exists
  3. Compare Hammer's performance on complex vs simple function-calling tasks to validate the masking advantage for complex scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the function masking ratio affect model performance across different function-calling tasks (Simple, Multiple, Parallel, Parallel Multiple)?
- Basis in paper: [explicit] The authors conducted an ablation study on different masking ratios and observed that larger ratios improved generalization on cross-task scenarios but hindered learning speed within the same task.
- Why unresolved: While the paper provides some insights into the trade-off between masking ratio and performance, it doesn't fully explore how this ratio affects performance across different task complexities.
- What evidence would resolve it: Detailed performance comparisons of Hammer models trained with varying masking ratios on each function-calling task type would clarify the optimal ratio for each scenario.

### Open Question 2
- Question: What is the long-term impact of the irrelevance-augmented dataset on model performance in real-world applications?
- Basis in paper: [explicit] The authors introduced an augmented dataset with 7,500 irrelevance detection examples and observed an inverse relationship between irrelevance detection and function-calling accuracy during fine-tuning.
- Why unresolved: The paper doesn't provide data on how this augmented dataset affects model performance over extended use or in diverse real-world scenarios beyond the benchmarks tested.
- What evidence would resolve it: Longitudinal studies comparing Hammer's performance with and without the augmented dataset in practical applications over time would demonstrate the dataset's lasting impact.

### Open Question 3
- Question: How does the function masking technique generalize to function-calling tasks in languages other than Python?
- Basis in paper: [inferred] The paper mentions that Hammer performs well on non-Python environments in the Berkeley Function-Calling Leaderboard, but doesn't specifically address how the function masking technique translates to other programming languages.
- Why unresolved: The effectiveness of function masking in non-Python environments is not thoroughly explored, leaving questions about its broader applicability.
- What evidence would resolve it: Comparative studies of Hammer's performance with and without function masking on function-calling benchmarks in multiple programming languages would clarify its cross-language effectiveness.

## Limitations

- Performance generalization uncertainty: Results may be specific to evaluated function-calling domains and may not transfer to completely unseen function libraries or real-world production scenarios
- Dataset bias concerns: The 7,500 irrelevance examples may not adequately capture the diversity of real-world scenarios where no suitable function exists
- Fine-tuning dependency: Requires access to base model weights and substantial computational resources for fine-tuning, limiting accessibility

## Confidence

- **High confidence**: The core function masking mechanism and its impact on reducing overfitting to naming conventions
- **Medium confidence**: The irrelevance detection capability claims
- **Medium confidence**: The scalability claims from smaller models (1.5B) to larger ones (7B)

## Next Validation Checks

1. Cross-domain function library testing: Evaluate Hammer-7B on completely new function libraries that weren't represented in any of the training or evaluation datasets

2. Real-world deployment study: Deploy Hammer in a real application environment where it must handle diverse user queries and dynamically changing function libraries

3. Ablation study on masking ratios: Conduct systematic experiments varying the function masking ratio (0%, 25%, 50%, 75%, 100%) across different task complexities to identify the optimal masking strategy