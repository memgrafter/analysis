---
ver: rpa2
title: On-Device Training of Fully Quantized Deep Neural Networks on Cortex-M Microcontrollers
arxiv_id: '2407.10734'
source_url: https://arxiv.org/abs/2407.10734
tags:
- training
- memory
- learning
- gradient
- on-device
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of enabling on-device training
  of deep neural networks on Cortex-M microcontrollers, which have limited computational
  resources and memory. The authors propose a method based on fully quantized training
  (FQT) and dynamic partial gradient updates.
---

# On-Device Training of Fully Quantized Deep Neural Networks on Cortex-M Microcontrollers

## Quick Facts
- arXiv ID: 2407.10734
- Source URL: https://arxiv.org/abs/2407.10734
- Reference count: 40
- Primary result: 34.8% less memory and 49.0% lower latency per training sample compared to related work

## Executive Summary
This paper addresses the challenge of enabling on-device training of deep neural networks on resource-constrained Cortex-M microcontrollers. The authors propose a method combining fully quantized training (FQT) with dynamic partial gradient updates to overcome the severe memory and computational limitations of these embedded devices. This approach allows for direct training with quantized weights and tensors while selectively updating gradients based on their magnitude to reduce computational complexity. The method demonstrates significant improvements in memory efficiency and training latency while maintaining accuracy levels close to floating-point training, making it particularly valuable for edge AI applications requiring continuous learning.

## Method Summary
The proposed approach employs fully quantized training (FQT) to enable direct training of neural networks using quantized weights and tensors, eliminating the need for conversion between quantized and floating-point representations. This is combined with dynamic partial gradient updates, a technique that selectively updates weights based on gradient magnitude thresholds, thereby reducing computational complexity. The implementation leverages the limited resources of Cortex-M microcontrollers by optimizing memory usage through quantization and minimizing computational overhead through selective gradient updates. This dual strategy enables both transfer learning and full DNN training on embedded devices across vision and time-series datasets while maintaining competitive accuracy.

## Key Results
- Requires 34.8% less memory compared to related approaches
- Achieves 49.0% lower latency per training sample
- Dynamic partial gradient updates provide up to 8.7× speedup compared to full weight updates
- Maintains accuracy levels close to floating-point training across multiple datasets

## Why This Works (Mechanism)
The approach works by eliminating the computational bottleneck of dequantization/re-quantization cycles through FQT, while the dynamic partial gradient updates reduce the number of weight updates by focusing computational resources on parameters with larger gradients. This selective update mechanism prioritizes learning in network regions with the most significant changes, effectively distributing limited computational resources where they have the greatest impact on model convergence.

## Foundational Learning

**Quantized Neural Networks** - Neural networks using low-precision weights and activations
*Why needed:* Essential for reducing memory footprint and computational requirements on resource-constrained microcontrollers
*Quick check:* Verify quantization schemes (e.g., INT8, INT4) and their impact on model accuracy

**Gradient-based Optimization** - Methods for updating network weights using gradient information
*Why needed:* Core mechanism for training neural networks through backpropagation
*Quick check:* Confirm gradient computation and update rules are correctly implemented in quantized domain

**Dynamic Partial Updates** - Selective weight updates based on gradient magnitude
*Why needed:* Reduces computational load by focusing updates on parameters with largest gradients
*Quick check:* Validate the threshold selection mechanism and its effect on convergence

**Transfer Learning** - Adapting pre-trained models to new tasks with limited data
*Why needed:* Enables efficient model adaptation on resource-constrained devices
*Quick check:* Ensure fine-tuning process preserves knowledge while adapting to new data

**Memory-constrained Computing** - Optimization techniques for limited memory environments
*Why needed:* Critical for deploying models on microcontrollers with strict memory constraints
*Quick check:* Verify memory allocation and deallocation strategies are optimized

## Architecture Onboarding

**Component Map:** Input Data -> Quantization Layer -> Neural Network Forward Pass -> Loss Computation -> Gradient Calculation -> Dynamic Partial Gradient Update -> Weight Update -> Output

**Critical Path:** The most performance-critical sequence is: Forward Pass → Loss Computation → Gradient Calculation → Dynamic Partial Gradient Update → Weight Update

**Design Tradeoffs:** The approach trades some convergence speed for reduced computational load through selective gradient updates. While this may extend training time in some cases, it enables training on hardware that would otherwise be incapable of such operations.

**Failure Signatures:** Potential issues include: (1) poor convergence due to overly aggressive gradient pruning, (2) accuracy degradation from quantization noise, and (3) memory fragmentation from frequent allocation/deallocation cycles.

**First Experiments:**
1. Baseline accuracy comparison: Full precision training vs. proposed FQT method on a simple CNN architecture
2. Memory usage profiling: Track peak memory consumption during training across different quantization levels
3. Speedup validation: Measure actual vs. theoretical speedup from dynamic partial gradient updates

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily focused on vision and time-series datasets, with limited testing on other data modalities
- Claims regarding memory and latency improvements lack detailed baseline specifications for independent verification
- Dynamic partial gradient update strategy's impact on convergence across different network architectures needs further exploration

## Confidence

**Major Claim Clusters and Confidence:**
- Memory and latency improvements: Medium confidence. Percentages are stated but lack detailed baseline specifications for independent verification
- Dynamic partial gradient updates effectiveness: Medium confidence. Speedup claims are supported but generalizability across architectures needs validation
- Accuracy preservation compared to floating-point: Medium confidence. Claims are made for tested datasets but broader testing is needed

## Next Checks

1. Evaluate the approach on diverse data modalities (e.g., audio, text) beyond vision and time-series to assess robustness
2. Provide detailed specifications of the baseline implementations used for comparison to enable independent verification of memory and latency claims
3. Conduct ablation studies on the dynamic partial gradient update strategy to determine its impact on convergence across different network architectures and datasets