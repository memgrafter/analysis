---
ver: rpa2
title: A Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial
  Scenarios
arxiv_id: '2408.01963'
source_url: https://arxiv.org/abs/2408.01963
tags:
- original
- effect
- robustness
- size
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates the robustness of large language models (LLMs)
  to naturally-occurring, non-adversarial input perturbations by introducing variants
  such as superficial changes, paraphrases, and distractions into three datasets:
  PopQA, BoolQ, and SIGA. The authors propose using Cohen''s h effect size as a symmetric,
  easily interpretable metric to measure model robustness, which overcomes limitations
  of traditional performance drop rate (PDR) metrics.'
---

# A Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial Scenarios

## Quick Facts
- arXiv ID: 2408.01963
- Source URL: https://arxiv.org/abs/2408.01963
- Reference count: 8
- Key result: Proposes Cohen's h as a symmetric, interpretable metric for measuring LLM robustness to naturally-occurring perturbations, showing paraphrasing causes larger performance drops than superficial changes.

## Executive Summary
This study evaluates the robustness of large language models to naturally-occurring, non-adversarial input perturbations by introducing variants such as superficial changes, paraphrases, and distractions into three datasets: PopQA, BoolQ, and SIGA. The authors propose using Cohen's h effect size as a symmetric, easily interpretable metric to measure model robustness, which overcomes limitations of traditional performance drop rate (PDR) metrics. Experiments with multiple LLMs show that perturbations cause only very small performance decreases, with Cohen's h values indicating mostly "essentially zero" to "very small" effects. The study finds that paraphrasing typically causes larger performance drops than superficial changes, and that more accurate models tend to be less robust to perturbations.

## Method Summary
The method evaluates LLM robustness by applying three types of perturbations (superficial changes, paraphrases, and distractions) to three datasets (PopQA, BoolQ, SIGA). The key innovation is using Cohen's h effect size, calculated as H(score_o, score_p) = ψ(score_p) - ψ(score_o) where ψ(score_i) = 2*arcsin(√score_i), as a symmetric, interpretable metric for measuring robustness. This metric is compared against traditional PDR metrics. Zero-shot evaluations are performed on multiple LLMs including Llama3-Instruct, Mistral-Instruct, and Granite models. Bootstrapped confidence intervals are used to assess statistical significance, with group-level bootstrapping where each group consists of an original instance and its perturbations.

## Key Results
- Perturbations cause very small performance decreases across all models and datasets, with Cohen's h values indicating "essentially zero" to "very small" effects
- Paraphrasing causes larger performance drops than superficial changes across all three datasets
- More accurate models tend to be less robust to perturbations (inverse relationship between accuracy and robustness)
- Cohen's h provides a more reliable and interpretable metric than PDR, especially when original scores are near 0 or 1

## Why This Works (Mechanism)

### Mechanism 1
Cohen's h provides a symmetric, interpretable measure of robustness that avoids the asymmetry and undefined cases inherent in PDR. By using the arcsine square root transformation on proportions, Cohen's h creates a metric that is symmetric around zero and defined for all pairs of scores in [0,1]. This transformation makes the metric sensitive to differences in a way that correlates with perceptual detectability of changes.

### Mechanism 2
The proposed metric enables statistical significance testing with small sample sizes (one original instance plus a few perturbations). Cohen's h is defined even when there is no sample variation, which allows meaningful comparison between the original and perturbed instances. This contrasts with p-values and some other effect sizes that become undefined with zero variance.

### Mechanism 3
The metric reveals that paraphrasing causes larger performance drops than superficial changes, providing actionable insights about perturbation types. By breaking down the robustness measurements by perturbation type, the metric can distinguish between different types of input variations and their impact on model performance.

## Foundational Learning

- Concept: Effect sizes and their role in statistical analysis
  - Why needed here: Understanding effect sizes is crucial for interpreting the Cohen's h metric and why it's superior to PDR for robustness evaluation. The concept explains why metrics that are independent of sample size are valuable for comparing model performance.
  - Quick check question: Why is it problematic that PDR is asymmetric and undefined when the original score is zero?

- Concept: Arcsine square root transformation and its statistical properties
  - Why needed here: This transformation is the mathematical foundation of Cohen's h. Understanding it helps explain why the metric behaves the way it does and why it correlates with perceptual detectability of differences.
  - Quick check question: How does the arcsine square root transformation make differences more detectable when proportions are close to 0 or 1?

- Concept: Bootstrapping and confidence interval estimation
  - Why needed here: The paper uses bootstrapped confidence intervals to assess the reliability of the robustness measurements. Understanding this concept is essential for interpreting the statistical significance of the results.
  - Quick check question: Why does the paper use group-level bootstrapping (resampling original instances with their perturbations) rather than resampling individual instance scores?

## Architecture Onboarding

- Component map: Dataset perturbation generation (superficial changes, paraphrases, distractions) -> Model inference on original and perturbed instances -> Metric calculation (Cohen's h and PDR) -> Statistical analysis with bootstrapped confidence intervals. The open-source repository unitxt implements these components.
- Critical path: The most critical path is perturbation generation → model inference → metric calculation, as errors in any of these steps directly impact the robustness assessment.
- Design tradeoffs: The choice of Cohen's h over other metrics trades interpretability for mathematical elegance. Using bootstrapped confidence intervals trades computational cost for more reliable statistical inference. The perturbation generation approach trades diversity for semantic preservation.
- Failure signatures: Poor robustness scores across all perturbation types might indicate fundamental model weaknesses. Large discrepancies between PDR and Cohen's h could indicate issues with the metric choice or implementation. High variance in bootstrapped confidence intervals might suggest insufficient perturbation diversity or sample size.
- First 3 experiments:
  1. Run the evaluation pipeline on a simple dataset (like BoolQ) with one model (like Llama3-Instruct) to verify the complete workflow works end-to-end.
  2. Test the perturbation generation code separately to ensure it's producing the expected types and quantities of perturbations.
  3. Compare the Cohen's h metric implementation against a known-good implementation on synthetic data with known effect sizes to verify correctness.

## Open Questions the Paper Calls Out

- Question: How do different types of perturbations (superficial, paraphrase, distraction) affect the Cohen's h effect size metric differently across various datasets and model architectures?
  - Basis in paper: The paper compares different perturbation types and shows that paraphrasing typically causes larger performance drops than superficial changes.
  - Why unresolved: The paper provides initial findings but does not deeply explore the underlying reasons for these differences or how different model architectures respond differently to various perturbation types.

- Question: What is the relationship between model accuracy and robustness to perturbations, and how does this relationship vary across different model sizes and architectures?
  - Basis in paper: The paper observes that "more accurate models tend to be less robust to perturbations" and shows a scatter plot comparing mean accuracy versus robustness.
  - Why unresolved: While the paper identifies a general trend, it does not explain the underlying mechanisms or explore whether this relationship holds consistently across all model sizes and architectures.

- Question: How does the proposed Cohen's h metric perform in scenarios with continuous-valued evaluation scores compared to binary-valued outcomes, and what alternative effect size metrics might be more appropriate?
  - Basis in paper: The paper discusses limitations of Cohen's h for continuous-valued evaluation scores and suggests that alternative metrics like Cohen's d or Hedges' g might be more appropriate.
  - Why unresolved: The paper acknowledges that Cohen's h is specifically designed for binary-valued outcomes but does not empirically compare these metrics or validate their effectiveness in real-world scenarios.

## Limitations
- Evaluation focuses primarily on models with relatively high accuracy (>80%), potentially limiting insights about robustness in lower-performing models.
- Perturbations, while natural, may not fully capture the diversity of real-world input variations that models encounter.
- The use of bootstrapped confidence intervals may not fully account for the correlation between original and perturbed instances.

## Confidence
- **High Confidence**: The Cohen's h metric provides a mathematically sound and interpretable measure of robustness differences between original and perturbed instances.
- **Medium Confidence**: The claim that paraphrasing causes larger performance drops than superficial changes is supported by the data but may be dataset-dependent.
- **Low Confidence**: The generalizability of the robustness findings to other domains beyond the three evaluated datasets and to models with significantly different architectures.

## Next Checks
1. Test the Cohen's h metric and robustness findings on additional datasets from different domains to verify the consistency of perturbation effects and the relationship between accuracy and robustness.
2. Evaluate a broader range of models including those with lower accuracy scores to determine if the observed inverse relationship between accuracy and robustness holds across the full spectrum of model performance.
3. Systematically vary the intensity and diversity of superficial changes and paraphrasing to identify thresholds where performance degradation becomes more substantial.