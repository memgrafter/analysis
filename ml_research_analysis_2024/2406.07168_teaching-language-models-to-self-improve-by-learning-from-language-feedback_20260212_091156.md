---
ver: rpa2
title: Teaching Language Models to Self-Improve by Learning from Language Feedback
arxiv_id: '2406.07168'
source_url: https://arxiv.org/abs/2406.07168
tags:
- feedback
- language
- stage
- learning
- refinements
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Self-Refinement Tuning (SRT), a method that
  uses model-generated feedback to align language models with human intentions, reducing
  the need for costly human annotations. SRT works by having a powerful model critique
  and refine outputs from a base model, which is then fine-tuned on this feedback.
---

# Teaching Language Models to Self-Improve by Learning from Language Feedback

## Quick Facts
- arXiv ID: 2406.07168
- Source URL: https://arxiv.org/abs/2406.07168
- Reference count: 14
- Primary result: SRT improves 70B model win rate from 9.6% to 25.8% on AlpacaEval 2.0

## Executive Summary
This paper introduces Self-Refinement Tuning (SRT), a method that uses model-generated feedback to align language models with human intentions, reducing the need for costly human annotations. SRT works by having a powerful model critique and refine outputs from a base model, which is then fine-tuned on this feedback. The base model is further optimized using its own self-generated feedback. Empirical results show that SRT significantly improves model performance across various tasks and model sizes. For instance, when applied to a 70B parameter model, SRT increases the win rate from 9.6% to 25.8% on the AlpacaEval 2.0 benchmark, surpassing well-established systems such as GPT-4-0314, Claude 2, and Gemini.

## Method Summary
SRT is a two-stage approach for aligning language models using model-generated feedback. In stage 1, a base model generates responses to instructions, which are then critiqued and refined by a powerful critic model (GPT-4-Turbo). The base model is fine-tuned on the instruction→response→feedback→refinement sequences. In stage 2, the fine-tuned model generates its own feedback and refinements, creating preference pairs that are used for Direct Preference Optimization (DPO) training. The method aims to teach models to self-evaluate and improve without requiring human annotations.

## Key Results
- SRT improves 70B model win rate from 9.6% to 25.8% on AlpacaEval 2.0 benchmark
- SRT outperforms well-established systems like GPT-4-0314, Claude 2, and Gemini
- The method significantly improves performance on GSM8K, BBH, TyDiQA, and HH-RLHF tasks
- Larger models (13B and 70B) benefit more from the second stage of SRT compared to smaller ones (7B)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language feedback provides richer alignment signals than binary preference labels.
- Mechanism: The critic model identifies specific weaknesses, gives actionable suggestions, and rates responses on a 1-10 scale, creating a structured feedback loop that guides the base model toward improvements.
- Core assumption: Natural language critiques contain information that binary preferences cannot express, such as reasoning gaps and suggested improvements.
- Evidence anchors:
  - [abstract]: "Current methods primarily rely on human preferences, which are costly and insufficient in capturing nuanced feedback expressed in natural language."
  - [section 3.1.2]: "The base model is then fine-tuned on critiques and refinements, enabling self-evaluation and improvement."
  - [corpus]: Weak - corpus neighbors focus on different feedback mechanisms (RL, ranking) and do not provide direct evidence about language feedback superiority.
- Break condition: If the critic model fails to generate meaningful critiques, the feedback signal degrades to noise and no longer improves the base model.

### Mechanism 2
- Claim: Self-generated feedback creates a scalable learning loop without requiring human annotations.
- Mechanism: The fine-tuned model critiques and refines its own outputs, producing preference pairs that can be used for DPO training, effectively creating a feedback loop.
- Core assumption: A model fine-tuned on external feedback can reliably critique and refine its own outputs to generate useful training data.
- Evidence anchors:
  - [section 3.2]: "We leverage Mself to generate preference data and for further model optimization... we directly generate an improved response via self-refinement."
  - [section 4.2]: "In the second stage, SRT further boosts performance by scaling feedback with self-generated responses and refinements."
  - [corpus]: Weak - neighbors discuss different self-improvement approaches but do not specifically address self-generated feedback for DPO.
- Break condition: If the self-feedback quality drops significantly (as observed in 7B/13B models), the DPO training may reinforce incorrect patterns.

### Mechanism 3
- Claim: Fine-tuning on feedback and refinement sequences teaches the model to predict and apply improvements.
- Mechanism: By reformatting data as "Instruction → Response → Feedback → Refinement" and training with a language modeling objective, the model learns to generate the entire improvement sequence.
- Core assumption: The model can learn to predict feedback and refinement given an instruction and response, internalizing the improvement process.
- Evidence anchors:
  - [section 3.1.2]: "We reformat the collected data into sequences... By doing so, we can train the model using a language modeling objective."
  - [section 4.1]: "The base model is trained using the Tulu-2 Mixture dataset... In the first stage of SRT, we fine-tune the base models for five epochs."
  - [corpus]: Missing - neighbors do not provide evidence about this specific fine-tuning approach.
- Break condition: If the model fails to generate coherent feedback or refinements during inference, the fine-tuning did not successfully internalize the improvement process.

## Foundational Learning

- Concept: Supervised Fine-Tuning (SFT)
  - Why needed here: The base model must be capable of following instructions and generating coherent text before applying SRT, which builds on this foundation.
  - Quick check question: What is the difference between pre-training and fine-tuning in the context of language models?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: SRT uses DPO in the second stage to optimize the model using self-generated preference pairs, requiring understanding of how DPO works with pairwise data.
  - Quick check question: How does DPO differ from traditional reinforcement learning approaches in language model alignment?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: SRT is positioned as an alternative to RLHF that reduces human annotation costs, so understanding RLHF's limitations is crucial.
  - Quick check question: What are the main challenges of RLHF that SRT aims to address?

## Architecture Onboarding

- Component map: Base model (Tulu2 variants) -> Critic model (GPT-4 Turbo) -> Feedback template (weaknesses, scores, suggestions) -> Fine-tuning pipeline (stage 1: feedback+refinement, stage 2: self-DPO) -> Evaluation framework (AlpacaEval, GSM8K, BBH, TydiQA)

- Critical path: 1. Generate initial responses with base model 2. Obtain critiques and refinements from critic model 3. Fine-tune base model on instruction→response→feedback→refinement sequences 4. Generate self-feedback and refinements 5. Apply DPO using self-generated preference pairs 6. Evaluate on benchmarks

- Design tradeoffs:
  - Using GPT-4 as critic vs. open-source alternatives: better feedback quality but higher cost and dependency
  - Single iteration refinement vs. multiple: simpler pipeline but potentially less improvement
  - Language feedback vs. binary preferences: richer signals but more complex processing

- Failure signatures:
  - Performance degradation when removing feedback components
  - Significant accuracy drop in HH-RLHF after stage 2 for smaller models
  - Model generates incoherent feedback or refinements during self-evaluation

- First 3 experiments:
  1. Test self-refinement vs. re-ranking on a small validation set to confirm the paper's finding that self-refinement is more effective
  2. Ablate feedback components (weaknesses, suggestions, scores) to measure their individual contributions to performance
  3. Vary the number of training samples in stage 1 to find the optimal data volume for different model sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SRT scale with model size beyond 70B parameters?
- Basis in paper: [explicit] The paper mentions that larger models (13B and 70B) benefit more from the second stage of SRT compared to smaller ones (7B), but does not explore models larger than 70B.
- Why unresolved: The paper only tests SRT on models up to 70B parameters, leaving the scalability to even larger models unexplored.
- What evidence would resolve it: Testing SRT on models larger than 70B, such as those with hundreds of billions of parameters, and comparing their performance gains across the two stages of SRT.

### Open Question 2
- Question: Can SRT be effectively applied to non-English languages or multilingual tasks?
- Basis in paper: [inferred] The paper focuses on English benchmarks and does not mention multilingual applications or performance on non-English languages.
- Why unresolved: The experiments are conducted solely on English-language tasks, and there is no analysis of SRT's effectiveness in multilingual settings.
- What evidence would resolve it: Applying SRT to multilingual datasets and evaluating its performance on non-English languages, as well as testing its ability to handle code-switching or cross-lingual tasks.

### Open Question 3
- Question: What is the impact of iterative refinement on the performance of SRT, and how many iterations are optimal?
- Basis in paper: [explicit] The paper mentions that a single iteration of refinement is typically sufficient, with additional iterations providing only marginal improvements.
- Why unresolved: While the paper briefly discusses the sufficiency of a single iteration, it does not explore the optimal number of iterations or the diminishing returns of further refinement.
- What evidence would resolve it: Conducting experiments with varying numbers of refinement iterations and measuring the performance gains to determine the point of diminishing returns.

### Open Question 4
- Question: How does the quality of the critic model (e.g., GPT-4 Turbo) affect the performance of SRT?
- Basis in paper: [explicit] The paper uses GPT-4 Turbo as the critic model, which has a high agreement rate with human preferences, but does not explore the impact of using different critic models or varying the quality of the critic.
- Why unresolved: The experiments rely on a single, high-quality critic model, and there is no analysis of how different critic models or varying critic quality would impact SRT's performance.
- What evidence would resolve it: Testing SRT with different critic models of varying quality and comparing the resulting performance to determine the importance of critic quality.

## Limitations

- The method relies heavily on GPT-4-Turbo as a critic, raising scalability and cost concerns
- Self-generated feedback quality degrades significantly for smaller models (7B and 13B), limiting the effectiveness of the second stage
- The paper lacks a quantitative comparison of SRT's cost-effectiveness versus human annotation

## Confidence

- **High Confidence**: The core claim that language feedback improves alignment compared to binary preferences is well-supported by the empirical results across multiple benchmarks.
- **Medium Confidence**: The claim that self-generated feedback enables further improvement is partially supported - strong for 70B but degraded for smaller models.
- **Low Confidence**: The assertion that this approach is cost-effective compared to human annotation lacks quantitative comparison of actual costs.

## Next Checks

1. **Cross-Critic Validation**: Repeat the experiment using an open-source critic model (e.g., LLaMA-2-70B-Chat) instead of GPT-4-Turbo to assess whether the improvements are specific to the critic's capability or generalize to the feedback mechanism.

2. **Multi-Iteration Refinement Study**: Implement a 3-iteration refinement loop where each refinement becomes the input for the next critique cycle, measuring whether performance continues to improve or plateaus.

3. **Cost-Benefit Analysis**: Calculate the actual compute costs of generating GPT-4-Turbo feedback for 22K and 63K instances versus the human annotation costs for comparable preference data, providing concrete economic comparison.