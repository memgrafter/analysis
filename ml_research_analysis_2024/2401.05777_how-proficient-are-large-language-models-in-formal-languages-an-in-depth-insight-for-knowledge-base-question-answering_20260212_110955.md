---
ver: rpa2
title: How Proficient Are Large Language Models in Formal Languages? An In-Depth Insight
  for Knowledge Base Question Answering
arxiv_id: '2401.05777'
source_url: https://arxiv.org/abs/2401.05777
tags:
- language
- formal
- call
- string
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates the understanding and generation abilities
  of large language models (LLMs) in different formal languages used in knowledge
  base question answering (KBQA). The authors propose two tasks: formal language understanding
  (translating logical forms to natural language questions) and formal language generation
  (translating natural language questions to logical forms).'
---

# How Proficient Are Large Language Models in Formal Languages? An In-Depth Insight for Knowledge Base Question Answering

## Quick Facts
- **arXiv ID**: 2401.05777
- **Source URL**: https://arxiv.org/abs/2401.05777
- **Reference count**: 40
- **Primary result**: LLMs approach human-level performance in understanding formal languages but struggle with generation, showing sensitivity to language structure with more natural-language-like formalisms performing better.

## Executive Summary
This paper evaluates large language models' proficiency in understanding and generating formal languages used in knowledge base question answering (KBQA). The authors propose two tasks: formal language understanding (translating logical forms to natural language) and formal language generation (translating natural language to logical forms). Testing three representative formal languages (Lambda DCS, SPARQL, and KoPL) across different KBQA datasets, the study reveals that while LLMs can achieve human-level understanding performance, they still face challenges in generating correct logical forms given few examples. Importantly, LLMs show considerable sensitivity to different formal language structures, with languages more similar to natural language being more "LLM-friendly."

## Method Summary
The study uses in-context learning with carefully selected demonstration examples to evaluate LLM proficiency in formal languages without fine-tuning. For understanding tasks, demonstrations are retrieved based on logical structure similarity and shared content using minimum edit distance. For generation tasks, BM25 algorithm finds similar questions, and entity linking incorporates knowledge base entity and relation names. Performance is evaluated using semantic parsers trained on LLM-generated data versus human-labeled data for understanding tasks, and exact match scores or execution accuracy for generation tasks.

## Key Results
- LLMs approach human-level performance in understanding formal languages across all three tested languages
- LLMs struggle with generating correct logical forms given few examples, showing a significant gap between understanding and generation capabilities
- LLMs exhibit sensitivity to formal language structure, with KoPL (most similar to natural language) outperforming SPARQL and Lambda DCS
- Entity linking significantly improves SPARQL generation performance by grounding generated entities to the knowledge base

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs perform better on formal languages more similar to natural language due to pre-training corpus distribution
- Mechanism: Pre-training data contains more natural language patterns, making structurally similar formal languages easier to parse and generate
- Core assumption: Pre-training data distribution influences model proficiency on downstream formal language tasks
- Evidence anchors: [abstract], [section 7.2] - The paper observes KoPL is most similar to natural language and performs best
- Break condition: New identifiers or structures not seen in pre-training degrade performance regardless of similarity

### Mechanism 2
- Claim: Structurally similar examples improve generation quality more than random examples
- Mechanism: Greedy search algorithm based on minimum edit distance ensures demonstrations share structural similarity with target logical forms
- Core assumption: Structural similarity between demonstrations and targets is more important than content similarity for in-context learning
- Evidence anchors: [section 5.1.1], [section 5.1.2] - Paper describes using edit distance for demonstration retrieval
- Break condition: Edit distance metric fails to capture semantic similarity, misleading rather than helping the model

### Mechanism 3
- Claim: Entity linking improves SPARQL generation by grounding generated entities to knowledge base
- Mechanism: Adding 2-hop-related entity and relation names helps model generate correct identifiers
- Core assumption: LLMs struggle with generating correct knowledge base identifiers but can use them effectively when provided in context
- Evidence anchors: [section 5.2.1], [section 7.2] - Paper shows incorporating entity names enhances SPARQL results
- Break condition: Large knowledge bases with diverse entity names make 2-hop linking insufficient

## Foundational Learning

- **Tree edit distance (TED) and minimum edit distance (ED)**: Used to measure structural similarity between logical forms for demonstration retrieval. *Quick check: What's the difference between TED and ED, and why does the paper use ED despite mentioning TED?*
- **In-context learning (ICL)**: Core methodology for evaluating LLM proficiency without fine-tuning. *Quick check: How does ICL differ from few-shot learning, and what are its limitations for formal language tasks?*
- **Chain-of-thought reasoning**: Used in formal language generation to break down complex tasks into manageable steps. *Quick check: Why might chain-of-thought prompting be particularly useful for generating structured formal languages?*

## Architecture Onboarding

- **Component map**: Data pipeline -> Skeleton transformation -> Demonstration retrieval -> Prompt construction -> LLM generation -> Evaluation
- **Critical path**: Data loading → Skeleton transformation → Demonstration retrieval → Prompt construction → LLM generation → Evaluation
- **Design tradeoffs**: 
  - Accuracy vs. speed: Using ED instead of TED for faster computation but potentially less accurate similarity measurement
  - Prompt length vs. performance: More demonstrations improve quality but hit context window limits
  - Generalization vs. specificity: Broader demonstrations may help more examples but provide less targeted guidance
- **Failure signatures**: 
  - Poor performance across all formal languages suggests pre-training data mismatch
  - Excellent understanding but poor generation suggests generation-specific issues
  - Good performance on one language but not others suggests language-specific challenges
- **First 3 experiments**: 
  1. Test retrieval strategy: Compare random demonstrations vs. structurally similar demonstrations
  2. Test prompt structure: Compare chain-of-thought vs. direct generation for each formal language
  3. Test entity linking: Compare SPARQL generation performance with and without entity linking augmentation

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does performance vary across different KB structures (e.g., graph-based vs. tree-based)?
- **Basis in paper**: [inferred] The paper compares three formal languages but doesn't explicitly analyze KB structure impact
- **Why unresolved**: Focuses on formalization level without isolating effect of underlying KB structure
- **What evidence would resolve it**: Systematic experiments comparing LLMs on languages tied to different KB structures while controlling for formalization level

### Open Question 2
- **Question**: What is the upper limit of LLMs' ability to generate correct logical forms given fixed demonstration examples?
- **Basis in paper**: [explicit] Paper states "generating correct logical forms given a few examples remains a challenge" but doesn't quantify this limitation
- **Why unresolved**: Shows performance improves with more examples but doesn't establish a plateau or maximum achievable accuracy
- **What evidence would resolve it**: Experiments testing generation performance with varying numbers of examples until performance saturates

### Open Question 3
- **Question**: How do different pre-training strategies (e.g., code-pretrained vs. text-only) affect LLM proficiency in formal languages?
- **Basis in paper**: [inferred] Mentions code-pretrained models like CODEX but doesn't systematically compare them to text-only models
- **Why unresolved**: Acknowledges code models exist but doesn't analyze their potential advantages for formal language tasks
- **What evidence would resolve it**: Direct comparison of code-pretrained and text-only LLMs on same formal language tasks

## Limitations
- Conclusions based on relatively small-scale experiments with limited dataset sizes
- Performance differences between formal languages observed but underlying reasons not fully validated
- Entity linking approach effectiveness may not generalize to knowledge bases with different entity naming conventions or larger scales
- Greedy search algorithm uses minimum edit distance instead of tree edit distance, potentially compromising structural similarity matching quality

## Confidence

- **High confidence**: LLMs approach human-level performance in understanding formal languages is well-supported by experimental results across all three formal languages
- **Medium confidence**: LLMs struggle with generation but excel at understanding is supported by data, though exact magnitude may vary with different metrics or dataset sizes
- **Medium confidence**: LLMs' sensitivity to different formal languages and KoPL > SPARQL > Lambda DCS ranking is demonstrated, but underlying mechanism (similarity to natural language) is primarily hypothesized rather than experimentally validated

## Next Checks

1. **Cross-dataset validation**: Test the same formal language understanding and generation tasks on additional KBQA datasets beyond Overnight, GrailQA, and KQA Pro to verify whether observed performance patterns are consistent across different domains and data distributions.

2. **Ablation study on structural similarity**: Compare greedy search retrieval using minimum edit distance against random demonstration selection and tree edit distance to quantify actual impact of structural similarity on in-context learning performance for formal language generation.

3. **Pre-training data analysis**: Analyze the pre-training corpora of the LLMs used to determine actual distribution of formal language patterns and verify whether observed proficiency differences correlate with frequency and diversity of each formal language structure in training data.