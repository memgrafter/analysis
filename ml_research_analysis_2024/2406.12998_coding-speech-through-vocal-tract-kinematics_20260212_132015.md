---
ver: rpa2
title: Coding Speech through Vocal Tract Kinematics
arxiv_id: '2406.12998'
source_url: https://arxiv.org/abs/2406.12998
tags:
- speech
- articulatory
- speaker
- features
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel framework for speech coding using
  vocal tract kinematics. The key idea is to encode speech as articulatory features,
  which are kinematic traces of vocal tract articulators and source features, and
  then decode these features back into speech.
---

# Coding Speech through Vocal Tract Kinematics

## Quick Facts
- arXiv ID: 2406.12998
- Source URL: https://arxiv.org/abs/2406.12998
- Reference count: 40
- Primary result: Introduces SPARC, a novel articulatory speech coding framework achieving high intelligibility (5.43% WER) and quality through kinematic traces of vocal tract movements

## Executive Summary
This paper introduces SPARC (Speech Articulatory Coding), a novel framework for speech coding that encodes speech as articulatory features - kinematic traces of vocal tract articulators and source features - rather than directly encoding acoustic signals. By leveraging the physiological grounding of speech production, SPARC aims to provide a more interpretable and controllable coding system compared to existing methods. The authors demonstrate a high-performance, universal articulatory encoder and decoder that can generalize across speakers, achieving fully intelligible, high-quality articulatory synthesis. The framework effectively disentangles speaker identity from articulations, enabling accent-preserving zero-shot voice conversion.

## Method Summary
The SPARC framework uses a three-stage pipeline: (1) a SSL-linear articulatory inversion model that maps WavLM features to EMA articulatory features, (2) a speaker identity encoder using WavLM CNN features with periodicity-weighted pooling, and (3) a HiFi-GAN vocoder conditioned on articulatory features and speaker embedding. The model is trained on large-scale speech data, with the articulatory space based on MNGU0 EMA data. The speaker identity is disentangled from articulations through FiLM conditioning, enabling accent-preserving voice conversion. The framework achieves high intelligibility and quality in resynthesized speech, with WER of 5.43% and 3.73% on LibriTTS-R and VCTK datasets respectively.

## Key Results
- Achieves high intelligibility with WER of 5.43% on LibriTTS-R and 3.73% on VCTK datasets
- Maintains high quality with MOS and UTMOS scores showing minimal degradation compared to ground truth
- Successfully disentangles speaker identity from articulations, enabling accent-preserving zero-shot voice conversion
- Demonstrates interpretability and controllability of place and manner of articulation in speech

## Why This Works (Mechanism)
SPARC leverages the physiological grounding of speech production by encoding speech as articulatory features rather than acoustic signals directly. This approach capitalizes on the universal nature of articulatory movements across speakers while allowing for speaker-specific variations through disentangled identity features. The framework uses SSL features as an intermediate representation that captures linguistic content, which is then mapped to a physiologically meaningful articulatory space. By conditioning the vocoder on both articulatory features and speaker identity, the model can reconstruct speech that preserves both intelligibility and speaker characteristics while maintaining controllability over articulation.

## Foundational Learning

**Articulatory Inversion**: The process of mapping acoustic speech signals to corresponding vocal tract configurations (EMA data). Why needed: Essential for extracting kinematic features from speech audio. Quick check: Correlation between predicted and ground truth EMA channels should exceed 0.8 for major articulators.

**Speaker Identity Disentanglement**: Separating speaker-specific characteristics from linguistic content in speech representations. Why needed: Enables voice conversion while preserving accent and articulation patterns. Quick check: Speaker recognition accuracy should remain high (>95%) when using embeddings from resynthesized speech.

**HiFi-GAN Conditioning**: Modifying the vocoder architecture to accept multiple conditioning inputs (articulatory features + speaker embedding). Why needed: Allows synthesis that combines controlled articulation with speaker identity. Quick check: MOS scores should be comparable to ground truth when both conditioning signals are present.

## Architecture Onboarding

**Component Map**: WavLM SSL features -> Linear articulatory mapping -> EMA articulatory features + Speaker embedding -> HiFi-GAN vocoder -> Resynthesized speech

**Critical Path**: SSL feature extraction → Articulatory inversion → Speaker embedding generation → HiFi-GAN synthesis

**Design Tradeoffs**: 
- Linear mapping chosen for interpretability and computational efficiency versus nonlinear mappings that might capture more complex relationships
- Single-speaker EMA template (MNGU0) provides universal articulatory space but may not capture all possible articulatory variations
- FiLM conditioning enables speaker disentanglement but adds complexity to the vocoder architecture

**Failure Signatures**: 
- Poor articulatory inversion: Low correlation between predicted and ground truth EMA channels
- Quality degradation: Significant drop in MOS scores compared to ground truth
- Speaker identity issues: Low speaker recognition accuracy on resynthesized speech

**First 3 Experiments**:
1. Train SSL-linear articulatory inversion model on MNGU0 EMA data using WavLM features
2. Implement speaker identity encoder using WavLM CNN features with periodicity-weighted pooling
3. Train HiFi-GAN vocoder conditioned on articulatory features and speaker embedding

## Open Questions the Paper Calls Out

**Open Question 1**: How does SPARC perform in extremely noisy or reverberant environments? The paper mentions improving robustness to noisy environments as future work, but current evaluation focuses on clean speech datasets without explicit testing in noisy conditions.

**Open Question 2**: Can SPARC effectively encode and synthesize expressive speech, including emotions and paralinguistic information? The paper states that scaling SPARC to incorporate expressive speech is a future direction, as the current implementation focuses on neutral speech.

**Open Question 3**: How does the choice of template articulatory space (MNGU0) impact performance and generalizability? The paper discusses using MNGU0 but doesn't explore alternative template spaces or analyze the impact of different choices on model performance.

## Limitations

- Relies on EMA data that captures only mid-sagittal vocal tract movements, missing many articulatory features
- Linear mapping from SSL to EMA assumes consistent acoustic-articulatory relationships that may not hold across all speakers
- Datasets used (LibriTTS-R, VCTK) represent controlled recording conditions and may not reflect spontaneous speech variability
- Speaker identity disentanglement may not fully capture complex interplay between speaker characteristics and articulatory patterns

## Confidence

**High confidence**: Articulatory inversion performance (correlation metrics, MSE values) and resynthesis quality (MOS, UTMOS scores) are well-supported by quantitative metrics and represent the most robust findings.

**Medium confidence**: Speaker recognition accuracy and speaker identity disentanglement results are methodologically sound but depend on specific evaluation protocols and datasets used.

**Medium confidence**: Interpretability and controllability demonstrations are primarily shown through qualitative examples and would benefit from more systematic user studies.

## Next Checks

1. Test the framework's performance on spontaneous or noisy speech data to evaluate robustness beyond controlled datasets like LibriTTS-R and VCTK.

2. Conduct perceptual studies with speech scientists and clinicians to validate interpretability claims and assess practical utility of articulatory control features.

3. Implement and evaluate proposed extensions for real-time applications, including impact of different frame rates on intelligibility and computational requirements for streaming scenarios.