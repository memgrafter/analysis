---
ver: rpa2
title: A layer-wise analysis of Mandarin and English suprasegmentals in SSL speech
  models
arxiv_id: '2408.13678'
source_url: https://arxiv.org/abs/2408.13678
tags:
- english
- mandarin
- tone
- representations
- stress
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how self-supervised speech models represent
  suprasegmental categories like Mandarin lexical tone, English lexical stress, and
  English phrasal accents. The authors use probing tasks to analyze layer-wise representations
  in English and Mandarin 12-layer monolingual models.
---

# A layer-wise analysis of Mandarin and English suprasegmentals in SSL speech models

## Quick Facts
- **arXiv ID**: 2408.13678
- **Source URL**: https://arxiv.org/abs/2408.13678
- **Reference count**: 0
- **Primary result**: Self-supervised speech models develop abstract, contextual representations of suprasegmental categories strongest in middle transformer layers, with language-specific information emerging from enriched transformer context rather than local acoustic features.

## Executive Summary
This study investigates how self-supervised speech models represent suprasegmental categories like Mandarin lexical tone, English lexical stress, and English phrasal accents through layer-wise probing analysis. Using English and Mandarin 12-layer monolingual models (wav2vec 2.0, HuBERT, WavLM), the authors find that models learn contextual representations of these features strongest in the middle third of the network, independent of their ability to track F0 accurately. Language-specific representations emerge from enriched transformer context rather than local acoustic encoding, and fine-tuning for ASR particularly improves performance on lexically contrastive features in later layers.

## Method Summary
The study employs linear probes with L1 regularization to analyze layer-wise representations in English and Mandarin SSL speech models. Researchers extract embeddings from each layer (0-12) of wav2vec 2.0, HuBERT, and WavLM models, then train frame-by-frame probes on aligned annotations from English (Switchboard) and Mandarin (Global TIMIT Mandarin-Chinese) corpora. Models are evaluated in both pre-trained and fine-tuned (ASR) conditions, with probe performance measured using F1 scores for classification tasks and R-squared for F0 regression.

## Key Results
- Model representations of suprasegmental categories are strongest in the middle third of the transformer network
- Language-specific representations emerge from enriched transformer context, not local acoustic features in the CNN
- Fine-tuned models show improved performance in later layers for lexically contrastive features like tone and stress
- HuBERT and WavLM learn similar representations to wav2vec 2.0, differing mainly in later layer performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Suprasegmental representations are abstract and not directly tied to local acoustic features like F0
- Mechanism: Probing classifiers perform best in middle transformer layers while F0 regression performance fluctuates independently, indicating suprasegmental knowledge emerges from contextual processing rather than raw acoustic tracking
- Core assumption: Models learn categorical, linguistically-meaningful representations that abstract away from exact pitch values
- Evidence anchors: Abstract states model representations are independent of F0 tracking; section shows F0 regression fluctuates independently through context network
- Break condition: If F0 regression and suprasegmental classification peaks occurred in same layer, or if middle-layer classifiers failed when controlling for F0

### Mechanism 2
- Claim: Language-specific suprasegmental representations are built through enriched transformer context, not accurate acoustic encoding in CNN
- Mechanism: All models start with similar CNN embeddings (layer 0), but diverge in transformer layers with better performance on native language due to language-specific contextual enrichment
- Core assumption: Transformer blocks are where cross-lingual differences in suprasegmental encoding emerge
- Evidence anchors: Abstract states language-specificity driven by enriched context in transformer blocks; section shows layer 0 performance similar for all models, implying language-specific info encoded only in context network
- Break condition: If layer 0 representations already showed strong language-specific differences, or if transformer performance gains were uniform across languages

### Mechanism 3
- Claim: Fine-tuning improves suprasegmental representations in later layers, especially for lexically contrastive features (tone, stress), because ASR training implicitly encodes lexical identity
- Mechanism: Fine-tuning forces model to learn better lexical representations, boosting performance on features contrastive at word level but not encoded orthographically
- Core assumption: Lexical identity knowledge is implicitly encoded during ASR fine-tuning and benefits lexically contrastive suprasegmentals more than phrasal ones
- Evidence anchors: Abstract states fine-tuned wav2vec 2.0 improves performance in later layers for lexically contrastive features; section suggests fine-tuning emphasizes lexical information, benefiting both features which are lexical
- Break condition: If fine-tuning improved phrasal features (like accent) as much as lexical ones, or if improvements occurred uniformly across all layers

## Foundational Learning

- **Suprasegmental categories (tone, stress, pitch accent)**: These are the target linguistic phenomena the probes are designed to detect; understanding their definitions and cross-linguistic distribution is essential for interpreting probe results
  - Quick check: What distinguishes lexical tone from phrasal pitch accent in terms of linguistic function and distribution?

- **Layer-wise probing methodology**: The study's core analysis compares classifier performance across model layers to understand where and how suprasegmental knowledge is represented
  - Quick check: Why might a linear probe perform differently on early vs. middle vs. late layers in a transformer-based SSL model?

- **Masked prediction pre-training tasks (wav2vec 2.0, HuBERT, WavLM)**: Different pre-training objectives may shape layer-wise representation development differently; the study compares these to understand architectural vs. task-driven effects
  - Quick check: How might the quantization strategy in wav2vec 2.0 vs. clustering in HuBERT affect what information is preserved in intermediate layers?

## Architecture Onboarding

- **Component map**: Feature extractor (7-layer CNN) → Context network (12-layer transformer) → Output embeddings (768-dim)
- **Critical path**: Data → Model forward pass → Save layer embeddings → Align with annotations → Train probe → Evaluate per layer → Compare across languages/tasks
- **Design tradeoffs**: Probing with fixed hyperparameters (L1 regularization, C=1) sacrifices some accuracy for comparability; frame-by-frame probing vs. pooling trades granularity for simplicity
- **Failure signatures**: Poor probe convergence (especially in fine-tuned tone tasks); layer 0 vs. layer 1 performance drops (e.g., WavLM on stress/accent); mismatched peak layers between languages
- **First 3 experiments**:
  1. Run monolingual wav2vec 2.0 probe on layer 0 only for all three tasks to confirm cross-lingual CNN parity
  2. Probe all layers for English stress to identify peak layer and compare F0 regression trends
  3. Compare pre-trained vs. fine-tuned English model on tone task to test lexical vs. phrasal effect hypothesis

## Open Questions the Paper Calls Out

- **Open Question 1**: How do multilingual SSL models represent suprasegmentals compared to monolingual models?
  - Basis: The paper notes that multilingual models exhibit similar behavior regarding layer-wise language-specific representation development, but only examined monolingual models
  - Why unresolved: Study didn't directly compare multilingual approaches, leaving open how cross-linguistic training affects suprasegmental representation
  - What evidence would resolve it: Direct comparison of suprasegmental probe performance across layers in multilingual vs monolingual models

- **Open Question 2**: Does conditioning out lexical information improve phrasal accent representation in SSL models?
  - Basis: The paper suggests phrasal context divergence may be due to architectural limitation on context width for phrasal vs lexical suprasegmentals
  - Why unresolved: Study used frame-by-frame probing without controlling for lexical context, leaving open whether lexical information confounds phrasal accent representation
  - What evidence would resolve it: Probing experiments that control for or condition out lexical identity when predicting phrasal accents

- **Open Question 3**: Are the best-performing layers for suprasegmental tasks consistent across different SSL model architectures?
  - Basis: The paper found English and Mandarin wav2vec 2.0 models learn contextual representations strongest in middle third of network
  - Why unresolved: While examining three model types, the study didn't systematically compare whether optimal layers for different suprasegmental tasks are consistent across architectures
  - What evidence would resolve it: Layer-wise probe performance comparison across multiple SSL architectures to determine if "middle third" finding generalizes

## Limitations

- Limited probe architecture control through fixed linear probe hyperparameters (L1 regularization, C=1) sacrifices accuracy for comparability
- Weak evidence for mechanism claims, particularly the inference that fine-tuning improves lexical suprasegmental representations through implicit lexical identity encoding
- Potential annotation alignment issues in frame-by-frame probing approach that could create noise affecting layer-wise patterns

## Confidence

- **High confidence**: Finding that model representations of suprasegmental categories are strongest in middle third of network is well-supported by consistent probe performance patterns across multiple models, tasks, and languages
- **Medium confidence**: Claim that language-specific representations emerge from transformer context rather than CNN embeddings is reasonably supported by layer 0 parity across languages
- **Low confidence**: Mechanism explaining why fine-tuning particularly benefits lexically contrastive features through implicit lexical identity encoding remains speculative without direct validation

## Next Checks

1. **Probe architecture ablation study**: Test sensitivity of layer-wise patterns to probe architecture choices (linear vs. nonlinear, different regularization strengths, multi-layer probes) to determine if observed patterns are robust to probe design

2. **Cross-lingual fine-tuning experiment**: Fine-tune English models on Mandarin ASR and vice versa to directly test whether improvements in suprasegmental representation require language-matched fine-tuning, or if lexical identity encoding transfers across languages

3. **Layer 0 CNN representation analysis**: Conduct direct comparison of CNN embeddings across languages using additional probes or similarity metrics to validate that layer 0 representations are indeed language-neutral before transformer processing