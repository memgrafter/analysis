---
ver: rpa2
title: Generative Information Retrieval Evaluation
arxiv_id: '2404.08137'
source_url: https://arxiv.org/abs/2404.08137
tags:
- information
- retrieval
- evaluation
- https
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This chapter explores generative information retrieval (GenIR)
  evaluation from two perspectives: using large language models (LLMs) for evaluation
  and evaluating GenIR system outputs. LLMs show promise in generating relevance judgments
  and query variants, potentially reducing evaluation costs.'
---

# Generative Information Retrieval Evaluation

## Quick Facts
- arXiv ID: 2404.08137
- Source URL: https://arxiv.org/abs/2404.08137
- Reference count: 40
- Primary result: LLMs show promise for evaluation but require careful validation and new evaluation models beyond traditional ranking

## Executive Summary
This chapter examines generative information retrieval (GenIR) evaluation through two key perspectives: using large language models (LLMs) for evaluation and evaluating GenIR system outputs. LLMs demonstrate potential in generating relevance judgments and query variants, which could significantly reduce evaluation costs while maintaining accuracy. The chapter highlights the need for new evaluation models that go beyond traditional document ranking, particularly for GenIR systems that combine retrieval and generation components. It proposes viewing LLM-based assessment as "slow search" for evaluating faster production systems and emphasizes the continuing importance of human assessment in the evaluation pipeline.

## Method Summary
The paper explores LLM-based evaluation methodologies including using LLMs for generating relevance judgments, creating query variants, and implementing nugget-based evaluation for GenIR systems. It proposes a "slow search" framework where LLMs serve as reference standards for evaluating faster production systems. The methodology involves creating test collections with LLM-generated relevance judgments, evaluating RAG system components using both traditional metrics and LLM assessment, and implementing nugget-based evaluation to address the limitations of traditional relevance judgments in GenIR contexts.

## Key Results
- LLMs can generate relevance judgments as accurate as human assessors at lower cost
- LLM-based evaluation enables "slow search" methodology for assessing production systems
- Traditional document ranking evaluation models are insufficient for GenIR systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generate relevance judgments that are as accurate as human assessors at lower cost.
- Mechanism: LLMs simulate human judgment patterns by processing document-query pairs through pre-trained language understanding, producing binary or graded relevance labels.
- Core assumption: LLM outputs align sufficiently with human assessors' decisions when evaluated on the same document-query pairs.
- Evidence anchors:
  - [abstract] "LLMs may be superior to crowdsource workers and other paid assessors on basic relevance judgement tasks"
  - [section 2.2] "researchers at Microsoft Bing announced their use of GPT-4 in generating relevance labels, which was later shared in a paper [70]. The LLM generated labels were found to be as accurate as labels created by crowd source workers"
  - [corpus] Weak - no specific citation data in neighbor papers yet
- Break condition: If LLM assessments systematically diverge from human assessors on edge cases or domain-specific relevance criteria.

### Mechanism 2
- Claim: LLM-based evaluation creates "slow search" that enables better assessment of fast production systems.
- Mechanism: LLMs can take more time and computational resources to generate ideal or near-ideal responses, which are then used as reference standards to evaluate the efficiency-effectiveness trade-offs of faster systems.
- Core assumption: Taking more time allows LLMs to perform more thorough query reformulation, document analysis, and synthesis, producing higher quality responses.
- Evidence anchors:
  - [abstract] "by viewing LLM-based assessment as a form of 'slow search', where a slower IR system is used for evaluation and training of a faster production IR system"
  - [section 2.8] "Teevan et al. [69] in advocating for 'Slow Search' write, 'With even just a little extra time to invest, search engines can relax existing restrictions to improve search result quality.'"
  - [corpus] Weak - no direct citations about slow search methodology
- Break condition: If the time invested in LLM-based evaluation does not produce substantially better reference responses that justify the additional cost.

### Mechanism 3
- Claim: LLM-based query generation can simulate diverse user query variants, improving evaluation robustness.
- Mechanism: LLMs generate multiple query formulations for the same information need, reflecting different user properties like domain expertise, device usage, age, and language proficiency.
- Core assumption: LLM-generated query variants accurately represent the distribution of actual user queries across different demographics and contexts.
- Evidence anchors:
  - [section 2.3] "research has shown that LLMs can, to a limited extent, reproduce human query variants, yielding a similar pool of documents of that obtained by using human generated query variants [4]"
  - [section 2.3] "research has demonstrated that factors – such as the used device [26, 41], domain expertise [51, 81], age [73, 14], and language proficiency [25] – influence query formulation"
  - [corpus] Weak - neighbor papers focus on different aspects of GenIR
- Break condition: If LLM-generated queries fail to capture important aspects of human query formulation or introduce systematic biases.

## Foundational Learning

- Concept: Test Collection Construction
  - Why needed here: Understanding how test collections work is essential for grasping how LLM-based evaluation changes traditional IR evaluation paradigms
  - Quick check question: What are the three main components of a traditional test collection and why is each important?

- Concept: Relevance Judgment Consistency
  - Why needed here: The paper discusses LLM consistency vs human inconsistency, which is central to the evaluation methodology shift
  - Quick check question: How do human assessors' judgments vary across time and similar documents, and why does this matter for evaluation?

- Concept: Retrieval Augmented Generation (RAG) Architecture
  - Why needed here: The paper extensively discusses evaluating RAG systems, which combine retrieval and generation components
  - Quick check question: What are the key components of a RAG system and how do they interact during query processing?

## Architecture Onboarding

- Component map:
  - Evaluation Pipeline: Human Assessment → LLM Assessment → Reference Generation
  - RAG System: Query → Retrieval Component → Generative Component → Response
  - Corpus Management: Document Collection → Filtering → Nugget Extraction
  - Simulation Layer: User Properties → Query Generation → Session Simulation

- Critical path: Document relevance assessment → Query variant generation → System response evaluation → Nugget-based comparison

- Design tradeoffs:
  - Speed vs. Accuracy: LLM assessment is slower but potentially more thorough than human assessment
  - Cost vs. Coverage: Automated assessment enables larger test collections but may miss nuanced relevance judgments
  - Black-box vs. Explainable: LLM-based evaluation is less transparent than human assessment

- Failure signatures:
  - Systematic bias in LLM relevance judgments that differs from human patterns
  - Over-reliance on LLM assessment leading to evaluation circularity
  - Failure to capture nuanced relevance dimensions that humans consider

- First 3 experiments:
  1. Compare LLM-generated relevance judgments against human judgments on a standard test collection to measure alignment
  2. Generate multiple query variants using LLMs for the same information needs and measure impact on retrieval effectiveness
  3. Implement nugget-based evaluation using LLMs to extract and match nuggets against documents, comparing results with traditional relevance judgments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be effectively used to generate query variants that accurately reflect diverse user behaviors and demographic influences?
- Basis in paper: [explicit] The paper discusses the impact of query variants on system effectiveness and suggests that LLMs might help generate these variants cost-effectively.
- Why unresolved: While LLMs show potential, the paper notes that research on their use for query variants is in its infancy, and there's uncertainty about how well they can simulate human behavior and diverse user properties.
- What evidence would resolve it: Comparative studies evaluating the effectiveness of LLM-generated query variants against human-generated ones, considering various user demographics and contexts.

### Open Question 2
- Question: What are the best practices for using LLMs to create nugget-based test collections for evaluating GenIR systems?
- Basis in paper: [explicit] The paper proposes using nuggets as atomic units of relevance for GenIR evaluation and mentions the potential of LLMs to extract and match nuggets.
- Why unresolved: While the concept is promising, the paper doesn't provide detailed methodologies or best practices for implementing nugget-based evaluation using LLMs.
- What evidence would resolve it: Case studies or experimental results demonstrating effective nugget extraction and matching using LLMs, along with validation of their alignment with human relevance judgments.

### Open Question 3
- Question: How can we effectively evaluate and mitigate hallucinations in GenIR system outputs?
- Basis in paper: [explicit] The paper discusses the potential for GenIR systems to hallucinate misinformation and emphasizes the need for accuracy evaluation in outputs.
- Why unresolved: The paper acknowledges the challenge but doesn't provide specific solutions or evaluation methods for detecting and mitigating hallucinations in GenIR outputs.
- What evidence would resolve it: Development and testing of evaluation metrics and tools specifically designed to detect hallucinations in GenIR outputs, along with successful mitigation strategies.

## Limitations

- Limited empirical validation of LLM-based relevance judgments across diverse domains
- Uncertainty about LLM ability to accurately simulate human query variants across different user demographics
- Lack of detailed implementation guidelines for nugget-based evaluation methodology

## Confidence

- **LLM-based relevance judgment accuracy**: Medium confidence. Supported by external industry reports but lacking comprehensive peer-reviewed validation across diverse domains and query types.
- **LLM-generated query variants improving evaluation robustness**: Low confidence. Based on limited research showing "to a limited extent" reproduction of human variants, with no evidence of superiority over human-generated queries.
- **"Slow search" evaluation framework**: Medium confidence. Conceptually sound but lacks empirical validation of the efficiency-effectiveness trade-offs claimed.

## Next Checks

1. **Validation Study**: Conduct a controlled experiment comparing LLM-generated relevance judgments against human judgments across multiple domains, using standard agreement metrics (e.g., Cohen's kappa) and measuring consistency across different LLM models and prompting strategies.

2. **Query Generation Evaluation**: Test LLM-generated query variants by measuring their impact on retrieval effectiveness compared to human-generated variants, using both pooled document coverage and relevance judgment quality as metrics.

3. **RAG System Assessment**: Implement and evaluate a RAG system using both traditional document ranking metrics and the proposed nugget-based evaluation approach, measuring hallucination rates and factuality of generated responses.