---
ver: rpa2
title: Learning Unlabeled Clients Divergence for Federated Semi-Supervised Learning
  via Anchor Model Aggregation
arxiv_id: '2407.10327'
source_url: https://arxiv.org/abs/2407.10327
tags:
- clients
- learning
- semianagg
- unlabeled
- client
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of aggregating unreliable models
  from unlabeled clients in federated semi-supervised learning (FedSemi). The authors
  propose SemiAnAgg, a novel method that learns the importance of unlabeled clients
  using a consistently initialized anchor model.
---

# Learning Unlabeled Clients Divergence for Federated Semi-Supervised Learning via Anchor Model Aggregation

## Quick Facts
- arXiv ID: 2407.10327
- Source URL: https://arxiv.org/abs/2407.10327
- Authors: Marawan Elbatel; Hualiang Wang; Jixiang Chen; Hao Wang; Xiaomeng Li
- Reference count: 40
- Key outcome: Achieves 9% accuracy increase on CIFAR-100 and 7.6% recall improvement on ISIC-18 in federated semi-supervised learning

## Executive Summary
This paper addresses the challenge of aggregating unreliable models from unlabeled clients in federated semi-supervised learning (FedSemi). The authors propose SemiAnAgg, a novel method that learns the importance of unlabeled clients using a consistently initialized anchor model. By comparing feature representations of unlabeled data with the anchor model, SemiAnAgg assigns higher weights to clients with more diverse and informative data. The method achieves state-of-the-art results on four FedSemi benchmarks, including significant improvements on CIFAR-100 and the medical dataset ISIC-18.

## Method Summary
SemiAnAgg introduces a consistently initialized anchor model for each client to measure data diversity. During training, clients store feature representations of their unlabeled data using this anchor model. Client weights are computed based on the cosine similarity between anchor and global model features, with lower similarity indicating higher diversity and receiving higher weights. The method also introduces FedAvg-Semi, a stronger baseline that disentangles aggregation for labeled and unlabeled clients. Together, these innovations improve model aggregation in FedSemi by emphasizing informative clients and avoiding bias from unreliable pseudo-labels.

## Key Results
- 9% accuracy increase on CIFAR-100 compared to state-of-the-art methods
- 7.6% improvement in recall on the medical dataset ISIC-18
- Achieves SOTA performance on four FedSemi benchmarks including SVHN, CIFAR-100, CIFAR-100LT, and ISIC-18
- FedAvg-Semi baseline achieves comparable performance to existing SOTA methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using a consistently initialized anchor model allows measurement of client diversity by comparing feature representations between anchor and global model.
- Mechanism: For each client, compute feature representations on unlabeled data using both the current global model and the same anchor model (randomly initialized at the start). Clients whose features are more distant from the anchor model are considered more diverse and informative.
- Core assumption: The optimal feature representation for client data should be distant from random, and inter-client variability can be measured consistently when all clients use the same anchor model.
- Evidence anchors:
  - [abstract]: "Our key idea is that by feeding local client data to the same global model and the same consistently initialized anchor model (i.e., random model), we can measure the importance of each unlabeled client accordingly."
  - [section]: "Specifically, our intuition is two-fold: 1) The optimal feature representation for client data should be distant from random. Therefore, clients contributing to a more distant step are more likely to the global optima. 2) Since all clients share the same global and anchor model, the inter-client data variability can be measured consistently."

### Mechanism 2
- Claim: SemiAnAgg effectively reweights unlabeled clients during aggregation based on their diversity, avoiding bias toward unreliable pseudo-labels.
- Mechanism: Compute a weight for each unlabeled client based on the average cosine similarity between anchor model features and global model features. Clients with lower similarity (more diverse) receive higher weights. These weights are normalized across clients and used in the aggregation formula.
- Core assumption: Lower similarity to the anchor model indicates higher diversity and more informative data, especially when pseudo-labels become reliable.
- Evidence anchors:
  - [abstract]: "By comparing the feature representations of unlabeled data with the anchor model, SemiAnAgg assigns higher weights to clients with more diverse and informative data."
  - [section]: "A high ˆwc indicates that the class representation of the client is close to random. Consequently, clients with lower ˆwc are more likely to approximate the global optimum... rc = 1−ˆwc reflects the class importance of each client based on diversity measurement."

### Mechanism 3
- Claim: FedAvg-Semi, which disentangles aggregation based on labeled vs. unlabeled data, serves as a strong baseline by aligning with semi-supervised learning optimization principles.
- Mechanism: Instead of traditional FedAvg, aggregate labeled and unlabeled clients separately using their respective data sizes, with weights that sum to 1. This avoids skewing the global model toward potentially incorrect pseudo-labels from unlabeled clients.
- Core assumption: Semi-supervised learning requires balancing empirical risks from labeled and unlabeled data, which should be reflected in the global optimization objective.
- Evidence anchors:
  - [abstract]: "we present a stronger baseline for FedSemi termed FedAvg-Semi. Figure 1 shows that our simple baseline, FedAvg-Semi, can achieve comparable performance to SOTA of FedSemi, CBAFed (Li et al., 2023)."
  - [section]: "Unlike existing FedSemi approaches that aggregate clients on the server based on traditional FedAvg, we disentangle the aggregation based on labeled and unlabeled data on the server to write a more generic form, termed FedAvg-Semi."

## Foundational Learning

- Concept: Federated Learning and Federated Semi-Supervised Learning
  - Why needed here: The paper operates within the federated learning framework, specifically addressing the semi-supervised variant where some clients have only unlabeled data. Understanding the federated learning paradigm and its challenges (data privacy, heterogeneity, communication efficiency) is essential.
  - Quick check question: What is the key difference between traditional federated learning and federated semi-supervised learning?

- Concept: Pseudo-labeling and its reliability in semi-supervised learning
  - Why needed here: Unlabeled clients rely on pseudo-labels generated by the global model. The reliability of these pseudo-labels affects client weighting and overall performance. Understanding how pseudo-labels are generated and their potential errors is crucial.
  - Quick check question: How are pseudo-labels generated for unlabeled clients, and what factors affect their reliability?

- Concept: Client drift and heterogeneity in federated learning
  - Why needed here: The paper addresses client drift due to heterogeneous class distributions and erroneous pseudo-labels. Understanding how data heterogeneity across clients impacts model training and aggregation is fundamental.
  - Quick check question: What are the main sources of client drift in federated learning, and how do they affect model performance?

## Architecture Onboarding

- Component map:
  Local client training -> Anchor model initialization -> Feature dictionary storage -> Client weighting computation -> Global aggregation (FedAvg-Semi)

- Critical path:
  1. Initialize anchor model and store features for unlabeled data.
  2. Perform local training using FlexMatch.
  3. Compute client weights based on feature similarity.
  4. Aggregate models on server using FedAvg-Semi with client weights.
  5. Repeat until convergence.

- Design tradeoffs:
  - Storage overhead: Requires storing anchor model features for unlabeled data (manageable compared to dataset size).
  - Privacy: Shares only scalar diversity weights, not raw features or labels, preserving privacy.
  - Computational complexity: Maintains O(NE) per client per round, same as baselines.

- Failure signatures:
  - If anchor model initialization varies across clients, client weighting becomes inconsistent.
  - If pseudo-labels are highly unreliable (early rounds), diversity measurement may not reflect true informativeness.
  - If class distribution mismatch is extreme, size-based weighting in FedAvg-Semi may still be suboptimal.

- First 3 experiments:
  1. Reproduce FedAvg-Semi baseline on a simple dataset (e.g., SVHN) to verify disentangled aggregation works.
  2. Implement SemiAnAgg with anchor model and verify client weighting changes over rounds.
  3. Compare SemiAnAgg vs. FedAvg-Semi on a balanced dataset to observe impact of diversity-based weighting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SemiAnAgg perform when using different types of anchor models, such as supervised models or models trained with different objectives?
- Basis in paper: [explicit] The paper mentions using a consistently initialized random anchor model, but does not explore other types of anchor models.
- Why unresolved: The paper focuses on the use of a random anchor model and does not investigate the impact of using other anchor model types.
- What evidence would resolve it: Experimental results comparing the performance of SemiAnAgg using different types of anchor models, such as supervised models or models trained with different objectives.

### Open Question 2
- Question: What is the impact of using different feature similarity metrics in SemiAnAgg, such as Euclidean distance or cosine similarity?
- Basis in paper: [explicit] The paper uses cosine similarity to measure feature similarity between the anchor model and global model features, but does not explore other similarity metrics.
- Why unresolved: The paper does not investigate the impact of using different feature similarity metrics on the performance of SemiAnAgg.
- What evidence would resolve it: Experimental results comparing the performance of SemiAnAgg using different feature similarity metrics, such as Euclidean distance or cosine similarity.

### Open Question 3
- Question: How does SemiAnAgg perform in scenarios with a larger number of clients or more severe class imbalance?
- Basis in paper: [inferred] The paper presents results on datasets with a limited number of clients and class imbalance, but does not explore scenarios with a larger number of clients or more severe class imbalance.
- Why unresolved: The paper does not investigate the performance of SemiAnAgg in scenarios with a larger number of clients or more severe class imbalance.
- What evidence would resolve it: Experimental results on datasets with a larger number of clients or more severe class imbalance, comparing the performance of SemiAnAgg with other FedSemi methods.

## Limitations
- Limited evaluation scope: Results are validated on only four benchmark datasets, which may not capture the full diversity of real-world federated learning scenarios.
- Theoretical foundation: The connection between anchor model distance and data diversity relies on empirical observations rather than rigorous proofs.
- Computational overhead: While claimed to be manageable, the storage and computational overhead of maintaining anchor model features is not explicitly quantified.

## Confidence
- High Confidence: The FedAvg-Semi baseline consistently outperforms traditional FedAvg across all experiments, validating the importance of disentangling aggregation for labeled and unlabeled clients.
- Medium Confidence: The SemiAnAgg weighting mechanism effectively identifies informative clients, as evidenced by improved performance over FedAvg-Semi. However, the exact relationship between anchor model distance and data diversity needs further theoretical grounding.
- Medium Confidence: The 9% CIFAR-100 accuracy improvement and 7.6% ISIC-18 recall enhancement are significant, but the limited dataset scope reduces generalizability confidence.

## Next Checks
1. Evaluate SemiAnAgg on additional federated learning benchmarks with varying degrees of data heterogeneity and class imbalance to assess generalizability.
2. Develop formal proofs or bounds connecting anchor model feature distance to data informativeness and diversity metrics.
3. Measure and report the exact storage and computational overhead of maintaining anchor model features across different dataset sizes and client counts.