---
ver: rpa2
title: A Critical Evaluation of AI Feedback for Aligning Large Language Models
arxiv_id: '2402.12366'
source_url: https://arxiv.org/abs/2402.12366
tags:
- rlaif
- feedback
- completions
- language
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper critically evaluates reinforcement learning with AI
  feedback (RLAIF) for aligning large language models. The authors show that improvements
  from RLAIF are largely due to using a stronger critic model (e.g., GPT-4) for AI
  feedback generation than the teacher model (e.g., GPT-3.5) used for supervised fine-tuning
  (SFT) data collection.
---

# A Critical Evaluation of AI Feedback for Aligning Large Language Models

## Quick Facts
- arXiv ID: 2402.12366
- Source URL: https://arxiv.org/abs/2402.12366
- Reference count: 20
- Authors: Archit Sharma; Sedrick Keh; Eric Mitchell; Chelsea Finn; Kushal Arora; Thomas Kollar
- Primary result: Improvements from RLAIF are largely due to using stronger critic models (GPT-4) compared to teacher models (GPT-3.5)

## Executive Summary
This paper critically evaluates reinforcement learning with AI feedback (RLAIF) for aligning large language models. The authors demonstrate that the perceived improvements from RLAIF are primarily attributable to using a stronger critic model (GPT-4) for AI feedback generation compared to the teacher model (GPT-3.5) used for supervised fine-tuning data collection. Surprisingly, simple supervised fine-tuning on GPT-4 completions outperforms existing RLAIF pipelines. The study reveals that RLAIF gains vary significantly across different base model families, test-time evaluation protocols, and critic models. The authors provide mechanistic explanations for when supervised fine-tuning may outperform RLAIF and suggest practical improvements for making RLAIF more effective in practice.

## Method Summary
The authors conducted a comprehensive evaluation comparing RLAIF against supervised fine-tuning (SFT) approaches. They tested multiple base model families including Mistral, LLaMA, and Pythia, using both GPT-3.5 and GPT-4 as critic models. The study employed different evaluation protocols including AlpacaEval and MT-bench, with and without majority voting. Key experiments included SFT on GPT-3.5, SFT on GPT-4, and various RLAIF configurations. The authors systematically analyzed when RLAIF provides benefits versus when simple SFT is superior, examining factors such as critic strength, base model characteristics, and evaluation methodology.

## Key Results
- Improvements from RLAIF are largely due to using stronger critic models (GPT-4) versus teacher models (GPT-3.5)
- Simple SFT on GPT-4 completions outperforms existing RLAIF pipelines
- RLAIF gains vary significantly across base model families, evaluation protocols, and critic models

## Why This Works (Mechanism)
The effectiveness of AI feedback approaches depends critically on the relative strength of the critic model compared to the teacher model used for data collection. When a stronger model (GPT-4) is used to generate feedback, it can identify and reward better responses than those collected during supervised fine-tuning with a weaker model (GPT-3.5). This creates an apparent improvement in model alignment that is actually due to the critic's superior capability rather than the reinforcement learning process itself. The mechanism reveals that alignment quality is fundamentally constrained by the strength of the feedback provider, and that simpler approaches like SFT on high-quality outputs can achieve comparable or better results.

## Foundational Learning
- **Reinforcement Learning with AI Feedback (RLAIF)**: A training paradigm where AI models provide feedback signals for reinforcement learning instead of human annotations. Needed to scale alignment beyond human feedback limitations. Quick check: Verify that reward model is trained on AI-generated preferences rather than human preferences.

- **Critic Model Strength**: The relative capability of the model providing feedback compared to the model being aligned. Critical because alignment quality is bounded by critic strength. Quick check: Compare critic model size and capability to base model being aligned.

- **Supervised Fine-Tuning (SFT) vs RLAIF**: Two different alignment approaches where SFT directly copies outputs from a stronger model while RLAIF uses reinforcement learning to optimize for critic preferences. Quick check: Compare final model outputs between SFT and RLAIF approaches.

- **Evaluation Protocol Sensitivity**: How alignment method performance varies depending on whether majority voting is used in evaluation. Important because it affects the reliability of performance comparisons. Quick check: Run evaluations with and without majority voting to assess consistency.

- **Base Model Family Characteristics**: Different model architectures and training approaches respond differently to alignment techniques. Crucial for understanding generalizability of findings. Quick check: Test alignment methods across multiple base model families.

## Architecture Onboarding

**Component Map:**
Base Model -> SFT/RLAIF Pipeline -> Reward Model (Critic) -> Aligned Model

**Critical Path:**
1. Data collection with teacher model (GPT-3.5)
2. AI feedback generation with critic model (GPT-4 or GPT-3.5)
3. Reward model training on AI preferences
4. Policy optimization via SFT or RLAIF

**Design Tradeoffs:**
- Using stronger critics improves alignment but increases computational cost
- SFT is simpler and more stable than RLAIF but may not capture complex preference structures
- RLAIF can potentially learn beyond the teacher's capabilities but requires careful reward modeling

**Failure Signatures:**
- RLAIF underperforms SFT when critic and teacher models have similar capabilities
- Poor alignment when reward model fails to capture critic preferences accurately
- Inconsistent results across different evaluation protocols

**First Experiments:**
1. Compare SFT on GPT-4 vs RLAIF using same GPT-4 critic
2. Test RLAIF with GPT-3.5 critic vs GPT-4 critic on same base model
3. Evaluate alignment performance across multiple base model families

## Open Questions the Paper Calls Out
The paper highlights several open questions regarding the broader applicability and limitations of their findings. These include whether the results generalize to non-English languages and specialized domains beyond general instruction following, the impact of reward model size on RLAIF performance, and how RLAIF performs on smaller base models that were not tested in this study. The authors also note the need for systematic safety alignment evaluations to determine whether RLAIF provides benefits beyond helpfulness and harmlessness that were not captured in their evaluation.

## Limitations
- Evaluation focused primarily on helpfulness and harmlessness alignment objectives, with potential gaps in safety alignment effectiveness
- Scope limited to instruction tuning on English datasets, may not generalize to other languages or specialized domains
- Did not explore RLAIF's performance on smaller base models or the impact of reward model size on outcomes

## Confidence
- **High**: Improvements from RLAIF are largely attributable to using stronger critic models (GPT-4) compared to teacher models (GPT-3.5), and simple SFT on GPT-4 completions can outperform existing RLAIF pipelines
- **Medium**: RLAIF gains vary across base model families and evaluation protocols, as this depends on specific experimental conditions tested
- **Low**: Broader claims about RLAIF's ineffectiveness in general, since evaluation focused on specific settings and objectives

## Next Checks
1. Test RLAIF with larger reward models to determine if reward model size impacts performance
2. Evaluate RLAIF's effectiveness on non-English languages and specialized domains to assess generalizability
3. Conduct systematic safety alignment evaluations to determine whether RLAIF provides benefits beyond helpfulness and harmlessness that were not captured in this study