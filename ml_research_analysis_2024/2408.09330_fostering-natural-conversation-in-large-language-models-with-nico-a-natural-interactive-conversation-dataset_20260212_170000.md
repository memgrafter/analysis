---
ver: rpa2
title: 'Fostering Natural Conversation in Large Language Models with NICO: a Natural
  Interactive COnversation dataset'
arxiv_id: '2408.09330'
source_url: https://arxiv.org/abs/2408.09330
tags:
- llms
- dialog
- dataset
- arxiv
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NICO, a Chinese dialogue dataset designed
  to improve the naturalness of responses generated by Large Language Models (LLMs).
  The dataset covers 20 daily-life topics and 5 types of social interactions, ensuring
  a broad and realistic range of conversations.
---

# Fostering Natural Conversation in Large Language Models with NICO: a Natural Interactive COnversation dataset

## Quick Facts
- arXiv ID: 2408.09330
- Source URL: https://arxiv.org/abs/2408.09330
- Reference count: 19
- Authors: Renliang Sun; Mengyuan Liu; Shiping Yang; Rui Wang; Junqing He; Jiaxing Zhang
- Primary result: NICO dataset improves naturalness of LLM-generated conversations, especially when models are fine-tuned on it

## Executive Summary
This paper introduces NICO, a Chinese dialogue dataset designed to improve the naturalness of responses generated by Large Language Models (LLMs). The dataset covers 20 daily-life topics and 5 types of social interactions, ensuring a broad and realistic range of conversations. The dialogues were initially generated using GPT-4-turbo and then manually revised by workers to eliminate grammatical errors and unnatural utterances. The authors define four tasks to evaluate and improve LLMs' ability to engage in natural dialogue, identify unnatural sentences, and rewrite them. Experiments with multiple open-source and closed-source LLMs show that NICO helps improve the naturalness of responses, especially when models are fine-tuned on the dataset. However, LLMs still struggle with conflict-type dialogues, and identifying unnatural sentences remains challenging. Overall, NICO demonstrates its potential to enhance the naturalness of LLM-generated conversations.

## Method Summary
The NICO dataset was created through a multi-step process: first, GPT-4-turbo generated initial dialogues covering 20 topics and 5 social interaction types; second, human annotators manually revised these dialogues to correct grammatical errors and unnatural utterances; finally, the cleaned dataset was used to fine-tune various LLMs including LLaMA3, Qwen, and InternLM. The authors evaluated the models using multiple metrics including BLEU, ROUGE, Distinct, and BERTScore across four tasks: open-domain dialogue generation, naturalness detection, sentence rewriting, and interactive conversation. The dataset construction emphasized covering all five social interaction types (exchange, cooperation, conformity, coercion, conflict) as essential for realistic dialogue.

## Key Results
- Fine-tuning LLaMA3 on NICO significantly improves naturalness of responses compared to zero-shot baselines
- Models still perform poorly on conflict-type dialogues, indicating remaining challenges in simulating human-like conflict
- Human annotation is essential for dataset quality, as unannotated dialogues contain high rates of grammatical errors and unnatural utterances
- LLaMA3 outperforms both open-source and closed-source alternatives when fine-tuned on NICO

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning LLMs on NICO improves naturalness of dialogue responses compared to zero-shot baselines.
- Mechanism: Supervised fine-tuning with manually corrected dialogues teaches the model to generate more colloquial and contextually appropriate responses.
- Core assumption: The dataset's manually corrected dialogues accurately represent human-like conversation patterns.
- Evidence anchors:
  - [abstract] "Experiments with multiple open-source and closed-source LLMs show that NICO helps improve the naturalness of responses, especially when models are fine-tuned on the dataset."
  - [section 6.1] "The supervised fine-tuned LLaMA3 achieves the best results on both tasks, outperforming the closed-source LLMs."
  - [corpus] Weak - no direct corpus evidence linking to mechanism, but general pattern matches other dialogue datasets showing fine-tuning effectiveness.
- Break condition: If the manual corrections introduce biases that don't generalize to real human conversations.

### Mechanism 2
- Claim: Covering all five social interaction types in training data improves LLM ability to handle diverse conversation scenarios.
- Mechanism: Exposure to exchange, cooperation, conformity, coercion, and conflict types during training enables better generalization across real-world interaction patterns.
- Core assumption: All five interaction types are essential for simulating human-like conversations.
- Evidence anchors:
  - [section 3.1] "We ensure that the dataset covers all interaction types" and "high-quality daily dialog datasets need to cover all types of social interactions"
  - [section 7.2] "LLaMA3 demonstrates poor performance across all evaluation metrics in conflict dialogues" and "simulating human-like conflict dialogues remains a challenge"
  - [corpus] Moderate - related papers show multi-turn interaction frameworks improve reasoning, supporting the importance of interaction type coverage.
- Break condition: If certain interaction types are too rare in real usage to justify inclusion in training data.

### Mechanism 3
- Claim: Human annotation and correction of LLM-generated dialogues is necessary to ensure naturalness and quality.
- Mechanism: Manual review identifies and fixes grammatical errors and unnatural utterances that LLMs generate, creating higher quality training data.
- Core assumption: Human judgment is superior to automated methods for identifying unnatural language patterns.
- Evidence anchors:
  - [section 4.1] "more than one-third of the dialogs contain grammatical errors, and more than half of the dialogs contain unnatural utterances" in raw GPT-4-turbo output
  - [section 4.2] "LCCC without human filtering has the lowest scores, which suggests that human annotation is essential"
  - [corpus] Strong - multiple papers emphasize human annotation quality for dialogue datasets.
- Break condition: If automated quality control methods become sufficiently advanced to replace human annotation.

## Foundational Learning

- Concept: Social interaction types (exchange, cooperation, conformity, coercion, conflict)
  - Why needed here: Understanding these interaction types is crucial for both constructing appropriate training data and evaluating model performance across different conversation scenarios.
  - Quick check question: Can you provide a brief example of how a dialogue would differ between an exchange interaction versus a conflict interaction?

- Concept: Fine-tuning methodology for LLMs
  - Why needed here: The paper relies on supervised fine-tuning as the primary method for improving model performance, so understanding how fine-tuning works is essential.
  - Quick check question: What is the difference between zero-shot, few-shot, and fine-tuning approaches for LLMs?

- Concept: Dialogue evaluation metrics (BLEU, ROUGE, Distinct, BERTScore)
  - Why needed here: These metrics are used throughout the paper to quantify improvements in dialogue quality, so understanding what they measure is important.
  - Quick check question: Which metric would be most appropriate for measuring the diversity of responses in a dialogue system?

## Architecture Onboarding

- Component map: Data generation (GPT-4-turbo) → Human annotation → Dataset creation → Model fine-tuning → Evaluation
- Critical path: Human annotation is the bottleneck - the quality of the final model depends entirely on the quality of human corrections.
- Design tradeoffs: Using GPT-4-turbo for initial generation saves time but requires extensive human correction; using human writers from the start would be more expensive but potentially require less correction.
- Failure signatures: Models perform poorly on conflict dialogues, struggle to identify unnatural sentences, and may produce responses that are grammatically correct but contextually inappropriate.
- First 3 experiments:
  1. Fine-tune LLaMA3 on NICO and evaluate on open-domain dialogue task to verify improvement over zero-shot baseline
  2. Test model performance separately on each interaction type to identify which types remain challenging
  3. Attempt to fine-tune on only specific interaction types to determine which are most critical for overall performance improvement

## Open Questions the Paper Calls Out
None

## Limitations
- Data quality dependency: The entire approach relies heavily on human annotation quality, which introduces both cost and consistency concerns
- Generalizability gap: The dataset's effectiveness for languages other than Chinese remains untested
- Conflict dialogue challenge: Models perform poorly on conflict-type dialogues, suggesting the dataset may not fully address human conversational complexity

## Confidence
- **High Confidence**: The claim that fine-tuning on NICO improves dialogue naturalness compared to zero-shot baselines
- **Medium Confidence**: The assertion that covering all five social interaction types is essential for realistic dialogue
- **Low Confidence**: The claim that human annotation is strictly necessary for quality dialogue datasets

## Next Checks
1. Test whether fine-tuning LLaMA3 on NICO improves performance on English dialogue tasks, or vice versa, to validate cross-linguistic applicability
2. Replace human annotation with state-of-the-art automated quality assessment tools and compare resulting model performance to test necessity of manual review
3. Create subsets of NICO excluding conflict dialogues and compare model performance to quantify impact of poor conflict performance on overall quality