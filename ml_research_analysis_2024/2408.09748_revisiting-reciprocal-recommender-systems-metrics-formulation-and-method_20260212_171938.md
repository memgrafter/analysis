---
ver: rpa2
title: 'Revisiting Reciprocal Recommender Systems: Metrics, Formulation, and Method'
arxiv_id: '2408.09748'
source_url: https://arxiv.org/abs/2408.09748
tags:
- recommendation
- metrics
- recommendations
- causal
- reciprocal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper revisits reciprocal recommender systems (RRS) by introducing
  new evaluation metrics, a causal formulation, and a model-agnostic method. Traditional
  RRS evaluation separately assesses each side's performance, overlooking the coupling
  between both sides.
---

# Revisiting Reciprocal Recommender Systems: Metrics, Formulation, and Method

## Quick Facts
- **arXiv ID:** 2408.09748
- **Source URL:** https://arxiv.org/abs/2408.09748
- **Reference count:** 40
- **Primary result:** Introduces new evaluation metrics, causal formulation, and reranking method for reciprocal recommender systems that improve overall matching coverage

## Executive Summary
This paper addresses fundamental challenges in reciprocal recommender systems (RRS) where traditional evaluation metrics fail to capture the coupled nature of two-sided matching markets. The authors identify that separately evaluating each side's performance leads to double-counting of successful bilateral matches and overlooks coverage and stability considerations. They propose a comprehensive solution including five new metrics (coverage-adjusted recall/precision, stability-adjusted recall/precision, and balanced ranking NDCG) and a model-agnostic causal framework with reranking strategy that significantly improves overall matching performance.

## Method Summary
The proposed method introduces a model-agnostic Causal Reciprocal Recommender System (CRRS) that treats recommendations as bilateral interventions in a potential outcome framework. The approach involves pre-training backbone recommender models, counterfactual learning with different treatment assignments (recommending one side, both sides, or neither), and a reranking strategy to maximize matching outcomes. The method operates on two real-world datasets - recruitment and dating - and evaluates performance using both traditional and newly proposed metrics that account for coverage, bilateral stability, and population imbalance.

## Key Results
- Proposed metrics (CRecall, CPrecision, SRecall, SPrecision, RNDCG) provide more holistic evaluation than traditional per-side metrics
- CRRS method outperforms baseline approaches on overall matching coverage metrics
- Reranking strategy effectively maximizes matching outcomes as measured by proposed metrics
- Experimental results show improved performance on both recruitment and dating datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Coverage-adjusted metrics better capture overall system effectiveness
- Mechanism: Adjusts for redundant recommendations that occur when both sides recommend each other
- Core assumption: Bilateral matches should count as single successes, not double successes
- Evidence anchors:
  - [abstract] "These metrics provide a more holistic understanding of the system's effectiveness"
  - [section] "The primary goal should be the overall count of matching pairs"
- Break condition: If redundant recommendations are beneficial in certain domains

### Mechanism 2
- Claim: Causal framework better models reciprocal matching effects
- Mechanism: Estimates matching probabilities under different treatment combinations (10, 11, 01, 00)
- Core assumption: Similar users have similar treatment effects
- Evidence anchors:
  - [abstract] "formulating recommendations as bilateral interventions"
  - [section] "When considering a reciprocal recommendation scenario, we define outcomes as matching relationships"
- Break condition: If treatment effect similarity assumption breaks down

### Mechanism 3
- Claim: Reranking maximizes overall matching performance
- Mechanism: Selects optimal treatment strategy by comparing expected outcomes
- Core assumption: Best strategy can be determined by comparing expected outcomes
- Evidence anchors:
  - [abstract] "we introduce a reranking strategy to maximize matching outcomes"
  - [section] "The aim of IsMax is to ensure that the final recommendation method is targeted at maximizing overall matching performance"
- Break condition: If vacant slot estimation is poor

## Foundational Learning

- **Concept:** Causal inference and potential outcome framework
  - Why needed: To model how bilateral recommendations jointly affect matching outcomes
  - Quick check: What is the fundamental problem of causal inference when only one treatment assignment is observed per sample?

- **Concept:** Ranking metrics for imbalanced populations
  - Why needed: To ensure fair evaluation when sides have different user counts
  - Quick check: How does RNDCG differ from standard NDCG in handling population imbalance?

- **Concept:** Coverage-adjusted evaluation metrics
  - Why needed: To prevent double-counting of successful bilateral matches
  - Quick check: Why would counting a bilateral match as two separate successes lead to misleading evaluation results?

## Architecture Onboarding

- **Component map:** User features -> Three treatment models (10, 11, 01) -> Effect estimation -> Reranking -> Ranked recommendations
- **Critical path:**
  1. Pre-train three recommendation models using standard relevance labels
  2. Fine-tune each model using direction-specific labels under different treatments
  3. Estimate potential outcomes for each pair under all treatments
  4. Apply reranking strategy to select optimal treatment combination
  5. Generate final ranked recommendations
- **Design tradeoffs:**
  - Model complexity vs. generalization: Three separate models vs. single shared model
  - Coverage vs. stability: Maximizing matches may reduce bilateral recommendations
  - Computational cost vs. accuracy: Reranking requires evaluating multiple strategies
- **Failure signatures:**
  - Poor pre-training leads to unreliable potential outcome estimates
  - Imbalanced treatment data causes biased causal effect estimation
  - Vacant slot estimation errors propagate through reranking decisions
- **First 3 experiments:**
  1. Verify traditional metrics fail to distinguish between different recommendation strategies with identical matching counts
  2. Test CRRS with reranking achieves higher CRecall than without on synthetic dataset
  3. Compare single-sided metrics vs. proposed metrics on balanced dataset with equal user counts

## Open Questions the Paper Calls Out

- **Open Question 1:** How can redundant recommendations be effectively minimized without sacrificing coverage or stability metrics?
  - Basis: Authors identify redundancy reduction as essential but lack concrete solution
  - Why unresolved: Problem identified but no algorithmic solution proposed
  - Evidence needed: Experimental results comparing coverage/stability before/after redundancy reduction

- **Open Question 2:** How do causal formulation and reranking perform without ground truth relevance data?
  - Basis: Authors claim method works without labels but don't test this
  - Why unresolved: Experiments use datasets with available matching labels
  - Evidence needed: Comparative evaluation using only implicit feedback signals

- **Open Question 3:** Can proposed metrics be generalized to non-reciprocal recommendation scenarios?
  - Basis: Authors introduce metrics specifically for RRS but acknowledge similarities to existing metrics
- Why unresolved: Paper doesn't explore applicability to traditional recommendation tasks
  - Evidence needed: Application to standard recommendation benchmarks with conventional metrics comparison

## Limitations
- Causal formulation assumes similar treatment effects across users, which may not hold in practice
- Reranking strategy performance depends heavily on accurate vacant slot estimation
- Effectiveness of coverage-adjusted metrics relies on assumption that bilateral matches should not be double-counted

## Confidence
- **High confidence:** Need for better evaluation metrics in reciprocal systems (clear identification of double-counting issues)
- **Medium confidence:** Causal formulation's ability to improve recommendation quality (theoretical framework sound but empirical validation limited)
- **Medium confidence:** Overall method performance (results show improvement but with relatively small datasets)

## Next Checks
1. Conduct sensitivity analysis on vacant slot estimation accuracy and its impact on reranking performance
2. Test method on larger-scale datasets with different population imbalances to validate RNDCG effectiveness
3. Compare proposed metrics against alternative coverage-aware evaluation approaches to ensure unique value of specific formulations