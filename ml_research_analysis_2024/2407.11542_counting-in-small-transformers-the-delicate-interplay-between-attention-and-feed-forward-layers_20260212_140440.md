---
ver: rpa2
title: 'Counting in Small Transformers: The Delicate Interplay between Attention and
  Feed-Forward Layers'
arxiv_id: '2407.11542'
source_url: https://arxiv.org/abs/2407.11542
tags:
- token
- counting
- attention
- sftm
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies how transformer architecture design choices\
  \ affect their ability to count token frequencies in sequences. The authors analyze\
  \ a histogram task\u2014counting how many times each token appears in a sequence\u2014\
  across different transformer variants including linear mixing, dot-product attention\
  \ (with and without softmax), and a beginning-of-sequence token."
---

# Counting in Small Transformers: The Delicate Interplay between Attention and Feed-Forward Layers

## Quick Facts
- arXiv ID: 2407.11542
- Source URL: https://arxiv.org/abs/2407.11542
- Authors: Freya Behrens; Luca Biggio; Lenka Zdeborová
- Reference count: 40
- Primary result: Minor architectural changes in transformers (dot-product vs linear mixing, with/without softmax, with/without BOS token) dramatically impact their ability to count token frequencies, revealing complex interplay between attention and feed-forward layers

## Executive Summary
This paper studies how transformer architecture design choices affect their ability to count token frequencies in sequences. The authors analyze a histogram task—counting how many times each token appears in a sequence—across different transformer variants including linear mixing, dot-product attention (with and without softmax), and a beginning-of-sequence token. They identify two distinct counting strategies: relation-based counting (using dot-product attention to compare tokens locally) and inventory-based counting (memorizing the full token vocabulary in the feed-forward layer). Theoretical constructions show when perfect accuracy is possible, depending on embedding dimension d, hidden layer size p, and token mixing type. Empirical results confirm these regimes: dot-product models can implement relation-based counting efficiently, while linear mixing requires inventory-based counting with p ≥ T. The softmax activation provides robustness for small d by reducing noise from non-orthogonal embeddings.

## Method Summary
The authors study 1-layer transformers with embedding dimension d, ReLU feed-forward layers with p hidden neurons, and either dot-product attention or linear mixing for token mixing. They train models on the histogram task using Adam (lr=10^-3) for 500 epochs with batch size 32 and cross-entropy loss. The study examines four architectural variants: dot-product attention, dot-product with softmax, linear mixing, and linear mixing with softmax, plus a variant with beginning-of-sequence (BOS) token. Theoretical analysis provides bounds on when perfect accuracy is achievable based on d, p, and embedding orthogonality.

## Key Results
- Dot-product attention with BOS token can implement relation-based counting with only p=1 hidden neuron
- Linear mixing requires p ≥ T (alphabet size) to implement inventory-based counting through memorization
- Softmax activation provides robustness for small d by reducing noise from non-orthogonal embeddings
- Different architectures have distinct parameter regimes for achieving perfect counting accuracy
- Minor architectural changes significantly impact counting performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dot-product attention with a beginning-of-sequence (BOS) token enables relation-based counting with only one hidden neuron
- Mechanism: The BOS token's embedding is set to the sum of all token embeddings. In the attention matrix, the BOS at position 1 attends strongly to tokens at positions where xℓ = xm (same tokens) and weakly to different tokens. The dot-product ⟨¯x′ℓ, eBOS⟩ then yields T + histx(ℓ) + 1, providing a linear relationship between the mixed token and the count
- Core assumption: Token embeddings are mutually orthogonal with norm 1, and the BOS token is positioned at the first index
- Evidence anchors:
  - [abstract] "relation-based counting (using dot-product attention to compare tokens locally)"
  - [section 4.1.1] "a simple dot-product operation in the attention mechanism (with Q, K = d1/4 Id) will lead to an attention matrix with entries: aℓm = T if ℓ = m = 1; 1 if (ℓ > 1, m = 1) or (ℓ, m > 1, xℓ = xm); 0 if ℓ, m > 1, xℓ ̸= xm"
  - [corpus] Weak - no direct citation but related to transformer attention mechanisms
- Break condition: If embeddings are not orthogonal, or if the BOS token is not at position 1, the linear relationship breaks down

### Mechanism 2
- Claim: Linear mixing with sufficient hidden neurons (p ≥ T) enables inventory-based counting through memorization
- Mechanism: With p = T, each hidden neuron corresponds to one token in the alphabet. The mixing layer produces a position-independent sum of token embeddings, and the feed-forward layer extracts the count by comparing the mixed token to each token embedding separately
- Core assumption: Hidden layer size p is at least the alphabet size T, and embeddings are mutually orthogonal
- Evidence anchors:
  - [abstract] "inventory-based counting (memorizing the full token vocabulary in the feed-forward layer)"
  - [section 4.1.2] "When the feed-forward hidden layer has one neuron for each distinct token available in the alphabet, it can detect as many directions"
  - [corpus] Weak - related to feed-forward memorization in transformers
- Break condition: If p < T, the feed-forward layer cannot separately detect all token directions

### Mechanism 3
- Claim: Softmax activation provides robustness for small embedding dimensions by reducing noise from non-orthogonal embeddings
- Mechanism: The softmax function concentrates attention scores on the largest values, effectively driving attention scores for different tokens close to zero relative to same-token scores. This allows models to achieve perfect accuracy even when d < T
- Core assumption: The softmax temperature is high enough to sufficiently concentrate attention scores
- Evidence anchors:
  - [abstract] "The softmax activation provides robustness for small d by reducing noise from non-orthogonal embeddings"
  - [section 4.2] "The idea is that a softmax function with a high enough inverse temperature can non-linearly scale down the attention scores for different token pairs relative to those of the same tokens"
  - [corpus] Weak - softmax robustness is mentioned but not extensively explored
- Break condition: If the softmax temperature is too low or if finite precision computations cause numerical instability

## Foundational Learning

- Concept: Orthogonal embeddings and their properties
  - Why needed here: The theoretical constructions rely on mutually orthogonal token embeddings to enable perfect counting accuracy
  - Quick check question: What happens to the dot-product ⟨et, es⟩ when t ≠ s and embeddings are mutually orthogonal?

- Concept: Softmax function and inverse temperature
  - Why needed here: Softmax with high inverse temperature can reduce noise from non-orthogonal embeddings and enable counting with smaller d
  - Quick check question: How does increasing the inverse temperature of softmax affect the concentration of attention scores?

- Concept: Relation-based vs inventory-based counting strategies
  - Why needed here: Understanding these two distinct counting strategies explains why different architectures require different parameter regimes
  - Quick check question: What is the key difference between relation-based counting (using dot-product attention) and inventory-based counting (using feed-forward memorization)?

## Architecture Onboarding

- Component map:
  - Input sequence → Embedding layer (d dimensions) → Token mixing (attention or linear) → Residual connection → Feed-forward layer (p hidden neurons) → Output classification
  - Key architectural variants: dot-product attention vs linear mixing, with/without softmax, with/without BOS token

- Critical path:
  - Embedding quality → Token mixing mechanism → Feed-forward layer capacity → Output mapping
  - The bottleneck varies by architecture: dot-product attention can work with p=1, linear mixing requires p≥T

- Design tradeoffs:
  - Parameter efficiency vs robustness: dot-product with BOS token is most parameter-efficient but requires careful embedding design
  - Memory vs computation: inventory-based counting uses more parameters but is more robust to non-orthogonal embeddings
  - Softmax vs no softmax: softmax provides robustness but may require more precise computations

- Failure signatures:
  - Undercounting when embeddings are non-orthogonal and softmax is absent
  - Overcounting when d is too small relative to T
  - Poor performance when p < T for linear mixing architectures

- First 3 experiments:
  1. Test dot-product attention with BOS token on counting task with d=T and p=1 to verify relation-based counting
  2. Test linear mixing with p=T on counting task to verify inventory-based counting
  3. Test the effect of softmax on counting accuracy for d<T to verify robustness mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do dot+sftm models fail to implement relation-based counting even when they have the architectural capability, while dot models succeed?
- Basis in paper: [explicit] The paper shows dot+sftm requires p ≥ T while dot works with p = 1, despite both using dot-product attention. The authors note "the normalization effect of the softmax activation prevents the development of a meaningful counter subspace that is needed in the relation-based algorithm."
- Why unresolved: The theoretical construction for dot+sftm with p ≥ T uses inventory-based counting instead, but the fundamental reason why softmax breaks the relation-based mechanism remains unclear. The authors observe that dot+sftm "only starts performing close to 100% accuracy when both the model dimension d and the number of hidden neurons p are larger than the number of tokens T."
- What evidence would resolve it: Analyzing the singular value decomposition of learned W1 matrices for dot vs dot+sftm models, or systematically testing how different softmax temperatures affect the ability to implement relation-based counting.

### Open Question 2
- Question: What algorithm do models actually implement when both relation-based and inventory-based counting strategies are theoretically possible (p ≥ T, d ≥ T)?
- Basis in paper: [inferred] The authors note that "models that can implement relation-based counting for p = 1 can also implement the solutions for inventory-based counting for p ≥ T" and observe that "the model dot witnesses a very slight decrease in maximal learned performance from 100% accuracy to 99% despite its capacity being increased to p = T."
- Why unresolved: The singular value decomposition analysis shows "the largest T = 32 singular values are larger than the surplus singular values when p > T for models that can implement only IC" but "this behavior is less pronounced for models that can implement RC." It's unclear if models use one strategy, the other, or a superposition.
- What evidence would resolve it: Detailed mechanistic analysis of learned models in the overlapping regime, including intervention studies where model components are ablated or modified to test which counting strategy is essential.

### Open Question 3
- Question: How does the discrete nature of counting tasks enable perfect accuracy with d < T, and what are the fundamental limits of this robustness?
- Basis in paper: [explicit] The authors state "the discrete nature of the histogram task... makes the prediction inherently more robust to the effect of noise stemming from entangled embeddings" and provide theoretical bounds based on mutual coherence and softmax error reduction.
- Why unresolved: While the paper provides bounds on d for different architectures, the connection between task discreteness and robustness isn't fully characterized. The authors note that "as L grows, we require stronger concentration from the softmax by adjusting its temperature" and computational precision becomes limiting.
- What evidence would resolve it: Systematic experiments varying L, T, and d to map the exact boundary between solvable and unsolvable regimes, and analysis of how different error tolerances in the embedding space affect counting accuracy.

## Limitations
- Theoretical analysis assumes mutually orthogonal token embeddings, which rarely holds in practice
- Recursive data generation algorithm lacks sufficient detail for precise reproduction
- Study focuses exclusively on 1-layer transformers, limiting generalizability to deeper architectures
- Does not address how counting mechanisms interact across multiple attention heads

## Confidence

**High Confidence**: The identification of two distinct counting strategies (relation-based and inventory-based) and their relationship to architectural choices. The theoretical construction showing dot-product attention with BOS token requires only p=1 is well-supported by the mathematical analysis. Empirical results confirming that linear mixing requires p≥T for inventory-based counting are consistent across experiments.

**Medium Confidence**: The softmax robustness mechanism for enabling counting with d<T is theoretically sound but relies on unstated assumptions about temperature scaling and numerical precision. The claim that dot-product attention can achieve perfect accuracy with d=T and p=1 assumes perfectly orthogonal embeddings, which is a strong theoretical assumption not met in practical settings.

**Low Confidence**: The generalization of these findings to deeper transformers, different sequence lengths beyond the tested range, and real-world tokenization schemes. The analysis does not address how counting mechanisms might interact across multiple attention heads or transformer layers.

## Next Checks
1. **Test Non-Orthogonal Embeddings**: Implement experiments with non-orthogonal token embeddings (using random Gaussian embeddings) to validate whether the softmax mechanism provides the claimed robustness when d<T. Measure counting accuracy degradation as a function of embedding non-orthogonality.

2. **Multi-Head Extension**: Extend the analysis to multi-head attention transformers to test whether the relation-based counting mechanism scales across multiple heads or if head competition/interaction affects counting performance differently than in single-head settings.

3. **Deeper Architecture Validation**: Implement 2-3 layer transformers to test whether the counting strategies identified for 1-layer models persist in deeper architectures, or whether deeper models develop alternative counting mechanisms that bypass the architectural constraints identified in this work.