---
ver: rpa2
title: 'IRLab@iKAT24: Learned Sparse Retrieval with Multi-aspect LLM Query Generation
  for Conversational Search'
arxiv_id: '2411.14739'
source_url: https://arxiv.org/abs/2411.14739
tags:
- retrieval
- user
- query
- ptkb
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents IRLab\u2019s submission to the iKAT24 track,\
  \ which focuses on conversational search with personalized user knowledge. The authors\
  \ propose enhancing the MQ4CS framework by using multiple LLM-generated queries\
  \ for retrieval followed by single-query reranking, combined with learned sparse\
  \ retrieval (SPLADE) and ensemble reranking models."
---

# IRLab@iKAT24: Learned Sparse Retrieval with Multi-aspect LLM Query Generation for Conversational Search

## Quick Facts
- arXiv ID: 2411.14739
- Source URL: https://arxiv.org/abs/2411.14739
- Reference count: 24
- One-line primary result: Multi-aspect query generation with learned sparse retrieval outperforms both automatic and manual baselines on conversational search metrics

## Executive Summary
This paper presents IRLab's submission to the iKAT24 track, focusing on conversational search with personalized user knowledge. The authors propose enhancing the MQ4CS framework by using multiple LLM-generated queries for retrieval followed by single-query reranking, combined with learned sparse retrieval (SPLADE) and ensemble reranking models. Their approach demonstrates superior performance on most metrics compared to both automatic and manual baselines, validating the effectiveness of multi-aspect query generation for conversational search personalization.

## Method Summary
The system combines multi-aspect query generation using GPT-4's MQ4CS framework (producing φ=5 queries), learned sparse retrieval with SPLADE, and ensemble cross-encoder reranking models. The pipeline first generates multiple queries covering different aspects of user intent, retrieves passages using SPLADE, then re-ranks using a single query rewrite with an ensemble of cross-encoders (DebertaV3, DebertaV2, Roberta, Albert, Electra). Response generation uses GPT-4 with RAG, and PTKB statements are classified as relevant/irrelevant for each turn.

## Key Results
- Outperforms both automatic and manual baselines on most metrics including nDCG, MRR, and recall
- Multi-aspect query generation demonstrates effectiveness when integrated with advanced retrieval and reranking models
- Ensembling rankers leads to an average 2-point increase across all metrics compared to using single DebertaV3 model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-aspect query generation improves recall by decomposing complex conversational intents into simpler sub-queries
- Mechanism: MQ4CS framework generates φ=5 queries covering different aspects of the user's information need, allowing broader coverage of the document collection compared to a single query rewrite
- Core assumption: The user's information need can be effectively decomposed into multiple simpler, independent sub-queries that collectively capture the full intent
- Evidence anchors:
  - [abstract] "Our findings indicate that multi-aspect query generation is effective in enhancing performance when integrated with advanced retrieval and reranking models"
  - [section 3.1] "This allows for better coverage of the collections, as a query expansion, but also enables a decomposition of the query into several sub-pieces of simpler information needs"
  - [corpus] Weak - neighboring papers show multi-aspect approaches exist but don't validate this specific decomposition claim
- Break condition: If conversational context is too simple or the user's intent cannot be meaningfully decomposed, the multi-query approach may introduce noise rather than improve retrieval

### Mechanism 2
- Claim: Single-query reranking after multi-query retrieval improves precision by leveraging the strengths of both approaches
- Mechanism: First stage uses multiple queries for broad coverage with SPLADE retrieval, then re-ranks all retrieved passages using a single query rewrite that benefits from the LLM's ability to resolve conversational context
- Core assumption: The initial multi-query retrieval will produce a high-recall candidate set that the single query reranker can effectively filter to improve precision
- Evidence anchors:
  - [section 3.1] "we re-rank them using a single query rewrite... This would benefit from a large recall, as the different queries cover different aspects, and using these queries ensures having a diverse set of passages from different sources of information, but still have high precision, as relying on a single query rewrite to re-rank them"
  - [section 5.1] "After the reranking, still comparing (1) and (3), we see that the improvement in recall for the first stage also leads to better precision after reranking"
  - [corpus] Moderate - ensemble methods are well-documented in IR literature for improving precision
- Break condition: If the single query rewrite fails to capture important aspects of the information need, the reranking stage may discard relevant passages from the high-recall set

### Mechanism 3
- Claim: Ensembling cross-encoder rerankers improves robustness and effectiveness compared to single reranker approaches
- Mechanism: Multiple fine-tuned cross-encoders (DebertaV3, DebertaV2, Roberta, Albert, Electra) are combined using min-max normalization to produce a final ranked list
- Core assumption: Different cross-encoders capture different aspects of relevance, and their combination produces more robust rankings than any individual model
- Evidence anchors:
  - [section 4.1] "We also rely on the ensembling of rankers, using a min-max normalization on the produced ranked list"
  - [section 5.1] "we see that ensembling leads to better performance compared to using only DebertaV3... we observe a similar trend on automatic runs combined with MQ4CS... with an average 2 points increase on all metrics"
  - [corpus] Strong - ensemble methods are standard practice in IR competitions with documented effectiveness
- Break condition: If the individual rerankers are highly correlated in their errors, ensembling may not provide significant improvement and could add computational overhead

## Foundational Learning

- Concept: Conversational search context modeling
  - Why needed here: The system must understand how conversational context affects information needs across multiple turns, especially with personalization from PTKB
  - Quick check question: How does the MQ4CS framework handle pronouns and ellipses in user utterances that reference previous conversation turns?

- Concept: Sparse neural retrieval with SPLADE
  - Why needed here: SPLADE provides a balance between the efficiency of sparse retrieval and the effectiveness of learned representations, crucial for first-stage retrieval in conversational search
  - Quick check question: What is the difference between SPLADE and traditional BM25, and when would you choose one over the other?

- Concept: Cross-encoder reranking architecture
  - Why needed here: Cross-encoders provide more accurate relevance judgments by jointly encoding query-passage pairs, essential for the final ranking stage after initial retrieval
  - Quick check question: How does the cross-encoder architecture differ from bi-encoder architectures in terms of computational cost and effectiveness?

## Architecture Onboarding

- Component map: Query Generation (GPT-4/MQ4CS) → First-stage Retrieval (SPLADE) → Reranking (Cross-encoders ensemble) → Response Generation (GPT-4/RAG) → PTKB Classification (GPT-4)

- Critical path: Query Generation → First-stage Retrieval → Reranking → Response Generation

- Design tradeoffs:
  - Multi-query vs single-query: Multiple queries provide better recall but increase computational cost; single query is more efficient but may miss aspects
  - SPLADE vs BM25: SPLADE is more effective but computationally heavier than BM25
  - Cross-encoder vs bi-encoder: Cross-encoders are more accurate but must score all query-passage pairs, making them slower than bi-encoders

- Failure signatures:
  - Low recall metrics: Indicates multi-query generation may not be covering all aspects of user intent
  - Poor precision after reranking: Suggests the single query rewrite isn't effectively filtering the high-recall set
  - Inconsistent performance across topics: May indicate the approach doesn't generalize well to certain conversation domains

- First 3 experiments:
  1. Compare φ=3, φ=5, and φ=7 multi-query generation to find optimal number of queries for this dataset
  2. A/B test SPLADE vs BM25 as first-stage retrieval to quantify the effectiveness gain from learned sparse retrieval
  3. Evaluate individual cross-encoders vs ensemble to measure the benefit of ensembling and identify which models contribute most to performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the MQ4CS framework's multi-aspect query generation compare to single-query rewriting approaches in terms of computational efficiency and resource utilization?
- Basis in paper: [inferred]
- Why unresolved: The paper focuses on the effectiveness of multi-aspect query generation but does not provide detailed analysis of computational costs or resource usage compared to single-query approaches
- What evidence would resolve it: A comprehensive comparison of runtime, memory usage, and computational resources required for multi-aspect vs. single-query approaches would provide clarity on the trade-offs between effectiveness and efficiency

### Open Question 2
- Question: What are the specific limitations of using learned sparse retrieval (SPLADE) in conversational search, particularly in handling complex user intents and personalization?
- Basis in paper: [explicit]
- Why unresolved: While the paper demonstrates the effectiveness of SPLADE, it does not delve into potential limitations or challenges when dealing with complex conversational contexts and personalized user knowledge
- What evidence would resolve it: A detailed analysis of SPLADE's performance on various conversational search tasks, especially those involving complex user intents and personalization, would highlight its strengths and limitations

### Open Question 3
- Question: How does the ensemble reranking approach affect the robustness and consistency of the retrieval system across different conversational topics and user personas?
- Basis in paper: [explicit]
- Why unresolved: The paper mentions the use of ensemble reranking but does not explore how this approach impacts the system's performance across diverse conversational scenarios and user profiles
- What evidence would resolve it: An in-depth study of the ensemble reranking approach's performance on various conversational topics and user personas would reveal its robustness and consistency in handling diverse information needs

## Limitations

- Critical implementation details like exact GPT-4 prompt templates and ensembling strategy normalization are unspecified
- Computational costs and efficiency metrics are not reported, making deployment feasibility assessment difficult
- Limited analysis of how the approach performs across different conversational domains and user personas

## Confidence

- **High Confidence**: The effectiveness of ensembling multiple cross-encoder models for reranking
- **Medium Confidence**: The SPLADE sparse retrieval component
- **Low Confidence**: The exact implementation details of multi-aspect query generation using GPT-4 and the MQ4CS framework

## Next Checks

1. **Prompt Specification Verification**: Request the exact GPT-4 prompt templates used for multi-aspect query generation and single-query rewriting to ensure faithful reproduction of the query generation pipeline.

2. **Ensembling Implementation Details**: Clarify the normalization and combination strategy for the cross-encoder ensemble (min-max normalization parameters, any weighting schemes, or alternative combination methods).

3. **Ablation Study on Query Generation**: Conduct controlled experiments varying φ (number of queries) from 3 to 7 to empirically determine the optimal number of multi-aspect queries for this conversational search task and validate the claim that decomposition improves recall.