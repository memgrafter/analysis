---
ver: rpa2
title: Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed
  Preference Optimization
arxiv_id: '2411.10442'
source_url: https://arxiv.org/abs/2411.10442
tags:
- answer
- question
- arxiv
- each
- total
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limited multimodal reasoning capabilities
  of existing open-source multimodal large language models (MLLMs), which struggle
  particularly with Chain-of-Thought (CoT) reasoning due to distribution shifts from
  supervised fine-tuning. The authors introduce Mixed Preference Optimization (MPO),
  a method that combines preference optimization with supervised fine-tuning to enhance
  multimodal reasoning.
---

# Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization

## Quick Facts
- arXiv ID: 2411.10442
- Source URL: https://arxiv.org/abs/2411.10442
- Authors: Weiyun Wang; Zhe Chen; Wenhai Wang; Yue Cao; Yangzhou Liu; Zhangwei Gao; Jinguo Zhu; Xizhou Zhu; Lewei Lu; Yu Qiao; Jifeng Dai
- Reference count: 40
- Primary result: InternVL2-8B-MPO achieves 67.0 accuracy on MathVista, outperforming baseline by 8.7 points and matching 10x larger model

## Executive Summary
This paper addresses the limited multimodal reasoning capabilities of existing open-source multimodal large language models (MLLMs), which struggle particularly with Chain-of-Thought (CoT) reasoning due to distribution shifts from supervised fine-tuning. The authors introduce Mixed Preference Optimization (MPO), a method that combines preference optimization with supervised fine-tuning to enhance multimodal reasoning. They construct MMPR, a large-scale multimodal reasoning preference dataset using an automated pipeline. Experiments show that InternVL2-8B-MPO achieves 67.0 accuracy on MathVista, outperforming the baseline by 8.7 points and matching the 10x larger InternVL2-76B, demonstrating significant improvement in multimodal reasoning.

## Method Summary
The authors propose Mixed Preference Optimization (MPO), which integrates preference optimization with supervised fine-tuning to enhance multimodal reasoning in MLLMs. The method uses MMPR, a large-scale multimodal reasoning preference dataset constructed through an automated pipeline. The MPO approach addresses the distribution shift problem in Chain-of-Thought reasoning by training models to optimize for both supervised targets and preference-based rewards. This dual optimization strategy helps the model better align with human reasoning preferences while maintaining task-specific accuracy. The framework is evaluated on MathVista and MMVR datasets, demonstrating substantial improvements in multimodal reasoning performance.

## Key Results
- InternVL2-8B-MPO achieves 67.0 accuracy on MathVista, outperforming baseline by 8.7 points
- The 8B model matches the performance of the 10x larger InternVL2-76B model
- MPO successfully addresses Chain-of-Thought reasoning limitations in MLLMs
- The MMPR dataset enables effective preference-based training for multimodal reasoning

## Why This Works (Mechanism)
The effectiveness of MPO stems from its ability to bridge the gap between supervised fine-tuning and human reasoning preferences. By combining both optimization objectives, the model learns to generate more aligned and interpretable reasoning chains while maintaining accuracy on multimodal tasks. The preference optimization component specifically addresses the distribution shift problem that occurs when models trained on supervised data attempt complex reasoning tasks, helping them better generalize to novel problem-solving scenarios.

## Foundational Learning
- **Chain-of-Thought reasoning**: Sequential reasoning approach that breaks down problems into intermediate steps; needed for complex multimodal problem solving; quick check: can the model explain its reasoning path
- **Distribution shift in fine-tuning**: Gap between training data and real-world reasoning scenarios; needed to understand CoT limitations; quick check: compare training vs. inference reasoning patterns
- **Preference optimization**: Training method that optimizes for human-preferred outputs; needed for alignment with human reasoning; quick check: measure preference model agreement scores
- **Multimodal reasoning**: Integration of visual and textual information for problem solving; needed for comprehensive understanding; quick check: performance on cross-modal tasks
- **Supervised fine-tuning vs. preference learning**: Different training objectives for model optimization; needed to understand MPO's dual approach; quick check: compare model outputs from different training regimes

## Architecture Onboarding
**Component Map**: MLLM Backbone -> Supervised Fine-tuning Head -> Preference Optimization Module -> Combined Output Layer

**Critical Path**: Input multimodal data → Backbone processing → Supervised fine-tuning stage → Preference optimization stage → Final reasoning output

**Design Tradeoffs**: The MPO approach trades computational efficiency for improved reasoning quality, requiring additional preference model training and dual optimization phases. This adds complexity but significantly improves performance compared to standard fine-tuning approaches.

**Failure Signatures**: Models may overfit to preference data patterns, exhibit inconsistent reasoning across similar problems, or show preference optimization artifacts in generated chains-of-thought. These failures manifest as reduced generalization or unnatural reasoning patterns.

**First 3 Experiments**: 1) Ablation study comparing MPO vs. supervised fine-tuning only, 2) Cross-dataset generalization testing on unseen multimodal reasoning tasks, 3) Preference model quality assessment through human evaluation of generated reasoning chains

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on MathVista and MMVR datasets, potentially missing broader reasoning challenges
- Quality and diversity of synthetically generated preference pairs may not represent real-world scenarios
- Computational cost comparison between MPO and standard fine-tuning is not explicitly quantified

## Confidence
- High Confidence: Core claim that MPO improves multimodal reasoning performance is well-supported by experimental results
- Medium Confidence: Distribution shift as primary cause of CoT difficulties is plausible but not definitively proven
- Medium Confidence: Scalability claims based on comparison with single larger model need broader validation

## Next Checks
1. Generalization Testing: Evaluate InternVL2-8B-MPO on additional multimodal reasoning benchmarks beyond MathVista and MMVR
2. Ablation Studies: Isolate contributions of preference optimization versus supervised fine-tuning components
3. Long-term Reasoning Capability: Test model performance on multi-step reasoning problems over extended contexts