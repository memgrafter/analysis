---
ver: rpa2
title: 'TODO: Enhancing LLM Alignment with Ternary Preferences'
arxiv_id: '2411.02442'
source_url: https://arxiv.org/abs/2411.02442
tags:
- todo
- preference
- data
- ratio
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation of current alignment methods,
  such as DPO, which struggle to capture nuanced human preferences, particularly in
  the presence of noisy or inconsistent labels and frequent ties in preference data.
  The authors introduce the Tie-rank Oriented Bradley-Terry (TOBT) model, an extension
  of the BT model that explicitly incorporates ties, enabling more nuanced preference
  representation.
---

# TODO: Enhancing LLM Alignment with Ternary Preferences

## Quick Facts
- **arXiv ID**: 2411.02442
- **Source URL**: https://arxiv.org/abs/2411.02442
- **Reference count**: 40
- **Primary result**: Introduces TOBT model and TODO algorithm that consistently outperforms DPO on both ternary and binary preference alignment tasks

## Executive Summary
This paper addresses limitations in current alignment methods like DPO that struggle with noisy labels and frequent ties in preference data. The authors propose TOBT (Tie-rank Oriented Bradley-Terry), an extension of the Bradley-Terry model that explicitly incorporates ties, and TODO (Tie-rank Oriented Direct Preference Optimization), which leverages TOBT's ternary ranking system. Experiments on Mistral-7B and Llama 3-8B show TODO consistently outperforms DPO across in-distribution and out-of-distribution datasets, with strong results even on binary preference alignment.

## Method Summary
The paper extends the Bradley-Terry model to explicitly handle ties through the TOBT model, which introduces a margin parameter α to distinguish between clear preferences, ties, and dis-preferences. Building on this, TODO implements a ternary optimization objective that accounts for preference uncertainty and noise. The method is evaluated by training Mistral-7B and Llama 3-8B models on pairwise preference datasets with varying tie proportions, comparing performance against DPO and other baselines across multiple benchmarks.

## Key Results
- TODO consistently outperforms DPO on both ternary and binary preference alignment tasks
- Performance improvements are robust across different tie data ratios (0%, 10%, 20%, 30%)
- TODO shows strong results on out-of-distribution datasets and multiple benchmarks (MT Bench, PIQA, ARC-c/e, Hellaswag, MMLU, Winogrande)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: TOBT improves alignment by explicitly modeling tie relationships, allowing learning from data that DPO would discard
- **Core assumption**: Tie data contains meaningful information about response quality similarity
- **Evidence anchors**: TOBT can directly model ternary preferences and learn more information from tied data
- **Break condition**: If tie data is truly random noise without underlying quality similarity

### Mechanism 2
- **Claim**: The α parameter acts as regularization making the model robust to noise in binary preference data
- **Core assumption**: Binary preference datasets contain noisy labels with small preference differences
- **Evidence anchors**: Refined handling of tied data and enriched diversity of information learned
- **Break condition**: If binary preference data is perfectly clean with large preference differences

### Mechanism 3
- **Claim**: Ternary preference system enables more diverse information extraction from the same training data
- **Core assumption**: Tied responses contain valuable information about both response qualities
- **Evidence anchors**: TODO mitigates artificial preference forcing by incorporating tie ranks
- **Break condition**: If tied responses are truly identical in all aspects

## Foundational Learning

- **Concept**: Bradley-Terry model for pairwise comparison
  - Why needed here: Forms the foundation for TOBT and TODO
  - Quick check question: How does the Bradley-Terry model compute the probability that one item is preferred over another in pairwise comparison?

- **Concept**: Sigmoid function and its derivative in optimization
  - Why needed here: Used in computing preference probabilities and gradients
  - Quick check question: What is the derivative of the sigmoid function σ(x), and how does it relate to σ(x) itself?

- **Concept**: Maximum likelihood estimation for parameter learning
  - Why needed here: Both DPO and TODO are derived using maximum likelihood principles
  - Quick check question: How does maximum likelihood estimation work in the context of learning from preference data?

## Architecture Onboarding

- **Component map**: Data → TOBT probability computation → TODO loss calculation → Gradient update → Model parameter update
- **Critical path**: Data must flow through TOBT computation before TODO loss calculation, with gradients computed before parameter updates
- **Design tradeoffs**: α parameter controls conservatism vs sensitivity - larger values increase noise robustness but may miss subtle preferences
- **Failure signatures**: Poor performance on binary data may indicate high tie data proportion or α set too large; failure to converge may indicate α too small or noisy tie data
- **First 3 experiments**:
  1. Implement TOBT probability computation and verify valid probabilities for different α values
  2. Compare TODO loss computation with DPO on synthetic dataset with known ties
  3. Test TODO on binary preference dataset to verify DPO-like performance recovery

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TODO's performance vary when integrated with other alignment algorithms like DPO, KTO, or SimPO?
- Basis in paper: TODO's TOBT model can be integrated into other online policies or used to train reward models
- Why unresolved: Paper primarily focuses on comparing TODO against DPO, not exploring integration with other algorithms
- What evidence would resolve it: Empirical results comparing TODO's performance when combined with other alignment algorithms

### Open Question 2
- Question: What is the impact of different tie data ratios on TODO's performance in various downstream tasks?
- Basis in paper: Experiments with tie data ratios (0, 0.1, 0.2, 0.3) showing optimal performance at specific ratios
- Why unresolved: Paper doesn't explore full range of tie data ratios or impact on all downstream tasks
- What evidence would resolve it: Comprehensive analysis across wider range of tie data ratios and downstream tasks

### Open Question 3
- Question: How does TODO compare to other ternary preference models in handling ties and improving alignment?
- Basis in paper: Mentions other ternary models (Glenn & David, 1960; Davidson, 1970) that could be adapted for LLM alignment
- Why unresolved: Paper doesn't explore or compare TODO with other ternary preference models
- What evidence would resolve it: Empirical results comparing TODO's performance with other ternary preference models

## Limitations
- Assumption that tie data contains meaningful signal is not empirically validated
- α=0.5 optimality is presented without systematic sensitivity analysis
- Lacks comparison against other ternary preference methods on the same datasets

## Confidence

**High confidence**: Experimental results showing TODO outperforming DPO on both ternary and binary datasets; mathematical derivation of TOBT from Bradley-Terry framework

**Medium confidence**: Claims about performance improvements from better handling of tied data and noise robustness; evidence is correlational rather than causal

**Low confidence**: Assertion that TODO extracts "more diverse information" from training data; lacks quantitative measurement of information diversity

## Next Checks

1. **Ablation study on tie data**: Systematically vary tie proportions (0%, 10%, 20%, 30%) to test whether TODO's advantage scales with tie data availability

2. **α parameter sensitivity analysis**: Systematically vary α across range (0.1 to 1.0) on multiple datasets to determine robustness to this hyperparameter

3. **Information extraction measurement**: Design experiment quantifying what specific information is learned from tied versus non-tied data through prediction consistency and downstream task performance analysis