---
ver: rpa2
title: 'HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination Tendency
  of LLMs'
arxiv_id: '2402.16211'
source_url: https://arxiv.org/abs/2402.16211
tags:
- term
- valid
- terms
- question
- hypothetical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces HypoTermQA, an automated framework for benchmarking
  the hallucination tendency of large language models (LLMs). It creates a dataset
  of hypothetical terms and generates challenging questions that combine real and
  non-existent concepts.
---

# HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination Tendency of LLMs

## Quick Facts
- arXiv ID: 2402.16211
- Source URL: https://arxiv.org/abs/2402.16211
- Reference count: 40
- Key outcome: State-of-the-art models achieve 3-11% performance on HypoTermQA benchmark, with LLM evaluator agents showing 6% error rate in hallucination prediction

## Executive Summary
HypoTermQA introduces an automated framework for benchmarking LLM hallucination tendencies by creating challenging questions that combine real and non-existent concepts. The framework generates hypothetical terms from common words, retrieves similar valid terms from Wikipedia, and creates questions that force LLMs to distinguish between valid and fabricated concepts. LLM agents evaluate responses to classify hallucinations, with the publicly available dataset showing state-of-the-art models perform poorly (3-11% accuracy), demonstrating the benchmark's effectiveness in exposing LLM weaknesses.

## Method Summary
The HypoTermQA framework automatically generates hypothetical terms by combining common words and validates their non-existence through web search. For each hypothetical term, the system retrieves similar valid terms from Wikipedia using three similarity approaches: LLM suggestion, title similarity (DistilBERT embeddings), and text similarity (definition embeddings). Questions are generated combining hypothetical and valid terms in three formats: hypothetical questions, valid questions, and replaced questions. Responses are evaluated by LLM agents using term inclusion checks, acceptance checks, and meaning checks to classify hallucinations at both term and answer levels.

## Key Results
- State-of-the-art models achieve 3-11% performance on HypoTermQA benchmark
- LLM evaluator agents demonstrate 6% error rate in hallucination prediction
- ChatGPT (with RLHF) outperformed GPT-4 in hallucination resistance
- The framework is domain-agnostic and applicable to law, health, and finance domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM hallucination tendency can be measured by generating and evaluating responses to questions containing a mix of valid and non-existent terms.
- Mechanism: The HypoTermQA framework creates questions pairing valid Wikipedia terms with hypothetical terms, then uses LLM agents to classify responses as valid, hallucination, or irrelevant based on term inclusion, acceptance, and meaning checks.
- Core assumption: LLM agents can reliably detect whether a response accepts, rejects, or ignores a term, enabling automated hallucination scoring.
- Evidence anchors: [abstract] "state-of-the-art models' performance ranged between 3% and 11%"; [section 3.1] "Term Inclusion Check: Initially, the answer undergoes a programmatic string check for the presence of the specified term"
- Break condition: If LLM agents themselves hallucinate or misclassify, the scoring becomes unreliable.

### Mechanism 2
- Claim: Generating hypothetical terms by combining common words creates adversarial test cases that trigger LLM hallucination.
- Mechanism: The framework uses GPT-3.5 with high temperature to generate plausible but non-existent multi-word terms, then validates non-existence via web search.
- Core assumption: Terms constructed from common words will be semantically plausible yet absent from training data, forcing LLMs to generate fabricated content.
- Evidence anchors: [section 2.2] "The generated terms were searched within quotation marks across the web, and any term with a 'total results' count greater than zero was excluded"
- Break condition: If a generated term accidentally matches real-world usage not indexed online, the adversarial nature fails.

### Mechanism 3
- Claim: Using valid terms similar to hypothetical terms in questions increases task complexity and realism.
- Mechanism: Three similarity approaches (LLM suggestion, title similarity, text similarity) retrieve valid Wikipedia terms close to hypothetical terms in latent space or context.
- Core assumption: Questions mixing closely related real and fake concepts will better simulate real-world adversarial prompts where LLMs must discern truth.
- Evidence anchors: [section 2.3] "To increase task complexity, terms similar to the hypothetical ones retrieved and used in the questions"
- Break condition: If similarity metrics retrieve terms with unrelated contexts, questions become nonsensical rather than challenging.

## Foundational Learning

- Concept: Vector similarity search using DistilBERT embeddings
  - Why needed here: To find Wikipedia terms semantically similar to hypothetical terms for creating realistic adversarial questions
  - Quick check question: How does L2 distance between sentence embeddings help identify related concepts in Wikipedia titles and definitions?

- Concept: Programmatic string matching with preprocessing
  - Why needed here: To reliably detect term presence in LLM responses while handling case, punctuation, and bracket variations
  - Quick check question: Why must we remove brackets and punctuation before checking if a term appears in a response?

- Concept: LLM-based reflective evaluation
  - Why needed here: To classify whether responses accept, reject, or are uncertain about term existence
  - Quick check question: How can we prompt an LLM to determine if another LLM's response hallucinates about a specific term?

## Architecture Onboarding

- Component map: Topic selection -> Hypothetical term generation -> Valid term retrieval -> Question composition -> LLM response generation -> LLM agent evaluation -> HypoTermQA score calculation
- Critical path: Question generation → LLM response → LLM agent evaluation → HypoTermQA score calculation
- Design tradeoffs: High creativity (temperature=1) vs reproducibility in term generation; Automated evaluation vs potential bias from using LLMs to evaluate themselves; Comprehensive similarity search vs computational cost
- Failure signatures: Low HypoTermQA scores across all models (suggests questions too easy or evaluation too strict); High irrelevant answer rates (suggests question generation produces nonsensical queries); LLM agent misclassification patterns (suggests bias in evaluation methodology)
- First 3 experiments:
  1. Generate 10 hypothetical terms for a single topic and manually verify non-existence to validate the generation pipeline
  2. Test LLM agent classification on known hallucination cases to benchmark evaluation accuracy
  3. Compare HypoTermQA scores across different LLM combinations (generator vs evaluator) to identify bias patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different language model architectures (e.g., transformer-based vs. other) affect their susceptibility to hypothetical term hallucinations?
- Basis in paper: [inferred] The paper compares GPT-3.5 and Llama2-70B, but does not explore the impact of different underlying architectures.
- Why unresolved: The study focuses on specific models rather than architectural differences.
- What evidence would resolve it: Systematic comparison of hallucination rates across different model architectures using the same evaluation framework.

### Open Question 2
- Question: What is the long-term effectiveness of reinforcement learning from human feedback (RLHF) in reducing hallucination tendencies compared to other training methods?
- Basis in paper: [explicit] The paper notes that ChatGPT (with heavy RLHF) outperformed GPT-4 in hallucination resistance, suggesting RLHF's effectiveness.
- Why unresolved: The study provides a snapshot comparison rather than long-term analysis of RLHF's impact.
- What evidence would resolve it: Longitudinal studies tracking hallucination rates in models with different training methodologies over time.

### Open Question 3
- Question: How does the complexity and plausibility of hypothetical terms affect the hallucination rates of language models?
- Basis in paper: [inferred] The paper generates various hypothetical terms but does not systematically analyze how their characteristics impact hallucination rates.
- Why unresolved: The study focuses on creating diverse hypothetical terms without exploring their individual impact on model behavior.
- What evidence would resolve it: Controlled experiments varying term complexity, plausibility, and relation to real concepts while measuring hallucination rates.

## Limitations
- LLM agent evaluation introduces potential circular bias as LLMs evaluate their own hallucination tendencies
- Web search validation for term non-existence may miss specialized or emerging concepts not indexed online
- Similarity-based question generation may produce nonsensical questions if context mismatches occur

## Confidence
- High confidence: The automated generation pipeline structure and dataset creation methodology are well-defined and reproducible
- Medium confidence: The LLM agent evaluation framework appears sound, though circular bias remains a concern
- Low confidence: The effectiveness of similarity-based adversarial question generation lacks external validation and may produce suboptimal challenge questions

## Next Checks
1. Conduct inter-annotator agreement study between human experts and LLM agents on a subset of 100 questions to quantify evaluation bias and reliability
2. Test generated questions with LLMs from different training eras (pre-2023 vs post-2023) to verify whether non-existent terms remain truly unknown to newer models
3. Analyze error patterns in LLM agent classifications to identify systematic biases in how agents detect hallucinations versus genuine uncertainty or valid alternative interpretations