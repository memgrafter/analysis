---
ver: rpa2
title: 'From Single to Multi: How LLMs Hallucinate in Multi-Document Summarization'
arxiv_id: '2410.13961'
source_url: https://arxiv.org/abs/2410.13961
tags:
- insights
- subtopic
- documents
- llms
- news
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates hallucination in large language models
  (LLMs) when summarizing information from multiple documents. The authors create
  two novel benchmarks using existing news and conversation datasets with fine-grained
  annotations, evaluating five popular LLMs on their tendency to generate hallucinated
  content.
---

# From Single to Multi: How LLMs Hallucinate in Multi-Document Summarization

## Quick Facts
- **arXiv ID**: 2410.13961
- **Source URL**: https://arxiv.org/abs/2410.13961
- **Reference count**: 40
- **Primary result**: Up to 75% of LLM-generated content can be hallucinated in multi-document summarization, with errors more likely towards summary end

## Executive Summary
This paper investigates hallucination patterns in large language models when summarizing information from multiple documents. The authors create two novel benchmarks using existing news and conversation datasets with fine-grained annotations, evaluating five popular LLMs on their tendency to generate hallucinated content. Their analysis reveals that up to 75% of LLM-generated content can be hallucinated, with errors more likely to occur towards the end of summaries. Notably, even when instructed to summarize non-existent topics, GPT-3.5-Turbo and GPT-4o generate summaries 79.35% and 44% of the time respectively. Manual evaluation of 700+ insights shows most errors stem from failing to follow instructions or producing overly generic insights. The paper also explores simple post-processing approaches to mitigate hallucinations, finding only marginal improvements at best, underscoring the need for more effective solutions to systematically address this challenge in multi-document summarization.

## Method Summary
The study evaluates five LLMs (GPT-3.5-Turbo, GPT-4o, Gemini-1.5-Flash, Llama 3.1, Qwen 2) on their ability to summarize topic-specific information from combinations of 2-10 documents. The authors create two novel benchmarks (SummHay-News and SummHay-Conv) using existing datasets with fine-grained insight-level annotations. Document combinations are created to ensure topic-related insights are shared across at least 2 documents. The evaluation protocol uses an LLM-as-a-judge approach with gpt-4o-mini-2024-07-18 to determine whether predicted insights faithfully cover reference insights, classifying them as covered, hallucinated, or ignored. Manual error analysis categorizes hallucinations into pedantic errors, instruction inconsistencies, and context inconsistencies.

## Key Results
- Hallucination rates range from 20-75% across models and domains, with conversations showing higher rates than news
- Increasing document count from 2 to 10 results in only marginal changes in hallucination rates (±5% for most models)
- Hallucinations are more likely to appear in later sections of summaries regardless of input document count
- When summarizing non-existent topics, GPT-3.5-Turbo and GPT-4o generate summaries 79.35% and 44% of the time respectively
- Post-processing approaches reduce hallucinated content by up to 7% but exclude relevant information by up to 6%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM hallucinations in multi-document summarization increase with input document count
- Mechanism: As document count increases, the model encounters more potential distraction information that competes with the target subtopic, leading to decreased precision in identifying relevant insights
- Core assumption: The model's ability to maintain focus on the target subtopic degrades linearly with increasing document volume
- Evidence anchors:
  - [abstract] "increasing the volume of input documents affects LLMs differently: for instance, as the document count increases from 2 to 10, most models experience only marginal changes in hallucinated content (± 5%) whereas gemini-1.5-flash shows up to a 10% increase"
  - [section] "Figure 2 shows that, although there is an overall downward trend in recall, the error rate remains almost constant (±5%), increasing only for Gemini (Flash) (<10%)"
  - [corpus] Weak evidence - most related papers focus on improving summarization quality rather than investigating hallucination patterns with document count

### Mechanism 2
- Claim: Hallucinations are more likely to occur towards the end of LLM-generated summaries
- Mechanism: The model may prioritize generating content that appears coherent and complete, leading to less careful verification of later insights, or it may use discourse patterns that place less critical information at the end
- Core assumption: The model processes summary generation sequentially and attention/resources diminish for later positions
- Evidence anchors:
  - [abstract] "examining the composition of LLM-generated summaries, we find that regardless of the number of input documents and summary focus, hallucinations are more likely to appear in the later sections of the summaries"
  - [section] "Figure 5: Accuracy rate of GPT-4o-generated insights by position (when summarizing 10 input documents). Overall, accuracy rate declines as insight position increases"
  - [corpus] Weak evidence - related work focuses on improving summary quality but doesn't investigate positional hallucination patterns

### Mechanism 3
- Claim: LLMs show strong predisposition to generate summaries even when no topic-specific insights exist in the input
- Mechanism: LLMs are trained to be helpful and complete tasks, leading them to fabricate plausible-sounding content rather than refusing to generate when faced with impossible requests
- Core assumption: The model's training objective prioritizes task completion over truthfulness when faced with impossible requests
- Evidence anchors:
  - [abstract] "when summarizing non-existent topic-related information, gpt-3.5-turbo and GPT-4o still generate summaries about 79.35% and 44% of the time, raising concerns about their tendency to fabricate content"
  - [section] "Figure 4 shows that models actually generate summaries for non-existent subtopics: LLMs only abstain in 20.65% (GPT-3.5-Turbo, conversation) and up to 71.08% (Llama 3.1 (70B) , news)"
  - [corpus] Weak evidence - related papers focus on improving summarization but don't investigate model behavior on non-existent topics

## Foundational Learning

- Concept: Information coverage evaluation
  - Why needed here: The paper uses an LLM-as-a-judge approach to determine whether predicted insights faithfully cover reference insights, which requires understanding how coverage is measured
  - Quick check question: What are the three possible coverage labels used in the evaluation, and how are they combined into a single correctness label?

- Concept: Multi-document summarization task formulation
  - Why needed here: Understanding how the MDS task is defined (N documents, K conditions) is crucial for interpreting the experimental setup and results
  - Quick check question: What are the key differences between the "subtopic" and "subtopic+shared" settings in terms of reference insights used for evaluation?

- Concept: Hallucination taxonomy
  - Why needed here: The paper identifies three main types of hallucination errors (pedantic, instruction inconsistency, context inconsistency) that are essential for understanding the nature of model failures
  - Quick check question: What distinguishes a "pedantic" error from an "instruction inconsistency" error in the context of multi-document summarization?

## Architecture Onboarding

- Component map: Document combination → LLM summarization → Automatic evaluation → Performance metrics → Manual error analysis
- Critical path: Document combination → LLM summarization → Automatic evaluation → Performance metrics → Manual error analysis. The bottleneck is the automatic evaluation step, which processes each reference insight against each predicted insight.
- Design tradeoffs: Using fine-grained insight-level annotations enables precise hallucination detection but requires significant manual annotation effort. The choice of automatic evaluation trades off perfect accuracy for scalability across thousands of examples.
- Failure signatures: Low recall with high hallucination rate indicates the model is generating plausible but incorrect information. Uniform distribution of errors across documents suggests the model isn't systematically ignoring certain sources. Recency bias in errors indicates positional processing issues.
- First 3 experiments:
  1. Replicate the document count analysis (N=2,3,4,5,10) on a smaller subset to verify the marginal increase in hallucination rate
  2. Test the "subtopic+shared" setting with different confidence thresholds in the automatic evaluator to understand the impact on error categorization
  3. Implement a simple position-based filtering (keeping only first K insights) to measure if it reduces hallucination rate without severely impacting recall

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the most effective approach to mitigate hallucinations in multi-document summarization tasks while preserving recall?
- Basis in paper: [explicit] The paper investigates five simple post-processing approaches (rule-based and LLM-based) to mitigate hallucinations but finds them only marginally effective, with up to 7% reduction in hallucinated content at the cost of excluding relevant information by up to 6%.
- Why unresolved: The current methods show limited success in balancing hallucination reduction with maintaining relevant information, indicating a need for more sophisticated approaches.
- What evidence would resolve it: Development and evaluation of new mitigation strategies that demonstrate significant hallucination reduction (e.g., >20%) while maintaining or improving recall rates compared to current methods.

### Open Question 2
- Question: How does the propensity for hallucination in LLMs vary across different domains and document types in multi-document summarization?
- Basis in paper: [explicit] The paper observes that hallucination rates differ between news (20-45%) and conversation (52-75%) domains, suggesting domain-specific factors influence hallucination behavior.
- Why unresolved: While initial differences are observed, the underlying reasons for domain-specific variations and their relationship to document characteristics remain unclear.
- What evidence would resolve it: Comprehensive analysis comparing hallucination patterns across multiple domains and document types, identifying specific features that correlate with increased hallucination rates.

### Open Question 3
- Question: What is the relationship between document position in input and hallucination likelihood in multi-document summarization?
- Basis in paper: [explicit] The paper finds that GPT-3.5-Turbo, Llama 3.1 (70B), and Gemini (Flash) show recency bias, with hallucinations more likely to originate from later documents, while other models show near-uniform distribution.
- Why unresolved: The causes of position-based differences in hallucination patterns are not fully understood, and it's unclear how this relates to model architecture or input processing.
- What evidence would resolve it: Systematic investigation of positional encoding effects and input processing mechanisms across different model architectures to explain variations in hallucination patterns based on document position.

## Limitations

- The evaluation relies on synthetic document combinations rather than naturally occurring multi-document scenarios, which may not capture real-world complexity
- The automatic evaluation approach, while scalable, introduces uncertainty in hallucination detection accuracy
- The study focuses on specific LLM models and prompt configurations, limiting conclusions about model-agnostic behaviors

## Confidence

- **High confidence**: The finding that hallucinations are more likely to occur towards the end of summaries
- **Medium confidence**: The claim that increasing document count has only marginal effects on hallucination rates
- **Medium confidence**: The observation that LLMs generate summaries even for non-existent topics

## Next Checks

1. Replicate the document count analysis using naturally occurring multi-document datasets to verify if synthetic combinations accurately reflect real-world behavior
2. Conduct human evaluation of a stratified sample of predicted insights to validate the accuracy of the automatic LLM-as-a-judge evaluation approach
3. Test additional prompt variations (including refusal instructions) to determine if the observed tendency to generate content for non-existent topics is prompt-dependent