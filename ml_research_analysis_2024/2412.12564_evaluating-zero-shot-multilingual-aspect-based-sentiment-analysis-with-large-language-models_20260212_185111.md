---
ver: rpa2
title: Evaluating Zero-Shot Multilingual Aspect-Based Sentiment Analysis with Large
  Language Models
arxiv_id: '2412.12564'
source_url: https://arxiv.org/abs/2412.12564
tags:
- sentiment
- entity
- performance
- each
- absa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates zero-shot multilingual aspect-based sentiment
  analysis (ABSA) using large language models (LLMs). Nine LLMs are tested across
  five prompting strategies (vanilla zero-shot, chain-of-thought, self-improvement,
  self-debate, and self-consistency) on five languages using the SemEval-2016 dataset.
---

# Evaluating Zero-Shot Multilingual Aspect-Based Sentiment Analysis with Large Language Models

## Quick Facts
- arXiv ID: 2412.12564
- Source URL: https://arxiv.org/abs/2412.12564
- Reference count: 40
- Primary result: Zero-shot LLM approaches underperform fine-tuned models in multilingual ABSA, with simpler prompts outperforming complex strategies

## Executive Summary
This paper evaluates nine large language models across five prompting strategies for zero-shot multilingual aspect-based sentiment analysis using the SemEval-2016 dataset. The study tests performance across five languages (English, French, Spanish, Dutch, Russian) and finds that simpler zero-shot prompts consistently outperform more complex multi-turn dialogue strategies. Results show significant performance degradation with multi-turn approaches due to over-correction and catastrophic forgetting. The research highlights that higher-resource languages yield better results, and that current zero-shot LLM approaches still lag behind fine-tuned models, indicating the need for further refinement in cross-lingual ABSA tasks.

## Method Summary
The study evaluates nine LLMs (Llama-3.1, Mistral, Gemma-2, Qwen-2.5, Zephyr, Phi-3.5, Gemini-1.5, Claude-3.5, GPT-4o) across five prompting strategies (vanilla zero-shot, chain-of-thought, self-improvement, self-debate, self-consistency) on the SemEval-2016 multilingual ABSA dataset. The evaluation uses micro-F1 scores for aspect-term and sentiment prediction accuracy with greedy decoding (temperature=0.0). Experiments cover five languages, with few-shot variants also tested. The method compares zero-shot performance against fine-tuned models and analyzes error patterns in aspect boundaries and sentiment polarity.

## Key Results
- Zero-shot LLM performance significantly underperforms fine-tuned models in multilingual ABSA tasks
- Simpler zero-shot prompts consistently outperform complex multi-turn dialogue strategies across all tested languages
- Multi-turn approaches (self-improvement and self-debate) show progressive performance degradation due to over-correction effects
- Higher-resource languages (English, French, Spanish) yield better performance than morphologically rich languages (Russian)
- Chain-of-thought prompting does not improve results and often introduces reasoning errors

## Why This Works (Mechanism)

### Mechanism 1
Zero-shot prompts outperform complex multi-turn dialogue strategies in multilingual ABSA. Simpler prompts reduce noise and task ambiguity, leading to more focused entity extraction and sentiment prediction. Additional reasoning steps and self-correction rounds do not compensate for the complexity they introduce, especially when linguistic cues are subtle.

### Mechanism 2
Multi-turn dialogue prompts degrade performance due to over-correction and catastrophic forgetting. Models assume prior outputs contain errors and revise them, introducing noise and drifting away from correct predictions. Models do not reliably distinguish when to self-correct versus when to preserve correct outputs.

### Mechanism 3
Higher-resource languages yield better LLM performance in ABSA due to better pre-training coverage and tokenization quality. LLMs trained on more data from a language can better capture morphological, syntactic, and sentiment nuances specific to that language. Multilingual corpora are unevenly distributed and tokenization is less optimal for morphologically rich languages.

## Foundational Learning

- **Concept**: Aspect-based sentiment analysis (ABSA) as a token-level sequence labeling task
  - **Why needed here**: ABSA requires precise identification of both aspect terms and their sentiment, not just document-level sentiment
  - **Quick check question**: Can you describe the difference between ABSA and traditional sentiment analysis?

- **Concept**: Zero-shot prompting vs. few-shot prompting
  - **Why needed here**: Understanding how the number of examples in prompts affects performance is key to designing experiments
  - **Quick check question**: What is the main difference between zero-shot and few-shot prompting?

- **Concept**: Chain-of-thought (CoT) and self-consistency prompting
  - **Why needed here**: These methods are used to test whether reasoning steps help in ABSA, and results show they may not
  - **Quick check question**: How does chain-of-thought prompting differ from vanilla zero-shot prompting?

## Architecture Onboarding

- **Component map**: Dataset loader -> Prompt generator -> LLM executor -> Evaluation engine -> Error analyzer
- **Critical path**: Load dataset and preprocess sentences → Generate prompts per strategy and language → Run inference on LLMs with greedy decoding → Parse JSON outputs and compute micro-F1 → Aggregate results and analyze error types
- **Design tradeoffs**: Simple prompts offer better performance but less flexibility; complex multi-turn provides potential for refinement but risks performance drop; few-shot improves performance but limited by context length; greedy decoding offers stability while sampling provides diversity
- **Failure signatures**: Zero-shot shows low recall for rare aspects; CoT results in over-prediction and reasoning errors; multi-turn shows gradual degradation across rounds; self-consistency produces inconsistent outputs at higher temperatures
- **First 3 experiments**: 1) Compare zero-shot vs. CoT on English dataset only; 2) Test self-consistency at temperatures {0.0, 0.5, 1.0} for one model; 3) Measure performance drop across rounds in self-debate setup

## Open Questions the Paper Calls Out

### Open Question 1
How do different multi-turn dialogue strategies (self-improvement vs self-debate) compare in effectiveness for multilingual ABSA, and what mechanisms could be implemented to mitigate the performance degradation observed in later rounds? The paper identifies that multi-turn approaches show significant performance drops but doesn't explore underlying mechanisms or propose specific mitigation strategies.

### Open Question 2
What is the relationship between model size and multilingual ABSA performance across different prompting strategies, and how does this relationship vary by language? The paper tests Qwen models of different sizes but doesn't deeply analyze how this relationship varies across languages or prompting strategies.

### Open Question 3
How can zero-shot cross-lingual transfer performance be improved for morphologically complex languages like Russian, and what role does prompt language (English vs native language) play in this improvement? The paper observes Russian performance lags significantly but doesn't explore native-language prompting or targeted transfer improvements.

## Limitations

- Relies on a single multilingual ABSA dataset (SemEval-2016) which may not capture full diversity of challenges across languages
- Evaluation focuses on micro-F1 scores without deeper analysis of error patterns across different aspect types or sentiment categories
- All prompts are English-centric, introducing inherent bias when evaluating multilingual performance

## Confidence

- **High confidence**: Simpler zero-shot prompts outperform complex multi-turn dialogue strategies
- **Medium confidence**: Higher-resource languages yield better performance due to resource effects
- **Low confidence**: Multi-turn dialogue prompts lead to catastrophic forgetting (lacks strong ABSA-specific evidence)

## Next Checks

1. **Dataset Generalization Test**: Replicate experiments using a different multilingual ABSA dataset (such as M-ABSA) to verify whether observed performance patterns hold across different data sources and annotation schemes.

2. **Prompt Translation Study**: Implement language-specific prompt templates translated into each target language and compare performance against English-centric prompts to isolate the effect of language-specific prompting.

3. **Fine-tuning Baseline Extension**: Conduct controlled experiments comparing zero-shot LLMs against fine-tuned smaller models (such as BERT variants) on the same computational budget to better understand practical trade-offs between model size, training data, and prompting complexity.