---
ver: rpa2
title: Towards Multilingual Audio-Visual Question Answering
arxiv_id: '2406.09156'
source_url: https://arxiv.org/abs/2406.09156
tags:
- question
- multilingual
- datasets
- languages
- answering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multilingual extension of audio-visual
  question answering (AVQA) by leveraging machine translation to create new datasets
  from existing AVQA benchmarks in eight languages (English, French, Hindi, German,
  Spanish, Italian, Dutch, Portuguese). The authors propose the MERA framework, which
  uses state-of-the-art pre-trained models for video (VideoMAE), audio (AST), and
  text (multilingual BERT) to extract multimodal features.
---

# Towards Multilingual Audio-Visual Question Answering

## Quick Facts
- arXiv ID: 2406.09156
- Source URL: https://arxiv.org/abs/2406.09156
- Authors: Orchid Chetia Phukan; Priyabrata Mallick; Swarup Ranjan Behera; Aalekhya Satya Narayani; Arun Balaji Buduru; Rajesh Sharma
- Reference count: 0
- One-line primary result: Introduces multilingual AVQA datasets and MERA framework, achieving best performance with CNN-based fusion (MERA-C) and ensemble methods.

## Executive Summary
This paper addresses the challenge of extending audio-visual question answering (AVQA) to multilingual settings. The authors propose a novel framework called MERA that leverages state-of-the-art foundation models for video, audio, and text to extract multimodal features, which are then fused using different architectures (LSTM, CNN, Transformer). They create two new multilingual AVQA datasets by machine-translating existing benchmarks into eight languages, and evaluate their models on these datasets. The results show that MERA-C (CNN-based fusion) performs best, and ensemble methods further improve performance across all languages and question types.

## Method Summary
The authors introduce the MERA framework, which uses frozen foundation models (VideoMAE, AST, mBERT) to extract video, audio, and text features respectively. Three model variants (MERA-L, MERA-C, MERA-T) are proposed with different fusion architectures: LSTM, CNN, and Transformer. The models are trained on machine-translated AVQA datasets created from MUSIC-AVQA and AVQA benchmarks in eight languages. Weighted ensemble of the three models is used to further improve performance. The models are trained for 50 epochs with cross-entropy loss, Adam optimizer (lr=1e-3), and batch size 32, evaluated using accuracy metric.

## Key Results
- MERA-C (CNN-based fusion) achieves the highest accuracy among individual models on multilingual AVQA datasets
- Ensemble methods with equal weights (α=β=γ=0.33) improve performance across all languages and question types
- Weighted-ensemble outperforms individual models in most instances
- MERA framework establishes strong baselines for multilingual AVQA research

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The MERA framework improves multilingual AVQA performance by using frozen foundation models for feature extraction.
- Mechanism: VideoMAE, AST, and mBERT are pre-trained models that provide robust, high-quality embeddings for video, audio, and text. Freezing these models during MERA training preserves their learned representations while MERA focuses on multimodal fusion and classification.
- Core assumption: The foundation models generalize well across the eight languages and capture the relevant semantic features for answering AVQA questions.
- Evidence anchors:
  - [abstract] "MERA framework, by leveraging state-of-the-art (SOTA) video, audio, and textual foundation models"
  - [section] "We use VideoMAE 3, AST 4, and mBERT 5, openly available in Hugginface...We leverage the foundation models based on the SOTA performance in respective modalities"
  - [corpus] Weak: Corpus shows no direct evidence about foundation model effectiveness in AVQA. Evidence comes from the paper itself.

### Mechanism 2
- Claim: Ensemble methods improve multilingual AVQA performance by combining diverse model predictions.
- Mechanism: MERA-L (LSTM), MERA-C (CNN), and MERA-T (Transformer) have different architectures that capture different aspects of the data. Weighted averaging of their softmax outputs leverages their complementary strengths, improving robustness and accuracy across languages and question types.
- Core assumption: The models are sufficiently diverse in their decision boundaries and predictions to provide complementary information.
- Evidence anchors:
  - [abstract] "Furthermore, we show that weighted-ensemble of the models output probabilities leads to improvements across all the languages and different question types compared to the individual models."
  - [section] "With α = β = γ = 0.33, we got the topmost performance in comparison to the individual models in most of instances"
  - [corpus] Weak: No corpus evidence for ensemble effectiveness in multilingual AVQA specifically. Based on general ensemble theory.

### Mechanism 3
- Claim: Machine translation provides a scalable way to create multilingual AVQA datasets without manual annotation.
- Mechanism: The original MUSIC-AVQA and AVQA datasets are translated from English into seven other languages using Google's API. Human verification ensures quality. This avoids the resource-intensive process of collecting questions and answers in multiple languages from scratch.
- Core assumption: Machine translation quality is sufficient to preserve the semantic content and question-answer relationships needed for AVQA.
- Evidence anchors:
  - [abstract] "we leverage machine translation and present two multilingual AVQA datasets for eight languages created from existing benchmark AVQA datasets"
  - [section] "we translated questions and answers from these datasets into seven additional languages using Google's machine translation API...Evaluation with standard metrics such as BLEU [15], ROUGE [16], and METEOR [17] confirmed the reliability of our translations"
  - [corpus] Moderate: Corpus shows related work on using translation for multilingual QA (Changpinyo et al., Behera et al.), supporting the general approach.

## Foundational Learning

- Concept: Multimodal representation learning
  - Why needed here: AVQA requires understanding and combining information from video, audio, and text modalities. Effective fusion depends on good individual representations.
  - Quick check question: How do self-supervised pre-training methods like MAE and AST help in learning robust multimodal representations?

- Concept: Cross-modal fusion techniques
  - Why needed here: MERA-L, MERA-C, and MERA-T use different fusion strategies (LSTM, CNN, Transformer). Understanding these is key to designing and debugging the models.
  - Quick check question: What are the advantages and disadvantages of concatenation-based fusion versus attention-based fusion in multimodal learning?

- Concept: Machine translation evaluation metrics
  - Why needed here: The quality of the multilingual datasets depends on translation accuracy. BLEU, ROUGE, and METEOR are used to assess this.
  - Quick check question: What do BLEU, ROUGE, and METEOR measure, and why are they important for evaluating machine-translated QA datasets?

## Architecture Onboarding

- Component map: Foundation Models (frozen) -> MERA Model(s) -> Ensemble (optional) -> Classification Head -> Answer Prediction

- Critical path: Foundation Models → MERA Model(s) → Ensemble (optional) → Classification Head → Answer Prediction

- Design tradeoffs:
  - Freezing foundation models vs. fine-tuning: Freezing preserves pre-trained knowledge and reduces parameters, but may miss task-specific nuances.
  - Model diversity vs. ensemble complexity: More diverse models can improve ensemble performance, but increase inference time and complexity.
  - Translation quality vs. annotation cost: Machine translation is cheaper but may introduce errors; manual annotation is expensive but potentially more accurate.

- Failure signatures:
  - Low performance across all languages: Likely foundation model or fusion issues.
  - Good performance on some languages but poor on others: Possibly translation quality issues or language-specific model biases.
  - High variance in performance across question types: May indicate model struggles with certain reasoning types (e.g., temporal vs. spatial).

- First 3 experiments:
  1. Train and evaluate MERA-C on the English subset of m-MUSIC-AVQA to establish a baseline.
  2. Compare MERA-L, MERA-C, and MERA-T on the same English subset to assess architectural differences.
  3. Implement and test the weighted ensemble of the three MERA models on the English subset to verify performance gains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of multilingual AVQA models compare to English-only models on the same tasks, and what factors contribute to any performance gaps?
- Basis in paper: [inferred] The paper introduces multilingual AVQA datasets but does not provide a direct comparison with English-only models on the same tasks.
- Why unresolved: The study focuses on developing and benchmarking models on multilingual datasets without comparing their performance to English-only models.
- What evidence would resolve it: Conducting experiments to compare the performance of multilingual models with English-only models on the same tasks, and analyzing factors such as dataset size, language complexity, and model architecture that contribute to performance differences.

### Open Question 2
- Question: How does the choice of machine translation tool affect the quality of the multilingual AVQA datasets, and what are the implications for model performance?
- Basis in paper: [explicit] The paper mentions that Google's machine translation API was chosen for its superior translation accuracy compared to other tools.
- Why unresolved: The study does not explore the impact of using different machine translation tools on dataset quality and model performance.
- What evidence would resolve it: Conducting experiments using multiple machine translation tools to create multilingual datasets and comparing the resulting model performance to determine the impact of translation quality on AVQA tasks.

### Open Question 3
- Question: What are the limitations of using weighted-ensemble methods for improving multilingual AVQA performance, and how can these limitations be addressed?
- Basis in paper: [explicit] The paper shows that weighted-ensemble of models improves performance across languages and question types but notes the increased inference time.
- Why unresolved: The study does not explore the trade-offs between performance improvement and computational cost, nor does it propose solutions to address these limitations.
- What evidence would resolve it: Investigating alternative ensemble methods that balance performance gains with computational efficiency, and developing strategies to optimize ensemble weights dynamically based on input characteristics.

## Limitations

- Translation-based dataset creation may introduce semantic drift across languages, though translation quality metrics were reported
- The paper does not provide detailed ablation studies on the foundation models' frozen versus fine-tuned performance
- Equal ensemble weights (α=β=γ=0.33) were used without exploring optimal weighting strategies, potentially limiting performance gains

## Confidence

**High confidence**: The MERA framework architecture and general approach to multilingual AVQA are sound and well-motivated by existing multimodal learning principles.

**Medium confidence**: The specific performance improvements from ensemble methods and the relative ranking of MERA variants, as these depend on implementation details not fully specified.

**Low confidence**: The absolute translation quality and its impact on downstream model performance, given the limited details on human verification processes.

## Next Checks

1. **Translation Quality Verification**: Re-evaluate a random sample of translated questions and answers using multiple translation quality metrics to confirm the reported BLEU/ROUGE/METEOR scores and identify potential semantic inconsistencies.

2. **Foundation Model Sensitivity**: Conduct ablation studies comparing frozen versus fine-tuned foundation models across all eight languages to determine if freezing is optimal or limiting performance.

3. **Ensemble Weight Optimization**: Systematically explore different ensemble weighting strategies (learned weights vs. equal weights) to verify that the equal weighting approach is indeed optimal or near-optimal for this task.