---
ver: rpa2
title: TrackGPT -- A generative pre-trained transformer for cross-domain entity trajectory
  forecasting
arxiv_id: '2402.00066'
source_url: https://arxiv.org/abs/2402.00066
tags:
- data
- trackgpt
- forecasting
- forecast
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TrackGPT is a GPT-based model for entity trajectory forecasting
  that works across maritime and air domains. It uses geohashes as input tokens, requires
  only position and time features, and employs a decoder-only transformer architecture.
---

# TrackGPT -- A generative pre-trained transformer for cross-domain entity trajectory forecasting

## Quick Facts
- arXiv ID: 2402.00066
- Source URL: https://arxiv.org/abs/2402.00066
- Reference count: 26
- Cross-domain GPT-based trajectory forecasting achieving 15.07 NM forecast error at 15 hours ahead for vessels and 29.3 m ADE for aircraft

## Executive Summary
TrackGPT is a GPT-based model for entity trajectory forecasting that works across maritime and air domains. It uses geohashes as input tokens, requires only position and time features, and employs a decoder-only transformer architecture. The model was benchmarked on two datasets: DMA AIS (vessel) and TrajAir ADS-B (aircraft). On the AIS dataset, TrackGPT achieved an average forecast error of 15.07 NM at 15 hours ahead, outperforming other models despite using fewer input features. On the ADS-B dataset, it achieved 29.3 m ADE and 49.7 m FDE, surpassing a purpose-built model. The Forecast Regulator module eliminates hallucinations and ensembles predictions. TrackGPT's cross-domain applicability, minimal feature requirements, and strong performance demonstrate its potential for real-world trajectory forecasting applications.

## Method Summary
TrackGPT uses a decoder-only transformer architecture that processes trajectory data represented as geohash sequences. The model takes position (latitude/longitude) and time as inputs, converting them to geohashes through a Geospatial Encoder that handles interpolation and resampling. A 792-million parameter GPT model generates future position predictions autoregressively, with a Forecast Regulator module that eliminates hallucinations by enforcing spatial continuity constraints and ensembles multiple predictions. The model uses 16-bit vocabulary for geohashes, with 5-character precision for AIS data and 8-character for ADS-B data, and employs fixed sampling intervals for uniform track representation.

## Key Results
- On AIS dataset: Achieved 15.07 NM average forecast error at 15 hours ahead, outperforming competing models
- On ADS-B dataset: Achieved 29.3 m Average Displacement Error and 49.7 m Final Displacement Error
- Cross-domain performance: Demonstrated strong results on both maritime vessels and aircraft using identical architecture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The autoregressive decoder-only transformer architecture enables accurate trajectory forecasting by modeling sequential dependencies without access to future information
- Mechanism: The masked self-attention in the decoder ensures the model only attends to past and present tokens when predicting future positions, preventing data leakage and encouraging genuine forecasting capability
- Core assumption: The temporal ordering of geohash tokens preserves sufficient information about entity movement patterns
- Evidence anchors:
  - [abstract]: "TrackGPT stands as a pioneering GPT model capable of producing accurate predictions across diverse entity time series datasets, demonstrating proficiency in generating both long-term forecasts with sustained accuracy and short-term forecasts with high precision"
  - [section]: "The decoder in a GPT model uses a specific type of attention mechanism known as masked self-attention. In a decoder-only transformer like GPT, the attention mechanism is 'masked' to prevent it from looking at future parts of the input when generating the output"
  - [corpus]: Weak evidence - corpus focuses on time series forecasting but doesn't specifically address autoregressive vs encoder-decoder architectures for trajectory forecasting
- Break condition: If the temporal sequence loses critical movement information that requires bidirectional context, the autoregressive constraint could limit performance

### Mechanism 2
- Claim: The hierarchical geohash representation enables domain-agnostic forecasting by abstracting spatial positions into discrete tokens
- Mechanism: Geohashes provide a hierarchical grid system where each character represents a refinement of spatial resolution, allowing the model to learn spatial patterns at multiple scales through token embeddings
- Core assumption: The spatial relationships between geohash tokens can be effectively learned through the transformer's attention mechanism
- Evidence anchors:
  - [section]: "Geohashing offers a convenient method of representing locations with arbitrary degrees of precision, using a hierarchical, grid-based approach"
  - [section]: "TrackGPT may also shift datasets to neighboring geohashes on input if it results in a larger fixed geohash prefix and therefore increased forecast resolution"
  - [corpus]: Missing - corpus neighbors focus on time series but don't discuss spatial encoding methods
- Break condition: If the geohash grid resolution is too coarse for the domain, important movement details may be lost; if too fine, vocabulary size becomes unmanageable

### Mechanism 3
- Claim: The Forecast Regulator eliminates hallucinations by enforcing spatial continuity constraints on predictions
- Mechanism: The Forecast Regulator checks that successive predictions are within N hops of each other, where N is tuned to the entity type and resolution, and ensembles multiple predictions to improve accuracy
- Core assumption: Real entity trajectories exhibit spatial continuity that can be used as a constraint on predictions
- Evidence anchors:
  - [section]: "To eliminate hallucinations, the Forecast Regulator examines each forecast output by the TrackGPT decoder. Since the Geospatial Encoder ensures tracks contain few geohash gaps, the Forecast Regulator can simply check that successive predictions are no more than N hops apart"
  - [section]: "TrackGPT leverages batch predictions to perform multiple simultaneous forecasts per input track... The Forecast Regulator can then ensemble N forecasts and determine mean routes, consensus destinations, and waypoints to include in final output"
  - [corpus]: Missing - corpus neighbors don't discuss hallucination detection or spatial continuity constraints
- Break condition: If N is set too conservatively, valid predictions may be rejected; if too lenient, hallucinations may slip through

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Understanding how self-attention and masked attention work is critical for grasping why TrackGPT can model sequential dependencies in trajectory data
  - Quick check question: How does masked self-attention differ from regular self-attention, and why is this distinction important for autoregressive forecasting?

- Concept: Geohash encoding and hierarchical spatial representation
  - Why needed here: The geohash system is fundamental to how TrackGPT represents spatial positions as discrete tokens, enabling the transformer to process location data
  - Quick check question: How does the hierarchical nature of geohashes (each character representing a refinement) enable multi-scale spatial modeling?

- Concept: Time series forecasting and sequence modeling
  - Why needed here: Understanding the challenges of time series forecasting (temporal dependencies, noise, irregular sampling) helps explain why TrackGPT's approach is effective
  - Quick check question: What are the key differences between forecasting with spatial-temporal data versus purely temporal data?

## Architecture Onboarding

- Component map: Raw track data -> Geospatial Encoder -> TrackGPT model -> Geospatial Decoder -> Forecast Regulator -> Final forecasts

- Critical path:
  1. Raw track data → Geospatial Encoder (interpolation, resampling, geohash conversion)
  2. Geohash sequences → TrackGPT model (autoregressive prediction)
  3. Predicted tokens → Geospatial Decoder (geohash reconstruction)
  4. Reconstructed tracks → Forecast Regulator (ensemble, hallucination elimination)
  5. Final forecasts output

- Design tradeoffs:
  - Resolution vs. vocabulary size: Higher geohash precision requires larger vocabularies and more parameters
  - Autoregressive vs. bidirectional: Autoregressive prevents data leakage but may miss context
  - Minimal features vs. rich features: Fewer features improve generalization but may lose important signals

- Failure signatures:
  - Geohash gaps in predictions indicate hallucination or insufficient context
  - Consistently poor performance on one domain suggests resolution mismatch
  - High variance in ensemble predictions indicates temperature setting issues

- First 3 experiments:
  1. Test interpolation and resampling: Feed simple linear tracks and verify correct geohash sequence generation
  2. Validate autoregressive behavior: Check that model predictions only depend on past tokens during inference
  3. Benchmark hallucination detection: Generate predictions with forced spatial discontinuities and verify Forecast Regulator catches them

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the model architecture generalize effectively across other entity types beyond maritime vessels and aircraft?
- Basis in paper: [explicit] The authors state they "expect [TrackGPT] to perform well in others" beyond maritime and air domains
- Why unresolved: The paper only demonstrates results on two specific domains (AIS and ADS-B). Testing on other domains like ground vehicles, pedestrians, or spacecraft would validate the cross-domain claim.
- What evidence would resolve it: Successful performance on multiple new entity types (e.g., vehicles, pedestrians, drones) using the same unmodified architecture and geohash-based input representation.

### Open Question 2
- Question: What is the optimal precision level for different forecasting scenarios and how does it trade off against computational requirements?
- Basis in paper: [inferred] The authors note that 5-character geohashes provided 1.3NM resolution but ideal would be 6-character for 0.33NM resolution, limited by hardware constraints
- Why unresolved: The paper shows hardware limitations prevented testing higher precision, but doesn't establish what precision is needed for different use cases or the computational scaling behavior
- What evidence would resolve it: Systematic testing across multiple precision levels showing forecast accuracy gains versus GPU memory requirements and inference latency for different domain-specific applications.

### Open Question 3
- Question: How does TrackGPT's performance compare when incorporating domain-specific features that other models use?
- Basis in paper: [explicit] TrackGPT achieved superior results despite using only location and time features, while competing models used additional features like SOG, COG, and weather data
- Why unresolved: The paper suggests TrackGPT might perform better or worse if augmented with domain-specific features, but doesn't test this hypothesis
- What evidence would resolve it: Comparative testing showing whether adding traditional features (speed, course, weather) to TrackGPT's input improves or degrades performance relative to purpose-built models.

## Limitations

- Domain Specificity Constraints: While TrackGPT demonstrates cross-domain capability between maritime and air domains, its performance on domains with fundamentally different movement patterns (e.g., pedestrian trajectories, animal migration) remains untested.
- Feature Restriction Trade-off: The model's reliance solely on position and time features, while enabling cross-domain generalization, inherently limits its ability to capture domain-specific contextual factors like weather conditions, traffic density, or regulatory constraints.
- Scalability Concerns: The fixed vocabulary size and parameter count may not scale efficiently to extremely high-resolution trajectory data or very long forecast horizons, potentially requiring architectural modifications.

## Confidence

- **High Confidence**: The core mechanism of using autoregressive transformer architecture for trajectory forecasting is well-supported by empirical results on both AIS and ADS-B datasets.
- **Medium Confidence**: The cross-domain applicability claim is supported by strong performance on two distinct domains, but the generalization to additional domains remains theoretical without further testing.
- **Low Confidence**: The long-term forecast accuracy (15+ hours) predictions rely heavily on the Forecast Regulator's ensemble mechanism, and the model's behavior in extreme edge cases or rare trajectory patterns has not been thoroughly characterized.

## Next Checks

1. **Domain Transferability Test**: Evaluate TrackGPT performance on a third trajectory domain (e.g., vehicle traffic data or pedestrian movement) using the same geohash resolution and parameters, measuring whether the cross-domain capability extends beyond the two validated domains.
2. **Feature Sensitivity Analysis**: Systematically reintroduce domain-specific features (e.g., vessel type, aircraft speed, weather conditions) to determine the performance trade-off between feature richness and cross-domain generalization.
3. **Stress Testing for Hallucinations**: Generate synthetic tracks with extreme but physically plausible movement patterns and verify that the Forecast Regulator consistently identifies and rejects unrealistic predictions while preserving valid trajectories.