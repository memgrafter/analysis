---
ver: rpa2
title: 'GPD-1: Generative Pre-training for Driving'
arxiv_id: '2412.08643'
source_url: https://arxiv.org/abs/2412.08643
tags:
- driving
- scene
- agent
- tokens
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents GPD-1, a generative pre-training model for autonomous
  driving that jointly models scene evolution, agent movements, and map changes. It
  formulates autonomous driving as a unified token generation problem, using an autoregressive
  transformer with scene-level attention masks to capture spatial and temporal relationships.
---

# GPD-1: Generative Pre-training for Driving

## Quick Facts
- arXiv ID: 2412.08643
- Source URL: https://arxiv.org/abs/2412.08643
- Authors: Zixun Xie; Sicheng Zuo; Wenzhao Zheng; Yunpeng Zhang; Dalong Du; Jie Zhou; Jiwen Lu; Shanghang Zhang
- Reference count: 40
- Primary result: GPD-1 achieves competitive performance on nuPlan benchmark with strong ADE/FDE metrics and low collision rates without fine-tuning

## Executive Summary
GPD-1 presents a generative pre-training model for autonomous driving that jointly models scene evolution, agent movements, and map changes. The model formulates autonomous driving as a unified token generation problem using an autoregressive transformer with scene-level attention masks. By employing hierarchical positional tokenizers for agents and a VQ-VAE for map compression, GPD-1 successfully performs scene generation, traffic simulation, closed-loop simulation, and motion planning without fine-tuning on the nuPlan benchmark.

## Method Summary
GPD-1 uses a two-stage training approach with the nuPlan dataset. First, a map VQ-VAE tokenizer is trained to compress ego-centric semantic maps into discrete tokens. Then, an autoregressive transformer with scene-level attention mask is trained to predict future scene tokens. The model employs hierarchical quantization for agent positions and headings, and uses DETR decoders to convert predicted tokens back into vectorized representations for evaluation.

## Key Results
- Achieves competitive ADE and FDE metrics on nuPlan benchmark for trajectory prediction
- Demonstrates low collision rates in closed-loop simulation scenarios
- Successfully performs multiple downstream tasks (scene generation, traffic simulation, motion planning) without fine-tuning
- Shows strong generalization across 4 urban areas and 75 scenario types from nuPlan dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The hierarchical positional tokenizer enables precise and noise-resistant encoding of agent positions and headings.
- Mechanism: By quantizing coordinates at multiple scales (s1=1, s2=0.01 for position; s1=20, s2=1 for heading), the tokenizer reduces the continuous feature space into discrete tokens. Each quantization level captures progressively finer detail, and sinusoidal embeddings provide positional context within the discrete space.
- Core assumption: Multi-level quantization effectively compresses spatial information without significant loss of fidelity, and the discretization reduces noise from continuous sensor data.
- Evidence anchors:
  - [section]: "For the first level, the quantized value q1 is calculated as: q1 = floor(p/s1). For levels i > 1, the quantization is performed on the residual after accounting for the previous levels: qi = floor((p - Σj=1i-1 qj · sj)/si)."
  - [abstract]: "For the ego and agent tokens, we propose a hierarchical positional tokenizer to effectively encode both 2D positions and headings."
  - [corpus]: Weak evidence; neighboring papers focus on world models or trajectory prediction, not hierarchical tokenization specifics.
- Break condition: If quantization thresholds are poorly chosen relative to the scale of agent movements, the tokenizer may lose critical spatial resolution or introduce aliasing artifacts.

### Mechanism 2
- Claim: VQ-VAE compression of map vectors into discrete tokens simplifies map prediction and improves generalization.
- Mechanism: Rasterizing vector maps into binary images, encoding them via ResNet-50 into compact latent features, and quantizing these features to the nearest codebook entry produces discrete map tokens. This discrete representation eliminates the need to predict continuous map coordinates, reducing the complexity of the generative task.
- Core assumption: The codebook captures the essential variability of map features, and the quantization preserves enough information for accurate map reconstruction.
- Evidence anchors:
  - [section]: "We rasterize the map vectors into a 2D canvas centered at the ego vehicle... We use a vector-quantized autoencoder (VQ-VAE) [42] that converts continuous map features into discrete tokens."
  - [abstract]: "For the map tokens, we train a map vector-quantized autoencoder to efficiently compress ego-centric semantic maps into discrete tokens."
  - [corpus]: No direct evidence; corpus focuses on world models and radar-camera fusion, not VQ-VAE map tokenization.
- Break condition: If the codebook is too small or poorly trained, map reconstruction quality degrades, leading to inaccurate or incomplete scene representations.

### Mechanism 3
- Claim: The scene-level attention mask enables intra-frame bi-directional interactions while preserving autoregressive temporal modeling.
- Mechanism: The attention mask is initialized as an upper triangular matrix to prevent tokens from attending to future frames, enforcing an autoregressive structure. For each time step t, the mask is adjusted to allow full interaction among tokens within the same frame, defined by: M[t·N:(t+1)·N, t·N:(t+1)·N] = 0. This configuration allows for intra-frame spatial interactions among map and agent tokens while blocking information flow from future frames.
- Core assumption: The autoregressive temporal modeling is compatible with bi-directional spatial interactions within each frame, and the mask design correctly enforces these constraints.
- Evidence anchors:
  - [section]: "The attention mechanism uses a scene-level attention mask, M, to control interactions within and across tokens in a frame... For each time step t, the mask is adjusted to allow full interaction among tokens within the same frame, defined by: M[t·N:(t+1)·N, t·N:(t+1)·N] = 0."
  - [abstract]: "We adopt the autoregressive transformer architecture and use a scene-level attention mask to enable intra-scene bi-directional interactions."
  - [corpus]: No direct evidence; corpus focuses on world models and trajectory prediction, not attention mask design.
- Break condition: If the mask is incorrectly implemented, tokens may attend to future frames (breaking causality) or fail to interact within the same frame (losing spatial coherence).

## Foundational Learning

- Concept: Vector quantization and discrete representation learning
  - Why needed here: The paper relies on VQ-VAE to compress continuous map features into discrete tokens, which are then used as inputs to the transformer. Understanding how VQ-VAE works, including codebook learning and quantization, is essential for grasping how the map tokenizer functions.
  - Quick check question: How does the VQ-VAE encoder-decoder architecture ensure that the quantized map tokens can be accurately reconstructed, and what role does the commitment loss play in this process?

- Concept: Autoregressive transformer architectures and attention masks
  - Why needed here: The model uses an autoregressive transformer with a custom scene-level attention mask. Understanding how autoregressive transformers predict sequences and how attention masks control token interactions is crucial for understanding the model's ability to generate future scenes.
  - Quick check question: In a standard autoregressive transformer, how does the upper triangular attention mask enforce causality, and how does the scene-level mask in GPD-1 extend this to allow intra-frame interactions?

- Concept: Hierarchical quantization and multi-scale feature representation
  - Why needed here: The agent tokenizer uses hierarchical quantization to encode positions and headings at multiple scales. Understanding how multi-scale quantization captures both coarse and fine details, and how it differs from single-scale quantization, is important for understanding the tokenizer's effectiveness.
  - Quick check question: What are the advantages of using hierarchical quantization with multiple thresholds (e.g., s1=1, s2=0.01) compared to a single quantization step, and how does this affect the model's ability to represent agent movements accurately?

## Architecture Onboarding

- Component map:
  - Map Tokenizer: VQ-VAE-based encoder that rasterizes vector maps, encodes them into latent features, and quantizes them to discrete tokens using a learned codebook
  - Agent Tokenizer: Hierarchical positional tokenizer that quantizes agent positions and headings at multiple scales and applies sinusoidal embeddings to create discrete agent tokens
  - Transformer Decoder: Autoregressive transformer with a scene-level attention mask that enables intra-frame bi-directional interactions and predicts future scene tokens
  - DETR Decoders: Two DETR decoders that decode agent and map tokens into vectorized representations for evaluation
  - Scene-Level Attention Mask: Custom attention mask that allows intra-frame interactions while preserving autoregressive temporal modeling

- Critical path:
  1. Input: Ground truth or predicted scene tokens from previous time steps
  2. Map Tokenizer: If using ground truth, rasterize and encode map vectors; if using predicted tokens, pass them through the frozen VQ-VAE decoder
  3. Agent Tokenizer: Quantize agent positions and headings using hierarchical quantization and apply sinusoidal embeddings
  4. Transformer Decoder: Process the combined map and agent tokens with the scene-level attention mask to predict the next set of tokens
  5. DETR Decoders: Decode the predicted tokens into vectorized agent and map representations for evaluation

- Design tradeoffs:
  - Discrete vs. continuous representations: Using discrete tokens simplifies the generative task and improves generalization but may lose some fine-grained information compared to continuous representations
  - Hierarchical quantization: Provides multi-scale representation of agent states but increases the complexity of the tokenizer and the number of tokens to process
  - Scene-level attention mask: Enables intra-frame interactions but requires careful design to balance spatial coherence and temporal causality

- Failure signatures:
  - Poor map reconstruction: Indicates issues with the VQ-VAE codebook or quantization process, leading to inaccurate map tokens
  - Inaccurate agent predictions: Suggests problems with the hierarchical quantization thresholds or sinusoidal embeddings, resulting in noisy or imprecise agent tokens
  - Temporal inconsistency: Points to errors in the scene-level attention mask implementation, causing tokens to attend to future frames or fail to interact within the same frame

- First 3 experiments:
  1. Ablation study on quantization: Train GPD-1 with and without hierarchical quantization for agent positions and headings, and compare the quality of generated trajectories using ADE and FDE metrics
  2. Map tokenizer evaluation: Evaluate the map reconstruction quality of the VQ-VAE on a held-out set of rasterized maps, using F1 score, lateral distance, and Chamfer distance as metrics
  3. Attention mask ablation: Train GPD-1 with different attention mask configurations (e.g., standard upper triangular mask, full bi-directional mask within frames, no mask) and assess the impact on scene generation quality and collision rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GPD-1 scale with increased iteration counts during autoregressive prediction, and what are the optimal trade-offs between prediction accuracy and computational cost?
- Basis in paper: [explicit] The paper notes that "the autoregressive model performs best with fewer iterations" and that "cumulative errors grow at an approximately quadratic rate" as iterations increase.
- Why unresolved: The paper does not provide a detailed analysis of the scaling behavior or quantify the trade-offs between prediction accuracy and computational cost.
- What evidence would resolve it: A systematic study varying the number of iterations and measuring both prediction accuracy (ADE/FDE) and computational cost (time/memory) would clarify the optimal trade-off.

### Open Question 2
- Question: How does the hierarchical positional tokenizer for agents perform in extreme driving scenarios, such as sharp turns or congested areas, compared to non-hierarchical methods?
- Basis in paper: [inferred] The paper introduces a hierarchical positional tokenizer to encode agent positions and headings, but does not compare its performance to non-hierarchical methods in extreme scenarios.
- Why unresolved: The paper lacks a comparative analysis of the hierarchical tokenizer's performance against other methods in challenging driving conditions.
- What evidence would resolve it: Experimental results comparing the hierarchical tokenizer to non-hierarchical methods in extreme driving scenarios, using metrics like ADE/FDE and collision rates, would provide insights into its effectiveness.

### Open Question 3
- Question: What are the limitations of GPD-1 in predicting the entry of new vehicles into the field of view, and how can these limitations be addressed?
- Basis in paper: [explicit] The paper states that "it struggles to anticipate new vehicles entering the field of view, which is challenging due to their absence in the input data."
- Why unresolved: The paper does not propose solutions or explore the reasons behind this limitation in detail.
- What evidence would resolve it: Analyzing the model's performance on datasets with varying rates of new vehicle entries and proposing architectural or training modifications to improve predictions would address this limitation.

## Limitations

- The model struggles to anticipate new vehicles entering the field of view due to their absence in the input data
- Performance degrades over longer prediction horizons as cumulative errors grow at an approximately quadratic rate
- The hierarchical quantization parameters are not fully specified, making it difficult to reproduce the optimal representation accuracy

## Confidence

- High confidence: The overall architecture design and training procedure are clearly specified, with detailed loss functions and evaluation metrics
- Medium confidence: The VQ-VAE map compression approach is well-explained, but the specific codebook size and training details could affect reproducibility
- Medium confidence: The hierarchical positional tokenizer concept is clear, but implementation details like quantization thresholds and sinusoidal embedding parameters are underspecified

## Next Checks

1. Implement and test the hierarchical positional tokenizer with varying quantization thresholds (s1, s2) to determine the optimal balance between compression and representation accuracy
2. Conduct ablation studies on the scene-level attention mask configurations to verify the claimed benefits of intra-frame bi-directional interactions while maintaining autoregressive temporal modeling
3. Evaluate the sensitivity of the model's performance to the VQ-VAE codebook size by training with different numbers of codebook entries and measuring map reconstruction quality and downstream task performance