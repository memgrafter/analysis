---
ver: rpa2
title: Automated Design of Agentic Systems
arxiv_id: '2408.08435'
source_url: https://arxiv.org/abs/2408.08435
tags:
- agent
- agents
- search
- meta
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Automated Design of Agentic Systems (ADAS) is a new research area
  that aims to automatically invent novel building blocks and design powerful agentic
  systems. This paper proposes a promising approach where agents are defined in code,
  allowing new agents to be automatically discovered by a "meta" agent programming
  them in code.
---

# Automated Design of Agentic Systems

## Quick Facts
- arXiv ID: 2408.08435
- Source URL: https://arxiv.org/abs/2408.08435
- Reference count: 40
- Primary result: Meta Agent Search discovers novel agent designs that outperform hand-designed baselines and transfer across domains

## Executive Summary
Automated Design of Agentic Systems (ADAS) is a new research area that aims to automatically invent novel building blocks and design powerful agentic systems. This paper proposes a promising approach where agents are defined in code, allowing new agents to be automatically discovered by a "meta" agent programming them in code. Following this idea, the authors present Meta Agent Search, where the meta agent iteratively builds on previous discoveries to program interesting new agents. Experiments show that Meta Agent Search consistently outperforms state-of-the-art hand-designed agents across multiple domains, and the discovered agents transfer well across models and domains.

## Method Summary
Meta Agent Search uses a meta agent (an LLM) to iteratively program new agent designs in code based on an ever-growing archive of previous discoveries. The algorithm initializes with baseline agents (Chain-of-Thought, Self-Refine, COT-SC, LLM Debate, Quality-Diversity) and then runs for N iterations (30 for reasoning domains, 25 for ARC). In each iteration, the meta agent designs a new agent, which is evaluated on target tasks, refined through self-reflection to ensure novelty and correctness, and added to the archive. The discovered agents are defined using a simple framework with FM_Module and AgentSystem classes, and the search space theoretically encompasses all possible agentic system designs due to the Turing Completeness of programming languages.

## Key Results
- Meta Agent Search consistently outperforms state-of-the-art hand-designed agents across multiple domains
- Discovered agents maintain superior performance when transferred across different models and domains
- The approach enables discovery of novel agent designs beyond existing building blocks like prompts and workflows

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Meta Agent Search discovers novel agentic system designs by iteratively programming agents in code based on an ever-growing archive of previous discoveries
- Mechanism: A meta agent uses insights from previously discovered agents to generate new agent designs in code. Each new design is evaluated, refined through self-reflection, and added to the archive. This creates a self-improving cycle where each generation builds on previous innovations
- Core assumption: The code space is expressive enough to represent any possible agentic system design, and a meta agent can effectively navigate this space to discover novel, high-performing agents
- Evidence anchors:
  - [abstract] "we show that there is an unexplored yet promising approach within ADAS where agents can be defined in code and new agents can be automatically discovered by a meta agent programming ever better ones in code"
  - [section 3] "The core idea of Meta Agent Search is to adopt FMs as meta agents to iteratively program interestingly new agents based on an ever-growing archive of previous discoveries"

### Mechanism 2
- Claim: Discovered agents transfer well across domains and models, demonstrating robustness and generality
- Mechanism: Agents discovered through Meta Agent Search capture fundamental design patterns that generalize beyond the specific domain or model they were trained on. This suggests the search process finds transferable building blocks rather than overfitting to specific contexts
- Core assumption: The design patterns that work well in one domain contain generalizable principles that can be applied to other domains
- Evidence anchors:
  - [abstract] "agents invented by Meta Agent Search maintain superior performance even when transferred across domains and models"
  - [section 4.3] "we consistently observe the surprising result that agents invented by Meta Agent Search maintain superior performance even when transferred across domains and models"

### Mechanism 3
- Claim: Defining agents in code space enables learning of all possible components (prompts, tool use, workflows) rather than being limited to specific aspects like prompts
- Mechanism: By representing agents as code, the search space encompasses all possible ways to combine and implement agentic system components. This is more expressive than approaches limited to optimizing only prompts or only workflows
- Core assumption: Most programming languages are Turing Complete, enabling representation of any agentic system design
- Evidence anchors:
  - [abstract] "Given that most programming languages, such as Python, which we use in this paper, are Turing Complete, searching within a code space theoretically enables an ADAS algorithm to discover any possible agentic systems"
  - [section 2] "we argue that the approach of using programming languages as the search space should be studied more in ADAS"

## Foundational Learning

- Concept: Turing Completeness
  - Why needed here: The paper claims that using code as the search space enables discovering "any possible agentic systems" because programming languages are Turing Complete
  - Quick check question: Can a Turing Complete language simulate any algorithmic process, and does this property directly translate to being able to represent any agentic system design?

- Concept: Open-endedness in algorithm design
  - Why needed here: Meta Agent Search is positioned within the tradition of open-ended algorithms that continuously generate novel solutions. Understanding how this differs from traditional optimization is crucial
  - Quick check question: How does the concept of "interestingness" in open-endedness differ from traditional objective maximization in optimization?

- Concept: Self-reflection and refinement in LLM-based systems
  - Why needed here: The meta agent uses self-reflection steps to improve generated agents. Understanding how this process works and its limitations is important for implementation
  - Quick check question: What are the key differences between the self-reflection process described here and traditional debugging or code review processes?

## Architecture Onboarding

- Component map:
  - Meta Agent (FM) -> Code Generation -> Self-reflection -> Evaluation -> Agent Archive -> Meta Agent (with updated archive)

- Critical path: Meta Agent → Code Generation → Self-reflection → Evaluation → Archive Update → Meta Agent (with updated archive)

- Design tradeoffs:
  - Expressiveness vs. Search Efficiency: More expressive code space enables more designs but makes search harder
  - Evaluation Cost vs. Search Quality: More thorough evaluation improves search but increases cost
  - Archive Size vs. Search Direction: Larger archives provide more context but may bias search toward similar designs

- Failure signatures:
  - Stagnation: Meta agent generates similar designs repeatedly without meaningful novelty
  - Code Errors: Generated agents contain syntax errors or runtime failures
  - Poor Transferability: Agents perform well on training domains but poorly when transferred
  - High Cost: Evaluation becomes prohibitively expensive relative to performance gains

- First 3 experiments:
  1. Implement basic Meta Agent Search with simple Chain-of-Thought baseline and test on a single domain (e.g., MGSM math problems)
  2. Add self-reflection steps and test whether they improve novelty and correctness of generated agents
  3. Test transferability by training on one domain and evaluating on held-out domains with different models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice and quality of initialization impact search effectiveness across different domains in Meta Agent Search?
- Basis in paper: [explicit] Section 6, "Future Work" subsection, discussing seeding ADAS with existing building blocks and the impact of initialization on search performance
- Why unresolved: The paper mentions that starting from scratch in the math domain led to superior performance compared to using initial solutions, but a comprehensive study on the effects of different initialization strategies across various domains is lacking
- What evidence would resolve it: Systematic experiments comparing Meta Agent Search performance with different initialization strategies (e.g., random initialization, using different sets of baseline agents, or incorporating more complex agentic building blocks) across multiple domains and tasks

### Open Question 2
- Question: Can ADAS algorithms design even better generalist agents when specifically searching for agents capable of performing well across multiple domains?
- Basis in paper: [explicit] Section 6, "Future Work" subsection, discussing the potential for ADAS to discover generalist agents capable of performing well across multiple domains
- Why unresolved: The current implementation of Meta Agent Search targets only one domain during the search. While some transferability is observed, it's unclear if optimizing for multiple domains simultaneously would yield superior generalist agents
- What evidence would resolve it: Experiments comparing the performance of agents discovered by Meta Agent Search when optimizing for single domains versus multiple domains simultaneously, measuring both domain-specific and generalist performance

### Open Question 3
- Question: What are the potential safety risks associated with ADAS-generated code, and how can they be effectively mitigated?
- Basis in paper: [explicit] Section 6, "Safety Considerations" subsection, discussing the potential for generated code to act destructively due to model limitations or alignment issues
- Why unresolved: While the paper mentions safety measures like containerized execution and manual inspections, a comprehensive analysis of potential safety risks and effective mitigation strategies for ADAS-generated code is not provided
- What evidence would resolve it: A thorough risk assessment of potential safety issues arising from ADAS-generated code, along with the development and evaluation of robust safety mechanisms, such as automated code analysis tools, formal verification techniques, or constitutional AI approaches

## Limitations

- The approach's success critically depends on the expressiveness of the code space and the meta agent's ability to navigate it effectively
- While transferability is claimed, the experiments only test across models and domains within similar task families, not truly different problem types
- The assertion that searching in code space can discover "any possible agentic systems" conflates theoretical expressiveness with practical discoverability

## Confidence

- **High Confidence**: The mechanism that code-based representation enables discovery of novel agent designs through iterative programming. The experimental results showing Meta Agent Search outperforms baseline agents are robust and well-documented
- **Medium Confidence**: The claim that discovered agents transfer well across domains and models. While the paper shows improved performance compared to baselines when transferred, the evaluation doesn't test truly out-of-distribution scenarios
- **Low Confidence**: The assertion that searching in code space can discover "any possible agentic systems." This conflates theoretical expressiveness with practical discoverability, and the paper doesn't demonstrate discovery of truly novel building blocks beyond existing agent architectures

## Next Checks

1. **Search Space Exploration Analysis**: Implement tracking of the diversity of generated agent designs over iterations. Analyze whether the meta agent genuinely explores novel regions of the design space or converges to local optima. This validates the open-endedness claim.

2. **Transferability Stress Test**: Design experiments that test transferability across more distant domains (e.g., from math to vision tasks) and with different model families (LLMs vs code generation models). This would validate whether the discovered agents capture truly generalizable design patterns.

3. **Cost-Benefit Analysis**: Measure the computational cost of Meta Agent Search relative to the performance gains achieved. Compare against alternative approaches like direct prompt optimization or workflow search in restricted spaces. This validates whether the added expressiveness justifies the increased complexity.