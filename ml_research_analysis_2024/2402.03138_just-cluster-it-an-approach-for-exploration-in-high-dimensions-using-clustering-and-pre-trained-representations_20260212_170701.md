---
ver: rpa2
title: 'Just Cluster It: An Approach for Exploration in High-Dimensions using Clustering
  and Pre-Trained Representations'
arxiv_id: '2402.03138'
source_url: https://arxiv.org/abs/2402.03138
tags:
- cluster
- clustering
- episodic
- features
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles exploration in 3-D environments by viewing it
  as a density estimation problem. The authors propose clustering episodic and global
  pre-trained DINO representations (and random features) to estimate pseudo-counts
  for intrinsic rewards.
---

# Just Cluster It: An Approach for Exploration in High-Dimensions using Clustering and Pre-Trained Representations

## Quick Facts
- **arXiv ID**: 2402.03138
- **Source URL**: https://arxiv.org/abs/2402.03138
- **Reference count**: 40
- **Primary result**: Clustering pre-trained DINO features with episodic GMM and global cosine similarity thresholding achieves up to 17K visitation counts in Habitat, outperforming prediction-error-based exploration methods.

## Executive Summary
This paper addresses exploration in 3-D environments by reframing it as a density estimation problem. The authors propose a novel approach that performs episodic clustering using Gaussian Mixture Models on pre-trained DINO representations, followed by global clustering with cosine similarity thresholding to estimate pseudo-counts for intrinsic rewards. Surprisingly, random features work well in simpler 3-D environments, but pre-trained DINO features excel in complex real-world observations. Their method outperforms other clustering and prediction-error-based exploration approaches in VizDoom and Habitat environments.

## Method Summary
The method uses a two-stage clustering approach: first, episodic clustering with a Gaussian Mixture Model on pre-trained DINO or random feature embeddings; second, global clustering where episodic cluster centers are aggregated into a cluster table using cosine similarity thresholding. The policy network (PPO) receives intrinsic rewards based on pseudo-counts derived from cluster visitation frequencies. The approach combines extrinsic rewards from the environment with intrinsic rewards from novelty, using a 42x42x3 CNN+LSTM policy network trained on downscaled observations.

## Key Results
- DINO features achieve 17K visitation counts on average in Habitat, outperforming RECODE and E3B baselines
- Random features surprisingly work well in simpler VizDoom environments but fail in complex Habitat scenes
- Optimal cosine similarity threshold κ = 0.8 for DINO features, κ = 0.9 for random features in VizDoom
- The method shows consistent improvement over baselines across both sparse and very-sparse reward settings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Clustering episodic representations with a GMM followed by global cosine similarity thresholding approximates pseudo-counts in high-dimensional state spaces.
- **Mechanism**: Episodic GMM clustering aggregates similar high-dimensional embeddings within an episode, reducing noise from individual transitions. The global cluster table then aggregates these episodic clusters across episodes using cosine similarity thresholding, controlling granularity via κ.
- **Core assumption**: The representation space has sufficient structure such that semantically similar states produce nearby embeddings, enabling effective clustering even in 3-D environments.
- **Evidence anchors**:
  - [abstract] "We propose a method that performs episodic and global clustering on random representations and on pre-trained DINO representations to count states, i.e, estimate pseudo-counts."
  - [section 3] "We define an episode as a sequence of transitions... we fit a Gaussian mixture model on the embeddings... The GMM induces the mapping g : E → { 1, . . . M} which assigns each embedding e(t) to one of the clusters."
  - [corpus] Weak - no direct corpus evidence for GMM-based pseudo-counts.
- **Break condition**: If representations lack structure (e.g., random features in complex scenes), episodic clustering fails to aggregate meaningful states, leading to sparse pseudo-counts and poor exploration.

### Mechanism 2
- **Claim**: Pre-trained DINO features provide better inductive biases for clustering in visually complex 3-D environments compared to random features.
- **Mechanism**: DINO features capture object-level invariances through self-supervised learning, creating semantically meaningful clusters even when pixel changes between transitions are large but not salient.
- **Core assumption**: Self-supervised pre-training on large-scale data produces features that generalize to unseen 3-D environments, capturing relevant visual priors.
- **Evidence anchors**:
  - [abstract] "Surprisingly, even random features can be clustered effectively to count states in 3-D environments, however when these become visually more complex, pre-trained DINO representations are more effective thanks to the pre-trained inductive biases in the representations."
  - [section 4.2] "When clustering pre-trained DINO features with our method (dashed, blue curve), we achieve 17K visitation counts on average, more than any other method."
  - [corpus] Weak - no corpus evidence for DINO vs random feature comparison in exploration.
- **Break condition**: If the pre-trained features are not aligned with the specific visual characteristics of the target environment, their advantage over random features diminishes.

### Mechanism 3
- **Claim**: The cosine similarity threshold κ controls the trade-off between exploration granularity and coverage in the cluster table.
- **Mechanism**: Higher κ values create more granular clusters (lower pseudo-counts per cluster), increasing exploration but potentially causing sparse rewards. Lower κ values create coarser clusters (higher pseudo-counts), improving stability but reducing exploration efficiency.
- **Core assumption**: The cluster table size and pseudo-count distribution directly influence the quality of the intrinsic reward signal.
- **Evidence anchors**:
  - [section 4.3] "We see that increasing the cosine similarity threshold κ increases the number of global cluster centers in the cluster table. If the number of clusters in the cluster table is too high, performance decreases as shown by lower visitation counts for higher values of κ."
  - [section 3] "This is akin to the stick breaking process in Dirichlet processes and thus allows us to control the granularity of the inter-episodic clustering."
  - [corpus] Weak - no corpus evidence for κ parameter tuning in clustering-based exploration.
- **Break condition**: If κ is set too low, the method may under-explore; if too high, the intrinsic reward becomes too sparse to be useful.

## Foundational Learning

- **Concept**: Gaussian Mixture Models (GMM)
  - Why needed here: GMM provides episodic clustering by fitting a probabilistic mixture model to representations, producing cluster centers that represent the distribution of states within an episode.
  - Quick check question: What happens to GMM performance if the number of components M is set too low or too high relative to the true cluster structure?

- **Concept**: Cosine similarity for high-dimensional vector comparison
  - Why needed here: Cosine similarity measures angular distance between cluster centers, enabling comparison of episodic clusters to global cluster table entries regardless of vector magnitude.
  - Quick check question: Why is cosine similarity preferred over Euclidean distance for comparing pre-trained embeddings like DINO features?

- **Concept**: Pseudo-counts and intrinsic motivation
  - Why needed here: Pseudo-counts estimate state visitation frequency in continuous spaces, enabling count-based exploration bonuses that drive agents to novel states.
  - Quick check question: How does the intrinsic reward formula 1/√ρ(t) relate to information gain and the theoretical guarantees of count-based exploration?

## Architecture Onboarding

- **Component map**: Observation → Feature extraction → Episodic GMM clustering → Global cluster table update → Pseudo-count calculation → Intrinsic reward → PPO update
- **Critical path**: Observation → Feature extraction → Episodic GMM clustering → Global cluster table update → Pseudo-count calculation → Intrinsic reward → PPO update
- **Design tradeoffs**:
  - Feature choice: DINO features capture semantic structure but require pre-training; random features are simpler but may lack structure in complex scenes
  - Cluster granularity: Higher κ increases exploration but may create sparse rewards; lower κ improves stability but may under-explore
  - Episodic vs global clustering: Episodic clustering reduces noise but requires memory; global clustering provides long-term statistics but may be slower to adapt
- **Failure signatures**:
  - Poor exploration: Cluster table grows too slowly, intrinsic rewards remain low
  - Unstable learning: Cluster table grows too quickly, pseudo-counts become sparse
  - No improvement over baselines: Representations lack structure for effective clustering
- **First 3 experiments**:
  1. Compare DINO vs random features on VizDoom with varying κ values to identify optimal granularity
  2. Test episodic clustering ablation (run without GMM, just use raw embeddings) to measure noise reduction benefit
  3. Evaluate cluster table growth rate vs visitation counts to verify proper state aggregation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the transition saliency problem in 3-D environments affect prediction-error-based exploration methods compared to clustering-based methods?
- Basis in paper: [explicit] The paper discusses how prediction-error-based methods like RND and Curiosity perform worse in high-dimensional 3-D environments compared to clustering-based methods, attributing this to the transition saliency problem.
- Why unresolved: The paper provides evidence for the effectiveness of clustering-based methods but doesn't directly compare the performance of prediction-error-based methods in 3-D environments with varying levels of pixel change saliency.
- What evidence would resolve it: A direct comparison of prediction-error-based methods in 3-D environments with different levels of pixel change saliency, showing how their performance degrades as pixel changes become less salient.

### Open Question 2
- Question: What is the optimal balance between episodic and global clustering for exploration in 3-D environments?
- Basis in paper: [explicit] The paper shows that both episodic and global clustering are beneficial, but doesn't explore the optimal balance between them or how this balance might change with different environment complexities.
- Why unresolved: The paper demonstrates the effectiveness of combining episodic and global clustering but doesn't investigate the optimal ratio or how it might vary across different environments.
- What evidence would resolve it: Experiments varying the relative importance of episodic vs. global clustering across multiple 3-D environments with different levels of visual complexity.

### Open Question 3
- Question: How do different types of pre-trained representations (beyond DINO) affect exploration performance in 3-D environments?
- Basis in paper: [explicit] The paper shows that DINO features are effective for exploration in 3-D environments, but doesn't compare them to other pre-trained representations like CLIP or contrastive learning models.
- Why unresolved: The paper demonstrates the effectiveness of DINO features but doesn't explore whether other pre-trained representations might be even more effective for exploration in 3-D environments.
- What evidence would resolve it: A comparison of various pre-trained representations (e.g., CLIP, MoCo, SimCLR) on the same 3-D environments used in the paper.

## Limitations
- The approach relies heavily on the quality of the representation space - if embeddings lack semantic structure, clustering will fail to aggregate meaningful states
- Random feature ablation shows surprisingly good performance in simpler environments, suggesting the method may not always need sophisticated pre-trained features
- The paper doesn't provide systematic analysis of representation quality requirements or failure modes when representations are inadequate

## Confidence
- **High confidence**: The core mechanism of episodic GMM clustering followed by global cosine similarity thresholding is well-specified and reproducible
- **Medium confidence**: Claims about DINO features outperforming random features in complex environments are supported by results but lack systematic comparison across multiple representation types
- **Medium confidence**: The relationship between κ parameter values and exploration performance is demonstrated but could benefit from more extensive hyperparameter sensitivity analysis

## Next Checks
1. Test the method with different pre-trained models (CLIP, MoCo, SimCLR) to determine if DINO features have unique advantages for exploration
2. Evaluate performance with corrupted or random representations to establish minimum quality requirements for effective clustering
3. Compare against non-clustering exploration methods (RND, curiosity) in the same environments to validate relative performance claims