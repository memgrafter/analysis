---
ver: rpa2
title: Video Motion Transfer with Diffusion Transformers
arxiv_id: '2412.07776'
source_url: https://arxiv.org/abs/2412.07776
tags:
- motion
- video
- diffusion
- ditflow
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DiTFlow, the first method for transferring
  video motion using Diffusion Transformers (DiTs). The key innovation is Attention
  Motion Flow (AMF), a patch-wise motion signal extracted by analyzing cross-frame
  attention maps in DiTs.
---

# Video Motion Transfer with Diffusion Transformers

## Quick Facts
- arXiv ID: 2412.07776
- Source URL: https://arxiv.org/abs/2412.07776
- Authors: Alexander Pondaven; Aliaksandr Siarohin; Sergey Tulyakov; Philip Torr; Fabio Pizzati
- Reference count: 40
- This paper introduces DiTFlow, the first method for transferring video motion using Diffusion Transformers (DiTs)

## Executive Summary
This paper introduces DiTFlow, the first method for transferring video motion using Diffusion Transformers (DiTs). The key innovation is Attention Motion Flow (AMF), a patch-wise motion signal extracted by analyzing cross-frame attention maps in DiTs. DiTFlow optimizes latents and positional embeddings in a training-free manner to guide video synthesis, achieving superior motion transfer performance compared to existing methods. On the DA VIS dataset, DiTFlow outperforms baselines in motion fidelity and human evaluation, scoring 0.785 MF versus 0.766 for the best baseline on CogVideoX-5B. Optimizing positional embeddings also enables zero-shot motion transfer without re-optimization.

## Method Summary
DiTFlow extracts a patch-wise motion signal called Attention Motion Flow (AMF) from cross-frame attention maps in pre-trained Diffusion Transformers. The method processes a reference video through the DiT to obtain cross-frame attention matrices, from which displacement vectors are computed for each patch. During generation, latents are optimized using an AMF loss that minimizes the difference between the reference AMF and the AMF extracted from current latents. The optimization targets either latents or positional embeddings, with the latter enabling zero-shot transfer. The method uses a soft AMF loss based on continuous displacement values to preserve gradients, and optimizes for the first 20% of denoising steps before proceeding with standard denoising.

## Key Results
- DiTFlow achieves 0.785 Motion Fidelity (MF) on DA VIS dataset, outperforming best baseline (0.766) on CogVideoX-5B
- Human evaluation shows superior motion adherence compared to existing methods
- Optimizing positional embeddings enables zero-shot motion transfer without re-optimization
- DiTFlow successfully transfers complex motions like walking, jumping, and waving

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DiTFlow extracts a patch-wise motion signal by analyzing cross-frame attention maps in DiTs, enabling precise motion transfer.
- Mechanism: The method computes cross-frame attention matrices (A⊗i,j) by averaging over M attention heads. It then applies an argmax operation to assign each patch in frame i to the most attended patch in frame j, constructing a displacement matrix ∆i,j that captures how content moves spatially over time.
- Core assumption: Cross-frame attention patterns in DiTs encode meaningful motion information that can be extracted and used for guidance.
- Evidence anchors:
  - [abstract] "We first process the reference video with a pre-trained DiT to analyze cross-frame attention maps and extract a patch-wise motion signal called the Attention Motion Flow (AMF)."
  - [section 4.1] "We aim to extract the diffusion features of a reference video xref with a pretrained T2V DiT model in order to obtain a signal for motion."
  - [corpus] Weak evidence for DiT-specific motion extraction; related work focuses on UNet or general attention control.
- Break condition: If attention maps do not encode coherent motion patterns, or if patch assignments are noisy due to argmax operation.

### Mechanism 2
- Claim: Optimizing positional embeddings in DiTs enables zero-shot motion transfer without re-optimization.
- Mechanism: By backpropagating the AMF loss to the positional embeddings ρt, the model learns to reorganize patches in a way that encodes motion patterns. This learned embedding can then be reused for new prompts without additional optimization.
- Core assumption: Positional embeddings in DiTs encode spatiotemporal location information that can be manipulated to guide motion without affecting content.
- Evidence anchors:
  - [abstract] "We also apply our optimization strategy to transformer positional embeddings, granting us a boost in zero-shot motion transfer capabilities."
  - [section 4.3] "By manipulating the positional information of latent patches, we guide the reorganization of patches for motion transfer. This is also disentangled from latents that encode content."
  - [corpus] Weak evidence; most related work focuses on latent optimization rather than positional embedding optimization for motion transfer.
- Break condition: If positional embeddings become entangled with content information, or if zero-shot transfer degrades significantly.

### Mechanism 3
- Claim: The soft AMF loss based on continuous displacement values preserves gradients and provides smoother guidance than discrete argmax.
- Mechanism: Instead of using discrete argmax to compute displacement matrices, a weighted sum of attention values is used to calculate continuous displacement values, preserving gradients during optimization.
- Core assumption: Continuous displacement values provide better gradient flow and smoother motion guidance than discrete indices.
- Evidence anchors:
  - [section 4.2] "Rather than using argmax, we perform a weighted sum of attention values to identify continuous displacement values that preserve gradients where: ˜∆i,j[(u, v)] = Σ(u′,v′) ˜A⊗i,j[(u, v), (u′, v′)] · (u′ − u, v′ − v)"
  - [corpus] Weak evidence for continuous vs discrete displacement comparison in motion transfer literature.
- Break condition: If continuous displacements introduce too much noise or if discrete argmax provides cleaner guidance in practice.

## Foundational Learning

- Concept: Diffusion models and the denoising process
  - Why needed here: Understanding how latent variables are iteratively denoised is crucial for implementing the optimization-based guidance strategy.
  - Quick check question: What is the shape of the latent representation z0 after decoding in a Latent Diffusion Model?

- Concept: Transformer attention mechanisms and positional embeddings
  - Why needed here: The method relies on analyzing attention maps and manipulating positional embeddings, which are core transformer concepts.
  - Quick check question: How do rotary positional embeddings (RoPE) differ from sinusoidal embeddings in their application to attention?

- Concept: Video representation and motion tracking
  - Why needed here: The method involves extracting motion patterns from videos and comparing them using motion fidelity metrics.
  - Quick check question: What is the difference between optical flow and the patch-wise displacement vectors used in AMF?

## Architecture Onboarding

- Component map:
  Reference video processing pipeline: E(xref) -> zref -> DiT attention blocks -> AMF extraction
  Generation pipeline: zT ~ N(0,I) -> iterative denoising with optimization -> D(z0)
  AMF guidance: Compute cross-frame attention -> displacement matrices -> loss minimization
  Optimization targets: latents zt OR positional embeddings ρt

- Critical path:
  1. Extract AMF from reference video using cross-frame attention analysis
  2. Initialize random latents zT
  3. For each denoising step t, compute soft AMF from current latents
  4. Minimize element-wise Euclidean distance between reference AMF and current AMF
  5. Update either latents or positional embeddings
  6. Continue denoising to generate final video

- Design tradeoffs:
  - Latents vs positional embeddings: Latents provide better overall performance but require optimization per video; positional embeddings enable zero-shot transfer but with slightly lower performance
  - Discrete vs continuous displacement: Discrete argmax provides cleaner motion maps but loses gradient information; continuous displacements preserve gradients but may introduce noise
  - Single block vs multi-block guidance: Single block (20th) is computationally efficient; multi-block (20+15+10) provides marginal improvement at higher cost

- Failure signatures:
  - Motion transfer fails if AMF extraction produces noisy displacement maps
  - Zero-shot transfer degrades if positional embeddings become entangled with content
  - Optimization gets stuck if learning rate is too high or too low
  - Generated videos lack coherence if AMF loss weight is inappropriate

- First 3 experiments:
  1. Verify AMF extraction: Process a simple reference video with known motion and visualize the extracted displacement vectors to ensure they point in the correct direction
  2. Test optimization convergence: Run the full DiTFlow pipeline on a short video and monitor the AMF loss and motion fidelity metric across optimization steps
  3. Compare latents vs positional embeddings: Optimize both targets on the same video and evaluate zero-shot transfer capability with new prompts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Attention Motion Flow (AMF) compare to other motion representations in terms of computational efficiency and accuracy for different types of motion (e.g., camera motion vs. object motion)?
- Basis in paper: [explicit] The paper introduces AMF as a patch-wise motion signal extracted from cross-frame attention maps, but does not provide a detailed comparison with other motion representations.
- Why unresolved: The paper focuses on demonstrating the effectiveness of AMF for motion transfer but does not explore its computational efficiency or accuracy compared to other methods.
- What evidence would resolve it: A detailed analysis comparing AMF to other motion representations (e.g., optical flow, trajectory-based methods) in terms of computational cost and accuracy across various motion types would provide insights into its strengths and limitations.

### Open Question 2
- Question: Can the optimization of positional embeddings in DiTFlow be extended to other transformer-based architectures beyond DiTs, and what are the potential challenges?
- Basis in paper: [inferred] The paper discusses optimizing positional embeddings in DiTs for zero-shot motion transfer, suggesting potential applicability to other architectures.
- Why unresolved: The paper does not explore the extension of this optimization strategy to other transformer-based models, leaving open questions about its broader applicability.
- What evidence would resolve it: Experiments applying the positional embedding optimization strategy to other transformer architectures (e.g., Vision Transformers, GPT models) and analyzing the results would clarify its potential and challenges.

### Open Question 3
- Question: What are the limitations of DiTFlow in handling complex or out-of-distribution motion patterns, and how can these be addressed?
- Basis in paper: [explicit] The paper mentions limitations in transferring complex motions like backflips and the ambiguity in motion transfer without prompts.
- Why unresolved: The paper identifies these limitations but does not provide solutions or detailed analysis of their impact on motion transfer quality.
- What evidence would resolve it: Further experiments and analysis focusing on complex motion patterns and the development of strategies to handle out-of-distribution motions would address these limitations.

## Limitations

- Architecture Specificity: The method's reliance on DiT-specific attention mechanisms raises questions about generalizability to other transformer architectures
- Dataset Bias: Evaluation is limited to DA VIS dataset with only 50 unique videos, which may not capture full diversity of motion patterns
- Optimization Stability: Fixed optimization schedule (20% of denoising steps) without thorough exploration of hyperparameter sensitivity

## Confidence

**High Confidence (8/10)**:
- The AMF extraction mechanism is technically sound and the mathematical formulation is correct
- The optimization-based guidance approach is well-established in diffusion literature
- The core idea of using cross-frame attention for motion extraction is novel and plausible

**Medium Confidence (6/10)**:
- The quantitative improvements over baselines are significant but based on a limited dataset
- The zero-shot transfer capability is demonstrated but the performance gap versus optimized latents needs more exploration
- The human evaluation results support the quantitative findings but sample sizes aren't specified

**Low Confidence (4/10)**:
- Generalizability to other DiT architectures beyond CogVideoX
- Robustness across diverse video content and motion patterns
- Sensitivity to hyperparameters and optimization settings

## Next Checks

1. **Cross-architecture Validation**: Test AMF extraction and motion transfer with a different DiT architecture (e.g., Stable Video Diffusion or Hunyuan Video) to verify the method's generalizability beyond CogVideoX models.

2. **Dataset Diversity Assessment**: Evaluate DiTFlow on a more diverse video dataset with varied motion patterns, camera movements, and scene complexities to test robustness beyond the DA VIS dataset.

3. **Hyperparameter Sensitivity Analysis**: Systematically vary the optimization schedule (percentage of denoising steps), learning rate, and AMF loss weight to determine the stability and optimal settings across different video types.