---
ver: rpa2
title: Quality Estimation with $k$-nearest Neighbors and Automatic Evaluation for
  Model-specific Quality Estimation
arxiv_id: '2404.18031'
source_url: https://arxiv.org/abs/2404.18031
tags:
- metrics
- quality
- translation
- output
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces kNN-QE, a model-specific unsupervised Quality
  Estimation approach that leverages k-nearest neighbors from an MT model''s training
  data to assess translation quality. The method retrieves nearest neighbors for each
  generated token and derives four metrics: kNN token distance, kNN sentence similarity,
  number of distinct kNN tokens, and kNN tokens matching the output token.'
---

# Quality Estimation with $k$-nearest Neighbors and Model-specific Quality Estimation

## Quick Facts
- arXiv ID: 2404.18031
- Source URL: https://arxiv.org/abs/2404.18031
- Authors: Tu Anh Dinh; Tobias Palzer; Jan Niehues
- Reference count: 13
- kNN-QE improves baseline MT output probability method from 0.535 to 0.650 Spearman correlation on TED data

## Executive Summary
This paper introduces kNN-QE, an unsupervised Quality Estimation approach that leverages k-nearest neighbors from an MT model's training data to assess translation quality. The method retrieves nearest neighbors for each generated token and derives four metrics to estimate translation quality without reference translations. The approach works best with a small number of neighbors (k=1) and partial access to training data, improving inference efficiency. An automatic evaluation method using reference-based metrics as gold standard is proposed and validated, with MetricX-23 XL identified as most robust for ranking QE metrics.

## Method Summary
kNN-QE is a model-specific unsupervised Quality Estimation approach that retrieves k-nearest neighbors from an MT model's training data for each generated token. The method computes four metrics: kNN token distance, kNN sentence similarity, number of distinct kNN tokens, and kNN tokens matching the output token. These metrics collectively assess translation quality by measuring how well the generated tokens align with patterns seen during training. The approach requires only the trained MT model and its training data, making it unsupervised and model-specific.

## Key Results
- kNN-QE achieves 0.650 Spearman correlation on TED data, outperforming MT output probability baseline (0.535)
- Best performance achieved with k=1 nearest neighbors
- Method works effectively with partial training data access
- MetricX-23 XL identified as most robust reference-based metric for automatic QE evaluation
- Demonstrates effectiveness on out-of-domain test sets with appropriate training data

## Why This Works (Mechanism)
kNN-QE leverages the intuition that high-quality translations should resemble patterns seen during the MT model's training. By retrieving nearest neighbors for each generated token, the method captures semantic and syntactic similarities between the output and the training distribution. Tokens that deviate significantly from their nearest neighbors or have dissimilar sentence contexts likely indicate translation errors. The approach benefits from the MT model's implicit knowledge encoded in its training data, using this information to assess the quality of its own outputs.

## Foundational Learning
- **Quality Estimation (QE)**: Task of predicting translation quality without reference translations; needed to evaluate MT systems in real-world scenarios where references are unavailable; quick check: identify use cases where references cannot be obtained
- **k-nearest Neighbors (kNN)**: Retrieval method finding most similar instances in a dataset; needed to measure semantic similarity between generated tokens and training examples; quick check: verify distance metrics and k parameter selection
- **Model-specific QE**: Quality estimation tailored to a specific MT model using its training data; needed to leverage model-specific patterns and distributions; quick check: confirm access to training data and model architecture
- **Reference-based metrics**: Automated metrics comparing translations to references; needed as proxy for human judgment in evaluation; quick check: assess correlation between metrics and human scores
- **Unsupervised learning**: Learning without labeled training data; needed to avoid costly human annotations; quick check: verify no reference translations are required
- **Spearman correlation**: Rank-based correlation metric; needed to evaluate ranking quality of QE methods; quick check: confirm monotonic relationship with human judgments

## Architecture Onboarding

Component Map:
MT Model -> Token Generation -> kNN Retrieval -> Metric Computation -> Quality Estimation

Critical Path:
Token Generation → kNN Retrieval → Metric Computation → Quality Estimation

Design Tradeoffs:
- k parameter: Small k (k=1) provides better performance but may miss broader context
- Training data access: Full access provides better coverage but increases computational cost
- Metric selection: Multiple metrics capture different aspects but increase complexity
- Model specificity: Model-specific approach leverages internal knowledge but lacks generalization

Failure Signatures:
- Low correlation with human judgments indicates metric insensitivity
- High computational cost suggests inefficient kNN implementation
- Poor out-of-domain performance indicates over-reliance on training data distribution
- Inconsistent results across different k values suggest metric instability

Three First Experiments:
1. Evaluate kNN-QE with different k values (1, 3, 5, 10) to determine optimal parameter
2. Test performance with varying percentages of training data (10%, 50%, 100%) to assess data efficiency
3. Compare kNN-QE against probability-based baseline on multiple datasets to verify generalizability

## Open Questions the Paper Calls Out
The paper identifies several open questions: generalizability across different MT architectures and domains, optimal data selection strategies for minimal training data, whether reference-based metrics adequately capture human judgment quality, and the computational efficiency gains from partial training data access.

## Limitations
- Limited evaluation to transformer-based models and TED data raises generalizability concerns
- Performance gap with supervised QE may limit applicability in high-stakes scenarios
- Computational efficiency analysis not fully quantified for practical deployment
- Automatic evaluation methodology assumes reference-based metrics accurately reflect human judgment

## Confidence
High confidence in technical implementation and basic effectiveness of kNN-QE.
Medium confidence in automatic evaluation methodology robustness and result generalizability.
Low confidence in scalability and practical deployment scenarios due to limited computational analysis.

## Next Checks
1. Evaluate kNN-QE performance across broader range of MT architectures beyond transformers
2. Conduct experiments with truly minimal training data subsets to determine minimum viable requirements
3. Perform comprehensive computational efficiency analysis comparing kNN-QE with supervised methods across different hardware configurations