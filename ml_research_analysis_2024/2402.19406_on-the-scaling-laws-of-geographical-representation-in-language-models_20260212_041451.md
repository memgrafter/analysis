---
ver: rpa2
title: On the Scaling Laws of Geographical Representation in Language Models
arxiv_id: '2402.19406'
source_url: https://arxiv.org/abs/2402.19406
tags:
- language
- geographical
- performance
- association
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how geographical knowledge is represented in
  language models across different model sizes and architectures. Using a probing
  approach, the authors train linear regressors to predict geographic coordinates
  from model representations, measuring performance with the R2 coefficient.
---

# On the Scaling Laws of Geographical Representation in Language Models

## Quick Facts
- **arXiv ID:** 2402.19406
- **Source URL:** https://arxiv.org/abs/2402.19406
- **Reference count:** 0
- **Key outcome:** Geographical knowledge emerges in tiny models and improves consistently with size, but larger models exhibit increased geographical bias

## Executive Summary
This paper systematically investigates how geographical knowledge is represented in language models across different model sizes and architectures. Using a probing approach, the authors train linear regressors to predict geographic coordinates from model representations and measure performance with the R² coefficient. They demonstrate that geographical knowledge emerges even in very small models and scales predictably with model size across both decoder (GPT-2, OPT, Pythia, GPT-Neo, mGPT, Llama-2) and encoder (BERT, RoBERTa, ELECTRA, DeBERTa-v3) architectures. However, they also find that larger models exhibit increased geographical bias, performing worse on locations from the Southern Hemisphere and countries less represented in training data.

## Method Summary
The authors use a probing approach to study geographical knowledge representation. They prompt models with "Where is [location] in the world?" and extract the last token representation for each location from the World dataset (39,504 location names and coordinates). A Ridge regression model is then trained to predict coordinates from these representations, with performance evaluated using the R² coefficient on a held-out test set. The study examines 12 model families ranging from 14M to 2.8B parameters, including both decoder-only and encoder-only architectures. The authors also compute the Gini coefficient of MSE across countries to measure geographical bias, and correlate performance with country name frequency in the training data.

## Key Results
- Geographical knowledge emerges in tiny models (14M parameters) and improves consistently with model size across all architectures
- Larger models show increased geographical bias with higher Gini coefficients (0.15 for 14M parameters, 0.41 for 2.8B parameters)
- Performance correlates with country name frequency in training data (Pearson correlation -0.21, p=0.06 for smallest model) but not with population counts
- Encoder-only models generally outperform decoder-only models at comparable parameter counts

## Why This Works (Mechanism)
The mechanism underlying geographical knowledge representation in language models appears to be learned through exposure to geographically relevant text during pretraining. The models implicitly learn to associate location names with their approximate coordinates through co-occurrence patterns and contextual information in the training data. This knowledge is encoded in the token representations, which can be extracted using linear probes. The consistent scaling behavior across architectures suggests that this is a fundamental property of transformer-based language models when trained on geographically diverse text.

## Foundational Learning
- **Ridge Regression:** A linear regression technique with L2 regularization used to predict coordinates from model representations. Why needed: Provides a simple, interpretable method to extract geographical knowledge from model embeddings. Quick check: Verify that R² values are positive, indicating the model captures some geographical signal.
- **R² Coefficient:** A statistical measure of how well predictions approximate actual data points. Why needed: Quantifies the proportion of variance in coordinates explained by the model representations. Quick check: Ensure R² values are significantly above 0, indicating meaningful geographical knowledge extraction.
- **Gini Coefficient:** A measure of statistical dispersion representing inequality among values. Why needed: Quantifies geographical bias by measuring inequality in prediction accuracy across different countries. Quick check: Compare Gini coefficients across model sizes to verify the trend of increasing bias with scale.

## Architecture Onboarding
**Component Map:** World dataset -> Model prompting -> Token representation extraction -> Ridge regression training -> R²/Gini coefficient calculation
**Critical Path:** Prompt generation → Representation extraction → Linear regression training → Performance evaluation
**Design Tradeoffs:** Simple linear probing vs. more complex fine-tuning approaches; coordinate prediction vs. broader geographical reasoning tasks
**Failure Signatures:** Low R² values across all models might indicate issues with the probing methodology or dataset quality; inconsistent scaling trends could suggest problems with model selection or evaluation
**First Experiments:** 1) Test the exact prompt format "Where is [location] in the world?" on a small subset of locations to verify basic functionality. 2) Extract representations from a single model and verify they are being correctly processed by the Ridge regression. 3) Compute R² on a small validation set before scaling to the full evaluation.

## Open Questions the Paper Calls Out
The paper identifies several open questions: (1) Whether geographical knowledge representation scales linearly with model size across different architectures, (2) What specific aspects of geographical knowledge are most affected by model size and training data bias, (3) How geographical representations differ between encoder-only, decoder-only, and encoder-decoder architectures at comparable parameter counts, (4) What mechanisms cause larger models to exhibit greater geographical bias despite better overall performance, and (5) How geographical knowledge representation would change with more geographically diverse training data versus larger model sizes.

## Limitations
- The study only measures coordinate prediction accuracy rather than broader geographical reasoning capabilities
- The exact train/test split methodology for the World dataset is not fully specified
- Specific model checkpoints and configurations for each family are not detailed
- The analysis focuses on a single dataset and may not generalize to all forms of geographical knowledge

## Confidence
- **High confidence:** Scaling trends and architectural comparisons are robust and clearly demonstrated
- **Medium confidence:** Geographical bias findings are well-supported but the frequency correlation is only marginally significant (p=0.06)
- **Medium confidence:** Comparative performance across model families pending clarification of specific configurations used

## Next Checks
1. Verify the exact World dataset train/test split used and reproduce the 80/20 partitioning to ensure consistent evaluation conditions
2. Test multiple random seeds for the Ridge regression training to confirm that the R² values and Gini coefficients are stable across different training runs
3. Conduct ablation studies removing location-related text from training data subsets to better understand whether geographical knowledge requires explicit geographic content in training data