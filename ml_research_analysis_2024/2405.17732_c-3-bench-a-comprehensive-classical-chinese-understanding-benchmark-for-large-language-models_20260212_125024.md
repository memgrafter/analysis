---
ver: rpa2
title: 'C$^{3}$Bench: A Comprehensive Classical Chinese Understanding Benchmark for
  Large Language Models'
arxiv_id: '2405.17732'
source_url: https://arxiv.org/abs/2405.17732
tags:
- chinese
- classical
- llms
- language
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces C3bench, the first comprehensive benchmark
  for evaluating Large Language Models (LLMs) on Classical Chinese Understanding (CCU)
  tasks. The benchmark includes 50,000 text pairs across five tasks: classification,
  retrieval, named entity recognition, punctuation, and translation, covering ten
  classical Chinese domains.'
---

# C$^{3}$Bench: A Comprehensive Classical Chinese Understanding Benchmark for Large Language Models

## Quick Facts
- arXiv ID: 2405.17732
- Source URL: https://arxiv.org/abs/2405.17732
- Reference count: 40
- Key outcome: All 15 evaluated LLMs scored below 50% on average for Classical Chinese Understanding tasks, with translation performance particularly poor (max 34.58 BLEU vs 52.02 BLEU for supervised models)

## Executive Summary
This paper introduces C3bench, the first comprehensive benchmark for evaluating Large Language Models on Classical Chinese Understanding tasks. The benchmark includes 50,000 text pairs across five tasks (classification, retrieval, NER, punctuation, translation) spanning ten classical Chinese domains. Extensive evaluations on 15 representative LLMs reveal that existing models struggle significantly with CCU tasks, scoring below 50% on average and particularly underperforming in translation tasks. The findings highlight the unique challenges of CCU and the need for specialized approaches, as current LLMs show unbalanced performance and limited transferability from modern Chinese.

## Method Summary
The C3bench benchmark was constructed by collecting 50,000 text pairs from ten classical Chinese domains and designing evaluation prompts in natural language generation format for five CCU tasks. The evaluation involved running 15 representative LLMs (including LLaMA2-Chinese, Bloom-7B-Chunhua5, Baichuan2, GPT-3.5-turbo, and GPT-4) through the benchmark using specified inference settings and post-processing to ensure format compliance. Results were aggregated using task-specific metrics: accuracy for classification/retrieval, F1 score for NER/punctuation, and BLEU score for translation.

## Key Results
- All 15 tested LLMs scored below 50% average on CCU tasks
- Translation performance was particularly poor, with maximum 34.58 BLEU score versus 52.02 BLEU for supervised models
- Performance varied significantly across classical Chinese domains, with history and Confucianism being predominant
- Current LLMs show limited transferability from modern Chinese to classical Chinese understanding

## Why This Works (Mechanism)

### Mechanism 1
The benchmark's multi-task design enables comprehensive evaluation of LLMs across diverse classical Chinese understanding domains. By structuring the benchmark around five distinct tasks (classification, retrieval, NER, punctuation, translation) across ten classical Chinese domains, the evaluation captures both breadth and depth of model capabilities.

### Mechanism 2
Natural language generation format for evaluation prompts provides more realistic assessment than multiple-choice formats. Using NLG prompts requires models to generate answers rather than select from options, testing genuine comprehension and generation capabilities.

### Mechanism 3
Large-scale training corpus with diverse classical Chinese domains creates evaluation scenarios that reveal model limitations. The 50,000 text pairs spanning ten classical Chinese domains create sufficient complexity and variety to expose systematic weaknesses in LLMs.

## Foundational Learning

- **Benchmark design principles**: Understanding how to construct comprehensive, multi-task benchmarks that fairly evaluate LLM capabilities across different domains. *Quick check: What are the key differences between multiple-choice and natural language generation evaluation formats for language models?*

- **Classical Chinese linguistic features**: Recognizing the unique challenges of classical Chinese (different vocabulary, grammar, cultural context) that make it distinct from modern Chinese. *Quick check: How does classical Chinese differ structurally from modern Chinese in ways that affect language model performance?*

- **Statistical evaluation metrics**: Understanding when to use accuracy, F1 score, BLEU, etc., and how to interpret results across different task types. *Quick check: When would you use BLEU score versus F1 score for evaluating language model outputs?*

## Architecture Onboarding

- **Component map**: Dataset construction pipeline (data collection → annotation → quality control) → Evaluation framework (prompts → model inference → post-processing) → Results aggregation and analysis
- **Critical path**: Data collection and annotation quality → Prompt design and consistency → Model inference configuration → Results post-processing → Analysis and interpretation
- **Design tradeoffs**: Comprehensive multi-task design increases evaluation coverage but requires more resources; NLG format is more realistic but harder to evaluate consistently; large scale provides better coverage but increases computational costs
- **Failure signatures**: Inconsistent model outputs across similar prompts indicating prompt sensitivity; large performance gaps between domains suggesting dataset imbalance; poor translation performance despite good classification indicating task-specific weaknesses
- **First 3 experiments**: 
  1. Run all models on a small subset (100 examples) of each task to verify prompt effectiveness and establish baseline performance
  2. Test model consistency by running the same prompts multiple times to measure output variance
  3. Compare model performance on classification vs. translation tasks to identify which capabilities transfer from modern Chinese and which require specialized classical Chinese knowledge

## Open Questions the Paper Calls Out
None

## Limitations
- Significant performance limitations across all tested LLMs, with average scores below 50% on CCU tasks
- Translation tasks show particularly poor performance, with maximum 34.58 BLEU score versus 52.02 BLEU for supervised models
- Performance varies significantly across different classical Chinese domains, indicating domain-dependent capabilities

## Confidence
- **High confidence**: Current LLMs struggle with CCU tasks is well-supported by extensive empirical evidence
- **Medium confidence**: NLG format provides more realistic evaluation than multiple-choice formats is reasonable but not definitively proven
- **Medium confidence**: Multi-task design enables comprehensive evaluation is supported but could benefit from additional validation

## Next Checks
1. **Domain distribution analysis**: Verify current domain distribution doesn't bias results and conduct sensitivity analysis on balanced domain subsets
2. **Prompt sensitivity testing**: Run multiple evaluations with varied prompts to measure output consistency and determine if results reflect genuine understanding
3. **Transferability assessment**: Compare model performance on classical versus modern Chinese for similar tasks to quantify specific degradation and identify transferable capabilities