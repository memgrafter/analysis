---
ver: rpa2
title: 'SWAN: SGD with Normalization and Whitening Enables Stateless LLM Training'
arxiv_id: '2412.13148'
source_url: https://arxiv.org/abs/2412.13148
tags:
- adam
- gradient
- gradwhitening
- learning
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes SWAN, a stateless optimizer for training large
  language models (LLMs) that achieves Adam-level performance without maintaining
  optimizer states. The key idea is to preprocess stochastic gradients using two stateless
  operators: GradNorm for gradient stabilization and GradWhitening for counteracting
  local curvature.'
---

# SWAN: SGD with Normalization and Whitening Enables Stateless LLM Training

## Quick Facts
- arXiv ID: 2412.13148
- Source URL: https://arxiv.org/abs/2412.13148
- Authors: Chao Ma; Wenbo Gong; Meyer Scetbon; Edward Meeds
- Reference count: 40
- Key outcome: Stateless optimizer achieving Adam-level performance with 50% memory reduction and 2x convergence speedup

## Executive Summary
SWAN (SGD with Normalization and Whitening) is a novel stateless optimizer for training large language models that eliminates the need for optimizer states while maintaining Adam-level performance. The key innovation lies in preprocessing stochastic gradients using two stateless operators: GradNorm for gradient stabilization and GradWhitening for curvature adaptation. By applying these operators to linear projection weights in transformer layers, SWAN achieves approximately 50% reduction in total memory consumption compared to Adam, while delivering up to 2x speedup in convergence for 350M and 1.3B parameter LLaMA models.

## Method Summary
SWAN is a stateless optimizer that preprocesses stochastic gradients using two key operators: GradNorm applies row-wise normalization to stabilize gradient distributions across training steps, while GradWhitening performs matrix whitening to counteract local curvature without requiring optimizer state storage. Applied only to 2D linear projection weights in transformer layers, SWAN achieves Adam-level performance while reducing memory consumption by approximately 50% and delivering up to 2x convergence speedup on LLaMA models.

## Key Results
- Achieves 50% memory reduction compared to Adam by eliminating optimizer state storage
- Delivers 2x convergence speedup on 350M and 1.3B parameter LLaMA models
- Maintains Adam-level validation perplexity while using half the training tokens

## Why This Works (Mechanism)

### Mechanism 1: Gradient Noise Stabilization
GradNorm stabilizes gradient distributions across training steps by applying row-wise normalization (LayerNorm) to backward gradients, removing batch-induced noise while preserving gradient scaling information. This exploits transformer architecture's specific gradient flow patterns where row-wise standardization effectively removes time-varying noise without harming convergence. The mechanism works by implicitly aligning with transformer dynamics and removing time-heterogeneity in gradient covariance structures.

### Mechanism 2: Curvature Adaptation via Whitening
GradWhitening approximates second-order updates by performing matrix whitening (GG⊤)^(-1/2)G on normalized gradients, effectively preconditioning based on estimated local curvature. This exploits structured Hessian properties that emerge in transformer architectures, specifically a block-diagonal structure at equilibrium where diagonal blocks share identical patterns. The mechanism provides non-diagonal second-order update capabilities without storing optimizer states.

### Mechanism 3: Synergistic Optimizer Design
The combination of GradNorm and GradWhitening creates an optimizer that achieves Adam-level performance without state storage by handling complementary aspects: GradNorm addresses gradient noise stabilization while GradWhitening handles curvature adaptation. Together they provide the key benefits of adaptive optimizers - stable gradient distributions and effective curvature handling - without requiring EMA state tracking or other optimizer states.

## Foundational Learning

- **Matrix operations and linear algebra**: Understanding eigenvalues, eigenvectors, matrix square roots, and Kronecker products is essential for implementing GradWhitening's matrix whitening operations. Quick check: Can you explain why (GG⊤)^(-1/2)G orthogonalizes the gradient matrix and what this means geometrically?

- **Stochastic optimization and gradient noise properties**: Understanding how mini-batch sampling introduces noise and why stabilizing this noise is crucial for convergence. Quick check: What's the difference between gradient noise that helps escape local minima versus noise that hinders convergence, and how does GradNorm distinguish between them?

- **Transformer architecture and attention mechanisms**: Understanding how attention mechanisms create specific gradient flow patterns that GradNorm can exploit. Quick check: How does the winner-takes-all behavior in attention contribute to the Hessian structure assumptions made for GradWhitening?

## Architecture Onboarding

- **Component map**: GradNorm operator (row-wise normalization) -> GradWhitening operator (matrix whitening using Newton-Schulz) -> Parameter application (2D linear projection weights) -> State management (stateless) -> Integration (replaces Adam in existing pipelines)

- **Critical path**: Compute raw gradient G for 2D weight matrices during backpropagation -> Apply GradNorm: G_normalized = (G - mean)/std across output dimensions -> Apply GradWhitening: G_whitened = Newton-Schulz iteration on (G_normalized * G_normalized⊤)^(-1/2) * G_normalized -> Update weights: W_new = W_old - learning_rate * G_whitened -> Convert to appropriate precision (BF16 recommended)

- **Design tradeoffs**: Memory vs accuracy (BF16 trades precision for memory savings), computational overhead (Newton-Schulz adds matrix multiplications but is highly parallelizable), precision handling (FP32 conversion improves stability at minor cost), warmup requirement (eliminated but requires trusting GradWhitening's robustness)

- **Failure signatures**: Training divergence (likely needs GradWhitening iteration adjustment), poor convergence speed (may indicate GradNorm over-regularization or GradWhitening not properly preconditioning), memory leaks (should not occur as SWAN is stateless), precision artifacts (BF16 throughout without FP32 conversion may cause instability)

- **First 3 experiments**: Single linear layer training to compare convergence with Adam using identical hyperparameters, gradient distribution analysis to verify noise stabilization by plotting covariance statistics over time, quadratic optimization test to verify curvature adaptation by testing on a simple quadratic problem with known condition number

## Open Questions the Paper Calls Out

### Open Question 1
Does GradWhitening's performance advantage over Adam extend to architectures beyond transformers, such as convolutional neural networks or graph neural networks? The paper focuses exclusively on transformer-based LLM architectures for empirical validation, leaving generalization to other architectures untested.

### Open Question 2
What is the theoretical limit of SWAN's speedup compared to Adam as model size scales to the largest LLMs? The authors observe multiplicative speedup on 350M and 1.3B models but don't extrapolate to larger scales, leaving scaling behavior uncertain.

### Open Question 3
How does the memory efficiency of SWAN compare to other low-rank methods when training with mixed precision or quantization? The paper benchmarks against full Adam and Galore but doesn't explore mixed precision scenarios, assuming BF16 format throughout.

### Open Question 4
What is the impact of GradNorm's layer-wise application (on W instead of U⊤_C W) on the theoretical guarantees versus practical performance? The authors acknowledge this approximation but don't quantify the performance gap between exact versus approximate GradNorm application.

### Open Question 5
Can the computational overhead of GradWhitening be further reduced while maintaining its convergence advantages? The paper presents one practical approximation using Newton-Schulz iteration but doesn't explore alternatives or optimality, suggesting room for optimization.

## Limitations
- Simplified Hessian structure assumption may not hold for all transformer architectures
- Limited validation to LLaMA models, generalization to other architectures needs testing
- GradWhitening matrix square root computation may introduce numerical instability
- Memory reduction claims assume specific hardware configurations
- Performance on non-transformer architectures remains unexplored

## Confidence
- **High confidence**: GradNorm's gradient stabilization mechanism and effectiveness in reducing gradient noise
- **Medium confidence**: GradWhitening's curvature adaptation claims based on simplified transformer Hessian analysis
- **Medium confidence**: Overall 2x convergence speedup claims demonstrated on 350M and 1.3B LLaMA models

## Next Checks
1. **Architecture generalization test**: Apply SWAN to GPT-style architectures and compare performance against LLaMA results to validate transformer-specific gradient pattern effectiveness across different architectures.

2. **Scale-up validation**: Train a 7B+ parameter model using SWAN and verify that 50% memory reduction claim holds at scale and convergence speed benefits persist beyond 1.3B parameters.

3. **Hessian structure verification**: Empirically measure actual Hessian structure in transformer layers during training to validate whether assumed block-diagonal pattern with identical blocks exists, or whether observed benefits arise from different mechanisms.