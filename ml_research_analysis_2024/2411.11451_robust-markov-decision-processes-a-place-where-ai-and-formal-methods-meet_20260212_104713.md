---
ver: rpa2
title: 'Robust Markov Decision Processes: A Place Where AI and Formal Methods Meet'
arxiv_id: '2411.11451'
source_url: https://arxiv.org/abs/2411.11451
tags:
- mdps
- robust
- uncertainty
- policy
- rmdps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey unifies the theory of robust Markov decision processes
  (RMDPs) from the AI and formal methods communities, addressing the problem of decision-making
  under uncertainty when transition probabilities are not precisely known. The paper
  introduces RMDPs as an extension of standard MDPs where transition probabilities
  belong to an uncertainty set, and discusses robust dynamic programming methods to
  solve them.
---

# Robust Markov Decision Processes: A Place Where AI and Formal Methods Meet

## Quick Facts
- arXiv ID: 2411.11451
- Source URL: https://arxiv.org/abs/2411.11451
- Reference count: 40
- Primary result: Comprehensive survey unifying RMDP theory from AI and formal methods communities, covering semantics, solution methods, connections to other models, and applications.

## Executive Summary
This survey provides a comprehensive overview of robust Markov decision processes (RMDPs), which extend standard MDPs to handle uncertainty in transition probabilities by defining them as belonging to uncertainty sets. The paper unifies perspectives from both AI and formal methods communities, presenting RMDPs as a framework where an agent must plan against an adversarial nature that selects worst-case transitions from the uncertainty set. The survey covers RMDP semantics, solution methods like robust dynamic programming, connections to other models (stochastic games, parametric MDPs, multi-environment MDPs), and applications in learning and abstraction.

## Method Summary
The paper reviews literature on RMDPs from both AI and formal methods perspectives, synthesizing approaches to handling uncertainty in transition probabilities. It covers the extension of standard MDP solution methods (value iteration, policy iteration) to the robust setting, where inner minimization problems are solved to account for worst-case transitions. The survey also examines connections to related models and discusses practical applications, emphasizing the need for tool support and identifying open research questions, particularly around non-rectangular uncertainty sets and richer temporal logic objectives.

## Key Results
- RMDPs provide a unified framework for decision-making under uncertainty by modeling transition probabilities as belonging to uncertainty sets
- Under (s,a)-rectangular uncertainty sets, robust dynamic programming methods can efficiently solve RMDPs by solving inner minimization problems at each iteration
- The survey identifies key research gaps, including the need for better tool support and exploration of non-rectangular uncertainty sets with richer temporal logic objectives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Robust MDPs enable decision-making under uncertainty by replacing precise transition probabilities with uncertainty sets, allowing agents to plan against worst-case scenarios.
- Mechanism: By modeling transition probabilities as belonging to uncertainty sets, RMDPs transform the planning problem into a game between the agent and an adversarial nature. The agent optimizes for the worst-case transition function selected by nature, ensuring robustness to estimation errors.
- Core assumption: The uncertainty set is (s,a)-rectangular, meaning dependencies between transitions at different state-action pairs are absent, enabling tractable dynamic programming solutions.
- Evidence anchors:
  - [abstract] "RMDPs overcome this assumption by instead defining the transition probabilities to belong to some uncertainty set."
  - [section] "An RMDP contains an uncertainty set that captures all possible transition functions from which an adversary, typically called nature, may choose."
- Break condition: If the uncertainty set becomes non-rectangular or if dependencies exist between transitions at different state-action pairs, the problem becomes NP-hard and the tractable solution breaks down.

### Mechanism 2
- Claim: Robust value iteration extends standard value iteration by incorporating an inner minimization problem to account for the worst-case transition probabilities at each step.
- Mechanism: Instead of using a fixed transition function P, robust value iteration computes V(n+1)(s) = max_a { R(s,a) + inf_P∈P { sum_{s'} P(s,a,s')V(n)(s') } }, where the inner minimization selects the transition function that minimizes the expected future reward.
- Core assumption: The uncertainty set is convex, allowing the inner minimization problem to be solved efficiently via convex optimization methods.
- Evidence anchors:
  - [section] "Robust value iteration adapts value iteration by accounting for the worst-case P ∈ P at each iteration."
  - [section] "If, additionally, the uncertainty set U is convex... the inner minimization problem can be solved efficiently via, e.g., convex optimization methods."
- Break condition: If the uncertainty set is non-convex or if dependencies exist between transitions, the inner minimization becomes computationally intractable.

### Mechanism 3
- Claim: RMDPs can be solved using policy iteration by alternating between robust policy evaluation and robust policy improvement steps.
- Mechanism: Policy evaluation computes the value of a given policy under worst-case transitions, while policy improvement selects actions that maximize the robust value function. This process continues until the policy stabilizes.
- Core assumption: Stationary deterministic policies are sufficient for optimality in (s,a)-rectangular RMDPs with convex uncertainty sets.
- Evidence anchors:
  - [section] "Policy iteration consists of two alternating steps: policy evaluation and policy improvement."
  - [section] "Under our assumption that the uncertainty set U is (s,a)-rectangular, we may replace the global minimization problem inf P∈P by a local one."
- Break condition: If the uncertainty set is non-rectangular or if the uncertainty set is non-convex, stationary deterministic policies may no longer be sufficient for optimality.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: Understanding standard MDPs is essential before grasping how RMDPs extend them to handle uncertainty.
  - Quick check question: What are the key components of an MDP and how do they differ from those in an RMDP?

- Concept: Dynamic Programming for MDPs
  - Why needed here: Robust dynamic programming builds directly on standard dynamic programming methods, extending them to handle uncertainty.
  - Quick check question: How does value iteration in standard MDPs differ from robust value iteration in RMDPs?

- Concept: Uncertainty Sets and Rectangularity
  - Why needed here: The structure of uncertainty sets (rectangular vs non-rectangular) determines the computational complexity of solving RMDPs.
  - Quick check question: What is the difference between (s,a)-rectangular and s-rectangular uncertainty sets, and how does this affect the tractability of RMDP solutions?

## Architecture Onboarding

- Component map:
  - Uncertainty Set Definition -> Dynamic Programming Solver -> Policy Evaluation Module -> Policy Improvement Module -> Convex Optimization Engine

- Critical path:
  1. Define uncertainty set with appropriate rectangularity constraints
  2. Initialize value function or policy
  3. For value iteration: perform inner minimization at each step
  4. For policy iteration: alternate between evaluation and improvement
  5. Check for convergence and extract optimal policy

- Design tradeoffs:
  - Rectangularity vs Computational Tractability: (s,a)-rectangular uncertainty sets enable polynomial-time solutions, while non-rectangular sets lead to NP-hard problems
  - Convexity vs Expressiveness: Convex uncertainty sets allow efficient convex optimization but may not capture all types of uncertainty
  - Pessimistic vs Optimistic Interpretation: Pessimistic (robust) vs optimistic (best-case) approaches yield different optimal policies and values

- Failure signatures:
  - Non-convergence in value iteration: May indicate non-convex uncertainty set or inappropriate initialization
  - Policy oscillation in policy iteration: Could signal that stationary policies are insufficient for the given uncertainty structure
  - Inner minimization failure: Suggests that the uncertainty set is too complex for efficient solution

- First 3 experiments:
  1. Implement robust value iteration for a simple (s,a)-rectangular RMDP with convex uncertainty set and verify convergence to known optimal values
  2. Compare robust policy iteration vs robust value iteration on the same problem to observe differences in convergence behavior and computational cost
  3. Test the effect of non-rectangular uncertainty sets on solution complexity by incrementally adding dependencies and measuring computational time

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the computational trade-offs between robust value iteration and robust policy iteration for non-rectangular uncertainty sets?
- Basis in paper: [explicit] The paper mentions that robust policy iteration is preferred for (s, a)-rectangular RMDPs due to fewer inner minimization problems, but does not provide experimental evidence or extend this comparison to non-rectangular cases.
- Why unresolved: No comprehensive experimental evaluation exists comparing these methods across different uncertainty set structures.
- What evidence would resolve it: Empirical studies comparing convergence rates, computational costs, and memory usage of both methods on diverse non-rectangular RMDP benchmarks.

### Open Question 2
- Question: How do different uncertainty assumptions (static vs. dynamic, last-action observability) affect the complexity of temporal logic objectives in RMDPs?
- Basis in paper: [explicit] The paper notes that these assumptions are often left implicit and may influence optimality for reward maximization in RPOMDPs, but their impact on richer temporal logic objectives remains unexplored.
- Why unresolved: Existing research primarily focuses on reachability and reward objectives, with limited work on temporal logic under varying uncertainty assumptions.
- What evidence would resolve it: Formal complexity analyses and experimental comparisons of temporal logic verification/synthesis algorithms under different uncertainty assumptions.

### Open Question 3
- Question: Can robust abstraction techniques for MDPs be extended to handle epistemic uncertainty in continuous-state systems?
- Basis in paper: [explicit] The paper discusses abstraction methods using IMDPs for discrete systems but does not address continuous-state systems with epistemic uncertainty.
- Why unresolved: Current abstraction approaches primarily target aleatoric uncertainty or discrete models, with limited theoretical frameworks for epistemic uncertainty in continuous domains.
- What evidence would resolve it: Development and validation of abstraction algorithms that quantify and bound epistemic uncertainty propagation from continuous to discrete models.

## Limitations
- The survey does not provide empirical validation of computational tractability claims for robust dynamic programming methods
- Practical guidance on constructing appropriate uncertainty sets and validating robustness against real-world data is lacking
- Applications in learning and abstraction are mentioned but not deeply explored with concrete examples or validation

## Confidence
- **High Confidence**: The foundational definitions and semantics of RMDPs, including the extension from standard MDPs and the role of uncertainty sets in capturing transition probability ambiguity
- **Medium Confidence**: The theoretical claims about computational tractability under rectangular uncertainty sets, as these depend on assumptions about convexity and independence that may not hold in practical applications
- **Low Confidence**: The survey's claims about applications in learning and abstraction, which are mentioned but not deeply explored with concrete examples or validation

## Next Checks
1. Implement and benchmark robust value iteration on a standard test problem (e.g., grid world) with varying levels of uncertainty to empirically verify computational tractability claims
2. Compare the performance of robust policies against standard MDP policies under different types of transition probability estimation errors to validate the practical benefits of robustness
3. Test the limits of rectangularity assumptions by constructing small non-rectangular uncertainty sets and measuring the increase in computational complexity compared to rectangular cases