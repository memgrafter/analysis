---
ver: rpa2
title: Improving Image Clustering with Artifacts Attenuation via Inference-Time Attention
  Engineering
arxiv_id: '2410.04801'
source_url: https://arxiv.org/abs/2410.04801
tags:
- clustering
- attention
- artifacts
- pretrained
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies and addresses artifacts in pretrained ViT
  models that negatively impact image clustering performance. The authors propose
  Inference-Time Attention Engineering (ITAE), a method that attenuates artifacts
  during inference by manipulating attention values in the final transformer layer.
---

# Improving Image Clustering with Artifacts Attenuation via Inference-Time Attention Engineering

## Quick Facts
- arXiv ID: 2410.04801
- Source URL: https://arxiv.org/abs/2410.04801
- Authors: Kazumoto Nakamura; Yuji Nozawa; Yu-Chieh Lin; Kengo Nakata; Youyang Ng
- Reference count: 40
- One-line primary result: Inference-Time Attention Engineering (ITAE) improves image clustering accuracy by 2.24% on average across multiple datasets and model sizes

## Executive Summary
This paper identifies artifacts in pretrained Vision Transformer (ViT) models that negatively impact image clustering performance. The authors propose ITAE, a method that attenuates these artifacts during inference by manipulating attention values in the final transformer layer. By analyzing L2 norms of Query-Key-Value patches, ITAE identifies anomalous tokens and reduces their influence on the attention map. Experiments show consistent accuracy improvements across various model sizes and datasets without requiring re-training or fine-tuning.

## Method Summary
The method involves computing L2 norms of QKV patches in the final transformer layer to identify artifacts, then attenuating their attention values by replacing them with minimum values across all patches in each head. This process improves the quality of CLS token features used for clustering. The approach is applied during inference only, making it a practical solution for enhancing pretrained models in clustering tasks.

## Key Results
- Average clustering accuracy improvement of 2.24% across multiple datasets
- Consistent performance gains across ViT model sizes (S, B, L, g)
- Effectiveness demonstrated on CIFAR-10/100, STL-10, and Tiny ImageNet
- Generalizes well to different pretraining paradigms (DINOv2, CLIP, DeiT III)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Artifacts in ViT attention maps are identifiable through L2 norms of QKV patches in the final layer
- Mechanism: The L2 norms of query patches (QKV) in the self-attention block exhibit bimodal distribution, where the minority group represents artifacts with disproportionately large attention values
- Core assumption: Artifacts manifest as anomalous tokens with abnormally large norms in QKV patches that can be detected before they corrupt the output features
- Evidence anchors:
  - [abstract] "We identify the artifacts by computing the L2 norms of one of the Query-Key-Value (QKV) patches within the attention block of the final layer"
  - [section 3.2] "Through this process, we found that artifacts exist even in the smaller models... We utilize the L2 norms distribution of the QKV patches"
  - [corpus] Weak evidence - no direct citations about L2 norm artifact detection in transformers

### Mechanism 2
- Claim: Attenuating identified artifact patches during inference redistributes attention to more meaningful tokens
- Mechanism: By replacing attention values of identified artifacts with the minimum attention value across all patches in each head, the model shifts focus away from corrupted tokens to normal tokens
- Core assumption: The minimum attention value across patches represents a safe baseline that preserves model functionality while removing artifact influence
- Evidence anchors:
  - [abstract] "attenuate their corresponding attention values inside the pretrained models"
  - [section 3.3] "For every identified artifact, k in A, the attention value of the artifact is transformed into a minimum value independently for every head"
  - [corpus] Weak evidence - no direct citations about minimum-value attention attenuation strategies

### Mechanism 3
- Claim: Improved attention maps lead to more expressive feature representations in latent space
- Mechanism: By removing artifact influence from the final attention layer, the CLS token output becomes a better representation of meaningful image content rather than corrupted information
- Core assumption: The final layer's attention directly influences the CLS token output that is used for clustering, so improving attention quality improves feature quality
- Evidence anchors:
  - [abstract] "exhibiting more expressive features in latent space"
  - [section 3.3] "we apply the same function in Eq. (6) to models pretrained with register tokens... we extract the output features of CLS tokens"
  - [section 4.4] "Our focus on applying the proposed method to the final layer... is justified by the simplicity of its modification, and the observability of the effect"

## Foundational Learning

- Concept: Vision Transformer architecture and self-attention mechanism
  - Why needed here: Understanding how attention blocks work is essential to grasp how artifacts form and how ITAE manipulates them
  - Quick check question: How does the softmax operation in self-attention affect the distribution of attention values across patches?

- Concept: L2 norm calculation and distribution analysis
  - Why needed here: The core artifact identification relies on computing and thresholding L2 norms of QKV patches
  - Quick check question: What does a bimodal L2 norm distribution of QKV patches indicate about the presence of artifacts?

- Concept: Zero-shot image clustering and feature extraction
  - Why needed here: The method aims to improve clustering performance without fine-tuning, relying on raw features from pretrained models
  - Quick check question: Why does using CLS token output from the final layer make sense for zero-shot clustering tasks?

## Architecture Onboarding

- Component map: Pretrained ViT model → Final transformer layer → Self-attention block → QKV projection matrices → L2 norm computation → Artifact identification → Attention manipulation → CLS token output → K-Means clustering
- Critical path: QKV L2 norm computation → Artifact identification → Attention attenuation → Feature extraction → Clustering performance
- Design tradeoffs: ITAE vs. register tokens (re-training required) vs. no intervention; inference-time manipulation vs. architectural changes
- Failure signatures: Unimodal L2 norm distribution (no artifacts to identify), clustering accuracy below baseline, model instability after attention manipulation
- First 3 experiments:
  1. Compute L2 norm distribution of QKV patches on CIFAR-100 with ViT-B/14 to verify bimodal pattern
  2. Apply ITAE with θ=3.0 and measure clustering accuracy improvement on CIFAR-10
  3. Test ITAE on models pretrained with register tokens to verify complementary effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of ITAE vary with different image clustering datasets beyond those tested (CIFAR-10/100, STL-10, Tiny ImageNet)?
- Basis in paper: [explicit] The paper states "our method improves clustering accuracy on multiple datasets" but only tests four specific datasets.
- Why unresolved: The paper only tests four standard image datasets, leaving uncertainty about ITAE's effectiveness on other types of datasets (medical images, satellite imagery, etc.).
- What evidence would resolve it: Testing ITAE on diverse image datasets across different domains and comparing accuracy improvements against baseline models.

### Open Question 2
- Question: What is the computational overhead of applying ITAE during inference compared to the baseline model?
- Basis in paper: [inferred] The paper discusses "inference-time attention engineering" but doesn't provide quantitative data on computational cost or inference time differences.
- Why unresolved: While the method is described as efficient, there's no concrete data on processing time, memory usage, or computational complexity compared to the original model.
- What evidence would resolve it: Benchmarking inference time, memory usage, and FLOPs for models with and without ITAE across different hardware configurations.

### Open Question 3
- Question: How does ITAE's performance compare to other attention manipulation techniques when applied to the same pretrained models?
- Basis in paper: [explicit] The paper mentions related works like SATA and LSA but only provides limited comparative analysis in supplementary material.
- Why unresolved: The paper claims superiority but only briefly compares to one other method (LSA) in supplementary material, lacking comprehensive comparison to other attention engineering approaches.
- What evidence would resolve it: Systematic comparison of ITAE against multiple attention manipulation techniques on the same models and datasets, measuring both accuracy and computational efficiency.

## Limitations
- The method relies on the assumption that L2 norm distributions of QKV patches are consistently bimodal across different models and datasets
- Computational overhead during inference is not thoroughly characterized, limiting understanding of practical deployment implications
- The optimal threshold values (θ=3.0) are presented without exploration of alternative strategies or sensitivity analysis

## Confidence

**High Confidence Claims:**
- Pretrained ViT models exhibit artifacts that degrade clustering performance
- L2 norm distributions of QKV patches show characteristic patterns that can identify anomalous tokens
- Attention manipulation during inference can improve clustering metrics without re-training

**Medium Confidence Claims:**
- Minimum attention value replacement is the optimal attenuation strategy
- Final layer intervention provides the best balance of effectiveness and simplicity
- ITAE generalizes well across different pretraining paradigms (DINOv2, CLIP, DeiT III)

**Low Confidence Claims:**
- The specific threshold values (θ=3.0) are universally optimal
- No significant performance degradation occurs from over-attenuation
- The method scales efficiently to larger models and more complex datasets

## Next Checks

1. **Distribution Validation**: Compute and analyze L2 norm distributions of QKV patches across multiple model sizes (ViT-S, B, L, g) and datasets to verify the claimed bimodal pattern consistently identifies artifacts rather than model-specific artifacts.

2. **Alternative Attenuation Strategies**: Implement and compare alternative attention attenuation methods (mean replacement, soft masking, head-specific thresholds) against the minimum value approach to determine if the current strategy is truly optimal.

3. **Layer-wise Analysis**: Conduct ablation studies applying ITAE to intermediate transformer layers versus only the final layer to empirically validate whether the final layer intervention provides superior clustering performance compared to multi-layer approaches.