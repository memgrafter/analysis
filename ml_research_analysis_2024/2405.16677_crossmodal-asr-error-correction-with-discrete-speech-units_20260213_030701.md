---
ver: rpa2
title: Crossmodal ASR Error Correction with Discrete Speech Units
arxiv_id: '2405.16677'
source_url: https://arxiv.org/abs/2405.16677
tags:
- speech
- transcript
- data
- word
- acoustic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of Automatic Speech Recognition
  (ASR) error correction in low-resource out-of-domain (LROOD) scenarios, where ASR
  systems perform poorly due to differences between the training and target data.
  The authors propose a crossmodal approach that incorporates Discrete Speech Units
  (DSUs) derived from self-supervised speech representations (HuBERT) to enhance error
  correction.
---

# Crossmodal ASR Error Correction with Discrete Speech Units

## Quick Facts
- arXiv ID: 2405.16677
- Source URL: https://arxiv.org/abs/2405.16677
- Authors: Yuanchao Li; Pinzhen Chen; Peter Bell; Catherine Lai
- Reference count: 0
- Key outcome: This work addresses the challenge of Automatic Speech Recognition (ASR) error correction in low-resource out-of-domain (LROOD) scenarios, where ASR systems perform poorly due to differences between the training and target data. The authors propose a crossmodal approach that incorporates Discrete Speech Units (DSUs) derived from self-supervised speech representations (HuBERT) to enhance error correction.

## Executive Summary
This paper addresses the challenge of Automatic Speech Recognition (ASR) error correction in low-resource out-of-domain (LROOD) scenarios. The authors propose a crossmodal approach that incorporates Discrete Speech Units (DSUs) derived from self-supervised speech representations (HuBERT) to enhance error correction. They investigate the impact of pre-training and fine-tuning strategies and uncover an ASR domain discrepancy phenomenon, highlighting the importance of using the same ASR model across training phases. Experimental results on multiple corpora demonstrate that their approach reduces Word Error Rate (WER) and improves transcript quality, outperforming generative ASR correction methods. Additionally, a speech emotion recognition task validates the improved usability of corrected transcripts in downstream applications.

## Method Summary
The method involves pre-training an ASR error correction (AEC) model on Common Voice 13.0 English transcripts, then fine-tuning on IEMOCAP transcripts. The key innovation is incorporating Discrete Speech Units (DSUs) from HuBERT layer 7 through cross-attention alignment with word embeddings. The model uses frozen pre-trained encoders (HuBERT for speech, RoBERTa for text) and a trainable Transformer decoder. The approach is validated across multiple corpora (Common Voice, IEMOCAP, CMU-MOSI, MSP-Podcast) using WER, BLEU, and GLEU metrics, with additional validation through speech emotion recognition.

## Key Results
- Incorporating DSUs from HuBERT layer 7 improves AEC performance, reducing WER from 18.47 to 16.86 on IEMOCAP fine-tuning.
- Pre-training on Common Voice followed by fine-tuning on IEMOCAP achieves the best results (16.40 WER), outperforming either approach alone.
- Using the same ASR model across pre-training and fine-tuning phases is crucial, with significant performance degradation when mismatched ASR models are used.

## Why This Works (Mechanism)

### Mechanism 1
Discrete Speech Units (DSUs) derived from HuBERT self-supervised representations improve alignment between acoustic and textual modalities in low-resource out-of-domain (LROOD) ASR error correction. DSUs are fixed-dimensional acoustic embeddings generated through forced alignment and mean pooling of HuBERT SSRs, creating discrete tokens that map more easily to word embeddings than continuous features like Mel-spectrograms. The discrete nature of DSUs allows for better cross-modal alignment with word embeddings in the Transformer decoder, overcoming the misalignment issues that plague continuous acoustic feature fusion in LROOD scenarios.

### Mechanism 2
Pre-training on large out-of-domain corpora followed by fine-tuning on small LROOD data significantly improves AEC performance compared to either approach alone. Pre-training establishes a general error correction capability on a large dataset, while fine-tuning adapts this capability to the specific error patterns and linguistic features of the LROOD domain. The model can leverage general error correction patterns learned during pre-training and refine them during fine-tuning to address domain-specific errors without catastrophic forgetting.

### Mechanism 3
Using the same ASR model to generate transcripts across pre-training, fine-tuning, and inference phases prevents ASR domain discrepancy and improves AEC performance. Different ASR models produce distinct error patterns (insertion, substitution, deletion distributions), so training an AEC model on errors from one ASR model and testing on errors from another creates a domain shift that degrades performance. The AEC model learns to map specific error patterns from a particular ASR model to corrections, and this mapping does not generalize well across different ASR systems.

## Foundational Learning

- Concept: Sequence-to-Sequence (S2S) models for text correction
  - Why needed here: The AEC task requires mapping erroneous ASR transcripts to corrected ground truth, which is fundamentally a sequence transduction problem.
  - Quick check question: What is the difference between teacher forcing during training and inference-time decoding strategies like beam search in S2S models?

- Concept: Self-supervised speech representation learning (HuBERT)
  - Why needed here: HuBERT provides powerful speech representations that can be discretized into DSUs, enabling cross-modal alignment without requiring large amounts of labeled speech data.
  - Quick check question: How does HuBERT's masked prediction objective differ from wav2vec 2.0's contrastive loss, and what implications does this have for the quality of extracted speech representations?

- Concept: Cross-modal attention mechanisms
  - Why needed here: The model must align acoustic features (DSUs) with word embeddings to create acoustically-informed word representations for the decoder.
  - Quick check question: In the cross-attention equation A′ = softmax(QwKᵀ/√dk)V, what is the role of the scaling factor √dk and how does it affect training stability?

## Architecture Onboarding

- Component map: Audio signal → HuBERT encoder → SSR → Mean pooling → AWEs (DSUs) → Cross-attention → RoBERTa embeddings → Transformer decoder → Corrected transcript
- Critical path: ASR transcript → RoBERTa embeddings → Cross-attention alignment → Transformer decoder → Corrected output
- Design tradeoffs: Using frozen pre-trained encoders (HuBERT, RoBERTa) saves parameters and training time but limits adaptation to the specific LROOD domain; incorporating DSUs only during fine-tuning and inference saves resources but may miss opportunities for learning optimal acoustic-text integration during pre-training.
- Failure signatures: High WER on corrected transcripts despite low WER on original transcripts suggests the model is making unnecessary corrections (over-correction); performance degradation when switching ASR models confirms ASR domain discrepancy.
- First 3 experiments:
  1. Implement the text-only S2S model (without DSUs) and verify baseline performance on IEMOCAP fine-tuning; this establishes the foundation before adding complexity.
  2. Add HuBERT AWEs with cross-attention and compare against baseline; this isolates the impact of DSUs from other architectural changes.
  3. Test the ASR domain discrepancy by fine-tuning on transcripts from different ASR models (Whisper vs Random); this validates the importance of consistent ASR model usage across training phases.

## Open Questions the Paper Calls Out

### Open Question 1
How do different ASR models' error patterns (insertions, deletions, substitutions, disfluency handling) affect the performance of crossmodal AEC, and can these patterns be characterized to improve model adaptation? While the paper demonstrates that using the same ASR model across pre-training and fine-tuning improves AEC performance, it does not characterize the specific error patterns of different ASR models or develop methods to systematically adapt AEC models to different error patterns.

### Open Question 2
How does the size and domain similarity of downstream data affect the relative importance of pre-training versus fine-tuning in low-resource out-of-domain scenarios? The paper notes that "the more data available for FT, the better the performance" and observes varying effectiveness of pre-training across different corpora with different data sizes and domain characteristics, but does not systematically investigate how varying amounts of fine-tuning data or domain similarity affect the optimal balance between pre-training and fine-tuning.

### Open Question 3
Can the alignment between discrete speech units and word embeddings be further optimized to improve crossmodal AEC performance, particularly for low-resource scenarios? While the paper shows that DSUs improve AEC performance, it does not explore alternative methods for creating or aligning speech units, nor does it investigate how different alignment strategies might affect correction of specific error types.

## Limitations

- The specific mechanism by which DSUs enable better cross-modal alignment is not rigorously established and could stem from factors beyond discretization.
- The ASR domain discrepancy phenomenon requires further validation across more ASR systems to understand the underlying causal mechanisms.
- Results are primarily validated on English corpora with relatively modest size, limiting generalizability to truly low-resource languages or domains with severe data scarcity.

## Confidence

**High confidence**:
- The superiority of discrete speech units (DSUs) over continuous acoustic features for AEC in LROOD scenarios is well-supported by systematic ablation studies across multiple corpora.
- The additive benefit of combining pre-training and fine-tuning strategies for LROOD AEC is empirically validated with consistent improvements in WER, BLEU, and GLEU metrics.

**Medium confidence**:
- The ASR domain discrepancy phenomenon is demonstrated but the underlying causal mechanisms are not fully characterized.
- The SER validation confirms improved downstream usability but the effect size and practical significance require further investigation with more diverse downstream tasks.

**Low confidence**:
- Claims about the specific mechanism by which DSUs enable better cross-modal alignment are largely intuitive rather than empirically validated.
- The optimal selection of HuBERT layers for DSU extraction is based on preliminary analysis rather than systematic hyperparameter optimization.

## Next Checks

1. **Cross-attention ablation study**: Implement and compare three variants: (a) text-only S2S model, (b) S2S model with cross-attention but no acoustic features, and (c) full model with cross-attention and HuBERT AWEs. This isolates whether the improvement comes from cross-modal alignment or simply from the presence of acoustic information.

2. **ASR model generalization test**: Fine-tune the AEC model on transcripts from one ASR system (e.g., Whisper) and evaluate on transcripts from two other systems (Conformer and Wav2Vec 2.0). This quantifies the ASR domain discrepancy and identifies whether certain error patterns are more transferable across ASR models than others.

3. **Downstream task expansion**: Apply the corrected transcripts to two additional downstream tasks beyond SER: (a) sentiment analysis on CMU-MOSI, and (b) named entity recognition on a subset of Common Voice. This validates the broader usability improvements claimed in the paper across diverse NLP applications.