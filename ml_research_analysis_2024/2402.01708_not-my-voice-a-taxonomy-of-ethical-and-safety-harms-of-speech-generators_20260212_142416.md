---
ver: rpa2
title: Not My Voice! A Taxonomy of Ethical and Safety Harms of Speech Generators
arxiv_id: '2402.01708'
source_url: https://arxiv.org/abs/2402.01708
tags:
- harms
- speech
- harm
- voice
- incidents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a taxonomy of ethical and safety harms from
  speech generators by analyzing 35 real-world incidents. The authors develop a conceptual
  framework that categorizes harms based on affected entity exposure (subject of,
  interacts with, suffers due to, excluded from) and responsible entity intent (creation
  vs deployment motives).
---

# Not My Voice! A Taxonomy of Ethical and Safety Harms of Speech Generators

## Quick Facts
- arXiv ID: 2402.01708
- Source URL: https://arxiv.org/abs/2402.01708
- Authors: Wiebke Hutiri; Oresiti Papakyriakopoulos; Alice Xiang
- Reference count: 40
- Primary result: Taxonomy of ethical and safety harms from speech generators based on 35 real-world incidents

## Executive Summary
This paper introduces a taxonomy of ethical and safety harms from speech generators by analyzing 35 real-world incidents. The authors develop a conceptual framework that categorizes harms based on affected entity exposure (subject of, interacts with, suffers due to, excluded from) and responsible entity intent (creation vs deployment motives). Their taxonomy identifies specific harms including voice hijacking, identity theft, impersonation, and social harms like erosion of trust and influence operations. The framework enables modeling pathways to harm by connecting stakeholders, harm types, and motives, providing actionable insights for policy interventions and responsible development of speech generation systems.

## Method Summary
The authors used an iterative design science research approach, collecting 35 reported AI incidents involving speech generation from multiple databases (OECD AI Incident Database, AIID, AIAAIC) and internal sources. They annotated incidents using existing harm taxonomies as starting points, then developed a conceptual framework through iterative application and revision. The framework was then used to create a comprehensive taxonomy of harms, distinguishing between affected entity exposure types and responsible entity intents to enable pathway modeling and policy intervention analysis.

## Key Results
- Taxonomy identifies specific harms including voice hijacking, identity theft, impersonation, and erosion of trust
- Framework enables modeling pathways to harm by connecting stakeholders, harm types, and motives
- Distinguishes between creation-based harms (profiteering motives) and deployment-based harms (narration, entertainment)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The taxonomy enables modeling of harm pathways by mapping affected entity exposure to responsible entity intent.
- Mechanism: By structuring harms around who is harmed (subject, interacts, suffers, excluded) and why (creation vs deployment intent), the framework captures the sociotechnical complexity that leads to specific harms.
- Core assumption: Harm pathways are best understood through the intersection of exposure type and responsible entity motive.
- Evidence anchors:
  - [abstract] "Their taxonomy identifies specific harms including voice hijacking, identity theft, impersonation, and social harms like erosion of trust and influence operations. The framework enables modeling pathways to harm by connecting stakeholders, harm types, and motives"
  - [section] "We found that patterns of specific harms depend on whether individuals are a subject of, interact with, suffer due to, or are excluded from speech generators. At the same time, we found that specific harms are also a consequence of the motives of the creators and deployers of the systems"
  - [corpus] Weak - no directly relevant corpus papers identified specific pathway modeling mechanisms
- Break Condition: If new incident types emerge that don't fit the exposure/intent framework, the pathway modeling capability breaks down.

### Mechanism 2
- Claim: The taxonomy extends existing AI harm frameworks to generative speech systems by adding specific harm types and exposure categories.
- Mechanism: By incorporating incident-based analysis, the taxonomy captures harms unique to speech generation (voice hijacking, copyright theft, identity hijack) while maintaining compatibility with broader AI harm taxonomies.
- Core assumption: Incident analysis reveals harm patterns that literature-based taxonomies miss for generative systems.
- Evidence anchors:
  - [abstract] "Our relational approach captures the complexity of risks and harms in sociotechnical AI systems, and yields a taxonomy that can support appropriate policy interventions and decision making"
  - [section] "We found it difficult to harmonise the categories of existing taxonomies into a scheme that captured the salient characteristics of speech generation harms"
  - [corpus] Weak - corpus neighbors focus on queer voices and privacy risks but don't address the specific extension mechanism
- Break Condition: If speech generation capabilities evolve beyond voice-based outputs, the taxonomy's focus becomes limiting.

### Mechanism 3
- Claim: The framework supports policy intervention by distinguishing between creation-based and deployment-based harms.
- Mechanism: By separating harms due to creation (profiteering motives) from harms from deployment (narration, featurisation, entertainment), the taxonomy enables targeted policy responses for different stakeholder types.
- Core assumption: Policy interventions must target different responsible entity types differently based on their harm motives.
- Evidence anchors:
  - [abstract] "Our relational approach captures the complexity of risks and harms in sociotechnical AI systems, and yields a taxonomy that can support appropriate policy interventions and decision making"
  - [section] "Responsible entities are classified as creators of AI if they invest resources into training an AI system. If a responsible entity uses or deploys an existing AI system with minimal training...we classify them as a deployer of AI"
  - [corpus] Weak - corpus neighbors don't address policy intervention mechanisms
- Break Condition: If responsible entities blur the line between creation and deployment (e.g., fine-tuning existing models), the policy distinction becomes less actionable.

## Foundational Learning

- Concept: Sociotechnical systems perspective
  - Why needed here: The taxonomy recognizes that AI harms arise from interactions between technical systems and human stakeholders, not just technical failures
  - Quick check question: Can you explain why swatting attacks involve both anonymous perpetrators AND police officers as responsible entities?

- Concept: Incident-based taxonomy development
  - Why needed here: The taxonomy is built from real-world incidents rather than theoretical frameworks, ensuring relevance to actual harm patterns
  - Quick check question: How does analyzing 35 real incidents help identify specific harms that literature reviews might miss?

- Concept: Pathway modeling in risk assessment
  - Why needed here: The framework adapts environmental risk assessment concepts to AI, showing how specific harms materialize through stakeholder interactions
  - Quick check question: What makes a pathway to harm different from simply listing possible harms?

## Architecture Onboarding

- Component map: 
  - Conceptual Framework (exposure/intent matrix) -> Harm Type Categories (subject, interacts, suffers, excluded) -> Specific Harm Definitions (violation of right to self-determination, identity theft, etc.) -> Incident Database (35 annotated cases) -> Policy Intervention Guidelines (based on harm pathways)

- Critical path: 
  1. Identify affected entity exposure type
  2. Determine responsible entity intent (creation vs deployment)
  3. Map to specific harm categories
  4. Analyze pathway patterns across incidents
  5. Recommend policy interventions

- Design tradeoffs:
  - Incident-based vs literature-based: Incident analysis captures real patterns but may miss theoretical possibilities
  - Granular vs high-level: Specific harms provide actionable detail but may become unwieldy
  - Static vs dynamic: Fixed taxonomy provides stability but may not capture emerging harms

- Failure signatures:
  - New harm types that don't fit exposure/intent framework
  - Responsible entities that don't fit creator/deployer distinction
  - Incidents with multiple simultaneous exposure types
  - Policy interventions that don't align with harm pathway analysis

- First 3 experiments:
  1. Apply taxonomy to 5 new speech generation incidents to test coverage
  2. Map existing AI incident database entries to the taxonomy to identify gaps
  3. Conduct stakeholder interviews to validate harm pathway interpretations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we measure and evaluate the impact of speech generators on different demographic groups, including marginalized communities?
- Basis in paper: [inferred] The paper mentions the need to study bias in model outputs and understand the extent to which demographic and other personal attributes of affected entities impact their harm exposure.
- Why unresolved: Current evaluations rely on listener tests, which are subjective and oftentimes unreliable. Further research is required to determine which capabilities can and should be measured to assess harmful behavior, how these capabilities can be operationalized and measured, and to identify potential shortcomings of capability evaluations.
- What evidence would resolve it: Development and validation of objective evaluation methods for measuring the impact of speech generators on different demographic groups, including marginalized communities. This could involve creating standardized datasets, developing new metrics for assessing bias, and conducting rigorous studies to understand the differential impact of speech generators on various populations.

### Open Question 2
- Question: How can we effectively mitigate the risks posed by speech generators, particularly those related to system-level risks that have an extensive scope, occur over a long time horizon, affect society at large, and are not directly identifiable from model outputs?
- Basis in paper: [inferred] The paper highlights the need for further research to understand how harm pathways emerge, and their dependence on affected and responsible entities, as well as mediating factors. It also mentions the need to develop and validate approaches to mitigate testable and non-testable harms.
- Why unresolved: Harm pathways present a promising direction for analyzing the emergence and propagation of harm, but further research is required to study how these pathways emerge and how they can be effectively mitigated. Additionally, the paper mentions the need for regulatory, policy, and civil society interventions to develop institutional strategies to mitigate generative AI harms, but does not provide specific recommendations or evidence of successful interventions.
- What evidence would resolve it: Development and validation of effective mitigation strategies for both testable and non-testable harms, including regulatory, policy, and civil society interventions. This could involve conducting case studies of successful interventions, developing new frameworks for analyzing harm pathways, and creating tools and resources for stakeholders to assess and mitigate risks.

### Open Question 3
- Question: How can we ensure the responsible development and release of speech generation models, considering the potential for misuse and the need to balance innovation with safety?
- Basis in paper: [inferred] The paper mentions the need for safety-oriented strategies for model release, such as one (default) voice only, and mechanisms for tighter access control, such as for multi-speaker models. It also highlights the importance of considering harm pathways to assign accountability, identify gaps, and analyze the efficacy of mitigation strategies.
- Why unresolved: The paper does not provide specific recommendations or evidence of successful approaches to ensure the responsible development and release of speech generation models. It also does not address the potential tension between innovation and safety, or the need to balance the benefits of speech generation technology with the risks it poses.
- What evidence would resolve it: Development of best practices and guidelines for the responsible development and release of speech generation models, including safety-oriented strategies for model release and mechanisms for tighter access control. This could involve conducting case studies of successful approaches, creating tools and resources for developers and stakeholders to assess and mitigate risks, and engaging with policymakers and civil society to develop regulatory frameworks that balance innovation with safety.

## Limitations
- Taxonomy's generalizability is uncertain beyond the 35 analyzed incidents, particularly for emerging capabilities
- Exposure/intent framework may not capture harms from complex organizational structures with overlapping responsibilities
- Incident corpus may have selection bias toward publicly reported cases, potentially missing subtle or underreported harms

## Confidence

- **High Confidence**: The taxonomy's core structure (exposure types Ã— responsible entity intent) is well-supported by incident analysis and provides actionable harm categorization
- **Medium Confidence**: The framework's applicability to policy intervention and pathway modeling requires further validation across diverse incident types and stakeholder contexts
- **Low Confidence**: The taxonomy's coverage of future harm scenarios, particularly as speech generation technology evolves beyond voice-based outputs

## Next Checks

1. Apply the taxonomy to 10 new speech generation incidents from 2024-2025 to test coverage of emerging harm patterns and identify framework gaps
2. Conduct structured interviews with 5-10 domain experts (policy makers, developers, affected individuals) to validate the framework's practical utility for risk assessment and intervention design
3. Compare taxonomy coverage against 50 randomly sampled AI incidents from broader databases to assess whether speech generation harms are uniquely captured or overlap with general AI harm patterns