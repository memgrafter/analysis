---
ver: rpa2
title: Exploring Defeasibility in Causal Reasoning
arxiv_id: '2401.03183'
source_url: https://arxiv.org/abs/2401.03183
tags:
- causal
- strength
- effect
- cause
- defeaters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Defeasibility in causal reasoning captures the dynamic nature of
  causal relationships, where the strength of causality can be influenced by supporting
  or opposing arguments. Existing benchmarks and metrics fail to account for this
  aspect, leading to a gap in understanding and measuring defeasibility in causal
  reasoning.
---

# Exploring Defeasibility in Causal Reasoning

## Quick Facts
- **arXiv ID:** 2401.03183
- **Source URL:** https://arxiv.org/abs/2401.03183
- **Reference count:** 40
- **Key outcome:** δ-CAUSAL is the first benchmark dataset for defeasibility in causal reasoning; CESAR achieves 69.7% relative improvement over existing metrics

## Executive Summary
This paper addresses the critical gap in measuring defeasibility in causal reasoning - the ability to capture how supporting or opposing arguments affect the strength of causal relationships. The authors introduce δ-CAUSAL, a benchmark dataset of 11K events across ten domains specifically designed to study defeasibility, and propose CESAR, a novel metric that leverages token-level causal embeddings and attention mechanisms. CESAR demonstrates a significant 69.7% relative improvement over existing metrics in capturing causal strength changes brought by supporters and defeaters, while also highlighting the challenges faced by current LLMs like GPT-3.5 in generating appropriate supporters and defeaters.

## Method Summary
The research introduces two main contributions: δ-CAUSAL dataset construction and CESAR metric development. δ-CAUSAL contains 11K events across ten domains, with cause-effect pairs annotated with supporters (strengthening causality) and defeaters (weakening causality). The CESAR metric builds upon a transformer-based model with causal embeddings, calculating causal strength as a weighted aggregation of token-level causal strength using attention scores. CESAR is trained on an augmented e-CARE dataset (68,220 examples) and evaluated on δ-CAUSAL and COPA benchmarks. The approach also includes human evaluation of supporter and defeater generation by LLMs including GPT-3.5.

## Key Results
- CESAR achieves 69.7% relative improvement over existing metrics, increasing from 47.2% to 80.1% in capturing causal strength changes
- CESAR attains state-of-the-art performance on COPA causal reasoning tasks
- GPT-3.5 lags significantly behind humans by up to 4.5 and 10.7 points in generating correct supporters and defeaters respectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CESAR uses token-level causal embeddings and attention mechanisms to measure causal strength changes caused by supporters and defeaters
- Mechanism: CESAR builds upon a transformer-based model with causal embeddings, calculating causal strength as a weighted aggregation of token-level causal strength using attention scores
- Core assumption: Token-level associations between cause and effect events accurately reflect causal relationships, and attention mechanisms can prioritize relevant token pairs
- Evidence anchors:
  - [abstract] "CESAR achieves a significant 69.7% relative improvement over existing metrics, increasing from 47.2% to 80.1% in capturing the causal strength change brought by supporters and defeaters."
  - [section] "We propose CESAR, a robust and versatile metric for measuring causal strength... CESAR achieves a significant 69.7% improvement in quantifying changes in causal strength resulting from supporters and defeaters."
  - [corpus] Weak - limited citations but relevant focus on causal strength metrics and defeasibility in causal reasoning
- Break condition: If token-level associations fail to capture complex causal relationships or if attention mechanisms prioritize irrelevant token pairs

### Mechanism 2
- Claim: δ-CAUSAL serves as a benchmark for evaluating existing causal strength metrics in defeasible settings
- Mechanism: δ-CAUSAL includes cause-effect pairs with supporters and defeaters, allowing evaluation of how well metrics capture causal strength changes
- Core assumption: Existing metrics fail to capture causal strength changes in defeasible settings, and δ-CAUSAL provides a valid test for these metrics
- Evidence anchors:
  - [abstract] "We further show that current causal strength metrics fail to reflect the change of causal strength with the incorporation of supporters or defeaters in δ-CAUSAL."
  - [section] "With the presence of supporters and defeaters, δ-CAUSAL serves as an ideal touchstone for assessing the efficacy of existing metrics in capturing the causal strength change brought by supporters and defeaters."
  - [corpus] Moderate - several related papers on causal reasoning and evaluation metrics, but limited direct focus on defeasibility
- Break condition: If δ-CAUSAL does not adequately represent defeasible causal reasoning scenarios or if existing metrics unexpectedly perform well on δ-CAUSAL

### Mechanism 3
- Claim: Existing LLMs like GPT-3.5 struggle with generating correct supporters and defeaters for causal pairs
- Mechanism: Human evaluation of generated supporters and defeaters shows that LLMs lag significantly behind human performance
- Core assumption: Generating supporters and defeaters requires understanding defeasibility in causal reasoning, which current LLMs lack
- Evidence anchors:
  - [abstract] "Our experiments reveal that state-of-the-art pre-trained models, including GPT-3.5, lag behind humans by up to 4.5 and 10.7 points in generating correct supporters and defeaters, respectively."
  - [section] "The results show that even GPT-3.5 falls significantly short, lagging behind humans by 4.5 and 10.7 points in generating accurate supporters and defeaters, respectively."
  - [corpus] Moderate - related work on LLMs and causal reasoning, but limited focus on defeasibility and argument generation
- Break condition: If LLMs improve significantly on defeasibility tasks or if human evaluation criteria are not well-aligned with the task

## Foundational Learning

- Concept: Causal reasoning and defeasibility
  - Why needed here: Understanding the relationship between cause and effect events, and how supplementary information can strengthen or weaken this relationship
  - Quick check question: Can you explain how a supporter or defeater affects the causal strength between a cause and effect?

- Concept: Causal strength metrics and evaluation
  - Why needed here: Evaluating the effectiveness of existing metrics in capturing causal strength changes, and proposing a new metric (CESAR) to address limitations
  - Quick check question: How do you think existing metrics like CEQ and ROCK fail to capture causal strength changes in defeasible settings?

- Concept: Large language models and generation tasks
  - Why needed here: Assessing the ability of LLMs like GPT-3.5 to generate correct supporters and defeaters for causal pairs
  - Quick check question: What challenges do you think LLMs face in generating accurate supporters and defeaters for causal reasoning tasks?

## Architecture Onboarding

- Component map:
  δ-CAUSAL -> Existing metrics (CEQ, ROCK) -> CESAR -> LLMs (GPT-3.5, BART, T5)

- Critical path:
  1. Construct δ-CAUSAL dataset with diverse domains and defeasible causal pairs
  2. Evaluate existing metrics on δ-CAUSAL to identify limitations
  3. Propose CESAR and train on augmented e-CARE dataset
  4. Assess CESAR's performance on δ-CAUSAL and COPA
  5. Evaluate LLMs' ability to generate supporters and defeaters

- Design tradeoffs:
  - Token-level vs. sequence-level causal strength measurement
  - Fine-tuning vs. pre-trained models for CESAR
  - Automated vs. human evaluation for supporter and defeater generation

- Failure signatures:
  - CESAR failing to capture causal strength changes in δ-CAUSAL
  - Existing metrics unexpectedly performing well on δ-CAUSAL
  - LLMs generating supporters and defeaters with low accuracy

- First 3 experiments:
  1. Evaluate existing metrics (CEQ, ROCK) on δ-CAUSAL to establish baseline performance
  2. Train CESAR on augmented e-CARE data (68,220 examples) and evaluate on δ-CAUSAL using geometric mean accuracy
  3. Fine-tune LLMs (BART, T5, GPT-2) on δ-CAUSAL and evaluate their ability to generate supporters and defeaters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal temporal threshold for identifying defeaters in causal reasoning, and how does it vary across different domains?
- Basis in paper: [explicit] The paper mentions that "longer time intervals provide a broader temporal scope, allowing for the observation and annotation of more complex interactions and changes that might influence or negate the initial cause-effect relationship."
- Why unresolved: The paper sets time intervals to "relatively long-term periods" but doesn't explore whether this is optimal or if the ideal threshold varies by domain.
- What evidence would resolve it: Experiments varying time interval thresholds across domains and measuring the accuracy of defeater identification and generation would provide empirical evidence for optimal temporal thresholds.

### Open Question 2
- Question: How does the performance of CESAR scale with different sizes of training data and varying proportions of different types of training samples (causal, non-causal, causal with explanations, and causal with opposite explanations)?
- Basis in paper: [explicit] The paper mentions "there are four kinds of data samples for CESAR's training" and conducts an ablation study removing data augmentation.
- Why unresolved: The paper shows the importance of data augmentation but doesn't explore how different proportions of these sample types or training data sizes affect CESAR's performance.
- What evidence would resolve it: Systematic experiments varying training data sizes and proportions of different sample types while measuring CESAR's performance on supporter and defeater detection would reveal scaling properties.

### Open Question 3
- Question: How well does CESAR generalize to causal reasoning tasks in domains not represented in its training data, such as medicine or chemistry?
- Basis in paper: [inferred] The paper notes that "the domains covered by δ-CAUSAL remain limited in scope" and mentions medicine and chemistry as potential areas for expansion.
- Why unresolved: While CESAR is tested on COPA (which covers general causal reasoning), it hasn't been evaluated on domain-specific causal reasoning tasks like those in medicine or chemistry.
- What evidence would resolve it: Evaluating CESAR on domain-specific causal reasoning benchmarks from medicine, chemistry, or other scientific fields would demonstrate its generalization capabilities beyond the domains in its training data.

## Limitations

- The δ-CAUSAL dataset, while diverse across ten domains, may not capture the full complexity of real-world defeasible reasoning scenarios
- The absolute performance of CESAR on δ-CAUSAL remains moderate at 80.1% geometric mean accuracy, suggesting room for improvement
- The comparison with GPT-3.5 uses a single LLM model, and results might vary with different prompting strategies or more recent models

## Confidence

Our confidence in the core claims is **Medium-High** for CESAR's performance improvements, but **Medium** for the broader claims about defeasibility in causal reasoning.

## Next Checks

1. **Cross-dataset validation**: Test CESAR on additional causal reasoning benchmarks beyond δ-CAUSAL and COPA to assess generalization across different task types and domains

2. **Human evaluation expansion**: Conduct more extensive human evaluation studies with domain experts to validate the quality of generated supporters and defeaters, and to assess whether the 4.5 and 10.7 point gaps represent meaningful differences in practical applications

3. **Ablation studies**: Systematically evaluate the contribution of individual components in CESAR (token-level embeddings, attention mechanisms) to quantify their relative importance and identify potential optimization opportunities