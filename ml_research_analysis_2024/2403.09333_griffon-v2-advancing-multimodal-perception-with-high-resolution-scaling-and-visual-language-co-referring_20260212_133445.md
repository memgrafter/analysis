---
ver: rpa2
title: 'Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and
  Visual-Language Co-Referring'
arxiv_id: '2403.09333'
source_url: https://arxiv.org/abs/2403.09333
tags:
- visual
- object
- image
- arxiv
- griffon
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Griffon v2, a unified multimodal model that
  achieves state-of-the-art performance in fine-grained object perception by addressing
  the resolution limitation of existing large vision-language models. The authors
  introduce a high-resolution visual encoder combined with a lightweight down-sampling
  projector to efficiently scale input resolution beyond 1K while preserving context
  and fine details, enabling superior detection of small objects in complex scenes.
---

# Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring

## Quick Facts
- arXiv ID: 2403.09333
- Source URL: https://arxiv.org/abs/2403.09333
- Reference count: 40
- Primary result: Achieves 38.5% mAP on MSCOCO object detection and sets SOTA on REC (90.0 mAP), phrase grounding (84.8% ANY-BOX), and REG (72.5 CIDEr)

## Executive Summary
Griffon v2 introduces a unified multimodal model that addresses the resolution limitations of existing large vision-language models. By employing a high-resolution visual encoder with a lightweight down-sampling projector, the model scales input resolution beyond 1K while preserving context and fine details, enabling superior detection of small objects in complex scenes. Additionally, it proposes a visual-language co-referring paradigm that supports flexible user interaction through coordinates, textual descriptions, screenshots, or cross-image prompts, enhancing both accuracy and usability across multiple perception tasks.

## Method Summary
The model employs a three-stage end-to-end training pipeline: (1) High-resolution vision-language alignment using EV A2-CLIP-L/14 encoder and Llama2-13B LLM; (2) Multi-task co-referring pre-training on 12M localization-related instances; (3) Intent-enhanced instruction tuning with ~900K examples. The high-resolution visual encoder directly processes inputs up to 1022px, while a lightweight down-sampling projector compresses visual tokens efficiently. The visual-language co-referring paradigm supports diverse input formats including coordinates, text, screenshots, and cross-image prompts for enhanced flexibility.

## Key Results
- Achieves 38.5% mAP on MSCOCO object detection
- Sets new SOTA on REC (90.0 mAP) and phrase grounding (84.8% ANY-BOX)
- Achieves 20.3 MAE on object counting (FSCD-LVIS)
- Outperforms specialist models and advanced large vision-language models across multiple benchmarks

## Why This Works (Mechanism)
The high-resolution visual encoder with down-sampling projector preserves context and fine details better than image partitioning methods. Direct high-resolution input avoids loss of context and edge details that occur when dividing images into patches. The lightweight down-sampling projector compresses visual tokens efficiently while maintaining semantic information. Bilinear interpolation of position embeddings combined with strided convolution can effectively upscale resolution without introducing artifacts that degrade localization performance.

## Foundational Learning
- **High-resolution visual encoding**: Direct processing of high-resolution inputs (up to 1K) without patch partitioning preserves spatial context and fine details essential for small object detection.
- **Down-sampling projector design**: Lightweight projection with strided convolution efficiently compresses visual tokens while maintaining semantic information, enabling integration with LLMs.
- **Visual-language co-referring**: Unified framework supporting coordinates, text, screenshots, or cross-image prompts for flexible user interaction and enhanced localization accuracy.
- **Multi-task pre-training strategy**: Three-stage end-to-end training balances high-resolution alignment, multi-task localization, and instruction-following capabilities.
- **Position embedding interpolation**: Bilinear interpolation enables smooth position embedding scaling to higher resolutions without introducing localization artifacts.

## Architecture Onboarding
**Component Map**: High-resolution encoder -> Down-sampling projector -> LLM
**Critical Path**: Image input (1022px) → EV A2-CLIP-L/14 encoder → Down-sampling projector → LLM → Output predictions
**Design Tradeoffs**: Direct high-resolution encoding preserves context but increases computational load; down-sampling projector reduces token count but may lose fine details; multi-task training improves generalization but risks task interference.
**Failure Signatures**: Poor localization accuracy indicates insufficient high-resolution training data or suboptimal projector design; overfitting suggests imbalanced data distribution; computational bottlenecks indicate projector inefficiency.
**First Experiments**: 1) Compare mAP on MSCOCO val2017 across different resolution scaling methods; 2) Evaluate performance on diverse datasets (FSCD-LVIS, RefCOCO) to assess generalization; 3) Measure GPU memory consumption and inference latency at different resolutions.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but raises implicit questions about scalability to extremely high resolutions (e.g., 4K) given LLM token constraints, and the model's performance on non-natural imagery domains.

## Limitations
- Dataset composition uncertainty with unspecified data sources and preprocessing pipelines
- Architecture generalization concerns for extreme aspect ratios and non-natural imagery
- Computational efficiency trade-offs not substantiated with runtime benchmarks
- Potential multi-task interference between fine-grained detection and higher-level reasoning

## Confidence
**High confidence**: Claims about achieving SOTA on REC (90.0 mAP), phrase grounding (84.8% ANY-BOX), and object counting (20.3 MAE on FSCD-LVIS) are supported by benchmark comparisons.
**Medium confidence**: Architectural claims about preserving context through direct high-resolution encoding are plausible given evaluation results but lack ablation studies.
**Low confidence**: Efficiency claims regarding the lightweight down-sampling projector are not substantiated with runtime benchmarks or memory usage comparisons.

## Next Checks
1. **Ablation study on resolution scaling**: Compare high-resolution encoder with bilinear interpolation against patch-based approaches across multiple resolution levels (512px, 768px, 1024px) to quantify context preservation benefits.
2. **Cross-domain generalization test**: Evaluate on non-natural imagery datasets (medical imaging, satellite imagery, document analysis) to assess generalization beyond standard object detection benchmarks.
3. **Computational overhead measurement**: Measure actual GPU memory consumption and inference latency of the down-sampling projector across different input resolutions and batch sizes, comparing against traditional patch-based approaches.