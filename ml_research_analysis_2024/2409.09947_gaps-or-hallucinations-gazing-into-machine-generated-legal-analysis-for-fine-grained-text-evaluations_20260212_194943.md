---
ver: rpa2
title: Gaps or Hallucinations? Gazing into Machine-Generated Legal Analysis for Fine-grained
  Text Evaluations
arxiv_id: '2409.09947'
source_url: https://arxiv.org/abs/2409.09947
tags:
- legal
- generation
- case
- citation
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the concept of "gaps" as a neutral term to
  distinguish between human-written and machine-generated legal analysis, contrasting
  with the notion of strict hallucinations. The authors propose a detailed taxonomy
  of gaps, identifying intrinsic and extrinsic types, and develop a fine-grained detector
  to classify these gaps.
---

# Gaps or Hallucinations? Gazing into Machine-Generated Legal Analysis for Fine-grained Text Evaluations

## Quick Facts
- arXiv ID: 2409.09947
- Source URL: https://arxiv.org/abs/2409.09947
- Authors: Abe Bohan Hou; William Jurayj; Nils Holzenberger; Andrew Blair-Stanek; Benjamin Van Durme
- Reference count: 39
- Primary result: The best gap detector achieves 67% F1 score and 80% precision on test data, while 80% of LLM-generated legal analysis contains some form of gap

## Executive Summary
This paper introduces the concept of "gaps" as a neutral framework for evaluating machine-generated legal analysis, distinguishing between valid differences in interpretation and actual errors. The authors develop a detailed taxonomy of four gap categories and create a fine-grained detector using few-shot prompting with in-context demonstrations. When applied to state-of-the-art LLMs on legal analysis tasks, the detector reveals that around 80% of outputs contain hallucinations of various kinds, highlighting the challenges in automated legal reasoning generation.

## Method Summary
The authors created a taxonomy of four gap types (intrinsic gaps, target mismatch, citation content mismatch, no gaps) and manually annotated 40 examples to train an LLM-based gap detector using few-shot prompting with in-context demonstrations. The detector was evaluated on 20 test examples, achieving 67% F1 and 80% precision. This detector was then applied to 500 generations each from GPT-4o and Llama-3-8B-Instruct on the CLERC legal analysis task, computing GAPSCORE and GAPHALU metrics. The methodology emphasizes distinguishing between model failures (intrinsic gaps) and differences in citation organization or interpretation (extrinsic gaps).

## Key Results
- The gap detector achieves 67% F1 and 80% precision on the test set using few-shot prompting with in-context demonstrations
- GPT-4o generations have less hallucination compared to Llama-3-8B-Instruct, with lower GAPHALU scores
- Around 80% of LLM-generated legal analysis contains gaps, with citation content mismatch being the most common type
- The detector tends to over-predict citation content mismatch and under-predict intrinsic gaps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using a neutral term "gaps" rather than "hallucinations" allows more accurate classification of machine-generated legal analysis by distinguishing between actual errors and valid but different analysis styles.
- Mechanism: The taxonomy separates intrinsic gaps (model failures) from extrinsic gaps (differences in citation organization or interpretation). Target mismatches are explicitly classified as not necessarily indicating invalidity.
- Core assumption: Legal analysis can have multiple valid interpretations and organizations of the same facts.
- Evidence anchors:
  - [abstract] "We introduce the neutral notion of gaps, as opposed to hallucinations in a strict erroneous sense, to refer to the difference between human-written and machine-generated legal analysis. Gaps do not always equate to invalid generation."
  - [section 3.3] "On the other hand, we should not consider target mismatches as necessarily wrong since they mainly organize the information in a different way from the target paragraph."

### Mechanism 2
- Claim: Fine-grained gap detection using few-shot prompting with in-context demonstrations achieves reasonable accuracy for automated legal analysis evaluation.
- Mechanism: The detector uses an LLM to classify gaps into four categories (intrinsic gaps, target mismatch, citation content mismatch, no gaps) using manually labeled examples as demonstrations.
- Core assumption: LLMs can effectively learn to recognize different types of gaps when provided with clear examples and explanations.
- Evidence anchors:
  - [section 4.3] "Our best detector achieves 67% F1 and 80% precision on the test set."
  - [section 4.2] "Our detector is based on prompting a long-context LLM with in-context demonstrations of examples labeled by humans."

### Mechanism 3
- Claim: The proposed GAPSCORE and GAPHALU metrics effectively distinguish between different levels of validity in machine-generated legal analysis.
- Mechanism: GAPSCORE measures the proportion of examples with any gaps, while GAPHALU measures the proportion with actual hallucinations (intrinsic gaps or citation content mismatch).
- Core assumption: Different types of gaps have different implications for the validity of legal analysis.
- Evidence anchors:
  - [section 5.1] "GAPSCORE measures the ratio of N examples having gaps, and GAPHALU measures the ratio of hallucinations."
  - [section 5.2] "We discover that GPT-4o generations have less hallucination compared to Llama-3-8B-Instruct, as indicated by a lower GAPHALU score."

## Foundational Learning

- Concept: Legal citation systems and Bluebook formatting
  - Why needed here: The taxonomy includes specific categories for citation format mismatches, requiring understanding of legal citation standards.
  - Quick check question: What are the key differences between proper Bluebook citation format and common LLM generation errors?

- Concept: Legal reasoning structure and argumentation
  - Why needed here: Understanding how legal arguments are organized (chain vs parallel citation, agree vs disagree characterization) is essential for classifying target mismatches.
  - Quick check question: How does a chain citation structure differ from a parallel citation structure in legal writing?

- Concept: Intrinsic vs extrinsic model errors
  - Why needed here: The gap taxonomy explicitly distinguishes between model failures (intrinsic) and differences in interpretation/organization (extrinsic).
  - Quick check question: What distinguishes an intrinsic gap from an extrinsic gap in the context of legal analysis generation?

## Architecture Onboarding

- Component map: Dataset preparation -> Human annotation pipeline -> LLM-based gap detector -> GAPSCORE/GAPHALU evaluation metrics
- Critical path: Human annotation → Detector training with in-context demonstrations → Automated evaluation of generated legal analysis
- Design tradeoffs: Using LLM-based detection trades off accuracy for scalability compared to full human annotation, but achieves reasonable performance (67% F1) while being much more efficient
- Failure signatures: Low precision in gap detection (many false positives) suggests the model is too sensitive to differences; low recall suggests it's missing important error categories
- First 3 experiments:
  1. Vary the number of in-context demonstrations to find the optimal balance between performance and computational cost
  2. Test different base models (GPT-4o, Llama-3.1-8B-instruct, Mistral-Nemo) to identify which performs best for legal gap detection
  3. Compare the detector's performance on different types of gaps to identify which categories are most challenging to classify

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the detection of intrinsic gaps, given that current detectors tend to under-predict this category?
- Basis in paper: [explicit] The paper notes that intrinsic gaps are generally considered intrinsic hallucinations and that the Mistral-Nemo detector tends to under-predict the presence of intrinsic gaps.
- Why unresolved: The paper suggests that this might be due to a relative lack of G1 training data or that the detection of G1 is challenging per se. However, it does not provide a concrete solution to improve the detection of intrinsic gaps.
- What evidence would resolve it: Further research could focus on developing new techniques or approaches specifically designed to detect intrinsic gaps, or on increasing the amount of training data for this category.

### Open Question 2
- Question: How can we improve the retrieval architecture for legal analysis generation to mitigate extrinsic hallucinations?
- Basis in paper: [explicit] The paper suggests that improving retrieval architecture, especially with long-context retrieval strategy with awareness of the latent logical structure, can be one critical direction to improve generation and mitigate hallucinations.
- Why unresolved: While the paper suggests this direction, it does not provide a concrete solution or methodology for improving the retrieval architecture.
- What evidence would resolve it: Further research could focus on developing new retrieval strategies or architectures specifically designed for legal analysis generation, and testing their effectiveness in mitigating extrinsic hallucinations.

### Open Question 3
- Question: How can we effectively decompose the reasoning structure in legal analysis to improve generation quality and mitigate hallucinations?
- Basis in paper: [explicit] The paper suggests that decomposition of the reasoning structure in legal analysis may critically improve generation quality and mitigate hallucinations.
- Why unresolved: While the paper suggests this direction, it does not provide a concrete solution or methodology for decomposing the reasoning structure in legal analysis.
- What evidence would resolve it: Further research could focus on developing new techniques or approaches for decomposing the reasoning structure in legal analysis, and testing their effectiveness in improving generation quality and mitigating hallucinations.

## Limitations

- The manually annotated dataset is relatively small (40 examples), which may limit the detector's ability to generalize to more diverse legal scenarios
- The gap taxonomy, while detailed, may not capture all possible error types in legal analysis generation, particularly subtle contextual errors
- The detection accuracy (67% F1) suggests the method is useful but not yet reliable for critical legal applications where errors could have significant consequences

## Confidence

**High Confidence**: The core finding that approximately 80% of LLM-generated legal analysis contains some form of gap is well-supported by the data and methodology. The distinction between intrinsic and extrinsic gaps is clearly articulated and demonstrated through the manual annotation process.

**Medium Confidence**: The relative performance of different base models (GPT-4o vs Llama-3-8B-Instruct) and the effectiveness of the GAPSCORE and GAPHALU metrics are reasonably well-established, though the small test set size introduces some uncertainty. The proposed mitigation strategies (continued pre-training, improved retrieval) are logical extensions of the findings but lack direct empirical validation in this study.

**Low Confidence**: The generalizability of the gap taxonomy beyond the specific legal domain and citation style examined remains uncertain. The detector's performance on real-world legal analysis tasks outside the controlled evaluation environment is not yet established.

## Next Checks

1. **Dataset Expansion Validation**: Expand the manually annotated dataset to 200+ examples across multiple legal domains and citation styles, then retrain and evaluate the detector to assess whether performance scales with more diverse training data.

2. **Cross-Domain Applicability Test**: Apply the gap taxonomy and detection framework to non-legal domains (medical analysis, technical writing) to determine if the four-category structure generalizes or requires domain-specific modifications.

3. **Real-World Impact Assessment**: Conduct a study with legal professionals to evaluate whether the gap categories identified by the detector correlate with actual legal reasoning errors that would impact case outcomes or professional judgment.