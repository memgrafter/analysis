---
ver: rpa2
title: Hypothesis-only Biases in Large Language Model-Elicited Natural Language Inference
arxiv_id: '2410.08996'
source_url: https://arxiv.org/abs/2410.08996
tags:
- language
- snli
- computational
- association
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) introduce
  annotation artifacts when generating Natural Language Inference (NLI) hypotheses,
  similar to human crowd-workers. The authors recreate portions of the SNLI corpus
  by prompting LLMs (GPT-4, Llama-2, and Mistral) with the same instructions given
  to human annotators.
---

# Hypothesis-only Biases in Large Language Model-Elicited Natural Language Inference

## Quick Facts
- arXiv ID: 2410.08996
- Source URL: https://arxiv.org/abs/2410.08996
- Reference count: 16
- LLMs can generate annotation artifacts in NLI datasets similar to human annotators

## Executive Summary
This study investigates whether large language models (LLMs) introduce annotation artifacts when generating Natural Language Inference (NLI) hypotheses. The authors recreate portions of the SNLI corpus by prompting GPT-4, Llama-2, and Mistral with the same instructions given to human annotators. Hypothesis-only classifiers achieve high accuracy (86-96%) on LLM-generated datasets, indicating the presence of artifacts. The study also identifies frequent "give-away" words associated with NLI labels, such as "swimming in a pool" appearing in over 10,000 contradictions generated by GPT-4.

## Method Summary
The authors generate NLI hypotheses using three LLMs (GPT-4, Llama-2, Mistral) prompted with SNLI premise sentences and instructions to create entailed, neutral, and contradicted hypotheses. They manually validate generated hypotheses for label accuracy, then train hypothesis-only classifiers (Naive Bayes and BERT-based) on these datasets. The classifiers' performance on both LLM-generated and human-generated datasets is evaluated to detect annotation artifacts. The study also analyzes word frequencies to identify "give-away" words strongly correlated with specific labels.

## Key Results
- BERT-based hypothesis-only classifiers achieve 86-96% accuracy on LLM-elicited NLI datasets
- Frequent "give-away" words like "swimming in a pool" appear in over 10,000 contradictions generated by GPT-4
- LLM-elicited give-away words employ similar strategies as human-elicited NLI, with generic words like "person" and "activity" appearing in entailed hypotheses
- Many give-away words for all three LLMs appear directly in the prompt, suggesting context copying behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hypothesis-only classifiers detect annotation artifacts because LLMs generate hypotheses using similar patterns and heuristics as human annotators.
- Mechanism: When LLMs generate hypotheses, they often rely on surface-level patterns (e.g., specific words or phrases) that correlate with certain labels, creating annotation artifacts. Hypothesis-only classifiers can exploit these patterns to achieve high accuracy without considering the premise.
- Core assumption: LLMs and human annotators use similar strategies when generating hypotheses, such as adding or removing details to create entailed, neutral, or contradicted hypotheses.
- Evidence anchors:
  - [abstract] "We recreate a portion of the Stanford NLI corpus using GPT-4, Llama-2 and Mistral 7b, and train hypothesis-only classifiers to determine whether LLM-elicited hypotheses contain annotation artifacts."
  - [section] "LLM-elicited give-away words seem to employ similar strategies or heuristics as human-elicited NLI. Entailed examples in SNLI often contain generic words like humans, activity, and interacting. We find a similar pattern in LLM-generated entailed hypotheses, e.g. person and activity in GPT-4 and Llama."
- Break condition: If LLMs are explicitly trained or prompted to avoid common patterns and heuristics, the hypothesis-only artifacts would be less detectable.

### Mechanism 2
- Claim: The presence of frequent "give-away" words in LLM-generated hypotheses contributes to high hypothesis-only classification accuracy.
- Mechanism: LLM-generated hypotheses contain specific words or phrases that are highly indicative of certain labels. These "give-away" words allow hypothesis-only classifiers to achieve high accuracy by focusing on these features alone.
- Core assumption: LLMs tend to generate hypotheses with predictable word choices that correlate strongly with specific labels.
- Evidence anchors:
  - [abstract] "We also find frequent 'give-aways' in LLM-generated hypotheses, e.g. the phrase 'swimming in a pool' appears in more than 10,000 contradictions generated by GPT-4."
  - [section] "Many give-away words for all three LLMs appear directly in the prompt. For example, 'There is an entailment give-away for GPT-4, 'Someone and catch are neutral give-aways for Llama, and 'The, 'sitting, and 'couch are contradiction give-aways for Mistral."
- Break condition: If the prompts are modified to reduce the influence of specific words or if LLMs are trained to diversify their word choices, the frequency of give-away words would decrease.

### Mechanism 3
- Claim: LLMs often copy features from the prompt when generating hypotheses, leading to annotation artifacts.
- Mechanism: LLMs tend to incorporate words and phrases from the prompt into their generated hypotheses. This copying behavior results in hypotheses that contain predictable elements, which hypothesis-only classifiers can exploit.
- Core assumption: LLMs have a tendency to replicate context from the prompt in their outputs.
- Evidence anchors:
  - [abstract] "Many give-away words for all three LLMs appear directly in the prompt."
  - [section] "This might be explained by the well-documented phenomena of LLMs often copying features in context (Elhage et al., 2021; Olsson et al., 2022; Bansal et al., 2023; Zhang et al., 2024)."
- Break condition: If LLMs are trained with techniques that reduce context copying, such as using more diverse training data or implementing mechanisms to encourage novel word choices, the impact of prompt copying would diminish.

## Foundational Learning

- Concept: Natural Language Inference (NLI)
  - Why needed here: Understanding NLI is crucial because the study focuses on the quality of hypotheses generated for this task and the presence of annotation artifacts.
  - Quick check question: What are the three possible relationships between a premise and a hypothesis in NLI?

- Concept: Annotation Artifacts
  - Why needed here: Annotation artifacts are the main focus of the study, as they can lead to models exploiting dataset biases rather than learning the intended task.
  - Quick check question: How do annotation artifacts affect the performance of NLI models?

- Concept: Hypothesis-only Classification
  - Why needed here: Hypothesis-only classification is the method used to detect annotation artifacts in the LLM-generated datasets.
  - Quick check question: Why would a high accuracy in hypothesis-only classification indicate the presence of annotation artifacts?

## Architecture Onboarding

- Component map:
  Data Generation -> Quality Assurance -> Feature Extraction -> Evaluation
  LLMs -> Manual Validation -> Hypothesis-only Classifiers -> Classifier Performance

- Critical path:
  1. Prompt LLMs with instructions to generate hypotheses.
  2. Validate generated hypotheses for label accuracy.
  3. Train hypothesis-only classifiers on the datasets.
  4. Evaluate classifier performance to detect annotation artifacts.

- Design tradeoffs:
  - Using high temperatures for generation increases diversity but may reduce reproducibility.
  - Manual validation ensures quality but is time-consuming and may introduce human bias.
  - Simple models like Naive Bayes are interpretable but may miss complex patterns captured by BERT.

- Failure signatures:
  - Low accuracy of hypothesis-only classifiers may indicate a lack of annotation artifacts.
  - High lexical overlap between LLM-generated and SNLI hypotheses suggests potential data copying.
  - Disagreement with NLI labels during manual validation indicates issues in hypothesis generation.

- First 3 experiments:
  1. Train a hypothesis-only Naive Bayes classifier on the SNLI dataset and evaluate its accuracy on both SNLI and LLM-generated evaluation sets.
  2. Analyze the frequency of specific words in LLM-generated hypotheses and correlate them with NLI labels to identify give-away words.
  3. Modify the prompt to reduce the presence of specific words and regenerate hypotheses, then re-evaluate the hypothesis-only classifiers to see if accuracy decreases.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompting strategies (e.g., temperature, top-p, top-k values) affect the prevalence of annotation artifacts in LLM-generated NLI datasets?
- Basis in paper: [explicit] The paper uses specific temperature (0.75) and top-p (0.9) values for all LLMs but acknowledges this as a design choice.
- Why unresolved: The study only uses one set of prompting parameters for all models. Different values could yield different levels of annotation artifacts.
- What evidence would resolve it: Comparative analysis of hypothesis-only classifier performance using different prompting strategies across the same LLMs and premises.

### Open Question 2
- Question: To what extent do the observed annotation artifacts in LLM-generated NLI data transfer to other NLP tasks (e.g., sentiment analysis, question answering)?
- Basis in paper: [inferred] The study focuses exclusively on NLI, but annotation artifacts are a general concern in NLP dataset creation.
- Why unresolved: The paper demonstrates artifacts exist in NLI but doesn't explore whether this finding generalizes to other tasks or domains.
- What evidence would resolve it: Replication of the hypothesis-only classifier methodology on LLM-generated datasets for multiple NLP tasks to compare artifact prevalence.

### Open Question 3
- Question: What is the relationship between model size/capability and the tendency to generate annotation artifacts in NLI datasets?
- Basis in paper: [explicit] The study uses models of varying sizes (7B to 70B parameters) and observes different levels of artifact generation.
- Why unresolved: While the paper notes differences between models, it doesn't systematically analyze how model scale correlates with artifact generation.
- What evidence would resolve it: Controlled study varying only model size while keeping other factors constant, measuring hypothesis-only classifier accuracy as a function of parameter count.

## Limitations

- The study doesn't sufficiently distinguish between LLM-specific biases and replication of known human annotation patterns
- Temperature settings for generation were not specified, which could significantly impact the diversity and quality of generated hypotheses
- The manual validation process may introduce its own biases that aren't fully accounted for in the analysis

## Confidence

- **High Confidence**: The existence of annotation artifacts in LLM-generated datasets is well-supported by the high accuracy of hypothesis-only classifiers (86-96%). The identification of specific "give-away" words correlated with NLI labels is robust.
- **Medium Confidence**: The claim that LLMs use similar strategies as human annotators to create artifacts is plausible but not definitively proven. The mechanism of context copying from prompts is documented in literature but needs more direct evidence in this specific context.
- **Low Confidence**: The assertion that these artifacts will necessarily propagate through fine-tuning is speculative. The study doesn't test downstream effects on trained models, making this an extrapolation rather than a demonstrated finding.

## Next Checks

1. **Temperature Sensitivity Analysis**: Re-run the hypothesis generation with varying temperature settings (low, medium, high) and measure how hypothesis-only classifier accuracy changes. This would reveal whether the artifacts are robust to generation parameters or merely artifacts of specific sampling strategies.

2. **Cross-dataset Artifact Transfer Test**: Train hypothesis-only classifiers on LLM-generated data and evaluate them on human-generated SNLI, then vice versa. If artifacts transfer bidirectionally, it suggests shared underlying mechanisms; if not, it indicates LLM-specific bias patterns.

3. **Premise-Context Impact Study**: Generate hypotheses with varying levels of premise information (no premise, partial premise, full premise) and measure how this affects the presence and nature of annotation artifacts. This would determine whether artifacts are truly hypothesis-only phenomena or require premise context to manifest.