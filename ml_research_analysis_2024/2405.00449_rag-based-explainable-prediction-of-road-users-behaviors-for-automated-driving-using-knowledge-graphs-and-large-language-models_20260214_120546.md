---
ver: rpa2
title: RAG-based Explainable Prediction of Road Users Behaviors for Automated Driving
  using Knowledge Graphs and Large Language Models
arxiv_id: '2405.00449'
source_url: https://arxiv.org/abs/2405.00449
tags:
- pedestrian
- vehicle
- road
- prediction
- lane
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces an explainable road users\u2019 behavior\
  \ prediction system that leverages Knowledge Graphs (KG) and Large Language Models\
  \ (LLM) through Retrieval Augmented Generation (RAG). By integrating Knowledge Graph\
  \ Embeddings (KGE) and Bayesian inference, the system enables fully inductive reasoning\
  \ using both historical data in the KG and real-time sensor inputs."
---

# RAG-based Explainable Prediction of Road Users Behaviors for Automated Driving using Knowledge Graphs and Large Language Models

## Quick Facts
- arXiv ID: 2405.00449
- Source URL: https://arxiv.org/abs/2405.00449
- Reference count: 40
- One-line primary result: A KG+LLM system achieves superior pedestrian crossing and lane change prediction with interpretable explanations.

## Executive Summary
This work presents an explainable behavior prediction system for automated driving that integrates Knowledge Graphs (KGs) with Large Language Models (LLMs) via Retrieval Augmented Generation (RAG). By encoding contextual and social features into KGs, learning embeddings, and applying Bayesian inference, the approach enables inductive reasoning for pedestrian crossing and vehicle lane change prediction. The system also provides human-readable explanations through RAG, improving interpretability without sacrificing accuracy. Performance exceeds state-of-the-art baselines on both pedestrian and driver datasets.

## Method Summary
The method builds KGs from sensor-derived features (pedestrian motion, driver intent) using ontologies, then learns embeddings (TransE, ComplEx) to represent entities and relations in continuous space. Bayesian inference computes probabilities P(h|e) over reified triples for predictions. Fuzzy logic rules mined from data are added to the KG for richer evidence. Explainability is achieved via RAG, which retrieves context-aware textual chunks and feeds them to an LLM to generate predictions' explanations. The pipeline uses AmpliGraph for KG operations, LangChain for RAG, and ChromaDB for vector storage.

## Key Results
- Achieves higher F1-score and anticipation time in pedestrian crossing prediction vs. state-of-the-art methods.
- Outperforms baseline lane change prediction models at multiple time horizons before maneuver onset.
- Provides interpretable, context-aware explanations for both pedestrian and driver behavior predictions using RAG.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The system achieves explainable behavior prediction by integrating Knowledge Graphs (KGs) with Bayesian inference over reified triples.
- Mechanism: Knowledge Graph Embeddings (KGE) encode entities and relations into latent vectors; Bayesian inference then computes P(h|e) using the probabilities of reified triples, allowing probabilistic reasoning over contextual features.
- Core assumption: The embeddings preserve relational semantics such that probability estimates from reified triples are meaningful for inference.
- Evidence anchors:
  - [abstract] "Knowledge Graph Embeddings (KGE) and Bayesian inference are combined to allow the deployment of a fully inductive reasoning system"
  - [section III-C] "The Bayes rule in Equation (1) is used to compute the probability of a hypothesis given some evidence"
  - [corpus] Weak - no direct citations in neighbor set
- Break condition: If embeddings do not capture relational semantics, P(e|h) estimates become unreliable and predictions degrade.

### Mechanism 2
- Claim: Fuzzy logic rules enhance the KG's predictive power by providing explicit, interpretable conditions that can be incorporated into the Bayesian inference.
- Mechanism: Rules are mined from data (e.g., IVTURS-FARC), converted into reified triples, and added to the KG; the system then conditions predictions on both sensor-derived features and rule-derived features.
- Core assumption: Fuzzy rules mined from training data generalize to unseen scenarios and their reified forms remain semantically valid in the KG.
- Evidence anchors:
  - [section IV-A] "fuzzy rules structured as follows: Rule R j : if x 1 is A j1 and...and x n is A jn then Class = Cj with RW j"
  - [abstract] "incorporates contextual features into a knowledge-based representation that can also encode other sources of information, such as human knowledge representing driving experience"
  - [corpus] Weak - no direct citations in neighbor set
- Break condition: If mined rules are overfit or contradictory, the KG's probabilistic consistency breaks down.

### Mechanism 3
- Claim: Retrieval-Augmented Generation (RAG) supplies human-readable explanations by combining KG-derived context with a language model.
- Mechanism: Textual chunks describing pedestrian or driver states are embedded and stored; queries are augmented with retrieved context before being fed to an LLM to generate explanations.
- Core assumption: The retrieved chunks are semantically relevant and sufficient for the LLM to generate coherent, context-aware explanations.
- Evidence anchors:
  - [abstract] "explainable descriptions of the behavioral predictions have been implemented using Retrieval Augmented Generation Techniques (RAG)"
  - [section IV-B] "Retrieval Augmented Generation (RAG) is a machine learning model that combines the power of pre-trained language models with the ability of a retrieval system to retrieve relevant information"
  - [corpus] Weak - no direct citations in neighbor set
- Break condition: If retrieval fails to find relevant chunks, generated explanations become generic or incorrect.

## Foundational Learning

- Concept: Knowledge Graph Embeddings (KGE)
  - Why needed here: To convert symbolic KG triples into continuous vector space for probabilistic inference and reasoning.
  - Quick check question: What is the difference between TransE and ComplEx in how they model relations?
- Concept: Bayesian Inference with Reified Triples
  - Why needed here: To perform probabilistic reasoning over KG facts by treating relations as first-class entities in the probability space.
  - Quick check question: How is P(e|h) computed when evidence consists of multiple independent reified triples?
- Concept: Fuzzy Logic and Rule Mining
  - Why needed here: To translate human-readable, linguistic rules into structured triples that can be integrated into the KG and used for inference.
  - Quick check question: Why might a Takagi-Sugeno fuzzy inference system be preferred for this application?

## Architecture Onboarding

- Component map:
  Data extraction → Feature transformation → KG generation → KGE learning → Bayesian inference → Prediction → RAG explanation
- Critical path:
  Feature extraction → KG generation → KGE training → Bayesian inference → Prediction
  RAG explanation is a parallel add-on that does not affect prediction correctness.
- Design tradeoffs:
  - Using reified triples increases expressiveness but also KG size and computational cost.
  - ComplEx vs TransE: ComplEx handles asymmetric relations better but may be slower; TransE is simpler and faster but less expressive.
  - RAG-based explanations depend on quality of textual corpus; poor retrieval yields low-quality explanations.
- Failure signatures:
  - Prediction accuracy drops → Check embedding training (loss, early stopping).
  - Explanations are nonsensical → Check retrieval recall and chunking strategy.
  - System slow → Check KG size and batch sizes for KGE training.
- First 3 experiments:
  1. Train KGE with a small KG (500 triples) and test Bayesian inference accuracy on a held-out set.
  2. Add fuzzy rules to KG and measure change in F1-score vs baseline.
  3. Test RAG retrieval recall by querying known pedestrian states and checking chunk relevance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed knowledge graph and Bayesian inference approach compare to state-of-the-art deep learning methods in terms of handling occluded pedestrians or vehicles?
- Basis in paper: [explicit] The paper mentions the capability of the proposed system to predict missing entities (e.g. occluded pedestrians) or relations (e.g. lane change intention) in road scenes that may have been missed by purely data-driven techniques.
- Why unresolved: While the paper claims the system can handle occluded entities, it does not provide experimental results or quantitative comparisons specifically focusing on occluded scenarios.
- What evidence would resolve it: Conducting experiments on datasets with occluded entities and comparing the performance of the proposed approach to deep learning methods in terms of prediction accuracy and F1-score.

### Open Question 2
- Question: What is the impact of incorporating fuzzy rules and human knowledge into the knowledge graph on the overall performance and explainability of the system?
- Basis in paper: [explicit] The paper discusses the integration of fuzzy rules and human knowledge into the knowledge graph to enhance explainability and provide additional evidence for Bayesian inference.
- Why unresolved: The paper does not provide a detailed analysis of the specific impact of fuzzy rules and human knowledge on the system's performance and explainability. It only mentions that the inclusion of fuzzy rules enhances the robustness of the knowledge graph and provides additional evidence.
- What evidence would resolve it: Conducting ablation studies to compare the performance and explainability of the system with and without fuzzy rules and human knowledge, and analyzing the specific contributions of each component.

### Open Question 3
- Question: How well does the proposed approach generalize to different cultural contexts and road environments?
- Basis in paper: [inferred] The paper mentions the need for further research to understand road users' behaviors in cross-cultural settings and the intention to gather data in regions with different social rules.
- Why unresolved: The paper does not provide any experimental results or analysis on the generalization capabilities of the proposed approach to different cultural contexts and road environments.
- What evidence would resolve it: Conducting experiments on datasets from different regions and cultures, and analyzing the performance and explainability of the proposed approach in each context. Additionally, investigating the impact of cultural differences on the knowledge graph structure and the effectiveness of the Bayesian inference process.

## Limitations

- Exact thresholds for converting numerical features to linguistic categories are not specified, which may affect KG construction and downstream predictions.
- Detailed architectures of neural models used for feature extraction (motion activity transformer, PedRecNet) are not provided, limiting reproducibility.
- Performance in complex multi-agent scenarios or long-term forecasting beyond immediate maneuvers is not evaluated.

## Confidence

- **High confidence** in the integration of KGE and Bayesian inference for probabilistic reasoning over contextual features, supported by established literature and clear mathematical formulation.
- **Medium confidence** in the added value of fuzzy rules and RAG-based explanations, as their impact on prediction accuracy is shown but qualitative benefits are harder to quantify.
- **Low confidence** in the exact thresholds and model architectures for feature extraction, as these are critical for faithful reproduction but not fully specified.

## Next Checks

1. Replicate the Bayesian inference pipeline with a small synthetic KG and verify P(h|e) estimates match expected probabilities under controlled conditions.
2. Test the impact of adding fuzzy rules to the KG by measuring changes in F1-score versus a baseline without rules.
3. Evaluate RAG retrieval recall by querying known pedestrian states and checking whether the returned chunks are semantically relevant and sufficient for generating accurate explanations.