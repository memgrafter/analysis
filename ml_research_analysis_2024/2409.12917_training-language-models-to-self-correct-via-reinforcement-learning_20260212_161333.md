---
ver: rpa2
title: Training Language Models to Self-Correct via Reinforcement Learning
arxiv_id: '2409.12917'
source_url: https://arxiv.org/abs/2409.12917
tags:
- self-correction
- training
- arxiv
- learning
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces SCoRe, a multi-turn reinforcement learning
  approach that significantly improves the self-correction capabilities of large language
  models using entirely self-generated data. Unlike previous methods that rely on
  supervised fine-tuning or external supervision, SCoRe trains models to correct their
  own mistakes through two stages: an initialization stage that decouples attempts
  to avoid behavior collapse, and a second stage that jointly optimizes both attempts
  with a reward shaping term that incentivizes self-correction progress.'
---

# Training Language Models to Self-Correct via Reinforcement Learning

## Quick Facts
- arXiv ID: 2409.12917
- Source URL: https://arxiv.org/abs/2409.12917
- Reference count: 40
- Key outcome: SCoRe achieves 15.6% improvement in self-correction on MATH and 9.1% on HumanEval without external supervision

## Executive Summary
This paper introduces SCoRe (Self-Correction via Reinforcement Learning), a novel approach for training language models to correct their own mistakes without requiring external supervision. Unlike previous methods that rely on supervised fine-tuning or teacher models, SCoRe uses entirely self-generated data through a two-stage reinforcement learning process. The method significantly improves self-correction capabilities on mathematical reasoning and code generation tasks, achieving state-of-the-art performance while addressing the distribution shift problem inherent in offline approaches.

## Method Summary
SCoRe employs a two-stage multi-turn reinforcement learning approach to train language models for self-correction. Stage I initializes the policy by optimizing second-attempt rewards while constraining the first-attempt distribution to match the base model, preventing behavior collapse. Stage II jointly optimizes both attempts using a reward shaping term that incentivizes genuine progress toward correctness rather than superficial edits. The method uses on-policy sampling to address distribution shift, collecting correction traces from the current policy during training rather than from a fixed base model.

## Key Results
- 15.6% improvement in self-correction accuracy on MATH problems
- 9.1% improvement in self-correction accuracy on HumanEval code generation tasks
- Achieves state-of-the-art performance without requiring multiple models or teacher supervision
- Successfully addresses distribution shift problem that plagues offline supervised fine-tuning approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage RL training with KL regularization prevents behavior collapse
- Mechanism: Stage I initializes a policy that decouples first-attempt and second-attempt behaviors by constraining first-attempt distribution to the base model while optimizing second-attempt reward. Stage II then jointly optimizes both attempts with a reward shaping term that emphasizes progress toward self-correction.
- Core assumption: Learning a high-reward second attempt without shifting the first-attempt distribution creates a less biased initialization for subsequent multi-turn RL.
- Evidence anchors:
  - [abstract]: "SCoRe employs two-stage training: in the first stage, it produces an initialization that is less susceptible to behavior collapse by training to correct second-attempt responses while constraining the first-turn distribution to be close to the base model; followed by training on both attempts to maximize reward in the second stage."
  - [section 5.1]: "The objective is: max E[r̂(y2, y*) - β2 DKL(πθ(·|x1)||πref(·|x1))], where β2 is a hyperparameter designed to enforce a strict KL penalty only on the first attempt to avoid shifting of the first-turn responses."
- Break condition: If β2 is too small, the first-attempt distribution drifts and behavior collapse re-emerges; if too large, the policy cannot learn meaningful second-attempt improvements.

### Mechanism 2
- Claim: Reward shaping term encourages genuine self-correction instead of superficial edits
- Mechanism: In Stage II, a bonus term b(y2|y1, y*) = α · (r̂(y2, y*) - r̂(y1, y*)) is added to the second-attempt reward, penalizing transitions that change correct responses to incorrect ones and rewarding progress toward correctness.
- Core assumption: The shaped reward makes the "direct" strategy of optimizing first-attempt accuracy followed by no correction less attractive than the "meta" strategy of self-correction.
- Evidence anchors:
  - [abstract]: "The second stage of multi-turn RL employs a reward shaping term that rewards 'progress' towards self-correction as opposed to the correctness of the final response."
  - [section 5.2]: "We modify the reward r̂(y2, y*) in Equation 4, at the second attempt with a bonus b̂(y2|y1, y*) := α · (r̂(y2, y*) - r̂(y1, y*)), where α is a positive constant multiplier, ideally larger than 1.0."
- Break condition: If α is too small, the shaping has negligible effect; if too large, the policy may over-correct and flip correct responses to incorrect ones.

### Mechanism 3
- Claim: On-policy sampling addresses distribution shift inherent in offline SFT approaches
- Mechanism: By collecting correction traces from the current policy during training rather than from a fixed base model, the learned policy sees first-attempt errors that match its own error distribution, enabling effective self-correction.
- Core assumption: The distribution of mistakes made by the current policy is sufficiently similar to the distribution of mistakes it will make at test time.
- Evidence anchors:
  - [section 4]: "We observe that training via SFT falls prey to either a distribution mismatch between mistakes made by the data-collection policy and the model's own responses, or to behavior collapse..."
  - [section 5]: "Utilizing on-policy RL is a natural way to address distribution shift, and our method will do so by extending Equation 2 to multiple turns..."
- Break condition: If the policy changes too rapidly, the on-policy data may become stale relative to the current policy's error distribution.

## Foundational Learning

- Concept: Multi-turn Markov Decision Process (MDP) formalism for sequential decision-making
  - Why needed here: The self-correction problem requires modeling the interaction between consecutive attempts as a sequential decision process where each attempt conditions on previous attempts.
  - Quick check question: In the two-turn MDP for self-correction, what is the state at turn 2 and what action does the agent take?

- Concept: Policy gradient methods with baseline (REINFORCE) for discrete action spaces
  - Why needed here: The LLM outputs tokens sequentially, creating a discrete action space; policy gradient methods can optimize expected reward without requiring differentiability of the reward function.
  - Quick check question: How does the KL-divergence penalty in Equation 2 affect the policy gradient update direction?

- Concept: Reward shaping and potential-based shaping in reinforcement learning
  - Why needed here: The shaping term in Stage II provides denser learning signals by rewarding intermediate progress rather than only final correctness, helping the policy discover the self-correction strategy.
  - Quick check question: What property must a reward shaping function satisfy to preserve the optimal policy while providing better learning signals?

## Architecture Onboarding

- Component map: Base model → Stage I RL trainer (KL-constrained) → Stage II RL trainer (reward shaping) → Evaluation pipeline
- Critical path: Data generation (on-policy sampling) → Stage I training (KL-constrained RL) → Stage II training (joint RL with shaping) → Evaluation on held-out problems
- Design tradeoffs: Stage I's strict KL constraint ensures stability but may limit exploration; reward shaping helps discovery but requires careful α tuning; on-policy sampling is more data-efficient than offline SFT but requires careful replay buffer management.
- Failure signatures: 
  - If Δ(t1,t2) is near zero or negative after Stage II, likely behavior collapse occurred (check edit distance ratios between attempts)
  - If Δ(t1,t2) is positive but Accuracy@t2 is low, likely reward shaping is too aggressive (check correction rate vs incorrect-to-correct rate)
  - If training diverges, likely β2 or β1 are mis-tuned (monitor KL divergence values)
- First 3 experiments:
  1. Run Stage I with varying β2 values and measure edit distance ratios between first and second attempts on training data
  2. Run Stage II with different α values and evaluate Δ(t1,t2) on a fixed validation set of first attempts
  3. Compare on-policy vs offline SFT performance by training identical models with both approaches and measuring distribution shift via edit distance analysis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SCoRe's performance scale when trained for more than two attempts at self-correction?
- Basis in paper: [explicit] The paper states "We did not train SCoRe for more than one round of iterative self-correction due to infrastructural reasons, which means that subsequent rounds may not be as effective as the first."
- Why unresolved: The paper only evaluates SCoRe for two-turn self-correction, leaving the performance on multiple correction rounds unexplored.
- What evidence would resolve it: Experimental results showing SCoRe's performance across 3+ correction attempts on the same problem set.

### Open Question 2
- Question: Can Stages I and II of SCoRe be effectively unified into a single training stage?
- Basis in paper: [explicit] The paper mentions "Unifying Stages I and II would also be interesting, since it would alleviate the limitation of running multiple runs."
- Why unresolved: The paper uses a two-stage approach but doesn't explore whether this separation is necessary or optimal.
- What evidence would resolve it: Comparative experiments showing performance of a unified single-stage approach versus the current two-stage method.

### Open Question 3
- Question: How does SCoRe's reward shaping term (α) affect the trade-off between first-attempt accuracy and self-correction capability?
- Basis in paper: [explicit] The paper states "We use α = 10" but doesn't explore how varying this hyperparameter affects the learning dynamics.
- Why unresolved: The paper fixes α at 10 without examining sensitivity to this hyperparameter or its impact on the learning process.
- What evidence would resolve it: Experiments varying α across a range of values (e.g., 1, 5, 10, 20) and measuring resulting accuracy@t1, accuracy@t2, and Δ(t1,t2).

## Limitations

- Method requires two-stage training with carefully tuned hyperparameters (β2 and α), increasing complexity
- Evaluation focuses primarily on mathematical reasoning and code generation tasks, limiting generalizability assessment
- Requires substantial computational resources for multi-turn RL training process
- Does not explore performance beyond two-turn self-correction due to infrastructural constraints

## Confidence

- **High Confidence**: The experimental results showing SCoRe's improvements over base models and offline SFT baselines on MATH and HumanEval tasks. The ablation studies demonstrating the importance of both stages and the reward shaping term are well-supported.
- **Medium Confidence**: The claim that the two-stage approach prevents behavior collapse compared to naive multi-turn RL. While supported by theoretical arguments and empirical results, the exact conditions under which behavior collapse occurs versus when the method succeeds are not fully characterized.
- **Medium Confidence**: The assertion that reward shaping specifically encourages genuine self-correction rather than superficial edits. The shaping mechanism is intuitive and supported by results, but a more detailed analysis of what types of corrections are being learned would strengthen this claim.

## Next Checks

1. **Behavior Collapse Analysis**: Systematically vary β2 in Stage I across a wider range and track the edit distance ratios between first and second attempts over training to identify the precise boundary conditions for behavior collapse.

2. **Shaping Effectiveness**: Conduct a detailed error analysis categorizing the types of corrections made (e.g., conceptual errors vs. arithmetic errors) and measure whether reward shaping disproportionately affects certain error types, potentially revealing limitations in what can be self-corrected.

3. **Generalization Testing**: Apply SCoRe to additional domains beyond MATH and HumanEval, such as commonsense reasoning or factual question answering, to evaluate the method's robustness across different types of self-correction tasks.