---
ver: rpa2
title: Pixel-Aligned Multi-View Generation with Depth Guided Decoder
arxiv_id: '2408.14016'
source_url: https://arxiv.org/abs/2408.14016
tags:
- multi-view
- depth
- attention
- diffusion
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of pixel-level misalignment in
  multi-view image generation from a single image, which impacts downstream 3D reconstruction
  quality. The authors propose improving the VAE decoder of a latent video diffusion
  model by introducing depth-truncated epipolar attention, which aggregates cross-view
  features at higher resolutions while focusing on spatially adjacent regions using
  depth information.
---

# Pixel-Aligned Multi-View Generation with Depth Guided Decoder

## Quick Facts
- arXiv ID: 2408.14016
- Source URL: https://arxiv.org/abs/2408.14016
- Authors: Zhenggang Tang; Peiye Zhuang; Chaoyang Wang; Aliaksandr Siarohin; Yash Kant; Alexander Schwing; Sergey Tulyakov; Hsin-Ying Lee
- Reference count: 40
- Primary result: Improves pixel alignment in multi-view image generation from single images using depth-truncated epipolar attention, achieving better PSNR, SSIM, LPIPS metrics and 3D reconstruction quality

## Executive Summary
This paper addresses pixel-level misalignment in multi-view image generation from a single image, which impacts downstream 3D reconstruction quality. The authors propose improving the VAE decoder of a latent video diffusion model by introducing depth-truncated epipolar attention, which aggregates cross-view features at higher resolutions while focusing on spatially adjacent regions using depth information. To handle the lack of ground-truth depth during inference and imperfect latents, they also introduce structured-noise depth augmentation during training. Their method achieves better pixel alignment across views compared to baselines like Wonder3D and SyncDreamer, with quantitative improvements in PSNR, SSIM, and LPIPS metrics, and demonstrates superior 3D reconstruction quality when combined with NeuS.

## Method Summary
The method improves the SVD decoder by incorporating depth-truncated epipolar attention in its Up-blocks and conditioning on the front view input. The depth-truncated epipolar attention samples points near the 3D location of each pixel along the epipolar line instead of sampling all points, reducing irrelevant information while allowing attention at higher resolutions. Structured-noise depth augmentation adds both high-frequency and low-frequency noise to ground-truth depth maps during training to create a distribution matching predicted depth noise characteristics. The model is fine-tuned from the SVD decoder for 3k iterations with batch size 1 on 8 H100 GPUs.

## Key Results
- Achieves better pixel alignment across views compared to baselines like Wonder3D and SyncDreamer
- Quantitative improvements in PSNR, SSIM, and LPIPS metrics for generated multi-view images
- Demonstrates superior 3D reconstruction quality when combined with NeuS
- Effective on datasets including a subset of Objaverse (~23k objects) and Google Scanned Objects (30 objects)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Depth-truncated epipolar attention improves pixel alignment by aggregating cross-view features at higher resolutions while focusing only on spatially adjacent regions.
- Mechanism: The method samples points near the 3D location of each pixel along the epipolar line instead of sampling all points, which reduces irrelevant information and allows attention to be applied at higher resolutions.
- Core assumption: Sampling fewer points around the correct 3D location will reduce noise and improve focus on critical regions without losing important alignment information.
- Evidence anchors:
  - [abstract] "enabling the model to focus on spatially adjacent regions while remaining memory efficient"
  - [section 3.2] "we only sample points around the regions of interest... This approach not only aggregates multi-view information at higher resolutions, but also further improves the quality by enabling the model to focus on crucial regions."
  - [corpus] Weak evidence - no directly comparable mechanisms found in neighbors
- Break condition: If the depth estimation is too inaccurate, sampling around incorrect 3D locations could introduce more misalignment than using full epipolar lines.

### Mechanism 2
- Claim: Structured-noise depth augmentation mitigates the domain gap between ground-truth depth used during training and predicted depth used during inference.
- Mechanism: The method adds both high-frequency and low-frequency noise to ground-truth depth maps during training, creating a distribution that better matches the noise characteristics of predicted depths.
- Core assumption: Adding structured noise that preserves both high and low frequency components will make the model more robust to depth estimation errors during inference.
- Evidence anchors:
  - [abstract] "we perturb depth inputs during training" and "structured-noise depth augmentation strategy"
  - [section 3.3] "We formulate the process as follows: Zi ~ U(-si, si) i×i, i ∈ {3, 64, 128}" and "The noisy depths D' now contain both high and low frequency noise"
  - [section 3.4] "Note that the depth map will be pooled to different resolution for different hierarchies of Up-blocks"
- Break condition: If the noise scale is too high or the noise distribution doesn't match real depth estimation errors, the model may learn to ignore depth information entirely.

### Mechanism 3
- Claim: Conditioning the decoder on the front view input improves texture fidelity in generated multi-view images.
- Mechanism: The decoder takes the front view image as additional input alongside the latents, providing high-resolution texture information that guides the generation process.
- Core assumption: The front view contains high-quality texture information that can be propagated to other views through the decoder's attention mechanisms.
- Evidence anchors:
  - [section 3.1] "Moreover, we find that additionally conditioning the decoder on the available front view improves the results"
  - [section 3.4] "Besides low resolution latents, we also condition our model on the front view of the input resolution, which is necessary to maintain detailed texture"
  - [corpus] Weak evidence - no directly comparable mechanisms found in neighbors
- Break condition: If the front view is occluded or contains artifacts, conditioning on it could propagate errors to all generated views.

## Foundational Learning

- Concept: Epipolar geometry and how points project between views
  - Why needed here: The depth-truncated epipolar attention mechanism relies on understanding which pixels in different views correspond to the same 3D point
  - Quick check question: Given two camera views with known intrinsics and relative pose, how would you find the epipolar line for a point in one view on the other view's image plane?

- Concept: Variational Autoencoders and latent diffusion models
  - Why needed here: The method modifies the VAE decoder of a latent video diffusion model, so understanding how latents are encoded and decoded is crucial
  - Quick check question: In a standard VAE, what is the relationship between the encoder output (mean and variance) and the latent vector used for decoding?

- Concept: Depth estimation and its uncertainty
  - Why needed here: The method requires depth information for the attention mechanism and uses predicted depth during inference
  - Quick check question: What are the main sources of error in depth estimation from multi-view images, and how do these errors typically manifest (e.g., smooth surfaces, textureless regions)?

## Architecture Onboarding

- Component map: Input image → VAE encoder → diffusion model → latents → depth-truncated epipolar attention → Up-blocks → VAE decoder → output images
- Critical path: Input image → VAE encoder → diffusion model → latents → depth-truncated epipolar attention → Up-blocks → VAE decoder → output images
- Design tradeoffs:
  - Resolution vs. memory: Depth-truncated attention allows higher resolution processing but requires depth information
  - Noise scale vs. robustness: Too little noise doesn't help with inference depth errors; too much noise makes learning difficult
  - Number of sampled points vs. accuracy: More points give better coverage but increase computation and memory
- Failure signatures:
  - Blurry textures across views: Indicates attention mechanism not properly aggregating features
  - Inconsistent colors between views: Suggests depth estimation errors or insufficient noise augmentation
  - Artifacts at object boundaries: Could indicate incorrect depth values or sampling range issues
- First 3 experiments:
  1. Ablation study: Compare full epipolar attention vs. depth-truncated attention on a simple synthetic dataset with known ground truth
  2. Noise sensitivity: Test model performance with varying noise scales in structured-depth augmentation to find optimal range
  3. Depth quality impact: Evaluate how different depth estimation methods (ground truth vs. predicted) affect final image quality

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the content, several important questions remain unresolved:

1. How does the proposed depth-truncated epipolar attention mechanism compare to alternative cross-view attention approaches (e.g., transformer-based methods) in terms of both quality and computational efficiency?
2. How does the quality of the generated multi-view images degrade as the input image becomes more complex (e.g., highly textured, occluded, or with multiple objects)?
3. Can the proposed method be extended to handle dynamic scenes or videos, where the object's appearance and geometry change over time?

## Limitations

- The method relies heavily on accurate depth estimation, which itself is a challenging problem, particularly for textureless or reflective surfaces
- The structured-noise depth augmentation assumes that the noise characteristics during training will generalize to real inference scenarios, which may not always hold
- The evaluation is primarily on synthetic datasets with controlled camera poses, and performance on real-world images with varying viewpoints remains to be seen

## Confidence

- **High Confidence**: The quantitative improvements in PSNR, SSIM, and LPIPS metrics over baselines are well-supported by the experimental results
- **Medium Confidence**: The qualitative improvements in pixel alignment and the downstream 3D reconstruction benefits are demonstrated but could be more thoroughly validated with additional datasets and ablation studies
- **Low Confidence**: The claim that the method can handle arbitrary camera poses during inference is not explicitly validated, as the experiments focus on a fixed set of views

## Next Checks

1. **Ablation Study on Depth Quality**: Evaluate the model's performance using ground-truth depth versus predicted depth from different estimation methods to quantify the impact of depth accuracy on final image quality
2. **Generalization to Real-World Data**: Test the model on real-world multi-view datasets (e.g., CO3D) with varying camera poses to assess robustness beyond the synthetic setup
3. **Camera Pose Variation**: Validate the method's ability to generate high-quality multi-view images for arbitrary camera poses, not just the fixed set of views used in the current experiments