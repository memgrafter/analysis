---
ver: rpa2
title: An incremental preference elicitation-based approach to learning potentially
  non-monotonic preferences in multi-criteria sorting
arxiv_id: '2409.02760'
source_url: https://arxiv.org/abs/2409.02760
tags:
- preference
- decision
- information
- each
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an incremental preference elicitation-based
  approach to learning potentially non-monotonic preferences in multi-criteria sorting
  problems. The core method involves developing a max-margin optimization model to
  handle inconsistent assignment example preference information and potentially non-monotonic
  preferences.
---

# An incremental preference elicitation-based approach to learning potentially non-monotonic preferences in multi-criteria sorting

## Quick Facts
- arXiv ID: 2409.02760
- Source URL: https://arxiv.org/abs/2409.02760
- Reference count: 17
- Primary result: Information amount measurement methods and question selection strategies outperform benchmark strategies in accuracy and cost saving rate

## Executive Summary
This paper proposes an incremental preference elicitation-based approach for learning potentially non-monotonic preferences in multi-criteria sorting problems. The core method combines a max-margin optimization model with uncertainty sampling to handle inconsistent assignment example preference information and identify the most informative alternatives for querying. Two algorithms with different termination criteria are developed, and the approach is validated through computational experiments on artificial and real-world datasets, demonstrating superior performance compared to benchmark strategies.

## Method Summary
The approach involves constructing a max-margin optimization model to handle inconsistent assignment example preference information and potentially non-monotonic preferences, then devising information amount measurement methods and question selection strategies to identify the most informative alternative in each iteration. Two incremental preference elicitation-based algorithms are developed considering different termination criteria. The approach is applied to a credit rating problem and validated through computational experiments on artificial and real-world datasets.

## Key Results
- Information amount measurement methods and question selection strategies effectively identify the most informative alternatives
- The proposed approach outperforms benchmark strategies in terms of accuracy and cost saving rate
- The approach successfully handles potentially non-monotonic preferences and inconsistent assignment example preference information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The max-margin optimization-based model effectively handles both inconsistent assignment example preference information and potentially non-monotonic preferences by introducing auxiliary variables and optimizing for discriminative power while minimizing inconsistencies.
- Mechanism: The model introduces auxiliary variables δ⁺i and δ⁻i to tolerate inconsistencies in the assignment example preference information. By optimizing the objective function to maximize discriminative power (measured by ε) while minimizing inconsistencies (sum of δ⁺i and δ⁻i), the model finds a solution that balances model performance with tolerance for inconsistencies.
- Core assumption: The extent of inconsistency can be quantified as the sum of auxiliary variables, and this sum provides a meaningful measure of model quality.
- Evidence anchors:
  - [abstract]: "Specifically, we first construct a max-margin optimization-based model to model potentially non-monotonic preferences and inconsistent assignment example preference information in each iteration of the incremental preference elicitation process."
  - [section 4.1]: "To overcome the above challenges and motivated by the study of Teso et al. (2016), we propose a max-margin optimization-based model in this subsection, which aims to optimize two objectives. One is to maximize the discriminative power of the model, while the other is to be tolerant of inconsistent assignment example preference information."
- Break condition: If the assumption that inconsistencies can be adequately captured by auxiliary variables fails, the model may not effectively handle inconsistent information.

### Mechanism 2
- Claim: Information amount measurement methods and question selection strategies identify the most informative alternative by leveraging uncertainty sampling in active learning.
- Mechanism: The approach uses the optimal objective function values from the max-margin optimization model to measure information amount for each alternative. Different strategies (entropy-based, least confidence-based, margin of confidence-based) are employed to identify alternatives with the highest uncertainty, under the assumption that these alternatives provide the most information for learning preferences.
- Core assumption: Alternatives with higher uncertainty in their sorting results are more informative for learning preferences.
- Evidence anchors:
  - [abstract]: "Using the optimal objective function value of the max-margin optimization-based model, we devise information amount measurement methods and question selection strategies to pinpoint the most informative alternative in each iteration within the framework of uncertainty sampling in active learning."
  - [section 4.3.1]: "Building upon these points, we can use vi to analyze the probability of the alternative being assigned to each category. This allows us to measure the uncertainty inherent in sorting result for each alternative and subsequently calculate the information amount associated with each alternative."
- Break condition: If the uncertainty sampling assumption doesn't hold (i.e., uncertain alternatives don't provide more information), the question selection strategies may be suboptimal.

### Mechanism 3
- Claim: The complexity controlling optimization model ensures interpretable results by managing the slope change of piecewise linear marginal utility functions.
- Mechanism: After obtaining initial solutions from the max-margin model, a secondary optimization model minimizes the complexity of the marginal utility functions by controlling the slope changes between consecutive subintervals. This prevents overfitting and maintains interpretability of the sorting results.
- Core assumption: Controlling the slope changes in marginal utility functions leads to more interpretable and generalizable models.
- Evidence anchors:
  - [section 4.4]: "The model complexity can be controlled by managing the slope change of the piecewise linear marginal utility functions. The slope change between two consecutive subintervals... can be represented as... Consequently, the objective of the complexity controlling optimization model is formulated as min sum of these slope changes."
  - [section 4.4]: "It is important to note that the model (M-1) does not take into account the model complexity, which may deteriorate interpretability of the sorting results and increase the risk of overfitting."
- Break condition: If controlling slope changes doesn't effectively reduce overfitting or improve interpretability, the complexity control may be unnecessary or counterproductive.

## Foundational Learning

- Concept: Multi-criteria sorting (MCS)
  - Why needed here: The entire approach is designed for MCS problems, where alternatives need to be assigned to ordered categories based on multiple criteria.
  - Quick check question: Can you explain the difference between utility function-based, outranking-based, and distance-based MCS approaches?

- Concept: Active learning and uncertainty sampling
  - Why needed here: The question selection strategies are based on uncertainty sampling, which assumes that the most uncertain alternatives provide the most information for learning.
  - Quick check question: What are the three main metrics used in uncertainty sampling, and how do they differ in measuring uncertainty?

- Concept: Preference elicitation and incremental learning
  - Why needed here: The approach uses incremental preference elicitation, where the decision maker provides information sequentially, and the model improves iteratively.
  - Quick check question: How does incremental elicitation differ from batch elicitation in terms of cognitive load on the decision maker?

## Architecture Onboarding

- Component map:
  - Max-margin optimization model (M-1) -> Complexity controlling model (M-3) -> Information amount measurement -> Question selection strategies -> Incremental elicitation algorithms -> Threshold-based MCS model

- Critical path:
  1. Initialize with reference alternatives and assignment examples
  2. Solve max-margin model to handle inconsistencies and preferences
  3. Calculate information amount for each alternative
  4. Select most informative alternative using chosen strategy
  5. Elicit preference information from decision maker
  6. Check termination criterion
  7. If not terminated, repeat from step 2
  8. If terminated, solve complexity control model and determine final assignments

- Design tradeoffs:
  - Flexibility vs. interpretability: Allowing non-monotonic preferences increases flexibility but may reduce interpretability
  - Cognitive load vs. model performance: Incremental elicitation reduces cognitive load but may require more iterations
  - Complexity vs. accuracy: The complexity control model improves interpretability but may slightly reduce accuracy

- Failure signatures:
  - High inconsistency in assignment examples despite non-monotonic preferences: May indicate that the model isn't effectively handling inconsistencies
  - Slow convergence or many iterations needed: Could suggest suboptimal question selection strategies
  - Overfitting to training data: May indicate need for stronger complexity control

- First 3 experiments:
  1. Test the max-margin model on a simple MCS problem with known inconsistencies to verify it handles them correctly
  2. Compare different question selection strategies on a small dataset to identify which performs best
  3. Evaluate the impact of the complexity control model on interpretability by visualizing marginal utility functions with and without it

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal value of the parameter α in the max-margin optimization-based model for balancing discriminative power and inconsistency tolerance?
- Basis in paper: [explicit] The paper mentions that α ∈ (0, 1) is used to make a trade-off between maximizing discriminative power and minimizing inconsistencies in the model (M-1).
- Why unresolved: The paper suggests that α can be assigned by the decision analyst based on experience or determined based on model performance, but does not provide a definitive method for finding the optimal value.
- What evidence would resolve it: Empirical studies comparing the performance of the proposed approach with different α values on various datasets, identifying the value that consistently yields the best results.

### Open Question 2
- Question: How does the proposed approach perform compared to other preference elicitation methods when dealing with complex MCS problems with many criteria and alternatives?
- Basis in paper: [inferred] The paper mentions that the proposed approach can handle potentially non-monotonic preferences and inconsistent assignment example preference information, which are common challenges in complex MCS problems.
- Why unresolved: The paper only provides computational experiments on artificial and real-world datasets with relatively small numbers of criteria and alternatives, without comparing the proposed approach to other methods on more complex problems.
- What evidence would resolve it: Extensive computational experiments on large-scale MCS problems with many criteria and alternatives, comparing the proposed approach to other state-of-the-art preference elicitation methods in terms of accuracy, cost saving rate, and computational efficiency.

### Open Question 3
- Question: How sensitive is the proposed approach to the initial set of reference alternatives and assignment example preference information provided by the decision maker?
- Basis in paper: [inferred] The paper mentions that the proposed approach starts with an initial set of reference alternatives and incrementally improves the preference model based on the decision maker's answers, but does not discuss the impact of the initial information on the final results.
- Why unresolved: The paper does not provide any analysis on how different initial sets of reference alternatives and assignment example preference information affect the accuracy and efficiency of the proposed approach.
- What evidence would resolve it: Sensitivity analysis experiments varying the initial set of reference alternatives and assignment example preference information, measuring the impact on the accuracy, cost saving rate, and convergence speed of the proposed approach.

## Limitations
- The handling of inconsistent assignment example preference information relies heavily on auxiliary variables, but the paper doesn't provide empirical validation that these variables effectively capture true inconsistencies versus noise in the data.
- While the complexity controlling model aims to improve interpretability, its impact on overall sorting accuracy remains unclear from the results presented.
- The comparative advantage of the proposed question selection strategies over simpler alternatives needs stronger empirical support, particularly for real-world datasets.

## Confidence

- **High Confidence**: The fundamental framework of combining max-margin optimization with uncertainty sampling for incremental preference elicitation is well-established in the literature.
- **Medium Confidence**: The specific implementation details for handling non-monotonic preferences and the information amount measurement methods are sound but require more empirical validation across diverse problem types.
- **Low Confidence**: The comparative advantage of the proposed question selection strategies over simpler alternatives needs stronger empirical support, particularly for real-world datasets.

## Next Checks

1. **Sensitivity Analysis**: Conduct experiments varying the inconsistency tolerance parameter α to determine how sensitive the model performance is to this hyperparameter across different problem types.
2. **Real-World Application**: Apply the approach to a credit rating problem with actual financial data to validate whether the non-monotonic preference handling provides practical benefits over monotonic alternatives.
3. **Scalability Testing**: Evaluate the computational efficiency and solution quality on larger datasets (n > 100 alternatives, m > 10 criteria) to assess practical applicability for industrial-scale problems.