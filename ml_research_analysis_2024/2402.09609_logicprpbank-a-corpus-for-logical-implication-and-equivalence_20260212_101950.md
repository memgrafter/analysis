---
ver: rpa2
title: 'LogicPrpBank: A Corpus for Logical Implication and Equivalence'
arxiv_id: '2402.09609'
source_url: https://arxiv.org/abs/2402.09609
tags:
- logic
- plss
- propositional
- corpus
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LogicPrpBank, a corpus of 7,093 propositional
  logic statements across six mathematical subjects to study the reasoning of logical
  implication and equivalence. The authors leverage ChatGPT to generate atomic PLSs
  and develop a template-based proposition composer to automatically compose atomic
  to compound PLSs.
---

# LogicPrpBank: A Corpus for Logical Implication and Equivalence

## Quick Facts
- arXiv ID: 2402.09609
- Source URL: https://arxiv.org/abs/2402.09609
- Reference count: 13
- Primary result: Small-scale LMs outperform large-scale LMs on propositional logic reasoning tasks

## Executive Summary
This paper introduces LogicPrpBank, a corpus of 7,093 propositional logic statements across six mathematical subjects to study logical implication and equivalence reasoning. The authors leverage ChatGPT to generate atomic PLSs and develop a template-based proposition composer to automatically compose atomic to compound PLSs. Experiments show that small-scale LMs like BERT and RoBERTa perform well on propositional logic reasoning, while large-scale LMs like Llama2-7B do not perform as well. The results challenge the assumption that increasing LM size necessarily leads to better performance in propositional logic reasoning.

## Method Summary
The authors created LogicPrpBank using a two-step process: first, ChatGPT generated atomic propositional logic statements (PLSs) which were then verified by human annotators; second, a template-based proposition composer automatically combined atomic PLSs into compound implication and equivalence statements. The corpus was benchmarked using both fine-tuning approaches with small-scale LMs (BERT, RoBERTa, DistilRoBERTa) and few-shot learning with large-scale LMs (Llama2-7B, BLOOM-560m, GPT-2-medium), with evaluation based on macro F1 scores for classifying statement correctness.

## Key Results
- Small-scale LMs (BERT, RoBERTa) outperform large-scale LMs on propositional logic reasoning across most subjects
- Performance varies significantly by mathematical subject, with arithmetic and number theory being particularly challenging
- The template-based composition approach successfully generated 5,816 compound PLSs from 1,277 atomic statements
- Increasing LM size does not necessarily improve propositional logic reasoning performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using ChatGPT to generate atomic PLSs followed by human verification yields a high-quality corpus with reasonable annotation efficiency.
- Mechanism: ChatGPT's training on internet text covering educational materials allows it to generate mathematically sound atomic PLSs, while human verification catches errors and ensures correctness.
- Core assumption: ChatGPT's training data includes sufficient propositional logic content to generate accurate atomic statements across multiple mathematical subjects.
- Evidence anchors:
  - [section] "We use ChatGPT API 2 to generate atomic PLSs... ChatGPT is trained with vast amounts of data from the internet written by humans, which covers propositional logic lectures across educational and tutoring webpages, thus it is able to generate a large number of high-quality PLSs."
  - [section] "To validate the correctness (true/false) of ChatGPT-generated PLSs, we employ qualified human annotators who pass a qualification test to check the ChatGPT-generated PLSs."
  - [corpus] Corpus contains 7,093 PLSs across six mathematical subjects, indicating substantial coverage and diversity.
- Break condition: If ChatGPT's training data lacks sufficient propositional logic content or if human verification is inconsistent, the corpus quality would degrade significantly.

### Mechanism 2
- Claim: Template-based proposition composition effectively generates compound implication and equivalence PLSs from atomic statements.
- Mechanism: Predefined templates systematically combine two atomic PLSs into compound statements, with truth values derived from propositional logic truth tables.
- Core assumption: The templates cover sufficient logical variations to create meaningful compound statements that test different reasoning capabilities.
- Evidence anchors:
  - [section] "To generate implication and equivalence PLSs, we develop a template-based proposition composer with curated templates to automatically compose two atomic PLSs into one compound PLS."
  - [section] "An implication composer uses a set of templates: (1) if [P] then [Q]; (2) [P] implies [Q]; (3) [P], therefore, [Q]."
  - [corpus] 5,816 compound PLSs were generated from 1,277 atomic PLSs, demonstrating the scalability of this approach.
- Break condition: If templates are too limited or don't capture the complexity of propositional logic reasoning, the compound statements would fail to adequately test LM capabilities.

### Mechanism 3
- Claim: Small-scale LMs outperform large-scale LMs on propositional logic reasoning due to task-specific training dynamics.
- Mechanism: Small LMs trained on this corpus learn the formal logical rules effectively, while large LMs may overfit to natural language patterns that don't transfer well to formal logic.
- Core assumption: The corpus provides sufficient training signal for small LMs to learn propositional logic reasoning without requiring the vast parameter capacity of large LMs.
- Evidence anchors:
  - [abstract] "Experiments with various LMs show that small-scale LMs can effectively reason complex propositional logic in most subjects, while large-scale LMs like Llama2-7B do not perform as well."
  - [section] "BERT and RoBERTa (LMs) have good performance but medium-scale LMs (BLOOM-560m and GPT-2-medium) and LLMs have poor performance."
  - [section] "Although large-scale LMs are supposed to have better performance and sample efficiency in many downstream NLP tasks, they do not hold true in reasoning with propositional logic."
- Break condition: If the corpus were significantly larger or if large LMs were specifically fine-tuned for formal logic reasoning, this performance pattern might reverse.

## Foundational Learning

- Concept: Propositional logic truth tables and evaluation rules
  - Why needed here: Understanding how implication (P → Q) and equivalence (P ↔ Q) truth values are determined is fundamental to generating and evaluating the corpus
  - Quick check question: Given P is false and Q is true, what are the truth values of P → Q and P ↔ Q?

- Concept: Natural language processing and language model architectures
  - Why needed here: The work involves benchmarking different LM architectures (BERT, RoBERTa, Llama2, etc.) on logical reasoning tasks
  - Quick check question: What is the key architectural difference between BERT's bidirectional context modeling and GPT's autoregressive approach?

- Concept: Corpus construction and annotation methodologies
  - Why needed here: The paper describes a novel approach to corpus generation using AI assistance followed by human verification
  - Quick check question: What are the advantages and disadvantages of using AI-generated data with human verification versus fully human-annotated data?

## Architecture Onboarding

- Component map: ChatGPT atomic PLS generation → human verification → template-based composition → LM fine-tuning/few-shot → performance evaluation
- Critical path: Generating atomic PLSs → verifying truth values → composing compound statements → training/fine-tuning LMs → evaluating on test set
- Design tradeoffs: Using ChatGPT for corpus generation saves time but introduces potential quality issues; small LMs are faster to train but may lack generalization; few-shot learning avoids fine-tuning but provides less control
- Failure signatures: Low inter-annotator agreement on truth values; poor LM performance across all subjects; template composition producing nonsensical compound statements
- First 3 experiments:
  1. Verify the inter-annotator agreement on a random sample of atomic PLSs to assess annotation quality
  2. Test the proposition composer with a small set of atomic statements to ensure compound statements are logically valid
  3. Fine-tune a small LM (e.g., DistilRoBERTa) on a subset of the corpus and evaluate on a held-out validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do small-scale LMs like BERT and RoBERTa perform better than large-scale LMs like Llama2-7B on propositional logic reasoning tasks?
- Basis in paper: [explicit] The paper states that "BERT and RoBERTa (LMs) have good performance but medium-scale LMs (BLOOM-560m and GPT-2-medium) and LLMs have poor performance."
- Why unresolved: The paper suggests that propositional logic is a formal language distinct from natural languages, but it does not provide a definitive explanation for why larger LMs perform worse.
- What evidence would resolve it: Comparative studies on the training data and methodologies of different LMs, focusing on their exposure to formal language logic, could provide insights.

### Open Question 2
- Question: How does the performance of LMs on propositional logic tasks vary with different mathematical subjects?
- Basis in paper: [explicit] The paper notes that "LMs struggle with reasoning arithmetic and number theory but promising in calculus, geometry, and statistics."
- Why unresolved: While the paper observes performance differences across subjects, it does not explore the underlying reasons or provide a detailed analysis of subject-specific challenges.
- What evidence would resolve it: Detailed error analysis and subject-specific benchmarks could elucidate the strengths and weaknesses of LMs in handling different types of propositional logic problems.

### Open Question 3
- Question: What are the limitations of using ChatGPT-generated data for constructing corpora in specialized domains like propositional logic?
- Basis in paper: [explicit] The paper mentions that "Our corpus was collected from ChatGPT and then verified and/or annotated by a qualified annotator, thus there could be annotating errors."
- Why unresolved: The paper acknowledges potential errors but does not thoroughly investigate the impact of these errors on the corpus's reliability or explore alternative methods for corpus construction.
- What evidence would resolve it: Comparative studies on the accuracy and reliability of ChatGPT-generated data versus human-annotated data in similar domains could provide clarity.

## Limitations

- Corpus generation relies heavily on ChatGPT, which may introduce systematic biases based on the model's training data distribution
- Template-based composition may not capture the full complexity of logical reasoning across all mathematical domains
- Evaluation focuses primarily on macro F1 scores without deeper analysis of error patterns or model calibration

## Confidence

- **High Confidence**: The corpus construction methodology and its basic validation through human verification
- **Medium Confidence**: The performance comparison between small and large LMs
- **Low Confidence**: The generalizability of findings to other logical reasoning tasks beyond propositional logic

## Next Checks

1. Conduct a detailed inter-annotator agreement analysis by having multiple annotators independently verify a stratified sample of atomic PLSs across all six mathematical subjects, calculating Cohen's kappa to establish annotation quality benchmarks.

2. Perform a systematic error pattern analysis on LM predictions, categorizing failures by subject domain, statement complexity, and logical structure to identify specific reasoning weaknesses that aggregate metrics might obscure.

3. Test the best-performing LMs from this study on related logical reasoning tasks (such as first-order logic or syllogistic reasoning) to assess whether the observed performance patterns extend beyond propositional logic.