---
ver: rpa2
title: 'From Data to Commonsense Reasoning: The Use of Large Language Models for Explainable
  AI'
arxiv_id: '2407.03778'
source_url: https://arxiv.org/abs/2407.03778
tags:
- reasoning
- gpt-3
- commonsense
- llms
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates the effectiveness of large language models\
  \ (LLMs) on commonsense reasoning and explainability. The authors evaluate three\
  \ LLMs\u2014GPT-3.5, Gemma, and Llama 3\u2014on eleven different question-answering\
  \ datasets that require commonsense reasoning."
---

# From Data to Commonsense Reasoning: The Use of Large Language Models for Explainable AI

## Quick Facts
- arXiv ID: 2407.03778
- Source URL: https://arxiv.org/abs/2407.03778
- Authors: Stefanie Krause; Frieder Stolzenburg
- Reference count: 40
- LLMs achieve near-human-level performance on commonsense reasoning tasks

## Executive Summary
This study evaluates three large language models—GPT-3.5, Gemma, and Llama 3—on eleven commonsense reasoning question-answering datasets. Llama 3 achieved the highest accuracy at 90% mean across all datasets, outperforming both GPT-3.5 (73.33%) and Gemma (70.91%). Notably, Llama 3 surpassed human performance on all datasets with an average 21% higher accuracy. The authors also assessed GPT-3.5's explanation quality through a questionnaire, finding that 66% of participants rated the explanations as "good" or "excellent." The results demonstrate that LLMs can handle commonsense reasoning tasks with near-human-level performance while providing helpful explanations that enhance AI decision transparency.

## Method Summary
The study evaluates three LLMs (GPT-3.5, Gemma, and Llama 3) on 11 benchmark datasets for commonsense reasoning by selecting 30 random examples from each dataset. The models are assessed on their accuracy in answering questions, with Llama 3 achieving the highest performance. Additionally, the authors conduct a questionnaire to evaluate the quality of GPT-3.5's explanations for a subset of questions. Human accuracy is compared to LLM accuracy to determine the relative performance of models versus human reasoning capabilities.

## Key Results
- Llama 3 achieved 90% mean accuracy across 11 datasets, outperforming GPT-3.5 (73.33%) and Gemma (70.91%)
- Llama 3 outperformed humans on all datasets with an average 21% higher accuracy
- 66% of participants rated GPT-3.5's explanations as either "good" or "excellent"

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs outperform humans on commonsense reasoning QA tasks due to implicit probabilistic knowledge learned from training data.
- Mechanism: The LLM encodes world knowledge through co-occurrence patterns, enabling zero-shot reasoning without explicit logic rules.
- Core assumption: Commonsense reasoning can be approximated by learned language statistics rather than formal axioms.
- Evidence anchors:
  - [abstract] "LLMs are capable of handling commonsense reasoning tasks with near-human-level performance"
  - [section] "Llama 3 achieved the highest accuracy, with a mean of 90% across all datasets, outperforming both GPT-3.5 (mean 73.33%) and Gemma (mean 70.91%)"
- Break condition: When questions require knowledge not present in training data or involve novel combinations outside learned co-occurrence patterns.

### Mechanism 2
- Claim: LLM-generated explanations provide intuitive understanding by mimicking human reasoning style.
- Mechanism: LLMs produce explanations in natural language that align with human cognitive patterns, making decisions transparent.
- Core assumption: Human-readable text explanations are sufficient for users to understand AI decisions (a posteriori explainability).
- Evidence anchors:
  - [abstract] "66% of participants rated the explanations as either 'good' or 'excellent'"
  - [section] "Our questionnaire revealed that 66% of participants rated GPT-3.5's explanations as either 'good' or 'excellent'"
- Break condition: When explanations are too generic or when the underlying reasoning process is too complex to capture in natural language.

### Mechanism 3
- Claim: Zero-shot performance advantage stems from instruction-tuning and scale rather than explicit reasoning capabilities.
- Mechanism: Larger parameter count and instruction tuning enable better pattern matching and task adaptation without formal reasoning.
- Core assumption: Scale and fine-tuning can substitute for explicit reasoning mechanisms.
- Evidence anchors:
  - [section] "Llama 3 achieved a mean accuracy of 90% on all eleven datasets. Thereby Llama 3 is outperforming humans on all datasets"
  - [section] "Llama 3... 70 billion parameters" vs "Gemma... 7 billion parameters"
- Break condition: When tasks require explicit logical manipulation that pattern matching cannot handle.

## Foundational Learning

- Concept: Question answering benchmarks for commonsense reasoning
  - Why needed here: Provides standardized evaluation framework for measuring LLM performance
  - Quick check question: What distinguishes CODAH from CosmosQA in terms of commonsense reasoning requirements?

- Concept: Explainability measurement via human evaluation
  - Why needed here: Quantifies whether LLM explanations are actually helpful to users
  - Quick check question: Why might Likert scale ratings be insufficient for measuring explanation quality?

- Concept: Zero-shot vs few-shot learning in LLMs
  - Why needed here: Determines whether explicit prompting strategies are needed for optimal performance
  - Quick check question: How might chain-of-thought prompting affect commonsense reasoning accuracy?

## Architecture Onboarding

- Component map: Question → LLM model → Answer selection → Explanation generation → Human evaluation interface
- Critical path: Input question → LLM inference → Answer selection → Explanation generation → User feedback collection
- Design tradeoffs: Larger models (Llama 3) offer better accuracy but require more computational resources; smaller models (Gemma) are more deployable but less accurate
- Failure signatures: Invalid responses (asking for more context), incorrect answers on medical/science domains, poor performance on semantic relations
- First 3 experiments:
  1. Replicate baseline accuracy measurements across all 11 datasets using same random sample
  2. Test few-shot prompting strategies on CommonsenseQA to see if accuracy improves
  3. Compare explanation quality between GPT-3.5 and Llama 3 using same questionnaire protocol

## Open Questions the Paper Calls Out
None

## Limitations
- Human baseline comparison lacks transparency in methodology and the evaluation sample size is limited
- Explanation quality assessment is limited to Likert scale ratings, which may not capture depth or completeness needed for true explainability
- The study uses only 30 random examples per dataset, which may not capture the full difficulty spectrum of these benchmarks

## Confidence

- **Medium Confidence**: The claim that LLMs can handle commonsense reasoning with near-human-level performance
- **Medium Confidence**: The finding that 66% of participants rated GPT-3.5's explanations as "good" or "excellent"
- **High Confidence**: The comparative performance ranking of the three models (Llama 3 > GPT-3.5 > Gemma)

## Next Checks

1. Conduct a controlled experiment comparing human performance against LLM performance on the same 30-question samples from each dataset, ensuring identical evaluation criteria and question interpretation to validate the claimed 21% accuracy advantage.

2. Evaluate the effect of few-shot prompting and chain-of-thought strategies on CommonsenseQA and CosmosQA datasets to determine if the reported accuracy gaps between models could be reduced through better prompting techniques.

3. Implement a structured rubric-based evaluation of explanations beyond Likert scale ratings, analyzing explanation completeness, logical coherence, and coverage of key reasoning steps for a subset of questions where LLMs made correct predictions.