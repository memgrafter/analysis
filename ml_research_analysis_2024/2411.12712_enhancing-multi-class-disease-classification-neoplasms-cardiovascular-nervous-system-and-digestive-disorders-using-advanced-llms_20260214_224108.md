---
ver: rpa2
title: 'Enhancing Multi-Class Disease Classification: Neoplasms, Cardiovascular, Nervous
  System, and Digestive Disorders Using Advanced LLMs'
arxiv_id: '2411.12712'
source_url: https://arxiv.org/abs/2411.12712
tags:
- medical
- classification
- training
- accuracy
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates advanced language models for multi-class\
  \ disease classification using the Medical-Abstracts-TC-Corpus. The research compares\
  \ four models\u2014BioBERT, XLNet, BERT, and a custom lightweight model called LastBERT\u2014\
  across four disease categories."
---

# Enhancing Multi-Class Disease Classification: Neoplasms, Cardiovascular, Nervous System, and Digestive Disorders Using Advanced LLMs

## Quick Facts
- arXiv ID: 2411.12712
- Source URL: https://arxiv.org/abs/2411.12712
- Reference count: 13
- Primary result: BioBERT achieved 97% accuracy in multi-class disease classification on Medical-Abstracts-TC-Corpus

## Executive Summary
This study evaluates advanced language models for multi-class disease classification across four disease categories: Neoplasms, Cardiovascular, Nervous System, and Digestive Disorders. Four models were compared: BioBERT (pre-trained on medical data), XLNet (general-purpose), BERT (general-purpose), and LastBERT (custom lightweight model). BioBERT achieved the highest accuracy at 97%, while XLNet reached 96% despite lacking medical pre-training. The custom LastBERT model, with only 29M parameters, achieved 87.10% accuracy, demonstrating competitive performance with reduced computational requirements. The findings highlight the importance of domain-specific pre-training and suggest that well-optimized smaller models can achieve strong results in medical text classification tasks.

## Method Summary
The study used the Medical-Abstracts-TC-Corpus containing 14,438 records across four disease categories, split 80-20 for training/testing after excluding general pathological conditions. Four models were evaluated: BioBERT (110M parameters, medical pre-training), XLNet (110M parameters), BERT (110M parameters), and LastBERT (29M parameters, lighter BERT variant). The models were fine-tuned on the dataset with class balancing through up/down-sampling techniques. Training was performed for 10 epochs with batch size 16, warmup steps 100, weight decay 0.1, and early stopping with patience 2-3. Performance was evaluated using accuracy, precision, recall, and F1-score metrics.

## Key Results
- BioBERT achieved the highest accuracy at 97% due to medical domain pre-training
- XLNet reached 96% accuracy, demonstrating strong generalizability despite not being pre-trained on medical data
- LastBERT achieved 87.10% accuracy with only 29M parameters, showing competitive performance with reduced computational requirements
- The lightweight LastBERT model performed close to BERT (89.33%) while using significantly fewer parameters (29M vs 110M)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BioBERT achieves superior performance because it was pre-trained on medical corpora, giving it domain-specific semantic knowledge.
- Mechanism: Domain-specific pre-training embeds medical terminology and contextual usage patterns into the model, allowing better generalization to biomedical text classification tasks.
- Core assumption: Medical abstracts share consistent terminology and structure that general pre-training (e.g., BERT) does not capture.
- Evidence anchors:
  - [abstract]: "BioBERT, which was pre-trained on medical data, demonstrated superior performance in medical text classification (97% accuracy)."
  - [section]: "BioBERT achieved the highest accuracy at 97%, while XLNet reached 96% despite not being pre-trained on medical data."
- Break condition: If the medical corpus lacks diversity or coverage, pre-training may introduce bias or overfitting, reducing performance on unseen data.

### Mechanism 2
- Claim: XLNet performs competitively without medical pre-training due to its permutation-based autoregressive training, which captures bidirectional context.
- Mechanism: The permutation language modeling objective in XLNet allows the model to learn contextual dependencies more flexibly than standard BERT, making it robust across domains.
- Core assumption: General-purpose language understanding is transferable to specialized domains with fine-tuning.
- Evidence anchors:
  - [abstract]: "Surprisingly, XLNet followed closely (96% accuracy), demonstrating its generalizability across domains even though it was not pre-trained on medical data."
  - [section]: "Although it is the smaller model, lastBERT produced respectable results with only 29M compared to all the others."
- Break condition: If the target domain contains highly specialized vocabulary or structure not present in general training, the benefit diminishes without domain-specific pre-training.

### Mechanism 3
- Claim: LastBERT's competitive performance despite reduced parameters demonstrates that well-optimized smaller models can be effective with proper architectural tuning.
- Mechanism: Custom architecture design (e.g., reduced hidden size, fewer layers, dropout regularization) balances efficiency and performance, avoiding overfitting while maintaining capacity.
- Core assumption: Medical text classification can be effectively learned with fewer parameters if the architecture is well-matched to task complexity.
- Evidence anchors:
  - [abstract]: "LastBERT, a custom model based on the lighter version of BERT, also proved competitive with 87.10% accuracy (just under BERT's 89.33%)."
  - [section]: "The model achieved an accuracy of 87.06 %, which is very comparable to the BERT performance at 89.32% but with only 29M parameters vs the 110M in the case of BERT."
- Break condition: If the dataset grows significantly or task complexity increases, reduced capacity may become a bottleneck, requiring model scaling.

## Foundational Learning

- Concept: Tokenization and padding/truncation
  - Why needed here: Ensures uniform input length for transformer models and consistent vocabulary mapping.
  - Quick check question: What is the maximum sequence length used for tokenization in this study?

- Concept: Fine-tuning vs. pre-training
  - Why needed here: Distinguishes between training on general language corpora (pre-training) and adapting to specific task data (fine-tuning).
  - Quick check question: Which model in this study was not pre-trained on medical data?

- Concept: Imbalanced dataset handling
  - Why needed here: The dataset has uneven class distribution, requiring techniques like up-sampling/down-sampling to prevent bias.
  - Quick check question: Which two techniques were used to balance the training data?

## Architecture Onboarding

- Component map:
  Tokenizer (BERT/BioBERT/XLNet variants) -> Pre-trained model backbone -> Sequence classification head -> Training pipeline (epochs, batch size, learning rate, early stopping) -> Evaluation metrics (accuracy, F1, precision, recall, confusion matrix, ROC-AUC)

- Critical path:
  Load pre-trained model → Tokenize and batch data → Fine-tune with early stopping → Evaluate on test set

- Design tradeoffs:
  - Model size vs. accuracy: BioBERT (110M) outperforms LastBERT (29M) but requires more resources
  - Pre-training domain vs. flexibility: BioBERT excels on medical text; XLNet generalizes better
  - Training time vs. overfitting: Smaller models train faster but may underfit; early stopping mitigates this

- Failure signatures:
  - High training accuracy but low validation accuracy → overfitting
  - Low accuracy across classes → underfitting or poor tokenization
  - Confusion between similar classes → need better feature extraction or more data

- First 3 experiments:
  1. Compare training loss curves for BioBERT and LastBERT to assess overfitting
  2. Evaluate confusion matrix to identify systematically misclassified classes
  3. Test impact of increasing batch size on convergence speed and final accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do BioBERT's performance metrics change when evaluated on a larger, more diverse medical text corpus that includes additional disease categories beyond the four examined?
- Basis in paper: [explicit] The paper demonstrates BioBERT's superior performance (97% accuracy) on the current Medical-Abstracts-TC-Corpus dataset but only examines four specific disease categories.
- Why unresolved: The study is limited to a specific dataset with four disease categories, making it unclear how the model generalizes to broader medical classification tasks.
- What evidence would resolve it: Performance evaluation of BioBERT on a larger medical corpus with 10+ disease categories and comparison of accuracy, precision, and recall metrics across different medical domains.

### Open Question 2
- Question: What specific architectural modifications to LastBERT could improve its classification accuracy for distinguishing between closely related disease categories like neoplasms and other conditions?
- Basis in paper: [inferred] The confusion matrix shows LastBERT struggles particularly with neoplasms classification and similar disease categories, suggesting architectural limitations.
- Why unresolved: The paper identifies performance gaps but doesn't explore specific architectural improvements or modifications that could address these classification challenges.
- What evidence would resolve it: Comparative analysis of LastBERT variants with different attention mechanisms, layer configurations, or domain-specific pre-training approaches and their impact on classification accuracy.

### Open Question 3
- Question: How does the computational efficiency of LastBERT (29M parameters) compare to BioBERT (110M parameters) when scaled to industrial-sized medical text classification systems processing millions of documents?
- Basis in paper: [explicit] The paper highlights LastBERT's reduced parameter count and satisfactory performance, suggesting potential computational advantages.
- Why unresolved: The study only evaluates model performance on the given dataset without analyzing scalability, processing speed, or resource requirements for large-scale deployment.
- What evidence would resolve it: Benchmarking experiments measuring training/inference time, memory usage, and throughput for both models when processing large-scale medical document datasets.

## Limitations

- Model Architecture Details: The exact parameter configuration for LastBERT remains unspecified, particularly the specific reductions from BERT base configuration (hidden size, attention heads, layers).
- Training Configuration: Key hyperparameters such as learning rate schedules, batch normalization settings, and dropout rates are not provided.
- Dataset Preprocessing: The specific implementation of up-sampling/down-sampling techniques for handling class imbalance is not detailed.

## Confidence

**High Confidence** in domain-specific pre-training benefits (BioBERT 97% accuracy): The superiority of BioBERT over general models is well-supported by substantial accuracy differences and aligns with established findings that medical pre-training improves biomedical text classification performance.

**Medium Confidence** in XLNet's domain transferability (96% accuracy): While XLNet performs competitively without medical pre-training, the mechanism explanation relies on permutation-based training benefits that are theoretically sound but require additional validation on broader domain transfer tasks.

**Medium Confidence** in LastBERT's efficiency claims: The competitive performance of the smaller model is demonstrated, but the specific architectural optimizations enabling this efficiency are not fully specified, making it difficult to assess whether similar results would generalize to other lightweight implementations.

## Next Checks

1. **Architecture ablation study**: Systematically vary LastBERT's parameters (hidden size, layers, attention heads) to identify the minimum viable configuration that maintains acceptable accuracy, validating whether the 29M parameter choice represents true optimization rather than arbitrary reduction.

2. **Cross-domain generalization test**: Evaluate all four models on a separate medical text corpus (e.g., clinical notes or different abstract database) to verify whether BioBERT's advantage persists and whether XLNet's general-purpose training provides genuine transferability.

3. **Imbalance sensitivity analysis**: Repeat experiments with different up-sampling/down-sampling ratios and SMOTE variations to determine whether the observed performance differences between models are robust to preprocessing choices or sensitive to specific balancing techniques.