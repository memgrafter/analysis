---
ver: rpa2
title: Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations
arxiv_id: '2403.18167'
source_url: https://arxiv.org/abs/2403.18167
tags:
- hallucinations
- language
- knowledge
- answer
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates mechanistic causes of non-factual hallucinations
  in language models through interpretability analyses. It identifies two distinct
  mechanisms: knowledge enrichment hallucinations, where lower-layer MLPs lack sufficient
  subject attribute knowledge, and answer extraction hallucinations, where upper-layer
  attention heads fail to select the correct object attribute.'
---

# Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations

## Quick Facts
- arXiv ID: 2403.18167
- Source URL: https://arxiv.org/abs/2403.18167
- Authors: Lei Yu; Meng Cao; Jackie Chi Kit Cheung; Yue Dong
- Reference count: 27
- Primary result: Identifies two distinct hallucination mechanisms and achieves up to 47.6% accuracy on paraphrased questions while preserving over 90% specificity

## Executive Summary
This paper investigates the mechanistic causes of non-factual hallucinations in language models through interpretability analyses. The authors identify two distinct mechanisms: knowledge enrichment hallucinations, where lower-layer MLPs lack sufficient subject attribute knowledge, and answer extraction hallucinations, where upper-layer attention heads fail to select the correct object attribute. They propose a mechanistic hallucination mitigation method (MHM) that restores the model's internal fact recall pipeline through targeted training, achieving significant improvements on two open-domain question answering datasets while preserving general knowledge.

## Method Summary
The authors construct a diagnostic dataset from the ParaRel dataset by selecting N-to-1 relational classes and filtering for capitalized object entities. They apply logit lens and causal mediation analysis to examine semantic information in intermediate hidden representations and identify malfunctioning components. The proposed MHM method targets both MLP enrichment and attention extraction components by injecting information about true answers to MLPs and suppressing incorrect information from self-attention heads. The method is evaluated on Natural Questions and TruthfulQA datasets, comparing against baselines including ICL, SFT, MEND, and DoLa.

## Key Results
- Identifies two distinct hallucination mechanisms: knowledge enrichment (lower-layer MLP failures) and answer extraction (upper-layer attention head failures)
- MHM achieves up to 47.6% accuracy on paraphrased questions while preserving over 90% specificity on original questions
- MHM outperforms baseline methods on both Natural Questions and TruthfulQA datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lower-layer MLPs fail to retrieve sufficient subject attribute knowledge, causing knowledge enrichment hallucinations.
- Mechanism: Early MLP layers should extract relevant attributes about the subject from parametric knowledge. Failure at this enrichment stage prevents the model from distinguishing true answers from plausible but incorrect attributes.
- Core assumption: A two-step pipeline where knowledge enrichment must precede answer extraction, with independent failure modes.
- Evidence anchors: Logit values analysis showing insufficient subject-object ranking in early MLP projections (ρ_s(o) > 0.01|V|).
- Break condition: If early MLPs retrieve sufficient subject information but upper layers still fail to select correct objects.

### Mechanism 2
- Claim: Upper-layer attention heads fail to select the correct object attribute, causing answer extraction hallucinations.
- Mechanism: After successful subject knowledge enrichment, upper-layer self-attention mechanisms must distinguish correct objects from other retrieved attributes. Failure to properly weight and select the most relevant object leads to incorrect but plausible outputs.
- Core assumption: The model has already retrieved sufficient subject knowledge in early layers, with failure occurring specifically at the attention-based selection stage.
- Evidence anchors: Attention-extracted attribute information analysis showing poor discrimination in later layers despite good early-layer performance.
- Break condition: If attention heads successfully distinguish correct objects but the model still outputs incorrect answers.

### Mechanism 3
- Claim: MHM works by restoring the internal fact recall pipeline through targeted training of both MLP enrichment and attention extraction components.
- Mechanism: Simultaneously encourages MLPs to retrieve more information about true answers while suppressing information propagation of incorrect answers from attention heads.
- Core assumption: Improving both enrichment and extraction stages leads to better factuality than addressing only one stage, and the model can be fine-tuned to enhance these specific capabilities without catastrophic forgetting.
- Evidence anchors: External validation showing improved performance on paraphrased questions while maintaining specificity on original questions.
- Break condition: If the model cannot learn to improve both enrichment and extraction simultaneously, or if improving one stage negatively impacts the other.

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The paper analyzes specific transformer components (MLPs and attention heads) and their roles in fact recall, requiring understanding of how these components work together in the inference pipeline.
  - Quick check question: How do multi-head self-attention mechanisms differ from feed-forward MLPs in their role within transformer layers?

- Concept: Interpretability methods (logit lens and causal mediation analysis)
  - Why needed here: The paper uses these techniques to trace information flow and identify which components contribute to hallucinations, requiring understanding of how to interpret intermediate representations and measure causal effects.
  - Quick check question: What is the key difference between logit lens analysis and causal mediation analysis in terms of what they reveal about model behavior?

- Concept: Knowledge representation in language models
  - Why needed here: The paper investigates how factual knowledge is stored and retrieved in LMs, requiring understanding of parametric knowledge and how models encode and access factual associations.
  - Quick check question: How do language models typically store factual knowledge about entities and their relationships during pre-training?

## Architecture Onboarding

- Component map: Input embeddings → lower-layer MLPs (layers 0-15) for knowledge enrichment → upper-layer attention heads (layers 16-31) for answer extraction → final prediction
- Critical path: Subject token processing → lower MLP layers extract subject attributes → upper attention layers select correct object → final prediction. Hallucinations occur when either the enrichment or extraction stage fails.
- Design tradeoffs: The paper trades model generality for specificity by targeting specific layer ranges for fine-tuning, which may limit knowledge editing to specific domains but preserves general knowledge.
- Failure signatures: Knowledge enrichment failures show low object ranking in early MLP projections (ρ_s(o) > 0.01|V|), while extraction failures show high object ranking in early layers but poor attention-based discrimination in later layers.
- First 3 experiments:
  1. Run logit lens analysis on a sample dataset to identify which examples have low subject-object ranking in early MLP outputs.
  2. Apply causal patching to measure the effect of adding noise to subject embeddings on hallucination rates.
  3. Fine-tune a small model using the MHM objective on a simple question-answering dataset and evaluate on paraphrased questions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms in early-layer MLPs and late-layer attention heads cause knowledge enrichment versus answer extraction hallucinations?
- Basis in paper: [explicit] The paper identifies two distinct hallucination mechanisms but doesn't fully explain the mechanistic differences between these failures.
- Why unresolved: While the paper demonstrates these are distinct phenomena through logit lens and causal mediation analysis, it doesn't explain what makes MLPs fail to retrieve subject knowledge versus what makes attention heads fail to select correct objects.
- What evidence would resolve it: Detailed circuit analysis showing whether knowledge enrichment failures stem from missing subject embeddings versus corrupted retrieval processes, and whether answer extraction failures result from attention head dysfunction versus context representation issues.

### Open Question 2
- Question: How can the mechanistic hallucination mitigation method be extended to handle more complex queries beyond simple subject-relation-object triples?
- Basis in paper: [inferred] The current MHM method is tested on cloze-style factual knowledge queries from ParaRel, but real-world hallucinations often occur in more complex contexts.
- Why unresolved: The paper focuses on straightforward factual queries, leaving unclear whether the identified mechanisms and mitigation approach generalize to scenarios where hallucinations arise from more complex reasoning failures or context confusion.
- What evidence would resolve it: Testing MHM on multi-hop reasoning tasks, long-form generation, or summarization where hallucinations involve more sophisticated information processing errors.

### Open Question 3
- Question: What is the relationship between the identified hallucination mechanisms and other forms of model failures like confabulation or logical inconsistency?
- Basis in paper: [explicit] The paper focuses specifically on non-factual hallucinations but acknowledges other hallucination types exist.
- Why unresolved: The mechanistic analysis is limited to factual knowledge recall failures, but it's unclear whether knowledge enrichment and answer extraction failures also contribute to other hallucination types.
- What evidence would resolve it: Comparative mechanistic analysis across different hallucination types to determine if the same failures drive all hallucination mechanisms or if different failure modes exist for different error types.

## Limitations
- The mechanistic claims rely heavily on specific diagnostic metrics that may not fully capture complex information flow in large language models.
- The effectiveness of MHM on more diverse, real-world datasets remains unclear, as the evaluation focuses on specific question-answering benchmarks.
- The paper assumes a clean two-stage pipeline (knowledge enrichment → answer extraction), but real model behavior may involve more complex, overlapping mechanisms.

## Confidence
- **High Confidence**: The identification of two distinct hallucination mechanisms is well-supported by diagnostic analyses and ablation studies. The MHM method's general approach of targeting specific transformer components is theoretically sound.
- **Medium Confidence**: The quantitative results showing MHM's superiority over baselines are promising but limited to specific datasets. The claim that MHM "restores" the internal fact recall pipeline is somewhat overstated given the narrow scope of evaluation.
- **Low Confidence**: The generalizability of the mechanistic insights to other model architectures and the long-term stability of MHM's effects on model behavior are not established.

## Next Checks
1. **Cross-architecture validation**: Test whether the same two-mechanism framework applies to different model families (e.g., GPT, Mistral) and sizes to assess generalizability.
2. **Long-term stability analysis**: Evaluate MHM's effects after extended inference to check for degradation or drift in factuality preservation over time.
3. **Multi-hop reasoning extension**: Apply MHM to datasets requiring compositional reasoning (e.g., HotpotQA) to test whether the mechanistic insights scale to more complex reasoning tasks.