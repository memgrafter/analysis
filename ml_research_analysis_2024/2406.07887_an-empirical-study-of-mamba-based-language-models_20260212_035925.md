---
ver: rpa2
title: An Empirical Study of Mamba-based Language Models
arxiv_id: '2406.07887'
source_url: https://arxiv.org/abs/2406.07887
tags:
- transformer
- layers
- tasks
- mamba
- hybrid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a controlled comparison between 8B-parameter
  Mamba, Mamba-2, and Transformer models trained on up to 3.5T tokens. The key finding
  is that while pure SSM models (Mamba and Mamba-2) match or exceed Transformers on
  many tasks, they lag behind on tasks requiring strong copying, in-context learning,
  and long-context reasoning (e.g., 5-shot MMLU, Phonebook, and long-context benchmarks).
---

# An Empirical Study of Mamba-based Language Models

## Quick Facts
- arXiv ID: 2406.07887
- Source URL: https://arxiv.org/abs/2406.07887
- Reference count: 40
- Pure SSM models (Mamba, Mamba-2) match or exceed Transformers on many tasks but lag on copying, in-context learning, and long-context reasoning

## Executive Summary
This paper presents a controlled empirical comparison of 8B-parameter Mamba, Mamba-2, and Transformer language models trained on up to 3.5T tokens. The study reveals that while pure state space models (SSMs) perform competitively with Transformers on standard benchmarks, they struggle with tasks requiring strong copying capabilities, in-context learning, and long-context reasoning. To address these limitations, the authors introduce a hybrid Mamba-2-Hybrid architecture that combines 43% Mamba-2 layers with 7% self-attention and 50% MLP layers, achieving superior performance across all evaluated tasks and predicted significant inference speedups.

## Method Summary
The paper conducts a systematic empirical study comparing three model architectures: Mamba, Mamba-2, and Transformers, all with 8B parameters trained on up to 3.5T tokens. The evaluation spans 12 standard benchmarks including 5-shot MMLU, Phonebook, and long-context tasks. The key innovation is the hybrid Mamba-2-Hybrid architecture that strategically combines Mamba-2 layers (43%), self-attention layers (7%), and MLP layers (50%) to leverage the strengths of each component while mitigating their individual weaknesses.

## Key Results
- Pure SSM models match or exceed Transformers on many tasks but lag on copying, in-context learning, and long-context reasoning
- Hybrid Mamba-2-Hybrid architecture exceeds Transformer performance on all 12 standard tasks (+2.65 points on average)
- Hybrid model predicted to be up to 8x faster at inference time
- Hybrid architecture shows improved long-context capabilities compared to pure Transformers

## Why This Works (Mechanism)
The hybrid architecture succeeds by combining the computational efficiency of Mamba-2 layers for most processing with the selective use of self-attention for critical tasks requiring copying and in-context learning. The 43/7/50% split allows the model to maintain the linear complexity benefits of SSMs while retaining the ability to perform selective attention operations where they provide the most value. This design exploits the complementary strengths of different architectures rather than forcing a single approach to handle all scenarios.

## Foundational Learning
- State Space Models (SSMs): Sequential processing mechanisms that capture long-range dependencies efficiently through convolution-like operations. Needed because traditional RNNs suffer from vanishing gradients while SSMs provide linear complexity.
- Mamba Architecture: Hardware-aware algorithm that applies SSMs with selective scan to improve efficiency. Quick check: Verify the scan operation is correctly implemented with proper state initialization.
- Self-Attention Mechanisms: Allow selective focus on relevant tokens through pairwise interactions. Needed for tasks requiring copying and in-context learning where global token relationships matter.
- Hybrid Architectures: Combine multiple processing paradigms to leverage complementary strengths. Quick check: Validate that the routing mechanism correctly directs tokens to appropriate layers.

## Architecture Onboarding

**Component Map:** Input -> Mamba-2 Layers (43%) -> Self-Attention Layers (7%) -> MLP Layers (50%) -> Output

**Critical Path:** The critical path involves the Mamba-2 layers handling the bulk of sequential processing, with self-attention layers providing targeted copying capabilities at key positions, and MLPs performing nonlinear transformations and final predictions.

**Design Tradeoffs:** The 43/7/50% split represents a balance between computational efficiency and functional capability. Increasing Mamba-2 proportion improves speed but may reduce copying performance, while increasing attention improves copying but reduces efficiency.

**Failure Signatures:** Pure SSM models fail on tasks requiring copying (Phonebook), in-context learning (5-shot MMLU), and long-context reasoning. The hybrid architecture should fail only on tasks requiring extremely high attention density.

**First Experiments:** 1) Compare hybrid performance against pure SSM and pure Transformer on Phonebook task to verify copying improvements. 2) Measure inference latency at different sequence lengths to validate speedup predictions. 3) Test long-context performance on NarrativeQA to confirm long-range reasoning improvements.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to 8B-parameter models on 3.5T tokens, may not generalize to other scales
- Hybrid architecture's 43/7/50% split appears arbitrary without sensitivity analysis
- 8x inference speedup is predicted rather than empirically measured
- Long-context evaluation focuses on synthetic benchmarks without real-world document testing
- No analysis of distribution shifts between pretraining and downstream tasks

## Confidence

**Major Claim Clusters:**
- Comparative performance analysis between pure SSMs and Transformers: **High**
- Hybrid architecture's superior performance (+2.65 points): **Medium**
- Limitations of pure SSMs on copying and in-context learning: **High**

## Next Checks

1. Conduct ablation studies varying the Mamba-2/attention/MLP ratio in the hybrid architecture to determine optimal proportions and verify the 43/7/50% split is not overfitting to specific benchmarks.

2. Measure actual inference latency of the hybrid model across different sequence lengths and batch sizes to validate or refute the predicted 8x speedup claim.

3. Evaluate the hybrid model on real-world long-document tasks (legal documents, technical manuals, research papers) to assess practical long-context reasoning capabilities beyond synthetic benchmarks.