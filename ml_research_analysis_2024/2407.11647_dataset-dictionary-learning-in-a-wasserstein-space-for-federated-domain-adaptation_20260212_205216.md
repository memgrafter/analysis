---
ver: rpa2
title: Dataset Dictionary Learning in a Wasserstein Space for Federated Domain Adaptation
arxiv_id: '2407.11647'
source_url: https://arxiv.org/abs/2407.11647
tags:
- learning
- domain
- adaptation
- dictionary
- decentralized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel federated learning method for multi-source
  domain adaptation that addresses privacy concerns in decentralized settings. The
  method, FedDaDiL, leverages Wasserstein barycenters to model distributional shifts
  across multiple clients while preserving data privacy.
---

# Dataset Dictionary Learning in a Wasserstein Space for Federated Domain Adaptation

## Quick Facts
- arXiv ID: 2407.11647
- Source URL: https://arxiv.org/abs/2407.11647
- Authors: Eduardo Fernandes Montesuma; Fabiola Espinoza Castellon; Fred Ngolè Mboula; Aurélien Mayoue; Antoine Souloumiac; Cédric Gouy-Pailler
- Reference count: 40
- Novel federated learning method using Wasserstein barycenters for privacy-preserving multi-source domain adaptation

## Executive Summary
This paper introduces FedDaDiL, a federated learning framework for multi-source domain adaptation that addresses privacy concerns in decentralized settings. The method models each client's data distribution as a Wasserstein barycenter of synthetic public atoms, weighted by private barycentric coordinates. This approach enables effective domain adaptation while keeping raw data and model parameters private. The framework demonstrates superior performance on five visual domain adaptation benchmarks compared to existing decentralized MSDA techniques, particularly in scenarios with varying client parallelism.

## Method Summary
FedDaDiL leverages Wasserstein barycenters to represent each client's data distribution without sharing raw data or model parameters. Each client's distribution is expressed as a weighted combination of synthetic public atoms, where the weights (barycentric coordinates) remain private. The framework performs federated optimization across clients while maintaining privacy guarantees. By operating in the Wasserstein space, the method effectively captures distributional shifts across multiple sources while preserving data privacy through synthetic representation.

## Key Results
- Superior performance over existing decentralized MSDA techniques on five visual domain adaptation benchmarks
- Enhanced robustness to client parallelism compared to previous approaches
- Relative communication efficiency compared to transmitting deep neural network parameters
- Maintained performance across varying levels of client parallelism
- State-of-the-art results while keeping barycentric coordinates private throughout adaptation

## Why This Works (Mechanism)
The method works by representing each client's data distribution as a Wasserstein barycenter of synthetic public atoms. This representation captures the essential distributional characteristics while keeping the actual data private. The barycentric coordinates, which determine the contribution of each synthetic atom, remain private to each client. Through federated optimization, the system learns to align distributions across clients while preserving privacy. The Wasserstein metric provides a meaningful distance measure between distributions, enabling effective adaptation even when source and target domains differ significantly.

## Foundational Learning
- **Wasserstein Distance**: Measures distance between probability distributions; needed for meaningful comparison of distributional shifts across domains; quick check: verify implementation computes Wasserstein-1 distance correctly
- **Barycentric Coordinates**: Weights that express one distribution as a combination of others; needed to represent client distributions using synthetic atoms; quick check: ensure coordinates sum to 1 and are non-negative
- **Federated Learning**: Decentralized training across multiple clients; needed to enable distributed adaptation while preserving privacy; quick check: verify aggregation step properly combines client updates
- **Domain Adaptation**: Transferring knowledge from source to target domains; needed to handle distributional shifts between clients; quick check: validate adaptation improves target performance
- **Synthetic Data Generation**: Creating representative data samples without privacy concerns; needed to provide common reference points across clients; quick check: ensure synthetic atoms capture real data characteristics
- **Optimal Transport**: Framework for moving mass between distributions; needed to compute Wasserstein barycenters efficiently; quick check: verify transport plans satisfy marginal constraints

## Architecture Onboarding

**Component Map:**
Data Clients -> Synthetic Atoms Generator -> Wasserstein Barycenter Computation -> Federated Optimization -> Target Adaptation

**Critical Path:**
1. Each client computes their distribution as Wasserstein barycenter of synthetic atoms
2. Private barycentric coordinates are used for federated optimization
3. Model parameters are updated through distributed aggregation
4. Adapted model is applied to target domain

**Design Tradeoffs:**
- Privacy preservation vs. representation fidelity (synthetic atoms may not perfectly capture complex distributions)
- Communication efficiency vs. optimization accuracy (fewer updates vs. better convergence)
- Model complexity vs. computational overhead (richer representations vs. slower training)

**Failure Signatures:**
- Poor target performance when synthetic atoms poorly represent true data distributions
- Degraded adaptation when client distributions are highly heterogeneous
- Convergence issues when barycentric coordinate optimization becomes unstable
- Communication bottlenecks when handling high-dimensional synthetic atoms

**First Experiments:**
1. Test adaptation performance with varying numbers of synthetic atoms to find optimal trade-off
2. Evaluate robustness to different levels of client parallelism (1 to N clients active simultaneously)
3. Compare communication costs against baseline methods transmitting full model parameters

## Open Questions the Paper Calls Out
None

## Limitations
- Limited exploration of scenarios with heterogeneous client data distributions
- Absence of ablation studies isolating the contribution of individual methodological components
- No analysis of computational overhead introduced by the Wasserstein optimization process
- Restricted evaluation to image-based tasks without validation on other data modalities

## Confidence
- **High confidence**: The mathematical framework for Wasserstein barycenter-based representation is sound and the privacy-preserving aspects are well-founded
- **Medium confidence**: The empirical performance gains over existing methods are demonstrated but may not generalize to all domain adaptation scenarios
- **Medium confidence**: The communication efficiency claims relative to parameter transmission are supported but lack comprehensive comparison across different model architectures

## Next Checks
1. Test the method's robustness across heterogeneous client distributions with varying levels of domain shift and dataset sizes
2. Conduct extensive ablation studies to quantify the individual contributions of the Wasserstein barycenter representation, federated optimization strategy, and synthetic atom generation
3. Evaluate performance on non-visual domain adaptation tasks (e.g., natural language processing or speech recognition) to assess cross-modal applicability