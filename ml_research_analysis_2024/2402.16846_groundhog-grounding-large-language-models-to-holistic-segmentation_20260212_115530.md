---
ver: rpa2
title: 'GROUNDHOG: Grounding Large Language Models to Holistic Segmentation'
arxiv_id: '2402.16846'
source_url: https://arxiv.org/abs/2402.16846
tags:
- image
- grounding
- visual
- segmentation
- mask
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GROUNDHOG, a multimodal large language model
  (MLLM) that enhances language grounding through holistic segmentation rather than
  bounding boxes. By incorporating a masked feature extractor and a mask proposal
  network, GROUNDHOG connects groundable phrases to pixel-level segmentation masks,
  enabling more precise visual understanding and diagnosis.
---

# GROUNDHOG: Grounding Large Language Models to Holistic Segmentation

## Quick Facts
- arXiv ID: 2402.16846
- Source URL: https://arxiv.org/abs/2402.16846
- Authors: Yichi Zhang; Ziqiao Ma; Xiaofeng Gao; Suhaila Shakiah; Qiaozi Gao; Joyce Chai
- Reference count: 40
- Primary result: Introduces GROUNDHOG, an MLLM that connects groundable phrases to pixel-level segmentation masks using holistic segmentation, outperforming specialist models on complex grounding tasks.

## Executive Summary
This paper introduces GROUNDHOG, a multimodal large language model (MLLM) that enhances language grounding through holistic segmentation rather than bounding boxes. By incorporating a masked feature extractor and a mask proposal network, GROUNDHOG connects groundable phrases to pixel-level segmentation masks, enabling more precise visual understanding and diagnosis. The model is trained on M3G2, a dataset of 2.5M text-image pairs covering diverse tasks like grounded captioning, referential segmentation, grounded VQA, and referential dialogue. GROUNDHOG outperforms specialist models on complex grounding tasks, reduces object hallucination, and provides interpretable failure cases. It also generalizes well to tasks without fine-tuning, such as spatial prompt understanding and novel category grounding.

## Method Summary
GROUNDHOG introduces a holistic segmentation approach that replaces bounding boxes with pixel-level masks for language grounding. The model uses a decoupled architecture where a mask proposal network (Mask2Former+) first identifies entity segments, and a masked feature extractor (combining CLIP and DINOv2 features) converts these into visual entity tokens. A pair of <GRD> and </GRD> tokens marks groundable phrases in the input, allowing the MLLM to generate grounding queries that retrieve and merge entity masks. The model is trained on M3G2, a curated dataset of 2.5M text-image pairs across four task types, using LoRA fine-tuning with task-specific loss functions based on annotation availability.

## Key Results
- GROUNDHOG achieves superior performance on language grounding tasks without task-specific fine-tuning
- The model reduces object hallucination compared to existing approaches
- Provides interpretable failure cases by decoupling mask proposal and language grounding components
- Generalizes well to novel tasks including spatial prompt understanding and novel category grounding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The decoupling of mask proposal and language grounding enables independent optimization and interpretability.
- Mechanism: The model separates entity mask proposal (using a specialized model like Mask2Former+) from language grounding (handled by the MLLM), allowing each component to be improved independently and failures to be diagnosed at the component level.
- Core assumption: Mask proposal quality is independent of language grounding ability, and failures can be isolated to either component.
- Evidence anchors:
  - [abstract] "Building upon this framework, we formulate the grounding process as an entity segment selection problem, which involves (1) proposing entity segmentation masks where the masks encapsulate regions with discernible semantic content, and (2) recognizing the retrieved entities through the understanding of both visual and language context."
  - [section 2] "This decoupled design of entity mask proposal and language-guided grounding brings several advantages. First, it allows independent improvement of the mask proposal model and MLLM, where specialized data, training, and inference setups can be applied. Second, by decoupling language grounding, it becomes straightforward to determine if a failure is due to the model's inability to propose the entity segment, or its misalignment of the object with the language, thus improving the interpretability of the whole framework."
  - [corpus] Weak evidence; no direct corpus neighbors discuss this specific decoupling mechanism.
- Confidence: Medium

### Mechanism 2
- Claim: The use of <GRD> and </GRD> tokens provides interpretable grounding queries for the MLLM.
- Mechanism: Groundable phrases are enclosed in <GRD> and </GRD> tokens, and the MLLM's output embedding for these tokens is used as a grounding query to retrieve and merge entity masks.
- Core assumption: The MLLM can learn to generate meaningful embeddings for <GRD> tokens that correlate with the correct entity masks.
- Evidence anchors:
  - [abstract] "The MLLM then connects groundable phrases to unified grounding masks by retrieving and merging the entity masks."
  - [section 2.2] "To alleviate this disconnect, we introduce a pair of grounding tokens <GRD> and </GRD> to indicate the start and end of groundable phrases, with the assumption that grounding these phrases requires mapping to certain representations of visual entities irrespective of the visual modality."
  - [corpus] Weak evidence; no direct corpus neighbors discuss this specific grounding token mechanism.
- Confidence: Medium-Low

### Mechanism 3
- Claim: The combination of CLIP and DINOv2 features provides robust entity representations.
- Mechanism: Mask features are extracted from both CLIP and DINOv2, and their combination yields the best results for representing visual entities.
- Core assumption: CLIP and DINOv2 capture complementary aspects of visual information, and their combination is more effective than either alone.
- Evidence anchors:
  - [abstract] "We empirically find the combination of CLIP and DINOv2 features yields the best result, and these features are added to obtain the final input visual entity tokens to the MLLM."
  - [section 2.1] "The pooled features are then fed into a Multi-Layer Perceptron (MLP) network to align with the input embeddings of the MLLM. We empirically find the combination of CLIP and DINOv2 features yields the best result, and these features are added to obtain the final input visual entity tokens to the MLLM."
  - [corpus] Weak evidence; no direct corpus neighbors discuss this specific feature combination mechanism.
- Confidence: Medium

## Foundational Learning

- Concept: Semantic granularity in segmentation
  - Why needed here: The model needs to handle entities at different levels of detail (things, stuff, parts, text) to provide holistic grounding.
  - Quick check question: What are the four levels of semantic granularity mentioned in the paper for entity segmentation?

- Concept: Multimodal instruction tuning
  - Why needed here: The model is trained on a diverse dataset of grounded instructions to learn to connect language to visual entities.
  - Quick check question: How many sub-problems and datasets are used to construct the M3G2 dataset for instruction tuning?

- Concept: Visual entity tokens
  - Why needed here: The model converts extracted mask features into visual entity tokens that can be processed by the MLLM.
  - Quick check question: What is the purpose of the MLP network in the masked feature extractor?

## Architecture Onboarding

- Component map: Input image → Mask proposal network (Mask2Former+) → Masked feature extractor (CLIP + DINOv2 + MLP) → MLLM backbone → Grounding token embeddings → Mask retrieval and merging → Output segmentation masks
- Critical path: The path from input image to output segmentation masks, including mask proposal, feature extraction, grounding query generation, and mask retrieval/merge.
- Design tradeoffs: Decoupling mask proposal and grounding allows independent optimization but adds complexity; using <GRD> tokens provides interpretability but requires the MLLM to learn meaningful embeddings; combining CLIP and DINOv2 features may improve robustness but increases computational cost.
- Failure signatures: Poor mask proposal quality, inaccurate grounding query embeddings, failure in mask retrieval or merging, object hallucination.
- First 3 experiments:
  1. Evaluate mask proposal quality on a held-out set of images.
  2. Test grounding query generation by inspecting <GRD> token embeddings.
  3. Assess mask retrieval and merging accuracy on a validation set of grounded phrases.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GROUNDHOG scale with the size of the M3G2 dataset?
- Basis in paper: [inferred] The paper mentions that the M3G2 dataset contains 2.5M text-image pairs and discusses the benefits of using this dataset for training GROUNDHOG. However, it does not explore how the performance changes with different dataset sizes.
- Why unresolved: The paper does not provide experiments or analysis on the impact of dataset size on GROUND