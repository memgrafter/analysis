---
ver: rpa2
title: Controllable Decontextualization of Yes/No Question and Answers into Factual
  Statements
arxiv_id: '2401.09775'
source_url: https://arxiv.org/abs/2401.09775
tags:
- question
- constraints
- answers
- input
- rewriting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the task of rewriting answers to yes/no questions
  into decontextualized factual statements. The proposed method, SMF (Soft Mention
  Flags), is a transformer-based sequence-to-sequence model that uses soft constraints
  extracted from constituency parse trees of the input.
---

# Controllable Decontextualization of Yes/No Question and Answers into Factual Statements

## Quick Facts
- arXiv ID: 2401.09775
- Source URL: https://arxiv.org/abs/2401.09775
- Authors: Lingbo Mo; Besnik Fetahu; Oleg Rokhlenko; Shervin Malmasi
- Reference count: 38
- Primary result: SMF achieves BLEU 52.6, ROUGE-L 69.5, BERTScore 95.5 on PAR test set, outperforming baselines

## Executive Summary
This paper introduces SMF (Soft Mention Flags), a transformer-based approach for rewriting yes/no question-answer pairs into decontextualized factual statements. The method uses soft constraints extracted from constituency parse trees and encoded via a soft-mention flags matrix to ensure semantic constraint satisfaction during generation. Evaluated on a new 1500-instance dataset from Amazon PQA plus out-of-domain Reddit and SemEval data, SMF achieves state-of-the-art performance with strong results in both automated metrics and human evaluation.

## Method Summary
The SMF approach uses a T5 transformer with a modified decoder that incorporates a soft-mention flags matrix encoding constraints extracted from constituency parse trees. Constraints (noun phrases, verb phrases, and other syntactic constituents) are automatically identified and their satisfaction is tracked during decoding using semantic similarity between embeddings. The model also enforces factual style constraints by transforming first-person narratives to second-person. The method is trained on 1000 examples from a manually created PAR dataset and evaluated on both in-domain and out-of-domain test sets.

## Key Results
- SMF achieves BLEU 52.6, ROUGE-L 69.5, and BERTScore 95.5 on the PAR test set
- Outperforms baselines (T5, CBS, GPT-2, COLD, MF) on both automated and human evaluations
- Demonstrates strong zero-shot performance on Reddit and SemEval out-of-domain datasets
- Human evaluation shows superior syntactic clause coverage and correctness/coherence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The soft-mention flags matrix enforces semantic constraint satisfaction at the decoder level.
- Mechanism: The SMF approach uses a matrix M where each row corresponds to an input token and each column to a generated output token. Values of 0, 1, or 2 indicate whether a token is not part of a constraint, part of a constraint but unsatisfied, or part of a constraint and satisfied, respectively. During decoding, the model uses this matrix to guide attention and ensure that extracted constraints from the input are represented in the output, even if paraphrased.
- Core assumption: Semantic similarity computed via sentence embeddings can reliably detect when a paraphrased constraint is satisfied in the output.
- Evidence anchors:
  - [abstract]: "The constraints are encoded using a soft-mention flags matrix, which allows the model to ensure constraint satisfaction at the semantic level."
  - [section]: "We propose two strategies to encode constraint satisfaction. ... (1) threshold a, where simt > a, (2) threshold b of the difference between the current similarity score and the score in step t − 1, namely simt − simt−1 > b."
  - [corpus]: "Weak or missing corpus evidence for semantic constraint satisfaction thresholds. No direct experimental data provided in corpus signals."

### Mechanism 2
- Claim: Automated constraint extraction from constituency parse trees allows the model to generalize across diverse question types without manual annotation.
- Mechanism: The approach parses the input PQA into a constituency tree, then extracts noun phrases (NPs), verb phrases (VPs), and other phrases (PPs, ADVPs, ADJPs) as constraints. These constraints are prioritized by importance and automatically encoded into the soft-mention flags matrix. This allows the model to handle different syntactic shapes of answers (e.g., embedded clauses, conditionals, alternatives) without requiring manual constraint specification for each example.
- Core assumption: Constituency parse trees reliably capture the syntactic structure of polar questions and answers, and the extracted phrases correspond to the key semantic content that needs to be preserved in the rewritten statement.
- Evidence anchors:
  - [section]: "Our goal in controllable rewriting is for the target statement to contain the interrogative clause from the question, and the corresponding affirmation/negation clauses in the answer, along with the any embedded conditional or alternative clauses. This boils down to two main tasks for constraint extraction: (1) determining the input tokens that must be present in the output, and (2) ensuring that the decoder satisfies such constraints."
  - [section]: "Following the syntactic rewriting cases in Table 1, we extract constraints, shown in order of importance. (the algorithm is provided in the paper's appendix)."
  - [corpus]: "No direct evidence in corpus signals about parse tree accuracy or constraint extraction performance."

### Mechanism 3
- Claim: Encoding style constraints (e.g., transforming first-person to second-person narrative) improves the coherence and usability of the generated factual statements.
- Mechanism: In addition to semantic constraints, the model encodes factual style constraints by marking first-person pronouns with a flag of 2 and changing them to 1 once a second-person pronoun is generated. This ensures that the output is framed in a second-person narrative, making it more suitable for factual statements and downstream applications like voice assistants.
- Core assumption: The presence of first-person pronouns in the answer indicates a need for transformation to second-person for factual framing, and this transformation can be reliably detected and enforced through the soft-mention flags matrix.
- Evidence anchors:
  - [section]: "Factual Style Constraints: To frame the output statement in a factual style, first person narratives are transformed into a second person narrative. Such a seemingly small change (i.e., 1st to 2nd person pronouns), incurs a series of syntactic rewrite operations required to ensure coherence of the output statement."
  - [section]: "Table 2 (b) provides an example. In this case, we represent in the M 1st person pronouns with score 2, and convert them to 1, once the model has generated a second person pronoun in the output."
  - [corpus]: "No direct evidence in corpus signals about the effectiveness of style constraint enforcement."

## Foundational Learning

- Concept: Constituency parsing and phrase extraction
  - Why needed here: The SMF approach relies on automatically extracting constraints from the syntactic structure of the input PQA. Understanding how constituency parsers work and how to extract noun phrases, verb phrases, and other constituents is essential for implementing the constraint extraction algorithm.
  - Quick check question: Given a sentence "The quick brown fox jumps over the lazy dog", what noun phrases and verb phrases would a constituency parser extract?

- Concept: Semantic similarity and sentence embeddings
  - Why needed here: The model uses cosine similarity between sentence embeddings to determine if a constraint is satisfied in the output, even when paraphrased. Knowledge of how sentence embeddings capture semantic meaning and how cosine similarity works is crucial for tuning the thresholds and understanding the mechanism.
  - Quick check question: If sentence A is "The cat is on the mat" and sentence B is "The feline rests on the rug", would their sentence embeddings likely have high cosine similarity? Why or why not?

- Concept: Transformer decoder attention and cross-attention mechanisms
  - Why needed here: The soft-mention flags matrix is injected into the cross-multi-head attention module of the decoder. Understanding how transformer decoders use attention to generate tokens and how cross-attention incorporates encoder outputs is necessary for grasping how the constraints influence generation.
  - Quick check question: In a transformer decoder, what is the difference between self-attention and cross-attention, and how does each contribute to generating the next token?

## Architecture Onboarding

- Component map: Input -> Constraint Extraction -> M Matrix Construction -> Encoder -> Decoder (with M) -> Output
- Critical path: Input → Constraint Extraction → M Matrix Construction → Encoder → Decoder (with M) → Output
- Design tradeoffs:
  - Using semantic similarity for constraint satisfaction allows paraphrasing but introduces sensitivity to threshold tuning.
  - Automatic constraint extraction avoids manual annotation but depends on parser accuracy.
  - Encoding style constraints improves factual framing but may not generalize to all contexts.
- Failure signatures:
  - Poor BLEU/ROUGE scores: Likely due to incorrect constraint extraction or semantic similarity thresholds.
  - Low human evaluation scores on coherence: May indicate issues with style constraint enforcement or overly strict semantic thresholds.
  - High context coverage but low equivalence: Suggests the model copies context but fails to properly rewrite the answer.
- First 3 experiments:
  1. Ablation study: Remove the soft-mention flags matrix and compare performance to SMF to confirm the matrix improves constraint satisfaction.
  2. Threshold sensitivity: Vary the semantic similarity thresholds (a and b) and measure impact on constraint satisfaction accuracy and output quality.
  3. Parser dependency: Replace the constituency parser with a simpler rule-based extractor and compare constraint extraction accuracy and final performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed SMF approach perform on out-of-domain datasets beyond the three evaluated (PAR, Reddit, SemEval)?
- Basis in paper: [explicit] The paper evaluates performance on PAR, Reddit, and SemEval datasets, but does not explore other potential out-of-domain datasets.
- Why unresolved: The paper only evaluates on three datasets, leaving open the question of generalizability to other domains or data sources.
- What evidence would resolve it: Evaluating the SMF approach on a wider range of out-of-domain datasets, including those from different domains or with different characteristics, would provide evidence for its generalizability.

### Open Question 2
- Question: How does the performance of the SMF approach vary with different window sizes for constraint satisfaction assessment?
- Basis in paper: [explicit] The paper mentions using a sliding window of length equal to the constraint for semantic similarity assessment, but does not explore the impact of varying window sizes.
- Why unresolved: The choice of window size could impact the accuracy of constraint satisfaction assessment, but the paper does not investigate this aspect.
- What evidence would resolve it: Evaluating the performance of SMF with different window sizes for constraint satisfaction assessment would provide insights into the optimal window size and its impact on overall performance.

### Open Question 3
- Question: How does the SMF approach handle complex or ambiguous polar questions that require deeper understanding of context or world knowledge?
- Basis in paper: [inferred] The paper focuses on the decontextualization of answers to polar questions, but does not explicitly address the handling of complex or ambiguous questions.
- Why unresolved: The paper does not provide insights into how the SMF approach would perform on polar questions that require deeper understanding of context or world knowledge.
- What evidence would resolve it: Evaluating the SMF approach on a dataset of complex or ambiguous polar questions, and comparing its performance to other approaches, would provide evidence for its ability to handle such questions.

## Limitations

- The approach depends heavily on the accuracy of constituency parsing for constraint extraction, with no analysis of how parser errors propagate through the system.
- The semantic similarity thresholds (a and b) are not thoroughly analyzed for sensitivity or optimal selection across different domains.
- Human evaluation focuses on syntactic clause coverage and coherence but doesn't assess factual accuracy or maintenance of true meaning in the rewritten statements.

## Confidence

**High Confidence Claims**:
- The SMF approach achieves state-of-the-art performance on the PAR dataset for the task of decontextualizing yes/no questions and answers into factual statements, as evidenced by automated metrics (BLEU: 52.6, ROUGE-L: 69.5, BERTScore: 95.5) and human evaluation scores.
- The soft-mention flags matrix is an effective mechanism for enforcing constraint satisfaction at the semantic level, outperforming baselines that don't use this approach.

**Medium Confidence Claims**:
- The method generalizes well to out-of-domain datasets (Reddit and SemEval), though this is based on a relatively small number of examples (50 each) and zero-shot evaluation.
- The automated constraint extraction from constituency parse trees is reliable enough to capture the key semantic content that needs to be preserved in the rewritten statements.

**Low Confidence Claims**:
- The specific values of the semantic similarity thresholds (a and b) are optimal and will work well across different datasets or domains.
- The soft-mention flags matrix will maintain its effectiveness when applied to other rewriting tasks beyond yes/no question decontextualization.

## Next Checks

**Check 1: Threshold Sensitivity Analysis**
Vary the semantic similarity thresholds (a and b) systematically across a wide range of values and measure the impact on both constraint satisfaction accuracy and output quality metrics. This would reveal whether the current thresholds are truly optimal or if the approach is sensitive to these parameters in ways that could limit its robustness.

**Check 2: Parser Robustness Testing**
Evaluate the SMF approach using multiple constituency parsers with different architectures and training data. Additionally, test the model with intentionally degraded parser outputs to understand how parser errors affect constraint extraction and final performance. This would quantify the dependency on parser quality and identify potential failure modes.

**Check 3: Factual Accuracy Evaluation**
Conduct a comprehensive human evaluation that specifically assesses whether the rewritten statements maintain factual accuracy and true meaning relative to the original PQA. This should include examples where the answer is "no" or contains conditional information to test whether the model handles negation and complex logical relationships correctly.