---
ver: rpa2
title: Symmetry-Breaking Augmentations for Ad Hoc Teamwork
arxiv_id: '2402.09984'
source_url: https://arxiv.org/abs/2402.09984
tags:
- training
- agents
- agent
- conventions
- population
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Symmetry-Breaking Augmentations (SBA) is a novel policy augmentation
  technique for improving ad hoc teamwork (AHT) in multi-agent settings. SBA addresses
  the challenge of agents adapting to novel teammates with unforeseen strategies by
  applying symmetry-flipping operations during training to increase behavioural diversity.
---

# Symmetry-Breaking Augmentations for Ad Hoc Teamwork

## Quick Facts
- arXiv ID: 2402.09984
- Source URL: https://arxiv.org/abs/2402.09984
- Authors: Ravi Hammond; Dustin Craggs; Mingyu Guo; Jakob Foerster; Ian Reid
- Reference count: 33
- Primary result: Symmetry-Breaking Augmentations (SBA) improves ad hoc teamwork performance by up to 17% over baseline best-response agents when tested against unseen teammates

## Executive Summary
Symmetry-Breaking Augmentations (SBA) is a novel policy augmentation technique designed to improve ad hoc teamwork (AHT) in multi-agent settings. The method addresses the challenge of agents adapting to novel teammates with unforeseen strategies by applying symmetry-flipping operations during training to increase behavioral diversity. This exposes agents to a wider range of conventions, enabling robust responses to unknown strategies. SBA introduces Augmentation Impact (AugImp), a metric for estimating symmetry dependency in policies, to predict SBA's effectiveness. In Hanabi experiments, SBA consistently enhances generalization across various training set sizes and demonstrates state-of-the-art AHT results, showing promise for better AI alignment with diverse human conventions.

## Method Summary
SBA is a policy augmentation technique that applies symmetry-flipping operations to training teammates during AHT agent training. The method works by identifying environmental symmetries and applying equivalence mappings (ϕ ∈ Φ) to the AHT agent's observations and actions, then inversely relabeling actions before applying them to the environment. This creates transformed versions of policies that break symmetry-equivalent conventions, allowing the AHT agent to experience a wider variety of teammate behaviors without changing the fundamental task. The approach uses deep RL (specifically R2D2 architecture with recurrent networks, prioritized experience replay, and distributed training) and introduces AugImp, a metric for estimating symmetry dependency in policies to predict SBA's effectiveness.

## Key Results
- SBA improves AHT performance by up to 17% over baseline best-response agents when tested against unseen teammates
- SBA consistently enhances generalization across various training set sizes (small: 1 policy, medium: 6 policies, large: 11 policies)
- SBA demonstrates state-of-the-art AHT results in Hanabi experiments, particularly effective with populations that rely on symmetry-based conventions (SAD, IQL)
- AugImp metric successfully predicts SBA's effectiveness, with high scores indicating greater potential for improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SBA improves AHT by exposing agents to combinatorially more diverse conventions during training
- Mechanism: By applying symmetry-flipping operations to training teammates, SBA creates transformed versions of policies that break symmetry-equivalent conventions, allowing the AHT agent to experience a wider variety of teammate behaviors
- Core assumption: The training population contains agents that rely on symmetry-based conventions, making SBA's augmentations meaningful
- Evidence anchors:
  - [abstract]: "By applying a symmetry-flipping operation to increase behavioural diversity among training teammates, SBA encourages agents to learn robust responses to unknown strategies"
  - [section 3.1]: "If πi j is a physical agent acting in the real world, then ϕ can't easily be applied as it requires us to modify its actions and observations"
  - [corpus]: Weak - no direct corpus evidence for this specific mechanism
- Break condition: If training policies don't rely on symmetry-based conventions, SBA has minimal effect (AugImp score approaches zero)

### Mechanism 2
- Claim: SBA creates symmetry-equivalent environments that preserve expected returns while changing observed conventions
- Mechanism: The symmetry operators ϕ are automorphisms that leave the environment dynamics and rewards unchanged up to relabeling, ensuring the AHT agent learns robust responses without changing the fundamental task
- Core assumption: The symmetry operators are true automorphisms that preserve the Dec-POMDP structure
- Evidence anchors:
  - [section 2.3]: "ϕ ∈ Φ ⇐ ⇒ T(ϕ(st+1)|ϕ(st), ϕ(at)) = T (st+1|st, at) ∧R(ϕ(rt+1)|ϕ(st+1), ϕ(at)) = R(rt+1|st+1, at) ∧U i(ϕ(oi t+1)|ϕ(st+1), ϕ(at)) = U i(oi t+1|st+1, at)"
  - [abstract]: "SBA acts as an operator that can be applied to other agents in an environment, flipping their behaviours along environmental symmetries"
  - [corpus]: Weak - no direct corpus evidence for this specific mechanism
- Break condition: If symmetry operators don't preserve the Dec-POMDP structure, SBA could fundamentally alter the task

### Mechanism 3
- Claim: SBA reduces over-reliance on specific color-based conventions in Hanabi by exposing agents to different color permutations
- Mechanism: When trained with SBA, agents learn to use rank-based hints more frequently than color-based hints, making them more robust to teammates using different color conventions
- Core assumption: Color symmetry exists in Hanabi and teammates use color-based conventions
- Evidence anchors:
  - [section 5.3]: "When an SBA agent is trained with SAD and IQL splits, it hints colours significantly less often compared to a baseline agent"
  - [section 5.4]: "We hypothesize that this is one of the main reasons why SBA harms performance in this case" (referring to reduced color hints when coordinating with OBL agents)
  - [corpus]: Weak - no direct corpus evidence for this specific mechanism
- Break condition: If teammates don't use color-based conventions or color symmetry doesn't exist in the environment

## Foundational Learning

- Concept: Dec-POMDP (Decentralized Partially Observable Markov Decision Process)
  - Why needed here: The paper formalizes the ad hoc teamwork problem within the Dec-POMDP framework, which captures the partial observability and decentralized decision-making aspects
  - Quick check question: What are the key differences between a standard MDP and a Dec-POMDP?

- Concept: Symmetry and automorphisms in multi-agent systems
  - Why needed here: SBA relies on identifying and exploiting environmental symmetries to create diverse training experiences without changing the underlying task
  - Quick check question: How does a symmetry operator preserve the Dec-POMDP structure while changing observed conventions?

- Concept: Best Response (BR) in game theory
  - Why needed here: The AHT agent learns a best response to the training population, and SBA aims to improve this best response's generalization to unseen teammates
  - Quick check question: What is the mathematical definition of a best response policy in the context of ad hoc teamwork?

## Architecture Onboarding

- Component map: Pre-trained teammate policies -> Symmetry operator selection -> SBA augmentation -> AHT agent training loop -> Replay buffer -> RL algorithm update
- Critical path:
  1. Sample teammate policy from training population
  2. Sample symmetry operator ϕ from available symmetries
  3. Apply ϕ to AHT agent's observations, generate actions, apply ϕ⁻¹ to actions
  4. Store AHT agent's experience in replay buffer
  5. Periodically update AHT agent's policy using standard RL algorithm
- Design tradeoffs:
  - SBA vs. training larger, more diverse populations from scratch (computational cost vs. flexibility)
  - Random symmetry selection vs. adaptive symmetry selection based on teammate behavior
  - Applying SBA to AHT agent vs. attempting to modify teammate policies directly
- Failure signatures:
  - No improvement in AugImp score indicates training policies don't rely on symmetry-based conventions
  - Performance degradation on specific test sets (like OBL in Hanabi) suggests SBA is over-correcting
  - Training instability or poor learning curves suggest implementation issues with symmetry application
- First 3 experiments:
  1. Implement SBA on iterated lever coordination game with 5 training policies and 10 test policies
  2. Calculate AugImp scores for different Hanabi populations (SAD, IQL, OP, OBL) to identify best candidates for SBA
  3. Train SBA and baseline agents on small Hanabi training splits and compare generalization to held-out test policies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can symmetry-breaking augmentations be effectively applied to partial or imperfect symmetries in real-world environments?
- Basis in paper: [explicit] The paper mentions that symmetries must exist in both the environment and teammate strategies, and may require expert knowledge to define. It also suggests that SBA could be applied to partial or imperfect symmetries.
- Why unresolved: The paper does not provide specific methods or examples of how to apply SBA to partial or imperfect symmetries, nor does it discuss how to automatically detect these symmetries.
- What evidence would resolve it: Developing algorithms that can identify and apply symmetry-breaking augmentations to partial or imperfect symmetries in various real-world environments, along with empirical results demonstrating improved AHT performance.

### Open Question 2
- Question: Can symmetry-breaking augmentations be combined with other AHT improvement techniques, such as search, to further enhance performance?
- Basis in paper: [explicit] The paper suggests investigating how SBA can be utilized in combination with other AHT improvements, such as search, for instance by shuffling symmetries in search rollouts.
- Why unresolved: The paper does not explore or provide results on combining SBA with other AHT techniques like search-based methods.
- What evidence would resolve it: Experiments comparing AHT performance using SBA alone versus SBA combined with search-based methods, demonstrating whether the combination leads to significant improvements in coordination with novel teammates.

### Open Question 3
- Question: How does the performance of symmetry-breaking augmentations scale with the size and diversity of the training population?
- Basis in paper: [inferred] The paper shows that SBA improves performance in Hanabi with different training set sizes, but it does not extensively explore how performance scales with increasing population size or diversity.
- Why unresolved: While the paper demonstrates SBA's effectiveness across various training set sizes, it does not provide a comprehensive analysis of how performance changes as the training population grows larger or more diverse.
- What evidence would resolve it: Systematic experiments varying the size and diversity of training populations, measuring AHT performance and generalization to novel teammates, to determine the optimal conditions for applying SBA.

## Limitations
- SBA's effectiveness critically depends on the existence of symmetry-based conventions in the training population, with minimal benefits when applied to populations with low symmetry dependency
- The method may over-correct when transferring to populations with fundamentally different conventions, potentially harming performance on specific test sets
- The paper does not provide a clear method for determining when SBA is likely to be harmful versus helpful for specific test populations

## Confidence

**High Confidence**: SBA's core mechanism of applying symmetry-flipping operations to increase behavioral diversity during training is well-defined and theoretically sound

**Medium Confidence**: The empirical results showing SBA's effectiveness on Hanabi are robust, but the transfer results to OBL agents reveal limitations that aren't fully explained

**Medium Confidence**: The AugImp metric provides a useful heuristic for predicting SBA's effectiveness, but its predictive power across diverse scenarios needs more validation

## Next Checks

1. **Cross-population AugImp validation**: Calculate AugImp scores for multiple Hanabi populations (including potential future populations) to establish a stronger correlation between AugImp and SBA effectiveness across different convention types

2. **Adaptive symmetry selection**: Implement a mechanism that selects symmetry operators based on the current teammate's behavior rather than random sampling, to test whether targeted symmetry breaking improves results on challenging populations like OBL

3. **Zero-shot transfer analysis**: Conduct more detailed analysis of SBA's performance when transferring to completely out-of-distribution populations, including visualization of learned policies to understand what conventions the AHT agent actually learned