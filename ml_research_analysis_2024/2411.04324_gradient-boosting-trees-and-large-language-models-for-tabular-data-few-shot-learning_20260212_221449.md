---
ver: rpa2
title: Gradient Boosting Trees and Large Language Models for Tabular Data Few-Shot
  Learning
arxiv_id: '2411.04324'
source_url: https://arxiv.org/abs/2411.04324
tags:
- data
- lightgbm
- performance
- gbdt
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of gradient boosting decision trees
  (GBDT) and large language models (LLM) for few-shot learning (FSL) on tabular data.
  The authors replicate previous work showing LLMs outperform GBDT in FSL settings
  and demonstrate that this underperformance is due to improper parameter tuning in
  GBDT implementations.
---

# Gradient Boosting Trees and Large Language Models for Tabular Data Few-Shot Learning

## Quick Facts
- arXiv ID: 2411.04324
- Source URL: https://arxiv.org/abs/2411.04324
- Reference count: 40
- Primary result: GBDT can outperform LLMs in few-shot learning for tabular data when properly tuned

## Executive Summary
This paper challenges the recent consensus that large language models (LLMs) are superior to gradient boosting decision trees (GBDT) for few-shot learning on tabular data. Through systematic parameter tuning of LightGBM, the authors demonstrate that GBDT can achieve a 290% improvement over previous baselines, becoming competitive with LLMs for 8+ samples and potentially outperforming them with sufficient training data. The work highlights the critical importance of proper baseline selection and algorithm understanding in machine learning research.

## Method Summary
The authors modify LightGBM parameters specifically for few-shot learning scenarios, focusing on relaxing minimum samples per leaf constraints, enabling extremely randomized trees, and adjusting feature/instance subsampling rates. They test their approach on 6 binary tabular datasets across various shot sizes (4, 8, 16, 32, 64 samples per class) and validate their method in a real-world ML competition, achieving first place using ensemble stacking of multiple GBDT models trained on non-overlapping samples.

## Key Results
- LightGBM with FSL-tuned parameters shows 290% improvement over previous baselines
- GBDT becomes competitive with LLMs for 8+ samples per class
- GBDT can outperform LLMs when sufficient samples are available
- First place ranking in FedCSIS 2024 stock prediction competition validates real-world applicability

## Why This Works (Mechanism)

### Mechanism 1
LightGBM underperforms in few-shot learning due to its default minimum samples per leaf constraint. The min_samples_leaf parameter prevents LightGBM from splitting nodes until enough training samples are available, which is problematic in few-shot settings where data is scarce. The default min_samples_leaf value of 20 is too restrictive for few-shot learning scenarios.

### Mechanism 2
Using extremely randomized trees instead of standard decision trees improves few-shot learning performance. ExtraTrees introduce more randomness in the splitting process, which helps when data is limited and prevents overfitting to noise in small samples. Randomness in tree construction is beneficial when training data is scarce.

### Mechanism 3
Feature subsampling (feature_fraction) and instance subsampling (bagging_fraction) improve generalization in few-shot learning. By reducing the number of features and instances used in each tree, the model becomes more robust to noise and better at generalization when data is limited. Reducing information per tree prevents overfitting when training samples are scarce.

## Foundational Learning

- Concept: Gradient Boosting Decision Trees (GBDT) algorithm mechanics
  - Why needed here: Understanding how GBDT builds trees sequentially and optimizes splitting criteria is crucial for knowing why parameter tuning affects few-shot performance
  - Quick check question: How does GBDT calculate information gain for node splitting?

- Concept: Parameter tuning trade-offs in machine learning
  - Why needed here: Recognizing that parameters beneficial for few-shot learning can be detrimental for larger datasets helps avoid inappropriate generalization
  - Quick check question: What happens to model performance when min_samples_leaf is set too low on large datasets?

- Concept: Ensemble methods and model diversity
  - Why needed here: Understanding how few-shot learning can be used to create diverse models for stacking/ensembling approaches is key to the competition results
  - Quick check question: How does training models on non-overlapping samples improve ensemble performance?

## Architecture Onboarding

- Component map: LightGBM configuration -> Feature engineering (minimal) -> Few-shot training with parameter tuning -> Model stacking/ensembling -> Meta-learner optimization
- Critical path: Parameter tuning -> Few-shot training -> Performance validation -> Competition application
- Design tradeoffs: Lower min_samples_leaf improves few-shot performance but increases overfitting risk; feature subsampling helps generalization but may miss important signals
- Failure signatures: Random-guess performance (AUC ~0.5) indicates parameter constraints preventing tree growth; overfitting manifests as poor generalization on validation sets
- First 3 experiments:
  1. Replicate baseline: Train LightGBM with default parameters on few-shot dataset and measure AUC
  2. Parameter tuning test: Apply recommended parameters (min_data_in_leaf=1, extra_trees=True) and compare performance
  3. Feature subsampling experiment: Test different feature_fraction values to find optimal balance between performance and generalization

## Open Questions the Paper Calls Out

### Open Question 1
What is the precise sample size threshold at which GBDT consistently outperforms LLMs in few-shot learning for tabular data? The paper only provides approximate ranges (8+ shots) rather than specific numerical thresholds.

### Open Question 2
How does the performance gap between GBDT and LLMs vary across different types of tabular data (numerical vs categorical vs mixed)? The paper doesn't systematically categorize datasets by feature type or analyze how this affects the relative performance differential.

### Open Question 3
What is the impact of increasing model complexity (number of trees, depth) on GBDT performance in few-shot learning scenarios? The paper uses fixed recommended parameters rather than systematically varying model complexity.

## Limitations

- Claims are based on limited experimental scope (6 specific datasets)
- Parameter tuning recommendations may not generalize to all tabular data types
- Computational costs of recommended parameter configurations are not addressed

## Confidence

**High Confidence**: The core finding that improper parameter tuning can severely impact GBDT performance in few-shot settings is well-supported by experimental evidence and aligns with known GBDT mechanics.

**Medium Confidence**: The claim that GBDT can outperform LLMs in few-shot learning for 8+ samples is supported but may be dataset-dependent. The competition results provide strong evidence but represent a single application.

**Low Confidence**: The mechanism explanations for why ExtraTrees specifically helps few-shot learning are somewhat speculative, with limited theoretical grounding or empirical validation beyond the presented experiments.

## Next Checks

1. **Cross-dataset validation**: Test the recommended parameter tuning on additional tabular datasets not included in the original study to assess generalizability of the 290% improvement claim.

2. **Parameter sensitivity analysis**: Systematically vary each recommended parameter (min_data_in_leaf, feature_fraction, bagging_fraction) independently to quantify their individual contributions to performance gains.

3. **Computational cost evaluation**: Measure training time and resource usage for both the tuned GBDT approach and the LLM baseline across different sample sizes to assess practical deployment implications.