---
ver: rpa2
title: Comparing Inferential Strategies of Humans and Large Language Models in Deductive
  Reasoning
arxiv_id: '2402.14856'
source_url: https://arxiv.org/abs/2402.14856
tags:
- marble
- statement
- conclusion
- reasoning
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares inferential strategies used by large language
  models (LLMs) and humans in deductive reasoning tasks involving propositional logic.
  Drawing from cognitive psychology, the authors analyze LLM responses to propositional
  logic problems, identifying strategies such as supposition following, chain construction,
  and compound reasoning.
---

# Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning

## Quick Facts
- **arXiv ID**: 2402.14856
- **Source URL**: https://arxiv.org/abs/2402.14856
- **Reference count**: 40
- **Primary result**: LLMs employ deductive reasoning strategies similar to humans, but correctness doesn't guarantee sound reasoning

## Executive Summary
This study investigates how large language models perform deductive reasoning compared to human cognitive strategies. The authors analyze LLM responses to 12 propositional logic problems using zero-shot chain-of-thought prompting, identifying three main inferential strategies: supposition following, chain construction, and compound reasoning. Results reveal that while LLMs use similar strategies to humans, the frequency of strategy usage varies significantly across model families and sizes. Notably, the study finds that models can arrive at correct answers through flawed reasoning processes, highlighting a critical limitation in current LLM evaluation methods.

## Method Summary
The study evaluates five language models (Llama 2-7B/13B/70B, Mistral-7B-Instruct, Zephyr-7B-β) on 12 propositional logic problems from cognitive psychology research. Using zero-shot chain-of-thought prompting with "Let's think step by step," models generate reasoning chains that are manually annotated for strategy type, soundness, and final answer correctness. The researchers employ nucleus sampling (top-p=0.9, T=0.6) with 5 random seeds per model to capture variability in responses. Manual annotation follows detailed guidelines to classify reasoning strategies and evaluate the logical validity of intermediate steps, distinguishing between correct conclusions reached through valid versus invalid reasoning paths.

## Key Results
- LLMs demonstrate three distinct inferential strategies in propositional logic reasoning: supposition following, chain construction, and compound reasoning
- Strategy usage frequencies vary significantly across model families and sizes, with larger models showing different patterns than smaller ones
- Correct final answers do not necessarily indicate sound reasoning processes, as models can arrive at right conclusions through flawed logical chains

## Why This Works (Mechanism)
The study reveals that LLMs can mimic human-like deductive reasoning strategies despite lacking explicit logical reasoning mechanisms. Models appear to leverage their pattern-matching capabilities to identify and follow structured reasoning paths, even when these paths may contain logical errors. The variation in strategy usage across different model sizes and families suggests that architectural differences influence how models approach logical problems, with larger models potentially developing more sophisticated reasoning patterns through their increased parameter space.

## Foundational Learning

**Propositional Logic**: Formal system of reasoning using propositions and logical connectives (AND, OR, NOT, IF-THEN). *Why needed*: Forms the basis for evaluating deductive reasoning validity. *Quick check*: Can verify if conclusions logically follow from premises using truth tables.

**Chain of Thought Prompting**: Technique that encourages models to generate intermediate reasoning steps before answering. *Why needed*: Reveals the model's reasoning process rather than just final answers. *Quick check*: Compare reasoning chains with and without explicit prompting instructions.

**Strategy Classification**: Systematic categorization of reasoning approaches used to solve problems. *Why needed*: Enables comparison between human and machine reasoning patterns. *Quick check*: Verify consistent annotation across multiple raters using inter-rater reliability metrics.

## Architecture Onboarding

**Component Map**: Task input → Prompt template → LLM generation → Sampling parameters → Output reasoning chain → Manual annotation → Strategy classification

**Critical Path**: Prompt construction → Model generation → Intermediate step extraction → Strategy identification → Soundness evaluation → Accuracy measurement

**Design Tradeoffs**: Zero-shot prompting avoids few-shot bias but may limit reasoning depth; manual annotation provides nuanced analysis but introduces subjectivity

**Failure Signatures**: 
- Correct answers from invalid reasoning chains
- Misinterpretation of logical negations
- Inconsistent strategy application across similar problems

**First Experiments**:
1. Test prompt variations ("Let's think step by step" vs. alternative phrasings)
2. Compare reasoning strategies on problems with different complexity levels
3. Evaluate the impact of temperature variations on reasoning chain quality

## Open Questions the Paper Calls Out

**Open Question 1**: How does the inclusion of vision-language models affect the frequency of incremental diagram strategy usage compared to text-only LLMs? The paper notes humans use incremental diagram strategy frequently, but this was not observed in text-only LLMs. Vision-language models could potentially enable diagrammatic reasoning similar to humans through comparative experiments.

**Open Question 2**: How does problem complexity (number of premises, logical connectives, or sequential vs. non-sequential arrangement) influence the effectiveness of different inferential strategies? The study used fixed problems without systematically varying complexity parameters, yet observed varying performance. Controlled experiments manipulating complexity factors while measuring strategy effectiveness would resolve this.

**Open Question 3**: What internal mechanisms drive LLMs to select particular inferential strategies, and can these be identified through mechanistic interpretability? The paper offers only behavioral analysis without exploring internal mechanisms. Mechanistic studies using activation patching, probing, or circuit analysis could identify how different strategies are implemented internally.

## Limitations
- Manual annotation introduces potential subjectivity despite good inter-rater reliability
- Sample size of 12 propositional logic problems may not capture full breadth of deductive reasoning strategies
- Zero-shot prompting without exploring how few-shot examples might influence reasoning patterns

## Confidence
- **High confidence**: Identification of three distinct inferential strategies and their frequency differences across model families
- **Medium confidence**: Claim that LLMs employ strategies similar to humans but with varying frequencies
- **Medium confidence**: Observation that correctness does not necessarily indicate sound reasoning

## Next Checks
1. Replicate the study with a larger set of propositional logic problems to test generalizability of findings
2. Implement automated detection of reasoning strategies using fine-tuned classifiers to reduce annotation subjectivity
3. Compare zero-shot results with few-shot prompting using worked examples to assess impact on reasoning strategy selection