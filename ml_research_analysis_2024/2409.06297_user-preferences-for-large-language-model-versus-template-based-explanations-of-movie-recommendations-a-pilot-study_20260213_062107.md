---
ver: rpa2
title: 'User Preferences for Large Language Model versus Template-Based Explanations
  of Movie Recommendations: A Pilot Study'
arxiv_id: '2409.06297'
source_url: https://arxiv.org/abs/2409.06297
tags:
- explanations
- user
- explanation
- which
- research
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates the effectiveness of large language models
  (LLMs) for generating explanations of movie recommendations, compared to traditional
  template-based approaches. The research addresses the problem of opaque recommendation
  systems and the need for more engaging and understandable explanations for users.
---

# User Preferences for Large Language Model versus Template-Based Explanations of Movie Recommendations: A Pilot Study

## Quick Facts
- arXiv ID: 2409.06297
- Source URL: https://arxiv.org/abs/2409.06297
- Reference count: 33
- Key outcome: LLM-based explanations may provide richer, more engaging user experiences compared to template-based approaches for movie recommendations

## Executive Summary
This pilot study investigates user preferences between large language model (LLM) and template-based explanations for movie recommendations. The research separates recommendation and explanation processes, using graph-based methods (Personalized PageRank, RippleNet) to generate recommendations with interpretable paths, then applying either templates or LLMs to create textual explanations. A user evaluation with 25 participants assessed three explanation types against seven explanatory goals and subjective properties. Preliminary findings suggest LLM-based explanations offer more engaging and detailed experiences that better align with user expectations.

## Method Summary
The study implements a pipeline separating recommendation and explanation processes using graph-based methods on the Movielens-1M dataset. Three explanation types were evaluated: template-based explanations from graph paths, LLM-based rephrasing of template outputs, and purely LLM-based graph-to-text explanations using models like Llama 2 70B Chat and GPT-4. Twenty-five participants assessed explanations based on seven explanatory goals including effectiveness, efficiency, persuasiveness, satisfaction, scrutability, transparency, and trust, along with subjective properties.

## Key Results
- LLM-based explanations were perceived as more detailed and pleasant to read compared to template-based approaches
- The separation of recommendation and explanation processes enabled cleaner evaluation of LLM versus template methods
- Graph-based recommendation methods provided interpretable paths that could be converted to textual explanations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating recommendation and explanation processes allows cleaner evaluation of LLM-based explanations versus template-based baselines
- Mechanism: The pipeline first generates recommendations using classic methods (Personalized PageRank, RippleNet), then generates explanations via either templates or LLMs. This isolation ensures valid recommendations aren't compromised by LLM hallucinations while allowing direct comparison of explanation quality
- Core assumption: Template-based explanations provide a consistent, verifiable baseline against which LLM-generated explanations can be fairly compared
- Evidence anchors:
  - [abstract] "The most important design choice is to separate the recommendation and explanation processes, only using LLMs to explain items previously recommended by a standalone recommendation method."
  - [section] "As a baseline, we use a template-based approach, which transcripts path-based explanations into text. We compare this baseline method to LLM-based methods for generating explanations."

### Mechanism 2
- Claim: LLM-based explanations provide richer, more engaging user experiences compared to template-based explanations
- Mechanism: LLMs generate explanations in natural language that incorporate contextual information and can elaborate beyond the basic template structure, potentially creating more satisfying explanations for users
- Core assumption: Users prefer explanations that feel personalized and detailed rather than mechanical and formulaic
- Evidence anchors:
  - [abstract] "preliminary findings suggest that LLM-based explanations may provide a richer and more engaging user experience, further aligning with user expectations."
  - [section] "According to the participants, this explanation type is mainly preferred because it's often more detailed and more pleasant to read."

### Mechanism 3
- Claim: Graph-based recommendation methods provide interpretable explanation paths that can be converted to natural language
- Mechanism: Methods like Personalized PageRank and RippleNet generate recommendations with explanation paths through a user-item interaction graph, which can be converted to textual explanations via templates or LLMs
- Core assumption: Graph paths contain meaningful associations that users can understand when translated to natural language
- Evidence anchors:
  - [abstract] "While some systems employ a graph-based approach, offering inherent explainability through paths associating recommended items and seed items"
  - [section] "Regarding the recommendation methods, we focused on graph-based methods... Those methods also provide explanations for recommendations in the form of paths from the seed items to the recommended ones."

## Foundational Learning

- Concept: Recommender Systems and Collaborative Filtering
  - Why needed here: Understanding how recommendations are generated is crucial for evaluating explanation methods and their effectiveness
  - Quick check question: What is the difference between content-based and collaborative filtering approaches in recommender systems?

- Concept: Graph Neural Networks and Knowledge Graphs
  - Why needed here: The study uses graph-based recommendation methods that leverage knowledge graphs to enhance recommendations and provide explanation paths
  - Quick check question: How do knowledge graphs enhance traditional collaborative filtering approaches?

- Concept: Large Language Models and Prompt Engineering
  - Why needed here: The study compares template-based explanations with LLM-generated explanations, requiring understanding of how LLMs can be effectively prompted for explanation tasks
  - Quick check question: What are the key considerations when designing prompts for LLMs to generate explanations?

## Architecture Onboarding

- Component map:
  - User preference input → Recommendation engine (Personalized PageRank/RippleNet) → Graph-based recommendations → Explanation generator (template-based or LLM-based) → User interface with multiple explanation options

- Critical path:
  1. User provides movie preferences
  2. Recommendation engine generates item recommendations
  3. Explanation paths are extracted from the recommendation process
  4. Explanations are generated using either template or LLM approach
  5. User evaluates explanations based on seven explanatory goals

- Design tradeoffs:
  - Template-based vs. LLM-based explanations: Consistency vs. richness
  - Model size vs. computational efficiency: Larger models may generate better explanations but require more resources
  - Personalization depth vs. generalization: More personalized explanations may be more engaging but harder to scale

- Failure signatures:
  - LLM hallucinations introducing incorrect information
  - Template-based explanations appearing too mechanical or impersonal
  - Users unable to understand explanation paths even when converted to natural language
  - High variance in user preferences making it difficult to draw conclusions

- First 3 experiments:
  1. Implement template-based explanation generation from graph paths and verify consistency
  2. Implement LLM-based rephrasing of template explanations and compare output quality
  3. Implement LLM-based graph-to-text explanations and conduct small-scale user evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different sizes of large language models (LLMs) affect the quality and efficiency of generated explanations for movie recommendations?
- Basis in paper: [explicit] The authors mention their intention to explore various models, particularly smaller LLMs, to understand how model size affects efficiency in the future works section
- Why unresolved: The current study only uses Llama 2 70B Chat and does not compare it with smaller models, leaving the impact of model size on explanation quality and efficiency unexplored
- What evidence would resolve it: Comparative studies using different LLM sizes (e.g., 7B, 13B, 70B parameters) to evaluate explanation quality, user satisfaction, and computational efficiency

### Open Question 2
- Question: To what extent do LLM-generated explanations introduce unverified details, and how does this impact user trust and satisfaction?
- Basis in paper: [explicit] The authors observe that LLMs often introduce additional details not present in the knowledge graph, which may be due to the model's ability to draw on cultural references, and note this as a potential issue
- Why unresolved: The study identifies the introduction of unverified details as a concern but does not assess the impact on user trust or satisfaction, nor does it explore methods to mitigate this issue
- What evidence would resolve it: User studies measuring trust and satisfaction when exposed to explanations with and without unverified details, and experiments testing methods to limit LLM outputs to in-context information

### Open Question 3
- Question: How do different prompting techniques, such as chain-of-thought and self-consistency, influence the quality and detail of explanations generated by LLMs?
- Basis in paper: [explicit] The authors mention planning to investigate advanced prompting techniques like chain-of-thought and self-consistency in future works, suggesting these techniques may affect the detail of reasoning in explanations
- Why unresolved: The current study uses basic greedy decoding without exploring how advanced prompting techniques might improve or alter the explanations
- What evidence would resolve it: Comparative evaluations of explanations generated using different prompting techniques, assessing the clarity, detail, and user satisfaction of the explanations

## Limitations
- Small sample size (n=25) may not provide sufficient statistical power for definitive conclusions
- Evaluation focused primarily on subjective properties without measuring objective metrics like explanation accuracy
- Limited specification of LLM prompting strategies and model configurations

## Confidence

- High Confidence: The separation of recommendation and explanation processes as a valid experimental design (Mechanism 1) is well-established in the literature
- Medium Confidence: The claim that LLM-based explanations provide richer and more engaging user experiences (Mechanism 2) is supported by user feedback but requires larger-scale validation
- Low Confidence: The assumption that graph-based recommendation methods provide interpretable explanation paths (Mechanism 3) is mentioned but not empirically validated in the study itself

## Next Checks

1. Scale up user evaluation: Conduct the user study with a larger and more diverse participant pool (n > 100) to improve statistical reliability and generalizability of findings
2. Measure objective metrics: Evaluate explanations based on accuracy, consistency, and computational efficiency in addition to subjective user preferences to provide a more comprehensive assessment
3. Test prompt engineering variations: Systematically test different prompting strategies for LLM-based explanations to identify optimal configurations and reduce variance in output quality