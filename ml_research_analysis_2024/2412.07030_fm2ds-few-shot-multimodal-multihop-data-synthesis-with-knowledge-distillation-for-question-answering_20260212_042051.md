---
ver: rpa2
title: 'FM2DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge Distillation
  for Question Answering'
arxiv_id: '2412.07030'
source_url: https://arxiv.org/abs/2412.07030
tags:
- data
- answer
- question
- documents
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FM2DS is a framework that automatically synthesizes high-quality
  multimodal multihop question answering datasets from Wikipedia, requiring minimal
  human effort. It generates diverse, non-templated questions and answers that demand
  reasoning across multiple documents and modalities, validated through structured
  pipelines.
---

# FM2DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge Distillation for Question Answering

## Quick Facts
- arXiv ID: 2412.07030
- Source URL: https://arxiv.org/abs/2412.07030
- Authors: Amirhossein Abaskohi; Spandana Gella; Giuseppe Carenini; Issam H. Laradji
- Reference count: 33
- Models trained on FM2DS synthetic data outperform human-collected datasets by 1.9 exact match points on average

## Executive Summary
FM2DS is a framework that automatically synthesizes high-quality multimodal multihop question answering datasets from Wikipedia, requiring minimal human effort. It generates diverse, non-templated questions and answers that demand reasoning across multiple documents and modalities, validated through structured pipelines. Models trained on this synthetic data—often using as few as 5k samples—outperform those trained on human-collected datasets by 1.9 exact match points on average, while learning more efficiently. The approach supports domain adaptation and enables smaller models to perform complex reasoning through query-guided knowledge distillation. FM2DS also introduces M2QA-Bench, a 1k-sample benchmark for long-document MMQA, further advancing evaluation standards.

## Method Summary
FM2DS employs a five-stage pipeline: document acquisition from Wikipedia via WikiWeb2M dataset, document grouping using topic modeling and hyperlink connections, question generation with GPT-4o using 3-shot in-context examples, answer generation with image captioning support, and validation through NER, relation extraction, and hallucination checking. The framework then generates structured queries with GPT-4o validated using MuRAG-based methods, followed by knowledge distillation training on the structured query-answer format. The entire process is fully automated, using long documents rather than snippets and requiring only a few thousand samples to train effective models.

## Key Results
- Models trained on FM2DS synthetic data outperform those trained on human-collected datasets by 1.9 exact match points on average
- Smaller models (Llama-3.1-8B, Qwen2.5-7B) achieve strong performance on complex MMQA tasks when trained on synthetic data
- The framework demonstrates effective domain adaptation capabilities, with models transferring well across different MMQA datasets
- M2QA-Bench, a new 1k-sample benchmark for long-document MMQA, establishes new evaluation standards

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Query-guided knowledge distillation enables smaller models to learn complex multihop reasoning by providing structured retrieval steps.
- Mechanism: The framework generates structured queries that break down complex questions into sequential retrieval steps, mimicking the reasoning process of larger models. These queries guide the model to explicitly retrieve relevant information from multiple documents and modalities before generating the final answer.
- Core assumption: Structured reasoning steps improve learning efficiency and reduce hallucination compared to direct answer generation.
- Evidence anchors:
  - [section]: "Queries are essential for guiding models to use the provided context, as without explicit multimodal grounding they struggle to answer reliably despite strong pretrained knowledge."
  - [section]: "The teacher produces both structured queries (chain-of-thought style reasoning) and answers, which serve as distillation targets."
  - [corpus]: Weak evidence - no direct citations in the corpus linking query-guided distillation to performance improvements in MMQA.
- Break condition: If the generated queries fail to capture relevant information or introduce hallucinated reasoning steps, the distillation process would degrade rather than improve performance.

### Mechanism 2
- Claim: Few-shot synthetic data generation can produce high-quality MMQA samples that match or exceed human-annotated data.
- Mechanism: The framework uses 3-shot in-context examples to guide the generation of non-templated, multihop questions that require reasoning across multiple documents and modalities. Automated validation ensures quality through named entity alignment, relation consistency, and hallucination checks.
- Core assumption: Large language models can generate complex MMQA samples that are indistinguishable from or better than human-created ones when properly guided and validated.
- Evidence anchors:
  - [abstract]: "Models trained on this synthetic data—often using as few as 5k samples—outperform those trained on human-collected datasets by 1.9 exact match points on average"
  - [section]: "models trained on FM2DS data outperform those trained on original datasets, despite using longer documents"
  - [corpus]: Weak evidence - the related papers focus on synthetic generation but don't directly compare to human-annotated data quality.
- Break condition: If the validation steps fail to catch hallucinations or generate questions that don't truly require multihop reasoning, the synthetic data quality would degrade significantly.

### Mechanism 3
- Claim: Using full Wikipedia documents rather than snippets creates more challenging and generalizable MMQA tasks.
- Mechanism: The framework leverages Wikipedia's hyperlink structure and topical relationships to create document pairs with shared thematic relevance, then generates questions requiring information synthesis across complete documents rather than isolated snippets.
- Core assumption: Longer context with multiple related documents creates more realistic and challenging reasoning scenarios than short, isolated snippets.
- Evidence anchors:
  - [abstract]: "Unlike traditional datasets that rely on human annotators, templates, or snippets, FM2DS is fully automated, using long documents as sources"
  - [section]: "Our approach synthesizes MMQA data from documents that are interconnected through various relationships, such as thematic similarities or sequential events"
  - [corpus]: Weak evidence - the corpus mentions similar approaches but doesn't directly validate the superiority of full-document vs. snippet-based data.
- Break condition: If the document selection process fails to create meaningful relationships or the questions don't truly require cross-document reasoning, the advantage of full documents would be negated.

## Foundational Learning

- Concept: Multimodal reasoning across text, images, and tables
  - Why needed here: MMQA requires integrating information from multiple modalities to answer complex questions
  - Quick check question: Can you explain how a model would combine information from a table and an image to answer a question about historical events?

- Concept: Multihop reasoning across multiple documents
  - Why needed here: Questions require synthesizing information from multiple sources rather than single documents
  - Quick check question: Given two Wikipedia articles about related topics, how would you determine if a question requires information from both?

- Concept: Knowledge distillation and structured query generation
  - Why needed here: Smaller models need to learn complex reasoning patterns from larger models through explicit intermediate steps
  - Quick check question: How would you design a query that breaks down a complex question into sequential retrieval steps?

## Architecture Onboarding

- Component map: Document acquisition -> Document grouping -> Question generation -> Answer generation -> Validation pipeline -> Query generation -> Knowledge distillation

- Critical path: Document grouping → Question generation → Answer generation → Query generation → Validation → Knowledge distillation

- Design tradeoffs:
  - Using GPT-4o throughout provides high quality but increases cost
  - Full document approach increases complexity but improves realism
  - Structured queries add training overhead but improve reasoning
  - Few-shot generation reduces annotation but may limit diversity

- Failure signatures:
  - Low validation pass rates indicate generation quality issues
  - Hallucinations in answers suggest validation pipeline weaknesses
  - Poor cross-dataset performance indicates overfitting to synthetic patterns
  - High hallucination rates in downstream models suggest distillation problems

- First 3 experiments:
  1. Generate 100 samples with different in-context shot counts (0, 1, 3) to find optimal generation quality
  2. Validate sample quality with human evaluation on a subset to measure hallucination rates
  3. Train a small model on 1k synthetic samples and evaluate on MMQA benchmark to test initial effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the FM2DS framework perform when applied to multilingual documents and non-English question-answering datasets?
- Basis in paper: [inferred]
- Why unresolved: The paper focuses on English Wikipedia data and English benchmarks (MultiModalQA, WebQA, M2QA-Bench). It does not explore cross-lingual synthesis or evaluate performance on non-English datasets, despite mentioning future plans to expand to multilingual sources.
- What evidence would resolve it: Experimental results showing FM2DS performance on multilingual datasets (e.g., Japanese Wikipedia, multilingual QA benchmarks) and comparison with existing multilingual QA models.

### Open Question 2
- Question: What is the impact of increasing the number of in-context examples beyond three in the FM2DS pipeline, and does it continue to yield diminishing returns or eventually plateau?
- Basis in paper: [explicit]
- Why unresolved: The paper reports that performance gains from increasing in-context examples diminish after three samples, attributing this to context window limitations. However, it does not test higher numbers (e.g., five or more) to determine if a plateau effect exists or if gains resume with larger models/context windows.
- What evidence would resolve it: Systematic evaluation of model performance with varying numbers of in-context examples (e.g., 1, 2, 3, 5, 10) across different model sizes and tasks to identify the optimal number and potential plateau points.

### Open Question 3
- Question: How does the hallucination rate and factual accuracy of FM2DS-generated data compare to human-annotated datasets when evaluated by human annotators on a large scale?
- Basis in paper: [explicit]
- Why unresolved: While the paper includes a small-scale human evaluation (12 evaluators on 100 samples) showing that answer validation reduces hallucinations, it does not provide large-scale human validation of the entire synthesized dataset. The automated validation steps may miss subtle factual inaccuracies.
- What evidence would resolve it: Large-scale human annotation study (e.g., 1000+ samples) comparing factual accuracy and hallucination rates between FM2DS-generated data and human-annotated datasets using consistent evaluation criteria.

## Limitations

- The framework's reliance on GPT-4o for both generation and validation creates potential circular validation issues, as the same model serves as judge and creator
- The claim that synthetic data outperforms human-collected datasets by 1.9 EM points requires further validation across diverse domains and question types
- The assertion that query-guided knowledge distillation is the primary mechanism enabling smaller models to learn complex reasoning needs more ablation studies

## Confidence

- **High confidence**: The technical implementation of the five-stage pipeline and its ability to generate diverse, non-templated questions requiring multihop reasoning across multiple modalities
- **Medium confidence**: The claim that models trained on synthetic data consistently outperform those trained on human-collected datasets across all settings
- **Low confidence**: The assertion that query-guided knowledge distillation is the primary mechanism enabling smaller models to learn complex reasoning, as this requires more ablation studies

## Next Checks

1. Conduct a human evaluation study comparing sample quality between FM2DS-generated data and human-annotated datasets across multiple dimensions (relevance, complexity, multimodality)
2. Perform ablation studies removing query-guided knowledge distillation to quantify its contribution to model performance improvements
3. Test cross-domain generalization by training on synthetic Wikipedia data and evaluating on domain-specific MMQA benchmarks to assess real-world applicability