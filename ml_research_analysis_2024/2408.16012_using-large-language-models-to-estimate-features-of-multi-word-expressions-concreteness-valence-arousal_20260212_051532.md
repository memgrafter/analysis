---
ver: rpa2
title: 'Using large language models to estimate features of multi-word expressions:
  Concreteness, valence, arousal'
arxiv_id: '2408.16012'
source_url: https://arxiv.org/abs/2408.16012
tags:
- estimates
- ratings
- words
- expressions
- valence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated the ability of large language models to
  estimate concreteness, valence, and arousal for multi-word expressions. ChatGPT-4o
  showed strong correlations with human concreteness ratings (r = .8) for multi-word
  expressions, and valence and arousal ratings for individual words, matching or outperforming
  previous AI models.
---

# Using large language models to estimate features of multi-word expressions: Concreteness, valence, arousal

## Quick Facts
- arXiv ID: 2408.16012
- Source URL: https://arxiv.org/abs/2408.16012
- Reference count: 11
- ChatGPT-4o shows strong correlations (r = .8) with human concreteness ratings for multi-word expressions

## Executive Summary
This study investigates the ability of large language models to estimate psycholinguistic features for multi-word expressions. ChatGPT-4o demonstrates strong correlations with human ratings for concreteness (r = .8) in multi-word expressions and comparable performance for valence and arousal in individual words. The findings suggest LLMs can serve as valuable tools for generating psycholinguistic data, particularly for multi-word expressions where traditional methods are limited. The authors provide extensive datasets containing AI-generated ratings for over 126,000 words and 63,000 multi-word expressions.

## Method Summary
The study used ChatGPT-4o API to estimate concreteness, valence, and arousal ratings for English words and multi-word expressions. Researchers sent API requests with specific prompts at temperature=0, collecting both the dominant rating and probability distribution for each expression. They calculated correlation coefficients between AI estimates and human ratings from established datasets, comparing different methods including probability-weighted ratings and rank-based approaches. The results were validated against existing human ratings datasets and stored in structured Excel files.

## Key Results
- ChatGPT-4o achieved r = .8 correlation with human concreteness ratings for multi-word expressions
- Probability-weighted ratings from LLM probability distributions provided slightly better estimates than mode ratings alone
- Rank-based conversion maintained high correlations when distributions differed between human and AI ratings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can estimate concreteness, valence, and arousal for multiword expressions by treating them as unified semantic units.
- Mechanism: The model processes the full expression as input, not just individual words, and outputs a rating on the requested scale. The high correlations (r = .8 for concreteness, comparable for valence/arousal) suggest the LLM captures the meaning of the phrase holistically.
- Core assumption: The LLM's internal representations encode enough contextual and compositional information to approximate human ratings.
- Evidence anchors:
  - [abstract]: "ChatGPT-4o showed strong correlations with human concreteness ratings (r = .8) for multi-word expressions."
  - [section]: "ChatGPT-4o gives good estimates and that adding a few examples of words at each end of the continuum slightly improved the results."
  - [corpus]: Weak/no direct match; this is an inference from the paper's results.
- Break condition: The model fails when idioms or multiword expressions have meanings that are not predictable from the individual words (e.g., opaque idioms like "a golden key can open any door").

### Mechanism 2
- Claim: Using probability-weighted ratings from the LLM provides more accurate estimates than the mode rating alone.
- Mechanism: The LLM outputs both a top rating and probabilities for each possible rating. Multiplying each rating by its probability and summing gives a weighted average, which correlates slightly better with human ratings than just taking the top rating.
- Core assumption: The probability distribution reflects the model's confidence and uncertainty in a meaningful way.
- Evidence anchors:
  - [section]: "The sum of the ratings times the probabilities gave slightly more information than the rating with the highest probability."
  - [corpus]: No direct evidence; this is based on the paper's methodological choice.
- Break condition: If the probability distribution is uniform or uninformative, the weighted average may not improve over the mode.

### Mechanism 3
- Claim: Ranks of LLM estimates correlate almost as well as raw values, making rank-based selection a viable alternative when distributions differ.
- Mechanism: Human ratings and LLM estimates may have different distributions (e.g., LLM estimates are more extreme). Converting to ranks (relative or 1-100 scale) aligns the two sets of data, preserving the ordering and yielding high Spearman correlations.
- Core assumption: The relative ordering of items is what matters for many research tasks (e.g., selecting stimuli).
- Evidence anchors:
  - [section]: "Therefore, a better way to use LLM ratings as an alternative for human ratings may be to use rank scores rather than raw scores. Indeed, the Spearman correlation between the two variables is close to the Pearson correlation."
  - [corpus]: No direct evidence; this is an inference from the paper's analysis.
- Break condition: If the goal requires absolute values (not just ranking), using ranks is not appropriate.

## Foundational Learning

- Concept: Concreteness, valence, and arousal as psycholinguistic variables.
  - Why needed here: The study's goal is to estimate these variables for multiword expressions using LLMs.
  - Quick check question: What is the difference between valence and arousal in the context of word meaning?

- Concept: Large language models and their output format (top rating + probability distribution).
  - Why needed here: The method relies on both the top rating and the probability distribution to generate estimates.
  - Quick check question: What does the "temperature" setting control in LLM API calls?

- Concept: Spearman vs. Pearson correlation.
  - Why needed here: The paper uses both to evaluate the quality of LLM estimates, especially when distributions differ.
  - Quick check question: When would you prefer Spearman correlation over Pearson correlation?

## Architecture Onboarding

- Component map:
  - LLM API (ChatGPT-4o) -> Prompt processing -> Rating output (top + probabilities) -> Data storage (Excel files)
  - Input: Multiword expressions (63,680 multiword, 126,397 single words)
  - Output: Four columns per variable (raw rating, probability-weighted rating, relative rank, 1-100 rank)

- Critical path:
  1. Prepare list of expressions to rate
  2. Send API call with appropriate prompt and temperature=0
  3. Parse top rating and probability distribution
  4. Compute probability-weighted rating
  5. Rank expressions within the dataset
  6. Export results to Excel

- Design tradeoffs:
  - Using temperature=0 ensures reproducibility but may miss diverse outputs.
  - Including examples in prompts improves correlations but adds verbosity.
  - Storing ranks allows for easier stimulus selection but loses absolute values.

- Failure signatures:
  - Low correlations with human ratings -> prompts may be ineffective or LLM's representations are inadequate.
  - Uninformative probability distributions -> model may be uncertain or unable to rate the expression.
  - Rank correlations much lower than Pearson -> distributions are mismatched and rank conversion may not help.

- First 3 experiments:
  1. Replicate Study 1: Use ChatGPT-4o to rate a small sample of multiword expressions from Muraki et al. (2023) with temperature=0 and compare to human ratings.
  2. Test prompt variations: Compare correlations when using different prompt instructions (with/without examples, different scales).
  3. Validate probability weighting: For a subset, compare Pearson correlations of mode rating vs. probability-weighted rating vs. human ratings.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well do LLM-based valence and arousal estimates generalize across different languages?
- Basis in paper: [explicit] The paper mentions that ChatGPT-4o is available for many languages but notes that outcomes are not as good as in English.
- Why unresolved: The study only tested English, and while the model is available in other languages, the quality of estimates in those languages is not well-established.
- What evidence would resolve it: Conducting similar studies in multiple languages and comparing the correlations between LLM estimates and human ratings across those languages.

### Open Question 2
- Question: What is the impact of using ranks versus raw scores from LLM estimates in psycholinguistic research?
- Basis in paper: [explicit] The paper suggests that using rank scores may be better than raw scores for some purposes, as the distributions of AI estimates differ from human ratings.
- Why unresolved: The paper does not provide empirical evidence on whether using ranks improves research outcomes compared to using raw scores.
- What evidence would resolve it: Conducting studies that compare research outcomes using LLM estimates in both raw and ranked forms.

### Open Question 3
- Question: How do LLM estimates of multiword expressions compare to human ratings for rare or highly context-dependent expressions?
- Basis in paper: [explicit] The paper mentions that ChatGPT-4o may have difficulties with idioms and rare expressions, but does not provide detailed analysis for these cases.
- Why unresolved: The study did not extensively test the model's performance on rare or highly context-dependent expressions.
- What evidence would resolve it: Collecting human ratings for a large set of rare and context-dependent multiword expressions and comparing them to LLM estimates.

## Limitations

- The study relies on correlations with existing human ratings datasets, which may have their own limitations in representativeness and scale
- The effectiveness of probability-weighted ratings versus mode ratings was only tested implicitly through slightly improved correlations
- The treatment of idioms and multiword expressions with non-compositional meanings is acknowledged as a potential failure mode but not extensively tested

## Confidence

- High confidence: The overall finding that ChatGPT-4o can estimate concreteness, valence, and arousal ratings for multiword expressions is well-supported by the strong correlations reported (r = .8 for concreteness)
- Medium confidence: The specific mechanisms by which LLMs process multiword expressions holistically versus compositionally remain unclear, particularly for idioms with opaque meanings
- Medium confidence: The generalizability of these findings to other languages or to newer LLM models beyond ChatGPT-4o is uncertain

## Next Checks

1. **Prompt Engineering Validation**: Systematically test variations in prompt wording, temperature settings, and inclusion of examples to determine the optimal configuration for estimating psycholinguistic variables

2. **Cross-Dataset Verification**: Apply the LLM rating method to a new dataset of multiword expressions not included in the original validation sets to test generalizability

3. **Comparison with Alternative Models**: Benchmark ChatGPT-4o's performance against other LLMs (GPT-3.5, Claude, etc.) and traditional AI models to establish relative effectiveness for this task