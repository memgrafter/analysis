---
ver: rpa2
title: Are Generative Language Models Multicultural? A Study on Hausa Culture and
  Emotions using ChatGPT
arxiv_id: '2406.19504'
source_url: https://arxiv.org/abs/2406.19504
tags:
- responses
- chatgpt
- your
- feel
- would
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether ChatGPT accurately represents Hausa
  culture and emotions, a low-resource language spoken by over 100 million people.
  Researchers compared ChatGPT responses to those of native Hausa speakers on 37 culturally
  relevant questions.
---

# Are Generative Language Models Multicultural? A Study on Hausa Culture and Emotions using ChatGPT

## Quick Facts
- arXiv ID: 2406.19504
- Source URL: https://arxiv.org/abs/2406.19504
- Reference count: 40
- Primary result: ChatGPT responses show semantic similarity to Hausa speakers but exhibit emotional mismatches and cultural biases

## Executive Summary
This study investigates whether ChatGPT accurately represents Hausa culture and emotions, a low-resource language spoken by over 100 million people. Researchers compared ChatGPT responses to those of native Hausa speakers on 37 culturally relevant questions using emotion analysis and similarity metrics (BERTScore, METEOR). While ChatGPT responses showed some semantic similarity to human responses, they exhibited emotional mismatches and cultural biases. Human participants rated ChatGPT responses as less culturally aligned compared to their own. The study highlights the need for improved cultural awareness in LLMs for low-resource languages, suggesting better training data, human feedback integration, and refined evaluation metrics.

## Method Summary
The study used 37 culturally relevant prompts (18 from Havaldar et al. 2023, 19 crafted using literature on African cultures and emotions). Responses were collected from 18 native Hausa speakers and ChatGPT for identical prompts. Analysis included emotion classification (positive, negative, compound, neutral), semantic similarity using BERTScore and METEOR metrics, and human participant ratings on a Likert scale (1-5). The study compared emotional diversity, semantic similarity, and cultural alignment between human and ChatGPT responses.

## Key Results
- ChatGPT responses showed semantic similarity to human responses but exhibited emotional mismatches and cultural biases
- Human responses displayed richer emotional diversity (positive, negative, compound, neutral) compared to ChatGPT's predominantly neutral or compound responses
- Human participants rated ChatGPT responses as less culturally aligned than their own responses
- BERTScore showed high semantic similarity while METEOR showed lower similarity, suggesting cultural misalignment in word choice and phrasing

## Why This Works (Mechanism)

### Mechanism 1
ChatGPT's responses align with human responses on some semantic level but miss cultural authenticity due to lack of real-world cultural grounding. ChatGPT relies on internet-scale training data biased toward high-resource languages and cultures. Without lived experience or culturally embedded learning, it can mimic semantic structure but not cultural nuance.

### Mechanism 2
Emotion analysis reveals a mismatch in emotional diversity between human and ChatGPT responses. Human responses exhibit a broader emotional spectrum while ChatGPT responses are predominantly neutral or compound. This indicates that ChatGPT fails to capture the full range of culturally appropriate emotional expressions.

### Mechanism 3
Lower METEOR scores compared to BERTScore indicate that while semantic similarity is high, word choice and phrasing do not align with cultural norms. BERTScore captures semantic similarity but ignores surface-level linguistic features, while METEOR evaluates unigram matching, stemming, and synonymy, revealing that ChatGPT's word choices deviate from culturally appropriate phrasing even when meaning is similar.

## Foundational Learning

- **Cultural grounding in language models**
  - Why needed: The study highlights that LLMs like ChatGPT lack the experiential and cultural background that humans use to generate culturally appropriate responses
  - Quick check: Can an LLM generate culturally authentic responses without being trained on culturally diverse and representative data?

- **Emotion analysis and its limitations**
  - Why needed: The study uses emotion analysis to compare human and ChatGPT responses, revealing emotional mismatches
  - Quick check: How might the results change if the emotion classifier were trained on Hausa-specific emotional expressions?

- **Evaluation metrics for text generation**
  - Why needed: The study uses BERTScore and METEOR to compare semantic and surface-level similarity, respectively
  - Quick check: Why might BERTScore show high similarity while METEOR shows lower similarity for the same pair of responses?

## Architecture Onboarding

- **Component map**: Data collection -> Analysis pipeline -> Evaluation
- **Critical path**: 
  1. Generate prompts and collect human responses
  2. Generate ChatGPT responses
  3. Perform emotion analysis on all responses
  4. Calculate BERTScore and METEOR similarity scores
  5. Collect human ratings on ChatGPT responses
  6. Analyze and interpret results
- **Design tradeoffs**: Single ChatGPT response per prompt vs. multiple responses; general-purpose emotion classifier vs. Hausa-specific one; standard metrics vs. culturally tuned evaluation
- **Failure signatures**: High BERTScore but low METEOR indicates semantic similarity without cultural phrasing alignment; predominantly neutral emotions in ChatGPT indicates lack of emotional diversity; low human ratings indicate cultural misalignment
- **First 3 experiments**:
  1. Generate multiple ChatGPT responses per prompt and compare their similarity to human responses
  2. Train a Hausa-specific emotion classifier and re-run emotion analysis
  3. Develop a culturally tuned evaluation metric and compare its results to BERTScore and METEOR

## Open Questions the Paper Calls Out

### Open Question 1
How can we develop more effective evaluation metrics for assessing cultural alignment in LLM-generated text beyond similarity-based approaches like BERTScore and METEOR? The paper states that evaluating open-ended text poses limitations and suggests proposing evaluation metrics that can measure various aspects of natural language considering coherence, relevance, fluency, or sentiment.

### Open Question 2
What is the impact of increasing the diversity and quantity of training data for low-resource languages on LLM performance in culturally sensitive contexts? The paper discusses the importance of training data quality, stating that LLMs lack background knowledge and suggesting using more diverse and representative data.

### Open Question 3
How can human feedback be effectively integrated into the fine-tuning process of LLMs for low-resource languages to improve cultural awareness? The paper suggests incorporating human feedback and perspective that can improve the model's performance through techniques like user testing, surveys, interviews, or focus groups.

## Limitations
- Reliance on a single ChatGPT response per prompt may not capture the model's full variability in cultural responses
- Emotion analysis depends on a general-purpose classifier rather than one specifically trained on Hausa emotional expressions
- METEOR metric's focus on surface-level linguistic features may not adequately capture deeper cultural meanings

## Confidence
- **High Confidence**: ChatGPT