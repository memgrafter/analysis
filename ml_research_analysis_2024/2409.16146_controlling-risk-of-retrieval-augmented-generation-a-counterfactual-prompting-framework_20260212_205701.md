---
ver: rpa2
title: 'Controlling Risk of Retrieval-augmented Generation: A Counterfactual Prompting
  Framework'
arxiv_id: '2409.16146'
source_url: https://arxiv.org/abs/2409.16146
tags:
- answer
- risk
- arxiv
- language
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of controlling the risk of retrieval-augmented
  generation (RAG) by enabling models to proactively refuse low-confidence answers.
  The proposed counterfactual prompting framework induces the model to imagine scenarios
  of poor retrieval quality or improper usage, then assesses confidence based on how
  these factors affect the answer.
---

# Controlling Risk of Retrieval-augmented Generation: A Counterfactual Prompting Framework

## Quick Facts
- arXiv ID: 2409.16146
- Source URL: https://arxiv.org/abs/2409.16146
- Authors: Lu Chen; Ruqing Zhang; Jiafeng Guo; Yixing Fan; Xueqi Cheng
- Reference count: 21
- Key outcome: The counterfactual prompting framework reduces risk by up to 2.88% and increases carefulness by up to 14.76% compared to baselines, with 3 out of 4 settings outperforming in these metrics.

## Executive Summary
This paper addresses the critical problem of controlling risk in retrieval-augmented generation (RAG) systems by enabling models to proactively refuse low-confidence answers. The authors propose a counterfactual prompting framework that induces the model to imagine scenarios of poor retrieval quality or improper usage, then assesses confidence based on how these factors affect the answer. Experimental results on two datasets demonstrate that the method effectively reduces risk and increases carefulness compared to baselines, though performance gains vary depending on the specific LLM and retriever combination used.

## Method Summary
The counterfactual prompting framework consists of three modules: prompting generation, judgment, and fusion. The prompting generation module creates counterfactual scenarios that challenge the quality and usage of retrieved results. The judgment module compares answers regenerated under these scenarios with the initial RAG output to assess confidence. The fusion module aggregates these judgments using either direct selection or probability comparison strategies. The framework is evaluated on two QA datasets (Natural Questions and TriviaQA) using both dense and sparse retrievers with two different LLM backbones (Mistral and ChatGPT), measuring risk, carefulness, alignment, and coverage metrics.

## Key Results
- Risk reduction of up to 2.88% compared to baselines
- Carefulness increase of up to 14.76% compared to baselines
- 3 out of 4 experimental settings outperformed baselines in risk and carefulness metrics
- Performance varies significantly with LLM strength, with weaker models showing better risk control

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Counterfactual prompting induces models to assess confidence by simulating scenarios where retrieved results are poor or improperly used.
- Mechanism: The framework generates answers under two counterfactual scenarios—poor retrieval quality and improper usage—then compares these regenerated answers with the initial RAG output. If answers change across scenarios, the model recognizes uncertainty and discards the response.
- Core assumption: Large language models can recognize and react to counterfactual scenarios when prompted appropriately.
- Evidence anchors:
  - [abstract]: "we develop a counterfactual prompting framework that induces the models to alter these factors and analyzes the effect on their answers"
  - [section]: "we induce the model to imagine scenarios where the quality of the retrieved results and their usage are poor, then measure its confidence based on the effect of these imagined scenarios on the answers"
  - [corpus]: Weak - corpus provides related work on RAG risk control but doesn't directly address counterfactual prompting mechanisms
- Break condition: The model fails to recognize the counterfactual scenario or produces identical answers across all scenarios, indicating overconfidence that the prompting cannot overcome.

### Mechanism 2
- Claim: Risk control effectiveness depends on the relative strength of the LLM - weaker models show better risk control performance.
- Mechanism: Stronger LLMs (like Mistral) have higher confidence in both their internal knowledge and retrieved results, leading to lower discard rates. Weaker models (like ChatGPT in some comparisons) are more likely to recognize when retrieved results may be unreliable and discard accordingly.
- Core assumption: LLM confidence scales with model capability, creating an inverse relationship with risk control effectiveness.
- Evidence anchors:
  - [section]: "Risk control ability is dependent on the LLM ability. We compare the performance of RC-RAG when using different LLMs as generators. We find that risk control works better with ChatGPT than with Mistral."
  - [section]: "The underlying reason is that more capable models are more confident in both their internal knowledge and retrieved results."
  - [corpus]: Weak - corpus neighbors discuss RAG systems but don't address the relationship between model strength and risk control effectiveness
- Break condition: If model confidence calibration improves significantly, this inverse relationship may weaken or reverse.

### Mechanism 3
- Claim: The quality of retrieved results significantly impacts risk control effectiveness, with sparse retrieval leading to more cautious but riskier behavior.
- Mechanism: Sparse retrieval produces fewer but potentially higher-quality passages, making the model more sensitive to quality challenges. This leads to higher discard rates but also higher risk scores because more unanswerable samples are retained in the system.
- Core assumption: Retrieval quality directly affects model confidence assessment and subsequent judgment decisions.
- Evidence anchors:
  - [section]: "We compared the performance of risk control of RAG with different retrievers. Results are shown in Table 4, conducted on the RC-NQ test set using Mistral as a generator."
  - [section]: "The sparse retriever results in significantly more unanswerable samples than the dense retriever"
  - [corpus]: Weak - corpus provides related work on RAG but doesn't specifically address retrieval quality impact on risk control
- Break condition: If retrieval quality improves uniformly across both methods, the differences in risk control effectiveness may diminish.

## Foundational Learning

- Concept: Counterfactual thinking
  - Why needed here: The framework relies on models imagining alternative scenarios to assess confidence, which requires understanding how counterfactual reasoning works in LLMs
  - Quick check question: Can you explain how "what if" scenarios help models recognize their own uncertainty?

- Concept: Retrieval-augmented generation pipeline
  - Why needed here: Understanding the retrieve-then-generate process is essential for grasping why external knowledge quality affects confidence assessment
  - Quick check question: What are the two main stages of a typical RAG system and how do they interact?

- Concept: Confidence calibration in language models
  - Why needed here: The framework addresses overconfidence issues in RAG systems, requiring understanding of how LLMs typically miscalibrate their confidence
  - Quick check question: Why do language models tend to be overconfident in their predictions, especially in RAG settings?

## Architecture Onboarding

- Component map: Question -> Retrieval -> RAG Generation -> Counterfactual Prompting (Quality Scenario) -> Counterfactual Prompting (Usage Scenario) -> Judgment Comparison -> Fusion Decision -> Final Output
- Critical path: Question → Retrieval → RAG Generation → Counterfactual Prompting (Quality Scenario) → Counterfactual Prompting (Usage Scenario) → Judgment Comparison → Fusion Decision → Final Output
- Design tradeoffs:
  - Risk vs. Coverage: Higher risk control leads to more discards but potentially missing correct answers
  - Computational Cost vs. Accuracy: Multiple iterations improve accuracy but increase latency
  - Prompt Complexity vs. Model Capability: More sophisticated prompts may not work well with weaker models
- Failure signatures:
  - High risk scores with low carefulness: Model isn't effectively identifying unanswerable samples
  - Very low coverage: Model is overly cautious and discarding too many valid answers
  - Inconsistent judgments across scenarios: Fusion strategy may not be working properly
- First 3 experiments:
  1. Test baseline performance without counterfactual prompting to establish reference metrics
  2. Implement single counterfactual scenario (quality only) to measure isolated impact
  3. Test both scenarios with different fusion strategies to compare direct selection vs. probability comparison approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the computational efficiency of the counterfactual prompting framework be improved without sacrificing its effectiveness in risk control?
- Basis in paper: [inferred] The paper mentions that the prompting generation approach is computationally intensive and suggests the need for more efficient prompting strategies.
- Why unresolved: The paper acknowledges the computational intensity but does not provide specific solutions or strategies to address this limitation.
- What evidence would resolve it: Experiments comparing the performance of different prompting strategies (e.g., iterative vs. non-iterative, different fusion methods) on computational efficiency and risk control effectiveness.

### Open Question 2
- Question: What are the potential sources of risk in RAG beyond the quality and usage of retrieved results, and how can these be incorporated into the counterfactual prompting framework?
- Basis in paper: [explicit] The paper identifies two critical factors affecting RAG's confidence: the quality of retrieved results and the manner in which they are utilized. However, it acknowledges that these human-defined factors may not encompass the full spectrum of risk sources.
- Why unresolved: The paper suggests exploring more diverse factors identified by LLMs but does not provide a concrete approach or examples of such factors.
- What evidence would resolve it: Analysis of additional risk factors identified by LLMs, combined with statistical analysis, to expand the counterfactual prompting framework.

### Open Question 3
- Question: How can the counterfactual prompting framework be extended to improve RAG answers in the zero-shot scenario, beyond just risk control?
- Basis in paper: [inferred] The paper focuses on risk control in the zero-shot scenario and mentions the need to explore how to improve RAG answers in this scenario.
- Why unresolved: The paper does not provide a clear direction or methodology for improving RAG answers in the zero-shot scenario.
- What evidence would resolve it: Experiments demonstrating the effectiveness of the counterfactual prompting framework in improving the quality of RAG answers in the zero-shot scenario, compared to baselines.

### Open Question 4
- Question: How can objective functions based on risk-related metrics be designed to guide the joint learning of the risk control framework and the RAG model?
- Basis in paper: [inferred] The paper suggests exploring the design of objective functions based on risk-related metrics for joint training with the RAG framework to achieve a balanced trade-off between risk control and response quality.
- Why unresolved: The paper does not provide a specific approach or methodology for designing such objective functions.
- What evidence would resolve it: Development and evaluation of objective functions based on risk-related metrics, integrated with the RAG model, to achieve improved performance in both risk control and response quality.

## Limitations

- Moderate improvements in risk control metrics (up to 2.88% risk reduction and 14.76% carefulness increase) suggest limited practical impact
- Performance gains vary significantly across different model and retriever combinations
- Computational overhead of multiple counterfactual generations may become prohibitive at scale

## Confidence

- **High confidence**: The core mechanism of counterfactual prompting and its basic implementation approach are sound and well-supported by the experimental design.
- **Medium confidence**: The relative performance improvements over baselines are reproducible, though the absolute magnitude of gains may vary with different datasets or model configurations.
- **Low confidence**: The generalizability of the inverse relationship between model strength and risk control effectiveness across different LLM families and domains.

## Next Checks

1. **Cross-domain validation**: Test the framework on domains with different knowledge distributions (e.g., medical, legal, scientific) to assess generalizability beyond the current QA datasets.

2. **Ablation study on prompt complexity**: Systematically vary the complexity and specificity of counterfactual prompts to identify the minimum effective prompt formulation and isolate which components drive performance.

3. **Real-world deployment simulation**: Implement a latency-aware evaluation that measures both accuracy gains and computational costs under realistic usage patterns, including concurrent request handling scenarios.