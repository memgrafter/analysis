---
ver: rpa2
title: On the Effects of Data Scale on UI Control Agents
arxiv_id: '2406.03679'
source_url: https://arxiv.org/abs/2406.03679
tags:
- action
- control
- android
- tasks
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how fine-tuning LLMs with UI task demonstrations
  scales for building computer control agents. A new AndroidControl dataset is introduced
  with 15,283 tasks across 833 apps, providing both high-level and low-level human
  instructions.
---

# On the Effects of Data Scale on UI Control Agents
## Quick Facts
- arXiv ID: 2406.03679
- Source URL: https://arxiv.org/abs/2406.03679
- Authors: Wei Li; William Bishop; Alice Li; Chris Rawles; Folawiyo Campbell-Ajala; Divya Tyamagundlu; Oriana Riva
- Reference count: 40
- Key outcome: Fine-tuned UI control agents achieve 71.5% high-level and 86.6% low-level task accuracy in-domain, but require 10-150x more data for comparable out-of-domain performance

## Executive Summary
This paper investigates how fine-tuning large language models with UI task demonstrations scales for building computer control agents. The authors introduce ANDROID CONTROL, a dataset with 15,283 tasks across 833 Android apps, providing both high-level and low-level human instructions. Fine-tuned models outperform zero-shot baselines and scale predictably with more data for in-domain tasks, but require significantly more data for robust out-of-domain generalization, particularly for high-level tasks that require reasoning about intent rather than simple action matching.

## Method Summary
The authors fine-tune pre-trained LLMs using LoRA on SeqIO tasks derived from human demonstrations, where models learn to map natural language instructions to sequences of UI actions. They experiment with PaLM-2 and other models, using rank 4 LoRA for smaller datasets and rank 64 for larger ones. The training procedure involves behavioral cloning on ANDROID CONTROL, which contains 14,548 unique tasks across 833 apps. Evaluation includes both in-domain (same app distribution) and out-of-domain (different app distribution) performance, with specific splits for task-unseen, app-unseen, and category-unseen scenarios.

## Key Results
- Fine-tuned models achieve 71.5% step accuracy on high-level instructions and 86.6% on low-level instructions in-domain
- Out-of-domain performance scales significantly slower, requiring 10-150x more data for comparable accuracy
- Low-level instructions transfer better out-of-domain than high-level instructions due to their concrete, action-focused nature
- Increasing training data from 5 to 15k episodes improves in-domain accuracy predictably, but OOD benefits diminish

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning scales predictably with data for in-domain performance but hits diminishing returns out-of-domain. In-domain, performance improves linearly with log(data) because the model learns task patterns from similar contexts. Out-of-domain, the model faces distribution shift, requiring orders of magnitude more data to generalize. This assumes the agent's environment follows consistent UI structure within apps but varies widely across unseen apps.

### Mechanism 2
Low-level task instructions transfer better out-of-domain than high-level ones because they are more concrete and less context-dependent. Low-level instructions ("click OK", "type name") are action-focused and match specific UI elements, so the model can generalize patterns. High-level instructions ("add an alarm") require reasoning about intent, which is more app-specific.

### Mechanism 3
Fine-tuning with diverse apps (high app diversity) improves generalization more than increasing data from a few apps. Diverse app exposure forces the model to learn robust UI patterns rather than memorizing app-specific layouts, improving OOD transfer. This assumes the diversity metric captures structural variation across apps rather than just counting different app names.

## Foundational Learning

- **Sequence-to-sequence modeling (SeqIO tasks)**: The agent must map natural language instructions to sequences of UI actions. Quick check: What is the difference between HL and LL SeqIO tasks in this work?

- **Behavioral cloning**: The model is trained to mimic human demonstrations rather than learn from rewards. Quick check: Why is a terminate action added at the end of every episode?

- **Distribution shift**: Understanding why OOD performance degrades helps design better training strategies. Quick check: How does the number of unique apps in train vs test affect OOD performance?

## Architecture Onboarding

- **Component map**: Screen parser → Screen representation (accessibility tree → flat UI elements) → Instruction encoder → History encoder → Action predictor → Environment executor
- **Critical path**: Receive instruction → Generate screen representation → Predict action → Execute action → Update history → Loop until terminate
- **Design tradeoffs**: Screen representation: Flat list vs. hierarchical tree (flat is simpler, tree is richer); Action space: Click by coordinates vs. click by element index (coordinates are more general); History: Include only current screen vs. multi-step history (current is faster, multi-step is more context)
- **Failure signatures**: High terminate mispredictions → Model cannot infer task completion conditions; Click predictions outside UI element bounds → Poor element localization; Frequent navigate_back → Model fails to anticipate necessary back navigation
- **First 3 experiments**: 1) Compare step accuracy of zero-shot vs. fine-tuned on Random-500. 2) Vary LoRA rank (4 vs. 64) with 1k episodes to measure parameter efficiency. 3) Train on HL-only vs. LL-only vs. mixed SeqIO tasks to measure instruction-level impact.

## Open Questions the Paper Calls Out

### Open Question 1
What is the minimum number of episodes required to achieve a 95% step-wise accuracy for in-domain high-level tasks? The paper extrapolates that 2M episodes would be required to reach 99% step-wise accuracy to achieve 95% episode completion for high-level tasks, but this is based on a linear trend that may not reflect reality.

### Open Question 2
How does the performance of fine-tuned models on high-level tasks scale with the number of episodes when tested out-of-domain? The paper extrapolates that 150M episodes would be required to achieve 99% step accuracy for out-of-domain high-level tasks, but this extrapolation may not capture the true scaling behavior.

### Open Question 3
What are the limitations of fine-tuning models for high-level tasks compared to low-level tasks, and what alternative approaches might be more effective? The paper suggests fine-tuning alone may be insufficient for robust out-of-domain performance on high-level tasks but doesn't explore alternative approaches in detail.

## Limitations
- Out-of-domain scaling assumptions rely on linear extrapolation from limited data points, which may not reflect true scaling behavior
- Task diversity vs. structural similarity: The dataset's 833 apps may share similar UI paradigms, potentially overstating the benefit of app diversity
- Instruction-level transfer distinction assumes fundamental differences in generalization properties without testing specialized fine-tuning approaches

## Confidence

- **High confidence**: In-domain scaling results and step accuracy metrics (71.5% HL, 86.6% LL on Random-500) are directly measured and reproducible
- **Medium confidence**: OOD scaling extrapolation and the one-to-two orders of magnitude claim rely on linear extrapolation beyond observed data
- **Medium confidence**: Low-level vs. high-level transfer advantage is observed but the underlying mechanism needs further validation

## Next Checks

1. Validate OOD scaling non-linearity: Collect intermediate data points (50k, 100k episodes) to test whether the scaling curve remains linear or exhibits diminishing returns, directly testing the extrapolation assumption.

2. Test structural diversity impact: Stratify the 833 apps by UI category (navigation patterns, widget types, layout structures) and measure whether app structural diversity, not just app count, predicts OOD performance.

3. Compare instruction-level specialization: Train separate models on HL-only vs. LL-only vs. mixed instructions and test their relative performance on both HL and LL OOD tasks to validate whether mixed training truly outperforms specialization.