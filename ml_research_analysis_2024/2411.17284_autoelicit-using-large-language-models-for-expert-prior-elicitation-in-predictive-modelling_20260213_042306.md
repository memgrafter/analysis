---
ver: rpa2
title: 'AutoElicit: Using Large Language Models for Expert Prior Elicitation in Predictive
  Modelling'
arxiv_id: '2411.17284'
source_url: https://arxiv.org/abs/2411.17284
tags:
- prior
- language
- in-context
- feature
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoElicit is a method that uses large language models to generate
  prior distributions for linear predictive models, improving accuracy with fewer
  training samples. It extracts knowledge from LLMs through natural language prompts,
  creating Gaussian mixtures over model parameters.
---

# AutoElicit: Using Large Language Models for Expert Prior Elicitation in Predictive Modelling

## Quick Facts
- arXiv ID: 2411.17284
- Source URL: https://arxiv.org/abs/2411.17284
- Reference count: 40
- AutoElicit uses LLMs to generate informative priors for linear predictive models, improving accuracy with fewer training samples

## Executive Summary
AutoElicit is a novel method that leverages large language models to generate informative prior distributions for predictive modeling. By extracting knowledge from LLMs through natural language prompts, AutoElicit creates Gaussian mixtures over model parameters, significantly improving model performance especially in low-data regimes. The approach addresses the challenge of expert prior elicitation, which is typically expensive and time-consuming, by using LLMs as knowledge proxies to generate synthetic data and construct informative priors. Across multiple datasets and tasks, AutoElicit consistently outperforms uninformative priors and in-context learning approaches, with practical applications demonstrated in clinical settings where it reduced labeling effort by over six months.

## Method Summary
AutoElicit employs a two-stage process for prior elicitation. First, it generates synthetic data by prompting an LLM with domain-specific knowledge and task descriptions, creating labeled datasets that reflect expert understanding. Second, it uses these synthetic datasets to fit prior distributions over model parameters through a weighted least squares approach, constructing Gaussian mixture priors that encode domain knowledge. The method works with linear regression and logistic regression models, using natural language prompts to guide the LLM in generating contextually relevant synthetic data. This approach transforms the challenge of direct prior elicitation into a more manageable synthetic data generation task, while maintaining interpretability through the use of standard probabilistic models.

## Key Results
- AutoElicit consistently outperformed uninformative priors across synthetic and clinical datasets
- In dementia care study, reduced labeling effort by over six months for UTI prediction
- Performance improvements were most pronounced in low-data regimes with fewer than 200 samples

## Why This Works (Mechanism)
AutoElicit works by leveraging LLMs as knowledge repositories that can be queried through natural language to extract domain expertise. The method transforms the complex task of direct prior elicitation into a more tractable synthetic data generation problem. By generating synthetic data that reflects expert knowledge, AutoElicit can construct informative priors that capture domain-specific patterns without requiring extensive human expert involvement. The Gaussian mixture formulation allows for flexible representation of uncertainty while maintaining computational tractability.

## Foundational Learning
- **Bayesian inference with priors**: Why needed - forms the theoretical foundation for incorporating domain knowledge; Quick check - verify understanding of posterior updating with conjugate priors
- **Gaussian mixture models**: Why needed - enables flexible prior representation over model parameters; Quick check - ensure ability to fit and sample from GMMs
- **LLM prompt engineering**: Why needed - critical for generating high-quality synthetic data; Quick check - test prompt variations on simple synthetic data tasks
- **Weighted least squares**: Why needed - used to fit prior distributions from synthetic data; Quick check - verify understanding of weighted vs ordinary least squares
- **Synthetic data generation**: Why needed - core mechanism for translating LLM knowledge into usable priors; Quick check - evaluate synthetic data quality metrics
- **Logistic vs linear regression priors**: Why needed - different approaches needed for classification vs regression tasks; Quick check - understand the distinction in prior formulation

## Architecture Onboarding

Component Map:
Synthetic Data Generator (LLM + Prompts) -> Prior Constructor (Weighted Least Squares) -> Predictive Model (Linear/Logistic Regression)

Critical Path:
1. Domain knowledge prompt engineering
2. Synthetic data generation and validation
3. Prior distribution fitting via weighted least squares
4. Model training with informative priors
5. Performance evaluation and iteration

Design Tradeoffs:
- LLM knowledge extraction vs direct expert elicitation (cost vs accuracy)
- Synthetic data quantity vs quality (computational cost vs prior informativeness)
- Gaussian mixture complexity vs interpretability
- Prompt specificity vs generalizability across domains

Failure Signatures:
- Poor synthetic data quality leading to uninformative priors
- Overfitting to synthetic data patterns
- LLM hallucinations manifesting in generated priors
- Computational bottlenecks in weighted least squares fitting
- Domain mismatch between LLM knowledge and actual task requirements

First Experiments:
1. Simple linear regression with synthetic data from LLM to verify basic functionality
2. Comparison of different prompt formulations on synthetic data quality
3. Prior elicitation for a well-understood domain to validate against ground truth

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Reliance on LLMs introduces potential bias or hallucination in generated priors
- Performance on high-dimensional or complex non-linear models remains untested
- Cost-effectiveness analysis based on single case study, limiting generalizability

## Confidence
- Core methodology: Medium
- Experimental results: Medium
- Real-world application validity: Medium
- Generalizability across domains: Low
- Scalability to complex models: Low

## Next Checks
1. Test AutoElicit on non-linear models (e.g., neural networks, tree-based methods) to assess generalizability beyond linear regression/classification.

2. Conduct a multi-domain validation study across different application areas (e.g., finance, engineering, social sciences) to evaluate robustness and domain-specific performance variations.

3. Perform ablation studies to quantify the individual contributions of synthetic data generation versus LLM knowledge extraction to the final performance improvements.