---
ver: rpa2
title: A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question
  Answering
arxiv_id: '2409.13483'
source_url: https://arxiv.org/abs/2409.13483
tags:
- retriever
- dense
- question
- retrieval
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an end-to-end trained multimodal dense retriever
  for speech-based open-domain question answering that operates directly on spoken
  questions, bypassing the need for an ASR model. The method replaces the text-based
  question encoder with a self-supervised speech encoder (HuBERT) while keeping a
  BERT-based passage encoder, training both end-to-end to maximize question-passage
  relevance.
---

# A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering

## Quick Facts
- **arXiv ID:** 2409.13483
- **Source URL:** https://arxiv.org/abs/2409.13483
- **Authors:** Georgios Sidiropoulos; Evangelos Kanoulas
- **Reference count:** 13
- **Primary result:** ASR-free multimodal dense retriever achieves competitive performance on short questions and better stability under high word error rates

## Executive Summary
This paper introduces an end-to-end trained multimodal dense retriever for speech-based open-domain question answering that operates directly on spoken questions, bypassing the need for an ASR model. The method replaces the text-based question encoder with a self-supervised speech encoder (HuBERT) while keeping a BERT-based passage encoder, training both end-to-end to maximize question-passage relevance. The authors benchmark this approach against traditional ASR+Retriever pipelines, demonstrating that the multimodal retriever is competitive on shorter questions and significantly more stable under high ASR word error rates, especially when important words are mistranscribed or previously unseen mistranscriptions occur. The proposed method also offers faster query times and does not require annotated speech data, addressing key limitations of ASR-based pipelines.

## Method Summary
The proposed approach uses a dual-encoder architecture where spoken questions are encoded using HuBERT and text passages are encoded using BERT. Both encoders are trained end-to-end using in-batch negatives with dual learning rates (2e-4 for the question encoder, 2e-5 for the passage encoder). The system is trained to maximize the relevance between question and passage embeddings without requiring transcribed speech data for training. The method leverages hard negative mining during training and evaluates performance using standard retrieval metrics including MRR@10, Recall@50/1000 for Spoken-MSMARCO, and Answer Recall at top-k (AR@20/100) for Spoken-NQ.

## Key Results
- The multimodal retriever achieves competitive performance with ASR-based methods on shorter questions
- The method shows significantly better stability under high word error rates compared to ASR+Retriever pipelines
- The approach offers faster query times and does not require annotated speech data for training

## Why This Works (Mechanism)
The multimodal approach works because it directly encodes the acoustic information of spoken questions without the intermediate error-prone ASR step. By using HuBERT's self-supervised learning from raw audio, the system can capture phonetic and semantic patterns that might be lost or mistranscribed by ASR systems, especially for out-of-vocabulary words or noisy speech. The end-to-end training allows the question and passage encoders to develop complementary representations optimized for the retrieval task, while the dual learning rates enable more effective optimization of the speech encoder.

## Foundational Learning
- **HuBERT (Hidden Unit BERT)**: Self-supervised speech representation learning using masked prediction of contextualized representations. Needed because traditional ASR systems introduce errors that propagate to retrieval; quick check: verify HuBERT base model produces meaningful embeddings for spoken questions.
- **Dual-encoder architecture**: Separate encoders for questions and passages with learned similarity scoring. Needed to handle the modality mismatch between speech and text; quick check: ensure both encoders produce embeddings in compatible vector spaces.
- **In-batch negative sampling**: Using other examples in the training batch as negative samples for contrastive learning. Needed to provide efficient negative examples during training; quick check: verify batch size is sufficient to provide meaningful negative samples.

## Architecture Onboarding
- **Component map:** Spoken question -> HuBERT encoder -> Embedding -> Similarity score <- BERT encoder <- Text passage
- **Critical path:** Speech input → HuBERT pooling → Dot product with passage embedding → Ranking of passages
- **Design tradeoffs:** Single-vector encoding enables fast retrieval but may lose information from longer questions; dual learning rates optimize different modalities appropriately but increase complexity
- **Failure signatures:** Poor performance on longer questions due to single-vector encoding; instability under high WER when using ASR baselines
- **First experiments:** 1) Train with in-batch negatives only, 2) Train with explicit hard negative mining, 3) Compare performance across different question length bins

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on relatively small spoken QA datasets (Spoken-MSMARCO and Spoken-NQ), limiting generalizability to broader domains
- The HuBERT-BERT architecture may struggle with longer, more complex queries due to single-vector encoding
- The training methodology depends on in-batch negatives without explicit mention of hard negative mining strategies

## Confidence
- **High confidence**: The core claim that the multimodal retriever is competitive on shorter questions and more stable under high WER compared to ASR-based pipelines is well-supported by the experimental results
- **Medium confidence**: The claim about faster query times is plausible given the architecture but lacks direct experimental validation
- **Low confidence**: The claim about not requiring annotated speech data is technically true but misleading, as the method still requires transcribed speech data for training, just not at the passage-retrieval level

## Next Checks
1. **WER Robustness Validation**: Group dev set questions by WER bins (e.g., 0-10%, 10-20%, 20-30%, >30%) and analyze retrieval performance trends to confirm the claimed stability under high WER conditions
2. **Query Length Impact Study**: Segment questions by length (e.g., <5 words, 5-10 words, 10-15 words, >15 words) and compare retrieval performance across bins to verify the claimed advantage for shorter questions
3. **Hard Negative Mining Experiment**: Implement and train an additional model variant using explicit hard negative mining strategies and compare performance to the in-batch negative baseline to assess the impact of negative sampling quality