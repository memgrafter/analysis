---
ver: rpa2
title: 'SCENE: Evaluating Explainable AI Techniques Using Soft Counterfactuals'
arxiv_id: '2408.04575'
source_url: https://arxiv.org/abs/2408.04575
tags:
- scene
- soft
- tokens
- mean
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SCENE introduces a novel approach to evaluate XAI techniques in
  NLP using soft counterfactuals generated via zero-shot masked language modeling.
  It measures XAI effectiveness through Validitysoft and Csoft metrics, focusing on
  token-based perturbations.
---

# SCENE: Evaluating Explainable AI Techniques Using Soft Counterfactuals

## Quick Facts
- arXiv ID: 2408.04575
- Source URL: https://arxiv.org/abs/2408.04575
- Authors: Haoran Zheng; Utku Pamuksuz
- Reference count: 0
- Primary result: SCENE achieves stronger Spearman correlation with human agreement than traditional infidelity metrics when evaluating XAI techniques on CNN, RNN, and Transformer models.

## Executive Summary
SCENE introduces a novel evaluation framework for explainable AI (XAI) techniques in NLP using soft counterfactuals generated via zero-shot masked language modeling. The method creates contextually appropriate counterfactuals by replacing significant tokens identified by XAI methods with predictions from a BERT-based masked language model. SCENE measures XAI effectiveness through two metrics: Validitysoft (probability changes) and Csoft (incorporating semantic distance). When applied to multiple architectures (CNN, RNN, Transformer), SCENE reveals varying strengths across different XAI methods and shows stronger correlation with human agreement compared to traditional infidelity metrics.

## Method Summary
SCENE generates soft counterfactuals by masking top V significant tokens identified by XAI techniques and replacing them using zero-shot BertForMaskedLM. The method creates K counterfactuals per instance and computes two evaluation metrics: Validitysoft (measures probability shifts from original to counterfactual predictions) and Csoft (extends Validitysoft with distance function using Universal Sentence Encoder). The framework evaluates multiple XAI methods including LIME, SHAP, gradient-based approaches, and others on the Stanford Sentiment Treebank dataset. Parameters are set to K=10 counterfactuals and V=5 significant tokens.

## Key Results
- SCENE metrics show stronger Spearman correlation with human agreement than infidelity metrics across all three architectures
- Different XAI methods exhibit varying strengths across CNN, RNN, and Transformer architectures
- The method addresses instability issues in perturbation-based XAI methods by generating realistic counterfactuals within the data distribution

## Why This Works (Mechanism)

### Mechanism 1
- Zero-shot masked language modeling generates semantically meaningful counterfactuals without fine-tuning by predicting token replacements in context while maintaining semantic coherence.
- Core assumption: Masked language models can generate contextually appropriate replacements that preserve grammaticality and semantic plausibility without task-specific training.
- Evidence anchors: [abstract] "SCENE creates contextually appropriate and semantically meaningful Soft Counterfactuals without extensive fine-tuning"; [section 3.2.1] "we utilize the power of the Transformer to ensure that these counterfactuals are realistic and contextually appropriate"

### Mechanism 2
- Soft Counterfactuals enable stable evaluation by avoiding the instability of perturbation-based methods through realistic token substitution rather than masking or noise addition.
- Core assumption: Token substitution via masked language modeling produces counterfactuals that are both realistic and relevant to the original instance's context.
- Evidence anchors: [section 3.2.1] "Without relying on an optimization function to minimize the distance, we utilize the power of the Transformer to ensure that these counterfactuals are realistic and contextually appropriate"

### Mechanism 3
- Validitysoft and Csoft metrics provide more reliable evaluation than traditional infidelity metrics by combining probability change and semantic distance.
- Core assumption: The combination of probability change and semantic distance captures both the effectiveness and faithfulness of XAI explanations better than simple infidelity measures.
- Evidence anchors: [section 3.2.2] "ð¶soft builds on the foundation of Validitysoft by incorporating a distance function to evaluate the faithfulness of the saliency explanations"

## Foundational Learning

- Concept: Counterfactual explanations in XAI
  - Why needed here: Understanding what counterfactuals are and how they differ from other explanation methods is crucial for grasping SCENE's approach and why it addresses limitations of existing methods.
  - Quick check question: What distinguishes counterfactual explanations from other XAI methods like LIME or SHAP?

- Concept: Masked language modeling and its zero-shot capabilities
  - Why needed here: SCENE relies on BERT's masked language modeling capability without fine-tuning, so understanding how this works is essential for implementing and potentially extending the method.
  - Quick check question: How does a masked language model generate contextually appropriate token replacements without task-specific training?

- Concept: Evaluation metrics for XAI techniques
  - Why needed here: SCENE introduces new metrics (Validitysoft and Csoft) and compares them with existing ones (infidelity, human agreement), requiring understanding of what each metric measures and their limitations.
  - Quick check question: What are the key differences between infidelity, human agreement, and the new SCENE metrics in terms of what they evaluate?

## Architecture Onboarding

- Component map: Input preprocessing -> Tokenization and filtering -> XAI method integration -> Token importance extraction -> Masked language model -> Counterfactual generation -> Probability computation -> Metric calculation -> Results comparison

- Critical path: Original text â†’ XAI attribution â†’ Top token selection â†’ Masking and replacement â†’ Counterfactual generation â†’ Probability computation â†’ Metric calculation â†’ Results comparison

- Design tradeoffs:
  - Token selection (V): Higher values capture more context but may introduce noise; lower values focus on most important tokens but may miss broader context
  - Number of counterfactuals (K): More counterfactuals provide better statistical reliability but increase computation time
  - Masked language model choice: Different models may produce different quality counterfactuals
  - Semantic distance measure: Universal Sentence Encoder vs alternatives affects Csoft computation

- Failure signatures:
  - Counterfactuals that are grammatically incorrect or semantically unrelated
  - Probability changes that don't align with token importance rankings
  - High computational time for certain XAI methods
  - Low correlation between SCENE metrics and human agreement

- First 3 experiments:
  1. Test SCENE on a simple CNN model with one XAI method (e.g., LIME) to verify basic functionality and metric computation
  2. Compare Validitysoft and Csoft results across different values of V (token selection) to find optimal configuration
  3. Run SCENE with multiple XAI methods on RNN architecture to observe differences in metric performance and identify any implementation issues

## Open Questions the Paper Calls Out

- How does SCENE's performance vary across different types of NLP tasks beyond sentiment analysis, such as named entity recognition, machine translation, or question answering? (Basis: Future work focus on integrating SCENE with diverse NLP tasks)

- How sensitive is SCENE to the choice of masking strategy and number of tokens masked when generating soft counterfactuals? (Basis: Parameters K=10 and V=5 used but sensitivity not explored)

- How does SCENE perform when evaluated on multi-class classification tasks, given that the paper notes its metrics "may not fully capture the behavior of counterfactual outputs in multi-class classification scenarios"? (Basis: Metrics designed for binary classification tasks)

## Limitations

- Zero-shot counterfactual quality may vary depending on masked language model and context complexity
- Dataset generalization limited to binary sentiment analysis; performance on other NLP tasks unknown
- XAI method coverage limited to subset of available techniques, may not extend to newer methods

## Confidence

- High Confidence: Zero-shot masked language modeling mechanism and metric implementation
- Medium Confidence: Stronger Spearman correlation with human agreement and varying XAI method strengths across architectures
- Low Confidence: Claims about addressing instability and misleading nature of popular XAI methods

## Next Checks

1. Apply SCENE to multiple NLP datasets beyond SST2, including datasets with different linguistic characteristics to assess generalizability

2. Conduct controlled experiments comparing zero-shot counterfactual generation with fine-tuned masked language models to quantify trade-offs

3. Extend evaluation to include additional XAI techniques not covered in the current study, particularly newer methods that address specific limitations of traditional approaches