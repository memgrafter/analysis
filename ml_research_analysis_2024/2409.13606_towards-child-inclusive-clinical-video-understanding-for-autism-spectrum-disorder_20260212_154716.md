---
ver: rpa2
title: Towards Child-Inclusive Clinical Video Understanding for Autism Spectrum Disorder
arxiv_id: '2409.13606'
source_url: https://arxiv.org/abs/2409.13606
tags:
- video
- autism
- activity
- these
- refinement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the use of foundation models for analyzing
  clinical videos in the context of Autism Spectrum Disorder (ASD). The authors propose
  a multimodal pipeline that combines speech, video, and text modalities to improve
  performance on downstream tasks such as activity recognition and abnormal behavior
  detection.
---

# Towards Child-Inclusive Clinical Video Understanding for Autism Spectrum Disorder

## Quick Facts
- arXiv ID: 2409.13606
- Source URL: https://arxiv.org/abs/2409.13606
- Authors: Aditya Kommineni; Digbalay Bose; Tiantian Feng; So Hyun Kim; Helen Tager-Flusberg; Somer Bishop; Catherine Lord; Sudarsana Kadiri; Shrikanth Narayanan
- Reference count: 38
- Primary result: Multimodal pipeline combining video, audio, and text modalities with LLMs improves clinical video analysis performance for ASD assessment by up to 20% relative gains

## Executive Summary
This work investigates foundation models for analyzing clinical videos in Autism Spectrum Disorder (ASD) assessment contexts. The authors propose a multimodal pipeline that combines speech, video, and text modalities using large language models (LLMs) as reasoning agents. The method processes natural language descriptions of video and speech content to improve performance on downstream tasks such as activity recognition and abnormal behavior detection. The study demonstrates that multimodal integration provides robustness to modality-specific limitations and outperforms unimodal approaches, while also highlighting challenges including model hallucinations and the need for more precise object-centered grounding information.

## Method Summary
The method uses pre-trained Video Language Models (VLMs) and Large Language Models (LLMs) for zero-shot inference on clinical autism assessment videos. Video frames are extracted at 1 fps and processed by VLMs to generate descriptive captions. Audio transcripts are generated using Whisper ASR. For each task, prompts combining video descriptions and transcripts are created and processed by an LLM (Meta-Llama-3-8B-Instruct) to perform multimodal refinement and generate predictions. The approach is evaluated on two clinical datasets (Remote-NLS and ADOSMod3) for activity recognition (macro F1) and abnormal behavior detection (PR-AUC).

## Key Results
- Multimodal pipeline provides up to 20% relative gains compared to unimodal settings
- LLMs as reasoning agents improve performance over direct zero-shot VLM classification
- Increasing transcript context length from 16s to 64s leads to significant improvements in activity recognition
- The approach shows robustness to modality-specific limitations and provides improved performance across different activity tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using LLMs as reasoning agents over descriptive captions improves task performance compared to direct zero-shot VLM classification.
- Mechanism: VLMs generate natural language descriptions of video content, which are then provided to LLMs. The LLM uses these descriptions to reason about task-specific labels rather than relying on predefined classification outputs from the VLM.
- Core assumption: Descriptive captions contain sufficient task-relevant context for LLMs to infer labels more accurately than VLMs can classify directly.
- Evidence anchors:
  - [abstract] "We find that the proposed multimodal pipeline provides robustness to modality-specific limitations and improves performance on the clinical video analysis compared to unimodal settings."
  - [section] "we see that augmenting the prompt with contextual rich descriptions from videos can improve the LLM refinement performance ( ∼20% relative) compared to zero-shot inference from VLMs"
  - [corpus] Weak evidence - corpus neighbors focus on gesture recognition and robot therapy rather than multimodal LLM reasoning pipelines.
- Break condition: If VLM-generated captions are too generic or miss task-specific details, LLM reasoning will not outperform direct VLM classification.

### Mechanism 2
- Claim: Multimodal integration via LLM reasoning provides robustness to individual modality failures.
- Mechanism: When one modality (e.g., video) lacks sufficient information for a task, the LLM can leverage complementary information from another modality (e.g., speech transcripts) to improve inference accuracy.
- Core assumption: Different tasks rely on different modalities, and LLMs can effectively combine complementary information from multiple sources.
- Evidence anchors:
  - [abstract] "We propose a unified methodology to combine multiple modalities by using large language models as reasoning agents."
  - [section] "The proposed approach of combining modality-specific information followed by LLM refinement enables inferring from the more informative modalities, thereby providing improved performance across the different activity tasks."
  - [corpus] Weak evidence - corpus neighbors do not discuss multimodal fusion strategies or LLM-based reasoning.
- Break condition: If both modalities are equally poor for a task or if LLM hallucinations dominate, multimodal refinement may not improve over unimodal approaches.

### Mechanism 3
- Claim: Increasing context length in transcripts improves activity recognition performance.
- Mechanism: Longer transcript segments provide more context about the ongoing activity, allowing LLMs to better distinguish between similar activities.
- Core assumption: Activity-specific information is distributed across the conversation and requires sufficient context length to be captured.
- Evidence anchors:
  - [section] "comparing the rows whisper-L (16s) and whisper-L (64s) indicates that increasing the context length of audio information from 16s to 64s leads to a significant improvement in the performance for activity tasks."
  - [abstract] No direct evidence for context length effects.
  - [corpus] Weak evidence - corpus neighbors do not discuss transcript context length or its impact on classification performance.
- Break condition: If activities are too short or if context beyond a certain length introduces irrelevant information, longer transcripts may not improve performance.

## Foundational Learning

- Concept: Multimodal learning and fusion
  - Why needed here: The clinical videos contain complementary information across video, audio, and text modalities that can be combined to improve analysis accuracy
  - Quick check question: What are the advantages and disadvantages of early fusion versus late fusion approaches in multimodal learning?

- Concept: Foundation models and their limitations
  - Why needed here: Understanding how VLMs and LLMs work, their training objectives, and common failure modes (like hallucinations) is crucial for interpreting results and designing improvements
  - Quick check question: What are the key differences between how VLMs and LLMs process information, and how does this affect their suitability for different tasks?

- Concept: Clinical video analysis and autism assessment
  - Why needed here: The specific characteristics of clinical autism assessment videos (long-form, complex behaviors, domain-specific coding schemes) influence model design and evaluation
  - Quick check question: What are the main challenges in analyzing clinical autism assessment videos compared to general video understanding tasks?

## Architecture Onboarding

- Component map: Video frames -> VLM -> Video captions; Audio input -> ASR -> Transcripts; Captions + Transcripts + Task prompts -> LLM -> Classification output

- Critical path: 1. Video input → VLM → Video captions; 2. Audio input → ASR → Transcripts; 3. Captions + Transcripts + Task-specific prompts → LLM → Classification output

- Design tradeoffs:
  - VLM vs LLM for direct classification: VLMs are specialized for visual tasks but may lack reasoning capability; LLMs can reason but need good descriptions
  - Context length vs computational cost: Longer transcripts provide more context but increase inference time and cost
  - Model size vs performance: Larger models may capture more nuances but require more resources

- Failure signatures:
  - Poor performance when VLM captions are too generic or miss task-relevant details
  - Degradation when ASR produces many errors or misses speaker attributions
  - Hallucinations in LLM outputs leading to incorrect classifications
  - Class imbalance causing poor performance on minority classes

- First 3 experiments:
  1. Compare zero-shot VLM classification vs LLM reasoning with VLM-generated captions on a simple activity recognition task
  2. Test unimodal (video-only, transcript-only) vs multimodal refinement performance on the same task
  3. Vary context length in transcripts (16s vs 64s) to measure impact on classification accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multimodal refinement be improved to handle hallucinations in VLMs and ASR models more effectively?
- Basis in paper: [explicit] The paper mentions that hallucinations in VLMs and ASR models lead to incorrect inferences during multimodal refinement, and suggests considering precise object-centered grounding information as a potential solution.
- Why unresolved: The paper identifies the problem of hallucinations but does not provide a concrete solution for mitigating their impact on multimodal refinement.
- What evidence would resolve it: A method that significantly reduces the impact of hallucinations on multimodal refinement performance, as measured by improved accuracy on downstream tasks.

### Open Question 2
- Question: Can increasing the context length of audio information beyond 64 seconds lead to further improvements in activity recognition and segmentation tasks?
- Basis in paper: [explicit] The paper shows that increasing the context length of audio information from 16s to 64s leads to a significant improvement in performance for activity tasks, but does not explore longer context lengths.
- Why unresolved: The paper only tests context lengths up to 64 seconds and does not investigate whether even longer context lengths could provide additional benefits.
- What evidence would resolve it: Results showing the performance of activity recognition and segmentation tasks with audio context lengths longer than 64 seconds, and whether they lead to further improvements.

### Open Question 3
- Question: How can foundation models be extended to identify a larger set of ASD-related behaviors, such as repetitive behaviors, gestures, and atypical gaze?
- Basis in paper: [explicit] The paper mentions that future work will focus on extending the reasoning capabilities to identification of a larger set of ASD-related behaviors, including repetitive behaviors, gestures, and atypical gaze.
- Why unresolved: The paper only evaluates the performance of foundation models on activity recognition and abnormal behavior detection tasks, and does not explore their ability to identify other ASD-related behaviors.
- What evidence would resolve it: A study demonstrating the performance of foundation models in identifying a broader range of ASD-related behaviors, with improved accuracy compared to existing methods.

## Limitations

- Data and generalizability limitations due to small sample sizes (89 and 83 videos) and specific clinical assessment protocols
- Heavy dependence on prompt engineering quality without systematic analysis of prompt variations
- Hallucination and reliability concerns not fully quantified for clinical decision-making contexts

## Confidence

- High confidence: The multimodal approach outperforms unimodal settings, as evidenced by consistent improvements across multiple tasks and datasets
- Medium confidence: The mechanism by which LLM reasoning improves over direct VLM classification, as the evidence shows relative improvements but doesn't fully explain the underlying reasons
- Medium confidence: The specific impact of context length on activity recognition performance, as the results show improvement but don't explore the optimal context length or its limitations

## Next Checks

1. **Cross-dataset validation**: Test the proposed multimodal pipeline on additional clinical video datasets from different assessment protocols (e.g., ADOS-2, CARS-2) to evaluate generalizability beyond the two datasets used in this study.

2. **Prompt ablation study**: Systematically vary the prompt templates and examples used for LLM refinement to determine how sensitive the performance improvements are to specific prompting strategies, and identify which prompt components are most critical for success.

3. **Hallucination impact assessment**: Design experiments that specifically measure the frequency and severity of VLM/LLM hallucinations and their impact on clinical task performance, including analysis of false positive vs false negative rates in abnormal behavior detection.