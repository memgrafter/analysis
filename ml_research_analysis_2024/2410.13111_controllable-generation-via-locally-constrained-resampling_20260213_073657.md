---
ver: rpa2
title: Controllable Generation via Locally Constrained Resampling
arxiv_id: '2410.13111'
source_url: https://arxiv.org/abs/2410.13111
tags:
- constraint
- distribution
- language
- sentence
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of generating complex outputs
  that adhere to logical constraints in autoregressive models. Current methods struggle
  with this due to the distribution's lack of structure and the hardness of the constraints.
---

# Controllable Generation via Locally Constrained Resampling

## Quick Facts
- arXiv ID: 2410.13111
- Source URL: https://arxiv.org/abs/2410.13111
- Reference count: 22
- Primary result: Method achieves perfect accuracy on Sudoku puzzles compared to <50% for GPT4-o and Gemini 1.5

## Executive Summary
This paper introduces Locally Constrained Resampling, a novel approach for generating complex outputs that adhere to logical constraints in autoregressive models. The method uses Bayesian conditioning on locally induced, factorized distributions to sample sequences that satisfy constraints while maintaining high fidelity to the underlying model distribution. By considering the entire sequence globally rather than making myopic decisions, the approach outperforms current greedy methods on various tasks including Sudoku puzzles, LLM detoxification, and shortest path prediction.

## Method Summary
The method starts with a sample from the autoregressive model, then induces a local tractable distribution using contextualized pseudolikelihood around that sample. This local distribution is conditioned on the constraint using a constraint circuit, which provides exponentially more succinct representations than traditional DFAs. Samples are drawn from the conditioned distribution, corrected for biases using importance weighting, and resampled to produce the final output that satisfies the constraint while maintaining high quality according to the original model.

## Key Results
- Perfect accuracy on Sudoku puzzles (compared to <50% for GPT4-o and Gemini 1.5)
- Improved results in LLM detoxification tasks compared to greedy masking methods
- Better performance in shortest path prediction tasks while maintaining model quality

## Why This Works (Mechanism)

### Mechanism 1
The locally constrained resampling approach outperforms greedy masking methods because it considers the entire sequence globally rather than making myopic decisions at each token step. Instead of masking invalid tokens at each step, the method first samples a sequence from the model, then induces a local tractable distribution around that sample. This distribution is conditioned on the constraint and samples are drawn from it, then corrected using importance weighting and resampling.

Core assumption: The contextualized pseudolikelihood distribution centered at the model sample is a high-fidelity local approximation of the true autoregressive distribution.

Evidence anchors:
- [abstract]: "Our approach considers the entire sequence, leading to a more globally optimal constrained generation than current greedy methods."
- [section 3.1]: "Ahmed et al. (2023a) have shown the contextualized pseudolikelihood distribution to be a local, high-fidelity approximation of the LLM distribution."

### Mechanism 2
Constraint circuits enable exponentially more succinct representation of constraints compared to DFAs, allowing for more complex constraint specifications. The method compiles constraints into constraint circuits that can represent decisions over entire variable sets rather than just single variables, enabling representation of constraints that would require exponentially-sized DFAs.

Core assumption: Constraint circuits can efficiently represent classes of functions that DFAs cannot represent in polynomial size.

Evidence anchors:
- [section 3.3]: "There are families of functions can only be represented by a exponentially-sized DFAs, but have a polynomially-sized constraint circuit ( Bova, 2016)."
- [section 3.2]: "At every step in the function decomposition we can branch not only on the value of a single variable, rather an entire sentence (Darwiche, 2011)."

### Mechanism 3
The importance weighting and resampling step corrects for the bias introduced by sampling from the tractable proposal distribution. Samples drawn from the conditioned local distribution are reweighted by the ratio of their probability under the true autoregressive model versus the proposal distribution, then resampled according to these weights.

Core assumption: The importance weights accurately capture the discrepancy between the proposal and target distributions, and the resampling step produces samples from the true conditional distribution.

Evidence anchors:
- [section 3.4]: "We calculate the self-normalized importance weights... These weights quantify the discrepancy between the proposal and target distributions, allowing us to correct for the biased sampling."
- [section 3.4]: "To transform our weighted particles into samples drawn from p(y | α ) we need to apply a resampling step according to the importance weights."

## Foundational Learning

- **Concept:** Bayesian conditioning and probabilistic inference
  - Why needed here: The core of the approach is performing Bayesian conditioning on a constraint, which requires understanding conditional probability and how to sample from conditional distributions.
  - Quick check question: If p(y) is the probability of sentence y and p(α) is the probability of constraint α, what is the formula for the conditional probability p(y|α)?

- **Concept:** Tractable probabilistic models and knowledge compilation
  - Why needed here: The approach relies on compiling constraints into tractable circuits that support efficient probabilistic reasoning operations like conditioning and marginalization.
  - Quick check question: What properties must a circuit have to support tractable marginalization over arbitrary sets of variables?

- **Concept:** Autoregressive models and sequence generation
  - Why needed here: The method is specifically designed for autoregressive models that generate sequences token by token, so understanding how these models work is crucial.
  - Quick check question: In an autoregressive model, how is the probability of a sequence decomposed into conditional probabilities of individual tokens?

## Architecture Onboarding

- **Component map:** Constraint compiler -> Local distribution generator -> Conditioning engine -> Sampling module -> Importance weighting system -> Resampling component -> Output

- **Critical path:** Model sample → Local distribution generation → Constraint conditioning → Biased sampling → Importance weighting → Resampling → Output

- **Design tradeoffs:**
  - Pseudolikelihood approximation vs exact likelihood: The pseudolikelihood is tractable but introduces approximation error
  - Circuit compilation time vs runtime efficiency: More complex constraints require more compilation time but enable faster inference
  - Sample size for importance weighting vs accuracy: More samples reduce variance in importance weights but increase computation time

- **Failure signatures:**
  - Constraint violations in output: Indicates issues with conditioning or constraint circuit compilation
  - Poor quality samples (low likelihood under true model): Suggests the local distribution poorly approximates the true distribution
  - High variance in importance weights: Indicates the proposal distribution poorly matches the target distribution

- **First 3 experiments:**
  1. Implement the local distribution generation and verify it produces high-likelihood samples near the model sample
  2. Add constraint conditioning and test on simple constraints (e.g., banning specific words)
  3. Implement importance weighting and resampling, then test on a simple task like word banning to verify constraint satisfaction

## Open Questions the Paper Calls Out

None identified in the provided material.

## Limitations

- Computational efficiency concerns due to the overhead of local distribution generation, constraint conditioning, and importance weighting/resampling operations
- Critical dependence on the quality of the contextualized pseudolikelihood approximation of the true autoregressive distribution
- Potential scalability issues with constraint circuit compilation for highly complex constraints

## Confidence

**High Confidence:** The core mechanism of using local distributions and importance weighting for constrained sampling is theoretically sound and well-established in probabilistic inference literature. The empirical results on Sudoku puzzles are compelling and directly demonstrate the approach's effectiveness.

**Medium Confidence:** The Sudoku results are strong, but the detoxification and shortest path prediction results are less thoroughly validated. The paper doesn't provide detailed analysis of failure modes or robustness to constraint specification errors.

**Low Confidence:** Claims about computational efficiency improvements over greedy methods are not adequately supported with runtime comparisons or complexity analysis. The relationship between constraint complexity and compilation overhead is not well-characterized.

## Next Checks

1. **Runtime Analysis:** Measure and compare the computational overhead of the proposed method against baseline greedy approaches across varying sequence lengths and constraint complexities. Include compilation time, sampling time, and total inference time.

2. **Distribution Quality Evaluation:** Conduct ablation studies comparing samples from the proposed method against those from the true autoregressive model (when available) or against samples from simpler constraint satisfaction methods, using likelihood-based metrics and human evaluation of sample quality.

3. **Constraint Circuit Scalability Testing:** Systematically evaluate how constraint circuit size and compilation time scale with constraint complexity across different constraint types (e.g., regular expressions, logical constraints, arithmetic constraints) to identify practical limits of the approach.