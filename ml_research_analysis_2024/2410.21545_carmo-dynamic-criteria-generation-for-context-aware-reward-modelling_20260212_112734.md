---
ver: rpa2
title: 'CARMO: Dynamic Criteria Generation for Context-Aware Reward Modelling'
arxiv_id: '2410.21545'
source_url: https://arxiv.org/abs/2410.21545
tags:
- response
- reward
- criteria
- instruction
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CARMO introduces context-aware reward modeling by dynamically generating
  task-specific evaluation criteria using large language models, which are then used
  to score responses. This approach addresses reward hacking in language model alignment,
  where models exploit spurious correlations in static rubrics.
---

# CARMO: Dynamic Criteria Generation for Context-Aware Reward Modelling

## Quick Facts
- arXiv ID: 2410.21545
- Source URL: https://arxiv.org/abs/2410.21545
- Reference count: 40
- Primary result: CARMO achieves 2.1% improvement on RewardBench in zero-shot settings and 22.5% LC-WR/21.1% WR on Mistral-Base (7B) for preference optimization

## Executive Summary
CARMO introduces context-aware reward modeling by dynamically generating task-specific evaluation criteria using large language models. This approach addresses reward hacking in language model alignment, where models exploit spurious correlations in static rubrics. CARMO improves RewardBench performance by 2.1% in zero-shot settings and achieves 22.5% LC-WR and 21.1% WR on Mistral-Base (7B) when used for preference optimization. Theoretical analysis shows that adaptive criteria generation prevents reward hacking by avoiding reliance on spurious features, and CARMO can be distilled into smaller models for cost-effective deployment.

## Method Summary
CARMO implements a two-stage pipeline where a powerful LLM first generates context-specific evaluation criteria for each instruction, then uses these criteria to evaluate responses. The system creates a dataset by running this pipeline on existing evaluation benchmarks, generating criterion-response-feedback triples. These triples are then used to fine-tune smaller open-source models to replicate both the criteria generation and evaluation steps. The approach is designed to prevent reward hacking by ensuring evaluation criteria remain relevant to the specific task at hand, rather than relying on static, potentially exploitable rubrics.

## Key Results
- 2.1% improvement on RewardBench in zero-shot evaluation settings
- 22.5% LC-WR and 21.1% WR achieved on Mistral-Base (7B) through preference optimization
- Successful knowledge distillation of CARMO functionality to smaller, cost-effective models
- Theoretical proof that adaptive criteria generation prevents reward hacking by avoiding spurious feature exploitation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic criteria generation prevents reward hacking by avoiding reliance on spurious features.
- Mechanism: CARMO generates task-specific evaluation criteria tailored to each user query, which are then used to score responses. This contrasts with static rubrics that models can exploit by optimizing for superficial features.
- Core assumption: The LLM can generate relevant criteria that capture the essential aspects of quality for each specific task.
- Evidence anchors:
  - [abstract]: "CARMO introduces context-aware reward modeling by dynamically generating task-specific evaluation criteria using large language models, which are then used to score responses. This approach addresses reward hacking in language model alignment, where models exploit spurious correlations in static rubrics."
  - [section 3.1]: "The primary motivation for CARMO stems from the limitations of fixed rubrics in a rapidly evolving environment of inference-time queries. Specifically, fixed rubrics are prone to reward hacking, especially when distribution shifts cause certain features to become spurious."
- Break condition: If the LLM fails to generate relevant criteria or if the criteria generation process is too slow/costly for practical deployment.

### Mechanism 2
- Claim: Knowledge distillation enables cost-effective deployment by transferring CARMO's functionality to smaller models.
- Mechanism: CARMO uses a large LLM to generate criteria and evaluate responses, then fine-tunes smaller open-source models to replicate this behavior, reducing computational costs.
- Core assumption: Smaller models can effectively learn to generate context-aware criteria and evaluate responses after training on CARMO's output.
- Evidence anchors:
  - [abstract]: "We further demonstrate that CARMO can be distilled into smaller models, thereby lowering the computational cost of alignment."
  - [section 3.4]: "We begin with a feedback collection dataset D containing tuples of the form {(x, r, y)}, possibly augmented by human or existing automated feedback. We then use M (e.g., GPT-4) to create dynamic criteria C(x) for each tuple and to produce a feedback label F and final score S(x, C(x), r, y)."
- Break condition: If the distilled models fail to maintain the same level of performance as the original CARMO or if the distillation process requires too much training data.

### Mechanism 3
- Claim: CARMO improves reward model performance by generating more reliable preference data.
- Mechanism: CARMO generates dynamic rubrics to compare candidate responses, producing more robust preference labels for training reward models, particularly in DPO and multi-preference settings.
- Core assumption: Preference data generated using context-aware criteria leads to better alignment than data from static rubrics.
- Evidence anchors:
  - [abstract]: "In addition, preference fine-tuning on CARMO -curated data yields strong gains for the Mistral-Base (7B) model, attaining 22.5% LC-WR (%) and 21.1% WR (%) in preference optimization."
  - [section 3.5]: "Using CARMO's dynamically generated rubrics to compare ya and yb yields more robust preferences, allowing subsequent fine-tuning methods such as Direct Preference Optimization (DPO) to focus on genuinely relevant features."
- Break condition: If the preference data generated by CARMO doesn't lead to better alignment than traditional methods or if the improvement is marginal in practice.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: CARMO operates within the RLHF framework, addressing reward hacking that occurs when models optimize for spurious features in static reward models.
  - Quick check question: What are the three main stages of traditional RLHF, and where does CARMO intervene in this process?

- Concept: Knowledge Distillation
  - Why needed here: CARMO uses knowledge distillation to transfer the capabilities of large LLMs to smaller, more cost-effective models while maintaining performance.
  - Quick check question: What is the key difference between standard fine-tuning and knowledge distillation in the context of model compression?

- Concept: Distribution Shift
  - Why needed here: CARMO addresses the problem of distribution shift where features that were correlated with quality during training become spurious in new contexts.
  - Quick check question: How does distribution shift relate to reward hacking in language model alignment?

## Architecture Onboarding

- Component map:
  LLM (GPT-4) -> Criteria Generation -> Response Evaluation -> Score Aggregation
  Feedback Collection -> Criteria Generation -> Evaluation -> Knowledge Distillation
  Preference Data Generation -> Fine-tuning -> Model Evaluation

- Critical path:
  1. User query → Criteria generation → Response evaluation → Score aggregation
  2. Feedback collection → Criteria generation → Evaluation → Knowledge distillation
  3. Preference data generation → Fine-tuning → Model evaluation

- Design tradeoffs:
  - Single-stage vs. two-stage prompt (efficiency vs. consistency)
  - Normal vs. detailed prompts (speed vs. thoroughness)
  - Closed-source vs. open-source models (performance vs. cost)
  - Full criteria generation vs. distilled models (accuracy vs. deployment cost)

- Failure signatures:
  - Inconsistent criteria generation across runs (sampling variability)
  - High token usage and computational cost
  - Degraded performance in distilled models
  - Bias in criteria generation due to probabilistic sampling

- First 3 experiments:
  1. Compare single-stage vs. two-stage CARMO prompts on RewardBench to measure consistency vs. efficiency tradeoff
  2. Test knowledge distillation by comparing CARMO vs. CARMO-Dist performance on HHH Alignment benchmark
  3. Evaluate preference data quality by comparing DPO performance using UltraFeedback vs. CARMO-curated datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact computational overhead of generating dynamic criteria in CARMO compared to static rubrics, and how does this scale with query complexity?
- Basis in paper: [explicit] The paper mentions that CARMO may generate a large number of tokens, increasing computational costs, especially when scaling up or deploying in resource-constrained environments.
- Why unresolved: While the paper acknowledges the potential for high token generation and computational costs, it does not provide specific metrics or benchmarks quantifying the overhead of dynamic criteria generation versus static rubrics.
- What evidence would resolve it: Detailed benchmarking data comparing the token generation and processing time of CARMO's dynamic criteria generation to static rubric evaluation across various query complexities and dataset sizes.

### Open Question 2
- Question: How does CARMO perform when evaluated on tasks requiring subjective judgment, such as creative writing or artistic critique, where evaluation criteria are inherently ambiguous?
- Basis in paper: [inferred] The paper demonstrates CARMO's effectiveness on structured tasks like programming and factual accuracy, but does not address its performance on subjective or creative tasks where evaluation criteria are less well-defined.
- Why unresolved: The paper focuses on objective metrics and structured evaluation tasks, leaving uncertainty about CARMO's ability to handle subjective judgment where criteria may be contested or context-dependent.
- What evidence would resolve it: Comparative studies evaluating CARMO against human judges on subjective tasks like creative writing assessment, art critique, or humor evaluation, measuring correlation with human judgments and consistency across different evaluators.

### Open Question 3
- Question: What is the long-term stability of CARMO's performance when the underlying distribution of queries shifts significantly over time, and how frequently must the model be retrained or updated?
- Basis in paper: [explicit] The paper's theoretical analysis discusses distribution shifts and reward hacking, suggesting that adaptive criteria generation helps with these issues, but does not provide empirical evidence of long-term performance stability.
- Why unresolved: While the theoretical framework addresses distribution shifts, the paper does not present longitudinal studies or degradation analysis showing how CARMO's performance changes over extended periods with evolving query distributions.
- What evidence would resolve it: Longitudinal studies tracking CARMO's performance metrics over months or years across multiple domains, comparing degradation rates with static rubric methods and identifying optimal retraining intervals based on performance thresholds.

## Limitations

- The paper relies heavily on closed-source LLMs (GPT-4) for core CARMO functionality, which may limit accessibility and raise deployment costs
- The performance gap between CARMO and CARMO-Dist remains unclear, and cost-effectiveness in production settings needs further validation
- The theoretical analysis of reward hacking prevention is conceptual rather than rigorously proven with quantitative analysis

## Confidence

- **High confidence**: The core claim that dynamic criteria generation improves reward model performance over static rubrics, supported by concrete benchmark improvements (2.1% RewardBench gain)
- **Medium confidence**: The effectiveness of knowledge distillation for CARMO, as the paper shows successful distillation but doesn't provide extensive ablation studies on distillation hyperparameters or performance degradation
- **Medium confidence**: The claim about preventing reward hacking through adaptive criteria, which is theoretically sound but lacks quantitative analysis of how much reward hacking is actually prevented

## Next Checks

1. **Ablation on prompt engineering**: Test the CARMO pipeline with different prompt variations (normal vs. detailed, single-stage vs. two-stage) across multiple benchmark datasets to quantify the impact of prompt design on performance and consistency.

2. **Robustness to distribution shift**: Evaluate CARMO's performance on out-of-distribution prompts where static rubrics typically fail, measuring both absolute evaluation accuracy and preference data quality compared to UltraFeedback.

3. **Cost-benefit analysis of distillation**: Compare the computational costs and performance of full CARMO (GPT-4-based) versus CARMO-Dist across different model sizes, including latency measurements and token usage analysis to assess real-world deployment viability.