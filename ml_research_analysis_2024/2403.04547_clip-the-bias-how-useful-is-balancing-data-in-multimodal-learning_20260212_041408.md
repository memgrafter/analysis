---
ver: rpa2
title: 'CLIP the Bias: How Useful is Balancing Data in Multimodal Learning?'
arxiv_id: '2403.04547'
source_url: https://arxiv.org/abs/2403.04547
tags:
- data
- representation
- bias
- size
- balancing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Data balancing can reduce representation and association biases
  in CLIP models, but with mixed effects on model performance. The proposed Multi-Modal
  Moment Matching (M4) algorithm balances both first- and second-order statistics
  in multimodal data.
---

# CLIP the Bias: How Useful is Balancing Data in Multimodal Learning?

## Quick Facts
- arXiv ID: 2403.04547
- Source URL: https://arxiv.org/abs/2403.04547
- Authors: Ibrahim Alabdulmohsin; Xiao Wang; Andreas Steiner; Priya Goyal; Alexander D'Amour; Xiaohua Zhai
- Reference count: 40
- One-line primary result: Data balancing can reduce representation and association biases in CLIP models, but with mixed effects on model performance

## Executive Summary
This paper investigates whether data balancing techniques can effectively mitigate biases in CLIP models. The authors introduce Multi-Modal Moment Matching (M4), an algorithm that balances both first- and second-order statistics in multimodal data. M4 effectively reduces representation bias when trained on balanced data, particularly when including proxy variables. However, the impact on association bias is less consistent, with proxies sometimes increasing AB. Fine-tuning on balanced data is effective for reducing representation bias but less so for association bias. While data balancing improves classification performance, it can hurt retrieval. The study concludes that data balancing is a promising strategy for mitigating biases in CLIP models, but should be combined with other intervention methods and carefully evaluated for downstream effects.

## Method Summary
The paper proposes Multi-Modal Moment Matching (M4), a data balancing algorithm that reweights training examples to ensure the marginal distribution of sensitive attributes matches a target distribution π. M4 operates on both first-order statistics (prevalence of sensitive attributes) and second-order statistics (correlations between sensitive attributes and proxy variables). The algorithm is applied during CLIP training using the WebLI dataset (1B image-text pairs), with perceived gender as the sensitive attribute and occupations as labels. The method is evaluated using two-stage training with different intervention percentages (0%, 10%, 90% first), various ViT architectures, and data quality filtering steps.

## Key Results
- M4 effectively reduces representation bias in CLIP models when trained on balanced data
- Adding proxy variables particularly benefits models with large representations trained on large datasets for representation bias
- Fine-tuning on balanced data is effective for reducing representation bias but less effective for association bias
- Data balancing improves classification performance but can hurt retrieval performance
- Improving data quality and model architecture can mitigate negative performance impacts of data balancing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data balancing can reduce representation bias (RB) by adjusting the prevalence of sensitive attributes in the training data.
- Mechanism: The M4 algorithm reweights training examples to ensure the marginal distribution of sensitive attributes matches a target distribution π, thereby reducing the maximum difference between π and the expected value of sensitive attributes in the data.
- Core assumption: Reducing RB in the data will transfer to the model, especially when the model has limited evidence to make subgroup distinctions.
- Evidence anchors:
  - [abstract]: "M4 effectively reduces representation bias (RB) in CLIP models when trained on balanced data"
  - [section]: "balancing the data – with or without proxies – helps in mitigating RB"
  - [corpus]: Weak evidence; corpus papers focus on bias identification rather than data balancing interventions.
- Break condition: If the model learns to rely on spurious correlations or proxy variables, data balancing may be insufficient to fully eliminate RB.

### Mechanism 2
- Claim: Adding proxy variables can help reduce representation bias by preventing the model from associating sensitive attributes with unrelated concepts.
- Mechanism: By decorrelating sensitive attributes against proxy variables (e.g., occupations, objects), M4 prevents the model from forming unintended associations that could perpetuate bias.
- Core assumption: Proxy variables capture indirect pathways through which sensitive attributes might be correlated with other features in the data.
- Evidence anchors:
  - [abstract]: "adding proxies seems particularly beneficial for models with large representations trained on large datasets"
  - [section]: "adding proxies seems particularly beneficial for models with large representations trained on large datasets"
  - [corpus]: Weak evidence; corpus papers do not discuss proxy-based debiasing.
- Break condition: If the set of proxies is incomplete or introduces new spurious correlations, adding proxies could increase association bias (AB) instead.

### Mechanism 3
- Claim: Fine-tuning on balanced data is effective for reducing representation bias but less effective for association bias.
- Mechanism: The last distribution seen by the model heavily influences its bias, so fine-tuning on balanced data can "unlearn" RB acquired from the original data. However, AB is learned gradually and is less sensitive to the last distribution.
- Core assumption: The model's bias is dynamic and depends on the training distribution seen during fine-tuning.
- Evidence anchors:
  - [abstract]: "fine-tuning is effective in countering representation biases, though its impact diminishes for association biases"
  - [section]: "the last distribution seen by the model, even if it is a meager 10% of the training duration, heavily impacts its parity"
  - [corpus]: Weak evidence; corpus papers do not discuss fine-tuning dynamics for bias mitigation.
- Break condition: If the balanced data does not cover the full range of contexts where bias manifests, fine-tuning may not fully eliminate RB or AB.

## Foundational Learning

- Concept: Contrastive learning in multimodal systems
  - Why needed here: CLIP models rely on aligning image and text representations via contrastive loss; understanding this is key to analyzing how biases transfer from data to model.
  - Quick check question: How does the contrastive loss encourage alignment between image and text embeddings?

- Concept: Representation and association bias definitions
  - Why needed here: The paper distinguishes between RB (differences in group prevalence) and AB (correlations between sensitive attributes and labels); understanding these is essential for interpreting results.
  - Quick check question: What is the difference between representation bias and association bias in the context of multimodal learning?

- Concept: Data balancing and reweighting techniques
  - Why needed here: M4 uses reweighting to adjust the distribution of sensitive attributes; understanding this is crucial for implementing and extending the algorithm.
  - Quick check question: How does the M4 algorithm ensure that sensitive attributes are balanced in the training data?

## Architecture Onboarding

- Component map: WebLI dataset (1B image-text pairs) -> M4 algorithm (reweighting) -> CLIP model (ViT-S/B with patch sizes 32x32 and 16x16; SigLIP with ViT-B/16) -> Evaluation (RB/AB metrics, classification/retrieval performance)
- Critical path: (1) Preprocess data with M4 to balance sensitive attributes and decorrelate with proxies. (2) Train CLIP on balanced data. (3) Evaluate bias reduction and model quality on downstream tasks.
- Design tradeoffs: Balancing data improves bias metrics but may hurt retrieval performance due to distribution shifts; adding proxies helps RB but can hurt AB; fine-tuning is effective for RB but less so for AB.
- Failure signatures: If RB or AB does not improve after data balancing, check if the target distribution π is appropriate or if proxies are missing key confounders. If model quality degrades, verify that the balanced data does not overly skew the human/non-human distribution.
- First 3 experiments:
  1. Train CLIP on original data and evaluate RB/AB to establish baseline.
  2. Apply M4 with balanced data (no proxies) and compare RB/AB and model quality.
  3. Apply M4 with balanced data and proxies, then compare RB/AB and model quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of data balancing scale with dataset size beyond 1 billion examples?
- Basis in paper: [inferred] The paper evaluates data balancing on 1 billion examples but does not explore scaling effects with larger datasets.
- Why unresolved: The paper does not investigate whether the observed benefits of data balancing persist or diminish as dataset size increases further.
- What evidence would resolve it: Experiments training CLIP models on datasets significantly larger than 1 billion examples (e.g., 10B+) with and without data balancing, comparing bias metrics and downstream performance.

### Open Question 2
- Question: Can data balancing effectively mitigate biases related to sensitive attributes beyond perceived gender, such as race, age, or intersectional identities?
- Basis in paper: [explicit] The paper focuses on perceived gender as the sensitive attribute but mentions that data balancing could potentially handle other attributes.
- Why unresolved: The paper only evaluates data balancing on perceived gender, leaving the effectiveness for other sensitive attributes unexplored.
- What evidence would resolve it: Experiments training CLIP models with data balancing applied to sensitive attributes like race, age, or intersectional identities, evaluating changes in bias metrics and downstream performance.

### Open Question 3
- Question: How do different data balancing algorithms compare in terms of bias mitigation and impact on model quality for multimodal systems?
- Basis in paper: [explicit] The paper introduces a novel data balancing algorithm (M4) but does not compare it against other state-of-the-art methods.
- Why unresolved: The paper only evaluates the proposed M4 algorithm without benchmarking against alternative data balancing approaches.
- What evidence would resolve it: Comparative experiments training CLIP models with different data balancing algorithms (e.g., M4, correlation remover, reduce-to-binary) and evaluating their relative effectiveness in bias mitigation and impact on model quality.

## Limitations
- Effectiveness of data balancing varies significantly across different bias types and model architectures
- Impact of proxy variables is inconsistent - can increase association bias despite reducing representation bias
- Study focuses primarily on gender as sensitive attribute, limiting generalizability to other protected characteristics

## Confidence
- High confidence: Data balancing effectiveness for reducing representation bias
- Medium confidence: Mixed effects on association bias, particularly regarding proxy variables
- Medium confidence: Performance trade-offs between classification and retrieval tasks
- Low confidence: Generalization to other sensitive attributes beyond gender

## Next Checks
1. **Cross-attribute validation**: Test the M4 algorithm and data balancing approach on additional sensitive attributes (race, age) to verify generalizability beyond gender bias.
2. **Retrieval performance optimization**: Investigate techniques to maintain retrieval performance while balancing data, such as curriculum learning or adaptive balancing strategies that preserve retrieval-relevant distribution characteristics.
3. **Proxy variable selection methodology**: Develop systematic criteria for selecting proxy variables that reduce representation bias without introducing new association biases, potentially through causal analysis of proxy-sensitive attribute relationships.