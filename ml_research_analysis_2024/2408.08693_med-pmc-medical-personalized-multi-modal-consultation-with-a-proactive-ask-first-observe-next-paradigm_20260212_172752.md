---
ver: rpa2
title: 'Med-PMC: Medical Personalized Multi-modal Consultation with a Proactive Ask-First-Observe-Next
  Paradigm'
arxiv_id: '2408.08693'
source_url: https://arxiv.org/abs/2408.08693
tags:
- patient
- doctor
- information
- medical
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces Med-PMC, a novel framework for evaluating
  Multi-modal Large Language Models (MLLMs) in medical clinical scenarios. Med-PMC
  simulates a clinical environment where MLLMs interact with a patient simulator to
  gather multimodal information and make diagnostic decisions.
---

# Med-PMC: Medical Personalized Multi-modal Consultation with a Proactive Ask-First-Observe-Next Paradigm

## Quick Facts
- arXiv ID: 2408.08693
- Source URL: https://arxiv.org/abs/2408.08693
- Authors: Hongcheng Liu; Yusheng Liao; Siqv Ou; Yuhao Wang; Heyang Liu; Yanfeng Wang; Yu Wang
- Reference count: 40
- Primary result: Med-PMC framework evaluates MLLMs in medical clinical scenarios, revealing limitations in multimodal information gathering and potential biases.

## Executive Summary
Med-PMC introduces a novel framework for evaluating Multi-modal Large Language Models (MLLMs) in medical clinical scenarios through a simulated clinical environment with patient simulators. The framework incorporates personalized patient actors to enhance realism and diversity in interactions, allowing for comprehensive assessment of MLLMs' clinical performance. Experiments on 12 MLLMs demonstrate that current models struggle with multimodal information gathering and exhibit potential biases, providing valuable insights for developing more robust medical AI systems.

## Method Summary
Med-PMC builds a simulated clinical environment where MLLMs interact with a patient simulator and technician agent to gather multimodal information and make diagnostic decisions. The framework uses 30 medical cases from a medical website focusing on General Surgery, including patient information, chief complaint, present illness, medical history, examination, diagnosis, treatment, and 1-2 medical images. Twelve types of MLLMs serve as doctor models, with Qwen-Max as the backbone for the patient-actor agent and GPT-4o for evaluation. The evaluation focuses on information gathering (recall scores for basic info, examination info, multimodal info) and decision-making (recall rates for diagnosis and treatment), along with LLM-based evaluation on a scale of 1-5.

## Key Results
- Current MLLMs fail to effectively gather multimodal information, particularly medical images, falling short of clinical standards
- Personalized patient actors significantly impact MLLMs' decision-making performance, revealing potential biases in clinical reasoning
- Zero-shot Chain-of-Thought prompting mitigates patient-actor variability effects, with only -1.38 point decline in decision-making performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Med-PMC's simulated clinical environment with patient-actor agents improves the realism of evaluating MLLMs in medical tasks.
- Mechanism: The patient-actor agent simulates diverse patient personas (gender, occupation) and generates responses based on detected doctor actions, creating dynamic multi-turn interactions.
- Core assumption: MLLMs perform differently when interacting with personalized actors compared to standard patients, reflecting real-world variability.
- Evidence anchors:
  - [abstract] "the patient simulator is decorated with personalized actors to simulate diverse patients in real scenarios"
  - [section] "The patient-actor agent also simulates patient diversity, providing a more realistic and varied clinical setting"
  - [corpus] Weak evidence; related works focus on similar multi-modal agents but not specifically patient-actor diversity in clinical diagnosis
- Break condition: If MLLMs cannot handle varied patient expressions or if actor responses introduce artifacts unrelated to real patient behavior.

### Mechanism 2
- Claim: Med-PMC's multi-modal information gathering assessment captures MLLMs' limitations in handling complex medical data.
- Mechanism: The framework evaluates information gathering across three modalities (basic, examination, multi-modal) and decision-making (diagnosis, treatment), revealing specific weaknesses.
- Core assumption: Performance gaps in multi-modal analysis versus other modalities indicate MLLMs' current inability to fully leverage medical images.
- Evidence anchors:
  - [abstract] "current MLLMs fail to gather multimodal information and show potential bias in the decision-making task"
  - [section] "MLLMs are currently unable to effectively analyze medical images, falling short of clinical standards"
  - [corpus] Weak evidence; no direct comparison of multi-modal vs single-modal performance in existing benchmarks
- Break condition: If MLLMs show equivalent performance across all modalities or if the assessment metric fails to differentiate model capabilities.

### Mechanism 3
- Claim: Step-by-step reasoning (CoT) prompts mitigate the impact of patient-actor variability on MLLMs' decision-making.
- Mechanism: Zero-shot and one-shot CoT encourage systematic reasoning before generating responses, improving consistency across diverse patient expressions.
- Core assumption: MLLMs benefit from structured reasoning when handling variable patient information.
- Evidence anchors:
  - [section] "Zero-shot CoT significantly mitigates the impact of the patient-actor, with decision-making performance declining by only -1.38 points"
  - [section] "one-shot CoT noticeably enhances model performance, particularly in the ability to gather examination information"
  - [corpus] No direct evidence in corpus; CoT application in medical MLLMs not well-established
- Break condition: If CoT prompts do not improve consistency or if they introduce new failure modes unrelated to patient variability.

## Foundational Learning

- Concept: Multi-modal information processing
  - Why needed here: MLLMs must integrate text, images, and structured data to perform clinical tasks
  - Quick check question: Can the MLLM correctly interpret both a patient's verbal description and corresponding X-ray image to make a diagnosis?

- Concept: Interactive simulation and role-playing
  - Why needed here: The patient-actor agent requires understanding of dialogue dynamics and persona consistency
  - Quick check question: Does the patient-actor maintain consistent persona while responding appropriately to different types of doctor inquiries?

- Concept: Clinical decision-making and bias
  - Why needed here: Evaluating MLLMs requires understanding of diagnostic reasoning and potential biases in medical AI
  - Quick check question: Can you identify whether an MLLM's diagnostic suggestion shows bias based on patient demographics?

## Architecture Onboarding

- Component map:
  Doctor models (12 types of MLLMs) -> Patient-actor agent (state tracker, response generator, personalized actor) -> Technician agent (examination detection and result provision) -> Evaluation framework (automatic metrics, LLM-based evaluation) -> Case database (30 medical cases with multi-modal data)

- Critical path:
  1. Load medical case with patient information (text, images, examination results)
  2. Initialize doctor model interaction
  3. Patient-actor agent processes doctor's action through state detection
  4. Generate appropriate patient response based on action type and case data
  5. Continue multi-turn dialogue until sufficient information gathered
  6. Doctor model generates diagnosis and treatment report
  7. Evaluate performance using automatic and LLM-based metrics

- Design tradeoffs:
  - Realism vs. control: Patient-actor adds variability but may introduce artifacts
  - Complexity vs. interpretability: Multi-modal evaluation provides comprehensive assessment but is harder to analyze
  - Generalizability vs. specificity: Framework tests specific clinical skills but may not capture all real-world scenarios

- Failure signatures:
  - MLLMs consistently fail to gather examination information despite patient-actor providing it
  - Performance drops significantly when interacting with patient-actor vs. standard patient
  - MLLMs show high bias in diagnosis based on patient demographics

- First 3 experiments:
  1. Test all doctor models with a standard patient case to establish baseline performance
  2. Run the same case with patient-actor to measure impact of persona variability
  3. Compare performance with and without CoT prompting to assess reasoning benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MLLMs differ when interacting with patient simulators that have different levels of complexity in their personality traits?
- Basis in paper: [inferred] The paper discusses the use of personalized patient actors with different personas (e.g., farmer, student, worker, office worker, doctor) but does not explore the impact of varying the complexity of these personalities on model performance.
- Why unresolved: The paper only mentions the use of 10 personas but does not analyze whether more complex or nuanced personalities lead to better or worse model performance.
- What evidence would resolve it: Comparative studies of MLLM performance with patient simulators having varying levels of personality complexity, including both simple and highly nuanced personas.

### Open Question 2
- Question: Can the Med-PMC framework be extended to evaluate MLLMs in multi-disease scenarios where patients present with overlapping symptoms from multiple conditions?
- Basis in paper: [explicit] The paper mentions that the framework uses 30 medical cases from the Department of General Surgery, but it does not explore scenarios involving patients with multiple concurrent diseases.
- Why unresolved: The paper focuses on single-disease cases and does not address the challenge of evaluating MLLMs in more complex, real-world scenarios where patients may have multiple overlapping conditions.
- What evidence would resolve it: Experimental results showing how MLLMs perform in multi-disease scenarios, including metrics for accuracy, reasoning, and decision-making.

### Open Question 3
- Question: What is the impact of incorporating real-time patient feedback (e.g., facial expressions, tone of voice) into the Med-PMC framework on MLLM performance?
- Basis in paper: [inferred] The paper describes a text-based interaction between MLLMs and patient simulators but does not explore the potential benefits of incorporating non-verbal cues or real-time feedback.
- Why unresolved: The current framework relies solely on textual information, which may not fully capture the nuances of patient-doctor interactions in real-world clinical settings.
- What evidence would resolve it: Comparative studies of MLLM performance with and without the inclusion of real-time patient feedback, such as facial expressions or tone of voice, in the Med-PMC framework.

## Limitations
- The evaluation relies on simulated interactions rather than actual clinical data, which may not capture all nuances of real doctor-patient communication
- Performance metrics, while comprehensive, are based on automated and LLM-based evaluation rather than clinical expert validation
- The framework focuses on General Surgery cases, potentially limiting applicability to other medical specialties

## Confidence
- Core findings about MLLMs' multimodal information gathering limitations: High confidence
- Claims about patient-actor variability impact: Medium confidence
- Effectiveness of CoT prompting in mitigating variability: Low confidence

## Next Checks
1. Validate framework results with a panel of clinical experts across different medical specialties to assess real-world applicability
2. Test model performance on a larger, more diverse set of medical cases including rare conditions and complex comorbidities
3. Conduct A/B testing comparing MLLM performance in the simulated environment versus actual clinical consultation transcripts to measure ecological validity