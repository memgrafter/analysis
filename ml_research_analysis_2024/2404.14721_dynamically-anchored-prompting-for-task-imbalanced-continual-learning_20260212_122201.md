---
ver: rpa2
title: Dynamically Anchored Prompting for Task-Imbalanced Continual Learning
arxiv_id: '2404.14721'
source_url: https://arxiv.org/abs/2404.14721
tags:
- prompt
- learning
- task
- tasks
- ticl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses task-imbalanced continual learning (TICL),
  where tasks arrive with non-uniform data distributions. The authors identify that
  imbalanced tasks challenge the trade-off between stability (retaining past knowledge)
  and plasticity (adapting to new tasks) in prompt-based continual learning methods.
---

# Dynamically Anchored Prompting for Task-Imbalanced Continual Learning

## Quick Facts
- arXiv ID: 2404.14721
- Source URL: https://arxiv.org/abs/2404.14721
- Authors: Chenxing Hong; Yan Jin; Zhiqi Kang; Yizhou Chen; Mengke Li; Yang Lu; Hanzi Wang
- Reference count: 12
- Primary result: Achieves 4.5% to 15% absolute improvements over state-of-the-art methods on task-imbalanced continual learning benchmarks

## Executive Summary
This paper addresses task-imbalanced continual learning (TICL), where tasks arrive with non-uniform data distributions. The authors identify that imbalanced tasks challenge the trade-off between stability (retaining past knowledge) and plasticity (adapting to new tasks) in prompt-based continual learning methods. To tackle this, they propose Dynamically Anchored Prompting (DAP), a method that maintains a single general prompt and uses two specialized anchors - boosting and stabilizing - to regularize the prompt space. DAP dynamically adjusts the balance between stability and plasticity based on task attributes. The method achieves 4.5% to 15% absolute improvements over state-of-the-art methods on benchmarks under task-imbalanced settings, while only storing a single prompt, offering a substantial advantage in rehearsal-free CL.

## Method Summary
DAP employs a two-phase learning scheme for each training task. In the first phase, a task-specific prompt is optimized for the current task. In the second phase, this task-specific prompt is used to regularize a general prompt through anchor alignment with two specialized anchors: a boosting anchor (pb) that maintains plasticity by capturing task-relevant information, and a stabilizing anchor (ps) that ensures stability by monitoring learned task-specific prompts and preventing knowledge forgetting. The general prompt is dynamically updated across the data stream using a novel dynamic stability-plasticity regularization (DSPR) strategy that adjusts the balance between stability and plasticity based on task size differences.

## Key Results
- Achieves 4.5% to 15% absolute improvements over state-of-the-art methods on benchmarks under task-imbalanced settings
- Maintains only a single general prompt across all tasks, offering substantial memory efficiency in rehearsal-free CL
- Demonstrates effectiveness on CIFAR-100 and ImageNet-R datasets under various task-imbalanced scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dynamic stability-plasticity regularization (DSPR) strategy effectively balances stability and plasticity by adjusting the regularization coefficient λ based on task size differences.
- Mechanism: The factor λ modulates the balance between stability and plasticity by considering the size of the current task t and the sizes of past tasks. A larger current task size results in a smaller λ, emphasizing stability to prevent forgetting, while a smaller current task size results in a larger λ, emphasizing plasticity for learning new information.
- Core assumption: Task size is a reliable indicator of learning difficulty, with larger tasks being easier to learn and smaller tasks requiring more emphasis on plasticity.
- Evidence anchors:
  - [abstract]: "The general prompt is updated by a novel dynamic stability-plasticity regularization (DSPR) strategy, which dynamically regularizes the general prompt in the prompt space based on task attributes, ensuring a flexible and adaptive learning process."
  - [section]: "We propose dynamic stability-plasticity factor λ as the coefficient between two anchor alignments. The factor λ modulates the balance between stability and plasticity by considering the size of the current task t and the sizes of past tasks."
- Break condition: If task size is not a reliable indicator of learning difficulty, or if the min-max normalization fails to capture the appropriate balance between stability and plasticity.

### Mechanism 2
- Claim: The anchored prompting approach with boosting and stabilizing anchors effectively decouples stability and plasticity, allowing for targeted regularization of the general prompt.
- Mechanism: The boosting anchor (pb) maintains model plasticity by capturing task-relevant information for the current task, while the stabilizing anchor (ps) ensures model stability by monitoring the learned task-specific prompts and preventing knowledge forgetting from past tasks. These anchors are used to regularize the general prompt through anchor alignment using cosine similarity.
- Core assumption: The task-specific prompts optimized for each task can effectively capture task-relevant information and represent stability when combined as a weighted average.
- Evidence anchors:
  - [abstract]: "The general prompt is regularized in the prompt space with two specifically designed prompt anchors, called boosting anchor and stabilizing anchor, to balance stability and plasticity in TICL."
  - [section]: "The stabilizing anchor ps is designed to prevent knowledge forgetting from past tasks by monitoring the learned task-specific prompts. To maintain the knowledge of all of the learned tasks, ps is calculated by the weighted center of the boosting anchors of all learned tasks in the prompt space from task 0 to the current task t."
- Break condition: If the task-specific prompts do not effectively capture task-relevant information or if the weighted average of past prompts does not adequately represent stability.

### Mechanism 3
- Claim: Using a single general prompt across all tasks offers significant advantages in rehearsal-free continual learning by reducing memory requirements and avoiding prompt selection errors.
- Mechanism: DAP maintains a single general prompt that is dynamically updated across the data stream using the two-phase learning scheme and dynamic stability-plasticity regularization. This approach avoids the need for a prompt pool and prompt selection, making it more efficient and less prone to errors.
- Core assumption: A single, well-regularized general prompt can effectively generalize across all tasks in the data stream, even with imbalanced task distributions.
- Evidence anchors:
  - [abstract]: "Remarkably, DAP achieves this balance by only storing a prompt across the data stream, therefore offering a substantial advantage in rehearsal-free CL."
  - [section]: "The proposed DAP adopts a two-phase learning scheme for each training task, termed in-task phases. Given a specific training task, during the first in-task phase, we optimize a task-specific prompt, which is then utilized to regularize the general prompt during the second in-task phase."
- Break condition: If the general prompt cannot effectively generalize across all tasks, or if the two-phase learning scheme fails to adequately regularize the general prompt.

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: Understanding catastrophic forgetting is crucial to grasp the importance of balancing stability and plasticity in TICL.
  - Quick check question: What is catastrophic forgetting, and why is it a significant challenge in continual learning?

- Concept: Prompt-based learning and fine-tuning
  - Why needed here: DAP is a prompt-based method, so understanding the basics of prompt-based learning and how it differs from traditional fine-tuning is essential.
  - Quick check question: How does prompt-based learning differ from traditional fine-tuning, and what are the advantages of using prompts in continual learning?

- Concept: Long-tail distribution and class imbalance
  - Why needed here: TICL assumes a long-tail distribution of task sizes, which is a common characteristic of real-world data. Understanding this concept helps in appreciating the challenges and solutions proposed in DAP.
  - Quick check question: What is a long-tail distribution, and how does it relate to class imbalance in machine learning?

## Architecture Onboarding

- Component map:
  Pre-trained ViT-Base model (frozen feature extractor) -> General prompt (single, dynamically updated) -> Task-specific prompts (optimized per task) -> Boosting anchor (pb) -> Stabilizing anchor (ps) -> Dynamic stability-plasticity regularization (DSPR)

- Critical path:
  1. Initialize general prompt and task-specific prompt for the first task
  2. Optimize task-specific prompt in the first in-task phase
  3. Update stabilizing anchor using the optimized task-specific prompt
  4. Optimize general prompt in the second in-task phase using DSPR
  5. Repeat steps 2-4 for each subsequent task
  6. Use the final general prompt for inference across all tasks

- Design tradeoffs:
  - Memory efficiency vs. task-specific performance: Using a single general prompt reduces memory requirements but may sacrifice some task-specific performance compared to maintaining separate prompts for each task.
  - Complexity of DSPR vs. manual tuning: The dynamic nature of DSPR eliminates the need for manual tuning of the balance between stability and plasticity but introduces additional complexity in the algorithm.

- Failure signatures:
  - Poor performance on data-rich tasks: May indicate insufficient emphasis on stability, leading to forgetting of previously learned knowledge.
  - Poor performance on data-scarce tasks: May indicate insufficient emphasis on plasticity, hindering the ability to learn new information from limited data.
  - Gradual performance degradation across tasks: May indicate issues with the DSPR strategy or the general prompt's ability to effectively generalize across tasks.

- First 3 experiments:
  1. Verify the effectiveness of the dynamic stability-plasticity factor λ by comparing performance with fixed values of λ (0, 0.5, 1) on a simple TICL benchmark.
  2. Evaluate the impact of the two-phase learning scheme by comparing performance with and without the first in-task phase (task-specific prompt optimization) on a TICL benchmark.
  3. Assess the memory efficiency of DAP by comparing the number of parameters stored during training and inference with other prompt-based CL methods on a TICL benchmark.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Dynamically Anchored Prompting (DAP) method perform on other real-world datasets with long-tail distributions beyond CIFAR-100 and ImageNet-R?
- Basis in paper: [explicit] The paper evaluates DAP on CIFAR-100 and ImageNet-R datasets, but does not explore its performance on other datasets.
- Why unresolved: The paper only provides results on two datasets, limiting the generalizability of the findings.
- What evidence would resolve it: Testing DAP on a diverse set of real-world datasets with long-tail distributions and comparing its performance to other state-of-the-art methods.

### Open Question 2
- Question: How does the dynamic stability-plasticity regularization factor λ in DAP adapt to different task imbalance scenarios, such as more extreme imbalances or different types of imbalance distributions?
- Basis in paper: [explicit] The paper proposes a dynamic stability-plasticity regularization factor λ that adjusts based on task sizes, but does not explore its behavior under various imbalance scenarios.
- Why unresolved: The paper only evaluates DAP under the three specific cases of task-imbalanced continual learning (TICL) and does not investigate its adaptability to other imbalance scenarios.
- What evidence would resolve it: Conducting experiments with DAP on datasets with different types and degrees of imbalance, and analyzing the behavior of the λ factor in these scenarios.

### Open Question 3
- Question: How does the performance of DAP compare to other state-of-the-art methods in task-balanced continual learning scenarios, where tasks have equal data distribution?
- Basis in paper: [inferred] The paper focuses on task-imbalanced continual learning and does not provide a comparison with other methods in task-balanced scenarios.
- Why unresolved: The paper does not explore the performance of DAP in the traditional task-balanced continual learning setting, limiting its applicability.
- What evidence would resolve it: Comparing the performance of DAP with other state-of-the-art methods on task-balanced continual learning benchmarks and analyzing its effectiveness in this setting.

## Limitations

- The reliance on task size as the sole indicator for adjusting the stability-plasticity balance may not capture the full complexity of learning difficulty in real-world scenarios
- The assumption that a single general prompt can effectively generalize across all tasks may face challenges with highly diverse task distributions or significant domain shifts
- The method's performance on tasks with significant domain shifts or semantic differences is not explicitly addressed in the current evaluation

## Confidence

- Mechanism 1 (DSPR): Medium - The approach is well-motivated but relies on a simplifying assumption about task size
- Mechanism 2 (Anchored prompting): High - The theoretical foundation is solid and the empirical results are convincing
- Mechanism 3 (Single prompt efficiency): High - The memory efficiency claim is clearly demonstrated and significant

## Next Checks

1. Test DAP's performance on datasets with tasks that have similar sizes but different levels of complexity to validate whether the task size heuristic appropriately captures learning difficulty

2. Evaluate DAP's robustness to domain shifts by introducing tasks with different semantic characteristics while maintaining the same task size distribution

3. Conduct ablation studies on the two-phase learning scheme by comparing performance when skipping the task-specific prompt optimization phase versus maintaining it across all tasks