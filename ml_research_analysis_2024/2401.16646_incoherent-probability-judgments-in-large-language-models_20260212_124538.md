---
ver: rpa2
title: Incoherent Probability Judgments in Large Language Models
arxiv_id: '2401.16646'
source_url: https://arxiv.org/abs/2401.16646
tags:
- probability
- judgments
- llms
- bayesian
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the coherence of probability judgments
  generated by large language models (LLMs), finding that they exhibit systematic
  deviations from probability theory similar to human judgments. The authors evaluate
  four state-of-the-art LLMs using probabilistic identities and repeated judgments
  on weather and political events.
---

# Incoherent Probability Judgments in Large Language Models

## Quick Facts
- arXiv ID: 2401.16646
- Source URL: https://arxiv.org/abs/2401.16646
- Authors: Jian-Qiao Zhu; Thomas L. Griffiths
- Reference count: 3
- Primary result: LLMs exhibit systematic deviations from probability theory in ways that mirror human judgments

## Executive Summary
This paper investigates the coherence of probability judgments generated by large language models (LLMs), finding that they exhibit systematic deviations from probability theory similar to human judgments. The authors evaluate four state-of-the-art LLMs using probabilistic identities and repeated judgments on weather and political events. Results show that LLM probability judgments display human-like biases in probabilistic identities and inverted-U-shaped mean-variance relationships in repeated judgments. These patterns persist across different model sizes and temperature settings. The authors propose that autoregressive training objectives lead LLMs to perform implicit Bayesian inference, aligning their behavior with the Bayesian Sampler model of human probability judgments. This suggests that coherence and accuracy in probability judgments are distinct properties, with implications for improving AI probability estimation.

## Method Summary
The study evaluates four LLMs (GPT-4, GPT-3.5, LLaMA-2-70b, LLaMA-2-7b) using 24 event pairs (weather and political events) to assess probability judgment coherence. The authors query models with structured prompts to obtain probability judgments for basic events and their combinations, then calculate eight probabilistic identities (Z1-Z8) to measure deviations from coherence. They also analyze repeated judgments to examine mean-variance relationships. Temperature settings of 0 and 1 are used to control stochasticity. Results are compared against human judgment patterns to identify similarities in systematic biases.

## Key Results
- LLMs show systematic deviations from probability theory in probabilistic identities, similar to human judgments
- Repeated probability judgments exhibit inverted-U-shaped mean-variance relationships like human judgments
- Larger models show improved coherence with reduced deviations from zero in probabilistic identities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Autoregressive LLMs perform implicit Bayesian inference during probability judgment generation
- Mechanism: The autoregressive next-token prediction objective can be reformulated as Bayesian inference over latent event probabilities, where the model updates beliefs about θ (underlying event probability) based on the prompt context
- Core assumption: The exchangeability assumption holds such that p(wn+1|w1:n) can be decomposed into an integral over p(θ|w1:n)
- Evidence anchors:
  - [abstract] "We propose that these deviations from rationality can be explained by linking autoregressive LLMs to implicit Bayesian inference"
  - [section] "Equation 10 links the concept of learning to fit the autoregressive distribution with conducting Bayesian inference on the latent generative process"
  - [corpus] Weak evidence - no direct corpus support for this Bayesian interpretation mechanism
- Break condition: If the exchangeability assumption fails or if the model's internal representations don't align with Bayesian updating principles

### Mechanism 2
- Claim: Probability judgments follow an inverted-U shaped mean-variance relationship similar to human judgments
- Mechanism: When probability judgments are generated through sampling from posterior distributions (as in Bayesian inference), values near 0.5 have higher variance due to uncertainty, while extreme values have lower variance
- Core assumption: The variance of repeated judgments follows the theoretical relationship derived from Beta-binomial sampling
- Evidence anchors:
  - [abstract] "when prompted to judge the same event, the mean-variance relationship of probability judgments produced by LLMs shows an inverted-U-shaped like that seen in humans"
  - [section] "By rearranging the predicted mean and variance, we derive the following relationship... In this equation, the inverted-U shape is a result of the quadratic relationship between mean and variance"
  - [corpus] Weak evidence - no direct corpus support for the inverted-U relationship
- Break condition: If repeated judgments don't show increased variance near 0.5 values or if temperature settings don't affect the relationship

### Mechanism 3
- Claim: Larger model size correlates with improved coherence in probability judgments
- Mechanism: Increased model parameters provide better approximation of underlying event probabilities, reducing deviations from zero in probabilistic identities
- Core assumption: Model capacity directly impacts the quality of implicit Bayesian inference
- Evidence anchors:
  - [section] "On average, model variants characterized by an increased number of model parameters exhibit reduced deviations from zero in probabilistic identities"
  - [section] "Variants with a greater number of model parameters display reduced variability and a less pronounced 'shrinkage effect'"
  - [corpus] Weak evidence - no direct corpus support for model size affecting coherence
- Break condition: If increased parameters don't reduce incoherence or if other factors (like training data quality) dominate the effect

## Foundational Learning

- Concept: Probabilistic identities and coherence
  - Why needed here: The paper uses probabilistic identities (Z1-Z8) as a rigorous measure of coherence, where deviations from zero indicate incoherence
  - Quick check question: What should be the value of a probabilistic identity if probability judgments are perfectly coherent according to probability theory?

- Concept: Bayesian inference and posterior distributions
  - Why needed here: The proposed mechanism links LLM behavior to Bayesian inference, where probability judgments are samples from posterior distributions over latent event probabilities
  - Quick check question: How does the Bayesian Sampler model differ from the PT+N model in explaining human probability judgment patterns?

- Concept: Autoregressive models and next-token prediction
  - Why needed here: LLMs generate probability judgments through autoregressive processes, which the paper connects to implicit Bayesian inference
  - Quick check question: What role does the temperature parameter play in controlling the precision of the probability distribution over next tokens?

## Architecture Onboarding

- Component map: LLM architecture (GPT-4, GPT-3.5, LLaMA-2 variants) -> System prompt -> Probability judgment generation -> Probabilistic identity calculation -> Coherence analysis
- Critical path: Model -> Prompt -> Probability judgment -> Identity calculation -> Coherence assessment
- Design tradeoffs: Larger models show better coherence but require more computational resources; temperature settings affect stochasticity vs determinism in judgments
- Failure signatures: Consistent deviation from zero in probabilistic identities; mean-variance relationships not following inverted-U shape; lack of improvement with increased model size
- First 3 experiments:
  1. Test probabilistic identity coherence across different temperature settings (0, 0.5, 1.0) for a single model
  2. Compare mean-variance relationships for repeated judgments at different model sizes
  3. Validate whether increasing sample size N in the Bayesian Sampler model reduces incoherence in identities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed link between autoregressive training and Bayesian inference fully account for all observed patterns in LLM probability judgments, or are there additional factors at play?
- Basis in paper: [explicit] The authors propose that autoregressive training leads to implicit Bayesian inference, explaining the human-like biases in LLM probability judgments. However, they acknowledge that their results do not qualitatively differentiate between the PT+N and Bayesian Sampler models.
- Why unresolved: The paper focuses on the Bayesian Sampler model but doesn't definitively rule out the PT+N model or other potential explanations for the observed patterns.
- What evidence would resolve it: Further research comparing the predictions of the PT+N model and the Bayesian Sampler model on LLM responses, along with experiments designed to isolate the effects of autoregressive training from other factors, could help determine the most accurate explanation.

### Open Question 2
- Question: How do the size and architecture of LLMs influence the coherence and accuracy of their probability judgments, and is there a point of diminishing returns?
- Basis in paper: [explicit] The authors observe that larger LLMs tend to exhibit reduced deviations from zero in probabilistic identities and less variability in repeated judgments. However, they don't explore the relationship between model size, architecture, and the trade-off between coherence and accuracy.
- Why unresolved: The paper doesn't investigate the specific mechanisms by which model size and architecture affect probability judgment coherence and accuracy, nor does it explore potential limits to the benefits of increasing model size.
- What evidence would resolve it: Systematic experiments varying model size, architecture, and training data while measuring coherence and accuracy could reveal the relationships and potential limits of these factors.

### Open Question 3
- Question: Can the coherence of LLM probability judgments be improved through targeted training or fine-tuning, and what would be the most effective approaches?
- Basis in paper: [inferred] The authors suggest that adjusting the degree of incoherence in LLM outputs could be a promising approach to improving probability estimation accuracy. However, they don't explore specific methods for achieving this.
- Why unresolved: The paper identifies the problem of incoherent probability judgments in LLMs but doesn't propose concrete solutions or training strategies to address this issue.
- What evidence would resolve it: Experiments testing different training techniques, such as incorporating coherence constraints into the loss function or fine-tuning on datasets designed to promote coherent probability judgments, could determine the most effective approaches to improving LLM probability estimation.

## Limitations

- The interpretation of LLM probability judgments as samples from Bayesian posterior distributions remains a theoretical construct without direct empirical validation
- The study uses only 24 event pairs, which may limit generalizability despite providing sufficient statistical power for detecting systematic patterns
- The observed correlation between model size and coherence could be influenced by confounding factors such as training data quality or optimization procedures

## Confidence

**High Confidence**: The empirical findings that LLMs produce coherent probability judgments (deviations from zero in probabilistic identities) and exhibit mean-variance relationships similar to human judgments.

**Medium Confidence**: The interpretation that these patterns arise from implicit Bayesian inference during autoregressive generation.

**Low Confidence**: The claim that larger model size necessarily improves coherence, as this could be influenced by multiple confounding factors.

## Next Checks

1. **Ablation Study on Autoregressive Architecture**: Test whether removing the autoregressive component eliminates the observed coherence patterns to provide direct evidence for or against the proposed mechanism.

2. **Internal State Analysis**: Examine internal representations and attention patterns during probability judgment generation to determine if they align with Bayesian updating principles.

3. **Cross-Cultural Validation**: Repeat experiments with event pairs from different cultural contexts to assess whether observed patterns generalize beyond the specific events used in this study.