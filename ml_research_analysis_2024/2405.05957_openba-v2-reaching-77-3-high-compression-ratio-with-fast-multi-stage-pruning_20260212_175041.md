---
ver: rpa2
title: 'OpenBA-V2: Reaching 77.3% High Compression Ratio with Fast Multi-Stage Pruning'
arxiv_id: '2405.05957'
source_url: https://arxiv.org/abs/2405.05957
tags:
- pruning
- data
- arxiv
- training
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OpenBA-V2, a 3.4B parameter language model
  derived from a 15B model via multi-stage compression and continual pre-training.
  The authors employ layer pruning, neural pruning, and vocabulary pruning techniques
  to achieve a 77.3% compression ratio with minimal performance loss.
---

# OpenBA-V2: Reaching 77.3% High Compression Ratio with Fast Multi-Stage Pruning

## Quick Facts
- **arXiv ID**: 2405.05957
- **Source URL**: https://arxiv.org/abs/2405.05957
- **Reference count**: 20
- **Primary result**: 77.3% compression ratio from 15B to 3.4B parameters with minimal performance loss

## Executive Summary
OpenBA-V2 is a compressed 3.4B parameter language model derived from a 15B parameter OpenBA model through multi-stage pruning techniques. The authors employ layer pruning, neural pruning, and vocabulary pruning to achieve a 77.3% compression ratio while maintaining competitive performance. The model is trained on 700B tokens using a dynamic UL2 objective and demonstrates results close to or on par with the original 15B OpenBA model in downstream tasks including commonsense reasoning and Named Entity Recognition.

## Method Summary
The method employs a three-stage compression pipeline: first layer pruning sequentially reduces model depth with recovery training between stages, then neural pruning removes non-critical weights, and finally vocabulary pruning eliminates low-frequency tokens. The model is continually pre-trained on 700B tokens using a dynamic UL2 objective that adapts noise type ratios based on validation loss. This multi-stage approach with intermittent recovery training aims to prevent catastrophic performance loss while achieving high compression ratios.

## Key Results
- Achieves 77.3% compression ratio (15B → 3.4B parameters)
- Maintains competitive performance compared to other 3B models
- Performs close to or on par with original 15B OpenBA in commonsense reasoning and NER tasks
- Demonstrates effectiveness of multi-stage pruning with recovery training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-stage pruning preserves model performance better than single-stage pruning.
- Mechanism: Stage-wise pruning + recovery training prevents catastrophic loss of capability.
- Core assumption: Gradual reduction of parameters with intermittent retraining allows the model to adapt without degrading too much.
- Evidence anchors:
  - [abstract]: "OpenBA-V2 utilizes more data, more flexible training objectives, and techniques such as layer pruning, neural pruning, and vocabulary pruning to achieve a compression rate of 77.3% with minimal performance loss."
  - [section]: "we adopt a staged approach to pruning the model's layers. After each stage, we use a small amount of data to recover the model performance."
  - [corpus]: No strong evidence; neighboring works discuss pruning strategies but do not directly validate staged recovery. Explicit comparison missing.
- Break condition: If recovery training fails to restore lost performance, additional stages become counterproductive.

### Mechanism 2
- Claim: Dynamic-UL2 objective adapts training to the needs of the pruned model.
- Mechanism: Adjusts noise type ratios based on validation loss per denoising task, prioritizing weaker capabilities.
- Core assumption: Different denoising tasks target different model abilities; imbalance after pruning can be corrected by task weighting.
- Evidence anchors:
  - [abstract]: "OpenBA-V2 utilizes more data, more flexible training objectives, and techniques such as layer pruning, neural pruning, and vocabulary pruning to achieve a compression rate of 77.3% with minimal performance loss."
  - [section]: "we propose Dynamic-UL2, which dynamically adjusts the proportion of each type of noise based on the loss for each noise on the valid set."
  - [corpus]: No direct corpus evidence. Neighboring papers discuss adaptive strategies but none cite this specific dynamic adjustment mechanism.
- Break condition: If loss per task is already balanced, dynamic adjustment yields no benefit and may add unnecessary complexity.

### Mechanism 3
- Claim: Vocabulary pruning removes redundancy without harming performance.
- Mechanism: High-frequency token retention + reordering token IDs to compress embedding matrix.
- Core assumption: Low-frequency tokens contribute little to model utility and can be pruned safely.
- Evidence anchors:
  - [abstract]: "Finally, we prune the model's vocabulary because of its redundancy, reducing the model size from 3.8B to 3.4B with almost no performance loss."
  - [section]: "We conduct a comprehensive analysis of token occurrences in the pre-training corpus and sort all tokens based on their frequency of occurrence. Then, we retain the top K tokens with the highest occurrence frequencies while pruning the remaining tokens."
  - [corpus]: No evidence from neighboring works. Corpus does not provide validation of vocabulary pruning effectiveness.
- Break condition: If low-frequency tokens encode rare but important concepts, pruning will degrade performance.

## Foundational Learning

- **Transformer architecture and encoder-decoder attention**: Understanding how pruning affects attention layers and token embeddings.
  - Why needed here: Understanding how pruning affects attention layers and token embeddings.
  - Quick check question: What happens to the self-attention computation if a layer is removed? Does it break the model or simply reduce depth?

- **Structured vs unstructured pruning**: This work uses structured pruning (layer, neural) and vocabulary pruning; unstructured pruning is not used.
  - Why needed here: This work uses structured pruning (layer, neural) and vocabulary pruning; unstructured pruning is not used.
  - Quick check question: How does removing an entire layer differ from zeroing out individual weights in terms of inference speed and model size?

- **Training objectives for masked language modeling**: UL2 and Dynamic-UL2 use denoising tasks; understanding how noise types affect capability learning is essential.
  - Why needed here: UL2 and Dynamic-UL2 use denoising tasks; understanding how noise types affect capability learning is essential.
  - Quick check question: What is the difference between R-Denoising and S-Denoising in UL2, and why are both included?

## Architecture Onboarding

- **Component map**: 15B OpenBA → Layer pruning → Neural pruning → Dynamic-UL2 training → Vocabulary pruning → 3.4B OpenBA-V2
- **Critical path**: 
  1. Load 15B OpenBA weights
  2. Apply layer pruning masks sequentially
  3. Apply neural pruning masks
  4. Fine-tune with Dynamic-UL2 on 700B tokens
  5. Apply vocabulary pruning
- **Design tradeoffs**:
  - More pruning stages → less performance loss but longer training time.
  - Dynamic-UL2 adds complexity but improves adaptation.
  - Vocabulary pruning reduces embedding size but may harm rare token tasks.
- **Failure signatures**:
  - Training loss spikes after pruning → recovery training insufficient.
  - Validation perplexity increases steadily → too much pruning.
  - Vocabulary pruning reduces performance on rare-token tasks → pruning threshold too aggressive.
- **First 3 experiments**:
  1. Layer pruning only: Remove 20% of layers and retrain to measure recovery.
  2. Neural pruning only: Randomly prune 30% of non-critical weight rows/columns and retrain.
  3. Vocabulary pruning: Reduce vocab from 260k to 120k and measure impact on Chinese vs English tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of OpenBA-V2 compare to models trained with higher-quality data but fewer parameters?
- Basis in paper: [inferred] The paper states that OpenBA-V2 outperforms all models with 3B or fewer parameters trained on open-sourced data, but acknowledges that it is weaker than larger models. It also highlights the importance of high-quality data for LLMs.
- Why unresolved: The paper does not directly compare OpenBA-V2 to models with similar parameter counts but trained on different quality data. It only compares to models with similar parameter counts trained on open-sourced data.
- What evidence would resolve it: A direct comparison of OpenBA-V2 to models with similar parameter counts but trained on higher-quality data would resolve this question.

### Open Question 2
- Question: What is the optimal balance between the number of parameters and the amount of training data for achieving the best performance in language models?
- Basis in paper: [explicit] The paper discusses the importance of both data quality and quantity, stating that smaller models can potentially outperform larger ones with lower training costs when trained with more data. It also mentions the scaling law and how it may not hold when training smaller models for longer periods.
- Why unresolved: The paper does not provide a definitive answer on the optimal balance between parameters and training data. It only suggests that smaller models can achieve comparable performance with the right training strategies.
- What evidence would resolve it: A comprehensive study comparing the performance of models with different parameter counts and training data sizes would resolve this question.

### Open Question 3
- Question: How does the Dynamic-UL2 training strategy compare to other methods for improving model performance after pruning?
- Basis in paper: [explicit] The paper introduces Dynamic-UL2 as a strategy for dynamically adjusting the sampling ratio for each domain according to the model's loss, and states that it effectively facilitates performance recovery by adapting to more challenging tasks.
- Why unresolved: The paper does not compare Dynamic-UL2 to other methods for improving model performance after pruning. It only demonstrates the effectiveness of Dynamic-UL2 within the context of the OpenBA-V2 model.
- What evidence would resolve it: A comparison of Dynamic-UL2 to other post-pruning training strategies would resolve this question.

## Limitations
- The Dynamic-UL2 algorithm's superiority over standard UL2 is asserted but not rigorously proven through ablation studies.
- Vocabulary pruning effectiveness is not validated across different language pairs or rare-token tasks.
- The 700B token training dataset composition is vaguely specified, making domain-specific generalization assessment difficult.

## Confidence
- **High Confidence**: The fundamental feasibility of multi-stage compression (layer → neural → vocabulary) is well-established in the literature.
- **Medium Confidence**: The specific parameter counts at each pruning stage and the final 77.3% compression ratio appear plausible given the methodology described.
- **Low Confidence**: The Dynamic-UL2 algorithm's superiority over standard UL2 is asserted but not rigorously proven.

## Next Checks
1. **Ablation Study**: Compare OpenBA-V2's performance against a 3.4B model compressed via single-stage pruning (layer + neural + vocabulary simultaneously) trained with standard UL2. Measure both final performance and training efficiency.

2. **Vocabulary Sensitivity Analysis**: Train OpenBA-V2 variants with different vocabulary sizes (e.g., 50k, 120k, 260k) and evaluate on tasks requiring rare tokens (scientific terminology, proper nouns, low-resource languages) to quantify the performance-cost tradeoff.

3. **Dynamic-UL2 Validation**: Implement a baseline version of OpenBA-V2 using standard UL2 with fixed noise ratios. Compare convergence speed, final performance, and task-specific capability development against the Dynamic-UL2 version across multiple random seeds.