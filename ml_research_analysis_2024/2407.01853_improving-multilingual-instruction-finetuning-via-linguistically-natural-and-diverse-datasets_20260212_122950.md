---
ver: rpa2
title: Improving Multilingual Instruction Finetuning via Linguistically Natural and
  Diverse Datasets
arxiv_id: '2407.01853'
source_url: https://arxiv.org/abs/2407.01853
tags:
- datasets
- language
- instruction
- response
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel approach to create high-quality multilingual
  instruction fine-tuning (IFT) datasets by leveraging English-focused LLMs and monolingual
  corpora. Instead of relying on translation or templating, it generates instructions
  directly from natural responses in the target language, preserving linguistic nuances
  and ensuring diversity.
---

# Improving Multilingual Instruction Finetuning via Linguistically Natural and Diverse Datasets

## Quick Facts
- arXiv ID: 2407.01853
- Source URL: https://arxiv.org/abs/2407.01853
- Authors: Sathish Reddy Indurthi; Wenxuan Zhou; Shamil Chollampatt; Ravi Agrawal; Kaiqiang Song; Lingxiao Zhao; Chenguang Zhu
- Reference count: 19
- Models fine-tuned on the proposed datasets outperform those trained on translation-based datasets by 17.57% and template-based datasets by 15.23% on multilingual summarization tasks

## Executive Summary
This paper introduces a novel approach to create high-quality multilingual instruction fine-tuning datasets by leveraging English-focused LLMs and monolingual corpora. The method generates instructions directly from natural responses in target languages, avoiding translation errors and preserving linguistic nuances. A scoring function filters high-quality instruction-response pairs, resulting in datasets that improve multilingual LLM performance on both generative and discriminative tasks. The approach achieves significant improvements over traditional translation-based and template-based datasets across multiple languages and task types.

## Method Summary
The method creates multilingual IFT datasets by first selecting text fragments from monolingual corpora and NLP datasets, then translating responses to English. An English-focused LLM generates instructions using task-specific prompts, and a scoring function evaluates instruction-response pairs. High-scoring pairs are translated back to the original language, creating linguistically natural and diverse datasets. The approach uses task-specific prompts to increase instruction diversity and employs an LLM judge to filter quality pairs. Models are fine-tuned on these datasets and evaluated on summarization, machine translation, and multilingual MMLU tasks.

## Key Results
- Models fine-tuned on proposed datasets outperform translation-based approaches by 17.57% on multilingual summarization tasks
- Achieved 15.23% improvement over template-based datasets on the same summarization benchmark
- Demonstrated 6.9% average gain on multilingual MMLU tasks, showing effectiveness on discriminative tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generating instructions from natural responses preserves linguistic naturalness better than translating from English
- Mechanism: The model uses monolingual corpora and existing NLP datasets to create instruction-response pairs directly in the target language, avoiding translation errors and unnatural phrasing
- Core assumption: Natural text in monolingual corpora is free from translation errors and preserves linguistic nuances
- Evidence anchors:
  - [abstract]: "Instead of relying on translation or templating, it generates instructions directly from natural responses in the target language, preserving linguistic nuances and ensuring diversity"
  - [section 2]: "These text fragments are natural and most likely error-free output since they are from the monolingual corpus or human-curated answers from existing NLP datasets"
  - [corpus]: Weak evidence. The corpus signals show related work but do not directly confirm that monolingual corpora preserve naturalness better than translation
- Break condition: If the monolingual corpora contain errors or unnatural text, the approach would fail to preserve linguistic naturalness

### Mechanism 2
- Claim: The scoring function ensures high-quality instruction-response pairs by filtering out misaligned or poorly generated instructions
- Mechanism: An English-focused LLM acts as a judge, scoring instruction-response pairs. Only pairs with scores above a threshold are used for fine-tuning
- Core assumption: The LLM judge can accurately assess the quality and alignment of instruction-response pairs
- Evidence anchors:
  - [abstract]: "The method includes a scoring function to filter high-quality instruction-response pairs"
  - [section 2]: "We use LLM as a judge, employing the prompt Ps to assess the quality of (Ien, Ren) pair. This results in a score, denoted as s"
  - [corpus]: Weak evidence. The corpus signals mention related work on instruction alignment but do not directly confirm the effectiveness of the scoring function
- Break condition: If the LLM judge is biased or unable to accurately assess quality, the scoring function would fail to filter effectively

### Mechanism 3
- Claim: Using task-specific prompts increases instruction diversity by generating varied instructions for the same response
- Mechanism: The model randomly selects from a pool of task-specific prompts when generating instructions, leading to diverse instructions even for similar responses
- Core assumption: Different task prompts will lead to different instructions for the same response
- Evidence anchors:
  - [abstract]: "Instead of relying on translation or templating, it generates instructions directly from natural responses in the target language, preserving linguistic nuances and ensuring diversity"
  - [section 4.3.1]: "We also report the average length of instructions and responses from all data creation approaches. As shown in Table 2, the average number of characters in the instructions generated using our approach varies significantly compared to the other two approaches"
  - [corpus]: No direct evidence. The corpus signals do not mention instruction diversity or task-specific prompts
- Break condition: If the task prompts are too similar or the LLM generates similar instructions regardless of prompt, diversity would not increase

## Foundational Learning

- Concept: Multilingual instruction fine-tuning
  - Why needed here: The paper aims to improve LLM performance in non-English languages by creating high-quality multilingual IFT datasets
  - Quick check question: What is the main difference between monolingual and multilingual instruction fine-tuning?

- Concept: Natural language processing (NLP) tasks
  - Why needed here: The paper evaluates the effectiveness of the approach on generative tasks (summarization, machine translation) and discriminative tasks (multilingual MMLU)
  - Quick check question: What are the key differences between generative and discriminative NLP tasks?

- Concept: Data quality and diversity in machine learning
  - Why needed here: The paper emphasizes the importance of high-quality, diversified datasets for effective multilingual IFT
  - Quick check question: How does data quality and diversity impact the performance of machine learning models?

## Architecture Onboarding

- Component map: Monolingual corpora -> English-focused LLM -> Translation model -> Scoring function -> Fine-tuning pipeline

- Critical path:
  1. Select responses from monolingual corpora
  2. Translate responses to English
  3. Generate instructions using English-focused LLM and task prompts
  4. Score instruction-response pairs using LLM judge
  5. Translate high-scoring instructions back to target language
  6. Fine-tune LLMs on the resulting dataset

- Design tradeoffs:
  - Using English-focused LLMs leverages their capabilities but may introduce English-centric biases
  - Generating instructions from responses preserves naturalness but requires a robust scoring function
  - The approach balances quality and diversity but may be more computationally expensive than translation or templating

- Failure signatures:
  - Poor performance on non-English tasks despite using the generated datasets
  - High variance in instruction quality or diversity
  - The scoring function consistently rejects or accepts most pairs, indicating bias or poor calibration

- First 3 experiments:
  1. Evaluate the quality of generated instructions by human annotators for a sample of instruction-response pairs
  2. Compare the diversity of instructions generated using different task prompts on the same set of responses
  3. Fine-tune a small LLM on datasets created using different scoring thresholds and evaluate performance to find the optimal threshold

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method scale to low-resource languages where monolingual corpora are limited or unavailable?
- Basis in paper: [inferred] The paper focuses on Telugu, Hindi, Japanese, and Spanish, which are mid to high-resource languages, but acknowledges that Telugu and Nepali are low-resource languages
- Why unresolved: The paper does not address how the method would perform when monolingual corpora are scarce or non-existent
- What evidence would resolve it: Experimental results comparing the method's performance on low-resource languages versus high-resource languages, or proposed adaptations for low-resource scenarios

### Open Question 2
- Question: What is the impact of using different scoring thresholds (λ) on the final model performance, and how does this vary across languages?
- Basis in paper: [explicit] The paper discusses the effect of scoring thresholds on dataset quality and model performance, showing improvements up to λ = 3
- Why unresolved: The analysis is limited to aggregate results across languages, without examining language-specific variations
- What evidence would resolve it: Language-specific performance curves showing optimal scoring thresholds for each language

### Open Question 3
- Question: How does the quality of generated instructions compare to human-annotated instructions in terms of linguistic naturalness and diversity?
- Basis in paper: [inferred] The paper emphasizes preserving linguistic naturalness and ensuring diversity but does not provide a direct comparison with human-annotated data
- Why unresolved: The paper focuses on quantitative improvements over translation and template-based methods but lacks qualitative comparison with human annotations
- What evidence would resolve it: Human evaluations comparing the linguistic quality and diversity of generated instructions versus human-annotated instructions

## Limitations

- The approach relies heavily on English-focused LLMs, which may introduce English-centric biases even when generating instructions for other languages
- Effectiveness depends on the availability of high-quality monolingual corpora and translation models, which may be limited for low-resource languages
- The scoring function's accuracy is not directly validated against human judgments, creating uncertainty about its reliability across different languages and instruction types

## Confidence

- **High Confidence**: The core claim that generating instructions from natural responses preserves linguistic naturalness better than translation-based approaches
- **Medium Confidence**: The claim that the scoring function effectively filters high-quality instruction-response pairs
- **Medium Confidence**: The claim about improved diversity through task-specific prompts

## Next Checks

1. **Human Evaluation Study**: Conduct blind human evaluation comparing instruction quality between translation-based, template-based, and the proposed approach for a sample of instruction-response pairs across multiple languages

2. **Scoring Function Calibration**: Perform sensitivity analysis on the scoring threshold (λ) and evaluate the scoring function's agreement with human judgments

3. **Cross-Lingual Transfer Analysis**: Evaluate whether models fine-tuned on the proposed datasets show improved zero-shot performance on tasks in languages not seen during fine-tuning, compared to models trained on translation-based datasets