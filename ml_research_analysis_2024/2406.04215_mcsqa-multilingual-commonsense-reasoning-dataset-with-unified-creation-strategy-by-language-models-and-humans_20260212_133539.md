---
ver: rpa2
title: 'mCSQA: Multilingual Commonsense Reasoning Dataset with Unified Creation Strategy
  by Language Models and Humans'
arxiv_id: '2406.04215'
source_url: https://arxiv.org/abs/2406.04215
tags:
- language
- association
- linguistics
- computational
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces mCSQA, a multilingual commonsense reasoning
  dataset designed to evaluate language models' natural language understanding capabilities
  across eight languages. Unlike existing multilingual datasets created through translation,
  mCSQA is constructed from scratch for each language using a novel method that leverages
  language models to generate questions and answers, with reduced human verification
  effort.
---

# mCSQA: Multilingual Commonsense Reasoning Dataset with Unified Creation Strategy by Language Models and Humans

## Quick Facts
- arXiv ID: 2406.04215
- Source URL: https://arxiv.org/abs/2406.04215
- Reference count: 40
- Key outcome: LM-generated multilingual commonsense reasoning dataset achieves 100x cost reduction while enabling cross-lingual evaluation

## Executive Summary
This paper introduces mCSQA, a multilingual commonsense reasoning dataset designed to evaluate language models' natural language understanding capabilities across eight languages. Unlike existing multilingual datasets created through translation, mCSQA is constructed from scratch for each language using a novel method that leverages language models to generate questions and answers, with reduced human verification effort. The authors demonstrate that mCSQA enables effective evaluation of cross-lingual language-transfer capabilities, revealing high transfer performance for easier questions but lower performance for those requiring deep knowledge or commonsense. The method significantly reduces dataset creation costs to one-hundredth of manual creation while maintaining quality, showing that language models can effectively generate language-specific knowledge-based questions.

## Method Summary
The method extracts subgraphs from ConceptNet containing specific relations and three distinct concept entities, then uses multilingual language models (primarily GPT-3.5) to generate questions and distractor choices following language-specific prompt templates. Quality verification involves both the language model and human annotators, with the LM filtering out questions it can answer correctly to reduce human verification workload. The process creates questions from scratch in each language rather than through translation, enabling direct cross-lingual comparison while significantly reducing creation costs.

## Key Results
- Dataset creation cost reduced from $0.33 to $0.002 per question (100x reduction)
- LM performance: 73.6-85.7% accuracy on easy questions, 50.1-67.8% on difficult questions
- Cross-lingual transfer evaluation enabled across eight high-resource languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual LMs can generate language-specific commonsense questions when prompted in the target language.
- Mechanism: The LMs internalize language-specific knowledge during pretraining, and prompting in the target language activates this knowledge for generation.
- Core assumption: The pretraining data includes sufficient language-specific content for the LMs to have internalized the relevant commonsense knowledge.
- Evidence anchors:
  - [abstract] "multilingual LMs could create QA including language-specific knowledge"
  - [section] "our method has shown that the use of multilingual LMs enables the construction of multilingual datasets"
  - [corpus] Weak - no direct corpus evidence about the internal knowledge of LMs
- Break condition: If the pretraining data lacks sufficient language-specific content, the LMs cannot generate appropriate questions.

### Mechanism 2
- Claim: Using LM-generated questions followed by human verification significantly reduces dataset creation costs while maintaining quality.
- Mechanism: LMs handle the bulk of question generation, with humans only verifying questions that the LM cannot answer, reducing human workload by focusing on harder cases.
- Core assumption: The LM can correctly answer most easy questions, allowing human verification to focus on harder questions.
- Evidence anchors:
  - [abstract] "significantly reducing the dataset creation cost compared to manual creation"
  - [section] "total cost per question is 0.002 dollars for mCSQA compared to 0.33 dollars for CSQA"
  - [corpus] Weak - no direct corpus evidence about the efficiency of the verification process
- Break condition: If the LM cannot correctly answer a substantial portion of questions, human verification workload remains high.

### Mechanism 3
- Claim: mCSQA enables evaluation of cross-lingual transfer capabilities beyond English.
- Mechanism: By creating questions from scratch in multiple languages from the same knowledge base, mCSQA allows direct comparison of model performance across language pairs.
- Core assumption: Questions created from the same knowledge base but in different languages test equivalent reasoning skills.
- Evidence anchors:
  - [abstract] "Constructed dataset is a benchmark for cross-lingual language-transfer capabilities"
  - [section] "mCSQA supports the evaluation of cross-lingual language transfer performance in any directions among multilingual LMs"
  - [corpus] Moderate - related work on multilingual benchmarks supports this capability
- Break condition: If questions in different languages test fundamentally different reasoning skills, cross-lingual transfer evaluation becomes invalid.

## Foundational Learning

- Concept: Knowledge Graph Extraction
  - Why needed here: The method relies on extracting subgraphs from ConceptNet to create questions, so understanding how knowledge graphs work is essential.
  - Quick check question: How would you extract a subgraph containing specific entities and relations from ConceptNet?

- Concept: Prompt Engineering for LMs
  - Why needed here: The quality of generated questions depends heavily on the prompt design, requiring understanding of effective prompt strategies.
  - Quick check question: What are the key components of an effective prompt for generating multiple-choice questions?

- Concept: Active Learning for Annotation
  - Why needed here: The verification process uses an active learning approach where LMs first filter questions, requiring understanding of active learning principles.
  - Quick check question: How does active learning reduce annotation costs in dataset creation?

## Architecture Onboarding

- Component map: ConceptNet extraction -> GPT-3.5 question generation -> Human verification (for questions LMs cannot answer)
- Critical path: Knowledge graph extraction → Question generation → Human verification (for questions LMs cannot answer)
- Design tradeoffs: The method trades some question quality for significant cost reduction, relying on LM capabilities and human verification for harder cases.
- Failure signatures: Low-quality questions slipping through verification, LMs failing to generate appropriate questions for certain languages, or verification costs becoming too high.
- First 3 experiments:
  1. Test question generation with a small sample of ConceptNet subgraphs for one language
  2. Evaluate the quality of LM-generated questions vs. human-written questions
  3. Measure the cost reduction achieved by the LM-assisted verification process

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the multilingual capabilities of different large language models (LLMs) compare when generating language-specific commonsense questions?
- Basis in paper: [explicit] The paper compares GPT-3.5, GPT-4, Llama2-70B, and various encoder-based models, but only in the context of answering questions, not generating them.
- Why unresolved: The paper focuses on using GPT-3.5 for question generation and quality verification, but does not explore how other models might perform in these tasks.
- What evidence would resolve it: Conducting a systematic comparison of different LLMs' performance in generating language-specific commonsense questions across multiple languages, evaluating both the quality and efficiency of the generated questions.

### Open Question 2
- Question: To what extent do language-specific knowledge and cultural context influence the difficulty of commonsense reasoning questions across different languages?
- Basis in paper: [explicit] The paper discusses the importance of language-specific knowledge and cultural context in creating effective commonsense reasoning questions, but does not quantify their impact on question difficulty.
- Why unresolved: The paper presents examples of language-specific questions but does not analyze how the inclusion of cultural and linguistic nuances affects the overall difficulty of the dataset.
- What evidence would resolve it: Analyzing the performance of multilingual models on questions with varying degrees of language-specific knowledge and cultural context, correlating these factors with question difficulty scores.

### Open Question 3
- Question: How does the cost-effectiveness of using large language models for dataset creation compare to traditional human annotation methods across different languages and task types?
- Basis in paper: [explicit] The paper reports a significant reduction in cost when using GPT-3.5 for mCSQA creation compared to manual methods, but does not explore this across different languages or task types.
- Why unresolved: The cost analysis is limited to the specific case of mCSQA creation, and it's unclear whether similar cost savings would be observed for other languages or different types of NLU tasks.
- What evidence would resolve it: Conducting a comprehensive cost analysis comparing LLM-based and human annotation methods for dataset creation across multiple languages and various NLU tasks, considering factors such as quality, time, and resource requirements.

## Limitations
- The dataset focuses on eight high-resource languages, limiting generalizability to low-resource languages
- LM performance on difficult questions remains relatively low (50.1-67.8%), suggesting limitations in capturing complex reasoning
- Quality verification still requires human annotators, though significantly reduced compared to manual creation

## Confidence
**High confidence** in the cost-effectiveness claim: The paper provides clear quantitative evidence showing a 100-fold reduction in dataset creation costs (from $0.33 to $0.002 per question), supported by detailed cost calculations and comparison with the existing CSQA dataset.

**Medium confidence** in the cross-lingual transfer evaluation capability: While the paper demonstrates that mCSQA can be used to evaluate cross-lingual transfer, the relatively low performance on difficult questions (50.1-67.8%) suggests limitations in the dataset's ability to capture complex reasoning skills that transfer across languages.

**Medium confidence** in the language model generation mechanism: The paper shows that multilingual LMs can generate language-specific questions, but does not provide systematic analysis of quality differences across languages or the impact of varying pretraining data distributions.

## Next Checks
1. Conduct a detailed error analysis of the 50.1-67.8% accuracy range on difficult questions to identify whether failures stem from dataset quality issues or genuine limitations in cross-lingual reasoning capabilities.

2. Test the dataset creation method on truly low-resource languages (e.g., regional languages with limited web presence) to evaluate whether the language model generation mechanism scales beyond high-resource languages.

3. Implement a longitudinal study comparing human-written vs. LM-generated questions over multiple iterations to quantify the trade-off between cost reduction and question quality degradation.