---
ver: rpa2
title: Tri-Modal Motion Retrieval by Learning a Joint Embedding Space
arxiv_id: '2403.00691'
source_url: https://arxiv.org/abs/2403.00691
tags:
- motion
- retrieval
- text
- video
- modality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LAVIMO, a three-modality learning framework
  for motion retrieval that incorporates human-centric videos as an intermediary modality
  to enhance the alignment between text and motion. The method employs contrastive
  learning to construct a joint embedding space and a custom attention mechanism that
  leverages motion as queries to extract relevant information from text and video
  during motion reconstruction.
---

# Tri-Modal Motion Retrieval by Learning a Joint Embedding Space

## Quick Facts
- arXiv ID: 2403.00691
- Source URL: https://arxiv.org/abs/2403.00691
- Authors: Kangning Yin; Shihao Zou; Yuxuan Ge; Zheng Tian
- Reference count: 40
- Primary result: LAVIMO achieves state-of-the-art performance on HumanML3D and KIT-ML datasets, improving text-to-motion retrieval by up to 6.37% R@1

## Executive Summary
This paper introduces LAVIMO, a three-modality learning framework for motion retrieval that incorporates human-centric videos as an intermediary modality to enhance the alignment between text and motion. The method employs contrastive learning to construct a joint embedding space and a custom attention mechanism that leverages motion as queries to extract relevant information from text and video during motion reconstruction. Experiments on the HumanML3D and KIT-ML datasets demonstrate state-of-the-art performance in text-to-motion, motion-to-text, video-to-motion, and motion-to-video retrieval tasks.

## Method Summary
LAVIMO is a three-modality learning framework that jointly aligns text, video, and motion embeddings in a shared space for retrieval tasks. The framework uses contrastive learning with KL divergence loss to align the three modalities, while employing motion embeddings as queries in an attention mechanism to extract relevant information from text and video during motion reconstruction. The model is trained with both alignment and reconstruction losses on the HumanML3D and KIT-ML datasets, which include text descriptions and 3D motion capture data along with generated RGB videos.

## Key Results
- Achieves up to 6.37% improvement in R@1 for text-to-motion retrieval compared to previous methods
- Demonstrates state-of-the-art performance across all retrieval tasks (text-to-motion, motion-to-text, video-to-motion, motion-to-video)
- Shows good generalization to real-world human-centric videos despite being trained on rendered avatar footage

## Why This Works (Mechanism)

### Mechanism 1
Video acts as an intermediary modality to reduce the "spatial distance" between text and motion embeddings, making alignment easier. Human-centric videos provide a compact, low-dimensional representation of 3D motion that is semantically closer to motion data than raw text. By embedding video alongside text and motion, the model can bridge the semantic gap between the two traditionally distant modalities. The core assumption is that the visual and temporal structure in human-centric videos shares enough semantic overlap with both text descriptions and motion capture data to serve as an effective bridge.

### Mechanism 2
Motion-as-queries attention mechanism allows extraction of modality-specific contextual information during reconstruction. The motion embedding is treated as a query to retrieve complementary information from text and video modalities. This allows the model to fill in missing details (like action pacing or visual context) that are present in other modalities but absent in motion alone. The core assumption is that motion sequences lack certain high-level semantic cues (e.g., "quickly", "counterclockwise") that text or video can provide, and this attention mechanism can effectively recover them.

### Mechanism 3
Contrastive learning with negative filtering improves alignment quality by reducing false positives from semantically similar text descriptions. The model uses KL divergence-based contrastive loss with negative filtering to ensure that texts with similar meanings are treated as positive pairs, improving the precision of the embedding space. The core assumption is that text descriptions can be semantically similar yet syntactically different, and standard contrastive learning without filtering would incorrectly penalize such pairs.

## Foundational Learning

- Concept: Tri-modal contrastive learning
  - Why needed here: To jointly align three distinct data types (text, video, motion) into a shared embedding space for retrieval tasks
  - Quick check question: Can you explain how triplet loss differs from pairwise contrastive loss in the context of three modalities?

- Concept: Cross-modal attention mechanisms
  - Why needed here: To allow one modality (motion) to query information from others (text, video) during reconstruction, improving semantic completeness
  - Quick check question: How does using motion as a query differ from standard self-attention in terms of information flow?

- Concept: Motion parameterization and encoding
  - Why needed here: Motion data is complex (3D joint positions, velocities, rotations) and needs specialized encoding (e.g., using SMPL-based representations) before embedding
  - Quick check question: What are the benefits of using joint velocities and local joint positions over raw joint coordinates in motion encoding?

## Architecture Onboarding

- Component map:
  Text Encoder -> Video Encoder -> Motion Encoder -> Attention Fusion Module -> Motion Decoder

- Critical path:
  Text/Video/Motion → Encoders → Joint Embedding Space (via contrastive loss) → Attention Fusion → Motion Decoder → Reconstruction Loss

- Design tradeoffs:
  - Using video as an intermediary modality increases model complexity but improves alignment accuracy
  - Motion-as-queries attention allows richer fusion but may introduce additional parameters and training instability
  - Negative filtering in contrastive loss improves precision but requires a threshold tuning process

- Failure signatures:
  - If video-to-motion retrieval fails, the video modality likely lacks motion-relevant semantics
  - If text-to-motion retrieval underperforms, the contrastive loss or negative filtering may be misconfigured
  - If reconstruction loss dominates training, the attention fusion may be overfitting to modality-specific noise

- First 3 experiments:
  1. Train 2-modal (text-motion) baseline and compare retrieval metrics to 3-modal version
  2. Visualize attention weights to confirm motion queries retrieve relevant text/video features
  3. Test retrieval performance with varying negative filtering thresholds to find optimal balance

## Open Questions the Paper Calls Out

### Open Question 1
How would incorporating real human-centric videos instead of animated and rendered avatars impact the performance of LAVIMO on video-to-motion retrieval tasks? The paper mentions that the video modality is currently derived from animating and rendering avatars, which deviates from authentic human-centric videos. It suggests that substituting rendered footage with actual human-centric videos may yield improvements. This remains unresolved due to the lack of such datasets for the HumanML3D and KIT-ML datasets used in their experiments.

### Open Question 2
What is the optimal number of frames to sample from a video sequence for the video encoder to balance computational efficiency and retrieval performance? The paper samples 8 frames from a video sequence but does not explore the impact of varying the number of frames on performance. The authors have fixed the number of frames to 8 based on implementation details but have not investigated how different numbers of frames might affect the model's performance.

### Open Question 3
How does the performance of LAVIMO on text-to-motion retrieval tasks compare to state-of-the-art methods when applied to larger and more diverse motion datasets? The paper demonstrates that LAVIMO outperforms previous methods on the HumanML3D and KIT-ML datasets, but it acknowledges that the model does not yet match the precision achieved in video and image retrieval tasks due to the limited availability of comprehensive motion datasets. The experiments are conducted on relatively small and specific motion datasets.

## Limitations
- The video-as-intermediary hypothesis is plausible but unproven - the authors assume human-centric videos are semantically closer to motion than text, but this isn't empirically validated
- The effectiveness of the attention mechanism depends heavily on the quality of motion embeddings as queries, which could fail if motion sequences lack distinctive semantic features
- The negative filtering approach for contrastive learning may be sensitive to threshold tuning, potentially leading to embedding space collapse if misconfigured

## Confidence
- **High Confidence**: The architectural framework (tri-modal encoders, attention fusion, contrastive learning) is well-specified and follows established patterns in multimodal learning
- **Medium Confidence**: The performance improvements over baselines are demonstrated, but the attribution to specific mechanisms (especially video intermediary and attention design) requires further ablation studies
- **Low Confidence**: The claim that video serves as an effective semantic bridge between text and motion is largely theoretical, with limited empirical validation of this bridging effect

## Next Checks
1. **Ablation of Video Modality**: Train a 2-modal (text-motion) baseline without video to quantify the specific contribution of the video intermediary and verify it's not just adding parameters that improve performance
2. **Attention Visualization**: Generate heatmaps showing which text/video tokens the motion queries attend to during retrieval tasks, confirming the mechanism extracts semantically relevant information rather than learning spurious correlations
3. **Cross-Modal Retrieval Stress Test**: Design retrieval tasks using text/video pairs with similar surface forms but different underlying motions (e.g., "jump high" vs "jump low") to test whether negative filtering and the joint embedding space truly capture semantic distinctions