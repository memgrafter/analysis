---
ver: rpa2
title: 'Safety Arithmetic: A Framework for Test-time Safety Alignment of Language
  Models by Steering Parameters and Activations'
arxiv_id: '2406.11801'
source_url: https://arxiv.org/abs/2406.11801
tags:
- safety
- arithmetic
- language
- arxiv
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces Safety Arithmetic, a training-free framework\
  \ for aligning large language models (LLMs) with human values to reduce harmful\
  \ content generation. The method operates in two stages: Harm Direction Removal,\
  \ which eliminates harmful parameters by computing and negating a harm vector, and\
  \ Safety Alignment, which steers the model\u2019s latent space toward safe responses\
  \ using in-context safety vectors."
---

# Safety Arithmetic: A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations

## Quick Facts
- arXiv ID: 2406.11801
- Source URL: https://arxiv.org/abs/2406.11801
- Authors: Rima Hazra; Sayan Layek; Somnath Banerjee; Soujanya Poria
- Reference count: 30
- The framework significantly improves safety measures, reduces over-safety, and maintains model utility without extensive fine-tuning.

## Executive Summary
Safety Arithmetic is a training-free framework for aligning large language models with human values to reduce harmful content generation. The method operates in two stages: Harm Direction Removal, which eliminates harmful parameters by computing and negating a harm vector, and Safety Alignment, which steers the model's latent space toward safe responses using in-context safety vectors. The framework is tested on base, supervised fine-tuned, and edited models, showing significant improvements in safety across multiple datasets while maintaining model utility.

## Method Summary
Safety Arithmetic is a two-stage framework for safety alignment of LLMs. First, Harm Direction Removal (HDR) computes a harm vector by fine-tuning a model on harmful data, then prunes and applies the negated harm vector to the target model. Second, Safety Alignment (Safe-Align) computes an in-context safety vector (ICV) from safe/unsafe prompt pairs and adds it to the latent states of the intermediate model, normalizing to preserve existing capabilities while biasing toward safe responses. The method is tested on base, SFT, and edited models across multiple datasets, showing significant safety improvements while maintaining utility.

## Key Results
- Reduces attack success rates from 19.81% to 6.15% on AdvBench for Llama2
- Maintains model utility while significantly improving safety measures
- Effectively mitigates over-safety, reducing excessive refusal rates

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Harm Direction Removal (HDR) reduces harmful content by removing harmful parameters from the model.
- **Mechanism:** The HDR stage computes a harm vector by subtracting the base aligned model parameters from a model fine-tuned on harmful data. This harm vector is then pruned to retain only the top k% parameters by magnitude, and subtracted from the target model to steer it away from harmful directions.
- **Core assumption:** Harmful behavior is localized in a small subset of model parameters, and removing these parameters will reduce harmful outputs without significantly impacting overall model performance.
- **Evidence anchors:**
  - [abstract] "Safety Arithmetic involves Harm Direction Removal to avoid harmful content and Safety Alignment to promote safe responses."
  - [section 3.2] "We apply the negated harm vector τH to the target model θt through element-wise subtraction."
  - [corpus] Weak evidence - related papers focus on parameter steering but don't directly validate harm parameter pruning.
- **Break condition:** If harmful behavior is distributed across many parameters or if the harm vector captures non-harmful but correlated features, removing these parameters could degrade model utility or fail to reduce harm.

### Mechanism 2
- **Claim:** Safety Alignment (Safe-Align) steers the model's latent space toward safe responses using in-context learning.
- **Mechanism:** The Safe-Align stage computes an in-context safety vector (ICV) by contrasting hidden representations of unsafe and safe prompts. This ICV is then added to the latent states of the intermediate model at each layer, normalizing to preserve the model's existing capabilities while biasing generation toward safe responses.
- **Core assumption:** In-context learning can effectively guide model behavior, and the difference in latent representations between safe and unsafe prompts can be captured by a single vector direction.
- **Evidence anchors:**
  - [abstract] "Safety Alignment, which steers the model's latent space toward safe responses using in-context safety vectors."
  - [section 3.3] "We compute the in-context safety vector (ICV) using the Dicl dataset. We then apply the ICV to the model ˆθt to obtain a safer model θsf."
  - [corpus] Moderate evidence - related papers like "SafeSteer" and "SCAR" use latent steering but don't validate ICV specifically.
- **Break condition:** If the safe/unsafe distinction isn't linearly separable in the latent space, or if the ICV introduces bias that causes over-safety (excessive refusals), the alignment may fail or degrade utility.

### Mechanism 3
- **Claim:** The two-stage framework preserves model utility while enhancing safety.
- **Mechanism:** HDR minimally intervenes by only removing top k% harmful parameters, and Safe-Align uses normalized vector addition to preserve the model's existing capabilities. This design balances safety improvement with utility preservation.
- **Core assumption:** Both stages are designed to be minimally invasive, and their combined effect is additive rather than destructive to the model's general abilities.
- **Evidence anchors:**
  - [abstract] "Our experiments show that Safety Arithmetic significantly improves safety measures, reduces over-safety, and maintains model utility."
  - [section 6] "We assess the utility preserved in our framework and the original model using several utility benchmark datasets... After applying our framework, the refusal rate significantly drops compared to the base model."
  - [corpus] Weak evidence - no direct corpus support for utility preservation claims, only general parameter editing literature.
- **Break condition:** If either stage causes interference (e.g., HDR removes too many parameters or Safe-Align overly biases responses), utility could degrade despite the design intent.

## Foundational Learning

- **Concept:** Task Arithmetic for Model Editing
  - **Why needed here:** The HDR stage relies on computing and applying task vectors (harm vectors) to modify model parameters, similar to how task arithmetic is used for adding or removing capabilities in language models.
  - **Quick check question:** What is the key difference between using task vectors for capability addition versus harm direction removal in Safety Arithmetic?

- **Concept:** In-Context Learning (ICL) and Latent Space Steering
  - **Why needed here:** The Safe-Align stage uses ICL principles to compute an in-context safety vector that steers the model's latent space, requiring understanding of how demonstrations influence model behavior through latent representations.
  - **Quick check question:** How does the in-context safety vector (ICV) in Safety Arithmetic differ from traditional ICL demonstrations in terms of its computation and application?

- **Concept:** Parameter Pruning and Redundancy in Neural Networks
  - **Why needed here:** The HDR stage selects only the top k% parameters by magnitude for removal, relying on the assumption that redundant or less influential parameters can be pruned without significant performance loss.
  - **Quick check question:** Why might removing only the top k% parameters (rather than all harmful parameters) help preserve model utility in the HDR stage?

## Architecture Onboarding

- **Component map:** DH dataset -> HDR module -> ˆθt -> Dicl dataset -> Safe-Align module -> θsf
- **Critical path:**
  1. Prepare datasets DH and Dicl
  2. Run HDR: Fine-tune θH, compute τH, select top k%, apply to θt → ˆθt
  3. Run Safe-Align: Compute ICV, apply to ˆθt with normalization → θsf
  4. Evaluate safety and utility
- **Design tradeoffs:**
  - HDR: Higher k% removes more harmful parameters but risks utility loss; lower k% preserves utility but may be less effective
  - Safe-Align: Stronger ICV application (higher α) increases safety but risks over-safety; weaker application preserves utility but may be less effective
  - Dataset size: Larger DH and Dicl improve vector quality but increase computation
- **Failure signatures:**
  - Utility degradation: Check MMLU/utility scores after HDR and Safe-Align
  - Ineffective safety: Monitor ASR on safety datasets
  - Over-safety: Check refusal rates on safe queries
  - Inconsistency across model types: Test on base, SFT, and edited models separately
- **First 3 experiments:**
  1. **Sanity check:** Apply Safety Arithmetic to a base model and verify that ASR decreases on AdvBench while MMLU remains stable.
  2. **Ablation study:** Compare Safety Arithmetic with HDR-only and Safe-Align-only versions to isolate each stage's contribution.
  3. **Cross-model validation:** Apply Safety Arithmetic to an edited model and verify that it reduces the increased ASR from editing while maintaining utility.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal value of k (top k parameters) in the Harm Direction Removal stage for different model sizes and types?
- Basis in paper: [inferred] The paper mentions that selecting the top k parameters is crucial and shows results for k=10%, but notes that the results could improve with proper pruning percentages.
- Why unresolved: The paper does not perform a comprehensive hyperparameter search for k across different models and scenarios.
- What evidence would resolve it: A systematic study varying k across different model sizes, types, and datasets to determine the optimal value for each case.

### Open Question 2
- Question: How does the Safety Arithmetic framework perform on larger language models (>7 billion parameters)?
- Basis in paper: [explicit] The paper explicitly states that experiments were conducted on models with up to 7 billion parameters and notes this as a limitation.
- Why unresolved: The paper does not evaluate the framework on larger models, which are increasingly common in practice.
- What evidence would resolve it: Testing Safety Arithmetic on models with 10B, 30B, and 70B+ parameters to assess scalability and performance.

### Open Question 3
- Question: How does the framework perform against more sophisticated jailbreak attacks that target the safety alignment itself?
- Basis in paper: [explicit] The paper mentions that ORTHO can jailbreak models aligned with Safety Alignment, but the ASR is reduced when Safe-Align is used together with ORTHO jailbreak.
- Why unresolved: The paper only tests against one specific attack method (ORTHO) and does not explore the full spectrum of potential jailbreak strategies.
- What evidence would resolve it: Comprehensive testing against a diverse set of state-of-the-art jailbreak attacks to evaluate the robustness of the safety alignment.

## Limitations
- Scalability concerns: The method's effectiveness on models larger than 7 billion parameters remains untested.
- Limited attack robustness: Performance against sophisticated jailbreak attacks beyond ORTHO is not evaluated.
- Parameter distribution assumption: The framework assumes harmful behavior is localized in a small parameter subset, which may not hold for all safety issues.

## Confidence
**High Confidence Claims:**
- The two-stage framework structure (HDR + Safe-Align) is technically sound and well-defined
- The method demonstrates measurable improvements on the specific benchmarks tested (AdvBench, DangerousQA, etc.)
- The approach successfully reduces attack success rates on the tested datasets

**Medium Confidence Claims:**
- The framework maintains model utility across all tested scenarios
- The method effectively mitigates over-safety and excessive refusals
- The harm parameter pruning approach generalizes beyond the tested model architectures

**Low Confidence Claims:**
- The framework's effectiveness on truly open-domain safety tasks
- Long-term stability of the alignment after extended deployment
- Performance on models significantly larger than those tested

## Next Checks
1. **Parameter Distribution Analysis**: Conduct experiments to map the distribution of harmful parameters across different model architectures and safety domains. Test whether harmful behavior remains localized in parameter space as model size and complexity increase, and identify threshold points where the current pruning approach may break down.

2. **Cross-Domain Safety Transfer**: Evaluate Safety Arithmetic's performance when applied to safety domains not represented in the training datasets (DH and Dicl). Test the framework on emerging safety challenges like misinformation detection, bias mitigation, and multi-modal safety alignment to assess its generalizability.

3. **Longitudinal Stability Testing**: Implement a deployment simulation that exposes Safety Arithmetic-aligned models to continuous safety challenges over extended periods. Monitor for degradation in safety performance, emergence of new failure modes, or adaptation of adversarial strategies that bypass the alignment.