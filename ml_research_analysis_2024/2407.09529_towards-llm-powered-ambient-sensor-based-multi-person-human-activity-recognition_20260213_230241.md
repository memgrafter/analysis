---
ver: rpa2
title: Towards LLM-Powered Ambient Sensor Based Multi-Person Human Activity Recognition
arxiv_id: '2407.09529'
source_url: https://arxiv.org/abs/2407.09529
tags:
- sensor
- activity
- recognition
- events
- activities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents LAHAR, an LLM-based framework for multi-person
  human activity recognition using ambient sensors. It addresses key challenges in
  HAR, including data scarcity, model generalization, context integration, and multi-person
  recognition.
---

# Towards LLM-Powered Ambient Sensor Based Multi-Person Human Activity Recognition

## Quick Facts
- arXiv ID: 2407.09529
- Source URL: https://arxiv.org/abs/2407.09529
- Reference count: 23
- Key outcome: LAHAR achieves macro-average F1-score of 50.14% across 17 activity classes in multi-person settings, comparable to single-person performance

## Executive Summary
This paper introduces LAHAR, an LLM-based framework for multi-person human activity recognition using ambient sensors. LAHAR addresses key challenges in HAR including data scarcity, model generalization, context integration, and multi-person recognition through a two-stage prompt engineering approach. The framework first generates action-level descriptions for individual subjects by separating mixed sensor events, then reasons about activities based on these descriptions. Evaluated on the ARAS dataset, LAHAR demonstrates comparable accuracy to state-of-the-art methods at higher resolution while maintaining robustness in multi-person scenarios.

## Method Summary
LAHAR is a two-stage LLM-based framework that processes ambient sensor events for multi-person human activity recognition. The first stage separates mixed sensor events from different individuals using chain-of-thought prompting and in-context learning, generating action-level descriptions for each subject. The second stage reasons about activities by integrating contextual information (house layout, sensor descriptions, activity lists, user schedules) to predict activity timelines. The framework processes data at second-level resolution, higher than typical minute-level resolutions, and uses GPT-4-32k via Azure OpenAI API with specific chunk sizes (N=20, M=15) and temperature=0.

## Key Results
- LAHAR achieves macro-average F1-score of 50.14% across 17 activity classes in multi-person settings
- Performance comparable to state-of-the-art methods at higher second-level resolution versus minute-level resolution
- Maintains robustness in multi-person scenarios with similar performance to single-person recognition

## Why This Works (Mechanism)

### Mechanism 1
LLMs can separate mixed sensor events from different individuals by leveraging common sense reasoning and in-context learning. The framework uses a two-stage prompt engineering approach where the LLM first processes event-level sensor data, assigns actions to individual subjects, and generates natural language descriptions. This works under the assumptions that related sensor events are more likely triggered by the same person, and a person cannot simultaneously trigger two unrelated sensors.

### Mechanism 2
LLMs can integrate contextual information to improve activity recognition accuracy. The framework provides contextual information through language prompts, including house layout, sensor descriptions, activity lists, and user schedules. This contextual information is crucial for understanding correlations between sensors and for activity recognition, enabling the LLM to align sensor events with relevant activities.

### Mechanism 3
LLMs can provide explainable activity recognition through natural language reasoning. The framework uses chain-of-thought prompting to guide the LLM's reasoning process, generating natural language explanations for activity predictions in a predefined JSON format containing start/end times, duration, last activity, reasoning, and activity labels.

## Foundational Learning

- **Chain-of-Thought Prompting**: Guides the LLM's reasoning process, enabling accurate activity predictions and explanations. Quick check: How does chain-of-thought prompting differ from standard prompting in terms of guiding LLM reasoning?
- **In-Context Learning**: Allows the LLM to adapt to novel environments and sensor configurations without extensive training data. Quick check: What are the key differences between in-context learning and traditional fine-tuning approaches?
- **Sensor Event Processing**: Transforms raw sensor data into a format suitable for LLM input, enabling fine-grained activity recognition. Quick check: How does the framework handle continuous sensor readings versus discrete sensor events?

## Architecture Onboarding

- **Component map**: Data Preprocessing → Context Integration → Action-Level Separation → Activity-Level Reasoning
- **Critical path**: Data Preprocessing → Context Integration → Action-Level Separation → Activity-Level Reasoning
- **Design tradeoffs**: Resolution vs. LLM capacity (higher resolution requires more processing power), context richness vs. prompt length (more context improves accuracy but increases prompt size), explainability vs. performance (detailed explanations may reduce prediction accuracy)
- **Failure signatures**: Incorrect event assignment (subjects' activities are mixed), missing activities (some activities not recognized), inaccurate timing (incorrect start/end times)
- **First 3 experiments**: 1) Validate event separation on simple multi-person scenario with distinct sensor patterns, 2) Assess context integration impact on accuracy, 3) Measure explainability quality against ground truth activity sequences

## Open Questions the Paper Calls Out

### Open Question 1
How does LAHAR's performance compare to traditional supervised machine learning models trained on large labeled datasets? The paper does not include a comparison to traditional supervised learning approaches, leaving this question unanswered.

### Open Question 2
How does LAHAR's performance scale with the number of residents in a household? The paper does not provide data on performance in households with more than two residents.

### Open Question 3
How does the choice of LLM affect LAHAR's performance and explainability? The paper mentions using GPT-4-32k but does not explore the impact of using different LLMs.

### Open Question 4
How robust is LAHAR to sensor noise and failures? The paper does not provide a systematic evaluation of performance under various sensor noise and failure scenarios.

## Limitations

- Limited evaluation scope to only the ARAS dataset containing data from two houses, making generalizability difficult to assess
- Prompt engineering opacity with unspecified exact prompt templates and examples, creating reproduction challenges
- Resource constraints not addressed, with no discussion of computational costs or inference time requirements for high-resolution processing

## Confidence

**High confidence**: The core concept that LLMs can process natural language descriptions of sensor events and reason about activities is well-supported, and the claim of comparable accuracy at higher resolution is reasonable given demonstrated results.

**Medium confidence**: The effectiveness of the two-stage prompt engineering approach for resident separation and activity recognition, though implementation details remain unclear.

**Low confidence**: Claims about practical deployment feasibility and resource efficiency due to lack of discussion on computational costs, latency, or scalability concerns.

## Next Checks

1. **Cross-dataset validation**: Test LAHAR on multiple HAR datasets beyond ARAS to verify generalizability claims across different sensor types, layouts, and environmental conditions.

2. **Prompt template reproducibility**: Implement and test multiple variations of the prompt templates to identify critical components for successful activity recognition and resident separation.

3. **Resource efficiency benchmarking**: Measure and compare computational resources, inference time, and cost per inference between LAHAR and traditional HAR methods to assess practical deployment feasibility.