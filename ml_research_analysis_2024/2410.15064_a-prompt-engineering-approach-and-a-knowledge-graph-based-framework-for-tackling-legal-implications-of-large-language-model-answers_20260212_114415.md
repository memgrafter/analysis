---
ver: rpa2
title: A Prompt Engineering Approach and a Knowledge Graph based Framework for Tackling
  Legal Implications of Large Language Model Answers
arxiv_id: '2410.15064'
source_url: https://arxiv.org/abs/2410.15064
tags:
- legal
- prompt
- user
- llms
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the problem of LLMs recommending actions that
  have potential legal implications, often without warning users. To address this,
  the authors propose a two-pronged approach: first, a prompt engineering method that
  uses a template to encourage LLMs to identify and highlight legal issues in their
  responses; second, a framework that leverages a legal knowledge graph to provide
  accurate legal citations for these issues.'
---

# A Prompt Engineering Approach and a Knowledge Graph based Framework for Tackling Legal Implications of Large Language Model Answers

## Quick Facts
- arXiv ID: 2410.15064
- Source URL: https://arxiv.org/abs/2410.15064
- Authors: George Hannah; Rita T. Sousa; Ioannis Dasoulas; Claudia d'Amato
- Reference count: 40
- Primary result: Proposed framework uses prompt engineering and legal knowledge graph to identify and cite legal implications in LLM responses

## Executive Summary
This paper addresses the critical issue of Large Language Models (LLMs) providing recommendations with potential legal implications without appropriate warnings or citations. The authors propose a two-pronged solution: a prompt engineering approach using structured templates to encourage LLMs to identify legal issues, and a framework powered by a legal knowledge graph to provide accurate citations for these issues. The work represents a significant step toward making LLM interactions more legally aware and trustworthy, particularly important as these models become more integrated into decision-making processes.

## Method Summary
The authors present a framework that combines prompt engineering with knowledge graph technology to address legal implications in LLM responses. The prompt engineering component uses a structured template to encourage LLMs to identify and highlight potential legal issues in their recommendations. The knowledge graph component provides a legal database that can be queried to generate specific citations for identified legal issues. The framework acts as an intermediary layer between users and LLMs, offering both accurate legal citations and lay summaries to improve comprehension. The approach aims to create a more trustworthy system by combining LLM reasoning capabilities with authoritative legal references.

## Key Results
- Prompt engineering with structured templates successfully encouraged LLMs to identify legal implications in user prompts
- Knowledge graph framework provided accurate legal citations that LLMs cannot reliably generate themselves
- Combined approach offers a more comprehensive solution than either prompt engineering or knowledge graphs alone
- Framework demonstrated potential for cross-LLM applicability and improved user comprehension through lay summaries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The prompt template forces LLMs to surface legal implications by specifying a response format that includes a "Potential Legal Issues" section.
- Mechanism: The template explicitly instructs the LLM to provide its usual recommendation followed by a bulleted list of legal issues if the prompt may involve illegal or dangerous activities.
- Core assumption: LLMs will follow the structural format of the prompt template and populate the placeholders with relevant legal content.
- Evidence anchors:
  - [abstract] "a short-term solution consisting in an approach for isolating these legal issues through prompt re-engineering"
  - [section 4.1] "The template is meant to be used by all audiences... The template aims to function as an additional safety layer for LLM conversations"
  - [corpus] Weak - no direct citation of template effectiveness in other works
- Break condition: The LLM ignores the template format and continues with its default response style, or fails to generate legal content even when instructed.

### Mechanism 2
- Claim: The knowledge graph (KG) provides accurate, specific legal citations that LLMs cannot reliably generate themselves.
- Mechanism: The KG stores legislation in granular form (article, paragraph, subparagraph, etc.) with metadata, allowing the framework to query for precise legal text that justifies why an action is illegal.
- Core assumption: The KG contains comprehensive, up-to-date legal data with appropriate metadata to map LLM-identified issues to exact legal provisions.
- Evidence anchors:
  - [abstract] "a framework powered by a legal knowledge graph (KG) to generate legal citations for these legal issues"
  - [section 5.2] "The cornerstone of our framework is the utilisation of a Legal KG that can provide a deeper level of insight into legal issues and enable more accurate references to the laws"
  - [corpus] Weak - no external validation of KG completeness or accuracy
- Break condition: The KG lacks relevant legislation for the identified legal issue, or the SPARQL queries fail to return the correct text fragment.

### Mechanism 3
- Claim: Combining LLM reasoning with KG lookup creates a more trustworthy system than either component alone.
- Mechanism: The LLM identifies potential legal issues from user prompts, the KG provides authoritative legal citations, and a summary manager generates lay explanations to improve user comprehension.
- Core assumption: The LLM's identification of legal issues is sufficiently accurate to enable effective KG querying, and the KG's citations are correctly matched to the issues.
- Evidence anchors:
  - [abstract] "The framework is meant as intermediary layer between the user and the LLM... offering a solution that ensures trustability across different scenarios"
  - [section 5.1] "The cornerstone of our framework is the utilisation of a Legal KG that can provide a deeper level of insight into legal issues and enable more accurate references to the laws"
  - [corpus] Weak - no comparative analysis of combined vs. individual approaches
- Break condition: LLM-identified issues don't map to KG content, or the lay summaries misrepresent the legal citations.

## Foundational Learning

- Concept: Prompt engineering and template patterns
  - Why needed here: The template pattern is the core mechanism for guiding LLM responses to include legal considerations
  - Quick check question: What are the three main components of the prompt template described in section 4.1?

- Concept: Knowledge graph structure and SPARQL querying
  - Why needed here: The framework relies on querying a legal KG to retrieve specific legislation
  - Quick check question: What granularity of legal text does the framework aim to retrieve from the KG?

- Concept: Legal domain knowledge and terminology
  - Why needed here: Understanding legal concepts is essential for mapping LLM-identified issues to appropriate KG content
  - Quick check question: What challenge does section 6 identify regarding legal jargon and LLM integration?

## Architecture Onboarding

- Component map: User Interface → Prompt Reformatter → LLM → Knowledge Graph Search → Legal KG → Citation and Lay Summary Manager → User Interface
- Critical path: User prompt → Prompt reformatter (template application) → LLM (legal issue identification) → KG search (query generation) → Legal KG (citation retrieval) → Summary manager (lay summary) → User alert card
- Design tradeoffs: Using a KG intermediary layer adds complexity but provides accuracy; prompt engineering alone is simpler but less reliable
- Failure signatures: LLM refuses to follow template format; KG returns no relevant legislation; lay summaries misrepresent legal citations
- First 3 experiments:
  1. Test the prompt template with ChatGPT-3.5 on a prompt with minor legal implications to verify the "Potential Legal Issues" section appears
  2. Query the KG with a known legal issue to confirm it returns the correct legislation fragment
  3. Combine the prompt template and KG lookup to process a user prompt and verify the complete alert card is generated with both citation and summary

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can legal knowledge graphs be effectively enriched with additional sources of information, such as understanding that gin is a type of spirit?
- Basis in paper: [inferred] from the discussion of the gin example and the need to integrate the newly developed KGs with existing KGs, following the Linked Data principles.
- Why unresolved: The paper acknowledges the need for enrichment but does not provide specific methods or approaches for integrating diverse sources of information into legal KGs.
- What evidence would resolve it: A detailed methodology or framework for integrating external data sources into legal KGs, along with empirical results demonstrating improved accuracy and completeness.

### Open Question 2
- Question: How can prompt engineering be automated to minimize dependence on manual curation and improve consistency across different LLMs?
- Basis in paper: [explicit] from the conclusion stating the need for future research to prioritize the automation of prompt engineering.
- Why unresolved: The paper identifies the importance of automation but does not provide concrete solutions or approaches for achieving this goal.
- What evidence would resolve it: A comprehensive framework or algorithm for automating prompt engineering, along with experimental results showing improved performance and reduced manual effort.

### Open Question 3
- Question: How can legal citations be generated more accurately and efficiently within the framework, considering the dynamic nature of legal domains?
- Basis in paper: [inferred] from the discussion of the challenges in generating accurate legal citations and the need for periodically re-indexing the legal KG.
- Why unresolved: The paper acknowledges the importance of accurate legal citations but does not provide specific techniques or approaches for improving their generation.
- What evidence would resolve it: A novel method or algorithm for generating accurate legal citations, along with empirical results demonstrating improved accuracy and efficiency compared to existing approaches.

## Limitations
- The prompt engineering approach relies heavily on LLM compliance with template structures, which may not generalize across different models
- The legal knowledge graph requires extensive, up-to-date legal content with comprehensive metadata to be truly effective
- Limited empirical validation of the framework's real-world performance and user comprehension of generated lay summaries
- No comparative analysis demonstrating that the combined approach outperforms either component alone

## Confidence
**Medium Confidence**: The core hypothesis that prompt engineering can encourage LLMs to identify legal implications is supported by the paper's description of the template approach, but lacks rigorous empirical validation. The effectiveness of the template in diverse legal scenarios remains uncertain.

**Medium Confidence**: The knowledge graph-based citation framework is conceptually sound and addresses a clear limitation of LLMs, but the paper doesn't provide evidence of the KG's completeness or the accuracy of SPARQL queries in retrieving relevant legislation.

**Low Confidence**: The claim that combining LLM reasoning with KG lookup creates a more trustworthy system than either component alone is asserted but not empirically demonstrated through comparative analysis or user studies.

## Next Checks
1. **Prompt Template Effectiveness Test**: Apply the proposed prompt template to 50 diverse prompts with varying degrees of legal implications across different LLM models (GPT-4, Claude, LLaMA) and measure the consistency and accuracy of legal issue identification.

2. **Knowledge Graph Coverage and Accuracy Assessment**: Conduct a systematic evaluation of the legal knowledge graph's coverage by testing it against a benchmark set of 100 known legal scenarios, measuring both the presence of relevant legislation and the accuracy of SPARQL query results.

3. **End-to-End User Comprehension Study**: Recruit 30 legal professionals and 30 lay users to interact with the complete framework (prompt template + KG lookup + lay summaries) and measure their ability to correctly understand the legal implications and appropriate citations provided for 20 test prompts.