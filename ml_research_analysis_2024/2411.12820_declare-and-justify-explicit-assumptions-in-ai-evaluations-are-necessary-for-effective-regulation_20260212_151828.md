---
ver: rpa2
title: 'Declare and Justify: Explicit assumptions in AI evaluations are necessary
  for effective regulation'
arxiv_id: '2411.12820'
source_url: https://arxiv.org/abs/2411.12820
tags:
- capabilities
- evaluations
- assumptions
- these
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies critical assumptions underlying AI evaluations
  used in safety regulations, including comprehensive threat modeling, proxy task
  validity, and adequate capability elicitation. Many of these assumptions cannot
  currently be robustly justified, particularly for autonomous AI risks.
---

# Declare and Justify: Explicit assumptions in AI evaluations are necessary for effective regulation

## Quick Facts
- arXiv ID: 2411.12820
- Source URL: https://arxiv.org/abs/2411.12820
- Reference count: 39
- AI developers should explicitly declare and justify assumptions underlying safety evaluations, with inadequate justifications warranting development halts

## Executive Summary
This paper analyzes the critical assumptions underlying AI safety evaluations used in regulatory frameworks, identifying that many of these assumptions cannot currently be robustly justified. The authors examine evaluation workflows from major AI developers and find that assumptions about comprehensive threat modeling, proxy task validity, and adequate capability elicitation are particularly problematic for autonomous AI risks. They propose that developers should be required to explicitly declare and justify these assumptions as part of safety cases, with inadequate justifications warranting halting development. This approach aims to enhance transparency and regulatory effectiveness by ensuring that safety claims based on evaluations are properly scrutinized.

## Method Summary
The authors conducted a literature review and analysis of current AI evaluation workflows and their underlying assumptions, examining frameworks from major AI developers including Anthropic, OpenAI, and Google DeepMind. They identified key assumptions in safety evaluations, including comprehensive threat modeling, proxy task validity, and adequate capability elicitation, then analyzed the validity of these assumptions across different risk types (misuse vs autonomous risks). The analysis focused on whether these assumptions could be justified given current methodologies and the specific challenges posed by autonomous AI systems.

## Key Results
- Many assumptions underlying AI safety evaluations cannot currently be robustly justified, particularly for autonomous AI risks
- The validity of proxy tasks as indicators of dangerous capabilities lacks rigorous justification, especially for risks where AI systems may exploit novel threat vectors
- Current capability elicitation methods may fail to assess full model capabilities, particularly if models can strategically underperform or if unknown elicitation methods exist
- Developers should explicitly declare and justify evaluation assumptions as part of safety cases, with inadequate justifications warranting development halts

## Why This Works (Mechanism)
The proposed mechanism works by forcing transparency about the foundational assumptions that safety evaluations rely upon. By requiring explicit declaration and justification of assumptions like threat model completeness and proxy task validity, regulators and stakeholders can better understand the limitations of safety claims. This approach addresses the fundamental problem that current AI safety evaluations often make unstated assumptions that cannot be rigorously justified, particularly for autonomous risks where AI systems may discover novel threat vectors or strategically withhold capabilities.

## Foundational Learning
**Threat Modeling Completeness**: Understanding all potential ways an AI system could cause harm - needed because incomplete threat models can lead to false confidence in safety assessments; quick check: Does the threat model account for AI-discovered threat vectors beyond human imagination?

**Proxy Task Validity**: Verifying that success on safety evaluation tasks is necessary for dangerous real-world behavior - needed because proxy tasks may not capture the full complexity of dangerous capabilities; quick check: Has empirical evidence demonstrated that inability to perform proxy tasks prevents dangerous task completion?

**Capability Elicitation Adequacy**: Ensuring evaluation methods can reliably elicit near-maximal model capabilities - needed because models may strategically underperform or unknown elicitation methods may exist; quick check: Do evaluation protocols consistently achieve near-maximal performance across different domains?

**Precursor Capability Necessity**: Determining whether precursor capabilities must emerge before dangerous capabilities - needed because some capabilities may arise unpredictably or in non-linear ways; quick check: Is there historical evidence of consistent precursor capability development patterns across AI systems?

## Architecture Onboarding

**Component Map**: Safety Case Framework -> Assumption Declaration -> Justification Assessment -> Development Decision

**Critical Path**: Developer creates safety case with explicit assumptions → Regulatory review of justification adequacy → Decision to continue or halt development based on justification quality

**Design Tradeoffs**: 
- Transparency vs proprietary concerns: Requiring explicit assumption declaration may reveal sensitive information about evaluation methodologies
- Regulatory burden vs safety assurance: More rigorous justification requirements increase compliance costs but improve safety verification
- Development speed vs thorough evaluation: More comprehensive assumption analysis slows development but reduces risk of overlooked vulnerabilities

**Failure Signatures**:
- False confidence in safety: When proxy task validity is assumed without justification, leading to undetected dangerous capabilities
- Missed autonomous risks: When threat models are incomplete and fail to account for AI-discovered threat vectors
- Strategic underperformance: When capability elicitation methods fail to assess true model capabilities due to strategic behavior

**3 First Experiments**:
1. Audit current AI safety evaluation frameworks to catalog all unstated assumptions
2. Test proxy task validity by attempting dangerous tasks with models that fail corresponding proxy tasks
3. Develop metrics for assessing the adequacy of threat model completeness across different risk domains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we rigorously validate that proxy tasks used in AI evaluations are truly necessary and sufficient indicators of dangerous capabilities?
- Basis in paper: The paper identifies "Proxy Task Validity" as a core assumption, noting that proxy success must be a necessary requirement for dangerous task success
- Why unresolved: Current methods lack robust ways to verify this assumption, particularly for autonomous AI risks where AI systems may exploit threat vectors in inhuman ways that don't require competence at proxy tasks
- What evidence would resolve it: Empirical studies demonstrating that models unable to perform proxy tasks cannot complete corresponding dangerous tasks in practice, across multiple capability domains

### Open Question 2
- Question: What methodology could reliably elicit and measure the full capabilities of AI systems, especially for autonomous risk assessment?
- Basis in paper: The paper identifies "Adequate Capability Elicitation" as a core assumption that is particularly difficult to justify for autonomous AI systems
- Why unresolved: Current evaluation methods may fail if there are unknown capability elicitation methods or if models strategically underperform on evaluations
- What evidence would resolve it: Development of evaluation protocols that can consistently elicit near-maximal model capabilities across different domains, with empirical validation against ground truth performance

### Open Question 3
- Question: Is there a reliable method to forecast whether and when precursor capabilities will emerge before dangerous capabilities in AI systems?
- Basis in paper: The paper identifies "Necessity of Precursor Capabilities" as an assumption that cannot currently be rigorously justified
- Why unresolved: Current understanding of how capabilities arise in AI models is limited, and there are no good methods for determining that certain capabilities will arise before others
- What evidence would resolve it: Historical analysis of capability emergence patterns across multiple AI systems showing consistent precursor capability development timelines

## Limitations
- Uncertainty about specific methods AI developers currently use to elicit capabilities and whether they achieve comprehensive capability assessment
- Lack of concrete examples demonstrating when proxy tasks have failed to capture dangerous capabilities in practice
- Practical challenges in implementing development halting decisions based on inadequate justification assessments

## Confidence
**Medium** for the core recommendation of explicit assumption declaration, but **Low** for specific implementation details regarding when development should be halted.

## Next Checks
1. Analyze concrete case studies where proxy tasks have failed to capture dangerous capabilities in deployed AI systems
2. Survey current AI developers to document their specific capability elicitation methodologies and their justifications
3. Develop a framework for systematically evaluating the adequacy of threat model completeness for different risk types