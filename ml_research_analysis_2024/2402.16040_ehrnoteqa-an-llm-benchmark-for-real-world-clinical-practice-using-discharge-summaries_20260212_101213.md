---
ver: rpa2
title: 'EHRNoteQA: An LLM Benchmark for Real-World Clinical Practice Using Discharge
  Summaries'
arxiv_id: '2402.16040'
source_url: https://arxiv.org/abs/2402.16040
tags:
- patient
- discharge
- answer
- ehrnoteqa
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EHRNoteQA, a new benchmark designed to evaluate
  Large Language Models (LLMs) on real-world clinical tasks using patient discharge
  summaries. Unlike existing clinical benchmarks that focus on general medical knowledge,
  EHRNoteQA includes 962 question-answer pairs that require information extraction
  from multiple discharge summaries and cover 10 clinically relevant topics.
---

# EHRNoteQA: An LLM Benchmark for Real-World Clinical Practice Using Discharge Summaries

## Quick Facts
- arXiv ID: 2402.16040
- Source URL: https://arxiv.org/abs/2402.16040
- Reference count: 40
- Key outcome: New benchmark for evaluating LLMs on clinical tasks using discharge summaries, covering 10 topics with 962 question-answer pairs, demonstrating strong correlation with clinician evaluations (Spearman 0.78, Kendall 0.62)

## Executive Summary
This paper introduces EHRNoteQA, a benchmark designed to evaluate Large Language Models (LLMs) on real-world clinical tasks using patient discharge summaries. Unlike existing clinical benchmarks that focus on general medical knowledge, EHRNoteQA requires information extraction from multiple discharge summaries across 10 clinically relevant topics. The dataset was generated using GPT-4 and refined by clinicians to ensure relevance, supporting both open-ended and multi-choice evaluation formats with GPT-4-based scoring methods.

The benchmark demonstrates strong correlation with clinician evaluations and shows performance variation based on model size, foundation model, and instruction tuning across 27 evaluated LLMs. EHRNoteQA addresses the gap in clinical LLM evaluation by focusing on practical information extraction tasks that mirror real-world clinical practice, while being publicly available under PhysioNet credential access.

## Method Summary
The EHRNoteQA benchmark was created using a two-stage process involving GPT-4 for initial question-answer pair generation followed by clinician review for relevance. The dataset includes 962 question-answer pairs covering 10 clinically relevant topics, with questions requiring information extraction from multiple discharge summaries. The benchmark supports both open-ended and multi-choice evaluation formats and employs GPT-4-based scoring methods for assessment. Evaluation was conducted across 27 LLMs to assess performance variation based on model size, foundation model, and instruction tuning approaches.

## Key Results
- EHRNoteQA demonstrates strong correlation with clinician evaluations (Spearman 0.78, Kendall 0.62)
- Performance varies significantly based on model size, foundation model, and instruction tuning
- Outperforms other benchmarks in representing real-world clinical practice scenarios
- Covers 10 clinically relevant topics with 962 question-answer pairs requiring multi-summary information extraction

## Why This Works (Mechanism)
The benchmark works by bridging the gap between general medical knowledge benchmarks and real-world clinical practice through targeted information extraction tasks. By requiring LLMs to extract and synthesize information from multiple discharge summaries across diverse clinical topics, EHRNoteQA evaluates practical clinical reasoning skills that directly translate to clinical workflows. The GPT-4-based scoring provides consistent evaluation while the clinician refinement ensures clinical relevance and accuracy of the dataset.

## Foundational Learning
**Clinical Information Extraction**: Understanding how to identify and synthesize relevant clinical information from text documents
- Why needed: Core skill for LLMs in healthcare applications
- Quick check: Can the model accurately identify key clinical events and their relationships across multiple documents

**Multi-Document Reasoning**: Ability to integrate information across multiple clinical documents
- Why needed: Real-world clinical practice requires synthesizing information from various sources
- Quick check: Can the model maintain consistency and identify contradictions across documents

**Clinical Domain Knowledge**: Understanding medical terminology, procedures, and clinical workflows
- Why needed: Essential for accurate interpretation of clinical text
- Quick check: Can the model correctly interpret medical abbreviations and clinical context

## Architecture Onboarding
**Component Map**: GPT-4 (question generation) -> Clinician review (relevance validation) -> Dataset curation (formatting) -> LLM evaluation (performance assessment) -> GPT-4 scoring (result validation)

**Critical Path**: Question generation → Clinician review → Dataset creation → LLM evaluation → Performance analysis

**Design Tradeoffs**: Uses GPT-4 for scalability in question generation but relies on human clinicians for quality control, balancing efficiency with accuracy. The multi-choice format provides objective scoring but may not capture nuanced clinical reasoning.

**Failure Signatures**: Poor performance may indicate inadequate multi-document reasoning, insufficient clinical domain knowledge, or inability to handle complex clinical scenarios requiring synthesis across multiple summaries.

**First Experiments**:
1. Evaluate baseline performance using general medical knowledge benchmarks for comparison
2. Test model performance across different clinical specialties to identify domain-specific strengths/weak