---
ver: rpa2
title: LLMs as On-demand Customizable Service
arxiv_id: '2401.16577'
source_url: https://arxiv.org/abs/2401.16577
tags:
- language
- knowledge
- learning
- llms
- architecture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a hierarchical, distributed LLM architecture
  that addresses deployment challenges by organizing language models across multiple
  layers based on language, domain, and application specificity. The proposed architecture
  enables on-demand customization and deployment of LLMs across heterogeneous computing
  platforms, from powerful computers to resource-constrained IoT devices.
---

# LLMs as On-demand Customizable Service

## Quick Facts
- arXiv ID: 2401.16577
- Source URL: https://arxiv.org/abs/2401.16577
- Authors: Souvika Sarkar; Mohammad Fakhruddin Babar; Monowar Hasan; Shubhra Kanti Karmaker
- Reference count: 22
- One-line primary result: Presents hierarchical, distributed LLM architecture enabling on-demand customization and deployment across heterogeneous computing platforms

## Executive Summary
This paper proposes a hierarchical, distributed architecture for organizing language models across multiple layers based on language, domain, and application specificity. The architecture addresses deployment challenges by enabling users to select appropriate models matching their hardware capabilities and application needs, from powerful computers to resource-constrained IoT devices. The layered approach supports continual learning and knowledge transfer across the hierarchy while preventing catastrophic forgetting. A healthcare use case demonstrates the practical potential of this approach for enabling advanced language model capabilities even with limited computational resources.

## Method Summary
The paper introduces a hierarchical LLM architecture that organizes language models in layers from general-purpose master models down to domain-specific and sub-domain models, ultimately deployed on heterogeneous end devices. The method employs continual learning mechanisms with bidirectional upstream and downstream knowledge transfer to keep all models synchronized. A virtual assistant interfaces with users to recommend suitable models based on their requirements and resource constraints. The architecture aims to democratize AI access by allowing resource-constrained users to leverage domain-specific models that provide adequate performance without requiring expensive hardware.

## Key Results
- Hierarchical organization enables on-demand customization and deployment of LLMs across heterogeneous computing platforms
- Layered approach provides optimal trade-offs between computational resources and user application needs
- Architecture supports continual learning and knowledge transfer across the hierarchy
- Healthcare use case demonstrates advanced language model capabilities accessible with limited computational resources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical organization reduces computational burden by distributing knowledge across layers based on specificity and size.
- Mechanism: Structure models in a top-down hierarchy where upper layers contain general-purpose, larger models and lower layers contain domain-specific, smaller models, allowing users to select models matching their computational resources.
- Core assumption: Knowledge distribution across layers is efficient enough that domain-specific tasks can be handled by smaller models without significant performance degradation.
- Evidence anchors: [abstract] "optimal trade-offs between available computational resources and user's application needs"; [section] "vast knowledge learned from big data corpora is distributed across multiple layers based on target language, domain, and application-oriented sub-domains."
- Break condition: If knowledge transfer mechanisms between layers are inefficient, causing significant performance degradation when using smaller models.

### Mechanism 2
- Claim: Continual learning with bidirectional knowledge transfer keeps all models synchronized and up-to-date.
- Mechanism: When models are updated with new information, a two-way knowledge transfer process occurs both upstream and downstream across all layers, ensuring all models benefit from new data insights.
- Core assumption: Upstream knowledge transfer can be efficiently implemented without requiring raw data sharing, preserving privacy while maintaining performance.
- Evidence anchors: [abstract] "layered approach allows users to select appropriate models...while supporting continual learning and knowledge transfer"; [section] "language model shares new knowledge with both preceding and succeeding layer models through Upstream and Downstream mechanisms."
- Break condition: If upstream knowledge transfer cannot effectively share knowledge without raw data, or if downstream transfer causes significant information loss.

### Mechanism 3
- Claim: Architecture enables democratization of AI by allowing resource-constrained users to access advanced capabilities.
- Mechanism: Organize models in layers based on size and resource requirements, allowing users with limited resources to select smaller, domain-specific models that still provide adequate performance.
- Core assumption: Smaller, domain-specific models can provide sufficient performance for practical applications when properly fine-tuned.
- Evidence anchors: [abstract] "enables on-demand customization and deployment...from powerful computers to resource-constrained IoT devices"; [section] "Local devices with limited computational resources can opt for smaller, more resource-friendly language models from lower layers."
- Break condition: If performance gap between smaller models and larger models is too large for practical applications.

## Foundational Learning

- Concept: Hierarchical knowledge organization in machine learning
  - Why needed here: Understanding how to structure knowledge across multiple model layers based on specificity and resource requirements is fundamental to this architecture
  - Quick check question: Can you explain the difference between a general-purpose language model and a domain-specific language model in terms of their knowledge representation and computational requirements?

- Concept: Knowledge distillation techniques
  - Why needed here: The architecture relies on distillation methods to transfer knowledge from larger "teacher" models to smaller "student" models across layers
  - Quick check question: What is the primary challenge when using knowledge distillation to compress a large language model into a smaller one while maintaining acceptable performance?

- Concept: Continual learning and catastrophic forgetting
  - Why needed here: The architecture implements continual learning across the hierarchy, which must address preventing previously learned knowledge from being forgotten when new information is incorporated
  - Quick check question: What are the two main strategies mentioned in the paper for addressing catastrophic forgetting in hierarchical LLM architectures?

## Architecture Onboarding

- Component map:
  - Master LLM Layer (Root) -> Language Specific Language Model (LSLM) Layer -> Domain Language Model (DLM) Layer -> Sub-Domain Language Model (SDLM) Layer -> End Devices Layer
  - Virtual Assistant (interface) -> Language Model Recommender (selection) -> Continual Learning Mechanisms (updates) -> Upstream/Downstream Knowledge Transfer (synchronization)

- Critical path:
  1. User interacts with Virtual Assistant to specify requirements
  2. Language Model Recommender selects appropriate model from hierarchy
  3. User acquires and fine-tunes the recommended model on local device
  4. Model is continually updated with new data
  5. Knowledge transfer propagates updates throughout the hierarchy

- Design tradeoffs:
  - Model size vs. performance: Smaller models are more accessible but may have reduced capabilities
  - Privacy vs. knowledge sharing: Raw data sharing enables better transfer but compromises privacy
  - Update frequency vs. system stability: Frequent updates keep models current but may introduce instability
  - Resource allocation vs. accessibility: More powerful models provide better performance but limit accessibility

- Failure signatures:
  - Poor model selection recommendations from the recommender system
  - Significant performance degradation when using smaller models compared to master model
  - Knowledge transfer failures causing models to become desynchronized
  - Catastrophic forgetting when models are continually updated
  - Malicious nodes poisoning the system through data or model manipulation

- First 3 experiments:
  1. Benchmark performance comparison between master model and distilled domain-specific models on representative tasks to quantify the performance gap
  2. Test upstream knowledge transfer mechanisms using synthetic data generation to verify privacy preservation while maintaining knowledge transfer effectiveness
  3. Implement a simple two-layer hierarchy (master + one domain layer) and measure continual learning performance and catastrophic forgetting prevention

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific criteria should be used to determine when an Upstream language model should be updated with knowledge from downstream models?
- Basis in paper: [explicit] The paper identifies "When should we update the parent language model?" as Challenge 4, noting that not all updates from end devices are pertinent to the Master language model.
- Why unresolved: The paper mentions evaluating the significance of new data but does not provide specific quantitative criteria or thresholds for triggering updates.
- What evidence would resolve it: Empirical studies comparing different update criteria (e.g., frequency thresholds, data significance scores, resource utilization metrics) and their impact on model performance and efficiency.

### Open Question 2
- Question: How can the architecture effectively prevent catastrophic forgetting while maintaining continual learning capabilities across the hierarchy?
- Basis in paper: [explicit] The paper identifies this as Challenge 3, stating that catastrophic forgetting is a common side-effect of continual learning where previously acquired knowledge is at risk of being lost.
- Why unresolved: The paper acknowledges this as a challenge but does not propose specific techniques or evaluation methods to address it within the hierarchical LLM context.
- What evidence would resolve it: Comparative studies of different continual learning techniques (e.g., regularization methods, rehearsal approaches, architectural modifications) specifically applied to hierarchical LLM architectures.

### Open Question 3
- Question: What specific techniques can be developed to detect and isolate malicious nodes while preventing information poisoning in the hierarchical architecture?
- Basis in paper: [explicit] The paper identifies this as Challenge 5, noting that malicious nodes could conduct data poisoning, model poisoning, or free-riding attacks.
- Why unresolved: The paper only outlines the types of attacks but does not propose detection mechanisms, isolation strategies, or evaluation frameworks for security in this distributed architecture.
- What evidence would resolve it: Implementation and testing of anomaly detection algorithms, trust metrics, and isolation protocols specifically designed for hierarchical LLM architectures, along with their effectiveness in preventing various attack vectors.

## Limitations
- No quantitative evaluation results for critical components including performance metrics and knowledge transfer efficiency
- Limited discussion of computational overhead introduced by coordination mechanisms
- No security analysis of potential attack vectors beyond brief mentions
- Healthcare use case described conceptually without implementation details or performance benchmarks

## Confidence

**High Confidence**: The hierarchical architecture design concept and its general feasibility for distributing LLMs across resource-constrained devices.

**Medium Confidence**: The effectiveness of upstream/downstream knowledge transfer mechanisms and continual learning approach.

**Low Confidence**: The practical performance of smaller domain-specific models compared to larger general models, and the overall system's ability to maintain model quality while enabling resource-constrained deployment.

## Next Checks

1. **Performance Benchmarking Validation**: Implement a minimal prototype with two layers (master + one domain layer) and benchmark the performance of distilled domain models against the master model on representative tasks, measuring both accuracy degradation and computational efficiency gains to quantify the trade-offs.

2. **Knowledge Transfer Efficiency Test**: Conduct experiments comparing upstream knowledge transfer using synthetic data generation versus parameter sharing approaches, measuring knowledge retention effectiveness and privacy preservation while quantifying the computational overhead of each method.

3. **Security Vulnerability Assessment**: Design and execute attack simulations targeting the hierarchical architecture, including data poisoning, model poisoning, and free-riding attacks, to identify specific vulnerabilities in the coordination mechanisms and evaluate the effectiveness of proposed security measures.