---
ver: rpa2
title: 'CCoE: A Compact and Efficient LLM Framework with Multi-Expert Collaboration
  for Resource-Limited Settings'
arxiv_id: '2407.11686'
source_url: https://arxiv.org/abs/2407.11686
tags:
- expert
- ccoe
- framework
- performance
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the CCoE framework, which addresses the challenge
  of scaling large language models (LLMs) to support multiple downstream domain applications
  under resource constraints. The CCoE architecture integrates domain-specific experts
  into a unified LLM through independently trained expert subnetworks on a shared
  backbone partition.
---

# CCoE: A Compact and Efficient LLM Framework with Multi-Expert Collaboration for Resource-Limited Settings

## Quick Facts
- arXiv ID: 2407.11686
- Source URL: https://arxiv.org/abs/2407.11686
- Authors: Shaomang Huang; Jianfeng Pan; Min Peng; Hanzhong Zheng
- Reference count: 40
- Reduces memory usage by 61.3% compared to multi-domain model ensemble methods

## Executive Summary
This paper introduces CCoE, a framework that integrates multiple domain-specific expert subnetworks into a unified large language model (LLM) architecture for resource-constrained multi-domain applications. The framework addresses the challenge of scaling LLMs to support diverse downstream tasks while maintaining efficiency through parameter sharing and selective expert activation. CCoE employs a frozen shared backbone with independently trained domain experts, rule-based gating for routing, and an expert planning mechanism for complex task decomposition.

## Method Summary
The CCoE framework consists of a frozen shared backbone LLM with independently trained domain expert subnetworks inserted at specific FFN layers. Each expert is trained on a frozen backbone using domain-specific data, preventing cross-domain interference. Task routing uses a combination of rule-based gating with an expert-domain mapping matrix and an optional expert planning mechanism that decomposes complex tasks and allocates them to appropriate specialists. The framework supports dynamic expert expansion through a pop-and-push training strategy, allowing new experts to be added without retraining the entire model.

## Key Results
- Achieves performance comparable to domain-specific LLMs across five domains (Math, Code, Law, Medical, Text-to-SQL)
- Reduces memory usage by 61.3% compared to multi-domain model ensemble methods
- Improves inference efficiency by 0.76x over parameter-efficient multi-expert integration approaches

## Why This Works (Mechanism)

### Mechanism 1
Independent expert subnetworks reduce knowledge interference compared to mixed training. Each domain expert is trained on a frozen shared backbone, preventing cross-domain gradient updates that cause catastrophic forgetting. The core assumption is that the shared backbone learns general features sufficiently well to support multiple domain-specific fine-tuned subnetworks. Break condition: If domain tasks require frequent shared representation updates, the frozen backbone may limit adaptation.

### Mechanism 2
Rule-based gating plus expert planning balances determinism and flexibility in routing. Rule-based routing uses a domain-expert mapping matrix for high-confidence tasks; planning expert decomposes complex tasks and dynamically allocates subtasks to specialists. The core assumption is that simple rules suffice for routine domains, while complex tasks benefit from hierarchical decomposition. Break condition: If rule mapping is incomplete or planning expert misestimates task complexity, routing accuracy degrades.

### Mechanism 3
Pop-and-push training allows expert expansion without retraining the full model. New experts are trained independently, deep-copied into the CCoE framework, and memory of temporary copies is released after weight transfer. The core assumption is that deep copying preserves expert weights exactly, and memory release prevents interference with other experts. Break condition: If deep copy fails or memory management is imperfect, weight corruption or leaks occur.

## Foundational Learning

- Concept: Catastrophic forgetting in multi-task fine-tuning
  - Why needed here: CCoE must preserve performance across domains while adding new experts
  - Quick check question: What happens to a model's performance on task A after fine-tuning on task B with full parameter updates?

- Concept: Mixture-of-Experts (MoE) routing
  - Why needed here: CCoE uses a simplified, static routing variant; understanding MoE helps compare trade-offs
  - Quick check question: How does MoE differ from CCoE in terms of gating granularity and parameter loading?

- Concept: Low-Rank Adaptation (LoRA) for efficient fine-tuning
  - Why needed here: CCoE's expert subnetworks are analogous to adapters but integrated into the backbone
  - Quick check question: What is the computational difference between applying multiple LoRA adapters vs. CCoE experts?

## Architecture Onboarding

- Component map: Shared backbone LLM → Rule-based router → Planning expert (optional) → Domain expert subnetworks inserted at FFN layers → Output fusion
- Critical path: Input → Router/Planner → Selected expert(s) → Backbone layers + expert FFNs → Final output
- Design tradeoffs: Fixed expert count vs. dynamic scaling; frozen backbone vs. joint fine-tuning; rule-based determinism vs. planner flexibility
- Failure signatures: Degraded performance in untouched domains (forgetting), high latency (overly complex routing), memory spikes (improper pop-push)
- First 3 experiments:
  1. Measure accuracy retention on one domain after adding a second expert
  2. Compare inference latency with rule-based routing vs. planner-based routing on mixed queries
  3. Profile memory usage during pop-push expert updates to confirm no leaks

## Open Questions the Paper Calls Out

### Open Question 1
How does the CCoE framework's performance scale when integrating more than five domain experts, and what are the theoretical limits of its memory efficiency gains compared to multi-domain model ensemble methods? The paper states that CCoE reduces memory usage by 61.3% compared to multi-domain model ensemble methods, and theoretically can reduce GPU memory usage by up to three-quarters as the number of domain experts increases. This remains unresolved as the paper only evaluates CCoE with five domain experts.

### Open Question 2
How does the choice of expert layer insertion strategy (e.g., global, front-back, middle) affect the performance of different domain experts, and are there domain-specific optimal strategies? The paper mentions that the distribution of expert layers varies by domain among different backbone models, and the best performance for all experts is achieved with the global strategy. However, it does not explore domain-specific optimal strategies.

### Open Question 3
How does the CCoE framework handle catastrophic forgetting when updating existing domain experts, and what are the mechanisms in place to preserve prior knowledge? The paper mentions that CCoE can seamlessly scale the number of experts and update their knowledge through a pop-and-push training strategy, and that it mitigates knowledge conflicts and forgetting among experts. However, it does not provide details on the specific mechanisms used to prevent catastrophic forgetting when updating existing domain experts.

## Limitations
- Scalability concerns with highly divergent domains where a single frozen backbone may not capture sufficient general features
- Dependence on complete expert-domain mapping matrices that require manual curation for new domains
- Uncertainty about long-term stability of the pop-and-push mechanism under continuous expert updates

## Confidence

**High Confidence**: The core architectural design of combining a frozen backbone with independently trained expert subnetworks is well-founded and supported by established principles in transfer learning and parameter-efficient fine-tuning.

**Medium Confidence**: The performance claims relative to domain-specific LLMs and other multi-expert approaches are credible but depend on the quality of the baseline implementations and dataset characteristics.

**Low Confidence**: The generalization capability of the rule-based gating system to unseen domains or tasks not represented in the training mapping matrix remains uncertain.

## Next Checks

1. Cross-domain interference test: Evaluate CCoE's performance on Domain A after adding experts for Domains B, C, and D, measuring performance degradation on the original domain to validate the frozen-backbone interference prevention claim.

2. Routing robustness analysis: Systematically evaluate the rule-based router's accuracy across all possible domain-task combinations in the five domains, including ambiguous cases that could fall under multiple domains, to assess the completeness of the expert-domain mapping matrix.

3. Memory leak detection under pop-push stress: Implement a continuous cycle of adding, removing, and re-adding experts in varying sequences over extended periods, monitoring memory usage and expert weight integrity to validate the pop-and-push mechanism's reliability in production scenarios.