---
ver: rpa2
title: '"You are an expert annotator": Automatic Best-Worst-Scaling Annotations for
  Emotion Intensity Modeling'
arxiv_id: '2403.17612'
source_url: https://arxiv.org/abs/2403.17612
tags:
- annotations
- rating
- emotion
- annotation
- scales
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using large language models to automatically
  annotate text data with continuous values, specifically emotion intensity, via Best-Worst
  Scaling (BWS). BWS, a comparative annotation method, is hypothesized to be more
  reliable than rating scales for LLM-based annotation.
---

# "You are an expert annotator": Automatic Best-Worst-Scaling Annotations for Emotion Intensity Modeling

## Quick Facts
- arXiv ID: 2403.17612
- Source URL: https://arxiv.org/abs/2403.17612
- Authors: Christopher Bagdon; Prathamesh Karmarkar; Harsha Gurulingappa; Roman Klinger
- Reference count: 11
- Primary result: BWS-LLM outperforms rating scales for continuous emotion intensity annotation

## Executive Summary
This paper introduces an automated approach for annotating text data with continuous emotion intensity values using Best-Worst Scaling (BWS) through large language models (LLMs). The authors hypothesize that BWS, a comparative annotation method, is more reliable than traditional rating scales for LLM-based annotation tasks. The approach is evaluated on the Affect-in-Tweets dataset, comparing BWS against rating scales, paired comparisons, and direct rating scales. Results demonstrate that BWS consistently outperforms other methods in both direct comparison to human annotations and indirect downstream evaluation using a trained regressor. The study suggests BWS as a promising approach for automated continuous value annotation that could potentially reduce the need for manual labeling in emotion intensity prediction tasks.

## Method Summary
The proposed method employs Best-Worst Scaling (BWS) as a comparative annotation framework for LLM-based continuous emotion intensity annotation. BWS presents annotators with tuples of text samples and asks them to identify which sample exhibits the highest and lowest intensity for a target emotion. The approach is implemented through carefully designed prompts to LLMs, which are then used to annotate text data automatically. The method is compared against three baseline approaches: traditional rating scales, paired comparisons, and direct rating scales. A trained regressor is used for indirect downstream evaluation, measuring the quality of automatically generated annotations by their performance in emotion intensity prediction tasks. The evaluation uses the Affect-in-Tweets dataset as the primary testbed, with experiments examining the impact of increasing the number of annotated tuples on annotation quality.

## Key Results
- BWS-LLM annotation consistently outperforms rating scales, paired comparisons, and direct rating scales in both direct comparison to human annotations and indirect downstream evaluation
- Increasing the number of annotated tuples further improves BWS-LLM annotation performance
- BWS shows promise as an approach for automated continuous value annotation that could reduce manual labeling requirements for emotion intensity prediction tasks

## Why This Works (Mechanism)
The mechanism behind BWS-LLM success lies in the comparative nature of Best-Worst Scaling, which forces the LLM to make relative judgments rather than absolute intensity ratings. This comparative framework helps mitigate the ambiguity and subjectivity inherent in continuous emotion intensity annotation by providing contextual anchors within each tuple. The BWS approach leverages the LLM's ability to understand relative differences between text samples, potentially leading to more consistent and reliable annotations than direct rating methods that require absolute intensity judgments on arbitrary scales.

## Foundational Learning

**Best-Worst Scaling (BWS)**: A comparative annotation method where annotators identify the best and worst items from a set of options. Needed because it provides relative context for annotation decisions, reducing ambiguity in continuous value assignments. Quick check: Can BWS effectively capture ordinal relationships in emotion intensity?

**Large Language Model (LLM) Prompt Engineering**: The process of designing effective prompts to elicit desired responses from LLMs. Needed because the quality of LLM-generated annotations heavily depends on prompt formulation and instruction clarity. Quick check: How sensitive is BWS annotation quality to prompt variations?

**Downstream Evaluation with Trained Regressor**: Using a trained model to evaluate annotation quality by measuring performance on a target task. Needed because direct comparison to human annotations may be limited, requiring alternative evaluation metrics. Quick check: Does downstream task performance correlate with annotation quality?

## Architecture Onboarding

Component Map: Text Data -> BWS Tuples -> LLM Annotation -> Aggregated Scores -> Evaluation (Direct + Downstream)

Critical Path: The core workflow involves generating BWS tuples from text data, submitting them to LLMs for annotation, aggregating the comparative responses into continuous intensity scores, and evaluating the resulting annotations through both direct human comparison and downstream task performance.

Design Tradeoffs: The approach trades computational cost (multiple LLM calls per tuple) for potentially higher annotation quality compared to single-rating methods. The BWS format requires careful tuple construction to ensure meaningful comparisons, while direct rating scales offer simplicity at the potential cost of reliability.

Failure Signatures: Poor annotation quality may manifest as inconsistent relative judgments across similar text samples, systematic bias toward certain emotion intensities, or failure to capture subtle intensity variations. LLM-specific failures could include hallucination of emotion content not present in the text or inconsistent application of intensity scales across different prompts.

First Experiments:
1. Generate BWS tuples from a subset of Affect-in-Tweets and evaluate LLM annotation consistency across multiple runs
2. Compare BWS annotation quality with varying tuple sizes (3-item vs 4-item tuples)
3. Test different LLM architectures (GPT-3.5 vs GPT-4) for BWS annotation to assess model sensitivity

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation is limited to a single dataset (Affect-in-Tweets) and task domain, raising questions about generalizability
- The computational costs and annotation time for BWS-LLM versus traditional methods are not analyzed
- Potential LLM biases in annotation and sensitivity to different LLM architectures are not addressed
- Experiments focus exclusively on English text, leaving multilingual applicability unexplored

## Confidence
High: BWS-LLM consistently outperforms baseline methods in controlled experiments
Medium: Claims about generalizability to other datasets and tasks are supported by limited evidence
Low: Assertions about reducing manual labeling requirements lack direct comparative validation

## Next Checks
1. Evaluate BWS-LLM annotation across multiple emotion datasets and domains to test generalizability
2. Compare computational costs and annotation time between BWS-LLM and traditional manual annotation methods
3. Conduct ablation studies with different LLM architectures to assess sensitivity to model choice