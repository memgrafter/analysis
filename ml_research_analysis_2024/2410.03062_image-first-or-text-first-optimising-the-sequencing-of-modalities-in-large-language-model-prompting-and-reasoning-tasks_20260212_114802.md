---
ver: rpa2
title: Image First or Text First? Optimising the Sequencing of Modalities in Large
  Language Model Prompting and Reasoning Tasks
arxiv_id: '2410.03062'
source_url: https://arxiv.org/abs/2410.03062
tags:
- image
- reasoning
- sequencing
- text
- modalities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study systematically evaluated how the sequencing of images\
  \ and text in multi-modal prompts affects the reasoning performance of large language\
  \ models (LLMs) across two benchmarks (M3Exam and M3COTS). Three commercial models\u2014\
  GPT-4o, Gemini-1.5 Flash, and Claude-3-Haiku\u2014were tested using zero-shot prompting\
  \ with chain-of-thought reasoning."
---

# Image First or Text First? Optimising the Sequencing of Modalities in Large Language Model Prompting and Reasoning Tasks

## Quick Facts
- arXiv ID: 2410.03062
- Source URL: https://arxiv.org/abs/2410.03062
- Authors: Grant Wardle; Teo Susnjak
- Reference count: 40
- Primary result: Modality sequencing significantly affects LLM reasoning performance, with image-first sequencing improving results on simpler tasks and text ordering effects persisting even without images.

## Executive Summary
This study systematically evaluated how the sequencing of images and text in multi-modal prompts affects the reasoning performance of large language models (LLMs) across two benchmarks (M3Exam and M3COTS). Three commercial models—GPT-4o, Gemini-1.5 Flash, and Claude-3-Haiku—were tested using zero-shot prompting with chain-of-thought reasoning. Results showed that modality sequencing significantly influenced accuracy, especially in simpler tasks (M3COTS with one image per prompt). Image-first sequencing improved performance on M3COTS, while embedding images within text yielded better results on M3Exam. Performance differences between sequencing strategies were less pronounced on M3Exam, likely due to increased task complexity.

## Method Summary
The study used zero-shot prompting with chain-of-thought reasoning across three commercial LLMs (GPT-4o, Gemini-1.5 Flash, Claude-3-Haiku) on two multi-modal datasets (M3Exam and M3COTS). Three prompt configurations were tested: Image First, Text First, and Interleaved. Statistical significance was assessed using McNemar's test and mean rank analysis. Text-only versions of prompts were also tested to isolate the effect of ordering from modality properties.

## Key Results
- Image-first sequencing improved performance on M3COTS (simpler tasks with one image per prompt)
- Image-inline sequencing yielded better results on M3Exam (complex tasks with multiple images)
- Text-only versions of prompts mirrored sequencing effects, indicating instruction order drives performance differences
- Prompt priming to prioritize specific modalities did not improve performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The order of modality presentation in prompts affects LLM reasoning performance by influencing attention distribution across modalities.
- Mechanism: In early fusion architectures, positional encoding causes earlier modalities to receive disproportionately higher attention weights during self-attention. This creates a primacy effect where the first modality is more heavily weighted in the reasoning process, potentially underutilizing later modalities.
- Core assumption: The study's findings that image-first sequencing improved performance on M3COTS and image-inline improved performance on M3Exam are primarily due to attention distribution effects rather than other factors.
- Evidence anchors:
  - [abstract] "Our results demonstrate that the order in which modalities are presented can significantly affect performance"
  - [section] "The study suggests that attention mechanisms in transformer-based LLMs likely influences modality bias which affects the reasoning performance based on the sequencing of modalities in prompts."
  - [corpus] "Weak - corpus mentions 'reasoning' and 'attention' but no direct connection to modality ordering effects"

### Mechanism 2
- Claim: Information ordering within prompts affects LLM performance more than the inherent properties of different modalities.
- Mechanism: Transformer architectures use positional encoding to maintain sequence order, and the model's attention mechanism weights tokens based on their positions. When images are converted to text and presented in different orders, the performance patterns remain similar to when the actual images are used, indicating that the sequence itself drives the effect rather than modality-specific processing.
- Core assumption: The study's finding that text-only versions of prompts mirrored the sequencing effects observed with images demonstrates that ordering, not modality properties, is the key factor.
- Evidence anchors:
  - [abstract] "Text-only versions of prompts mirrored these sequencing effects, indicating that instruction order—rather than modality properties—plays a critical role"
  - [section] "Our experiments revealed that the sequence in which information is presented significantly influences LLM performance, outweighing the inherent properties of the modalities themselves"
  - [corpus] "Weak - corpus discusses multimodal reasoning but doesn't directly address ordering vs modality properties"

### Mechanism 3
- Claim: The effectiveness of modality sequencing depends on task complexity and the logical flow required by the reasoning task.
- Mechanism: For simpler tasks with single images (M3COTS), models can leverage the primacy effect of early modality presentation. However, for complex tasks with multiple images (M3Exam), the increased cognitive load and reasoning demands reduce the impact of modality ordering as the model must integrate multiple information sources regardless of sequence.
- Core assumption: The study's observation that performance differences between sequencing strategies were less pronounced on M3Exam compared to M3COTS is primarily due to task complexity effects on modality ordering sensitivity.
- Evidence anchors:
  - [abstract] "However, in more complex tasks involving multiple images and intricate reasoning steps, the effect of sequencing diminished, likely due to the increased cognitive demands of the task"
  - [section] "For complex tasks that involved numerous image inputs, the high reasoning demands appeared to reduce the impact of modality ordering within the prompts"
  - [corpus] "Weak - corpus mentions task complexity but doesn't specifically connect to modality sequencing effects"

## Foundational Learning

- Concept: Transformer architecture and self-attention mechanisms
  - Why needed here: Understanding how transformers process sequences and allocate attention is crucial for interpreting why modality ordering affects performance
  - Quick check question: How does positional encoding in transformers influence the attention weights assigned to different tokens in a sequence?

- Concept: Multimodal fusion strategies (early, late, hybrid)
  - Why needed here: The study infers different fusion strategies across models based on their sensitivity to modality ordering, requiring understanding of how these strategies process information differently
  - Quick check question: What are the key differences between early fusion, late fusion, and hybrid fusion approaches in multimodal models?

- Concept: Chain-of-thought reasoning and multi-hop reasoning
  - Why needed here: The study discusses how models handle complex reasoning tasks that require maintaining context across multiple steps, which is relevant to understanding limitations in modality processing
  - Quick check question: What is the difference between single-step reasoning and multi-hop reasoning in the context of language models?

## Architecture Onboarding

- Component map: Prompt construction -> API call with specific modality ordering -> Model response generation -> Answer extraction and evaluation -> Statistical analysis of results across different configurations and datasets
- Critical path: Prompt construction → API call with specific modality ordering → Model response generation → Answer extraction and evaluation → Statistical analysis of results across different configurations and datasets
- Design tradeoffs: Early fusion provides better cross-modal integration but is more sensitive to ordering; late fusion is more robust to ordering but may miss nuanced cross-modal relationships; complex tasks may overwhelm ordering effects but also require more sophisticated reasoning capabilities
- Failure signatures: If modality ordering has minimal effect across all datasets, it may indicate the models use late fusion; if performance is consistently poor regardless of ordering, it may indicate the models lack sufficient reasoning capabilities for the task complexity
- First 3 experiments:
  1. Test simple image-text ordering (image first vs text first) on a small subset of M3COTS to establish baseline sensitivity
  2. Convert images to text and repeat the ordering experiment to verify the ordering effect is independent of modality properties
  3. Test different fusion strategy assumptions by analyzing performance patterns across datasets with varying complexity levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific positional encoding strategies affect the integration of multiple images in complex reasoning tasks?
- Basis in paper: [explicit] The study discusses that in early fusion architectures, positional encoding causes earlier modalities to receive disproportionately higher attention, potentially underutilizing later modalities and hindering effective multi-modal integration in complex reasoning tasks.
- Why unresolved: The study does not test different positional encoding strategies or provide quantitative evidence on how they impact multi-image reasoning tasks.
- What evidence would resolve it: Experiments comparing different positional encoding methods (e.g., sinusoidal vs learned embeddings) across varying numbers of images in reasoning tasks.

### Open Question 2
- Question: What are the relative contributions of image and text modalities in multi-hop reasoning tasks, and how does this vary across different question types?
- Basis in paper: [inferred] The study mentions that LLMs often struggle with multi-hop reasoning tasks, particularly when they need to revisit earlier information to select the correct option. It also shows that certain question types are more sensitive to modality sequencing than others.
- Why unresolved: The study does not explicitly quantify the relative contributions of image and text modalities in multi-hop reasoning tasks or provide a detailed analysis of how this varies across different question types.
- What evidence would resolve it: Ablation studies that systematically remove image or text information from multi-hop reasoning tasks and measure the impact on performance across different question types.

### Open Question 3
- Question: How do different fusion strategies (early, late, hybrid) affect the sensitivity of LLMs to modality sequencing in prompts?
- Basis in paper: [explicit] The study hypothesizes that the impact of modality sequencing on model performance will vary depending on the unknown fusion strategy employed by the underlying LLMs. It suggests that early fusion models should be more sensitive to image sequencing, while late fusion models should show minimal sensitivity.
- Why unresolved: The study does not have access to the internal fusion strategies of the commercial LLMs used and cannot directly test how different fusion strategies affect modality sequencing sensitivity.
- What evidence would resolve it: Comparative experiments using open-source LLMs with known fusion strategies, testing how different fusion strategies affect modality sequencing sensitivity in various reasoning tasks.

## Limitations

- The study's findings on modality sequencing effects are primarily correlational rather than causal, as the research does not directly measure attention weights or internal model mechanisms.
- The study focuses on three commercial models, limiting generalizability to other architectures or open-source models.
- The datasets used (M3Exam and M3COTS) represent specific domains that may not capture the full range of multimodal reasoning tasks where these effects could manifest differently.

## Confidence

**High Confidence**: The core finding that modality sequencing affects LLM reasoning performance is well-supported by the systematic experiments across multiple models and datasets. The replication of sequencing effects with text-only prompts provides strong evidence that ordering, rather than modality properties, drives the observed differences.

**Medium Confidence**: The inference about attention mechanisms and fusion strategies underlying the sequencing effects is reasonable but indirect. While the patterns are consistent with these mechanisms, alternative explanations (such as training data biases or instruction following capabilities) cannot be ruled out without deeper architectural analysis.

**Low Confidence**: The claim that task complexity specifically reduces sensitivity to modality ordering is tentative, as the study compares fundamentally different datasets rather than systematically varying complexity within a single benchmark.

## Next Checks

1. **Attention Pattern Analysis**: Use attention visualization tools to directly measure how positional encoding affects attention weights across different modality sequences in the same model. This would validate whether the observed performance patterns correlate with actual attention distribution changes.

2. **Controlled Complexity Variation**: Design experiments that systematically vary task complexity while holding other factors constant, using a single dataset with controlled difficulty levels. This would isolate whether complexity specifically modulates modality sequencing effects.

3. **Architecture-Specific Testing**: Test the sequencing effects across models with known different fusion strategies (early, late, hybrid) to verify whether the observed patterns align with the predicted effects of each fusion approach on modality ordering sensitivity.