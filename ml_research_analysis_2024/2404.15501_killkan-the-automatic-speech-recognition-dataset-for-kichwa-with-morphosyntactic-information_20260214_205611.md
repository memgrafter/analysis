---
ver: rpa2
title: 'Killkan: The Automatic Speech Recognition Dataset for Kichwa with Morphosyntactic
  Information'
arxiv_id: '2404.15501'
source_url: https://arxiv.org/abs/2404.15501
tags:
- kichwa
- dataset
- language
- spanish
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Killkan, the first dataset for automatic
  speech recognition (ASR) in the Kichwa language, an endangered indigenous language
  of Ecuador. The dataset contains approximately 4 hours of audio with transcription,
  Spanish translation, and morphosyntactic annotation in Universal Dependencies format.
---

# Killkan: The Automatic Speech Recognition Dataset for Kichwa with Morphosyntactic Information

## Quick Facts
- arXiv ID: 2404.15501
- Source URL: https://arxiv.org/abs/2404.15501
- Reference count: 0
- First dataset for automatic speech recognition in the Kichwa language, containing approximately 4 hours of audio with transcription, Spanish translation, and morphosyntactic annotation

## Executive Summary
This paper introduces Killkan, the first dataset for automatic speech recognition (ASR) in the Kichwa language, an endangered indigenous language of Ecuador. The dataset contains approximately 4 hours of audio with transcription, Spanish translation, and morphosyntactic annotation in Universal Dependencies format. The audio data was retrieved from a publicly available radio program in Kichwa. The authors trained an ASR model on this dataset by fine-tuning the pretrained wav2vec2-xlsr-53 model. The experiments show that the model achieved a Character Error Rate (CER) of 2.04%, which is comparable to ASR models fine-tuned on high-resource languages. The dataset, ASR model, and code are publicly available, showcasing resource building and applications for low-resource languages.

## Method Summary
The authors collected Kichwa speech data from publicly available radio programs, transcribing the audio and providing Spanish translations. They manually annotated the text with morphosyntactic information following the Universal Dependencies format. To create an ASR system, they fine-tuned the wav2vec2-xlsr-53 model, which is pretrained on 53 languages, using their Kichwa dataset. The fine-tuning process adapted the model to recognize Kichwa speech patterns and produce accurate transcriptions. The resulting model achieved a Character Error Rate of 2.04% on the test set.

## Key Results
- Introduced the first ASR dataset for Kichwa with approximately 4 hours of audio data
- Achieved a Character Error Rate (CER) of 2.04% on the test set
- Demonstrated that wav2vec2-xlsr-53 fine-tuning can produce competitive results for low-resource languages
- Provided a comprehensive dataset including audio, transcriptions, Spanish translations, and morphosyntactic annotations

## Why This Works (Mechanism)
The success of the Killkan ASR model stems from leveraging the wav2vec2-xlsr-53 architecture, which has been pretrained on a diverse set of 53 languages. This pretraining allows the model to capture general speech patterns and acoustic features that transfer well to Kichwa. Fine-tuning this pretrained model on the Killkan dataset enables it to adapt to the specific phonetic and phonological characteristics of Kichwa while retaining the robust feature extraction capabilities learned from the broader multilingual pretraining. The Universal Dependencies format for morphosyntactic annotation provides a standardized framework that facilitates linguistic analysis and potential downstream applications.

## Foundational Learning

**wav2vec2-xlsr-53 Model**
*Why needed:* This model provides a strong foundation for ASR in low-resource languages through multilingual pretraining
*Quick check:* Verify the model's performance on other low-resource languages to assess generalizability

**Universal Dependencies Format**
*Why needed:* Standardizes morphosyntactic annotation across languages for consistent linguistic analysis
*Quick check:* Ensure all annotations follow UD guidelines and are internally consistent

**Character Error Rate (CER)**
*Why needed:* Measures the accuracy of character-level predictions in ASR systems
*Quick check:* Compare CER results with other ASR systems on similar datasets

**Fine-tuning Process**
*Why needed:* Adapts a pretrained model to a specific language while preserving learned features
*Quick check:* Monitor validation loss during fine-tuning to prevent overfitting

## Architecture Onboarding

**Component Map**
Kichwa audio data -> wav2vec2-xlsr-53 model -> Fine-tuning -> ASR model -> CER evaluation

**Critical Path**
The critical path involves processing raw audio through the wav2vec2-xlsr-53 architecture, fine-tuning on the Kichwa dataset, and evaluating performance using CER. The success of this path depends on the quality and quantity of the training data, the effectiveness of the fine-tuning process, and the appropriateness of the evaluation metric.

**Design Tradeoffs**
The choice to use wav2vec2-xlsr-53 balances the need for strong baseline performance with the constraints of a low-resource language. While more specialized architectures might yield better results, they would require significantly more training data. The decision to include morphosyntactic annotation adds value for linguistic analysis but increases the annotation burden.

**Failure Signatures**
Potential failure modes include overfitting due to the small dataset size, poor generalization to different Kichwa dialects or speaking styles, and limitations in handling code-switching or loanwords. The CER metric may not fully capture errors in morphosyntactic annotation or semantic understanding.

**First Experiments**
1. Evaluate model performance on held-out test set using CER
2. Conduct ablation study by fine-tuning on subsets of the training data
3. Test model robustness by evaluating on audio from different speakers or recording conditions

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset size of approximately 4 hours is relatively small for comprehensive ASR training
- Data source from radio programs may introduce domain-specific biases
- Morphosyntactic annotation quality depends on manual annotation, which may introduce human error
- Limited evaluation to CER metric, lacking assessment of morphosyntactic annotation accuracy

## Confidence
- Dataset novelty and public availability: High
- CER result of 2.04%: Medium
- Comparability to high-resource language models: Low

## Next Checks
1. Evaluate the model on diverse Kichwa speech samples beyond radio broadcasts to assess domain robustness
2. Conduct human evaluation of the morphosyntactic annotations to verify annotation quality and consistency
3. Test the model's performance across different Kichwa dialects and speaker demographics to identify potential bias or coverage gaps