---
ver: rpa2
title: Toward the Evaluation of Large Language Models Considering Score Variance across
  Instruction Templates
arxiv_id: '2408.12263'
source_url: https://arxiv.org/abs/2408.12263
tags:
- evaluation
- templates
- llms
- language
- constrained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the issue of unfair evaluation of large language
  models (LLMs) in natural language understanding (NLU) tasks due to variance in performance
  caused by different instruction templates. To solve this, the authors create cross-lingual
  benchmark datasets (English and Japanese) with multiple instruction templates for
  each task, and propose a new evaluation metric called the Sharpe score that accounts
  for variance across templates.
---

# Toward the Evaluation of Large Language Models Considering Score Variance across Instruction Templates

## Quick Facts
- arXiv ID: 2408.12263
- Source URL: https://arxiv.org/abs/2408.12263
- Reference count: 40
- Key outcome: This paper addresses the issue of unfair evaluation of large language models (LLMs) in natural language understanding (NLU) tasks due to variance in performance caused by different instruction templates.

## Executive Summary
This paper addresses a critical gap in LLM evaluation by highlighting how different instruction templates can lead to significant variance in performance across NLU tasks. The authors propose a new evaluation framework that uses multiple instruction templates and introduces the Sharpe score metric to account for this variance, inspired by the financial Sharpe ratio. Through experiments with English and Japanese datasets, they demonstrate that using multiple templates provides a fairer assessment of LLM capabilities and that the Sharpe score effectively captures both accuracy and robustness of model outputs.

## Method Summary
The authors create cross-lingual benchmark datasets (English and Japanese) with multiple instruction templates for each NLU task, including CoLA, STS-B, MNLI, SQuAD, and CommonsenseQA. They propose the Sharpe score metric, which measures both accuracy and robustness by accounting for variance across templates. The evaluation employs constrained decoding with regular expressions to ensure consistent output formats. Experiments are conducted in both zero-shot and fine-tuning settings to assess model performance and the impact of continuous training on cross-lingual transfer capabilities.

## Key Results
- Using multiple instruction templates significantly reduces unfair evaluation bias in LLMs
- The Sharpe score effectively measures both accuracy and robustness by accounting for template variance
- Constrained decoding with regular expressions improves zero-shot evaluation by ensuring consistent output formats

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using multiple instruction templates reduces unfair evaluation bias in LLMs.
- Mechanism: Different instruction templates lead to variance in LLM performance. By using multiple templates, the evaluation captures a more comprehensive and fair assessment of the model's capabilities.
- Core assumption: The variance in performance across different templates is significant enough to affect evaluation outcomes.
- Evidence anchors:
  - [abstract] "The existing evaluation methods, however, do not take into account the variance in scores due to differences in prompts, which leads to unfair evaluation and comparison of NLU performance."
  - [section] "We demonstrated its effectiveness for the evaluation of template-based NLU capability, as well as for analysis of the NLU performance of multiple LLMs in various experimental scenarios, such as zero-shot versus fine-tuning settings and English versus Japanese settings."
  - [corpus] Weak evidence; the related papers do not directly address the variance in performance due to instruction templates.
- Break condition: If the variance in performance across different templates is minimal, using multiple templates would not significantly improve evaluation fairness.

### Mechanism 2
- Claim: The Sharpe score effectively measures both accuracy and robustness of LLM outputs by accounting for variance across templates.
- Mechanism: The Sharpe score is derived from the Sharpe ratio used in finance, incorporating both the mean performance and the standard deviation of performance across different templates. This provides a measure of both accuracy and robustness.
- Core assumption: The variance in performance across templates is a valid indicator of the model's robustness and generalization ability.
- Evidence anchors:
  - [abstract] "Furthermore, we propose the Sharpe score as an evaluation metric that takes into account the variance in scores between templates."
  - [section] "We define the Sharpe score as follows: Sharpe score = µscore / (ασscore + 1), where α is a parameter that controls the impact of variance in scores among templates."
  - [corpus] Weak evidence; the related papers do not directly address the use of Sharpe ratio or similar metrics for evaluating LLM performance.
- Break condition: If the variance in performance across templates is not correlated with the model's actual robustness or generalization ability, the Sharpe score would not be a reliable metric.

### Mechanism 3
- Claim: Constrained decoding with regular expressions improves zero-shot evaluation by ensuring the generated text follows the expected answer format.
- Mechanism: Regular expressions are used to constrain the output format during generation. This eliminates errors due to differences in output formats and ensures that the generated text can be directly used for evaluation.
- Core assumption: The use of regular expressions in constrained decoding effectively guides the model to produce outputs in the desired format.
- Evidence anchors:
  - [abstract] "Furthermore, we proposed a new evaluation metric, the Sharpe score, which accounts for the variance in LLM outputs due to template differences, inspired by the Sharpe ratio (Sharpe, 1966) used in finance to assess investment efficiency."
  - [section] "We accompanied each instruction template with a regular expression of the expected output for each task. The regular expressions are employed in constrained decoding methods as implemented in Outlines (Willard and Louf, 2023)."
  - [corpus] Weak evidence; the related papers do not directly address the use of constrained decoding with regular expressions for LLM evaluation.
- Break condition: If the model cannot effectively follow the constraints imposed by the regular expressions, the use of constrained decoding would not improve the evaluation process.

## Foundational Learning

- Concept: Natural Language Understanding (NLU) tasks
  - Why needed here: The paper focuses on evaluating the NLU performance of LLMs across various tasks.
  - Quick check question: What are some examples of NLU tasks used in the evaluation?

- Concept: Instruction tuning
  - Why needed here: The paper discusses the impact of instruction tuning on the performance of LLMs in NLU tasks.
  - Quick check question: How does instruction tuning aim to improve the performance of LLMs in NLU tasks?

- Concept: Cross-lingual transfer
  - Why needed here: The paper evaluates the cross-lingual transfer capability and performance of multilingual LLMs.
  - Quick check question: What is cross-lingual transfer, and why is it important for evaluating multilingual LLMs?

## Architecture Onboarding

- Component map:
  - Benchmark datasets with multiple instruction templates for each NLU task
  - Sharpe score metric for evaluating performance considering variance across templates
  - Regular expressions for constraining output format
  - Evaluation scripts and code for running experiments

- Critical path:
  1. Create benchmark datasets with multiple instruction templates for each NLU task.
  2. Implement the Sharpe score metric for evaluating performance considering variance across templates.
  3. Use regular expressions to constrain the output format during generation.
  4. Run experiments using the created datasets and evaluate the performance using the Sharpe score.
  5. Analyze the results and draw conclusions about the effectiveness of the proposed evaluation method.

- Design tradeoffs:
  - Using multiple instruction templates increases the complexity of the evaluation process but provides a more comprehensive and fair assessment of the model's capabilities.
  - Implementing the Sharpe score metric requires additional computation but provides a more robust evaluation of the model's performance.
  - Using regular expressions for constrained decoding may limit the model's ability to generate creative or unexpected outputs but ensures that the generated text follows the expected answer format.

- Failure signatures:
  - If the variance in performance across different templates is minimal, the Sharpe score may not provide a significant improvement over traditional evaluation metrics.
  - If the model cannot effectively follow the constraints imposed by the regular expressions, the use of constrained decoding may not improve the evaluation process.
  - If the benchmark datasets are not representative of the target NLU tasks, the evaluation results may not accurately reflect the model's capabilities.

- First 3 experiments:
  1. Evaluate the performance of a pre-trained LLM on a single NLU task using a single instruction template and compare it to the performance using multiple instruction templates.
  2. Implement the Sharpe score metric and evaluate its effectiveness in measuring both accuracy and robustness of LLM outputs compared to traditional evaluation metrics.
  3. Use regular expressions for constrained decoding and evaluate its impact on the performance of LLMs in zero-shot and fine-tuning settings.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Sharpe score parameter α affect the ranking of models in different tasks, and what does this variance reveal about model stability?
- Basis in paper: Explicit. The paper discusses the Sharpe score and its relationship to variance among templates, showing how rankings change with different α values.
- Why unresolved: The paper provides examples of how rankings change with α, but does not offer a comprehensive analysis of why certain tasks show more variance than others or how this relates to model architecture or training data.
- What evidence would resolve it: A detailed study comparing model architectures and training data across tasks with high and low variance, analyzing the correlation between model characteristics and Sharpe score stability.

### Open Question 2
- Question: What are the specific reasons for the poor performance of LLMs in linguistic acceptability tasks (JCoLA and CoLA) in the zero-shot setting, and how can this be improved?
- Basis in paper: Explicit. The paper notes that LLMs struggle with linguistic acceptability judgment in the zero-shot setting, suggesting a potential mismatch between linguistic expert annotations and general domain training data.
- Why unresolved: The paper does not explore the specific linguistic features that LLMs struggle with or propose concrete solutions to bridge the gap between linguistic expert knowledge and LLM training data.
- What evidence would resolve it: A detailed linguistic analysis of the JCoLA and CoLA datasets, identifying the specific linguistic phenomena that LLMs find challenging, followed by targeted fine-tuning or instruction tuning on these phenomena.

### Open Question 3
- Question: How does continuous training on Japanese data affect the cross-lingual transfer capabilities of English LLMs, and what are the mechanisms behind this effect?
- Basis in paper: Explicit. The paper discusses the impact of continuous training on Japanese data, showing both improvements and declines in performance across different tasks.
- Why unresolved: The paper does not provide a detailed analysis of the mechanisms behind the observed effects, such as how continuous training influences the model's internal representations or attention patterns.
- What evidence would resolve it: A study using techniques like probing tasks, attention visualization, or representation analysis to understand how continuous training on Japanese data modifies the model's internal representations and affects cross-lingual transfer.

### Open Question 4
- Question: What is the optimal number of instruction templates needed for fair evaluation of LLMs, and how does this number vary across different tasks and model sizes?
- Basis in paper: Explicit. The paper emphasizes the importance of using multiple templates for evaluation but does not provide a specific recommendation for the optimal number.
- Why unresolved: The paper does not explore the relationship between the number of templates and evaluation fairness, nor does it investigate how this relationship might differ across tasks and model sizes.
- What evidence would resolve it: A systematic study varying the number of templates used for evaluation across different tasks and model sizes, analyzing the impact on evaluation fairness and identifying the point of diminishing returns.

### Open Question 5
- Question: How does the use of constrained decoding with regular expressions impact the evaluation of LLMs in the fine-tuning setting, and what are the trade-offs between constrained and greedy decoding?
- Basis in paper: Explicit. The paper discusses the use of constrained decoding for zero-shot evaluation but notes a shift in effectiveness in the fine-tuning setting.
- Why unresolved: The paper does not provide a detailed analysis of why constrained decoding becomes less effective in the fine-tuning setting or explore the trade-offs between constrained and greedy decoding in terms of evaluation accuracy and efficiency.
- What evidence would resolve it: A comprehensive comparison of constrained and greedy decoding across different tasks and model sizes in both zero-shot and fine-tuning settings, analyzing the impact on evaluation accuracy, efficiency, and the types of errors each method produces.

## Limitations
- Limited to two languages (English and Japanese), restricting generalizability to other language families
- Focus on a relatively small set of NLU tasks may not capture all evaluation scenarios
- Some implementation details are not fully specified in the paper

## Confidence
- High: Using multiple instruction templates significantly reduces unfair evaluation bias in LLMs
- Medium: The Sharpe score effectively measures both accuracy and robustness by accounting for template variance
- Medium: Constrained decoding with regular expressions improves zero-shot evaluation by ensuring consistent output formats

## Next Checks
1. Test the Sharpe score metric on a broader range of NLU tasks and model architectures to verify its general applicability and robustness.
2. Evaluate the effectiveness of constrained decoding with regular expressions across different language families and model sizes to assess its general utility.
3. Investigate the correlation between Sharpe score variance and other measures of model robustness to validate its effectiveness as a robustness metric.