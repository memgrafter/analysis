---
ver: rpa2
title: The Comparison of Translationese in Machine Translation and Human Transation
  in terms of Translation Relations
arxiv_id: '2404.08661'
source_url: https://arxiv.org/abs/2404.08661
tags:
- translation
- data
- machine
- translationese
- relations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines differences between neural machine translation
  (NMT) and human translation (HT) using translation relations as a metric. The research
  compares how NMT and HT differ in overall translation relations, non-literal translation
  technique usage, and factors influencing specific non-literal techniques.
---

# The Comparison of Translationese in Machine Translation and Human Transation in terms of Translation Relations

## Quick Facts
- arXiv ID: 2404.08661
- Source URL: https://arxiv.org/abs/2404.08661
- Reference count: 40
- Primary result: NMT relies on literal translation 17.41% more than HT across all genres

## Executive Summary
This study examines differences between neural machine translation (NMT) and human translation (HT) using translation relations as a metric. The research compares how NMT and HT differ in overall translation relations, non-literal translation technique usage, and factors influencing specific non-literal techniques. Using parallel corpora of 9 genres, translation relations were manually annotated for aligned token pairs. Results show NMT relies on literal translation 17.41% more than HT across all genres. While NMT performs comparably to HT in syntactic non-literal techniques like lexical shift and transposition, it falls significantly behind in semantic techniques like particularization and generalization.

## Method Summary
The study prepared parallel corpora with source English texts and target Chinese translations (NMT from Google Translate, HT from existing corpus), then tokenized and aligned tokens using automatic alignment followed by manual verification. Translation relations were manually annotated on aligned token pairs using YAWAT tool following established guidelines. Statistical comparisons were then made between MT and HT corpora to quantify differences in translation relation distributions, with particular focus on non-literal translation techniques and genre-specific variations.

## Key Results
- NMT relies on literal translation 17.41% more than HT across all genres
- NMT performs comparably to HT in syntactic non-literal techniques (lexical shift, transposition) but falls behind in semantic techniques (particularization, generalization, figuration)
- Genre significantly influences translationese differences, with subtitles and spoken genres showing the largest non-literal usage gaps (97.8% and 52.0% respectively)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translation relations provide a measurable metric to compare MT and HT outputs at the token level.
- Mechanism: The paper manually annotates aligned token pairs in English-Chinese corpora with 14 translation relation categories, then computes the frequency and type distribution to quantify differences.
- Core assumption: Translation relation categories are exhaustive and mutually exclusive for identifying translationese.
- Evidence anchors:
  - [abstract]: "Translation relations are manually annotated for aligned token pairs with one side as the English token group and the other side as its corresponding translated Chinese token group."
  - [section 3.2.1]: Detailed definition of 14 translation relations including literal, equivalence, non-literal, and unaligned techniques.
  - [corpus]: Corpus contains 10,916 source tokens, 9,314 MT-aligned pairs, and 9,235 HT-aligned pairs; counts per technique vary widely.
- Break condition: If translation relation categories overlap or omit key translation phenomena, the metric will misclassify or miss translationese.

### Mechanism 2
- Claim: Non-literal translation techniques can be mapped to syntactic vs semantic linguistic features.
- Mechanism: The paper clusters translation relations into syntax-related (lexical shift, transposition, unaligned-reduction/explicitation) and semantics-related (equivalence, generalization, modulation, particularization, figurative) categories, then measures performance gaps.
- Core assumption: Syntactic features are easier for MT to capture than semantic features requiring context understanding.
- Evidence anchors:
  - [abstract]: "NMT performs comparably to HT in employing syntactic non-literal translation techniques, it falls behind in semantic-level performance."
  - [section 4.2]: Breaks down each technique into sub-categories and compares MT vs HT usage frequencies.
  - [corpus]: Part-of-speech and dependency analysis shows MT better at syntactic transpositions but worse at semantic generalization.
- Break condition: If syntactic vs semantic distinction is artificial or techniques span both domains, the comparison will conflate effects.

### Mechanism 3
- Claim: Genre influences the degree of translationese in MT vs HT.
- Mechanism: The paper computes translation relation distributions per genre and finds disparities, especially in genres with complex syntax or figurative language.
- Core assumption: Genre-specific linguistic properties interact differently with MT and HT translation strategies.
- Evidence anchors:
  - [section 4.1.2]: Shows subtitle and spoken genres have largest non-literal usage gaps (97.8% and 52.0% respectively).
  - [corpus]: Subtitle corpus has 20.9% literal translation in MT vs 41.4% in HT; scientific article has 32.8% vs 59.4%.
  - [section 4.1.1]: Edit distance of translation relations is highest for subtitles, indicating greater divergence.
- Break condition: If genre classification is coarse or sample size per genre is small, genre-based conclusions may be unreliable.

## Foundational Learning

- Concept: Translationese definition and features
  - Why needed here: Establishes the phenomenon being measured and compared between MT and HT.
  - Quick check question: What linguistic traits characterize translationese, and how do they differ from native text?

- Concept: Translation relations taxonomy
  - Why needed here: Provides the framework for annotation and comparison.
  - Quick check question: How many translation relation categories are defined, and what linguistic aspects do they cover?

- Concept: Token alignment and annotation process
  - Why needed here: Explains how parallel corpora are prepared for relation-based comparison.
  - Quick check question: What tool is used for token alignment, and how are translation relations labeled on aligned pairs?

## Architecture Onboarding

- Component map: Source texts -> Tokenization -> MT generation (Google Translate) -> Human translation -> Alignment -> Manual annotation -> Analysis
- Critical path:
  1. Prepare parallel corpora (English source, MT target, HT target)
  2. Tokenize and align source-target pairs
  3. Annotate each aligned pair with translation relations
  4. Aggregate counts and compute metrics per technique and genre
  5. Compare MT vs HT distributions and infer performance gaps
- Design tradeoffs:
  - Manual annotation vs automatic labeling: Higher accuracy but slower
  - Token-level vs sentence-level analysis: More granular but noisier
  - Fixed taxonomy vs flexible categories: Consistency vs coverage
- Failure signatures:
  - Imbalanced category counts (e.g., >80% literal) suggest annotation or metric issues
  - High variance in edit distance across genres indicates inconsistent annotation
  - MT and HT distributions overlapping entirely means the metric cannot distinguish systems
- First 3 experiments:
  1. Re-run annotation on a small subset and measure inter-annotator agreement to validate consistency
  2. Compute translation relation distributions on a held-out test set to check reproducibility
  3. Perform ablation by removing syntactic vs semantic techniques separately to see which drives performance gaps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific linguistic factors cause NMT systems to underperform human translators in semantic-level translation techniques like particularization, figuration, and generalization?
- Basis in paper: [explicit] The paper explicitly states that NMT falls behind HT in semantic-level performance, particularly in techniques like particularization, figuration, and generalization, and mentions the need for contextual information in these techniques.
- Why unresolved: While the paper identifies these techniques as areas where NMT underperforms, it does not provide a detailed analysis of the specific linguistic factors that cause this underperformance. The paper mentions that contextual information is needed but does not delve into what specific linguistic elements are missing or inadequately captured by NMT systems.
- What evidence would resolve it: A detailed linguistic analysis comparing the contextual information processing capabilities of NMT and HT, including specific examples of how contextual elements are handled differently by each system, would provide insights into the linguistic factors causing the underperformance.

### Open Question 2
- Question: How does the performance of NMT systems vary across different genres, and what are the underlying reasons for these variations?
- Basis in paper: [explicit] The paper compares NMT and HT performance across nine genres and notes variations in the use of non-literal translation techniques, with some genres showing larger discrepancies than others.
- Why unresolved: Although the paper identifies that there are variations in NMT performance across genres, it does not explore the underlying reasons for these differences. Understanding why certain genres pose more challenges for NMT could lead to targeted improvements in translation systems.
- What evidence would resolve it: A comprehensive study analyzing the linguistic and structural characteristics of each genre, combined with an assessment of how NMT systems handle these characteristics compared to HT, would reveal the reasons for genre-specific performance variations.

### Open Question 3
- Question: Can the integration of specific linguistic knowledge, such as semantic and syntactic features, into NMT systems significantly improve their performance in non-literal translation techniques?
- Basis in paper: [explicit] The paper discusses the potential for optimizing NMT by reducing translationese through the application of translation relations and mentions the need for integrating linguistics-related knowledge.
- Why unresolved: The paper suggests that integrating linguistic knowledge could improve NMT performance but does not provide empirical evidence or a detailed methodology for how this integration should be implemented or its potential impact.
- What evidence would resolve it: Experimental studies that implement specific linguistic features (e.g., semantic networks, syntactic parsers) into NMT systems and measure the resulting improvements in non-literal translation techniques would provide concrete evidence of the benefits of such integration.

## Limitations
- Manual annotation introduces potential subjectivity and inter-annotator variability
- Analysis focuses on English-Chinese translation, limiting generalizability to other language pairs
- Corpus size may not capture rare translation phenomena across all genres

## Confidence
**High Confidence**: The finding that NMT uses literal translation 17.41% more than HT is supported by robust statistical analysis across all genres and is unlikely to be affected by annotation subjectivity.

**Medium Confidence**: The conclusion that NMT performs comparably to HT in syntactic non-literal techniques but falls behind in semantic techniques is based on reasonable linguistic assumptions but could be influenced by how techniques are categorized and annotated.

**Medium Confidence**: Genre-specific differences, particularly the large gaps in subtitles and spoken genres, are observed but may be affected by smaller sample sizes in these categories and potential genre classification ambiguities.

## Next Checks
1. **Inter-annotator Agreement Analysis**: Conduct a formal inter-annotator agreement study on a subset of the corpus (at least 200 aligned pairs) using Cohen's kappa to quantify annotation consistency and identify categories with the most disagreement.

2. **Cross-linguistic Validation**: Replicate the analysis on an English-Spanish parallel corpus to test whether the observed MT vs HT differences in syntactic vs semantic techniques are language-pair specific or represent a more general phenomenon.

3. **Temporal MT Performance Check**: Compare the current findings with a replication study using a more recent NMT system (e.g., transformer-based models from 2022-2023) to determine if improvements in contextual modeling have reduced the semantic technique gap.