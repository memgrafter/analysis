---
ver: rpa2
title: 'On Transfer in Classification: How Well do Subsets of Classes Generalize?'
arxiv_id: '2403.03569'
source_url: https://arxiv.org/abs/2403.03569
tags:
- classes
- pairs
- number
- learning
- separated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a theoretical framework for understanding
  transferability between sets of classes in classification. It establishes a partially
  ordered set of subsets of classes, where each model is associated with a specific
  pair of classes and separates other pairs accordingly.
---

# On Transfer in Classification: How Well do Subsets of Classes Generalize?

## Quick Facts
- arXiv ID: 2403.03569
- Source URL: https://arxiv.org/abs/2403.03569
- Reference count: 40
- This work introduces a theoretical framework for understanding transferability between sets of classes in classification.

## Executive Summary
This paper introduces a theoretical framework for analyzing transferability between subsets of classes in classification tasks. The authors establish a partial order on models based on the sets of class pairs they can separate, allowing them to predict which subsets of classes can generalize to others. Through experiments on various datasets and learning scenarios including fine-tuning, training from scratch, and few-shot learning, they demonstrate that separability achieved by classes before training is a strong indicator of transferability potential.

## Method Summary
The method involves computing separability metrics for pairs of classes using pretrained feature extractors (ResNet-50, Vision Transformer). Linear classifiers are trained on frozen features to determine which class pairs can be separated. Separability is measured by counting how many novel pairs can be separated with error below a threshold ε. This metric is then used to predict transfer performance across different learning scenarios: fine-tuning, training from scratch, and few-shot learning with nearest mean classifier.

## Key Results
- Separability of class pairs before fine-tuning strongly predicts post-fine-tuning performance
- The separability provided by pretrained networks remains relevant for assessing future performance in training from scratch and few-shot learning
- Training from scratch on subsets with high separability yields better generalization than random subsets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separability of class pairs before fine-tuning strongly predicts post-fine-tuning performance
- Mechanism: The model learns to separate pairs of classes in a feature space; this learned separability transfers to new classes because the feature extractor preserves discriminative structure
- Core assumption: Feature extractor trained via self-supervised learning (DINO-ViT) learns generalizable representations that maintain class separability
- Evidence anchors:
  - [abstract]: "the separability achieved by classes before fine-tuning is a strong indicator of the transferability potential during the fine-tuning process"
  - [section]: "we observe a high correlation between class separability before and after finetuning on CIFAR10"
  - [corpus]: Weak. No direct citations; claim is novel.
- Break condition: If the feature extractor changes architecture (e.g., train from scratch), separability becomes a weaker predictor.

### Mechanism 2
- Claim: Fundamental pairs are the minimal set needed to separate all classes
- Mechanism: Models learned on fundamental pairs form a minimal covering set of separators; other pairs can be derived via transitivity
- Core assumption: The order relationship defined by subset containment on separable pairs is a valid partial order
- Evidence anchors:
  - [section]: "Definition 6 (Fundamental pair)...The pairs on which they are learned are called fundamental since they cannot be removed."
  - [section]: "Theorem 1...the cardinality of P equals F(C)"
  - [corpus]: Weak. No external validation of partial order property.
- Break condition: If classes overlap in feature space, the partial order may collapse or not exist.

### Mechanism 3
- Claim: Training from scratch on a subset with high separability yields better generalization than random subsets
- Mechanism: High separability indicates the subset contains diverse, well-separated classes, providing a good initialization for learning other classes
- Core assumption: The network trained from scratch on these classes will learn representations aligned with the pretrained feature extractor's structure
- Evidence anchors:
  - [section]: "the separability provided by pretrained networks remains relevant for assessing future performance in training from scratch"
  - [section]: "We note few differences with finetuning" (suggesting separability still useful)
  - [corpus]: Weak. No external study comparing random vs high-separability subsets.
- Break condition: If dataset is too small or classes too similar, separability becomes noisy.

## Foundational Learning

- Concept: Partial order on models by set containment of separable pairs
  - Why needed here: Provides the mathematical foundation to identify which subsets of classes generalize to others
  - Quick check question: If model A separates pairs P and model B separates pairs Q, when is A ≤ B?
- Concept: Hyperplane separability with ε-tolerance
  - Why needed here: Makes the theory applicable to continuous data with noise, not just separable classes
  - Quick check question: How does Perr(H, Ci, Cj) behave as ε increases?
- Concept: Binary encoding of class partitions for minimal hyperplane sets
  - Why needed here: Connects separability theory to coding theory bounds (Kraft-McMillan)
  - Quick check question: How many hyperplanes are needed to separate 8 classes minimally?

## Architecture Onboarding

- Component map:
  - Feature extractor (ResNet50/DINO-ViT-8) → frozen during separability computation
  - Pairwise linear heads (binary classifiers) → one per class pair
  - Separability metric → counts how many novel pairs are separated with error < ε
- Critical path:
  1. Compute features for all classes
  2. Train one binary head per pair on frozen features
  3. Measure separability of new class set
  4. Use separability to predict transfer performance
- Design tradeoffs:
  - Freeze feature extractor → fast separability computation but ignores feature adaptation
  - Use ε tolerance → robust to noise but may overestimate separability
  - Count pairs → interpretable but ignores class imbalance
- Failure signatures:
  - Low correlation between pre- and post-training separability → feature extractor not transferable
  - Separability near maximum for all subsets → metric not discriminative
  - High variance across runs → insufficient samples or unstable training
- First 3 experiments:
  1. Compute separability of all 2-class subsets on CIFAR-10; plot correlation with 2-class fine-tuning accuracy
  2. Identify top-5 most separable 4-class subsets; fine-tune and compare to random 4-class subsets
  3. Repeat experiment on FASHION-MNIST; compare class vs pair separability as predictors

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the separability metric change when using more complex models (e.g., deeper networks, different architectures) compared to linear hyperplanes?
- Basis in paper: [explicit] The paper focuses on linear hyperplanes for simplicity and to capture high-level information from feature extractors. However, the authors mention that their theoretical framework could be extended to include more complex models.
- Why unresolved: The paper does not provide empirical evidence on the impact of using more complex models on separability. It only uses linear hyperplanes as a practical approximation.
- What evidence would resolve it: Conducting experiments with different architectures (e.g., deeper networks, transformers) and comparing their separability metrics would provide insights into the relationship between model complexity and separability.

### Open Question 2
- Question: How does the separability metric generalize to multi-class classification problems beyond pairwise separability?
- Basis in paper: [inferred] The paper focuses on pairwise separability and establishes an order relationship among models based on the pairs they separate. However, it does not explicitly address how this framework extends to multi-class scenarios.
- Why unresolved: The paper's theoretical framework and experiments are primarily based on pairwise separability. The authors mention that their framework could be extended to include subsets of several pairs, but they do not explore this extension.
- What evidence would resolve it: Extending the separability metric to multi-class scenarios and evaluating its performance on datasets with more than two classes would provide insights into the framework's generalizability.

### Open Question 3
- Question: How does the separability metric perform in transfer learning scenarios where the source and target domains have different data distributions?
- Basis in paper: [inferred] The paper explores transfer learning scenarios where the source and target domains have different class sets. However, it does not explicitly address the impact of different data distributions on separability.
- Why unresolved: The paper's experiments focus on transfer learning between datasets with similar data distributions (e.g., CIFAR-10, FASHION-MNIST). It does not investigate scenarios where the source and target domains have significantly different data distributions.
- What evidence would resolve it: Conducting experiments with transfer learning tasks involving source and target domains with different data distributions (e.g., natural images vs. medical images) would provide insights into the robustness of the separability metric in such scenarios.

## Limitations

- The theory assumes the partial order on models is well-defined and stable across training, yet no empirical validation confirms this property holds beyond the studied datasets
- The separability metric depends heavily on the choice of ε tolerance and the frozen feature extractor, both of which may not generalize to architectures with different inductive biases
- The claim that separability from pretrained networks predicts training-from-scratch performance is weakly supported and may not hold for smaller datasets or dissimilar class distributions

## Confidence

- **High**: The correlation between pre- and post-training separability in fine-tuning scenarios is empirically supported and aligns with intuition about feature transferability
- **Medium**: The minimal covering set (fundamental pairs) provides a useful conceptual tool, but its practical impact on subset selection remains underexplored
- **Low**: The claim that separability from pretrained networks predicts training-from-scratch performance is weakly supported and may not hold for smaller datasets or dissimilar class distributions

## Next Checks

1. Test the stability of the partial order on models by measuring separability consistency across multiple training runs with different random seeds
2. Compare separability-based subset selection to random and diversity-based methods on a new dataset (e.g., CIFAR-100) to quantify practical gains
3. Vary ε tolerance systematically and measure its impact on separability predictions to establish robustness to hyperparameter choice