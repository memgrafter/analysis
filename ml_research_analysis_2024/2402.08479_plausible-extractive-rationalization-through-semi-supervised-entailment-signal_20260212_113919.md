---
ver: rpa2
title: Plausible Extractive Rationalization through Semi-Supervised Entailment Signal
arxiv_id: '2402.08479'
source_url: https://arxiv.org/abs/2402.08479
tags:
- arxiv
- task
- rationales
- predictor
- explainer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a semi-supervised approach to improve the plausibility
  and robustness of extracted rationales in Explain-Then-Predict (ETP) models using
  Natural Language Inference (NLI) signals. The core idea is to fine-tune an NLI predictor
  on a small set of supervised rationales (10%) to generate augmented labels, which
  then guide the training of the explainer.
---

# Plausible Extractive Rationalization through Semi-Supervised Entailment Signal

## Quick Facts
- arXiv ID: 2402.08479
- Source URL: https://arxiv.org/abs/2402.08479
- Authors: Wei Jie Yeo; Ranjan Satapathy; Erik Cambbia
- Reference count: 10
- Primary result: Semi-supervised NLI-based rationalization improves plausibility and robustness, outperforming unsupervised methods by >100% in plausibility metrics on ERASER dataset.

## Executive Summary
This paper presents a semi-supervised approach to improve the plausibility and robustness of extracted rationales in Explain-Then-Predict (ETP) models using Natural Language Inference (NLI) signals. The core idea is to fine-tune an NLI predictor on a small set of supervised rationales (10%) to generate augmented labels, which then guide the training of the explainer. By enforcing alignment between the explanation and answer in a question-answering task, the approach improves performance without access to ground truth labels. Evaluated on the ERASER dataset, the method achieves comparable results with supervised models and outperforms unsupervised approaches by >100% in plausibility metrics. It also demonstrates superior robustness to adversarial inputs, with lower normalized performance drops and attack rates.

## Method Summary
The method involves fine-tuning a DeBERTa-large NLI predictor on 10% of annotated rationales to generate entailment/contradiction labels for each sentence. These labels are then transformed into binary rationale masks to train an ETP model with a shared RoBERTa-base encoder, separate explainer and predictor decoders, and cross-entropy loss. At inference, the explainer generates rationales, and the NLI predictor evaluates each rationale sentence against the predicted task label to scale the predictor's output probabilities, improving robustness.

## Key Results
- The approach achieves comparable plausibility scores to fully supervised models on the ERASER dataset.
- Outperforms unsupervised methods by >100% in plausibility metrics (Sentence-F1).
- Demonstrates superior robustness to adversarial prefixes, with lower normalized performance drops and attack rates.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The NLI predictor acts as an auxiliary supervisor by transforming human-annotated rationales into NLI labels (entailment/contradiction/neutral) and then back to binary rationale masks for training the explainer.
- Mechanism: A small set of human-annotated rationales is used to fine-tune the NLI predictor. The NLI predictor then generates sentence-level entailment signals for each input sentence relative to the task label. These signals are converted back into binary masks that serve as training targets for the explainer, which otherwise has no access to ground truth rationales.
- Core assumption: Human-annotated rationales reliably align with NLI relations—supporting rationales correspond to entailment when the label is SUPPORT, and refuting rationales correspond to contradiction when the label is REFUTE.
- Evidence anchors:
  - [abstract] "We adopt a pre-trained natural language inference (NLI) model and further fine-tune it on a small set of supervised rationales (10%). The NLI predictor is leveraged as a source of supervisory signals to the explainer via entailment alignment."
  - [section 2.2] "As the explainer is trained to predict a binary mask, Algorithm 1 can be implemented in reverse to transform the NLI outputs back to rationale labels, ˜z for the explainer’s training."
  - [corpus] Weak evidence; no direct studies cited comparing NLI-based rationalization to pure self-supervised extraction.
- Break condition: If the NLI predictor misclassifies neutral sentences as entailment/contradiction, the explainer will be trained on false positives, leading to poor plausibility and robustness.

### Mechanism 2
- Claim: Fine-tuning the NLI predictor on a small annotated subset improves its alignment between rationales and task labels, which in turn yields better supervision signals for the explainer.
- Mechanism: The NLI predictor, pre-trained on general NLI tasks, is further fine-tuned on the target dataset using the annotated rationales. This adaptation aligns the NLI predictor's decision boundary with the dataset-specific semantics, enabling it to produce more accurate entailment signals for the explainer.
- Core assumption: Domain-specific semantic differences in the target dataset (e.g., fact verification vs. QA) are significant enough that a generic NLI predictor's outputs are unreliable without fine-tuning.
- Evidence anchors:
  - [section 5.1] "Without fine-tuning, NFI struggles to provide meaningful feedback to the explainer, primarily because of its limited capability to accurately determine whether a specific sentence should support or contradict the query based on the given task label."
  - [section 5.1] "Incorporating additional fine-tuning on the NLI predictor is still essential in filtering out false positives such as sentences with neutral relationships in inferring the output."
  - [corpus] No explicit prior studies cited on domain adaptation of NLI predictors for rationalization supervision.
- Break condition: If the annotated rationale set is too small or unrepresentative, fine-tuning will overfit and produce misleading entailment signals, degrading explainer performance.

### Mechanism 3
- Claim: Using the fine-tuned NLI predictor as a cross-checker during inference improves robustness by down-weighting rationales that contradict the predicted task label.
- Mechanism: At inference, the explainer generates rationales and the NLI predictor evaluates each rationale sentence against the predicted task label. NLI probabilities are used to scale the predictor's output probabilities, reducing the influence of rationales that are inconsistent with the task label.
- Core assumption: NLI predictors trained on the same dataset as the explainer maintain sufficient accuracy at inference to act as a reliable consistency filter.
- Evidence anchors:
  - [section 2.4] "Given ˆzi and a prefix... fN LI denotes if ˆzi contradicts or entails the prefix... The task probabilities, p(ˆyC_i) are then scaled with the NLI probabilities."
  - [section 4.2] "Our approach suffers the lowest drop in task and plausibility performance... while having the lowest AR in both datasets."
  - [corpus] No direct prior work cited showing cross-checking improves adversarial robustness in rationalization.
- Break condition: If the NLI predictor's inference is corrupted by distributional shift or adversarial examples, the scaling will misguide the final prediction, potentially worsening robustness.

## Foundational Learning

- Concept: Natural Language Inference (NLI) classification (entailment, contradiction, neutral)
  - Why needed here: The method relies on converting rationale extraction into an NLI alignment problem—sentences are judged as supporting or refuting the task label.
  - Quick check question: Given a sentence and a claim, can you classify whether the sentence entails, contradicts, or is neutral with respect to the claim?

- Concept: Semi-supervised learning via auxiliary predictors
  - Why needed here: Human-annotated rationales are scarce (10%); the NLI predictor provides scalable, automatic supervision without requiring full manual annotation.
  - Quick check question: How does using an auxiliary model's outputs as training targets differ from standard supervised learning?

- Concept: Adversarial prefix attacks on extractive models
  - Why needed here: The robustness evaluation tests whether the model can ignore irrelevant but semantically similar sentences prepended to the input.
  - Quick check question: If an adversarial sentence shares a noun with the original claim but provides no supporting evidence, how should a robust rationale extractor treat it?

## Architecture Onboarding

- Component map: Shared encoder (RoBERTa-base) -> Explainer decoder (MLP) -> Rationale mask; Shared encoder (RoBERTa-base) -> Predictor decoder (cross-entropy head) -> Task label; DeBERTa-large NLI predictor (fine-tuned on 10% rationales) -> NLI labels -> Reverse label transformation -> Explainer training targets
- Critical path:
  1. Fine-tune NLI predictor on 10% annotated rationales using label transformation.
  2. Train explainer-predictor jointly using explainer targets from NLI predictor.
  3. At inference, generate rationales with explainer; optionally scale predictor outputs using NLI predictor consistency check.
- Design tradeoffs:
  - Using a shared encoder couples explainer and predictor representations, improving alignment but risking overfitting to one task.
  - Fine-tuning on only 10% rationales reduces annotation cost but may limit NLI predictor accuracy.
  - Adding the NLI cross-checker improves robustness at negligible compute cost but adds inference latency.
- Failure signatures:
  - Explainer produces overly sparse rationales (high precision, low recall) -> NLI predictor over-classifies neutral sentences.
  - Explainer over-generates rationales (low precision, high recall) -> NLI predictor under-classifies neutral sentences.
  - Robustness drops sharply under adversarial prefix -> NLI cross-checker misaligns with predictor predictions.
- First 3 experiments:
  1. Train explainer-predictor with randomly initialized NLI predictor (no fine-tuning) and measure plausibility drop vs. fine-tuned baseline.
  2. Vary NLI supervision percentage (5%, 10%, 25%) and measure trade-off between annotation cost and plausibility.
  3. Evaluate robustness under adversarial prefixes with and without NLI cross-checker enabled.

## Open Questions the Paper Calls Out
- The paper suggests extending the approach to abstractive rationalization settings, where NLI signals could act as verification feedback.
- The authors propose extending the NLI predictor's coverage beyond single sentences to better capture correspondence in longer documents.
- Future work could explore scaling the approach to larger language models and comparing performance and resource implications.

## Limitations
- The approach's reliance on a small annotated rationale set (10%) for NLI predictor fine-tuning may limit generalizability to datasets with different semantic structures or adversarial patterns.
- The robustness improvements under adversarial prefixes, while statistically significant, are evaluated on synthetic attacks that may not reflect real-world distributional shifts.
- The claim of outperforming unsupervised methods by >100% in plausibility metrics is based on a specific comparison set and may not hold across all rationalization benchmarks.

## Confidence
- High Confidence: The core mechanism of using fine-tuned NLI signals for rationale supervision is well-supported by empirical results and ablation studies showing the necessity of fine-tuning for meaningful supervision.
- Medium Confidence: The robustness improvements under adversarial prefixes are supported by controlled experiments but may not generalize to all types of adversarial perturbations or real-world distributional shifts.
- Low Confidence: The generalizability of the approach to datasets beyond ERASER, particularly those with different semantic structures or task types, remains an open question due to limited evaluation scope.

## Next Checks
1. Evaluate the approach on additional rationalization datasets (e.g., e-SNLI, CoS-E) to test generalizability across different semantic structures and task types.
2. Test robustness under a wider range of adversarial perturbations, including natural language adversarial examples and real-world distributional shifts, to assess the approach's robustness beyond synthetic attacks.
3. Conduct an ablation study varying the NLI supervision percentage (e.g., 5%, 15%, 25%) to quantify the trade-off between annotation cost and plausibility/robustness gains.