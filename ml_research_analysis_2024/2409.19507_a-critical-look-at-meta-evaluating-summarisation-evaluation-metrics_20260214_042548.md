---
ver: rpa2
title: A Critical Look at Meta-evaluating Summarisation Evaluation Metrics
arxiv_id: '2409.19507'
source_url: https://arxiv.org/abs/2409.19507
tags:
- evaluation
- metrics
- summary
- summaries
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This position paper critically examines the practices of meta-evaluating
  summarisation evaluation metrics. The authors find that current evaluation metrics
  are primarily tested on news summarisation datasets and there has been a shift towards
  evaluating faithfulness.
---

# A Critical Look at Meta-evaluating Summarisation Evaluation Metrics

## Quick Facts
- arXiv ID: 2409.19507
- Source URL: https://arxiv.org/abs/2409.19507
- Authors: Xiang Dai; Sarvnaz Karimi; Biaoyan Fang
- Reference count: 16
- Primary result: Current summarization evaluation metrics are primarily tested on news datasets and need more diverse benchmarks to assess generalization ability across domains and constraints

## Executive Summary
This position paper critically examines the practices of meta-evaluating summarization evaluation metrics, identifying key limitations in current approaches. The authors find that existing evaluation practices are heavily biased toward news summarisation datasets and have shifted focus toward evaluating faithfulness, while other important quality dimensions receive less attention. They argue that the research community needs to develop more diverse benchmarks to properly assess the generalization ability of automatic metrics across different domains and summarization constraints. The paper calls for research focusing on user-centric quality dimensions that consider the communicative goals of summaries and their role in real-world workflows.

## Method Summary
The authors conducted a comprehensive literature review of recent meta-evaluation benchmarks and practices in summarization evaluation. They analyzed existing benchmarks including SUMM EVAL, REALSUMM, FRANK, FFCI, FIB, BUMP, SEAHORSE, DIAL SUMM EVAL, DIASUMFACT, GO FIGURE, ROSE, SUMM EDITS, and DIVER SUMM. Through this analysis, they identified four critical decisions in assessing automatic metrics: choosing data to annotate, defining quality dimensions, collecting human judgements, and comparing metrics against human judgements. The study synthesizes findings from multiple sources to identify research trends, gaps, and provide recommendations for improving meta-evaluation practices.

## Key Results
- Current evaluation metrics are primarily tested on news summarisation datasets, limiting their generalizability
- There has been a shift towards evaluating faithfulness as a quality dimension, with less attention to other aspects
- The paper identifies four critical decisions in meta-evaluation that need standardization and improvement
- Recommendations include creating more diverse benchmarks, standardizing human evaluation practices, and ensuring comprehensive assessments under various usage scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Evaluation metrics need diverse benchmarks to generalize across domains and constraints.
- Mechanism: Diverse benchmarks expose evaluation metrics to varied data distributions, ensuring they perform well beyond their training domain (e.g., news).
- Core assumption: Evaluation metrics trained on news data will fail to generalize to other domains without exposure to those domains.
- Evidence anchors:
  - [abstract]: "We argue that the time is ripe to build more diverse benchmarks that enable the development of more robust evaluation metrics and analyze the generalization ability of existing evaluation metrics."
  - [section]: "Because of the lack of meta-evaluation benchmarks covering various data distributions (i.e., source texts from different domains and output summaries from different systems under different task constraints), NLP practitioners may take the risk of overestimating the generalization ability of automatic metrics."
  - [corpus]: Weak evidence - corpus doesn't directly address domain generalization.
- Break condition: If benchmarks are not diverse enough to cover key domains or constraints, evaluation metrics may overfit to news data.

### Mechanism 2
- Claim: Human evaluation standardization is critical for reproducible and extensible meta-evaluation.
- Mechanism: Standardizing human evaluation practices ensures consistency in collected judgments, making resources reusable and extensible.
- Core assumption: Inconsistent human evaluation practices lead to unreliable and non-reproducible results.
- Evidence anchors:
  - [abstract]: "we call for research focusing on user-centric quality dimensions that consider the generated summary's communicative goal and the role of summarisation in the workflow."
  - [section]: "Given the costly nature of eliciting human judgements and the rapid pace of ongoing development in summarisation models, we believe there is an urgent need to standardise human evaluation practices."
  - [corpus]: Weak evidence - corpus doesn't directly address standardization of human evaluation.
- Break condition: If human evaluation practices are not standardized, results will be inconsistent and unreliable.

### Mechanism 3
- Claim: Quality dimensions must be precisely defined to avoid confusion and ensure reliable comparisons.
- Mechanism: Clear definitions of quality dimensions prevent ambiguity and ensure that different studies are measuring the same underlying concepts.
- Core assumption: Ambiguous quality dimension definitions lead to inconsistent judgments and unreliable comparisons.
- Evidence anchors:
  - [abstract]: "we call for research focusing on user-centric quality dimensions that consider the generated summary's communicative goal and the role of summarisation in the workflow."
  - [section]: "However, we also notice that different studies may investigate the same quality dimension following slightly different definitions, resulting in confusing conclusions."
  - [corpus]: Weak evidence - corpus doesn't directly address quality dimension definitions.
- Break condition: If quality dimensions are not precisely defined, comparisons between studies will be unreliable.

## Foundational Learning

- Concept: Meta-evaluation
  - Why needed here: Understanding meta-evaluation is crucial for assessing the effectiveness of automatic evaluation metrics.
  - Quick check question: What is the difference between evaluation and meta-evaluation in the context of summarization?

- Concept: Correlation coefficients
  - Why needed here: Correlation coefficients are used to measure the agreement between automatic metrics and human judgments.
  - Quick check question: What are the different types of correlation coefficients used in meta-evaluation, and when should each be used?

- Concept: Human annotation quality
  - Why needed here: Ensuring high-quality human annotations is essential for reliable meta-evaluation results.
  - Quick check question: What are the key factors that influence the quality of human annotations in summarization evaluation?

## Architecture Onboarding

- Component map: Automatic evaluation metrics -> Human judgment collection -> Benchmark datasets -> Correlation analysis tools
- Critical path: Collect human judgments -> Compare automatic metrics against human judgments -> Analyze correlation -> Draw conclusions about metric effectiveness
- Design tradeoffs: Balancing the cost and effort of human annotation with the need for high-quality, standardized judgments
- Failure signatures: High correlation between metrics and human judgments but poor performance on detecting specific errors; inconsistent judgments across different annotator groups
- First 3 experiments:
  1. Test correlation between a simple metric (e.g., ROUGE) and human judgments on a small, diverse dataset
  2. Compare correlation results when using different quality dimensions (e.g., fluency vs. faithfulness)
  3. Analyze the impact of using reference summaries versus not using them on correlation results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop standardized human evaluation practices that ensure reproducible human judgments over time and across different annotators for summarization evaluation?
- Basis in paper: [explicit] The paper explicitly calls for standardizing human evaluation practices to ensure reproducible human judgments over time and across different annotators.
- Why unresolved: Despite some existing quality control practices, there is currently no standardized approach for human evaluation in summarization that ensures consistency and reproducibility across different annotators and time periods.
- What evidence would resolve it: Development and validation of a standardized annotation protocol with clear guidelines, training procedures, and quality control mechanisms that can be consistently applied across different datasets, annotator pools, and time periods.

### Open Question 2
- Question: What is the impact of reference summaries on human judgments and how does this affect meta-evaluation results for different quality dimensions?
- Basis in paper: [explicit] The paper discusses how reference summaries are commonly used in human evaluation but notes that "the impact of reference summaries on human judgements and thus on meta-evaluation results is not well understood and examined."
- Why unresolved: Different studies show varying effects of reference summaries on human judgments, and there's confusion about whether metrics perform differently depending on whether reference summaries are used during human annotations.
- What evidence would resolve it: Systematic studies comparing human judgments with and without reference summaries across multiple quality dimensions, examining how the presence or absence of reference summaries affects both human judgments and the performance of automatic metrics.

### Open Question 3
- Question: How can we build more diverse benchmarks that enable testing the generalization ability of evaluation metrics across different domains and summarizations constraints?
- Basis in paper: [explicit] The paper argues that "the time is ripe to build more diverse benchmarks that enable the development of more robust evaluation metrics and analyze the generalization ability of existing evaluation metrics."
- Why unresolved: Most current benchmarks focus on news and dialogue domains, with limited exploration of how evaluation metrics perform across different domains and under various summarization constraints like summary length.
- What evidence would resolve it: Creation of comprehensive benchmarks covering multiple domains (medical, legal, academic, etc.) and summarization constraints, along with systematic evaluation of how existing metrics perform across these diverse scenarios.

## Limitations
- Heavy reliance on news domain datasets that may not represent real-world summarization scenarios
- Current lack of standardized human evaluation practices across the research community
- Subjective nature of quality dimensions leading to inconsistent interpretations by different annotators

## Confidence
- Need for diverse benchmarks: Medium confidence - supported by observed trends but lacks comprehensive empirical validation across multiple domains
- Standardization of human evaluation: Medium confidence - addresses a recognized problem but requires significant coordination to implement
- Quality dimension standardization: Low confidence - acknowledges that some degree of subjectivity will likely persist despite precise definitions

## Next Checks
1. Conduct a systematic survey of recent summarization systems to quantify the extent to which they operate outside the news domain, providing empirical support for the need for diverse benchmarks.

2. Design and execute a controlled experiment comparing correlation results when using standardized versus non-standardized human evaluation protocols on the same set of summaries.

3. Perform a detailed analysis of annotation guidelines from multiple meta-evaluation studies to identify specific areas where quality dimension definitions diverge, and propose standardized definitions for the most problematic dimensions.