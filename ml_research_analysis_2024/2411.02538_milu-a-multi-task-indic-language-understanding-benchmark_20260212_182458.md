---
ver: rpa2
title: 'MILU: A Multi-task Indic Language Understanding Benchmark'
arxiv_id: '2411.02538'
source_url: https://arxiv.org/abs/2411.02538
tags:
- science
- shot
- arts
- language
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MILU, a Multi-task Indic Language Understanding
  Benchmark designed to evaluate Large Language Models (LLMs) across 11 Indic languages.
  MILU spans 8 domains and 41 subjects, covering both general and culturally specific
  knowledge, with a focus on India-centric topics like local history, arts, festivals,
  and laws.
---

# MILU: A Multi-task Indic Language Understanding Benchmark

## Quick Facts
- arXiv ID: 2411.02538
- Source URL: https://arxiv.org/abs/2411.02538
- Reference count: 40
- Primary result: MILU benchmark evaluates 45 LLMs across 11 Indic languages, finding GPT-4o achieves highest accuracy at 74%, with open multilingual models outperforming language-specific fine-tuned models

## Executive Summary
MILU is a comprehensive benchmark designed to evaluate Large Language Models across 11 Indic languages, covering 8 domains and 41 subjects. The benchmark addresses the gap in existing evaluations by focusing on culturally specific knowledge through exam-style questions from Indian national and state civil services. MILU reveals significant performance disparities between high-resource and low-resource languages, with models struggling particularly in culturally relevant domains like Arts & Humanities and Law & Governance compared to general STEM fields.

## Method Summary
The MILU benchmark consists of 79,617 questions across 11 Indic languages, collected from authentic Indian exam sources and classified into 8 domains. The evaluation methodology employs both log-likelihood and generative approaches, testing 45 different LLMs including closed, open multilingual, and language-specific models. The benchmark uses zero-shot, one-shot, and five-shot evaluation settings to assess model generalization capabilities across different language resource levels and domain types.

## Key Results
- GPT-4o achieves the highest average accuracy at 74% across all models evaluated
- Open multilingual models outperform language-specific fine-tuned models, which perform only slightly better than random baselines
- Models show significantly better performance in high-resource languages compared to low-resource ones
- Domain-wise analysis reveals models perform poorly in culturally relevant areas like Arts & Humanities and Law & Governance compared to general fields like STEM

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MILU benchmarks expose LLM performance gaps in culturally specific knowledge domains.
- Mechanism: By curating exam-style questions from India-centric sources, MILU evaluates models on culturally relevant subjects rather than generic translations.
- Core assumption: Culture-specific knowledge cannot be reliably acquired through translation alone.
- Evidence anchors: Abstract states MILU reflects "culturally specific knowledge"; section describes India-first perspective with national/state exam questions; corpus analysis shows specialized benchmark space.

### Mechanism 2
- Claim: MILU reveals performance disparities between high-resource and low-resource Indic languages.
- Mechanism: The benchmark includes both high-resource languages (en, hi, bn) and low-resource agglutinative languages (ta, te, kn), allowing direct performance comparison.
- Core assumption: Language resource availability correlates with model performance on linguistically diverse tasks.
- Evidence anchors: Abstract notes models perform better in high-resource languages; section highlights need for robust multilingual strategies; corpus analysis indicates resource disparity focus.

### Mechanism 3
- Claim: MILU distinguishes between general STEM performance and culturally specific domain performance.
- Mechanism: By including both STEM and culturally specific domains, MILU enables domain-specific capability assessment beyond general knowledge.
- Core assumption: STEM performance does not generalize to culturally specific domains without targeted training.
- Evidence anchors: Abstract notes poor performance in culturally relevant areas like Arts & Humanities; section indicates better performance in general fields; corpus analysis shows domain-specific assessment capability.

## Foundational Learning

- Concept: Question curation from authentic cultural sources
  - Why needed here: MILU relies on exam-style questions from national/state exams to evaluate culturally specific knowledge
  - Quick check question: How would you ensure that a question about a regional Indian festival is both culturally accurate and linguistically appropriate for the target language?

- Concept: Domain classification and clustering
  - Why needed here: MILU organizes 41 subjects into 8 domains using clustering on embeddings
  - Quick check question: If you had 50 fine-grained subject tags, how would you cluster them into broader domains without losing cultural specificity?

- Concept: Few-shot evaluation methodology
  - Why needed here: MILU uses 0-shot, 1-shot, and 5-shot setups to evaluate LLM generalization
  - Quick check question: How would you design a 5-shot prompt for a question about Indian constitutional law that maximizes model performance without leaking the answer?

## Architecture Onboarding

- Component map: Data curation pipeline -> Question classification -> Translation pipeline -> Evaluation harness -> Result aggregation and analysis tools
- Critical path: 1. Collect questions from exam portals → 2. Clean/filter → 3. Classify → 4. Translate if needed → 5. Evaluate → 6. Analyze
- Design tradeoffs:
  - Translation vs. authentic data: Translation risks cultural nuance loss; authentic data ensures accuracy but may be scarce
  - Log-likelihood vs. generative evaluation: Log-likelihood is reproducible but may differ from human-like generation; generative evaluation is more realistic but less reproducible
  - Domain granularity: Fine-grained subjects capture specificity but complicate clustering; broader domains simplify but may lose nuance
- Failure signatures:
  - Low performance across all languages → likely data quality or model capability issue
  - High performance in high-resource languages, low in low-resource → resource disparity
  - High STEM performance, low Arts/Humanities → cultural knowledge gap
- First 3 experiments:
  1. Evaluate a baseline LLM (e.g., GPT-4o) on MILU to establish performance baseline
  2. Test few-shot learning impact by comparing 0-shot, 1-shot, and 5-shot results across model types
  3. Analyze domain-wise performance to identify cultural knowledge gaps

## Open Questions the Paper Calls Out

- Question: What specific mechanisms or architectural changes could improve LLM performance on culturally relevant subjects in low-resource languages?
  - Basis in paper: Explicit - The paper notes that models perform poorly in culturally relevant areas like Arts & Humanities and Law & Governance compared to general fields like STEM
  - Why unresolved: The paper identifies the performance gap but does not propose specific solutions
  - What evidence would resolve it: Experimental results showing improved performance after implementing specific architectural changes

- Question: How does the performance gap between high-resource and low-resource languages change with increasing model scale and parameter count?
  - Basis in paper: Explicit - The paper notes that models perform significantly better in high-resource languages than low-resource ones
  - Why unresolved: The paper does not analyze the relationship between resource level, model scale, and performance gap
  - What evidence would resolve it: Comparative analysis of performance across languages at different model scales

- Question: What specific aspects of in-context learning cause the performance degradation in instruction-tuned models when using few-shot examples?
  - Basis in paper: Explicit - The paper observes that instruction-tuned models exhibit varied behavior with some degrading in performance
  - Why unresolved: The paper notes the phenomenon but does not investigate the underlying causes
  - What evidence would resolve it: Detailed analysis of how different types of in-context examples affect instruction-tuned models

## Limitations

- The translation pipeline using GPT-4 O may introduce cultural or linguistic artifacts that affect downstream evaluation
- Log-likelihood evaluation method may not align perfectly with human judgment of answer quality
- Evaluation focuses on closed-book question answering without considering retrieval-augmented approaches

## Confidence

- High Confidence: MILU successfully identifies performance gaps in culturally specific domains
- Medium Confidence: Language resource disparity findings, as training data composition wasn't controlled
- Medium Confidence: STEM vs. culturally specific domain performance differences, as methodology may not capture nuanced cultural knowledge

## Next Checks

1. Conduct a human evaluation of a sample of translated questions to assess cultural nuance and linguistic accuracy preservation
2. Compare log-likelihood scores with human-annotated correctness for a subset of questions to validate evaluation method accuracy
3. Implement and evaluate a retrieval-augmented generation approach on MILU to determine if external knowledge access significantly improves performance in culturally specific domains