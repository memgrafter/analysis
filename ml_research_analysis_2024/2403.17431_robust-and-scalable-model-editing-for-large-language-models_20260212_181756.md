---
ver: rpa2
title: Robust and Scalable Model Editing for Large Language Models
arxiv_id: '2403.17431'
source_url: https://arxiv.org/abs/2403.17431
tags:
- edit
- edits
- knowledge
- language
- editing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the problem of model editing for large language
  models, which aims to quickly modify the behavior of a deployed LLM on specific
  examples while preserving its performance on unrelated instances. Existing in-context
  model editing methods have three major limitations: they are not scalable to large
  numbers of edits, they assume the relevant edit of a certain instance is given while
  in real-world scenarios the model needs to determine whether the current instance
  is related to any edits, and LLMs sometimes ignore the knowledge presented in the
  context or fail to ignore irrelevant knowledge, negatively impacting their result.'
---

# Robust and Scalable Model Editing for Large Language Models

## Quick Facts
- arXiv ID: 2403.17431
- Source URL: https://arxiv.org/abs/2403.17431
- Reference count: 0
- The paper proposes EREN, a robust and scalable model editing method that outperforms current state-of-the-art methods by a large margin on question answering and fact-checking tasks.

## Executive Summary
The paper addresses the problem of model editing for large language models, aiming to quickly modify the behavior of a deployed LLM on specific examples while preserving its performance on unrelated instances. Existing in-context model editing methods have three major limitations: they are not scalable to large numbers of edits, they assume the relevant edit of a certain instance is given while in real-world scenarios the model needs to determine whether the current instance is related to any edits, and LLMs sometimes ignore the knowledge presented in the context or fail to ignore irrelevant knowledge, negatively impacting their result. The paper proposes EREN (Edit models by REading Notes), a robust and scalable model editing method that can handle large numbers of edits and irrelevant edits. EREN is a retrieval-based method that stores all edits in a notebook and uses a reader to determine whether the input is relevant to any edit. If relevant, the reader makes a prediction based on the notebook; otherwise, it directly answers the question using its memorized knowledge. The paper also proposes a two-step inference pipeline to improve the robustness of the reader to irrelevant context. The paper evaluates EREN on question answering and fact-checking tasks and shows that it outperforms current state-of-the-art methods by a large margin.

## Method Summary
EREN (Edit models by REading Notes) is a retrieval-based model editing method that stores all edits in a notebook and uses a reader to determine whether the input is relevant to any edit. The method employs a two-step inference pipeline to improve robustness to irrelevant context. First, a rough relevance estimation using a dual-encoder retrieval eliminates highly dissimilar edits, retrieving the top-k most relevant notes. Then, the reader checks whether the input is relevant to any edit and, if so, makes a prediction based on the notebook; otherwise, it directly answers the question using its memorized knowledge. EREN is evaluated on question answering and fact-checking tasks, demonstrating superior performance compared to current state-of-the-art methods.

## Key Results
- EREN outperforms current state-of-the-art methods by a large margin on question answering and fact-checking tasks.
- EREN can handle large numbers of edits and irrelevant edits, addressing major limitations of existing in-context model editing methods.
- The two-step inference pipeline improves the robustness of the reader to irrelevant context, preventing negative impacts from irrelevant edits.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction-tuned LLMs can reliably ground on contextual knowledge and ignore irrelevant context when properly prompted
- Mechanism: Instruction tuning enables LLMs to treat contextual knowledge as authoritative when it conflicts with parametric knowledge, and to fall back to parametric knowledge when context is irrelevant
- Core assumption: The instruction tuning process instills in LLMs the ability to distinguish between relevant and irrelevant context and respond appropriately
- Evidence anchors:
  - [abstract] "with proper prompting methods, instruction-finetuned LLMs can be highly controllable by contextual knowledge and robust to irrelevant context"
  - [section 3.2.1] "LLMs, regardless of whether it is instruction-finetuned, are generally easily controllable by grounding on relevant contexts, but they are not robust to irrelevant context"
  - [corpus] Weak - the related papers discuss similar themes but don't directly validate this specific mechanism
- Break condition: If the LLM hasn't been properly instruction-tuned or if the prompts don't clearly distinguish relevant from irrelevant context

### Mechanism 2
- Claim: The two-step inference pipeline (relevance estimation followed by conditional generation) prevents negative impacts from irrelevant edits
- Mechanism: By first determining if input is relevant to any edit before performing conditional generation, the system avoids the problem where irrelevant context changes model predictions
- Core assumption: LLMs can be reliably prompted to determine relevance and distinguish between answerable and unanswerable questions
- Evidence anchors:
  - [section 3.2.1] "we design a two-step inference pipeline. The LLM is first prompted to determine whether an input is relevant to existing edits"
  - [section 4.1.1] "To determine whether the current instance is related to a certain edit, we reformat the task of model editing into reading comprehension with an 'unanswerable' option"
  - [corpus] Weak - related papers discuss relevance estimation but not this specific two-step approach
- Break condition: If the LLM cannot reliably distinguish relevant from irrelevant context or if the prompt format is unclear

### Mechanism 3
- Claim: The rough relevance estimation via dual-encoder retrieval eliminates highly dissimilar edits, reducing prompt length while maintaining performance
- Mechanism: By retrieving only the top-k most relevant edits before prompting, the system avoids exceeding context limits while still having access to relevant information
- Core assumption: A dual-encoder can effectively estimate rough relevance between questions and edits
- Evidence anchors:
  - [section 3.2.2] "we perform a rough relevance estimation to eliminate irrelevant edits that are easily identified. To this end, an embedding-based note retriever is employed to retrieve the top-k most relevant notes"
  - [section 5.4] "Figure 4 plots the retrieval recall rate and EREN's performance on CounterFact with varying numbers of retrievals"
  - [corpus] Weak - related papers discuss retrieval methods but not this specific dual-encoder approach for model editing
- Break condition: If the retriever fails to capture relevant edits or if k is set too low/high

## Foundational Learning

- Concept: Working memory in neural models
  - Why needed here: The paper frames context as working memory that should override parametric knowledge when relevant
  - Quick check question: How does the concept of working memory explain why irrelevant context can negatively impact LLM performance?

- Concept: Reading comprehension task formulation
  - Why needed here: The method reformats model editing as a reading comprehension task with an "unanswerable" option
  - Quick check question: What is the advantage of treating model editing as reading comprehension rather than a standard QA task?

- Concept: Retrieval-augmented generation
  - Why needed here: The method uses a retriever to select relevant edits before prompting the LLM
  - Quick check question: How does retrieval-augmented generation differ from simply concatenating all edits into the prompt?

## Architecture Onboarding

- Component map: Notebook (stores all edits in natural text) → Retriever (rough relevance estimation) → Reader (two-step inference: relevance check → conditional generation or direct answer)

- Critical path: Input → Retriever (get top-k relevant edits) → Reader (check relevance → generate answer using edits or parametric knowledge)

- Design tradeoffs: More retrievals (higher k) increases chance of capturing relevant edits but also increases noise and context length; two-step inference adds complexity but prevents negative impact from irrelevant context

- Failure signatures: Low edit success (EREN failing to apply edits correctly) or low behavior preservation (EREN changing answers to unrelated questions); poor retriever recall (missing relevant edits); reader failing to distinguish relevant from irrelevant context

- First 3 experiments:
  1. Test baseline performance on a small set of edits without retrieval to verify the reader works
  2. Test retriever recall on a diverse set of questions to find optimal k value
  3. Test end-to-end performance on a small dataset with varying numbers of edits to verify scalability

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the limitations and scope of the work, potential open questions include:
- How does the EREN model perform when scaling to extremely large numbers of edits (e.g., 10,000+)?
- How robust is EREN to edits that conflict with each other (e.g., contradictory facts about the same entity)?
- How does EREN's performance compare to fine-tuning approaches when the number of edits is small (e.g., < 10)?

## Limitations
- The paper's claims about robustness to irrelevant context and scalability to large numbers of edits are primarily supported by experimental results on specific datasets (CounterFact and FEVER) rather than theoretical analysis.
- The two-step inference pipeline's effectiveness depends heavily on the reader's ability to accurately distinguish relevant from irrelevant context, which may degrade with more complex or nuanced edits.
- The Contriever-based rough relevance estimation assumes that relevant edits will be sufficiently similar in embedding space to the input question, which may not hold for all types of knowledge edits.

## Confidence
- **High confidence**: The retrieval-augmented approach with notebook storage is technically sound and the two-step inference pipeline is a reasonable engineering solution to prevent negative impacts from irrelevant context
- **Medium confidence**: The experimental results showing EREN's superiority over baselines are compelling but limited to specific datasets and edit types
- **Medium confidence**: The claim that instruction-tuned LLMs can reliably ground on contextual knowledge while ignoring irrelevant context, though this depends heavily on prompt quality and may vary across model architectures

## Next Checks
1. Test EREN's performance on datasets with more diverse edit types beyond simple fact modification, including conceptual or procedural knowledge edits
2. Evaluate the method's robustness when edits themselves contain conflicting information or when the notebook contains contradictory edits
3. Assess the scaling properties with notebook size by testing EREN with progressively larger numbers of edits (100s to 1000s) to identify the practical limits of the retrieval-based approach