---
ver: rpa2
title: 'Hate Personified: Investigating the role of LLMs in content moderation'
arxiv_id: '2410.02657'
source_url: https://arxiv.org/abs/2410.02657
tags:
- hate
- language
- llms
- speech
- statement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates how Large Language Models (LLMs) perform
  in hate speech detection when primed with contextual cues such as geographical origin,
  annotator personas, and numerical voting statistics. Using two LLMs (FlanT5-XXL
  and GPT-3.5), five languages, and six datasets, the research finds that geographical
  cues significantly improve alignment with human annotations, while persona-based
  attributes introduce variability.
---

# Hate Personified: Investigating the role of LLMs in content moderation

## Quick Facts
- **arXiv ID**: 2410.02657
- **Source URL**: https://arxiv.org/abs/2410.02657
- **Reference count**: 40
- **Primary result**: LLM performance in hate speech detection improves significantly with geographical cues but varies with persona and numerical anchoring prompts.

## Executive Summary
This study investigates how contextual cues in prompts affect Large Language Models' (LLMs) performance in hate speech detection. Using two LLMs (FlanT5-XXL and GPT-3.5) across six multilingual datasets, the research examines the impact of geographical origin, annotator personas, and numerical voting statistics on prediction alignment with human annotations. The findings reveal that geographical cues enhance regional alignment with human judgments, while persona-based attributes introduce variability. Numerical anchoring also affects predictions, suggesting LLMs' sensitivity to community feedback or potential manipulation.

## Method Summary
The study employs zero-shot prompting with two LLMs across six hate speech datasets in five languages. Researchers construct prompts with base variants plus contextual cues (geographical, demographic, numerical) and evaluate using weighted F1 score and Cohen's kappa for inter-annotator agreement. The methodology involves binarizing labels, filtering for hallucinations, and conducting statistical significance testing through paired t-tests and ANOVA. Temperature sampling ensures robustness, and results are analyzed for trends across prompt types and datasets.

## Key Results
- Geographical cues significantly improve LLM alignment with human annotators across multiple datasets and languages
- Persona-based prompting introduces variability in hate speech annotations, with different framing approaches yielding divergent results
- Numerical anchors in prompts cause LLM predictions to be biased by majority voting cues, demonstrating sensitivity to community feedback

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding geographical cues in prompts increases LLM alignment with human annotators.
- Mechanism: Explicit context about the origin of a post primes the LLM to adjust its understanding of cultural nuances, improving prediction accuracy.
- Core assumption: LLMs encode regional variations in language use and cultural references that can be accessed via explicit prompting.
- Evidence anchors:
  - [abstract] "incorporating geographical signals leads to better regional alignment."
  - [section] "Geographical Cues...we observe that geographical cues lead to visible and significant increases in human-LLM agreement"
  - [corpus] Weak evidence: no direct corpus studies cited for this specific mechanism.
- Break condition: If LLM training data lacks sufficient regional variation, cues may have no effect.

### Mechanism 2
- Claim: LLM persona-based prompting introduces variability in hate speech annotations.
- Mechanism: Assigning demographic traits to the LLM alters its internal representation of the task, causing shifts in prediction patterns.
- Core assumption: LLMs possess implicit biases tied to demographic constructs that can be activated through persona framing.
- Evidence anchors:
  - [abstract] "mimicking persona-based attributes leads to annotation variability."
  - [section] "Our work establishes that the manner of personification of demographic attributes can lead to variation in hate labeling"
  - [corpus] Weak evidence: no specific corpus-based validation of demographic bias in LLMs provided.
- Break condition: If persona framing is too generic, it may not trigger meaningful bias shifts.

### Mechanism 3
- Claim: Numerical anchors in prompts cause LLM predictions to be biased by majority voting cues.
- Mechanism: Mentioning voting percentages shifts LLM reasoning toward community consensus, overriding content-based judgment.
- Core assumption: LLMs treat numerical context as a strong signal, affecting classification decisions disproportionately.
- Evidence anchors:
  - [abstract] "the LLMs are sensitive to numerical anchors, indicating the ability to leverage community-based flagging efforts"
  - [section] "LLMs do not have a clear way of discovering noise from the informativeness in the prompt"
  - [corpus] Weak evidence: no corpus-level analysis of numerical anchoring bias in LLMs.
- Break condition: If numerical cues are vague or inconsistent, the effect may diminish.

## Foundational Learning

- Concept: Zero-shot prompting
  - Why needed here: The study relies on prompting without task-specific training.
  - Quick check question: Can you define zero-shot learning in the context of LLMs?

- Concept: Inter-annotator agreement (IAA)
  - Why needed here: Used to measure alignment between LLM predictions and human labels.
  - Quick check question: How does IAA differ from F1 score in subjective annotation tasks?

- Concept: Demographic priming
  - Why needed here: Core to analyzing how persona cues influence LLM outputs.
  - Quick check question: What is demographic priming and why is it relevant in NLP?

## Architecture Onboarding

- Component map: Prompt → LLM → Prediction → IAA/F1 Evaluation
- Critical path: Construct prompt with cue → Run LLM → Compare prediction to ground truth → Measure alignment
- Design tradeoffs: Trade between prompt complexity and consistency vs. simplicity and reproducibility
- Failure signatures: High hallucination rates, low IAA despite cue addition, or contradictory predictions across persona variants
- First 3 experiments:
  1. Run base prompt (no cues) on HateXplain and measure F1/IAA
  2. Add geographical cue to prompt and compare performance
  3. Add persona cue and analyze variability across demographic subclasses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the size and training paradigm of multilingual LLMs affect their sensitivity to geographical and demographic cues compared to general-purpose models like GPT-3.5?
- Basis in paper: [inferred] The study briefly compares language-specific models (e.g., Mistral-Ar, Airavata) with GPT-3.5, noting that "not just demographic-based training corpus but the manner of training and size of models impact multilingualism." However, the analysis is limited, and deeper exploration of these factors is not conducted.
- Why unresolved: The paper does not provide a detailed comparison of how model architecture, training data, or instruction-tuning influence sensitivity to cues. It only hints at the importance of these factors without empirical validation.
- What evidence would resolve it: Systematic experiments comparing multiple multilingual models of varying sizes and training paradigms (e.g., instruction-tuned vs. general-purpose) on the same tasks, analyzing their performance differences in response to geographical and demographic cues.

### Open Question 2
- Question: To what extent does the format of persona cues (e.g., assumed persona vs. labeled persona) influence the alignment of LLM predictions with human annotations, and are there optimal framing strategies?
- Basis in paper: [explicit] The study finds that "nudging the model to assume a persona (pA_trait) is different from presenting the LLMs with a persona (pH/N_trait)," and that the manner of personification "can lead to variation in hate labeling." However, it does not explore the nuances of framing or identify optimal strategies.
- Why unresolved: The paper identifies differences in persona framing but does not test alternative prompt structures or provide guidelines for effective framing.
- What evidence would resolve it: Controlled experiments testing multiple persona prompt formats (e.g., different phrasings, levels of detail) to determine which framing strategies yield the highest alignment with human annotations.

### Open Question 3
- Question: How does the inclusion of numerical metadata (e.g., voting percentages) in prompts affect LLM predictions in zero-shot settings, and can this be mitigated to avoid anchoring bias?
- Basis in paper: [explicit] The study demonstrates that "LLMs are prone to labeling bias even under zero-shot settings if the context mentions voting percentages," with significant variability in predictions based on the provided percentages. However, it does not explore mitigation strategies or the underlying mechanisms of this bias.
- Why unresolved: The paper identifies the problem of anchoring bias but does not propose or test solutions to reduce its impact on LLM predictions.
- What evidence would resolve it: Experiments testing techniques to mitigate anchoring bias, such as normalizing numerical inputs, providing context about the reliability of metadata, or using alternative representations of community feedback.

## Limitations

- The study uses binarized labels which may oversimplify nuanced hate speech categories
- Results may not generalize to all possible prompting strategies due to focus on specific variants
- Zero-shot prompting approach may limit LLMs' ability to capture complex contextual relationships compared to fine-tuned models

## Confidence

- **High Confidence**: Findings regarding geographical cue improvements in human-LLM agreement are supported by statistically significant results across multiple datasets and languages.
- **Medium Confidence**: The observation that persona-based prompting introduces variability is supported by the data but requires further investigation to determine whether this represents genuine demographic bias or random variation.
- **Low Confidence**: The claims about numerical anchoring effects, while showing statistical significance, may be artifacts of the specific datasets used rather than generalizable phenomena.

## Next Checks

1. **Cross-dataset validation**: Test the prompt variants on additional hate speech datasets, particularly those with different annotation schemes and cultural contexts, to verify the robustness of geographical cue benefits.

2. **Adversarial prompt testing**: Systematically test whether the observed numerical anchoring effects can be manipulated through adversarial prompt engineering, and assess the LLMs' robustness to such attacks.

3. **Fine-tuning comparison**: Compare the zero-shot prompting approach with fine-tuned models on the same datasets to determine whether the contextual cue benefits would persist or be amplified with task-specific training.