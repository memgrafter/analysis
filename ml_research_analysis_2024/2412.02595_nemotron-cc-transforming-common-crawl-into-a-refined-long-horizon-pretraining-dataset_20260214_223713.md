---
ver: rpa2
title: 'Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon Pretraining
  Dataset'
arxiv_id: '2412.02595'
source_url: https://arxiv.org/abs/2412.02595
tags: []
core_contribution: Nemotron-CC is a 6.3 trillion token English Common Crawl dataset
  for LLM pretraining, built using classifier ensembling, synthetic data generation,
  and optimized filtering. It achieves 4x more unique real tokens than prior datasets
  while matching or exceeding their accuracy.
---

# Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon Pretraining Dataset

## Quick Facts
- **arXiv ID**: 2412.02595
- **Source URL**: https://arxiv.org/abs/2412.02595
- **Reference count**: 34
- **Primary result**: 6.3T token English Common Crawl dataset achieving 4x more unique real tokens than prior datasets while matching or exceeding their accuracy

## Executive Summary
Nemotron-CC addresses the challenge of constructing high-quality pretraining datasets from Common Crawl by introducing an innovative combination of model-based filtering, synthetic data generation, and quality-aware data blending. The dataset achieves 4x more unique real tokens (4.4T vs 1.1T) than prior work while maintaining or improving accuracy across multiple benchmarks. An 8B model trained for 15T tokens on this data outperforms Llama 3.1 8B on MMLU (+5.0), ARC-Challenge (+3.1), and other tasks. The key innovation is using classifier ensembling to increase high-quality token recall from 9% to 25%, combined with rephrasing low-quality data to improve downstream performance.

## Method Summary
The dataset construction pipeline uses Justext for HTML-to-text extraction, followed by language filtering and model-based quality classification using an ensemble of three classifiers. Documents are bucketed by quality scores and synthetic data is generated differently for low-quality (rephrased Wikipedia-style) versus high-quality (knowledge extraction and diverse QA pairs) documents. The final dataset combines 73% English Common Crawl with 27% specialized datasets, totaling 6.3T tokens including 1.9T synthetic tokens.

## Key Results
- 4x increase in unique real tokens (4.4T vs 1.1T in C4) while maintaining accuracy
- 8B model trained on 15T tokens achieves MMLU score of 70.3 vs Llama 3.1 8B at 65.3 (+5.0)
- Ensembled classifiers increase high-quality token recall from 9% to 25%
- Rephrasing low-quality data improves average accuracy by 1.5 points

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ensembling different model-based classifiers increases the recall of high-quality tokens from Common Crawl data.
- Mechanism: Using multiple classifiers with different quality preferences allows capturing a more diverse set of high-quality documents than any single classifier alone.
- Core assumption: Different classifiers will have non-overlapping sets of high-quality documents they identify, leading to greater coverage when combined.
- Evidence anchors: Only 10% of documents are predicted as high quality by both classifiers, while 35.4% are predicted by one classifier only and 54.4% by the other.

### Mechanism 2
- Claim: Rephrasing low-quality data improves downstream task performance by reducing noise and errors while preserving useful information.
- Mechanism: Using a language model to rewrite low-quality documents in a cleaner style removes noise and errors while maintaining the core factual content.
- Core assumption: The rephrasing process will effectively reduce noise and errors without introducing significant misinformation or losing important details.
- Evidence anchors: Boosts from 1.80% to 4.75% on ARC-Easy, ARC-Challenge, OpenbookQA, CommonsenseQA; slight accuracy drops on some tasks indicate potential misinformation.

### Mechanism 3
- Claim: Synthetic data generation from high-quality documents provides fresh unique tokens and diverse styles, improving model performance.
- Mechanism: Using prompts to extract knowledge, create diverse QA pairs, and distill information from high-quality documents generates new data that helps the model learn specific abilities and absorb knowledge more efficiently.
- Core assumption: The synthetic data will be diverse and high-quality enough to provide meaningful learning signals beyond the original documents.
- Evidence anchors: Prompts require clear and concise responses while preserving factual information and concrete details.

## Foundational Learning

- Concept: Model-based filtering vs. heuristic filtering
  - Why needed here: Understanding the difference between learned quality classifiers and rule-based filters is crucial for grasping the dataset construction approach.
  - Quick check question: What is the key advantage of model-based filtering over traditional heuristic filtering in this context?

- Concept: Token deduplication and its impact on training
  - Why needed here: The dataset's size and uniqueness depend heavily on deduplication strategies, which directly affect long-horizon training.
  - Quick check question: Why is having more unique real tokens important for training over long horizons like 15T tokens?

- Concept: Synthetic data generation techniques
  - Why needed here: The synthetic data component is a key differentiator of this dataset and requires understanding how it's created and used.
  - Quick check question: How does the approach to synthetic data differ between low-quality and high-quality source documents?

## Architecture Onboarding

- Component map: HTML-to-text extraction (Justext) -> Initial filtering (language, basic deduplication) -> Model-based quality classification (ensemble) -> Data bucketing -> Synthetic data generation -> Final deduplication and assembly

- Critical path: Quality classification → Data bucketing → Synthetic data generation → Final deduplication

- Design tradeoffs:
  - Justext vs. Trafilatura: Higher token yield vs. potential quality differences
  - Heuristic filters on vs. off: Data cleanliness vs. token quantity
  - Classifier ensemble size: Coverage vs. complexity
  - Synthetic data prompts: Diversity vs. coherence

- Failure signatures:
  - Low MMLU scores despite high dataset size: Potential issues with quality classification or synthetic data generation
  - Highly skewed quality distribution: May indicate classifier bias or data imbalance
  - Poor downstream performance on specific tasks: Could suggest issues with synthetic data diversity or quality

- First 3 experiments:
  1. Compare Justext vs. Trafilatura extraction on a small Common Crawl sample, measuring both token yield and downstream task performance.
  2. Test the impact of removing heuristic filters on a subset of high-quality data, measuring changes in both token yield and accuracy.
  3. Evaluate the effectiveness of different synthetic data prompts on a small scale, comparing their impact on downstream task performance.

## Open Questions the Paper Calls Out

1. How can the tradeoff between data quality and quantity be further optimized for long-horizon pretraining? (Explicit)
2. What is the impact of rephrased data on factual accuracy and content diversity? (Inferred)
3. How can the model-based filter ensembling and quality bucketing be improved? (Explicit)
4. How can the methods be adapted to languages other than English? (Explicit)
5. What are the optimal parameters for the HTML-to-text extraction and filtering pipeline? (Inferred)

## Limitations

- Synthetic data generation introduces potential factual inaccuracies without verification mechanisms
- Heavy computational requirements (1024 H100 GPUs for 40 hours) limit reproducibility
- Evaluation focuses primarily on English language tasks, limiting generalizability to multilingual contexts

## Confidence

**High Confidence**: The claim that Nemotron-CC achieves 4x more unique real tokens than prior datasets is well-supported by systematic comparison methodology. The improvement in MMLU score from 65.3 to 70.3 when training on 15T tokens is highly reliable given controlled training setup.

**Medium Confidence**: The assertion that ensembling classifiers increases high-quality token recall from 9% to 25% is reasonably supported but depends on specific classifier implementations. The claim about rephrasing low-quality data improving accuracy by 1.5 points is supported by ablation studies but shows variability across tasks.

**Low Confidence**: The broader claim that this dataset construction methodology represents a fundamental advance in LLM pretraining requires long-term validation across diverse model architectures and task types beyond those tested.

## Next Checks

1. **Factual Accuracy Audit**: Conduct a systematic audit of the synthetic data generation process by sampling 1,000 rephrased documents and 1,000 knowledge extraction outputs, then having human annotators rate factual accuracy and identify potential misinformation introduced during the rephrasing process.

2. **Classifier Generalization Test**: Train the three quality classifiers on Nemotron-CC data, then evaluate their performance on held-out Common Crawl samples and external high-quality datasets to verify that the ensembling approach generalizes beyond the specific data distribution used in training.

3. **Long-Horizon Stability Analysis**: Train multiple 8B models for 5T, 10T, and 15T tokens on Nemotron-CC using identical hyperparameters except for training duration, then analyze the stability and saturation of downstream task performance across different training lengths to validate the claimed benefits of long-horizon pretraining.