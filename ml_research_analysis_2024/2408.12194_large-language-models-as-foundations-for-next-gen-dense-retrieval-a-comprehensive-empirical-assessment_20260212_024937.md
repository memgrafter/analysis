---
ver: rpa2
title: 'Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive
  Empirical Assessment'
arxiv_id: '2408.12194'
source_url: https://arxiv.org/abs/2408.12194
tags:
- retrieval
- arxiv
- dense
- llms
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study systematically evaluates large language models (LLMs)
  as backbone encoders for dense retrieval across six critical dimensions: in-domain
  accuracy, data efficiency, zero-shot generalization, lengthy retrieval, instruction-based
  retrieval, and multi-task learning. Using over 15 models ranging from 0.1B to 32B
  parameters, the research demonstrates that LLMs consistently outperform non-LLM
  retrievers across all tasks.'
---

# Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment

## Quick Facts
- arXiv ID: 2408.12194
- Source URL: https://arxiv.org/abs/2408.12194
- Reference count: 5
- LLMs consistently outperform non-LLM retrievers across six evaluation dimensions

## Executive Summary
This study systematically evaluates large language models (LLMs) as backbone encoders for dense retrieval tasks across six critical dimensions: in-domain accuracy, data efficiency, zero-shot generalization, lengthy retrieval, instruction-based retrieval, and multi-task learning. Using over 15 models ranging from 0.1B to 32B parameters, the research demonstrates that LLMs consistently outperform non-LLM retrievers across all tasks. Key findings include that larger models and extensive pre-training improve in-domain accuracy and data efficiency, model size is the primary factor for zero-shot generalization, and LLMs generalize better to lengthy passages than traditional models. The research highlights the advantages of LLMs as versatile and effective backbone encoders for dense retrieval tasks.

## Method Summary
The study employs LoRA fine-tuning with InfoNCE loss to train various models (BERT, T5, Llama, Phi, Gemma, Qwen series) on the MS MARCO passage ranking dataset for in-domain tasks. Models are evaluated across six dimensions using datasets including MS MARCO, BEIR benchmark for zero-shot evaluation, NarrativeQA for lengthy retrieval, and custom instruction-based retrieval tasks. The training uses Adam optimizer with learning rate 3e-4, batch size 128, 7 hard negative samples, and temperature 0.02. Evaluation metrics include NDCG@10, MRR@10, Recall@10, and Recall@1000 across different retrieval tasks.

## Key Results
- Larger models and extensive pre-training consistently enhance in-domain accuracy and data efficiency
- Model size is the primary factor for zero-shot generalization performance
- Instruction-based retrieval benefits LLMs but not non-LLMs
- LLMs demonstrate superior ability to handle lengthy passages compared to traditional dense retrievers
- Larger models show better multi-task learning capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger model parameters consistently improve in-domain accuracy for dense retrieval tasks.
- Mechanism: Increasing parameter count enhances the model's capacity to capture complex semantic relationships between queries and documents, leading to better representation learning.
- Core assumption: Model capacity directly correlates with retrieval accuracy when trained on the same dataset.
- Evidence anchors:
  - [abstract]: "larger models and extensive pre-training consistently enhance in-domain accuracy"
  - [section]: "the results indicate that model performance generally improves with an increase in parameter numbers"
  - [corpus]: Weak evidence - corpus contains related papers but no direct citations supporting this specific mechanism
- Break condition: If training data quality or diversity is insufficient to utilize the increased model capacity

### Mechanism 2
- Claim: LLMs demonstrate superior zero-shot generalization compared to non-LLM retrievers.
- Mechanism: Extensive pre-training on diverse text corpora gives LLMs richer world knowledge and better semantic understanding, enabling them to generalize to unseen retrieval tasks without task-specific fine-tuning.
- Core assumption: Pre-training diversity and scale are more important than task-specific adaptation for generalization.
- Evidence anchors:
  - [abstract]: "LLMs consistently outperform non-LLM retrievers across all tasks"
  - [section]: "LLM retrievers significantly outperform non-LLM retrievers in zero-shot retrieval tasks"
  - [corpus]: Weak evidence - related papers exist but don't directly support this generalization claim
- Break condition: When retrieval tasks require highly specialized domain knowledge not covered in pre-training

### Mechanism 3
- Claim: Instruction-based retrieval benefits LLMs but not non-LLMs.
- Mechanism: LLMs' superior natural language understanding allows them to interpret and adapt to retrieval instructions, modifying their retrieval behavior accordingly, while non-LLMs lack this semantic flexibility.
- Core assumption: LLMs can parse and act upon natural language instructions in a retrieval context.
- Evidence anchors:
  - [abstract]: "instruction-based retrieval benefits LLMs but not non-LLMs"
  - [section]: "training with instructions significantly improves the performance of LLM retrievers, whereas for BERT retrievers results in decreased performance"
  - [corpus]: Weak evidence - corpus lacks direct citations supporting instruction comprehension in retrieval
- Break condition: When instructions are too complex or ambiguous for the LLM to interpret correctly

## Foundational Learning

- Concept: Dense Retrieval Fundamentals
  - Why needed here: Understanding how dense retrieval encodes queries and documents into shared embedding spaces is essential for grasping why model architecture matters
  - Quick check question: What is the key difference between dense and sparse retrieval methods in how they represent text?

- Concept: Contrastive Learning for Retrieval
  - Why needed here: The paper uses InfoNCE loss for training, so understanding this objective function is crucial for implementing and modifying the training process
  - Quick check question: How does InfoNCE loss encourage the model to distinguish between relevant and irrelevant document-query pairs?

- Concept: Transfer Learning and Zero-shot Generalization
  - Why needed here: The paper evaluates zero-shot performance across multiple domains, requiring understanding of how pre-training affects generalization
  - Quick check question: What factors determine whether a model can successfully transfer to a new retrieval domain without fine-tuning?

## Architecture Onboarding

- Component map: Backbone encoder ‚Üí Text embedding (CLS or EOS token) ‚Üí Scoring function (cosine similarity) ‚Üí ANN search index
- Critical path: Model pre-training ‚Üí Dense retrieval fine-tuning ‚Üí Inference with ANN search
- Design tradeoffs: Model size vs. computational efficiency; pre-training sufficiency vs. fine-tuning data requirements; encoder-only vs. decoder architecture for retrieval tasks
- Failure signatures: Poor in-domain performance suggests insufficient model capacity or pre-training; zero-shot failures indicate over-specialization; lengthy retrieval issues suggest context window limitations
- First 3 experiments:
  1. Compare BERT-large vs. Llama-2-7B on MS MARCO in-domain accuracy to validate size advantage
  2. Test zero-shot performance on BEIR benchmark with different model sizes to confirm generalization patterns
  3. Evaluate instruction-based retrieval with simple query modifications to observe LLM flexibility vs. non-LLM brittleness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training differences between LLMs and non-LLMs contribute most to their superior in-domain accuracy in dense retrieval tasks?
- Basis in paper: Explicit
- Why unresolved: While the paper demonstrates that LLMs consistently outperform non-LLMs in in-domain accuracy, it does not explicitly analyze the underlying architectural or training differences that lead to this superiority.
- What evidence would resolve it: A detailed comparative analysis of the architectural features and training procedures of LLMs and non-LLMs, along with ablation studies isolating the impact of each difference on retrieval performance.

### Open Question 2
- Question: How does the pre-training sufficiency of LLMs (measured in tokens) interact with model size to influence data efficiency and convergence speed in dense retrieval tasks?
- Basis in paper: Inferred
- Why unresolved: The paper shows that both model size and pre-training sufficiency contribute to data efficiency, but it does not explicitly explore the interaction between these two factors.
- What evidence would resolve it: Controlled experiments varying both model size and pre-training sufficiency independently and in combination, measuring data efficiency and convergence speed across a range of dense retrieval tasks.

### Open Question 3
- Question: What are the specific mechanisms by which instruction-based retrieval improves LLM performance, and why do these mechanisms not apply to non-LLM models?
- Basis in paper: Explicit
- Why unresolved: The paper demonstrates that training with instructions benefits LLMs but not non-LLMs, suggesting fundamental differences in how these models process and utilize instructions.
- What evidence would resolve it: Detailed analysis of how instructions are processed and incorporated into the retrieval process for both LLM and non-LLM models, including attention patterns, embedding modifications, and decision-making processes during retrieval.

### Open Question 4
- Question: What is the optimal balance between model size and pre-training sufficiency for achieving the best zero-shot generalization performance in dense retrieval tasks?
- Basis in paper: Inferred
- Why unresolved: The paper finds that model size is the primary factor for zero-shot generalization, but it does not explore the optimal balance with pre-training sufficiency.
- What evidence would resolve it: Comprehensive experiments varying both model size and pre-training sufficiency across multiple zero-shot retrieval tasks, identifying the point of diminishing returns for each factor and the optimal combination for maximizing generalization performance.

### Open Question 5
- Question: How do different alignment processes (e.g., human preference alignment, task-specific alignment) impact the performance of LLMs as backbone encoders for dense retrieval tasks?
- Basis in paper: Explicit
- Why unresolved: The paper compares base LLMs and chat LLMs but does not explore the impact of different alignment processes on retrieval performance.
- What evidence would resolve it: Comparative studies of LLMs with different alignment processes across various dense retrieval tasks, measuring performance differences and identifying the most effective alignment strategies for each task type.

## Limitations

- Model Architecture Bias: The study primarily evaluates encoder-only and decoder-only models, but does not extensively test encoder-decoder architectures for dense retrieval tasks.
- Pre-training Data Heterogeneity: The pre-training corpora differ significantly across models, making it difficult to isolate the effect of model size from pre-training quality.
- Evaluation Scope Constraints: The study focuses on English-language retrieval tasks and does not evaluate multilingual or cross-lingual retrieval capabilities.

## Confidence

**High Confidence (‚òëÔ∏è)**:
- Larger models consistently outperform smaller models in in-domain accuracy when trained on identical datasets
- Model size is the primary determinant of zero-shot generalization performance
- LLMs demonstrate superior ability to handle lengthy passages compared to traditional dense retrievers

**Medium Confidence (ü§î)**:
- Instruction-based retrieval benefits are exclusive to LLMs and not replicable with traditional retrievers
- Multi-task learning improvements scale linearly with model parameter count
- Data efficiency gains from larger models remain consistent across all retrieval domains

**Low Confidence (‚ùå)**:
- The optimal model size for balancing performance and computational efficiency is universal across all retrieval tasks
- Pre-training diversity guarantees better retrieval performance regardless of task specificity
- All observed improvements are solely attributable to parameter count rather than architectural innovations

## Next Checks

1. **Architectural Ablation Study**: Compare encoder-decoder models (T5 variants) against encoder-only and decoder-only models across all six evaluation dimensions to isolate architectural effects from size effects.

2. **Cross-Lingual Generalization Test**: Evaluate the best-performing models from this study on multilingual retrieval benchmarks (e.g., XOR-TyDi, mBEIR) to assess whether size advantages transfer to non-English languages.

3. **Pre-training Data Analysis**: Conduct controlled experiments where models of similar size but different pre-training corpora are evaluated on the same retrieval tasks to quantify the impact of pre-training data composition versus model capacity.