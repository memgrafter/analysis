---
ver: rpa2
title: 'BendVLM: Test-Time Debiasing of Vision-Language Embeddings'
arxiv_id: '2411.04420'
source_url: https://arxiv.org/abs/2411.04420
tags:
- debiasing
- attribute
- gender
- embedding
- wilful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses bias in vision-language model embeddings, which
  can encode societal biases related to race and gender. The authors propose BEND-VLM,
  a fine-tuning-free method that debiases embeddings by making them orthogonal to
  local attribute subspaces and equalizing distances to relevant images from a reference
  dataset.
---

# BendVLM: Test-Time Debiasing of Vision-Language Embeddings

## Quick Facts
- arXiv ID: 2411.04420
- Source URL: https://arxiv.org/abs/2411.04420
- Reference count: 40
- Key outcome: BEND-VLM reduces bias in vision-language embeddings while maintaining or improving accuracy across retrieval, classification, and image captioning tasks

## Executive Summary
This paper introduces BEND-VLM, a fine-tuning-free method for debiasing vision-language model (VLM) embeddings by making them orthogonal to local attribute subspaces and equalizing distances to relevant images from a reference dataset. The method addresses societal biases related to race and gender in VLM embeddings, which can encode spurious correlations that lead to biased downstream task performance. BEND-VLM is specifically designed for online, open-set tasks where classes are unknown beforehand, making it more practical for real-world applications than methods requiring class-specific training.

The proposed approach works by first projecting query embeddings onto a subspace orthogonal to attribute-specific directions constructed from augmented queries, then finding embeddings that maintain minimal distance to the first-stage debiased representation while being equally similar to reference images across attribute groups. Experiments on multiple datasets show that BEND-VLM consistently outperforms compared methods in reducing bias metrics like KL divergence and MaxSkew while maintaining or improving accuracy. The method demonstrates effectiveness across different downstream tasks including retrieval, zero-shot classification, and image captioning.

## Method Summary
BEND-VLM employs a two-step approach to debias vision-language embeddings. First, it constructs a local attribute subspace matrix using augmented queries (e.g., "male nurse" and "female nurse") and applies orthogonal projection to remove attribute directions from the original query embedding. Second, it selects the n most similar reference images for each attribute value and solves a constrained optimization problem to find an updated embedding that maintains minimal distance to the first-stage debiased representation while equalizing similarity scores across attribute groups. The method requires only a reference dataset with attribute labels and can process queries online without prior knowledge of the class space.

## Key Results
- BEND-VLM reduces KL divergence and MaxSkew bias metrics significantly compared to baselines across retrieval, classification, and image captioning tasks
- The method maintains or improves accuracy while reducing bias, with some improvements in worst-group AUC for classification tasks
- BEND-VLM outperforms compared methods including Baseline CLIP, Orth-Proj., Orth-Cal., and DebiasCLIP in bias reduction metrics
- The approach is effective for open-set scenarios with unknown classes, demonstrating practical utility for real-world applications

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The method removes bias by projecting query embeddings onto a subspace orthogonal to attribute-specific directions.
- **Mechanism:** For each query, the method constructs an attribute subspace matrix `A` using augmented queries (e.g., "male nurse" and "female nurse"). It then applies an orthogonal projection matrix `V = I - A(A^T A)^-1 A^T` to the original query embedding to remove the attribute direction.
- **Core assumption:** The attribute subspace can be accurately estimated from a small set of augmented queries for each input.
- **Evidence anchors:**
  - [abstract] "BEND-VLM... debiases embeddings by making them orthogonal to local attribute subspaces"
  - [section 3.1] "Let A be a matrix whose columns are f_T^θ(t_c,a_i) - f_T^θ(t_c)... We then obtain the initial debiased embedding z'_c as: z'_c = V f_T^θ(t_c)"
  - [corpus] Weak - related work mentions projection-based methods but doesn't validate the local subspace construction specifically
- **Break condition:** If the augmented queries don't capture the true attribute direction, or if the attribute relationship is too nonlinear for a linear projection to remove.

### Mechanism 2
- **Claim:** The method equalizes similarity scores between the debiased embedding and reference images across attribute groups.
- **Mechanism:** After the initial projection, the method finds the n most similar images from the reference dataset for each attribute value. It then solves a constrained optimization problem to find an embedding that maintains minimal distance to the first-stage debiased embedding while equalizing distances to images from each attribute group.
- **Core assumption:** Reference images can be reliably selected as relevant to the query embedding, and the constrained optimization yields a meaningful debiased representation.
- **Evidence anchors:**
  - [abstract] "...equalizing distances to relevant images from a reference dataset"
  - [section 3.2] "We find an updated, debiased query representation by solving a constrained optimization equation with the goal of finding an embedding with minimal distance to the first-stage debiased representation while being equally similar to the example images for each attribute value"
  - [corpus] Weak - related work mentions equalization but doesn't detail the constrained optimization approach
- **Break condition:** If the reference dataset doesn't contain representative images for each attribute, or if the optimization problem is ill-conditioned.

### Mechanism 3
- **Claim:** The method can handle open-set scenarios where classes are unknown during inference.
- **Mechanism:** By using only a reference dataset with attribute labels (not class labels) and processing queries online, the method can debias embeddings for any query without prior knowledge of the class space.
- **Core assumption:** The reference dataset's attribute annotations are sufficient to debias queries regardless of the specific class being queried.
- **Evidence anchors:**
  - [abstract] "Additionally, we do not require knowledge of the set of inputs a priori to inference time, making our method more appropriate for online, open-set tasks"
  - [section 2] "Let D_ref = {(m_i, a_i)}_{i=1}^N consisting of N images with labeled attributes... We focus on both the image retrieval and zero-shot classification setting"
  - [corpus] Moderate - some related work addresses open-set scenarios but not with this specific reference dataset approach
- **Break condition:** If the attribute-label relationship varies significantly across classes, or if the reference dataset is too small to represent all possible query classes.

## Foundational Learning

- **Concept:** Linear algebra (projection matrices, constrained optimization)
  - **Why needed here:** The method relies on constructing projection matrices to remove attribute directions and solving constrained optimization problems to equalize similarity scores
  - **Quick check question:** Given a matrix A, how do you construct the orthogonal projection matrix that projects onto the subspace orthogonal to A's columns?

- **Concept:** Vision-language model embeddings and similarity metrics
  - **Why needed here:** Understanding how VLMs like CLIP represent images and text in a joint embedding space, and how cosine similarity is used for zero-shot tasks
  - **Quick check question:** What is the relationship between cosine similarity and the dot product of normalized embeddings?

- **Concept:** Bias measurement metrics (KL divergence, MaxSkew)
  - **Why needed here:** Evaluating the effectiveness of debiasing requires understanding how to measure bias in retrieval distributions
  - **Quick check question:** How does KL divergence between attribute distributions before and after debiasing indicate bias reduction?

## Architecture Onboarding

- **Component map:**
  ATTRIBUTE_AUGMENT module (LLM or text processing) -> Local attribute subspace construction -> Orthogonal projection step -> Reference image selection and distance equalization -> Downstream task interface

- **Critical path:**
  1. Receive query embedding from VLM
  2. Generate augmented queries for attribute subspace
  3. Construct local attribute subspace and apply orthogonal projection
  4. Select reference images and solve constrained optimization
  5. Output debiased embedding to downstream task

- **Design tradeoffs:**
  - Accuracy vs bias reduction: More aggressive debiasing may hurt accuracy
  - Reference dataset size vs effectiveness: Larger datasets may improve debiasing but increase computation
  - n value for image selection: Higher n may improve stability but increase computation

- **Failure signatures:**
  - KL divergence or MaxSkew not improving: Issues with reference dataset quality or optimization
  - Accuracy significantly dropping: Over-aggressive debiasing or poor subspace estimation
  - Slow processing: Inefficient reference image selection or optimization solving

- **First 3 experiments:**
  1. Test debiasing on a simple binary attribute dataset (e.g., gender on CELEB-A) with known classes to verify accuracy preservation
  2. Measure bias reduction metrics (KL divergence, MaxSkew) on stereotype queries to verify bias mitigation
  3. Evaluate on an open-set scenario with unseen classes to verify generalization capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does BEND-VLM's effectiveness generalize to datasets with more than binary gender attributes or intersectional protected attributes?
- Basis in paper: [explicit] The paper mentions that the current evaluation datasets contain only binary gender labels and that the race and gender labels are not from self-identification.
- Why unresolved: The current experiments are limited to binary gender and race attributes, leaving uncertainty about performance with non-binary or intersectional attributes.
- What evidence would resolve it: Testing BEND-VLM on datasets with non-binary gender labels and intersectional protected attributes, measuring performance metrics across these categories.

### Open Question 2
- Question: How does BEND-VLM's computational efficiency scale with larger reference datasets and more complex protected attribute categories?
- Basis in paper: [inferred] The paper mentions that the method requires a reference dataset with protected attribute annotations and that the ATTRIBUTE AUGMENT module uses a relatively small 7B LLM, which could incur computational overhead.
- Why unresolved: The paper doesn't provide detailed analysis of computational scaling with dataset size or attribute complexity.
- What evidence would resolve it: Runtime analysis of BEND-VLM with varying reference dataset sizes and attribute category counts, comparing to baseline methods.

### Open Question 3
- Question: What is the optimal strategy for selecting the number of relevant images (n) when constructing Dref(ai, c) for different query types?
- Basis in paper: [explicit] The paper mentions that n could be found using change-point detection but uses a fixed hyperparameter approach in practice.
- Why unresolved: The paper doesn't systematically evaluate different strategies for choosing n or analyze how this choice affects debiasing performance.
- What evidence would resolve it: Comparative analysis of different n selection strategies (change-point detection vs fixed values) across various query types and protected attributes.

## Limitations

- The method's effectiveness depends on having a representative reference dataset with comprehensive attribute coverage, which may not be available for all domains
- The computational cost of solving constrained optimization problems for each query could be prohibitive for real-time applications
- The approach may not generalize well to intersectional biases (multiple attributes simultaneously) since it processes attributes sequentially

## Confidence

- **High confidence:** The mathematical formulation of the two-step debiasing approach (orthogonal projection and constrained optimization) is sound and well-defined
- **Medium confidence:** The empirical results showing bias reduction and accuracy preservation, though limited to specific datasets and attribute combinations
- **Low confidence:** The generalizability to other VLMs beyond CLIP, and the method's performance on highly intersectional bias scenarios

## Next Checks

1. **Ablation study on n value:** Systematically vary the number of reference images (n) selected per attribute group and measure the impact on both bias metrics and computational efficiency to find optimal tradeoffs

2. **Intersectional bias testing:** Evaluate the method's performance when debiasing multiple attributes simultaneously (e.g., gender AND race) to identify potential negative interactions between sequential debiasing steps

3. **Cross-VLM validation:** Apply the same debiasing pipeline to embeddings from a different VLM architecture (e.g., BLIP-2 or Flamingo) to assess whether the method's effectiveness is CLIP-specific or generalizes to other models