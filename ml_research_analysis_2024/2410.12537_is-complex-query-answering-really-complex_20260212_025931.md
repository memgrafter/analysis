---
ver: rpa2
title: Is Complex Query Answering Really Complex?
arxiv_id: '2410.12537'
source_url: https://arxiv.org/abs/2410.12537
tags:
- query
- queries
- complex
- cqd-hybrid
- pairs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper critically reexamines the perceived complexity of complex
  query answering (CQA) on knowledge graphs by analyzing existing benchmarks. The
  authors demonstrate that the majority of queries in standard benchmarks (up to 98%)
  can be reduced to simpler tasks like link prediction due to training data leakage,
  inflating reported performance.
---

# Is Complex Query Answering Really Complex?

## Quick Facts
- arXiv ID: 2410.12537
- Source URL: https://arxiv.org/abs/2410.12537
- Reference count: 40
- Authors show that existing CQA benchmarks are largely solvable through link prediction, undermining claims of complex reasoning

## Executive Summary
This paper challenges the prevailing narrative that significant progress has been made on complex query answering over knowledge graphs. Through systematic analysis of three major benchmarks (FB15k237, NELL995, and ICEWS18), the authors demonstrate that up to 98% of queries can be solved using simple link prediction techniques due to extensive training data leakage. To address this benchmarking problem, they propose new benchmarks with balanced distributions of query difficulty, including both partial-inference and full-inference queries. Their evaluation reveals that state-of-the-art methods experience substantial performance drops on these new benchmarks, indicating that previous progress was largely based on easy query types. The work provides a more accurate assessment of CQA model capabilities and calls for better benchmarking practices in the field.

## Method Summary
The authors systematically analyze existing CQA benchmarks by decomposing queries into sub-queries and checking for overlaps with training data. They identify that most queries in standard benchmarks can be reduced to simple link prediction tasks, creating an illusion of complex reasoning. To address this, they construct new benchmarks (FB15k237+H, NELL995+H, ICEWS18+H) with balanced distributions of query types, including partial-inference queries (solvable through link prediction) and full-inference queries (requiring genuine multi-hop reasoning). They evaluate state-of-the-art CQA methods on these new benchmarks and introduce CQD-Hybrid, a simple hybrid solver that combines neural link prediction with training data memorization to achieve state-of-the-art performance.

## Key Results
- Up to 98% of queries in standard CQA benchmarks can be solved using simple link prediction due to training data leakage
- New benchmarks with balanced query difficulty reveal significant performance drops for state-of-the-art methods
- CQD-Hybrid, a simple hybrid solver combining neural link prediction and memorization, outperforms existing models
- The performance gap between partial-inference and full-inference queries demonstrates the limited reasoning capabilities of current methods

## Why This Works (Mechanism)
The core mechanism relies on identifying and eliminating training data leakage in CQA benchmarks. By analyzing how queries decompose into sub-queries that overlap with training data, the authors reveal that most "complex" queries are actually simple link prediction problems in disguise. The new benchmarks address this by ensuring a balanced distribution of query types, forcing models to demonstrate genuine multi-hop reasoning capabilities rather than exploiting data leakage.

## Foundational Learning
- **Query decomposition**: Breaking complex queries into simpler sub-queries to analyze their structure and dependencies
  - Why needed: Essential for understanding query complexity and identifying training data overlap
  - Quick check: Can decompose a conjunctive query into individual triple patterns

- **Training data leakage analysis**: Checking if sub-queries overlap with training data to identify benchmark contamination
  - Why needed: Critical for ensuring benchmarks actually test intended capabilities
  - Quick check: Can verify if a query can be solved using only training data information

- **Partial vs full inference classification**: Distinguishing between queries solvable by link prediction alone versus those requiring multi-hop reasoning
  - Why needed: Necessary for creating balanced benchmarks that test different reasoning capabilities
  - Quick check: Can categorize queries based on their minimum inference requirements

## Architecture Onboarding

**Component Map**: Query Parser -> Decomposition Analyzer -> Training Data Checker -> Benchmark Generator -> Model Evaluator

**Critical Path**: Query decomposition → Training data overlap detection → Benchmark construction → Model evaluation → Performance analysis

**Design Tradeoffs**: The main tradeoff involves balancing benchmark difficulty (requiring more complex queries) against dataset size and practical usability. More complex queries typically have fewer training examples, making learning harder but more meaningful.

**Failure Signatures**: If performance on new benchmarks remains high, it indicates either (1) insufficient data leakage elimination, (2) models have learned genuine multi-hop reasoning, or (3) the benchmark construction process missed edge cases.

**Three First Experiments**:
1. Apply the decomposition and leakage analysis to an additional knowledge graph benchmark to verify generalizability
2. Perform ablation studies on CQD-Hybrid to isolate the contributions of neural link prediction versus memorization components
3. Compare performance distributions between partial-inference and full-inference queries to quantify the reasoning gap

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- The analysis depends on assumptions about what constitutes "true" complex reasoning versus simple memorization
- Manual analysis of query complexity may contain subjective elements in classification
- The effectiveness of the hybrid solver raises questions about whether improvements represent genuine reasoning advancement
- Results may not generalize to all knowledge graph domains and query types

## Confidence

- **High Confidence**: Empirical evidence showing performance drops on new benchmarks is robust
- **Medium Confidence**: Characterization of "true" complex query difficulty and proposed taxonomy
- **Medium Confidence**: Claim that current research has made limited progress on genuinely complex queries

## Next Checks

1. Apply the same analysis methodology to additional knowledge graph benchmarks beyond the three studied to verify if the 98% leakage finding generalizes

2. Systematically isolate and measure the contributions of neural link prediction versus memorization components in CQD-Hybrid

3. Conduct expert annotation of query difficulty and complexity to validate the automated classification scheme