---
ver: rpa2
title: Can Large Language Models Understand DL-Lite Ontologies? An Empirical Study
arxiv_id: '2406.17532'
source_url: https://arxiv.org/abs/2406.17532
tags:
- llms
- ontology
- dl-lite
- ontologies
- roles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether large language models (LLMs) can
  understand DL-Lite ontologies, a formal representation of structured knowledge with
  model-theoretic semantics. The authors design a comprehensive evaluation framework
  covering six tasks across syntactic and semantic aspects: syntax checking, subsumption
  of concepts or roles, instance checking, query answering, ontology satisfiability
  checking, and property characteristics probing.'
---

# Can Large Language Models Understand DL-Lite Ontologies? An Empirical Study

## Quick Facts
- **arXiv ID**: 2406.17532
- **Source URL**: https://arxiv.org/abs/2406.17532
- **Reference count**: 31
- **Primary result**: LLMs demonstrate strong capability in understanding DL-Lite syntax and semantics of concepts, roles, and some property characteristics, but struggle with TBox NI transitivity rules and handling ontologies with large ABoxes.

## Executive Summary
This paper investigates whether large language models can understand DL-Lite ontologies, a formal representation of structured knowledge with model-theoretic semantics. The authors design a comprehensive evaluation framework covering six tasks across syntactic and semantic aspects including syntax checking, subsumption of concepts or roles, instance checking, query answering, ontology satisfiability checking, and property characteristics probing. Using datasets from five real-world DL ontologies and five UOBM benchmarks, they test GPT-3.5, GPT-4o, and LLaMA3-8B with carefully designed prompts.

Results show that LLMs demonstrate strong capability in understanding DL-Lite syntax and semantics of concepts, roles, and some property characteristics. However, they struggle with TBox NI transitivity rules and handling ontologies with large ABoxes. The findings highlight both the potential and limitations of LLMs in understanding formal ontologies, providing valuable insights for future knowledge engineering solutions.

## Method Summary
The authors developed a comprehensive evaluation framework to test LLM understanding of DL-Lite ontologies across six tasks: syntax checking, concept subsumption, role subsumption, instance checking, query answering, and ontology satisfiability. They created datasets from five real-world DL ontologies and five UOBM benchmarks, then tested three LLM models (GPT-3.5, GPT-4o, and LLaMA3-8B) using carefully designed prompts. The evaluation measured both accuracy and reasoning capability across different aspects of DL-Lite semantics, with particular attention to how LLMs handle formal logic constructs versus natural language descriptions.

## Key Results
- LLMs show strong performance in understanding DL-Lite syntax and semantics of concepts, roles, and some property characteristics
- LLMs struggle significantly with TBox NI transitivity rules and ontologies containing large ABoxes
- Performance varies across different LLM models, with GPT-4o generally outperforming GPT-3.5 and LLaMA3-8B

## Why This Works (Mechanism)
The study demonstrates that LLMs can leverage their pattern recognition capabilities to understand formal logical structures when presented with appropriate prompts. Their success with syntactic tasks suggests they can effectively parse and validate formal ontology syntax. For semantic tasks, LLMs appear to use their reasoning capabilities to infer relationships between concepts and roles, though their performance degrades with more complex logical constructs like transitivity rules.

## Foundational Learning
- **DL-Lite ontology structure**: Understanding the distinction between TBox (terminological knowledge) and ABox (assertional knowledge) is crucial for interpreting evaluation results
  - Why needed: Results show different performance patterns for TBox vs ABox handling
  - Quick check: Can identify TBox vs ABox components in a given ontology

- **Model-theoretic semantics**: LLMs must map formal logical constructs to their semantic interpretations
  - Why needed: Evaluation tasks require understanding how logical statements translate to semantic relationships
  - Quick check: Can explain the semantic meaning of a given DL-Lite axiom

- **Property characteristics**: Understanding reflexive, symmetric, transitive, and other property characteristics is essential for evaluating LLM performance
  - Why needed: Results show varying success rates across different property types
  - Quick check: Can identify and explain property characteristics in given axioms

## Architecture Onboarding

**Component Map**: Ontology -> Prompt Engineering -> LLM Model -> Output Evaluation -> Performance Metrics

**Critical Path**: The critical path involves converting DL-Lite ontology constructs into natural language prompts that preserve formal semantics, then mapping LLM outputs back to formal logic evaluations. This conversion process is crucial for maintaining the integrity of formal reasoning while leveraging LLM capabilities.

**Design Tradeoffs**: The study balances between preserving formal DL-Lite semantics in prompts versus making them accessible to LLMs' natural language processing capabilities. Using carefully crafted prompts that maintain logical structure while being LLM-friendly represents a key tradeoff.

**Failure Signatures**: LLMs show consistent failure patterns with TBox NI transitivity rules and large ABoxes, suggesting limitations in handling complex logical chains and large-scale assertional data. Performance degradation with increasing ontology size indicates scalability challenges.

**First 3 Experiments**:
1. Test additional LLM models including fine-tuned variants to establish broader performance patterns
2. Evaluate performance on larger and more diverse ontology datasets to assess scalability limitations
3. Conduct ablation studies on prompt formulations to determine the impact of different interaction patterns on LLM performance

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation focuses on three specific LLM models, limiting generalizability to other architectures
- Dataset composition may not fully represent the diversity of DL-Lite applications across different domains
- Prompt engineering approach relies on specific formulations that may not capture all potential interaction patterns

## Confidence

**High**: LLMs' syntactic understanding capabilities are well-established through consistent performance across multiple tests and ontology types.

**Medium**: Semantic reasoning abilities, particularly for concept and role subsumption, show reliable patterns but with some variability across models and tasks.

**Low**: Performance with large ABoxes and TBox transitivity rules is poorly understood due to limited testing scope and inconsistent results.

## Next Checks

1. Test additional LLM models including fine-tuned variants to establish broader performance patterns and identify whether current limitations are model-specific or fundamental to LLM-architecture approaches.

2. Evaluate performance on larger and more diverse ontology datasets to assess scalability limitations and determine whether current performance patterns hold under different domain conditions and ontology sizes.

3. Conduct ablation studies on prompt formulations to determine the impact of different interaction patterns on LLM performance, identifying optimal prompt structures for different types of DL-Lite reasoning tasks.