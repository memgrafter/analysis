---
ver: rpa2
title: Non-Determinism of "Deterministic" LLM Settings
arxiv_id: '2408.04667'
source_url: https://arxiv.org/abs/2408.04667
tags:
- tasks
- llms
- output
- same
- tarr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates non-determinism in large language models
  (LLMs) despite deterministic configurations, using TAR metrics to quantify output
  stability. Experiments on 8 tasks with 5 LLM models show accuracy variations up
  to 15% and performance spread up to 70%, with no model achieving perfect stability.
---

# Non-Determinism of "Deterministic" LLM Settings

## Quick Facts
- arXiv ID: 2408.04667
- Source URL: https://arxiv.org/abs/2408.04667
- Reference count: 8
- Accuracy variations up to 15% and performance spread up to 70% despite temperature=0 and top-p=1 settings

## Executive Summary
This study reveals significant non-determinism in large language models despite configurations that should theoretically produce deterministic outputs. Through experiments on 8 tasks using 5 major LLM models, researchers found accuracy variations up to 15% and performance spread up to 70%, with no model achieving perfect stability. The introduction of TAR (Task Answer Reproducibility) metrics - both TARr and TARa - provides new quantitative measures for assessing output stability across different task types.

The findings demonstrate that non-determinism persists even when using temperature=0 and top-p=1 settings, challenging assumptions about deterministic LLM behavior. This has critical implications for developers building downstream systems and unit testing frameworks that assume consistent model outputs. The research highlights the need for the AI development community to handle non-deterministic AI results gracefully in production systems.

## Method Summary
The researchers conducted experiments using 8 tasks from the HumanEval benchmark across 5 LLM models (GPT-3.5-turbo, GPT-4, Claude-3-Haiku, Claude-3-Sonnet, Claude-3-Opus). They measured output stability through newly introduced TAR metrics that quantify reproducibility of model responses. The experiments specifically tested models under temperature=0 and top-p=1 configurations, which should theoretically produce deterministic outputs. Task-specific stability was analyzed across different domains, comparing European history tasks against college mathematics problems to identify patterns in non-deterministic behavior.

## Key Results
- Non-determinism observed with accuracy variations up to 15% and performance spread up to 70%
- No model achieved perfect stability across all tested configurations
- European history tasks showed greater stability than college mathematics problems
- TARr and TARa metrics successfully quantified task-specific stability differences

## Why This Works (Mechanism)
Non-determinism in supposedly deterministic LLM settings arises from multiple sources including floating-point precision variations, parallel processing effects, hardware differences, and inherent stochasticity in the sampling algorithms themselves. Even when temperature and top-p parameters are set to their deterministic extremes, the underlying inference engines may introduce subtle variations through parallel computation, GPU scheduling, or numerical precision handling that propagate to final outputs.

## Foundational Learning
- TAR metrics (why needed: to quantify output stability; quick check: can reproduce stability scores within 5% tolerance)
- Temperature and top-p parameters (why needed: core LLM configuration settings; quick check: understand how these affect probability distributions)
- Task-specific variability (why needed: different domains have different stability profiles; quick check: can identify which task types are more stable)
- Model family differences (why needed: understand how architecture affects determinism; quick check: can compare stability across model types)

## Architecture Onboarding
- Component map: Task input -> LLM inference engine -> Output generation -> TAR metric calculation -> Stability assessment
- Critical path: Model configuration (temp=0, top-p=1) -> Multiple inference runs -> Output comparison -> TAR calculation
- Design tradeoffs: Deterministic settings vs. computational efficiency vs. output quality
- Failure signatures: Inconsistent outputs across identical prompts, varying accuracy scores, unstable performance metrics
- First experiments: 1) Test single model across multiple seeds, 2) Compare stability across different task types, 3) Measure impact of hardware variations

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow model coverage (only OpenAI and Anthropic models)
- Limited task diversity (single HumanEval benchmark)
- TAR metrics lack external validation
- No analysis of open-source or smaller specialized models

## Confidence
High confidence in: The empirical observation that non-determinism exists even with temperature=0 and top-p=1 across the tested models and tasks.
Medium confidence in: The claim that non-determinism impacts downstream systems and unit testing, as this is inferred rather than directly measured in the study.
Low confidence in: The specific TAR thresholds for "stability" (TARr ≥ 0.9 or TARa ≤ 0.1) being universally applicable, as these cutoffs appear somewhat arbitrary without broader validation.

## Next Checks
1. Replicate the experiments across additional model families (including open-source models like Llama, Mistral, and smaller specialized models) to test if non-determinism patterns hold across different architectures and training approaches.
2. Extend testing to domain-specific tasks beyond the current programming-focused HumanEval benchmark, including medical, legal, and creative writing domains, to map how task characteristics influence non-determinism.
3. Implement controlled experiments measuring the actual impact of non-determinism on downstream applications by building identical systems with different random seeds and measuring functional differences in real-world scenarios.