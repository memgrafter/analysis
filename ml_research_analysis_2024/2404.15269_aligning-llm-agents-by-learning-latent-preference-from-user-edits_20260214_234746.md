---
ver: rpa2
title: Aligning LLM Agents by Learning Latent Preference from User Edits
arxiv_id: '2404.15269'
source_url: https://arxiv.org/abs/2404.15269
tags:
- user
- preference
- edits
- agent
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PRELUDE, a framework for aligning LLM-based
  agents with user preferences through learning from user edits. The key idea is to
  infer textual descriptions of user preferences from edit history and use these descriptions
  as prompts to generate aligned responses.
---

# Aligning LLM Agents by Learning Latent Preference from User Edits

## Quick Facts
- arXiv ID: 2404.15269
- Source URL: https://arxiv.org/abs/2404.15269
- Authors: Ge Gao; Alexey Taymanov; Eduardo Salinas; Paul Mineiro; Dipendra Misra
- Reference count: 22
- Key outcome: CIPHER reduces user edit costs through preference description learning while maintaining low computational expense

## Executive Summary
This paper introduces PRELUDE, a framework for aligning LLM-based agents with user preferences through learning from user edits. The key innovation is CIPHER, which infers short textual descriptions of user preferences from edit history and uses these descriptions as prompts to generate aligned responses. By avoiding the use of entire user edits as context, CIPHER achieves significant reductions in computational expense while maintaining or improving performance on user alignment tasks. Experiments on interactive writing assistant tasks demonstrate that CIPHER significantly reduces user edit costs compared to baselines that use raw user edits or other personalization methods.

## Method Summary
PRELUDE addresses the challenge of aligning LLM agents with user preferences in interactive settings by learning descriptive preferences from user edits rather than using the edits directly as context. The CIPHER algorithm works by inferring short preference descriptions from pairs of agent-generated responses and user edits, storing these in a history with context representations, and retrieving/agglomerating preferences from similar historical contexts for future response generation. This approach avoids the computational expense of using entire user edits as context while capturing the essential user preference. The framework uses an LLM to both infer preferences from edit pairs and aggregate retrieved preferences for the current context, with a context representation function enabling efficient similarity-based retrieval from historical data.

## Key Results
- CIPHER significantly reduces user edit costs compared to baselines using raw user edits as context
- Learned preference descriptions show significant similarity to ground truth preferences
- CIPHER maintains lower computational expense through shorter prompts than methods using entire user edits
- The approach outperforms explore-then-exploit and continual learning methods on preference classification accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CIPHER achieves lower edit distance cost than baselines by learning descriptive user preferences instead of using raw user edits in the prompt.
- Mechanism: CIPHER infers short preference descriptions from user edits, stores these in a history, and retrieves/agglomerates preferences from similar contexts for future response generation. This avoids the computational expense of using entire user edits as context.
- Core assumption: Short descriptive preferences can capture the essential user preference as well as raw user edits, while significantly reducing prompt length.
- Evidence anchors:
  - [abstract]: "CIPHER significantly reduces user edit costs compared to baselines, while maintaining low computational expense."
  - [section 3]: "A key advantage of CIPHER is that it typically leads to significantly shorter prompts compared to other retrieval methods that use the entire documents or context, as inferred preferences are much shorter than retrieved documents or contexts."
  - [corpus]: Weak. Related work focuses on fine-tuning or using user edits directly, not on preference description learning.
- Break condition: If descriptive preferences cannot capture the full nuance of user edits, or if the LLM cannot reliably infer preferences from edit pairs.

### Mechanism 2
- Claim: CIPHER achieves higher preference classification accuracy by retrieving and aggregating preferences from similar historical contexts.
- Mechanism: CIPHER uses a context representation function to find the k-nearest historical contexts, retrieves their inferred preferences, and queries the LLM to aggregate these into a single preference for the current context.
- Core assumption: Similar contexts have similar user preferences, and aggregating preferences from similar contexts improves preference inference.
- Evidence anchors:
  - [abstract]: "Experiments on two interactive writing assistant tasks show that CIPHER significantly reduces user edit costs compared to baselines."
  - [section 4.2]: "ICL-edit baselines perform significantly better on the summarization task... However, using a list of user edits in the prompt results in a higher token expense cost."
  - [corpus]: Weak. Related work uses ICL or direct fine-tuning, not preference aggregation from similar contexts.
- Break condition: If the context representation function fails to capture similarity relevant to user preference, or if aggregation of preferences from similar contexts does not improve preference inference.

### Mechanism 3
- Claim: CIPHER achieves lower computational expense than baselines by using short preference descriptions instead of long user edits in the prompt.
- Mechanism: CIPHER stores only context representations and short preference descriptions in the history, reducing memory storage and prompt length compared to storing entire documents and user edits.
- Core assumption: Storing and using short preference descriptions is sufficient for response generation and is more efficient than storing and using entire documents and user edits.
- Evidence anchors:
  - [abstract]: "CIPHER has a lower computational expense, as using learned preference results in a shorter prompt than directly using user edits."
  - [section 3]: "CIPHER further reduces the memory storage by only storing the representation of contexts in the preference string instead of the input itself."
  - [corpus]: Weak. Related work does not focus on computational efficiency of preference learning methods.
- Break condition: If short preference descriptions are insufficient for response generation, or if the computational savings are outweighed by the cost of preference inference and aggregation.

## Foundational Learning

- Concept: Interactive learning from user edits
  - Why needed here: The paper studies learning from user edits to align LLM agents with user preferences in interactive settings like writing assistants.
  - Quick check question: What is the difference between user edits and traditional preference feedback like pairwise comparisons in RLHF?

- Concept: Preference learning and representation
  - Why needed here: The paper proposes learning descriptive preferences from user edits and using these descriptions to drive response generation, instead of fine-tuning the LLM.
  - Quick check question: How does learning descriptive preferences differ from learning a reward model in RLHF?

- Concept: Retrieval and aggregation of preferences
  - Why needed here: CIPHER retrieves preferences from similar historical contexts and aggregates them to infer a preference for the current context.
  - Quick check question: Why might aggregating preferences from similar contexts improve preference inference compared to using a single retrieved preference?

## Architecture Onboarding

- Component map: LLM agent -> Preference inference module -> Context representation function -> Preference history -> Aggregation module
- Critical path: User provides context → CIPHER retrieves and aggregates preferences → LLM generates response → User edits response → CIPHER infers new preference and updates history
- Design tradeoffs:
  - Short vs. long preference descriptions: Short descriptions are more efficient but may lose nuance
  - Number of retrieved preferences (k): More preferences may improve aggregation but increase computation
  - Context representation function: Different functions may capture similarity relevant to user preference differently
- Failure signatures:
  - High edit distance: Preference inference or aggregation is not capturing user preference well
  - Low preference classification accuracy: Context representation function is not capturing similarity relevant to user preference
  - High computational expense: Preference descriptions are not short enough, or retrieval/aggregation is too expensive
- First 3 experiments:
  1. Run CIPHER with k=1 and k=5 to see the effect of the number of retrieved preferences on performance
  2. Compare CIPHER with different context representation functions (e.g., MPNET vs. BERT) to see their effect on preference classification accuracy
  3. Analyze failure cases to understand when and why CIPHER fails to capture user preference

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CIPHER scale with the number of users and interactions?
- Basis in paper: [inferred] The paper discusses the challenges of scaling fine-tuning approaches with the number of users, but does not directly address how CIPHER scales with user interactions.
- Why unresolved: The paper only presents results for a fixed number of interactions (T=200) and does not explore how performance changes as the number of interactions grows over time or across different users.
- What evidence would resolve it: Experiments varying the number of interactions and users, and analyzing how CIPHER's performance metrics (edit distance, preference accuracy, computational cost) change with scale.

### Open Question 2
- Question: How robust is CIPHER to changes in user preferences over time (non-stationary preferences)?
- Basis in paper: [explicit] The paper mentions that user preferences can evolve with time but does not test CIPHER's performance under changing preferences.
- Why unresolved: The experiments use a simulated user with fixed preferences based on document source, not accounting for potential shifts in user preferences over the course of interactions.
- What evidence would resolve it: Experiments where the simulated user's preferences change over time, and measuring how quickly CIPHER adapts to these changes while maintaining low edit costs.

### Open Question 3
- Question: How does CIPHER perform when user preferences depend on information not available in the context (partially observed preferences)?
- Basis in paper: [explicit] The paper acknowledges that user preferences can depend on information unavailable in the context but does not test this scenario.
- Why unresolved: All experiments use document content as the sole context, not testing how CIPHER handles additional contextual information that may influence user preferences.
- What evidence would resolve it: Experiments incorporating additional contextual information (e.g., time of day, user's location, recent interactions) and measuring how well CIPHER learns preferences that depend on this information.

### Open Question 4
- Question: How sensitive is CIPHER's performance to the choice of context representation function (ϕ) and retrieval hyperparameters (k)?
- Basis in paper: [explicit] The paper experiments with different ϕ functions (MPNET, BERT) and k values (1, 5), but does not provide a comprehensive sensitivity analysis.
- Why unresolved: The paper only tests a limited set of ϕ functions and k values, and does not explore the full space of possible choices or their impact on performance.
- What evidence would resolve it: Systematic experiments varying ϕ functions and k values across a wider range, and analyzing how these choices affect CIPHER's edit distance, preference accuracy, and computational cost.

### Open Question 5
- Question: How does CIPHER compare to other personalization methods that do not rely on user edits (e.g., supervised learning from demonstrations, RL from preference feedback)?
- Basis in paper: [inferred] The paper focuses on learning from user edits but does not compare CIPHER to other personalization approaches that use different forms of feedback.
- Why unresolved: The experiments only compare CIPHER to baselines that also learn from user edits, not to methods that use other types of feedback such as demonstrations or explicit preference rankings.
- What evidence would resolve it: Experiments comparing CIPHER's performance to personalization methods that use demonstrations or preference feedback, measuring edit distance, preference accuracy, and computational cost across tasks.

## Limitations
- Evaluation relies entirely on simulated user edits rather than real human interactions, potentially missing real-world complexity
- Context representation functions lack detailed specification of model versions and hyperparameters, hindering exact reproduction
- Paper does not provide error analysis for failure cases, limiting understanding of when and why CIPHER fails

## Confidence
- **High confidence**: The core claim that CIPHER reduces computational expense by using short preference descriptions instead of full user edits is well-supported by both theoretical arguments and experimental results. The demonstration that learned preferences show significant similarity to ground truth preferences is also robust.
- **Medium confidence**: The claim that CIPHER significantly reduces edit distance costs compared to baselines is supported by experiments, but the simulation-based evaluation introduces uncertainty about real-world performance. The aggregation mechanism's effectiveness is demonstrated but could benefit from more ablation studies.
- **Low confidence**: The paper's claims about computational efficiency relative to fine-tuning approaches are difficult to verify without implementation details, and the trade-offs between short preference descriptions and nuance preservation are not thoroughly explored.

## Next Checks
1. Implement a human-in-the-loop study to validate that CIPHER's learned preferences align with actual human preferences across diverse user populations and writing tasks.
2. Conduct an ablation study varying the number of retrieved preferences (k) and different context representation functions to quantify their impact on preference classification accuracy and edit distance costs.
3. Perform a failure mode analysis by systematically identifying cases where CIPHER produces incorrect preferences, analyzing whether failures stem from context representation, preference inference, or aggregation issues.