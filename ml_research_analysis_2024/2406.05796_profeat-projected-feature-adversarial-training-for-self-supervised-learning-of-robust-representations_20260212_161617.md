---
ver: rpa2
title: 'ProFeAT: Projected Feature Adversarial Training for Self-Supervised Learning
  of Robust Representations'
arxiv_id: '2406.05796'
source_url: https://arxiv.org/abs/2406.05796
tags:
- training
- teacher
- adversarial
- student
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of scaling self-supervised adversarial
  training (SSL-AT) to larger models by bridging the performance gap with supervised
  adversarial training. The core idea is to use a projection head in the student model
  during distillation from a self-supervised pretrained teacher, which allows the
  student to leverage weak supervision while learning distinct adversarial robust
  representations.
---

# ProFeAT: Projected Feature Adversarial Training for Self-Supervised Learning of Robust Representations

## Quick Facts
- arXiv ID: 2406.05796
- Source URL: https://arxiv.org/abs/2406.05796
- Reference count: 33
- Key outcome: ProFeAT achieves significant improvements in clean and robust accuracy for self-supervised adversarial training, outperforming DeACL by 3.5-8% in clean accuracy and ~3% in robust accuracy on CIFAR-10 and CIFAR-100 with WideResNet-34-10.

## Executive Summary
This paper addresses the challenge of scaling self-supervised adversarial training (SSL-AT) to larger models by bridging the performance gap with supervised adversarial training. The core idea is to use a projection head in the student model during distillation from a self-supervised pretrained teacher, which allows the student to leverage weak supervision while learning distinct adversarial robust representations. The authors propose appropriate attack and defense losses at both feature and projector spaces, along with a combination of weak and strong augmentations for the teacher and student respectively. The proposed method, ProFeAT, achieves significant improvements in both clean and robust accuracy compared to existing SSL-AT methods, especially for larger model capacities.

## Method Summary
ProFeAT uses a teacher-student distillation framework where a self-supervised pretrained teacher provides supervision to a student model during adversarial training. The key innovation is the use of a frozen projection head from the teacher in the student model, which isolates the impact of the distillation loss on learned features. The method employs appropriate attack and defense losses at both feature and projector spaces, and uses a combination of weak augmentations for the teacher and strong augmentations for the student to improve training data diversity without increasing complexity.

## Key Results
- ProFeAT outperforms the best baseline DeACL by 3.5-8% in clean accuracy and ~3% in robust accuracy on WideResNet-34-10 architecture
- Achieves on-par or improved performance compared to supervised TRADES method on CIFAR-10 and CIFAR-100 datasets
- Demonstrates effective scaling of SSL-AT to larger model capacities where previous methods struggled
- Shows improved robustness against GAMA, AutoAttack, and PGD-20 attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The projection head enables the student to learn adversarially robust representations distinct from the teacher by isolating the impact of distillation loss on learned features.
- Mechanism: By reusing the pretrained projection head from the teacher and freezing it during student training, the student can leverage weak supervision while preventing overfitting to the teacher's training objective. This allows the student to retain more generic features useful for downstream robust classification.
- Core assumption: The representations of standard and adversarially trained models are inherently different, and the projection head can isolate the impact of the distillation loss on learned representations.
- Evidence anchors:
  - [abstract]: "we propose to use a projection head at the student, that allows it to leverage weak supervision from the teacher while also being able to learn adversarially robust representations that are distinct from the teacher."
  - [section 4.1]: "we propose to utilize a projection head following the student backbone, so as to isolate the impact of the enforced loss on the learned representations."
- Break condition: If the projection head is not pretrained or is trainable, the student may overfit to the teacher's training objective.

### Mechanism 2
- Claim: Using weak augmentations for the teacher and strong augmentations for the student improves attack diversity while maintaining low training complexity.
- Mechanism: Strong augmentations increase training complexity in adversarial training, potentially reducing performance. By using weak augmentations for the teacher and strong augmentations for the student, the method increases attack diversity without significantly increasing training complexity.
- Core assumption: The domain shift between augmented train and unaugmented test set images, combined with the increased complexity of the adversarial training task, reduces the effectiveness of strong augmentations.
- Evidence anchors:
  - [abstract]: "alongside a combination of weak and strong augmentations for the teacher and student respectively, to improve the training data diversity without increasing the training complexity."
  - [section 4.2]: "we propose to use a combination of weak and strong augmentations as inputs to the teacher and student respectively."
- Break condition: If the same augmentations are used for both teacher and student, or if the augmentations are not appropriately chosen.

### Mechanism 3
- Claim: Appropriate attack and defense losses at feature and projector spaces enable robust representation learning while leveraging teacher supervision.
- Mechanism: The defense loss combines distillation loss on clean samples (enforced at projector space) and smoothness loss on adversarial samples (enforced at feature space). The attack loss maximizes smoothness loss in feature space and causes misalignment between teacher and student in projected space.
- Core assumption: The ideal locations for clean and adversarial losses are the projected and feature spaces, respectively, and a combination of these losses with appropriate weighting enables robust representation learning.
- Evidence anchors:
  - [abstract]: "We further propose appropriate attack and defense losses at the feature and projector, alongside a combination of weak and strong augmentations for the teacher and student respectively."
  - [section 4.2]: "We thus use a complimentary loss as a regularizer in the respective spaces. This results in a combination of losses at the feature and projector spaces."
- Break condition: If the losses are not appropriately weighted or applied at the correct spaces.

## Foundational Learning

- Concept: Self-supervised learning (SSL)
  - Why needed here: SSL is used to pretrain the teacher model, which provides supervision for the student model's adversarial training. Understanding SSL is crucial for grasping the motivation behind using a teacher-student distillation setting for self-supervised adversarial training.
  - Quick check question: What is the main difference between supervised and self-supervised learning, and how does this difference motivate the use of a teacher-student distillation setting for self-supervised adversarial training?

- Concept: Adversarial training
  - Why needed here: Adversarial training is used to train the student model to be robust against adversarial attacks. Understanding adversarial training is essential for comprehending the challenges in scaling self-supervised adversarial training to larger models and the proposed solutions.
  - Quick check question: How does adversarial training differ from standard training, and what are the main challenges in scaling self-supervised adversarial training to larger models?

- Concept: Distillation
  - Why needed here: Distillation is used to transfer knowledge from the teacher model to the student model. Understanding distillation is important for grasping the motivation behind using a teacher-student distillation setting and the proposed modifications to improve its effectiveness in the context of self-supervised adversarial training.
  - Quick check question: What is the main goal of distillation, and how does the proposed method modify the standard distillation setting to better suit the needs of self-supervised adversarial training?

## Architecture Onboarding

- Component map: Teacher model -> Frozen projection head -> Student model (with projection head) -> Defense losses (clean at projector, adversarial at feature) -> Attack losses (maximize smoothness, cause misalignment) -> Augmentations (weak for teacher, strong for student)

- Critical path:
  1. Pretrain teacher model using standard self-supervised learning (SimCLR) for 1000 epochs
  2. Freeze pretrained projection head from teacher and use in student model
  3. Train student using ProFeAT loss (Lf p + Lf) with Î²=8, 5-step PGD attack
  4. Use weak augmentations (Pad and Crop) for teacher, strong augmentations (AutoAugment) for student
  5. Evaluate student via linear probing on clean samples and robust accuracy tests

- Design tradeoffs:
  - Frozen vs. trainable projection head: Frozen prevents overfitting to teacher's training objective but may limit adaptation to downstream tasks
  - Weak vs. strong augmentations for teacher: Weak reduces complexity but may limit attack diversity; strong increases diversity but may reduce teacher supervision effectiveness
  - Loss weighting: Equal weighting may not be optimal for all tasks; requires hyperparameter tuning

- Failure signatures:
  - Poor clean accuracy: May indicate overfitting to teacher's training objective or insufficient attack diversity
  - Poor robust accuracy: May indicate ineffective attack generation or insufficient smoothness in student's loss surface
  - Unstable training: May indicate imbalanced loss weighting or inappropriate augmentation choices

- First 3 experiments:
  1. Reproduce baseline results using teacher-student distillation with frozen projection head but without proposed augmentations and attack/defense losses
  2. Introduce proposed augmentations (weak for teacher, strong for student) and evaluate impact on clean/robust accuracy
  3. Implement proposed attack and defense losses and evaluate their impact on clean/robust accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal architecture and configuration of the projector layer for different model capacities and datasets?
- Basis in paper: [explicit] The paper discusses the impact of projector architecture and configuration on performance, but does not determine the optimal configuration.
- Why unresolved: The optimal projector architecture and configuration likely depend on the specific model capacity and dataset used, requiring further research to determine these optimal configurations.
- What evidence would resolve it: A comprehensive study varying the projector architecture and configuration across different model capacities and datasets, evaluating the impact on performance.

### Open Question 2
- Question: How does the proposed method perform on other self-supervised training algorithms for the teacher model, and how does the ranking of SSL algorithms change with hyperparameter tuning?
- Basis in paper: [explicit] The paper compares the proposed method with different self-supervised training algorithms for the teacher, but uses default hyperparameters for most algorithms.
- Why unresolved: The ranking of SSL algorithms on final performance would likely change if pretraining SSL algorithms were used with appropriate hyperparameter tuning, requiring further research to determine the optimal SSL algorithm and hyperparameters for each specific setting.
- What evidence would resolve it: A fair comparison across different SSL pretraining algorithms using appropriate hyperparameter tuning for each algorithm, evaluating the impact on final student performance.

### Open Question 3
- Question: How does the proposed method scale to larger datasets and more complex tasks, and what are the limitations of the current approach?
- Basis in paper: [inferred] The paper demonstrates effectiveness on CIFAR-10 and CIFAR-100 datasets but does not explore performance on larger datasets or more complex tasks.
- Why unresolved: Scalability to larger datasets and more complex tasks is crucial for practical applicability, and further research is needed to determine limitations and potential improvements.
- What evidence would resolve it: Experiments on larger datasets and more complex tasks, evaluating performance and identifying limitations and potential improvements.

## Limitations

- The paper does not provide extensive ablation studies to isolate the impact of individual components (projection head, augmentation strategy, loss formulation) on overall performance
- Scalability to significantly larger models than WideResNet-34-10 is not thoroughly investigated
- Performance on datasets beyond CIFAR-10/100 is not demonstrated, limiting generalizability claims

## Confidence

- **High confidence**: The core claim that ProFeAT improves clean and robust accuracy compared to existing SSL-AT methods, and that using a projection head with appropriate losses and augmentations is beneficial
- **Medium confidence**: The specific mechanisms by which the projection head and augmentation strategy improve performance, as the paper provides limited ablation studies isolating these effects
- **Low confidence**: The scalability of ProFeAT to models significantly larger than WideResNet-34-10, and its performance on datasets beyond CIFAR-10/100

## Next Checks

1. Conduct ablation studies to isolate the impact of the projection head, augmentation strategy, and loss formulation on the student's clean and robust accuracy
2. Evaluate ProFeAT on larger models (e.g., ResNet-50, WideResNet-50-2) and different datasets (e.g., SVHN, ImageNet) to assess its scalability and generalizability
3. Compare ProFeAT's performance to other state-of-the-art self-supervised adversarial training methods (e.g., DeACL, SimSiam-AT) on a common benchmark to provide a more comprehensive evaluation of its effectiveness