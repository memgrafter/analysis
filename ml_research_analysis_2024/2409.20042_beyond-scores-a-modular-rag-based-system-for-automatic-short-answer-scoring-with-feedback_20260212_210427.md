---
ver: rpa2
title: 'Beyond Scores: A Modular RAG-Based System for Automatic Short Answer Scoring
  with Feedback'
arxiv_id: '2409.20042'
source_url: https://arxiv.org/abs/2409.20042
tags:
- feedback
- system
- answer
- llama3
- scoring
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a modular retrieval-augmented generation
  (RAG) system for automatic short answer scoring with feedback (ASAS-F). The system
  uses ColBERT to retrieve relevant examples from training data, which are then used
  as few-shot examples to guide large language models in generating both scores and
  detailed feedback.
---

# Beyond Scores: A Modular RAG-Based System for Automatic Short Answer Scoring with Feedback

## Quick Facts
- arXiv ID: 2409.20042
- Source URL: https://arxiv.org/abs/2409.20042
- Authors: Menna Fateen; Bo Wang; Tsunenori Mine
- Reference count: 24
- One-line primary result: RAG-based system achieves 9% improvement in scoring accuracy on unseen questions compared to fine-tuned baselines

## Executive Summary
This paper introduces a modular retrieval-augmented generation (RAG) system for automatic short answer scoring with feedback (ASAS-F). The system uses ColBERT to retrieve relevant examples from training data, which are then used as few-shot examples to guide large language models in generating both scores and detailed feedback. Unlike previous methods that rely on extensive fine-tuning or complex prompt engineering, this approach operates in zero-shot and few-shot settings without resource-intensive training. Experiments on the SAF dataset show that the RAG-based system achieves a 9% improvement in scoring accuracy on unseen questions compared to fine-tuned baselines.

## Method Summary
The system implements a modular DSPy-based approach that combines ColBERT retrieval with large language models for automatic short answer scoring and feedback generation. The method operates in zero-shot, few-shot, and RAG-enhanced settings without fine-tuning the LLMs. ColBERT retrieves the most similar student answers from the training set based on contextual token-level embeddings, providing relevant examples for the LLM to use as few-shot demonstrations. The DSPy framework automates prompt generation and refinement, eliminating the need for extensive manual prompt engineering. The system uses Mistral:7b and Llama3 variants as executors, with Llama3:70b serving as the prompt generator in some configurations.

## Key Results
- 9% improvement in scoring accuracy on unseen questions compared to fine-tuned baselines
- Zero-shot ASAS-F system outperforms all baselines in all metrics on unseen questions split
- RAG-enhanced approach with k=3 or k=5 retrieved examples shows consistent performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented generation improves ASAS-F performance by grounding LLM feedback in similar training examples.
- Mechanism: ColBERT retrieves top-k most similar student answers from the training set. These examples are used as few-shot demonstrations to guide the LLM in generating both scores and feedback, reducing reliance on extensive prompt engineering.
- Core assumption: Similar student answers contain contextually relevant information that helps the LLM produce more accurate and pedagogically sound feedback.
- Evidence anchors:
  - [abstract] "retrieve the most similar answers from a short answer scoring feedback dataset using the ColBERT retrieval model"
  - [section 3.4.1] "ColBERT encodes the input into a matrix of contextual token-level embeddings... allowing us to capture fine-grained similarities"
  - [corpus] Weak. No direct citation evidence found.
- Break condition: If retrieved examples are too dissimilar or irrelevant, the LLM may hallucinate or produce inaccurate feedback.

### Mechanism 2
- Claim: Modular DSPy framework enables zero-shot and few-shot ASAS-F without extensive prompt engineering.
- Mechanism: DSPy replaces manual prompt crafting with automated prompt generation using predefined modules (e.g., 'Predictor', 'Chain-Of-Thought') that process input/output types and generate instructions.
- Core assumption: Pre-trained LLMs embedded with domain knowledge can score and provide feedback without labeled data when guided by well-structured prompts.
- Evidence anchors:
  - [abstract] "design our system to be adaptable to various educational tasks without extensive prompt engineering using an automatic prompt generation framework"
  - [section 3.2] "we utilize DSPy... to automate prompt generation and refinement"
  - [corpus] Weak. No direct citation evidence found.
- Break condition: If DSPy fails to generate appropriate prompts, the system performance degrades to random or baseline levels.

### Mechanism 3
- Claim: Few-shot learning with retrieved examples outperforms fine-tuning on unseen questions.
- Mechanism: By providing contextually relevant examples via RAG, the LLM can generalize better to novel questions compared to models fine-tuned on limited datasets.
- Core assumption: The subjectivity of scoring benefits more from contextually relevant examples than from statistical patterns learned during fine-tuning.
- Evidence anchors:
  - [abstract] "improvement in scoring accuracy by 9% on unseen questions compared to fine-tuning"
  - [section 5.1.4] "in the UQ split... the zero-shot ASAS-F system is able to outperform all baselines in all metrics"
  - [corpus] Weak. No direct citation evidence found.
- Break condition: If retrieved examples come from different questions (UQ split), performance may degrade due to contextual mismatch.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: Combines information retrieval with generative models to provide context-rich examples for scoring and feedback
  - Quick check question: What is the primary benefit of using RAG in ASAS-F compared to pure generation?
  - Answer: It grounds the LLM's output in real, similar examples, improving accuracy and relevance

- Concept: ColBERT retrieval model
  - Why needed here: Efficiently retrieves contextually relevant examples using token-level embeddings
  - Quick check question: How does ColBERT differ from traditional single-vector retrieval methods?
  - Answer: ColBERT uses a matrix of contextual token-level embeddings instead of encoding the entire input into a single vector

- Concept: Zero-shot vs Few-shot learning
  - Why needed here: Determines how the system operates when labeled data is unavailable (zero-shot) or limited (few-shot)
  - Quick check question: What is the key difference between zero-shot and few-shot ASAS-F?
  - Answer: Zero-shot uses only pre-trained LLM knowledge, while few-shot incorporates a small number of labeled examples

## Architecture Onboarding

- Component map:
  - Student answer → ColBERT retriever → DSPy prompt generation → LLM processing → Score + Feedback output

- Critical path: Student answer → ColBERT retrieval → DSPy prompt generation → LLM processing → Score + Feedback output

- Design tradeoffs:
  - ColBERT vs. other retrievers: ColBERT offers better contextual matching but requires more storage
  - DSPy vs. manual prompts: DSPy reduces engineering effort but may produce less optimized prompts
  - Few-shot vs. zero-shot: Few-shot improves accuracy but requires some labeled data

- Failure signatures:
  - No output from typed predictors → fallback to normal predictors
  - Generated feedback doesn't match reference → possible hallucination or poor example retrieval
  - Scores inconsistent with human raters → calibration issue

- First 3 experiments:
  1. Test ColBERT retrieval accuracy on training set to ensure quality examples
  2. Evaluate DSPy prompt generation with simple input/output pairs
  3. Run zero-shot ASAS-F on a small subset to verify baseline functionality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the ASAS-F system vary across different educational domains beyond communication networks?
- Basis in paper: [inferred] The paper tests the system only on the SAF dataset focused on communication networks, but suggests the modular design should adapt to various educational tasks.
- Why unresolved: The study does not evaluate the system on datasets from other domains such as mathematics, science, or humanities to demonstrate cross-domain generalizability.
- What evidence would resolve it: Testing the ASAS-F system on multiple educational datasets from different domains and comparing performance metrics would demonstrate its adaptability and robustness across varied subject matters.

### Open Question 2
- Question: What is the optimal number of retrieved examples (k) for balancing feedback quality and computational efficiency in the RAG-based approach?
- Basis in paper: [explicit] The paper experiments with k=3 and k=5 examples but notes that increasing examples beyond a threshold does not necessarily improve feedback quality, and observes performance variations across different models and splits.
- Why unresolved: The study does not systematically explore a wider range of k values or analyze the trade-off between computational cost and feedback quality to determine an optimal k.
- What evidence would resolve it: Conducting experiments with a broader range of k values, measuring both feedback quality and computational resources used, would identify the optimal balance for practical deployment.

### Open Question 3
- Question: How can the evaluation of feedback quality be improved to reduce rater disagreement and better capture the educational value of feedback?
- Basis in paper: [explicit] The paper reports low inter-rater agreement (Krippendorff's alpha) for accuracy and clarity of feedback, and suggests the 5-point scale may be too granular.
- Why unresolved: The study identifies the issue of rater disagreement but does not propose or test alternative evaluation frameworks or scales to improve reliability and validity of feedback assessment.
- What evidence would resolve it: Developing and testing new feedback evaluation frameworks, such as simplified scales, rubrics, or automated metrics validated against human judgments, would enhance the assessment of feedback quality and reduce subjectivity.

## Limitations

- ColBERT retrieval quality is assumed but not independently validated through manual evaluation of retrieved examples
- DSPy framework operates as a black box with no ablation studies on prompt generation effectiveness
- Limited comparison to fine-tuned baselines without reporting computational costs or training time

## Confidence

**High confidence**: The modular system architecture and general methodology are well-described and reproducible. The distinction between zero-shot, few-shot, and RAG approaches is clearly articulated.

**Medium confidence**: The 9% improvement claim is based on specific dataset splits and evaluation metrics that may not generalize. The paper doesn't explore how sensitive these results are to different training/validation splits.

**Low confidence**: The superiority of ColBERT retrieval over other methods is asserted but not empirically demonstrated. The paper lacks ablation studies showing how individual components (ColBERT, DSPy, LLM choice) contribute to overall performance.

## Next Checks

1. **Retrieval quality validation**: Conduct a manual evaluation of top-5 ColBERT retrieved examples for random test questions to verify contextual relevance. Calculate precision@k for retrieved examples that would actually help an LLM generate better feedback.

2. **Ablation study on DSPy modules**: Systematically remove DSPy's automatic prompt generation and replace with manually crafted prompts to quantify the actual contribution of the DSPy framework versus the underlying LLM capabilities.

3. **Cross-dataset generalization**: Test the trained RAG-based system on a different ASAS dataset (if available) or evaluate performance on out-of-distribution questions within the SAF dataset to assess real-world robustness beyond the controlled unseen questions split.