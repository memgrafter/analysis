---
ver: rpa2
title: 'ESC-Eval: Evaluating Emotion Support Conversations in Large Language Models'
arxiv_id: '2406.14952'
source_url: https://arxiv.org/abs/2406.14952
tags:
- dialogue
- assistant
- role
- cards
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ESC-Eval addresses the challenge of evaluating emotion support
  conversations (ESC) in large language models (LLMs) by proposing a novel framework
  that uses role-playing agents to interact with ESC models, followed by manual evaluation
  of the resulting dialogues. The method involves reorganizing 2,801 role-playing
  cards from seven datasets to define roles for the agent, training a dedicated role-playing
  model called ESC-Role that behaves more like a confused person than GPT-4, and systematically
  conducting experiments using 14 LLMs as ESC models.
---

# ESC-Eval: Evaluating Emotion Support Conversations in Large Language Models

## Quick Facts
- arXiv ID: 2406.14952
- Source URL: https://arxiv.org/abs/2406.14952
- Reference count: 38
- Primary result: ESC-RANK automated scoring outperforms GPT-4 by 35 points in ESC evaluation accuracy

## Executive Summary
ESC-Eval introduces a novel framework for evaluating emotion support conversations (ESC) in large language models by using role-playing agents to simulate distressed users, followed by comprehensive human evaluation. The framework addresses the challenge of ESC evaluation by training a dedicated role-playing model (ESC-Role) that behaves more consistently like a confused person than general LLMs like GPT-4. Through systematic experiments with 14 LLMs, ESC-Eval demonstrates that domain-specific emotional support models significantly outperform general AI assistants in ESC tasks, though still fall short of human performance.

## Method Summary
ESC-Eval reconstructs role cards from seven existing ESC datasets using GPT-4 extraction and human filtering, creating a three-tier classification system covering 37 categories. A dedicated role-playing model (ESC-Role) is trained by fine-tuning Qwen1.5-14B-Chat on combined ESC dialogue data and general role-playing instruction data. The framework evaluates 14 LLMs through multi-turn dialogues with ESC-Role using 655 high-quality role cards, generating 8.5K dialogues that undergo comprehensive human annotation across seven dimensions. ESC-RANK is then developed by training InternLM2-7B-Chat on 59,654 manually annotated evaluation results to automate the scoring process.

## Key Results
- Domain-specific LLMs (ChatCounselor, EmoLLM) significantly outperform general LLMs (GPT-4, ChatGPT, Llama3) in ESC tasks
- ESC-RANK automated scoring achieves accuracy surpassing GPT-4 by 35 points
- Comprehensive human evaluation across seven dimensions reveals clear performance gaps between model groups and humans
- Role-playing agent ESC-Role demonstrates stronger human-like attributes than GPT-4 in domain-specific metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Role-playing agents can simulate distressed users more consistently than human evaluators.
- Mechanism: ESC-Eval trains a dedicated role-playing model (ESC-Role) on multi-turn emotional support dialogues and general role-playing datasets, enabling it to generate naturalistic, context-rich user personas that remain emotionally consistent across turns.
- Core assumption: A model fine-tuned on ESC data will behave more like a real distressed person than general LLMs (e.g., GPT-4).
- Evidence anchors:
  - [abstract]: "we train a specific role-playing model called ESC-Role which behaves more like a confused person than GPT-4."
  - [section 2.3.3]: "the trained ESC-Role not only demonstrates stronger human-like attributes in ESC’s domain-specific metrics but also shows impressive results in generic metrics."
  - [corpus]: Weak — the corpus analysis shows the ESC-Eval framework is novel, but lacks direct comparison studies of role-playing agents in ESC.
- Break condition: If the fine-tuned ESC-Role generates overly scripted or inconsistent emotional responses, the simulated user personas lose realism, undermining evaluation validity.

### Mechanism 2
- Claim: Automated scoring (ESC-RANK) can outperform GPT-4 in ESC evaluation accuracy.
- Mechanism: ESC-RANK is trained on 59,654 human-annotated dialogue evaluations, learning to predict human-assigned scores across seven dimensions, leveraging domain-specific features.
- Core assumption: Human annotations on large, diverse ESC dialogue datasets can train a scoring model to generalize beyond GPT-4’s general reasoning.
- Evidence anchors:
  - [abstract]: "we developed ESC-RANK, which trained on the annotated data, achieving a scoring performance surpassing 35 points of GPT-4."
  - [section 3.4]: "ESC-RANK demonstrates the best scoring capability, surpassing GPT-4 by 35 points in terms of accuracy."
  - [corpus]: Weak — no direct literature comparing automated ESC scoring models against GPT-4.
- Break condition: If ESC-RANK overfits to the training annotation style or fails to generalize to new ESC model outputs, scoring accuracy will degrade.

### Mechanism 3
- Claim: Domain-specific LLMs outperform general LLMs in ESC tasks.
- Mechanism: ESC-Eval compares general models (GPT-4, ChatGPT, Llama3, etc.) with domain-specific ones (ExTES-llama, ChatCounselor, MindChat, SoulChat, EmoLLM) using identical role-played dialogues and human evaluation.
- Core assumption: Models trained or fine-tuned on emotional support data will exhibit superior empathy, suggestion quality, and user preference.
- Evidence anchors:
  - [section 3.2]: "domain-specific LLMs (ChatCounselor and EmoLLM), respectively achieved the best results... the general models perform better in terms of fluency, expression diversity, and emotional comfort skills."
  - [section 3.2]: "domain-specific models, due to their remarkable human-like qualities and human convenience, surpass the general models."
  - [corpus]: Weak — no existing comparative studies with identical evaluation protocols.
- Break condition: If general models close the gap through continued scaling or alignment, domain-specific advantages may diminish.

## Foundational Learning

- Concept: Role-playing agent fine-tuning on multi-turn dialogue data.
  - Why needed here: ESC-Eval depends on consistent, realistic simulated users; general LLMs tend to drift or become too formal in emotional contexts.
  - Quick check question: Can you explain why fine-tuning on ESC dialogues improves role-playing agent consistency compared to zero-shot prompting?

- Concept: Multi-dimensional human evaluation for ESC.
  - Why needed here: ESC effectiveness involves multiple subjective qualities (empathy, fluency, humanoid, etc.) that single-score metrics cannot capture.
  - Quick check question: How does ESC-Eval’s seven-dimensional evaluation design differ from standard BLEU/ROUGE approaches?

- Concept: Supervised fine-tuning for scoring model development.
  - Why needed here: ESC-RANK needs to predict human ratings across multiple dimensions; supervised fine-tuning aligns its outputs with human judgment patterns.
  - Quick check question: What training objective would you use to minimize the gap between ESC-RANK’s predictions and human annotations?

## Architecture Onboarding

- Component map:
  - Role card construction (dataset collection → GPT-4 extraction → manual filtering → annotation)
  - ESC-Role (fine-tuned Qwen1.5 on ESC dialogues + general role-play data)
  - ESC-Eval pipeline (ESC-Role + role cards → multi-turn dialogues → human annotation)
  - ESC-RANK (trained on human annotations → automated scoring)

- Critical path:
  1. Build diverse, high-quality role cards
  2. Train ESC-Role to simulate distressed users
  3. Run ESC-Eval to generate dialogue datasets
  4. Collect human annotations
  5. Train ESC-RANK for automation

- Design tradeoffs:
  - Manual annotation ensures quality but is expensive and slow; ESC-RANK partially offsets this at the cost of potential scoring bias
  - Using GPT-4 for role card extraction speeds processing but risks introducing LLM-generated biases; human filtering mitigates but doesn’t eliminate them

- Failure signatures:
  - Role cards lacking specificity → poor role-playing realism
  - ESC-Role generating generic or inconsistent emotional states → invalid evaluation
  - ESC-RANK overfitting to annotation quirks → poor generalization
  - Human annotator fatigue → inconsistent scoring

- First 3 experiments:
  1. Validate ESC-Role’s emotional consistency by running 100+ multi-turn dialogues and measuring affective drift
  2. Test ESC-RANK’s generalization by holding out a subset of models during training and comparing predicted vs. actual human scores
  3. Compare general vs. domain-specific LLMs on a fixed ESC-Eval dataset to confirm performance gaps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed ESC-Eval framework compare to traditional evaluation metrics like BLEU and ROUGE in terms of reliability and consistency across diverse emotional support scenarios?
- Basis in paper: [explicit] The paper states that ESC-Eval demonstrates the best correlation with human evaluation metrics, except for Fluency and Empathy indicators, where automated metrics outperform ESC-Eval.
- Why unresolved: While the paper provides correlation analysis, it does not directly compare the reliability and consistency of ESC-Eval with traditional metrics across a wide range of emotional support scenarios.
- What evidence would resolve it: A comprehensive study comparing ESC-Eval with BLEU, ROUGE, and other traditional metrics across diverse emotional support datasets, focusing on reliability, consistency, and sensitivity to nuances in emotional support conversations.

### Open Question 2
- Question: What are the limitations of the current ESC-RANK model in terms of scalability and generalizability to new, unseen emotional support scenarios?
- Basis in paper: [inferred] The paper mentions that ESC-RANK was trained on manually annotated data, but does not discuss its performance on new, unseen scenarios or its scalability to larger datasets.
- Why unresolved: The paper does not provide information on how ESC-RANK performs when faced with new emotional support scenarios or how it scales with larger datasets.
- What evidence would resolve it: Experiments evaluating ESC-RANK’s performance on new, unseen emotional support scenarios and its scalability to larger datasets, comparing it to other scoring models like GPT-4.

### Open Question 3
- Question: How does the performance of ESC-Eval and ESC-RANK differ when evaluating emotional support conversations in languages other than English and Chinese?
- Basis in paper: [explicit] The paper mentions that the crowdsourced annotators are not native English speakers but are proficient English users, and it provides evaluation results for both English and Chinese. However, it does not discuss the performance of ESC-Eval and ESC-RANK in other languages.
- Why unresolved: The paper does not provide any information on the performance of ESC-Eval and ESC-RANK in languages other than English and Chinese.
- What evidence would resolve it: Experiments evaluating ESC-Eval and ESC-RANK’s performance on emotional support conversations in various languages, comparing their effectiveness and identifying any language-specific challenges or biases.

## Limitations

- Role-playing realism gap: ESC-Role evaluation lacks direct comparison with actual distressed human users, raising questions about ecological validity
- Automated scoring generalization: ESC-RANK’s performance on entirely new ESC model architectures or emerging dialogue patterns remains uncertain
- Human annotation consistency: The paper does not report inter-annotator agreement scores or discuss potential annotation drift over the large-scale evaluation

## Confidence

- High confidence: The core finding that domain-specific ESC models outperform general LLMs is well-supported by comprehensive human evaluation across seven dimensions
- Medium confidence: The mechanism by which ESC-Role generates more consistent emotional states than GPT-4 is plausible but relies primarily on internal metrics
- Low confidence: The 35-point accuracy improvement of ESC-RANK over GPT-4 is difficult to fully assess without knowing the exact evaluation protocol

## Next Checks

1. **Ecological validity test**: Conduct a small-scale evaluation comparing ESC-Eval's role-playing agents against actual distressed users to measure the gap between simulated and real emotional support scenarios

2. **ESC-RANK generalization benchmark**: Evaluate ESC-RANK on a held-out set of dialogue pairs from models not seen during training to assess scoring robustness and potential overfitting

3. **Annotation reliability analysis**: Calculate and report inter-annotator agreement scores across all seven evaluation dimensions to establish the reliability of the human evaluation data