---
ver: rpa2
title: Random Sampling for Diffusion-based Adversarial Purification
arxiv_id: '2411.18956'
source_url: https://arxiv.org/abs/2411.18956
tags:
- sampling
- steps
- methods
- random
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes random sampling as a robust enhancement for
  diffusion-based adversarial purification. The key insight is that the original DDPM
  sampling is designed for stable generation and may not be optimal for adversarial
  purification, which benefits from increased randomness.
---

# Random Sampling for Diffusion-based Adversarial Purification

## Quick Facts
- **arXiv ID**: 2411.18956
- **Source URL**: https://arxiv.org/abs/2411.18956
- **Reference count**: 40
- **Primary result**: Achieves more than 20% robustness advantage with 10× sampling acceleration under strong attacks

## Executive Summary
This paper addresses the challenge of adversarial purification using diffusion models by introducing random sampling as a robust enhancement to standard DDPM and DDIM sampling methods. The key insight is that traditional diffusion sampling, designed for stable generation, may be suboptimal for adversarial purification where increased randomness provides better robustness. The authors propose mediator conditional guidance to maintain prediction consistency between clean and purified images, significantly improving both performance and defensive stability. The DiffAP method outperforms state-of-the-art approaches while achieving 10× sampling acceleration.

## Method Summary
The paper proposes mediator-guided random sampling for diffusion-based adversarial purification. The core innovation involves sampling from random noisy spaces during each diffusion step rather than the adjacent or original noisy space, introducing unpredictability that enhances robustness against adversarial attacks. To maintain prediction consistency between clean and purified images, the authors introduce mediator conditional guidance that applies gradient-based guidance to the mediator variable (the denoised estimate) rather than the noisy sampling point, avoiding gradient bias. This combination of random sampling and mediator guidance achieves significant improvements in both standard and robust accuracy rates while reducing computational requirements.

## Key Results
- Random sampling achieves more than 20% robustness advantage under strong attacks compared to DDPM and DDIM sampling
- DiffAP method significantly outperforms state-of-the-art adversarial purification approaches
- Achieves 10× sampling acceleration while maintaining high purification quality
- Maintains prediction consistency between clean and purified images through mediator guidance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Random sampling increases robustness by introducing unpredictability into the sampling trajectory
- **Core assumption**: Attack methods rely on predictability of the sampling process to craft effective adversarial perturbations
- **Evidence**: The paper demonstrates that random sampling samples from random noisy spaces during each diffusion step, breaking attacker's ability to predict and exploit the sampling trajectory
- **Break condition**: If attackers develop methods that can model or approximate the random sampling distribution, the advantage diminishes

### Mechanism 2
- **Claim**: Mediator conditional guidance maintains prediction consistency between clean and purified images
- **Core assumption**: The denoised estimate is a better target for conditional guidance than the noisy sampling point itself
- **Evidence**: The paper shows that applying gradient-based guidance to the mediator variable rather than noisy sampling point avoids gradient bias and significantly improves consistency
- **Break condition**: If the mediator variable becomes unreliable (e.g., with very high noise levels), the guidance may become ineffective

### Mechanism 3
- **Claim**: The combination of random sampling and mediator guidance provides robustness even with 10× sampling acceleration
- **Core assumption**: The two mechanisms complement each other - randomness for robustness and guidance for stability
- **Evidence**: The paper demonstrates that leveraging both mechanisms achieves significant robustness advantages while reducing sampling steps
- **Break condition**: If either mechanism degrades, the combined benefit may be lost

## Foundational Learning

- **Concept**: Denoising Diffusion Probabilistic Models (DDPM)
  - **Why needed here**: Understanding the base diffusion model is essential since the paper builds upon and modifies DDPM sampling
  - **Quick check question**: What is the key difference between the forward noising process and reverse sampling process in DDPM?

- **Concept**: Adversarial purification
  - **Why needed here**: The paper is specifically addressing how to remove adversarial perturbations from images using diffusion models
  - **Quick check question**: What is the fundamental trade-off between noise addition and purification effectiveness in adversarial purification?

- **Concept**: Conditional guidance in diffusion models
  - **Why needed here**: The paper introduces a new form of conditional guidance (mediator guidance) and compares it to existing methods
  - **Quick check question**: How does conditional guidance typically affect the robustness of diffusion-based purification methods?

## Architecture Onboarding

- **Component map**: Input adversarial image -> Forward noising -> Reverse sampling with random sampling -> Mediator-guided conditional updates -> Output purified image -> Classification prediction

- **Critical path**:
  1. Input adversarial image
  2. Forward noising (add controlled noise)
  3. Reverse sampling with random sampling strategy
  4. Mediator-guided conditional updates
  5. Output purified image
  6. Classification prediction

- **Design tradeoffs**:
  - Randomness vs stability: More randomness improves robustness but may reduce stability
  - Guidance frequency: More frequent guidance improves consistency but increases computational cost
  - Forward steps vs denoising steps: More forward steps improve purification but increase computation

- **Failure signatures**:
  - Loss of standard accuracy despite high robust accuracy (guidance too aggressive)
  - Degradation of robust accuracy with increased forward steps (over-noising)
  - Performance collapse under asynchronous attack (sampling predictability exploited)

- **First 3 experiments**:
  1. Compare random sampling vs DDPM/DDIM sampling with mediator guidance on CIFAR-10 with PGD attack
  2. Evaluate prediction consistency between clean and purified images across different guidance frequencies
  3. Test robustness under asynchronous attack (attacker uses different forward steps than defender)

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text provided. However, based on the evaluation and methodology, several implicit open questions emerge:

- How does the performance of random sampling vary with different types of adversarial attacks beyond PGD+EOT?
- What is the impact of the random noise ratio on computational efficiency and the trade-off between randomness and speed?
- How does the mediator-guided random sampling perform on more complex datasets like ImageNet compared to CIFAR-10?

## Limitations

- Scalability to larger datasets beyond CIFAR-10 remains uncertain, with resource limitations restricting evaluation
- Computational overhead for real-time applications may be significant despite 10× acceleration claims
- Robustness against adaptive attacks that specifically target the random sampling mechanism has not been thoroughly evaluated
- The method requires pre-trained diffusion models and classifiers, limiting flexibility in deployment scenarios

## Confidence

- **High confidence**: The core mechanism of random sampling providing robustness benefits is well-supported by theoretical reasoning and experimental results
- **Medium confidence**: The mediator guidance implementation and its consistency benefits show promise but require more extensive validation across diverse attack scenarios
- **Medium confidence**: The claimed 20% robustness advantage under strong attacks is demonstrated but may vary with different threat models
- **Low confidence**: The method's performance against novel or adaptive attack strategies specifically targeting random sampling remains largely unexplored

## Next Checks

1. **Cross-dataset validation**: Test DiffAP on ImageNet and other larger datasets to evaluate scalability and performance degradation patterns

2. **Adaptive attack evaluation**: Design attacks specifically targeting the random sampling mechanism (e.g., by modeling the random distribution) to assess vulnerability to adaptive adversaries

3. **Real-time performance benchmarking**: Measure inference latency and memory usage on GPU/CPU to quantify the practical deployment constraints of DiffAP compared to standard DDPM