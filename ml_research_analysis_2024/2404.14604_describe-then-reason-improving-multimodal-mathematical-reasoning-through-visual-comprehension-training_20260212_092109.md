---
ver: rpa2
title: 'Describe-then-Reason: Improving Multimodal Mathematical Reasoning through
  Visual Comprehension Training'
arxiv_id: '2404.14604'
source_url: https://arxiv.org/abs/2404.14604
tags:
- visual
- training
- reasoning
- vcar
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes VCAR, a two-step training pipeline to enhance
  multimodal mathematical reasoning by emphasizing visual comprehension. It first
  trains a model to generate detailed image descriptions, then uses those descriptions
  to guide the generation of rationales for solving math problems.
---

# Describe-then-Reason: Improving Multimodal Mathematical Reasoning through Visual Comprehension Training

## Quick Facts
- **arXiv ID**: 2404.14604
- **Source URL**: https://arxiv.org/abs/2404.14604
- **Reference count**: 14
- **Key outcome**: VCAR significantly outperforms baselines on visual-heavy math problems, with 34.3% and 13.3% relative improvements on visual-only and visual-dominant categories, respectively.

## Executive Summary
This paper introduces VCAR, a two-step training pipeline designed to enhance multimodal mathematical reasoning by first training models to generate detailed image descriptions, then using those descriptions to guide rationale generation for solving math problems. The approach addresses the visual comprehension gap in existing multimodal large language models (MLLMs) by providing explicit visual supervision. Experiments on MathVista and MathVerse benchmarks demonstrate substantial improvements, particularly for problems requiring strong visual understanding. Ablation studies confirm the necessity of both visual comprehension and mathematical reasoning training, and that the two-step pipeline is critical for maximizing performance gains.

## Method Summary
VCAR is a two-step training pipeline that enhances multimodal mathematical reasoning by first improving visual comprehension and then leveraging that understanding for reasoning. The method uses two separate LoRA modules: one for generating image descriptions (visual comprehension) and another for generating rationales (reasoning). The model is first trained to produce detailed descriptions of math-related images, then trained to solve problems using those descriptions as context. GPT-4V or Gemini-Pro serves as an oracle to generate the descriptions and rationales used for supervision. The approach is evaluated on benchmarks like MathVista and MathVerse, with performance measured by accuracy.

## Key Results
- VCAR achieves significant improvements on MathVista and MathVerse, especially in visual-heavy categories.
- Ablation studies confirm that both visual comprehension and reasoning training are essential for optimal performance.
- The two-step pipeline outperforms joint training approaches, indicating that separating visual and reasoning tasks reduces interference.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating visual comprehension from reasoning improves multimodal mathematical reasoning by preventing visual errors from corrupting the reasoning path.
- Mechanism: The VCAR pipeline trains the model first on generating accurate image descriptions, isolating visual understanding from reasoning. The descriptions then serve as a clean textual context for the reasoning step, reducing dependence on imperfect visual perception.
- Core assumption: The visual encoder in the MLLM is imperfect and generates noise; textual descriptions extracted from the image can act as a more reliable substitute for the reasoning stage.
- Evidence anchors:
  - [abstract] states that VCAR "substantially outperforms baseline methods solely relying on rationale supervision, especially on problems with high visual demands."
  - [section 3.3] describes the two-step training where visual comprehension is trained first, followed by rationale generation using the description.
  - [corpus] shows related work like "Image-of-Thought Prompting" and "SpatialMath," indicating ongoing community interest in refining visual reasoning, but these do not implement the same separation strategy.
- Break condition: If the visual encoder's errors are minimal or if the model already achieves strong visual understanding, the benefit of separating the two steps diminishes.

### Mechanism 2
- Claim: Providing fine-grained visual supervision (image descriptions) is essential for tasks where visual perception is a bottleneck.
- Mechanism: By explicitly training on image description generation, the model receives direct supervision about the visual content, rather than inferring it indirectly from reasoning rationales. This addresses the inadequacy of "visual-centric supervision" mentioned in the paper.
- Core assumption: Math problems with diagrams or figures contain critical information that is best understood through targeted visual comprehension training, not just general reasoning training.
- Evidence anchors:
  - [abstract] notes that baseline methods "fall short in visual comprehension due to inadequate visual-centric supervision, which leads to inaccurate interpretation of math figures."
  - [section 4.2] shows that VCAR's improvements are especially pronounced in "visual-only" and "visual-dominant" categories, confirming the visual bottleneck hypothesis.
  - [corpus] neighbors include "MV-MATH" and "SpatialMath," which also evaluate visual comprehension in math, supporting the relevance of this mechanism.
- Break condition: If a problem is text-dominant and does not require strong visual interpretation, the extra visual comprehension step may not yield significant gains.

### Mechanism 3
- Claim: Using two independent LoRA modules for visual comprehension and reasoning avoids interference between the two learning objectives.
- Mechanism: Separate LoRA modules allow each capability to be trained independently without conflicting gradients, ensuring that the visual training does not corrupt the reasoning parameters and vice versa.
- Core assumption: Joint training of both tasks in a single model can lead to imbalanced learning or interference, especially when the two tasks have different input demands (visual vs. textual).
- Evidence anchors:
  - [section 3.3] explicitly describes using "two sets of LoRA (Hu et al., 2022) modules" for separate optimization of Ld and Lr.
  - [section 4.3] ablation shows that "Concatenation Training" (joint optimization) performs worse than the two-step approach, supporting the interference hypothesis.
  - [corpus] neighbors like "MM-PRM" discuss "scalable step-level supervision," which may imply fine-grained training, but not the same separation strategy.
- Break condition: If the base model already has strong multimodal alignment, the benefit of separate LoRA modules may be reduced.

## Foundational Learning

- Concept: Visual grounding in multimodal models
  - Why needed here: MLLMs often rely heavily on textual inputs and underutilize visual features; grounding ensures that visual information is correctly interpreted and integrated.
  - Quick check question: In a diagram showing a rectangle with labeled sides, can you describe what visual features are critical for computing the perimeter?

- Concept: Chain-of-thought reasoning
  - Why needed here: Math problems often require multi-step reasoning; generating rationales helps the model break down complex problems into manageable sub-steps.
  - Quick check question: Given a word problem about buying items, can you outline the reasoning steps to compute the total cost?

- Concept: Supervised fine-tuning with intermediate supervision
  - Why needed here: Directly training on final answers misses the reasoning process; rationales and descriptions provide intermediate supervision signals that guide learning.
  - Quick check question: Why might training on rationales alone be insufficient for problems that require strong visual comprehension?

## Architecture Onboarding

- Component map:
  - Visual encoder (ViT) -> Language model (LLaMA/TinyLLaVA) -> Two LoRA modules (visual comprehension, reasoning) -> Final answer decoder.
  - Gemini-Pro/GPT-4V used as oracle to generate descriptions and rationales.

- Critical path:
  1. Collect image descriptions and rationales from oracle.
  2. Train LoRA₁ on image description generation (visual comprehension).
  3. Train LoRA₂ on rationale generation using descriptions (reasoning).
  4. During inference, run LoRA₁ to generate description, then LoRA₂ to generate rationale and answer.

- Design tradeoffs:
  - Using separate LoRA modules simplifies training but limits parameter updates compared to full fine-tuning.
  - Relying on GPT-4V/Gemini-Pro for supervision is efficient but introduces dependency on proprietary models.
  - Training on synthetic data avoids privacy issues but may inherit biases from the oracle.

- Failure signatures:
  - Poor visual comprehension: model fails on diagram-heavy problems, generates incorrect descriptions.
  - Poor reasoning: model generates correct descriptions but fails to derive correct answers.
  - Overfitting to oracle style: model's rationales mimic the oracle's tone or structure too closely.

- First 3 experiments:
  1. Compare VCAR against CoT-GT baseline on a small subset of MathVerse to verify improvement on visual-only problems.
  2. Ablation: remove LoRA₁ (visual comprehension) and observe drop in visual-dominant categories.
  3. Ablation: reverse the training order (reason first, then describe) and compare performance to confirm the necessity of the current pipeline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different visual description formats (e.g., question-augmented vs. standalone descriptions) impact the performance of multimodal mathematical reasoning models?
- Basis in paper: [explicit] The paper discusses the comparison between VCAR's standalone descriptions and question-augmented descriptions in Section 5.2, showing that standalone descriptions yield better performance.
- Why unresolved: The paper provides a comparison but does not explore a wide range of visual description formats or their nuanced impacts on model performance.
- What evidence would resolve it: Experiments comparing multiple visual description formats (e.g., standalone, question-augmented, and combined) across various benchmarks and tasks would provide clearer insights into the optimal format for different scenarios.

### Open Question 2
- Question: Does the order of training steps in VCAR (description first, then rationale) significantly affect the model's ability to generalize to unseen problems?
- Basis in paper: [explicit] Section 5.4 discusses the impact of switching the order of training steps, showing that the original order (description first, then rationale) performs better.
- Why unresolved: The paper only tests one alternative order and does not explore other potential sequences or their effects on generalization.
- What evidence would resolve it: Experiments testing multiple training step orders and evaluating their impact on generalization to diverse and unseen mathematical problems would provide more comprehensive insights.

### Open Question 3
- Question: How does the length and complexity of rationales affect the performance of multimodal mathematical reasoning models?
- Basis in paper: [inferred] Section 5.3 discusses the impact of increasing the length of rationales by adding more details or using multiple rationales, but does not fully explore the relationship between rationale complexity and model performance.
- Why unresolved: The paper provides limited exploration of how varying the length and complexity of rationales influences model performance.
- What evidence would resolve it: Systematic experiments varying the length and complexity of rationales and measuring their impact on model performance across different benchmarks would clarify this relationship.

### Open Question 4
- Question: Can the effectiveness of VCAR be maintained or improved when using full model fine-tuning instead of LoRA?
- Basis in paper: [explicit] The limitations section mentions that all models, including baselines, are trained with LoRA due to computational constraints, and future work plans to explore full model fine-tuning.
- Why unresolved: The paper does not provide experimental results comparing LoRA with full model fine-tuning.
- What evidence would resolve it: Experiments comparing the performance of VCAR using both LoRA and full model fine-tuning on the same benchmarks would determine if full fine-tuning enhances or maintains the effectiveness of VCAR.

## Limitations
- The approach depends heavily on the quality and consistency of descriptions generated by GPT-4V/Gemini-Pro as an oracle, which may introduce variability.
- The exact contribution of each step in the VCAR pipeline to the final performance gain is not fully disentangled.
- The improvements have not been tested for scalability to larger models or more diverse datasets.

## Confidence
- **High confidence**: The claim that VCAR significantly improves performance on visual-heavy math problems is well-supported by experimental results and ablation studies.
- **Medium confidence**: The assertion that separating visual comprehension and reasoning training is key to maximizing performance gains is plausible but not fully validated.
- **Low confidence**: The claim that the two-step pipeline is universally superior to other multimodal training approaches is not rigorously tested against a broader set of baselines.

## Next Checks
1. **Consistency of Oracle Descriptions**: Repeat the oracle consistency check on a larger sample of images and measure the variability in generated descriptions.
2. **Ablation of Individual Steps**: Perform an ablation study where each step of the VCAR pipeline (description generation and rationale generation) is removed independently, and compare the results to the full pipeline.
3. **Scalability Test**: Evaluate the VCAR framework on a larger dataset or with a more diverse set of math problems to test whether the improvements generalize beyond the current benchmarks.