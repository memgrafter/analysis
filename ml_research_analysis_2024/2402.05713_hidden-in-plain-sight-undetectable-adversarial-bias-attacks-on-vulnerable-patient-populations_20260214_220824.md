---
ver: rpa2
title: 'Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on Vulnerable
  Patient Populations'
arxiv_id: '2402.05713'
source_url: https://arxiv.org/abs/2402.05713
tags:
- bias
- performance
- group
- adversarial
- groups
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates demographically targeted adversarial bias
  attacks on deep learning models using chest x-ray data. The researchers demonstrate
  that manipulating ground truth labels can introduce undetectable underdiagnosis
  bias in models, particularly affecting underrepresented groups.
---

# Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on Vulnerable Patient Populations

## Quick Facts
- arXiv ID: 2402.05713
- Source URL: https://arxiv.org/abs/2402.05713
- Reference count: 40
- This study demonstrates that demographically targeted label poisoning attacks can introduce undetectable underdiagnosis bias in DL models, particularly affecting underrepresented patient groups.

## Executive Summary
This study investigates how demographically targeted adversarial bias attacks can compromise deep learning models used for chest x-ray analysis. The researchers demonstrate that manipulating ground truth labels can introduce undetectable underdiagnosis bias that disproportionately affects underrepresented demographic groups. Their experiments show that smaller demographic groups are more vulnerable to such attacks, with female patients and those over 80 years old being most affected. The study introduces a vulnerability metric that correlates a group's susceptibility to its representation in training data, highlighting critical concerns for medical imaging AI deployment.

## Method Summary
The researchers used the RSNA Pneumonia Detection Challenge dataset (n=26,684 CXRs) with patient demographics extracted from DICOM tags. They implemented demographically targeted label poisoning attacks by flipping positive pneumonia labels to "no finding" for specific demographic groups at varying rates (0-100%). A DenseNet121 model pre-trained on ImageNet was fine-tuned using 5-fold cross-validation with binary cross-entropy loss. Model performance was evaluated using AUROC, False Negative Rate (FNR), and False Omission Rate (FOR), while a vulnerability metric quantified the difference in performance degradation between targeted groups and overall model performance.

## Key Results
- Adversarial bias attacks demonstrated high-selectivity for bias in targeted demographic groups while maintaining overall model performance
- Female patients and those over 80 years old showed the highest vulnerability to underdiagnosis bias attacks
- A strong monotonic decreasing relationship was observed between demographic group vulnerability and sample size in training data (rs = -0.88 to -0.74)
- The vulnerability metric effectively quantified differential performance degradation between targeted groups and overall model

## Why This Works (Mechanism)

### Mechanism 1
Demographically targeted label poisoning attacks introduce undetectable underdiagnosis bias by selectively flipping ground truth labels from positive to negative for specific demographic groups during training. The model learns to interpret features from the targeted group as indicative of no finding, leading to systematic underdiagnosis.

### Mechanism 2
Smaller demographic groups are more vulnerable because degrading their performance through label poisoning has minimal impact on overall model metrics. Since smaller groups contribute less to overall performance, the attack remains undetected.

### Mechanism 3
The vulnerability of a demographic group to adversarial bias attacks is directly correlated with its representation in training data. The model's learned features are optimized for majority groups, and these features may not generalize well to minority groups with different characteristics.

## Foundational Learning

- Concept: Label poisoning attacks
  - Why needed here: Understanding how manipulating ground truth labels impacts model performance is crucial to comprehending the mechanism of proposed adversarial bias attacks
  - Quick check question: How does changing the labels of a subset of training data affect the model's learned features and subsequent predictions?

- Concept: Demographic bias in machine learning
  - Why needed here: Recognizing how differences in representation of demographic groups in training data can lead to biased model performance is essential to understanding vulnerability of smaller groups to adversarial attacks
  - Quick check question: Why are smaller demographic groups more likely to experience biased model performance compared to larger groups?

- Concept: Performance metrics for bias detection
  - Why needed here: Knowing how to measure and detect bias in model performance across different demographic groups is necessary to evaluate the effectiveness of proposed adversarial bias attacks
  - Quick check question: What metrics can be used to quantify the difference in model performance between a targeted demographic group and the overall model?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Demographic extraction -> Label poisoning -> Model training -> Performance evaluation -> Vulnerability quantification

- Critical path:
  1. Load and preprocess dataset, extracting patient demographics
  2. Implement label poisoning function to target specific demographic groups
  3. Train DenseNet121 model on poisoned training data with 5-fold cross-validation
  4. Evaluate model performance on held-out test set for each demographic group
  5. Calculate vulnerability metric for each demographic group
  6. Analyze correlation between vulnerability and demographic representation

- Design tradeoffs:
  - Using pre-trained DenseNet121 allows faster training and better generalization but may introduce pre-training data biases
  - Label poisoning at ground truth level is simpler but potentially less effective than input data manipulation
  - Held-out test set evaluation ensures unbiased performance estimates but may not capture full extent of adversarial bias in real-world scenarios

- Failure signatures:
  - Vulnerability metric not showing clear correlation with demographic representation suggests mechanism may not hold
  - Significant impact on overall model performance indicates attack may not be undetectable
  - Lack of performance degradation in targeted demographic group suggests attack may not be effective

- First 3 experiments:
  1. Implement label poisoning function and verify it correctly flips ground truth labels for targeted demographic group
  2. Train DenseNet121 model on poisoned training data and evaluate performance on held-out test set
  3. Calculate vulnerability metric for targeted demographic group and verify positive correlation with label poisoning level

## Open Questions the Paper Calls Out

- What is the minimum sample size threshold for a demographic group to be resistant to undetectable adversarial bias attacks?
- How do undetectable adversarial bias attacks affect model performance when multiple demographic groups are targeted simultaneously?
- Do the vulnerabilities to undetectable adversarial bias attacks persist when the model is evaluated on external datasets?

## Limitations

- The study focuses on binary pneumonia detection using a single pre-trained architecture, limiting generalizability to other medical imaging tasks
- Findings are based on controlled label-poisoning experiments that may not fully capture real-world attack scenarios
- The study does not address potential defensive mechanisms or mitigation strategies for the proposed attacks

## Confidence

- **High Confidence**: The core mechanism of demographic-specific label poisoning introducing performance degradation in targeted groups while maintaining overall model performance is well-supported
- **Medium Confidence**: The correlation between vulnerability and demographic representation may be influenced by specific dataset characteristics and requires validation across diverse medical imaging datasets
- **Low Confidence**: The study's implications for real-world clinical impact and effectiveness of proposed vulnerability metrics in detecting actual adversarial attacks remain largely theoretical

## Next Checks

1. Replicate label poisoning experiments on multiple medical imaging datasets with different demographic distributions to verify robustness of vulnerability metric
2. Implement and evaluate potential defense strategies (e.g., adversarial training, bias-aware learning) to assess their effectiveness in mitigating proposed label poisoning attacks
3. Design and execute more sophisticated attack scenarios combining label poisoning with input perturbations to test vulnerability metric's ability to detect complex adversarial bias attacks in realistic deployment settings