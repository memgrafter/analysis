---
ver: rpa2
title: 'BMRetriever: Tuning Large Language Models as Better Biomedical Text Retrievers'
arxiv_id: '2404.18443'
source_url: https://arxiv.org/abs/2404.18443
tags:
- retrieval
- biomedical
- data
- tasks
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents BMR ETRIEVER, a series of dense retrieval
  models that leverage large language models (LLMs) as backbones to improve biomedical
  text retrieval performance. The method employs a two-stage framework: unsupervised
  contrastive pre-training on large-scale biomedical corpora to inject domain knowledge,
  followed by instruction fine-tuning on labeled datasets and synthetic data to align
  with downstream tasks.'
---

# BMRetriever: Tuning Large Language Models as Better Biomedical Text Retrievers

## Quick Facts
- arXiv ID: 2404.18443
- Source URL: https://arxiv.org/abs/2404.18443
- Authors: Ran Xu; Wenqi Shi; Yue Yu; Yuchen Zhuang; Yanqiao Zhu; May D. Wang; Joyce C. Ho; Chao Zhang; Carl Yang
- Reference count: 40
- Key outcome: BMR ETRIEVER achieves state-of-the-art biomedical retrieval performance, with the 410M variant outperforming baselines up to 11.7Ã— larger and the 7B variant matching models with over 5B parameters.

## Executive Summary
BMRetriever presents a series of dense retrieval models that leverage large language models as backbones to improve biomedical text retrieval performance. The approach employs a two-stage framework: unsupervised contrastive pre-training on large-scale biomedical corpora to inject domain knowledge, followed by instruction fine-tuning on labeled datasets and synthetic data to align with downstream tasks. Extensive experiments across five biomedical retrieval tasks on eleven datasets demonstrate BMR ETRIEVER's strong performance and parameter efficiency.

## Method Summary
BMRetriever uses a two-stage training framework with large language models as backbones. First, unsupervised contrastive pre-training on biomedical corpora (PubMed, arXiv, MedRxiv, BioRxiv, textbooks, and other sources) injects domain-specific knowledge by aligning semantically related text sequences. Second, instruction fine-tuning on labeled biomedical datasets and synthetic data generated by GPT models aligns the model representations with specific retrieval tasks. The method scales across different model sizes (410M, 1B, 2B, 7B parameters) while maintaining strong parameter efficiency.

## Key Results
- The 410M parameter variant outperforms baselines up to 11.7 times larger in biomedical retrieval tasks
- The 7B parameter variant matches the performance of models with over 5B parameters
- BMR ETRIEVER achieves 94-98% of the 7B model's performance using only 6-29% as many parameters

## Why This Works (Mechanism)

### Mechanism 1
Unsupervised contrastive pre-training on biomedical corpora injects domain-specific knowledge into the model by aligning semantically related text sequences. The model learns to distinguish relevant from irrelevant query-passage pairs using InfoNCE loss, building representations that capture domain-specific terminology and context.

### Mechanism 2
Instruction fine-tuning with synthetic data augments task diversity beyond limited public biomedical datasets. LLM-generated synthetic query-passage pairs simulate diverse biomedical retrieval scenarios, allowing the model to generalize to unseen tasks and input formats.

### Mechanism 3
Scaling up model size with autoregressive LLM backbones provides parameter-efficient domain adaptation. Larger autoregressive models can capture more complex biomedical patterns during pre-training, and instruction fine-tuning aligns these representations to retrieval tasks.

## Foundational Learning

- **Concept: Contrastive learning with InfoNCE loss**
  - Why needed here: Enables the model to distinguish relevant from irrelevant query-passage pairs during pre-training
  - Quick check question: How does InfoNCE loss encourage the model to rank positive pairs higher than negative pairs?

- **Concept: Instruction fine-tuning with task instructions**
  - Why needed here: Aligns the model's representations with specific biomedical retrieval tasks and input formats
  - Quick check question: What role do task instructions play in adapting the model to different biomedical retrieval scenarios?

- **Concept: Synthetic data augmentation with LLM generation**
  - Why needed here: Expands the diversity of training examples beyond limited public biomedical datasets
  - Quick check question: How does generating synthetic query-passage pairs help the model generalize to unseen retrieval tasks?

## Architecture Onboarding

- **Component map:** Pre-trained autoregressive transformer (Pythia/Gemma/BioMistral) -> Contrastive pre-training on biomedical corpora -> Instruction fine-tuning on labeled + synthetic data -> Dense embeddings for retrieval scoring

- **Critical path:**
  1. Load backbone model
  2. Pre-train with contrastive loss on biomedical corpora
  3. Fine-tune with instruction data (labeled + synthetic)
  4. Generate embeddings for queries and passages
  5. Compute dot product similarity for retrieval

- **Design tradeoffs:**
  - Larger models provide better performance but require more compute
  - Synthetic data increases diversity but may introduce noise
  - Autoregressive vs. dual-encoder architectures trade encoding speed for representation quality

- **Failure signatures:**
  - Low separation between positive and negative similarity scores
  - Poor performance on tasks not seen during fine-tuning
  - Degradation when using smaller pre-training corpora

- **First 3 experiments:**
  1. Evaluate pre-trained model on biomedical retrieval tasks without fine-tuning
  2. Test synthetic data generation quality by checking consistency filtering results
  3. Compare performance of dot product vs cosine similarity for embedding scoring

## Open Questions the Paper Calls Out

### Open Question 1
How can the inference latency and storage costs of BMR ETRIEVER's text embeddings be reduced while maintaining performance? The paper acknowledges efficiency limitations but does not propose solutions or investigate potential optimizations.

### Open Question 2
How does BMR ETRIEVER perform on non-biomedical retrieval tasks compared to general domain retrieval models? The paper only evaluates BMR ETRIEVER on biomedical and scientific tasks, leaving its performance on general domain tasks unexplored.

### Open Question 3
What is the optimal amount and type of synthetic data for augmenting BMR ETRIEVER's training? The paper uses a fixed amount of synthetic data without exploring the effects of varying quantity or quality of synthetic data on performance.

## Limitations

- The effectiveness of unsupervised contrastive pre-training assumes title-abstract pairs from biomedical corpora reliably represent semantically relevant content
- Limited evidence that GPT-generated synthetic query-passage pairs accurately reflect real biomedical retrieval distributions
- No direct comparisons with BERT-based dual-encoder architectures for biomedical retrieval

## Confidence

- **High confidence**: The overall two-stage training framework (contrastive pre-training + instruction fine-tuning) and its general effectiveness in improving biomedical retrieval performance across multiple tasks and datasets
- **Medium confidence**: The specific mechanisms of unsupervised contrastive pre-training with title-abstract pairs and synthetic data augmentation with LLM generation, due to limited validation of their assumptions
- **Medium confidence**: The parameter efficiency claims, as they are based on comparisons with published baselines rather than direct controlled experiments

## Next Checks

1. Conduct ablation studies to measure the impact of removing synthetic data augmentation versus removing contrastive pre-training on model performance across different biomedical tasks.

2. Validate the quality of synthetic data by measuring distribution similarity between GPT-generated pairs and human-labeled pairs using embedding-based divergence metrics or manual annotation.

3. Perform controlled experiments comparing autoregressive LLM backbones (used in BMRETriever) against BERT-based dual-encoder architectures using identical training procedures and datasets.