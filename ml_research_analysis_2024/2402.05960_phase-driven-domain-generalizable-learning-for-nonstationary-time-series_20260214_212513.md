---
ver: rpa2
title: Phase-driven Domain Generalizable Learning for Nonstationary Time Series
arxiv_id: '2402.05960'
source_url: https://arxiv.org/abs/2402.05960
tags:
- domain
- time
- series
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the challenge of building models that generalize
  well to unseen data distributions in time series classification, a common problem
  in real-world applications where data can be nonstationary and exhibit distribution
  shifts. The proposed PhASER framework introduces three key innovations: (1) Hilbert
  Transform-based phase augmentation to diversify nonstationarity while preserving
  discriminatory features, (2) separate magnitude-phase encoding to treat time-varying
  magnitude and phase as independent modalities, and (3) phase-residual feature broadcasting
  to integrate phase features with a residual connection for inherent regularization.'
---

# Phase-driven Domain Generalizable Learning for Nonstationary Time Series

## Quick Facts
- arXiv ID: 2402.05960
- Source URL: https://arxiv.org/abs/2402.05960
- Authors: Payal Mohapatra; Lixu Wang; Qi Zhu
- Reference count: 40
- Primary result: Outperforms 13 SOTA baselines by average 5%, up to 13% on 5 time series datasets

## Executive Summary
This paper introduces PhASER, a framework for domain generalizable learning in nonstationary time series classification. The method addresses the challenge of building models that generalize to unseen data distributions by leveraging phase information from Hilbert Transform and treating magnitude and phase as independent modalities. PhASER demonstrates consistent improvements over state-of-the-art baselines across five diverse datasets spanning human activity recognition, sleep-stage classification, and gesture recognition.

## Method Summary
PhASER operates by first augmenting time series with Hilbert Transform to introduce π/2 phase shifts, creating nonstationary variations while preserving discriminatory features. The augmented data undergoes STFT to extract magnitude and phase tensors, which are processed by separate convolutional encoders. A fusion layer combines these representations, followed by depthwise encoding and a temporal encoder (convolution or transformer). Crucially, phase features are incorporated via a residual connection to the temporal backbone, providing inherent regularization. The model is trained with cross-entropy loss and early stopping, evaluated through cross-person generalization scenarios without requiring domain labels.

## Key Results
- PhASER consistently outperforms 13 SOTA baselines by average 5% accuracy across five datasets
- Achieves up to 13% improvement in some cross-person generalization scenarios
- Demonstrates effectiveness in both multiple-source and one-person-to-another settings
- Ablation studies confirm importance of all three key components: phase augmentation, separate encoding, and phase residual

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Phase shift via Hilbert Transform preserves discriminatory features while diversifying non-stationarity.
- Mechanism: Hilbert Transform induces a π/2 phase shift, creating augmented samples with different temporal structure but identical magnitude response, which maintains class-separability while exposing the model to varied temporal statistics.
- Core assumption: Phase encodes sufficient non-stationarity information and magnitude encodes discriminative class features.
- Evidence anchors:
  - [abstract] "Hilbert transform-based augmentation, which diversifies non-stationarity while preserving discriminatory semantics"
  - [section] "Applying HT on a signal results in a phase shift of π/2, yielding a new out-of-phase signal"
  - [corpus] "No direct corpus match for HT phase-shift augmentation in TS DG"
- Break condition: If phase and magnitude are not truly independent or if phase shift destroys class-relevant temporal patterns.

### Mechanism 2
- Claim: Separate magnitude-phase encoding improves classification by treating them as independent modalities.
- Mechanism: Short-time Fourier Transform yields magnitude and phase tensors; two dedicated encoders learn modality-specific features; late fusion preserves modality independence.
- Core assumption: Magnitude and phase carry complementary, separable information for classification.
- Evidence anchors:
  - [abstract] "separate magnitude-phase encoding, viewing time-varying magnitude and phase as independent modalities"
  - [section] "we take Mag(x), Pha(x) as inputs of two separate encoders FMag and FPha"
  - [corpus] "No corpus evidence of magnitude/phase separation in TS DG"
- Break condition: If modality separation is redundant or harmful due to strong interdependency.

### Mechanism 3
- Claim: Phase-residual feature broadcasting provides regularization that improves domain invariance.
- Mechanism: Phase embeddings are projected and added as a residual to temporal backbone output; this forces backbone to maintain discriminative features under phase perturbations.
- Core assumption: Residual connection from phase path constrains backbone to preserve features robust to non-stationarity.
- Evidence anchors:
  - [abstract] "phase-residual feature broadcasting, integrating phase features with a residual connection for inherent regularization"
  - [section] "incorporate the phase information deeper in the layers through residual connections"
  - [corpus] "No corpus match for phase-driven residual in TS DG"
- Break condition: If residual degrades discriminative signal or introduces instability.

## Foundational Learning

- Concept: Hilbert Transform and its phase-shifting property.
  - Why needed here: HT is the core augmentation operator; understanding its frequency-domain interpretation is key to reasoning about its effect on non-stationarity.
  - Quick check question: What is the phase shift introduced by Hilbert Transform in the frequency domain?

- Concept: Short-time Fourier Transform (STFT) and time-frequency decomposition.
  - Why needed here: STFT provides the magnitude/phase split; knowing window size effects on resolution is important for design choices.
  - Quick check question: How does window length in STFT trade off time vs frequency resolution?

- Concept: Domain generalization theory and β-divergence bounds.
  - Why needed here: Theoretical motivation for minimizing domain discrepancy and addressing non-stationarity is central to PhASER's design.
  - Quick check question: In the context of β-divergence, what role does non-stationarity play in bounding generalization risk?

## Architecture Onboarding

- Component map: Input -> HT augmentation -> STFT -> separate encoding (FMag, FPha) -> fusion -> depthwise encoder (FDep) -> temporal encoder (FTem) -> phase residual (gRes) -> classifier (gCls)
- Critical path: Input → HT augmentation → STFT → separate encoding → fusion → depthwise → temporal → phase residual → classify
- Design tradeoffs:
  - Separate encoders add parameters but improve modality disentanglement.
  - Residual from phase may help regularization but could hurt if phase is noisy.
  - Window size in STFT affects frequency/time resolution; larger windows improve frequency but blur temporal cues.
- Failure signatures:
  - Phase-only input yields poor accuracy → magnitude is more discriminative.
  - Removing phase residual degrades performance → regularization is beneficial.
  - Concatenating mag/phase hurts vs. separate encoding → modality independence matters.
- First 3 experiments:
  1. Ablate phase augmentation: train without HT, compare accuracy drop.
  2. Ablate separate encoding: use single encoder for concatenated mag+phase, compare vs. separate.
  3. Ablate phase residual: remove gRes connection, compare regularization effect.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PhASER's performance scale with the number of source domains, particularly when approaching the one-person-to-another setting where only a single source domain is available?
- Basis in paper: [explicit] The paper demonstrates PhASER's performance in cross-person generalization with multiple source domains (NS > 1) and the more challenging one-person-to-another setting (NS = 1), showing consistent superiority over baselines.
- Why unresolved: The paper does not provide a systematic analysis of performance trends as the number of source domains decreases from multiple to single.
- What evidence would resolve it: A controlled experiment varying the number of source domains (e.g., 1, 2, 4, 8, all) while keeping other factors constant would reveal how PhASER's performance scales in this regime.

### Open Question 2
- Question: What is the impact of different window sizes in STFT on PhASER's classification accuracy, and is there an optimal window size that balances time and frequency resolution?
- Basis in paper: [explicit] The paper mentions using a randomly sampled power-of-2 window size for each feature dimension in STFT, but does not provide a systematic analysis of window size sensitivity.
- Why unresolved: The paper only briefly mentions the use of random window sizes without exploring the sensitivity of performance to different window sizes.
- What evidence would resolve it: A comprehensive sensitivity analysis varying the window size across a range of values (e.g., 2^4, 2^5, 2^6, 2^7) while keeping other factors constant would reveal the optimal window size and its impact on accuracy.

### Open Question 3
- Question: How does PhASER's performance compare to domain generalization methods that require domain labels during training, and what is the trade-off between performance and the need for domain labels?
- Basis in paper: [explicit] The paper highlights that PhASER does not require domain labels for source domains or target samples, unlike some domain generalization methods.
- Why unresolved: The paper does not directly compare PhASER's performance to domain generalization methods that require domain labels, nor does it discuss the trade-off between performance and the need for domain labels.
- What evidence would resolve it: A direct comparison of PhASER's performance to domain generalization methods that require domain labels (e.g., GroupDRO, DANN) on the same datasets would reveal the trade-off between performance and the need for domain labels.

## Limitations
- No ablation study on phase-residual effectiveness versus alternative regularization methods
- Theoretical connection between Hilbert Transform phase shifts and β-divergence generalization bounds remains implicit
- Lack of cross-application stress tests (e.g., finance, healthcare, climate) to confirm broad applicability claim

## Confidence
- **High**: Outperformance of 13 SOTA baselines on the five evaluated datasets
- **Medium**: Phase-residual mechanism's regularization effect due to indirect ablation evidence
- **Low**: Claims of broad applicability to existing TS models without empirical verification

## Next Checks
1. Replicate key ablation (separate encoding) on WISDM to verify modality independence benefit
2. Conduct stress test: apply PhASER to a non-stationary financial dataset with known distribution shifts
3. Compare phase-residual to dropout/batch norm baselines within the same framework