---
ver: rpa2
title: Adaptive Optimization for Prediction with Missing Data
arxiv_id: '2402.01543'
source_url: https://arxiv.org/abs/2402.01543
tags:
- missing
- adaptive
- data
- linear
- regression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses prediction with missing data, a common challenge
  in real-world datasets where features may be partially observed. The authors propose
  a new framework based on adaptive optimization to learn predictive models that can
  leverage information from missingness patterns and are agnostic to the missing data
  mechanism.
---

# Adaptive Optimization for Prediction with Missing Data

## Quick Facts
- arXiv ID: 2402.01543
- Source URL: https://arxiv.org/abs/2402.01543
- Authors: Dimitris Bertsimas; Arthur Delarue; Jean Pauphilet
- Reference count: 3
- Primary result: 2-10% improvement in out-of-sample accuracy using adaptive optimization for prediction with missing data

## Executive Summary
This paper addresses the challenge of prediction with missing data by introducing a framework based on adaptive optimization. The authors propose adaptive linear regression models where regression coefficients adapt to the set of observed features, and show these models are equivalent to jointly learning an imputation rule and downstream linear regression. The method achieves improved accuracy compared to state-of-the-practice methods, particularly when data is not missing at random (NMAR).

## Method Summary
The method treats prediction with missing data as a two-stage adaptive optimization problem. Instead of sequentially imputing missing values then training a regression model, it learns imputation and regression models simultaneously through a hierarchy of adaptive linear regression models (static, affine, polynomial, piecewise constant). For non-linear models, a joint impute-then-regress heuristic iteratively trains a predictive model on imputed data while updating the imputation based on prediction error. The approach is agnostic to the missing data mechanism and can leverage predictive information contained in missingness patterns.

## Key Results
- Adaptive linear regression models achieve 2-10% improvement in out-of-sample accuracy compared to mean imputation followed by regression
- Joint optimization of imputation and regression outperforms sequential approaches, especially when data is not missing at random
- The method performs comparably to or better than non-linear models while maintaining interpretability of linear models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Jointly optimizing imputation and regression achieves higher accuracy than sequential impute-then-regress, especially when data is not missing at random (NMAR)
- Mechanism: Instead of first imputing missing values and then training a regression model, it learns the imputation and regression models simultaneously, allowing the imputation step to account for the downstream prediction objective
- Core assumption: Correlation between missingness pattern and target variable contains predictive information lost when missingness is ignored
- Evidence anchors: [abstract] equivalence shown between adaptive models and joint learning; [section 3.1] mathematical equivalence demonstrated
- Break condition: If missingness is MCAR and relationship is linear, sequential methods may suffice

### Mechanism 2
- Claim: Adaptive linear regression models can achieve performance comparable to or better than non-linear models
- Mechanism: Regression coefficients adapt to missingness pattern through hierarchy of models (static, affine, polynomial, piecewise constant) balancing adaptivity and tractability
- Core assumption: Relationship between available features and target can be captured by adjusting regression coefficients based on missingness pattern
- Evidence anchors: [section 2.2] framework for adaptive models; [section 2.3] generalization bounds provided
- Break condition: If true relationship is highly non-linear and cannot be approximated by adjusting linear coefficients

### Mechanism 3
- Claim: The method is agnostic to missing data mechanism
- Mechanism: Learns model directly from observed data without assumptions about missingness mechanism, either through adaptive linear regression or joint impute-then-regress heuristic
- Core assumption: True missingness mechanism is unknown and cannot be tested from data
- Evidence anchors: [abstract] improved performance in NMAR settings; [section 1] motivation for agnostic approach
- Break condition: If missingness mechanism is known and satisfies MAR assumption

## Foundational Learning

- Concept: Missing Data Mechanisms (MCAR, MAR, NMAR)
  - Why needed here: Essential for evaluating effectiveness of proposed methods designed to be agnostic to missingness mechanism
  - Quick check question: What is the difference between MCAR, MAR, and NMAR, and why is the MAR assumption important for traditional imputation methods?

- Concept: Adaptive Optimization
  - Why needed here: Framework leverages concepts from adaptive optimization to create hierarchy of adaptive linear regression models
  - Quick check question: How does adaptive optimization differ from traditional optimization, and how is it applied in linear regression with missing data?

- Concept: Imputation Methods (Mean Imputation, MICE)
  - Why needed here: Proposed methods compared against traditional impute-then-regress strategies using mean imputation or MICE
  - Quick check question: What are advantages and disadvantages of mean imputation and MICE compared to joint impute-then-regress heuristic?

## Architecture Onboarding

- Component map: Data preprocessing -> Adaptive linear regression framework -> Joint impute-then-regress heuristic -> Model selection via cross-validation

- Critical path:
  1. Preprocess data by handling missing values (zero imputation for linear models, mean imputation for non-linear models)
  2. Implement adaptive linear regression framework with appropriate adaptivity level
  3. Alternatively, implement joint impute-then-regress heuristic with iterative updates
  4. Use cross-validation to select best model and evaluate on held-out test set

- Design tradeoffs:
  - Adaptivity vs. tractability: Higher adaptivity captures complex relationships but increases computational cost and overfitting risk
  - Linear vs. non-linear models: Adaptive linear models offer interpretability and efficiency but may not capture highly non-linear relationships
  - Joint vs. sequential optimization: Joint optimization leverages missingness information but is more complex than sequential methods

- Failure signatures:
  - Overfitting from overly adaptive models with many parameters
  - Underfitting from insufficient adaptivity in static models
  - Computational inefficiency with complex models or large datasets

- First 3 experiments:
  1. Implement static adaptive linear regression on synthetic MCAR data; compare to mean imputation + linear regression
  2. Implement joint impute-then-regress heuristic on synthetic NMAR data; compare to mean imputation and adaptive linear regression
  3. Implement affine adaptive linear regression on real-world dataset; compare to mean imputation and joint heuristic with linear model

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the methodology raises several areas for further investigation regarding performance in complex non-linear scenarios, theoretical justification for NMAR superiority, and extensive real-world testing across diverse datasets.

## Limitations
- Equivalence between adaptive linear regression and joint imputation+regression requires careful implementation verification
- Computational complexity of joint impute-then-regress heuristic not fully characterized for high-dimensional data
- Theoretical guarantees assume bounded feature values and known variance bounds

## Confidence
- High confidence: Theoretical framework connecting adaptive optimization to missing data prediction is sound
- Medium confidence: 2-10% empirical improvements based on specific experimental conditions may vary with different datasets
- Medium confidence: Agnostic approach to missing data mechanisms is conceptually valid but may have practical limitations

## Next Checks
1. Implement Algorithm 2 (joint impute-then-regress) with different stopping criteria and measure computational overhead
2. Test adaptive linear regression framework on datasets with known MAR vs NMAR mechanisms to verify mechanism-agnostic claims
3. Benchmark against more sophisticated imputation methods (e.g., MICE) beyond simple mean imputation to assess relative performance