---
ver: rpa2
title: 'Rewiring Techniques to Mitigate Oversquashing and Oversmoothing in GNNs: A
  Survey'
arxiv_id: '2411.17429'
source_url: https://arxiv.org/abs/2411.17429
tags:
- graph
- rewiring
- information
- nodes
- curvature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Graph neural networks (GNNs) face two critical challenges: oversquashing,
  where information from distant nodes is excessively compressed, and oversmoothing,
  where node representations become indistinguishable through repeated message passing.
  These issues are particularly problematic in heterophilic graphs where connected
  nodes have dissimilar labels.'
---

# Rewiring Techniques to Mitigate Oversquashing and Oversmoothing in GNNs: A Survey

## Quick Facts
- arXiv ID: 2411.17429
- Source URL: https://arxiv.org/abs/2411.17429
- Reference count: 40
- Graph neural networks face oversquashing (excessive information compression from distant nodes) and oversmoothing (node representations becoming indistinguishable through repeated message passing), particularly problematic in heterophilic graphs

## Executive Summary
This survey comprehensively reviews graph rewiring techniques that modify graph topology to address two critical challenges in GNNs: oversquashing and oversmoothing. These issues occur when information from distant nodes becomes excessively compressed or when node representations become indistinguishable through repeated message passing. The paper categorizes rewiring methods based on their approach (edge addition, edge removal, or graph rebuilding) and whether they use local metrics (like curvature measures) or global metrics (like spectral gap). The survey provides detailed analysis of various rewiring methods, their computational complexities, sensitivity to hyperparameters, and evaluation on standard benchmarks.

## Method Summary
The survey systematically categorizes graph rewiring techniques into edge addition, edge removal, and graph rebuilding approaches. Edge addition methods identify bottleneck structures and add edges to create alternative paths for information flow, often using local metrics like Ollivier curvature or global metrics like effective resistance. Edge removal techniques eliminate redundant or noisy edges, particularly in dense regions, using criteria such as random walks or spectral analysis. Graph rebuilding methods completely reconstruct the graph based on node features, using approaches like Delaunay triangulation. These rewiring techniques are implemented as preprocessing steps before GNN training, with evaluation performed on standard graph classification and node classification benchmarks, particularly heterophilic datasets.

## Key Results
- Graph rewiring techniques can effectively mitigate oversquashing by adding edges around bottleneck structures, improving information flow from distant nodes
- Edge removal methods help prevent oversmoothing by eliminating redundant connections in dense regions while preserving important local structure
- Many rewiring approaches face trade-offs between computational efficiency and effectiveness, particularly for large-scale graphs with complex topologies

## Why This Works (Mechanism)
Graph neural networks struggle with information propagation in graphs with complex topologies. Oversquashing occurs when information from distant nodes is compressed through narrow bottlenecks, while oversmoothing happens when repeated message passing makes node representations indistinguishable. Rewiring techniques address these issues by modifying the graph structure to create more efficient information pathways. Edge addition methods alleviate bottlenecks by providing alternative routes for information to travel, while edge removal techniques reduce redundancy and noise that contribute to oversmoothing. The effectiveness of these approaches depends on accurately identifying structural properties that cause these problems, whether through local metrics like curvature or global metrics like spectral gap.

## Foundational Learning
**Graph Neural Networks**: Neural networks designed to operate on graph-structured data, aggregating information from neighboring nodes through message passing. Why needed: Understanding GNN fundamentals is essential to grasp why oversquashing and oversmoothing occur. Quick check: Verify understanding of how information flows through GNN layers.

**Graph Topology**: The structure of connections between nodes in a graph, including properties like diameter, clustering coefficient, and centrality measures. Why needed: Rewiring techniques fundamentally modify graph topology to address GNN limitations. Quick check: Identify bottleneck structures and dense regions in sample graphs.

**Curvature Measures**: Mathematical tools like Ollivier curvature that quantify the "curvature" of graph edges, indicating how information flows through them. Why needed: Many rewiring methods use curvature to identify bottleneck edges that need additional connections. Quick check: Calculate Ollivier curvature for simple graph examples.

**Spectral Graph Theory**: The study of graphs through the eigenvalues and eigenvectors of matrices associated with the graph (adjacency matrix, Laplacian). Why needed: Global rewiring methods often rely on spectral properties to optimize information flow. Quick check: Compute the spectral gap of sample graphs.

## Architecture Onboarding

**Component Map**: Raw Graph Data -> Graph Rewiring (Local/Global Metrics) -> Modified Graph -> GNN Model -> Performance Evaluation

**Critical Path**: The most critical path is from the original graph through the rewiring step to the modified graph that serves as input to the GNN. The choice and implementation of rewiring technique directly impacts GNN performance, making this preprocessing step crucial.

**Design Tradeoffs**: The primary tradeoff is between computational efficiency and effectiveness. Local metrics are computationally efficient but may miss global structural issues, while global metrics are more comprehensive but computationally expensive. Another tradeoff exists between adding edges to alleviate oversquashing versus removing edges to prevent oversmoothing.

**Failure Signatures**: Rewiring may lead to loss of important local graph structure, degrading performance on tasks that rely on local information. Methods may be highly sensitive to hyperparameters, leading to poor results if not properly tuned. Some techniques may create disconnected components or significantly alter the graph's original properties.

**3 First Experiments**:
1. Implement a baseline GNN model (e.g., GCN) on standard graph datasets (e.g., Cora, Citeseer) to establish baseline performance
2. Implement curvature-based rewiring using Ollivier curvature as a preprocessing step and evaluate the rewired GNN's performance
3. Compare the performance of local metric-based rewiring versus global metric-based rewiring on the same dataset

## Open Questions the Paper Calls Out

### Open Question 1
How can we effectively combine structural rewiring with feature rewiring to maximize the benefits of both approaches in mitigating oversquashing and oversmoothing? The paper suggests that combining original feature rewiring with structural rewiring could be a particularly interesting avenue for future research. This remains unresolved because the interaction between graph structure and node features is a domain that remains largely unexplored. Empirical studies demonstrating improved performance on benchmark datasets when combining both rewiring approaches, along with theoretical analysis of how feature and structural modifications interact, would resolve this question.

### Open Question 2
What is the optimal balance between adding edges to alleviate oversquashing and removing edges to prevent oversmoothing, particularly for large-scale graphs? Current methods are highly sensitive to hyperparameters and the specific graph being analyzed, making it difficult to determine a universal optimal strategy. This question remains unresolved because finding the right balance between adding edges and preserving local structure is a delicate task. Comprehensive experimental studies across diverse graph types and sizes, showing the impact of different edge addition/removal ratios on both oversquashing and oversmoothing metrics, would provide evidence to resolve this question.

### Open Question 3
How does the interaction between graph structure and node features influence the effectiveness of rewiring techniques in heterophilic graphs? The paper highlights that this interaction plays a crucial role and warrants further exploration to improve GNN performance. This remains unresolved because current research has primarily focused on structural modifications without deeply investigating how node features might influence or be influenced by these changes. Detailed analysis of how feature distributions correlate with structural bottlenecks and how rewiring strategies might be adapted based on feature homophily/heterophily patterns would resolve this question.

## Limitations
- The classification of rewiring methods into local versus global metrics may oversimplify some hybrid approaches that combine both perspectives
- Computational complexity analyses are based on theoretical worst-case scenarios rather than empirical measurements of actual implementation performance
- The survey acknowledges trade-offs between effectiveness and efficiency but doesn't provide quantitative benchmarks across all methods for direct comparison

## Confidence

**High**: The existence of oversquashing and oversmoothing problems in GNNs
**Medium**: The categorization of rewiring techniques by approach (edge addition/removal/rebuilding)
**Low**: Specific performance claims for individual rewiring methods without direct empirical validation

## Next Checks
1. Implement and compare at least three rewiring methods (one from each category: local metric, global metric, hybrid) on a standard heterophilic dataset, measuring both performance gains and computational overhead
2. Conduct sensitivity analysis on key hyperparameters across different rewiring approaches to quantify their impact on both effectiveness and efficiency
3. Design ablation studies to isolate the contribution of rewiring from other factors (like GNN architecture choices) by comparing performance on rewired versus original graphs using identical GNN models