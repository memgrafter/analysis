---
ver: rpa2
title: Navigating Towards Fairness with Data Selection
arxiv_id: '2412.11072'
source_url: https://arxiv.org/abs/2412.11072
tags: []
core_contribution: The paper introduces a data selection method to mitigate label
  bias and improve fairness in machine learning, addressing the limitations of existing
  fairness techniques that require model modifications. The core idea is to use a
  zero-shot predictor as a proxy model to simulate training on a clean holdout set,
  eliminating the need for additional holdout data.
---

# Navigating Towards Fairness with Data Selection

## Quick Facts
- arXiv ID: 2412.11072
- Source URL: https://arxiv.org/abs/2412.11072
- Authors: Yixuan Zhang; Zhidong Li; Yang Wang; Fang Chen; Xuhui Fan; Feng Zhou
- Reference count: 27
- One-line primary result: Data selection method using zero-shot predictors achieves higher accuracy and fairness in image classification tasks

## Executive Summary
This paper introduces a data selection method to mitigate label bias and improve fairness in machine learning models without requiring model modifications. The approach leverages a zero-shot predictor as a proxy model to simulate training on a clean holdout set, eliminating the need for additional holdout data. The method incorporates a peer prediction mechanism to ensure fairness and handles selection bias through resampling techniques. Experiments on image classification tasks (CelebA and LFW+a) demonstrate that the proposed method achieves higher accuracy and fairness compared to baseline approaches.

## Method Summary
The proposed method uses a zero-shot predictor as a proxy model to simulate training on a clean holdout set, eliminating the need for additional holdout data. A peer prediction mechanism ensures fairness by comparing predictions from the proxy model and the main model. Selection bias is addressed through resampling techniques that balance the data distribution. The method iteratively selects data points that maximize both accuracy and fairness metrics, using the zero-shot predictor's outputs to guide the selection process without requiring labeled validation data.

## Key Results
- Under 40% label bias, achieved 85.2% accuracy with fairness violation (ΔDP) of 0.20 on CelebA dataset
- Demonstrated faster convergence compared to baseline methods
- Outperformed existing fairness techniques that require model modifications

## Why This Works (Mechanism)
The method works by using zero-shot predictors to estimate model performance on unbiased data without requiring additional labeled data. The peer prediction mechanism creates a feedback loop where the proxy model's unbiased predictions guide data selection for the main model. Resampling techniques address selection bias by ensuring balanced representation across different groups. The iterative selection process gradually improves both accuracy and fairness metrics by focusing on data points that contribute most to unbiased learning.

## Foundational Learning
- Zero-shot prediction: Using models that can make predictions without task-specific training data
  - Why needed: Enables performance estimation without additional labeled data
  - Quick check: Verify proxy model can produce meaningful predictions on target task

- Peer prediction mechanisms: Comparing outputs from multiple models to ensure reliability
  - Why needed: Provides fairness guarantees without explicit labels
  - Quick check: Confirm correlation between proxy and main model predictions

- Selection bias mitigation: Techniques to address biased data sampling
  - Why needed: Ensures fair representation across different groups
  - Quick check: Measure group balance before and after resampling

- Fairness metrics (ΔDP): Statistical measures of group fairness
  - Why needed: Quantifies fairness violations between protected groups
  - Quick check: Calculate disparity in false positive rates across groups

## Architecture Onboarding

**Component map:** Data → Zero-shot predictor → Peer prediction → Resampling → Selected data → Main model → Evaluation

**Critical path:** Zero-shot predictor provides unbiased performance estimates → Peer prediction ensures fairness → Resampling balances data → Main model trained on selected data → Evaluation shows improved accuracy and fairness

**Design tradeoffs:** Uses proxy models instead of real holdout data (saves resources but may introduce approximation errors), iterative selection (improves quality but increases computation time), resampling-based bias mitigation (simple to implement but may discard useful data)

**Failure signatures:** Proxy model predictions diverge from actual model performance, resampling creates extreme class imbalance, iterative selection converges to local optima, fairness metrics show minimal improvement despite selection

**First experiments to run:** 1) Test zero-shot predictor accuracy on clean holdout data, 2) Evaluate peer prediction correlation across different bias levels, 3) Measure impact of resampling ratio on final model performance

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- Evaluation limited to image classification tasks on two datasets (CelebA and LFW+a)
- Effectiveness on other data types (text, tabular) not explored
- Limited exploration of varying bias levels and different bias types
- No comprehensive comparison with model-modification fairness techniques

## Confidence
- High: Method's ability to mitigate label bias and improve fairness in image classification tasks
- Medium: Effectiveness of zero-shot predictor as proxy model and peer prediction mechanism
- Low: Generalizability to other data types and impact of varying bias types and levels

## Next Checks
1. Evaluate the method on a broader range of datasets, including text and tabular data, to assess generalizability
2. Conduct experiments with varying levels of label bias and different bias types to understand robustness
3. Compare the proposed method with other fairness techniques that require model modifications to establish relative performance