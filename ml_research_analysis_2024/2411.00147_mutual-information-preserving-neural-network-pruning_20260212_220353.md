---
ver: rpa2
title: Mutual Information Preserving Neural Network Pruning
arxiv_id: '2411.00147'
source_url: https://arxiv.org/abs/2411.00147
tags:
- pruning
- layer
- mipp
- activations
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses neural network pruning by introducing a method
  that preserves mutual information (MI) between activations of adjacent layers. The
  core idea is to use the transfer entropy redundancy criterion (TERC) to dynamically
  prune nodes whose activations do not transfer entropy to downstream layers, ensuring
  retrainability and maximizing MI between the data and pruning masks.
---

# Mutual Information Preserving Neural Network Pruning

## Quick Facts
- arXiv ID: 2411.00147
- Source URL: https://arxiv.org/abs/2411.00147
- Authors: Charles Westphal; Stephen Hailes; Mirco Musolesi
- Reference count: 40
- Key outcome: MIPP preserves mutual information between adjacent layers, outperforming state-of-the-art pruning methods on MNIST, CIFAR-10, and CIFAR-100 while resisting layer collapse

## Executive Summary
This paper introduces Mutual Information Preserving Pruning (MIPP), a novel neural network pruning method that maintains mutual information (MI) between adjacent layer activations. The approach uses Transfer Entropy Redundancy Criterion (TERC) to dynamically identify and remove nodes that don't transfer entropy to downstream layers, ensuring retrainability while maximizing sample efficiency. MIPP was evaluated across multiple datasets and models, demonstrating consistent improvements over existing pruning methods.

## Method Summary
MIPP applies structured pruning by preserving mutual information between activations of adjacent layers. The method uses TERC with MI ordering to iteratively remove nodes whose activations don't transfer entropy to downstream layers. The algorithm first computes MI between consecutive layers, then prunes nodes that don't contribute to information flow while maintaining a regularization constraint that controls per-layer sparsity ratios. After pruning, the network can be retrained to recover performance. The approach works both before and after training, adapting to different sparsity requirements.

## Key Results
- MIPP consistently outperformed state-of-the-art pruning methods (IMP, SOSP-H, OTO, ThiNet, IterSNIP, IterGraSP, ProsPr, SynFlow) across all tested datasets and models
- MIPP showed better resistance to layer collapse compared to baseline methods when applying high sparsity ratios
- The method successfully preserved accuracy while achieving significant computational savings on LeNet5 (MNIST), ResNet variants (CIFAR-10/100), and VGG variants (CIFAR-100)

## Why This Works (Mechanism)

### Mechanism 1
**Claim**: Preserving MI between adjacent layers ensures retrainability.
**Mechanism**: If MIPP preserves MI between activations of adjacent layers, then there exists a function that can map pruned upstream layer activations to downstream layer activations. This function can be discovered by gradient descent, enabling successful retraining.
**Core assumption**: MI preservation between layers implies functional equivalence between pruned and unpruned networks for those layers.
**Evidence anchors**:
- [abstract]: "The core principle of MIPP is to select nodes in a way that conserves MI shared between the activations of adjacent layers"
- [section 4]: "There exists a function g such that the activations of the subsequent layer can be re-formed from the pruned layer iff MI between these two layers is not affected by pruning"
- [corpus]: Weak evidence - no direct citation of MI-based pruning theory in related papers
**Break condition**: If MI preservation doesn't actually guarantee functional equivalence, or if gradient descent cannot discover the required function.

### Mechanism 2
**Claim**: Maximizing I(D; M) maximizes sample efficiency and accuracy.
**Mechanism**: By preserving MI between layers, MIPP ensures that the pruning masks retain maximal information about the training data. Since sample efficiency is proportional to MI between masks and data, this leads to better performance.
**Core assumption**: The relationship between I(D; M) and sample efficiency/accuracy holds across different pruning scenarios.
**Evidence anchors**:
- [abstract]: "the sample efficiency of robust pruned models is proportional to the mutual information (MI) between the pruning masks and the model's training datasets"
- [section 4]: "Theorem 2: If a pruning method preserves MI between layers activations then the upper bound on I(D; M) reaches its maximum"
- [corpus]: Weak evidence - related papers focus on pruning efficiency but not specifically on MI-based approaches
**Break condition**: If the relationship between I(D; M) and performance breaks down for certain network architectures or datasets.

### Mechanism 3
**Claim**: TERC with MI ordering dynamically removes redundant nodes while preserving information flow.
**Mechanism**: TERC sequentially evaluates nodes and removes those that don't transfer entropy to the downstream layer. MI ordering ensures that perfectly redundant variable subsets of different cardinalities don't cause issues.
**Core assumption**: Dynamic evaluation based on current network structure is more effective than static ranking for identifying redundant nodes.
**Evidence anchors**:
- [section 5.1.1]: "TERC (Westphal et al., 2024) to dynamically prune nodes whose activations do not transfer entropy to the downstream layer"
- [section 5.1.1]: "TERC alone selects unnecessary variables if there exists perfectly redundant variable subsets of different cardinalities"
- [corpus]: Weak evidence - no direct citation of TERC in related papers
**Break condition**: If the sequential evaluation process fails to identify all redundant nodes or removes too many informative nodes.

## Foundational Learning

- **Mutual Information (MI)**:
  - Why needed here: MI is the core metric used to determine whether nodes transfer useful information between layers and whether pruning masks retain information about the data
  - Quick check question: If X and Y are independent random variables, what is their mutual information?

- **Transfer Entropy**:
  - Why needed here: Transfer entropy measures the reduction in uncertainty about future states of a process given knowledge of another process, which is used to determine if nodes transfer useful information
  - Quick check question: How does transfer entropy differ from standard mutual information in measuring information flow?

- **Retrainability**:
  - Why needed here: Understanding when a pruned network can be successfully retrained is crucial for validating the pruning approach
  - Quick check question: What conditions must be met for a pruned network to be retrainable?

## Architecture Onboarding

- **Component map**: Input layer → Hidden layers → Output layer, with each layer having activation functions and weight matrices, pruning masks applied to weight matrices, and TERC algorithm applied layer by layer

- **Critical path**: 1) Forward pass to compute activations, 2) TERC evaluation to determine redundant nodes, 3) Apply pruning masks, 4) Retraining to recover performance

- **Design tradeoffs**: Global vs local pruning (MIPP uses global approach), speed vs accuracy in MI estimation, layer-wise vs network-wide sparsity ratios

- **Failure signatures**: Layer collapse (entire layer pruned), performance degradation after pruning, inability to retrain pruned network

- **First 3 experiments**: 1) Apply MIPP to a simple LeNet5 on MNIST and verify basic functionality, 2) Test MIPP at different sparsity levels on CIFAR-10 to find performance boundaries, 3) Compare MIPP against magnitude pruning on a small ResNet to validate improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does MIPP's effectiveness depend on the specific architecture of the neural network, or is it equally effective across different types of models?
- Basis in paper: [inferred] The paper demonstrates MIPP's effectiveness on various models including LeNet5, ResNet variants, and VGG variants, but does not explicitly test its effectiveness on recurrent neural networks or transformers.
- Why unresolved: The experiments only cover convolutional and fully connected neural networks, leaving the question of MIPP's applicability to other architectures unanswered.
- What evidence would resolve it: Applying MIPP to recurrent neural networks, transformers, or other architectures and comparing its performance to state-of-the-art pruning methods for those architectures.

### Open Question 2
- Question: How does MIPP perform in scenarios with limited or imbalanced training data compared to other pruning methods?
- Basis in paper: [explicit] The paper mentions that Kumar et al. (2024) showed that sample efficiency is proportional to mutual information between pruning masks and training data, but does not explicitly test MIPP's performance with limited or imbalanced data.
- Why unresolved: The experiments use standard benchmark datasets with balanced classes and sufficient samples, not exploring scenarios with data scarcity or imbalance.
- What evidence would resolve it: Evaluating MIPP's performance on datasets with varying levels of class imbalance and different amounts of training data, comparing it to other pruning methods under these conditions.

### Open Question 3
- Question: Can MIPP be extended to perform structured pruning of weights within layers, not just nodes or filters?
- Basis in paper: [inferred] MIPP is described as a structured activation-based pruning method that preserves mutual information between adjacent layers, but the paper focuses on node-level pruning rather than weight-level pruning.
- Why unresolved: The paper does not explore the possibility of extending MIPP to prune individual weights within layers while preserving mutual information.
- What evidence would resolve it: Developing an extension of MIPP that can prune weights within layers, preserving mutual information between adjacent layers, and evaluating its performance compared to existing weight-level pruning methods.

## Limitations

- Limited empirical validation of theoretical claims about MI preservation and retrainability through ablation studies
- Reliance on TERC for dynamic node evaluation introduces potential brittleness as sequential evaluation may miss optimal pruning patterns
- Claims about resistance to layer collapse are primarily supported by comparison results rather than fundamental architectural guarantees

## Confidence

- **High confidence**: Empirical results showing MIPP outperforms baselines across multiple datasets and models
- **Medium confidence**: The relationship between MI preservation and retrainability (based on theoretical arguments but limited empirical verification)
- **Low confidence**: The claim that maximizing I(D; M) directly translates to sample efficiency improvements (theoretical claim with minimal empirical support)

## Next Checks

1. Conduct ablation studies comparing MIPP against static pruning methods while measuring actual mutual information values between layers to verify the claimed preservation
2. Test MIPP on additional architectures (transformer-based models) and datasets (ImageNet) to evaluate generalizability beyond the current scope
3. Implement and verify the Cover et al. (2020) MI estimation method independently to confirm the reported efficiency gains aren't implementation-dependent