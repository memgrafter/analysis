---
ver: rpa2
title: Transformer-Based Contextualized Language Models Joint with Neural Networks
  for Natural Language Inference in Vietnamese
arxiv_id: '2411.13407'
source_url: https://arxiv.org/abs/2411.13407
tags:
- language
- neural
- vietnamese
- bilstm
- xlm-r
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of Vietnamese Natural Language
  Inference (NLI), where existing approaches are limited and performance needs improvement.
  The proposed method combines Transformer-based contextualized language models (CLMs)
  like XLM-R and PhoBERT with neural networks such as CNN and BiLSTM.
---

# Transformer-Based Contextualized Language Models Joint with Neural Networks for Natural Language Inference in Vietnamese

## Quick Facts
- arXiv ID: 2411.13407
- Source URL: https://arxiv.org/abs/2411.13407
- Authors: Dat Van-Thanh Nguyen; Tin Van Huynh; Kiet Van Nguyen; Ngan Luu-Thuy Nguyen
- Reference count: 40
- One-line primary result: Achieves 82.78% F1 score on ViNLI benchmark using XLM-R + BiLSTM joint model

## Executive Summary
This paper addresses Vietnamese Natural Language Inference by combining Transformer-based contextualized language models with neural networks. The proposed approach leverages the semantic understanding of models like XLM-R and PhoBERT while using CNN or BiLSTM networks for task-specific feature extraction. The method achieves state-of-the-art performance on the ViNLI dataset, outperforming fine-tuned transformer models alone by significant margins.

## Method Summary
The approach processes Vietnamese sentence pairs through transformer-based CLMs (XLM-R, PhoBERT, mBERT) to generate contextualized embeddings, which are then fed into either CNN or BiLSTM neural networks for classification. The system uses Vietnamese tokenization via VnCoreNLP, [CLS]/[SEP] special tokens for sentence pair processing, and is trained with batch size 64 (32 for XLM-R+BiLSTM), learning rate 1e-5, and dropout=0.1. The evaluation uses macro-averaged F1-score and accuracy on the ViNLI dataset with four labels: entailment, contradiction, neutral, and other.

## Key Results
- Achieves 82.78% F1 score on ViNLI benchmark, outperforming fine-tuned PhoBERT (+6.58%), mBERT (+19.08%), and XLM-R (+0.94%)
- XLM-R model with 355M parameters demonstrates superior performance across all experiments
- Joint models consistently outperform individual fine-tuned transformer models, validating the effectiveness of combining CLMs with neural networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contextualized vectors from transformer models outperform non-contextualized embeddings by encoding bidirectional context and semantic relationships between words.
- Mechanism: Transformer-based CLMs like XLM-R and PhoBERT generate contextualized embeddings by processing the entire sentence in both directions, allowing