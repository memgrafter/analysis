---
ver: rpa2
title: 'SuperMerge: An Approach For Gradient-Based Model Merging'
arxiv_id: '2412.10416'
source_url: https://arxiv.org/abs/2412.10416
tags:
- merging
- tasks
- merge
- task
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SuperMerge, a gradient-based approach for
  merging fine-tuned models trained on different tasks. Existing model merging techniques
  face challenges in selecting optimal hyperparameters and handling layer-specific
  variations across tasks.
---

# SuperMerge: An Approach For Gradient-Based Model Merging

## Quick Facts
- arXiv ID: 2412.10416
- Source URL: https://arxiv.org/abs/2412.10416
- Reference count: 17
- The paper introduces SuperMerge, achieving 5.8% average accuracy improvement and up to 49.4% per-task improvement in model merging.

## Executive Summary
This paper introduces SuperMerge, a gradient-based approach for merging fine-tuned models trained on different tasks. Existing model merging techniques face challenges in selecting optimal hyperparameters and handling layer-specific variations across tasks. SuperMerge addresses these issues by introducing trainable weights for each layer, which are optimized using a small validation set. This allows the method to adaptively merge models at the appropriate granularity, resulting in improved performance. The authors demonstrate that SuperMerge outperforms existing model merging methods on both natural language processing and computer vision tasks, achieving an average accuracy improvement of 5.8% and up to 49.4% per-task improvement. Furthermore, a hierarchical merging strategy is proposed to reduce memory requirements without sacrificing performance.

## Method Summary
SuperMerge is a gradient-based model merging technique that introduces trainable weights for each layer of fine-tuned models, optimizing these weights using a small validation set to adaptively combine models. Unlike existing methods that use a single global scaling hyperparameter, SuperMerge learns layer-specific weights with tanh constraints to prevent overfitting. The method employs hierarchical merging to reduce memory requirements when combining multiple models. The approach is evaluated on 11 NLP tasks using T0-3B with IA3 PEFT and 8 CV tasks using ViT-B/32 CLIP, demonstrating significant improvements over baselines like Task Arithmetic, TIES, DARE, and AdaMerging.

## Key Results
- Achieves 5.8% average accuracy improvement over existing model merging methods
- Demonstrates up to 49.4% per-task improvement in certain cases
- Successfully merges models across both NLP and computer vision domains
- Hierarchical merging reduces memory requirements without sacrificing performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** SuperMerge achieves better model merging performance than existing methods by learning layer-specific weights using a small validation set.
- **Mechanism:** Instead of using a single global scaling hyperparameter λ as in Task Arithmetic, DARE, and TIES, SuperMerge introduces a trainable weight w(i,j) for each layer j of each fine-tuned model i. These weights are optimized via gradient descent on a small validation set, allowing the method to adaptively combine layers based on their importance for each task.
- **Core assumption:** The contribution of each layer to the merged model's performance varies across tasks, and this variation can be learned from validation data.
- **Evidence anchors:**
  - [abstract] "SuperMerge addresses these issues by introducing trainable weights for each layer, which are optimized using a small validation set."
  - [section] "Crucially, the weighting parameter λ involved in the Task Arithmetic, TIES-Merging, and DARE can be characterized as a form of hyperparameter, whose optimal value is found using the validation data set Dval."
  - [corpus] Weak - No direct evidence from corpus papers, but related work like Sens-Merging and SE-Merging also focus on parameter balancing and self-enhancement.
- **Break condition:** If the validation set is too small or not representative of the test distribution, the learned layer weights may not generalize well, leading to poor performance.

### Mechanism 2
- **Claim:** The tanh non-linearity in SuperMerge's weight computation prevents overfitting and improves generalization by constraining merging weights to (-1, 1).
- **Mechanism:** By applying tanh to the trainable weights w(i,j), SuperMerge ensures that the contribution of each layer to the merged model is bounded between -1 and 1. This prevents any single layer or model from dominating the merged model and helps maintain balance across tasks.
- **Core assumption:** Unbounded merging weights can lead to overfitting or dominance by certain layers, harming generalization.
- **Evidence anchors:**
  - [abstract] "By adding the tanh non-linearity, we ensure that the merging weights can only wiggle between (−1, 1). As we will experimentally demonstrate in Section 6.3, this approach improves performance by avoiding overfitting and improving generalization."
  - [section] "Another observation is that the merging weights learned by AdaMerging are sparse and merging only occurs in certain layers, e.g., the layers correspond to vertical blue strips."
  - [corpus] Weak - No direct evidence from corpus papers, but related work on sensitivity-guided merging may implicitly use bounded parameters.
- **Break condition:** If the tanh constraint is too restrictive, it may prevent the model from learning optimal layer contributions, especially if some layers truly need to contribute more than a factor of 1.

### Mechanism 3
- **Claim:** Hierarchical merging in SuperMerge reduces memory requirements without sacrificing performance by merging related models first.
- **Mechanism:** Instead of merging all k models at once (requiring O(k) memory for task vectors), SuperMerge merges models in a tree-like structure. First, models fine-tuned on similar tasks are merged to form intermediate models, which are then recursively merged. This reduces the peak memory requirement from O(k) to O(log k).
- **Core assumption:** Models fine-tuned on similar tasks share more similar task vectors, so merging them first preserves task-specific knowledge more effectively.
- **Evidence anchors:**
  - [abstract] "Furthermore, we proposed a hierarchical model merging strategy to reduce the peak space requirement without sacrificing the performance of the merged model."
  - [section] "The hierarchical approach limits the number of models that are simultaneously merged by forming a tree-like merging structure."
  - [corpus] Weak - No direct evidence from corpus papers, but related work on modular delta merging may use similar hierarchical approaches.
- **Break condition:** If the task similarity grouping is poor (e.g., merging unrelated tasks first), the hierarchical approach may not preserve task-specific knowledge as effectively, leading to performance degradation.

## Foundational Learning

- **Concept:** Task vectors and their role in model merging
  - Why needed here: SuperMerge builds upon the concept of task vectors (τ(i) = θf(i) - θp) introduced in earlier work. Understanding how task vectors represent the difference between fine-tuned and pre-trained models is crucial for grasping SuperMerge's approach.
  - Quick check question: What is a task vector, and how is it computed in the context of model merging?

- **Concept:** Gradient-based optimization vs. hyperparameter tuning
  - Why needed here: SuperMerge replaces the grid search for λ (used in Task Arithmetic, DARE, and TIES) with gradient-based optimization of layer weights. Understanding this shift from hyperparameter tuning to end-to-end learning is key to appreciating SuperMerge's advantages.
  - Quick check question: How does SuperMerge's approach to finding optimal weights differ from the grid search used in Task Arithmetic?

- **Concept:** Parameter-efficient fine-tuning (PEFT) methods
  - Why needed here: SuperMerge works with models that have been fine-tuned using PEFT methods like LoRA and IA3. Understanding how PEFT modifies the model (e.g., adding low-rank matrices) is important for interpreting the experimental results and understanding the practical applicability of SuperMerge.
  - Quick check question: What is the main idea behind LoRA, and how does it differ from full fine-tuning?

## Architecture Onboarding

- **Component map:**
  Pre-trained model weights (θp) -> Fine-tuned model weights (θf(i)) -> Task vectors (τ(i,j)) -> Trainable layer weights (w(i,j)) -> Merged model weights (θm)

- **Critical path:**
  1. Compute task vectors for each layer of each fine-tuned model
  2. Initialize trainable layer weights w(i,j)
  3. For each iteration:
     a. Compute merged model weights using Eq. (4)
     b. Evaluate loss on validation set
     c. Backpropagate gradients to update w(i,j)
  4. Return final merged model weights θm

- **Design tradeoffs:**
  - SuperMerge introduces O(n*k) trainable parameters (n layers, k models), which is much smaller than the full model parameters but larger than the single λ used in other methods. This tradeoff allows for better performance at the cost of slightly more computation.
  - The hierarchical merging strategy trades off some memory efficiency (due to intermediate models) for reduced peak memory usage, which can be crucial for very large models.

- **Failure signatures:**
  - If the validation set is too small or not representative, the learned weights may not generalize, leading to poor test performance.
  - If the tanh constraint is too restrictive, the model may not be able to learn optimal layer contributions.
  - If the task similarity grouping in hierarchical merging is poor, the performance may degrade compared to flat merging.

- **First 3 experiments:**
  1. Merge two T5 models fine-tuned on different tasks using SuperMerge and compare performance to Task Arithmetic with optimal λ.
  2. Visualize the learned layer weights w(i,j) for a simple case (e.g., 2 models, 3 layers) to understand how SuperMerge adapts to task differences.
  3. Test the effect of the tanh constraint by running SuperMerge with and without tanh and comparing performance.

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but several areas warrant further investigation:

1. **Scalability to many tasks**: How does the hierarchical merging strategy's performance scale with the number of tasks being merged, particularly when merging more than 11 tasks?

2. **Cross-domain merging limits**: What is the theoretical limit of SuperMerge's ability to handle tasks with completely unrelated domains or incompatible representations?

3. **Heterogeneous architecture merging**: How does SuperMerge's performance compare when merging models with different architectures (e.g., BERT vs. T5, or CNN vs. Transformer) rather than identical architectures?

## Limitations

- **Validation set dependency**: Performance critically depends on having a representative small validation set to learn layer weights, with no thorough exploration of how validation set size or quality affects results.
- **Computational overhead**: Introduces O(n*k) trainable parameters, which could become computationally expensive for very large models or many tasks.
- **Architecture generalization**: Experiments focus on T5 variants for NLP and ViT for vision, with effectiveness on other architectures remaining untested.

## Confidence

- **High Confidence**: The core mechanism of using layer-specific trainable weights with gradient optimization is sound and well-explained.
- **Medium Confidence**: The experimental results showing 5.8% average improvement are impressive but rely heavily on specific implementation details of baseline methods.
- **Low Confidence**: The claim about hierarchical merging's effectiveness is based on limited ablation studies.

## Next Checks

1. **Ablation on validation set size**: Systematically vary the validation set size (1%, 5%, 10% of training data) to determine the minimum viable size and test robustness to distribution shifts.

2. **Cross-architecture testing**: Apply SuperMerge to merge models with different architectures (e.g., BERT + RoBERTa, ResNet + EfficientNet) to test architectural generalization.

3. **Extreme case analysis**: Test SuperMerge on merging models with completely unrelated tasks (e.g., medical imaging + natural language) to identify failure modes and limitations.