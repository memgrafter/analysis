---
ver: rpa2
title: 'Vidur: A Large-Scale Simulation Framework For LLM Inference'
arxiv_id: '2405.05465'
source_url: https://arxiv.org/abs/2405.05465
tags:
- vidur
- inference
- configuration
- latency
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Vidur is a high-fidelity, large-scale simulation framework for
  optimizing LLM inference deployment. It models performance using experimental profiling
  and predictive modeling to estimate latency, throughput, and cluster-level metrics
  with under 9% error across models.
---

# Vidur: A Large-Scale Simulation Framework For LLM Inference

## Quick Facts
- arXiv ID: 2405.05465
- Source URL: https://arxiv.org/abs/2405.05465
- Authors: Amey Agrawal; Nitin Kedia; Jayashree Mohan; Ashish Panwar; Nipun Kwatra; Bhargav Gulavani; Ramachandran Ramjee; Alexey Tumanov
- Reference count: 40
- Primary result: Simulation framework achieving <9% error in latency/throughput predictions for LLM inference optimization

## Executive Summary
Vidur is a high-fidelity, large-scale simulation framework for optimizing LLM inference deployment. It models performance using experimental profiling and predictive modeling to estimate latency, throughput, and cluster-level metrics with under 9% error across models. Vidur-Search leverages the simulator to automatically identify cost-effective configurations, reducing the search cost from 42K GPU hours ($218K) to one hour on a CPU machine ($9.93). The framework addresses the challenge of exploring vast configuration spaces formed by parallelization strategies, batching techniques, and scheduling policies while accounting for workload-specific optimal configurations.

## Method Summary
Vidur decomposes models into shared operators and uses predictive modeling for runtime estimation. It identifies token-level, sequence-level, and communication operators, profiling minimal input sizes and training machine learning models to predict kernel runtimes. The hierarchical scheduler architecture manages request routing, batching policies, and memory allocation across three tiers. Vidur-Search enumerates deployment configurations and uses binary search to find maximum QPS capacity, evaluating configurations through simulation rather than hardware deployment.

## Key Results
- Achieves <9% error in latency and throughput predictions across LLaMA2 7B/70B, InternLM-20B, and Qwen-72B models
- Reduces configuration search cost from 42K GPU hours ($218K) to one hour on CPU ($9.93) using Vidur-Search
- Validates fidelity on three diverse workloads: LMSys-Chat-1M, Arxiv-Summarization, and Bilingual-Web-Book with max 4096 tokens

## Why This Works (Mechanism)

### Mechanism 1
Vidur achieves high fidelity LLM inference simulation by decomposing models into shared operators and using predictive modeling. It analyzes model specifications to identify token-level, sequence-level, and communication operators, profiling minimal input sizes and training random forest models to predict kernel runtimes. Core assumption: most LLMs share similar transformer architecture patterns enabling unified modeling. Break condition: models deviating significantly from common transformer architecture may cause operator triaging to fail.

### Mechanism 2
Vidur's hierarchical scheduler accurately simulates request routing, batching, and memory management. The three-tier architecture (global, replica, stage schedulers) manages routing, batching policies, and KV-cache allocation independently. Core assumption: scheduling behavior at each tier can be modeled independently without significant cross-tier interference. Break condition: complex cross-tier dependencies not captured by the three-tier model.

### Mechanism 3
Vidur-Search finds optimal deployment configurations by leveraging simulation to avoid expensive hardware experimentation. It enumerates configurations and uses binary search to find maximum QPS capacity, evaluating through simulation metrics instead of actual deployment. Core assumption: simulation accurately predicts real-world performance across configuration space. Break condition: simulation errors accumulate or configuration space has discontinuities not captured by search algorithm.

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Essential for understanding how Vidur decomposes models into operators and why the approach works
  - Quick check question: What are the two dominant submodules in transformer-based LLMs, and how do they differ in their computational characteristics?

- Concept: Parallelism strategies (Tensor Parallelism vs Pipeline Parallelism)
  - Why needed here: Critical for interpreting simulation results and deployment recommendations
  - Quick check question: What are the primary differences between Tensor Parallelism and Pipeline Parallelism in terms of communication patterns and memory usage?

- Concept: Request scheduling and batching policies
  - Why needed here: Necessary to interpret how workload characteristics affect optimal configurations
  - Quick check question: How do prefill-prioritizing and decode-prioritizing scheduling policies differ in their approach to batching and their impact on latency vs throughput?

## Architecture Onboarding

- Component map: Model specification → Operator identification → Profiling → Runtime prediction → Configuration enumeration → Simulation → Optimization
- Critical path: Model specification → Operator identification → Profiling (minimal) → Runtime prediction model training → Configuration enumeration → Simulation → Optimization
- Design tradeoffs: Profiling comprehensiveness vs cost (minimal profiling with predictive modeling), simulation accuracy vs speed (fine-grained vs faster approximations), search space coverage vs computational resources (comprehensive vs targeted approaches)
- Failure signatures: High prediction errors (>9%) indicate modeling issues, search failures suggest constraints too tight or mischaracterized space, memory prediction divergence indicates KV-cache modeling problems
- First 3 experiments: 1) Profile LLaMA2-7B with single parallelism configuration and validate predictions, 2) Run simulation with static workloads for single configuration and compare metrics, 3) Execute Vidur-Search for simple case and verify cost-effectiveness improvement

## Open Questions the Paper Calls Out
No specific open questions were called out in the paper.

## Limitations
- Simulation fidelity boundaries not extensively characterized across diverse workload types, model architectures, and hardware configurations
- Static configuration assumption limits effectiveness in dynamic environments with fluctuating request rates or changing SLO requirements
- Three-bucket operator classification may not capture all performance nuances for models with specialized operators or non-standard attention mechanisms

## Confidence

**High Confidence**: Core simulation methodology using experimental profiling and predictive modeling for runtime estimation is well-supported by experimental results showing <9% average error across diverse models and configurations.

**Medium Confidence**: Hierarchical scheduler architecture's ability to accurately model complex scheduling behaviors across different policies is demonstrated but not extensively validated across edge cases or extreme load conditions.

**Medium Confidence**: Configuration search effectiveness is empirically validated but relies heavily on simulation accuracy, with potential simulation-to-reality gaps not addressed.

## Next Checks

1. **Error Distribution Analysis**: Validate the 9% error claim by examining error distribution across different model sizes, workload types, and hardware configurations, specifically testing edge cases like unusual attention patterns or extreme sequence length distributions.

2. **Dynamic Workload Testing**: Evaluate Vidur's performance under realistic, dynamic workloads with varying request rates and SLO requirements, comparing configuration recommendations against actual hardware experimentation under similar conditions.

3. **Operator Model Generalization**: Test the three-bucket operator classification on models outside the standard transformer family (Mamba, RWKV, or models with specialized operators), assessing whether predictive modeling maintains accuracy for these architectures.