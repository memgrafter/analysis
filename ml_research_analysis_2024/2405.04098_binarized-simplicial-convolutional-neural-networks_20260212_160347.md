---
ver: rpa2
title: Binarized Simplicial Convolutional Neural Networks
arxiv_id: '2405.04098'
source_url: https://arxiv.org/abs/2405.04098
tags:
- simplicial
- bi-scnn
- graph
- scnn
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Binarized Simplicial Convolutional Neural Networks
  (Bi-SCNN) to address the computational inefficiency and over-smoothing issues of
  existing simplicial complex-based neural networks. The key idea is to combine simplicial
  convolution with weighted binary-sign propagation and feature normalization, using
  the Hodge Laplacian for high-order structure representation.
---

# Binarized Simplicial Convolutional Neural Networks

## Quick Facts
- arXiv ID: 2405.04098
- Source URL: https://arxiv.org/abs/2405.04098
- Authors: Yi Yan; Ercan E. Kuruoglu
- Reference count: 6
- Primary result: Bi-SCNN achieves similar or better accuracy than SCNN, SNN, and MPNN while being significantly faster on citation and ocean-drifter datasets

## Executive Summary
This paper introduces Binarized Simplicial Convolutional Neural Networks (Bi-SCNN) to address computational inefficiency and over-smoothing issues in existing simplicial complex-based neural networks. The key innovation is combining simplicial convolution with weighted binary-sign propagation and feature normalization using the Hodge Laplacian for high-order structure representation. By binarizing features through the Sign() function and using length-1 simplicial convolution, Bi-SCNN reduces model complexity and speeds up training while maintaining accuracy. Experiments demonstrate Bi-SCNN achieves comparable or superior performance to baselines while requiring fewer parameters and less computation time.

## Method Summary
Bi-SCNN is a neural network architecture designed for simplicial complex data that combines simplicial convolution with weighted binary-sign forward propagation and feature normalization. The method uses the Hodge Laplacian to represent high-order structures and applies a Sign() function to binarize continuous feature values, reducing computational complexity by replacing floating-point multiplications with sign-based operations. The architecture processes upper and lower Hodge Laplacians separately using length-1 convolution to mitigate over-smoothing, while normalization weights scale binary features. This approach eliminates the need for separate activation functions and reduces the number of parameters compared to traditional simplicial convolutional networks.

## Key Results
- On citation data with 10% missing values, Bi-SCNN-2 achieves 90.65% accuracy in 21.21s versus 141.78s for SNN-2
- Bi-SCNN requires fewer parameters than SCNN while maintaining similar or better accuracy across multiple datasets
- The model successfully imputes missing data in citation networks and classifies ocean drifter trajectories with competitive performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Binarization reduces model complexity by replacing floating-point multiplications with sign-based operations
- Mechanism: The Sign() function converts continuous feature values into binary {+1, -1} outputs, and the l1-norm normalization weights scale these binary features, eliminating the need for full matrix multiplications in forward propagation
- Core assumption: The Sign() function approximation (hard tanh) maintains sufficient representational power while drastically reducing computational cost
- Evidence anchors:
  - [abstract]: "The combination of simplicial convolution with a weighted binary-sign forward propagation strategy significantly reduces the model complexity"
  - [section]: "Assuming each multiplication is 1 FLOP, each Bi-SCNN layer is at least rk FLOPS faster than the SCNN layer"
  - [corpus]: No direct evidence about computational complexity reduction in corpus
- Break condition: If the Sign() approximation fails to preserve important signal information, accuracy will degrade significantly

### Mechanism 2
- Claim: Feature normalization and binarization act as intrinsic nonlinearities, eliminating need for separate activation functions
- Mechanism: The Sign() function provides nonlinearity through its discontinuous nature, while the l1-norm normalization creates a ReLU-like behavior that scales feature magnitudes
- Core assumption: The combination of Sign() and normalization provides sufficient nonlinearity for learning complex patterns
- Evidence anchors:
  - [abstract]: "The Bi-SCNN inherently incorporates nonlinearity and activation functions through the use of the Sign() function and feature normalization"
  - [section]: "Notice that the l1-norm is nonnegative, so at each layer, Mk,p can be viewed as Mk,p = ReLu(Mk,p)"
  - [corpus]: No direct evidence about activation function elimination in corpus
- Break condition: If the learned representations require more complex nonlinear transformations than provided by Sign() + normalization

### Mechanism 3
- Claim: Length-1 simplicial convolution with separate upper/lower adjacency processing reduces over-smoothing
- Mechanism: By using J=1 and processing upper and lower Hodge Laplacians separately, each layer aggregates only immediate neighbors without propagating signals too far, preventing feature homogenization
- Core assumption: Over-smoothing in SCNN occurs primarily due to longer convolutional filters that aggregate from distant neighbors
- Evidence anchors:
  - [abstract]: "Bi-SCNN is less prone to over-smoothing as a result of the efficient architectural complexity"
  - [section]: "For the SCNN layer in (10), if J is large, one single SCNN layer is possible to aggregate the entire simplex as each j = 1...J will aggregate from neighbors further and further, which makes SCNN prone to over-smoothing"
  - [corpus]: No direct evidence about over-smoothing mitigation in corpus
- Break condition: If the reduced receptive field prevents the network from capturing necessary long-range dependencies

## Foundational Learning

- Concept: Simplicial complexes and Hodge Laplacians
  - Why needed here: Bi-SCNN operates on higher-order structures (edges, triangles) rather than just nodes, requiring the mathematical framework of simplicial complexes and Hodge Laplacians to define convolution operations
  - Quick check question: What is the difference between graph Laplacian L0 and Hodge Laplacian Lk for k > 0?

- Concept: Spectral graph signal processing and Graph Fourier Transform
  - Why needed here: Understanding how signals are transformed to frequency domain using eigenvectors of Laplacian matrices is crucial for grasping simplicial convolution and filtering operations
  - Quick check question: How does the Graph Fourier Transform differ from classical Fourier Transform in terms of basis functions?

- Concept: Message passing neural networks and their limitations
  - Why needed here: Bi-SCNN is compared against simplicial MPNN, and understanding MPNN's advantages (representation power) and disadvantages (complexity) provides context for Bi-SCNN's design choices
  - Quick check question: What is the main tradeoff between message passing networks and spectral approaches in terms of computational complexity?

## Architecture Onboarding

- Component map: Input simplicial features Xk → Normalization (l1-norm) → Binarization (Sign()) → Simplicial convolution (Lk,l, Lk,u) → Next layer (repeat) → Final output

- Critical path: Feature → Normalization (l1-norm) → Binarization (Sign()) → Simplicial convolution (Lk,l, Lk,u) → Next layer (repeat) → Final output

- Design tradeoffs:
  - Binarization vs accuracy: Reduces complexity but may lose information precision
  - Length-1 convolution vs receptive field: Faster but potentially less expressive
  - Separate upper/lower processing vs unified approach: More parameters but better decomposition

- Failure signatures:
  - Accuracy degradation despite faster runtime: Binarization may be too aggressive
  - Training instability or divergence: Sign() approximation may not be suitable for the data
  - Memory errors with large complexes: Incidence matrix computations may be bottlenecks

- First 3 experiments:
  1. Compare Bi-SCNN-2 vs SCNN-2 on citation dataset with 10% missing rate to verify runtime claims
  2. Test different Sign() approximation functions (hard tanh, sigmoid, etc.) to find optimal binarization
  3. Vary the number of layers P while monitoring over-smoothing indicators (feature variance across nodes)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the computational efficiency of Bi-SCNN scale with increasing simplicial complex order (K) and size (N_k), particularly compared to traditional SCNN and SNN architectures?
- Basis in paper: [inferred] The paper mentions Bi-SCNN's reduced model complexity and faster execution times, particularly highlighting the efficiency gains from binarization and feature normalization. However, the scaling behavior with increasing complex size and order is not explicitly analyzed.
- Why unresolved: The experiments only tested on two specific datasets with fixed simplicial orders (K=2 for ocean drifter, K=5 for citation). No analysis of computational complexity as a function of N_k or K was provided.
- What evidence would resolve it: Systematic experiments varying simplicial complex size and order, measuring execution time and memory usage across different architectures, would clarify the scaling properties of Bi-SCNN.

### Open Question 2
- Question: What is the theoretical relationship between the binarization threshold in Bi-SCNN and its impact on feature representation capacity and classification accuracy?
- Basis in paper: [explicit] The paper uses a Sign function for binarization and mentions it's approximated by a hard tanh function. The authors state this binarization reduces model complexity while maintaining accuracy, but don't explore how different binarization strategies affect performance.
- Why unresolved: The paper assumes a binary threshold at zero without exploring whether adaptive or data-dependent thresholds might improve performance. The trade-off between information loss from binarization and computational gains is not quantified.
- What evidence would resolve it: Experiments varying binarization thresholds or using multi-level quantization, measuring the resulting accuracy and computational trade-offs, would establish this relationship.

### Open Question 3
- Question: How does Bi-SCNN's performance on missing data imputation tasks compare to traditional matrix completion or deep learning methods when applied to the same citation complex data?
- Basis in paper: [inferred] The paper demonstrates Bi-SCNN's effectiveness on citation complex data with missing values, but doesn't compare against non-graph-based imputation methods or other neural network architectures not designed for simplicial complexes.
- Why unresolved: The experimental results only compare against other simplicial complex-based methods (SNN, SCNN, MPNN). No baseline comparison with standard imputation techniques or general-purpose neural networks is provided.
- What evidence would resolve it: Direct comparison of Bi-SCNN's imputation accuracy against methods like matrix factorization, autoencoders, or other neural networks on the same citation dataset would establish its relative performance.

## Limitations
- The paper only tests on two specific datasets (citation networks and ocean drifter trajectories) with limited generalizability
- Computational complexity claims are theoretical FLOPS calculations rather than measured wall-clock times
- Over-smoothing mitigation is primarily supported by architectural reasoning rather than extensive empirical validation

## Confidence
- Computational complexity claims: Medium - Theoretical FLOPS reduction calculations need empirical validation
- Over-smoothing mitigation: Medium - Architectural reasoning requires additional experimental support
- Generalization across datasets: Medium - Limited to two specific datasets without broader testing

## Next Checks
1. Reproduce Bi-SCNN implementation and validate exact results on citation dataset with 10% missing values, comparing both accuracy and runtime against reported values
2. Conduct systematic experiments varying the number of layers P from 1 to 10 on both datasets, measuring feature variance across nodes and class separation to quantify over-smoothing behavior
3. Test different binarization strategies including hard tanh, sigmoid-based thresholding, and learned binarization parameters, comparing their impact on both accuracy and computational efficiency