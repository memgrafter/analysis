---
ver: rpa2
title: 'AMONGAGENTS: Evaluating Large Language Models in the Interactive Text-Based
  Social Deduction Game'
arxiv_id: '2407.16521'
source_url: https://arxiv.org/abs/2407.16521
tags:
- player
- game
- agents
- actions
- crewmates
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces AMONG AGENTS, a text-based simulation of Among
  Us for evaluating large language models (LLMs) in complex social deduction games.
  The environment tests LLM agents' abilities in strategic reasoning, deception detection,
  and task collaboration through controlled and end-to-end evaluations.
---

# AMONGAGENTS: Evaluating Large Language Models in the Interactive Text-Based Social Deduction Game

## Quick Facts
- arXiv ID: 2407.16521
- Source URL: https://arxiv.org/abs/2407.16521
- Reference count: 36
- LLM agents effectively understand game rules and context-based decision-making with personality assignments influencing performance

## Executive Summary
This paper introduces AMONG AGENTS, a text-based simulation of the popular social deduction game Among Us for evaluating large language models (LLMs) in complex strategic environments. The system tests LLM agents' abilities in strategic reasoning, deception detection, and task collaboration through both controlled evaluations and end-to-end gameplay scenarios. Results demonstrate that LLM agents can effectively grasp game rules and make context-aware decisions, with personality assignments significantly influencing performance outcomes.

## Method Summary
The study employs LLM agents with planner modules and personality-based prompts to play Among Us in both controlled scenarios and full game environments. Agents are configured with memory systems to maintain state-action sequences and thought processes to generate plans before acting. The evaluation framework includes cognitive and strategic performance metrics, win rate analysis, and conversation strategy examination across different personality types (Crewmate and Impostor roles).

## Key Results
- LLM agents with planner modules show higher win rates compared to random baselines
- Crewmates outperform Impostors in self-awareness and reflection, while Impostors excel in planning and memory
- Personality assignments create distinct behavioral patterns, with Crewmates primarily engaging in truth-telling and Impostors frequently employing deception strategies
- Games without planner modules more frequently end by time limits, with reduced task completion rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM agents can effectively grasp game rules and context-based decision-making when equipped with structured memory and planning modules.
- Mechanism: The agent framework uses memory modules (H_i^t) to store state-action sequences and thought processes (T_i^t) to generate plans before acting, enabling coherent long-term strategies.
- Core assumption: LLMs can maintain context and consistency across multiple timesteps when provided with iterative summarization of past states.
- Evidence anchors:
  - [abstract] "Our work demonstrates that state-of-the-art large language models (LLMs) can effectively grasp the game rules and make decisions based on the current context."
  - [section 4.4] "The thought process of each agent can be modeled as an iterative decision-making process... allowing the agent to make decisions based on its current observations and past experiences stored in its memory."
  - [corpus] Weak evidence - corpus neighbors focus on social deduction evaluation but don't directly support memory-planning mechanisms.
- Break condition: If memory summarization (ϕ_H) fails to capture relevant context, agents lose coherence in decision-making across timesteps.

### Mechanism 2
- Claim: Personality assignments significantly influence LLM agent behavior and strategic choices in social deduction games.
- Mechanism: Personality prompts (P_i) are concatenated with base prompts (B_i) to form complete prompts (C_i), directly influencing the policy π_i and resulting action distributions.
- Core assumption: LLM outputs can be reliably steered by carefully crafted personality prompts that encode specific behavioral patterns.
- Evidence anchors:
  - [abstract] "Performance varies with assigned personalities."
  - [section 4.5] "The personality prompt P_i is concatenated with the agent's base prompt B_i to form the complete prompt C_i... leading to diverse and emergent strategies."
  - [section 5.2.1] "Crewmate personalities such as 'The Leader', 'The Skeptic', and 'The Tech Expert' show a strong inclination towards completing tasks" while "Impostor personalities like 'The Paranoid' and 'The Manipulator' exhibit more diverse actions."
- Break condition: If personality prompts don't create orthogonal behavioral patterns, agent strategies become indistinguishable regardless of assigned personality.

### Mechanism 3
- Claim: LLM agents without planner modules exhibit reduced strategic effectiveness and increased reliance on time-based outcomes.
- Mechanism: The planner module enables agents to retrieve and execute pre-formed plans (T_i^t with planner) versus generating decisions ad-hoc (T_i^t without planner), affecting win condition achievement.
- Core assumption: Pre-planning improves strategic execution compared to reactive decision-making in complex game environments.
- Evidence anchors:
  - [abstract] "LLM players with different assigned personalities exhibited varied performances in the game."
  - [section 5.2.2] "We also explore the case where LLM agents do not equip a planner module... the game where LLM agents don't have a planner module is more likely to end by hitting the time limit, and Crewmates are less likely to finish all the assigned tasks."
  - [section 4.4] "Each agent performs each timestamp asynchronously. We refer to the agents having a standard thought process as the agents with a planner module as the agents can recall what they planned in the past."
- Break condition: If the planner module doesn't provide meaningful strategic advantage, its absence shouldn't significantly impact win rates or task completion.

## Foundational Learning

- Concept: Game state representation and observation space design
  - Why needed here: The agent must perceive relevant information (location, phase, tasks) to make informed decisions within the Among Us environment
  - Quick check question: Can you list the three main categories of information available in the observation space?

- Concept: Memory architecture for maintaining agent coherence
  - Why needed here: Agents need to track past actions, observations, and plans to maintain consistent behavior across multiple timesteps
  - Quick check question: How does the memory module (H_i^t) differ from the thought process module (T_i^t) in the agent architecture?

- Concept: Prompt engineering for behavior control
  - Why needed here: Personality and role-specific behaviors are achieved through structured prompt concatenation, requiring understanding of how LLMs respond to prompt formatting
  - Quick check question: What is the exact formula for constructing the complete agent prompt from base prompt and personality?

## Architecture Onboarding

- Component map: Observation processor → Memory module (H_i^t) → Thought process planner (T_i^t) → Action selector → Game environment interface
- Critical path: Observation → Memory retrieval → Plan generation → Action execution → State update
- Design tradeoffs: Planner module increases computational overhead but improves strategic coherence; personality prompts add behavioral diversity but require careful tuning
- Failure signatures: Agents getting stuck in loops (memory issues), inconsistent behavior across timesteps (memory corruption), failure to complete tasks (planning issues)
- First 3 experiments:
  1. Test agent with planner module vs without planner module in simple task completion scenarios
  2. Verify personality prompt effectiveness by comparing action distributions across different personalities
  3. Validate observation space completeness by testing agent performance with partial vs full information

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different personality configurations impact the agents' ability to detect deception and successfully identify impostors?
- Basis in paper: [explicit] The paper states that crewmates with certain personalities like "The Skeptic" excel at questioning others and require solid evidence, while impostors like "The Manipulator" focus on deception strategies.
- Why unresolved: While the paper analyzes personality-action and personality-win condition correlations, it does not directly measure how personality traits affect deception detection accuracy.
- What evidence would resolve it: Experiments comparing detection accuracy of different personality types in controlled deception scenarios, or analysis of conversation data to identify which personality types are most successful at detecting lies.

### Open Question 2
- Question: What is the optimal balance between task completion and social deduction activities for crewmate agents to maximize win rates?
- Basis in paper: [inferred] The paper shows that crewmates focusing on task completion (like "The Tech Expert" and "The Leader") have varied success rates, suggesting there's a complex relationship between task focus and winning.
- Why unresolved: The paper presents task completion data but doesn't analyze the optimal strategy balance or conduct experiments varying the ratio of task vs. social activities.
- What evidence would resolve it: Controlled experiments with crewmate agents following different task/social ratios, measuring win rates for each configuration.

### Open Question 3
- Question: How does the absence of a planner module affect the agents' ability to execute long-term strategies and maintain coherent deception narratives?
- Basis in paper: [explicit] The paper notes that games with LLM agents without planner modules are more likely to hit time limits and crewmates are less likely to complete all tasks, but doesn't analyze strategic coherence.
- Why unresolved: While the paper shows time limit effects, it doesn't investigate how the lack of planning specifically impacts strategic deception or long-term planning capabilities.
- What evidence would resolve it: Comparative analysis of deception narratives and strategic planning quality between planner and non-planner agents, possibly using conversation analysis or expert evaluation of strategy coherence.

## Limitations
- Controlled evaluation uses 1 Crewmate vs 1 Impostor configuration, which may not generalize to typical Among Us player counts
- Relies on GPT-3.5-turbo without comparing against other LLM architectures or newer models
- Limited ablation studies on planner module effectiveness - only binary comparison without intermediate configurations
- Personality prompt engineering lacks systematic analysis of prompt quality and robustness

## Confidence
- **High confidence**: The fundamental claim that LLM agents can learn Among Us rules and make context-aware decisions is well-supported by both controlled and end-to-end evaluations
- **Medium confidence**: The comparative performance between Crewmates and Impostors in different cognitive dimensions is supported but based on limited scenario testing
- **Medium confidence**: The impact of personality assignments on behavior is demonstrated but lacks statistical power analysis across personality variations

## Next Checks
1. Scale controlled evaluations to include typical Among Us player counts (4-10) to verify findings hold in realistic game sizes
2. Conduct systematic ablation studies on the planner module - test with partial planning capabilities, different planning frequencies, and alternative planning architectures
3. Implement robustness testing by varying personality prompt formulations and measuring behavioral consistency across prompt perturbations