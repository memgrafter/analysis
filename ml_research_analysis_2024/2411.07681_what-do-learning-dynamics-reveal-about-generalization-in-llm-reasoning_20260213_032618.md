---
ver: rpa2
title: What Do Learning Dynamics Reveal About Generalization in LLM Reasoning?
arxiv_id: '2411.07681'
source_url: https://arxiv.org/abs/2411.07681
tags:
- training
- accuracy
- data
- train
- pre-memorization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates how learning dynamics in LLM fine-tuning
  relate to generalization in reasoning tasks. The key contribution is the introduction
  of pre-memorization train accuracy, which measures a model's accuracy on training
  examples before it begins to replicate target solution traces.
---

# What Do Learning Dynamics Reveal About Generalization in LLM Reasoning?

## Quick Facts
- arXiv ID: 2411.07681
- Source URL: https://arxiv.org/abs/2411.07681
- Authors: Katie Kang; Amrith Setlur; Dibya Ghosh; Jacob Steinhardt; Claire Tomlin; Sergey Levine; Aviral Kumar
- Reference count: 37
- Primary result: Pre-memorization train accuracy predicts test accuracy with R² ≈ 0.9 across models, datasets, and training configurations

## Executive Summary
This work investigates how learning dynamics in LLM fine-tuning relate to generalization in reasoning tasks. The key contribution is the introduction of pre-memorization train accuracy, which measures a model's accuracy on training examples before it begins to replicate target solution traces. This metric is shown to be highly predictive of test accuracy, achieving R² values of 0.9 or higher across various models, datasets, and training configurations. The metric also identifies training examples for which model predictions are less robust to input perturbations. Leveraging these insights, the authors demonstrate that data curation strategies prioritizing examples with low pre-memorization accuracy lead to 1.5-2x improvements in sample efficiency compared to standard approaches.

## Method Summary
The authors fine-tune pre-trained LLMs (Llama3 8B, Gemma2 9B) on reasoning datasets (GSM8k, MATH) using supervised learning. They track accuracy and perplexity over training epochs to identify when models transition from learning general strategies to memorizing specific solution traces. The pre-memorization accuracy metric measures performance before this memorization threshold is reached. They evaluate this metric's correlation with test accuracy, assess robustness to input perturbations, and use it to guide data curation that prioritizes challenging examples.

## Key Results
- Pre-memorization train accuracy achieves R² ≈ 0.9 correlation with test accuracy
- Low pre-memorization accuracy examples show significant accuracy degradation under input perturbations
- Data curation prioritizing low pre-memorization accuracy examples yields 1.5-2x sample efficiency improvements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Pre-memorization train accuracy measures how well a model solves training examples before it starts replicating exact solution traces, and this pre-memorization performance strongly predicts generalization to unseen test data.
- **Mechanism**: During fine-tuning on reasoning tasks, models progress through two stages: first learning general problem-solving strategies (high pre-memorization accuracy), then memorizing specific solution traces. Models that develop accurate strategies before memorizing generalize better because they've learned transferable problem-solving skills rather than just copying.
- **Core assumption**: Solution traces in reasoning tasks have multiple valid forms, so correct answers with different reasoning paths indicate general understanding rather than memorization.
- **Evidence anchors**:
  - [abstract] "pre-memorization train accuracy... is able to reliably predict test accuracy, achieving R² of around or exceeding 0.9"
  - [section] "We find that a model's generalization behavior can be effectively characterized by... the accuracy of model samples on training queries before they begin to copy the exact reasoning steps"
  - [corpus] Weak: No direct citations about memorization vs generalization trade-offs in LLM reasoning tasks
- **Break condition**: If reasoning tasks have only one valid solution path, then different reasoning traces wouldn't indicate general understanding, breaking the mechanism.

### Mechanism 2
- **Claim**: Low pre-memorization accuracy on training examples indicates that model predictions for those examples are not robust to input perturbations.
- **Mechanism**: Examples where models only produce correct answers after memorizing specific solution traces are brittle—small changes to the input format cause accuracy to drop significantly. In contrast, examples where models solve correctly through varied reasoning paths before memorization are more robust to perturbations.
- **Core assumption**: Perturbations like adding generic preambles ("First", "We know that") create valid but unseen input variations that test whether the model learned flexible problem-solving vs rigid pattern matching.
- **Evidence anchors**:
  - [abstract] "pre-memorization train accuracy... is also indicative of whether individual model predictions are robust to perturbations in the training query"
  - [section] "For train examples with low pre-memorization accuracies, adding small perturbations to the training prompt causes the accuracy of model predictions to significantly degrade"
  - [corpus] Weak: No corpus evidence about perturbation robustness metrics for LLM reasoning
- **Break condition**: If the model's architecture or training procedure inherently makes it robust to all input variations regardless of pre-memorization accuracy, this mechanism would break.

### Mechanism 3
- **Claim**: Data curation that prioritizes examples with low pre-memorization accuracy leads to more sample-efficient learning than i.i.d. sampling or other curation strategies.
- **Mechanism**: Examples with low pre-memorization accuracy represent the hardest cases where models struggle to develop robust solutions. Including these challenging examples early in training forces the model to develop more general problem-solving capabilities, leading to better generalization with fewer total examples.
- **Core assumption**: The difficulty of learning robust solutions on an example correlates with its pre-memorization accuracy, and including hard examples early accelerates learning of general strategies.
- **Evidence anchors**:
  - [abstract] "prioritizing examples with low pre-memorization accuracy leads to 1.5-2x improvements in sample efficiency"
  - [section] "training on data distributions that prioritize examples with low pre-memorization accuracy leads to a 1.5-2 × improvement in sample efficiency over i.i.d sampling"
  - [corpus] Weak: No corpus evidence about data curation strategies based on learning dynamics
- **Break condition**: If model capacity is too low to benefit from hard examples, or if the hardest examples are too difficult to be useful, this mechanism would break.

## Foundational Learning

- **Concept**: Distinction between memorization and generalization in machine learning
  - Why needed here: The entire paper hinges on understanding that memorization (exact replication) differs from generalization (correct answers through varied reasoning), and that this distinction predicts test performance
  - Quick check question: What metric would you use to determine if a model has memorized a training example versus learned a general solution?

- **Concept**: Learning dynamics and their relationship to generalization
  - Why needed here: The paper shows that tracking how accuracy evolves during training (specifically pre-memorization accuracy) provides insights into final generalization performance
  - Quick check question: How would you track and visualize the progression of model accuracy on training examples throughout fine-tuning?

- **Concept**: Robustness to input perturbations as a measure of learned generalization
  - Why needed here: The paper uses perturbation experiments to distinguish robust problem-solving from brittle memorization patterns
  - Quick check question: What types of input perturbations would best test whether a model has learned flexible reasoning versus rigid pattern matching?

## Architecture Onboarding

- **Component map**: Pre-train LLM -> Fine-tune on reasoning dataset -> Track pre-memorization accuracy -> Evaluate on test set -> Use pre-memorization accuracy for data curation -> Retrain with curated data
- **Critical path**: Pre-train → Fine-tune on reasoning dataset → Track pre-memorization accuracy during training → Evaluate on test set → Use pre-memorization accuracy to guide data curation → Retrain with curated data
- **Design tradeoffs**: Choosing the memorization threshold p involves balancing sensitivity to memorization vs computational cost; tracking pre-memorization accuracy adds overhead to training but provides valuable insights; perturbation analysis requires domain knowledge for creating effective perturbations
- **Failure signatures**: Low R² between pre-memorization accuracy and test accuracy suggests the mechanism doesn't apply to the current task; poor performance of data curation suggests incorrect threshold selection or that the hardest examples aren't actually beneficial
- **First 3 experiments**:
  1. Fine-tune Llama3 8B on GSM8K with different learning rates, track pre-memorization accuracy, and plot against test accuracy to verify the correlation
  2. Apply perturbations to training examples and measure accuracy degradation to confirm the robustness relationship
  3. Implement data curation prioritizing low pre-memorization accuracy examples and compare sample efficiency against i.i.d. sampling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the pre-memorization accuracy threshold p vary across different model architectures and reasoning tasks?
- Basis in paper: [explicit] The paper mentions that p depends on the task and pretrained model but not on training parameters, and provides examples of calibration procedures
- Why unresolved: The paper shows calibration is possible but doesn't systematically characterize how p varies across model families, task complexities, or dataset characteristics
- What evidence would resolve it: Empirical studies mapping p values across diverse model families (transformers, recurrent, etc.), task types (mathematical, logical, commonsense), and dataset properties (size, distribution, difficulty)

### Open Question 2
- Question: Can pre-memorization accuracy predict generalization beyond reasoning tasks to other domains like language modeling or code generation?
- Basis in paper: [inferred] The paper focuses on reasoning tasks where solution traces can be distinguished from correct answers, but the core concept of measuring accuracy before memorization could apply more broadly
- Why unresolved: The paper explicitly limits its analysis to reasoning tasks where intermediate steps exist, leaving open whether the metric transfers to domains without clear solution traces
- What evidence would resolve it: Experiments applying pre-memorization accuracy to diverse domains like natural language generation, code completion, or image classification, measuring correlation with test performance

### Open Question 3
- Question: What is the mechanistic relationship between pre-memorization accuracy and the emergence of generalizable representations in LLMs?
- Basis in paper: [explicit] The paper demonstrates correlation between pre-memorization accuracy and test performance but doesn't investigate the underlying mechanisms
- Why unresolved: The paper shows that pre-memorization accuracy predicts generalization but doesn't explain why this relationship exists or what internal representations are developing
- What evidence would resolve it: Analysis of intermediate model representations, attention patterns, or circuit structures that emerge during pre-memorization phase and correlate with later generalization ability

### Open Question 4
- Question: How does the effectiveness of pre-memorization accuracy for data curation change with model scale and capability?
- Basis in paper: [explicit] The paper notes that the performance gap between curation methods increases with dataset size, suggesting larger models might benefit more from sophisticated curation
- Why unresolved: The paper only tests on models up to 9B parameters and shows correlation with dataset size but doesn't systematically vary model scale to measure the relationship
- What evidence would resolve it: Scaling studies measuring the relative effectiveness of pre-memorization accuracy curation versus simpler methods across model sizes from small (1B) to frontier (100B+) models

## Limitations
- The correlation between pre-memorization accuracy and test performance needs validation on task types beyond mathematical reasoning
- The perturbation robustness analysis uses limited perturbation types that may not represent all real-world input variations
- The data curation strategy's effectiveness depends critically on correctly calibrating the memorization threshold p

## Confidence

- **High confidence**: The existence of distinct pre-memorization and post-memorization phases during fine-tuning is well-established through the learning curves presented.
- **Medium confidence**: The predictive power of pre-memorization accuracy for test performance is demonstrated empirically but requires validation on broader task sets.
- **Medium confidence**: The perturbation analysis methodology is sound, though the specific perturbations used may not be comprehensive.

## Next Checks

1. **Cross-task validation**: Apply the pre-memorization accuracy metric to reasoning tasks beyond GSM8K and MATH (e.g., coding problems, logical reasoning) to test generalizability of the R² ≈ 0.9 correlation.

2. **Ablation on perturbation types**: Systematically vary perturbation types beyond generic preambles to assess whether the robustness findings hold across different forms of input variation relevant to real-world usage.

3. **Threshold sensitivity analysis**: Conduct extensive experiments varying the memorization threshold p across different model scales and task difficulties to establish robust calibration procedures for the data curation strategy.