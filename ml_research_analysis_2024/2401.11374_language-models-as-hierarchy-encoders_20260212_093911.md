---
ver: rpa2
title: Language Models as Hierarchy Encoders
arxiv_id: '2401.11374'
source_url: https://arxiv.org/abs/2401.11374
tags:
- hyperbolic
- entity
- entities
- prediction
- hierarchy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the challenge of enabling language models\
  \ to interpret and encode hierarchical structures latent in language. The core method\
  \ involves re-training transformer encoder-based language models as Hierarchy Transformer\
  \ encoders (HiTs) within a Poincar\xE9 ball manifold, using hyperbolic clustering\
  \ and centripetal losses to cluster related entities and organize them hierarchically."
---

# Language Models as Hierarchy Encoders

## Quick Facts
- arXiv ID: 2401.11374
- Source URL: https://arxiv.org/abs/2401.11374
- Authors: Yuan He; Zhangdie Yuan; Jiaoyan Chen; Ian Horrocks
- Reference count: 40
- One-line primary result: Hierarchy Transformer (HiT) achieves F-scores up to 0.916 on WordNet and 0.954 on SNOMED CT for subsumption prediction

## Executive Summary
This work addresses the challenge of enabling language models to interpret and encode hierarchical structures latent in language. The core method involves re-training transformer encoder-based language models as Hierarchy Transformer encoders (HiTs) within a Poincaré ball manifold, using hyperbolic clustering and centripetal losses to cluster related entities and organize them hierarchically. HiTs significantly outperform pre-trained and fine-tuned LMs on tasks involving transitive inference and subsumption prediction across hierarchies, achieving F-scores up to 0.916 on WordNet and 0.954 on SNOMED CT, demonstrating superior generalization and transfer capabilities.

## Method Summary
The paper proposes HiTs, a method to re-train pre-trained language models as hierarchy encoders by situating their output embeddings within a Poincaré ball manifold. This is achieved through hyperbolic clustering loss (Lcluster) that groups related entities and centripetal loss (Lcentri) that positions parent entities closer to the origin than their children. The model is trained on entity names from hierarchies like WordNet and SNOMED CT, using triplet loss formulations to enforce hierarchical relationships in the hyperbolic space.

## Key Results
- HiTs achieves F-scores of 0.916 on WordNet and 0.954 on SNOMED CT for subsumption prediction
- Demonstrates superior generalization and transfer capabilities compared to pre-trained and fine-tuned LMs
- Shows effective encoding of hierarchical depth through hyperbolic norms, with grandparents having larger norms than parents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Re-training LMs within a Poincaré ball manifold explicitly encodes hierarchical relationships by constraining embeddings to a space where distances grow exponentially toward the boundary.
- Mechanism: The tanh activation in transformer encoders confines outputs to a d-dimensional hypercube. This space is circumscribed by a Poincaré ball of radius √d. By training with hyperbolic clustering and centripetal losses, related entities cluster together and parent-child relationships follow radial expansion from the origin.
- Core assumption: The hyperbolic geometry naturally matches the branching factor and depth of real-world hierarchies, so embedding distances correlate with semantic proximity and subordination.
- Evidence anchors:
  - [abstract] "situates the output embedding space of pre-trained LMs within a Poincaré ball with a curvature that adapts to the embedding dimension"
  - [section 2.2] Formal definition of Poincaré ball and distance metrics
  - [corpus] Neighbor titles reference "Hyperbolic Residual Quantization" and "Achieving Hyperbolic-Like Expressiveness" indicating related work
- Break condition: If the input hierarchy is shallow or non-tree-like, the manifold may not provide sufficient representational advantage over Euclidean space.

### Mechanism 2
- Claim: The hyperbolic clustering loss enforces that sibling entities are closer to each other than to unrelated entities, improving subsumption prediction.
- Mechanism: Uses a triplet loss format where the distance between an entity and its parent is minimized relative to a negative parent. This pulls siblings together while pushing them away from unrelated nodes.
- Core assumption: Hierarchical relationships can be approximated as "related entities are closer than unrelated entities" within the hyperbolic manifold.
- Evidence anchors:
  - [section 3] Formal definition of Lcluster and its triplet loss structure
  - [section 4.4] Results show higher precision/recall when hard negatives are used, implying effective clustering
  - [corpus] Neighbor paper "Hyperbolic Residual Quantization" implies related methods exist
- Break condition: If the hierarchy contains many cross-cutting or non-hierarchical relations, the clustering loss may incorrectly group unrelated entities.

### Mechanism 3
- Claim: The centripetal loss positions parent entities closer to the manifold origin than their children, mirroring the natural expansion of hierarchies.
- Mechanism: Adds a margin-based loss on hyperbolic norms, ensuring child embeddings have larger norms (are farther from origin) than parents, thus reflecting hierarchical depth.
- Core assumption: Hierarchical depth correlates with radial distance in hyperbolic space, so parents should have smaller hyperbolic norms than descendants.
- Evidence anchors:
  - [section 3] Formal definition of Lcentri and explanation of "centripetal" naming
  - [section 4.5] Table 5 shows that "pc" (grandchild) has larger norm than "computer" (ancestor)
  - [corpus] Neighbor paper "Hyperbolic Hypergraph Neural Networks" suggests related hierarchical embeddings
- Break condition: If the hierarchy is inverted (e.g., in some domain-specific ontologies), this assumption would fail and embeddings would be misaligned.

## Foundational Learning

- Concept: Poincaré ball geometry and Möbius addition
  - Why needed here: The model explicitly maps LM embeddings into a Poincaré ball and computes distances using its metric; understanding these operations is essential for debugging and extending the method.
  - Quick check question: What is the formula for the distance between two points u and v in a d-dimensional Poincaré ball with curvature c?

- Concept: Triplet loss and contrastive learning
  - Why needed here: The clustering loss is formulated as a triplet loss; understanding how positives and negatives are sampled is critical for dataset construction and loss tuning.
  - Quick check question: In the triplet (e, e+, e-), which entity is the "anchor", which is the positive, and which is the negative?

- Concept: Hyperbolic norm and its relation to hierarchical depth
  - Why needed here: The centripetal loss uses hyperbolic norms to enforce parent-child positioning; knowing how norm maps to depth helps interpret embeddings and set loss margins.
  - Quick check question: In the Poincaré ball, what is the hyperbolic norm of a point x, and how does it relate to its distance from the origin?

## Architecture Onboarding

- Component map:
  Entity names -> Tokenizer (pre-trained LM tokenizer) -> Encoder (Transformer encoder + mean pooling) -> Embedding space (d-dimensional Poincaré ball) -> Losses (hyperbolic clustering + centripetal) -> Probe (scoring function using distance + norm difference) -> Subsumption score

- Critical path:
  1. Tokenize entity names
  2. Generate token embeddings via transformer
  3. Apply mean pooling to get sentence embedding
  4. Apply hyperbolic losses during training
  5. Use scoring function for inference

- Design tradeoffs:
  - Using existing LM weights vs training from scratch: Leverages pre-trained semantics but may limit adaptation
  - Mean pooling vs CLS token: Simpler and more robust to input length
  - Poincaré ball radius √d vs scaling to unit ball: Circumscribing hypercube avoids projection layers

- Failure signatures:
  - Loss plateaus early: Check triplet sampling quality and margin values
  - Poor transfer performance: May indicate overfitting to source hierarchy or insufficient entity naming
  - Inconsistent norms: Verify that embeddings remain within Poincaré ball bounds during training

- First 3 experiments:
  1. Train on WordNet with default margins (α=5.0, β=0.1) and evaluate on Multi-hop Inference; verify F-score > 0.8
  2. Test transfer to Schema.org with same model; check if F-score improves over pre-trained LM
  3. Ablate centripetal loss (β=0) and observe change in correlation between norms and depths; expect weaker depth encoding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can catastrophic forgetting of pre-trained language understanding be measured and mitigated during hierarchy re-training?
- Basis in paper: [inferred] The paper acknowledges the limitation of not addressing potential loss of pre-trained language understanding during hierarchy re-training.
- Why unresolved: The paper mentions this as a limitation but does not propose methods to measure or mitigate this effect.
- What evidence would resolve it: Experiments comparing pre-trained LM performance on general language tasks before and after hierarchy re-training, and techniques to preserve general language understanding while learning hierarchical structures.

### Open Question 2
- Question: How does the performance of hierarchy re-trained LMs vary across different domains and ontology sizes?
- Basis in paper: [explicit] The paper evaluates on WordNet, SNOMED CT, Schema.org, FoodOn, and DOID, showing varying performance across these datasets.
- Why unresolved: While the paper shows performance differences, it does not systematically analyze how domain specificity and ontology size affect the effectiveness of hierarchy re-training.
- What evidence would resolve it: A comprehensive study across a wider range of ontologies varying in domain, size, and complexity, analyzing the correlation between these factors and HIT performance.

### Open Question 3
- Question: Can hierarchy re-trained LMs effectively handle multiple hierarchical relationships within a single model?
- Basis in paper: [inferred] The paper mentions extending HIT to accommodate multiple hierarchical relationships as a future work direction.
- Why unresolved: The current implementation focuses on single hierarchical relationships, and the paper does not explore the feasibility or performance of handling multiple hierarchies.
- What evidence would resolve it: Experiments training HIT models on datasets containing multiple types of hierarchical relationships and evaluating their ability to distinguish and predict different types of subsumptions.

## Limitations

- Domain Generalization: Strong performance on WordNet and SNOMED CT, but untested on messier, real-world hierarchies
- Hierarchy Quality Dependency: Assumes high-quality parent-child relationships; errors in source hierarchy may propagate
- Computational Overhead: Retraining transformers with Poincaré ball constraints may be prohibitive for very large LMs or hierarchies

## Confidence

- **High**: The mechanism of using hyperbolic geometry to encode hierarchical depth (Mechanism 3) is mathematically sound and well-supported by the evidence. The centripetal loss effectively positions parents closer to the origin than children.
- **Medium**: The clustering loss (Mechanism 2) is plausible, but its effectiveness depends heavily on the quality of triplet sampling. The paper doesn't provide ablation studies on different negative sampling strategies.
- **Medium**: The overall claim of superior generalization is supported by the reported F-scores, but the evaluation is limited to two datasets. Cross-domain generalization needs further validation.

## Next Checks

1. **Cross-Domain Transfer Test**: Evaluate the trained HiTs model on a completely different type of hierarchy (e.g., a product taxonomy from e-commerce) to test true generalization beyond WordNet and SNOMED CT.
2. **Hierarchy Quality Sensitivity**: Intentionally corrupt a subset of the WordNet hierarchy (e.g., swap parent-child relationships) and retrain to measure how much performance degrades. This would quantify the model's robustness to imperfect supervision.
3. **Ablation on Loss Components**: Train versions with only the clustering loss, only the centripetal loss, and neither, to quantify the individual contribution of each component to the final performance.