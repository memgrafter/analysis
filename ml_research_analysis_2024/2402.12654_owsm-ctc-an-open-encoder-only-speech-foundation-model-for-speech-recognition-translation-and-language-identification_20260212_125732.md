---
ver: rpa2
title: 'OWSM-CTC: An Open Encoder-Only Speech Foundation Model for Speech Recognition,
  Translation, and Language Identification'
arxiv_id: '2402.12654'
source_url: https://arxiv.org/abs/2402.12654
tags:
- speech
- owsm
- owsm-ctc
- data
- medium
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes OWSM-CTC, a novel encoder-only speech foundation
  model based on Connectionist Temporal Classification (CTC). It is trained on 180k
  hours of public audio data for multilingual automatic speech recognition (ASR),
  speech translation (ST), and language identification (LID).
---

# OWSM-CTC: An Open Encoder-Only Speech Foundation Model for Speech Recognition, Translation, and Language Identification

## Quick Facts
- arXiv ID: 2402.12654
- Source URL: https://arxiv.org/abs/2402.12654
- Reference count: 40
- Key outcome: OWSM-CTC achieves competitive ASR results and up to 24% relative improvement on ST while being 3-4x faster for inference and 20x faster for long-form ASR.

## Executive Summary
This paper introduces OWSM-CTC, a novel encoder-only speech foundation model based on Connectionist Temporal Classification (CTC) that performs multilingual automatic speech recognition (ASR), speech translation (ST), and language identification (LID). The model is trained on 180k hours of public audio data covering 151 languages and achieves competitive results on ASR while demonstrating up to 24% relative improvement on ST compared to its encoder-decoder counterpart. The key innovation is the use of multi-task self-conditioned CTC layers that enable a single encoder to handle all three tasks without task-specific decoders, while maintaining faster inference through parallel greedy decoding.

## Method Summary
OWSM-CTC is an encoder-only speech foundation model that uses a 27-layer E-Branchformer architecture with 4 intermediate CTC layers for self-conditioned training. The model processes log Mel filterbanks (downsampled via 2D CNN with 8x rate) and uses special tokens (<lang> and <task>) to condition the encoder for different tasks. During training, intermediate CTC layers are partitioned between ASR-only and task-dependent layers, with references selected based on the task token. The model is trained on 180k hours of public audio data using Adam optimizer with peak learning rate 2e-4 for 600k steps. Inference uses parallel greedy CTC decoding, enabling 3-4x speed-up compared to autoregressive models and 20x speed-up for long-form ASR through batched parallel processing of overlapped chunks.

## Key Results
- Achieves competitive WER on ASR tasks compared to encoder-decoder OWSM v3.1
- Demonstrates up to 24% relative improvement in BLEU score for speech translation
- Provides 3-4x inference speed-up through parallel greedy decoding and 20x speed-up for long-form ASR

## Why This Works (Mechanism)

### Mechanism 1
Encoder-only CTC-based models can match or exceed encoder-decoder performance on multilingual ASR and ST while being 3-4x faster at inference. CTC models perform parallel greedy decoding, avoiding autoregressive sequential generation. Self-conditioned intermediate CTC layers mitigate the conditional independence assumption by incorporating predictions from earlier layers back into the encoder.

### Mechanism 2
Multi-task self-conditioned CTC enables a single encoder to perform ASR, ST, and LID without task-specific decoders. Special tokens (<lang>, <task>) prepended to encoder input specify the desired task. The same CTC layers can output language IDs, ASR transcriptions, or ST translations depending on the task token.

### Mechanism 3
Batching and parallel decoding enable 20x speed-up for long-form ASR compared to chunk-wise sequential decoding. Long audio is split into overlapped chunks of 30s, processed in parallel batches, and decoded using greedy CTC. No need for timestamp prediction and sequential chunk shifting.

## Foundational Learning

- **Concept: Connectionist Temporal Classification (CTC) loss function**
  - Why needed here: CTC allows training without explicit alignment between input audio frames and output tokens, crucial for end-to-end speech modeling
  - Quick check question: What is the key assumption of CTC that self-conditioned layers help mitigate?

- **Concept: Multi-task learning with task-specific conditioning**
  - Why needed here: Enables a single model to handle ASR, ST, and LID by conditioning on special tokens and maintaining task-specific intermediate layers
  - Quick check question: How does the model distinguish between ASR and ST tasks during training and inference?

- **Concept: E-Branchformer architecture and its variants**
  - Why needed here: Provides efficient self-attention mechanisms with branch structures that improve speech representation learning compared to standard Transformers
  - Quick check question: What architectural differences between E-Branchformer and standard Transformer make it suitable for speech tasks?

## Architecture Onboarding

- **Component map**: Log Mel filterbanks -> 2D CNN downsampling -> Special tokens + positional encoding -> E-Branchformer layers -> CTC predictions

- **Critical path**: 
  1. Audio → Log Mel → 2D CNN downsampling
  2. Special tokens + speech features → positional encoding
  3. E-Branchformer layers with optional cross-attention from prompt encoder
  4. CTC loss computation at final layer and intermediate self-conditioned layers
  5. Greedy decoding for inference

- **Design tradeoffs**:
  - Larger downsampling (8x vs 4x) reduces GPU memory but slightly degrades accuracy
  - Self-conditioned layers improve accuracy but add complexity and training time
  - Task partitioning (ASR-only vs task-dependent layers) affects convergence and final performance
  - Prompt encoder adds flexibility but increases model size and complexity

- **Failure signatures**:
  - Model divergence during training: likely due to incorrect task partitioning in intermediate CTC layers
  - Poor LID accuracy: may indicate insufficient language token conditioning or imbalanced language representation in training data
  - Repetitive or hallucinated output: characteristic of autoregressive models, not expected in OWSM-CTC but could indicate training issues

- **First 3 experiments**:
  1. Verify CTC layer task partitioning by training a small model on MuST-C En-De and checking if intermediate CTC layers learn appropriate behavior
  2. Test inference speed-up by comparing greedy decoding vs autoregressive decoding on same hardware
  3. Evaluate LID accuracy on FLEURS test set to verify language token conditioning works as expected

## Open Questions the Paper Calls Out

### Open Question 1
How can we optimize the non-autoregressive OWSM-CTC model to achieve both high accuracy and fast inference speed, particularly for low-resource languages? The paper notes that the model might struggle with low-resource languages and does not provide specific strategies for optimizing the model for these languages.

### Open Question 2
What are the potential applications of the text prompt feature in OWSM-CTC beyond contextual biasing, and how can it be leveraged to improve the model's performance in other tasks? The paper discusses the use of text prompts but notes that the model is not really trained to perform this type of task and that fine-tuning is needed.

### Open Question 3
How can we further improve the robustness of OWSM-CTC to handle random noise and avoid error propagation in autoregressive decoding? The paper discusses the robustness of the non-autoregressive model but does not provide specific strategies for further improving the robustness of the model.

## Limitations

- Data scope uncertainty: Specific composition of the 25+ public corpora used for training is not fully detailed, making it difficult to assess potential biases or limitations in language coverage.
- Performance comparison gaps: Direct comparisons with other state-of-the-art encoder-only models are limited, primarily benchmarking against its own encoder-decoder counterpart.
- Long-form ASR validation: The claimed 20x speed-up is impressive but lacks comprehensive analysis of transcription quality for very long audio files or real-world robustness.

## Confidence

**High Confidence**:
- Encoder-only CTC models can achieve competitive ASR performance compared to encoder-decoder models
- Multi-task self-conditioned CTC is feasible for ASR, ST, and LID with appropriate task partitioning
- Parallel greedy decoding provides 3-4x inference speed-up compared to autoregressive decoding

**Medium Confidence**:
- 24% relative improvement on ST compared to encoder-decoder baseline
- 20x speed-up for long-form ASR with minimal accuracy degradation
- Task-specific conditioning via special tokens (<lang>, <task>) works effectively

**Low Confidence**:
- Model's LID accuracy across all 151 languages
- Performance on extremely long-form audio (hours) without chunk boundary artifacts
- Generalization to zero-shot language pairs for ST

## Next Checks

1. **Architecture ablation study**: Systematically vary the number of intermediate CTC layers and their task partitioning to identify optimal configurations and validate the claimed necessity of the specific partitioning approach.

2. **Cross-model comparison**: Benchmark OWSM-CTC against other leading encoder-only speech foundation models (e.g., XLS-R, Whisper) on standardized multilingual ASR and ST benchmarks to establish relative performance beyond the internal encoder-decoder comparison.

3. **Long-form robustness analysis**: Evaluate OWSM-CTC on progressively longer audio files (30 minutes to 3+ hours) to identify potential degradation points, chunk boundary artifacts, and the practical limits of the 20x speed-up claim under various context lengths and overlap settings.