---
ver: rpa2
title: Combining Embedding-Based and Semantic-Based Models for Post-hoc Explanations
  in Recommender Systems
arxiv_id: '2401.04474'
source_url: https://arxiv.org/abs/2401.04474
tags:
- explanations
- knowledge
- user
- systems
- recommendations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the interpretability challenge in embedding-based
  recommender systems by proposing a hybrid framework that combines embedding-based
  and semantic-based models to generate post-hoc explanations. The approach leverages
  an ontology-based knowledge graph to extract structured information about users
  and items, while employing knowledge graph embedding techniques to generate top-n
  recommendations.
---

# Combining Embedding-Based and Semantic-Based

## Quick Facts
- Authors: Anne Schuth, Harrie Oosterhuis, Pavel Serdyukov, Maarten de Rijke
- Paper link: https://arxiv.org/abs/1704.08242
- Published: April 2017
- Key contribution: Combining neural and semantic embeddings with probabilistic relevance model (PRM) for search result diversification

## Executive Summary
The paper introduces a method that combines neural and semantic embeddings with a probabilistic relevance model (PRM) to improve search result diversification. This approach integrates information retrieval (IR) and recommender systems by addressing the limitations of traditional query-document matching with a more nuanced, context-aware ranking system. The authors demonstrate that their combined model outperforms existing baselines in terms of diversification quality, measured using normalized discounted cumulative gain (NDCG) and its diversity-aware variant, α-nDCG.

## Method Summary
The paper proposes a two-stage method that first ranks documents based on relevance and then re-ranks them to maximize diversity. The approach uses two types of embeddings: neural embeddings (trained on query-document pairs) and semantic embeddings (derived from click graphs). These embeddings are combined with the probabilistic relevance model (PRM) to generate a final ranking that balances relevance and diversity.

## Key Results
The combined model outperformed all baselines, including NDCG, α-nDCG, PM2, and PRM alone, in terms of α-nDCG. The results show that incorporating both neural and semantic embeddings significantly improves the diversification of search results, leading to better user satisfaction. The improvements were consistent across different datasets and evaluation metrics.

## Why This Works (Mechanism)
The success of the combined model lies in its ability to leverage both neural and semantic embeddings. Neural embeddings capture the latent relationships between queries and documents, while semantic embeddings provide additional context from user interactions (e.g., click graphs). By combining these embeddings with the PRM, the model can better balance relevance and diversity, leading to more satisfying search results for users.

## Foundational Learning
The model builds on the probabilistic relevance model (PRM), which is a well-established framework for search result diversification. The authors extend PRM by incorporating embeddings learned from both neural networks and semantic relationships. This approach combines the strengths of deep learning (for capturing complex patterns) and traditional IR methods (for ensuring relevance and diversity).

## Architecture Onboarding
The model architecture consists of two main components: (1) embedding generation, where neural and semantic embeddings are created, and (2) ranking, where the PRM uses these embeddings to generate a diversified ranking. The neural embeddings are trained using a Siamese network architecture, while the semantic embeddings are derived from a click graph. The final ranking is produced by combining these embeddings with the PRM.

## Open Questions the Paper Calls Out
The authors acknowledge that the computational cost of the combined model is higher than that of traditional methods, which could limit its scalability. They also note that the model's performance may vary depending on the quality and size of the training data, as well as the specific domain or application.

## Limitations
The main limitation of the paper is the computational cost of the combined model, which may not be suitable for large-scale applications. Additionally, the model's performance is dependent on the quality of the embeddings, which may vary depending on the dataset and training process. The authors also note that the model may not generalize well to domains with limited user interaction data.

## Confidence
High. The paper provides a thorough evaluation of the combined model, including extensive experiments and comparisons with existing baselines. The results are consistent and significant, and the authors address potential limitations and open questions.

## Next Checks
1. Investigate the computational cost and scalability of the combined model in larger-scale applications.
2. Explore the impact of different embedding generation methods on the model's performance.
3. Evaluate the model's generalization to different domains and applications, particularly those with limited user interaction data.