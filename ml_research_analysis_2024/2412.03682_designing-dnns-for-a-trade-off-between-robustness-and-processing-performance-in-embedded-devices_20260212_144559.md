---
ver: rpa2
title: Designing DNNs for a trade-off between robustness and processing performance
  in embedded devices
arxiv_id: '2412.03682'
source_url: https://arxiv.org/abs/2412.03682
tags:
- error
- robustness
- sigmoid
- pruning
- dnns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of bounded activation functions
  (AFs) to enhance robustness against soft errors in DNNs for embedded systems. The
  authors evaluate ReLU, Sigmoid, and Hard Sigmoid AFs in an encoder-decoder fully
  convolutional network for hyperspectral image semantic segmentation.
---

# Designing DNNs for a trade-off between robustness and processing performance in embedded devices

## Quick Facts
- arXiv ID: 2412.03682
- Source URL: https://arxiv.org/abs/2412.03682
- Authors: Jon Gutiérrez-Zaballa; Koldo Basterretxea; Javier Echanobe
- Reference count: 14
- Primary result: Bounded activation functions (Sigmoid, Hard Sigmoid) significantly reduce error propagation compared to ReLU in fault-tolerant DNNs for embedded systems

## Executive Summary
This paper investigates the use of bounded activation functions (AFs) to enhance robustness against soft errors in DNNs for embedded systems. The authors evaluate ReLU, Sigmoid, and Hard Sigmoid AFs in an encoder-decoder fully convolutional network for hyperspectral image semantic segmentation. Through extensive fault injection campaigns, they show that bounded AFs significantly reduce error propagation compared to ReLU, with Hard Sigmoid achieving the best resilience. Model pruning and quantization are applied to improve computational efficiency, with ReLU-based models showing the highest throughput and energy efficiency on the KV260 SoM. However, bounded AFs provide superior robustness, making them a viable option for safety-critical applications requiring a balance between performance and reliability.

## Method Summary
The authors implement a U-Net encoder-decoder architecture for hyperspectral image semantic segmentation using three activation functions: ReLU, Sigmoid, and Hard Sigmoid. They apply model pruning to reduce computational complexity while maintaining accuracy above 90% global and 80% weighted IoU. Quantization is performed to 8-bit integer representation. Fault injection campaigns are conducted using a modified TensorFI2 framework to evaluate robustness against soft errors, measuring error rates, throughput, power consumption, and energy efficiency on the AMD-Xilinx KV260 SoM.

## Key Results
- Bounded activation functions (Sigmoid, Hard Sigmoid) reduce error propagation by 2-3 orders of magnitude compared to ReLU
- Hard Sigmoid achieves the best resilience with minimal accuracy loss (89.57% IoU vs 90.21% for ReLU)
- ReLU-based models show highest throughput (12.72 FPS) and energy efficiency on KV260 SoM
- Model pruning increases vulnerability to MSB bit-flips by concentrating parameters in the 1 < |x| < 2 range
- Quantization reduces error rates by preventing NaN and infinity states in parameter representation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bounded activation functions reduce error propagation by limiting output ranges, preventing unbounded amplification of parameter errors.
- Mechanism: ReLU allows infinite positive values; a perturbed parameter with a large positive spike can propagate unchecked. Sigmoid and Hard Sigmoid squash outputs to [0,1], so any parameter error, regardless of magnitude, is constrained within a finite range.
- Core assumption: The distribution of parameter perturbations follows a pattern where large magnitude errors are common; bounding AFs can truncate their impact.
- Evidence anchors:
  - [abstract] "bounded AFs significantly reduce error propagation compared to ReLU"
  - [section] "bounded AFs show the best resilience as the propagation of generated perturbations throughout the model layers is notably reduced"
  - [corpus] Weak. Corpus neighbors do not address bounded AFs and error propagation specifically.
- Break condition: If parameter perturbations are primarily small-magnitude, bounding AFs may offer little benefit. Also if the training regime or quantization pushes values outside the AF's effective range.

### Mechanism 2
- Claim: Model pruning increases the proportion of parameters in the critical range (1 < |x| < 2), making them more susceptible to MSB flips.
- Mechanism: Pruning removes low-magnitude parameters near zero, leaving a higher density of parameters in the 1 < |x| < 2 range. In this range, MSB bit flips convert values to ±∞ or NaN, causing large inference errors.
- Core assumption: Pruning strategy targets least essential parameters, which tend to have small magnitudes; removing them leaves behind a higher fraction of parameters in the sensitive range.
- Evidence anchors:
  - [section] "pruning also increases the proportion of values in the range 1 < |x| < 2 by removing channels with parameters that are usually close to 0"
  - [section] "MSB bit-flip errors in the central area of the U-Net ... mainly occur because of the MSB bit-flip of the parameters which are in the 1 < |x| < 2 interval"
  - [corpus] Weak. Corpus neighbors do not mention parameter value ranges or MSB flip vulnerabilities.
- Break condition: If pruning preserves a balanced parameter magnitude distribution, or if pruning targets parameters across the magnitude spectrum, the effect diminishes.

### Mechanism 3
- Claim: Quantization reduces error rate by limiting parameter representation, making NaN or ∞ impossible.
- Mechanism: In 8-bit integer quantization, parameter values are mapped to a finite discrete set; bit flips cannot produce undefined states like NaN or infinity that exist in floating point.
- Core assumption: The quantization scheme maps floating-point values to integer ranges that exclude the problematic ±∞ and NaN states.
- Evidence anchors:
  - [section] "the inherent binary representation prevents the occurrence of NaNs or infinities, significantly reducing the error rate"
  - [section] "the quantized model may initially seem less robust than its nonquantized counterpart, in this case only the biases are sensitive parameters"
  - [corpus] Weak. Corpus neighbors do not discuss quantization-induced robustness changes.
- Break condition: If quantization is improperly implemented (e.g., asymmetric scaling or bias overflow), the robustness gain may be lost.

## Foundational Learning

- Concept: Single Event Upsets (SEUs) and soft errors
  - Why needed here: The entire robustness analysis is based on how SEUs in memory cells (LUTs, Block RAMs, flip-flops) alter DNN parameters and propagate through the network.
  - Quick check question: What is the difference between a soft error and a hard error in semiconductor devices?
- Concept: Activation function boundedness
  - Why needed here: The choice between ReLU, Sigmoid, and Hard Sigmoid directly affects error propagation and model robustness.
  - Quick check question: What is the output range of Sigmoid and Hard Sigmoid activation functions?
- Concept: Model pruning and quantization impact
  - Why needed here: These compression techniques change parameter distributions and numerical representation, altering the error propagation behavior.
  - Quick check question: How does iterative pruning affect the magnitude distribution of remaining parameters?

## Architecture Onboarding

- Component map:
  - Input: Hyperspectral images (HSI) from HSI-Drive v2.0
  - Encoder-decoder U-Net: 5-level architecture with conv2D, batch norm, activation, pooling, and transposed conv2D
  - AF selection: ReLU, Sigmoid, or Hard Sigmoid per layer
  - Compression pipeline: Pruning → quantization → deployment
  - Deployment target: AMD-Xilinx KV260 SoM with DPU
- Critical path:
  - Parameter perturbation → AF application → skip-connection propagation → output classification
- Design tradeoffs:
  - Bounded AFs improve robustness but increase inference latency (Sigmoid runs on CPU)
  - Pruning improves efficiency but can degrade robustness if it increases critical parameter density
  - Quantization improves robustness but may reduce model accuracy
- Failure signatures:
  - Sudden accuracy drop after pruning: likely increased density of parameters in 1 < |x| < 2 range
  - High error rate in quantized model: likely quantization overflow or bias sensitivity
  - Latency spikes with bounded AFs: likely CPU offloading of non-piecewise-linear functions
- First 3 experiments:
  1. Inject SEUs into a ReLU model and measure error propagation per layer; repeat with Sigmoid and Hard Sigmoid.
  2. Apply MSB-only fault injection to a pruned model; analyze parameter distribution before and after pruning.
  3. Compare error rates between floating-point and quantized models under identical fault injection campaigns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the robustness of bounded activation functions like Hard Sigmoid compare to ReLU under different levels of soft error severity beyond single bit-flips, such as multi-bit errors or more severe radiation-induced faults?
- Basis in paper: [inferred] The paper focuses on single bit-flips but mentions that bounded AFs reduce error propagation. It does not explore the impact of more severe errors.
- Why unresolved: The study only examines single bit-flips using fault injection campaigns, leaving the behavior under more severe errors unexplored.
- What evidence would resolve it: Experimental results from fault injection campaigns testing multi-bit errors or more severe fault types, comparing the robustness of ReLU and bounded AFs under these conditions.

### Open Question 2
- Question: What is the trade-off between the robustness benefits of bounded activation functions and the computational overhead they introduce in real-time embedded systems?
- Basis in paper: [explicit] The paper notes that Hard Sigmoid offers better robustness but mentions computational efficiency concerns, especially for Sigmoid which requires CPU computation.
- Why unresolved: While the paper discusses performance metrics like throughput and energy efficiency, it does not quantify the exact trade-off between robustness and computational overhead in real-time scenarios.
- What evidence would resolve it: Detailed performance benchmarks comparing bounded AFs and ReLU in terms of latency, energy consumption, and robustness under various workloads in real-time embedded systems.

### Open Question 3
- Question: How do bounded activation functions affect the robustness of DNNs in tasks other than semantic segmentation, such as object detection or image classification?
- Basis in paper: [inferred] The study focuses on semantic segmentation for autonomous driving, but the authors suggest that bounded AFs could be beneficial for safety-critical applications.
- Why unresolved: The paper does not extend its analysis to other DNN tasks, leaving the generalizability of the findings uncertain.
- What evidence would resolve it: Comparative studies applying bounded AFs to different DNN architectures (e.g., object detection, image classification) and evaluating their robustness and performance across tasks.

## Limitations
- The paper focuses only on single bit-flip errors, leaving the impact of more severe soft errors unexplored
- Theoretical justification for why bounded activation functions perform better is limited
- The relationship between pruning strategy and parameter magnitude distribution is not fully established

## Confidence
- High Confidence: Fault injection methodology and experimental setup are well-defined. The KV260 SoM deployment and performance metrics are clearly specified.
- Medium Confidence: The comparative analysis between activation functions is robust, but the theoretical explanations for observed differences are limited.
- Low Confidence: The claims about pruning-induced vulnerabilities and quantization robustness lack sufficient theoretical backing and require further validation.

## Next Checks
1. **Theoretical Analysis of Bounded AFs**: Conduct a mathematical analysis of how bounded activation functions constrain error propagation. Derive bounds on error amplification for ReLU, Sigmoid, and Hard Sigmoid under different parameter perturbation scenarios.

2. **Pruning Strategy Impact**: Perform a detailed study of how different pruning strategies (magnitude-based, structured, unstructured) affect the distribution of parameter magnitudes. Analyze the correlation between pruning ratios and vulnerability to MSB flips.

3. **Quantization Robustness Validation**: Implement an exhaustive analysis of quantization artifacts, including overflow, underflow, and precision loss. Validate the claim that quantization prevents NaN/∞ states by testing edge cases in the quantization scheme.