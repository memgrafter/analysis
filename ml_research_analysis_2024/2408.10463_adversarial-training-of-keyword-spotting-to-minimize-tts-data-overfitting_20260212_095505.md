---
ver: rpa2
title: Adversarial training of Keyword Spotting to Minimize TTS Data Overfitting
arxiv_id: '2408.10463'
source_url: https://arxiv.org/abs/2408.10463
tags:
- data
- real
- adversarial
- speech
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of using synthetic text-to-speech
  (TTS) data for training keyword spotting (KWS) models, as TTS data may contain artifacts
  that lead to overfitting and degraded accuracy on real speech. To tackle this, the
  authors propose an adversarial training approach where an auxiliary classifier predicts
  whether input audio is synthetic or real based on the KWS model's hidden layer features.
---

# Adversarial training of Keyword Spotting to Minimize TTS Data Overfitting

## Quick Facts
- arXiv ID: 2408.10463
- Source URL: https://arxiv.org/abs/2408.10463
- Reference count: 0
- Key outcome: Adversarial training improves KWS accuracy on real speech by up to 12% when using both real and TTS data, and by up to 8% even when trained solely on TTS and real negative data without real positive examples.

## Executive Summary
This paper addresses the challenge of using synthetic text-to-speech (TTS) data for training keyword spotting (KWS) models, as TTS data may contain artifacts that lead to overfitting and degraded accuracy on real speech. The authors propose an adversarial training approach where an auxiliary classifier predicts whether input audio is synthetic or real based on the KWS model's hidden layer features. The KWS model is then adversarially trained to make these features domain-invariant. Experimental results show significant improvements in KWS accuracy on real speech data when combining real and synthetic data, and surprisingly, even when trained solely on TTS and real negative data without any real positive examples.

## Method Summary
The proposed method involves adding an auxiliary classifier to a baseline KWS model that predicts whether input audio is synthetic or real based on the KWS model's hidden layer features. The KWS model is trained with a combination of the original KWS loss and an adversarial loss that encourages domain-invariant feature representations. The adversarial loss is applied through a gradient reversal layer that scales the gradient back-propagated from the auxiliary classifier into the KWS model. The authors sweep over different values of the gradient reversal scaling factor λ and the amount of real positive data used in training to evaluate the approach's effectiveness.

## Key Results
- KWS model accuracy on real speech data improves by up to 12% when adversarial loss is used in addition to the original KWS loss.
- Surprisingly, adversarial setup improves accuracy by up to 8% even when trained solely on TTS and real negative speech data, without any real positive examples.
- The choice of gradient reversal scaling factor λ affects the model's performance and convergence.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial training aligns the feature representations of TTS and real speech to reduce domain-specific overfitting.
- Mechanism: An auxiliary classifier is trained to predict whether the input audio is synthetic or real based on the KWS model's hidden layer features. The KWS model is then adversarially trained to make these features domain-invariant, reducing the classifier's ability to distinguish between TTS and real speech.
- Core assumption: The hidden layer features of the KWS model contain information that differentiates TTS from real speech.
- Evidence anchors:
  - [abstract] "Experimental results demonstrate that KWS model accuracy on real speech data can be improved by up to 12% when adversarial loss is used in addition to the original KWS loss."
  - [section] "We then show that accuracy on a real speech evaluation set can be improved by applying adversarial loss to the KWS model under certain data mixture conditions."
  - [corpus] Weak evidence; related papers focus on synthetic data usage but do not discuss adversarial training directly.
- Break condition: If the hidden layer features do not contain significant information that differentiates TTS from real speech, the adversarial classifier will not be effective, and the KWS model will not benefit from adversarial training.

### Mechanism 2
- Claim: Real negative data can serve as contrasting examples against synthetic data to train the adversarial classifier, even without real positive examples.
- Mechanism: Real negative data, which does not contain the target keywords, can still be used to train the adversarial classifier to distinguish between synthetic and real speech. This allows the KWS model to benefit from adversarial training even when no real positive examples are available.
- Core assumption: Real negative data can effectively contrast with synthetic data for the adversarial classifier.
- Evidence anchors:
  - [abstract] "Surprisingly, we also observed that the adversarial setup improves accuracy by up to 8%, even when trained solely on TTS and real negative speech data, without any real positive examples."
  - [section] "We observe no improvement or degradation when there is an intermediate amount of real positive data."
  - [corpus] Weak evidence; related papers do not discuss the use of real negative data in adversarial training.
- Break condition: If real negative data does not provide sufficient contrast with synthetic data, the adversarial classifier will not be effective, and the KWS model will not benefit from adversarial training without real positive examples.

### Mechanism 3
- Claim: The choice of gradient reversal scaling factor (λ) affects the model's performance and convergence.
- Mechanism: The gradient reversal layer scales the gradient back-propagated from the adversarial classifier into the KWS model. A higher λ value increases the strength of the adversarial gradient, making the KWS model more sensitive to the adversarial loss. The optimal λ value depends on the specific data mixture and model architecture.
- Core assumption: The gradient reversal scaling factor (λ) has a significant impact on the model's performance and convergence.
- Evidence anchors:
  - [section] "We varied the sampling probability of the real positive data between 0% and 100% to simulate various conditions with different amounts of available real positive data."
  - [section] "In preliminary experiments, we found that the model's performance and convergence depended on the choice of gradient reversal scaling factor λ, so we swept over multiple λ values."
  - [corpus] Weak evidence; related papers do not discuss the impact of gradient reversal scaling factor on model performance.
- Break condition: If the gradient reversal scaling factor (λ) is set too high or too low, the adversarial training may not converge or may lead to suboptimal performance.

## Foundational Learning

- Concept: Adversarial training
  - Why needed here: To reduce the representation mismatch between synthetic TTS and real speech domains, preventing the KWS model from overfitting to TTS-specific features.
  - Quick check question: What is the purpose of the gradient reversal layer in adversarial training?

- Concept: Domain adaptation
  - Why needed here: To improve the generalization of the KWS model to real speech data when trained on large amounts of TTS data.
  - Quick check question: How does adversarial training encourage a neural network to learn domain-invariant features?

- Concept: Feature extraction and representation learning
  - Why needed here: To identify and extract the relevant features from the audio data that can be used to distinguish between synthetic and real speech.
  - Quick check question: What are the key features in the audio that can differentiate real from synthetic audio?

## Architecture Onboarding

- Component map:
  - Input features -> Baseline KWS model (7 factored convolution layers, 3 bottleneck projection layers) -> Hidden layer activations -> Adversarial classifier -> Synthetic/real prediction
  - KWS loss and adversarial loss -> Gradient reversal layer -> KWS model weights

- Critical path:
  1. Extract input features from the audio data.
  2. Pass the features through the KWS model to obtain hidden layer activations.
  3. Feed the hidden layer activations into the adversarial classifier.
  4. Compute the adversarial loss based on the classifier's predictions.
  5. Apply the adversarial loss to the KWS model weights using the gradient reversal layer.
  6. Minimize the combined KWS loss and adversarial loss to update the model weights.

- Design tradeoffs:
  - The choice of gradient reversal scaling factor (λ) affects the strength of the adversarial gradient and the model's performance.
  - The use of real negative data allows for adversarial training without real positive examples, but may lead to suboptimal performance compared to using real positive data.
  - The choice of input features for the adversarial classifier affects its ability to distinguish between synthetic and real speech.

- Failure signatures:
  - If the adversarial classifier's accuracy is low, it indicates that the hidden layer features do not contain significant information that differentiates TTS from real speech.
  - If the KWS model's accuracy on real speech data does not improve with adversarial training, it may indicate that the adversarial gradient is not strong enough or that the real negative data is not providing sufficient contrast.

- First 3 experiments:
  1. Train the baseline KWS model with only real positive and negative data to establish a performance baseline.
  2. Train the adversarial classifier with real and synthetic data to evaluate its ability to distinguish between the two domains.
  3. Train the KWS model with adversarial training using real positive, real negative, and synthetic data to evaluate the impact on real speech accuracy.

## Open Questions the Paper Calls Out
None

## Limitations
- The experiments rely on fixed amounts of real positive/negative data, making it unclear how the approach scales to more limited real data scenarios typical in KWS applications.
- The paper doesn't report the actual accuracy of the auxiliary classifier in distinguishing synthetic from real speech, which would provide important validation of whether the adversarial training is targeting meaningful differences.
- Results are reported only on English data; the approach's effectiveness across different languages, accents, and acoustic environments remains untested.

## Confidence
- High confidence: The core mechanism of using adversarial training to align feature distributions between synthetic and real speech is well-established in domain adaptation literature. The empirical results showing improved accuracy (up to 12%) on real speech data when combining real and synthetic data are clearly demonstrated.
- Medium confidence: The surprising finding that adversarial training works even without real positive examples (8% improvement) is demonstrated but not deeply explained. The mechanism by which real negative data provides sufficient contrast is plausible but not thoroughly validated.
- Medium confidence: The sensitivity to the gradient reversal scaling factor λ is reported, but the optimal values appear somewhat arbitrary (0.3-0.5) without clear theoretical justification for why these ranges work best.

## Next Checks
1. Report and analyze the accuracy of the auxiliary classifier in distinguishing synthetic from real speech. This would validate whether the adversarial training is targeting meaningful differences and help explain why certain data mixtures work better than others.

2. Systematically test whether the adversarial classifier needs to see all hidden layer activations versus just the final layer or specific intermediate layers. This would clarify the minimal requirements for effective adversarial training.

3. Repeat experiments with progressively smaller amounts of real positive data (e.g., 100K, 10K, 1K utterances) to determine the practical limits of the approach when real data is extremely scarce, which is often the case in real-world KWS deployments.