---
ver: rpa2
title: 'Dissertation: On the Theoretical Foundation of Model Comparison and Evaluation
  for Recommender System'
arxiv_id: '2411.01843'
source_url: https://arxiv.org/abs/2411.01843
tags:
- page
- recommendation
- sampling
- recall
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work provides a comprehensive analysis of recommendation evaluation
  metrics under sampling, revealing an alignment between global and sampling-based
  top-K Recall curves through mapping functions. It introduces novel estimators (M
  ES, M LE, and optimal bias-variance estimators) that accurately recover global metrics
  from sampled data, with the adaptive sampling method addressing the "blind spot"
  issue for small K values.
---

# Dissertation: On the Theoretical Foundation of Model Comparison and Evaluation for Recommender System

## Quick Facts
- arXiv ID: 2411.01843
- Source URL: https://arxiv.org/abs/2411.01843
- Authors: Dong Li
- Reference count: 0
- One-line primary result: Introduces novel estimators and adaptive sampling methods that accurately recover global recommendation metrics from sampled data with significant improvements in estimation accuracy

## Executive Summary
This dissertation provides a comprehensive theoretical foundation for evaluating recommender systems using sampling-based methods. The work addresses the critical challenge of efficiently assessing recommendation models at scale by developing novel estimators (M ES, M LE, and optimal bias-variance estimators) that can accurately recover global metrics from sampled data. The study introduces an adaptive sampling method that specifically addresses the "blind spot" issue for small K values, where traditional sampling methods lack resolution. Experimental results demonstrate that these methods can reduce relative errors to less than 2% for small K values while using fewer samples than traditional approaches, establishing theoretical foundations for reliable sampling-based recommendation evaluation.

## Method Summary
The dissertation develops a comprehensive framework for sampling-based evaluation of recommender systems. The method involves generating sampled datasets through item and user sampling strategies, applying various recommendation algorithms (matrix factorization, regression, neural methods), and using novel estimators to recover global metrics from sampled data. The approach includes mapping functions to align sampling-based and global metrics, optimal estimators that minimize bias-variance tradeoff, and adaptive sampling strategies that iteratively refine samples for users with top-ranked items. The framework is validated across multiple datasets (Pinterest-20, Yelp, ML-20M) using leave-one-out protocols and compared against ground truth global metrics to establish accuracy and efficiency improvements.

## Key Results
- Novel estimators (M ES, M LE, and optimal bias-variance estimators) accurately recover global metrics from sampled data with relative errors <2% for small K values
- Adaptive sampling method addresses "blind spot" issue for small K values by iteratively increasing sample resolution for top-ranked items
- User-sampling validation shows accurate metric estimation with only 1-10% of users sampled, significantly reducing computational costs
- Theoretical analysis reveals alignment between global and sampling-based top-K recall curves through mapping functions that preserve ranking trends

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Sampling-based top-K recall metrics can approximate global metrics through a mapping function that preserves the overall ranking curve trend.
- **Mechanism**: The sampling process preserves the relative ordering of recommendation models even though absolute values differ. By identifying a mapping function f(K) that aligns sampling recall at position k with global recall at position f(k), we can recover the global metric curve from sampled data.
- **Core assumption**: The empirical rank distribution of users under sampling is a scaled version of the global rank distribution, allowing a consistent mapping function across different algorithms on the same dataset.
- **Evidence anchors**:
  - [abstract]: "revealing an alignment between global and sampling-based top-K Recall curves through mapping functions"
  - [section 2.3.3]: "we may conjecture that it is the overall curve TRecall@K that is being approximated by T S Recall@K"
  - [corpus]: Weak - no direct corpus support for this specific mechanism
- **Break condition**: The mapping function fails when the rank distribution under sampling deviates significantly from a scaled version of the global distribution, such as when sampling is extremely small or when the dataset has highly skewed rank distributions.

### Mechanism 2
- **Claim**: Optimal bias-variance tradeoff estimators can accurately recover global metrics by directly minimizing estimation error.
- **Mechanism**: By formulating the estimation as a constrained least squares problem that explicitly optimizes the expected mean squared error between sampled and global metrics, we can derive estimators that outperform heuristic approaches.
- **Core assumption**: The user rank distribution can be estimated from sampled data, and this estimation error directly impacts the final metric estimation error in a predictable way.
- **Evidence anchors**:
  - [abstract]: "introduces novel estimators (M ES, M LE, and optimal bias-variance estimators) that accurately recover global metrics from sampled data"
  - [section 2.5]: "we introduce a new estimator which aims to directly minimize the expected errors between the item sampling-based top-K metrics and the global top-K metrics"
  - [corpus]: Weak - no direct corpus support for this specific mechanism
- **Break condition**: The estimator breaks when the underlying rank distribution is too complex to estimate accurately from limited samples, or when the user population is too small for statistical guarantees.

### Mechanism 3
- **Claim**: Adaptive sampling strategies can address the "blind spot" issue by increasing resolution for top-ranked items.
- **Mechanism**: By iteratively increasing sample size for users whose target items rank at the top of sampled lists, we can obtain more accurate estimates for small K values where standard sampling lacks resolution.
- **Core assumption**: The rank of an item in a small sample set is a poor predictor of its global rank, especially when it appears at the top, requiring additional sampling to resolve.
- **Evidence anchors**:
  - [abstract]: "adaptive sampling method addressing the 'blind spot' issue for small K values"
  - [section 2.6.1]: "current sampling approaches treat all items equally and particularly have difficulty in recovering the global top-K metrics when K is small"
  - [corpus]: Weak - no direct corpus support for this specific mechanism
- **Break condition**: The adaptive strategy becomes inefficient when most items consistently rank at the top of small samples, requiring excessive sampling to resolve differences.

## Foundational Learning

- **Concept**: Statistical inference and estimation theory
  - Why needed here: The entire dissertation relies on understanding how to estimate population parameters (global metrics) from sample statistics (sampled metrics) under various conditions and constraints.
  - Quick check question: What is the difference between a biased and unbiased estimator, and why does bias matter for recommendation evaluation?

- **Concept**: Matrix factorization and low-rank regression
  - Why needed here: The theoretical analysis compares these two fundamental recommendation approaches, showing how they relate through singular value decomposition and different regularization strategies.
  - Quick check question: How does the closed-form solution for low-rank regression differ from matrix factorization in terms of singular value scaling?

- **Concept**: Contrastive learning and mutual information estimation
  - Why needed here: The dissertation introduces contrastive learning losses to recommendation, showing how debiased contrastive objectives can improve recommendation quality over traditional losses.
  - Quick check question: What is the relationship between InfoNCE loss and mutual information, and how does debiasing help in recommendation settings?

## Architecture Onboarding

- **Component map**: Data preprocessing (splitting into training/test sets) -> Model training (various recommendation algorithms) -> Sampling evaluation (item/user sampling with various estimators) -> Metric estimation (mapping functions and optimal estimators) -> Adaptive sampling (iterative refinement)
- **Critical path**: The most critical path is from sampling data generation through metric estimation to final evaluation, as errors compound through each stage. The mapping function alignment and optimal estimator components are particularly critical for accuracy.
- **Design tradeoffs**: Sampling vs. computation tradeoff (more samples = better accuracy but higher cost), bias vs. variance tradeoff in estimators (different estimators prioritize different aspects), and adaptive vs. fixed sampling (adaptive is more accurate but more complex)
- **Failure signatures**: Large discrepancies between sampling and global metrics indicate mapping function failure or insufficient samples; poor model ranking preservation suggests the sampling process isn't capturing relative performance; high variance in repeated experiments suggests statistical instability
- **First 3 experiments**:
  1. Validate the alignment between sampling and global recall curves using the mapping function on a small dataset with known ground truth
  2. Compare the accuracy of different estimators (M ES, M LE, BV) on the same sampled data to establish relative performance
  3. Test the adaptive sampling strategy on a dataset with known difficulty (small K values) to demonstrate blind spot resolution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal hyperparameter λi vary across different recommendation algorithms and datasets in the closed-form solution Equation 69?
- Basis in paper: [explicit] The paper mentions searching optimal λi parameters using BPR loss and observing how they adjust singular values differently from constant λ, but doesn't provide comprehensive cross-algorithm/dataset comparisons.
- Why unresolved: The experimental section only shows results for a few algorithms on specific datasets, leaving open whether patterns exist across different recommendation paradigms.
- What evidence would resolve it: A systematic study varying λi across multiple algorithms (matrix factorization, regression, graph-based) and diverse datasets (dense vs sparse, user-item ratio) to identify whether optimal λi follows predictable patterns based on data characteristics.

### Open Question 2
- Question: What is the theoretical relationship between the debiased pointwise losses and the optimal ranking metrics (Recall, NDCG) they aim to approximate?
- Basis in paper: [inferred] The paper introduces debiased MSE and CCL losses following contrastive learning principles, but doesn't establish formal bounds or convergence guarantees between these losses and ranking metrics.
- Why unresolved: While empirical results show improvements, the paper lacks theoretical analysis of whether minimizing debiased pointwise losses actually optimizes the ranking metrics of interest.
- What evidence would resolve it: Mathematical proofs establishing error bounds between debiased pointwise loss minimization and ranking metric optimization, or counterexamples showing divergence.

### Open Question 3
- Question: How do MINE+ and other mutual information-based losses compare to contrastive losses when incorporating side information (content features, user attributes, context)?
- Basis in paper: [explicit] The paper introduces MINE+ for basic recommendation settings but doesn't explore its behavior with auxiliary information that's common in real systems.
- Why unresolved: The experimental section focuses on pure collaborative filtering without side information, leaving open questions about MINE+'s effectiveness in hybrid recommendation scenarios.
- What evidence would resolve it: Experiments comparing MINE+, InfoNCE, and CCL on datasets with rich side information, measuring both recommendation quality and computational efficiency trade-offs.

## Limitations

1. **Dataset Dependence**: Results heavily rely on specific implicit feedback datasets; performance on explicit feedback or sequential recommendation scenarios remains untested.
2. **Model Scope**: Primary validation focuses on traditional matrix factorization and neural approaches; emerging model families (graph neural networks, transformers) may exhibit different sampling behaviors.
3. **Computational Overhead**: While sampling reduces evaluation cost, the proposed optimal estimators and adaptive strategies add computational complexity that may offset gains for small datasets.

## Confidence

**Mechanism 1 Confidence: Medium**
- Strong empirical alignment but limited theoretical guarantees for universality across datasets

**Mechanism 2 Confidence: High**
- Strong empirical performance with quantifiable error bounds, though computational scaling remains a concern

**Mechanism 3 Confidence: Medium**
- Effective for blind spot resolution but requires careful tuning and may introduce variance in iterative process

## Next Checks

1. **Cross-Dataset Validation**: Test the mapping function alignment and estimator accuracy on at least three additional datasets with varying interaction patterns (explicit feedback, sequential data, different domain characteristics) to assess generalizability.

2. **Modern Model Compatibility**: Evaluate sampling-based metrics for state-of-the-art recommendation models including GNNs and transformer-based approaches to verify the theoretical framework extends beyond traditional methods.

3. **Scalability Analysis**: Measure computational overhead and accuracy trade-offs across datasets ranging from 10K to 10M users to establish practical bounds for when sampling-based evaluation becomes beneficial.