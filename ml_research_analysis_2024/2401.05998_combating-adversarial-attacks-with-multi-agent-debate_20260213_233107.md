---
ver: rpa2
title: Combating Adversarial Attacks with Multi-Agent Debate
arxiv_id: '2401.05998'
source_url: https://arxiv.org/abs/2401.05998
tags:
- debate
- multi-agent
- language
- agent
- toxicity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using multi-agent debate to reduce toxicity
  in language model responses to adversarial prompts. The authors implement a debate
  framework where language model agents discuss and refine responses to prompts known
  to elicit harmful outputs.
---

# Combating Adversarial Attacks with Multi-Agent Debate

## Quick Facts
- arXiv ID: 2401.05998
- Source URL: https://arxiv.org/abs/2401.05998
- Authors: Steffi Chern; Zhen Fan; Andy Liu
- Reference count: 5
- Primary result: Multi-agent debate reduces toxicity in language model responses to adversarial prompts

## Executive Summary
This paper explores using multi-agent debate to reduce toxicity in language model responses to adversarial prompts. The authors implement a debate framework where language model agents discuss and refine responses to prompts known to elicit harmful outputs. They test this on various models including GPT-3.5 and Llama-2, using prompts from the Anthropic red teaming dataset. Results show that multi-agent debate can significantly reduce toxicity, especially when pairing a harmful agent with a harmless one. The debate approach outperforms self-refinement baselines. However, harmful agents can still negatively influence harmless ones. Overall, the study demonstrates the potential of multi-agent debate as an inference-time method to improve model robustness against adversarial attacks, though further work is needed to optimize the debate framework and understand long-term dynamics.

## Method Summary
The authors implement a multi-agent debate framework where language models with different intentions (harmless, neutral, harmful) discuss and refine responses to adversarial prompts. They use prompts from the Anthropic red teaming dataset and test various model pairings including GPT-3.5 and Llama-2 variants. The framework involves generating initial responses, passing them between agents for debate rounds, and evaluating final outputs for toxicity. They compare debate performance against single-agent and self-refinement baselines, and analyze debate dynamics through embedding clustering of prompt types.

## Key Results
- Multi-agent debate significantly reduces toxicity, especially when pairing harmful with harmless agents
- 1-2 rounds of debate can substantially reduce harmful outputs while maintaining helpful content
- Debate approach outperforms self-refinement baselines but harmful agents can still influence harmless ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-agent debate reduces toxicity by having one agent critique another's harmful response
- Mechanism: When a harmful agent generates toxic content, a harmless agent can provide corrective feedback that leads to reduced toxicity in subsequent responses
- Core assumption: Language models can effectively critique and improve each other's outputs through debate
- Evidence anchors:
  - [abstract] "We find that multi-agent debate can reduce model toxicity when jailbroken or less capable models are forced to debate with non-jailbroken or more capable models"
  - [section] "when we pair a 'harmful' agent (agent with harmful initial response prompt) with a model instructed to follow safety principles, we observe that 1-2 rounds of discussion significantly reduce model output toxicity"
  - [corpus] Weak - related papers don't provide direct evidence for this mechanism
- Break condition: If the harmless agent fails to effectively critique the harmful response, or if the harmful agent dominates the debate

### Mechanism 2
- Claim: Self-reflection through debate leads to improved reasoning about harmful content
- Mechanism: Through iterative rounds of self-evaluation and feedback, agents can recognize potential downstream harms and revise their responses accordingly
- Core assumption: Language models have sufficient reasoning capability to evaluate their own outputs for potential harm
- Evidence anchors:
  - [abstract] "One approach proposed to improve the general quality of language model generations is multi-agent debate, where language models self-evaluate through discussion and feedback"
  - [section] "Methods such as self-refine (Madaan et al., 2023) built upon CoT by having language models repeatedly provide feedback for their own outputs, which was then used to iteratively improve LLM generations"
  - [corpus] Weak - related papers don't provide direct evidence for this specific mechanism
- Break condition: If the model cannot effectively evaluate its own outputs or if iterative refinement doesn't lead to meaningful improvements

### Mechanism 3
- Claim: Debate dynamics create a form of adversarial training at inference time
- Mechanism: By exposing models to harmful prompts in a controlled debate setting, they learn to recognize and resist such attacks
- Core assumption: Exposure to adversarial content in debate format can improve model robustness
- Evidence anchors:
  - [abstract] "We further perform adversarial prompt content classification via embedding clustering, and analyze the susceptibility of different models to different types of attack topics"
  - [section] "we analyze the susceptibility of different models to different types of attacks, we categorize each prompt we used for our experiments from the Anthropic red teaming dataset"
  - [corpus] Weak - related papers don't provide direct evidence for this mechanism
- Break condition: If the debate format doesn't provide sufficient exposure to diverse attack types or if models overfit to specific attack patterns

## Foundational Learning

- Concept: Chain-of-thought reasoning
  - Why needed here: Multi-agent debate builds on the idea that step-by-step reasoning can improve model outputs
  - Quick check question: How does chain-of-thought prompting differ from zero-shot generation?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: Understanding how RLHF affects model behavior helps explain why different model variants respond differently to debate
  - Quick check question: What is the key difference between base models and RLHF-tuned models in terms of safety alignment?

- Concept: Adversarial prompting techniques
  - Why needed here: To understand the nature of the attacks the debate framework is defending against
  - Quick check question: What makes a prompt "adversarial" in the context of language model safety?

## Architecture Onboarding

- Component map: Prompt templates -> Debate framework -> Toxicity classifier -> Embedding clustering system
- Critical path: 1. Generate initial responses from all agents 2. Pass responses between agents for debate rounds 3. Evaluate final outputs for toxicity 4. Analyze results by attack type
- Design tradeoffs:
  - Resource intensity vs. effectiveness: More debate rounds generally improve results but increase computational cost
  - Model capability vs. safety: More capable models tend to be more resistant to attacks but also more expensive to run
  - Prompt complexity vs. transferability: Complex prompts may work well for specific models but not generalize
- Failure signatures:
  - Harmful agents dominating the debate and increasing overall toxicity
  - Models failing to follow debate instructions and producing irrelevant outputs
  - Toxicity classifier failing to accurately identify harmful content
- First 3 experiments:
  1. Single-agent self-reflection vs. zero-shot generation on adversarial prompts
  2. Two-agent debate with both agents having harmless intentions
  3. Two-agent debate with one harmful and one harmless agent

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of multi-agent debate vary with the number of debate rounds and agents involved?
- Basis in paper: [inferred] The paper mentions that subsequent debate rounds have limited effect on output toxicity and that longer debate requires more sophisticated frameworks.
- Why unresolved: The paper only tests up to 2 rounds of debate and uses a simple framework. It doesn't explore the impact of varying numbers of agents or more complex debate structures.
- What evidence would resolve it: Experiments varying the number of debate rounds and agents, with more sophisticated debate frameworks and metrics beyond toxicity scores.

### Open Question 2
- Question: How do different types of adversarial prompts affect the success of multi-agent debate in reducing toxicity?
- Basis in paper: [explicit] The paper performs clustering analysis of adversarial prompts and finds that different models perform differently on various types of attacks, but doesn't fully explore how debate effectiveness varies by prompt type.
- Why unresolved: The paper provides initial analysis of model performance on different prompt types but doesn't deeply investigate how multi-agent debate specifically impacts toxicity reduction for each type.
- What evidence would resolve it: Detailed analysis of multi-agent debate effectiveness across different categories of adversarial prompts, potentially including qualitative analysis of debate outcomes.

### Open Question 3
- Question: How does the quality and capability of individual agents affect the overall effectiveness of multi-agent debate?
- Basis in paper: [explicit] The paper mentions that "if a model is not generally capable enough, it may lack the conversational ability to benefit from multi-agent debate" and compares different model families.
- Why unresolved: The paper only compares a few model types and doesn't systematically vary agent quality or capability to determine its impact on debate effectiveness.
- What evidence would resolve it: Experiments systematically varying the quality and capability of individual agents in debate scenarios, potentially including agents from different model families or with different fine-tuning approaches.

## Limitations
- Effectiveness varies significantly across different model pairings
- Results may not generalize to all types of adversarial attacks
- Computational overhead of multi-agent debate is not thoroughly analyzed

## Confidence
- Mechanism Effectiveness: Medium
- Generalizability: Low-Medium
- Computational Efficiency: Low

Key claims with confidence labels:
- Multi-agent debate reduces toxicity (High confidence)
- Debate outperforms self-refinement (Medium confidence)
- Debate dynamics can be analyzed through clustering (Medium confidence)

## Next Checks
1. **Adversarial robustness test**: Evaluate the debate framework against a broader range of adversarial attacks beyond the Anthropic dataset, including previously unseen attack patterns and more sophisticated jailbreaking techniques.

2. **Long-term debate dynamics**: Conduct experiments with extended debate sequences (more than 2-3 rounds) to understand whether the benefits of debate persist or if harmful agents eventually dominate through repeated exposure.

3. **Cross-model transferability**: Test whether debate prompts and strategies that work for one model family (e.g., GPT-3.5) remain effective when applied to different model architectures (e.g., Llama-2) without modification.