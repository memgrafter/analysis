---
ver: rpa2
title: 'Dissecting Out-of-Distribution Detection and Open-Set Recognition: A Critical
  Analysis of Methods and Benchmarks'
arxiv_id: '2408.16757'
source_url: https://arxiv.org/abs/2408.16757
tags:
- data
- detection
- shift
- training
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts a comprehensive empirical study comparing methods
  for out-of-distribution (OOD) detection and open-set recognition (OSR). The authors
  rigorously cross-evaluate state-of-the-art approaches on standard benchmarks, finding
  strong correlations in performance across the two tasks.
---

# Dissecting Out-of-Distribution Detection and Open-Set Recognition: A Critical Analysis of Methods and Benchmarks

## Quick Facts
- arXiv ID: 2408.16757
- Source URL: https://arxiv.org/abs/2408.16757
- Reference count: 40
- Key outcome: This paper conducts a comprehensive empirical study comparing methods for out-of-distribution (OOD) detection and open-set recognition (OSR). The authors rigorously cross-evaluate state-of-the-art approaches on standard benchmarks, finding strong correlations in performance across the two tasks. They introduce a new large-scale benchmark that disentangles semantic and covariate shifts, and surprisingly discover that Outlier Exposure (OE)—the top performer on existing benchmarks—struggles at scale, while magnitude-aware scoring rules like MLS consistently excel. Further analysis reveals OE's effectiveness depends heavily on auxiliary data overlap with test outliers, a condition difficult to meet in large-scale settings. The authors also propose a new metric, Outlier-Aware Accuracy (OAA), to better assess model robustness under distribution shift. Overall, magnitude-aware scoring rules are recommended as stable and effective across settings.

## Executive Summary
This paper presents a comprehensive empirical study of out-of-distribution (OOD) detection and open-set recognition (OSR) methods, rigorously evaluating state-of-the-art approaches across multiple benchmarks. The authors introduce a novel large-scale benchmark that disentangles semantic and covariate shifts, revealing that Outlier Exposure (OE)—previously the top performer—struggles when scaled up, while magnitude-aware scoring rules like Maximum Logit Score (MLS) consistently excel. The study also proposes a new metric, Outlier-Aware Accuracy (OAA), to better assess model robustness under distribution shift. Overall, the findings suggest that magnitude-aware scoring rules are the most stable and effective across diverse settings, and highlight the importance of carefully considering the characteristics of auxiliary data when using OE.

## Method Summary
The paper evaluates various methods for OOD detection and OSR on image classification tasks. It uses CIFAR-10 as in-distribution data and tests on six OOD datasets (SVHN, Textures, LSUN-Crop, LSUN-Resize, iSUN, Places365) as well as CIFAR-100 for OSR. For large-scale evaluation, it employs datasets like ImageNet-C, ImageNet-R, ImageNet-SSB, CUB-SSB, Waterbirds, Scars-SSB, and FGVC-Aircraft-SSB. The methods include ResNet-18, DenseNet-121, and DinoViT-S/8 architectures trained with cross-entropy (CE), ARPL+CS, and Outlier Exposure (OE) losses. Scoring rules evaluated include Maximum Softmax Probability (MSP), Maximum Logit Score (MLS), ODIN, GODIN, Energy, GradNorm, and SEM. The study also introduces the Outlier-Aware Accuracy (OAA) metric to assess model robustness under distribution shift.

## Key Results
- Outlier Exposure (OE), while the top performer on existing benchmarks, struggles at scale due to the difficulty of finding auxiliary data that reflects the range of possible distribution shifts
- Magnitude-aware scoring rules like MLS and Energy consistently excel across both small-scale and large-scale benchmarks, outperforming other methods
- OE's effectiveness depends heavily on the overlap between the auxiliary data distribution and the test-time OOD data distribution
- The newly proposed Outlier-Aware Accuracy (OAA) metric effectively balances OOD detection performance with model robustness to covariate shift

## Why This Works (Mechanism)
The paper's findings suggest that magnitude-aware scoring rules (MLS, Energy) are effective because they are sensitive to the magnitude of the deep image embeddings, which is a key factor in distinguishing between in-distribution and out-of-distribution data. The study also reveals that OE's effectiveness is contingent on the overlap between the auxiliary data distribution and the test-time OOD data distribution, which is difficult to achieve in large-scale settings. The OAA metric is designed to address the tension between detecting covariate shift and being robust to it, providing a more comprehensive evaluation of model performance under distribution shift.

## Foundational Learning
- **Out-of-Distribution (OOD) Detection**: Identifying data samples that do not belong to the training distribution during inference. Why needed: Essential for building robust models that can handle unexpected inputs in real-world applications. Quick check: Evaluate the model's ability to distinguish between in-distribution and OOD samples using metrics like AUROC.
- **Open-Set Recognition (OSR)**: Classifying known classes while detecting unknown classes that were not seen during training. Why needed: Real-world data often contains novel classes that the model has not been trained on. Quick check: Assess the model's performance on a dataset with both known and unknown classes, measuring its ability to correctly classify known classes and detect unknown classes.
- **Scoring Rules**: Functions used to quantify the likelihood of a sample being OOD or belonging to an unknown class. Why needed: Different scoring rules have varying sensitivities to different types of distribution shifts. Quick check: Compare the performance of different scoring rules (e.g., MSP, MLS, Energy) on a range of OOD detection and OSR tasks.

## Architecture Onboarding

**Component map**: CIFAR-10 -> ResNet-18/DenseNet-121/DinoViT-S/8 -> Cross-Entropy/ARPL+CS/OE -> MSP/MLS/ODIN/Energy/GODIN/GradNorm/SEM -> AUROC/Accuracy/OAA

**Critical path**: In-distribution data -> Model training with CE/ARPL+CS/OE -> Scoring rule application -> OOD/OSR evaluation -> Metric computation (AUROC/Accuracy/OAA)

**Design tradeoffs**: The choice of architecture (ResNet-18, DenseNet-121, DinoViT-S/8) and training method (CE, ARPL+CS, OE) affects the model's ability to detect OOD samples and recognize unknown classes. The selection of scoring rules (MSP, MLS, ODIN, Energy, etc.) further influences the performance on different types of distribution shifts.

**Failure signatures**: Poor OOD detection performance due to improper hyperparameter tuning of scoring rules (e.g., temperature scaling for ODIN). Unstable training when using OE with mismatched auxiliary data distribution - verify feature space overlap between auxiliary and test OOD data.

**First experiments**:
1. Train a baseline model using CE loss on CIFAR-10 and evaluate its performance on OOD detection using MSP and MLS scoring rules.
2. Implement and train models using ARPL+CS and OE losses, comparing their OOD detection performance against the baseline.
3. Evaluate all methods on the newly proposed large-scale benchmark, measuring their performance using AUROC, accuracy, and OAA metrics.

## Open Questions the Paper Calls Out
- **Open Question 1**: What is the precise mechanism by which magnitude-aware scoring rules (MLS, Energy) outperform other methods across diverse distribution shift scenarios?
  - Basis in paper: The paper demonstrates that MLS and Energy consistently achieve superior performance on both small-scale and large-scale benchmarks, outperforming other scoring rules like MSP, ODIN, and ReAct.
  - Why unresolved: While the paper observes that magnitude-aware methods excel, it does not provide a definitive explanation for why they are consistently superior. The authors hypothesize that these methods' sensitivity to the magnitude of the deep image embeddings is a key factor, but this is not rigorously proven.
  - What evidence would resolve it: Controlled experiments isolating the impact of feature magnitude on OOD detection performance, ablation studies examining the effect of different network architectures and training strategies on the effectiveness of magnitude-aware scoring rules, and theoretical analysis of the relationship between feature magnitude and distribution shift.

- **Open Question 2**: What are the key factors that determine the effectiveness of auxiliary data in Outlier Exposure (OE) for OOD detection?
  - Basis in paper: The paper reveals that OE's effectiveness depends heavily on the overlap between the auxiliary data distribution and the test-time OOD data distribution. It finds that OE struggles at scale due to the difficulty of finding auxiliary data that reflects the range of possible distribution shifts.
  - Why unresolved: The paper does not provide a comprehensive framework for selecting or generating auxiliary data that is effective for OE across diverse and large-scale datasets. The relationship between auxiliary data characteristics and OOD detection performance remains unclear.
  - What evidence would resolve it: Systematic studies examining the impact of different types of auxiliary data (e.g., real-world datasets, synthetic data, data with specific characteristics) on OE performance, analysis of the feature space overlap between auxiliary data and OOD data, and development of methods for automatically generating or selecting effective auxiliary data.

- **Open Question 3**: How can we develop a unified metric that effectively balances OOD detection performance with model robustness to covariate shift?
  - Basis in paper: The paper introduces the Outlier-Aware Accuracy (OAA) metric to address the tension between detecting covariate shift and being robust to it. However, the authors acknowledge that OAA is a starting point and that further research is needed to develop a more comprehensive metric.
  - Why unresolved: The OAA metric provides a single value that captures both OOD detection and model robustness, but it may not fully capture the nuances of these two tasks. A more sophisticated metric might be needed to provide a more nuanced evaluation of model performance under distribution shift.
  - What evidence would resolve it: Development of alternative metrics that incorporate additional factors such as the severity of covariate shift, the impact of distribution shift on model performance across different classes or subpopulations, and the trade-off between OOD detection and model accuracy.

## Limitations
- The analysis focuses primarily on image classification tasks with CIFAR-10 and related datasets, potentially limiting generalizability to other domains or more complex data distributions.
- The finding that OE's performance depends heavily on auxiliary data overlap raises questions about its applicability in real-world scenarios where such overlap may be difficult to achieve.
- While the large-scale benchmark provides valuable insights, the specific choice of datasets may influence the conclusions about method effectiveness.

## Confidence
- **High confidence**: The observation that magnitude-aware scoring rules (MLS) consistently outperform other methods across different benchmarks and scales
- **Medium confidence**: The claim that OE's effectiveness is contingent on auxiliary data overlap with test outliers, based on the synthetic covariate shift experiments
- **Medium confidence**: The recommendation to use magnitude-aware scoring rules as a default approach, given their demonstrated stability across various settings

## Next Checks
1. Evaluate the proposed methods on non-image datasets (e.g., text or tabular data) to assess cross-domain applicability
2. Conduct ablation studies on the impact of different auxiliary dataset choices for OE training across multiple OOD scenarios
3. Test the large-scale benchmark findings with additional backbone architectures beyond ResNet-18, DenseNet-121, and DinoViT-S/8