---
ver: rpa2
title: Towards Compatible Fine-tuning for Vision-Language Model Updates
arxiv_id: '2412.20895'
source_url: https://arxiv.org/abs/2412.20895
tags:
- clip
- methods
- prompts
- compatibility
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the compatibility of efficient fine-tuning
  methods for vision-language models when the underlying foundation model is updated.
  While existing fine-tuning approaches like shallow-layer prompt tuning and deep-layer
  adapter-based methods perform well on current models, they often fail to maintain
  effectiveness after model upgrades, requiring costly retraining.
---

# Towards Compatible Fine-tuning for Vision-Language Model Updates

## Quick Facts
- arXiv ID: 2412.20895
- Source URL: https://arxiv.org/abs/2412.20895
- Reference count: 9
- This paper proposes Class-conditioned Context Optimization (ContCoOp) to address compatibility issues when fine-tuning vision-language models undergo updates

## Executive Summary
This paper investigates the critical issue of compatibility in efficient fine-tuning methods for vision-language models when the underlying foundation model is updated. While existing approaches like shallow-layer prompt tuning and deep-layer adapter-based methods perform well on current models, they often fail to maintain effectiveness after model upgrades, requiring costly retraining. The authors propose Class-conditioned Context Optimization (ContCoOp), which learns class-conditioned prompts integrated through an attention network at the text encoder's input, allowing prompts to dynamically adapt to changes in class embeddings when the model is updated.

## Method Summary
The authors analyze compatibility issues by comparing parameter and feature changes across layers when CLIP is upgraded to EVA-CLIP, finding that shallow layers exhibit better transferability. They propose ContCoOp, which learns class-conditioned prompts using an attention network that integrates class embeddings with learnable prompts at the text encoder input. The method employs knowledge distillation from zero-shot prompts to provide a stable reference that evolves with the model. The approach is evaluated on 15 datasets with 16-shot training, measuring compatibility across original CLIP, upgraded EVA-CLIP, and harmonic mean performance.

## Key Results
- ContCoOp achieves average improvements of 3.57% on the upgraded model and 2.68% on harmonic mean of original and upgraded model performance compared to baseline methods
- Shallow-layer methods (like ContCoOp) demonstrate better compatibility with model updates than deep-layer methods (like CLIP-Adapter and Tip-Adapter)
- ContCoOp shows strong out-of-distribution generalization performance, particularly when training and testing on different domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Class-conditioned prompts adapt dynamically to model updates because they are conditioned on class embeddings that change with the model.
- Mechanism: ContCoOp integrates learnable prompts with class embeddings using an attention network at the text encoder input. When the model is updated, the class embeddings change, and the attention network automatically updates the prompts accordingly.
- Core assumption: The attention network effectively captures and adapts to changes in class embeddings.
- Evidence anchors:
  - [abstract]: "Consequently, the prompts can dynamically adapt to the changes in embedding space (due to model updates), ensuring continued effectiveness."
  - [section]: "Leveraging the attention network, our method integrates class information into learnable prompts... After fusion, we extract the output prompts as Attn(p, c) = O[: N] ∈ RN ×D."
- Break condition: If the attention network fails to capture the semantic changes in class embeddings, or if the embedding space changes in a way that breaks the attention mechanism's assumptions.

### Mechanism 2
- Claim: Shallow-layer methods exhibit better compatibility with model updates than deep-layer methods due to smaller parameter and feature changes in early layers.
- Mechanism: The study shows that when models are upgraded, shallower layers experience smaller absolute and relative changes in parameters and output features compared to deeper layers. This stability allows modules inserted at shallow layers to maintain their effectiveness.
- Core assumption: The transferability of layers is inversely correlated with the magnitude of changes in parameters and features during model updates.
- Evidence anchors:
  - [abstract]: "The study reveals that many high-performing fine-tuning methods fail to be compatible with the upgraded models... we posit that, for the model upgrade of VLMs, the shallow layers exhibit better transferability compared to the deeper layers."
  - [section]: "In Figure 2 (b-c), we calculate the average absolute and relative changes in parameters and output features across different layers. The results reveal that, after the model upgrade, shallow layers exhibit smaller changes in parameters and output features compared to deep layers."
- Break condition: If future model updates cause significant changes even in shallow layers, or if the relationship between layer depth and change magnitude does not hold for other model architectures.

### Mechanism 3
- Claim: Knowledge distillation from zero-shot prompts improves compatibility by providing a stable reference that evolves with the model.
- Mechanism: ContCoOp uses a knowledge distillation loss to fuse zero-shot classifier knowledge into learnable prompts. Since zero-shot prompts are generated by the text encoder, they automatically adapt to changes in the model, providing a stable yet evolving reference for the prompts.
- Core assumption: Zero-shot prompts encode sufficient semantic information that remains relevant across model versions, and the knowledge distillation effectively transfers this information to the learnable prompts.
- Evidence anchors:
  - [abstract]: "We employ the knowledge distillation lossLkd to fuse its knowledge into the learnable prompts as in [Yao et al., 2023]."
  - [section]: "This approach differs from deep-layer methods, as it integrates the knowledge into the input prompts in the shallow layer, enhancing its compatibility rather than simply ensembling two classifiers."
- Break condition: If zero-shot prompts become less reliable or relevant after model updates, or if the knowledge distillation fails to effectively transfer the information to the prompts.

## Foundational Learning

- Concept: Vision-Language Models (VLMs) and their architecture
  - Why needed here: Understanding how VLMs like CLIP work is crucial for grasping why compatibility issues arise and how ContCoOp addresses them.
  - Quick check question: What are the main components of a VLM, and how are they typically trained?

- Concept: Parameter-Efficient Fine-Tuning (PEFT) methods
  - Why needed here: The paper compares ContCoOp to various PEFT methods, so understanding these approaches is essential for evaluating the contributions.
  - Quick check question: What is the difference between shallow-layer and deep-layer PEFT methods, and why might one be more compatible than the other?

- Concept: Attention mechanisms and their role in adapting to changes
  - Why needed here: ContCoOp uses an attention network to integrate class information into prompts, which is key to its compatibility mechanism.
  - Quick check question: How does an attention network work, and why is it suitable for dynamically adapting to changes in input embeddings?

## Architecture Onboarding

- Component map:
  Image encoder -> Text encoder -> Attention network -> Learnable prompts -> Classification scores

- Critical path:
  1. Input image and class name
  2. Generate class embedding using text encoder
  3. Combine class embedding with learnable prompts using attention network
  4. Feed resulting class-conditioned prompts into text encoder
  5. Compute classification scores using cosine similarity
  6. Apply knowledge distillation loss to align with zero-shot classifier
  7. Optimize learnable prompts and attention network parameters

- Design tradeoffs:
  - Shallow vs. deep layer insertion: Shallow layers offer better compatibility but potentially less task-specific performance
  - Attention network complexity: More complex attention mechanisms might capture changes better but increase computational cost
  - Knowledge distillation strength: Balancing between zero-shot and fine-tuned knowledge is crucial for optimal performance

- Failure signatures:
  - Prompts fail to adapt to model updates, resulting in decreased performance on upgraded models
  - Overfitting to source dataset, leading to poor out-of-distribution generalization
  - Computational overhead becomes prohibitive for large-scale deployment

- First 3 experiments:
  1. Train ContCoOp on a small dataset (e.g., DTD) and test compatibility with a minor model update to verify the attention mechanism works as expected.
  2. Compare performance of ContCoOp with different context lengths (4, 8, 16) on a medium-sized dataset (e.g., UCF101) to find the optimal length.
  3. Test out-of-distribution generalization by training on ImageNet and evaluating on ImageNetV2, ImageNet-Sketch, ImageNet-A, and ImageNet-R to assess robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the compatibility of fine-tuning methods change when foundation models undergo multiple successive updates rather than a single upgrade?
- Basis in paper: [explicit] The paper investigates compatibility with a single upgrade from CLIP to EVA-CLIP, but does not explore multi-step model evolution scenarios
- Why unresolved: The study focuses on a binary comparison between original and upgraded models without examining intermediate or cascading updates
- What evidence would resolve it: Empirical studies measuring performance degradation across multiple sequential model updates, tracking compatibility over model evolution chains

### Open Question 2
- Question: What architectural modifications to the attention network could further improve compatibility while maintaining or enhancing performance?
- Basis in paper: [inferred] The authors use a single-head attention layer with Kaiming initialization, but explore only limited variations in the ablation study
- Why unresolved: The paper demonstrates that attention networks help compatibility but does not systematically explore architectural variations or initialization strategies
- What evidence would resolve it: Comparative experiments testing different attention architectures (multi-head, cross-attention, dynamic head counts) and initialization methods across various model upgrades

### Open Question 3
- Question: How do other vision-language models beyond CLIP and EVA-CLIP behave in terms of fine-tuning compatibility with model updates?
- Basis in paper: [explicit] The study focuses specifically on CLIP architecture and its EVA-CLIP upgrade, without testing other VLM architectures
- Why unresolved: The authors acknowledge they plan to extend research to other modalities but provide no empirical data on different VLM architectures
- What evidence would resolve it: Systematic evaluation of fine-tuning compatibility across multiple VLM architectures (e.g., ALIGN, FLAVA, BLIP) and their respective upgrades

## Limitations
- The compatibility claims are primarily validated on a single model upgrade path (CLIP to EVA-CLIP), which may not generalize to other VLM architectures or update patterns.
- The attention mechanism's effectiveness relies heavily on the assumption that class embedding changes are semantically meaningful and follow predictable patterns.
- The 16-shot training protocol may not reflect real-world fine-tuning scenarios where more data is available.

## Confidence
- High confidence: The empirical observation that shallow-layer methods show better compatibility than deep-layer methods (supported by layer-wise change analysis and extensive experiments across 15 datasets)
- Medium confidence: The effectiveness of class-conditioned attention mechanism in maintaining compatibility (well-motivated but relies on assumptions about embedding space changes)
- Low confidence: The generalizability of results to other VLM architectures and update patterns beyond the CLIP to EVA-CLIP transition

## Next Checks
1. Test ContCoOp's compatibility on additional model upgrade scenarios, such as different CLIP versions or other VLMs like ALIGN or OpenCLIP, to assess generalizability beyond the EVA-CLIP case.
2. Conduct ablation studies isolating the contributions of class conditioning versus attention mechanism versus knowledge distillation to better understand which components are essential for compatibility.
3. Evaluate ContCoOp's performance with varying amounts of training data (beyond 16-shot) to determine if the compatibility benefits persist under more realistic fine-tuning conditions.