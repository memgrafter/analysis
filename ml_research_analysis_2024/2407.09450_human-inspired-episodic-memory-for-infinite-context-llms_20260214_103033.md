---
ver: rpa2
title: Human-inspired Episodic Memory for Infinite Context LLMs
arxiv_id: '2407.09450'
source_url: https://arxiv.org/abs/2407.09450
tags:
- memory
- context
- event
- em-llm
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EM-LLM, a retrieval-based approach for extending
  transformer-based LLMs to handle extremely long sequences without fine-tuning. EM-LLM
  segments token sequences into episodic events using Bayesian surprise and graph-theoretic
  boundary refinement, then retrieves these events through a two-stage process combining
  similarity-based and temporally contiguous retrieval.
---

# Human-inspired Episodic Memory for Infinite Context LLMs

## Quick Facts
- arXiv ID: 2407.09450
- Source URL: https://arxiv.org/abs/2407.09450
- Reference count: 40
- Human-inspired episodic memory enables LLMs to handle extremely long sequences without fine-tuning

## Executive Summary
This paper introduces EM-LLM, a retrieval-based approach that extends transformer-based LLMs to handle extremely long sequences without fine-tuning. EM-LLM segments token sequences into episodic events using Bayesian surprise and graph-theoretic boundary refinement, then retrieves these events through a two-stage process combining similarity-based and temporally contiguous retrieval. The method consistently outperforms state-of-the-art InfLLM retrieval model across 5 different base LLMs, with up to 40% improvement on retrieval tasks and 29.7% on multi-document QA tasks, while achieving 100% accuracy on the 10M token retrieval task.

## Method Summary
EM-LLM integrates human episodic memory mechanisms into transformer-based LLMs by segmenting token sequences into episodic events using Bayesian surprise and graph-theoretic boundary refinement. The method employs a two-stage retrieval process combining similarity-based k-NN search with temporally contiguous access through a contiguity buffer. Events are stored in a KV cache and retrieved during inference, allowing the LLM to process extremely long sequences without fine-tuning. The approach maintains the original LLM architecture while adding episodic memory capabilities that parallel biological memory systems.

## Key Results
- EM-LLM outperforms InfLLM by up to 40% on retrieval tasks and 29.7% on multi-document QA tasks
- Achieves 100% accuracy on the 10M token retrieval task
- Strong correlations between EM-LLM's event segmentation and human-perceived events in podcast transcripts
- Consistent performance improvements across 5 different base LLMs (Mistral-7B, LLaMA-3, LLaMA-3.1, Phi-3, Phi-3.5)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EM-LLM's surprise-based event segmentation captures human-perceived event boundaries by identifying tokens with high prediction errors.
- Mechanism: The model computes surprise as negative log-likelihood of each token given previous context, then segments sequences at tokens exceeding a moving threshold. This mimics how the brain segments experiences at moments of prediction error.
- Core assumption: High prediction errors in LLMs correlate with human-perceived event boundaries.
- Evidence anchors:
  - [abstract] "our analysis reveals strong correlations between EM-LLM's event segmentation and human-perceived events"
  - [section 3.2] "Following work on cognitive modelling...we employ a thresholding mechanism to perform an initial identification of event boundaries"
- Break condition: If prediction errors don't align with human-perceived events across different domains, the segmentation becomes arbitrary.

### Mechanism 2
- Claim: Graph-theoretic refinement improves intra-event cohesion and inter-event separation, leading to better retrieval performance.
- Mechanism: After initial surprise-based segmentation, the algorithm refines boundaries by optimizing modularity (or conductance) metrics calculated from attention key similarities. This groups semantically related tokens within events while separating dissimilar events.
- Core assumption: Tokens with similar attention keys are semantically related and should be grouped together for efficient retrieval.
- Evidence anchors:
  - [section 3.3] "we introduce a boundary refinement step that looks to optimise this objective...treating the similarity matrix between all keys of an attention head...as an adjacency matrix"
  - [section 4.2] "human-perceived events achieve significantly higher scores in similarity metrics compared to fixed or random events"
- Break condition: If attention key similarity doesn't correlate with semantic relevance, the refinement degrades rather than improves segmentation quality.

### Mechanism 3
- Claim: Two-stage retrieval combining similarity-based and temporally contiguous access enables efficient long-context processing.
- Mechanism: First, k-NN search retrieves events most similar to the current query. Second, a contiguity buffer maintains temporal context by enqueuing neighboring events. This mimics human memory's use of both semantic cues and temporal relationships.
- Core assumption: Combining semantic similarity with temporal contiguity improves retrieval efficiency and accuracy compared to either approach alone.
- Evidence anchors:
  - [abstract] "When needed, these events are retrieved through a two-stage memory process, combining similarity-based and temporally contiguous retrieval"
  - [section 3.4] "our memory retrieval process employs a two-stage mechanism to select relevant episodic events"
- Break condition: If temporal contiguity doesn't improve retrieval for the task domain, the contiguity buffer adds unnecessary overhead.

## Foundational Learning

- Concept: Bayesian surprise and prediction error
  - Why needed here: Surprise quantifies novelty in token sequences and serves as the initial event boundary detector. Understanding how prediction errors trigger event segmentation is crucial for grasping why EM-LLM segments where it does.
  - Quick check question: How does the surprise threshold adapt to contextual shifts, and what happens if it's set too high or too low?

- Concept: Graph modularity and community detection
  - Why needed here: Refinement uses modularity to optimize intra-event cohesion. Knowledge of graph clustering helps understand how tokens are grouped and why some boundaries shift during refinement.
  - Quick check question: What's the difference between maximizing modularity and minimizing conductance, and how do they affect the resulting segmentation?

- Concept: Temporal contiguity and asymmetry in memory retrieval
  - Why needed here: The contiguity buffer exploits temporal relationships in retrieval. Understanding these effects explains why neighboring events are retrieved alongside semantically similar ones.
  - Quick check question: How does the contiguity buffer's queue size affect the balance between temporal context and semantic relevance?

## Architecture Onboarding

- Component map: Initial tokens -> Local context -> Evicted tokens -> Surprise-based segmentation -> Graph-theoretic refinement -> Two-stage retrieval -> KV cache
- Critical path: 1. Token sequence processed with surprise-based segmentation 2. Initial boundaries refined using graph-theoretic optimization 3. Events stored in KV cache with representative tokens 4. During inference, similarity-based k-NN retrieves relevant events 5. Contiguity buffer adds temporally adjacent events 6. Combined context window passed to LLM for generation
- Design tradeoffs: Segmentation granularity vs. computational cost; surprise threshold sensitivity; contiguity buffer size; graph metric choice
- Failure signatures: Poor task performance (incorrect segmentation or retrieval parameters); excessive memory usage (too many events or inefficient KV cache); slow inference (large contiguity buffers or inefficient k-NN); inconsistent results (unstable parameters)
- First 3 experiments: 1. Verify surprise-based segmentation on short text with known event structure 2. Test refinement impact by comparing modularity/conductance metrics before and after 3. Validate two-stage retrieval by measuring performance difference between retrieval methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does EM-LLM's event segmentation accuracy compare to human-perceived event boundaries in continuous, naturalistic videos?
- Basis in paper: [inferred] The paper shows strong correlations between EM-LLM's event segmentation and human-perceived events in podcast transcripts, but does not evaluate performance on continuous video data.
- Why unresolved: The paper only analyzes segmentation quality using text-based datasets, leaving uncertainty about EM-LLM's effectiveness in processing non-textual, time-continuous inputs like video.
- What evidence would resolve it: Direct comparison of EM-LLM's segmentation with human annotations on continuous video data, using metrics like Wasserstein distance or event boundary overlap.

### Open Question 2
- Question: Can EM-LLM's hierarchical event structure be extended to capture nested temporal scales, as observed in human cognition?
- Basis in paper: [explicit] The paper mentions that current implementation lacks the sophisticated nested event representations observed in human cognition (Baldassano et al., 2017).
- Why unresolved: While the paper discusses the potential for hierarchical event structures, it does not provide empirical evidence of how such structures would improve performance or align with human memory processes.
- What evidence would resolve it: Experiments comparing performance with and without hierarchical event segmentation, along with analysis of how different temporal scales affect retrieval accuracy and human-like behavior.

### Open Question 3
- Question: What is the impact of different graph clustering metrics (e.g., modularity vs. conductance) on EM-LLM's performance across various task types?
- Basis in paper: [explicit] The paper uses both modularity and conductance for boundary refinement but does not provide a comprehensive comparison of their effects on different tasks.
- Why unresolved: The paper shows that refinement improves performance but does not isolate the impact of specific metrics or explain why certain metrics might be better suited for particular tasks.
- What evidence would resolve it: Systematic ablation studies comparing task performance using different clustering metrics, with analysis of which metrics are most effective for retrieval, QA, summarization, etc.

## Limitations

- Performance generalization uncertainty: The method's effectiveness for non-standard long-context applications like creative writing or code generation remains untested
- Biological plausibility assumptions: Correlation with human-perceived events doesn't establish mechanistic similarity to human episodic memory
- Hyperparameter sensitivity: Performance could vary significantly with different parameter choices due to multiple critical hyperparameters

## Confidence

- EM-LLM architecture effectiveness: High - well-supported by experimental results across multiple models and benchmarks
- Event segmentation quality: Medium - strong correlations with human-perceived events, but relies on similarity metrics that may not capture all aspects of meaningful boundaries
- Two-stage retrieval superiority: High - well-motivated and empirically validated with clear performance benefits

## Next Checks

1. **Cross-domain generalization test**: Evaluate EM-LLM on non-standard long-context tasks including creative writing continuation, code completion with interleaved documentation, and mathematical problem solving to assess whether the 40% retrieval and 29.7% QA improvements generalize beyond tested domains.

2. **Hyperparameter robustness analysis**: Conduct systematic ablation studies varying the surprise threshold Î³ (0.5-2.0), graph refinement metric (modularity vs. conductance), and contiguity buffer sizes to identify performance sensitivity and establish recommended parameter ranges for different task types.

3. **Computational overhead quantification**: Measure the actual memory and latency costs of EM-LLM compared to baseline methods across different context lengths (1M-10M tokens) to determine the practical trade-off between performance gains and computational resources, including KV cache size and inference time impacts.