---
ver: rpa2
title: 'HEARTS: A Holistic Framework for Explainable, Sustainable and Robust Text
  Stereotype Detection'
arxiv_id: '2409.11579'
source_url: https://arxiv.org/abs/2409.11579
tags:
- stereotype
- dataset
- bias
- text
- mgsd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HEARTS is a framework for explainable, sustainable, and robust
  text stereotype detection that improves upon existing stereotype classifiers by
  expanding the EMGSD dataset to include under-represented demographics and applying
  interpretable feature attribution methods (SHAP and LIME) with confidence scores.
  BERT models fine-tuned on EMGSD achieve over 80% accuracy while maintaining low
  carbon footprint.
---

# HEARTS: A Holistic Framework for Explainable, Sustainable and Robust Text Stereotype Detection

## Quick Facts
- arXiv ID: 2409.11579
- Source URL: https://arxiv.org/abs/2409.11579
- Authors: Theo King; Zekun Wu; Adriano Koshiyama; Emre Kazim; Philip Treleaven
- Reference count: 40
- Primary result: HEARTS framework improves stereotype detection with over 80% accuracy while maintaining low carbon footprint

## Executive Summary
HEARTS is a framework for explainable, sustainable, and robust text stereotype detection that expands the EMGSD dataset to include under-represented demographics like LGBTQ+ and regional stereotypes. The framework uses interpretable feature attribution methods (SHAP and LIME) with confidence scores and applies BERT variants fine-tuned on EMGSD to achieve over 80% accuracy while maintaining a low carbon footprint. When applied to evaluate 12 LLMs, HEARTS reveals that 37-58% of outputs contain stereotypes, with some reduction in bias observed over time within model families.

## Method Summary
The HEARTS framework expands the EMGSD dataset by combining the original MGSD with WinoQueer and SeeGULL datasets, filtering out counterfactuals and duplicates while augmenting with LLM-generated neutral and unrelated sentences. BERT models (ALBERT-V2, DistilBERT, BERT) are fine-tuned on this expanded dataset with specific hyperparameters. Token-level explanations are generated using SHAP and LIME, with confidence scores calculated by comparing their outputs through cosine similarity, Pearson correlation, and Jensen-Shannon divergence. The framework is applied to evaluate 12 LLMs using neutral prompts, measuring stereotype prevalence across six demographic categories.

## Key Results
- BERT models fine-tuned on EMGSD achieve over 80% accuracy while maintaining low carbon footprint
- 37-58% of LLM outputs contain stereotypes across 12 evaluated models
- ALBERT-V2 fine-tuning produces approximately 200x lower carbon emissions compared to BERT baseline
- Explanation confidence scores reveal variation in stereotype prevalence across different demographic groups

## Why This Works (Mechanism)

### Mechanism 1
Expanding EMGSD to include under-represented demographics (LGBTQ+, regional stereotypes) provides more diverse training examples, allowing the model to learn broader and more nuanced patterns of stereotypical language. The additional data in EMGSD is both relevant and high-quality for stereotype detection tasks. Break condition: If the additional data introduces noise or bias not present in the original dataset, performance could degrade.

### Mechanism 2
SHAP and LIME token-level explanations provide interpretable feature importance that aligns with human understanding of stereotypes. SHAP values rank tokens by their influence on model predictions, while LIME explanations are used to calculate confidence scores by comparing similarity between SHAP and LIME outputs. Core assumption: SHAP and LIME produce similar token importance rankings for the same text instance. Break condition: If SHAP and LIME produce divergent explanations, the confidence scores lose meaning.

### Mechanism 3
ALBERT-V2 architecture reduces carbon footprint by approximately 200x compared to BERT while maintaining comparable performance. ALBERT-V2 uses parameter sharing and smaller embeddings to reduce model size and computational requirements, lowering emissions during training. Core assumption: The performance trade-off from reduced parameters is acceptable for the stereotype detection task. Break condition: If the performance degradation from reduced parameters exceeds acceptable thresholds for the application.

## Foundational Learning

- Text preprocessing and tokenization for BERT models
  - Why needed here: BERT models require input sequences to be tokenized and converted to numerical IDs for processing
  - Quick check question: What is the maximum sequence length typically used for BERT models in text classification tasks?

- Explainability methods (SHAP, LIME) for feature importance
  - Why needed here: Understanding which tokens contribute most to stereotype predictions is crucial for interpretability and trust
  - Quick check question: How do SHAP and LIME differ in their approach to calculating feature importance?

- Dataset creation and augmentation techniques
  - Why needed here: The EMGSD dataset was created by combining existing datasets and augmenting with LLM-generated neutral/unrelated sentences
  - Quick check question: What are the potential risks of using LLM-generated data for training stereotype classifiers?

## Architecture Onboarding

- Component map: Dataset preparation (EMGSD creation) -> Model training (BERT/ALBERT fine-tuning) -> Explainability (SHAP/LIME integration) -> Evaluation (bias detection in LLM outputs)
- Critical path: Dataset → Model Training → Explainability → Evaluation
- Design tradeoffs: Accuracy vs. carbon footprint (ALBERT-V2 vs BERT), Dataset size vs. computational requirements, Explainability confidence vs. computational overhead
- Failure signatures: Poor model performance on specific demographics, Inconsistent SHAP/LIME explanations, High carbon emissions during training
- First 3 experiments: 1) Compare model performance on EMGSD vs. original MGSD, 2) Test SHAP/LIME similarity across different text lengths, 3) Measure carbon emissions during ALBERT-V2 fine-tuning vs BERT baseline

## Open Questions the Paper Calls Out

### Open Question 1
Does incorporating intersectional stereotype data improve the performance of stereotype classification models? The paper mentions that current datasets focus on single-axis stereotypes and suggests incorporating intersectional stereotypes for multi-label classifiers. This is unresolved as the authors state that training multi-label classifiers to identify intersectional stereotypes is beyond the scope of their current research.

### Open Question 2
How do different feature importance methods (beyond SHAP and LIME) affect the reliability of token-level explanations in stereotype detection? The authors note that their current explainability framework relies on a single pairwise comparison between SHAP and LIME, suggesting future research could incorporate additional methods like integrated gradients. This is unresolved as the paper only uses SHAP and LIME for token-level explanations.

### Open Question 3
Can sentiment and Regard classifiers be improved to better identify stereotypes against underrepresented demographics like gender and profession? The authors show that sentiment and Regard classifiers often misclassify stereotypical statements about gender and profession as positive, while correctly identifying stereotypes against LGBTQ+ groups. This is unresolved as the paper identifies the limitation but does not propose or test solutions for improving these classifiers.

## Limitations
- Framework's generalizability is constrained by focus on six demographic categories, potentially missing intersectional stereotypes
- Reliance on LLM-generated augmentation introduces potential biases from underlying language models
- Explainability confidence scores depend on assumption that SHAP and LIME produce similar explanations
- Carbon footprint analysis based on single estimation tool (CodeCarbon) may not capture full environmental impact

## Confidence
- High Confidence: Expansion of EMGSD to include under-represented demographics, general approach of using SHAP and LIME for feature attribution, BERT models achieving over 80% accuracy
- Medium Confidence: Specific performance improvements from dataset expansion, effectiveness of explanation confidence scores, carbon footprint reduction estimate of 200x for ALBERT-V2
- Low Confidence: Generalizability to detect intersectional stereotypes across all demographic combinations, stability of explanation confidence scores across different prompts, long-term effectiveness as new LLMs emerge

## Next Checks
1. Conduct systematic analysis of F1 scores across all demographic groups to identify potential blind spots in the EMGSD, particularly for intersectional stereotypes
2. Test SHAP and LIME alignment across diverse text lengths (short phrases vs. longer sentences) and different prompt templates to quantify stability of explanation confidence scores
3. Replicate carbon footprint measurements using alternative estimation tools (e.g., experiment-impact-tracker) and across different computing environments to validate the 200x reduction claim for ALBERT-V2