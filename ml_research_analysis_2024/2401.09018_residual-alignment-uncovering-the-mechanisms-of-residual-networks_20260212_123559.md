---
ver: rpa2
title: 'Residual Alignment: Uncovering the Mechanisms of Residual Networks'
arxiv_id: '2401.09018'
source_url: https://arxiv.org/abs/2401.09018
tags:
- residual
- type
- figure
- singular
- trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the mechanisms underlying ResNet''s success
  through an empirical study of Residual Jacobians. The key finding is a phenomenon
  called Residual Alignment (RA), characterized by four properties: intermediate representations
  form equispaced lines in high-dimensional space (RA1), top singular vectors of Residual
  Jacobians align across depths (RA2), Residual Jacobians are rank-limited for fully-connected
  ResNets (RA3), and top singular values scale inversely with depth (RA4).'
---

# Residual Alignment: Uncovering the Mechanisms of Residual Networks

## Quick Facts
- arXiv ID: 2401.09018
- Source URL: https://arxiv.org/abs/2401.09018
- Authors: Jianing Li; Vardan Papyan
- Reference count: 40
- Primary result: Residual Alignment (RA) phenomenon explains ResNet success through geometric alignment of intermediate representations

## Executive Summary
This paper investigates why ResNets generalize well through an empirical study of Residual Jacobians. The authors discover a phenomenon called Residual Alignment (RA) characterized by four key properties: intermediate representations form equispaced lines in high-dimensional space, top singular vectors of Residual Jacobians align across depths, Residual Jacobians are rank-limited for fully-connected ResNets, and top singular values scale inversely with depth. RA consistently occurs in models that generalize well but disappears when skip connections are removed. The authors also prove RA occurs in a novel mathematical model they propose, providing theoretical justification for this phenomenon and its connection to Neural Collapse.

## Method Summary
The study involves training ResNet34 models on MNIST, FashionMNIST, and CIFAR10 datasets using SGD optimizer with cosine learning rate scheduler, weight decay (1e-1 or 1e-2), batch size of 128, and 500 epochs. For each trained model, Residual Jacobians are computed for each residual block and analyzed through Singular Value Decomposition (SVD). The presence of Residual Alignment is verified by checking for equispaced intermediate representations, alignment of top singular vectors across depths, rank limitation of Residual Jacobians, and inverse scaling of top singular values with depth.

## Key Results
- RA consistently occurs across various architectures, datasets, and hyperparameters in models that generalize well
- The phenomenon disappears when skip connections are removed, confirming their essential role
- RA provides insight into ResNet's generalization ability and its connection to Neural Collapse
- The authors prove RA occurs in a novel mathematical model (Unconstrained Jacobians Model) they propose

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Residual Alignment emerges because singular vectors of Residual Jacobians align across depths and with the classifier Jacobian, creating a rigid geometric structure for intermediate representations.
- Mechanism: Skip connections enable gradient flow that aligns top singular vectors of residual branches. This alignment, combined with inverse scaling of top singular values with depth, causes intermediate representations to lie on equispaced lines in high-dimensional space.
- Core assumption: Skip connections are essential for this alignment to emerge during training.
- Evidence anchors:
  - "RA consistently occurs in models that generalize well... but ceases to occur once the skip connections are removed."
  - "The properties are interrelated and, in fact, (RA1) can be logically derived from the other properties."
- Break condition: Removing skip connections or using architectures that don't support gradient flow through residual branches.

### Mechanism 2
- Claim: Rank limitation of Residual Jacobians (at most rank C for fully-connected ResNets) creates a low-dimensional subspace where alignment occurs.
- Mechanism: When the number of classes is C, Residual Jacobians become rank-limited, constraining the representational space. This constraint forces top singular vectors to align within this lower-dimensional subspace.
- Core assumption: Number of classes determines maximum rank of Residual Jacobians, and this rank limitation is necessary for the alignment phenomenon.
- Evidence anchors:
  - "(RA3) Residual Jacobians are at most rank C for fully-connected ResNets, where C is the number of classes"
  - "If the number of classes in the dataset is increased, the singular vector alignment occurs in a higher dimensional subspace"
- Break condition: Using architectures or datasets where rank limitation doesn't naturally emerge.

### Mechanism 3
- Claim: The Unconstrained Jacobians Model provides theoretical justification for why RA emerges as a global optimum during training.
- Mechanism: By abstracting away complex residual branch architectures and allowing Jacobians to be optimized directly with regularization, the model proves that aligned singular vectors, rank-one Jacobians, and equal singular values emerge at global optimum.
- Core assumption: Regularization terms in the model capture essential constraints leading to RA.
- Evidence anchors:
  - "It also provably occurs in a novel mathematical model we propose."
  - "Theorem 3.3. For binary classification, there exists a global optimum of the Unconstrained Jacobians Model where the top Jacobian singular vectors are aligned"
- Break condition: Using loss functions or regularization schemes that don't align with model assumptions.

## Foundational Learning

- Concept: Singular Value Decomposition (SVD) and its properties
  - Why needed here: Understanding how SVD decomposes Residual Jacobians into singular vectors and values is crucial for analyzing alignment properties (RA2) and rank limitations (RA3).
  - Quick check question: If a matrix has SVD A = UΣV^T, what do the columns of U and V represent, and how do singular values in Σ relate to the matrix's rank?

- Concept: Residual Networks and skip connections
  - Why needed here: The entire phenomenon depends on ResNet architecture and how skip connections enable gradient flow and RA emergence.
  - Quick check question: In a ResNet block h_{i+1} = h_i + F(h_i; W_i), what role does the skip connection play in gradient flow during backpropagation?

- Concept: Neural Collapse phenomenon
  - Why needed here: The paper connects RA to Neural Collapse, suggesting they co-occur and may be related phenomena in deep learning.
  - Quick check question: What are the key characteristics of Neural Collapse, and how might it relate to the geometric structure imposed by RA on intermediate representations?

## Architecture Onboarding

- Component map: Residual Jacobians -> SVD analysis -> RA properties verification
- Critical path: 
  1. Compute Residual Jacobians for each residual block
  2. Perform SVD on each Jacobian
  3. Measure alignment of top singular vectors across depths
  4. Check rank limitations and singular value scaling
  5. Verify co-occurrence with Neural Collapse metrics
- Design tradeoffs:
  - Fully-connected vs. convolutional architectures affects dimensionality of Residual Jacobians
  - Number of classes determines maximum Jacobian rank (RA3)
  - Weight decay strength influences RA strength
  - Network depth affects singular value scaling (RA4)
- Failure signatures:
  - No alignment in singular vectors when visualized (U_j^T J_i V_j should show diagonal structure)
  - Singular values not scaling inversely with depth
  - Jacobian ranks exceeding class count in fully-connected networks
  - RA disappearing when skip connections are removed
- First 3 experiments:
  1. Train ResNet34 on MNIST and visualize intermediate representations to check for equispaced line patterns (RA1)
  2. Compute and visualize singular vector alignment matrices U_j^T J_i V_j for consecutive residual blocks
  3. Measure singular value scaling with depth and check if top singular values approximately equal 1/depth

## Open Questions the Paper Calls Out

- How does Residual Alignment manifest in transformer architectures, which also incorporate residual connections?
- Can recurrent architectures like RNNs, Neural ODEs, or deep equilibrium models be designed to exhibit Residual Alignment?
- Can model compression techniques replicate the performance of the original network by distilling aligning layers into a single layer and iteratively applying it?

## Limitations

- Theoretical understanding remains incomplete despite the Unconstrained Jacobians Model providing mathematical proof
- Connection between RA and generalization remains correlational rather than proven causal
- Analysis is more straightforward for fully-connected networks than convolutional architectures

## Confidence

- RA observation and empirical validation: **High**
- Connection to generalization: **Medium**
- Theoretical understanding (Unconstrained Model): **Medium**
- Convolutional architecture analysis: **Medium-Low**

## Next Checks

1. **Ablation on regularization strength**: Systematically vary weight decay and other regularization parameters to determine the precise conditions under which RA emerges and strengthens.

2. **Architectures beyond standard ResNets**: Test RA in architectures with different skip connection patterns (dense connections, inception modules) and non-standard residual branches to understand which architectural features are essential versus incidental to RA emergence.

3. **Transfer to regression and generative tasks**: Evaluate whether RA occurs in regression problems and generative models, which would test whether the phenomenon is specific to classification or represents a more general property of deep networks with residual connections.