---
ver: rpa2
title: Towards Sim-to-Real Industrial Parts Classification with Synthetic Dataset
arxiv_id: '2404.08778'
source_url: https://arxiv.org/abs/2404.08778
tags:
- parts
- dataset
- synthetic
- industrial
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a synthetic dataset, SIP-17, designed to
  serve as a testbed for Sim-to-Real industrial parts classification research. The
  dataset consists of 17 objects representing six industrial use cases, including
  isolated and assembled parts.
---

# Towards Sim-to-Real Industrial Parts Classification with Synthetic Dataset

## Quick Facts
- arXiv ID: 2404.08778
- Source URL: https://arxiv.org/abs/2404.08778
- Reference count: 40
- Models trained on synthetic data with domain randomization outperform those without when tested on real images

## Executive Summary
This paper introduces the SIP-17 dataset, a synthetic benchmark for Sim-to-Real industrial parts classification research. The dataset contains 17 industrial objects across six use cases, with two versions: one with domain randomization (Syn R) and one without (Syn O). Five state-of-the-art classification models were trained exclusively on synthetic data and evaluated on real images. Results demonstrate that domain randomization significantly improves classification performance, with ConvNext achieving the best overall results. The study highlights challenges in classifying subcategories with similar appearance and suggests future work to address these limitations through albedo randomization and dataset expansion.

## Method Summary
The authors created a synthetic dataset of 17 industrial objects representing six use cases, rendered with and without domain randomization. They trained five classification models (ResNet152, EfficientNetB7, ConvNext, Vision Transformer, and DINO) on the synthetic data only, using ImageNet pre-trained weights. Models were evaluated on a small set of real test images. The experimental design compared performance across different architectures and synthetic data variants to assess the impact of domain randomization on Sim-to-Real generalization.

## Key Results
- Models trained on Syn R (with domain randomization) significantly outperformed those trained on Syn O (without randomization)
- ConvNext achieved the best overall performance, followed by DINO and Vision Transformer
- Classification accuracy varied substantially across use cases, with some subcategories showing particularly poor performance
- Models struggled most with objects sharing similar albedo or partially identical components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain randomization in synthetic training data improves generalization to real-world images in industrial parts classification
- Mechanism: By introducing varied backgrounds, lighting conditions, and post-processing effects in synthetic data, the model learns to ignore irrelevant visual variations and focus on object-specific features
- Core assumption: The real-world images are sufficiently similar to the distribution of synthetic images with domain randomization applied
- Evidence anchors: Abstract shows Syn R outperformed Syn O; section 4.2 demonstrates this across multiple models and use cases
- Break condition: If real-world images have characteristics not represented in the synthetic data with randomization

### Mechanism 2
- Claim: The split-attention mechanism in ConvNext improves robustness to cross-domain classification tasks
- Mechanism: Split-attention allows the network to learn more discriminative features by aggregating information from different spatial regions
- Core assumption: The attention mechanism effectively captures relevant features for classification across domains
- Evidence anchors: ConvNext achieved best overall performance; section 4.3 notes smallest performance difference between Syn O and Syn R training
- Break condition: If the attention mechanism overfits to synthetic data features not present in real images

### Mechanism 3
- Claim: Self-supervised contrastive learning (DINO) learns transferable features for cross-domain classification
- Mechanism: By learning to predict similarities between pairs of images, the model develops feature representations less dependent on domain-specific characteristics
- Core assumption: The contrastive learning strategy captures semantic features relevant across both synthetic and real domains
- Evidence anchors: DINO was second-best overall performer; abstract highlights its strong cross-domain performance
- Break condition: If the contrastive learning does not capture features that generalize across the synthetic-real domain gap

## Foundational Learning

- Concept: Domain randomization
  - Why needed here: To bridge the gap between synthetic and real images by exposing the model to varied visual conditions during training
  - Quick check question: How does domain randomization help in reducing the domain shift between synthetic and real images?

- Concept: Convolutional Neural Networks (CNNs) and Vision Transformers (ViT)
  - Why needed here: Understanding architectural differences helps explain why ConvNext and ViT perform differently in cross-domain tasks
  - Quick check question: What are the key differences between CNNs and ViT in terms of how they process image data?

- Concept: Self-supervised learning
  - Why needed here: DINO's performance indicates the potential of self-supervised methods in learning transferable features without labeled data
  - Quick check question: How does self-supervised learning differ from supervised learning, and why might it be advantageous in this context?

## Architecture Onboarding

- Component map: SIP-17 dataset (Syn R/Syn O) -> 5 classification models (ResNet152, EfficientNetB7, ConvNext, ViT, DINO) -> Real image classification evaluation
- Critical path: Generate synthetic data → Train models on synthetic data → Evaluate on real images → Analyze performance differences
- Design tradeoffs: Domain randomization adds complexity but improves generalization; advanced models improve performance but increase computational cost
- Failure signatures: Low accuracy on real images indicates insufficient domain randomization or model limitations; high confusion between similar objects suggests need for better feature learning
- First 3 experiments:
  1. Train baseline ResNet on Syn O and evaluate on real images to establish performance floor
  2. Train same model on Syn R and compare results to quantify domain randomization impact
  3. Test different models (ConvNext, DINO) on Syn R to identify best-performing architecture

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does randomizing the albedo of synthetic objects affect Sim-to-Real classification performance?
- Basis in paper: Authors suggest randomizing the albedo on synthetic images as future work to address challenges in classifying objects with similar albedo
- Why unresolved: Current dataset uses single-color textures for CAD models
- What evidence would resolve it: Experiments comparing classification performance with and without albedo randomization in synthetic data

### Open Question 2
- Question: How does increasing the size of the SIP-17 dataset impact Sim-to-Real classification performance?
- Basis in paper: Authors intend to increase dataset size by including more isolated and assembled parts with varying degrees of similarity
- Why unresolved: Current dataset contains only 17 objects from six industrial use cases
- What evidence would resolve it: Evaluating performance of models trained on expanded dataset with more objects and use cases

### Open Question 3
- Question: How effective are self-supervised contrastive learning models, such as DINO, in addressing challenges of Sim-to-Real industrial parts classification?
- Basis in paper: DINO was second-best overall performer, suggesting potential effectiveness in cross-domain classification tasks
- Why unresolved: Performance varied across different categories and use cases
- What evidence would resolve it: Further experiments with DINO and other self-supervised models, fine-tuning architectures and training strategies

## Limitations
- Dataset contains only 17 objects across 6 industrial use cases, limiting generalizability to broader industrial applications
- Exact rendering parameters, domain randomization ranges, and model hyperparameters are not specified, making precise reproduction difficult
- Paper does not quantify the exact nature or magnitude of the synthetic-to-real domain gap

## Confidence

- High Confidence: Domain randomization improves classification performance compared to non-randomized synthetic data, supported by direct experimental comparisons
- Medium Confidence: Ranking of model architectures reflects specific experimental conditions but may not generalize to all industrial parts classification scenarios
- Low Confidence: Attribution of ConvNext's superior performance specifically to its split-attention mechanism is speculative without ablation studies

## Next Checks

1. Conduct ablation study on domain randomization parameters to identify which factors most contribute to improved real-world performance
2. Evaluate trained models on a separate industrial parts dataset not seen during synthetic data generation to assess true generalization capabilities
3. Establish human classification accuracy on real test images to contextualize model performance and identify whether remaining errors are due to fundamental domain gaps or model limitations