---
ver: rpa2
title: Annotation-Efficient Language Model Alignment via Diverse and Representative
  Response Texts
arxiv_id: '2405.13541'
source_url: https://arxiv.org/abs/2405.13541
tags:
- aepo
- preference
- dataset
- responses
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of efficiently aligning large
  language models with human preferences when annotation budgets are limited. It proposes
  Annotation-Efficient Preference Optimization (AEPO), which selects a small, diverse,
  and representative subset of responses to annotate instead of exhaustively labeling
  all responses.
---

# Annotation-Efficient Language Model Alignment via Diverse and Representative Response Texts

## Quick Facts
- arXiv ID: 2405.13541
- Source URL: https://arxiv.org/abs/2405.13541
- Authors: Yuu Jinnai; Ukyo Honda
- Reference count: 40
- Primary result: AEPO achieves better alignment with human preferences using fewer annotations than random sampling or West-of-N baselines

## Executive Summary
This paper addresses the challenge of efficiently aligning large language models with human preferences when annotation budgets are limited. The proposed Annotation-Efficient Preference Optimization (AEPO) method selects a small, diverse, and representative subset of responses to annotate instead of exhaustively labeling all responses. By balancing diversity and representativeness in the selected responses, AEPO maximizes information gain from limited annotations while maintaining full coverage of the instruction set. Experiments on three datasets demonstrate that AEPO consistently outperforms baseline methods under the same annotation budget, achieving better alignment with human preferences and improved scalability as the number of available responses increases.

## Method Summary
AEPO generates N candidate responses per instruction using nucleus sampling, then selects k responses using an objective that maximizes both representativeness (similarity to other responses) and diversity (dissimilarity within the selected set). The method uses sentence embeddings (all-mpnet-base-v2) to quantify these properties and combines them with hyperparameter λ. The selected subset is then annotated using West-of-N strategy, and the resulting pairwise comparisons train a DPO model with LoRA. This approach focuses annotation effort on maximally informative pairs while reducing annotation cost from N to k per instruction.

## Key Results
- AEPO consistently outperforms random sampling and West-of-N baselines under the same annotation budget
- Performance improves as the number of candidate responses N increases, demonstrating scalability
- AEPO works effectively with both human and AI feedback
- The method achieves better alignment with human preferences while using fewer annotations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selecting responses that are both diverse and representative improves preference annotation efficiency
- Mechanism: AEPO uses a combined objective (diversity + representativeness) to subsample responses, ensuring each selected pair is informative for estimating reward values across the full candidate set
- Core assumption: Reward values of similar responses are correlated; thus, knowing preferences for diverse, representative pairs helps estimate preferences for all candidates
- Break Condition: If the reward model's similarity-to-reward correlation is weak, or if responses are too dissimilar to be informative via similarity heuristics

### Mechanism 2
- Claim: Annotating fewer but strategically chosen responses yields comparable or better alignment than annotating many random ones
- Mechanism: By reducing annotation budget from N to k per instruction, AEPO focuses annotation effort on maximally informative pairs while still covering the instruction set fully
- Core assumption: The chosen subset Y_ann is sufficient to infer the preference ranking of the entire Y_cand set
- Break Condition: If the representativeness/diversity heuristics do not correlate with actual preference quality, or if k is too small to capture the variation in responses

### Mechanism 3
- Claim: AEPO scales with the number of available responses under a fixed annotation budget, outperforming baselines that degrade when N grows
- Mechanism: As N increases, AEPO can find more diverse and representative pairs without increasing annotation cost, improving dataset quality
- Core assumption: Larger candidate pools enable better subset selection for representativeness and diversity
- Break Condition: If computational cost of selecting the subset becomes prohibitive, or if embedding similarity fails to capture response quality differences at scale

## Foundational Learning

- Concept: Pairwise preference learning
  - Why needed here: AEPO is built on DPO, which learns from pairwise comparisons (chosen vs rejected responses)
  - Quick check question: What is the input format for DPO training? (Answer: (x, yc, yr) triples.)

- Concept: Embedding-based similarity
  - Why needed here: AEPO uses sentence embeddings (cosine distance) to quantify diversity and representativeness of responses
  - Quick check question: What embedding model does AEPO use? (Answer: all-mpnet-base-v2.)

- Concept: Active learning / subset selection
  - Why needed here: AEPO is a form of active learning that selects a subset of responses to annotate, rather than all candidates
  - Quick check question: What are the two key heuristics AEPO uses for subset selection? (Answer: representativeness and diversity.)

## Architecture Onboarding

- Component map: Generate candidates -> Select subset (frep + fdiv) -> Annotate -> Train DPO
- Critical path: Generate candidates → Select subset (frep + fdiv) → Annotate → Train DPO
- Design tradeoffs:
  - k vs. annotation cost: Larger k increases annotation cost but may improve preference estimation
  - λ tuning: Balances diversity vs. representativeness; wrong λ can hurt performance
  - Embedding quality: Poor embeddings lead to bad subset selection
- Failure signatures:
  - Performance plateaus or degrades as N increases (subset selection failing)
  - High variance in results across runs (unstable selection)
  - Low correlation between selected pairs and ground truth preferences
- First 3 experiments:
  1. Run AEPO with k=2, λ=0 on a small dataset; verify it selects similar responses (low diversity)
  2. Run AEPO with k=2, λ=1.0; compare selected pairs' diversity to random sampling
  3. Train DPO using AEPO-selected pairs vs. random pairs; measure win rate improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of embedding model affect the performance of AEPO, and what is the impact of using a better sentence embedding model?
- Basis in paper: [inferred] The paper uses all-mpnet-base-v2 as the embedding model and mentions that the performance of AEPO will benefit from future improvements in embedding models, but does not evaluate different embedding models
- Why unresolved: The paper does not experimentally compare the performance of AEPO using different sentence embedding models. It only uses one model and makes a general statement about potential improvements
- What evidence would resolve it: Experiments comparing AEPO performance using various sentence embedding models (e.g., different BERT variants, sentence transformers, or domain-specific embeddings) while keeping all other variables constant

### Open Question 2
- Question: What is the optimal hyperparameter λ for AEPO, and can we develop a strategy to automatically find an effective λ for a given dataset?
- Basis in paper: [explicit] The paper observes that λ=1.0 works well across experiments but states that "developing a strategy to find an effective λ for a given dataset is future work"
- Why unresolved: The paper uses a fixed λ value without exploring methods to determine the optimal λ dynamically or automatically
- What evidence would resolve it: Development and evaluation of methods to automatically select or adapt λ based on dataset characteristics, instruction types, or model behavior

### Open Question 3
- Question: How does AEPO perform when combined with active learning algorithms, and what is the benefit of this combination?
- Basis in paper: [inferred] The paper mentions that AEPO is complementary to active learning methods and that combining AEPO with active learning algorithms is future work, but does not evaluate this combination
- Why unresolved: The paper focuses on AEPO as a standalone method and does not explore its integration with active learning techniques that could further improve annotation efficiency
- What evidence would resolve it: Experiments comparing AEPO alone versus AEPO combined with various active learning strategies, measuring the improvement in preference dataset quality and alignment performance

## Limitations
- Computational overhead of selection process with quadratic complexity as N grows
- Performance with very small annotation budgets (k=1) remains unclear
- Method's robustness across different embedding models and reward functions not thoroughly explored
- Theoretical justification for heuristics relies on intuitive arguments rather than formal guarantees

## Confidence

**High Confidence:** The empirical demonstration that AEPO outperforms random sampling and West-of-N baselines under the same annotation budget is well-supported by experimental results across three datasets.

**Medium Confidence:** The claim that AEPO scales with larger candidate pools is supported by experiments showing improved performance as N increases, but relies on assumptions about embedding quality and response diversity that may not generalize.

**Low Confidence:** The theoretical justification for why representativeness and diversity heuristics lead to better preference estimation is primarily intuitive rather than rigorously proven.

## Next Checks

1. **Diversity-Quality Correlation Test:** Systematically measure the correlation between AEPO-selected pairs' diversity scores and their actual preference quality on a held-out validation set.

2. **Scaling Efficiency Analysis:** Profile the computational cost of AEPO selection as N increases beyond the tested range (e.g., N=256, 512, 1024) and measure whether performance gains justify additional computational overhead.

3. **Embedding Robustness Evaluation:** Replace all-mpnet-base-v2 embeddings with alternative models (e.g., OpenAI embeddings, different sentence transformers) and measure sensitivity of AEPO performance to embedding quality.