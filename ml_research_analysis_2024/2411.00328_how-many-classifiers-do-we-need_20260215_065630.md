---
ver: rpa2
title: How many classifiers do we need?
arxiv_id: '2411.00328'
source_url: https://arxiv.org/abs/2411.00328
tags:
- classifiers
- theorem
- polarization
- error
- ensemble
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies how the performance of majority vote ensembles\
  \ depends on the number of classifiers, using concepts of polarization and restricted\
  \ entropy. The authors introduce the notion of polarization\u2014a measure of how\
  \ concentrated the error distribution is\u2014and conjecture that most interpolating\
  \ neural networks have polarization less than 4/3."
---

# How many classifiers do we need?

## Quick Facts
- arXiv ID: 2411.00328
- Source URL: https://arxiv.org/abs/2411.00328
- Reference count: 40
- This paper studies how the performance of majority vote ensembles depends on the number of classifiers, using concepts of polarization and restricted entropy.

## Executive Summary
This paper investigates how majority vote ensemble performance scales with the number of classifiers, introducing the concept of polarization and proving results about its asymptotic behavior. The authors propose a "neural polarization law" conjecturing that most interpolating neural networks have polarization less than 4/3, enabling tighter bounds on majority vote error. They also show that under restricted entropy conditions, disagreement between classifiers follows predictable asymptotic behavior, allowing performance prediction from small to large ensembles.

## Method Summary
The method involves training multiple independent neural network classifiers with different initializations and hyperparameters, then analyzing the ensemble's polarization, disagreement, and entropy characteristics. The authors derive theoretical bounds for majority vote error that depend on average error rate, disagreement, and polarization. They propose a neural polarization law (polarization ≤ 4/3 for interpolating networks) and show that disagreement converges to a predictable asymptotic behavior under entropy-restricted conditions. The approach enables predicting ensemble performance for larger numbers of classifiers from observations on smaller ensembles.

## Key Results
- Most interpolating neural networks have polarization less than 4/3 (neural polarization law conjecture)
- Disagreement between classifiers converges asymptotically to a sum of hyperbolic and random walk terms
- Restricted entropy conditions yield tighter, more scalable bounds that don't explicitly depend on the number of classes
- Theoretical bounds on majority vote error are tighter than previous results and can be accurately predicted from small samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Polarization of an ensemble is bounded by 4/3 for most interpolating neural networks, enabling tighter majority vote error bounds.
- Mechanism: Polarization measures how concentrated the error distribution is. If polarization is low, the majority vote error is closer to the average error rate minus half the disagreement.
- Core assumption: Most interpolating neural networks have polarization ≤ 4/3, and this value is nearly constant across different architectures and hyperparameters.
- Evidence anchors:
  - [abstract] "we propose what we call a neural polarization law: most interpolating neural network models are 4/3-polarized"
  - [section 3] "Inspired from the theorem, we propose Conjecture 1 which we call a neural polarization law: most interpolating (Definition 2) neural network models are 4/3-polarized"
  - [corpus] Weak evidence - no directly comparable papers in corpus, but related to ensemble performance prediction
- Break condition: If the neural networks are not interpolating (e.g., underfitting or poor performance), polarization can exceed 4/3, making the bound less tight.

### Mechanism 2
- Claim: Under entropy-restricted conditions, disagreement behaves asymptotically as a sum of a hyperbolic term and a random walk, allowing prediction of ensemble performance from small to large ensembles.
- Mechanism: The disagreement between classifiers converges to a limit as the number of classifiers increases, following a predictable pattern that can be extrapolated from a small sample.
- Core assumption: The entropy of the ensemble is restricted such that prediction probabilities concentrate on a small number of labels.
- Evidence anchors:
  - [abstract] "we prove results for the asymptotic behavior of the disagreement in terms of the number of classifiers, which we show can help in predicting the performance for a larger number of classifiers from that of a smaller number"
  - [section 5] "Theorem 5 suggests that the disagreement within N classifiers, Eρ2 N [D(h, h′)], can be approximated as N −1 N D∞"
  - [corpus] No direct evidence in corpus for this specific asymptotic behavior claim
- Break condition: If the entropy restriction is violated (e.g., classifiers spread predictions uniformly), the asymptotic behavior may not hold.

### Mechanism 3
- Claim: Restricted entropy ensembles provide tighter bounds on majority vote error that don't depend explicitly on the number of classes.
- Mechanism: By limiting the entropy of classifier predictions, the bound on majority vote error becomes more scalable and less dependent on the number of classes.
- Core assumption: The ensemble satisfies a condition where the entropy of wrong predictions is small and concentrated on a few labels.
- Evidence anchors:
  - [abstract] "The error of the majority vote classifier is considered under restricted entropy conditions, and we present a tight upper bound that indicates that the disagreement is linearly correlated with the target"
  - [section 4] "The theorem provides a tighter scalable bound that does not have explicit dependency on the total number of labels, with a small cost in terms of the entropy of constituent classifiers"
  - [corpus] No direct evidence in corpus for this specific entropy restriction mechanism
- Break condition: If the entropy restriction is not met, the bound may become looser and less useful.

## Foundational Learning

- Concept: Polarization and its role in ensemble performance
  - Why needed here: Polarization is a key metric that determines how well majority vote can improve over individual classifiers
  - Quick check question: If an ensemble has polarization η = 0, what does this tell you about the majority vote error compared to the average error rate?

- Concept: U-statistics and their asymptotic behavior
  - Why needed here: The asymptotic behavior of disagreement is expressed using U-statistics, which allows prediction of ensemble performance
  - Quick check question: What is the limiting value of disagreement as the number of classifiers approaches infinity, according to Theorem 5?

- Concept: Entropy restrictions and their impact on ensemble bounds
  - Why needed here: Restricted entropy conditions allow for tighter bounds that are more scalable and less dependent on the number of classes
  - Quick check question: How does the entropy restriction condition (11) relate to the concentration of wrong predictions on a small number of labels?

## Architecture Onboarding

- Component map:
  Classifier ensemble -> Polarization calculation -> Disagreement measurement -> Entropy restriction -> Majority vote error estimation

- Critical path:
  1. Train multiple classifiers on the same dataset
  2. Calculate polarization and disagreement from a small sample
  3. Apply entropy restriction if possible
  4. Use Theorem 5 to predict disagreement for larger ensembles
  5. Estimate majority vote error using the derived bounds

- Design tradeoffs:
  - Number of classifiers vs. computational cost
  - Tightness of bounds vs. entropy restriction requirements
  - Accuracy of polarization estimation vs. sample size

- Failure signatures:
  - Polarization significantly exceeding 4/3 (non-interpolating models)
  - Disagreement not following predicted asymptotic behavior
  - Entropy restriction condition not satisfied

- First 3 experiments:
  1. Train 3 ResNet18 classifiers on CIFAR-10 with different initializations, calculate polarization and disagreement
  2. Verify that polarization is close to 4/3 and disagreement follows Theorem 5's prediction
  3. Use the bounds to predict majority vote error for 10 classifiers and compare with actual performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the neural polarization law (Conjecture 1) universally true for all interpolating neural networks?
- Basis in paper: [explicit] The authors conjecture that most interpolating neural networks are 4/3-polarized and provide empirical evidence supporting this claim.
- Why unresolved: While empirical results on CIFAR-10 and other datasets support the conjecture, a formal proof is lacking. The conjecture is based on observed trends rather than theoretical guarantees.
- What evidence would resolve it: A formal proof or counterexample demonstrating the universal validity of the neural polarization law for all interpolating neural networks.

### Open Question 2
- Question: How can we accurately estimate the smallest possible value of ε in Theorem 4 for practical applications?
- Basis in paper: [explicit] The authors mention that ε = (K-2)/(2(K-1)) always satisfies the condition in Theorem 4, but note that this is not a good choice for their goal.
- Why unresolved: The paper does not provide a method for estimating the smallest possible value of ε in practice, which is crucial for applying the tighter bounds in Theorem 4.
- What evidence would resolve it: Development of a practical method or algorithm to estimate the smallest possible value of ε for various ensemble configurations and datasets.

### Open Question 3
- Question: Can the concept of polarization be extended to other ensembling methods beyond majority vote?
- Basis in paper: [inferred] The authors introduce polarization as a measure of how concentrated the error distribution is in majority vote ensembles. While they don't explicitly discuss other ensembling methods, the concept seems generalizable.
- Why unresolved: The paper focuses exclusively on majority vote strategies and does not explore the potential application of polarization to other ensembling methods.
- What evidence would resolve it: Theoretical and empirical studies applying the concept of polarization to other ensembling methods such as weighted voting, bagging, or boosting.

### Open Question 4
- Question: How does the asymptotic behavior of disagreement (Theorem 5) change for non-interpolating models or under different training regimes?
- Basis in paper: [explicit] Theorem 5 describes the asymptotic behavior of disagreement for ensembles of classifiers, but the proof and discussion focus on interpolating models.
- Why unresolved: The theorem is stated and proved for interpolating models, and the authors don't discuss how it might change for non-interpolating models or under different training regimes.
- What evidence would resolve it: Empirical studies and theoretical analysis of the asymptotic behavior of disagreement for non-interpolating models and under various training regimes (e.g., early stopping, regularization).

### Open Question 5
- Question: Is the probability Pρ(Wρ > 1/2) a more informative metric than majority vote error rate for evaluating ensemble performance in practical applications?
- Basis in paper: [explicit] The authors suggest in their discussion that Pρ(Wρ > 1/2) might be a more useful metric, especially in the context of large language models.
- Why unresolved: While the authors propose this as a potential alternative metric, they don't provide empirical evidence or theoretical justification for its superiority over majority vote error rate.
- What evidence would resolve it: Comparative studies evaluating the usefulness of Pρ(Wρ > 1/2) versus majority vote error rate in practical applications, including both traditional classification tasks and emerging areas like large language models.

## Limitations
- The neural polarization law conjecture lacks extensive empirical validation across diverse model architectures and datasets.
- The entropy restriction mechanism requires strict conditions that may not hold for many practical ensemble configurations.
- The asymptotic behavior of disagreement relies on assumptions about classifier independence and diversity that may not always be satisfied.

## Confidence
- **High Confidence**: The theoretical framework connecting polarization to majority vote bounds is mathematically rigorous.
- **Medium Confidence**: The empirical evidence supporting the neural polarization law is suggestive but limited to specific architectures and datasets.
- **Low Confidence**: The practical applicability of restricted entropy bounds depends heavily on whether real-world ensembles satisfy the entropy restriction conditions.

## Next Checks
1. **Cross-architecture validation**: Test the neural polarization law conjecture on diverse architectures (Transformers, LSTMs, different CNN variants) across multiple datasets to verify polarization consistently stays near 4/3 for interpolating models.

2. **Entropy restriction verification**: Systematically evaluate how often real ensembles satisfy the entropy restriction condition (11) and quantify the degradation in bound tightness when this condition is violated.

3. **Asymptotic behavior confirmation**: For ensembles of varying sizes (3, 10, 30, 100 classifiers), verify that disagreement follows the predicted hyperbolic + random walk pattern by testing if √t/σ1 ZN(t) converges to a Wiener process as specified in the proof of Theorem 5.