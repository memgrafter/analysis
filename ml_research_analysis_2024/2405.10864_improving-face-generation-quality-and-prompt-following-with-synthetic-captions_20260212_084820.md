---
ver: rpa2
title: Improving face generation quality and prompt following with synthetic captions
arxiv_id: '2405.10864'
source_url: https://arxiv.org/abs/2405.10864
tags:
- images
- face
- captions
- image
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving text-to-image diffusion
  models for generating photorealistic human faces that closely follow detailed text
  prompts. The authors propose a training-free pipeline that uses pre-trained face
  analysis models to extract detailed appearance attributes from face images, which
  are then converted into natural language captions using a large language model.
---

# Improving face generation quality and prompt following with synthetic captions

## Quick Facts
- arXiv ID: 2405.10864
- Source URL: https://arxiv.org/abs/2405.10864
- Authors: Michail Tarasiou; Stylianos Moschoglou; Jiankang Deng; Stefanos Zafeiriou
- Reference count: 22
- One-line primary result: A training-free pipeline using synthetic captions significantly improves photorealistic face generation and prompt adherence compared to baseline Stable Diffusion 2.1

## Executive Summary
This paper addresses the challenge of improving text-to-image diffusion models for generating photorealistic human faces that closely follow detailed text prompts. The authors propose a training-free pipeline that uses pre-trained face analysis models to extract detailed appearance attributes from face images, which are then converted into natural language captions using a large language model. These synthetic captions, generated for approximately 250,000 face images from public datasets, are used to fine-tune a text-to-image diffusion model. The resulting model significantly improves the generation of high-quality, realistic human faces and enhances prompt adherence compared to the baseline Stable Diffusion 2.1 model.

## Method Summary
The authors developed a pipeline that first detects and crops faces from images using a ResNet50 backbone model. They then apply a suite of specialized models to extract comprehensive attributes including facial features, emotions, age, gender, and ethnicity. These attributes are converted into natural language captions using Vicuna 13B, a large language model. The synthetic captions are used to fine-tune Stable Diffusion 2.1 using LoRA adapters. The approach is applied to three publicly available datasets (EasyPortrait, FFHQ, and LAION-Face) to create approximately 250,000 captions for fine-tuning.

## Key Results
- The fine-tuned model generates significantly more realistic human faces compared to baseline Stable Diffusion 2.1, eliminating cartoonish effects
- The model demonstrates improved prompt adherence with better generation of specific attributes like facial features, emotions, and demographic characteristics
- The approach enables consistent identity generation across varying attributes including age, gender, ethnicity, and emotion
- The synthetic captions and fine-tuned model are made publicly available for research use

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using pre-trained face analysis models to extract detailed appearance attributes from images significantly improves the quality and prompt adherence of generated human faces.
- Mechanism: The pipeline uses specialized models to extract comprehensive attributes like facial features, emotions, age, gender, and ethnicity, providing richer training signals than typical internet-sourced captions.
- Core assumption: Pre-trained face analysis models can reliably extract accurate and comprehensive attributes from face images.
- Evidence anchors: [abstract] "We apply this method to create approximately 250,000 captions for publicly available face datasets." [section] "We apply our caption generation pipeline to three publicly available datasets: EasyPortrait [7], FFHQ [8] and LAION-Face [21]."
- Break condition: If face analysis models fail to accurately extract attributes or the extracted attributes are not effectively translated into natural language captions, quality and prompt adherence will not improve.

### Mechanism 2
- Claim: Fine-tuning a text-to-image diffusion model with synthetic captions generated from face images improves its ability to generate realistic human faces and follow detailed text prompts.
- Mechanism: Synthetic captions provide more detailed and specific training signals, allowing the model to learn to generate more realistic human faces and better adhere to prompt details.
- Core assumption: Fine-tuning with more detailed and specific captions will improve the model's ability to generate realistic human faces and follow text prompts.
- Evidence anchors: [abstract] "Our results demonstrate that this approach significantly improves the model's ability to generate high-quality, realistic human faces and enhances adherence to the given prompts, compared to the baseline model." [section] "the finetuned model clearly outputs realistic face images in comparison with the baseline which often includes a cartoonish effect."
- Break condition: If fine-tuning is not effective or synthetic captions don't provide sufficiently detailed training signals, the model's ability to generate realistic human faces and follow prompts will not improve.

### Mechanism 3
- Claim: Using a large language model to convert extracted face attributes into natural language captions improves the quality and coherence of the captions compared to using attributes directly.
- Mechanism: The LLM converts bag-of-words descriptions into coherent, natural language sentences, making captions more readable and easier for the diffusion model to understand and learn from.
- Core assumption: A large language model can effectively convert a bag-of-words description of face attributes into a coherent, natural language sentence.
- Evidence anchors: [abstract] "These attributes are then converted into natural language captions using a large language model." [section] "we simply use an instruct-based pre trained LLM, which we condition to output a description based on the provided bag of words."
- Break condition: If the LLM fails to produce coherent, natural language captions from the bag-of-words descriptions, the quality and effectiveness of the captions will be reduced.

## Foundational Learning

- Concept: Face Analysis Models
  - Why needed here: To extract detailed attributes from face images that can be used to generate synthetic captions.
  - Quick check question: What are some examples of attributes that can be extracted from face images using pre-trained face analysis models?

- Concept: Text-to-Image Diffusion Models
  - Why needed here: To generate realistic human faces based on text prompts.
  - Quick check question: How do text-to-image diffusion models work, and what are some of the challenges in generating realistic human faces?

- Concept: Large Language Models
  - Why needed here: To convert the extracted face attributes into natural language captions.
  - Quick check question: What are some examples of large language models, and how can they be used to generate text based on input data?

## Architecture Onboarding

- Component map: Face detection model (ResNet50) -> Face analysis models (attributes, emotions, parsing, age/gender/ethnicity) -> Large language model (Vicuna 13B) -> Text-to-image diffusion model (Stable Diffusion 2.1 with LoRA) -> Datasets (EasyPortrait, FFHQ, LAION-Face)

- Critical path:
  1. Face detection and cropping
  2. Attribute extraction using face analysis models
  3. Conversion of attributes to natural language captions using LLM
  4. Fine-tuning of text-to-image diffusion model with synthetic captions
  5. Generation of realistic human faces based on text prompts

- Design tradeoffs:
  - Using pre-trained models vs. training custom models for attribute extraction
  - Using a large language model vs. a simpler approach for caption generation
  - Fine-tuning the entire text-to-image diffusion model vs. using LoRA for efficiency

- Failure signatures:
  - Poor face detection or attribute extraction leading to inaccurate or incomplete captions
  - LLM producing incoherent or irrelevant captions
  - Fine-tuning process not improving the quality or prompt adherence of generated faces
  - Generated faces still appearing cartoonish or not following text prompts closely

- First 3 experiments:
  1. Generate synthetic captions for a small subset of face images and manually inspect their quality and coherence.
  2. Fine-tune the text-to-image diffusion model on the synthetic captions and compare the quality of generated faces to the baseline model.
  3. Vary the prompt details and assess the model's ability to generate faces that closely match the provided descriptions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the synthetic captions perform compared to human-written captions for training text-to-image models?
- Basis in paper: [explicit] The authors state that their synthetic captions are "far more detailed than the information typically contained in large-scale web captions" but do not directly compare performance to human-written captions.
- Why unresolved: The paper focuses on comparing the finetuned model to the baseline Stable Diffusion 2.1, but does not include a comparison to models trained with human-written captions.
- What evidence would resolve it: Conducting an experiment where the synthetic captions are compared to human-written captions for training text-to-image models, and measuring the resulting model performance on various metrics.

### Open Question 2
- Question: How do the biases present in the pre-trained face analysis models impact the generated images and captions?
- Basis in paper: [explicit] The authors acknowledge that "biases may manifest in skewed representations across different ethnicities, emotions, age and genders due to the nature of the training data originally used for either the facial analysis models or Stable Diffusion 2.1."
- Why unresolved: While the authors mention the potential for biases, they do not provide a detailed analysis of how these biases impact the generated images and captions.
- What evidence would resolve it: Conducting a thorough analysis of the generated images and captions to identify and quantify the biases present, and exploring methods to mitigate these biases.

### Open Question 3
- Question: How does the performance of the finetuned model vary across different age ranges, genders, and ethnicities?
- Basis in paper: [explicit] The authors provide examples of generated images varying age, gender, and ethnicity, but do not provide a quantitative analysis of the model's performance across these attributes.
- Why unresolved: The paper focuses on demonstrating the overall improvement of the finetuned model compared to the baseline, but does not provide a detailed breakdown of the model's performance across different demographic attributes.
- What evidence would resolve it: Conducting a quantitative analysis of the model's performance across different age ranges, genders, and ethnicities, and comparing the results to the baseline model and human-written captions.

## Limitations

- The relatively small size of the synthetic caption dataset (250,000 images) compared to massive web-scale datasets used to train text-to-image models
- Reliance on pre-trained face analysis models introduces potential biases and limitations that may propagate through the caption generation pipeline
- The paper doesn't thoroughly address computational costs, though the use of LoRA suggests some efficiency considerations

## Confidence

- High confidence: The claim that the proposed pipeline generates more realistic human faces compared to baseline Stable Diffusion 2.1 is well-supported by qualitative comparisons and systematic approach to caption generation
- Medium confidence: The assertion that synthetic captions significantly improve prompt adherence is supported by qualitative examples but lacks comprehensive quantitative evaluation metrics
- Medium confidence: The claim about consistent identity generation across varying attributes is demonstrated through examples but would benefit from more rigorous evaluation protocols

## Next Checks

1. **Quantitative Evaluation of Prompt Adherence**: Conduct a systematic human evaluation study where participants rate prompt adherence for the fine-tuned model versus baseline across a diverse set of detailed prompts. This should include metrics like attribute accuracy rates, semantic similarity scores, and user preference studies to complement the qualitative comparisons.

2. **Generalization Testing Beyond Faces**: Apply the synthetic caption generation pipeline to non-face image datasets and fine-tune the text-to-image model to assess whether the approach generalizes to other image domains. This would help determine if the improvements are specific to face generation or represent a more broadly applicable technique.

3. **Bias and Fairness Analysis**: Perform a comprehensive analysis of the generated images across different demographic groups to identify potential biases introduced through the face analysis models or caption generation process. This should include statistical analysis of generated attributes across different ethnicities, ages, and genders compared to the training data distribution.