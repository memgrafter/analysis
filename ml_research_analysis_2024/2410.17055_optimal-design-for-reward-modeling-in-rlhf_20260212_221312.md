---
ver: rpa2
title: Optimal Design for Reward Modeling in RLHF
arxiv_id: '2410.17055'
source_url: https://arxiv.org/abs/2410.17055
tags:
- arxiv
- reward
- human
- optimal
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of selecting which language model
  generations should be rated by human labelers to maximize the information gained
  for training reward models in RLHF. The authors frame this as a simple regret minimization
  problem in a linear contextual dueling bandit setting and propose an offline method
  called ODPO (Optimal Design for Policy Optimization).
---

# Optimal Design for Reward Modeling in RLHF

## Quick Facts
- arXiv ID: 2410.17055
- Source URL: https://arxiv.org/abs/2410.17055
- Reference count: 40
- Authors propose an offline method for optimal dataset selection in RLHF that achieves near-optimal performance without requiring online feedback

## Executive Summary
This paper addresses the problem of selecting which language model generations should be rated by human labelers to maximize information gained for training reward models in RLHF. The authors frame this as a simple regret minimization problem in a linear contextual dueling bandit setting and propose ODPO (Optimal Design for Policy Optimization), an offline method based on Kiefer-Wolfowitz optimal design theory. Their approach achieves near-optimal performance without requiring online feedback loops, making it both theoretically sound and practically relevant for efficient dataset design in RLHF.

## Method Summary
The method leverages Kiefer-Wolfowitz optimal design theory to select the most informative pairs of language model generations before any human labeling occurs. The approach assumes a linear reward model in the embedding space and uses the Bradley-Terry preference model for human choices. The Frank-Wolfe algorithm approximates the optimal design distribution π* over the action space, selecting T pairs from the initial dataset Dini. Maximum likelihood estimation then computes the reward parameter from human preferences on the selected pairs. The entire process is offline, requiring only one computation of the optimal design rather than iterative online feedback.

## Key Results
- ODPO achieves near-optimal performance without requiring online feedback loops
- The method provides matching upper and lower bounds on simple regret up to constant and logarithmic factors
- Theoretical guarantees are established under assumptions of linearity in the embedding space and boundedness of reward parameters

## Why This Works (Mechanism)

### Mechanism 1
Optimal dataset selection for RLHF can be achieved offline without requiring online feedback loops. The method leverages Kiefer-Wolfowitz optimal design theory to select the most informative pairs of language model generations before any human labeling occurs. This is done by approximating the optimal design distribution π* using the Frank-Wolfe algorithm on the available dataset Dini. Core assumption: The Bradley-Terry model holds for human preferences, and the reward model is linear in the embedding space with bounded parameters and actions. Break condition: If the linearity assumption in the embedding space fails or if the Bradley-Terry model does not accurately represent human preferences, the optimal design approach would no longer guarantee maximum information gain.

### Mechanism 2
The simple regret minimization framework is more appropriate than best-arm identification for RLHF dataset selection. Since the quality difference between language model generations can be arbitrarily small, focusing on identifying a single "best" generation is less meaningful than minimizing the gap between the estimated best generation and the true best generation across all contexts. Core assumption: The reward gap between different generations can approach zero, making best-arm identification ill-defined in this setting. Break condition: If the reward gaps between generations are sufficiently large and well-separated, best-arm identification would become a viable alternative objective.

### Mechanism 3
The theoretical optimality of the proposed method is established through matching upper and lower bounds. The upper bound on simple regret is derived using concentration inequalities for logistic bandits and properties of the design matrix, while the lower bound is established through information-theoretic arguments using the Bretagnolle-Huber inequality and properties of the sigmoid function. Core assumption: The sigmoid function's self-concordance property and the boundedness of both the reward parameter and action space enable tight control of information-theoretic divergences. Break condition: If the boundedness assumptions are violated (e.g., unbounded embeddings or reward parameters), the matching bounds would no longer hold, and the method's optimality would be compromised.

## Foundational Learning

- Concept: Linear contextual dueling bandits
  - Why needed here: The RLHF dataset selection problem is formalized as a linear contextual dueling bandit problem where the reward model is assumed to be linear in the embedding space of context-generation pairs.
  - Quick check question: How does the linear assumption in the embedding space simplify the reward modeling compared to non-linear alternatives?

- Concept: Bradley-Terry preference model
  - Why needed here: This probabilistic model is used to represent the likelihood that one language model generation is preferred over another, forming the basis for the likelihood function used in parameter estimation.
  - Quick check question: What would be the implications if we used a different preference model, such as the Plackett-Luce model, instead of the Bradley-Terry model?

- Concept: Optimal experimental design (Kiefer-Wolfowitz theorem)
  - Why needed here: This theory provides the mathematical foundation for selecting the most informative samples from the available dataset to maximize the information gained from human feedback.
  - Quick check question: Why does the Kiefer-Wolfowitz theorem guarantee that an optimal design with support size at most d(d+1)/2 exists for this problem?

## Architecture Onboarding

- Component map: Dini (dataset) -> Embedding layer (ψ(x,y)) -> ODPO algorithm (Frank-Wolfe) -> Sampling module -> Labeling interface -> Estimation module (MLE) -> Prediction module

- Critical path: 1) Compute embeddings for all context-generation pairs in Dini 2) Run Frank-Wolfe algorithm to approximate optimal design distribution 3) Sample T pairs from Dini according to the distribution 4) Present pairs to labelers and collect preferences 5) Compute MLE of reward parameter using collected preferences 6) Use estimated parameter to select best generation for any context

- Design tradeoffs:
  - Offline vs. online selection: Offline selection reduces computational overhead but may miss dynamically changing preferences
  - Fixed vs. adaptive regularization: Fixed λ simplifies implementation but may not adapt to different dataset characteristics
  - Exact vs. approximate optimal design: Exact design is computationally expensive while approximation trades some optimality for efficiency

- Failure signatures:
  - Poor performance: Indicates either violation of linearity assumption or inadequate coverage of action space
  - High variance in estimates: Suggests insufficient sample size T or poor choice of regularization parameter λ
  - Slow convergence: May indicate numerical instability in Frank-Wolfe algorithm or ill-conditioned design matrix

- First 3 experiments:
  1. Validate embedding quality: Compare cosine similarity between embeddings of similar generations vs. dissimilar ones
  2. Test optimal design approximation: Compare performance using exact optimal design vs. different approximation levels
  3. Evaluate sample efficiency: Measure performance as a function of T to identify the point of diminishing returns

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of ODPO vary when using different embedding models (BERT, RoBERTa, SBERT) for the feature map ψ? The paper mentions that "Simple encoder models such as BERT, RoBERTa or SBERT (Reimers, 2019; Devlin, 2018; Liu, 2019) can be used for the embedding step" but doesn't investigate this variation. This remains unresolved because the paper focuses on theoretical guarantees and assumes a fixed feature map, without empirical comparison of different embedding models. What evidence would resolve it: Experimental results comparing ODPO's performance using different embedding models on the same dataset, measuring regret and computational efficiency.

### Open Question 2
Can the offline dataset selection approach be extended to handle more complex preference models beyond Bradley-Terry, such as the Plackett-Luce model mentioned in the paper? The paper states "It is worth noting that alternative preference models, such as the Plackett-Luce model (Plackett, 1975), can be used instead of this one" but doesn't explore this extension. This remains unresolved because the theoretical analysis and algorithm are specifically designed for the Bradley-Terry model, and extending to other models would require new derivations. What evidence would resolve it: A modified version of ODPO that handles the Plackett-Luce model, along with corresponding theoretical guarantees and empirical validation.

### Open Question 3
How does the choice of regularization parameter λ affect the performance of ODPO in practice, and is there an optimal way to select it? The paper uses λ as a regularization parameter in the likelihood function and design matrix but notes "Choosing λ = 1/d for the regularization" in Corollary 1 without deeper analysis. This remains unresolved because the theoretical analysis treats λ as a fixed parameter, but in practice, its selection could significantly impact performance. What evidence would resolve it: A systematic study of ODPO's performance across different λ values on various datasets, potentially leading to a principled method for selecting λ.

## Limitations
- Theoretical guarantees rely heavily on linearity assumption in embedding space, which may not hold for all types of language model generations or contexts
- The boundedness assumptions for both reward parameter and action space are critical for matching bounds but may be violated in practice with diverse or high-dimensional embeddings
- The paper does not specify exact feature embedding method or provide details about human preference collection process, impacting reproducibility

## Confidence

High confidence in the mechanism connecting optimal experimental design to information-theoretic efficiency gains in dataset selection. Medium confidence in the applicability of the Bradley-Terry model to human preferences in RLHF settings, as real human preferences may exhibit more complex patterns than the assumed pairwise comparison model. Medium confidence in the matching bounds claim, as the constants and logarithmic factors are not explicitly derived and may not hold under practical violations of the assumptions.

## Next Checks

1. **Linearity Validation**: Systematically test the linearity assumption by comparing the performance of the proposed method against non-linear alternatives (e.g., neural reward models) across different types of language model generations and contexts. Measure the degradation in performance when the linearity assumption is violated.

2. **Boundedness Stress Test**: Conduct experiments with varying embedding dimensionalities and action space sizes to identify the point at which the boundedness assumptions begin to fail. Quantify the impact on the theoretical guarantees and practical performance when these bounds are violated.

3. **Frank-Wolfe Approximation Quality**: Systematically evaluate the approximation quality of the Frank-Wolfe algorithm for different values of ε and dimensionalities d. Compare the performance of ODPO using exact optimal design versus different approximation levels to understand the trade-off between computational efficiency and optimality.