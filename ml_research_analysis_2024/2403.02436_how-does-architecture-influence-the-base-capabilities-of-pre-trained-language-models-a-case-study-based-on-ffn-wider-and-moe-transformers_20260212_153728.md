---
ver: rpa2
title: How does Architecture Influence the Base Capabilities of Pre-trained Language
  Models? A Case Study Based on FFN-Wider and MoE Transformers
arxiv_id: '2403.02436'
source_url: https://arxiv.org/abs/2403.02436
tags:
- pre-training
- ffn-wider
- bert
- base
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how transformer architecture affects the
  base capabilities of pre-trained language models. The authors analyze FFN-Wider
  Transformers, which have wider FFN layers but exhibit reduced performance in downstream
  tasks.
---

# How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider and MoE Transformers

## Quick Facts
- arXiv ID: 2403.02436
- Source URL: https://arxiv.org/abs/2403.02436
- Authors: Xin Lu; Yanyan Zhao; Bing Qin; Liangyu Huo; Qing Yang; Dongliang Xu
- Reference count: 40
- Key outcome: This paper investigates how transformer architecture affects the base capabilities of pre-trained language models. The authors analyze FFN-Wider Transformers, which have wider FFN layers but exhibit reduced performance in downstream tasks. Through mutual information and token prediction analysis, they discover that the contribution ratio of Multi-Head Attention (MHA) layers, which act as combination functions, is a key factor influencing model capabilities. To validate this, they propose Combination Adjustable Architecture (CAA), which splits FFN layers into Outer-FFN (transformation function) and Inner-FFN (combination function) with adjustable width ratios. They further develop Combination Enhanced Architecture (CEA) and apply it to both FFN-Wider and MoE Transformers. Experiments show that CEA improves out-of-distribution language modeling and few-shot learning performance, with a 14B MoE model achieving significant gains. This work provides valuable insights into architecture design and improvement for pre-trained language models.

## Executive Summary
This paper explores how transformer architecture affects the base capabilities of pre-trained language models through a case study of FFN-Wider Transformers and MoE models. The authors discover that widening FFN layers reduces the contribution ratio of Multi-Head Attention (MHA) layers, which act as combination functions, leading to degraded performance in downstream tasks. To address this, they propose the Combination Enhanced Architecture (CEA), which splits FFN layers into transformation and combination components with adjustable width ratios. Their experiments demonstrate that CEA improves out-of-distribution language modeling and few-shot learning performance, with particular success on a 14B MoE model.

## Method Summary
The authors investigate transformer architecture's impact on base capabilities by first analyzing FFN-Wider Transformers through mutual information and token prediction methods to measure layer contribution ratios. They discover that wider FFN layers reduce the contribution ratio of MHA layers (combination functions) relative to FFN layers (transformation functions). To validate this hypothesis, they propose the Combination Adjustable Architecture (CAA) and Combination Enhanced Architecture (CEA), which split FFN layers into Outer-FFN (transformation) and Inner-FFN (combination enhancement) with adjustable width ratios. They apply these architectures to both FFN-Wider and MoE Transformers, pre-training models with aligned performance and evaluating on out-of-distribution language modeling, transfer learning, and few-shot learning tasks.

## Key Results
- FFN-Wider Transformers reduce the contribution ratio of Multi-Head Attention (combination functions), leading to base capability decline
- Combination Enhanced Architecture (CEA) improves out-of-distribution language modeling and few-shot learning performance
- A 14B MoE model with CEA achieved significant gains in base capabilities
- The contribution ratio of combination functions is identified as a key factor affecting pre-trained language model base capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The actual contribution ratio of Multi-Head Attention (a combination function) is a key factor affecting base capabilities of pre-trained language models.
- Mechanism: MHA layers embody the combinability of language through weighted context aggregation, while FFN layers provide context-insensitive transformations. The relative contribution of these functions determines how well the model captures language's compositional structure.
- Core assumption: Language's essential characteristics are better captured through combination operations than pure transformation operations.
- Evidence anchors:
  - [abstract] "the actual contribution ratio of Multi-Head Attention (a combination function) is a key factor affecting base capabilities"
  - [section 4.1] "MHA (Multi-Head Attention) layer in the Transformer is a combination function, and the FFN (Feed-Forward Network) layer is a transformation function"
  - [corpus] Found 25 related papers, average neighbor FMR=0.369, no citations yet - indicates this is novel territory
- Break condition: If language modeling benefits more from context-insensitive transformations than context combination, this mechanism would fail.

### Mechanism 2
- Claim: Widening FFN layers reduces the contribution ratio of combination functions, leading to base capability decline.
- Mechanism: When FFN layers are widened, their increased capacity shifts the balance from combination to transformation operations, reducing the model's ability to leverage contextual information.
- Core assumption: Model architecture changes directly affect the relative contribution of different layer types to the final task.
- Evidence anchors:
  - [abstract] "FFN-Wider Transformers reduce the contribution ratio of this combination function, leading to a decline in base capabilities"
  - [section 4.2.2] Token Prediction analysis shows FFN contribution ratios are higher in FFN-Wider models
  - [corpus] No direct evidence found in neighbors, indicating this is a novel insight
- Break condition: If widening FFN layers could be done without affecting MHA contribution ratios, this mechanism would not hold.

### Mechanism 3
- Claim: The Combination Enhanced Architecture (CEA) can restore base capabilities by rebalancing transformation and combination functions.
- Mechanism: By splitting FFN into Outer-FFN (transformation) and Inner-FFN (combination enhancement), CEA allows precise control over the contribution ratio, optimizing for base capabilities.
- Core assumption: The contribution ratio can be manipulated through architectural changes to improve performance.
- Evidence anchors:
  - [abstract] "We confirmed this by experiments and proposed Combination Enhanced Architecture (CEA) to address the decline in base capabilities"
  - [section 5.2.2] Shows base capabilities improve as Outer-FFN width decreases
  - [corpus] Weak evidence - only 25 related papers found, none directly addressing this specific architectural intervention
- Break condition: If there exists an optimal contribution ratio beyond which further rebalancing harms performance, this mechanism would have limits.

## Foundational Learning

- Concept: Mutual Information (MI) estimation
  - Why needed here: To quantify the contribution ratios of different layer types to the final language modeling task
  - Quick check question: How does clustering representations help estimate mutual information between layer outputs and target tokens?

- Concept: Token Prediction (TP) method
  - Why needed here: To measure layer contributions when MI estimation is too computationally expensive for large models
  - Quick check question: Why does the paper use category vectors based on token means rather than individual representations?

- Concept: Pre-training performance alignment
  - Why needed here: To isolate architectural effects from scale and training effects when comparing base capabilities
  - Quick check question: What makes pre-training performance alignment more appropriate than parameter count alignment for cross-architecture analysis?

## Architecture Onboarding

- Component map:
  - MHA layer (combination function)
  - FFN layer (transformation function)
  - Outer-FFN (transformation function)
  - Inner-FFN (combination function)
  - Direct pathway in MHA (prevents information leakage)

- Critical path: MHA → FFN → Output (in standard Transformer), with CEA modifying this to MHA → Inner-FFN → MHA combination → Outer-FFN → Output

- Design tradeoffs:
  - Wider FFN increases model capacity but reduces combination function contribution
  - CEA adds architectural complexity but enables fine-grained control over contribution ratios
  - Direct pathway prevents information leakage but adds implementation complexity

- Failure signatures:
  - Base capabilities degrade when Outer-FFN width ratio approaches 0% in GPT models
  - Performance plateaus or declines when contribution ratios are pushed too far in either direction
  - MoE models show similar base capability decline patterns, suggesting general applicability

- First 3 experiments:
  1. Run MI analysis on a small BERT model to verify contribution ratio differences between vanilla and FFN-Wider architectures
  2. Implement CAA with varying Outer-FFN width ratios on a small model to observe contribution ratio trends
  3. Test CEA on a small MoE model to verify cross-architecture applicability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the contribution ratio of combination functions to pre-trained language modeling serve as a universal factor affecting base capabilities across different pre-training objectives beyond language modeling?
- Basis in paper: [explicit] The authors acknowledge their work mainly analyzes models using language modeling as the pre-training objective and lacks experiments on models with other pre-training objectives, limiting the conclusions to pre-trained language models.
- Why unresolved: The study focuses exclusively on language modeling pre-training objectives, leaving open whether the observed relationship between combination function contribution ratios and base capabilities extends to other tasks like image recognition or speech processing.
- What evidence would resolve it: Systematic experiments comparing base capabilities across architectures using different pre-training objectives (e.g., masked autoencoders for vision, contrastive learning for speech) while controlling for contribution ratios would establish universality or task-specificity.

### Open Question 2
- Question: What is the optimal contribution ratio of transformation functions that balances base capabilities without harming performance, particularly for models like BERT that retain some transformation leakage?
- Basis in paper: [inferred] The authors note that while reducing transformation function contribution generally improves base capabilities, completely eliminating it could be harmful, as evidenced by the anomalous result in GPT models and BERT's ability to retain some transformation contribution through bidirectional attention.
- Why unresolved: The study identifies trends showing improvement with reduced transformation contribution but doesn't establish a precise optimal ratio, particularly given architectural differences between BERT and GPT.
- What evidence would resolve it: Systematic ablation studies varying Outer-FFN width ratios across different model architectures (BERT, GPT, MoE) while measuring base capabilities on standardized benchmarks would identify optimal ratios for each architecture type.

### Open Question 3
- Question: How do different architectural modifications beyond FFN width (e.g., attention head count, layer depth, normalization schemes) interact with contribution ratios to influence base capabilities?
- Basis in paper: [inferred] The authors focus specifically on FFN width modifications and their impact on contribution ratios, but acknowledge this is one factor among many architectural choices that could affect base capabilities through different mechanisms.
- Why unresolved: The study isolates FFN width as a variable while keeping other architectural components constant, leaving open how these components might compound or counteract the observed effects.
- What evidence would resolve it: Experiments systematically varying multiple architectural parameters (FFN width, attention head count, layer depth, normalization) in combination while measuring their joint effects on contribution ratios and base capabilities would reveal interaction effects and optimal architectural configurations.

## Limitations
- The core hypothesis relies on the assumption that language modeling fundamentally requires a balance between combination and transformation functions, but this assumption lacks direct empirical validation
- The Mutual Information estimation method depends on clustering representations, a technique that can be sensitive to hyperparameters and may not capture full complexity
- The direct pathway in MHA that bypasses Inner-FFN is mentioned but not fully specified, making exact replication challenging
- The paper doesn't explore whether there might be an optimal contribution ratio beyond which further rebalancing becomes detrimental

## Confidence
**High Confidence:** The observation that FFN-Wider Transformers show reduced performance in downstream tasks is well-established through experimental results. The Token Prediction method for measuring layer contributions appears robust and produces consistent results across different architectures.

**Medium Confidence:** The mechanism by which widening FFN layers reduces MHA contribution ratios is plausible but relies on indirect evidence. While the paper shows correlation between FFN width and contribution ratios, the causal relationship needs more rigorous validation.

**Low Confidence:** The general applicability of the combination function hypothesis across all transformer architectures and tasks is asserted but not comprehensively tested. The paper focuses primarily on BERT and GPT-style models with limited exploration of other transformer variants.

## Next Checks
1. **Ablation Study on Direct Pathway:** Conduct controlled experiments removing the direct pathway in MHA to quantify its impact on information flow and base capabilities. Measure whether this affects the contribution ratio dynamics and whether the claimed "information leakage prevention" is empirically validated.

2. **Contribution Ratio Optimization Sweep:** Systematically vary Outer-FFN width ratios across a broader range (0-100%) on multiple model sizes and architectures to identify potential optimal ratios and test whether there are diminishing returns or negative effects beyond certain thresholds.

3. **Cross-Architecture Generalization Test:** Apply CEA to non-standard transformer architectures (e.g., Performer, Longformer) and tasks outside traditional NLP (such as code generation or mathematical reasoning) to validate whether the combination function hypothesis holds across different domains and architectural innovations.