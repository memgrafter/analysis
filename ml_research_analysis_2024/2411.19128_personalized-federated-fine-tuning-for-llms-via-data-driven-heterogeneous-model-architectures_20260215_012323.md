---
ver: rpa2
title: Personalized Federated Fine-Tuning for LLMs via Data-Driven Heterogeneous Model
  Architectures
arxiv_id: '2411.19128'
source_url: https://arxiv.org/abs/2411.19128
tags: []
core_contribution: This paper introduces FedAMoLE, a personalized federated fine-tuning
  framework for LLMs that enables data-driven heterogeneous model architectures. The
  core idea is to use an Adaptive Mixture of LoRA Experts (AMoLE) module, which globally
  maintains a shared expert and a pool of domain experts, assigning varying numbers
  of experts to each client based on their data distributions.
---

# Personalized Federated Fine-Tuning for LLMs via Data-Driven Heterogeneous Model Architectures

## Quick Facts
- arXiv ID: 2411.19128
- Source URL: https://arxiv.org/abs/2411.19128
- Reference count: 40
- Personalized federated fine-tuning framework for LLMs that improves client-side performance by an average of 5.97% over existing approaches

## Executive Summary
This paper introduces FedAMoLE, a personalized federated fine-tuning framework for large language models that enables data-driven heterogeneous model architectures. The core innovation is the Adaptive Mixture of LoRA Experts (AMoLE) module, which globally maintains a shared expert and a pool of domain experts, assigning varying numbers of experts to each client based on their data distributions. A reverse selection-based expert assignment (RSEA) strategy is proposed to adaptively determine the optimal model architecture for each client by having domain experts select the clients whose data best matches their knowledge domains. Experiments across six heterogeneous data scenarios demonstrate that FedAMoLE significantly outperforms existing approaches while maintaining practical memory, communication, and computation overhead.

## Method Summary
FedAMoLE implements personalized federated fine-tuning through an AMoLE module that combines a shared expert with domain-specific LoRA adapters. The RSEA strategy assigns experts to clients by solving an optimization problem based on token and expert embeddings similarity. During federated training, each client receives their assigned experts and performs local fine-tuning, while the server aggregates shared components via FedAvg and updates expert assignments each round. The framework uses lightweight LoRA adapters to minimize communication overhead and employs projection-based routing to enable heterogeneous model aggregation.

## Key Results
- FedAMoLE improves client-side performance by an average of 5.97% over existing approaches
- The framework maintains practical memory, communication, and computation overhead despite enabling heterogeneous model architectures
- RSEA strategy successfully matches domain experts to client data distributions across six heterogeneous data scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data-driven expert assignment improves personalization by matching domain experts to client data distributions.
- Mechanism: The RSEA strategy allows domain experts to select clients whose data features best match their knowledge domains by solving an optimization problem based on token and expert embeddings similarity.
- Core assumption: The scaled dot product between token embeddings and expert embeddings is a valid proxy for measuring data-expert relevance.
- Evidence anchors:
  - [abstract] "a reverse selection-based expert assignment (RSEA) strategy to adaptively determine the optimal model architecture for each client by having domain experts select the clients whose data best matches their knowledge domains."
  - [section] "Based on the above definitions, the relevance between client i's local data and expert j's domain in module m is: sm,i,j = (h^t_m,i)^T · h^e_m,j / √d"
- Break condition: If the token embeddings do not capture sufficient information about data characteristics, or if expert embeddings become too generic during training, the relevance measure would fail.

### Mechanism 2
- Claim: AMoLE modules enable heterogeneous model architectures with low communication overhead by using lightweight LoRA adapters as experts.
- Mechanism: Instead of dense expert sub-modules, AMoLE uses LoRA adapters which significantly reduce parameter size while maintaining personalization capability through sparse expert routing.
- Core assumption: LoRA adapters can capture sufficient adaptation knowledge to be effective as MoE experts while being small enough for efficient communication.
- Evidence anchors:
  - [abstract] "adaptive mixture of LoRA experts (AMoLE) module, which globally maintains a shared expert and a pool of domain experts, assigning varying numbers of experts to each client"
  - [section] "By leveraging the lightweight nature of LoRA adapters, MoLE substantially reduces the parameter size of MoE experts"
- Break condition: If LoRA adapters become too small to capture meaningful domain-specific knowledge, or if the router becomes the bottleneck, the effectiveness would degrade.

### Mechanism 3
- Claim: The FedAvg-compatible router design allows heterogeneous model aggregation by using projection matrices independent of expert count.
- Mechanism: The AMoLE router uses token projection matrices Wt that have consistent dimensions across clients, enabling direct FedAvg aggregation even when clients have different numbers of assigned experts.
- Core assumption: The projection-based router can effectively route tokens to appropriate experts without requiring router dimensions to match expert count.
- Evidence anchors:
  - [section] "This module replaces the traditional MoLE router with projection matrices independent of the number of domain experts to select the most similar domain experts for each token"
  - [section] "ensuring that instances of an AMoLE module on different clients have routers with consistent dimensions despite holding varying numbers of domain experts"
- Break condition: If the projection-based routing becomes less effective than traditional expert-count-based routing, or if the aggregation quality degrades significantly.

## Foundational Learning

- Concept: Mixture of Experts (MoE) architecture
  - Why needed here: Understanding MoE is essential to grasp how FedAMoLE achieves heterogeneous model architectures through expert collaboration
  - Quick check question: How does a traditional MoE layer select which experts to activate for a given input?

- Concept: Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning
  - Why needed here: LoRA is the foundation for making experts lightweight enough for federated learning with reasonable communication overhead
  - Quick check question: What is the mathematical relationship between the original weight matrix W and its LoRA approximation W + BA?

- Concept: Federated Learning aggregation mechanics (FedAvg)
  - Why needed here: Understanding how FedAvg works is crucial for comprehending how heterogeneous models with different expert assignments can be aggregated
  - Quick check question: What is the main challenge when trying to aggregate models with different architectures in FedAvg?

## Architecture Onboarding

- Component map:
  Server -> AMoLE module (shared expert, domain expert pool, RSEA strategy) -> Clients (assigned experts) -> Local fine-tuning -> Parameter updates -> Server aggregation -> Updated AMoLE module

- Critical path:
  1. Server initializes global AMoLE module with shared expert and domain expert pool
  2. Server assigns experts to clients via RSEA based on data similarity
  3. Clients receive their assigned experts and perform local fine-tuning
  4. Clients return updated parameters and embeddings to server
  5. Server aggregates shared components via FedAvg
  6. Server updates expert embeddings and runs RSEA for next round

- Design tradeoffs:
  - Expert assignment frequency: RSEA runs each round vs. periodically - more frequent gives better adaptation but higher overhead
  - Expert pool size: Larger pools provide more specialization options but increase communication and computation
  - Number of experts per client: More experts improve personalization but increase local computation and memory

- Failure signatures:
  - All clients converge to similar expert assignments → RSEA not working or data too homogeneous
  - Training stalls or accuracy plateaus → router not effectively routing, or experts too small to capture knowledge
  - Communication overhead too high → LoRA adapters not small enough, or expert pool too large

- First 3 experiments:
  1. Baseline test: Run FedAMoLE with all clients assigned same experts (no RSEA) to verify MoLE functionality
  2. RSEA validation: Test with synthetic data where expert-client matches are known to verify assignment quality
  3. Communication overhead measurement: Compare upload/download sizes across different expert pool sizes to verify lightweight nature

## Open Questions the Paper Calls Out

- Open Question 1: How does the optimal number of experts per module vary across different tasks and data distributions in FedAMoLE?
- Open Question 2: How would FedAMoLE perform in cross-device federated learning scenarios with potentially thousands of clients versus the cross-silo setting studied in the paper?
- Open Question 3: How does FedAMoLE's reverse selection strategy compare to alternative expert assignment methods like similarity-based or clustering-based approaches in terms of both performance and communication efficiency?

## Limitations

- Data distribution heterogeneity modeling is limited to specific Dirichlet parameters without clear justification or testing of more extreme scenarios
- RSEA optimization scalability is untested for larger client populations, with potential computational prohibitive for hundreds of clients
- LoRA adapter effectiveness as experts lacks ablation studies on adapter rank, with r=8 chosen without systematic justification

## Confidence

**High Confidence** (Strong empirical support and clear theoretical foundation):
- The core claim that data-driven expert assignment improves personalization through matching domain experts to client data distributions
- The mechanism that AMoLE modules enable heterogeneous model architectures with low communication overhead through lightweight LoRA adapters
- The FedAvg-compatible router design that allows heterogeneous model aggregation through projection matrices independent of expert count

**Medium Confidence** (Reasonable support but with notable gaps):
- The specific 5.97% average improvement over existing approaches, given limited ablation studies on critical hyperparameters
- The claim that reverse selection-based expert assignment outperforms random assignment, with only one comparative baseline
- The scalability of the approach to larger client populations and more complex data distributions

## Next Checks

1. **Ablation study on LoRA rank**: Systematically vary the LoRA adapter rank (r) from 4 to 32 and measure the impact on both model performance and communication overhead. This would validate whether r=8 is optimal or if there's a better trade-off point between effectiveness and efficiency.

2. **Scalability stress test**: Implement FedAMoLE with increasing numbers of clients (10 → 50 → 100) and measure both the MILP optimization time for RSEA and the quality of expert assignments. This would reveal practical limits of the current implementation and whether alternative assignment strategies are needed for larger deployments.

3. **Robustness to data distribution shifts**: After initial training, simulate gradual data distribution changes by modifying client data distributions mid-training and measure how quickly FedAMoLE adapts through RSEA. This would validate whether the approach maintains personalization under realistic federated learning conditions where client data evolves over time.