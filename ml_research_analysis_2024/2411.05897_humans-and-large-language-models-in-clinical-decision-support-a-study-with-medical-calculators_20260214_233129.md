---
ver: rpa2
title: 'Humans and Large Language Models in Clinical Decision Support: A Study with
  Medical Calculators'
arxiv_id: '2411.05897'
source_url: https://arxiv.org/abs/2411.05897
tags:
- page
- medical
- llms
- https
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the capability of large language models (LLMs)
  and medical trainees to recommend clinical calculators for patient cases. Researchers
  created MedQA-Calc, a dataset of 1,009 multiple-choice questions across 35 clinical
  calculators using real patient cases from PubMed Central.
---

# Humans and Large Language Models in Clinical Decision Support: A Study with Medical Calculators

## Quick Facts
- arXiv ID: 2411.05897
- Source URL: https://arxiv.org/abs/2411.05897
- Reference count: 23
- Human medical trainees outperformed GPT-4o in recommending clinical calculators for patient cases

## Executive Summary
This study compares the performance of large language models (LLMs) and medical trainees in recommending appropriate clinical calculators for patient cases. The researchers created MedQA-Calc, a novel dataset of 1,009 multiple-choice questions across 35 clinical calculators using real patient cases from PubMed Central. When tested on a subset of 100 questions, human annotators achieved 79.5% accuracy on average, with one trainee reaching 90%, while GPT-4o scored 74.3%. The findings demonstrate that despite significant advances in LLM capabilities, current models have not reached human-level performance in this complex clinical decision support task, particularly struggling with comprehension of clinical context and proper calculator selection.

## Method Summary
Researchers constructed MedQA-Calc, a dataset containing 1,009 multiple-choice questions covering 35 different clinical calculators based on real patient cases from PubMed Central. They evaluated eight different LLMs against two medical trainees using a subset of 100 questions. The evaluation measured accuracy in correctly recommending the appropriate clinical calculator for each patient case. The study employed comprehensive error analysis to categorize failure modes, identifying comprehension errors (56.6% of LLM errors) and calculator knowledge errors (8.1%) as the primary sources of incorrect recommendations.

## Key Results
- Human annotators averaged 79.5% accuracy in recommending clinical calculators, with one trainee achieving 90%
- GPT-4o achieved 74.3% accuracy, underperforming human participants
- LLMs made comprehension errors in 56.6% of cases and calculator knowledge errors in 8.1% of cases

## Why This Works (Mechanism)
This study works by providing a structured framework for evaluating clinical decision support capabilities through standardized patient cases and calculator recommendations. The MedQA-Calc dataset creates a controlled environment where both human and LLM performance can be directly compared on identical tasks, allowing for meaningful benchmarking of AI capabilities against human expertise in clinical contexts.

## Foundational Learning

1. Clinical calculators - why needed: Essential tools for medical decision-making that quantify risk, predict outcomes, or guide treatment based on patient-specific data
   Quick check: Can you identify the correct calculator for a patient with chest pain and suspected myocardial infarction?

2. Prompt engineering - why needed: Critical for optimizing LLM responses and ensuring consistent, reliable output in medical applications
   Quick check: What prompt format yields the most accurate clinical calculator recommendations?

3. Error analysis methodology - why needed: Necessary for understanding failure modes and identifying areas for model improvement
   Quick check: Can you categorize LLM errors into comprehension, calculator knowledge, and other types?

## Architecture Onboarding

**Component Map:**
MedQA-Calc Dataset -> LLM Evaluation Framework -> Error Analysis Pipeline -> Performance Benchmarking

**Critical Path:**
Dataset creation → Question formulation → LLM prompting → Response evaluation → Error categorization → Performance comparison

**Design Tradeoffs:**
- Dataset size (1,009 questions) vs. comprehensiveness across all clinical scenarios
- Multiple-choice format vs. open-ended clinical reasoning
- Automated evaluation vs. expert human review

**Failure Signatures:**
- Comprehension errors: LLM fails to understand patient case context or key clinical details
- Calculator knowledge errors: LLM lacks understanding of when specific calculators should be applied
- Generation errors: LLM provides incomplete or incorrectly formatted responses

**3 First Experiments:**
1. Test additional LLM architectures including domain-specific fine-tuned models
2. Expand evaluation to include time efficiency metrics for both human and AI performance
3. Conduct ablation studies on prompt engineering approaches to identify optimal prompting strategies

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation based on a relatively small sample of 100 questions from the 1,009-question dataset
- Dataset covers only 35 clinical calculators, potentially limiting generalizability to all clinical calculation needs
- Does not explore whether errors stem from training data limitations, prompt engineering approaches, or architectural constraints

## Confidence

**Major Claim Confidence:**
- LLMs underperform humans in clinical calculator recommendation: High confidence
- Comprehension errors are the primary failure mode: Medium confidence
- Current LLMs cannot match human performance in this domain: Medium confidence

## Next Checks
1. Expand evaluation to additional LLM architectures and versions, including open-source models with domain-specific fine-tuning
2. Test performance on a larger, more diverse set of clinical calculators beyond the current 35
3. Conduct time efficiency analysis comparing human and LLM performance to assess practical deployment implications