---
ver: rpa2
title: A Metric Driven Approach to Mixed Precision Training
arxiv_id: '2408.02897'
source_url: https://arxiv.org/abs/2408.02897
tags:
- quantization
- precision
- training
- error
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a metric-driven methodology for mixed-precision
  training in deep neural networks. The authors propose a systematic approach to select
  appropriate low-precision numeric formats (INT8, E4M3, E5M2) for different tensor
  types in BERT model training.
---

# A Metric Driven Approach to Mixed Precision Training

## Quick Facts
- arXiv ID: 2408.02897
- Source URL: https://arxiv.org/abs/2408.02897
- Reference count: 17
- Primary result: INT8 works well for weights and activations with fine-grained scaling, while FP8 formats are better suited for heavy-tailed gradient tensors in BERT training

## Executive Summary
This paper presents a metric-driven methodology for selecting appropriate low-precision numeric formats in mixed-precision deep learning training. The approach systematically analyzes tensor distributions and quantization errors to guide format selection across different tensor types in neural networks. By matching tensor characteristics with appropriate numeric formats (INT8, E4M3, E5M2), the methodology achieves training efficiency close to baseline bfloat16 while maintaining model quality.

## Method Summary
The methodology involves analyzing tensor distributions from matrix multiplication operations in neural networks, then selecting appropriate 8-bit numeric formats based on distribution characteristics. INT8 is used for weights and activations with fine-grained scaling, while FP8 formats (E4M3, E5M2) handle heavy-tailed gradient tensors. The approach uses dynamic quantization with per-tensor scaling and incorporates stochastic rounding for better convergence with INT8 on challenging distributions.

## Key Results
- INT8 achieves similar performance to bfloat16 for weights and activations with fine-grained scaling
- FP8 formats maintain baseline-level performance for heavy-tailed gradient tensors
- BERT training with the proposed approach achieves AUC scores of 17.05-17.15 compared to 17.13 for baseline
- Tensor-level INT8 quantization fails to converge for gradients without stochastic rounding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: INT8 works well for weights and activations due to their relatively normal distribution and narrower dynamic range
- Mechanism: The quantization error is minimized when the tensor distribution matches the numeric format's representable range. Normal distributions (like weights) and narrower dynamic ranges (like activations) align well with INT8's 128-level precision
- Core assumption: Tensor distributions remain stable enough during training for dynamic quantization to be effective
- Evidence anchors:
  - [section]: "In general we find that the weights have a normal distribution and the gradients have a log-normal distribution"
  - [abstract]: "INT8 works well for weights and activations with fine-grained scaling"
  - [corpus]: Weak evidence - corpus focuses on mixed-precision applications rather than distribution analysis
- Break Condition: If tensor distributions become highly heavy-tailed or shift dramatically during training, INT8's limited range would cause excessive clipping errors

### Mechanism 2
- Claim: FP8 formats (E4M3, E5M2) are better suited for heavy-tailed gradient tensors due to their wider dynamic range
- Mechanism: Heavy-tailed distributions (like gradients) require formats that can represent extreme values without clipping. FP8 sacrifices mantissa precision to gain exponent bits, enabling representation of large gradient values
- Core assumption: Gradient magnitudes vary significantly enough to require the extended range of FP8
- Evidence anchors:
  - [section]: "Meanwhile, applying INT8 to the LHS (mostly activations, with a higher dynamic range than the weights) produced a slightly more noticeable degradation at the tensor level"
  - [abstract]: "FP8 formats are better suited for heavy-tailed gradient tensors"
  - [corpus]: Weak evidence - corpus mentions mixed-precision but doesn't discuss distribution-specific format matching
- Break Condition: If gradient distributions become less heavy-tailed or if fine-grained scaling is applied to gradients, the advantage of FP8 may diminish

### Mechanism 3
- Claim: Finer granularity scaling overcomes INT8 limitations for tensors with higher dynamic ranges
- Mechanism: By scaling at channel or fine-grained levels rather than tensor level, the dynamic range requirements for each subset are reduced, allowing INT8 to maintain precision where it would otherwise fail
- Core assumption: Non-contracting dimensions (like channels) have sufficiently similar distributions to justify separate scaling
- Evidence anchors:
  - [section]: "However, this degradation can be overcome by using finer granularities"
  - [abstract]: "with fine-grained scaling" in reference to INT8 performance
  - [corpus]: No direct evidence - corpus focuses on mixed-precision selection rather than scaling granularity
- Break Condition: If the overhead of finer scaling outweighs the benefits, or if channel distributions are too heterogeneous, this approach may fail

## Foundational Learning

- Concept: Tensor distribution analysis (mean, variance, skew, kurtosis)
  - Why needed here: Understanding tensor characteristics is essential for selecting appropriate numeric formats and identifying potential quantization issues
  - Quick check question: What distribution characteristics would indicate a tensor is suitable for INT8 versus FP8?

- Concept: Dynamic quantization with per-tensor scaling
  - Why needed here: The methodology relies on adjusting quantization parameters at each training step based on current tensor statistics
  - Quick check question: How does dynamic quantization differ from static quantization in terms of handling distribution shifts during training?

- Concept: Quantization error components (clipping vs rounding error)
  - Why needed here: Different error types have different impacts on training stability and model quality, informing format selection
  - Quick check question: Which type of quantization error (clipping or rounding) is more detrimental to training convergence?

## Architecture Onboarding

- Component map:
  - Data flow: High-precision tensors → Distribution analysis → Format selection → Dynamic quantization → MAC operations → Descaling → Back to training loop
  - Key components: Tensor sampling module, distribution analyzer, format selector, quantization engine, scaling manager

- Critical path: Tensor sampling → Distribution analysis → Format decision → Quantization application → Training step
  - The bottleneck is typically the distribution analysis and format decision, which must be fast enough to not impede training throughput

- Design tradeoffs:
  - Precision vs performance: Higher precision formats (E5M2) provide better accuracy but less performance gain than INT8
  - Granularity vs overhead: Finer scaling granularity improves accuracy but increases computational overhead
  - Format diversity vs hardware support: More formats provide better optimization opportunities but require more hardware support

- Failure signatures:
  - Training divergence: Often indicates inappropriate format selection for heavy-tailed tensors
  - Significant accuracy drop: May indicate insufficient granularity or wrong format choice for a particular tensor type
  - Increased training time without quality improvement: Could signal over-engineering with unnecessary fine-grained scaling

- First 3 experiments:
  1. Baseline comparison: Run BERT training with bfloat16, INT8 (tensor-level), and mixed precision (selected formats) to establish performance/quality baseline
  2. Granularity sweep: Apply INT8 with tensor, channel, and fine-grained scaling to LHS tensors to measure accuracy impact
  3. Distribution analysis validation: Verify that the observed tensor distributions match theoretical expectations (normal for weights, log-normal for gradients) using statistical tests

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed metric-driven methodology for mixed-precision training generalize to different neural network architectures beyond BERT?
- Basis in paper: [explicit] The authors state that "The technique can be generalized to other model architectures" but only demonstrate results on BERT
- Why unresolved: The paper only presents results for a single model architecture (BERT), limiting understanding of how the methodology applies to different types of neural networks with varying tensor distributions and computational patterns
- What evidence would resolve it: Systematic evaluation of the methodology across multiple diverse neural network architectures (CNNs, RNNs, transformers with different scales, etc.) showing consistent effectiveness and identifying any architecture-specific considerations

### Open Question 2
- Question: What is the relationship between quantization granularity and training convergence when using stochastic rounding with INT8 for gradient tensors?
- Basis in paper: [explicit] The authors note that "Both FP8 formats performed at the baseline level" with stochastic rounding for gradients, but "INT8 results still did not converge when applying tensor level quantization" and required finer scales
- Why unresolved: The paper provides limited quantitative analysis of how different granularity levels affect convergence rates and final model quality when using stochastic rounding with INT8 for gradient tensors
- What evidence would resolve it: Controlled experiments varying quantization granularity levels (tensor, channel, fine-grained) with stochastic rounding on gradient tensors, measuring convergence speed, final accuracy, and comparing to baseline training

### Open Question 3
- Question: How do different rounding techniques (RTNE vs stochastic rounding) impact the backward error analysis for various tensor distributions in mixed-precision training?
- Basis in paper: [explicit] The authors compare RTNE and stochastic rounding, noting that "while the error can vary widely for INT8 depending on the heavy-tailedness of the inputs, it is much more constrained for FP8 formats" but don't provide comprehensive analysis across distributions
- Why unresolved: The paper only presents limited backward error analysis for a single t-distribution example, without systematic comparison of rounding techniques across the full range of tensor distributions encountered in neural network training
- What evidence would resolve it: Comprehensive backward error analysis comparing RTNE and stochastic rounding across multiple tensor distributions (normal, log-normal, heavy-tailed) and operations, quantifying the trade-offs between error magnitude and convergence behavior

## Limitations
- The methodology's effectiveness beyond BERT model architecture remains unverified
- The computational overhead of fine-grained scaling approaches is not fully quantified
- Long-term training stability with dynamic quantization across extended periods needs further investigation

## Confidence
**High Confidence**: The relationship between tensor distribution characteristics and appropriate numeric format selection is well-established through the presented analysis and experimental results.

**Medium Confidence**: The effectiveness of the mixed-precision approach across different model architectures and training scenarios requires further validation beyond the BERT experiments.

**Low Confidence**: The long-term stability of dynamic quantization across extended training periods and the impact of distribution shifts during fine-tuning phases need more investigation.

## Next Checks
1. **Distribution Stability Analysis**: Track tensor distribution statistics across multiple epochs to verify the assumption of distribution stability during training.

2. **Cross-Architecture Validation**: Apply the methodology to other transformer variants (GPT, RoBERTa) and convolutional architectures to test generalizability.

3. **Quantization Overhead Measurement**: Precisely measure the computational overhead of finer-grained scaling approaches versus their accuracy benefits across different batch sizes and hardware configurations.