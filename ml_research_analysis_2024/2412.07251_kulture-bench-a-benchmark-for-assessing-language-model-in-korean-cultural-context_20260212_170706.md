---
ver: rpa2
title: 'KULTURE Bench: A Benchmark for Assessing Language Model in Korean Cultural
  Context'
arxiv_id: '2412.07251'
source_url: https://arxiv.org/abs/2412.07251
tags:
- korean
- cultural
- dataset
- language
- news
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'KULTURE Bench is a benchmark for evaluating large language models''
  understanding of Korean culture across three datasets: idioms, poetry, and news.
  It assesses models at word, sentence, and paragraph levels using 3,584 instances.'
---

# KULTURE Bench: A Benchmark for Assessing Language Model in Korean Cultural Context

## Quick Facts
- arXiv ID: 2412.07251
- Source URL: https://arxiv.org/abs/2412.07251
- Reference count: 0
- Top models achieve only moderate accuracy (69.12% on idioms, 45.7% on poetry) in Korean cultural understanding tasks

## Executive Summary
KULTURE Bench is a benchmark designed to evaluate large language models' understanding of Korean culture across three culturally-rich datasets: idioms, poetry, and news headlines. The benchmark consists of 3,584 instances that assess models at word, sentence, and paragraph levels through cloze-style fill-in-the-blank tasks. Experiments with models trained on different corpora (Korean, Chinese, and English) reveal significant performance gaps, with native Korean models like Clova X outperforming others. Even the best models achieve only moderate accuracy, indicating substantial room for improvement in cultural reasoning capabilities.

## Method Summary
The benchmark was constructed using three datasets: KorID (idioms cloze test, 1,631 instances), KorPD (poetry cloze test, 453 instances), and KorCND (news headline matching, 1,500 instances). Dataset creation involved retrieving semantically similar negative candidates using KorBERT embeddings and manual validation to ensure clear distinctions. Models were evaluated using zero-shot prompting and Chain-of-Thought reasoning with varying reasoning lengths (1-2, 3-4, 5-6 sentences). The evaluation compared model accuracy against human performance from Korean Studies experts and non-experts.

## Key Results
- Native Korean models (Clova X) significantly outperform non-native models (GPT-3.5 Turbo, Tiangong) on cultural reasoning tasks
- Chain-of-Thought reasoning improves accuracy only when models have sufficient cultural background knowledge
- News-level cultural tasks are easier for models than idiom or poetry tasks, with all models showing higher performance on news
- Human performance significantly exceeds model performance, especially on culturally-rich texts like poetry and idioms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models trained on culturally relevant corpora (e.g., Clova X on Korean) outperform those trained on non-native corpora in cultural reasoning tasks.
- Mechanism: Native language models internalize culturally specific idioms, poetry, and news contexts during pretraining, giving them implicit cultural knowledge that aids in understanding subtle meanings and associations.
- Core assumption: The training corpus contains enough culturally rich and representative data to form accurate cultural representations.
- Evidence anchors:
  - [abstract] "results show that there is still significant room for improvement in the models' understanding of texts related to the deeper aspects of Korean culture."
  - [section 5.2] "Clova X, which is primarily trained on Korean language corpora, has a deep understanding of Korean culture; the Tiangong model, based on Chinese corpora, benefits from the cultural proximity between China and Korea; whereas GPT-3.5 Turbo, trained on English corpora, is more distanced from Korean culture."
  - [corpus] Weak evidence; corpus composition details are not provided in the paper.

### Mechanism 2
- Claim: Chain-of-thought reasoning improves model accuracy only when the model already has sufficient cultural background knowledge.
- Mechanism: Models with strong cultural grounding can use intermediate reasoning steps to clarify context and infer correct answers, while those without such knowledge may generate misleading or incorrect reasoning paths.
- Core assumption: Cultural knowledge is a prerequisite for effective reasoning in culturally rich tasks.
- Evidence anchors:
  - [section 5.2] "The analysis showed that the reasoning process of the Clova X model was correct 96% of the time. The Tiangong model reasoned correctly in 74% of the cases, while GPT-3.5 Turbo only demonstrated correct reasoning in 46% of instances."
  - [abstract] "Chain-of-thought reasoning helps when models have sufficient cultural knowledge but can degrade performance with excessive reasoning steps."
  - [corpus] No explicit corpus evidence; inference drawn from comparative model performance.

### Mechanism 3
- Claim: News-level cultural tasks are easier for models than idiom or poetry tasks because they involve more explicit, contemporary contexts.
- Mechanism: News texts contain more explicit cultural markers and less reliance on metaphorical or historical knowledge, making them more accessible to models trained on general web corpora.
- Core assumption: Contemporary news is more likely to be included in general pretraining corpora than classical idioms or poetry.
- Evidence anchors:
  - [abstract] "even the highest-performing Clova X achieves only a 69.12% accuracy on the dataset containing idiomatic expressions, and the best result for the dataset featuring poetry is a mere 45.7% accuracy by GPT-4."
  - [section 5.1] "we observe that all models exhibit relatively high performance when processing news texts, but significantly lower accuracy with texts containing rich cultural elements such as poetry and idioms."
  - [corpus] No corpus composition details provided; assumption based on general pretraining trends.

## Foundational Learning

- Concept: Cultural context in language models
  - Why needed here: Understanding that language models require culturally relevant training data to accurately interpret idioms, poetry, and culturally nuanced news.
  - Quick check question: Why do models trained on non-native corpora perform worse on culturally specific benchmarks?
- Concept: Chain-of-thought reasoning
  - Why needed here: Evaluating whether intermediate reasoning steps help or hinder model performance depending on existing cultural knowledge.
  - Quick check question: What happens to model accuracy when CoT is applied to models lacking cultural background knowledge?
- Concept: Dataset construction for cultural evaluation
  - Why needed here: Ensuring that evaluation datasets accurately reflect real-world cultural usage and avoid translation bias.
  - Quick check question: How does sourcing idioms and poetry from authentic Korean texts improve benchmark validity?

## Architecture Onboarding

- Component map: Raw cultural text -> KorBERT embedding generation -> Semantic similarity-based candidate retrieval -> Cloze test generation -> Model evaluation -> Error categorization
- Critical path: Raw cultural text -> Embedding generation -> Candidate selection -> Model input -> Accuracy measurement
- Design tradeoffs: High semantic similarity thresholds may yield clearer negative candidates but reduce dataset size; lower thresholds increase size but risk ambiguous distractors
- Failure signatures: Poor accuracy on idioms/poetry indicates insufficient cultural representation in training data; degraded CoT performance suggests reasoning overload or lack of background knowledge
- First 3 experiments:
  1. Evaluate baseline model accuracy on news vs. idioms vs. poetry to confirm dataset difficulty ordering
  2. Test CoT impact on each model type (native, proximate, distant corpora) to confirm knowledge prerequisite
  3. Vary reasoning length (short/medium/long) on a subset of models to identify optimal CoT step count

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do language models perform on Korean cultural benchmarks when trained with mixed corpora (e.g., both Korean and English) compared to models trained exclusively on Korean or English data?
- Basis in paper: [inferred] The paper evaluates models trained on different corpora (Korean, English, Chinese) but doesn't test mixed-corpus models
- Why unresolved: The study only examines monolingual training approaches, leaving open whether combining multiple language corpora would improve or hinder cultural comprehension
- What evidence would resolve it: Experimental results comparing mixed-corpus models against monolingual models on the KULTURE Bench datasets

### Open Question 2
- Question: Does increasing the diversity and volume of Korean cultural training data (beyond news articles to include more poetry, idioms, and historical texts) improve model performance on culturally-rich tasks?
- Basis in paper: [explicit] The paper notes that "there is a scarcity of such data in the training corpora" as a reason for poor performance on poetry and idioms
- Why unresolved: The study identifies data scarcity as a limitation but doesn't experimentally test whether increasing cultural data volume improves performance
- What evidence would resolve it: Performance comparisons of models trained on standard Korean corpora versus enhanced corpora with more cultural content

### Open Question 3
- Question: How does human performance on the KULTURE Bench change when comparing experts versus non-experts across different age groups and educational backgrounds?
- Basis in paper: [explicit] The paper compares human experts (Korean Studies graduate/doctoral students) with non-experts (undergraduates without Korean Studies focus) but doesn't explore age or broader educational diversity
- Why unresolved: The human evaluation only tested two narrow groups, leaving questions about how general population performance varies
- What evidence would resolve it: Expanded human trials including various age groups, educational levels, and cultural backgrounds taking the benchmark

## Limitations

- Lack of transparency regarding corpus composition and prompt templates, which are critical for reproducibility
- Embedding methodology and similarity thresholds used for dataset construction are only conceptually described
- Focus exclusively on Korean cultural context, limiting generalizability to other cultural benchmarks
- No experimental testing of whether increasing cultural data volume improves model performance

## Confidence

**High Confidence**: The comparative performance ranking of models (Clova X > Tiangong > GPT-3.5 Turbo on Korean cultural tasks) is well-supported by empirical results and consistent across multiple experiments. The finding that news-level tasks are easier than idiom or poetry tasks for all models is also robustly demonstrated.

**Medium Confidence**: The mechanism that Chain-of-Thought reasoning helps only when models have sufficient cultural background knowledge is supported by the data but could be influenced by other factors such as reasoning verbosity or task-specific characteristics. The claim about training corpus influence on cultural understanding is plausible but lacks direct evidence of corpus composition.

**Low Confidence**: The exact thresholds and conditions under which CoT reasoning degrades performance are not precisely specified, making it difficult to generalize these findings to other cultural contexts or model types.

## Next Checks

1. **Reproduce the idiom cloze test accuracy gap**: Using the same dataset, evaluate the same models (Clova X, Tiangong, GPT-3.5 Turbo) on the KorID dataset with zero-shot prompting to verify the reported 69.12% vs 45.7% performance difference between idiom and poetry tasks.

2. **Test CoT reasoning length sensitivity**: Conduct a controlled experiment varying Chain-of-Thought reasoning steps (1-2, 3-4, 5-6 sentences) on the KorPD poetry dataset with GPT-3.5 Turbo to confirm whether excessive reasoning steps degrade performance as claimed.

3. **Cross-cultural generalization test**: Apply the KULTURE Bench methodology to another cultural context (e.g., Chinese or Japanese) using models trained on native vs. non-native corpora to determine if the cultural corpus effect generalizes beyond Korean.