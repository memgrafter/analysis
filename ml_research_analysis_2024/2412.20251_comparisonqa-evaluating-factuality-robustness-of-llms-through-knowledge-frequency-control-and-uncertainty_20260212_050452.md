---
ver: rpa2
title: 'ComparisonQA: Evaluating Factuality Robustness of LLMs Through Knowledge Frequency
  Control and Uncertainty'
arxiv_id: '2412.20251'
source_url: https://arxiv.org/abs/2412.20251
tags:
- questions
- llms
- uncertainty
- knowledge
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COMPARISON QA, a large-scale benchmark with
  283K question pairs to evaluate the robustness of large language models (LLMs) on
  factual knowledge across different entity frequencies. The benchmark uses shared
  abstract questions with pairs of high-frequency and low-frequency entities to enable
  controlled comparisons.
---

# ComparisonQA: Evaluating Factuality Robustness of LLMs Through Knowledge Frequency Control and Uncertainty

## Quick Facts
- **arXiv ID**: 2412.20251
- **Source URL**: https://arxiv.org/abs/2412.20251
- **Reference count**: 30
- **Primary result**: Large-scale benchmark (283K question pairs) reveals LLMs' performance degradation on low-frequency factual knowledge and semantic shortcut usage

## Executive Summary
This paper introduces COMPARISON QA, a benchmark designed to evaluate the robustness of large language models (LLMs) on factual knowledge across different entity frequencies. The benchmark consists of 283K question pairs that share abstract questions but differ in entity frequency, enabling controlled comparisons. A two-round evaluation method combining correctness and uncertainty is proposed to measure LLM knowledge robustness and detect semantic shortcuts. The study reveals that LLMs, including GPT-4o, exhibit significant performance drops and increased uncertainty when answering questions about low-frequency knowledge. Additionally, the authors propose COMPARISON QA-Hard, an 81K subset of high-quality, shortcut-free questions that effectively tests LLM capabilities.

## Method Summary
The COMPARISON QA benchmark is constructed using a systematic approach that ensures question pairs share abstract questions but differ in entity frequency. The two-round evaluation method first assesses correctness across the entire dataset, then uses uncertainty measures to identify and filter questions susceptible to semantic shortcuts. This approach allows for the creation of COMPARISON QA-Hard, a subset of high-quality questions that maintain dataset size while improving evaluation quality. The benchmark employs knowledge frequency control through shared abstract questions with high-frequency and low-frequency entity pairs, enabling robust measurement of model performance across different knowledge types.

## Key Results
- LLMs show significant performance drops when answering questions about low-frequency knowledge compared to high-frequency knowledge
- Uncertainty proves more effective than correctness for identifying semantic shortcuts, maintaining dataset size while filtering low-quality content
- COMPARISON QA-Hard (81K subset) effectively tests LLM capabilities with high-quality, shortcut-free questions

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its controlled design that isolates knowledge frequency as a variable while maintaining consistent question structure. By using shared abstract questions with different entity frequencies, the evaluation can precisely measure how frequency affects model performance. The two-round evaluation method captures both correctness and uncertainty, providing a more comprehensive assessment of model capabilities than traditional accuracy metrics alone.

## Foundational Learning

**Knowledge Frequency Control**: Why needed - to isolate the impact of entity frequency on model performance. Quick check - verify that question pairs differ only in entity frequency while maintaining identical question structure.

**Uncertainty Quantification**: Why needed - to detect semantic shortcuts where models answer without full knowledge. Quick check - correlate uncertainty scores with known shortcut indicators.

**Two-Round Evaluation**: Why needed - to combine correctness and uncertainty for comprehensive assessment. Quick check - validate that filtered questions maintain challenging content while eliminating shortcuts.

## Architecture Onboarding

**Component Map**: Question Generation -> Frequency Analysis -> Two-Round Evaluation -> Subset Creation

**Critical Path**: Question pair creation with shared abstract questions -> Performance evaluation on high/low frequency pairs -> Uncertainty measurement -> Semantic shortcut filtering -> COMPARISON QA-Hard generation

**Design Tradeoffs**: Controlled frequency comparison vs. natural question diversity; comprehensive evaluation vs. computational cost of two-round assessment

**Failure Signatures**: Performance drop only on low-frequency questions indicates frequency dependency; consistent performance across frequencies suggests robust knowledge representation

**First 3 Experiments**:
1. Evaluate GPT-4o on full COMPARISON QA benchmark to establish baseline frequency performance
2. Apply two-round evaluation to identify semantic shortcuts in the dataset
3. Generate COMPARISON QA-Hard subset and validate its quality through human evaluation

## Open Questions the Paper Calls Out
None specified in the provided material.

## Limitations
- The underlying causes of frequency-dependent performance remain partially unexplored
- Semantic shortcut detection relies on proxy measures that may not perfectly align with actual shortcut usage
- Two-round evaluation methodology requires validation across diverse model families
- Role of training data distribution versus model architecture limitations is not fully disentangled

## Confidence

**High Confidence**: Core finding that LLMs show frequency-dependent performance variation, supported by robust statistical analysis across 283K question pairs.

**Medium Confidence**: Effectiveness of two-round evaluation method for identifying semantic shortcuts, as relationship between uncertainty and shortcut detection warrants further empirical validation.

**Medium Confidence**: Quality of COMPARISON QA-Hard subset, as filtering criteria, while rigorous, have not been independently verified across diverse evaluation scenarios.

## Next Checks

1. Conduct cross-model validation by testing COMPARISON QA benchmark on broader range of LLM architectures (including open-source models) to verify universality of observed frequency effects.

2. Perform ablation studies to isolate contribution of different knowledge frequency metrics to performance degradation, testing whether token frequency, entity co-occurrence patterns, or other factors drive observed effects.

3. Design human evaluation studies to validate semantic shortcut detection mechanism, comparing model uncertainty-based predictions against expert assessments of whether questions can be answered without full knowledge of specific entities involved.