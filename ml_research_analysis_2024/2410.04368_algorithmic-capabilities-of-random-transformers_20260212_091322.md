---
ver: rpa2
title: Algorithmic Capabilities of Random Transformers
arxiv_id: '2410.04368'
source_url: https://arxiv.org/abs/2410.04368
tags:
- random
- trained
- tasks
- task
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Randomly initialized transformers can perform meaningful algorithmic
  tasks when only their embedding layers are optimized. The key insight is that embedding-only
  training steers the model's hidden representations into low-dimensional subspaces
  where the target computation is already implemented.
---

# Algorithmic Capabilities of Random Transformers

## Quick Facts
- arXiv ID: 2410.04368
- Source URL: https://arxiv.org/abs/2410.04368
- Authors: Ziqian Zhong; Jacob Andreas
- Reference count: 40
- Key outcome: Randomly initialized transformers can perform meaningful algorithmic tasks when only their embedding layers are optimized through a "subspace selection" phenomenon

## Executive Summary
This paper demonstrates that randomly initialized transformers can perform various algorithmic tasks when only their embedding and unembedding layers are trained, while all intermediate layers remain frozen. The key insight is that embedding-only training steers the model's hidden representations into low-dimensional subspaces where the target computation is already implemented by the random architecture. This approach achieves perfect accuracy on tasks like modular arithmetic, associative recall, decimal addition, and parenthesis balancing, matching or exceeding the performance of fully trained recurrent models. However, this method is less effective for tasks requiring high-dimensional computation or large-scale memorization.

## Method Summary
The method involves training only the embedding (E) and unembedding (U) layers of a randomly initialized transformer, while keeping all intermediate layers (F) frozen. The transformer processes inputs through the embedding layer to hidden representations, which are then transformed by frozen intermediate layers, and finally mapped to outputs through the unembedding layer. The training procedure uses AdamW optimizer on synthetic tasks including modular arithmetic, needle-in-a-haystack associative recall, decimal addition, parenthesis balancing, and TinyStories language modeling. The approach is evaluated through test accuracy, cross-entropy loss, and explained variance from principal components and neurons.

## Key Results
- Random transformers with only embedding layers trained achieve perfect accuracy on modular arithmetic, associative recall, decimal addition, and parenthesis balancing tasks
- Performance matches or exceeds fully trained recurrent models (LSTMs) on these algorithmic tasks
- Language modeling on TinyStories is possible with embedding-only training, but requires much wider models (width 4096) compared to fully trained transformers (width 256)
- Learned embeddings form geometric structures (like circles) that align with the random transformer's internal computation patterns

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Embedding-only training steers hidden representations into low-dimensional subspaces where the target computation is already implemented by the random transformer.
- **Mechanism**: The transformer's randomly initialized layers already contain certain computational primitives. By optimizing only the embedding and unembedding layers, the model learns to map inputs and outputs into a subspace where these primitives can be composed to perform the desired task.
- **Core assumption**: The random initialization of transformer layers contains the necessary computational primitives for the target tasks.
- **Evidence anchors**: The abstract states that embedding-only training steers representations into subspaces where target computation is implemented; the paper shows that learned embeddings form circles in low-dimensional subspaces, similar to fully trained models.

### Mechanism 2
- **Claim**: Random transformers can implement algorithmic tasks by using structured embeddings that align with their internal computation patterns.
- **Mechanism**: The learned embeddings form specific geometric structures (like circles for modular arithmetic) that match the random transformer's internal computation patterns, allowing the model to perform the task using its existing architecture.
- **Core assumption**: The random transformer's internal computation patterns are sufficiently structured and regular to support meaningful algorithmic tasks when paired with appropriate embeddings.
- **Evidence anchors**: The paper observes that learned embeddings form circles in low-dimensional subspaces, and that random transformers exhibit interpretable attention patterns similar to induction heads previously described in fully trained models.

### Mechanism 3
- **Claim**: The transformer architecture has an inherent inductive bias that makes it particularly suited for certain algorithmic tasks, even before training.
- **Mechanism**: The transformer's attention mechanism and feed-forward layers create a computational landscape where certain tasks (like modular arithmetic and associative recall) have natural solutions that can be accessed through appropriate embeddings.
- **Core assumption**: The transformer architecture itself, independent of initialization, contains structural properties that make certain computations easier to perform.
- **Evidence anchors**: The abstract notes that some algorithmic capabilities are present in transformers even before training; the paper demonstrates that random transformers can perform a wide range of meaningful algorithmic tasks.

## Foundational Learning

- **Concept: Low-dimensional subspace selection**
  - Why needed here: Understanding how embedding-only training can steer representations into subspaces where computation is already possible requires grasping the concept of dimensionality reduction and subspace selection in neural networks.
  - Quick check question: Why would constraining representations to a low-dimensional subspace make certain computations easier for a randomly initialized model?

- **Concept: Geometric structure of embeddings**
  - Why needed here: The paper shows that learned embeddings form specific geometric patterns (like circles) that align with internal computations, so understanding how embeddings can encode information geometrically is crucial.
  - Quick check question: How might circular embeddings help with modular arithmetic in a way that linear embeddings cannot?

- **Concept: Attention mechanism as computation primitive**
  - Why needed here: The transformer's attention mechanism is central to how it performs computations, and understanding it as a computational primitive is key to grasping how random transformers can implement algorithms.
  - Quick check question: How does the attention mechanism enable the transformer to perform associative recall through induction heads?

## Architecture Onboarding

- **Component map**:
  Embedding layer (E) -> Frozen intermediate layers (F) -> Unembedding layer (U)

- **Critical path**:
  1. Input tokens → Embedding layer (E) → hidden representation
  2. Hidden representation → frozen intermediate layers (F) → transformed representation
  3. Transformed representation → Unembedding layer (U) → output distribution

- **Design tradeoffs**:
  - Width vs performance: Wider models can capture more complex computations but require more parameters
  - Depth vs expressivity: Additional layers may not help for embedding-only training since intermediate layers are frozen
  - Task complexity vs success rate: Simple algorithmic tasks work well; complex language modeling requires much wider models

- **Failure signatures**:
  - Poor performance on tasks requiring high-dimensional computation
  - Inability to learn arbitrary input-output mappings (memorization tasks)
  - Lack of structured attention patterns in trained models

- **First 3 experiments**:
  1. **Modular arithmetic**: Train a random transformer (width 1024) on (a + b) mod 199 to verify it can achieve perfect accuracy
  2. **Needle-in-a-haystack**: Test associative recall with markers and values to confirm attention patterns resemble induction heads
  3. **Decimal addition**: Evaluate multi-digit arithmetic to check if learned embeddings show structured patterns for carrying operations

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does embedding-only training in random transformers generalize to more complex algorithmic tasks beyond the four studied (modular addition, needle-in-a-haystack, decimal addition, and parenthesis balancing)?
- **Basis in paper**: The paper shows embedding-only training works for simple algorithmic tasks but acknowledges it may not scale to tasks requiring high-dimensional computation.
- **Why unresolved**: The study focuses on tasks with low intermediate complexity that can be solved in low-dimensional subspaces. Tasks requiring higher-dimensional representations were not explored.
- **What evidence would resolve it**: Experiments showing embedding-only training performance on tasks with higher intermediate complexity, such as sorting, graph algorithms, or more complex arithmetic operations, would determine the scalability limits.

### Open Question 2
- **Question**: How does the performance of embedding-only training in random transformers compare to other model architectures like Mamba or state-space models?
- **Basis in paper**: The paper focuses on transformers and mentions related linear attention and state-space models but does not compare embedding-only training across different architectures.
- **Why unresolved**: The study only examines transformer models, leaving open whether the subspace selection phenomenon is unique to transformers or applies to other architectures as well.
- **What evidence would resolve it**: Direct comparison of embedding-only training performance between transformers, Mamba, and state-space models on the same set of tasks would reveal whether the observed behavior is architecture-specific.

### Open Question 3
- **Question**: What is the relationship between the mechanisms discovered in embedding-only training and those used by fully trained models during normal training?
- **Basis in paper**: The paper notes that it leaves open whether mechanisms in embedding-only models evolve gradually into fully trained forms or use entirely different pathways.
- **Why unresolved**: The study focuses on random transformers with only embedding layers trained and does not track how these mechanisms change during full training.
- **What evidence would resolve it**: Mechanistic interpretability analysis tracking how the internal representations and attention patterns evolve from initialization through embedding-only training to full training would reveal the relationship between these mechanisms.

## Limitations
- The findings are primarily based on synthetic tasks and a relatively small language modeling dataset (TinyStories), leaving real-world task performance unexplored
- Performance comparisons rely heavily on simpler architectures like LSTMs rather than more sophisticated trained transformer models
- The mechanism explanations are largely post-hoc interpretations of observed phenomena rather than definitively proven causal explanations

## Confidence
- **High confidence**: The empirical observation that random transformers can perform modular arithmetic, associative recall, and simple arithmetic tasks with embedding-only training
- **Medium confidence**: The interpretation that low-dimensional subspace selection is the primary mechanism enabling these capabilities
- **Medium confidence**: The claim that this approach is less effective for high-dimensional computation and large-scale memorization

## Next Checks
1. **Benchmark against trained transformers**: Compare random transformer performance on these algorithmic tasks against fully trained transformers of similar size to quantify the performance gap and validate claims about relative effectiveness

2. **Scale complexity systematically**: Create a series of algorithmic tasks with varying dimensional complexity (starting from 1D modular arithmetic up to high-dimensional arithmetic) to empirically map the boundary where random transformer performance degrades significantly

3. **Ablation on embedding dimensionality**: Systematically vary the dimensionality of the embedding space while keeping the random transformer fixed to determine whether performance scales with embedding dimensionality or if there's a threshold effect where performance suddenly drops