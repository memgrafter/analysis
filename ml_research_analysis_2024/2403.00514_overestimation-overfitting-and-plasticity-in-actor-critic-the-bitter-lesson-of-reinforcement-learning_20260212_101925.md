---
ver: rpa2
title: 'Overestimation, Overfitting, and Plasticity in Actor-Critic: the Bitter Lesson
  of Reinforcement Learning'
arxiv_id: '2403.00514'
source_url: https://arxiv.org/abs/2403.00514
tags:
- learning
- regularization
- plasticity
- norm
- overestimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper conducts an extensive empirical study of over 60 regularized
  off-policy reinforcement learning agents across 14 diverse tasks from two simulation
  benchmarks. The authors examine the effects of three types of regularization: critic
  regularization to reduce overestimation, network regularization to prevent overfitting,
  and plasticity regularization to mitigate plasticity loss.'
---

# Overestimation, Overfitting, and Plasticity in Actor-Critic: the Bitter Lesson of Reinforcement Learning

## Quick Facts
- arXiv ID: 2403.00514
- Source URL: https://arxiv.org/abs/2403.00514
- Authors: Michal Nauman; Michał Bortkiewicz; Piotr Miłoś; Tomasz Trzciński; Mateusz Ostaszewski; Marek Cygan
- Reference count: 40
- Primary result: General network regularization techniques, particularly layer normalization, consistently outperform domain-specific critic regularization methods across reinforcement learning benchmarks.

## Executive Summary
This paper conducts an extensive empirical study of over 60 regularized off-policy reinforcement learning agents across 14 diverse tasks from two simulation benchmarks. The authors examine the effects of three types of regularization: critic regularization to reduce overestimation, network regularization to prevent overfitting, and plasticity regularization to mitigate plasticity loss. Their key finding is that general network regularization techniques, particularly layer normalization, consistently outperform domain-specific critic regularization methods across benchmarks and tasks. Specifically, appropriately regularized soft actor-critic agents achieve state-of-the-art performance on challenging dog locomotion tasks that were previously solved mainly through model-based approaches. The study also reveals complex interactions between overestimation, overfitting, and plasticity metrics, showing that interventions targeting one issue can significantly impact metrics associated with other issues, highlighting the multifaceted nature of learning problems in reinforcement learning.

## Method Summary
The study implements 64 model designs (all combinations of 12 SAC design choices from recent literature, ensuring no same-group combinations) across 14 tasks from two benchmarks (DeepMind Control Suite and MetaWorld) under two replay ratio regimes (RR=2 and RR=16). Each configuration is evaluated on 10 seeds. The analysis includes first-order, second-order, and third-order marginalization of results, as well as correlation analysis between metrics and performance. The study tracks training performance, overestimation proxies (state-action critic approximation error), overfitting proxies (ratio of TD error on validation to training set), and plasticity loss proxies (rank of penultimate layer representations, dormant neurons, L2 norm of weights, and gradient norm).

## Key Results
- General network regularization techniques, particularly layer normalization, consistently outperform RL-specific critic regularization methods across benchmarks and tasks
- Appropriately regularized SAC agents achieve state-of-the-art performance on challenging dog locomotion tasks previously dominated by model-based approaches
- Complex interactions exist between overestimation, overfitting, and plasticity metrics, where interventions targeting one issue significantly impact metrics associated with other issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layer normalization effectively reduces overestimation and improves performance more than critic-specific techniques.
- Mechanism: Layer normalization stabilizes gradient flow during training, preventing large updates that can cause Q-value overestimation. This stabilizes the critic's learning dynamics, reducing the bias toward overestimation that occurs when policies exploit potential model errors.
- Core assumption: The gradient instability that leads to overestimation can be mitigated through normalization techniques that are general to neural networks, not specific to RL.
- Evidence anchors:
  - [abstract] "Notably, network regularization enables agents to find effective policies on tasks previously impossible for model-free agents, such as those in the dog domain. Our findings also show that layer normalisation is more effective in reducing overestimation than techniques specifically designed for mitigating Q-value overestimation in critic networks."
  - [section 4.4] "Techniques like layer or spectral norm and resets are particularly effective in mitigating overestimation also compared to methods specifically designed for that purpose."
  - [corpus] Weak evidence - the corpus neighbors do not directly address the effectiveness of layer normalization for overestimation reduction.
- Break condition: If the network architecture or learning dynamics are such that normalization layers do not effectively stabilize gradients, the overestimation reduction benefit would disappear.

### Mechanism 2
- Claim: Full-parameter resets prevent plasticity loss and maintain learning capacity in high replay ratio regimes.
- Mechanism: Periodically resetting the entire network parameters prevents the network from entering a state where it cannot learn new information. This is particularly important in high replay ratio regimes where the agent updates its parameters many times per environment step, potentially leading to rank collapse or dead neurons.
- Core assumption: Plasticity loss is a significant issue in high replay ratio regimes that can be effectively mitigated through periodic network resets.
- Evidence anchors:
  - [abstract] "The study also reveals complex interactions between overestimation, overfitting, and plasticity metrics, showing that interventions targeting one issue can significantly impact metrics associated with other issues"
  - [section 4.1] "Periodical network resetting is the most robust intervention across two benchmarks in a high replay ratio regime, and highly surpasses other plasticity regularization techniques in both robustness and performance."
  - [corpus] Weak evidence - the corpus neighbors mention plasticity loss but do not provide strong evidence for the effectiveness of full-parameter resets.
- Break condition: If the environment is simple enough that plasticity loss is not a significant issue, or if the reset frequency is not properly tuned, the benefits of full-parameter resets would diminish.

### Mechanism 3
- Claim: Network regularization techniques have more general applicability and robustness across different task types than RL-specific methods.
- Mechanism: General network regularization methods like layer normalization and spectral normalization stabilize learning by addressing fundamental issues in gradient-based optimization that are common across different types of tasks, whereas RL-specific methods like Clipped Double Q-learning are tailored to specific problems in reinforcement learning.
- Core assumption: The fundamental issues that affect learning in neural networks (like gradient instability, overfitting) are more universally applicable than the specific problems in reinforcement learning (like overestimation).
- Evidence anchors:
  - [abstract] "Our main result is a bitter lesson: across varied tasks, general neural network regularizers significantly outperform most RL-specific algorithmic improvements in terms of agent performance."
  - [section 4.1] "Critic regularization methods exhibit limited effectiveness in enhancing performance. When using network or plasticity regularization, critic regularization leads to reduced performance."
  - [corpus] Moderate evidence - the corpus includes papers on plasticity loss and overestimation, suggesting these are recognized issues, but does not directly compare the effectiveness of general vs. RL-specific methods.
- Break condition: If the specific problem that an RL-specific method addresses is the dominant issue in a particular task or environment, then the general regularization methods may not be as effective.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The paper is about reinforcement learning, which is fundamentally about learning policies in MDPs.
  - Quick check question: What are the components of an MDP tuple (S, A, r, p, γ)?

- Concept: Actor-Critic methods
  - Why needed here: The paper focuses on actor-critic algorithms, specifically Soft Actor-Critic (SAC), which are a type of reinforcement learning algorithm.
  - Quick check question: How do actor-critic methods differ from value-based methods like Q-learning?

- Concept: Function approximation and generalization
  - Why needed here: The paper discusses overfitting and the use of neural network regularization techniques, which are related to how function approximators generalize from training data.
  - Quick check question: What is the difference between overfitting and underfitting in the context of function approximation?

## Architecture Onboarding

- Component map:
  - Soft Actor-Critic (SAC) algorithm with various regularization techniques
  - Critic network with two Q-value estimates (for Clipped Double Q-learning)
  - Actor network for policy representation
  - Replay buffer for storing past experiences
  - Regularization modules: Layer Normalization, Spectral Normalization, Weight Decay, Full-parameter Resets, etc.

- Critical path:
  1. Initialize SAC agent with chosen regularization techniques
  2. Collect experience in environment and store in replay buffer
  3. Sample batch from replay buffer and compute losses with regularization
  4. Update actor and critic networks
  5. Periodically evaluate policy and track metrics (overestimation, overfitting, plasticity)
  6. Adjust regularization if performance plateaus or degrades

- Design tradeoffs:
  - General vs. specific regularization: General techniques like layer normalization may be more robust but less targeted than RL-specific methods like Clipped Double Q-learning
  - Plasticity vs. stability: Techniques that increase plasticity (like resets) may lead to more unstable learning but prevent plateaus
  - Computational cost vs. performance: Some regularization techniques (like spectral normalization) may add computational overhead but improve performance

- Failure signatures:
  - High overestimation metrics despite using critic regularization techniques
  - Plateauing performance or degradation in high replay ratio regimes
  - Large variance in performance across different seeds or tasks
  - Exploding or vanishing gradients during training

- First 3 experiments:
  1. Compare SAC with and without layer normalization on a simple locomotion task to verify the overestimation reduction effect
  2. Test SAC with different replay ratios (2 and 16) to observe the impact of plasticity regularization techniques like full-parameter resets
  3. Evaluate SAC with a combination of layer normalization and full-parameter resets on a challenging task (like dog-trot) to verify the synergistic effect

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific mechanisms by which layer normalization reduces overestimation in reinforcement learning agents?
- Basis in paper: [explicit] The authors observe that layer normalization is more effective in reducing overestimation than techniques specifically designed for mitigating Q-value overestimation in critic networks.
- Why unresolved: The paper demonstrates the effectiveness of layer normalization but does not provide a detailed explanation of the underlying mechanisms.
- What evidence would resolve it: Experiments isolating the effects of layer normalization on different components of the reinforcement learning pipeline, or theoretical analysis explaining why layer normalization reduces overestimation.

### Open Question 2
- Question: How do the complex interactions between overestimation, overfitting, and plasticity metrics vary across different task complexities and domains?
- Basis in paper: [explicit] The authors find that interventions aimed at one type of issue (e.g., full-parameter resets) significantly affect metrics associated with other issues (e.g., overestimation and overfitting) more than interventions specifically designed for those other issues.
- Why unresolved: The paper provides initial observations on these interactions but does not explore how they change with task difficulty or domain type.
- What evidence would resolve it: Systematic experiments varying task complexity and domain type while measuring the interactions between overestimation, overfitting, and plasticity metrics.

### Open Question 3
- Question: What are the optimal combinations of network and plasticity regularization techniques for different reinforcement learning benchmarks and task types?
- Basis in paper: [explicit] The authors find that the effectiveness of regularization techniques varies significantly across different benchmarks (e.g., layer normalization is most effective for DMC, while spectral normalization is more effective for MW).
- Why unresolved: The paper identifies effective combinations for specific benchmarks but does not provide a comprehensive analysis of optimal combinations across all possible task types and domains.
- What evidence would resolve it: Extensive experiments testing various combinations of network and plasticity regularization techniques across a wide range of benchmarks and task types to determine optimal configurations.

## Limitations

- The study focuses on actor-critic methods and may not generalize to other reinforcement learning approaches
- The computational cost of testing 64 model designs across 14 tasks with 10 seeds each limits the scope of experiments that can be run
- The paper does not provide detailed theoretical analysis of why general regularization techniques outperform RL-specific methods

## Confidence

- High confidence: Empirical finding that general network regularization (particularly layer normalization) outperforms RL-specific critic regularization techniques across benchmarks
- Medium confidence: Mechanism explanation linking normalization to overestimation reduction
- Medium confidence: Effectiveness of full-parameter resets for plasticity loss mitigation, pending better understanding of optimal implementation details

## Next Checks

1. Conduct ablation studies on the layer normalization placement within the critic network to verify whether its overestimation reduction effect is consistent across different architectural positions
2. Test the full-parameter reset technique with varying reset frequencies and learning rates to identify optimal parameters and verify the robustness of the approach
3. Implement theoretical analysis or additional experiments to better understand the relationship between normalization layers, gradient stability, and overestimation reduction in actor-critic methods