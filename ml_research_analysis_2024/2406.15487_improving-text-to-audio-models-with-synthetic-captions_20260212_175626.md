---
ver: rpa2
title: Improving Text-To-Audio Models with Synthetic Captions
arxiv_id: '2406.15487'
source_url: https://arxiv.org/abs/2406.15487
tags:
- audio
- captions
- pretraining
- af-audioset
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of obtaining high-quality training
  data, especially captions, for text-to-audio models. The authors propose an audio
  captioning pipeline that uses a pre-trained audio language model to synthesize accurate
  and diverse captions for audio at scale.
---

# Improving Text-To-Audio Models with Synthetic Captions

## Quick Facts
- arXiv ID: 2406.15487
- Source URL: https://arxiv.org/abs/2406.15487
- Reference count: 0
- This paper proposes using synthetic captions generated by an audio language model to improve text-to-audio models, achieving state-of-the-art results on AudioCaps and MusicCaps.

## Executive Summary
This paper addresses the challenge of obtaining high-quality training data for text-to-audio models by proposing a pipeline that uses a pre-trained audio language model to synthesize captions at scale. The authors create AF-AudioSet, a synthetic caption dataset for AudioSet, and demonstrate that pretraining text-to-audio models on these captions significantly improves generation quality. Through systematic evaluations, they show consistent improvements over non-pretrained models, with optimal performance achieved at different CLAP similarity thresholds for different datasets.

## Method Summary
The method uses a pre-trained audio language model (Audio Flamingo) to generate multiple captions for each audio clip, then filters these captions using CLAP similarity scores to ensure alignment with the audio content. The filtered captions form AF-AudioSet, which is used to pretrain text-to-audio models (Tango with either FLAN-T5 or CLAP text encoder) before fine-tuning on real caption datasets like AudioCaps and MusicCaps. The pretraining process uses batch size 128 for 100K iterations, followed by fine-tuning with batch size 48 for 40 epochs.

## Key Results
- Pretraining on AF-AudioSet with τ=0.45 for AudioCaps and τ=0.35 for MusicCaps achieves state-of-the-art performance
- Combining synthetic and real data during pretraining leads to further improvements in generation quality
- Tango-CLAP outperforms Tango across model scales, with the best results at these CLAP thresholds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Audio language models generate captions that are more coherent with audio content than text-only models.
- Mechanism: The audio language model is trained on paired audio-text data, so its captioning output is grounded in the actual audio signal rather than relying solely on language patterns.
- Core assumption: Audio language models can generalize across diverse audio types when trained on sufficient and varied data.
- Evidence anchors:
  - [abstract] "Although prior methods have leveraged text-only language models to augment and improve captions, such methods have limitations related to scale and coherence between audio and captions."
  - [section] "Our approach uses a pretrained audio language model to automatically caption audio in the wild... Our approach does not require annotation nor metadata associated with audio and, as such, it can be easily scaled-up."
- Break condition: If the audio language model fails to generalize beyond its training distribution, generated captions will be inaccurate or irrelevant.

### Mechanism 2
- Claim: CLAP similarity filtering selects captions that are more semantically aligned with the corresponding audio.
- Mechanism: CLAP embeddings map both audio and text into a shared semantic space; high cosine similarity indicates good alignment between caption and audio.
- Core assumption: CLAP embeddings accurately capture the semantic relationship between audio and text.
- Evidence anchors:
  - [section] "To promote the accuracy of the generated captions, we filter them based on their CLAP similarities with the corresponding audios [14]."
  - [section] "We use the CLAP similarity [14] between the caption and the audio to rank and filter the synthetic captions."
- Break condition: If CLAP embeddings are not well-calibrated or biased toward certain audio types, filtering may discard useful captions or retain poor ones.

### Mechanism 3
- Claim: Pretraining on large synthetic caption datasets improves text-to-audio model performance more than training only on small real caption datasets.
- Mechanism: Large-scale pretraining exposes the model to a broader distribution of audio concepts and linguistic patterns, improving generalization before fine-tuning on the target task.
- Core assumption: The synthetic captions are of sufficient quality and diversity to provide meaningful learning signals during pretraining.
- Evidence anchors:
  - [abstract] "Through systematic evaluations on AudioCaps and MusicCaps, we find leveraging our pipeline and synthetic captions leads to significant improvements on audio generation quality, achieving a new state-of-the-art."
  - [section] "In consonance with these findings, this paper shows that pretraining on high quality datasets, even if they are synthetic, can drastically improve the quality of text-to-audio models."
- Break condition: If synthetic captions contain systematic errors or biases, the pretrained model may learn incorrect associations that hurt performance.

## Foundational Learning

- Concept: Diffusion models for generative tasks
  - Why needed here: The text-to-audio model uses latent diffusion to generate audio conditioned on text embeddings.
  - Quick check question: What are the two main components of a latent diffusion model in this context?

- Concept: Multimodal embeddings (CLAP)
  - Why needed here: CLAP provides a shared embedding space to measure semantic alignment between audio and text captions.
  - Quick check question: How is CLAP similarity computed between a caption and its audio?

- Concept: Pretraining vs. fine-tuning strategy
  - Why needed here: The paper uses pretraining on synthetic captions followed by fine-tuning on real caption datasets to maximize performance.
  - Quick check question: Why might pretraining on synthetic data still be beneficial even if the real data is small?

## Architecture Onboarding

- Component map: Audio Flamingo chat model → caption generation → CLAP model → similarity filtering → Tango (text encoder + latent diffusion decoder) → HiFi-GAN → waveform synthesis from mel spectrogram

- Critical path:
  1. Generate captions with Audio Flamingo
  2. Filter captions using CLAP similarity threshold
  3. Pretrain Tango on filtered synthetic captions
  4. Fine-tune Tango on real caption datasets (AudioCaps/MusicCaps)
  5. Evaluate using FD, FAD, IS, CLAP similarity

- Design tradeoffs:
  - Larger CLAP threshold τ → higher caption quality, smaller dataset size
  - Larger Tango model size → better generation quality, higher compute cost
  - Using CLAP text encoder vs. FLAN-T5 → affects conditioning strategy and performance

- Failure signatures:
  - Low IS or high FD after pretraining → synthetic captions may be too noisy or dataset too small
  - Degradation when mixing real and synthetic data → quality mismatch between datasets
  - No improvement over baseline → filtering threshold too high (too little data) or too low (too noisy)

- First 3 experiments:
  1. Generate captions with Audio Flamingo and inspect random samples to assess quality and diversity.
  2. Vary τ (e.g., 0.35, 0.45, 0.50) and check dataset sizes and sample captions to understand quality-size tradeoff.
  3. Pretrain Tango with τ=0.45 on AudioCaps, then fine-tune and evaluate on AudioCaps test set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal quality-size trade-off (CLAP similarity threshold τ) for synthetic captions across different text-to-audio tasks and model architectures?
- Basis in paper: [explicit] The paper investigates different τ values (0.35, 0.4, 0.45, 0.5) and finds τ=0.45 optimal for AudioCaps and τ=0.35 optimal for MusicCaps, but questions remain about generalizability.
- Why unresolved: The optimal τ appears to depend on the specific task and dataset, suggesting a need for task-specific optimization.
- What evidence would resolve it: Systematic experiments across diverse text-to-audio tasks and model architectures to establish general principles or task-specific guidelines for choosing τ.

### Open Question 2
- Question: How can the diversity and accuracy of synthetic captions be further improved beyond the current pipeline using Audio Flamingo and CLAP similarity filtering?
- Basis in paper: [explicit] The paper discusses challenges in ensuring caption diversity and accuracy, and suggests investigating better synthesis pipelines as a future direction.
- Why unresolved: The current approach relies on a single audio language model and similarity filtering, which may not capture all aspects of caption quality.
- What evidence would resolve it: Development and evaluation of alternative methods for generating and filtering synthetic captions, potentially incorporating multiple models or additional quality metrics.

### Open Question 3
- Question: What are the best pretraining strategies for leveraging synthetic captions to improve text-to-audio models?
- Basis in paper: [explicit] The paper investigates different pretraining datasets and combinations, finding that mixing synthetic and real captions can lead to further improvements, but questions remain about optimal strategies.
- Why unresolved: The study focuses on a limited set of pretraining approaches, and the impact of factors like pretraining duration, learning rate schedules, and curriculum learning is not explored.
- What evidence would resolve it: Systematic experiments varying pretraining strategies, including different dataset combinations, pretraining durations, and optimization techniques, to identify the most effective approaches.

## Limitations
- The reliance on Audio Flamingo model for caption generation may introduce biases based on its training data
- The paper doesn't fully explore how caption quality affects downstream performance beyond CLAP similarity filtering
- The CLAP similarity threshold represents a critical hyperparameter that wasn't extensively tuned across different model scales and datasets

## Confidence

- **High**: CLAP similarity filtering effectively selects higher-quality captions
- **Medium-High**: Pretraining on synthetic captions improves text-to-audio generation quality
- **Medium**: Synthetic and real data combination yields further improvements
- **Medium**: Tango-CLAP outperforms Tango across model scales

## Next Checks

1. **Cross-dataset robustness test**: Apply the AF-AudioSet pretraining approach to a completely different audio generation task (e.g., environmental sound synthesis) to verify generalization beyond the AudioCaps/MusicCaps domains.

2. **Ablation on caption quality**: Generate captions using both the Audio Flamingo model and a text-only language model, then compare the quality of pretraining datasets and downstream performance to isolate the benefit of audio-grounded captions.

3. **Temporal coherence analysis**: Implement a temporal alignment check between generated captions and audio content (e.g., using segment-level CLAP similarity) to quantify how temporal misalignment might affect pretraining effectiveness and identify potential improvements to the caption generation pipeline.