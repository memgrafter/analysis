---
ver: rpa2
title: 'Leveraging Audio and Text Modalities in Mental Health: A Study of LLMs Performance'
arxiv_id: '2412.10417'
source_url: https://arxiv.org/abs/2412.10417
tags:
- audio
- text
- gemini
- prompt
- depression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the use of Large Language Models (LLMs) for
  detecting depression and PTSD using text and audio modalities. Using the E-DAIC
  dataset, we compare text and audio modalities, and investigate the integration of
  both to enhance diagnostic accuracy.
---

# Leveraging Audio and Text Modalities in Mental Health: A Study of LLMs Performance

## Quick Facts
- arXiv ID: 2412.10417
- Source URL: https://arxiv.org/abs/2412.10417
- Reference count: 40
- Primary result: Gemini 1.5 Pro achieves 77.4% BA and 0.67 F1 in depression classification using combined text+audio modality, outperforming single-modality inputs.

## Executive Summary
This study investigates the effectiveness of large language models (LLMs) for detecting depression and PTSD using text and audio modalities from the E-DAIC dataset. The research compares single and multimodal approaches, finding that combining text and audio generally improves classification performance, with Gemini 1.5 Pro achieving the highest scores in depression detection. The study introduces custom metrics (Modal Superiority Score and Disagreement Resolution Score) to evaluate modality integration and uses zero-shot inference without fine-tuning. Results suggest that while text remains the most potent single modality, multimodal fusion offers measurable gains, particularly for depression classification tasks.

## Method Summary
The study uses the E-DAIC dataset (275 samples) containing clinical interviews for depression assessment. Audio files are transcribed using Whisper Large-V3, creating parallel text and audio datasets. Multiple LLMs (Gemini 1.5 Pro, GPT-4o mini, Deepseek_R1) are evaluated using zero-shot and few-shot inference without fine-tuning. Custom prompts are formulated for binary, severity, multiclass, and multi-label classification tasks. Performance is measured using Balanced Accuracy (BA), F1-score, MAE, and novel metrics (Modal Superiority Score, Disagreement Resolution Score). The study compares modality performance and investigates the impact of prompt engineering on classification accuracy.

## Key Results
- Gemini 1.5 Pro achieves highest performance with combined text+audio modality (BA: 77.4%, F1: 0.67) for depression classification.
- Text modality outperforms audio modality in most configurations for mental health classification tasks.
- Multimodal integration (text + audio) improves classification performance over single modalities, with Gemini 2.5 Pro showing 3% improvement over text-only and 2.3% over audio-only inputs.

## Why This Works (Mechanism)

### Mechanism 1
Text modality outperforms audio modality for mental health classification tasks in most configurations. Large language models process text directly using their training on language patterns, allowing them to extract subtle semantic and linguistic cues that indicate mental health conditions. Audio input requires either transcription or direct audio processing, both of which may lose or distort critical information before it reaches the model. Text contains sufficient and reliable signals for mental health assessment, and the LLMs are better at interpreting text than raw or transcribed audio.

### Mechanism 2
Multimodal integration (text + audio) improves classification performance over single modalities. Combining modalities leverages complementary information: text captures explicit verbal content, while audio captures paralinguistic cues such as tone, pace, and emotion. Models that support multimodal input can fuse these signals for richer representation. Both modalities carry non-redundant, complementary information about mental health states.

### Mechanism 3
Prompt engineering significantly influences LLM performance on mental health classification. Well-crafted prompts guide the model to focus on relevant features, constrain output formats, and reduce ambiguity, leading to more accurate and consistent predictions. Different prompt structures (e.g., binary vs. severity classification) elicit different model behaviors. LLMs are sensitive to prompt wording and structure, and small changes can lead to meaningful performance differences.

## Foundational Learning

- **Concept**: Balanced Accuracy (BA) as evaluation metric
  - Why needed here: The dataset has imbalanced classes (e.g., more non-depressed than depressed cases), so BA fairly reflects model performance across all classes.
  - Quick check question: What is the formula for Balanced Accuracy, and why is it preferred over simple accuracy in imbalanced datasets?

- **Concept**: Zero-shot vs. Few-shot learning
  - Why needed here: The study evaluates models without task-specific fine-tuning (zero-shot) and with minimal examples (few-shot) to assess their ability to generalize from prior training to new tasks without extensive retraining.
  - Quick check question: What is the difference between zero-shot and few-shot learning in the context of LLM evaluation?

- **Concept**: Multimodal fusion
  - Why needed here: The study investigates combining text and audio modalities to enhance diagnostic accuracy, requiring understanding of how different data types can be integrated for improved performance.
  - Quick check question: How does multimodal fusion differ from simple concatenation of features from different modalities?

## Architecture Onboarding

### Component Map
Text/audio data -> Whisper transcription -> LLM inference -> Output parsing -> Performance metrics

### Critical Path
E-DAIC dataset (audio + transcript) → Whisper Large-V3 transcription → LLM zero-shot inference → Custom parsers → Metric calculation (BA, F1, MSS, DRS)

### Design Tradeoffs
- Zero-shot inference avoids fine-tuning costs but may limit performance compared to specialized models
- Whisper transcription enables text processing but introduces potential errors from audio-to-text conversion
- Custom metrics provide task-specific evaluation but may not align with clinical standards

### Failure Signatures
- Inconsistent model outputs requiring robust parsing logic
- Transcription errors from Whisper affecting modality comparison validity
- Limited dataset size potentially causing overfitting or unreliable generalization

### 3 First Experiments
1. Validate Whisper transcriptions against reference texts to ensure audio-to-text quality
2. Test LLM outputs with different prompt formulations to assess prompt sensitivity
3. Compare modality performance using larger, more diverse mental health datasets

## Open Questions the Paper Calls Out

### Open Question 1
Does the combination of text and audio modalities consistently improve diagnostic accuracy across different mental health conditions, or is this benefit limited to specific conditions like depression? The study found that combining text and audio modalities generally improved performance metrics for depression classification, but the effectiveness for PTSD severity classification was less clear. Further studies with larger, more diverse datasets are needed to clarify generalizability.

### Open Question 2
How do the performance and reliability of "thinking" models, such as Gemini Flash Thinking, compare to their non-thinking counterparts in mental health diagnostics? The study found that thinking models occasionally achieved competitive F1 scores but did not consistently outperform non-thinking models. Limited evidence exists on the effectiveness of thinking models, requiring further comparative studies across various conditions and prompt formulations.

### Open Question 3
To what extent can the findings of this study be generalized to real-world clinical settings, given the limitations of the E-DAIC dataset? The study acknowledges that the E-DAIC dataset is relatively small and may not generalize to broader populations or clinical environments. Validation through rigorous controlled clinical studies with larger, more diverse datasets is needed to determine real-world effectiveness.

## Limitations
- Small dataset size (275 samples) may limit generalizability and robustness of findings
- Limited model comparison scope with only a few LLM families tested
- Study does not report performance on entirely new, unseen modalities (e.g., video or physiological signals)
- Prompt sensitivity is noted but not systematically explored across different model families

## Confidence

- **High Confidence**: Gemini 1.5 Pro's superior performance in multimodal depression classification (F1=0.67, BA=77.4%) is well-supported by reported results
- **Medium Confidence**: Text modality generally outperforms audio, though this may be context-dependent on specific LLMs or prompt structures
- **Low Confidence**: Assertion that multimodal integration always improves performance is not robustly validated across all tasks or models

## Next Checks

1. Validate findings on a larger, more diverse mental health dataset (e.g., DAIC-WOZ or multimodal clinical records) to assess scalability and robustness
2. Conduct systematic prompt ablation study testing broader range of prompt structures to quantify their impact on modality superiority
3. Extend study to include more LLM families (e.g., Claude, Llama, or domain-specific models) to determine if observed modality effects are model-agnostic