---
ver: rpa2
title: Extreme Compression of Large Language Models via Additive Quantization
arxiv_id: '2401.06118'
source_url: https://arxiv.org/abs/2401.06118
tags:
- aqlm
- quantization
- bits
- quip
- codebooks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of extreme compression of large
  language models (LLMs) to very low bit counts, specifically 2-3 bits per parameter.
  The authors propose a new algorithm called AQLM that extends Additive Quantization
  (AQ), a classic information retrieval method, to LLM weight compression.
---

# Extreme Compression of Large Language Models via Additive Quantization

## Quick Facts
- arXiv ID: 2401.06118
- Source URL: https://arxiv.org/abs/2401.06118
- Authors: Vage Egiazarian; Andrei Panferov; Denis Kuznedelev; Elias Frantar; Artem Babenko; Dan Alistarh
- Reference count: 40
- One-line primary result: AQLM achieves state-of-the-art extreme compression of LLMs at 2-3 bits per parameter, with WikiText-2 perplexity of 6.06 at 2 bits for LLAMA 2 13B

## Executive Summary
This paper introduces AQLM, a novel method for extreme compression of large language models to very low bit counts (2-3 bits per parameter). The approach extends additive quantization, a classic information retrieval technique, to LLM weight compression through learned codebooks and joint optimization across transformer blocks. The authors demonstrate significant improvements over previous state-of-the-art methods, achieving Pareto optimality in the accuracy-vs-model-size trade-off for compression below 3 bits per parameter. The method includes efficient GPU and CPU implementations for fast inference.

## Method Summary
AQLM combines additive quantization with learned codebooks and joint optimization across transformer blocks. The method uses beam search with MAP-MRF formulation to efficiently explore the discrete code space while minimizing layer output error. Codebooks are initialized via residual K-means and optimized using Adam. The approach includes block-level fine-tuning to propagate and compensate quantization errors through transformer blocks. The algorithm alternates between beam search for codes, codebook updates, and fine-tuning entire transformer blocks.

## Key Results
- Achieves WikiText-2 perplexity of 6.06 at 2 bits for LLAMA 2 13B, compared to 8.22 for previous best method (QuIP#)
- Demonstrates Pareto optimality in accuracy-vs-model-size trade-off for compression below 3 bits per parameter
- Provides efficient GPU and CPU implementations for fast inference
- Outperforms state-of-the-art methods across multiple model sizes (7B, 13B, 70B) and tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint optimization of codes and codebooks over layer blocks reduces quantization error accumulation
- Mechanism: Instead of compressing each weight matrix independently, AQLM fine-tunes codebooks and codes at the block level, propagating quantization errors through the block and compensating them
- Core assumption: Calibration data represents input token distribution sufficiently for block-level output error minimization
- Break condition: Unrepresentative calibration data may cause block-level fine-tuning to overfit and fail to generalize

### Mechanism 2
- Claim: Learned additive quantization with multiple codebooks provides better accuracy than fixed lattices or rotation-based schemes
- Mechanism: Each weight group is represented as a sum of multiple codes from learned codebooks, offering more flexibility than fixed approaches
- Core assumption: Learned codebooks can better capture weight distribution than pre-defined lattices
- Break condition: Insufficient codebook entries relative to weight dimensionality may limit expressiveness

### Mechanism 3
- Claim: Beam search with MAP-MRF formulation enables efficient exploration of discrete code space
- Mechanism: Reformulates quantization error minimization as MAP inference in a discrete MRF, allowing efficient beam search
- Core assumption: Beam size is sufficient to find near-optimal codes and MRF formulation accurately captures error landscape
- Break condition: Insufficient beam size may miss good solutions, leading to suboptimal quantization

## Foundational Learning

- Concept: Additive Quantization (AQ) and Multi-Codebook Quantization (MCQ)
  - Why needed here: AQLM builds directly on AQ and MCQ techniques for efficient low-bitrate compression
  - Quick check question: How does additive quantization differ from product quantization in terms of representation?

- Concept: Markov Random Fields (MRF) and MAP inference
  - Why needed here: Beam search optimization in AQLM relies on formulating code assignment as MAP inference in an MRF
  - Quick check question: What is the relationship between the MRF formulation and the beam search algorithm?

- Concept: Transformer block structure and layer dependencies
  - Why needed here: Block-wise fine-tuning exploits quantization error propagation within transformer blocks
  - Quick check question: How many layers typically constitute a transformer block in LLMs?

## Architecture Onboarding

- Component map:
  - Codebooks (C1...CM): Learned vectors forming quantization basis
  - Codes (b): Discrete indices selecting codebook entries for each weight group
  - Scales (s): Per-output-unit scaling factors
  - Block fine-tuning module: Adjusts all parameters within a transformer block

- Critical path:
  1. Initialize codebooks via residual K-means
  2. Alternate between beam search (codes) and Adam (codebooks/scales)
  3. Fine-tune entire transformer block
  4. Repeat for all blocks

- Design tradeoffs:
  - Beam size vs. runtime: Larger beams find better solutions but take longer
  - Codebook size vs. accuracy: Larger codebooks improve accuracy but increase memory
  - Group size vs. granularity: Smaller groups allow finer quantization but more parameters

- Failure signatures:
  - Poor perplexity: Likely indicates insufficient codebook capacity or inadequate calibration data
  - Slow quantization: May be caused by overly large beam size or inefficient implementation
  - Memory issues: Usually due to excessive codebook size or group size

- First 3 experiments:
  1. Quantize a single linear layer with default parameters and verify MSE reduction
  2. Run full block fine-tuning on a small model and check output preservation
  3. Measure speed-accuracy tradeoff by varying beam size and codebook configuration

## Open Questions the Paper Calls Out
- The paper does not explicitly call out specific open questions in the provided text.

## Limitations
- Calibration dataset size is not fully specified for all experiments, making it difficult to assess scalability of block-level fine-tuning
- Interaction between group size, codebook size, and model performance is not systematically explored
- CPU inference performance measurements and comparisons are lacking

## Confidence
- High confidence: Core claim of outperforming existing methods at 2-3 bits per parameter is well-supported by direct experimental comparisons
- Medium confidence: Mechanism explanation for block-level fine-tuning is plausible but relies on assumptions about calibration data quality
- Medium confidence: Pareto optimality claim below 3 bits is supported but evaluation space is limited

## Next Checks
1. **Ablation on calibration set size**: Systematically vary calibration set size for different model scales (7B, 13B, 70B) to determine minimum effective size and assess scalability

2. **Cross-dataset robustness**: Evaluate AQLM when calibration set comes from different distribution than test set to verify genuine input-adaptiveness

3. **Scaling analysis of hyperparameters**: Conduct grid search over group sizes and codebook dimensions for 13B model to identify optimal configurations and assess current choices