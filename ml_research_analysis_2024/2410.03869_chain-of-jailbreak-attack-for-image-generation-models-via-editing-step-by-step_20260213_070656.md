---
ver: rpa2
title: Chain-of-Jailbreak Attack for Image Generation Models via Editing Step by Step
arxiv_id: '2410.03869'
source_url: https://arxiv.org/abs/2410.03869
tags:
- image
- generation
- attack
- safety
- generate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Chain-of-Jailbreak (CoJ), a novel jailbreaking
  method that bypasses safeguards in text-to-image models by decomposing malicious
  queries into a sequence of harmless sub-queries. The method applies three edit operations
  (delete-then-insert, insert-then-delete, change-then-change-back) across three edit
  elements (word, character, image) to iteratively generate and edit images.
---

# Chain-of-Jailbreak Attack for Image Generation Models via Editing Step by Step

## Quick Facts
- arXiv ID: 2410.03869
- Source URL: https://arxiv.org/abs/2410.03869
- Reference count: 40
- One-line primary result: CoJ achieves up to 60% jailbreak success rate by decomposing harmful queries into harmless sub-queries

## Executive Summary
This paper introduces Chain-of-Jailbreak (CoJ), a novel method for bypassing safety filters in text-to-image generation models. The attack works by decomposing malicious queries into a sequence of harmless sub-queries using three edit operations across three edit elements, then iteratively generating and editing images. Experiments on four major models (GPT-4V, GPT-4o, Gemini 1.5, Gemini 1.5 Pro) show CoJ achieves a 60% jailbreak success rate, significantly outperforming existing methods. The authors also propose Think-Twice Prompting, a defense strategy that improves safety by prompting models to describe and assess generated content before creation, achieving over 95% defense success rate.

## Method Summary
The Chain-of-Jailbreak (CoJ) attack method decomposes harmful queries into multiple harmless sub-queries through edit operations (delete-then-insert, insert-then-delete, change-then-change-back) applied to three edit elements (word, character, image). These sub-queries are sequentially executed to generate images that collectively achieve the original malicious intent. The Think-Twice Prompting defense mitigates this attack by prompting models to describe and evaluate images before generation. The evaluation uses a dataset of 1070 decomposed query sequences across 9 safety scenarios, with success measured by whether harmful content is generated despite safeguards.

## Key Results
- CoJ achieves up to 60% jailbreak success rate on four widely-used image generation models
- CoJ outperforms existing jailbreak methods by a significant margin
- Think-Twice Prompting defense achieves over 95% success rate in preventing CoJ attacks
- Insert-then-delete is identified as the most effective edit operation
- Word-level editing performs better than character-level or image-level editing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposition of malicious queries into harmless sub-queries bypasses safety filters that only inspect single prompts.
- Mechanism: CoJ breaks toxic queries into multiple harmless steps using edit operations across different elements. Safety models focus on single-turn context rather than multi-turn conversation or generated content.
- Core assumption: Safety filters lack multi-turn contextual awareness and fail to reconstruct malicious intent from sequence of sub-queries.
- Evidence anchors:
  - [abstract] "The method applies three edit operations (delete-then-insert, insert-then-delete, change-then-change-back) across three edit elements (word, character, image) to iteratively generate and edit images."
  - [section 2.3] "We design three basic edit operations in our CoJ attack method... Insert: To propose a position in the prompt or the generated text slogan and an element to insert."
- Break condition: If safety models track multi-turn context or analyze semantic trajectory of prompt sequences.

### Mechanism 2
- Claim: Iterative editing with multiple steps further reduces detectability by progressively hiding malicious intent.
- Mechanism: Increasing editing steps allows finer-grained transformations less likely to trigger safeguards. Each intermediate step appears benign while cumulative effect achieves malicious goal.
- Core assumption: Longer chains of benign edits are harder to detect than single large transformations.
- Evidence anchors:
  - [section 4.3] "The success rate of jailbreak continues to improve as the number of editing steps increases."
  - [figure 4] "Jailbreak success rate of our CoJ attack with respect to the editing steps."
- Break condition: If models detect malicious intent from sequence of edits or scrutinize intermediate steps.

### Mechanism 3
- Claim: Think-Twice Prompting defense improves safety by forcing models to analyze content before generation.
- Mechanism: Models are prompted to describe and assess image they will generate before creating it, shifting focus from input queries to output content.
- Core assumption: Models can effectively evaluate their own generated content for safety when explicitly prompted.
- Evidence anchors:
  - [abstract] "The authors also propose Think-Twice Prompting, a defense strategy that improves model safety by prompting models to describe and assess generated content before creation."
  - [section 4.4] "Think Twice Prompting... can successfully defend over 95% of CoJ attack."
- Break condition: If models cannot accurately self-assess generated content or prompts are ignored.

## Foundational Learning

- Concept: Levenshtein Distance
  - Why needed here: CoJ uses edit operations inspired by Levenshtein Distance to measure and achieve semantic equivalence between malicious and edited prompts.
  - Quick check question: How does Levenshtein Distance relate to the edit operations used in CoJ?

- Concept: Multi-turn conversation modeling
  - Why needed here: Safety models need to understand context across multiple prompt-response pairs to detect CoJ attacks.
  - Quick check question: What information would a safety model need to track across turns to detect CoJ?

- Concept: Self-assessment in language models
  - Why needed here: Think-Twice Prompting relies on models' ability to describe and evaluate their own generated content for safety.
  - Quick check question: What challenges might models face when trying to self-assess generated content?

## Architecture Onboarding

- Component map: Malicious Query -> CoJ Attack Engine (Decomposition) -> Sequential Model Queries -> Generated Images -> Evaluation (Harmful Content Detection) -> Defense Layer (Think-Twice) -> Re-evaluation

- Critical path:
  1. Input malicious query
  2. Decompose into sub-queries via edit operations/elements
  3. Sequentially query model with sub-queries
  4. Evaluate generated content for harmful material
  5. Apply defense (if implemented) and re-evaluate

- Design tradeoffs:
  - Number of editing steps vs. attack success rate and computational cost
  - Granularity of edit operations vs. detectability
  - Defense effectiveness vs. generation efficiency (Think-Twice requires additional inference)

- Failure signatures:
  - Attack fails: Safety filter blocks sub-queries or detects malicious intent
  - Defense fails: Model generates harmful content despite self-assessment
  - Decomposition fails: Generated sub-queries don't follow edit operation principles

- First 3 experiments:
  1. Test CoJ with 2-step vs. 3-step vs. 4-step decomposition on a sample malicious query
  2. Evaluate effectiveness of each edit operation (delete-then-insert, insert-then-delete, change-then-change-back)
  3. Measure defense success rate of Think-Twice Prompting against successful CoJ attacks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of decomposition steps for the CoJ attack to balance effectiveness and efficiency?
- Basis in paper: [inferred] The paper mentions that increasing the number of editing steps can improve the success rate of the attack, but it also notes that more steps lead to higher query times and energy costs.
- Why unresolved: The paper does not provide a specific recommendation for the optimal number of steps, and further research is needed to determine the best trade-off between attack success and resource usage.
- What evidence would resolve it: Empirical studies comparing the success rates and costs of attacks with different numbers of decomposition steps, conducted on various image generation models.

### Open Question 2
- Question: How effective are other smaller models, such as mistral-8b and Llama-3.1-8b, in decomposing malicious queries for the CoJ attack?
- Basis in paper: [explicit] The paper mentions that these smaller models were tested for query decomposition, but their performance was not as good as Mistral-Large-2.
- Why unresolved: The paper does not provide detailed results or comparisons of the performance of these smaller models, and further investigation is needed to understand their potential for use in the CoJ attack.
- What evidence would resolve it: Detailed experimental results comparing the quality and reliability of query decomposition by mistral-8b, Llama-3.1-8b, and Mistral-Large-2, using the same set of malicious queries.

### Open Question 3
- Question: Can the CoJ attack be effectively applied to image generation models other than those provided by GPT-4V, GPT-4o, Gemini 1.5, and Gemini 1.5 Pro?
- Basis in paper: [inferred] The paper focuses on these four models, but it does not explore the effectiveness of the CoJ attack on other models like Midjourney and Stable Diffusion, which have weaker safeguards.
- Why unresolved: The paper does not provide evidence of the attack's effectiveness on a broader range of models, and further testing is needed to determine its generalizability.
- What evidence would resolve it: Experimental results showing the success rate of the CoJ attack on a diverse set of image generation models, including those with varying levels of safety mechanisms.

### Open Question 4
- Question: What are the most effective edit operations and elements for the CoJ attack in terms of bypassing safety filters?
- Basis in paper: [explicit] The paper identifies that insert-then-delete is the most effective edit operation and word-level editing performs the best among edit elements.
- Why unresolved: While the paper provides some insights into the effectiveness of different operations and elements, it does not comprehensively analyze their performance across all safety scenarios and models.
- What evidence would resolve it: A detailed analysis of the success rates of different combinations of edit operations and elements across various safety scenarios and models, using a large and diverse set of malicious queries.

### Open Question 5
- Question: How can the defense method, Think-Twice Prompting, be improved to be more efficient while maintaining its effectiveness?
- Basis in paper: [inferred] The paper introduces Think-Twice Prompting as an effective defense, but it notes that the method requires generating descriptions of images, which is not efficient.
- Why unresolved: The paper does not explore alternative approaches to improve the efficiency of the defense method, and further research is needed to find a balance between effectiveness and computational cost.
- What evidence would resolve it: Experimental results comparing the effectiveness and efficiency of Think-Twice Prompting with alternative defense methods, such as those that do not require image description generation.

## Limitations
- Evaluation relies on human judgment for determining harmful content, introducing subjectivity and potential inconsistency
- Defense effectiveness of Think-Twice Prompting may not generalize to all types of jailbreak attacks beyond CoJ
- Specific mechanisms by which safety models detect harmful content remain unclear

## Confidence
- High confidence: Empirical results showing CoJ's effectiveness against tested models (up to 60% success rate)
- Medium confidence: Generalizability of results across different models and attack scenarios
- Medium confidence: Effectiveness of Think-Twice Prompting as a defense mechanism

## Next Checks
1. Test CoJ on additional image generation models not included in the original evaluation (e.g., Claude 3, Llama 3) to assess generalizability
2. Implement and evaluate the Think-Twice Prompting defense against alternative jailbreak methods to verify its broader effectiveness
3. Conduct a formal ablation study varying the number of editing steps and types of edit operations to quantify their individual contributions to attack success