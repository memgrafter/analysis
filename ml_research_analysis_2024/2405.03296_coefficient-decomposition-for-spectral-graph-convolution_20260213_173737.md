---
ver: rpa2
title: Coefficient Decomposition for Spectral Graph Convolution
arxiv_id: '2405.03296'
source_url: https://arxiv.org/abs/2405.03296
tags:
- graph
- coef
- linear
- decomposition
- spectral
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a unified framework for spectral graph convolution
  based on polynomial filters. The authors observe that existing SGCNs store coefficients
  in a third-order tensor and can be derived by different coefficient decomposition
  operations.
---

# Coefficient Decomposition for Spectral Graph Convolution

## Quick Facts
- arXiv ID: 2405.03296
- Source URL: https://arxiv.org/abs/2405.03296
- Authors: Feng Huang; Wen Zhang
- Reference count: 40
- Key outcome: CoDeSGC-Tucker achieves 4.8% and 11.2% accuracy improvements over JacobiConv on Actor and Squirrel datasets respectively

## Executive Summary
This paper proposes a unified framework for spectral graph convolution based on polynomial filters by representing coefficients in a third-order tensor. The authors introduce two new spectral graph convolutions, CoDeSGC-CP and CoDeSGC-Tucker, obtained by performing CP and Tucker tensor decompositions on the coefficient tensor. Experiments on 10 real-world datasets for node classification show that the proposed models achieve favorable performance improvements over state-of-the-art methods, including JacobiConv, on 8 out of 10 datasets.

## Method Summary
The paper presents a unified view of spectral graph convolution where polynomial filter coefficients are stored in a third-order tensor W∈RI×J×(K+1). Based on this representation, the authors develop novel spectral graph convolutions by applying tensor decomposition techniques - specifically CP decomposition for CoDeSGC-CP and Tucker decomposition for CoDeSGC-Tucker. These decompositions allow learning more sophisticated multilinear relationships between polynomial coefficients, input channels, and output channels. The models are trained using the Jacobi polynomial basis with normalized adjacency matrix, employing 60%/20%/20% train/validation/test splits and hyperparameter tuning via Optuna.

## Key Results
- CoDeSGC-Tucker achieves 4.8% and 11.2% accuracy improvements over JacobiConv on Actor and Squirrel datasets respectively
- The proposed models achieve favorable performance on 8 out of 10 tested datasets
- CP and Tucker decompositions provide different expressiveness levels, with Tucker capturing more intricate multilinear relations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Storing polynomial coefficients in a third-order tensor allows decomposition operations to recover known SGCN architectures
- Mechanism: By representing coefficients as W∈RI×J×(K+1), different decompositions (identity, diagonal scaling, full matrix multiplication) correspond to different SGCN variants like GCN, GPR-GNN, and JacobiConv
- Core assumption: All SGCNs with polynomial filters can be expressed in the unified form Y=∑Kk=0Pk(S)XWk where Wk are frontal slices of the coefficient tensor
- Evidence anchors: [abstract] general form in terms of spectral graph convolution; [section 3.1] generalized point of view on spectral graph convolution; [corpus] Weak - corpus papers focus on polynomial basis selection

### Mechanism 2
- Claim: CP decomposition on the coefficient tensor creates a more expressive model than existing linear SGCNs
- Mechanism: CP decomposition W=[C,P,M] allows learning separate factor matrices C, P, M, enabling more complex multilinear relationships between input channels, output channels, and polynomial orders
- Core assumption: More expressive multilinear relationships in the coefficient tensor lead to better learning of graph filters
- Evidence anchors: [section 3.2] tensor decomposition to extend these convolution operators; [abstract] novel spectral graph convolutions CoDeSGC-CP and -Tucker; [corpus] Weak - corpus papers do not discuss tensor decomposition

### Mechanism 3
- Claim: Tucker decomposition provides even greater expressiveness by allowing different multilinear relationships across all dimensions
- Mechanism: Tucker decomposition W=G×1C×2P×3M introduces a core tensor G that modulates interactions between factor matrices, creating richer relationships than CP
- Core assumption: The additional flexibility from the core tensor G improves the model's ability to learn complex graph filters
- Evidence anchors: [section 3.2] Tucker decomposition decomposes a tensor into a core tensor multiplied by a matrix along each dimension; [abstract] novel spectral graph convolutions CoDeSGC-CP and -Tucker; [corpus] Weak - corpus papers do not discuss Tucker decomposition

## Foundational Learning

- Concept: Tensor decomposition (CP and Tucker)
  - Why needed here: These decompositions provide the mathematical framework for creating new SGCN architectures by factorizing the coefficient tensor in different ways
  - Quick check question: What is the key difference between CP and Tucker decomposition in terms of the relationships they can express between tensor modes?

- Concept: Polynomial graph filters
  - Why needed here: Understanding how polynomial filters approximate graph signal processing operations is essential for grasping why coefficient decomposition matters
  - Quick check question: Why do most SGCNs use polynomial approximations instead of computing the full eigendecomposition of the graph Laplacian?

- Concept: Graph Laplacian spectrum
  - Why needed here: The spectral properties of the graph Laplacian determine how information propagates through the network, which is the foundation of spectral graph convolution
  - Quick check question: How does the choice of graph matrix S (Laplacian, normalized adjacency, etc.) affect the spectral properties of the convolution operation?

## Architecture Onboarding

- Component map: Node features X∈R|V|×I → Linear transformation → Propagation (K iterations) → Coefficient decomposition → Filtered signals Y∈R|V|×J
- Critical path: X → Linear transformation → Propagation (K iterations) → Coefficient decomposition → Y
- Design tradeoffs:
  - CP vs Tucker: CP has fewer parameters (R vs P×Q×R) but less expressiveness; Tucker can capture more complex relationships but is harder to train
  - Order K: Higher K allows more complex filters but increases computational cost and risk of overfitting
  - Factor dimensions: Larger R/P/Q increases model capacity but also overfitting risk
- Failure signatures:
  - Poor performance on small datasets: Likely overfitting from too many parameters
  - Training instability: Learning rates or weight decay may need adjustment for the additional parameters
  - No improvement over baselines: May indicate that the dataset doesn't benefit from the increased expressiveness
- First 3 experiments:
  1. Compare CoDeSGC-CP vs JacobiConv on Cora/CiteSeer to verify the CP decomposition claim
  2. Test different values of R (4, 8, 16, 32) on a medium-sized dataset to find the sweet spot for expressiveness vs overfitting
  3. Compare CoDeSGC-Tucker with different core tensor ranks (P=Q=4,8,16) on a heterophilic dataset to test Tucker's advantage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of polynomial basis (Chebyshev, Monomial, Bernstein, Jacobi, etc.) interact with different coefficient decomposition strategies like CP and Tucker?
- Basis in paper: [explicit] The authors note that "different polynomial bases to substitute the Chebyshev basis" exist, but state "This work does not discuss these points."
- Why unresolved: The paper focuses on the coefficient decomposition aspect while keeping the polynomial basis fixed, leaving the interaction effects unexplored
- What evidence would resolve it: Systematic experiments varying both the polynomial basis and decomposition type across multiple datasets would reveal interaction effects

### Open Question 2
- Question: What is the theoretical relationship between the rank parameters in CP/Tucker decomposition and the model's expressive power in approximating arbitrary graph filters?
- Basis in paper: [inferred] The authors mention that "the Tucker decomposition is reduced to CP decomposition" when certain parameters are set equal, but don't explore the theoretical implications
- Why unresolved: While empirical results show performance differences, the paper doesn't provide theoretical bounds or analysis connecting decomposition ranks to filter approximation capabilities
- What evidence would resolve it: Mathematical analysis proving bounds on filter approximation error as a function of decomposition ranks, potentially validated through controlled experiments

### Open Question 3
- Question: How do CoDeSGC-CP and CoDeSGC-Tucker perform on graph classification and link prediction tasks compared to their performance on node classification?
- Basis in paper: [explicit] The authors state "Extensive experimental results demonstrate that the proposed convolutions achieve favorable performance improvements" but only report node classification results
- Why unresolved: The paper only evaluates on node classification tasks, leaving the generalization to other graph learning tasks unknown
- What evidence would resolve it: Experiments on standard graph classification benchmarks (like COLLAB, IMDBBINARY) and link prediction tasks would reveal whether the performance gains transfer to other problem domains

## Limitations

- The paper only evaluates on node classification tasks, leaving performance on other graph learning tasks unknown
- Implementation details for the Jacobi polynomial basis and exact hyperparameter ranges are not fully specified
- The theoretical relationship between decomposition ranks and model expressiveness remains unexplored

## Confidence

- **Medium** for the unified framework claim - Limited experimental validation across diverse architectures
- **Medium-High** for performance claims - Compelling results but magnitude varies significantly across datasets
- **Low** for generalization claims - No systematic analysis of performance across different graph types and homophily levels

## Next Checks

1. **Mechanism verification on diverse architectures**: Implement additional SGCN variants (e.g., ARMA, CayleyNet) and verify whether they can be expressed through the proposed coefficient tensor decomposition framework

2. **Parameter sensitivity analysis**: Conduct systematic experiments varying tensor decomposition ranks (R for CP, P/Q/R for Tucker) across a wider range of values on each dataset to measure the trade-off between model capacity and overfitting risk

3. **Cross-dataset generalization test**: Train models on combined datasets or use transfer learning approaches to evaluate whether the learned coefficient decompositions capture generalizable graph filtering patterns across different graph domains