---
ver: rpa2
title: LLMs are One-Shot URL Classifiers and Explainers
arxiv_id: '2409.14306'
source_url: https://arxiv.org/abs/2409.14306
tags:
- phishing
- benign
- llms
- prediction
- urls
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that large language models (LLMs) can effectively
  classify phishing URLs with one-shot learning, achieving performance comparable
  to fully supervised deep learning models. The proposed framework leverages Chain-of-Thought
  reasoning to prompt LLMs with a single benign URL example, enabling them to classify
  new URLs while providing human-readable explanations.
---

# LLMs are One-Shot URL Classifiers and Explainers

## Quick Facts
- arXiv ID: 2409.14306
- Source URL: https://arxiv.org/abs/2409.14306
- Reference count: 40
- Primary result: GPT-4 Turbo achieves 0.92 F1 score in one-shot phishing URL classification, only 0.07 below fully supervised models

## Executive Summary
This paper demonstrates that large language models can effectively classify phishing URLs using one-shot learning, achieving performance comparable to fully supervised deep learning models. The proposed framework leverages Chain-of-Thought reasoning to prompt LLMs with a single benign URL example, enabling them to classify new URLs while providing human-readable explanations. Tested across three datasets with five state-of-the-art LLMs, the framework shows robust performance across zero-shot, one-shot, and few-shot settings.

The key innovation lies in combining few-shot prompting with Chain-of-Thought reasoning to elicit both classification decisions and interpretable explanations from LLMs. The framework generates explanations that align well with post-hoc explanations from supervised classifiers while scoring high on readability and coherence metrics. GPT-4 Turbo consistently achieved the highest average F1 score of 0.92 regardless of the number of examples provided, demonstrating the potential of LLMs as effective one-shot URL classifiers.

## Method Summary
The framework uses Chain-of-Thought prompting to guide LLMs through reasoning about URL classification. A single benign URL example is provided as context, along with instructions to classify new URLs and generate explanations. The LLM processes the input URL through its reasoning chain and outputs both a classification decision (benign/malicious) and a human-readable explanation of the reasoning. The framework was tested on three datasets using five different LLMs, comparing performance against fully supervised deep learning models. Evaluations measured F1 scores for classification accuracy and various metrics for explanation quality, including alignment with post-hoc explanations and human readability scores.

## Key Results
- GPT-4 Turbo achieved the highest average F1 score of 0.92 in one-shot phishing URL classification
- LLM performance was only 0.07 below fully supervised deep learning models
- GPT-4 Turbo maintained an average F1 score of 0.92 across zero-shot, one-shot, and few-shot settings
- LLM-generated explanations aligned well with post-hoc explanations from supervised classifiers
- Explanations scored high on readability, coherence, and informativeness metrics

## Why This Works (Mechanism)
LLMs excel at pattern recognition and reasoning when provided with appropriate context. By using Chain-of-Thought prompting with a single benign example, the model learns the distinguishing features between benign and malicious URLs through its pre-trained knowledge. The reasoning process allows the LLM to articulate its decision-making process in human-readable form, making the classification transparent and interpretable. This approach leverages the LLM's ability to generalize from minimal examples while maintaining high accuracy.

## Foundational Learning
- Chain-of-Thought prompting: Enables step-by-step reasoning for complex tasks by breaking down problems into intermediate steps
- Few-shot learning: Allows models to learn from minimal examples by leveraging pre-trained knowledge
- URL feature extraction: Identifying patterns in domain names, paths, and parameters that indicate malicious intent
- Explainable AI: Generating human-understandable reasoning for automated decisions
- F1 score: Harmonic mean of precision and recall, used to evaluate classification performance
- Post-hoc explanations: Explanations generated after model predictions to understand decision-making

## Architecture Onboarding

Component map: URL input -> Chain-of-Thought prompt template -> LLM inference -> Classification + Explanation output

Critical path: The Chain-of-Thought reasoning sequence where the LLM processes the example URL, compares it with the input URL, identifies distinguishing features, and generates both classification and explanation.

Design tradeoffs: Using a single example maximizes efficiency but may miss edge cases; more examples could improve robustness but reduce the one-shot advantage. The balance between explanation detail and computational cost must be managed.

Failure signatures: Misclassifications occur when malicious URLs closely mimic benign patterns, or when benign URLs contain unusual but legitimate features. Explanation quality degrades when the reasoning chain becomes too complex or when the LLM hallucinates non-existent features.

First experiments: 1) Test classification accuracy with varying numbers of benign examples, 2) Evaluate explanation quality using human readability metrics, 3) Compare LLM performance against traditional supervised classifiers on the same datasets.

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Evaluation limited to phishing URLs without testing on broader URL classification tasks
- No analysis of robustness against adversarial examples designed to evade LLM detection
- Lack of formal user studies to validate whether LLM-generated explanations improve human understanding and trust
- No analysis of computational costs and latency implications for real-time classification at scale

## Confidence
- High confidence in one-shot learning performance claims due to direct measurement and comparison with supervised models
- Medium confidence in explanation quality assessment given lack of human evaluation studies
- Low confidence in generalizability to other URL classification domains beyond phishing

## Next Checks
1. Test framework performance on diverse URL classification tasks beyond phishing, including malicious domains and suspicious but benign URLs
2. Conduct user studies to validate whether LLM-generated explanations actually improve human understanding and trust in classification decisions
3. Evaluate framework robustness against adversarial URL examples specifically designed to evade LLM-based detection systems