---
ver: rpa2
title: 'NYT-Connections: A Deceptively Simple Text Classification Task that Stumps
  System-1 Thinkers'
arxiv_id: '2412.01621'
source_url: https://arxiv.org/abs/2412.01621
tags:
- hints
- llms
- reasoning
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NYT-Connections, a benchmark based on the
  New York Times Connections word game, designed to assess large language models'
  reasoning abilities by requiring deliberate, System 2 thinking rather than quick,
  intuitive System 1 responses. The dataset comprises 358 puzzles, with 100 median-difficulty
  instances used for evaluation.
---

# NYT-Connections: A Deceptively Simple Text Classification Task that Stumps System-1 Thinkers

## Quick Facts
- arXiv ID: 2412.01621
- Source URL: https://arxiv.org/abs/2412.01621
- Reference count: 10
- Primary result: Current LLMs show significant performance gaps on a benchmark designed to test System 2 reasoning, falling nearly 30% short of human performance

## Executive Summary
This paper introduces NYT-Connections, a benchmark based on the New York Times Connections word game, designed to assess large language models' reasoning abilities by requiring deliberate, System 2 thinking rather than quick, intuitive System 1 responses. The dataset comprises 358 puzzles, with 100 median-difficulty instances used for evaluation. Six recent LLMs, a simple machine learning heuristic, and human participants were tested across three configurations: single-attempt, multiple attempts without hints, and multiple attempts with contextual hints. Results show a significant performance gap, with even the best-performing model (Claude 3.5) falling nearly 30% short of human performance in the most favorable setup.

## Method Summary
The NYT-Connections benchmark evaluates LLMs on the Connections word game task, where 16 terms must be grouped into 4 sets of 4 related words. The evaluation uses 100 median-difficulty puzzles from a dataset of 358 instances collected between June 2023 and June 2024. Six pre-trained LLMs (Claude 3.5, GPT-4, GPT-4o, Gemini 1.5 Pro, LLaMA 3 70B, LLaMA 3.1 405B) are tested using three prompting methods (Input-Output, Chain-of-Thought, and Chain-of-Thought with Self-Consistency) across three configurations (One Try, No Hints, Full Hints). A simple heuristic baseline using Multilingual-E5-Large-Instruct embeddings with beam search is also implemented for comparison.

## Key Results
- Even top-performing LLMs like Claude 3.5 fall nearly 30% short of human performance on NYT-Connections
- Advanced prompting techniques (Chain-of-Thought, Self-Consistency) show diminishing returns as task difficulty increases
- A simple heuristic baseline using word embeddings performs comparably to some LLMs, suggesting models may rely on superficial similarity rather than deep reasoning
- The benchmark successfully resists shortcut learning and maintains difficulty even with multiple attempts and hints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NYT-Connections tasks require System 2 reasoning because they deliberately include misleading associations that tempt quick, intuitive (System 1) responses.
- Mechanism: The puzzle design creates "attractors" where certain word groupings appear correct but lead to dead ends, forcing solvers to reconsider and apply deliberate reasoning.
- Core assumption: The difficulty arises not from the complexity of the associations themselves, but from the presence of plausible-but-incorrect groupings that mislead System 1 thinking.
- Evidence anchors:
  - [abstract] "This benchmark is designed to penalize quick, intuitive 'System 1' thinking, isolating fundamental reasoning skills."
  - [section 2.1] "Its design intentionally tempts incorrect System 1 responses while requiring System 2 thinking for correct solutions."
- Break condition: If models develop heuristics to identify and ignore misleading groupings without truly understanding the semantic relationships.

### Mechanism 2
- Claim: Current LLMs show performance gaps because they lack the ability to engage in genuine deliberation when faced with ambiguous or misleading information.
- Mechanism: LLMs tend to latch onto the most statistically probable associations (System 1 behavior) rather than exploring multiple possibilities and reasoning through contradictions.
- Core assumption: The performance gap isn't due to insufficient knowledge or capability, but rather the inability to override intuitive responses when they lead to contradictions.
- Evidence anchors:
  - [abstract] "even top-performing LLMs like GPT-4 fall short of human performance by nearly 30%"
  - [section 4] "GPT-4 fails to consider multiple factors that may lead to better results — such as when the remaining words outside of the chosen group contain strong matches"
- Break condition: If models develop the capacity to recognize when their initial intuitions lead to contradictions and systematically explore alternatives.

### Mechanism 3
- Claim: Advanced prompting techniques like Chain-of-Thought show diminishing returns on harder puzzles because they don't truly simulate System 2 deliberation.
- Mechanism: While CoT makes reasoning more explicit, it doesn't necessarily make the reasoning more deliberate or thorough when faced with deceptive problem structures.
- Core assumption: The issue isn't visibility of reasoning steps, but the quality and thoroughness of the reasoning itself.
- Evidence anchors:
  - [abstract] "advanced prompting techniques such as Chain-of-Thought and Self-Consistency show diminishing returns as task difficulty increases"
  - [section 4] "Chain-of-Thought-based approaches...the model first latches on to words in a laundry category"
- Break condition: If new prompting techniques can effectively guide models to recognize and overcome deceptive associations.

## Foundational Learning

- Concept: System 1 vs System 2 thinking distinction
  - Why needed here: Understanding the benchmark's design philosophy and why it's challenging for LLMs
  - Quick check question: What's the key difference between System 1 and System 2 thinking in problem-solving contexts?

- Concept: Word embedding similarity and clustering
  - Why needed here: Understanding how the heuristic baseline works and why simple clustering fails
  - Quick check question: Why does k-means clustering fail to correctly classify the Connections puzzle terms?

- Concept: Prompt engineering techniques (IO, CoT, Self-Consistency)
  - Why needed here: Understanding the experimental methodology and interpreting the results
  - Quick check question: What's the fundamental difference between Input-Output and Chain-of-Thought prompting?

## Architecture Onboarding

- Component map: Dataset → Model Input → Model Output → Scoring → Statistical Analysis
- Critical path: Puzzle → Model Input → Model Output → Scoring → Statistical Analysis
- Design tradeoffs:
  - Using a fixed dataset vs. dynamic updates (data leakage vs. consistency)
  - Simple heuristic vs. complex baseline (transparency vs. performance)
  - Multiple evaluation configurations (comprehensive vs. resource-intensive)
- Failure signatures:
  - Performance gaps between LLM and human results
  - Diminishing returns of advanced prompting techniques
  - Heuristic baseline performing comparably to some LLMs
- First 3 experiments:
  1. Run the heuristic baseline on a small subset to verify it mimics System 1 thinking
  2. Test a single LLM with IO prompting across all three configurations
  3. Compare CoT vs IO prompting on medium-difficulty puzzles to observe the diminishing returns pattern

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the performance gap between LLMs and humans on NYT-Connections be bridged by developing new prompting techniques that better simulate System 2 reasoning?
- Basis in paper: [inferred] The paper shows that current prompting techniques like Chain-of-Thought and Self-Consistency have diminishing returns as task difficulty increases, and simpler Input-Output prompting often outperforms these approaches.
- Why unresolved: The study tested only a limited set of prompting techniques due to cost constraints. More advanced or innovative prompting methods that specifically target System 2 thinking have not been explored.
- What evidence would resolve it: Testing a broader range of novel prompting techniques, including those designed to explicitly encourage deliberate reasoning, and demonstrating significant improvement in LLM performance on NYT-Connections.

### Open Question 2
- Question: How does the performance of LLMs on NYT-Connections scale with the size and quality of the embedding models used in the heuristic baseline?
- Basis in paper: [explicit] The paper mentions that the heuristic uses a relatively small embedding model due to hardware constraints and suggests that larger embedding models could yield different results.
- Why unresolved: The study was limited by hardware constraints and could not explore the full potential of larger embedding models in the heuristic approach.
- What evidence would resolve it: Replicating the heuristic experiments using state-of-the-art large embedding models and comparing their performance to LLMs on NYT-Connections.

### Open Question 3
- Question: Would creating multilingual and culturally adapted versions of NYT-Connections reveal significant differences in LLM performance across different languages and cultural contexts?
- Basis in paper: [inferred] The paper notes that Connections puzzles are primarily designed for an English-speaking, Western audience, which may limit the benchmark's applicability across different cultures and languages.
- Why unresolved: The current benchmark is limited to English and Western cultural references, and its effectiveness in diverse linguistic and cultural contexts has not been explored.
- What evidence would resolve it: Developing and testing multilingual and culturally adapted versions of NYT-Connections with LLMs and comparing their performance to the original English version.

## Limitations
- The benchmark's cultural specificity may limit its generalizability to diverse linguistic and cultural contexts
- Performance differences may reflect general task difficulty rather than specifically System 2 reasoning deficiencies
- Hardware constraints limited exploration of larger embedding models in the heuristic baseline

## Confidence
- The claim that NYT-Connections specifically targets System 2 reasoning (High confidence)
- The performance gap between LLMs and humans (High confidence)
- The attribution of this gap specifically to System 2 reasoning deficiencies rather than general task difficulty (Medium confidence)
- The diminishing returns of advanced prompting techniques on harder puzzles (Medium confidence)
- The heuristic baseline's performance representing System 1 thinking (Medium confidence)

## Next Checks
1. **Controlled difficulty manipulation**: Create modified versions of the same puzzles with varying degrees of misleading associations to directly test whether the difficulty stems from System 1/System 2 distinction or general complexity.

2. **Ablation study on prompting techniques**: Systematically test which components of Chain-of-Thought prompting (explicit reasoning steps, self-consistency, or both) contribute most to performance gains, and whether these gains correlate with puzzle difficulty.

3. **Cross-domain generalization test**: Evaluate the same models on analogous reasoning tasks from different domains (e.g., logic puzzles, scientific reasoning) to determine whether the performance gaps are specific to word association tasks or reflect broader reasoning limitations.