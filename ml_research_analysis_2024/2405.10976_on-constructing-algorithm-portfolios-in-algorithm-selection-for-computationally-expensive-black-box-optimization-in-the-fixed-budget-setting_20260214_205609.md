---
ver: rpa2
title: On Constructing Algorithm Portfolios in Algorithm Selection for Computationally
  Expensive Black-box Optimization in the Fixed-budget Setting
arxiv_id: '2405.10976'
source_url: https://arxiv.org/abs/2405.10976
tags:
- algorithm
- selection
- function
- optimizers
- evaluations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the construction of algorithm portfolios
  for computationally expensive black-box optimization under the fixed-budget setting.
  The authors highlight issues in previous experimental setups, particularly the unrealistic
  consideration of the number of function evaluations used in the sampling phase when
  constructing algorithm portfolios.
---

# On Constructing Algorithm Portfolios in Algorithm Selection for Computationally Expensive Black-box Optimization in the Fixed-budget Setting

## Quick Facts
- arXiv ID: 2405.10976
- Source URL: https://arxiv.org/abs/2405.10976
- Reference count: 35
- Algorithm portfolios constructed accounting for sampling costs outperform previous methods for expensive black-box optimization

## Executive Summary
This paper addresses critical methodological issues in constructing algorithm portfolios for computationally expensive black-box optimization under the fixed-budget setting. The authors identify that previous approaches unrealistically assumed that the sampling phase for portfolio construction uses no function evaluations, which is particularly problematic when optimization budgets are limited. They propose a new approach that properly accounts for the number of function evaluations consumed during the sampling phase, ensuring that the remaining budget is accurately allocated to candidate optimizers. Through extensive experiments using the BBOB benchmark suite, the study demonstrates that algorithm portfolios constructed with this sampling-aware approach significantly outperform those built using traditional methods, especially for higher-dimensional problems. The results also reveal an important insight: prediction accuracy of the best optimizer does not directly correlate with algorithm selection system performance.

## Method Summary
The paper introduces a sampling-aware approach to algorithm portfolio construction that addresses the unrealistic assumption in previous work that the sampling phase requires no function evaluations. The proposed method partitions the available budget into two parts: one for sampling across different functions and dimensions, and the remainder for running selected optimizers. The authors develop a systematic procedure to account for the sampling budget, ensuring fair comparison between algorithm selection systems and individual optimizers. They conduct experiments using 23 noiseless functions from the BBOB suite across dimensions 2, 5, 10, and 20, comparing their approach against two baseline methods: random selection and the previous sampling-unaware approach. The experiments evaluate performance using the expected running time (ERT) metric, normalized against a baseline algorithm.

## Key Results
- Algorithm portfolios constructed using the sampling-aware approach outperform those built with previous methods, with statistically significant improvements in ERT values
- The performance advantage becomes more pronounced for higher-dimensional problems (10D and 20D), where sampling budget considerations are more critical
- Prediction accuracy of the best optimizer does not directly correlate with the overall performance of algorithm selection systems, challenging conventional assumptions about algorithm selection

## Why This Works (Mechanism)
The proposed approach works because it accurately models the real constraints of expensive optimization scenarios where function evaluations are limited. By accounting for the sampling budget, the method ensures that the portfolio construction process realistically reflects the available computational resources. This prevents the overestimation of algorithm selection system performance that occurs when sampling costs are ignored. The mechanism ensures that the remaining budget after sampling is optimally allocated to the selected optimizers, leading to more effective use of the available function evaluations.

## Foundational Learning
- Fixed-budget optimization: Understanding that optimization occurs within strict computational limits where each function evaluation has significant cost; needed to grasp why sampling budget matters.
- Algorithm selection problem: Recognizing that choosing the right optimizer for a given problem instance can significantly impact performance; quick check: can you explain why algorithm selection matters in expensive optimization?
- Portfolio construction: Understanding how to combine multiple algorithms to achieve better overall performance than any single algorithm; quick check: can you describe the difference between algorithm selection and algorithm portfolio?
- Expected Running Time (ERT): A performance metric that measures the expected number of function evaluations needed to reach a target value; quick check: can you explain how ERT normalizes performance across different problem instances?

## Architecture Onboarding

Component map:
Sampling Phase -> Portfolio Construction -> Algorithm Selection -> Performance Evaluation

Critical path:
Sampling Phase -> Portfolio Construction -> Algorithm Selection -> Performance Evaluation

Design tradeoffs:
The primary tradeoff is between sampling budget allocation and running budget for optimizers. Allocating more budget to sampling improves portfolio quality but reduces resources available for actual optimization. The authors address this by proposing a systematic approach to determine optimal sampling proportions based on problem dimensionality.

Failure signatures:
- If sampling budget is too small, the portfolio may not capture the diversity of problem characteristics, leading to poor selection performance.
- If sampling budget is too large, insufficient evaluations remain for the selected optimizers to converge effectively.
- Ignoring sampling costs leads to overestimation of algorithm selection system performance.

First experiments:
1. Compare ERT values of sampling-aware vs. sampling-unaware portfolios on a single BBOB function across different dimensions
2. Vary the sampling budget percentage (e.g., 10%, 20%, 30%) to observe its impact on portfolio performance
3. Evaluate the correlation between prediction accuracy and actual performance across different portfolio construction methods

## Open Questions the Paper Calls Out
None

## Limitations
- The experimental validation is limited to 23 noiseless functions from the BBOB suite, which may not generalize to noisy or non-continuous optimization problems.
- The analysis focuses primarily on low-dimensional problems (2-5D), though promising results are shown for 10D and 20D with limited function coverage.
- The empirical validation relies on a single sampling budget configuration, and results may vary with different sampling strategies.

## Confidence
- Sampling budget accounting claims: High
- Revised portfolio construction methodology: Medium
- Prediction accuracy vs. portfolio performance relationship: Medium

## Next Checks
1. Test the portfolio construction methodology across diverse optimization problem types (noisy, mixed-integer, constrained) to assess robustness beyond the current noiseless continuous function suite.
2. Conduct sensitivity analysis varying the sampling budget percentage and sampling strategy to determine the impact on portfolio performance across different dimensionalities.
3. Compare the proposed approach against alternative portfolio construction methods (e.g., clustering-based, evolutionary approaches) on larger benchmark suites to establish relative performance advantages.