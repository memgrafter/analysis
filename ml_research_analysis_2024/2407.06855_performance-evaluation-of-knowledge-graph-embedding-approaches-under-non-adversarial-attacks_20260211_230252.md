---
ver: rpa2
title: Performance Evaluation of Knowledge Graph Embedding Approaches under Non-adversarial
  Attacks
arxiv_id: '2407.06855'
source_url: https://arxiv.org/abs/2407.06855
tags:
- perturbation
- graph
- attack
- attacks
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates the robustness of knowledge graph embedding
  (KGE) models against non-adversarial attacks across three attack surfaces: graph
  perturbation, label perturbation, and parameter perturbation. The authors systematically
  perturb 5 state-of-the-art KGE algorithms (DistMult, ComplEx, QMult, MuRE, Keci)
  on 5 datasets using 8 different perturbation ratios.'
---

# Performance Evaluation of Knowledge Graph Embedding Approaches under Non-adversarial Attacks

## Quick Facts
- arXiv ID: 2407.06855
- Source URL: https://arxiv.org/abs/2407.06855
- Reference count: 11
- Key outcome: Label perturbation has the strongest negative impact on KGE performance, followed by parameter perturbation, while graph perturbation shows minimal effects except at high perturbation ratios

## Executive Summary
This paper presents a comprehensive evaluation of knowledge graph embedding (KGE) model robustness against non-adversarial attacks across three distinct attack surfaces: graph perturbation, label perturbation, and parameter perturbation. The authors systematically assess 5 state-of-the-art KGE algorithms on 5 benchmark datasets using 8 different perturbation ratios. Their findings reveal that label perturbation poses the greatest threat to KGE performance, while graph perturbation surprisingly shows minimal negative impact and can even improve performance at small perturbation levels by acting as a regularizer.

## Method Summary
The authors evaluate 5 KGE models (DistMult, ComplEx, QMult, MuRE, Keci) across 5 datasets under 8 perturbation ratios (0.01 to 0.25) for each of three attack types. They systematically perturb the knowledge graph structure, entity/relation labels, and model parameters to measure performance degradation. The study employs standard evaluation metrics for link prediction tasks and compares the robustness of different KGE architectures under identical attack conditions. Experiments are designed to isolate the effects of each attack surface while controlling for dataset characteristics.

## Key Results
- Label perturbation causes the most severe performance degradation across all KGE models
- Graph perturbation shows minimal negative impact except at high perturbation ratios (0.2-0.25)
- Small graph perturbations can sometimes improve performance by acting as regularizers
- Larger datasets demonstrate greater robustness to parameter perturbations compared to smaller datasets
- The ranking of attack surface impact is consistent across all tested KGE architectures

## Why This Works (Mechanism)
Knowledge graph embeddings are vulnerable to different types of perturbations due to their underlying mathematical formulations and training procedures. Label perturbations directly affect the semantic relationships encoded in the embeddings, causing fundamental misinterpretations of the knowledge graph structure. Parameter perturbations disrupt the learned transformation functions that map entities and relations into the embedding space. Graph perturbations, while potentially altering local neighborhood structures, may not significantly impact the global embedding space due to the regularization properties of KGE training objectives.

## Foundational Learning
- Knowledge Graph Embeddings: Vector representations of entities and relations that preserve semantic relationships - needed for understanding how perturbations affect learned representations
- Link Prediction Metrics: Standard evaluation metrics (MRR, Hits@10) for assessing KGE performance - needed for measuring attack impact
- Perturbation Theory: Mathematical framework for analyzing how small changes affect system behavior - needed for understanding robustness patterns
- Regularization in KGE: Techniques that prevent overfitting during training - needed to explain why small graph perturbations can improve performance
- Embedding Space Geometry: The geometric properties of KGE representations - needed for understanding parameter perturbation effects

## Architecture Onboarding

**Component Map**: KGE Model -> Training Pipeline -> Evaluation Metrics -> Attack Surface -> Performance Degradation

**Critical Path**: Input KG → Embedding Generation → Training → Link Prediction → Performance Evaluation → Attack Application → Degradation Measurement

**Design Tradeoffs**: 
- Model complexity vs. robustness to perturbations
- Embedding dimensionality vs. resistance to parameter attacks
- Training time vs. stability under graph perturbations
- Expressiveness vs. vulnerability to label attacks

**Failure Signatures**: 
- Sudden drops in MRR/Hits@10 after specific perturbation ratios
- Inconsistent performance across different KGE architectures
- Non-monotonic degradation patterns with increasing perturbation levels
- Dataset-size dependent robustness patterns

**First Experiments**:
1. Replicate the label perturbation attack on DistMult with 0.1 perturbation ratio
2. Test graph perturbation effects on ComplEx with 0.05 perturbation ratio
3. Evaluate parameter perturbation impact on MuRE across different dataset sizes

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Limited to 5 specific KGE architectures, may not generalize to newer models
- Focus on benchmark datasets may not reflect real-world knowledge graph characteristics
- Theoretical explanation lacking for why small graph perturbations can improve performance
- No analysis of attack surface interactions or compound perturbations

## Confidence
- Attack surface ranking (label > parameter > graph): High
- Dataset size robustness relationship: Medium
- Cross-architecture generalizability: Medium
- Theoretical explanations for observed patterns: Low

## Next Checks
1. **Cross-architecture validation**: Test the attack robustness patterns on recent KGE models like TuckER, RotatE, and neural link predictors to verify if the observed trends hold across different architectural families

2. **Scale validation**: Conduct experiments on knowledge graphs significantly larger than those tested here (e.g., Wikidata, DBpedia) to confirm the dataset size-robustness relationship

3. **Theoretical grounding**: Develop analytical frameworks explaining why certain perturbation types affect performance more severely and why small graph perturbations can act as regularizers