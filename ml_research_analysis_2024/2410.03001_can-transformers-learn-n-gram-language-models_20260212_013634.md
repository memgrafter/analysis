---
ver: rpa2
title: Can Transformers Learn $n$-gram Language Models?
arxiv_id: '2410.03001'
source_url: https://arxiv.org/abs/2410.03001
tags:
- n-gram
- neural
- performance
- representation-based
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper compares transformers to classical n-gram estimation\
  \ techniques for learning n-gram language models. It studies transformers\u2019\
  \ ability to learn both representation-based and general n-gram models of varying\
  \ complexity."
---

# Can Transformers Learn $n$-gram Language Models?

## Quick Facts
- arXiv ID: 2410.03001
- Source URL: https://arxiv.org/abs/2410.03001
- Authors: Anej Svete; Nadav Borenstein; Mike Zhou; Isabelle Augenstein; Ryan Cotterell
- Reference count: 40
- Transformers perform better on representation-based n-gram models but struggle with general n-gram models requiring arbitrary next-symbol distributions

## Executive Summary
This paper investigates whether transformers can learn $n$-gram language models by comparing their performance to classical count-based smoothing methods. The study examines both representation-based $n$-gram models (where parameters are shared) and general $n$-gram models (with arbitrary next-symbol probabilities) across varying complexity levels. Results show transformers excel at representation-based models, matching or outperforming specialized neural baselines, but underperform classical methods when learning general $n$-gram models that require arbitrary next-symbol distributions.

## Method Summary
The authors conduct experiments using synthetic languages with controlled complexity to systematically compare transformer architectures against classical $n$-gram estimation techniques. They evaluate both representation-based models (where parameters are shared across contexts) and general $n$-gram models (where each context has arbitrary next-symbol probabilities). The study varies the number of transformer heads and layers to assess their impact on performance, and compares soft attention versus sparse attention mechanisms. Performance is measured across different $n$-gram complexities to understand the limits of transformer learning capabilities.

## Key Results
- Transformers outperform specialized neural baselines on representation-based $n$-gram models
- For general $n$-gram models, transformers perform worse than classical count-based smoothing methods
- Increasing transformer heads and layers significantly improves performance, aligning with theoretical predictions
- Sparse attention mechanisms consistently outperform soft attention in learning $n$-gram models

## Why This Works (Mechanism)
Transformers show strong inductive biases for learning representation-based $n$-gram models because these models share parameters across contexts, which aligns well with the transformer's ability to learn distributed representations. The self-attention mechanism can effectively capture the shared patterns in representation-based models. However, transformers struggle with general $n$-gram models because these require learning arbitrary next-symbol distributions for each context, which conflicts with the transformer's tendency to learn shared representations. The sparse attention mechanism helps by allowing the model to focus on relevant context elements without the interference from soft attention's distributed weighting across all positions.

## Foundational Learning
- $n$-gram language models: Statistical models predicting next tokens based on previous $n-1$ tokens; needed to understand what transformers are being tested against
- Representation-based vs general $n$-gram models: The distinction between shared-parameter models and arbitrary distribution models; critical for interpreting transformer performance differences
- Sparse vs soft attention: Different mechanisms for focusing on relevant context; sparse attention's superiority indicates transformers benefit from more selective context processing
- Count-based smoothing methods: Classical techniques for handling unseen $n$-grams; serves as baseline for evaluating transformer performance
- Synthetic language experiments: Controlled test environments for systematic evaluation; allows precise measurement of transformer capabilities across complexity levels

## Architecture Onboarding
Component map: Input sequence -> Positional encoding -> Multi-head attention -> Feed-forward network -> Output distribution
Critical path: The multi-head attention mechanism is crucial as it determines how context is processed and representations are formed, directly impacting the ability to learn $n$-gram patterns
Design tradeoffs: Sparse attention provides better performance but at increased computational complexity; more heads and layers improve learning but increase model size and training time
Failure signatures: Poor performance on general $n$-gram models indicates the model is overfitting to shared representations rather than learning arbitrary distributions
First experiments: 1) Test varying numbers of attention heads to find optimal configuration 2) Compare performance across different $n$-gram orders (bigrams, trigrams, etc.) 3) Evaluate the impact of positional encoding schemes on learning capability

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on synthetic languages may not capture the full complexity of natural language processing tasks
- Comparison primarily against classical smoothing methods rather than modern neural approaches
- Does not explore hybrid approaches that might combine transformer architectures with traditional $n$-gram techniques

## Confidence
- High confidence in transformers excelling at representation-based $n$-gram models (clear empirical evidence and theoretical backing)
- Medium confidence in transformers struggling with general $n$-gram models (comparison primarily against classical methods)
- High confidence in sparse attention outperforming soft attention (specific experimental setup demonstrates this clearly)

## Next Checks
1. Test transformer performance on natural language datasets to assess real-world applicability of the findings
2. Compare transformer results against modern neural $n$-gram approaches rather than just classical smoothing methods
3. Investigate hybrid models that combine transformer architectures with traditional $n$-gram techniques to potentially leverage benefits of both approaches