---
ver: rpa2
title: 'Brevity is the soul of wit: Pruning long files for code generation'
arxiv_id: '2407.00434'
source_url: https://arxiv.org/abs/2407.00434
tags:
- pruning
- data
- code
- length
- files
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper demonstrates that embedding-based pruning methods for
  code generation models are confounded by file length, and proposes a simple heuristic
  of removing long files to improve training efficiency and performance. The authors
  find that long Python files in web-scraped datasets are often low-quality "spaghetti
  code" or large data arrays, yet disproportionately contribute to the dataset due
  to their length.
---

# Brevity is the soul of wit: Pruning long files for code generation

## Quick Facts
- **arXiv ID:** 2407.00434
- **Source URL:** https://arxiv.org/abs/2407.00434
- **Reference count:** 21
- **Primary result:** Embedding-based pruning methods are confounded by file length; simple length-based pruning removes low-quality long files, achieving 2x training efficiency improvement and 3.5% absolute performance gains on HumanEval.

## Executive Summary
This paper demonstrates that embedding-based pruning methods for code generation models are confounded by file length, and proposes a simple heuristic of removing long files to improve training efficiency and performance. The authors find that long Python files in web-scraped datasets are often low-quality "spaghetti code" or large data arrays, yet disproportionately contribute to the dataset due to their length. By pruning the longest files, they achieve up to 2x training efficiency improvement (matching performance) or 3.5% absolute improvement on HumanEval (matching compute). However, they also observe that aggressive pruning can lead to overfitting on shorter files and increased perplexity on held-out long files, suggesting a potential tradeoff between short- and long-form code generation capabilities.

## Method Summary
The authors propose length-based pruning as a simple heuristic for removing low-quality long files from code datasets. They create pruned datasets by removing specified percentages of tokens from the longest files (10%, 20%, 50%), train Llama2 7B models for 16,000 steps with batch size 1.3M tokens using AdamW optimizer, and evaluate performance on HumanEval and MBPP benchmarks. The approach is motivated by findings that long files in web-scraped datasets are disproportionately low-quality and that embedding-based pruning methods are confounded by file length.

## Key Results
- Length-based pruning achieves up to 2x training efficiency improvement (matching performance) by removing the longest files
- Pruning 20% of tokens from longest files (2% of total files) yields 3.5% absolute improvement on HumanEval while matching compute
- Embedding-based pruning methods are confounded by file length, with shorter files clustering tightly in embedding space while longer files are more dispersed
- Aggressive pruning (50% tokens) leads to overfitting on shorter files and increased perplexity on held-out long files

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Length-based pruning removes low-quality code files that contribute noise but dominate token count.
- Mechanism: Long files in web-scraped code datasets are disproportionately composed of low-quality content (e.g., large data arrays, repeated boilerplate), so removing them reduces noise while maintaining most useful signal.
- Core assumption: The longest files are low quality and do not contribute meaningfully to learning useful code generation patterns.
- Evidence anchors:
  - [abstract] "long Python files in web-scraped datasets are often low-quality 'spaghetti code' or large data arrays, yet disproportionately contribute to the dataset due to their length"
  - [section] "we find that long Python files, which are often believed to be 'higher quality' (Longpre et al., 2023), are often actually very low quality... yet make up disproportionately large amounts of training datasets due to their length"
  - [corpus] "Average neighbor FMR=0.443" - moderate relatedness to pruning topics, suggesting the corpus supports the general pruning discussion
- Break condition: If long files contain valuable, unique code patterns or if dataset composition differs (e.g., curated repositories rather than web scrapes).

### Mechanism 2
- Claim: Pruning long files improves training efficiency by reducing token count without sacrificing performance.
- Mechanism: Removing the longest files reduces the total number of tokens processed per epoch, allowing the model to train on fewer tokens while achieving equivalent downstream performance.
- Core assumption: The relationship between file length and token count is strong enough that removing a small percentage of files removes a large percentage of tokens.
- Evidence anchors:
  - [abstract] "By pruning the longest files, they achieve up to 2x training efficiency improvement (matching performance)"
  - [section] "pruning 20% of tokens from the longest files on the full dataset would correspond to pruning just 2% of files"
  - [corpus] No direct evidence; the corpus focuses on related pruning methods but doesn't quantify token/file ratios
- Break condition: If file length distribution changes or if shorter files become proportionally more numerous in the dataset.

### Mechanism 3
- Claim: Embedding-based pruning methods are confounded by file length, making length-based heuristics more reliable.
- Mechanism: StarEncoder embeddings cluster shorter files tightly in a small region of embedding space, while longer files are more dispersed; this creates a correlation between embedding distance and file length that biases pruning decisions.
- Core assumption: The embedding space structure reflects file length rather than semantic content, making length-based pruning a simpler and more direct approach.
- Evidence anchors:
  - [abstract] "embedding-based methods are often confounded by length"
  - [section] "we find that embeddings used in prior work are confounded by document length" and "shorter files appear to be mapped to a very small and dense region of embedding space"
  - [corpus] "Average neighbor FMR=0.443" - moderate relatedness suggests the corpus discussion of embedding methods is relevant but not definitive
- Break condition: If embedding methods are improved to be length-invariant or if file length no longer correlates with embedding distances.

## Foundational Learning

- Concept: Data pruning in machine learning
  - Why needed here: Understanding different pruning approaches (embedding-based, heuristic-based, classifier-based) is essential to evaluate why length-based pruning outperforms alternatives
  - Quick check question: What are the three main categories of data pruning methods mentioned in the paper?

- Concept: Code quality assessment
  - Why needed here: Recognizing what constitutes "low-quality" code (e.g., spaghetti code, large data arrays) is crucial for understanding why long files should be pruned
  - Quick check question: According to the paper, what are two examples of low-quality content commonly found in long Python files?

- Concept: Token vs. file count relationships
  - Why needed here: Understanding that file length distributions are right-skewed explains why removing a small percentage of files removes a large percentage of tokens
  - Quick check question: If 2% of files account for 20% of tokens, what does this tell you about the distribution of file lengths?

## Architecture Onboarding

- Component map: Data → Length-based pruning → Training → Evaluation → Analysis of results and potential overfitting
- Critical path: Data → Length-based pruning → Training → Evaluation → Analysis of results and potential overfitting
- Design tradeoffs: Aggressive pruning improves efficiency but risks overfitting to shorter files; embedding-based methods are more complex but potentially more nuanced; classifier-based methods could be more precise but require labeled data
- Failure signatures: Performance degradation on HumanEval/MBPP, increased perplexity on held-out long files, overfitting to shorter files, or failure to improve training efficiency
- First 3 experiments:
  1. Implement length calculation and sorting for The Stack Python dataset, verify the right-skewed distribution
  2. Create pruned datasets with 10%, 20%, and 50% token removal from longest files, confirm token/file count ratios
  3. Train baseline model (no pruning) and length-pruned models, compare training efficiency and downstream performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of document length pruning on long-form code generation capabilities, and how can this tradeoff be optimized for different use cases?
- Basis in paper: [explicit] The paper observes that aggressive pruning methods lead to overfitting on shorter files and increased perplexity on held-out long files, suggesting a potential tradeoff between short- and long-form code generation capabilities.
- Why unresolved: The paper does not provide a clear solution to this tradeoff, and further research is needed to determine how to optimize data pruning methods for different use cases and context lengths.
- What evidence would resolve it: A study comparing the performance of models trained on datasets with varying degrees of length pruning on both short-form and long-form code generation tasks, across different context lengths and use cases.

### Open Question 2
- Question: How do embedding-based data pruning methods perform when applied to code datasets with different length distributions or characteristics?
- Basis in paper: [explicit] The paper finds that StarEncoder embeddings are confounded by file length in Python code datasets, but does not explore how this finding generalizes to other code datasets or domains.
- Why unresolved: The paper only examines one specific code dataset (Python subset of The Stack) and one embedding method (StarEncoder), so it is unclear how these findings apply to other datasets or embedding methods.
- What evidence would resolve it: An empirical study comparing the performance of various embedding-based pruning methods on a diverse set of code datasets with different length distributions and characteristics.

### Open Question 3
- Question: Can classifier-based pruning methods effectively identify and remove low-quality long files from code datasets, and how do they compare to heuristic-based methods?
- Basis in paper: [inferred] The paper does not explore classifier-based pruning methods, but mentions that they could potentially be effective in filtering out low-quality long files from code datasets.
- Why unresolved: The paper focuses on comparing heuristic-based and embedding-based methods, and does not investigate the potential of classifier-based methods for code data pruning.
- What evidence would resolve it: A comparison of the performance of classifier-based pruning methods (e.g., using quality classifiers) and heuristic-based methods (e.g., length pruning) on code datasets, evaluated in terms of training efficiency and downstream performance.

## Limitations

- The claim that long files are universally low-quality in web-scraped datasets may not generalize to curated or enterprise codebases where longer files could contain valuable, complex implementations
- The tradeoff between short-form and long-form code generation capabilities remains unresolved, with potential negative impacts on long-form generation ability not fully characterized
- The effectiveness of length-based pruning depends heavily on the specific dataset characteristics, and results may not transfer to other code generation datasets or programming languages

## Confidence

**High Confidence:** The experimental results showing 2x training efficiency improvement and 3.5% absolute performance gains on HumanEval are well-supported by the controlled experiments and clear methodology. The mechanism of removing noisy, low-quality long files to improve training efficiency is directly demonstrated.

**Medium Confidence:** The claim that embedding-based pruning methods are confounded by file length is supported by qualitative observations but lacks comprehensive quantitative analysis of the embedding space structure and its relationship to file length across different embedding methods.

**Low Confidence:** The generalization of these findings to other code datasets, programming languages, or different model architectures remains uncertain without additional experiments. The long-term effects of aggressive pruning on model capabilities for different code generation tasks are not fully explored.

## Next Checks

1. **Dataset Diversity Validation:** Test length-based pruning on alternative code datasets (e.g., GitHub curated repositories, enterprise codebases) to verify if the pattern of long files being low-quality holds across different data sources and collection methods.

2. **Embedding Space Analysis:** Conduct comprehensive quantitative analysis of StarEncoder embedding distributions across different file lengths, including measuring embedding variance, density, and clustering patterns to confirm the confounding effect of file length on semantic embeddings.

3. **Long-Form Generation Impact:** Evaluate the pruned models' performance on tasks requiring long-form code generation (e.g., complete function implementations, multi-file projects) to quantify the potential tradeoff between efficiency gains and long-form generation capabilities.