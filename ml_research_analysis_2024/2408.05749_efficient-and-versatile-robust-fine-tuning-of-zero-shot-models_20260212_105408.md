---
ver: rpa2
title: Efficient and Versatile Robust Fine-Tuning of Zero-shot Models
arxiv_id: '2408.05749'
source_url: https://arxiv.org/abs/2408.05749
tags:
- fine-tuning
- learning
- clip
- tasks
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'R-Adapter improves robustness of parameter-efficient fine-tuning
  for zero-shot models by introducing three self-ensemble techniques: adapter dropping,
  temporal accumulation, and weight-space re-scaling. It combines lightweight adapter
  modules with these techniques to achieve state-of-the-art performance across diverse
  tasks including image classification, cross-modal retrieval, and open-vocabulary
  segmentation.'
---

# Efficient and Versatile Robust Fine-Tuning of Zero-shot Models

## Quick Facts
- **arXiv ID:** 2408.05749
- **Source URL:** https://arxiv.org/abs/2408.05749
- **Reference count:** 40
- **Primary result:** R-Adapter achieves 54.3% OOD accuracy on ImageNet while tuning only 13% of CLIP encoder parameters

## Executive Summary
R-Adapter introduces a parameter-efficient fine-tuning method for zero-shot models that significantly improves out-of-distribution (OOD) robustness through three self-ensemble techniques: adapter dropping, temporal accumulation, and weight-space re-scaling. By combining lightweight adapter modules with these techniques, R-Adapter achieves state-of-the-art performance across diverse tasks including image classification, cross-modal retrieval, and open-vocabulary segmentation. The method is particularly effective at maintaining pre-trained knowledge while adapting to specific tasks, requiring only a small fraction of the original model parameters to be tuned.

## Method Summary
R-Adapter employs lightweight adapter modules inserted after each transformer layer in pre-trained vision-language models like CLIP. During training, it applies adapter dropping (randomly disabling adapters with probability p) to create dynamic ensembles, accumulates adapter weights using exponential moving averages, and scales adapters before re-parameterization. At inference, adapters are merged into pre-trained layers through re-parameterization, enabling weight-space ensemble within a single model. The method uses a novel MPM-NCE loss that handles multiple positive pairs and incorporates angular margins to improve alignment and discriminative feature learning across vision-language tasks.

## Key Results
- Achieves 54.3% OOD accuracy on ImageNet while tuning only 13% of CLIP encoder parameters
- Outperforms existing robust fine-tuning methods by 2 percentage points on OOD benchmarks
- Demonstrates strong performance across multiple tasks: 98.6% R@1 for cross-modal retrieval and 48.6 mIOU for open-vocabulary segmentation

## Why This Works (Mechanism)

### Mechanism 1: Adapter Dropping
Adapter dropping creates a dynamic ensemble by stochastically deactivating adapters during training, forcing the model to rely on both pre-trained and adapter features. This improves robustness by preventing overfitting to in-distribution data and encouraging better generalization. The randomness introduced allows the model to learn features robust across different combinations of pre-trained and fine-tuned knowledge.

### Mechanism 2: Weight-Space Ensemble by Re-scaling
Weight-space ensemble by re-scaling enables interpolation between pre-trained and fine-tuned models without storing two separate models. The re-scaling coefficient α interpolates between original pre-trained weights and re-parameterized fine-tuned weights, allowing seamless weight-space ensemble within a single model. This captures the optimal balance between generalization and task-specific performance.

### Mechanism 3: MPM-NCE Loss
MPM-NCE loss improves alignment and discriminative feature learning by considering multiple positive pairs and adding angular margins. Instead of single positive pairs, MPM-NCE uses soft labels that assign equal probability to multiple positive pairs, promoting more precise alignment across various image-text pairs. The angular margin δ penalizes negative pairs, encouraging the model to learn more discriminative features.

## Foundational Learning

- **Concept: Contrastive learning and InfoNCE loss**
  - Why needed here: R-Adapter builds upon CLIP's pre-training objective and extends it with MPM-NCE for downstream tasks. Understanding contrastive learning is essential to grasp how the loss functions work.
  - Quick check question: What is the main difference between InfoNCE and MPM-NCE in terms of positive pairs?

- **Concept: Adapter modules and parameter-efficient fine-tuning**
  - Why needed here: R-Adapter uses lightweight adapter modules to fine-tune pre-trained models efficiently. Understanding adapter design is crucial for implementing and modifying R-Adapter.
  - Quick check question: How does R-Adapter's adapter design differ from standard Houlsby adapters?

- **Concept: Weight-space ensemble and model interpolation**
  - Why needed here: R-Adapter uses re-scaling and re-parameterization to achieve weight-space ensemble within a single model. This technique is key to its efficiency and robustness.
  - Quick check question: How does R-Adapter's re-scaling approach differ from traditional weight-space ensemble methods like WiSE-FT?

## Architecture Onboarding

- **Component map:** Pre-trained CLIP encoders (frozen) -> Adapter modules (tunable, one per transformer layer) -> Self-ensemble techniques (adapter dropping, temporal accumulation, weight-space re-scaling) -> MPM-NCE loss (multiple positives, angular margin)

- **Critical path:** 1) Initialize adapters after each transformer layer in CLIP encoders 2) During training: apply adapter dropping, accumulate adapter weights, and scale adapters 3) During inference: re-parameterize adapters into pre-trained layers using accumulated and scaled weights 4) Compute MPM-NCE loss with multiple positive pairs and angular margin

- **Design tradeoffs:** Adapter rank vs. performance (higher rank improves capacity but increases parameters), Re-scaling coefficient α (balances between pre-trained and fine-tuned knowledge), Drop probability p (controls regularization strength vs. task-specific learning)

- **Failure signatures:** Poor OOD performance (likely due to insufficient adapter dropping or re-scaling), Overfitting to ID data (may need stronger regularization - higher p or more accumulation), Slow convergence (could indicate suboptimal learning rate or momentum)

- **First 3 experiments:** 1) Verify adapter dropping works by checking if adapter outputs are zero during training with probability p 2) Test re-parameterization by comparing outputs with and without re-scaling at inference 3) Validate MPM-NCE by ensuring multiple positive pairs are correctly handled in the loss computation

## Open Questions the Paper Calls Out

### Open Question 1
How does the R-Adapter's performance scale with even larger vision-language models beyond ViT-L/14@336px? The paper only tests up to CLIP ViT-L/14@336px and does not investigate how the method performs on models with more parameters or larger input sizes.

### Open Question 2
What is the optimal strategy for selecting the re-scaling coefficient α for different tasks and datasets? The paper provides empirical values for specific tasks but does not offer a systematic approach for determining the optimal α for new, unseen tasks or datasets.

### Open Question 3
How does the R-Adapter's performance compare to other parameter-efficient fine-tuning methods when applied to video tasks? The paper focuses on image-based tasks and does not investigate the method's effectiveness on video data, which presents unique challenges due to temporal dynamics.

## Limitations

- The MPM-NCE loss formulation and its impact on different task types requires further investigation, particularly how the angular margin and multiple positive pairs affect performance across diverse domains.
- The computational overhead of temporal accumulation during training is not thoroughly quantified, though it's reduced at inference.
- The paper lacks ablation studies on the relative importance of each self-ensemble technique, making it difficult to isolate their individual contributions to observed performance gains.

## Confidence

- **High confidence:** The adapter dropping mechanism and its implementation details are well-specified with clear mathematical formulations and straightforward verification procedures. The empirical results on ImageNet classification show consistent improvements across multiple OOD datasets.
- **Medium confidence:** The weight-space ensemble via re-scaling and re-parameterization is theoretically sound, but the practical benefits depend heavily on the choice of α and may vary across different pre-trained models and tasks.
- **Low confidence:** The MPM-NCE loss formulation, while promising, lacks comprehensive ablation studies showing the individual contributions of multiple positives and angular margins. The generalization claims to other vision-language tasks beyond those tested remain unverified.

## Next Checks

1. **Ablation of self-ensemble techniques:** Systematically disable each of the three techniques (adapter dropping, temporal accumulation, weight-space re-scaling) individually and evaluate their isolated impact on OOD performance to determine which components are essential versus beneficial.

2. **Cross-model generalization:** Apply R-Adapter to pre-trained models beyond CLIP (e.g., BLIP, FLAVA) to verify the method's robustness across different vision-language model architectures and pre-training objectives.

3. **Memory and compute overhead validation:** Measure the actual training-time memory and computational overhead introduced by temporal accumulation and adapter dropping, then verify that the claimed inference-time efficiency improvements hold across different batch sizes and hardware configurations.