---
ver: rpa2
title: Enhancing Cross-Modal Contextual Congruence for Crowdfunding Success using
  Knowledge-infused Learning
arxiv_id: '2402.03607'
source_url: https://arxiv.org/abs/2402.03607
tags:
- knowledge
- image
- multimodal
- text
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study integrates external commonsense knowledge from ConceptNet
  into compact Visual Language Models to enhance cross-modal contextual congruence
  in multimodal representations for predicting crowdfunding campaign success. The
  approach retrieves relevant concepts using semantic search, generates knowledge
  embeddings with models like TransE and DistMult, and fuses them with multimodal
  representations via multi-head cross-attention.
---

# Enhancing Cross-Modal Contextual Congruence for Crowdfunding Success using Knowledge-infused Learning

## Quick Facts
- arXiv ID: 2402.03607
- Source URL: https://arxiv.org/abs/2402.03607
- Reference count: 40
- Primary result: Knowledge-infused multimodal models achieve up to 0.95 precision, 0.91 recall, and 0.94 AUC on crowdfunding success prediction

## Executive Summary
This study addresses the challenge of predicting crowdfunding campaign success by integrating external commonsense knowledge from ConceptNet into compact Visual Language Models (VLMs). The approach enhances cross-modal contextual congruence by retrieving relevant concepts using semantic search, generating knowledge embeddings with models like TransE and DistMult, and fusing them with multimodal representations via multi-head cross-attention. Experimental results demonstrate that knowledge-infused models consistently outperform baselines without knowledge, achieving significant improvements in precision, recall, and AUC metrics. The findings show that knowledge infusion bridges semantic gaps between text and image modalities, reduces hallucinations, and improves predictive performance.

## Method Summary
The method employs a multimodal learning framework using compact VLMs (BERT/RoBERTa for text, ResNet/ViT for images) trained on Kickstarter crowdfunding projects. Knowledge retrieval uses Sentence-BERT to fetch top-k relevant concepts from ConceptNet for each image-text pair. KG embedding models (TransE, RotatE, DistMult) generate knowledge representations, which are then fused with multimodal representations via multi-head cross-attention. The model is trained with cross-entropy loss and Adam optimizer, with performance evaluated using precision, recall, F1-score, and AUC metrics.

## Key Results
- Knowledge-infused models achieve up to 0.95 precision, 0.91 recall, and 0.94 AUC
- Models with knowledge consistently outperform baseline models without knowledge
- Integration bridges semantic gaps between text and image modalities, reducing hallucinations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: External knowledge from ConceptNet reduces the semantic distance between text and image modalities, improving cross-modal contextual congruence.
- Mechanism: Semantic search retrieves relevant concepts from ConceptNet, which are then embedded and fused with multimodal representations via multi-head cross-attention. This aligns latent representations across modalities.
- Core assumption: Semantic similarity in ConceptNet reflects the intended holistic meaning in multimodal content.
- Evidence anchors: [abstract] "Our results show that external knowledge commonsense bridges the semantic gap between text and image modalities..."; [section] "We utilize ConceptNet, a commonsense KG with rich semantic knowledge of the world, to synthesize these diverse multimodal cues..."
- Break condition: If ConceptNet lacks relevant concepts for the domain or if the semantic search fails to retrieve meaningful concepts, the bridging effect will diminish.

### Mechanism 2
- Claim: Knowledge infusion mitigates hallucinations in large vision-language models by providing factual grounding.
- Mechanism: Appending retrieved ConceptNet concepts to prompts constrains model outputs, reducing the generation of non-existent connections.
- Core assumption: Hallucinations arise from insufficient semantic grounding, and explicit knowledge can constrain generation.
- Evidence anchors: [abstract] "The inclusion of knowledge in the input gets these modalities closer by 9.9%."; [section] "We prompted large VLMs, such as LLaV A and BLIP, using pairs of text... with knowledge. As shown in Table I, the generated caption with knowledge produces less hallucination..."
- Break condition: If the retrieved knowledge is noisy or irrelevant, it may introduce contradictions that exacerbate hallucinations.

### Mechanism 3
- Claim: Enhanced cross-modal contextual congruence leads to more consistent and reliable predictive performance in crowdfunding success.
- Mechanism: Knowledge-infused multimodal representations capture richer semantic relationships, improving classification accuracy.
- Core assumption: Predictive performance is directly tied to the quality of cross-modal semantic alignment.
- Evidence anchors: [abstract] "Our findings indicate that knowledge-infused multimodal representations consistently outperform baseline models..."; [section] "When we compare the models with the same encoders but with or without knowledge, we observe a consistent enhancement in the performance of the models with knowledge."
- Break condition: If the classification task does not depend on cross-modal semantic relationships, the enhancement in congruence will not translate to performance gains.

## Foundational Learning

- Concept: Knowledge Graphs (KGs)
  - Why needed here: KGs provide structured commonsense knowledge to bridge semantic gaps between modalities.
  - Quick check question: What is the primary difference between a knowledge graph and a traditional database?

- Concept: Multimodal Learning
  - Why needed here: Multimodal learning integrates information from multiple modalities (text, image) for unified understanding.
  - Quick check question: How does multimodal learning differ from unimodal learning in terms of data representation?

- Concept: Knowledge Graph Embeddings (KGEs)
  - Why needed here: KGEs represent entities and relations as dense vectors, enabling integration with multimodal representations.
  - Quick check question: What is the purpose of using knowledge graph embeddings in multimodal learning?

## Architecture Onboarding

- Component map: Text encoder (BERT/RoBERTa) -> Image encoder (ResNet/ViT) -> Knowledge retrieval (Sentence-BERT + ConceptNet) -> KGE models (TransE, RotatE, DistMult) -> Knowledge fusion (linear projection + multi-head cross-attention) -> Classification layer (softmax)
- Critical path: Text/Image encoding → Knowledge retrieval → KGE generation → Knowledge fusion → Classification
- Design tradeoffs:
  - Using compact models (BERT, ResNet) for accessibility vs. large models (LLaVA, GPT-4) for potential performance gains
  - Retrieving top-k concepts (k=10) for relevance vs. completeness
  - KGE models (TransE, RotatE, DistMult) for relational encoding vs. simplicity
- Failure signatures:
  - High semantic distance between text and image clusters
  - Noisy or irrelevant knowledge concepts leading to misclassification
  - Hallucinations persisting despite knowledge infusion
- First 3 experiments:
  1. Measure semantic distance (cosine similarity) between text and image representations with and without knowledge.
  2. Compare classification performance (precision, recall, F1, AUC) of models with and without knowledge infusion.
  3. Analyze error cases to identify whether knowledge infusion reduces omissions or introduces noisy commissions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the incorporation of knowledge graphs specifically reduce hallucinations in large multimodal models, and what is the causal mechanism linking semantic distance reduction to hallucination mitigation?
- Basis in paper: [explicit] The paper demonstrates that knowledge graphs reduce semantic distance between modalities and mitigate hallucinations, as shown in Table I comparing captions with and without knowledge.
- Why unresolved: The paper shows correlation between knowledge incorporation and hallucination reduction but does not establish a causal mechanism or explain why reducing semantic distance specifically addresses hallucinations.
- What evidence would resolve it: Controlled experiments comparing hallucination rates when varying semantic distance while holding other factors constant, or ablation studies isolating the semantic distance reduction component.

### Open Question 2
- Question: What is the optimal size and composition of knowledge graph embeddings for maximizing cross-modal contextual congruence in multimodal learning?
- Basis in paper: [inferred] The paper uses 256-dimensional KG embeddings and three different embedding models (TransE, RotatE, DistMult) but does not explore the parameter space or compare different embedding dimensions and compositions.
- Why unresolved: The paper uses fixed embedding dimensions and models without exploring alternatives or justifying these choices through systematic comparison.
- What evidence would resolve it: Systematic experiments varying embedding dimensions and comparing different KG embedding models while measuring cross-modal congruence and downstream task performance.

### Open Question 3
- Question: How does knowledge infusion affect fairness and bias in multimodal models, particularly regarding underrepresented groups in crowdfunding campaigns?
- Basis in paper: [explicit] The paper mentions that higher AUC signals potential improvement in fairness and discusses implications for underrepresented groups, but does not conduct fairness assessments.
- Why unresolved: While the paper acknowledges fairness implications and suggests potential benefits, it does not implement fairness metrics or analyze bias in model predictions across demographic groups.
- What evidence would resolve it: Fairness evaluations including demographic parity, equal opportunity, and disparate impact metrics across different campaign categories and creator demographics.

## Limitations

- Study relies on a single crowdfunding dataset from Kickstarter, limiting generalizability to other domains
- Knowledge graph (ConceptNet) may contain domain-specific gaps or noisy triples that could introduce irrelevant information during retrieval
- Approach uses compact VLMs rather than state-of-the-art large models, potentially leaving performance gains unrealized

## Confidence

- **High confidence**: Claims about knowledge-infused models outperforming baselines without knowledge
- **Medium confidence**: Claims about semantic gap bridging and hallucination reduction
- **Low confidence**: Claims about generalizability to other multimodal domains

## Next Checks

1. Conduct cross-domain validation by applying the knowledge-infused framework to different multimodal datasets (e.g., social media content, e-commerce product listings) to assess generalizability.
2. Perform ablation studies isolating the impact of knowledge retrieval, KGE models, and fusion mechanisms to quantify individual contributions to performance improvements.
3. Implement quantitative hallucination measurement using metrics like semantic similarity between generated captions and ground truth, or human evaluation of caption factual accuracy.