---
ver: rpa2
title: 'FactGenius: Combining Zero-Shot Prompting and Fuzzy Relation Mining to Improve
  Fact Verification with Knowledge Graphs'
arxiv_id: '2406.01311'
source_url: https://arxiv.org/abs/2406.01311
tags:
- connections
- claim
- knowledge
- fact-checking
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FactGenius combines zero-shot prompting of large language models
  with fuzzy relation mining on knowledge graphs to improve fact-checking accuracy.
  By filtering LLM-generated connections using Levenshtein distance similarity and
  validating them against DBpedia, the system achieves superior performance on the
  FactKG dataset, with a fine-tuned RoBERTa classifier reaching 85% accuracy.
---

# FactGenius: Combining Zero-Shot Prompting and Fuzzy Relation Mining to Improve Fact Verification with Knowledge Graphs

## Quick Facts
- arXiv ID: 2406.01311
- Source URL: https://arxiv.org/abs/2406.01311
- Authors: Sushant Gautam
- Reference count: 11
- Primary result: 85% accuracy on FactKG dataset using RoBERTa classifier with two-stage LLM connection filtering and DBpedia validation

## Executive Summary
FactGenius presents a novel approach to fact verification that combines zero-shot prompting of large language models with fuzzy relation mining on knowledge graphs. The system addresses the limitations of traditional fact-checking methods by generating potential entity connections through LLM prompts, filtering these connections using Levenshtein distance similarity, and validating them against DBpedia knowledge graph data. This two-stage approach significantly improves fact verification accuracy, particularly for conjunction, existence, and negation reasoning types. The method demonstrates that integrating structured knowledge graph data with advanced language model capabilities can substantially enhance the reliability of automated fact-checking systems.

## Method Summary
FactGenius employs a two-stage pipeline for fact verification that leverages both large language models and knowledge graph data. In the first stage, the system uses zero-shot prompting to generate potential connections between entities based on input facts. These LLM-generated connections are then filtered using fuzzy relation mining with Levenshtein distance similarity to remove unlikely or incorrect connections. The second stage involves validating the filtered connections against the DBpedia knowledge graph, ensuring that only verified relationships are used for fact verification. A fine-tuned RoBERTa classifier is then applied to make the final verification decision, achieving 85% accuracy on the FactKG dataset. This approach effectively combines the generative capabilities of LLMs with the structured, verified information contained in knowledge graphs.

## Key Results
- Achieved 85% accuracy on FactKG dataset using fine-tuned RoBERTa classifier
- Outperformed existing baselines across conjunction, existence, and negation reasoning types
- Demonstrated significant improvement in fact verification through two-stage LLM connection filtering and DBpedia validation

## Why This Works (Mechanism)
FactGenius works by leveraging the complementary strengths of large language models and structured knowledge graphs. The zero-shot prompting capability of LLMs allows the system to generate potential entity relationships that may not be explicitly present in the knowledge graph, expanding the scope of fact verification. The fuzzy relation mining with Levenshtein distance similarity acts as a noise filter, removing spurious connections that could lead to incorrect verification decisions. By validating the remaining connections against DBpedia, the system ensures that only reliable, verified relationships are used in the final classification. This combination of generative expansion, intelligent filtering, and knowledge graph validation creates a robust fact-checking pipeline that can handle complex reasoning tasks while maintaining high accuracy.

## Foundational Learning
1. **Zero-shot prompting**: The ability of LLMs to generate responses without specific training on the task. Why needed: Enables generation of potential entity relationships without requiring labeled training data. Quick check: Test LLM's ability to generate relevant connections for unseen facts.

2. **Levenshtein distance similarity**: A string metric that measures the minimum number of single-character edits needed to change one word into another. Why needed: Provides a quantitative measure for filtering unlikely LLM-generated connections. Quick check: Verify that similarity scores effectively distinguish between correct and incorrect connections.

3. **Knowledge graph validation**: The process of checking generated relationships against a structured knowledge base. Why needed: Ensures that only verified, reliable information is used for final fact verification. Quick check: Confirm that DBpedia contains sufficient coverage for the target fact domain.

4. **Fine-tuning classifiers**: The process of adapting pre-trained models to specific tasks using task-relevant data. Why needed: Allows the system to learn domain-specific patterns for accurate fact verification. Quick check: Evaluate classifier performance on held-out validation set.

5. **Two-stage filtering pipeline**: A methodology that combines multiple filtering approaches to improve result quality. Why needed: Reduces noise and improves accuracy by applying sequential validation steps. Quick check: Compare performance with single-stage approaches.

## Architecture Onboarding

**Component Map:**
LLM (zero-shot prompting) -> Fuzzy Relation Mining (Levenshtein filtering) -> Knowledge Graph Validation (DBpedia) -> RoBERTa Classifier -> Final Verification Decision

**Critical Path:**
1. Fact input and entity extraction
2. LLM zero-shot prompting for connection generation
3. Fuzzy relation mining with Levenshtein distance filtering
4. DBpedia knowledge graph validation
5. RoBERTa classifier fine-tuning and inference
6. Final fact verification output

**Design Tradeoffs:**
- **LLM vs. Rule-based approaches**: LLMs offer flexibility and generalization but may generate incorrect connections requiring filtering. Rule-based systems are more predictable but less adaptable to new fact types.
- **Fuzzy matching precision vs. recall**: Stricter Levenshtein thresholds reduce false positives but may miss valid connections. Looser thresholds increase coverage but introduce more noise.
- **Knowledge graph coverage vs. specificity**: Larger knowledge graphs provide broader coverage but may include less verified information. Smaller, curated graphs offer higher reliability but limited scope.

**Failure Signatures:**
- High false positive rate: Indicates insufficient Levenshtein distance filtering or knowledge graph coverage gaps
- Low recall on conjunction facts: Suggests LLM prompting may not be generating sufficient connection candidates
- Classifier confusion between existence and negation: Points to need for better feature engineering or additional training data

**First Experiments:**
1. Evaluate LLM-generated connections against ground truth to measure initial generation quality
2. Test different Levenshtein distance thresholds to optimize filtering performance
3. Compare RoBERTa classifier performance with and without knowledge graph validation features

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on DBpedia knowledge graph limits coverage to entities and relations present in this specific knowledge base
- Fuzzy relation mining approach may introduce noise through false positive connections when Levenshtein distance similarity scores are insufficiently discriminative
- Performance evaluation limited to FactKG dataset, which may not represent diversity of real-world fact-checking scenarios

## Confidence
**High Confidence:** Technical implementation of two-stage filtering and validation pipeline demonstrates solid engineering principles with well-documented 85% accuracy on FactKG dataset.

**Medium Confidence:** Comparative performance against baselines presented, but generalizability to other knowledge graphs or fact-checking datasets remains uncertain. Levenshtein distance choice may not be optimal for all relation types.

**Low Confidence:** Long-term scalability to larger, more diverse knowledge graphs not thoroughly evaluated. Computational efficiency of generating and filtering LLM connections at scale remains unclear.

## Next Checks
1. **Cross-knowledge graph validation:** Test FactGenius performance using alternative knowledge graphs (e.g., Wikidata, YAGO) to assess generalizability beyond DBpedia and identify potential coverage limitations.

2. **Real-world deployment simulation:** Evaluate the system on a curated set of real-world fact-checking claims from news articles and social media to measure practical effectiveness and identify failure modes not captured in the FactKG dataset.

3. **Alternative similarity metric comparison:** Implement and compare alternative fuzzy matching techniques (e.g., semantic similarity, embedding-based methods) against Levenshtein distance to determine if performance can be further improved through different relation mining approaches.