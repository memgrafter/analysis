---
ver: rpa2
title: Fairness Risks for Group-conditionally Missing Demographics
arxiv_id: '2402.13393'
source_url: https://arxiv.org/abs/2402.13393
tags:
- fairness
- error
- ablation
- fairda
- ours
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles fair classification when demographic data is
  partially unavailable, with missingness that depends on the demographic group. The
  authors augment standard fairness risks with probabilistic imputations of missing
  sensitive features and jointly learn the group-conditional missing probabilities
  using a variational autoencoder.
---

# Fairness Risks for Group-conditionally Missing Demographics

## Quick Facts
- **arXiv ID**: 2402.13393
- **Source URL**: https://arxiv.org/abs/2402.13393
- **Authors**: Kaiqi Jiang; Wenzhe Fan; Mao li; Xinhua Zhang
- **Reference count**: 40
- **Primary result**: Fair-SS-VAE method achieves better accuracy-fairness tradeoffs than state-of-the-art baselines on CelebA and UCI Adult datasets with group-conditionally missing demographics

## Executive Summary
This paper addresses the challenge of fair classification when demographic data is partially unavailable, with missingness that depends on the demographic group. The authors propose augmenting standard fairness risks with probabilistic imputations of missing sensitive features, using a variational autoencoder (VAE) to jointly learn the group-conditional missing probabilities. They introduce a Monte Carlo method to efficiently estimate fairness risk with provable concentration bounds and use stop-gradient to prevent fairness optimization of the imputation model. Empirical results demonstrate significant improvements over baselines across different levels of missing demographic data.

## Method Summary
The method augments general fairness risks with probabilistic imputations of sensitive features while jointly learning group-conditional missing probabilities using a variational autoencoder (VAE). The approach models missingness with learnable noising probabilities P(Â|A) and uses stop-gradient to prevent fairness optimization of the imputation model. A Monte Carlo estimation method with O(1/ϵ²) sample complexity provides efficient and provably accurate approximation of fairness risk expectations. The framework is implemented as Fair-SS-VAE, which combines the VAE architecture with fairness-aware classification and achieves improved accuracy-fairness tradeoffs on datasets with varying levels of missing demographic data.

## Key Results
- Fair-SS-VAE significantly outperforms state-of-the-art baselines on CelebA and UCI Adult datasets
- The method achieves better accuracy-fairness tradeoffs under varying levels of missing demographic data
- Monte Carlo estimation with provable concentration bounds provides efficient fairness risk approximation
- Stop-gradient technique effectively prevents fairness optimization of the demographic imputation model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Jointly modeling group-conditionally missing demographics through VAE enables simultaneous imputation of missing sensitive features and fairness-aware classification.
- Mechanism: The VAE encoder estimates posterior probabilities qφ(A|X, Â) for missing demographics while the decoder learns pθ(X|Â,Y,Z). Fairness risk is computed over these probabilistic imputations, allowing differentiable optimization of both imputation and classification.
- Core assumption: Missingness is group-dependent but can be modeled by learnable noise probabilities P(Â|A).
- Evidence anchors:
  - [abstract]: "Our solution augments general fairness risks with probabilistic imputations of the sensitive features, while jointly learning the group-conditionally missing probabilities in a variational auto-encoder."
  - [section]: "We model the group-conditional unavailability with a learnable noising probability P(Â|A) (Yao et al., 2023)."

### Mechanism 2
- Claim: Stop-gradient prevents fairness optimization of the demographic imputation model, ensuring imputation focuses on accuracy rather than fairness.
- Mechanism: During training, gradients from the fairness risk are blocked from flowing back to qφ(A|X, Â), while allowing backpropagation to the classifier parameters. This enforces that demographic imputation remains unbiased toward fairness objectives.
- Core assumption: The demographic imputation model should prioritize accurate estimation over fairness, while fairness enforcement is left to the classifier.
- Evidence anchors:
  - [section]: "The onus of fairness is only supposed to be on the classifier Pf, while qφ(A|X, Â)... is supposed to be recused from fairness risk minimization."
  - [section]: "We resort to the commonly used stop-gradient technique... treating qφ(A|X, Â) as a constant."

### Mechanism 3
- Claim: Monte Carlo estimation with O(1/ϵ²) sample complexity provides efficient and provably accurate approximation of fairness risk expectations.
- Mechanism: Fairness risk E is estimated by drawing N samples from the posterior distributions and computing the empirical mean. Theorem 1 provides concentration bounds guaranteeing ϵ accuracy with confidence 1-δ using N = O(1/ϵ²) samples.
- Core assumption: The fairness risk function is bounded in [0,C], enabling application of McDiarmid's inequality.
- Evidence anchors:
  - [section]: "Our method simply draws N number of iid samples from Ai ~ qφ(A|xi, âi) and Yi ~ Pg(Y |xi)... We prove the following sample complexity for this estimator by leveraging McDiarmid's inequality."
  - [section]: "Theorem 1. Suppose F ∈ [0,C] where C > 0 is a constant. Then for all ϵ > 0, P(|Ên(Z1,...,ZN) - E| ≥ ϵ) ≤ 2 exp(-2Nϵ²/C²)."

## Foundational Learning

- **Concept: Variational Autoencoders (VAEs) and Semi-Supervised VAEs**
  - Why needed here: The VAE architecture provides a principled framework for modeling both observed and missing variables through probabilistic inference, essential for handling group-conditionally missing demographics.
  - Quick check question: What are the three latent variables in the SS-VAE encoder, and how are they assumed to be independent given the observed features?

- **Concept: Fairness metrics and their differentiable estimation**
  - Why needed here: The method requires differentiable approximations of fairness metrics like DEO and DEOPP to enable gradient-based optimization of fair classification.
  - Quick check question: How does the paper estimate P(Ŷ=1|A=a) from the classifier's probabilistic outputs for fairness computation?

- **Concept: Stop-gradient and its role in multi-objective optimization**
  - Why needed here: Prevents the demographic imputation model from being influenced by fairness objectives, ensuring fair classification is enforced only through the classifier.
  - Quick check question: What is the key difference between using stop-gradient versus a two-step approach for imputation and classification?

## Architecture Onboarding

- **Component map**:
  - Encoder (qφ) -> Latent distributions qφ(z|X), qφ(y|X, ŷ), qφ(a|X, â)
  - Decoder (pθ) -> Observations pθ(X|a,y,z) and missingness models Pθ(â|a), Pθ(ŷ|y)
  - Classifier (Pf) -> Uses qφ(y|X, ŷ) for prediction
  - Fairness loss module -> Computes Monte Carlo estimates of fairness risk
  - Stop-gradient layers -> Blocks fairness gradients from reaching imputation models

- **Critical path**:
  1. Forward pass through encoder to get latent distributions
  2. Sample from posteriors for missing demographics and labels
  3. Compute classifier outputs and fairness risk
  4. Backpropagate through stop-gradient to update classifier only
  5. Update encoder/decoder parameters using ELBO loss

- **Design tradeoffs**:
  - Accuracy vs fairness tradeoff controlled by λ hyperparameter
  - Sample size N for Monte Carlo estimation vs computational cost
  - Temperature T in Gumbel-Softmax vs gradient quality
  - Stop-gradient vs joint optimization of imputation and classification

- **Failure signatures**:
  - Poor imputation accuracy: Demographics poorly recovered, fairness metrics unreliable
  - Degraded classification: Accuracy drops significantly due to fairness constraints
  - Training instability: Gradient issues from Monte Carlo estimation or stop-gradient
  - Overfitting: Model performs well on training but poorly on test demographics

- **First 3 experiments**:
  1. Train without fairness regularization (λ=0) to establish baseline accuracy
  2. Train with fairness regularization on fully observed demographics to verify effectiveness
  3. Train with group-conditionally missing demographics at different sparsity levels to test the full pipeline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is the proposed Fair-SS-VAE method to varying degrees of bias in the group-conditional missing probabilities (α and β)?
- Basis in paper: [inferred] The paper shows that Fair-SS-VAE can infer α and β from the data, but the exact recovery values are not always close to the true values.
- Why unresolved: The paper only presents the recovered values for three datasets and three sparsity levels, and does not analyze the sensitivity of the method to errors in the estimated missing probabilities.
- What evidence would resolve it: A systematic study varying the true α and β values and analyzing the impact on the fairness and accuracy of Fair-SS-VAE.

### Open Question 2
- Question: Can the stop-gradient technique be extended to other fairness-aware learning scenarios beyond group-conditionally missing demographics?
- Basis in paper: [explicit] The paper uses stop-gradient to prevent the imputation model from being optimized for fairness, and shows its effectiveness in this setting.
- Why unresolved: The paper only applies stop-gradient to the specific problem of group-conditionally missing demographics, and does not explore its potential in other fairness scenarios.
- What evidence would resolve it: Experiments applying stop-gradient to other fairness problems, such as learning with noisy demographics or proxy features, and comparing the results to existing methods.

### Open Question 3
- Question: How does the proposed Monte Carlo evaluation of the fairness risk compare to other approximation methods in terms of accuracy and computational efficiency?
- Basis in paper: [explicit] The paper introduces a Monte Carlo method with provable concentration bounds for evaluating the fairness risk, and shows its effectiveness in practice.
- Why unresolved: The paper does not compare the Monte Carlo method to other approximation techniques, such as variational inference or importance sampling.
- What evidence would resolve it: A comparative study of different approximation methods for evaluating the fairness risk, including their accuracy, computational cost, and scalability to large datasets.

## Limitations

- The boundedness assumption for fairness risk metrics may not hold for all commonly used fairness measures, potentially limiting the applicability of the Monte Carlo concentration bounds.
- The stop-gradient approach assumes demographic imputation should be recused from fairness optimization, but this may not be optimal when missingness mechanisms themselves introduce bias.
- The empirical results are based on controlled missingness experiments that may not fully capture the complexity of real-world missingness patterns.

## Confidence

- **High confidence**: The VAE-based architecture for modeling group-conditionally missing demographics is well-established and the theoretical framework for Monte Carlo estimation with McDiarmid's inequality is sound when the boundedness assumption holds.
- **Medium confidence**: The empirical results showing improved Pareto frontiers on CelebA and Adult datasets are compelling, but the controlled missingness experiments may not fully capture real-world missingness patterns where group dependencies are more complex.
- **Low confidence**: The stop-gradient mechanism's effectiveness in preventing fairness optimization of the imputation model has limited empirical validation - the paper does not provide ablation studies comparing against alternative approaches like two-step imputation-then-classification or fairness-aware imputation.

## Next Checks

1. **Boundedness validation**: Systematically test the Monte Carlo estimation approach on unbounded fairness metrics (e.g., disparate impact) to verify whether the concentration bounds still provide reliable estimates, or if alternative estimation techniques are needed.

2. **Stop-gradient ablation**: Compare Fair-SS-VAE against variants where: (a) imputation is fairness-aware through two-step processing, and (b) fairness gradients are allowed to flow to the imputation model, to quantify the impact of the stop-gradient assumption on final performance.

3. **Real-world missingness patterns**: Evaluate the method on datasets with naturally occurring missing demographic data (rather than simulated missingness) to assess robustness to complex, unknown missingness mechanisms that may violate the group-conditional assumptions.