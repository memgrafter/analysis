---
ver: rpa2
title: Targeted View-Invariant Adversarial Perturbations for 3D Object Recognition
arxiv_id: '2412.13376'
source_url: https://arxiv.org/abs/2412.13376
tags:
- adversarial
- targeted
- images
- attacks
- viap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces View-Invariant Adversarial Perturbations
  (VIAP), a method for crafting adversarial examples that remain effective across
  multiple viewpoints of 3D objects. Unlike traditional methods, VIAP enables targeted
  attacks using a single universal perturbation.
---

# Targeted View-Invariant Adversarial Perturbations for 3D Object Recognition

## Quick Facts
- **arXiv ID:** 2412.13376
- **Source URL:** https://arxiv.org/abs/2412.13376
- **Reference count:** 8
- **Primary result:** VIAP achieves >95% targeted attack success rates across multiple viewpoints with single universal perturbations

## Executive Summary
This paper introduces View-Invariant Adversarial Perturbations (VIAP), a novel method for crafting adversarial examples that remain effective across multiple viewpoints of 3D objects. Unlike traditional adversarial attack methods that are view-specific, VIAP generates a single universal perturbation that can successfully attack a 3D object recognition system regardless of the viewing angle. The method was evaluated on a dataset of 1,210 images of 121 diverse 3D objects, demonstrating superior performance in both targeted and untargeted attack scenarios compared to baseline methods.

## Method Summary
VIAP operates by optimizing a universal perturbation that remains effective across multiple viewpoints of 3D objects. The method leverages geometric transformations and view-invariant feature representations to craft perturbations that maintain their adversarial properties when the object is viewed from different angles. The algorithm jointly optimizes for attack success while ensuring the perturbation remains consistent across the full range of possible viewpoints, enabling a single perturbation to compromise recognition systems across different object perspectives.

## Key Results
- Targeted attacks achieved top-1 target accuracies exceeding 95% across various epsilon values
- Single universal perturbation remained effective across multiple viewpoints of 3D objects
- Demonstrated robust performance in both targeted and untargeted attack scenarios
- Outperformed baseline methods in view-invariant adversarial attack capabilities

## Why This Works (Mechanism)
VIAP exploits the geometric consistency of 3D objects across different viewpoints. By optimizing perturbations that target view-invariant features rather than view-specific features, the method ensures that the adversarial effect persists regardless of viewing angle. The approach leverages the underlying 3D structure of objects, crafting perturbations that remain effective even as the object rotates or is viewed from different perspectives. This geometric understanding allows VIAP to create truly universal perturbations that work across the entire viewing space.

## Foundational Learning
**3D object recognition systems:** Deep learning models that process 3D objects from multiple viewpoints, essential for understanding how VIAP attacks these systems.
*Why needed:* Provides context for the attack surface and vulnerabilities in multi-view recognition systems.
*Quick check:* Verify the paper discusses standard 3D object recognition architectures like PointNet, 3D CNNs, or multi-view fusion methods.

**View-invariant feature representations:** Features that remain consistent across different viewing angles of 3D objects.
*Why needed:* Central to understanding how VIAP crafts perturbations that work across viewpoints.
*Quick check:* Confirm the paper explains which features are considered view-invariant and how they're leveraged.

**Geometric transformations:** Mathematical operations that represent rotations, translations, and other viewpoint changes of 3D objects.
*Why needed:* Critical for understanding how VIAP accounts for different viewing angles in perturbation optimization.
*Quick check:* Verify the paper details which geometric transformations are used and how they're incorporated into the optimization.

**Adversarial perturbation optimization:** Techniques for finding small input modifications that cause misclassification in neural networks.
*Why needed:* Core mechanism that VIAP builds upon for creating view-invariant attacks.
*Quick check:* Confirm the paper explains the optimization framework and how it's adapted for view invariance.

## Architecture Onboarding

**Component map:** VIAP Input Images → Geometric Transformation Layer → View-Invariant Feature Extractor → Universal Perturbation Generator → Adversarial Example

**Critical path:** The perturbation optimization loop that jointly considers multiple viewpoints, applying geometric transformations to generate adversarial examples that remain effective across all views.

**Design tradeoffs:** VIAP prioritizes attack effectiveness across viewpoints over computational efficiency, requiring optimization over multiple geometric transformations. This increases computational cost compared to single-view attacks but achieves true view invariance.

**Failure signatures:** The perturbation may lose effectiveness for extreme viewing angles or when the object undergoes significant deformation. Performance may degrade when tested on object categories outside the training distribution.

**3 first experiments:** 1) Test VIAP on a baseline 3D object recognition model with standard evaluation metrics. 2) Compare VIAP's targeted attack success rates against single-view adversarial methods. 3) Evaluate the perturbation's effectiveness across different epsilon values to understand the robustness-perturbation size tradeoff.

## Open Questions the Paper Calls Out
None

## Limitations
- Modest dataset size (1,210 images across 121 objects) may limit generalizability
- Evaluation focuses primarily on top-1 accuracy without complete confusion matrices or false positive rates
- Lacks analysis of computational efficiency for generating VIAP perturbations compared to baseline methods

## Confidence
The paper presents a method for creating view-invariant adversarial perturbations, but several critical limitations affect confidence in the results.

**Targeted attack claims:** Medium confidence - Results show >95% success rates but require validation on larger, more diverse datasets.

**Universal perturbation claims:** Medium confidence - The claim of "universal" perturbations working across all viewpoints needs more rigorous statistical validation across different object categories and viewing conditions.

**Baseline comparison claims:** Low confidence - Comparison with baseline methods is mentioned but specific quantitative differences are not provided in detail.

## Next Checks
1. Test VIAP on a significantly larger dataset (e.g., 10x more images) to verify scalability and robustness across diverse 3D object categories and viewing angles.

2. Conduct ablation studies to quantify the individual contributions of different components in the VIAP algorithm and compare computational efficiency against baseline methods.

3. Evaluate VIAP's transferability to different model architectures and its performance under common defense mechanisms to assess real-world applicability.