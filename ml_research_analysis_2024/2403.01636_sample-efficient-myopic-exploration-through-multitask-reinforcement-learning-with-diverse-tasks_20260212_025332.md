---
ver: rpa2
title: Sample Efficient Myopic Exploration Through Multitask Reinforcement Learning
  with Diverse Tasks
arxiv_id: '2403.01636'
source_url: https://arxiv.org/abs/2403.01636
tags:
- learning
- policy
- exploration
- function
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that myopic exploration, which is typically inefficient
  for single-task RL, can be sample-efficient in multitask RL when tasks are sufficiently
  diverse. The key insight is that a diverse set of tasks allows exploration in one
  task to benefit others.
---

# Sample Efficient Myopic Exploration Through Multitask Reinforcement Learning with Diverse Tasks

## Quick Facts
- arXiv ID: 2403.01636
- Source URL: https://arxiv.org/abs/2403.01636
- Reference count: 9
- One-line primary result: Myopic exploration can be sample-efficient in multitask RL when tasks are sufficiently diverse

## Executive Summary
This paper addresses a fundamental question in multitask reinforcement learning: when can simple exploration strategies like epsilon-greedy be efficient? The key insight is that myopic exploration, typically inefficient for single-task RL, can be sample-efficient in multitask RL when tasks are sufficiently diverse. The authors prove that a simple algorithm that explores one task using epsilon-greedy policies from other tasks is sample-efficient if the task set is diverse enough. They provide a generic sample complexity guarantee and demonstrate it for linear MDPs, showing that diversity enables simple exploration strategies to work effectively across multiple tasks.

## Method Summary
The paper proposes Algorithm 1, a generic MTRL algorithm with policy-sharing that explores one task using epsilon-greedy policies from other tasks. The method maintains separate datasets for each MDP and mixes policies from all tasks during exploration. For linear MDPs, diverse task sets are constructed by having tasks share the same feature extractor and measure but have orthogonal reward parameters. The algorithm uses an offline learning oracle for value function approximation, maintaining the function class F that satisfies realizability and completeness assumptions.

## Key Results
- Myopic exploration (epsilon-greedy) can be sample-efficient in multitask RL when tasks are sufficiently diverse
- The multitask myopic exploration gap is lower bounded by single-task gap up to a factor of 1/√|M|
- In linear MDPs, a diverse set of tasks can be constructed with orthogonal reward parameters to ensure lower bounded exploration gap
- Diverse task sets align with those selected by automatic curriculum learning algorithms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diverse task sets enable myopic exploration to be sample-efficient in multitask RL.
- Mechanism: When tasks are sufficiently diverse, exploring with policies from other tasks provides coverage across the task space. This allows learning of near-optimal policies for all tasks in polynomial time, even with simple exploration strategies.
- Core assumption: The task set is (α, c)-diverse as defined in Definition 5.
- Evidence anchors:
  - [abstract] "when an agent is trained on a sufficiently diverse set of tasks, a generic policy-sharing algorithm with myopic exploration design like ϵ-greedy that are inefficient in general can be sample-efficient for MTRL"
  - [section 4] "A large myopic exploration gap implies that within all the policies that can be learned by the current exploratory policy, there exists one that can make significant improvement on the current greedy policy"
  - [corpus] Weak. The corpus neighbors are all about multitask RL applications but do not specifically address diversity or myopic exploration efficiency.

### Mechanism 2
- Claim: Multitask myopic exploration gap is lower bounded by single-task myopic exploration gap up to a factor of 1/√|M|.
- Mechanism: For any suboptimal policy f in the multitask setting, the myopic exploration gap α(f, M, F) is at least as large as the single-task gap α(fM, {M}, F) divided by √|M|. This implies that if each task can be learned individually with myopic exploration, they can also be learned together with an extra factor of |M|², which is still polynomial.
- Core assumption: The function class F and task set M satisfy the realizability and completeness assumptions.
- Evidence anchors:
  - [section 4.3] "Proposition 1: α(f, M, F) ≥ α(fM, {M}, F)/√|M| for all f ∈ (F)⊗|M| and M ∈ M"
  - [section 5] "We provide a comprehensive comparison between Single-task and Multitask MEG"
  - [corpus] Weak. No corpus evidence directly supports this theoretical comparison.

### Mechanism 3
- Claim: In linear MDPs, a diverse set of tasks can be constructed by having tasks share the same feature extractor and measure but have orthogonal reward parameters.
- Mechanism: For linear MDPs, if the task set M is constructed such that for each step h, there exists a subset of d tasks with orthogonal reward parameters, then the feature covariance matrix induced by the optimal policies is full rank. This guarantees a lower bounded myopic exploration gap.
- Core assumption: Assumption 2 (Feature coverage) holds, i.e., for any unit vector ν, there exists a policy π such that Eπ[ν⊤ϕh(sh, ah)] ≥ b₁ for some constant b₁ > 0.
- Evidence anchors:
  - [section 5] "Definition 7: We say M is a diverse set of MDPs for the linear MDP case, if they share the same feature extractor ϕh and the same measure µh"
  - [section 5] "Theorem 2: Consider M to be a diverse set as in Definition 7. Suppose Assumption 2 holds and β ≤ b₁/2, then we have for any f ∈ F β, α(f, F, M) = Ω(β²b₁²/(|A||M|H))"
  - [corpus] Weak. No corpus evidence directly supports this specific construction for linear MDPs.

## Foundational Learning

- Concept: Multitask Reinforcement Learning (MTRL)
  - Why needed here: The paper's main contribution is showing that myopic exploration can be sample-efficient in MTRL when tasks are diverse. Understanding MTRL is crucial to grasp the context and significance of this result.
  - Quick check question: What is the key difference between single-task RL and multitask RL, and why does this difference matter for exploration efficiency?

- Concept: Myopic Exploration
  - Why needed here: The paper focuses on myopic exploration strategies like epsilon-greedy, which are typically inefficient for single-task RL but can be sample-efficient in MTRL with diverse tasks. Understanding myopic exploration is essential to follow the paper's arguments.
  - Quick check question: Why is myopic exploration often criticized in single-task RL, and how does the diversity of tasks in MTRL potentially overcome this limitation?

- Concept: Function Approximation in RL
  - Why needed here: The paper considers a setting where value functions are approximated by general function classes, which is common in modern RL. Understanding function approximation is necessary to follow the theoretical analysis.
  - Quick check question: What are the key assumptions (realizability and completeness) made about the function class in the paper, and why are they important for the theoretical guarantees?

## Architecture Onboarding

- Component map: Task Set M -> Function Class F -> Exploration Function expl -> Algorithm 1
- Critical path:
  1. Construct a diverse task set M (Definition 7 for linear MDPs)
  2. Initialize the algorithm with the function class F and exploration function expl
  3. For each round, explore every MDP with an exploratory policy that is the mixture of greedy policies of all MDPs in the task set
  4. Collect data and update the function class F
  5. Return the learned policies for each task
- Design tradeoffs:
  - Task Diversity vs. Task Similarity: More diverse tasks may lead to better exploration but could also increase the complexity of learning
  - Function Class Expressiveness vs. Sample Complexity: More expressive function classes may capture the value functions better but could require more samples to learn
  - Exploration Strategy: Myopic exploration is simple but may require more diverse tasks to be efficient, while strategic exploration may be more complex but require less diversity
- Failure signatures:
  - Exponential sample complexity: If the task set is not sufficiently diverse, myopic exploration may fail, leading to exponential sample complexity (Proposition 2)
  - Suboptimal policies: If the function class F does not satisfy realizability and completeness, the learned policies may not be optimal
  - Slow convergence: If the exploration function expl is not chosen appropriately (e.g., epsilon too small), the algorithm may converge slowly
- First 3 experiments:
  1. Implement Algorithm 1 for a simple multitask RL problem with a diverse set of tasks and evaluate the sample complexity compared to single-task learning
  2. Vary the diversity of the task set and measure the impact on the sample complexity of Algorithm 1 with myopic exploration
  3. Compare the performance of Algorithm 1 with myopic exploration to an algorithm with strategic exploration (e.g., UCB) on a diverse multitask RL problem

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does diversity improve myopic exploration in multi-task RL when tasks share similar reward functions but differ in transition dynamics?
- Basis in paper: [explicit] The paper discusses diverse tasks where tasks share same transition dynamics but differ in reward functions, but doesn't explicitly test when tasks share rewards but differ in transitions.
- Why unresolved: The theoretical framework and experiments focus on reward diversity, not transition diversity.
- What evidence would resolve it: Empirical comparison of myopic exploration performance on task sets with shared rewards but diverse transitions versus shared transitions but diverse rewards.

### Open Question 2
- Question: What is the theoretical sample complexity of myopic exploration in multi-task RL with infinite action spaces?
- Basis in paper: [inferred] The paper discusses myopic exploration for finite action spaces (ϵ-greedy) and mentions Gaussian noise for continuous action spaces but doesn't provide theoretical guarantees for the continuous case.
- Why unresolved: The Gaussian noise exploration is mentioned as an extension but not rigorously analyzed.
- What evidence would resolve it: Sample complexity bound for continuous action spaces with Gaussian exploration that matches the framework of Theorem 1.

### Open Question 3
- Question: How does task diversity impact myopic exploration efficiency when tasks have overlapping optimal policies?
- Basis in paper: [explicit] The paper defines diversity based on task set properties but doesn't analyze how much overlap in optimal policies affects efficiency.
- Why unresolved: The theoretical analysis assumes tasks are diverse enough but doesn't quantify the impact of policy overlap.
- What evidence would resolve it: Theoretical bounds showing how sample complexity degrades as optimal policies become more similar across tasks, or experimental validation of this relationship.

## Limitations

- The paper relies heavily on abstract diversity conditions without concrete empirical validation of these diversity metrics
- The linear MDP construction is specific and may not generalize to broader function classes
- The realizability and completeness assumptions for function approximation are strong requirements that may not hold in practice

## Confidence

- **High confidence**: The theoretical framework for multitask myopic exploration gap (Proposition 1) and the linear MDP construction (Theorem 2) appear sound
- **Medium confidence**: The claim that diverse task sets enable myopic exploration to be sample-efficient relies on abstract diversity definitions that need empirical validation
- **Low confidence**: The connection between automatic curriculum learning algorithms and task diversity is stated but not empirically demonstrated

## Next Checks

1. Implement quantitative measures of task diversity (e.g., feature covariance matrix rank, exploration gap metrics) and empirically verify their correlation with sample complexity across different task sets in controlled experiments.

2. Test Algorithm 1 with less restrictive function classes that may violate realizability/completeness assumptions to understand the practical impact of these theoretical requirements on performance.

3. Systematically compare task sets generated by automatic curriculum learning algorithms against randomly constructed diverse sets, measuring both diversity metrics and sample efficiency to validate the claimed relationship.