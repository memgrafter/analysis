---
ver: rpa2
title: 'Me LLaMA: Foundation Large Language Models for Medical Applications'
arxiv_id: '2402.12749'
source_url: https://arxiv.org/abs/2402.12749
tags:
- medical
- me-llama
- text
- clinical
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Me-LLaMA is a family of medical foundation large language models
  developed by continual pre-training and instruction tuning of LLaMA2 with extensive
  biomedical and clinical data. The models were trained on 129 billion tokens from
  PubMed, clinical notes, and general domain sources, and fine-tuned with 214,000
  instruction samples.
---

# Me LLaMA: Foundation Large Language Models for Medical Applications

## Quick Facts
- arXiv ID: 2402.12749
- Source URL: https://arxiv.org/abs/2402.12749
- Reference count: 0
- Me-LLaMA is a family of medical foundation large language models that outperformed existing open-source medical LLMs and commercial models on various medical text analysis tasks and clinical diagnosis benchmarks.

## Executive Summary
Me-LLaMA is a family of medical foundation large language models developed by continual pre-training and instruction tuning of LLaMA2 with extensive biomedical and clinical data. The models were trained on 129 billion tokens from PubMed, clinical notes, and general domain sources, and fine-tuned with 214,000 instruction samples. Me-LLaMA outperformed existing open-source medical LLMs on 12 benchmark datasets across six medical text analysis tasks, and surpassed ChatGPT on 7 out of 8 datasets and GPT-4 on 5 out of 8 datasets in supervised settings. In zero-shot settings, Me-LLaMA achieved performance comparable to GPT-4 on complex clinical case diagnosis. The models are publicly available and demonstrate the effectiveness of domain-specific data in building medical LLMs.

## Method Summary
Me-LLaMA models were developed through continual pre-training of LLaMA2 models with 129 billion tokens from biomedical literature (PubMed), clinical notes (MIMIC-III/IV/CXR), and general domain sources. This was followed by instruction tuning using 214,000 samples from diverse medical sources to enhance zero-shot performance. The models were evaluated on 12 benchmark datasets across six medical text analysis tasks and 70 complex clinical case diagnosis cases from NEJM. Training used AdamW optimizer, cosine learning rate scheduler, bf16 precision, and LoRA-based parameter-efficient fine-tuning.

## Key Results
- Me-LLaMA 13B outperformed PMC-LLaMA 13B on 11 out of 12 datasets and Meditron 70B on 10 out of 12 datasets
- In supervised settings, Me-LLaMA surpassed ChatGPT on 7 out of 8 datasets and GPT-4 on 5 out of 8 datasets
- In zero-shot clinical diagnosis, Me-LLaMA achieved performance comparable to GPT-4 on 70 complex NEJM cases

## Why This Works (Mechanism)

### Mechanism 1
Large-scale domain-specific pretraining substantially improves zero-shot and supervised performance across diverse medical tasks. Continual pretraining on a massive corpus (129B tokens) of biomedical and clinical data expands the model's medical knowledge base, enabling better pattern recognition and contextual understanding in medical text. Core assumption: The quality and diversity of the pretraining data directly correlates with downstream task performance. Evidence anchors: Me-LLaMA outperformed existing open-source medical LLMs on 12 benchmark datasets; The Me-LLaMA 13B model surpassed the similar-sized medical foundation model PMC-LLaMA 13B on 11 out of 12 datasets. Break condition: If pretraining data contains significant noise or domain-irrelevant content, performance gains may be minimal or reversed.

### Mechanism 2
Instruction tuning enhances the model's ability to follow medical instructions and generalize across tasks without task-specific training. Instruction tuning with 214K samples from diverse medical sources teaches the model to interpret and respond to task prompts in medical contexts. Core assumption: Instruction following ability transfers across medical task types when models are exposed to varied prompt-response pairs. Evidence anchors: With task-specific instruction tuning, Me-LLaMA models also surpass leading commercial LLMs; The instruction tuning was also found to provide great increases in zero-shot performance. Break condition: If instruction tuning data is too narrow or repetitive, generalization across novel tasks may fail.

### Mechanism 3
Combining biomedical literature and clinical notes provides comprehensive medical knowledge for both research and clinical applications. The dual-domain pretraining corpus bridges the gap between theoretical knowledge (literature) and practical application (clinical notes). Core assumption: Models trained on both domains develop richer semantic understanding than those trained on either domain alone. Evidence anchors: By combining biomedical literature and clinical notes, we generated the largest biomedical pre-training dataset; Only one model (Clinical LLaMA) used clinical notes from electronic health records. Break condition: If clinical notes contain too much noise or privacy artifacts, they may introduce harmful patterns.

## Foundational Learning

- Concept: Continual pretraining
  - Why needed here: Adapts general LLMs to specialized medical domains by expanding their knowledge base with domain-specific data
  - Quick check question: What's the difference between continual pretraining and fine-tuning in terms of parameter updates?

- Concept: Instruction following
  - Why needed here: Enables zero-shot performance on new tasks by teaching the model to interpret and execute task prompts
  - Quick check question: How does instruction tuning differ from standard supervised fine-tuning?

- Concept: Catastrophic forgetting
  - Why needed here: Critical consideration when adapting models to new domains while preserving existing capabilities
  - Quick check question: What strategies prevent catastrophic forgetting during domain adaptation?

## Architecture Onboarding

- Component map: Backbone LLaMA2 → Pretraining layer (129B tokens) → Instruction tuning layer (214K samples) → Task-specific fine-tuning → Evaluation
- Critical path: Pretraining → Instruction tuning → Zero-shot evaluation → Supervised fine-tuning → Task-specific evaluation
- Design tradeoffs: Larger models (70B) offer better performance but require significant computational resources; smaller models (13B) provide reasonable performance with better accessibility
- Failure signatures: Poor performance on clinical tasks may indicate insufficient clinical data; degraded general language ability may signal catastrophic forgetting
- First 3 experiments:
  1. Compare zero-shot performance on PubMedQA between Me-LLaMA 13B and LLaMA2 13B to verify pretraining impact
  2. Test instruction-following ability on MIMIC-CXR summarization before and after instruction tuning
  3. Evaluate top-K accuracy on NEJM CPC cases to benchmark clinical reasoning capabilities

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal ratio of general domain data to medical domain data for continual pre-training to maximize performance while minimizing catastrophic forgetting? Basis in paper: The paper discusses the importance of data diversity during pre-training and mentions empirical results showing different performance outcomes with varying data mix ratios. Why unresolved: The paper indicates that determining the optimal balance requires careful empirical analysis but does not provide a definitive answer. What evidence would resolve it: Systematic experiments varying the ratio of general to medical domain data during pre-training, measuring performance on both general and medical tasks, and identifying the point of diminishing returns.

### Open Question 2
How can the token handling capacity limitation of 4096 tokens be effectively addressed in medical LLMs? Basis in paper: The paper acknowledges this limitation inherited from LLaMA2 and suggests potential solutions like integrating sparse local attention techniques. Why unresolved: While potential solutions are mentioned, the paper does not implement or evaluate these approaches. What evidence would resolve it: Implementation and evaluation of techniques like sparse local attention or other context extension methods specifically for medical LLMs, demonstrating improved performance on longer medical texts.

### Open Question 3
What is the most effective approach to improve zero-shot performance on named entity recognition (NER) and relation extraction (RE) tasks in medical LLMs? Basis in paper: The paper notes that medical LLMs including GPT-4 displayed low performance on NER and RE tasks in zero-shot settings, with challenges including lack of conciseness and precision in outputs. Why unresolved: The paper identifies the problem but does not propose or test specific solutions for improving zero-shot performance on these tasks. What evidence would resolve it: Development and evaluation of specialized prompt engineering techniques, fine-tuning strategies, or architectural modifications specifically designed to enhance zero-shot NER and RE performance in medical contexts.

## Limitations

- The exact distribution of pretraining data across PubMed, clinical notes, and general domains is not specified, making it difficult to assess the claimed dual-domain benefits
- The instruction tuning dataset lacks detailed breakdown by task type and quality controls, raising questions about potential biases in the instruction distribution
- Evaluation methodology for clinical case diagnosis lacks detailed inter-annotator agreement metrics for human evaluation

## Confidence

- High Confidence: Claims about pretraining and instruction tuning improving zero-shot performance on text classification and named entity recognition tasks
- Medium Confidence: Claims about surpassing commercial LLMs in supervised settings and achieving GPT-4 comparable performance in zero-shot clinical diagnosis
- Low Confidence: Claims about the comprehensive nature of the pretraining corpus and the specific contribution of each data source to performance gains

## Next Checks

1. **Data Quality Assessment**: Reconstruct the pretraining data distribution by sampling and analyzing token composition across PubMed, MIMIC, and general domain sources to verify the claimed dual-domain benefits.

2. **Instruction Tuning Ablation**: Conduct controlled experiments removing instruction tuning to quantify its exact contribution to zero-shot performance across different task types.

3. **Clinical Case Evaluation Robustness**: Expand the clinical diagnosis evaluation beyond the 70 NEJM cases to include diverse clinical scenarios from multiple sources, implementing rigorous inter-annotator agreement metrics for human evaluation.