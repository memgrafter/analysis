---
ver: rpa2
title: 'Exploring Combinatorial Problem Solving with Large Language Models: A Case
  Study on the Travelling Salesman Problem Using GPT-3.5 Turbo'
arxiv_id: '2405.01997'
source_url: https://arxiv.org/abs/2405.01997
tags:
- station
- distance
- stations
- order
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of Large Language Models (LLMs),
  specifically GPT-3.5 Turbo, to solve the Travelling Salesman Problem (TSP). The
  study employs various approaches, including zero-shot and few-shot in-context learning,
  chain-of-thoughts (CoT), and fine-tuning the model on TSP instances of a fixed size.
---

# Exploring Combinatorial Problem Solving with Large Language Models: A Case Study on the Travelling Salesman Problem Using GPT-3.5 Turbo

## Quick Facts
- arXiv ID: 2405.01997
- Source URL: https://arxiv.org/abs/2405.01997
- Reference count: 40
- Key outcome: Fine-tuned GPT-3.5 Turbo achieves promising TSP performance with low randomness scores and small gaps relative to optimal solutions

## Executive Summary
This paper investigates the application of Large Language Models, specifically GPT-3.5 Turbo, to solve the Travelling Salesman Problem (TSP). The study explores various approaches including zero-shot and few-shot in-context learning, chain-of-thought reasoning, and fine-tuning the model on fixed-size TSP instances. Results demonstrate that fine-tuned models show strong performance on problems matching their training size and generalize well to larger instances. Self-ensemble techniques further enhance solution quality without additional fine-tuning costs, achieving low randomness scores and small gaps relative to optimal solutions.

## Method Summary
The study employs a comprehensive evaluation framework testing GPT-3.5 Turbo on TSP instances of varying sizes (10, 20, 50, 100). Multiple approaches are investigated: zero-shot prompting where the model solves problems without examples, few-shot learning with example problems, and few-shot with chain-of-thought reasoning. Fine-tuning is performed on fixed-size (size 10) TSP instances, and self-ensemble techniques are applied by prompting the model multiple times and selecting the best solution. Performance is evaluated using randomness score and gap metrics relative to optimal solutions.

## Key Results
- Fine-tuned models demonstrate promising performance on problems identical in size to training instances and generalize well to larger problems
- Self-ensemble techniques improve solution quality without additional fine-tuning costs
- Fine-tuned GPT-3.5 model achieves low randomness scores and small gaps relative to optimal solutions
- Challenges remain in scaling to larger instance sizes due to token limitations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning GPT-3.5 Turbo on fixed-size TSP instances improves performance on those same sizes and generalizes to larger sizes.
- Mechanism: Fine-tuning adapts the model's internal representations to better capture TSP-specific patterns, such as Euclidean distance calculations and path optimization, enabling it to solve problems of the same size as training and generalize to larger instances.
- Core assumption: The model can learn TSP-specific reasoning patterns from a fixed-size dataset and apply them to unseen instances, even if they are larger.
- Evidence anchors:
  - [abstract] "The fine-tuned models demonstrated promising performance on problems identical in size to the training instances and generalized well to larger problems."
  - [section C] "The fine-tuned model exhibited a high success rate, especially for instance sizes near the fine-tuning benchmark of 10, indicating robust performance in scenarios similar to its training."
- Break condition: If the model's token limit is exceeded for larger instances, causing incomplete responses or hallucinations, or if the generalization fails due to overfitting to the specific training size.

### Mechanism 2
- Claim: Self-ensemble (self-consistency) improves TSP solution quality without additional fine-tuning costs.
- Mechanism: By prompting the model multiple times with the same instance and selecting the best solution among the valid, non-hallucinated responses, self-ensemble reduces the impact of randomness in the model's outputs and increases the likelihood of finding a high-quality solution.
- Core assumption: The model's outputs are sufficiently diverse across multiple prompts, and at least one of the responses will be a valid, high-quality solution.
- Evidence anchors:
  - [abstract] "Self-ensemble techniques further enhance the quality of solutions without additional fine-tuning costs."
  - [section D] "Implementing self-ensemble techniques shows a marked improvement in solution quality. This is evidenced by reduced median gap and IQR across all instance sizes."
- Break condition: If the model consistently produces the same (suboptimal) solution across multiple prompts, or if all responses are hallucinations, self-ensemble will not improve solution quality.

### Mechanism 3
- Claim: In-context learning techniques (zero-shot, few-shot, CoT) can solve TSP, with few-shot methods generally performing better than zero-shot.
- Mechanism: In-context learning provides the model with examples or instructions to guide its reasoning, enabling it to solve TSP without fine-tuning. Few-shot learning is more effective because it provides concrete examples of the task, reducing ambiguity and improving performance.
- Core assumption: The model has sufficient prior knowledge about TSP and can apply it to new instances when provided with appropriate context.
- Evidence anchors:
  - [section B] "Few-shot and few-shot with CoT techniques show promising accuracy for smaller instances, particularly those close to the size of 10."
  - [section C] "In contrast, few-shot methods generally produce lower median randomness scores compared to zero-shot, indicating a tendency toward less random solutions."
- Break condition: If the model's token limit is exceeded for larger instances, causing incomplete responses or hallucinations, or if the in-context examples are not representative of the task, leading to poor performance.

## Foundational Learning

- Concept: Euclidean distance calculation
  - Why needed here: TSP requires calculating the distance between cities (represented as 2D points) to determine the optimal path.
  - Quick check question: How is the Euclidean distance between two points (x1, y1) and (x2, y2) calculated?

- Concept: Combinatorial optimization
  - Why needed here: TSP is a classic combinatorial optimization problem, requiring the model to find the optimal permutation of cities that minimizes the total distance.
  - Quick check question: What is the time complexity of solving TSP exactly using brute force?

- Concept: Chain-of-thought (CoT) reasoning
  - Why needed here: CoT prompts the model to break down the problem into steps, improving its ability to reason through complex tasks like TSP.
  - Quick check question: How does CoT differ from standard prompting, and why might it be beneficial for solving TSP?

## Architecture Onboarding

- Component map: Data generation -> Prompt engineering -> Fine-tuning (if applicable) -> Self-ensemble (if applicable) -> Evaluation

- Critical path: Data generation → Prompt engineering → Fine-tuning (if applicable) → Self-ensemble (if applicable) → Evaluation

- Design tradeoffs:
  - Fine-tuning vs. in-context learning: Fine-tuning requires training data and computational resources but can improve performance, while in-context learning is more flexible but may be less effective for complex tasks.
  - Self-ensemble size: Larger ensemble sizes increase the likelihood of finding a good solution but also increase computational cost.
  - Token limit: The model's token limit constrains the size of TSP instances that can be solved and the complexity of the prompts.

- Failure signatures:
  - Hallucinations: The model outputs invalid solutions (e.g., revisiting nodes or omitting nodes).
  - Randomness: The model's solutions are no better than random, as indicated by high randomness scores.
  - Token limit: The model's responses are incomplete or truncated due to token limit constraints.

- First 3 experiments:
  1. Test in-context learning techniques (zero-shot, few-shot, CoT) on small TSP instances (size 10) to establish baseline performance.
  2. Fine-tune the model on size-10 TSP instances and evaluate its performance on the same size and larger instances.
  3. Implement self-ensemble on the fine-tuned model and compare its performance to the non-ensemble version.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does fine-tuning LLMs on variable-sized TSP instances affect their generalization performance compared to fixed-size training?
- Basis in paper: [inferred] The paper fine-tunes on fixed-size instances and tests on larger sizes, noting performance degradation but not exploring variable-sized training.
- Why unresolved: The study only examines fine-tuning on fixed-size instances and testing on various sizes, without investigating the impact of training on variable-sized instances.
- What evidence would resolve it: Experiments comparing model performance when fine-tuned on variable-sized vs. fixed-size instances, testing generalization to unseen sizes.

### Open Question 2
- Question: What is the optimal self-ensemble size for balancing solution quality improvement against computational cost in LLM-based TSP solvers?
- Basis in paper: [explicit] The paper tests ensemble sizes from 1 to 11 but does not determine an optimal trade-off point.
- Why unresolved: While the study shows larger ensembles improve quality, it doesn't analyze the diminishing returns or computational overhead at different ensemble sizes.
- What evidence would resolve it: Cost-benefit analysis comparing solution quality improvements against computational costs (API calls, processing time) across various ensemble sizes.

### Open Question 3
- Question: How do different LLM architectures compare in solving TSP problems of varying sizes, particularly regarding token limitations and hallucination rates?
- Basis in paper: [inferred] The study uses only GPT-3.5 Turbo, noting token limitations but not comparing alternative architectures.
- Why unresolved: The paper identifies token limitations as a bottleneck but doesn't explore whether other LLM architectures (open-source or newer models) might perform better.
- What evidence would resolve it: Comparative experiments testing multiple LLM architectures on identical TSP instances, measuring hallucination rates, solution quality, and performance across different instance sizes.

## Limitations

- Token limit constraints significantly constrain the size of TSP instances that can be effectively solved
- Generalization from fixed-size fine-tuning is not extensively validated beyond size 20
- Limited comparison to state-of-the-art traditional and neural TSP solvers

## Confidence

1. **Fine-Tuning Performance on Training Size**: High Confidence
2. **Generalization to Larger Instances**: Medium Confidence
3. **Self-Ensemble Improvement**: Medium Confidence
4. **In-Context Learning Effectiveness**: Medium Confidence

## Next Checks

1. Evaluate generalization to larger instances by extending evaluation to TSP instances significantly larger than training size (e.g., size 50 or 100)
2. Quantify self-ensemble diversity by measuring pairwise similarity of solutions across multiple prompts
3. Compare to traditional and neural solvers by benchmarking against Concorde and state-of-the-art neural approaches on standard TSP datasets