---
ver: rpa2
title: Analysis and Detection of Differences in Spoken User Behaviors between Autonomous
  and Wizard-of-Oz Systems
arxiv_id: '2410.03147'
source_url: https://arxiv.org/abs/2410.03147
tags:
- listening
- interview
- autonomous
- attentive
- scenario
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzed spoken behaviors in human-robot interactions
  with an autonomous system and a Wizard-of-Oz (WoZ) setup. The research examined
  attentive listening and job interview scenarios, revealing significant differences
  in metrics such as speech length, speaking rate, fillers, backchannels, disfluencies,
  and laughter between the two conditions.
---

# Analysis and Detection of Differences in Spoken User Behaviors between Autonomous and Wizard-of-Oz Systems

## Quick Facts
- arXiv ID: 2410.03147
- Source URL: https://arxiv.org/abs/2410.03147
- Reference count: 0
- Primary result: Spoken behaviors differ significantly between WoZ and autonomous robot interactions, with acoustic features enabling reliable classification

## Executive Summary
This study analyzed spoken behaviors in human-robot interactions, comparing an autonomous system against a Wizard-of-Oz (WoZ) setup where a human operator controlled the robot. The research examined attentive listening and job interview scenarios, revealing significant differences in metrics such as speech length, speaking rate, fillers, backchannels, disfluencies, and laughter between the two conditions. Predictive models were developed to distinguish between WoZ and autonomous system interactions, achieving higher accuracy and precision compared to a baseline model. The random forest model demonstrated superior performance, with acoustic features playing a key role in classification.

## Method Summary
The study collected speech data from 48 participants interacting with the ERICA android robot in two scenarios: attentive listening and job interview. Participants were randomly assigned to either autonomous or Wizard-of-Oz conditions. The analysis focused on inter-pausal units (IPUs) as the basic unit of speech, extracting both acoustic features (pitch, power, speaking rate) and linguistic features (fillers, backchannels, disfluencies). Statistical analysis used Wilcoxon rank sum tests with Bonferroni correction for multiple comparisons, while predictive modeling employed logistic regression and random forest classifiers.

## Key Results
- Subjects exhibited longer IPU lengths and faster speaking rates with greater variability in the WoZ condition compared to autonomous conditions
- Random forest models outperformed baseline models in classifying interaction types, with acoustic features proving more predictive than linguistic features at the IPU level
- Behavioral differences varied by scenario context, with attentive listening showing increased speaking rates in WoZ conditions while job interviews showed decreased IPU lengths

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Subjects modify their spoken behavior when interacting with a WoZ system versus an autonomous system
- Mechanism: The perceived human presence in the WoZ condition alters user engagement patterns, leading to measurable differences in speech metrics such as increased speaking rate, filler usage, backchannel frequency, and laughter
- Core assumption: Users are unaware of the true nature of the WoZ condition and behave as if interacting with an autonomous system
- Evidence anchors:
  - [abstract] "Results revealed significant differences in metrics such as speech length, speaking rate, fillers, backchannels, disfluencies, and laughter between operator-controlled and autonomous conditions."
  - [section] "Our analysis revealed that subjects exhibit longer IPU lengths and speak at a faster rate (with greater variability) in the WoZ condition compared to the autonomous condition."
  - [corpus] Weak correlation with corpus neighbors; neighbor papers focus on different aspects of dialogue systems, not behavioral differences between WoZ and autonomous conditions
- Break condition: If subjects become aware of the WoZ manipulation, the behavioral differences would likely diminish or change character

### Mechanism 2
- Claim: Acoustic features are more predictive than linguistic features for distinguishing between WoZ and autonomous conditions at the IPU level
- Mechanism: Voice characteristics (pitch, power, speaking rate) carry more immediate and reliable signals about the interaction context than linguistic content, which may be influenced by topic or individual speaking style
- Core assumption: The acoustic signal reflects subconscious behavioral adjustments that are harder to consciously control than word choice
- Evidence anchors:
  - [abstract] "acoustic features playing a key role in classification."
  - [section] "In both the attentive listening and job interview datasets, acoustic features emerged as the primary drivers for the model's predictive performance."
  - [corpus] No direct corpus support for this specific mechanism; requires internal data validation
- Break condition: If linguistic patterns become more standardized across conditions or if acoustic differences are minimized through training, the predictive power would shift

### Mechanism 3
- Claim: The scenario context (attentive listening vs job interview) moderates how users adjust their behavior between WoZ and autonomous conditions
- Mechanism: Different social expectations and stress levels in each scenario create distinct behavioral baselines, making the WoZ effect manifest differently across contexts
- Core assumption: Users have different psychological models for what constitutes appropriate behavior in casual conversation versus formal interview settings
- Evidence anchors:
  - [abstract] "discrepancies may arise from the distinct nature of each scenario; while the attentive listening scenario entails lower stress and self-directed engagement, the job interview scenario typically involves higher stress levels."
  - [section] "Our analysis indicated that subjects use shorter IPU lengths (with less variability) in the WoZ condition compared to the autonomous condition" for job interviews, opposite to attentive listening patterns
  - [corpus] No corpus evidence supporting scenario-specific behavioral modulation
- Break condition: If users treat all robot interactions uniformly regardless of scenario, or if the robot's behavior dominates scenario expectations

## Foundational Learning

- Concept: Inter-pausal units (IPUs) as the basic unit of analysis
  - Why needed here: IPUs provide a consistent segmentation of speech that captures natural pauses and turn-taking boundaries, enabling meaningful comparison of speaking behaviors
  - Quick check question: What is the minimum pause duration used to define an IPU in this study?

- Concept: Statistical testing for non-normal data distributions
  - Why needed here: The study's behavioral metrics violate normality assumptions, requiring non-parametric tests (Wilcoxon rank sum) and appropriate corrections (Bonferroni) for multiple comparisons
  - Quick check question: Which test was used to compare IPU length between conditions?

- Concept: Permutation-based variable importance in random forests
  - Why needed here: This method quantifies feature contributions by measuring performance degradation when features are randomized, providing insight into which behavioral indicators best distinguish interaction types
  - Quick check question: What metric was used to evaluate feature importance in the random forest models?

## Architecture Onboarding

- Component map: Data collection (ERICA robot, audio/video capture) → Manual annotation (linguistic features) → Feature extraction (acoustic and linguistic metrics) → Statistical analysis → Predictive modeling (classification)
- Critical path: Data collection and annotation quality directly impacts all downstream analysis; any errors here propagate through the entire pipeline
- Design tradeoffs: Manual annotation provides high-quality linguistic feature labels but is labor-intensive and may introduce inter-annotator variability; automated extraction would scale better but might miss nuanced phenomena
- Failure signatures: Poor classification performance could indicate insufficient feature differentiation, annotation errors, or that subjects are not behaving as expected under WoZ conditions
- First 3 experiments:
  1. Validate IPU segmentation by comparing automated detection against human-verified boundaries on a subset of data
  2. Test classification performance using only acoustic features versus only linguistic features to confirm their relative contributions
  3. Conduct cross-validation with different random seeds to assess model stability and ensure no speaker leakage between training and test sets

## Open Questions the Paper Calls Out

None

## Limitations
- The study relies on a relatively small corpus of 48 subjects (24 per condition), which may limit generalizability across diverse user populations
- The WoZ deception assumes subjects remain unaware of the manipulation, but some participants may have suspected or deduced the true nature of the interaction
- The study focuses on Japanese-speaking participants interacting with a specific android robot (ERICA), raising questions about cross-cultural and cross-platform generalizability

## Confidence

- **High Confidence**: The existence of measurable behavioral differences between WoZ and autonomous conditions (supported by multiple statistical tests and effect sizes)
- **Medium Confidence**: The superiority of acoustic features over linguistic features for classification (based on internal model comparisons, but lacking external validation)
- **Medium Confidence**: The scenario-dependent moderation of behavioral differences (supported by within-study comparisons but no external corpus validation)

## Next Checks
1. Conduct a post-interaction questionnaire to assess participants' awareness of the WoZ manipulation and correlate suspicion levels with behavioral metrics
2. Test model performance on a holdout set from the same corpus to verify that results are not inflated by overfitting or speaker leakage
3. Replicate the analysis with a larger, more diverse participant pool and different robot platforms to assess generalizability of the findings