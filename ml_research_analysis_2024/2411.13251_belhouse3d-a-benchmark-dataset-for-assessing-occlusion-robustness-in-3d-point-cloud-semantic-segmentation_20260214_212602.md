---
ver: rpa2
title: 'BelHouse3D: A Benchmark Dataset for Assessing Occlusion Robustness in 3D Point
  Cloud Semantic Segmentation'
arxiv_id: '2411.13251'
source_url: https://arxiv.org/abs/2411.13251
tags:
- point
- data
- dataset
- segmentation
- ieee
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BelHouse3D is a synthetic 3D point cloud dataset for indoor scene
  semantic segmentation, created using real-world references from 32 Belgian houses.
  It addresses the challenge of occlusion in real-world point clouds by including
  an out-of-distribution test set with simulated occlusions.
---

# BelHouse3D: A Benchmark Dataset for Assessing Occlusion Robustness in 3D Point Cloud Semantic Segmentation

## Quick Facts
- arXiv ID: 2411.13251
- Source URL: https://arxiv.org/abs/2411.13251
- Authors: Umamaheswaran Raman Kumar; Abdur Razzaq Fayjie; Jurgen Hannaert; Patrick Vandewalle
- Reference count: 40
- Key outcome: Synthetic 3D point cloud dataset for indoor scene semantic segmentation with OOD occlusion test set

## Executive Summary
BelHouse3D is a synthetic 3D point cloud dataset designed to evaluate semantic segmentation under occlusion conditions. Created from real-world references of 32 Belgian houses, it contains 424 scenes with 19 object classes categorized into building structures, household objects, and clutter. The dataset uniquely includes an out-of-distribution test set with simulated occlusions based on original viewpoints, enabling realistic evaluation of model robustness. Evaluations show significant performance drops under occlusion, with few-shot learning methods demonstrating better generalization to novel classes.

## Method Summary
The dataset uses real-world scene references from 32 Belgian houses to generate synthetic point clouds in Blender, preserving room dimensions and object positions. Object referencing employs predefined models for common household items. The OOD test set introduces occlusions by filtering points visible from original capture viewpoints. The dataset is split 60-20-20 for supervised learning or by class for few-shot learning. Evaluations use standard point-based segmentation methods with 2048-point sampling and sparse tensor representations for transformers.

## Key Results
- Performance drops significantly under occlusion: mIoU decreases from 66.87 to 54.53 and OA from 84.90 to 79.86
- Few-shot methods show better robustness to OOD conditions than fully supervised approaches
- Advanced feature integration techniques (MPTI, AttMPTI) demonstrate stable or improved performance for novel classes under occlusion
- Models struggle particularly with clutter and partially occluded objects in real-world conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Real-world reference-based synthetic generation improves realism compared to procedural generation
- Mechanism: Blender room layouts based on actual room dimensions and object positions preserve spatial distribution and scale characteristics of real indoor environments
- Core assumption: Real-world references provide sufficient information for synthetic generation while maintaining computational efficiency
- Evidence anchors: [section] "This reference aids in replicating the positions of building structures such as walls, ceilings, floors, doors, and windows from collected real-world data into the simulated environment."
- Break condition: If real-world reference data is too sparse or unrepresentative, synthetic generation fails to capture important real-world variations

### Mechanism 2
- Claim: OOD test set with simulated occlusions provides realistic evaluation of model robustness
- Mechanism: Selectively removing points based on original viewpoints introduces occlusions that mimic real-world conditions while preserving spatial relationships
- Core assumption: Occlusion patterns from real-world viewpoints sufficiently represent real-world occlusion complexity
- Evidence anchors: [section] "This process involves using the original viewpoints from which the real-world data was acquired to filter the points of the synthetic point cloud generated in Sec. 3.2, retaining only those visible in the selected viewpoints while discarding the rest."
- Break condition: If synthetic occlusions don't capture full range of real-world occlusion patterns, evaluation may overestimate model robustness

### Mechanism 3
- Claim: Few-shot learning methods demonstrate better robustness to OOD conditions than fully supervised methods
- Mechanism: Few-shot models with attention mechanisms can adapt to novel classes with minimal examples and maintain performance under occlusion
- Core assumption: Generalization capability to novel classes extends to handling OOD conditions for previously seen classes
- Evidence anchors: [section] "For instance, while the ProtoNet and AttProto models exhibit noticeable declines in mIoU for both Base and Novel classes under OOD conditions, models like MPTI and AttMPTI demonstrate relatively stable or improved performance, especially for Novel classes."
- Break condition: If few-shot models rely on features vulnerable to occlusion, robustness advantage may diminish for certain object categories

## Foundational Learning

- Concept: Point cloud data representation and processing
  - Why needed here: Understanding how 3D point clouds differ from 2D images is fundamental to working with this dataset and associated segmentation methods
  - Quick check question: What are the key differences between processing 2D image data and 3D point cloud data for semantic segmentation tasks?

- Concept: Domain adaptation and generalization in machine learning
  - Why needed here: The dataset addresses out-of-distribution generalization, requiring understanding of how models handle distribution shifts
  - Quick check question: How does out-of-distribution (OOD) generalization differ from traditional supervised learning, and why is it particularly challenging for 3D point cloud segmentation?

- Concept: Synthetic data generation and its relationship to real-world data
  - Why needed here: The dataset's creation method involves generating synthetic point clouds from real-world references
  - Quick check question: What are the advantages and limitations of using synthetic data generated from real-world references compared to purely real-world datasets for training machine learning models?

## Architecture Onboarding

- Component map: RealSense capture → Dot3D processing → 3D reconstruction → Blender modeling → Wavefront file generation → Point sampling → Evaluation framework
- Critical path: The synthetic data generation pipeline is most critical, as it directly determines dataset quality and realism
- Design tradeoffs: Prioritizes clean data and accurate annotations over perfect realism by using predefined object models and controlled scene generation
- Failure signatures: Performance degradation on OOD test sets compared to IID test sets indicates models not generalizing well to occlusion conditions
- First 3 experiments:
  1. Baseline evaluation: Train and evaluate PointNet on both IID and OOD test sets to establish performance baselines
  2. Ablation study on synthetic generation: Compare model performance on procedural vs. reference-based synthetic data
  3. Few-shot adaptation test: Evaluate few-shot methods on novel classes under both IID and OOD conditions

## Open Questions the Paper Calls Out

- How does the performance of 3D point cloud semantic segmentation models vary when evaluated on real-world datasets versus synthetic datasets with similar occlusion patterns?
- To what extent do different point cloud sampling strategies impact performance of 3D semantic segmentation models under occlusion conditions?
- How does the inclusion of additional point cloud attributes beyond XYZ coordinates affect performance of 3D semantic segmentation models, particularly under occlusion conditions?

## Limitations

- Synthetic nature may not fully capture real-world variability and complexity despite reference-based generation
- Limited evaluation to point-based segmentation methods without exploring other 3D representation approaches
- Assumption that few-shot generalization to novel classes translates to robustness for previously seen classes under OOD conditions

## Confidence

- **High Confidence**: Dataset creation methodology and evaluation framework are well-specified and reproducible
- **Medium Confidence**: Claim about few-shot methods demonstrating better robustness is supported but could benefit from additional ablation studies
- **Medium Confidence**: Assumption about reference-based synthetic generation providing sufficient realism is plausible but needs validation

## Next Checks

1. Compare model performance trained on BelHouse3D versus a small real-world dataset on the same indoor scenes to quantify synthetic-real gap
2. Conduct detailed analysis of which specific object categories and occlusion types cause most significant performance degradation
3. Evaluate the dataset using point cloud representation methods beyond point-based approaches (voxel-based, multi-view, implicit representations)