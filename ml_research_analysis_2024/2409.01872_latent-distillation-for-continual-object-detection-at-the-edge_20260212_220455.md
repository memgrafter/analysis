---
ver: rpa2
title: Latent Distillation for Continual Object Detection at the Edge
arxiv_id: '2409.01872'
source_url: https://arxiv.org/abs/2409.01872
tags:
- distillation
- task
- latent
- learning
- edge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of continual object detection
  on resource-constrained edge devices, where models must adapt to new classes over
  time without forgetting previously learned ones. The authors propose a two-fold
  solution: (i) they demonstrate the effectiveness of NanoDet, a lightweight open-source
  object detector, for continual learning on edge devices, and (ii) they introduce
  Latent Distillation (LD), a novel distillation-based approach that significantly
  reduces computational overhead compared to existing methods.'
---

# Latent Distillation for Continual Object Detection at the Edge

## Quick Facts
- arXiv ID: 2409.01872
- Source URL: https://arxiv.org/abs/2409.01872
- Authors: Francesco Pasti; Marina Ceccon; Davide Dalle Pezze; Francesco Paissan; Elisabetta Farella; Gian Antonio Susto; Nicola Bellotto
- Reference count: 40
- Primary result: Reduces distillation parameter overhead by 74% and FLOPs by 56% per model update while maintaining competitive detection performance

## Executive Summary
This paper addresses the challenge of continual object detection on resource-constrained edge devices, where models must adapt to new classes over time without forgetting previously learned ones. The authors propose a two-fold solution: (i) they demonstrate the effectiveness of NanoDet, a lightweight open-source object detector, for continual learning on edge devices, and (ii) they introduce Latent Distillation (LD), a novel distillation-based approach that significantly reduces computational overhead compared to existing methods. The key innovation of LD lies in freezing lower layers of the network and performing distillation only on the upper layers, which reduces both memory usage and floating-point operations (FLOPs) during model updates.

## Method Summary
The method uses NanoDet-Plus-m (1.2M parameters, 320×320 input) with ShuffleNetV2 backbone, trained with AdamW optimizer (lr=0.001, wd=0.05), linear warmup (500 steps), and cosine annealing (tmax=100) for 100 epochs per task. Latent Distillation freezes the backbone layers and performs knowledge distillation only on the upper layers, sharing lower layer parameters between teacher and student models to reduce memory overhead. The distillation loss is computed as MSE on old-class logits, with a distillation coefficient of 1, allowing the model head to learn how to interpret frozen features for accurate predictions.

## Key Results
- Latent Distillation reduces distillation parameter overhead by 74% compared to other distillation methods
- Achieves 56% reduction in FLOPs per model update
- Maintains competitive detection performance (mAP@[0.50:0.95]) on VOC and COCO datasets
- Effective for sequential learning of new classes on edge devices with limited computational resources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Latent Distillation reduces computational overhead by freezing lower layers and distilling only upper layer outputs.
- Mechanism: The lower layers (feature extractor/backbone) are frozen, shared between teacher and student models, so their activations are computed once and reused. Distillation loss is computed on upper layer outputs, which reduces FLOPs and memory usage during model updates.
- Core assumption: Lower layers capture general features that remain relevant across tasks, so freezing them does not harm model plasticity significantly.
- Evidence anchors:
  - [abstract] "Latent Distillation (LD), a novel distillation-based approach that significantly reduces computational overhead compared to existing methods" and "freezing lower layers of the network and performing distillation only on the upper layers"
  - [section] "LD operates by feeding the training samples Xn to a frozen subset of initial layers fθ, shared between the student and teacher, generating representations z in the embedding space"
- Break condition: If the lower layers need to adapt significantly for new tasks, freezing them could cause catastrophic forgetting or poor performance on new classes.

### Mechanism 2
- Claim: Sharing lower layers between teacher and student reduces memory overhead by storing parameters only once.
- Mechanism: Since the lower layers are identical for both teacher and student models, only one copy needs to be stored in memory, halving the parameter overhead compared to standard distillation.
- Core assumption: The teacher and student share the same lower layer architecture and initialization.
- Evidence anchors:
  - [abstract] "reducing the distillation parameter overhead by 74%... compared to other distillation methods"
  - [section] "Moreover, as the weights of the lower layers are the same for the teacher and student models, they can be stored only once"
- Break condition: If the teacher and student architectures diverge (e.g., different backbones), this memory saving would be lost.

### Mechanism 3
- Claim: Latent Distillation achieves competitive detection performance with fewer trainable parameters.
- Mechanism: By freezing the backbone and only training upper layers, the model maintains old knowledge through distillation while adapting to new classes with fewer parameters, balancing stability and plasticity.
- Core assumption: Upper layers are responsible for class-specific decision boundaries, so updating them suffices for learning new classes.
- Evidence anchors:
  - [abstract] "maintaining competitive detection performance" despite "74%... distillation parameter overhead" reduction
  - [section] "the model head, instead, learns how to interpret these features to produce accurate predictions"
- Break condition: If new classes require fundamentally different features that the frozen backbone cannot provide, performance may degrade.

## Foundational Learning

- Concept: Continual Learning (CL) and Catastrophic Forgetting
  - Why needed here: The paper addresses learning new object detection classes without forgetting previously learned ones, a core CL challenge.
  - Quick check question: What is catastrophic forgetting, and why is it a problem in continual learning?

- Concept: Knowledge Distillation
  - Why needed here: Latent Distillation uses distillation to transfer knowledge from a frozen teacher model to a student model during updates.
  - Quick check question: How does knowledge distillation help mitigate catastrophic forgetting in continual learning?

- Concept: Object Detection Architectures (FCOS-style)
  - Why needed here: NanoDet is an anchor-free FCOS-style detector; understanding its structure is crucial for implementing Latent Distillation.
  - Quick check question: What are the main components of an FCOS-style object detector like NanoDet?

## Architecture Onboarding

- Component map:
  Backbone (ShuffleNetV2) -> Feature Pyramid (GhostPAN) -> Detection Head
  Distillation module computes loss between teacher and student upper layer outputs

- Critical path:
  1. Input image → frozen backbone → latent features
  2. Latent features → teacher upper layers → old class predictions
  3. Latent features → student upper layers → new class predictions + distillation loss
  4. Update student upper layers only

- Design tradeoffs:
  - Freezing more layers reduces computation but may hurt plasticity
  - Using distillation vs. replay affects memory usage and training speed
  - Choice of distillation loss (MSE) vs. other losses impacts stability

- Failure signatures:
  - High forgetting on old classes → backbone too frozen or distillation loss too weak
  - Poor learning on new classes → upper layers too constrained or insufficient training
  - High memory usage → not sharing lower layers or inefficient parameter storage

- First 3 experiments:
  1. Run Latent Distillation with full backbone frozen; compare mAP@0.5:0.95 to baseline methods on VOC 19p1.
  2. Vary the number of frozen backbone stages; measure trade-off between FLOPs reduction and performance drop.
  3. Compare memory usage (parameters + buffer) of Latent Distillation vs. standard Replay and Latent Replay on COCO 40p40.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of Latent Distillation vary when applied to different object detection architectures beyond NanoDet, such as two-stage detectors like Faster R-CNN or other one-stage detectors like YOLO?
- Basis in paper: [explicit] The paper mentions that most distillation methods in the literature are tailored for specific architectures like Faster R-CNN and that few works consider edge constraints. The authors state that Latent Distillation can be adapted to any distillation-based approach.
- Why unresolved: The paper only evaluates Latent Distillation on NanoDet, leaving its performance on other architectures unexplored.
- What evidence would resolve it: Comparative experiments applying Latent Distillation to various object detection architectures and evaluating performance, memory usage, and computational efficiency.

### Open Question 2
- Question: What is the impact of varying the size of the memory buffer in Replay-based methods when using NanoDet for continual learning, and how does this compare to the memory efficiency of Latent Distillation?
- Basis in paper: [explicit] The paper discusses the memory requirements of Replay methods, noting that a buffer of 250 images requires 77 MB for NanoDet, which can be challenging for edge devices. It also highlights that Latent Distillation reduces memory overhead by 74% compared to distillation methods.
- Why unresolved: The paper does not explore different buffer sizes or compare the trade-offs between buffer size and performance in Replay methods versus Latent Distillation.
- What evidence would resolve it: Experiments varying the memory buffer size in Replay methods and comparing performance, memory usage, and computational efficiency with Latent Distillation.

### Open Question 3
- Question: How does the choice of which layers to freeze in Latent Distillation affect the model's ability to learn new tasks, and is there an optimal strategy for layer freezing based on the specific characteristics of the new task data?
- Basis in paper: [explicit] The paper discusses the trade-off between freezing layers to reduce computation and maintaining model plasticity. It mentions that freezing the backbone affects learning ability and suggests that the choice of frozen layers depends on the edge application's constraints.
- Why unresolved: The paper does not provide a detailed analysis of the impact of different layer-freezing strategies on learning performance across various tasks.
- What evidence would resolve it: Systematic experiments testing different layer-freezing configurations in Latent Distillation and analyzing their effects on model performance and plasticity across diverse task scenarios.

### Open Question 4
- Question: What are the practical limitations and challenges of implementing Latent Distillation on actual edge devices with limited computational resources, and how does it perform under real-world constraints?
- Basis in paper: [explicit] The paper emphasizes the importance of reducing computational and memory overhead for edge deployment and mentions plans to implement the framework on a tinyML device in future work.
- Why unresolved: The paper does not provide empirical results or analysis of Latent Distillation's performance on real edge hardware.
- What evidence would resolve it: Deployment and testing of Latent Distillation on actual edge devices, measuring performance, resource usage, and identifying practical challenges in real-world scenarios.

## Limitations

- Experimental evaluation focuses primarily on mAP metrics without extensive ablation studies on critical design choices
- Memory overhead calculations assume specific implementation details that may vary across frameworks
- Generalizability to other detector architectures beyond NanoDet remains untested

## Confidence

- High Confidence: The core claim that Latent Distillation reduces computational overhead (74% parameter reduction, 56% FLOPs reduction) is well-supported by the methodology and experimental design.
- Medium Confidence: Claims about maintaining "competitive detection performance" are reasonable given the results but would benefit from more extensive validation across different dataset splits and random seeds.
- Low Confidence: The generalizability of the approach to other detector architectures beyond NanoDet remains untested, limiting claims about broader applicability.

## Next Checks

1. **Ablation Study**: Systematically vary the number of frozen backbone layers (e.g., freeze 0%, 25%, 50%, 75%, 100% of backbone) and measure the trade-off between computational savings and detection performance across all benchmarks.

2. **Statistical Validation**: Run each experiment 5 times with different random seeds and report mean ± std for mAP metrics to establish statistical significance of performance differences between LD and baseline methods.

3. **Architecture Generalization**: Implement LD on a different detector architecture (e.g., YOLO or SSD) and evaluate whether similar computational savings and performance maintenance are observed, testing the broader applicability of the approach.