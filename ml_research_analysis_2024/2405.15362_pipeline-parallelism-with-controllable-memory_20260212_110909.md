---
ver: rpa2
title: Pipeline Parallelism with Controllable Memory
arxiv_id: '2405.15362'
source_url: https://arxiv.org/abs/2405.15362
tags:
- memory
- building
- bubble
- pipeline
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a framework to systematically design pipeline
  parallelism schedules by decomposing them into repeating building blocks. It identifies
  that the lifespan of these blocks determines the peak activation memory and shows
  that existing schedules are memory-inefficient due to redundant dependencies and
  imbalanced memory usage.
---

# Pipeline Parallelism with Controllable Memory

## Quick Facts
- arXiv ID: 2405.15362
- Source URL: https://arxiv.org/abs/2405.15362
- Reference count: 40
- One-line primary result: A framework for designing pipeline parallelism schedules that achieves up to 1/3 of 1F1B's memory with comparable throughput, or 1/2 with higher throughput

## Executive Summary
This paper introduces a systematic framework for designing pipeline parallelism schedules by decomposing them into repeating building blocks. The key insight is that the lifespan of these building blocks directly determines peak activation memory. The authors identify that existing schedules are memory-inefficient due to redundant dependencies and imbalanced memory usage. They propose a family of memory-efficient V-Shape building blocks with controllable activation memory, achieving significant memory reductions while maintaining or improving throughput. The framework also includes an adaptive scheduler that can minimize pipeline bubbles while respecting memory constraints.

## Method Summary
The paper proposes a framework that decomposes pipeline schedules into repeating building blocks, where the lifespan of each block determines peak activation memory. The authors introduce V-Shape building blocks that achieve balanced memory usage across devices by collocating stages with long and short lifespans. An adaptive scheduler searches over constrained offset spaces to minimize bubbles while maintaining memory limits. The method is evaluated on large language models, showing 7%-55% throughput improvements in pure pipeline parallelism and 16% in hybrid settings.

## Key Results
- Peak activation memory reduced to 1/3 or 1/2 of 1F1B with comparable or higher throughput
- Achieved near-zero pipeline bubbles while maintaining 1F1B-level activation memory
- 7%-55% throughput improvement in pure pipeline parallelism and 16% in hybrid settings
- V-Shape building blocks enable systematic design of memory-efficient schedules

## Why This Works (Mechanism)

### Mechanism 1
The lifespan of a building block directly determines peak activation memory in pipeline parallelism. Activation memory is allocated at forward pass start and retained until both backward passes complete. The lifespan is the duration between forward start and last backward end. Peak memory is determined by overlapping lifespans of microbatches, following the formula: peak memory ≤ ⌈l/T⌉ * m, where l is lifespan, T is repeating interval, and m is single microbatch memory.

### Mechanism 2
V-Shape building blocks achieve balanced memory usage by collocating stages with long lifespans with those having short lifespans on the same device. This addresses the memory bottleneck problem where stages with long lifespans (like first stage) become unbalanced. The framework partitions the model into twice the number of devices and places the second half in reverse order, ensuring balanced peak memory across all devices.

### Mechanism 3
The adaptive scheduler minimizes pipeline bubbles by searching over a constrained space of offsets while maintaining memory limits. Instead of exhaustive search, it limits to uniform offsets across devices and ensures balanced peak memory. It splits the repeating module into two parts and applies constraints on offsets within each part, achieving O(d) complexity when treating repeating interval as constant.

## Foundational Learning

- Concept: Pipeline parallelism and its memory challenges
  - Why needed here: Understanding basics of model partitioning, device placement, and memory-communication tradeoffs is crucial for grasping the paper's contributions
  - Quick check question: What are the main disadvantages of pipeline parallelism that the paper aims to address?

- Concept: Building blocks and their role in pipeline schedules
  - Why needed here: The paper decomposes pipeline schedules into repeating building blocks; understanding their construction and repetition is essential
  - Quick check question: How does the paper define a building block in the context of pipeline parallelism?

- Concept: Memory-efficient design and optimization
  - Why needed here: The paper proposes memory-efficient building blocks and adaptive scheduling; familiarity with memory optimization in distributed training is necessary
  - Quick check question: What are the two primary reasons identified for why existing pipeline schedules are memory inefficient?

## Architecture Onboarding

- Component map: Building blocks -> Pipeline schedules -> V-Shape building blocks -> Adaptive scheduler -> Peak memory and throughput optimization

- Critical path: Decompose pipeline schedules into building blocks → Calculate lifespan for peak memory → Design V-Shape building blocks for balanced memory → Implement adaptive scheduler for bubble minimization

- Design tradeoffs: Memory vs. throughput (reducing memory may increase bubbles), uniform vs. non-uniform partitioning (simplifies calculation but may not be optimal), controlled vs. exhaustive search (reduces complexity but may miss optimal solutions)

- Failure signatures: High bubble rate (inefficient scheduling or insufficient memory budget), memory bottlenecks (imbalanced memory usage), collisions (overlapping building blocks when repeated)

- First 3 experiments: 1) Compare peak memory usage of V-Shape blocks against 1F1B under various settings, 2) Evaluate throughput of V-Shape schedules with different microbatch sizes and memory limits, 3) Test adaptive scheduler's ability to minimize bubbles while maintaining specified memory budget

## Open Questions the Paper Calls Out

### Open Question 1
How does the memory efficiency of V-Shape building blocks compare to other model parallelism strategies (e.g., tensor parallelism) in scenarios with varying computational costs and memory budgets? The paper shows V-Shape can reduce peak activation memory to 1/2 or 1/3 of 1F1B but doesn't compare these savings directly to tensor parallelism under different scenarios.

### Open Question 2
What are the implications of the proposed framework for pipeline schedules in terms of scalability and adaptability to different hardware architectures? While the framework is systematic, its scalability and adaptability to various hardware setups are not thoroughly investigated.

### Open Question 3
How does the choice of offsets within V-Shape building blocks affect overall performance in terms of throughput and memory usage in real-world applications? The paper discusses uniform offsets for memory control but doesn't provide detailed analysis of how different offset choices impact real-world performance metrics.

## Limitations

- The lifespan-memory relationship relies on uniform partitioning assumptions that may not hold for heterogeneous models
- V-Shape building blocks require doubling the number of stages, which may not be feasible for all architectures
- The adaptive scheduler uses constrained search that could miss optimal solutions in complex scenarios

## Confidence

- Mechanism 1 (lifespan-memory relationship): High - The mathematical framework is well-defined and internally consistent
- Mechanism 2 (V-Shape memory balancing): Medium - The concept is sound but relies on assumptions about model partitioning
- Mechanism 3 (adaptive scheduling): Low - The constrained search space may limit optimality in practice

## Next Checks

1. Test the framework with non-uniform model partitioning to verify lifespan-memory relationship holds
2. Implement the V-Shape building blocks on a heterogeneous architecture to assess memory balancing claims
3. Compare the adaptive scheduler against exhaustive search on smaller problems to quantify optimality loss