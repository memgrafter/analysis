---
ver: rpa2
title: Generating Chain-of-Thoughts with a Pairwise-Comparison Approach to Searching
  for the Most Promising Intermediate Thought
arxiv_id: '2402.06918'
source_url: https://arxiv.org/abs/2402.06918
tags:
- thoughts
- comparison
- intermediate
- each
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of noisy feedback in chain-of-thought
  (CoT) generation, where large language models (LLMs) are used to guide step-by-step
  reasoning. The authors propose a direct pairwise-comparison approach that randomly
  pairs intermediate thoughts and uses the LLM to select the more promising one from
  each pair, iteratively identifying the most promising thoughts.
---

# Generating Chain-of-Thoughts with a Pairwise-Comparison Approach to Searching for the Most Promising Intermediate Thought

## Quick Facts
- arXiv ID: 2402.06918
- Source URL: https://arxiv.org/abs/2402.06918
- Reference count: 40
- Average accuracy of 63.0% on AQuA dataset using C-ToT (Duel.) approach

## Executive Summary
This paper addresses the problem of noisy feedback in chain-of-thought (CoT) generation by proposing a direct pairwise-comparison approach. The method uses large language models (LLMs) to compare intermediate thoughts in pairs, iteratively identifying the most promising ones. Two variants are introduced: C-ToT (Stand.) for standard comparison and C-ToT (Duel.) using a dueling bandit approach. Experiments on three real-world reasoning tasks demonstrate the effectiveness of the proposed algorithm, with C-ToT (Duel.) achieving the highest average accuracy of 63.0% on the AQuA dataset.

## Method Summary
The method involves generating initial intermediate thoughts from a query, then using pairwise comparisons to select the most promising ones. Thoughts are randomly paired and compared using the LLM, with winners advancing to the next round. Two variants are introduced: C-ToT (Stand.) uses ensemble voting with multiple comparisons per pair, while C-ToT (Duel.) employs a dueling bandit approach. Previous unselected thoughts are maintained in a repository for comparison in subsequent rounds, allowing recovery of potentially valuable but misevaluated thoughts.

## Key Results
- C-ToT (Duel.) achieved 63.0% average accuracy on the AQuA dataset, outperforming other contenders
- C-ToT approaches demonstrated superior performance compared to S-ToT (57.1%) and SC-CoT (58.4%) on reasoning tasks
- The method successfully addressed noisy feedback issues in CoT generation through pairwise comparison

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pairwise comparison provides more robust evaluation than point-wise scoring for LLMs due to noise in individual thought evaluation.
- Mechanism: Randomly pair intermediate thoughts and have the LLM select the better option from each pair, iteratively narrowing down to the most promising thoughts.
- Core assumption: LLMs can more reliably distinguish between two intermediate thoughts than assign accurate absolute scores to each thought individually.
- Evidence anchors:
  - [abstract] "we use pairwise-comparison evaluation instead of point-wise scoring to search for promising intermediate thoughts with the noisy feedback from the LLM"
  - [section 1] "we argue that for LLMs, comparing two thoughts simultaneously provides a more robust evaluation than assigning individual scores because it leverages more information"
  - [corpus] Weak evidence - no direct corpus support found for this specific mechanism
- Break condition: The LLM consistently fails to distinguish between pairs or produces highly inconsistent pairwise judgments across multiple trials.

### Mechanism 2
- Claim: Maintaining previous intermediate thoughts in comparison helps recover valuable but misevaluated thoughts.
- Mechanism: Include all previous unselected intermediate thoughts in each round's pairwise comparison to allow backtracking and recovery of potentially valuable thoughts.
- Core assumption: Noise in LLM evaluation can cause valuable intermediate thoughts to be incorrectly discarded early in the process.
- Evidence anchors:
  - [section 1] "we take previous unselected intermediate thoughts into comparison to explore possibly valuable but misevaluated thoughts caused by the noise in LLM's feedback"
  - [section 3] "we maintain a repository of previous intermediate thoughts and take previous unselected thoughts into comparison at each round"
  - [corpus] Weak evidence - no direct corpus support found for this specific mechanism
- Break condition: The number of previous thoughts becomes too large, causing computational inefficiency or token limit issues.

### Mechanism 3
- Claim: Ensemble voting over multiple pairwise comparisons reduces noise in the comparison feedback.
- Mechanism: Compare each pair multiple times (n > 1) and use majority voting to determine the winner, improving robustness against noise.
- Core assumption: Individual pairwise comparisons contain noise, but majority voting over multiple comparisons can reduce this noise.
- Evidence anchors:
  - [section 3.2] "Inspired by the ensemble algorithms in CoT generation (Wang et al., 2022), we can improve the robustness of the comparison feedback by setting n >1"
  - [section 1] "we compare thoughts a and b with the LLM by asking which one is better, using different prompts with n times, where nâ‰¥1"
  - [corpus] Weak evidence - no direct corpus support found for this specific mechanism
- Break condition: The improvement from additional comparisons plateaus or the computational cost outweighs the benefit.

## Foundational Learning

- Concept: Noisy evaluation feedback in LLMs
  - Why needed here: Understanding that LLM scores are inherently noisy and unreliable is fundamental to grasping why pairwise comparison is beneficial
  - Quick check question: What evidence does the paper provide that LLM scores are noisy and unreliable?

- Concept: Best-arm identification with dueling feedback
  - Why needed here: The dueling bandit approach used in C-ToT (Duel.) requires understanding of how to identify the best option through pairwise comparisons rather than absolute scoring
  - Quick check question: How does the dueling bandit approach differ from standard bandit problems in terms of feedback?

- Concept: Sub-Gaussian noise assumption
  - Why needed here: Understanding the noise model is crucial for knowing when the ensemble approach (n > 1) will be effective
  - Quick check question: What is the difference between sub-Gaussian noise and the general noisy comparison assumption used in the dueling bandit approach?

## Architecture Onboarding

- Component map:
  Thought Generator -> LLM Comparator -> Selection Mechanism -> Repository Manager -> Voting System

- Critical path:
  1. Generate initial thoughts (Z1) from query
  2. Pair thoughts randomly and compare each pair
  3. Select winners and maintain unselected thoughts in repository
  4. Generate new thoughts from selected ones
  5. Repeat comparison including repository thoughts
  6. Continue until T rounds complete or answer found

- Design tradeoffs:
  - More comparisons (higher n) vs. computational cost
  - Larger repository of previous thoughts vs. memory constraints
  - Number of selected thoughts (K) vs. exploration vs. exploitation balance
  - Depth of tree (T) vs. reasoning capability vs. computational cost

- Failure signatures:
  - Inconsistent pairwise comparison results across trials
  - Selection mechanism consistently missing the correct answer
  - Repository grows too large, causing memory or token limit issues
  - LLM comparator produces similar responses for very different thoughts

- First 3 experiments:
  1. Implement the basic C-ToT (Stand.) with n=1 and test on a simple reasoning task to verify the pairwise comparison mechanism works
  2. Add the repository feature and test if it recovers previously discarded valuable thoughts
  3. Implement the dueling bandit version (C-ToT (Duel.)) with n=3 and compare performance against the standard version

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the token cost of C-ToT (Duel.) scale with the number of intermediate thoughts and tree depth in practice?
- Basis in paper: [explicit] The paper states the total number of comparisons is less than O(nT log(2mK)), and token costs are task-specific and generally incomparable, with C-ToT having higher token costs than S-ToT in QA tasks due to preserving all previous intermediate thoughts.
- Why unresolved: The paper provides theoretical complexity and task-specific token costs but does not give a general scaling analysis or formula for token cost as a function of problem size parameters.
- What evidence would resolve it: Empirical token cost measurements across varying numbers of intermediate thoughts and tree depths for multiple tasks, or a theoretical model predicting token cost growth.

### Open Question 2
- Question: Does the direct pairwise comparison approach in C-ToT outperform pointwise scoring methods in tasks where intermediate thoughts are inherently ordered or have a clear ranking?
- Basis in paper: [inferred] The paper argues that pairwise comparison provides more robust evaluation than individual scores due to leveraging more information, but this advantage is demonstrated mainly in tasks where individual scoring is noisy. The paper does not explore cases where intermediate thoughts might have inherent order.
- Why unresolved: The paper focuses on noisy evaluation scenarios and doesn't investigate whether pairwise comparison still provides benefits when individual scores are reliable or when there's an inherent ordering.
- What evidence would resolve it: Comparative experiments between C-ToT and S-ToT in tasks where intermediate thoughts can be objectively ranked, measuring both accuracy and robustness to noise.

### Open Question 3
- Question: What is the optimal number of comparisons (n) in the standard mode of C-ToT for balancing noise reduction and computational efficiency?
- Basis in paper: [explicit] The paper mentions that setting n > 1 in the standard mode can improve robustness through majority voting, but doesn't provide guidance on choosing the optimal n value.
- Why unresolved: The paper proposes using majority voting but doesn't analyze how the choice of n affects the trade-off between noise reduction and increased computational cost.
- What evidence would resolve it: Systematic experiments varying n across different tasks and noise levels, measuring accuracy gains versus computational overhead, to identify optimal n values for different scenarios.

## Limitations
- Limited empirical validation with only three reasoning tasks and small Sudoku sample sizes
- Missing ablation studies to quantify individual contributions of repository maintenance and ensemble voting
- Critical implementation details omitted, including exact prompts and edge case handling

## Confidence
- High confidence: The pairwise comparison mechanism itself is technically sound and the experimental setup is clearly specified
- Medium confidence: The noise reduction benefits from ensemble voting and repository maintenance are theoretically justified but lack comprehensive empirical validation
- Low confidence: The claim that pairwise comparison is inherently more robust than point-wise scoring for LLMs is primarily asserted rather than rigorously demonstrated

## Next Checks
1. Implement and test each variant component (repository maintenance, ensemble voting) separately on the AQuA dataset to quantify their individual contributions to performance improvement
2. Evaluate the C-ToT approach on additional reasoning tasks beyond the three reported datasets to assess generalization
3. Systematically vary the noise level in the LLM feedback and measure how different variants perform under varying noise conditions to validate the noise reduction claims