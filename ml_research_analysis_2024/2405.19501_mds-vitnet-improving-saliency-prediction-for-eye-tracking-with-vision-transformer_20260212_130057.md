---
ver: rpa2
title: 'MDS-ViTNet: Improving saliency prediction for Eye-Tracking with Vision Transformer'
arxiv_id: '2405.19501'
source_url: https://arxiv.org/abs/2405.19501
tags:
- transformer
- saliency
- maps
- image
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses visual saliency prediction, aiming to enhance
  eye-tracking accuracy. The authors propose MDS-ViTNet, a novel architecture combining
  a Swin Transformer encoder with multi-decoder CNNs.
---

# MDS-ViTNet: Improving saliency prediction for Eye-Tracking with Vision Transformer

## Quick Facts
- arXiv ID: 2405.19501
- Source URL: https://arxiv.org/abs/2405.19501
- Reference count: 32
- Key result: State-of-the-art AUC of 0.8684 on SALICON dataset

## Executive Summary
MDS-ViTNet introduces a novel architecture for visual saliency prediction that combines a Swin Transformer encoder with multi-decoder CNN structure. The model leverages transfer learning from Vision Transformer to capture long-range context while using dual decoders to process complementary feature sets. Trained on SALICON and CAT2000 datasets, it achieves state-of-the-art performance with AUC of 0.8684, significantly outperforming existing methods.

## Method Summary
MDS-ViTNet employs a Swin Transformer encoder to extract multi-scale features from input images, which are then processed by two parallel CNN decoders that handle different feature subsets. The decoder outputs are merged through an additional CNN layer to produce the final saliency map. The architecture integrates transfer learning to minimize information loss during feature processing. Training uses a combined loss function with weights on KL, CC, and SIM metrics, optimized with 8-bit Adam for 15 epochs.

## Key Results
- Achieved state-of-the-art AUC of 0.8684 on SALICON dataset
- KL divergence of 0.2171, CC of 0.8949, and SIM of 0.7855
- Significant performance improvement over existing saliency prediction methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Swin Transformer backbone enables better long-range context capture compared to traditional CNN backbones for saliency prediction.
- Mechanism: Swin Transformer uses shifted window multi-head attention to maintain local and global feature relationships across different scales, improving representation of complex spatial dependencies.
- Core assumption: Long-range visual dependencies are critical for accurate saliency prediction and are better captured by transformer-based architectures than by local receptive fields in CNNs.
- Evidence anchors:
  - [abstract]: "The framework adopts an encoder-decoder structure, with the encoder utilizing a Swin transformer to efficiently embed most important features."
  - [section]: "Swin transformers as the backbone is key to capturing the maximum number of essential qualitative features in images."
  - [corpus]: Weak - related papers focus on different tasks (e.g., traffic prediction, speech), not saliency prediction with Swin Transformers.
- Break condition: If saliency prediction tasks do not benefit from long-range dependencies, or if computational cost outweighs accuracy gains.

### Mechanism 2
- Claim: Multi-decoder architecture improves prediction accuracy by allowing independent processing of complementary feature sets.
- Mechanism: Two decoders process different subsets of features (x2,x4,x6 vs x1,x3,x5) in parallel, capturing distinct aspects of saliency, then merged via CNN-Merge.
- Core assumption: Different spatial feature scales contribute differently to saliency prediction, and parallel independent processing prevents information bottleneck.
- Evidence anchors:
  - [abstract]: "The decoder employs a multi-decoding technique, utilizing dual decoders to generate two distinct attention maps."
  - [section]: "Thus, it is possible to increase the number of parameters by adding new decoders in parallel, which will not increase the inference time as much as adding consecutive layers."
  - [corpus]: Weak - nearest neighbor papers do not directly validate multi-decoder benefits for saliency prediction.
- Break condition: If the merged saliency map quality does not exceed single-decoder performance, or if decoder parallelization introduces instability.

### Mechanism 3
- Claim: Integration of Swin Transformer with CNN decoders via transfer learning minimizes information loss and leverages complementary strengths.
- Mechanism: Swin Transformer features are extracted and then seamlessly integrated into CNN decoders, preserving fine-grained details while benefiting from transformer context modeling.
- Core assumption: Combining strengths of transformers (context) and CNNs (local detail processing) yields better saliency prediction than either alone.
- Evidence anchors:
  - [abstract]: "This process involves a Transfer Learning method, wherein layers from the Vision Transformer are converted by the Encoder Transformer and seamlessly integrated into a CNN Decoder."
  - [section]: "This methodology ensures minimal information loss from the original input image."
  - [corpus]: Missing - no direct evidence in corpus papers about transformer-CNN hybrid architectures for saliency.
- Break condition: If transfer learning introduces domain mismatch artifacts, or if integration complexity degrades performance.

## Foundational Learning

- Concept: Vision Transformers and Swin Transformer architecture
  - Why needed here: Understanding how Swin Transformer processes images using shifted windows and multi-head attention is crucial for implementing and modifying the encoder.
  - Quick check question: What is the difference between Swin Transformer and vanilla Vision Transformer in terms of computational efficiency and context modeling?

- Concept: Multi-decoder architectures and their benefits
  - Why needed here: The model uses two parallel decoders; understanding how they process different feature sets and how merging works is key for debugging and extension.
  - Quick check question: How does the parallel processing of feature maps in two decoders affect model capacity and inference time compared to a single deep decoder?

- Concept: Saliency prediction metrics (AUC, KL, CC, SIM)
  - Why needed here: Model evaluation and loss functions depend on these metrics; understanding their differences helps in interpreting results and tuning training.
  - Quick check question: Which saliency metric is most sensitive to false positive predictions, and why is it important for eye-tracking applications?

## Architecture Onboarding

- Component map:
  Image -> Swin Transformer backbone -> multi-scale features (x1-x6) -> Transformers (with position embedding) -> contextualized features -> Dual CNN Decoders -> two saliency maps -> CNN-Merge -> final saliency map -> Loss computation -> backpropagation

- Critical path:
  1. Image → Swin Transformer → multi-scale features
  2. Features → Transformers (with position embedding) → contextualized features
  3. Features → Dual Decoders → two saliency maps
  4. Two saliency maps → CNN-Merge → final saliency map
  5. Final map → Loss computation → backpropagation

- Design tradeoffs:
  - Swin Transformer vs ResNet/DenseNet: Better long-range context but higher computational cost
  - Dual decoders vs single decoder: Increased parameter count but no significant inference slowdown; potential for redundancy
  - Transfer learning integration: Leverages pretrained model but may introduce domain mismatch

- Failure signatures:
  - Poor saliency localization: Likely issue in CNN decoders or CNN-Merge stage
  - Blurry or low-contrast predictions: Possible transformer encoder saturation or inappropriate position embedding
  - Training instability: Possible from multi-decoder misalignment or loss function imbalance

- First 3 experiments:
  1. Replace Swin Transformer with ResNet-50 backbone and compare validation AUC to confirm Swin Transformer benefit.
  2. Remove CNN-Merge stage and directly average the two decoder outputs to test if merging adds value.
  3. Train with only a single decoder processing all feature maps to measure impact of parallel decoding.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the multi-decoder architecture affect the model's ability to generalize to different types of images, such as sketches or monochrome images, which were not well-represented in the training data?
- Basis in paper: [explicit] The paper mentions that the CAT2000 dataset, which includes sketches and monochrome images, was only partially used because some categories are very different from the main distribution of the SALICON dataset.
- Why unresolved: The paper does not provide experimental results or analysis on the model's performance on these underrepresented image types.
- What evidence would resolve it: Testing the model on a diverse set of images, including sketches and monochrome images, and comparing its performance to models trained on more diverse datasets.

### Open Question 2
- Question: What is the impact of using Swin Transformer as the encoder compared to other transformer-based architectures like ViT or DeepViT in terms of computational efficiency and prediction accuracy?
- Basis in paper: [explicit] The paper highlights the use of Swin Transformer due to its computational optimality and ability to extract features at different spatial scales, but does not compare it directly with other transformer architectures.
- Why unresolved: The paper does not provide a comparative analysis of different transformer architectures as encoders.
- What evidence would resolve it: Conducting experiments with different transformer architectures as encoders and comparing their performance in terms of accuracy and computational efficiency.

### Open Question 3
- Question: How does the choice of loss function coefficients affect the model's performance, and is there a more optimal way to determine these coefficients?
- Basis in paper: [explicit] The paper mentions that the coefficients for the linear combination of loss functions were assigned based on a study by TranSalNet colleagues, but does not provide a detailed analysis of the impact of these coefficients.
- Why unresolved: The paper does not explore the sensitivity of the model's performance to different loss function coefficients or propose a method for optimizing these coefficients.
- What evidence would resolve it: Performing a sensitivity analysis on the loss function coefficients and experimenting with different optimization techniques to find the most effective coefficients.

## Limitations
- Limited ablation study to isolate contributions of individual architectural components
- Missing implementation details for critical parameters like Swin Transformer depth and window size
- No cross-dataset validation to demonstrate generalization beyond SALICON and CAT2000

## Confidence

**High confidence** in: The technical feasibility of the MDS-ViTNet architecture, as it combines established components (Swin Transformer, CNN decoders, transfer learning) in a logical manner. The methodology for training and evaluation using standard saliency metrics is sound.

**Medium confidence** in: The claimed performance improvements over state-of-the-art methods, given that exact architectural parameters are unspecified and ablation studies are absent. The improvements could stem from multiple factors beyond the proposed innovations.

**Low confidence** in: The attribution of performance gains specifically to the Swin Transformer backbone versus the multi-decoder architecture, due to lack of systematic ablation experiments.

## Next Checks

1. **Ablation study**: Implement and evaluate variants of MDS-ViTNet with (a) ResNet-50 backbone instead of Swin Transformer, (b) single decoder instead of dual decoders, and (c) direct averaging of decoder outputs instead of CNN-Merge to quantify individual contribution of each architectural component.

2. **Cross-dataset evaluation**: Test the trained model on additional saliency prediction datasets (e.g., MIT1003, SALICON validation) to assess generalization beyond the training domains and verify that performance gains are not dataset-specific artifacts.

3. **Parameter sensitivity analysis**: Systematically vary key hyperparameters including transformer depth, number of attention heads, loss function weights, and batch size to determine the stability of performance improvements and identify potential overfitting to specific configurations.