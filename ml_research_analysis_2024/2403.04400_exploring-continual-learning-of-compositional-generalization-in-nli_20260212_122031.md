---
ver: rpa2
title: Exploring Continual Learning of Compositional Generalization in NLI
arxiv_id: '2403.04400'
source_url: https://arxiv.org/abs/2403.04400
tags:
- learning
- compositional
- inference
- continual
- primitive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new challenge called Continual Compositional
  Generalization in Natural Language Inference (C2Gen NLI), which evaluates models'
  ability to perform NLI in a continual learning scenario. The key idea is to simulate
  human-like learning by feeding models primitive inferences in a sequential order,
  requiring them to compose these primitives to solve unseen compositional inferences.
---

# Exploring Continual Learning of Compositional Generalization in NLI

## Quick Facts
- arXiv ID: 2403.04400
- Source URL: https://arxiv.org/abs/2403.04400
- Reference count: 26
- Key outcome: Models struggle with compositional inference in continual learning due to forgetting; experience replay strategies and difficulty-ordered learning improve performance.

## Executive Summary
This paper introduces Continual Compositional Generalization in Natural Language Inference (C2Gen NLI), a new challenge that evaluates models' ability to perform NLI in a continual learning scenario. The key idea is to simulate human-like learning by feeding models primitive inferences in a sequential order, requiring them to compose these primitives to solve unseen compositional inferences. Experiments show that models suffer from catastrophic forgetting when learning primitive inferences sequentially, leading to degraded compositional generalization performance. To address this, the authors benchmark continual learning strategies and find that experience replay strategies help alleviate forgetting. Further analysis reveals that ordering primitives and compositional inference types by difficulty improves performance.

## Method Summary
The paper constructs a compositional NLI dataset from existing NLI datasets (SICK and Ross & Pavlick 2019), with primitive inferences and compositional inference instances. The model uses RoBERTa-Large as backbone, with two classification heads: one for compositional inference (TaskCI) and one for primitive inference recognition (TaskP). The model is jointly optimized using cross-entropy losses. Experiments are run in CGen (offline) and C2Gen (continual) settings, applying continual learning strategies like experience replay to combat forgetting.

## Key Results
- Models experience catastrophic forgetting in C2Gen NLI, with TaskP accuracy dropping significantly after learning new primitive inferences.
- Experience replay strategies (ER-Res, ER-Buff, ER-Mir) alleviate forgetting by replaying stored samples from earlier stages during training of later stages.
- Ordering primitives and compositional inference types by difficulty (easy before hard) improves compositional generalization in continual learning.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Continual learning in compositional NLI suffers from catastrophic forgetting of primitive inference knowledge, which degrades compositional generalization performance.
- **Mechanism**: When a model learns a second primitive (e.g., customary NLI) in stage S2, its representations for the first primitive (e.g., veridical inference) become corrupted due to gradient updates, leading to accuracy drops (e.g., from 100% to 80.72% for veridical inference in ver→nat order).
- **Core assumption**: The model shares parameters across tasks and updates them jointly, so learning new tasks interferes with previously learned representations.
- **Evidence anchors**:
  - [abstract] "Experiments show that models struggle with compositional inference in continual learning due to forgetting."
  - [section] "We find that the accuracy for veridicality drops to 80.72 (↓19.18) after learning primitive NLI in S2."
  - [corpus] Weak: corpus neighbors do not directly discuss forgetting in compositional NLI; the related work focuses on continual learning and compositionality more broadly.
- **Break condition**: If the model uses task-specific parameters or strong regularization to isolate representations, forgetting would be mitigated.

### Mechanism 2
- **Claim**: Experience replay strategies (ER-Res, ER-Buff, ER-Mir) alleviate forgetting by replaying stored samples from earlier stages during training of later stages.
- **Mechanism**: Stored samples act as a constraint on gradient updates, encouraging the model to preserve previously learned knowledge while adapting to new tasks.
- **Core assumption**: Replaying a small, fixed-size episodic memory of earlier-stage samples is sufficient to counteract forgetting without overfitting.
- **Evidence anchors**:
  - [abstract] "To address this, we benchmark continual learning strategies and find that experience replay strategies help alleviate forgetting."
  - [section] "Compared to vanilla C2Gen, all continual strategies yield improved accuracy for both tasks and reduce the forgetting value."
  - [corpus] Weak: corpus neighbors discuss continual learning and knowledge editing but not specifically experience replay in compositional NLI.
- **Break condition**: If the episodic memory is too small or unrepresentative, replay may not preserve enough diversity to prevent forgetting.

### Mechanism 3
- **Claim**: Ordering primitives and compositional inference types by difficulty (easy before hard) improves compositional generalization in continual learning.
- **Mechanism**: Learning easier primitives first establishes stable representations that serve as a foundation for harder compositional tasks; similarly, learning easier compositional functions first allows the model to build compositional skills incrementally.
- **Core assumption**: The difficulty hierarchy is meaningful and consistent across primitives and compositional functions (e.g., fvn easier than fve easier than fvc).
- **Evidence anchors**:
  - [abstract] "Our analyses show that by learning subtasks continuously while observing their dependencies and increasing degrees of difficulty, continual learning can enhance composition generalization ability."
  - [section] "Table 8 shows that the 3 functions varies considerably: fvn ... exhibits significantly higher accuracy (85.74) compared to the other ones; fve ... performs much worse (35.76) but still better than fvc ... with 18.51 points."
  - [corpus] Weak: corpus neighbors discuss curriculum learning and compositional geometry but not specifically ordering by difficulty in continual NLI.
- **Break condition**: If the assumed difficulty ordering is incorrect or the model does not benefit from gradual increases in complexity, performance gains may not materialize.

## Foundational Learning

- **Concept: Catastrophic forgetting**
  - Why needed here: Forgetting is the central challenge in C2Gen; understanding its causes and mitigation is essential to interpreting results.
  - Quick check question: If a model achieves 100% accuracy on primitive V in S1 but drops to 80% after S2, what phenomenon explains this drop?
- **Concept: Episodic memory in continual learning**
  - Why needed here: Experience replay relies on episodic memory; knowing how it is constructed and used is key to benchmarking strategies.
  - Quick check question: What is the role of the episodic memory buffer in ER-Res versus ER-Mir?
- **Concept: Compositionality in NLI**
  - Why needed here: The task requires combining primitive inferences; understanding how composition is defined and evaluated is critical for interpreting CI performance.
  - Quick check question: Given primitive inferences "try to S ↛ S" (neutral) and "catch his dog → catch his pet" (entailment), what is the label of the compositional inference "try to catch his dog ↛ catch his pet"?

## Architecture Onboarding

- **Component map**: Input -> RoBERTa-Large encoder -> [TaskCI head, TaskP head] -> Loss computation -> Gradient update
- **Critical path**: Input -> shared encoder -> [TaskCI head, TaskP head] -> loss computation -> gradient update. Continual learning adds episodic memory replay during S2 updates.
- **Design tradeoffs**: Joint training vs. separate heads (parameter sharing reduces capacity but encourages shared representations); small episodic memory (100 samples) balances forgetting mitigation with computational cost.
- **Failure signatures**: Large accuracy drop in TaskP after S2 indicates forgetting; low TaskCI accuracy despite high TaskP accuracy indicates lack of compositionality; high TaskCI accuracy with low TaskP accuracy indicates shortcut learning.
- **First 3 experiments**:
  1. Run CGen baseline (random order) to establish upper bound on TaskCI accuracy (~46%).
  2. Run C2Gen (ver→nat) without continual strategy to confirm forgetting (~39% TaskCI, ~77% TaskP).
  3. Apply ER-Res to C2Gen (ver→nat) and compare forgetting and TaskCI accuracy to step 2 (~44% TaskCI, ~94% TaskP).

## Open Questions the Paper Calls Out
None

## Limitations
- The study's claims rely heavily on a specific dataset construction that combines two distinct sources (Ross & Pavlick 2019 and SICK), limiting generalizability to other compositional NLI tasks.
- The difficulty ordering assumption is based on observed accuracy patterns rather than a principled measure of difficulty, raising questions about optimality.
- The episodic memory size (100 samples) may not adequately represent the diversity of primitive inferences, potentially limiting the effectiveness of experience replay strategies.

## Confidence
- **High Confidence**: The core finding that models experience catastrophic forgetting in C2Gen NLI and that experience replay strategies can mitigate this forgetting.
- **Medium Confidence**: The claim that ordering primitives and compositional inference types by difficulty improves performance, given that the difficulty hierarchy is based on empirical observations rather than theoretical grounding.
- **Low Confidence**: The generalizability of the proposed C2Gen framework and findings to other domains or compositional tasks beyond NLI.

## Next Checks
1. **Dataset Generalization Test**: Apply the C2Gen framework to a different compositional NLI dataset (e.g., one with different primitive inference types) to assess whether the observed forgetting patterns and experience replay effectiveness generalize.
2. **Difficulty Ordering Validation**: Conduct a systematic study to establish a principled measure of difficulty for primitive and compositional inference types, rather than relying on observed accuracy patterns.
3. **Memory Size Ablation**: Experiment with varying episodic memory sizes (e.g., 50, 100, 200 samples) to determine the optimal memory size for balancing forgetting mitigation and computational efficiency.