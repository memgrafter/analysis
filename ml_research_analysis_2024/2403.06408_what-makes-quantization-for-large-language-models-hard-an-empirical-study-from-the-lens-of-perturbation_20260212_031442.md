---
ver: rpa2
title: What Makes Quantization for Large Language Models Hard? An Empirical Study
  from the Lens of Perturbation
arxiv_id: '2403.06408'
source_url: https://arxiv.org/abs/2403.06408
tags:
- quantization
- uniform
- perturbation
- perturbations
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new perspective for analyzing quantization
  of large language models (LLMs) called the "lens of perturbation". Viewing quantization
  as adding perturbations to weights and activations, the authors conduct systematic
  experiments to explore how different types of perturbations affect LLM performance.
---

# What Makes Quantization for Large Language Models Hard? An Empirical Study from the Lens of Perturbation

## Quick Facts
- **arXiv ID**: 2403.06408
- **Source URL**: https://arxiv.org/abs/2403.06408
- **Reference count**: 7
- **Primary result**: Proposes a non-uniform quantization method that achieves minimal performance degradation on 4-bit and 8-bit quantization for LLM weights and activations

## Executive Summary
This paper introduces a novel perspective for analyzing LLM quantization through the "lens of perturbation," treating quantization as the addition of perturbations to weights and activations. The authors systematically investigate how different perturbation types affect LLM performance, revealing that larger magnitude values are more robust to perturbations while smaller values are highly sensitive. They demonstrate that clipping operations cause significant performance degradation and that uniform quantization is suboptimal due to its equal treatment of values with different sensitivities. Based on these insights, the paper proposes a simple non-uniform quantization method that amplifies small values and compresses large ones, achieving improved performance on 4-bit and 8-bit quantization while maintaining model efficiency.

## Method Summary
The authors conduct post-training quantization (PTQ) experiments on various LLM architectures including BLOOM, OPT, and LLAMA models of different sizes. They systematically apply artificial perturbations to analyze sensitivity patterns, testing different perturbation types including uniform noise, Gaussian noise, and positively-correlated perturbations. The proposed non-uniform quantization method uses an x^(1/3) transformation to amplify smaller values and compress larger ones. Performance is evaluated on downstream tasks including LAMBADA (accuracy), Wikitext-2, and C4 (perplexity). The study compares uniform quantization with the proposed non-uniform approach across multiple model families and scales.

## Key Results
- Larger magnitude values in weights/activations are more robust to quantization perturbations than smaller magnitude values
- Clipping operations cause significantly more performance degradation than regular quantization perturbations
- Uniform quantization is suboptimal as it treats all values equally without considering their sensitivity to perturbations
- The proposed non-uniform quantization method achieves minimal performance degradation on 4-bit and 8-bit quantization for weights and activations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger magnitude values in weights/activations are more robust to quantization perturbations than smaller magnitude values
- Mechanism: The quantization error introduces perturbations that scale with the magnitude of values. Larger values have proportionally smaller relative perturbations, making them less sensitive to quantization noise
- Core assumption: The quantization error is proportional to the value magnitude rather than being absolute
- Evidence anchors:
  - [abstract] "Larger values can endure more significant perturbations with minimal performance degradation, while smaller values can only tolerate minor perturbations"
  - [section] "We have found that the sampling distribution has little impact on performance... On the other hand, the magnitude of perturbations plays a crucial role. Specifically, the Positively-correlated perturbation âˆ†M+ (where larger values have larger perturbations) exhibits a dominant advantage"
  - [corpus] No direct corpus evidence found for this specific mechanism
- Break condition: If quantization errors are found to be magnitude-independent or if smaller values show unexpected robustness

### Mechanism 2
- Claim: Clipping operations cause significantly more performance degradation than regular quantization perturbations
- Mechanism: Clipping creates extreme perturbations by forcing out-of-range values to the clipping threshold, introducing discontinuities that the model cannot adapt to
- Core assumption: The model relies on the full range of values, including outliers, for proper function
- Evidence anchors:
  - [section] "clipping can be viewed as an exceedingly large perturbation applied to out-of-range values" and "clipping has a considerably more adverse impact than regular perturbations"
  - [section] "This finding aligns with the outlier phenomenon, which posits that extremely large values are crucial for LLM performance"
  - [corpus] No direct corpus evidence found for this specific mechanism
- Break condition: If clipping operations are shown to be benign or if the model can learn to compensate for clipped values

### Mechanism 3
- Claim: Uniform quantization is suboptimal because it treats all values equally without considering their sensitivity to perturbations
- Mechanism: By applying uniform quantization intervals across all value ranges, the method applies inappropriate precision levels - too coarse for sensitive small values and too fine for robust large values
- Core assumption: Different value ranges have different sensitivities to quantization error
- Evidence anchors:
  - [abstract] "Uniform quantization is suboptimal as it treats all values equally without considering their sensitivity"
  - [section] "Our second observation pertains to the outlier phenomenon... These insights obtained through the lens of perturbation explain why uniform quantization may not always be optimal"
  - [corpus] No direct corpus evidence found for this specific mechanism
- Break condition: If empirical evidence shows uniform quantization performs equally well across all value ranges

## Foundational Learning

- Concept: Perturbation analysis framework
  - Why needed here: Understanding how artificial perturbations affect model performance provides insights into the natural perturbations caused by quantization
  - Quick check question: If you add Gaussian noise to weights with the same variance as quantization error, what performance metric would you monitor to compare quantization robustness?

- Concept: Non-uniform quantization
  - Why needed here: The paper proposes non-uniform quantization as a solution based on insights from perturbation analysis
  - Quick check question: How would you design a non-uniform quantization scheme that applies finer quantization to smaller values and coarser quantization to larger values?

- Concept: Outlier phenomenon in transformer models
  - Why needed here: Understanding why certain dimensions consistently exhibit large values helps explain quantization challenges
  - Quick check question: What percentage of values typically constitute outliers in LLM activations, and how does this vary across different model families?

## Architecture Onboarding

- Component map: Quantization module -> Perturbation analysis module -> Non-uniform transformation module -> Performance evaluation module
- Critical path:
  1. Load pre-trained model weights
  2. Apply quantization (uniform or non-uniform)
  3. Run inference on benchmark tasks
  4. Measure performance degradation
  5. Apply perturbation analysis if needed
  6. Iterate with modified quantization strategy
- Design tradeoffs:
  - Memory efficiency vs. computational overhead (non-uniform quantization adds computation)
  - Bit-width selection (4-bit vs 8-bit) based on model family sensitivity
  - Granularity choice (tensor-wise vs channel-wise) affecting precision
- Failure signatures:
  - Catastrophic perplexity increase (>10x) indicates severe quantization failure
  - Consistent performance degradation across all model families suggests fundamental issues
  - Model-specific failures (e.g., OPT more sensitive than BLOOM) indicate architecture-dependent challenges
- First 3 experiments:
  1. Apply uniform quantization to BLOOM-560M and measure performance on LAMBADA benchmark
  2. Apply positively-correlated perturbations to the same model and compare degradation patterns
  3. Implement non-uniform quantization with x^(1/3) transformation and measure improvement over uniform baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the sensitivity to perturbations vary across different types of layers in LLMs (e.g., attention vs. feed-forward)?
- Basis in paper: [inferred] The paper discusses varying sensitivity to perturbations across different model families and scales but does not analyze layer-specific sensitivity
- Why unresolved: The paper focuses on overall model performance without isolating the contribution of individual layer types to quantization robustness
- What evidence would resolve it: Systematic experiments applying targeted perturbations to specific layer types (attention, FFN, etc.) and measuring performance impact would clarify layer-wise sensitivity

### Open Question 2
- Question: What is the optimal non-linear transformation function for non-uniform quantization beyond the cubic root used in this study?
- Basis in paper: [explicit] The paper uses f(x) = x^(1/3) for non-uniform quantization but notes this is a simple choice without exploring alternatives
- Why unresolved: Only one non-linear transformation was tested, leaving open the question of whether other functions could yield better quantization performance
- What evidence would resolve it: Comparative experiments testing various non-linear transformations (e.g., different roots, sigmoid variants, learned transformations) on multiple LLM architectures would identify optimal functions

### Open Question 3
- Question: How does the perturbation sensitivity change during the course of training rather than just for pre-trained models?
- Basis in paper: [inferred] The study focuses on zero-shot post-training quantization without examining how sensitivity evolves during training
- Why unresolved: The paper assumes constant perturbation sensitivity for pre-trained models but doesn't investigate how this property develops during training
- What evidence would resolve it: Tracking perturbation sensitivity metrics throughout training of LLMs would reveal whether and how sensitivity patterns emerge and stabilize

## Limitations
- The proposed x^(1/3) non-uniform quantization method lacks rigorous theoretical justification for why this specific transformation works better than alternatives
- The perturbation analysis framework doesn't account for complex interactions between weight and activation quantization errors
- The study focuses primarily on 4-bit and 8-bit quantization, leaving uncertainty about whether findings generalize to lower bit-widths

## Confidence
- **High Confidence**: The empirical finding that larger magnitude values are more robust to quantization perturbations (supported by systematic experiments across multiple model families)
- **Medium Confidence**: The claim that clipping operations cause significantly more degradation than regular quantization (based on controlled experiments but with limited ablation studies)
- **Low Confidence**: The assertion that uniform quantization is fundamentally suboptimal (largely theoretical, with limited comparison to other non-uniform approaches)

## Next Checks
1. Conduct ablation studies testing alternative non-uniform transformations (logarithmic, piecewise linear) to determine if x^(1/3) is optimal or if the benefits generalize to other monotonic transformations
2. Perform perturbation analysis on mixed-precision quantization scenarios where different layers use different bit-widths to understand how sensitivity varies across model architecture
3. Test the robustness of findings on task-specific fine-tuned models rather than just base LLMs to verify whether sensitivity patterns hold after domain adaptation