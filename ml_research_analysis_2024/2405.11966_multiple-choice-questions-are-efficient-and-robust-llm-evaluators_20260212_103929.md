---
ver: rpa2
title: Multiple-Choice Questions are Efficient and Robust LLM Evaluators
arxiv_id: '2405.11966'
source_url: https://arxiv.org/abs/2405.11966
tags:
- gsm8k
- figure
- qwen1
- questions
- gsm-mc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents GSM-MC, MATH-MC, and PythonIO, three multiple-choice
  (MC) datasets constructed from existing LLM evaluation benchmarks (GSM8K, MATH,
  HumanEval/MBPP) by collecting distractor answers from 60 open-source models. Through
  extensive experiments, the authors demonstrate that LLMs' performance on these MC
  versions strongly correlates with their performance on the original generation benchmarks,
  with Pearson correlations ranging from 0.7989 to 0.8756 across different MC formats.
---

# Multiple-Choice Questions are Efficient and Robust LLM Evaluators

## Quick Facts
- arXiv ID: 2405.11966
- Source URL: https://arxiv.org/abs/2405.11966
- Reference count: 16
- Models tested achieve only 61.1% accuracy on GSM-MC, 60.3% on MATH-MC, and 70.1% on PythonIO

## Executive Summary
This paper introduces a novel approach to LLM evaluation by converting short-answer generation benchmarks into multiple-choice (MC) format. The authors construct three MC datasets (GSM-MC, MATH-MC, PythonIO) by collecting distractor answers from 60 open-source models and demonstrate that MC evaluation strongly correlates with original generation benchmarks while providing up to 30x speedup. Through extensive experiments, they show that MC evaluation maintains robustness against distractor quality and option ordering, offering a more efficient and noise-resistant alternative to traditional generation-based evaluation methods.

## Method Summary
The authors convert existing LLM evaluation benchmarks (GSM8K, MATH, HumanEval/MBPP) into multiple-choice format by collecting distractor answers from 60 open-source models. They evaluate models on these MC versions using logit extraction and softmax operations, comparing performance with original generation benchmarks. The approach simplifies evaluation from autoregressive generation to single-softmax operations, dramatically reducing computational requirements while maintaining strong correlations (0.7989 to 0.8756) with original benchmarks.

## Key Results
- MC evaluation achieves Pearson correlations of 0.7989 to 0.8756 with original generation benchmarks
- Evaluation time reduced by up to 30x factor compared to generation-based methods
- Even best models (LLaMA-3-70B-Instruct) achieve only 61.1% accuracy on GSM-MC, indicating challenging benchmark
- MC format demonstrates robustness against distractor choices and option ordering variations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting short-answer generation benchmarks to multiple-choice format reduces evaluation noise from invalid model outputs
- Mechanism: By providing explicit answer choices (A, B, C, D), the model no longer needs to generate the final answer format, eliminating errors from format misunderstanding or irrelevant outputs
- Core assumption: The original benchmark questions have unique, verifiable correct answers that can be converted into discrete options
- Evidence anchors:
  - [abstract] "MC evaluation approach reduces computation time by up to 30x while maintaining robustness against distractor choices and option orders"
  - [section] "However, as shown in Figure 1, LLMs may not always follow the required answer format during generation, which results in many false negatives when the answers are heuristically extracted from model generations and evaluated by exact match"
  - [corpus] Weak - no direct corpus evidence about format errors
- Break Condition: When benchmark questions have multiple valid interpretations or require open-ended reasoning that cannot be captured in discrete choices

### Mechanism 2
- Claim: Model performance on MC versions strongly correlates with original generation benchmarks
- Mechanism: The MC format preserves the underlying reasoning task while eliminating format-specific noise, maintaining the discriminative power between model capabilities
- Core assumption: The distractor choices effectively represent plausible but incorrect reasoning paths that models might take
- Evidence anchors:
  - [abstract] "LLMs' performance on these MC versions strongly correlates with their performance on the original generation benchmarks, with Pearson correlations ranging from 0.7989 to 0.8756 across different MC formats"
  - [section] "Through extensive experiments involving different numbers of choices (Section 3.4.1) and robustness against different distractor choices and option orders (Section 3.4.2), we show that LLMs' performance on GSM-MC is robust to distractors and option orders, and strongly correlated with the performance on the original GSM8K"
  - [corpus] Weak - no direct corpus evidence about correlation maintenance
- Break Condition: When distractor generation fails to capture meaningful alternative reasoning paths, reducing the discriminative power of the MC format

### Mechanism 3
- Claim: MC evaluation provides 30x speedup compared to generation-based evaluation
- Mechanism: MC evaluation simplifies the task from autoregressive generation to a single softmax operation over option logits, dramatically reducing computational requirements
- Core assumption: The model's output logits for option tokens can be reliably extracted and compared
- Evidence anchors:
  - [abstract] "the evaluation time is reduced by a factor of up to 30"
  - [section] "After converting to MC format, the evaluation process no longer involves auto-regressive generation but simplifies into one softmax operation over the option tokens' corresponding output logits at the end of the prompt. This leads to a significant reduction in computation cost: evaluating Qwen1.5-32B on the original GSM8K dataset takes 7 hours on our machine (distributed across 3 RTX 3090) while evaluating it on the newly constructed 4-way MC dataset takes only 13 minutes on the same machine"
  - [corpus] Weak - no direct corpus evidence about computational speedup
- Break Condition: When the model requires complex reasoning across multiple tokens that cannot be captured in a single softmax operation

## Foundational Learning

- Concept: Correlation between MC and generation performance
  - Why needed here: Understanding whether MC format preserves the relative ranking of model capabilities is crucial for its validity as an evaluation method
  - Quick check question: If Model A scores 70% on original GSM8K and Model B scores 50%, what range of correlation coefficients would indicate the MC format preserves this ranking relationship?

- Concept: Distractor generation and selection
  - Why needed here: Creating effective distractors is essential for maintaining the discriminative power of MC evaluation
  - Quick check question: What characteristics should distractors have to effectively differentiate between models with varying reasoning capabilities?

- Concept: Option tokenization and model understanding
  - Why needed here: Ensuring models understand the MC format and can properly select from discrete options is fundamental to the evaluation approach
- Quick check question: How can you verify that a model is not simply outputting the most frequent token but is actually understanding the MC format?

## Architecture Onboarding

- Component map: Original benchmark → distractor collection → MC dataset construction → model inference → logit extraction → option selection → accuracy calculation → correlation analysis

- Critical path: 1. Collect distractors from multiple models on each question → 2. Construct MC datasets with balanced option distributions → 3. Evaluate models on MC format using logit extraction → 4. Compare MC performance with original generation performance

- Design tradeoffs:
  - Number of choices vs. discriminative power: More choices may better capture model uncertainty but could introduce noise
  - Distractor quality vs. dataset size: High-quality distractors require more model evaluations but improve correlation
  - Computational efficiency vs. evaluation fidelity: MC format trades some nuance for significant speedup

- Failure signatures:
  - Low correlation between MC and generation performance indicates loss of discriminative power
  - Model performance concentrated on specific options suggests format misunderstanding
  - High variance across different distractor sets indicates robustness issues

- First 3 experiments:
  1. Evaluate correlation between MC and generation performance across different choice numbers (2-way to 8-way)
  2. Test robustness by randomizing option orders and comparing performance consistency
  3. Compare model-generated distractors versus random distractors in terms of correlation preservation

## Open Questions the Paper Calls Out
None

## Limitations
- Limited generalization beyond short-answer tasks with discrete correct answers
- Heavy dependency on distractor quality generated by open-source models
- Potential for models to develop format-specific optimization patterns

## Confidence
- **High Confidence**: Computational efficiency claim (30x speedup) with concrete timing measurements
- **Medium Confidence**: Correlation results (0.7989 to 0.8756) are statistically significant but need broader benchmark validation
- **Low Confidence**: Robustness claims against distractor choices and option orders need edge case testing

## Next Checks
1. Test MC conversion approach on diverse LLM evaluation benchmarks including subjective answers and open-ended tasks
2. Systematically evaluate how varying distractor quality affects correlation strength using human-generated, model-generated, and random distractors
3. Investigate whether models develop MC-specific heuristics by training exclusively on MC-formatted questions and analyzing attention patterns