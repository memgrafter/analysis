---
ver: rpa2
title: 'Lightweight Large Language Model for Medication Enquiry: Med-Pal'
arxiv_id: '2407.12822'
source_url: https://arxiv.org/abs/2407.12822
tags:
- llms
- agents
- med-pal
- performance
- drugs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study trained and evaluated Med-Pal, a lightweight LLM-based\
  \ chatbot for answering medication-related queries. Using a fine-grained dataset\
  \ of 1,100 Q&A pairs, five small open-source LLMs (\u22647B parameters) were fine-tuned\
  \ and assessed via a novel clinical rubric (SCORE)."
---

# Lightweight Large Language Model for Medication Enquiry: Med-Pal

## Quick Facts
- arXiv ID: 2407.12822
- Source URL: https://arxiv.org/abs/2407.12822
- Reference count: 40
- Primary result: Fine-tuned Mistral-7B achieved 71.9% high-quality responses in safety and accuracy domains for medication inquiries

## Executive Summary
This study presents Med-Pal, a lightweight LLM-based chatbot designed for answering medication-related queries. Using a fine-grained dataset of 1,100 Q&A pairs, five small open-source LLMs (≤7B parameters) were fine-tuned and assessed via a novel clinical rubric (SCORE). Mistral-7b emerged as the top performer, achieving 71.9% high-quality responses in safety and accuracy domains. Med-Pal was then guard-railed against adversarial prompts and benchmarked against BioMistral and Meerkat, showing comparable or superior performance, particularly in patient communication clarity. The approach demonstrates that fine-tuned lightweight LLMs can deliver safe, accurate, and clinically aligned responses while supporting deployment in low-connectivity environments.

## Method Summary
The researchers fine-tuned five lightweight LLMs (Llama-7B, Falcon-7B, Mistral-7B, Danube-1.8B, TinyLlama-1.1B) on a curated dataset of 1,100 medication Q&A pairs covering 110 medications across 14 ATC categories. Models were trained for 3 epochs with LORA fine-tuning and evaluated using a clinical rubric (SCORE) by multi-disciplinary raters. The best-performing model (Mistral-7B) was integrated with guardrails using llm-guard and tested against adversarial prompts before being benchmarked against larger biomedical models.

## Key Results
- Mistral-7B achieved 71.9% high-quality responses in safety and accuracy domains
- Fine-tuned lightweight models outperformed general-purpose and biomedical-pretrained models in patient communication clarity
- Med-Pal demonstrated comparable performance to larger models while enabling deployment in low-connectivity environments

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning lightweight LLMs on a task-specific medication dataset improves clinical accuracy and safety compared to general-purpose or biomedical-pretrained models. Domain-specific fine-tuning on expert-curated Q&A pairs aligns the model's outputs with medical consensus, reducing hallucinations and bias. The training dataset is representative of real-world medication inquiries and sufficiently comprehensive across drug classes and question types.

### Mechanism 2
Guard-railing via adversarial prompt testing and llm-guard integration prevents harmful or inappropriate medical advice. Proactive scanning of both input prompts and model outputs filters out unsafe content based on predefined content guidelines and toxicity thresholds. The adversarial prompt library and scanning thresholds are comprehensive enough to catch most harmful use cases.

### Mechanism 3
Lightweight models (≤7B parameters) enable deployment in low-connectivity environments while maintaining performance comparable to larger models. Smaller parameter counts reduce computational load and memory footprint, allowing local inference on edge devices. The performance gap between lightweight and large models is negligible for the target task when fine-tuned appropriately.

## Foundational Learning

- **Multi-disciplinary clinical evaluation rubric (SCORE)**: Ensures that model responses meet safety, accuracy, bias, reproducibility, and ease-of-understanding criteria from healthcare perspectives. Quick check: What are the five domains assessed by the SCORE rubric, and why is each critical for medical chatbot deployment?
- **Fine-tuning vs. RAG (Retrieval-Augmented Generation)**: Determines whether to adapt a base LLM to the domain (fine-tuning) or provide external knowledge at inference (RAG). Quick check: What are the main tradeoffs between fine-tuning and RAG for a lightweight medical chatbot?
- **Adversarial prompt testing ("red-teaming")**: Validates that safety mechanisms can withstand attempts to elicit harmful or misleading content. Quick check: What categories of adversarial prompts were tested against Med-Pal, and what was the observed outcome?

## Architecture Onboarding

- **Component map**: Expert-curated Q&A dataset → train/validation/test splits → Five base LLMs → fine-tuned variants → llm-guard integration → multi-disciplinary rater scoring → standardized inference configs
- **Critical path**: Fine-tuning → validation scoring → guardrail integration → adversarial testing → benchmarking
- **Design tradeoffs**: Model size vs. performance (7B parameters chosen to balance capability and resource constraints), guardrail strictness vs. user experience (high thresholds prevent harm but may limit conversational flexibility), dataset breadth vs. depth (1,100 Q&As cover many drug classes but may miss rare scenarios)
- **Failure signatures**: Guardrail bypasses (model generates harmful content despite safety filters), low reproducibility (inconsistent answers to repeated queries), dataset bias (underperformance on underrepresented medication classes or question types)
- **First 3 experiments**: 1) Replicate fine-tuning on a subset of the dataset to verify reproducibility of Mistral-7B performance, 2) Stress-test guardrails with novel adversarial prompts not in the original test set, 3) Benchmark inference latency and memory usage on target edge devices (e.g., smartphone)

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but several remain unresolved regarding the generalizability and long-term viability of the approach.

## Limitations

- Clinical evaluation rubric (SCORE) shows poor inter-rater reliability (Fleiss' Kappa 0.111), raising concerns about the consistency and validity of performance assessments
- Training dataset of 1,100 Q&A pairs may lack coverage of rare medication scenarios or complex patient inquiries
- Guardrail effectiveness was validated through visual inspection of random samples rather than systematic adversarial testing across all safety dimensions

## Confidence

- **High Confidence**: Lightweight models can be fine-tuned for medication Q&A and deployed on resource-constrained devices
- **Medium Confidence**: Fine-tuned Mistral-7B outperforms other lightweight models and demonstrates adequate safety guardrails for clinical use
- **Low Confidence**: SCORE rubric provides reliable clinical validation, and the model's performance generalizes to all real-world medication inquiries

## Next Checks

1. Conduct systematic adversarial testing with expanded prompt categories to validate guardrail robustness against novel harmful content patterns
2. Replicate clinical scoring with a refined SCORE rubric and trained evaluators to establish reliability (target Fleiss' Kappa > 0.4) and validate performance claims
3. Test model performance on a comprehensive benchmark of real-world medication queries from diverse patient populations to assess generalizability beyond the curated dataset