---
ver: rpa2
title: Scalable Transformer for High Dimensional Multivariate Time Series Forecasting
arxiv_id: '2408.04245'
source_url: https://arxiv.org/abs/2408.04245
tags:
- series
- time
- data
- forecasting
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of forecasting high-dimensional
  multivariate time series data, where the number of channels (dimensions) outpaces
  existing channel-dependent models. The authors propose STHD, a scalable transformer
  architecture designed specifically for high-dimensional MTS forecasting.
---

# Scalable Transformer for High Dimensional Multivariate Time Series Forecasting

## Quick Facts
- arXiv ID: 2408.04245
- Source URL: https://arxiv.org/abs/2408.04245
- Reference count: 40
- Key outcome: STHD achieves an average 8.91% performance improvement over state-of-the-art baselines on high-dimensional multivariate time series forecasting

## Executive Summary
This paper addresses the challenge of forecasting high-dimensional multivariate time series data, where the number of channels (dimensions) outpaces existing channel-dependent models. The authors propose STHD, a scalable transformer architecture designed specifically for high-dimensional MTS forecasting. STHD's key innovations include: a) Relation Matrix Sparsity to limit noise and reduce memory usage; b) ReIndex as a training strategy for flexible batch sizes and increased data diversity; and c) a 2-D Transformer to capture channel dependencies. Experiments on three high-dimensional datasets (Crime-Chicago, Wiki-People, and Traffic) demonstrate STHD's significant improvement over state-of-the-art baselines.

## Method Summary
STHD is a transformer-based architecture for high-dimensional multivariate time series forecasting that addresses scalability challenges through three key innovations. First, it uses correlation matrix sparsity via DeepGraph to identify and focus on only the most related series, reducing computational complexity. Second, it implements ReIndex as a training strategy that reshapes data to enable flexible batch sizes and improve training diversity. Third, it employs a 2-D Transformer that processes both temporal and channel dimensions simultaneously to capture cross-dimensional dependencies. The architecture is designed to maintain the benefits of channel-dependent modeling while overcoming the computational and memory limitations that typically prevent such models from scaling to high-dimensional data.

## Key Results
- STHD achieves an average 8.91% improvement in forecasting accuracy over state-of-the-art baselines
- The model successfully handles datasets with up to 6,107 channels (Wiki-People) without performance degradation
- Correlation matrix sparsity reduces memory usage by focusing only on top-K related series while maintaining accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparsifying the correlation matrix using DeepGraph reduces noise and computational load, enabling the model to focus on truly related series.
- Mechanism: The model computes pairwise correlations between all series, then retains only the top K most related series per target, creating a sparse adjacency graph. This reduces the effective dimensionality of the attention computation.
- Core assumption: Most series in high-dimensional data are uncorrelated or weakly correlated; only a small subset is relevant for accurate forecasting.
- Evidence anchors:
  - [abstract]: "Relation Matrix Sparsity that limits the noise introduced and alleviates the memory issue"
  - [section 4.1]: "We use correlation matrix sparsity to get K related series... The fast computation of pairwise correlation matrices in the DeepGraph framework is attributed to a combination of advanced data structures..."
  - [corpus]: No direct comparison; the neighbor papers propose similar sparsity strategies, but without evaluating correlation matrix sparsity specifically.
- Break condition: If high-dimensional data has dense correlations (many series are related), pruning to top-K will discard useful information and hurt accuracy.

### Mechanism 2
- Claim: ReIndex strategy reduces memory usage and improves training diversity by sampling across both series and window dimensions instead of only windows.
- Mechanism: Instead of sampling batches of shape (batch_size x M x (1+K)), ReIndex reshapes the data to (M x S) x (1+K) and samples batches of shape (batch_size x (1+K)), effectively reducing memory by factor M.
- Core assumption: Series are independent enough that shuffling windows across series does not harm temporal consistency, and diversity improves generalization.
- Evidence anchors:
  - [abstract]: "ReIndex applied as a training strategy to enable a more flexible batch size setting and increase the diversity of training data"
  - [section 4.2]: "ReIndex reshapes the windows to (M x S) x (1+K) and samples batches on the M x S dimension... improves the diversity of each training batch from specific windows in all M series to any window in any series."
  - [corpus]: No direct evidence; the neighbor papers do not describe similar batch sampling strategies.
- Break condition: If series are strongly temporally dependent on each other, shuffling windows across series could break temporal coherence and degrade performance.

### Mechanism 3
- Claim: 2-D Transformer with combined temporal and channel attention captures dependencies missed by separate-stage attention models.
- Mechanism: The model processes 2-D inputs (time x channels) with self-attention over both dimensions simultaneously, rather than using separate stages for time and channel attention.
- Core assumption: Cross-channel and cross-time dependencies are intertwined and better captured when modeled jointly.
- Evidence anchors:
  - [abstract]: "c) Transformer that handles 2-D inputs and captures channel dependencies"
  - [section 4.3]: "In our work, we use self-attention to learn across the time and channels of target series and related series"
  - [corpus]: The neighbor paper "TiVaT" also proposes a unified mechanism for capturing asynchronous dependencies, supporting this assumption.
- Break condition: If cross-time and cross-channel dependencies are truly separable, joint attention may introduce unnecessary complexity and reduce efficiency.

## Foundational Learning

- Concept: Multivariate time series forecasting with channel-dependent vs channel-independent modeling
  - Why needed here: The paper contrasts these two paradigms and claims channel-dependent models underperform on high-dimensional data unless properly scaled.
  - Quick check question: In channel-independent modeling, are the series processed independently or jointly?

- Concept: Transformer attention mechanism and computational complexity
  - Why needed here: The 2-D Transformer and ReIndex strategy both rely on understanding how attention scales with input dimensions.
  - Quick check question: What is the computational complexity of self-attention in terms of sequence length and feature dimension?

- Concept: Correlation-based similarity measures and graph construction
  - Why needed here: DeepGraph and correlation matrix sparsity are central to the proposed approach.
  - Quick check question: What is the time complexity of computing a full pairwise correlation matrix for M series?

## Architecture Onboarding

- Component map: Input → DeepGraph (sparse correlation) → ReIndex (data reshaping) → 2-D Transformer (joint attention) → Output
- Critical path: Data preprocessing (DeepGraph + ReIndex) → 2-D Transformer encoding → Forecasting
- Design tradeoffs: Memory vs. accuracy (sparsity), diversity vs. coherence (ReIndex), complexity vs. performance (joint attention)
- Failure signatures: Memory errors during training (batch size too large), degraded accuracy (over-sparsification), overfitting (insufficient diversity)
- First 3 experiments:
  1. Test DeepGraph correlation computation speed and correctness on a small synthetic dataset.
  2. Validate ReIndex reshaping preserves temporal order and improves batch diversity.
  3. Compare 2-D Transformer with separate-stage attention baseline on a low-dimensional dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does STHD's performance scale with extremely high-dimensional datasets beyond the three tested datasets?
- Basis in paper: [explicit] The authors mention evaluating STHD on three high-dimensional datasets (Crime-Chicago, Wiki-People, and Traffic) and achieving significant improvements, but do not explore performance beyond these specific datasets.
- Why unresolved: The paper only evaluates STHD on three specific datasets, leaving questions about its scalability to datasets with even higher dimensions or different characteristics.
- What evidence would resolve it: Testing STHD on datasets with significantly higher dimensions than the current benchmarks, such as those with millions of channels, and analyzing performance degradation or improvements.

### Open Question 2
- Question: What is the optimal balance between the number of related series (K) and model performance across different types of high-dimensional datasets?
- Basis in paper: [explicit] The authors discuss the impact of the hyperparameter K on performance and find that increasing K initially improves performance but then leads to degradation due to noise introduction, but do not provide a universal optimal K value.
- Why unresolved: The paper only shows the trend of K's impact on performance without determining a specific optimal value for different dataset characteristics or domain types.
- What evidence would resolve it: Systematic experiments varying K across diverse high-dimensional datasets and analyzing the point of diminishing returns or optimal K values for different data types.

### Open Question 3
- Question: How does STHD compare to other state-of-the-art methods when incorporating additional external features or metadata?
- Basis in paper: [inferred] The paper focuses on the core time series data without discussing the incorporation of external features, while modern forecasting systems often benefit from additional contextual information.
- Why unresolved: The paper evaluates STHD in isolation without considering how it would perform when enriched with external data sources that could improve forecasting accuracy.
- What evidence would resolve it: Comparative experiments where STHD is augmented with external features and compared against other methods that also incorporate such information, measuring the relative improvements.

## Limitations
- The assumption of sparse correlations in high-dimensional data is not rigorously tested across different domains
- The effectiveness of ReIndex strategy depends on series independence, which is not empirically validated
- The superiority of 2-D Transformer over separate-stage attention models lacks direct ablation evidence

## Confidence

**High Confidence**: The overall architecture design and the motivation for addressing scalability in high-dimensional MTS forecasting are well-founded. The use of correlation-based sparsity to reduce computational complexity is a reasonable and well-established approach.

**Medium Confidence**: The ReIndex strategy's effectiveness in improving training diversity and reducing memory usage is plausible but not rigorously validated. The claim that 2-D Transformer captures dependencies missed by separate-stage attention is supported by the overall results but lacks direct ablation evidence.

**Low Confidence**: The assumption that high-dimensional data is inherently sparse and that ReIndex preserves temporal coherence across shuffled series are critical but untested claims that could significantly impact performance.

## Next Checks

1. **Correlation Sparsity Validation**: Conduct experiments on datasets with varying correlation densities to test the robustness of the sparsity assumption. Compare performance when using different K values or alternative similarity measures.

2. **ReIndex Ablation Study**: Compare STHD with and without ReIndex on datasets with different levels of series independence. Measure both memory usage and forecasting accuracy to validate the tradeoff.

3. **2-D Transformer vs. Separate-Stage Attention**: Implement a direct ablation where the 2-D Transformer is replaced with separate temporal and channel attention stages. Compare performance on low-dimensional datasets where the computational overhead is manageable.