---
ver: rpa2
title: Multi-word Tokenization for Sequence Compression
arxiv_id: '2402.09949'
source_url: https://arxiv.org/abs/2402.09949
tags:
- sequence
- length
- distil
- language
- tokenizer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Multi-word tokenization (MWT) reduces LLM sequence length by adding
  frequent n-grams as single tokens, enabling up to 4x shorter inputs with negligible
  performance loss. On 3 text classification datasets, MWT with BERT and DistilBERT
  achieved inference speedups of 1.1-9.4x while maintaining or slightly improving
  macro-F1 scores.
---

# Multi-word Tokenization for Sequence Compression

## Quick Facts
- arXiv ID: 2402.09949
- Source URL: https://arxiv.org/abs/2402.09949
- Authors: Leonidas Gee; Leonardo Rigutini; Marco Ernandes; Andrea Zugarini
- Reference count: 33
- Key outcome: MWT reduces sequence length by adding frequent n-grams as single tokens, achieving up to 4x shorter inputs with negligible performance loss

## Executive Summary
Multi-word tokenization (MWT) extends standard subword tokenizers by adding statistically determined multi-word expressions (n-grams) to the vocabulary, creating more compact sequences. The method achieves significant sequence compression (up to 4x reduction) while maintaining or slightly improving classification performance. MWT demonstrates particular robustness to sequence truncation and compatibility with knowledge distillation, enabling inference speedups of 1.1-9.4x across three text classification datasets.

## Method Summary
The MWT approach enriches tokenizer vocabularies with top-K n-grams (1000, 2500, 5000) selected from training corpora using frequency counting. New n-gram tokens are merged left-to-right after pre-tokenization. Fast Vocabulary Transfer (FVT) initializes embeddings for new tokens by combining existing token embeddings. Models are fine-tuned using Masked-Language Modeling followed by downstream classification with specific learning rates and batch sizes. The method is evaluated on BERT and DistilBERT architectures across three text classification datasets.

## Key Results
- MWT achieved inference speedups of 1.1-9.4x while maintaining or slightly improving macro-F1 scores
- Sequence length reduction up to 4x with negligible performance loss across ADE, LEDGAR, and PATENT datasets
- MWT demonstrated superior robustness to sequence truncation compared to baseline tokenizers
- Compatible with knowledge distillation, compounding efficiency gains when combined with DistilBERT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MWT reduces sequence length by representing frequent n-grams as single tokens with negligible performance loss
- Mechanism: Adding statistically determined multi-word expressions to tokenizer vocabulary produces more compact sequences by encoding frequent n-grams as single tokens
- Core assumption: Frequent n-grams represent meaningful multi-word expressions that can be merged without losing semantic information
- Evidence anchors:
  - [abstract] "MWTs produce a more compact and efficient tokenization that yields two benefits: (1) Increase in performance due to a greater coverage of input data given a fixed sequence length budget; (2) Faster and lighter inference due to the ability to reduce the sequence length with negligible drops in performance."
  - [section 3] "To achieves this, we enrich the vocabulary of the tokenizer with statistically determined multi-word expressions. By encoding the frequent n-grams with single tokens, the sequences produced are both shorter and more informative"
- Break condition: If training corpus lacks frequent meaningful n-grams or selected n-grams don't capture domain-specific terminology

### Mechanism 2
- Claim: MWTs are more robust across shorter sequence lengths, allowing major speedups via early sequence truncation
- Mechanism: More compact tokenization preserves more information per token, maintaining performance better than generic tokenizers when maximum sequence length is reduced
- Core assumption: Information density per token is higher with MWT, so truncating sequences removes proportionally less information
- Evidence anchors:
  - [abstract] "Our results show that MWT is more robust across shorter sequence lengths, thus allowing for major speedups via early sequence truncation."
  - [section 4.3] "From Figure 4, we can see the performance of Tgen dropping more rapidly than MWTs as truncation increases (maximum sequence length decreases). In the extreme 8-times truncation, the performance of Tgen falls dramatically for both ADE and LEDGAR. However, MWTs are shown to be more robust to truncation"
- Break condition: If n-grams selected are not representative of most information-dense phrases in the domain

### Mechanism 3
- Claim: MWT is compatible with knowledge distillation and other compression techniques, compounding efficiency gains
- Mechanism: Vocabulary adaptation and embedding transfer mechanisms used in MWT don't interfere with model distillation processes, amplifying computational savings from distillation
- Core assumption: Embedding initialization and fine-tuning procedures for MWT don't disrupt knowledge transfer from teacher to student models
- Evidence anchors:
  - [section 4.3] "Additionally, we investigate the application of MWTs with tokenizers adapted to the dataset... we utilize a DistilBERT model with MWTs... our MWT is shown to retain most of its performance with a quarter of the sequence length and an inference speedup of ∼x8.8"
  - [section 3] "Fast V ocabulary Transfer. Given that the vocabulary of the tokenizer has changed, the newly added symbols GK must be included into the embedding matrix of the language model as well. To avoid retraining the entire model from scratch... we make use of Fast V ocabulary Transfer (FVT) instead"
- Break condition: If embedding initialization for new n-gram tokens conflicts with distillation process

## Foundational Learning

- Concept: Subword tokenization (BPE, WordPiece)
  - Why needed here: Understanding how standard tokenizers work is essential to grasp why MWT provides compression benefits. The paper builds on existing subword tokenization by extending it with n-grams.
  - Quick check question: How does BPE tokenization differ from word-level tokenization in terms of vocabulary size and out-of-vocabulary handling?

- Concept: Sequence length and computational complexity
  - Why needed here: The relationship between sequence length and inference speed is central to understanding the paper's contributions. Longer sequences require more memory and computation.
  - Quick check question: What is the computational complexity of self-attention with respect to sequence length, and how does this relate to the speedups claimed?

- Concept: Knowledge distillation and model compression
  - Why needed here: The paper demonstrates compatibility with distillation, so understanding how distillation works is important for appreciating the compounding effects.
  - Quick check question: What is the primary mechanism by which knowledge distillation reduces model size while maintaining performance?

## Architecture Onboarding

- Component map: Tokenizer (BERT/DistilBERT + n-gram vocabulary extension) -> N-gram selector (frequency counter) -> Embedding updater (Fast Vocabulary Transfer) -> Model (fine-tuned language model) -> Evaluation (macro-F1 score and inference speedup)

- Critical path: Training corpus → N-gram frequency counting → Top-K selection → Vocabulary extension → Fast Vocabulary Transfer → Model fine-tuning → Performance evaluation

- Design tradeoffs:
  - N-gram length (N): Setting N=2 (bigrams) provides good generalization; larger N may overfit to specific training data
  - K value: More n-grams provide greater compression but risk including less meaningful combinations
  - Vocabulary size: Must balance between compression benefits and maintaining semantic distinctions

- Failure signatures:
  - Performance degradation without corresponding speedup increase suggests poor n-gram selection
  - Model instability during fine-tuning may indicate embedding initialization issues
  - Unexpected OOV tokens suggest n-grams are not covering domain-specific terminology adequately

- First 3 experiments:
  1. Implement basic n-gram frequency counter on training corpus and verify top-K selection matches expected patterns
  2. Extend tokenizer vocabulary with top-1000 bigrams and measure sequence length reduction on sample texts
  3. Apply Fast Vocabulary Transfer to initialize embeddings for new tokens and validate they are reasonable combinations of existing token embeddings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MWTs vary with different values of N (the maximum number of words in an n-gram)?
- Basis in paper: [explicit] The paper mentions that N is set to 2 (bigrams only) in the experiments, but does not explore other values of N.
- Why unresolved: The paper does not provide empirical results for different values of N, so it is unclear how the performance of MWTs would change with larger or smaller n-grams.
- What evidence would resolve it: Conducting experiments with different values of N (e.g., N=3, N=4) and comparing the performance of MWTs with the baseline Tgen tokenizer.

### Open Question 2
- Question: How does the performance of MWTs compare to other sequence compression techniques, such as Mu et al. (2023) or Petrov et al. (2023)?
- Basis in paper: [inferred] The paper mentions these works in the related works section, but does not compare the performance of MWTs to them.
- Why unresolved: The paper only compares the performance of MWTs to the baseline Tgen tokenizer, so it is unclear how MWTs would fare against other sequence compression techniques.
- What evidence would resolve it: Conducting experiments comparing the performance of MWTs to other sequence compression techniques on the same datasets.

### Open Question 3
- Question: How does the performance of MWTs vary with different domains or languages?
- Basis in paper: [inferred] The paper only evaluates MWTs on three text classification datasets from different domains (medical, legal, and tech), but does not explore other domains or languages.
- Why unresolved: The paper does not provide empirical results for other domains or languages, so it is unclear how MWTs would perform in different contexts.
- What evidence would resolve it: Conducting experiments with MWTs on datasets from different domains or languages and comparing the performance to the baseline Tgen tokenizer.

## Limitations

- Generalizability across domains and languages remains untested beyond three specialized corpora
- Performance of n-grams longer than bigrams (N>2) is not explored
- Computational overhead of n-gram selection and vocabulary extension is not quantified

## Confidence

**High Confidence**: The core mechanism of sequence length reduction through n-gram tokenization is well-established and demonstrated with clear quantitative results. The inference speedups (1.1-9.4x) are directly measurable and reproducible.

**Medium Confidence**: The claim of "negligible performance loss" is somewhat subjective, as the paper reports maintaining or slightly improving macro-F1 scores. The robustness to truncation, while demonstrated, relies on limited comparison with only one baseline tokenizer.

**Low Confidence**: The scalability of MWT to extremely large vocabularies or very long n-grams (beyond bigrams) is not explored. The computational overhead of the n-gram selection process and vocabulary extension is not quantified.

## Next Checks

1. **Cross-domain robustness test**: Apply MWT to a general-purpose text corpus (e.g., Wikipedia or BookCorpus) and measure both sequence compression and performance degradation across multiple classification tasks to assess generalizability beyond specialized domains.

2. **Semantic coherence validation**: Implement an evaluation framework that measures whether the merged n-grams preserve semantic relationships by testing word analogy tasks and sentence similarity before and after MWT application, to verify that compression doesn't come at the cost of semantic integrity.

3. **Computational overhead measurement**: Quantify the time and memory costs of the n-gram frequency counting and vocabulary extension process, and compare the total computational budget (preprocessing + training + inference) against the baseline to determine the true net efficiency gains.