---
ver: rpa2
title: 'TS-RSR: A provably efficient approach for batch Bayesian Optimization'
arxiv_id: '2403.04764'
source_url: https://arxiv.org/abs/2403.04764
tags:
- batch
- algorithm
- where
- optimization
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents TS-RSR, a batch Bayesian optimization algorithm
  that coordinates batch samples by minimizing a Thompson Sampling approximation of
  a regret-to-uncertainty ratio. The method balances exploitation of high-predictive-mean
  points with exploration of high-uncertainty points, while penalizing redundancy
  between batch members.
---

# TS-RSR: A provably efficient approach for batch Bayesian Optimization

## Quick Facts
- arXiv ID: 2403.04764
- Source URL: https://arxiv.org/abs/2403.04764
- Reference count: 13
- Achieves order-of-magnitude improvement over state-of-the-art batch BO algorithms

## Executive Summary
This paper introduces TS-RSR, a batch Bayesian optimization algorithm that coordinates batch samples by minimizing a Thompson Sampling approximation of a regret-to-uncertainty ratio. The method balances exploitation of high-predictive-mean points with exploration of high-uncertainty points while penalizing redundancy between batch members. TS-RSR provides high-probability regret bounds scaling as O(ρₘ√(Tm log(Tm/δ) + log D)) and achieves state-of-the-art performance on synthetic and realistic test functions, outperforming benchmark batch BO algorithms by an order of magnitude on average. The method is parameter-free, unlike UCB-type approaches, and is effective for both moderate and large batch sizes.

## Method Summary
TS-RSR is a batch Bayesian optimization algorithm that addresses the challenge of selecting diverse, informative points in each batch. The core innovation is the Regret-to-Sigma Ratio (RSR) objective, which coordinates batch selection by minimizing a Thompson Sampling approximation of regret-to-uncertainty ratio. At each iteration, the algorithm samples m i.i.d. functions from the current GP posterior and selects batch points by optimizing the RSR objective conditioned on previously selected points. The method provides high-probability regret bounds and achieves parameter-free balance between exploration and exploitation without requiring careful tuning of confidence intervals.

## Key Results
- Outperforms benchmark batch BO algorithms (BUCB, TS, UCBPE, SP, EI) by an average order of magnitude on Ackley, Bird, and Rosenbrock test functions
- Provides rigorous regret bounds scaling as O(ρₘ√(Tm log(Tm/δ) + log D)) with probability at least 1-δ
- Demonstrates effectiveness for both moderate (m=5) and large batch sizes without parameter tuning
- Achieves parameter-free balance between exploration and exploitation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Regret-to-Sigma Ratio (RSR) objective coordinates batch samples by minimizing redundancy while balancing exploitation and exploration.
- Mechanism: At each iteration, the algorithm selects the next batch point by optimizing a Thompson Sampling approximation of regret-to-uncertainty ratio. The denominator (predictive standard deviation) penalizes selecting points that are similar to previously chosen batch members, while the numerator encourages selecting points with high predictive means.
- Core assumption: The Gaussian Process posterior variance decreases when conditioning on additional samples, creating natural diversity enforcement.
- Evidence anchors:
  - [abstract]: "Our objective is able to coordinate the actions chosen in each batch in a way that minimizes redundancy between points whilst focusing on points with high predictive means or high uncertainty."
  - [section 4]: The algorithm explicitly uses σt(x | {xTS-RSR t+1,j}i-1 j=1) in the denominator to condition on previously selected batch points.
  - [corpus]: No direct corpus evidence found for this specific mechanism.
- Break condition: If the GP kernel is highly correlated (e.g., extremely smooth), the variance penalty may become too weak to enforce meaningful diversity.

### Mechanism 2
- Claim: The algorithm achieves parameter-free balance between exploration and exploitation without requiring careful tuning of confidence intervals.
- Mechanism: Unlike UCB-type methods that require tuning βt parameter, TS-RSR uses a ratio of regret to uncertainty that automatically adjusts the exploration-exploitation trade-off based on the current posterior.
- Core assumption: The Thompson Sampling approximation provides a valid estimate of the regret that naturally scales with uncertainty.
- Evidence anchors:
  - [abstract]: "Our algorithm is able to coordinate the actions chosen in each batch in an intelligent way that minimizes redundancy between points whilst focusing on points with high predictive means or high uncertainty."
  - [section 3]: "The downside of TS-based methods is the lack of penalization for duplicating actions in a batch" - this is addressed by the RSR formulation.
  - [corpus]: Weak evidence - only 25 related papers found, none specifically discussing parameter-free batch BO.
- Break condition: In extremely high-noise settings, the Thompson Sampling approximation may become unreliable, potentially requiring parameter tuning.

### Mechanism 3
- Claim: The algorithm provides high-probability regret bounds that scale as O(ρₘ√(Tm log(Tm/δ) + log D)).
- Mechanism: The theoretical analysis combines martingale concentration inequalities with bounds on the information gain of the GP, showing that the cumulative regret grows sublinearly with the number of iterations.
- Core assumption: The GP kernel satisfies k(x,x) ≤ 1 for all x, enabling the information-theoretic bounds.
- Evidence anchors:
  - [abstract]: "Theoretically, we provide rigorous convergence guarantees on our algorithm's regret"
  - [section 5.2]: "Theorem 1... with probability at least 1 - δ, we have RT,m = O(ρm√(TmγTm)√(log(Tm/δ) + log D))"
  - [corpus]: No direct corpus evidence for this specific bound structure.
- Break condition: For kernels with unbounded information gain (e.g., certain non-stationary kernels), the regret bound may not hold.

## Foundational Learning

- Concept: Gaussian Process regression and posterior updating
  - Why needed here: The algorithm relies on GP posterior mean and variance to make decisions
  - Quick check question: How does the posterior variance σ²t(x) change when we condition on new observations?

- Concept: Information gain and submodularity
  - Why needed here: The regret bounds depend on the information gain γTm, which measures how much uncertainty is reduced by observing Tm points
  - Quick check question: Why does information gain typically scale logarithmically with the number of observations for smooth kernels?

- Concept: Martingale concentration inequalities
  - Why needed here: The proof uses martingale bounds to handle the stochasticity in Thompson Sampling and noise in observations
  - Quick check question: What is the difference between Azuma-Hoeffding and Hoeffding inequalities in the context of dependent random variables?

## Architecture Onboarding

- Component map:
  - GP prior (mean=0, kernel=k) → Posterior updates → Thompson Sampling draws → RSR optimization → Batch selection → Observation collection → Repeat
  - Key modules: GP inference engine, Thompson Sampling sampler, RSR optimizer, batch coordinator

- Critical path:
  1. Sample m i.i.d. functions from current GP posterior
  2. For each of m batch points, optimize RSR objective conditioned on previously selected points
  3. Execute batch, collect observations
  4. Update GP posterior with new data

- Design tradeoffs:
  - Batch size m vs. computational complexity: Larger m increases coordination complexity quadratically
  - Kernel choice vs. regret bounds: Smooth kernels give better theoretical guarantees but may require more samples
  - Discrete vs. continuous search space: Discrete allows exact optimization, continuous requires approximation

- Failure signatures:
  - Poor performance with highly correlated kernels (insufficient diversity)
  - Sensitivity to initialization (could get stuck in local optima)
  - Computational bottleneck in RSR optimization for large batch sizes

- First 3 experiments:
  1. Run on Ackley function with m=5, compare against BUCB and TS baselines, verify improvement order of magnitude
  2. Test sensitivity to batch size by varying m from 1 to 20 on Rosenbrock function
  3. Evaluate performance with different kernels (SE vs Matern) on Bird function to assess kernel impact

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions in the traditional sense, but several unresolved issues emerge from the analysis:

### Open Question 1
- Question: How does the algorithm's performance scale with higher-dimensional input spaces (d > 2)?
- Basis in paper: [inferred] The paper focuses on 2D test functions but mentions that the analysis focuses on the case when X is a discrete set, with the size D depending exponentially on the state dimension d.
- Why unresolved: The experimental results only demonstrate performance on 2D functions (Ackley, Bird, and Rosenbrock), leaving the scalability to higher dimensions unexplored.
- What evidence would resolve it: Experimental results on test functions with d ≥ 5, showing how regret scales with dimensionality and comparing performance to existing methods.

### Open Question 2
- Question: What is the computational complexity of TS-RSR for large batch sizes (m > 100) compared to the DPP-based method in [Nava et al., 2022]?
- Basis in paper: [explicit] The paper mentions that DPP-based methods have exponential complexity with batch size, but claims TS-RSR works for any setting of m. However, it doesn't provide computational complexity analysis for large m.
- Why unresolved: The paper states TS-RSR is appropriate for any batch size but doesn't quantify the computational overhead for very large batch sizes or compare it to alternative scalable methods.
- What evidence would resolve it: Empirical runtime measurements and computational complexity analysis for batch sizes ranging from m=10 to m=1000, comparing TS-RSR to DPP-based methods.

### Open Question 3
- Question: How robust is TS-RSR to model misspecification, such as using an incorrect kernel or non-Gaussian observation noise?
- Basis in paper: [inferred] The theoretical analysis assumes a GP model with Gaussian noise, and the experiments use a Matern kernel. The paper doesn't explore robustness to model assumptions.
- Why unresolved: The algorithm and theoretical guarantees are derived under strong assumptions (GP prior, Gaussian noise), but real-world functions may not satisfy these assumptions.
- What evidence would resolve it: Experiments with non-Gaussian noise (e.g., Student's t-distribution) and misspecified kernels (e.g., using squared exponential when the true function follows a Matern kernel), measuring performance degradation.

## Limitations

- Computational complexity increases quadratically with batch size due to coordination requirements
- Theoretical regret bounds depend on information gain γT, which can grow rapidly for certain kernel classes
- Assumes Gaussian Process prior and Gaussian observation noise, limiting applicability to functions that deviate from these assumptions

## Confidence

- Empirical performance improvements: High - Order of magnitude better than baselines demonstrated across multiple test functions
- Theoretical regret bounds: Medium - Rigorous analysis but dependent on kernel assumptions and information gain bounds
- Parameter-free nature: High - Clearly demonstrated through comparison with UCB-type methods that require tuning

## Next Checks

1. Test TS-RSR on high-dimensional problems (d > 10) to assess scalability and identify computational bottlenecks
2. Evaluate the algorithm with different kernel classes, particularly non-stationary kernels, to understand the limitations of the regret bounds
3. Implement a parallel version of the RSR optimization to handle larger batch sizes more efficiently and measure the impact on overall wall-clock time