---
ver: rpa2
title: Where is the answer? Investigating Positional Bias in Language Model Knowledge
  Extraction
arxiv_id: '2402.12170'
source_url: https://arxiv.org/abs/2402.12170
tags:
- answer
- training
- documents
- sentence
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates a positional bias issue in large language
  models during knowledge extraction from fine-tuned documents. It finds that models
  perform well on questions about early content in training documents but poorly on
  information from middle or end sections, despite minimizing document perplexity.
---

# Where is the answer? Investigating Positional Bias in Language Model Knowledge Extraction

## Quick Facts
- arXiv ID: 2402.12170
- Source URL: https://arxiv.org/abs/2402.12170
- Reference count: 40
- Large language models exhibit significant positional bias when extracting knowledge from fine-tuned documents, performing well on early content but poorly on middle or end sections despite minimizing document perplexity.

## Executive Summary
This paper investigates a critical limitation in large language models during knowledge extraction from fine-tuned documents: positional bias. The authors find that while models can achieve good perplexity scores across all document positions, their ability to extract factual knowledge varies dramatically based on where information appears in the training text. Early content is easily recalled through questions, but information from middle or end sections remains largely inaccessible. Through extensive experiments on synthetic and real datasets, the paper demonstrates that simple regularization techniques—particularly denoising auto-regressive training and attention dropout—can significantly improve knowledge extraction performance across all document positions. These findings provide practical solutions for adapting LLMs to new domains and highlight the importance of training methodology in knowledge memorization quality.

## Method Summary
The authors fine-tune pre-trained Llama-2 models (7B, 13B, 70B parameters) using standard auto-regressive training objectives while evaluating knowledge extraction performance across different document positions. They create position-annotated QA datasets by splitting training documents into sentences and generating questions that can be answered by individual sentences. The key experimental setup involves training with mixed sampling of QA and document data, then evaluating exact match (EM) and F1 scores while tracking which sentence position in the original document contained the answer. The proposed solutions include denoising auto-regressive training (randomly replacing 20% of input tokens during training), sentence shuffling, and attention dropout (randomly dropping attention connections). Training uses Adam optimizer with learning rate 1e-5, linear decay, and 3000 steps.

## Key Results
- Standard auto-regressive models show dramatic performance drop from position 1 (early content) to positions 2+ (middle/end content), with EM scores dropping from ~75% to ~40% on average across datasets
- Denoising auto-regressive training (D-AR) increases EM from 40.9% to 60.1% and F1 from 54.0% to 73.7% on Wiki2023+ dataset across all positions
- Attention dropout with 0.1 ratio consistently improves performance, especially for positions beyond the first sentence
- Combining all three techniques (D-AR + Shuffle + Attention Dropout) provides the best overall performance, though with diminishing returns compared to D-AR alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Auto-regressive training creates positional bias because the model learns to predict tokens based on specific preceding token sequences rather than extracting factual knowledge from diverse prompts.
- Mechanism: The auto-regressive training objective forces the model to memorize token sequences by relying heavily on previous tokens. This creates spurious correlations between specific token patterns and next-token predictions, making it difficult to extract the same information when queried with different phrasing.
- Core assumption: The model memorizes documents in a hard-to-extract manner where information retrieval depends on seeing the exact same token sequence used during training.
- Evidence anchors: [abstract]: "each token is prompted by reliance on all previous tokens, which hinders the model from recalling information from training documents by question prompts"; [section]: "perplexity increases as putting the s1 latter for all models. This trend is the most notable for the AR model, implying that the AR model relies much on the previous sentences to remember each token"
- Break condition: If the model could learn to extract factual knowledge independently of specific token sequences, this mechanism would fail.

### Mechanism 2
- Claim: Denoising auto-regressive training improves knowledge extraction by forcing the model to predict tokens from diverse and corrupted input conditions.
- Mechanism: By randomly replacing input tokens with random ones during training, the model learns to reconstruct correct tokens from various perturbed contexts. This breaks the reliance on specific token sequences and encourages more robust knowledge memorization that can be extracted through different prompts.
- Core assumption: The performance gain comes from the model learning to predict tokens given randomly perturbed observations rather than from denoising the noise-added tokens.
- Evidence anchors: [abstract]: "denoising auto-regressive loss can enhance the information extraction from diverse positions"; [section]: "We study if the improvement comes from denoising the noise-added tokens or predicting next-tokens given randomly perturbed observations... ablating the denoising role... still surpassing the AR model by a large margin"
- Break condition: If adding noise to input sequences didn't improve the model's ability to extract information from diverse prompts, this mechanism would fail.

### Mechanism 3
- Claim: Attention dropout reduces positional bias by forcing the model to reduce excessive reliance on previous tokens during self-attention computation.
- Mechanism: By randomly dropping attention connections, the model cannot depend heavily on all previous tokens to predict the next token. This encourages more distributed attention patterns and reduces the spurious correlations between specific token sequences and knowledge extraction.
- Core assumption: Randomly dropping attention connections during training will force the model to learn more robust patterns for knowledge extraction.
- Evidence anchors: [abstract]: "regularization such as denoising auto-regressive loss can enhance the information extraction from diverse positions" (includes attention dropout as a regularization technique); [section]: "we apply attention dropout, i.e., randomly dropping the attention mask, which should force the model to reduce the excessive reliance on previous tokens"
- Break condition: If attention dropout didn't reduce the model's reliance on previous tokens, this mechanism would fail.

## Foundational Learning

- Concept: Auto-regressive language modeling
  - Why needed here: Understanding how auto-regressive training creates positional bias is fundamental to grasping the paper's core problem and solutions
  - Quick check question: Why does auto-regressive training make it difficult for models to extract information when queried with different phrasing than what was used during training?

- Concept: Perplexity as a measurement
  - Why needed here: The paper uses perplexity to analyze why the AR model struggles with information retrieval beyond the first sentence
  - Quick check question: Why does the paper conclude that perplexity measured on training documents doesn't reflect knowledge extraction performance on diverse positions?

- Concept: Knowledge extraction vs. token prediction
  - Why needed here: The paper distinguishes between memorizing documents well versus memorizing in an extractable manner, which is crucial for understanding the positional bias problem
  - Quick check question: What is the difference between a model that memorizes documents well and one that memorizes in an extractable manner?

## Architecture Onboarding

- Component map: Pre-trained LLM -> Training data generation (with optional denoising) -> Fine-tuning with auto-regressive objective (with optional attention dropout) -> Evaluation on position-annotated QA data -> Analysis of positional bias performance

- Critical path: Pre-trained LLM → Training data generation (with optional denoising) → Fine-tuning with auto-regressive objective (with optional attention dropout) → Evaluation on position-annotated QA data → Analysis of positional bias performance

- Design tradeoffs: Auto-regressive training provides strong next-token prediction but creates positional bias. Denoising auto-regressive training adds noise to break spurious correlations but may require more training iterations. Attention dropout reduces reliance on previous tokens but may hurt performance if applied too aggressively.

- Failure signatures: High perplexity on training documents but poor QA performance indicates hard-to-extract memorization. Significant performance drop when answer position moves from beginning to middle/end of documents indicates positional bias. Poor performance with longer answers suggests the model struggles with information beyond simple facts.

- First 3 experiments:
  1. Replicate the positional bias experiment by modulating answer sentence position in training documents and measuring QA accuracy
  2. Compare standard auto-regressive training vs. denoising auto-regressive training on Wiki2023+ dataset
  3. Test attention dropout with different dropout ratios on the synthetic biography dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the denoising auto-regressive (D-AR) training objective primarily improve performance through denoising corrupted tokens or through learning to predict next tokens given noisy input?
- Basis in paper: [explicit] The authors investigate this by turning off loss computation on replaced positions and find that performance still improves significantly, though not as much as with full denoising.
- Why unresolved: While the ablation shows denoising contributes, it's unclear whether the primary benefit comes from denoising or from the regularization effect of noisy inputs. The authors note both factors likely play a role.
- What evidence would resolve it: A controlled experiment comparing D-AR with only noise addition (no denoising loss) versus only denoising (no noise addition) versus the full D-AR would clarify the relative contributions.

### Open Question 2
- Question: How does the positional bias phenomenon manifest in more complex knowledge acquisition scenarios beyond simple factual QA?
- Basis in paper: [inferred] The authors acknowledge their focus on simple factual knowledge and note the need for advanced datasets to study more complex knowledge scenarios.
- Why unresolved: The current study uses relatively simple question-answering tasks with short, clear answers. Real-world knowledge extraction often involves longer, more complex reasoning chains and multi-hop inference.
- What evidence would resolve it: Developing and testing on datasets that require complex reasoning, longer context understanding, and multi-sentence answer extraction would reveal whether positional bias persists in more challenging scenarios.

### Open Question 3
- Question: What is the optimal combination of regularization techniques to maximize knowledge extraction performance while minimizing computational overhead?
- Basis in paper: [explicit] The authors test various combinations of D-AR, Shuffle, and Attention Dropout, finding that combining all three provides the best results, but they don't explore the full parameter space or computational trade-offs.
- Why unresolved: While the authors show that combining techniques improves performance, they don't systematically explore the optimal balance between regularization strength and computational cost, nor do they test other potential regularization methods.
- What evidence would resolve it: A comprehensive ablation study varying regularization strengths, testing additional techniques (e.g., different dropout types, curriculum learning), and measuring both performance and training time would identify the most efficient combination.

## Limitations

- The study focuses on relatively simple factual QA tasks and may not generalize to more complex knowledge extraction scenarios involving multi-hop reasoning or longer context understanding.
- The analysis is limited to three specific regularization techniques without exploring the full space of potential solutions or systematically optimizing hyperparameters for each method.
- The evaluation framework relies on synthetic datasets (bioS) alongside real Wikipedia data, which may not fully capture the complexity and diversity of real-world document structures.

## Confidence

**High confidence**: The existence of positional bias in LLMs during knowledge extraction from fine-tuned documents is well-supported by experimental evidence across multiple datasets and model sizes. The observation that models perform well on early content but poorly on middle/end sections despite good perplexity scores is robust and reproducible.

**Medium confidence**: The claim that denoising auto-regressive training and attention dropout effectively reduce positional bias is supported by empirical results, but the exact mechanisms and optimal hyperparameters require further investigation. The assertion that performance gains come from predicting next-tokens given randomly perturbed observations rather than from denoising the noise-added tokens is plausible but could benefit from more rigorous ablation studies.

**Low confidence**: The broader claim that these findings "offer new insights for adapting LLMs to new domains" extrapolates beyond the scope of the experiments, which focused specifically on QA-style knowledge extraction from Wikipedia-style documents. The generalizability to other domains and task types remains unproven.

## Next Checks

1. **Cross-domain generalization test**: Evaluate the proposed techniques on diverse document types (e.g., legal documents, scientific papers, news articles) to assess whether positional bias reduction generalizes beyond Wikipedia-style content and whether different domains exhibit varying levels of positional bias susceptibility.

2. **Mechanism ablation study**: Design controlled experiments to isolate whether the benefits of denoising auto-regressive training come from breaking spurious correlations, improving robustness to input perturbations, or both. This could involve comparing different noise injection strategies and measuring their effects on both perplexity and QA performance.

3. **Long-term retention analysis**: Investigate whether the reduced positional bias persists after longer training periods or with larger model sizes. Specifically, test whether attention dropout's benefits scale with model capacity and whether denoising auto-regressive training's effectiveness changes with extended training beyond the 3000-step baseline.