---
ver: rpa2
title: The Potential of AutoML for Recommender Systems
arxiv_id: '2402.04453'
source_url: https://arxiv.org/abs/2402.04453
tags:
- recsys
- automl
- algorithms
- datasets
- lenskit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the potential of Automated Machine Learning
  (AutoML) for Recommender Systems (RecSys). The authors compare 60 AutoML, AutoRecSys,
  ML, and RecSys algorithms on 14 explicit feedback RecSys datasets, using default
  hyperparameters to simulate an inexperienced user's perspective.
---

# The Potential of AutoML for Recommender Systems

## Quick Facts
- arXiv ID: 2402.04453
- Source URL: https://arxiv.org/abs/2402.04453
- Authors: Tobias Vente; Joeran Beel
- Reference count: 40
- Primary result: AutoML libraries outperform RecSys libraries on average when using default hyperparameters

## Executive Summary
This study evaluates the potential of Automated Machine Learning (AutoML) for recommender systems by comparing 60 algorithms from 15 libraries on 14 explicit feedback datasets. The authors simulate an inexperienced user's perspective by using default hyperparameters and strict resource limits. Results show that AutoML and AutoRecSys libraries perform best, with AutoML libraries outperforming RecSys libraries on average. The study highlights the need for more sophisticated AutoRecSys libraries while demonstrating AutoML's strong potential for recommender system applications.

## Method Summary
The study evaluates 60 algorithms from 15 libraries (7 AutoML, 28 ML, 23 RecSys, 1 AutoRecSys, 1 baseline) on 14 explicit feedback RecSys datasets. All algorithms run with default hyperparameters using a hold-out validation approach (25% test split) with RMSE as the evaluation metric. Resource constraints are set at 4 hours 30 minutes runtime and 528GB memory per algorithm. The evaluation tool is implemented in Python with a Docker-based approach, executed on an AMD Ryzen Threadripper PRO 3975WX workstation with 32 cores and 528GB RAM.

## Key Results
- AutoML libraries performed best for 6 of 14 datasets (43%), with no single AutoML library consistently achieving top performance
- Auto-Surprise, an AutoRecSys library using Tree-structured Parzen Estimator optimization, achieved the best overall performance
- RecSys algorithms dominated the top-5 rankings but fell behind AutoML libraries on average across all datasets

## Why This Works (Mechanism)

### Mechanism 1
AutoML libraries achieve competitive performance on RecSys tasks even with default hyperparameters because they perform automated algorithm selection and hyperparameter tuning internally, compensating for the lack of user expertise in tuning individual RecSys algorithms. The default hyperparameter configurations in AutoML libraries are sufficiently diverse and well-tuned to handle a range of RecSys datasets without manual intervention.

### Mechanism 2
RecSys algorithms fall behind AutoML on average despite often achieving top-5 placements because they are optimized for specific task types and may underperform when their default hyperparameters are suboptimal for a given dataset, whereas AutoML explores multiple algorithms and hyperparameter combinations.

### Mechanism 3
Automated libraries reduce the risk of underperforming compared to a naive baseline by systematically evaluating multiple models and hyperparameter configurations, avoiding the pitfalls of selecting poorly performing algorithms or settings that an inexperienced user might choose.

## Foundational Learning

- Concept: Explicit feedback vs implicit feedback in RecSys
  - Why needed here: The study focuses on rating prediction tasks using explicit feedback; understanding the difference is crucial for correctly framing the problem and choosing appropriate algorithms.
  - Quick check question: What distinguishes explicit feedback from implicit feedback in recommender systems?

- Concept: Root Mean Squared Error (RMSE) as an evaluation metric
  - Why needed here: RMSE is used to compare predictive performance across algorithms and datasets; knowing how it is computed and interpreted is essential for understanding the results.
  - Quick check question: How does RMSE penalize prediction errors compared to Mean Absolute Error (MAE)?

- Concept: Hyperparameter tuning and its impact on model performance
  - Why needed here: The study simulates an inexperienced user by using default hyperparameters; understanding the role of hyperparameters explains why AutoML can outperform manually tuned RecSys algorithms.
  - Quick check question: Why might default hyperparameters lead to suboptimal performance in complex models?

## Architecture Onboarding

- Component map: Data preprocessing pipeline -> Algorithm wrapper -> Evaluation engine -> Resource manager -> Result aggregator

- Critical path: 1. Preprocess dataset → 2. Load algorithm with defaults → 3. Train/test with 25% hold-out → 4. Enforce memory/time limits → 5. Record RMSE → 6. Rank and compare

- Design tradeoffs: Using default hyperparameters simplifies the user scenario but may underutilize algorithm potential; strict resource limits ensure reproducibility but may cut short promising runs; excluding NLP preprocessing keeps data close to original but limits feature richness for ML algorithms

- Failure signatures: Memory limit exceeded: algorithm terminated early, ranked last; Time limit exceeded: algorithm did not complete, ranked last; Poor RMSE > baseline: algorithm underperforms naive mean predictor

- First 3 experiments: 1. Run Auto-Surprise on MovieLens 100k with default settings; verify RMSE ranking and memory usage; 2. Execute XGBoostRegressor on Amazon Electronics; confirm resource limit handling and error computation; 3. Test FLAML on Yelp dataset; observe failure patterns if any and compare performance against RecSys algorithms

## Open Questions the Paper Calls Out

### Open Question 1
What specific characteristics of datasets with many additional features (like Yelp or MovieLens-latest-small) make ML approaches dominate over RecSys algorithms? The paper notes that datasets with many additional features are "mostly dominated by ML approaches except for movielens-1M and movielens-100k" and mentions that these popular datasets are "tuned to work best on these datasets by default," but does not analyze the specific reasons why additional features favor ML approaches or why the two MovieLens datasets are exceptions.

### Open Question 2
How do different hyperparameter optimization strategies within AutoML frameworks affect their performance on RecSys tasks? The paper uses default hyperparameters for all algorithms to simulate an inexperienced user, but notes that AutoML libraries perform best overall and Auto-Surprise (which uses TPE optimization) is the single-best library, without exploring how different AutoML optimization strategies impact RecSys performance.

### Open Question 3
What is the impact of dataset size and sparsity on the relative performance of AutoML, AutoRecSys, ML, and RecSys algorithms? The paper shows that AutoML and AutoRecSys perform well across various datasets but does not explicitly analyze how dataset characteristics like size and sparsity affect performance differences between categories.

## Limitations
- Default hyperparameter usage limits generalizability; real-world performance may differ with expert tuning
- Resource constraints (4h 30m, 528GB) may have prevented full algorithm exploration, potentially biasing results
- Limited dataset diversity (primarily explicit feedback) restricts applicability to implicit feedback scenarios

## Confidence
- High confidence: AutoML libraries outperform RecSys libraries on average with default settings
- Medium confidence: Auto-Surprise achieves best overall performance; results depend on specific dataset characteristics
- Low confidence: Generalization to implicit feedback, cold-start scenarios, and real-time recommendation systems

## Next Checks
1. Re-run experiments with extended resource limits (8h runtime, 1TB memory) to verify if results stabilize or change
2. Evaluate the same algorithms on implicit feedback datasets to assess cross-domain performance
3. Compare AutoML performance when allowed to use dataset-specific preprocessing versus fixed defaults