---
ver: rpa2
title: 'MOSAIC: Multimodal Multistakeholder-aware Visual Art Recommendation'
arxiv_id: '2407.21758'
source_url: https://arxiv.org/abs/2407.21758
tags:
- user
- paintings
- recsys
- recommendations
- clip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MOSAIC, a novel multimodal multistakeholder-aware
  approach for visual art recommendation. MOSAIC leverages state-of-the-art CLIP and
  BLIP architectures to learn joint embeddings from textual descriptions and images
  of paintings.
---

# MOSAIC: Multimodal Multistakeholder-aware Visual Art Recommendation

## Quick Facts
- arXiv ID: 2407.21758
- Source URL: https://arxiv.org/abs/2407.21758
- Reference count: 40
- Primary result: Novel multimodal visual art recommendation system using BLIP architecture that optimizes for user preferences, popularity, and curatorial representativeness

## Executive Summary
This paper introduces MOSAIC, a novel multimodal multistakeholder-aware approach for visual art recommendation that leverages state-of-the-art CLIP and BLIP architectures to learn joint embeddings from textual descriptions and images of paintings. The system optimizes for multiple stakeholders by jointly considering user preferences, popularity, and representative selection across curated story groups. The approach addresses a gap in current museum recommendation systems by incorporating popularity signals alongside personalized recommendations.

## Method Summary
MOSAIC employs a two-stage multimodal learning framework that combines visual and textual representations of artworks. The system first extracts features using either CLIP or BLIP architectures, then jointly optimizes for user preferences, artwork popularity, and representativeness within curated story groups. The recommendation engine uses a weighted combination of these factors to generate personalized artwork sequences. An offline preference elicitation study was conducted with 213 users to train the model, followed by a user study with 100 crowdworkers to validate the recommendations.

## Key Results
- BLIP-based engines outperformed CLIP-based ones, with BLIP-M being the top-performer
- Popularity had a strong positive effect on user satisfaction and perceived quality
- Multimodal representation learning proved crucial for effective visual art recommendation
- The multistakeholder approach successfully balanced personalization with curatorial considerations

## Why This Works (Mechanism)
The success of MOSAIC stems from its ability to learn rich multimodal representations that capture both visual and semantic aspects of artworks. By incorporating popularity signals alongside personalized preferences, the system aligns with real-world museum visitor behavior patterns where crowd favorites often guide exploration. The joint optimization framework ensures that recommendations satisfy multiple stakeholders - users receive personalized suggestions while curators maintain control over the narrative and representativeness of the exhibition experience.

## Foundational Learning

**Multimodal Learning**: Learning joint representations from multiple data modalities (text, image)
- Why needed: Artworks have both visual features and textual descriptions that together provide richer semantic information
- Quick check: Verify that joint embeddings capture cross-modal relationships through retrieval tasks

**Recommendation Systems**: Algorithms that predict user preferences and suggest relevant items
- Why needed: Core functionality for personalized art discovery and exhibition planning
- Quick check: Evaluate recommendation accuracy through user preference prediction

**Popularity-based Recommender Systems**: Systems that incorporate item popularity into recommendation algorithms
- Why needed: Addresses the cold-start problem and aligns with typical visitor behavior patterns
- Quick check: Measure diversity and coverage of recommended items

## Architecture Onboarding

**Component Map**: CLIP/BLIP Feature Extractor -> Multistakeholder Optimizer -> Story Group Generator -> User Interface

**Critical Path**: Feature extraction → Joint optimization → Story generation → User interaction

**Design Tradeoffs**: 
- Chose BLIP over CLIP for better multimodal performance despite CLIP's larger pre-training dataset
- Prioritized multistakeholder optimization over pure personalization to support curatorial goals
- Selected pairwise preference elicitation over rating scales for more reliable user feedback

**Failure Signatures**: 
- Poor visual-textual alignment in embeddings leads to irrelevant recommendations
- Over-optimization for popularity results in homogenized recommendations
- Insufficient representativeness causes biased story group selections

**First Experiments**:
1. Validate multimodal feature quality through image-text retrieval tasks
2. Test popularity-weighting effects on recommendation diversity
3. Evaluate story group representativeness across different optimization weights

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on offline studies rather than real-world museum deployments
- Focus on paintings limits generalizability to other art forms
- BLIP's superior performance lacks theoretical justification

## Confidence

**High confidence**: Multimodal learning framework using CLIP and BLIP architectures is technically sound
**Medium confidence**: Effectiveness of popularity-based weighting in improving user satisfaction is context-dependent
**Medium confidence**: BLIP-M's superior performance is supported by experimental results but needs further validation

## Next Checks

1. Conduct A/B testing in real museum settings to validate whether popularity-weighted recommendations translate to actual visitor engagement and satisfaction
2. Test the framework's generalizability across different art forms (sculpture, photography, digital art) beyond paintings
3. Investigate the theoretical reasons behind BLIP's superior performance compared to CLIP through ablation studies and feature analysis