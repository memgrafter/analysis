---
ver: rpa2
title: Self-Data Distillation for Recovering Quality in Pruned Large Language Models
arxiv_id: '2410.09982'
source_url: https://arxiv.org/abs/2410.09982
tags:
- self-data
- distillation
- quality
- fine-tuning
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of quality degradation in pruned
  large language models, particularly for tasks requiring multi-step reasoning. The
  authors propose a novel self-data distillation method that uses the original unpruned
  model to generate a distilled dataset, preserving semantic richness and mitigating
  catastrophic forgetting.
---

# Self-Data Distillation for Recovering Quality in Pruned Large Language Models

## Quick Facts
- arXiv ID: 2410.09982
- Source URL: https://arxiv.org/abs/2410.09982
- Reference count: 40
- Primary result: Self-data distillation achieves up to 8% higher average accuracy on OpenLLM Leaderboard v1 compared to supervised fine-tuning for pruned models

## Executive Summary
This paper addresses the quality degradation problem in pruned large language models, particularly for multi-step reasoning tasks. The authors propose self-data distillation, a novel method that uses the original unpruned model to generate a distilled dataset for fine-tuning pruned models. This approach preserves semantic richness and mitigates catastrophic forgetting, achieving significant quality improvements over standard supervised fine-tuning while reducing computational costs through structured pruning.

## Method Summary
The method involves three main steps: first, identifying redundant layers using angular cosine distance between final token activations; second, pruning the identified layers from the original model; and third, fine-tuning the pruned model on a self-distilled dataset generated by the unpruned model. The approach uses LoRA adapters for efficient fine-tuning and can be combined with speculative decoding for additional inference efficiency gains.

## Key Results
- Self-data distillation achieves 91.2% accuracy retention on Llama3.1-8B Instruct (pruned by 6 decoder blocks) versus 81.7% with supervised fine-tuning
- Reduces real-world FLOPs by 16.3% while maintaining higher quality than pruned models without fine-tuning
- Achieves up to 8% higher average accuracy on HuggingFace OpenLLM Leaderboard v1 tasks compared to supervised fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
Self-data distillation preserves semantic richness by aligning the pruned model's output distribution with the original model's. The unpruned model generates distilled responses that remain within its learned data distribution, minimizing catastrophic forgetting. Core assumption: The original model's output distribution contains essential semantic information that can be transferred to the pruned model through fine-tuning. Break condition: If the original model's output distribution has shifted significantly from the true data distribution, the distilled responses may reinforce incorrect patterns.

### Mechanism 2
Self-data distillation reduces catastrophic forgetting by maintaining alignment with the base model's knowledge. The fine-tuning dataset is rewritten using the original model's distribution, preventing the pruned model from drifting too far from its original capabilities. Core assumption: Distribution shift during fine-tuning is the primary driver of catastrophic forgetting in pruned models. Break condition: If the pruned model's architecture is too different from the original, even distribution-aligned fine-tuning may not prevent forgetting.

### Mechanism 3
Angular cosine distance effectively identifies redundant layers for pruning by measuring output similarity. Layers producing highly similar outputs can be removed with minimal impact on model quality, as measured by angular distance between final token activations. Core assumption: The final token representation captures sufficient information about layer redundancy for pruning decisions. Break condition: If layer redundancy manifests primarily in intermediate representations rather than final token outputs, angular cosine distance may miss critical pruning opportunities.

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: Self-data distillation builds on KD principles but uses the same model as both teacher and student
  - Quick check question: How does self-data distillation differ from traditional knowledge distillation in terms of the teacher-student relationship?

- Concept: Catastrophic Forgetting
  - Why needed here: Understanding why pruned models lose capabilities during fine-tuning is crucial for designing mitigation strategies
  - Quick check question: What role does distribution shift play in catastrophic forgetting during fine-tuning of pruned models?

- Concept: Structured Pruning
  - Why needed here: The method specifically targets structured layer pruning rather than weight pruning
  - Quick check question: Why might structured pruning lead to more severe quality degradation than unstructured pruning for reasoning tasks?

## Architecture Onboarding

- Component map:
  Original unpruned model (teacher) -> Angular cosine distance metric -> Pruned model (student) -> Distillation dataset generator -> Fine-tuning pipeline with LoRA adapters -> Evaluation framework (OpenLLM Leaderboard)

- Critical path:
  1. Identify redundant layers using angular cosine distance
  2. Prune identified layers from original model
  3. Generate distilled dataset using unpruned model
  4. Fine-tune pruned model on distilled dataset
  5. Evaluate quality recovery

- Design tradeoffs:
  - Dataset size vs. quality recovery (larger datasets show better results)
  - Pruning aggressiveness vs. baseline quality retention
  - Self-data distillation computation cost vs. SFT computation cost

- Failure signatures:
  - Low embedding similarity to baseline model indicates distribution shift
  - Sharp accuracy drops on reasoning tasks suggest catastrophic forgetting
  - Poor token acceptance rates in speculative decoding indicate misalignment

- First 3 experiments:
  1. Compare angular cosine vs. BI metrics on small prune block sizes
  2. Test self-data distillation vs. SFT on GSM8k dataset with 8k samples
  3. Evaluate model merging effectiveness using SLERP on OpenMathInstruct + Alpaca

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of self-data distillation compare when using different base models (e.g., GPT-4, Gemini) instead of Llama3.1-8B Instruct? The paper focuses on Llama3.1-8B Instruct and Mistral-7B-v0.3 Instruct but suggests extending the methodology to next-generation LLM architectures. Conducting experiments using self-data distillation on models like GPT-4 or Gemini would provide insights into the method's effectiveness across different architectures.

### Open Question 2
What is the impact of combining self-data distillation with other model compression techniques, such as quantization or sparsity, on model quality and efficiency? The paper suggests that future work may involve integrating self-data distilled fine-tuning with complementary model compression techniques such as sparsity, quantization, or teacher distillation. Empirical studies comparing the performance of models using self-data distillation alone versus those using a combination of self-data distillation and other compression techniques would clarify the impact on model quality and efficiency.

### Open Question 3
How does the choice of calibration dataset affect the effectiveness of structured pruning and subsequent quality recovery through self-data distillation? The paper mentions an ablation study on the choice of calibration dataset for computing the angular cosine distance block importance metric, but the impact on self-data distillation effectiveness is not explored. Conducting experiments using various calibration datasets (e.g., C4, RedPajama, SlimPajama) and evaluating the subsequent quality recovery through self-data distillation would provide insights into the importance of dataset selection.

## Limitations
- Relies heavily on the original unpruned model's ability to generate semantically rich responses, which may be problematic if the base model has inherent biases or limited knowledge in specific domains
- Effectiveness is contingent on careful calibration of the pruning process and fine-tuning parameters, which may not generalize across different model architectures or tasks
- The method's performance may degrade when applied to models with significantly different architectural designs or training objectives

## Confidence
- Medium confidence: The effectiveness of self-data distillation for quality recovery compared to supervised fine-tuning
- Medium confidence: The scalability of self-data distillation with dataset size
- High confidence: The improvement in inference efficiency when combining self-data distillation with speculative decoding
- Low confidence: The generality of angular cosine distance as the optimal metric for identifying redundant layers

## Next Checks
1. **Cross-model generalization test**: Evaluate self-data distillation on a different model family (e.g., CodeLlama or DeepSeek) to assess whether the quality recovery patterns observed in Llama3.1-8B and Mistral-7B transfer to models with different architectural designs and training objectives.

2. **Ablation on distillation dataset quality**: Systematically vary the quality of the distilled dataset by introducing controlled noise or using different teacher models to quantify how sensitive the self-data distillation method is to the semantic richness of the generated responses.

3. **Long-range reasoning task evaluation**: Test the pruned and fine-tuned models on complex, multi-step reasoning tasks that require extended context tracking (e.g., advanced mathematical proofs or multi-document question answering) to validate whether the quality improvements observed in standard benchmarks translate to more challenging scenarios.