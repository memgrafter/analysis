---
ver: rpa2
title: Enhancing Question Answering for Enterprise Knowledge Bases using Large Language
  Models
arxiv_id: '2404.08695'
source_url: https://arxiv.org/abs/2404.08695
tags:
- knowledge
- retrieval
- training
- question
- enterprise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EKRG, a novel framework for question-answering
  in enterprise knowledge bases with limited annotation costs. The core idea is to
  leverage large language models (LLMs) for both document-question pair generation
  and answer generation.
---

# Enhancing Question Answering for Enterprise Knowledge Bases using Large Language Models

## Quick Facts
- arXiv ID: 2404.08695
- Source URL: https://arxiv.org/abs/2404.08695
- Authors: Feihu Jiang; Chuan Qin; Kaichun Yao; Chuyu Fang; Fuzhen Zhuang; Hengshu Zhu; Hui Xiong
- Reference count: 31
- Key outcome: EKRG achieves 55.74% Exact Match for fact-oriented questions, outperforming baselines by over 10 percentage points

## Executive Summary
This paper introduces EKRG, a novel framework for question-answering in enterprise knowledge bases with limited annotation costs. The core idea is to leverage large language models (LLMs) for both document-question pair generation and answer generation. For retrieval, an instruction-tuning method using LLMs generates diverse document-question pairs, followed by a relevance-aware teacher-student learning strategy to enhance training efficiency. For generation, a chain-of-thought (CoT) based fine-tuning method enables the LLM to provide coherent answers using retrieved documents. Experiments on real-world datasets demonstrate the effectiveness of EKRG, showing significant improvements over baselines in both retrieval and generation tasks.

## Method Summary
EKRG addresses enterprise knowledge base question-answering by utilizing LLMs in two key stages: retrieval and generation. In the retrieval stage, LLMs generate diverse document-question pairs through instruction-tuning, followed by a relevance-aware teacher-student learning strategy to improve training efficiency. In the generation stage, a chain-of-thought fine-tuning approach enables the LLM to produce coherent answers using retrieved documents. The framework aims to minimize annotation costs while maintaining high performance, particularly for fact-oriented questions in enterprise settings.

## Key Results
- EKRG achieves 55.74% Exact Match for fact-oriented questions in generation tasks
- Outperforms In-Context RALM baseline (45.44% EM) by over 10 percentage points
- Demonstrates effectiveness in both retrieval and generation tasks on real-world datasets

## Why This Works (Mechanism)
The framework leverages LLMs for synthetic data generation and fine-tuning, enabling effective learning from limited annotations. The teacher-student learning strategy in retrieval improves efficiency by transferring knowledge from synthetic to real data. The chain-of-thought approach in generation allows for more coherent and context-aware answers by breaking down the reasoning process.

## Foundational Learning
- **Large Language Models**: Pre-trained models capable of understanding and generating human-like text, used here for both data generation and fine-tuning
- **Chain-of-Thought Reasoning**: A method of breaking down complex problems into intermediate steps to improve reasoning and coherence in generated answers
- **Teacher-Student Learning**: A knowledge distillation approach where a "teacher" model guides the training of a "student" model, improving efficiency and performance
- **Instruction Tuning**: The process of adapting a model to follow specific instructions or prompts, used here to generate diverse document-question pairs
- **Relevance-Aware Learning**: A strategy that focuses on improving model performance on relevant examples, enhancing training efficiency in the retrieval stage
- **Enterprise Knowledge Bases**: Structured repositories of organizational knowledge, requiring specialized QA approaches due to their domain-specific nature and limited annotation availability

## Architecture Onboarding
- **Component Map**: LLM (data generation) -> Retrieval Model (fine-tuned with synthetic data) -> Generation Model (fine-tuned with CoT approach) -> Answer Output
- **Critical Path**: Synthetic data generation -> Teacher-student learning in retrieval -> Chain-of-thought fine-tuning in generation -> Answer production
- **Design Tradeoffs**: Balancing annotation costs against model performance, synthetic data quality versus real data scarcity, and reasoning depth versus generation efficiency
- **Failure Signatures**: Potential overfitting to synthetic patterns, reduced performance on unseen document types, and generation of factually inconsistent answers
- **First Experiment 1**: Evaluate retrieval performance using standard IR metrics (MRR, Recall) on a held-out test set
- **First Experiment 2**: Assess generation quality using Exact Match and other QA metrics on fact-oriented questions
- **First Experiment 3**: Conduct ablation studies on the teacher-student learning component to quantify its contribution

## Open Questions the Paper Calls Out
None

## Limitations
- Potential biases introduced by LLM-generated synthetic data not thoroughly examined
- Lack of comprehensive analysis of answer quality beyond Exact Match scores, including factual consistency and hallucination risks
- Limited description of real-world datasets used, affecting reproducibility and generalizability assessments

## Confidence
- **High confidence**: The overall experimental methodology and baseline comparisons are sound
- **Medium confidence**: The reported performance improvements and their statistical significance
- **Medium confidence**: The practical feasibility of the framework in actual enterprise settings

## Next Checks
1. Conduct human evaluation studies comparing synthetic data quality against human-annotated standards, focusing on diversity and relevance
2. Perform ablation studies on the teacher-student learning component to quantify its contribution and identify potential overfitting
3. Test the framework on multiple enterprise knowledge bases with varying characteristics to assess generalizability and robustness to different domain-specific knowledge structures