---
ver: rpa2
title: Solving Hidden Monotone Variational Inequalities with Surrogate Losses
arxiv_id: '2411.05228'
source_url: https://arxiv.org/abs/2411.05228
tags:
- inner
- surrogate
- gradient
- error
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses variational inequality (VI) problems arising
  in deep learning, such as min-max optimization and reinforcement learning, which
  lack efficient stationary point guarantees and diverge under naive gradient methods.
  The authors propose a surrogate loss approach that reduces VI problems to minimizing
  a sequence of scalar losses.
---

# Solving Hidden Monotone Variational Inequalities with Surrogate Losses

## Quick Facts
- arXiv ID: 2411.05228
- Source URL: https://arxiv.org/abs/2411.05228
- Reference count: 40
- One-line primary result: The authors propose a surrogate loss approach that reduces VI problems to minimizing a sequence of scalar losses, proving linear convergence under the α-descent condition when hidden monotonicity is present.

## Executive Summary
This work addresses variational inequality (VI) problems arising in deep learning, such as min-max optimization and reinforcement learning, which lack efficient stationary point guarantees and diverge under naive gradient methods. The authors propose a surrogate loss approach that reduces VI problems to minimizing a sequence of scalar losses. Under the α-descent condition on these surrogates—requiring sufficient progress relative to the best possible improvement—they prove linear convergence to the VI solution when hidden monotonicity is present and interpolation is possible.

## Method Summary
The authors propose a surrogate loss approach for solving variational inequalities (VI) in deep learning settings. At each iteration, they construct a surrogate loss ℓt(θ) = 1/2∥g(θ) − (g(θt) − ηF(g(θt)))∥² that approximates a projected gradient step. They minimize this surrogate using various optimizers (GD, Gauss-Newton, Damped Gauss-Newton, Levenberg-Marquardt) until the α-descent condition is satisfied: ℓt(θt+1) − ℓ*t ≤ α²(ℓt(θt) − ℓ*t). The method is extended to stochastic settings using sampled operator evaluations, making it scalable to large state spaces in reinforcement learning.

## Key Results
- Proves linear convergence to VI solutions under α-descent condition and hidden monotonicity
- Shows α < 1 is necessary (not just sufficient) for convergence, unlike scalar minimization
- Unifies existing preconditioning methods as special cases of minimizing surrogate losses via Gauss-Newton
- Demonstrates superior data efficiency and runtime in min-max games and reinforcement learning policy evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The α-descent condition guarantees convergence to a hidden monotone VI solution even when the surrogate loss is only approximately minimized.
- Mechanism: At each iteration, the surrogate loss is minimized just enough to satisfy ℓt(θt+1) - ℓ*t ≤ α²(ℓt(θt) - ℓ*t). This relative improvement ensures that the prediction error ∥zt+1 - z*t∥ remains bounded by αη∥F(zt) - F(z*)∥, which decays under strong monotonicity and Lipschitz continuity.
- Core assumption: The hidden mapping g and VI operator F are smooth and structured enough that the surrogate loss mimics the true projected gradient step, and α < 1 is enforced.
- Evidence anchors:
  - [abstract] "Under the α-descent condition on these surrogates—requiring sufficient progress relative to the best possible improvement—they prove linear convergence to the VI solution when hidden monotonicity is present"
  - [section 3.1] "Theorem 3.2... guarantees linear convergence to the solution z* at the following linear rate: ∥zt+1 - z*∥² ≤ ρt∥z1 - z*∥²"
- Break condition: If α ≥ 1 or the hidden structure (monotonicity, Lipschitz) is violated, the error ∥zt+1 - z*t∥ may not shrink, and the sequence can diverge.

### Mechanism 2
- Claim: The surrogate loss can be minimized via Gauss-Newton or related methods, which naturally precondition the gradient and improve stability in non-convex parameter space.
- Mechanism: By viewing the surrogate as a nonlinear least-squares problem, one step of Gauss-Newton on the residual r(θ) = g(θ) - g(θt) + ηF(g(θt)) yields the preconditioned update θt+1 = θt - (Dg⊤Dg)†Dg⊤F(zt), recovering existing preconditioning methods and offering faster local convergence.
- Core assumption: The Jacobian Dg has bounded singular values so that (Dg⊤Dg)† exists or is well-conditioned, and the residual is small enough for Gauss-Newton to be effective.
- Evidence anchors:
  - [section 4] "Interestingly, in Section 4 we show that PHGD is also equivalent to taking one step of the Gauss-Newton method (GN) on the surrogate loss"
  - [section 4.1] "Several conditions allow for fast linear convergence of gradient descent for ℓt(θ) - ℓ*t"
- Break condition: If Dg is rank-deficient or nearly so, the Gauss-Newton direction may be ill-defined or lead to slow progress; damping (LM) or GD may be preferable.

### Mechanism 3
- Claim: The stochastic surrogate loss removes the need for full operator evaluations, making VI methods scalable to large state spaces in RL.
- Mechanism: The stochastic surrogate ˜ℓt(θ) = ½∥g(θ) - (g(θt) - ηFξt(zt))∥² uses a single sampled transition to approximate the projected gradient step, and minimizing it via GD yields an update close to the true step in expectation.
- Core assumption: The sampled operator Fξ is an unbiased estimator of F and has bounded variance σ²; Z = Rn so no projection is needed.
- Evidence anchors:
  - [section 3.2] "We also assume the standard setup, the possibility to generate independent and identically distributed realizations (ξ1, ξ2, ···) of a random variable ξ such that Fξ(z) is an unbiased estimator of F(z)"
  - [section 5.2] "We must instead consider minimizing the stochastic surrogate ˜ℓt from Section 3.2"
- Break condition: If the variance σ² is large or the estimator is biased, the expected descent condition may fail and convergence to a neighborhood (not the exact solution) occurs.

## Foundational Learning

- Concept: Variational inequalities and monotone operators
  - Why needed here: The target problem is not a scalar minimization but a VI: find z* such that ⟨F(z*), z - z*)⟩ ≥ 0 for all z in a constraint set. Understanding monotone/Lipschitz structure is essential for proving convergence.
  - Quick check question: Given F(z) = [∇z1 f(z1,z2), -∇z2 f(z1,z2)]⊤ from a convex-concave f, is F monotone? (Yes, by definition of convex-concave.)

- Concept: Hidden convexity/monotonicity in deep models
  - Why needed here: Deep models parameterize predictions g(θ) ∈ Z, and many losses are convex/strongly monotone in z even if non-convex in θ. This hidden structure enables VI-based analysis.
  - Quick check question: If g(θ) = Φθ is linear and f(z) is strongly convex, is f(g(θ)) strongly convex in θ? (No, but f is strongly convex in z, enabling VI analysis.)

- Concept: Projected gradient methods and contraction mappings
  - Why needed here: The true VI solution can be reached by zt+1 = Π(zt - ηF(zt)), a contraction if η is small. The surrogate loss approximates this step, so understanding contraction is key to proving surrogate convergence.
  - Quick check question: If F is L-Lipschitz and µ-strongly monotone, for what η is zt+1 = zt - ηF(zt) a contraction? (η < 2µ/L.)

## Architecture Onboarding

- Component map: Collect batch -> Compute F(g(θt)) -> Build surrogate ℓt -> Minimize ℓt via GD/GN/DGN/LM -> Update θt+1 -> Repeat
- Critical path: F evaluation → surrogate construction → inner-loop optimization → parameter update
- Design tradeoffs:
  - More inner steps → better α-descent but higher per-iteration cost; fewer steps → cheaper but may need smaller α or η
  - GD: simple, robust; GN: faster near solution if Dg well-conditioned; LM: handles rank-deficiency but adds λ tuning
  - Stochastic surrogate: scalable to large Z but introduces variance; double sampling reduces bias at cost of extra samples
- Failure signatures:
  - Divergence: α too close to 1, η too large, or hidden structure violated
  - Slow convergence: α too small, inner loop steps insufficient, Dg ill-conditioned
  - Numerical instability: rank-deficient Dg in GN/LM, large condition number in (Dg⊤Dg)†
- First 3 experiments:
  1. Hidden matching pennies with varying α and inner steps; verify linear convergence and loss ratio behavior
  2. Linear TD(0) surrogate with exact minimizer vs. GD approximations; measure BEgap and convergence speed
  3. Nonlinear RL surrogate with double sampling vs. single batch; compare value prediction error and wall-clock time

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the α-descent condition generalize to non-Euclidean geometries (e.g., using Bregman divergences) for variational inequalities, and what are the convergence implications?
- Basis in paper: [inferred] The paper focuses on Euclidean projections and quadratic surrogates, but the related work mentions Bregman divergences for scalar minimization cases.
- Why unresolved: The authors do not explore alternative geometries, and it's unclear if their α-descent analysis extends to non-Euclidean settings.
- What evidence would resolve it: A theoretical analysis showing convergence rates for α-descent with Bregman divergences, or experimental validation in non-Euclidean VI problems.

### Open Question 2
- Question: How does the choice of inner-loop optimizer (e.g., GN, DGN, LM, GD) affect the practical efficiency of surrogate-based VI solvers, particularly in high-dimensional or ill-conditioned problems?
- Basis in paper: [explicit] The experiments compare GN, DGN, LM, and GD in min-max games, showing varying performance, but do not systematically study high-dimensional or ill-conditioned cases.
- Why unresolved: The experiments are limited to low-dimensional problems, and the paper does not analyze the impact of ill-conditioning on inner-loop convergence.
- What evidence would resolve it: Extensive experiments on high-dimensional or ill-conditioned VI problems, comparing different inner-loop optimizers, and theoretical bounds on their convergence.

### Open Question 3
- Question: Can the surrogate loss approach be extended to stochastic VI problems with biased gradients or non-stationary distributions, and what are the convergence guarantees?
- Basis in paper: [inferred] The paper addresses stochastic VI with unbiased gradients (Assumption 3.4) but does not consider biased or non-stationary cases, which are common in online or reinforcement learning.
- Why unresolved: The current analysis relies on unbiased gradient estimators and stationary distributions, and extending to biased or non-stationary cases would require new techniques.
- What evidence would resolve it: Theoretical convergence analysis for biased gradients or non-stationary distributions, or experimental validation in online or non-stationary VI problems.

## Limitations
- Hidden Structure Dependence: Relies critically on hidden monotonicity and Lipschitz continuity that may be difficult to verify in complex deep models
- Computational Overhead: Inner-loop optimization adds significant computational cost compared to standard gradient methods
- Stochastic Variance Impact: Variance in sampled operator evaluations can significantly affect practical convergence rates

## Confidence
- High Confidence: Theoretical framework for surrogate losses in VI problems and experimental validation on min-max games
- Medium Confidence: Practical effectiveness of stochastic surrogate variant in reinforcement learning and comparison with TD(0)
- Low Confidence: Scalability to very large-scale problems and robustness across diverse deep learning architectures

## Next Checks
1. Systematically test the algorithm's performance when the hidden monotonicity or Lipschitz continuity assumptions are violated or only approximately satisfied
2. Conduct a hyperparameter study varying α across a wide range (0.1 to 0.9) and measure the impact on convergence speed, stability, and final solution quality
3. Implement the approach on larger-scale min-max optimization problems (e.g., GAN training on standard datasets) and measure computational overhead, memory usage, and wall-clock time compared to baseline methods