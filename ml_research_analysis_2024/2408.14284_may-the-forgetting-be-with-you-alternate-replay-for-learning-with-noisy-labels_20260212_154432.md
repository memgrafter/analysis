---
ver: rpa2
title: 'May the Forgetting Be with You: Alternate Replay for Learning with Noisy Labels'
arxiv_id: '2408.14284'
source_url: https://arxiv.org/abs/2408.14284
tags:
- learning
- noisy
- buffer
- lrbuffer
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Alternate Experience Replay (AER) to address
  continual learning with noisy labels. AER alternates between buffer learning and
  buffer forgetting epochs to exploit forgetting as a tool to distinguish clean and
  noisy samples in the memory buffer.
---

# May the Forgetting Be with You: Alternate Replay for Learning with Noisy Labels

## Quick Facts
- arXiv ID: 2408.14284
- Source URL: https://arxiv.org/abs/2408.14284
- Reference count: 40
- Primary result: AER+ABS achieves 4.71% average improvement in accuracy over loss-based purification methods for continual learning with noisy labels

## Executive Summary
This paper introduces Alternate Experience Replay (AER) to address continual learning with noisy labels by leveraging forgetting as a tool to distinguish clean and noisy samples. The method alternates between buffer learning and buffer forgetting epochs, exploiting the observation that noisy samples experience greater loss increases during forgetting phases compared to clean samples. AER is paired with Asymmetric Balanced Sampling (ABS), which prioritizes purity for current task examples while retaining complex samples from past tasks. Experiments across multiple datasets and noise types demonstrate that AER+ABS achieves an average 4.71% improvement in accuracy over existing methods, with lower computational cost and higher buffer purity and diversity.

## Method Summary
AER alternates between buffer learning and forgetting epochs, where forgetting epochs cause the model to underfit and experience buffer forgetting. This creates a dynamic where clean samples retain low loss while noisy samples experience significant loss increases, enabling effective separation. Model checkpointing preserves parameters at the start of each forgetting epoch and restores them afterward to prevent buffer under-optimization. ABS is a sample selection strategy that uses asymmetric scoring to favor retaining high-loss (complex) samples from past tasks while prioritizing low-loss (clean) samples from the current task. The method also includes buffer consolidation using MixMatch-based refinement at the end of each task.

## Key Results
- AER+ABS achieves an average 4.71% improvement in accuracy over existing loss-based purification methods
- Buffer purity and diversity are significantly higher compared to baseline methods
- The method shows consistent performance across multiple datasets (Seq. CIFAR-100, Seq. NTU RGB+D, Seq. Food-101N) and noise types (symmetric, asymmetric, real-world)
- Computational cost is lower than traditional replay-based methods due to reduced need for extensive buffer optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Forgetting can be leveraged to separate clean and noisy samples in the buffer.
- Mechanism: Alternating between buffer learning and buffer forgetting epochs creates a dynamic where clean samples retain low loss while noisy samples experience a significant loss increase during forgetting phases. This separation enables effective filtering of noisy data.
- Core assumption: Mislabeled examples are more prone to being forgotten than complex but correctly labeled ones.
- Evidence anchors:
  - [abstract] "The idea is that complex or mislabeled examples, which hardly fit the previously learned data distribution, are most likely to be forgotten."
  - [section 3.1] "By halting regularization and causing the subsequent forgetting of buffer datapoints, the loss of noisy examples is likely to increase more rapidly than that of clean ones."
  - [corpus] Weak - no direct supporting papers found in corpus for this specific mechanism.

### Mechanism 2
- Claim: Asymmetric Balanced Sampling (ABS) improves buffer diversity while maintaining purity.
- Mechanism: ABS uses a loss-based sampling strategy that favors retaining high-loss (complex) samples from past tasks while prioritizing low-loss (clean) samples from the current task. This asymmetric scoring ensures both diversity and purity.
- Core assumption: High-loss samples from past tasks are likely to be complex but clean, while high-loss samples from the current task are more likely to be noisy.
- Evidence anchors:
  - [abstract] "Asymmetric Balanced Sampling (ABS): a new sample selection strategy that prioritizes purity on the current task while retaining relevant samples from the past."
  - [section 3.2] "we accept that a few mislabeled examples from past tasks might remain in the memory buffer; however, given the joint effect of the insertion policy and the LASS-like side of the replacement criterion – both of which tend to favor purity among examples from the current task – we can be cautiously optimistic that the examples from past tasks are correctly annotated."
  - [corpus] Weak - no direct supporting papers found in corpus for this specific mechanism.

### Mechanism 3
- Claim: Model checkpointing prevents buffer under-optimization during alternating epochs.
- Mechanism: By saving the model parameters at the start of each forgetting epoch and restoring them at the end, the model can be optimized for both buffer learning and forgetting phases without compromising the buffer content.
- Core assumption: The model can be effectively optimized in half the epochs if the alternating strategy provides better buffer quality.
- Evidence anchors:
  - [section 3.1] "At the start of each forgetting epoch, we save the parameters of the model fθ (line 12) and restore them at the end of the same epoch (line 2)."
  - [corpus] Weak - no direct supporting papers found in corpus for this specific mechanism.

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: The entire paper addresses how to mitigate forgetting while learning with noisy labels in a continual learning setup.
  - Quick check question: What happens to model performance on previous tasks when training on new tasks without any countermeasures?

- Concept: Memorization effect in noisy label learning
  - Why needed here: The paper relies on the observation that clean samples exhibit lower loss than noisy samples during initial training stages to design its sample selection strategy.
  - Quick check question: How does the loss value of clean samples compare to noisy samples during the early stages of training?

- Concept: Experience Replay in continual learning
  - Why needed here: The proposed method builds upon experience replay by alternating between buffer learning and forgetting epochs, and by introducing ABS for sample selection.
  - Quick check question: What is the primary purpose of using a replay buffer in continual learning?

## Architecture Onboarding

- Component map: Data stream -> Current batch -> Buffer samples -> Model training -> Buffer update -> Model checkpointing -> Buffer consolidation

- Critical path:
  1. Receive data stream and current batch
  2. Compute asymmetric scores for buffer samples
  3. Alternate between buffer learning and forgetting epochs
  4. Update buffer with new samples based on loss thresholds
  5. Replace buffer samples using ABS
  6. Apply buffer consolidation at task end

- Design tradeoffs:
  - Buffer size vs. computational cost: Larger buffers provide better representation but increase memory usage
  - Epoch alternation frequency: More frequent alternation may improve separation but could slow convergence
  - Loss threshold for sample insertion: Higher thresholds retain more samples but may include more noise

- Failure signatures:
  - Performance degradation on previous tasks indicates insufficient buffer quality
  - No improvement over baseline suggests the forgetting mechanism is not effective
  - High variance across runs may indicate instability in the alternating strategy

- First 3 experiments:
  1. Implement basic experience replay with alternating epochs (no ABS) to verify the forgetting separation mechanism works
  2. Add ABS to the alternating strategy to test the impact of asymmetric sampling on buffer quality
  3. Apply buffer consolidation to evaluate the final refinement step's contribution to overall performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of AER+ABS scale with different buffer sizes across datasets?
- Basis in paper: [explicit] The paper uses different buffer sizes for different datasets (500 for CIFAR-10, 2000 for CIFAR-100 and Food-101N, 500 for NTU RGB-D) but does not systematically evaluate the impact of varying buffer sizes on performance.
- Why unresolved: The authors chose buffer sizes based on dataset length but did not explore the relationship between buffer size and performance.
- What evidence would resolve it: Systematic experiments varying buffer sizes for each dataset while measuring Final Average Accuracy and buffer purity/diversity.

### Open Question 2
- Question: What is the theoretical limit of AER+ABS's effectiveness as noise rates approach 100%?
- Basis in paper: [inferred] The paper evaluates noise rates up to 60% but does not explore higher noise rates or establish theoretical bounds on performance degradation.
- Why unresolved: The experiments stop at 60% noise, leaving open questions about performance in extremely noisy regimes.
- What evidence would resolve it: Experiments with noise rates above 60% and theoretical analysis of the method's convergence properties in high-noise scenarios.

### Open Question 3
- Question: How does AER+ABS perform in non-class-incremental continual learning scenarios?
- Basis in paper: [explicit] The paper focuses on class-incremental scenarios but mentions that AER+ABS "can be equally applied to advanced choices of LR" in other settings.
- Why unresolved: The method is only evaluated in class-incremental settings, leaving its applicability to task-incremental or domain-incremental scenarios unexplored.
- What evidence would resolve it: Experiments applying AER+ABS to task-incremental and domain-incremental benchmarks, comparing performance against specialized methods for those scenarios.

## Limitations

- The core mechanisms rely on assumptions about forgetting dynamics and sample complexity that are not thoroughly validated through ablation studies or theoretical analysis
- The method's effectiveness in extremely high-noise regimes (above 60% noise rate) remains unexplored
- The applicability to non-class-incremental continual learning scenarios is only mentioned but not empirically evaluated

## Confidence

- High Confidence: The experimental results showing AER+ABS outperforming baseline methods on multiple datasets and noise types are robust and well-documented.
- Medium Confidence: The theoretical justification for using forgetting as a separation mechanism is plausible but lacks direct supporting evidence from the paper.
- Low Confidence: The assumption that high-loss past task samples are complex but clean is not validated and could be a potential failure point.

## Next Checks

1. Conduct ablation studies to verify that the forgetting mechanism effectively separates clean and noisy samples in the buffer.
2. Validate the assumption about high-loss past task samples being complex but clean through controlled experiments with varying noise levels.
3. Test the model checkpointing mechanism's stability and effectiveness across different alternating frequencies and buffer sizes.