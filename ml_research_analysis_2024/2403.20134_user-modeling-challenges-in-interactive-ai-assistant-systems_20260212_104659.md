---
ver: rpa2
title: User Modeling Challenges in Interactive AI Assistant Systems
arxiv_id: '2403.20134'
source_url: https://arxiv.org/abs/2403.20134
tags:
- user
- users
- mental
- states
- modeling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes user mental states during task execution and
  investigates large language models'' capabilities in interpreting user profiles
  for personalized guidance. The authors extend the WTaG dataset to include six user
  profile categories: Frustration, Eagerness to ask questions, Talkative, Experience,
  Familiarity with tools, and Detail-orientation.'
---

# User Modeling Challenges in Interactive AI Assistant Systems

## Quick Facts
- arXiv ID: 2403.20134
- Source URL: https://arxiv.org/abs/2403.20134
- Reference count: 4
- Authors: Megan Su; Yuwei Bao
- Key outcome: LLM achieves high F1 scores for detecting Detail-oriented (0.96), Eager to ask questions (0.92), and Talkative (0.88) behaviors, but struggles with Frustration (high false positive rate) and Experience (precision and recall of zero).

## Executive Summary
This paper analyzes user mental states during task execution and investigates large language models' capabilities in interpreting user profiles for personalized guidance. The authors extend the WTaG dataset to include six user profile categories and use ChatGPT to predict these profiles from dialog history. The study reveals promising capabilities for detecting certain behavioral patterns while highlighting significant challenges in identifying emotional states and knowledge levels. Results suggest that while LLMs can recognize consistent behavioral traits across tasks, they require substantial improvements for comprehensive user modeling in interactive AI assistant systems.

## Method Summary
The authors extended the WTaG dataset with six user profile categories (Frustration, Eagerness to ask questions, Talkative, Experience, Familiarity with tools, Detail-orientation) across 55 recordings from 17 users and 3 recipes. They used ChatGPT (GPT-3.5-turbo-0301) with temperature=0 to predict binary "yes"/"no" labels for each profile category from dialog history. Performance was evaluated using F1 scores, precision, and recall metrics, with additional consistency analysis across recipes to assess how user states transfer between different tasks.

## Key Results
- ChatGPT achieves high F1 scores for Detail-oriented (0.96), Eager to ask questions (0.92), and Talkative (0.88) behaviors
- Model shows poor performance on Frustration with high false positive rates and zero precision/recall for Experience detection
- User mental state consistency varies significantly across categories: high for Frustration and Detail-orientation, moderate for Eagerness/Talkativeness/Experience, and low for Familiarity with tools

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dataset extension enables systematic evaluation of LLM capabilities in detecting user mental states during task execution
- Mechanism: By annotating 55 recordings across 17 users and 3 recipes with six user profile categories, the authors created a structured dataset that allows quantitative assessment of how well LLMs can interpret user states from dialog history
- Core assumption: User mental states can be reliably captured through binary annotations based on dialog history
- Evidence anchors:
  - "We extended the human instructor-user dialog from the WTaG dataset to include 6 categories of user profiles for each recording"
  - "Using ChatGPT to predict these profiles from dialog history, they achieve high F1 scores for detecting Detail-oriented (0.96), Eager to ask questions (0.92), and Talkative (0.88) behaviors"
- Break condition: If the binary annotation scheme fails to capture nuanced user mental states, or if dialog history alone is insufficient for accurate detection

### Mechanism 2
- Claim: LLM performance varies significantly across different user profile categories, revealing both capabilities and limitations
- Mechanism: The study shows that ChatGPT achieves high performance (F1 > 0.88) for detecting Detail-oriented, Eager to ask questions, and Talkative behaviors, but fails completely on Frustration (high false positive rate) and Experience (precision and recall of zero)
- Core assumption: F1 score, precision, and recall metrics accurately reflect the LLM's ability to interpret user mental states
- Evidence anchors:
  - "Our results in Figure 3 indicate LLM's high ability to detect when users are 'Detail-oriented', 'Eager to ask questions', or 'Talkative', with F1 scores of 0.96, 0.92, and 0.88, respectively"
  - "However, the 'Frustrated' category reveals a stark contrast, in which it frequently misclassifies non-frustrated behavior, yielding a high false positive rate in this category"
- Break condition: If evaluation metrics don't capture practical utility of predictions, or if model failures are due to prompt engineering rather than fundamental limitations

### Mechanism 3
- Claim: User mental state consistency varies across profile categories, suggesting different adaptation strategies for AI assistants
- Mechanism: Analysis shows high consistency for Frustration and Detail-orientation across different tasks, moderate consistency for Eager to ask questions, Talkativeness, and Experience, and low consistency for Familiarity with tools
- Core assumption: Consistency across tasks is a valid measure of how well AI assistants can transfer user accommodations between different activities
- Evidence anchors:
  - "Among the results in Figure 2, a high consistency was observed on users for both 'Frustration' and 'Detail-orientation'"
  - "'Familiarity with tools' have the lowest consistency, which indicates the AI assistant would need to understand user's proficiency on each tool used in different tasks for more situated guidance"
- Break condition: If task similarity between recipes is too high, making consistency measures unreliable, or if consistency doesn't correlate with actual user satisfaction

## Foundational Learning

- Concept: User modeling in interactive AI systems
  - Why needed here: Understanding how to represent and interpret user states is fundamental to building personalized AI assistants that can adapt their guidance
  - Quick check question: What are the key differences between reactive and proactive user modeling approaches in AI assistance?

- Concept: Evaluation metrics for classification tasks
  - Why needed here: The paper uses F1 score, precision, and recall to evaluate LLM performance, requiring understanding of when each metric is appropriate and how to interpret them
  - Quick check question: Why would precision be zero for the "Experienced" category while recall is also zero, and what does this tell us about the model's predictions?

- Concept: Dataset annotation and consistency analysis
  - Why needed here: The study relies on manual annotation of user states and consistency analysis across tasks, requiring understanding of annotation reliability and statistical measures of consistency
  - Quick check question: How does calculating consistency as 1 (same annotation across all recipes) vs 0 (different annotations) provide insight into user behavior patterns?

## Architecture Onboarding

- Component map:
  - Data collection pipeline: WTaG dataset extension with manual annotation
  - LLM inference component: ChatGPT with temperature=0 for prediction
  - Evaluation module: F1 score, precision, recall calculation and comparison
  - Analysis tools: Consistency calculation across recipes and feature correlation analysis

- Critical path:
  1. Extend existing WTaG dataset with user profile annotations
  2. Prepare dialog history for each recording
  3. Run ChatGPT predictions with fixed temperature
  4. Calculate evaluation metrics and compare with ground truth
  5. Perform consistency and correlation analysis

- Design tradeoffs:
  - Binary vs multi-level annotation: Binary simplifies annotation but may lose nuance
  - Single LLM vs multiple models: Using only ChatGPT limits comparison but ensures consistency
  - Temperature=0 vs sampling: Fixed temperature ensures reproducibility but may limit exploration

- Failure signatures:
  - Low consistency across recipes might indicate poor annotation quality or task-specific behaviors
  - High false positive rate for Frustration suggests model confusion between frustration and other states
  - Zero precision/recall for Experience indicates fundamental mismatch between prompt and model capabilities

- First 3 experiments:
  1. Run same evaluation with different prompt formulations to test sensitivity to prompt engineering
  2. Compare ChatGPT performance with smaller, fine-tuned models on the same task
  3. Test consistency analysis with weighted scores (e.g., 1 for all same, 0.5 for two out of three) to capture partial consistency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific prompting strategies could improve LLM performance in detecting user frustration and experience levels?
- Basis in paper: [explicit] The authors state that "significant improvements are needed for LLMs to understand several categories" including Frustration and Experience, and suggest "investigate better prompting strategies" as future work.
- Why unresolved: The paper only mentions this as future direction without testing specific strategies or providing baseline prompts used in the current experiments.
- What evidence would resolve it: Comparative experiments testing different prompt engineering techniques (chain-of-thought prompting, few-shot examples, role-based prompting) on the same dataset with measurable improvements in precision and recall for Frustration and Experience detection.

### Open Question 2
- How do multimodal signals (beyond dialog history) improve user modeling accuracy for AI assistants?
- Basis in paper: [explicit] The authors suggest investigating "multimodal signals for better user modeling" as future work, noting that current systems rely on dialog history alone.
- Why unresolved: The paper only tests LLMs on text-based dialog history without incorporating visual or other sensory inputs that AR/VR assistants could access.
- What evidence would resolve it: Empirical comparison showing F1 scores for user profile prediction when incorporating multimodal data (visual cues, gaze patterns, voice tone) versus dialog-only approaches across the same user profiles.

### Open Question 3
- What are the optimal trade-offs between user-specific accommodation and recipe-dependent adjustment across different user profile categories?
- Basis in paper: [inferred] The consistency analysis reveals varying degrees of cross-task consistency (high for Frustration/Detail-orientation, moderate for Eagerness/Talkativeness/Experience, low for Familiarity with tools), but doesn't explore optimal modeling strategies.
- Why unresolved: The paper identifies consistency patterns but doesn't test whether models should prioritize transferring learned user states or adapting to task-specific contexts.
- What evidence would resolve it: Controlled experiments comparing personalized models (trained on multiple recipes per user) versus task-specific models, measuring prediction accuracy and user satisfaction across different profile categories.

## Limitations

- Binary annotation scheme may oversimplify complex user mental states, particularly for nuanced emotions like frustration
- Limited task diversity (only 3 recipes) constrains generalizability of consistency findings across different activity domains
- Reliance on ChatGPT introduces potential circularity and may not reflect capabilities of smaller, task-specific models

## Confidence

- High Confidence: High F1 scores for Detail-oriented (0.96), Eager to ask questions (0.92), and Talkative (0.88) behaviors are robust
- Medium Confidence: Poor performance on Frustration and zero precision/recall for Experience may be influenced by annotation quality and prompt engineering
- Low Confidence: Consistency analysis across tasks provides suggestive patterns but lacks statistical power due to small number of recipes

## Next Checks

1. Test the same evaluation pipeline with multiple prompt formulations for each user profile category to determine whether performance differences are robust to prompt engineering or reflect fundamental LLM capabilities.

2. Compare ChatGPT performance with smaller, task-specific fine-tuned models using the same WTaG-extended dataset to isolate whether failures on Frustration and Experience detection are due to model architecture limitations or dataset characteristics.

3. Replicate the consistency analysis using a larger set of diverse tasks (minimum 6-8) to validate whether observed patterns of high consistency for Frustration/Detail-orientation and low consistency for tool familiarity generalize beyond the initial three recipes.