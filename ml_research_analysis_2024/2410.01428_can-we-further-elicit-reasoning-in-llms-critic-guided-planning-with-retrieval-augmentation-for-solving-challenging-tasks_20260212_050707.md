---
ver: rpa2
title: Can We Further Elicit Reasoning in LLMs? Critic-Guided Planning with Retrieval-Augmentation
  for Solving Challenging Tasks
arxiv_id: '2410.01428'
source_url: https://arxiv.org/abs/2410.01428
tags:
- reasoning
- step
- critic
- cr-planner
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of solving complex reasoning
  and domain-knowledge-intensive tasks, such as competitive programming and mathematics,
  where large language models often struggle due to reasoning errors and irrelevant
  knowledge retrieval. The core method, Critic-guided planning with Retrieval-augmentation
  (CR-Planner), introduces a novel framework that uses fine-tuned critic models to
  guide both reasoning and retrieval processes through iterative planning.
---

# Can We Further Elicit Reasoning in LLMs? Critic-Guided Planning with Retrieval-Augmentation for Solving Challenging Tasks

## Quick Facts
- arXiv ID: 2410.01428
- Source URL: https://arxiv.org/abs/2410.01428
- Reference count: 37
- Key outcome: CR-Planner achieves 10.06% average improvement over baselines on challenging reasoning and retrieval tasks

## Executive Summary
This paper introduces CR-Planner, a novel framework that addresses the challenge of solving complex reasoning and domain-knowledge-intensive tasks where large language models often struggle due to reasoning errors and irrelevant knowledge retrieval. The method employs fine-tuned critic models to guide both reasoning and retrieval processes through iterative planning, using Monte Carlo Tree Search to train critics that evaluate and select the most promising sub-goals and their executions. CR-Planner significantly outperforms baselines on competitive programming, theorem-driven math reasoning, and complex domain retrieval tasks, demonstrating its effectiveness in improving both reasoning and retrieval processes.

## Method Summary
CR-Planner uses fine-tuned critic models to guide both reasoning and retrieval through iterative planning, preventing error propagation by catching and correcting mistakes early. The framework employs Monte Carlo Tree Search to collect comprehensive training data for critic models by exploring long-term impacts of action sequences. It separates reasoning and retrieval guidance to optimize both processes independently while maintaining flexibility across different base models. The method is validated on challenging tasks including competitive programming, theorem-driven math reasoning, and complex domain retrieval, where it significantly outperforms baselines with a 10.06% average improvement.

## Key Results
- Achieves 17.59% accuracy on USACO competitive programming tasks
- Achieves 53.40% accuracy on TheoremQA-Math theorem-driven math reasoning
- Achieves 29.51 nDCG@10 on StackBio complex domain retrieval tasks
- Demonstrates 10.06% average improvement over baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CR-Planner uses fine-tuned critic models to guide both reasoning and retrieval through iterative planning, preventing error propagation.
- Mechanism: At each step, a sub-goal critic selects the next action (reason, generate query, or retrieve) based on reward scores, then an execution critic selects the optimal execution (rationale, query, or document). This structured decision-making ensures that errors are caught and corrected early rather than cascading through the solution process.
- Core assumption: The critic models can accurately evaluate the long-term impact of actions and distinguish between good and bad execution choices.
- Evidence anchors:
  - [abstract] "CR-Planner significantly outperforms baselines, highlighting its effectiveness in addressing challenging problems by improving both reasoning and retrieval."
  - [section 2.2] "The critic models estimate the rewards and guide the decision-making process by encouraging actions that contribute the most towards solving the MDP."
- Break condition: If critic models cannot accurately estimate long-term rewards or become biased toward suboptimal actions, planning fails to prevent error propagation.

### Mechanism 2
- Claim: Monte Carlo Tree Search (MCTS) collects comprehensive training data for critic models by exploring long-term impacts of action sequences.
- Mechanism: MCTS simulates multiple possible trajectories from each state, collecting pairwise comparisons between chosen and rejected observations. This provides rich data about which actions lead to successful outcomes versus failures, enabling critic models to learn effective evaluation criteria.
- Core assumption: MCTS can reliably estimate long-term expected rewards by comparing simulated outcomes with gold labels.
- Evidence anchors:
  - [section 2.3] "MCTS estimates long-term expected rewards at each step by comparing simulated outcomes with gold labels and propagates the rewards back to the previous steps."
- Break condition: If MCTS simulations don't accurately reflect real-world outcomes or exploration space is too large, collected data may be insufficient for training effective critics.

### Mechanism 3
- Claim: Separating reasoning and retrieval guidance allows CR-Planner to optimize both processes independently while maintaining flexibility across different base models.
- Mechanism: CR-Planner uses distinct critic models for sub-goal selection (gg) and execution selection (ge), with ge having variants for different execution types. This separation allows each critic to specialize in its domain while the framework remains agnostic to the underlying generator model.
- Core assumption: Specialized critics can outperform general-purpose evaluation without requiring retraining of the base generator model.
- Evidence anchors:
  - [section 4.2] "Compared to previous methods like Self-RAG (Asai et al., 2024), CR-Planner does not require fine-tuning the base model. This flexibility allows CR-Planner to be applied across various base models, whether open-source or closed-source."
- Break condition: If overhead of maintaining multiple specialized critics outweighs benefits, or separation creates coordination problems between critic types.

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation
  - Why needed here: The problem is formally defined as an MDP with states, actions, transition functions, and rewards, providing mathematical framework for planning and critic training.
  - Quick check question: What are the five components of the MDP tuple (S, As, P, R, T) used in CR-Planner?

- Concept: Monte Carlo Tree Search (MCTS) and Upper Confidence Bound (UCB1)
  - Why needed here: MCTS is used to collect training data by exploring action sequences and estimating long-term rewards, with UCB1 balancing exploration and exploitation during selection.
  - Quick check question: How does the UCB1 formula vi/ni + c*sqrt(ln np/ni) balance exploration vs exploitation in MCTS?

- Concept: Pairwise ranking loss for training critic models
  - Why needed here: Critic models are trained using pairwise comparisons between chosen and rejected observations, learning to distinguish high-reward from low-reward actions.
  - Quick check question: What type of loss function is used to optimize the critic model parameters based on pairwise comparisons?

## Architecture Onboarding

- Component map:
  - Generator model (GPT-4) -> Creates rationales, queries, and answers
  - Sub-goal critic (gg) -> Selects next action type (reason, query, retrieve)
  - Execution critics (ge_reason, ge_query, ge_doc) -> Select best execution for each sub-goal type
  - Retriever (BM25) -> Fetches documents based on generated queries
  - MCTS system -> Collects training data through simulation
  - Fine-tuning pipeline -> Trains critic models on collected data

- Critical path:
  1. Problem input → Sub-goal selection → Action type determined
  2. If reason/query: sample candidates → Execution selection → Best candidate chosen
  3. If retrieve: generate query → Document retrieval → Top documents selected
  4. Continue until answer found or max steps reached

- Design tradeoffs:
  - Using fine-tuned critics vs. prompting GPT-4 directly: Critics provide domain-specific expertise but require training data collection
  - Sampling multiple candidates vs. single generation: Sampling improves quality but increases cost and latency
  - Separate critics vs. unified critic: Specialization improves accuracy but adds complexity

- Failure signatures:
  - Sub-goal critic consistently selects same action type → May indicate bias or poor reward estimation
  - Execution critics fail to distinguish between candidates → May indicate insufficient training data or poor reward signal
  - MCTS fails to explore diverse trajectories → May indicate exploration parameters need adjustment

- First 3 experiments:
  1. Test CR-Planner on a simple reasoning task without retrieval to validate basic planning functionality
  2. Evaluate critic model accuracy on held-out pairwise comparisons to ensure proper training
  3. Run ablation study with different sampling numbers to find optimal balance between quality and cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CR-Planner vary with different sizes of external knowledge sources (e.g., textbooks vs. problem banks)?
- Basis in paper: [explicit] The paper mentions using both textbooks and problem banks as external knowledge sources for competitive programming tasks, but does not explore how varying the size or type of these sources affects performance.
- Why unresolved: The paper does not provide experiments or analysis on the impact of different sizes or types of external knowledge sources on CR-Planner's performance.
- What evidence would resolve it: Conducting experiments with varying sizes and types of external knowledge sources and analyzing the resulting performance of CR-Planner would provide insights into the optimal configuration for different domains.

### Open Question 2
- Question: Can CR-Planner be effectively applied to other domains beyond competitive programming, mathematics, and retrieval tasks?
- Basis in paper: [inferred] While the paper demonstrates CR-Planner's effectiveness in three specific domains, it does not explore its applicability to other areas that require complex reasoning and knowledge retrieval.
- Why unresolved: The paper focuses on a limited set of domains and does not provide evidence or analysis of CR-Planner's performance in other fields.
- What evidence would resolve it: Applying CR-Planner to diverse domains (e.g., legal reasoning, medical diagnosis, scientific research) and evaluating its performance would determine its generalizability and potential limitations.

### Open Question 3
- Question: How does the fine-tuning process of critic models affect their performance, and can it be further optimized?
- Basis in paper: [explicit] The paper mentions fine-tuning critic models with LoRA and using them for guiding sub-goal and execution selection, but does not delve into the specifics of the fine-tuning process or potential optimizations.
- Why unresolved: The paper does not provide details on the fine-tuning methodology, hyperparameter tuning, or alternative fine-tuning strategies that could improve critic model performance.
- What evidence would resolve it: Conducting experiments with different fine-tuning approaches, hyperparameters, and architectures for critic models, and analyzing their impact on CR-Planner's performance would provide insights into optimal fine-tuning strategies.

## Limitations
- Limited exploration of how different sizes and types of external knowledge sources affect performance
- Focus on only three specific domains without testing generalizability to other fields
- Lack of detailed analysis on the fine-tuning process and potential optimizations for critic models

## Confidence
- Mechanism 1: Medium - Experimental results show improvement but limited ablation studies
- Mechanism 2: Medium - MCTS described but implementation details unclear
- Mechanism 3: Medium - Flexibility demonstrated but coordination challenges not explored
- Overall: Medium - Sound theoretical framework but practical implementation details and edge cases remain unclear

## Next Checks
1. Evaluate critic model performance on out-of-distribution problems to test generalization
2. Conduct ablation studies removing MCTS data collection to assess its true contribution
3. Compare performance against more recent competitive programming solvers to establish relative standing