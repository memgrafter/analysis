---
ver: rpa2
title: 'MASALA: Model-Agnostic Surrogate Explanations by Locality Adaptation'
arxiv_id: '2408.10085'
source_url: https://arxiv.org/abs/2408.10085
tags:
- explanations
- linear
- clustering
- instance
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MASALA addresses the challenge of defining appropriate locality
  sizes for local XAI methods like LIME and CHILLI. It automatically determines impactful
  local model behavior for each instance by clustering the input space into regions
  of linear behavioral trends, then fitting a linear surrogate model to points sharing
  the same linear region.
---

# MASALA: Model-Agnostic Surrogate Explanations by Locality Adaptation

## Quick Facts
- arXiv ID: 2408.10085
- Source URL: https://arxiv.org/abs/2408.10085
- Authors: Saif Anwar; Nathan Griffiths; Abhir Bhalerao; Thomas Popham
- Reference count: 40
- Primary result: MASALA achieves higher fidelity (35.94 vs 66.95 average error for PHM08) and perfect consistency compared to LIME and CHILLI

## Executive Summary
MASALA addresses the challenge of defining appropriate locality sizes for local XAI methods like LIME and CHILLI. It automatically determines impactful local model behavior for each instance by clustering the input space into regions of linear behavioral trends, then fitting a linear surrogate model to points sharing the same linear region. Experiments on PHM08 and MIDAS datasets show MASALA achieves higher fidelity and perfect consistency compared to LIME and CHILLI, without requiring manual locality hyperparameters.

## Method Summary
MASALA uses a novel clustering technique to identify linear regions of model behavior, then fits a Multivariate Linear Regression (MLR) surrogate model to points sharing the same linear region. Clustering uses a custom distance measure combining local linearity, feature value, and local density. The method automatically determines the number of clusters through constraint satisfaction, ensuring clusters are non-overlapping, sufficiently dense, and well-covered. The approach is evaluated on two datasets: PHM08 (turbofan engine RUL prediction) and MIDAS (UK weather observations).

## Key Results
- MASALA achieves explanation fidelity with 35.94 average error on PHM08 compared to 66.95 for LIME and CHILLI
- MASALA demonstrates perfect consistency (0 standard deviation) compared to LIME (0.4616) and CHILLI (0.3168)
- The method eliminates the need for manual locality hyperparameter tuning while improving explanation quality

## Why This Works (Mechanism)

### Mechanism 1
MASALA automatically finds the appropriate locality size for each instance by clustering the input space into regions of linear behavioral trends. The method uses local linear K-medoids clustering with pairwise dissimilarity that combines local linearity, feature distance, and neighborhood density. This works because base model behavior is locally linear for some region around all data instances. If the base model exhibits highly non-linear behavior with no local linearity, the clustering approach would fail to find meaningful linear regions.

### Mechanism 2
The pairwise dissimilarity measure ensures points with similar local linearity and neighborhood characteristics are clustered together. Dissimilarity is calculated as weighted combination of LLR parameter differences, feature distance, and neighborhood size differences, all normalized to [0,1]. This works because points that are close in feature space and have similar local linear regression parameters experience similar model behavior. If local neighborhoods are too sparse or contain outliers, the LLR parameters may not represent true local behavior.

### Mechanism 3
Automatic determination of the number of clusters ùêæ through constraint satisfaction produces clusters that are non-overlapping, sufficiently dense, and well-covered. The algorithm starts with large ùêæ, iteratively refines clustering by checking sparsity and coverage constraints, and merges clusters when they violate constraints. This works because there exists a natural number of linear regions that can be identified through constraint satisfaction without manual tuning. If the data distribution is highly irregular with no clear linear regions, the constraint satisfaction may not converge to a meaningful solution.

## Foundational Learning

- **Local linear modeling and surrogate explanations**: Why needed here - MASALA builds on the idea that local linear approximations can explain complex model behavior, but improves by automatically determining appropriate locality. Quick check - What is the key difference between MASALA's approach to locality and LIME's approach?

- **Clustering algorithms and distance metrics**: Why needed here - The method relies on K-medoids clustering with a custom pairwise dissimilarity measure that considers local linearity. Quick check - How does the pairwise dissimilarity measure differ from standard Euclidean distance in K-medoids?

- **Model fidelity and consistency metrics**: Why needed here - MASALA is evaluated using explanation fidelity (absolute error) and consistency (standard deviation of feature importance scores). Quick check - Why is consistency an important metric for explanation methods, and how does MASALA achieve perfect consistency?

## Architecture Onboarding

- **Component map**: Feature clustering ‚Üí ùêæ determination ‚Üí Instance selection ‚Üí Linear surrogate training ‚Üí Explanation output
- **Critical path**: The method clusters each feature dimension against model predictions into linear regions, automatically determines the number of clusters through constraint satisfaction, selects instances for training linear surrogate models, and generates explanations through linear regression coefficients
- **Design tradeoffs**: Deterministic clustering ensures consistency but may miss valid alternative explanations; adaptive locality improves fidelity but increases computational cost; linear assumptions simplify explanations but may not capture all model behavior
- **Failure signatures**: Poor clustering results in explanations with low fidelity; constraint satisfaction failure leads to inappropriate ùêæ values; sparse neighborhoods cause unreliable LLR parameters
- **First 3 experiments**:
  1. Run MASALA on a simple synthetic dataset with known linear regions and verify that clustering correctly identifies these regions
  2. Compare MASALA explanations to LIME explanations on a regression dataset, varying kernel width for LIME to demonstrate locality sensitivity
  3. Test MASALA on a dataset with highly non-linear relationships to identify break conditions for the linear assumption

## Open Questions the Paper Calls Out

### Open Question 1
How does MASALA's explanation fidelity compare to LIME and CHILLI when applied to datasets with different types of non-linear relationships between features and predictions? The paper shows MASALA outperforms LIME and CHILLI on PHM08 and MIDAS datasets, but doesn't test other types of non-linear relationships. Testing on more diverse datasets would be needed to establish generalizability.

### Open Question 2
How does the choice of neighborhood distance threshold affect MASALA's explanation fidelity and consistency? The paper mentions using a distance threshold in the feature space to define neighborhoods for weighted local linear regression, but doesn't systematically explore its impact. Experiments varying the neighborhood distance threshold and measuring the resulting impact on explanation fidelity and consistency across different datasets would resolve this.

### Open Question 3
How does MASALA's performance scale with high-dimensional input spaces compared to LIME and CHILLI? The paper mentions MASALA's computational cost scales linearly with feature dimensions, but doesn't empirically compare scaling behavior with other methods. Experiments comparing explanation fidelity, consistency, and computational time of MASALA, LIME, and CHILLI across datasets with varying numbers of features would resolve this.

## Limitations

- MASALA relies heavily on the assumption that base model behavior is locally linear, which may not hold for complex models with highly non-linear decision boundaries
- The clustering approach requires evaluating local linear models for each feature dimension, which could be computationally expensive for high-dimensional data
- The constraint satisfaction algorithm for determining the number of clusters may struggle with irregular data distributions or noise

## Confidence

- **Mechanism 1 (Adaptive locality)**: Medium - The core idea is sound but relies on the linear assumption being valid across all data instances
- **Mechanism 2 (Pairwise dissimilarity)**: High - The mathematical formulation is clear and well-defined, though empirical validation is limited
- **Mechanism 3 (Automatic K determination)**: Medium - The algorithm is described but lacks detailed validation of constraint satisfaction in edge cases

## Next Checks

1. **Synthetic data validation**: Test MASALA on synthetic datasets with known linear regions of varying complexity to verify clustering accuracy and locality adaptation
2. **Non-linear model stress test**: Apply MASALA to highly non-linear models (e.g., deep neural networks on image data) to identify break conditions for the linear assumption
3. **Computational complexity analysis**: Measure runtime and memory usage across datasets of increasing dimensionality to quantify the computational overhead compared to LIME/CHILLI