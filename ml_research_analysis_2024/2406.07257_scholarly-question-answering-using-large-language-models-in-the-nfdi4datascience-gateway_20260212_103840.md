---
ver: rpa2
title: Scholarly Question Answering using Large Language Models in the NFDI4DataScience
  Gateway
arxiv_id: '2406.07257'
source_url: https://arxiv.org/abs/2406.07257
tags:
- search
- scholarly
- gateway
- https
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a scholarly question answering system built
  on top of the NFDI4DataScience (NFDI4DS) Gateway using a Retrieval Augmented Generation
  (RAG) approach. The NFDI4DS Gateway provides federated search across multiple scientific
  databases, and the RAG-based QA system enhances filtering capabilities and enables
  conversational engagement with search results.
---

# Scholarly Question Answering using Large Language Models in the NFDI4DataScience Gateway

## Quick Facts
- arXiv ID: 2406.07257
- Source URL: https://arxiv.org/abs/2406.07257
- Reference count: 33
- RAG-based QA system achieves ROUGE-1, ROUGE-L, and BLEU-1 scores of 4.21%, 2.92%, and 38.94% respectively on the AI-QA dataset

## Executive Summary
This paper presents a scholarly question answering system built on the NFDI4DataScience (NFDI4DS) Gateway using a Retrieval Augmented Generation (RAG) approach. The system enhances the Gateway's federated search capabilities by enabling conversational engagement with search results across multiple scientific databases. The authors evaluate both the Gateway's performance and the QA system's effectiveness using automatically constructed datasets, demonstrating the potential of RAG-based approaches for scholarly research applications.

## Method Summary
The authors implement a RAG-based scholarly QA system that integrates with the NFDI4DS Gateway's federated search infrastructure. The system retrieves relevant documents from multiple scientific databases and uses these as context for generating answers to scholarly questions. Evaluation is conducted using two automatically constructed datasets (AI-QA and Comparison-QA) with standard metrics including ROUGE, BLEU, and BERTScore. The Gateway's performance is measured in terms of document retrieval speed and semantic similarity thresholds.

## Key Results
- Gateway retrieves an average of 123 documents within 4.93 seconds per query
- Achieves 50% semantic similarity at threshold of 0.3
- RAG-based scholarly QA system achieves ROUGE-1, ROUGE-L, and BLEU-1 scores of 4.21%, 2.92%, and 38.94% on AI-QA dataset

## Why This Works (Mechanism)
The RAG approach combines information retrieval with generative language models to provide contextually relevant answers. By first retrieving relevant scholarly documents and then using them as context for generation, the system can leverage both the precision of retrieval-based methods and the flexibility of generation-based approaches. This hybrid approach is particularly suited for scholarly domains where domain-specific knowledge is distributed across multiple specialized databases.

## Foundational Learning
- **Retrieval Augmented Generation (RAG)**: Combines information retrieval with text generation to improve answer quality and factual accuracy. Needed for scholarly QA where domain-specific knowledge is required. Quick check: Verify retrieval step returns relevant documents before generation.
- **Federated Search**: Enables querying multiple distributed databases through a single interface. Essential for scholarly research spanning multiple domains. Quick check: Test search across different database types for consistency.
- **Semantic Similarity Thresholds**: Determines relevance of retrieved documents through vector similarity measures. Critical for filtering relevant content in scholarly domains. Quick check: Validate similarity scores against human relevance judgments.

## Architecture Onboarding

**Component Map:**
NFDI4DS Gateway -> Document Retriever -> Context Filter -> LLM Generator -> Answer Output

**Critical Path:**
Query -> Federated Search -> Document Retrieval -> Semantic Filtering -> Context Generation -> Answer Generation

**Design Tradeoffs:**
- Retrieval vs Generation balance: More retrieval improves accuracy but increases latency
- Semantic threshold selection: Higher thresholds improve precision but may reduce recall
- Dataset construction: Automatic generation enables large-scale evaluation but may introduce biases

**Failure Signatures:**
- Low retrieval quality manifests as irrelevant context leading to poor answers
- Semantic filtering issues result in either too many irrelevant documents or missing relevant ones
- Generation problems appear as hallucinated or factually incorrect answers

**First Experiments:**
1. Test Gateway retrieval speed and document count with benchmark queries
2. Validate semantic similarity threshold with manual relevance assessment
3. Evaluate answer quality with domain expert review on sample questions

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Low evaluation scores (particularly for Comparison-QA dataset) suggest limited generation quality
- Use of automatically constructed datasets may introduce evaluation biases
- Absence of human evaluation limits assessment of practical utility in real scholarly research contexts

## Confidence
- Gateway performance metrics (123 documents, 4.93s response time): **High**
- RAG-based QA system scores: **Medium**
- System utility for scholarly research: **Low**

## Next Checks
1. Conduct human evaluation studies with domain experts to assess answer quality and relevance
2. Test system with manually curated scholarly datasets to verify performance consistency
3. Perform error analysis on failed queries to identify primary bottlenecks in retrieval or generation components