---
ver: rpa2
title: Training Language Models to Win Debates with Self-Play Improves Judge Accuracy
arxiv_id: '2409.16636'
source_url: https://arxiv.org/abs/2409.16636
tags:
- quote
- judge
- debate
- position
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Training models to win debates with self-play improves judge accuracy.
  In reading comprehension tasks with information asymmetry, models trained via debate
  yield 4% higher judge accuracy compared to supervised fine-tuning, while consultancy
  models show no improvement.
---

# Training Language Models to Win Debates with Self-Play Improves Judge Accuracy

## Quick Facts
- **arXiv ID**: 2409.16636
- **Source URL**: https://arxiv.org/abs/2409.16636
- **Reference count**: 40
- **Key outcome**: Training models to win debates with self-play improves judge accuracy

## Executive Summary
This paper demonstrates that training language models through self-play debate significantly improves judge accuracy compared to traditional supervised fine-tuning. The approach involves having models debate reading comprehension questions where each side has access to different subsets of information, forcing them to construct compelling arguments to convince a judge. The study finds that debate-trained models achieve 4% higher accuracy than supervised fine-tuning models on reading comprehension tasks with information asymmetry, while consultancy models show no improvement. The research suggests that the side-by-side comparison of arguments and prevention of judge exploitation are more important than explicit refutation in improving model performance.

## Method Summary
The authors employ a debate training framework where models engage in structured argumentation about reading comprehension questions. Each debating model receives a different subset of information about a given passage, creating an asymmetric information scenario. Models are trained to construct arguments that will convince a judge model of their correctness, with the judge having access to all available information. The training process uses reinforcement learning from debate outcomes, where models are rewarded based on their success in convincing judges. The framework is compared against traditional supervised fine-tuning and a consultancy model where one model provides information to help the judge without adversarial argumentation.

## Key Results
- Debate-trained models achieve 4% higher judge accuracy compared to supervised fine-tuning on reading comprehension tasks with information asymmetry
- Consultancy models show no improvement over supervised fine-tuning, suggesting adversarial argumentation is crucial for performance gains
- Debate training encourages more informative argumentation and stronger evidence use compared to other training methods
- Explicit refutation appears less important than side-by-side argument comparison and preventing judge exploitation

## Why This Works (Mechanism)
The debate framework works by forcing models to construct arguments that are not only correct but also persuasive to a judge with full information. This creates a selection pressure for models to provide comprehensive evidence and address potential counterarguments implicitly. The self-play aspect allows models to explore the space of possible arguments and refine their strategies based on what convinces judges most effectively. The asymmetric information setup ensures that models must rely on reasoning and evidence presentation rather than simply having access to the correct answer.

## Foundational Learning
- **Information asymmetry in debates**: Understanding how having different information subsets affects argumentation strategies and why it's needed to prevent models from simply sharing all information; quick check: verify models cannot access complete information during debate
- **Self-play reinforcement learning**: Grasping how models learn through iterative competition against themselves and why it's needed to explore argument space effectively; quick check: confirm models improve performance over training iterations
- **Judge modeling**: Understanding how to construct effective judge models that can evaluate arguments fairly and why it's needed to provide proper training signals; quick check: validate judge accuracy on human-annotated ground truth
- **Argument structure and evidence use**: Recognizing what makes arguments persuasive and why it's needed to ensure debate training produces useful outputs; quick check: analyze argument quality using established metrics
- **Debate versus consultancy paradigms**: Distinguishing between adversarial debate and collaborative information provision and why it's needed to understand different training dynamics; quick check: compare performance across both paradigms

## Architecture Onboarding

**Component map**: Debate models (Pro and Con) -> Judge model -> Reward signal -> Debate models (training loop)

**Critical path**: Information input → Argument generation → Judge evaluation → Reward calculation → Model update

**Design tradeoffs**: The choice between debate and consultancy paradigms represents a fundamental tradeoff between adversarial learning (which may produce more robust arguments but requires more complex training) and collaborative learning (which is simpler but less effective). The asymmetric information setup trades off between realism and training stability.

**Failure signatures**: Poor judge accuracy may indicate insufficient judge model capability, inadequate information asymmetry, or problems with the debate format itself. No improvement over supervised fine-tuning suggests the debate mechanism isn't providing useful training signals or the self-play dynamics aren't working effectively.

**First experiments**:
1. Test judge accuracy on debate-trained versus supervised models on held-out reading comprehension tasks
2. Compare argument quality metrics (evidence use, informativeness) between debate-trained and baseline models
3. Evaluate performance when varying the degree of information asymmetry between debating models

## Open Questions the Paper Calls Out
None

## Limitations
- The 4% accuracy improvement, while statistically significant, represents a moderate gain that may not generalize across all task types or model scales
- Experiments were conducted primarily on reading comprehension tasks with synthetic information asymmetry, which may not capture real-world debate complexity
- The consultancy model showed no improvement, but the reasons remain unclear - whether due to insufficient training signal, architectural limitations, or specific task setup
- The analysis of refutation versus side-by-side comparison is based on qualitative observations rather than systematic ablation studies

## Confidence
- **High confidence**: The core finding that debate training improves judge accuracy over supervised fine-tuning on the tested tasks
- **Medium confidence**: The observation that debate training produces more informative arguments with stronger evidence use
- **Low confidence**: The claim that explicit refutation is less important than side-by-side comparison and preventing judge exploitation

## Next Checks
1. Test debate training across a broader range of task types including multi-hop reasoning, mathematical problem-solving, and open-domain question answering to assess generalizability
2. Conduct ablation studies systematically removing different debate components (refutation, side-by-side comparison, self-play) to identify which mechanisms drive improvements
3. Evaluate model performance with varying judge capabilities (including weaker judges) to test the robustness of debate training across different scalability regimes