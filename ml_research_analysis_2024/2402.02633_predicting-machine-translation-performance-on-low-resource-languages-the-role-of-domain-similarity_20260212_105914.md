---
ver: rpa2
title: 'Predicting Machine Translation Performance on Low-Resource Languages: The
  Role of Domain Similarity'
arxiv_id: '2402.02633'
source_url: https://arxiv.org/abs/2402.02633
tags:
- language
- corpus
- fine-tuning
- size
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates predicting Machine Translation (MT) performance
  for low-resource languages (LRLs) using classical regression models. Three factors
  are examined: fine-tuning corpus size, domain similarity between fine-tuning and
  testing corpora, and language similarity between source and target languages.'
---

# Predicting Machine Translation Performance on Low-Resource Languages: The Role of Domain Similarity

## Quick Facts
- arXiv ID: 2402.02633
- Source URL: https://arxiv.org/abs/2402.02633
- Reference count: 22
- Key outcome: Domain similarity, measured by Jensen-Shannon divergence, is the most significant factor in predicting MT performance for low-resource languages.

## Executive Summary
This study investigates predicting Machine Translation (MT) performance for low-resource languages (LRLs) using classical regression models. Three factors are examined: fine-tuning corpus size, domain similarity between fine-tuning and testing corpora, and language similarity between source and target languages. Experiments are conducted using the mBART model and five South Asian LRLs. Results show that domain similarity, measured by Jensen-Shannon divergence, has the most significant impact on MT performance prediction. The best predictor functions are polynomial regression (degree 3) for domain similarity and scaling law for corpus size. The study demonstrates that domain similarity is a critical factor in predicting MT performance for LRLs.

## Method Summary
The study employs classical regression models to predict MT performance for low-resource languages, focusing on three factors: fine-tuning corpus size, domain similarity (measured by Jensen-Shannon divergence), and language similarity (using six lang2vec distance features). Experiments are conducted using the mBART model and five South Asian LRLs. The regression models include linear, polynomial, logarithmic, and scaling law functions, evaluated through k-fold cross-validation and statistical tests for normality and homoscedasticity. Feature importance is assessed using Pearson correlation, weight analysis, and Random Forest methods.

## Key Results
- Domain similarity, measured by Jensen-Shannon divergence, is the most significant factor in predicting MT performance for low-resource languages.
- Polynomial regression (degree 3) is the best predictor function for domain similarity, while scaling law is optimal for corpus size.
- Language similarity features from lang2vec have low correlations with spBLEU scores and are less important in regression models.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain similarity, measured by Jensen-Shannon divergence (JSD), is the most significant factor in predicting MT performance for low-resource languages.
- Mechanism: JSD quantifies the dissimilarity between the fine-tuning and testing corpora. Higher JSD indicates a larger domain shift, which negatively impacts translation performance. The study shows that JSD has the strongest correlation with spBLEU scores and the highest importance in regression models.
- Core assumption: The domains of the fine-tuning and testing corpora are well-represented by their word frequency distributions, and JSD accurately captures the semantic and stylistic differences between these domains.
- Evidence anchors:
  - [abstract] "Our results indicate that domain similarity has the most critical impact on predicting the performance of Machine Translation models."
  - [section 3.2] "We refer to this situation as domain shift where domain is a 'distribution over language characterizing a given topic or genre' (Gururangan et al., 2020)."
  - [section 3.2] "We refer to the domain feature, ϕd, as the JSD between fine-tuning and testing corpora, that is, ϕd = j = J SD(t, τ)."
- Break condition: If the corpora contain significant amounts of out-of-vocabulary words or if the JSD calculation does not adequately capture the domain shift due to limitations in the tokenization or stopword removal process.

### Mechanism 2
- Claim: The size of the fine-tuning corpus, when normalized and modeled using a scaling law, contributes to predicting MT performance.
- Mechanism: The scaling law suggests a power-law relationship between the amount of fine-tuning data and model performance. Normalizing the corpus size allows for comparison across different languages and corpora. The study finds that the scaling law model has the lowest RMSE for predicting spBLEU based on corpus size.
- Core assumption: The relationship between corpus size and MT performance follows a power law, and the normalization method appropriately scales the corpus sizes for comparison.
- Evidence anchors:
  - [section 3.1] "It has been observed that the cross-entropy loss of MT models behaves as a power-law with respect to the amount of fine-tuning data (Gordon et al., 2021; Ghorbani et al., 2021; Kaplan et al., 2020)."
  - [section 4.1] "We achieve this normalization by employing a minimum-maximum scaling method, which constrains it to a range of 0 ≤ ˜s ≤ 1."
  - [section 5.1] "On the best partitioning scheme, the scaling law model has the lowest RMSE (Figure 1a, RMSE = 2.2998)."
- Break condition: If the relationship between corpus size and performance deviates significantly from a power law, or if the normalization method introduces bias or fails to account for important differences in corpus characteristics.

### Mechanism 3
- Claim: Language similarity, measured by multiple distance features, has a weaker impact on MT performance prediction compared to domain similarity and corpus size.
- Mechanism: Language similarity features, such as geographical, genetic, syntactic, phonological, inventory, and featural distances, are used to quantify the similarity between the source and target languages. However, the study finds that these features have low correlations with spBLEU scores and are less important in regression models.
- Core assumption: The language similarity features accurately capture the relevant linguistic differences between the languages, and these differences have a measurable impact on translation performance.
- Evidence anchors:
  - [section 3.3] "To measure language similarity, we utilize six distance features queried from URIEL Typological Database using lang2vec (Littell et al., 2017)."
  - [section 5.3] "Surprisingly, all six language features show low correlations with spBLEU."
  - [section 6] "Furthermore, the high degree of similarity between the languages in our data set rendered the effectiveness of language features from lang2vec as predictors."
- Break condition: If the language similarity features are not representative of the actual linguistic differences that impact translation performance, or if the languages in the study are too similar to each other, making it difficult to distinguish their effects on performance.

## Foundational Learning

- Concept: Domain shift and its impact on MT performance
  - Why needed here: Understanding domain shift is crucial for interpreting the results related to domain similarity and its importance in predicting MT performance.
  - Quick check question: What is domain shift, and how does it affect the performance of MT models when the testing corpus is from a different domain than the fine-tuning corpus?

- Concept: Jensen-Shannon divergence (JSD) and its application in measuring domain similarity
  - Why needed here: JSD is the primary metric used to quantify domain similarity in this study, and understanding its calculation and interpretation is essential for evaluating the results.
  - Quick check question: How is JSD calculated, and what does a higher JSD value indicate about the similarity between two corpora?

- Concept: Scaling laws in machine learning and their application to MT performance prediction
  - Why needed here: The study employs a scaling law model to predict MT performance based on the size of the fine-tuning corpus, and understanding the concept of scaling laws is necessary for interpreting the results.
  - Quick check question: What is a scaling law, and how does it relate to the relationship between the amount of training data and model performance in machine learning?

## Architecture Onboarding

- Component map:
  - Data collection and preprocessing: Gathering corpora, tokenization, stopword removal, and JSD calculation.
  - Feature engineering: Normalizing corpus size, calculating language similarity features using lang2vec.
  - Regression modeling: Implementing and evaluating various regression models (linear, polynomial, logarithmic, scaling law) using single and multiple factors.
  - Statistical analysis: Assessing model reliability through normality and homoscedasticity tests, ranking feature importance using correlation, weight analysis, and Random Forest.

- Critical path:
  1. Collect and preprocess the corpora (tokenization, stopword removal).
  2. Calculate JSD between fine-tuning and testing corpora.
  3. Normalize corpus sizes and calculate language similarity features.
  4. Implement and evaluate regression models using single factors.
  5. Partition data based on experimental settings and re-evaluate models.
  6. Implement and evaluate regression models using multiple factors.
  7. Perform statistical analysis on model residuals and rank feature importance.

- Design tradeoffs:
  - Single-factor vs. multi-factor regression models: Single-factor models provide insights into individual factor impacts, while multi-factor models capture interactions but may be more complex to interpret.
  - Choice of regression functions: Different functions (linear, polynomial, logarithmic, scaling law) may be better suited for different factors and data characteristics.
  - Data partitioning schemes: Different partitioning strategies (by fine-tuning corpus, testing corpus, target language) can reveal different patterns and impact model performance.

- Failure signatures:
  - High RMSE values: Indicate poor model fit and inaccurate predictions.
  - Non-normal or heteroscedastic residuals: Suggest model assumptions are violated, potentially leading to unreliable predictions.
  - Low correlation between features and spBLEU: Indicate that the features may not be capturing the relevant information for predicting performance.

- First 3 experiments:
  1. Implement and evaluate a linear regression model using only the normalized corpus size as a predictor.
  2. Implement and evaluate a polynomial regression model (degree 3) using only the JSD as a predictor.
  3. Implement and evaluate a scaling law model using the normalized corpus size as a predictor, and compare its performance to the linear regression model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the prediction accuracy of MT performance vary across different low-resource language families beyond South Asian languages?
- Basis in paper: [explicit] The study focused on five South Asian languages, noting that "the high similarity among our South Asian languages could be a factor" in the observed low correlations between language features and performance.
- Why unresolved: The paper's limited scope to South Asian languages restricts generalizability to other language families with potentially different linguistic properties.
- What evidence would resolve it: Conducting similar experiments on diverse low-resource language families (e.g., African, Native American, Austronesian) and comparing the impact of domain similarity and other factors across these families.

### Open Question 2
- Question: What is the optimal polynomial degree for modeling the relationship between Jensen-Shannon divergence and MT performance in diverse linguistic contexts?
- Basis in paper: [explicit] The study found that "polynomial regression with degree 3 has the lowest RMSE" for JSD in South Asian languages, but also noted potential overfitting concerns.
- Why unresolved: The optimal degree may vary depending on the specific linguistic characteristics and domain similarities of different language pairs, which were not fully explored in this study.
- What evidence would resolve it: Systematic testing of polynomial regression models with varying degrees across multiple language pairs and domain similarity levels to determine the most robust degree for diverse contexts.

### Open Question 3
- Question: How does the inclusion of additional linguistic features beyond those provided by lang2vec affect the prediction accuracy of MT performance for low-resource languages?
- Basis in paper: [explicit] The study acknowledged that "the URIEL library may not have sufficient data to provide approximation that accurately describe the LRL" and suggested exploring "dataset-dependent language features."
- Why unresolved: The limited coverage and potential inaccuracies of existing linguistic databases for low-resource languages restrict the exploration of more comprehensive feature sets.
- What evidence would resolve it: Developing and testing new linguistic feature extraction methods tailored to low-resource languages, and comparing their predictive power against existing features in regression models.

## Limitations
- The study's focus on South Asian languages may limit the generalizability of findings to language pairs with greater linguistic diversity.
- The Jensen-Shannon divergence calculation may not fully capture semantic and stylistic differences between domains, especially with out-of-vocabulary words.
- The dataset-dependent language features were not explored, potentially missing important predictors of MT performance.

## Confidence
- **High Confidence**: The finding that domain similarity, measured by JSD, is the most significant factor in predicting MT performance is well-supported by the results across multiple regression models and data partitions.
- **Medium Confidence**: The scaling law model's effectiveness in predicting MT performance based on corpus size is supported by the lowest RMSE in certain data partitions, but may not hold for all low-resource language scenarios.
- **Low Confidence**: The weak impact of language similarity features on MT performance prediction is based on the specific set of South Asian languages studied and may not generalize to more diverse language pairs.

## Next Checks
1. **Replicate the study with a more diverse set of language pairs**: To validate the findings on language similarity, replicate the study with language pairs that have greater linguistic diversity, such as pairs from different language families or with distinct typological features.

2. **Investigate dataset-dependent language features**: Explore the use of dataset-dependent language features, such as type-token ratio (TTR) and subword overlap, to capture additional aspects of language similarity that may be missed by lang2vec features.

3. **Test the robustness of the JSD calculation**: Evaluate the robustness of the JSD calculation by testing its sensitivity to different tokenization methods, stopword lists, and the presence of out-of-vocabulary words. Assess whether alternative domain similarity measures, such as topic modeling or semantic similarity, can provide additional insights.