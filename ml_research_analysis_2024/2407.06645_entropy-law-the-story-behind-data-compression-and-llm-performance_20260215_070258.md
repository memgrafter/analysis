---
ver: rpa2
title: 'Entropy Law: The Story Behind Data Compression and LLM Performance'
arxiv_id: '2407.06645'
source_url: https://arxiv.org/abs/2407.06645
tags:
- data
- compression
- selection
- samples
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the relationship between data selection
  and LLM performance, introducing an "entropy law" that connects model performance
  with data compression ratio and training loss. It proposes ZIP, an efficient, model-free
  data selection method that prioritizes samples with low compression ratios to maximize
  effective information for LLM training.
---

# Entropy Law: The Story Behind Data Compression and LLM Performance

## Quick Facts
- arXiv ID: 2407.06645
- Source URL: https://arxiv.org/abs/2407.06645
- Authors: Mingjia Yin; Chuhan Wu; Yufei Wang; Hao Wang; Wei Guo; Yasheng Wang; Yong Liu; Ruiming Tang; Defu Lian; Enhong Chen
- Reference count: 24
- Key outcome: Introduces ZIP, an efficient data selection method based on compression ratio that outperforms quality-based baselines in LLM training

## Executive Summary
This paper introduces an "entropy law" that connects LLM performance with data compression ratio and training loss, challenging the conventional wisdom that focuses solely on data quality. The law states that model performance is negatively correlated with the compression ratio of training data when average data quality is held constant. Based on this principle, the authors propose ZIP, a model-free data selection method that efficiently selects diverse, low-redundancy subsets using a multi-stage greedy algorithm. Experiments on Mistral-7B and Llama-3-8B during SFT and RLHF stages demonstrate that ZIP achieves higher MT-bench scores while being more computationally efficient than existing approaches.

## Method Summary
The ZIP algorithm uses a three-stage greedy approach to select data subsets with low compression ratios: global selection filters candidates with high information density, coarse-grained local selection reduces redundancy with already selected samples, and fine-grained local selection ensures diversity among selected samples. The method operates on CPU to compute compression ratios using algorithms like DEFLATE, then iteratively selects diverse samples until reaching the token budget. The approach is designed to approximate optimal selection without exhaustive search, trading some approximation quality for significant speed and cost advantages over model-based methods.

## Key Results
- ZIP achieves MT-bench scores of 7.08 vs 6.85 for Mistral-7B in SFT, outperforming quality-based baselines
- The method demonstrates consistent performance improvements across both SFT and RLHF stages
- ZIP enables early detection of performance risks during incremental data updates through monitoring compression ratios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data compression ratio negatively correlates with LLM performance when average data quality is held constant.
- Mechanism: Lower compression ratio indicates higher information density in the dataset, which allows the LLM to learn more diverse and complementary knowledge. Since LLMs function as data compressors, datasets that resist compression contain more varied information patterns, leading to better model generalization.
- Core assumption: The average data quality across different subsets remains relatively uniform, so differences in performance are driven primarily by information redundancy rather than sample-level quality variations.
- Evidence anchors:
  - [abstract] "model performance is negatively correlated to the compression ratio of training data"
  - [section 3] "we can approximately regard the variable Q as a constant... the model performance is correlated with the data compression ratio and training loss"
  - [corpus] Weak - corpus papers discuss compression and entropy but not this specific performance-compression relationship
- Break condition: If the data quality distribution varies significantly across subsets, the correlation between compression ratio and performance may weaken or reverse.

### Mechanism 2
- Claim: Training loss reflects data consistency, which influences how effectively the model can learn the information encoded in the dataset.
- Mechanism: High training loss indicates poor data consistency, where similar contexts have inconsistent outputs. This inconsistency makes it harder for the model to form stable representations, reducing the effective knowledge gained from the data.
- Core assumption: Training loss is primarily determined by data consistency rather than the inherent difficulty of the learning task.
- Evidence anchors:
  - [abstract] "first-epoch training loss... reflect the mastery of inherent knowledge encoded in this dataset"
  - [section 3] "a higher training loss means a lower data consistency... the effective knowledge learned by the model may be more limited"
  - [corpus] Weak - corpus papers discuss loss and entropy but not this specific consistency-loss relationship
- Break condition: If the model architecture or optimization procedure introduces noise that dominates the consistency signal, loss may not reliably indicate data consistency.

### Mechanism 3
- Claim: The multi-stage greedy algorithm in ZIP efficiently approximates the optimal low-compression subset without exhaustive search.
- Mechanism: Global selection filters candidates with high information density, coarse-grained local selection reduces redundancy with already selected samples, and fine-grained local selection ensures diversity among selected samples. This staged approach balances computational efficiency with selection quality.
- Core assumption: Greedy selection in each stage provides a good approximation to the optimal subset, and the three-stage process effectively balances diversity and information density.
- Evidence anchors:
  - [section 4] "we propose an iterative multi-stage greedy algorithm to efficiently obtain an approximate solution with a relatively low compression ratio"
  - [section 5.1.2] "ZIP outperforms other data selection approaches on all backbones, which can be attributed to ZIP's ability to model the complex combinatorial effects among samples"
  - [corpus] Weak - corpus papers discuss greedy algorithms but not this specific multi-stage approach
- Break condition: If the dataset contains highly redundant clusters where greedy selection consistently picks suboptimal representatives, the approximation quality may degrade significantly.

## Foundational Learning

- Concept: Information theory and entropy
  - Why needed here: Understanding how data compression relates to information content is fundamental to the entropy law that connects compression ratio to model performance.
  - Quick check question: Why does a dataset with lower compression ratio contain more information for the model to learn?

- Concept: Loss functions and optimization in deep learning
  - Why needed here: The relationship between training loss, data consistency, and model performance requires understanding how loss functions capture learning dynamics.
  - Quick check question: How does training loss in the first epoch serve as an indicator of data consistency?

- Concept: Combinatorial optimization and greedy algorithms
  - Why needed here: The ZIP algorithm uses a multi-stage greedy approach to approximate optimal data selection, requiring understanding of when greedy methods work well.
  - Quick check question: Why might a greedy approach be sufficient for data selection rather than requiring exhaustive search?

## Architecture Onboarding

- Component map: Data preprocessing → ZIP selection (CPU-based) → LLM training (GPU-based) → Evaluation (MT-bench)
- Critical path: Data selection is the bottleneck; it must complete before training can begin. ZIP runs on CPU while training runs on GPU.
- Design tradeoffs: CPU-based selection trades some approximation quality for significant speed and cost advantages over model-based methods; shorter samples are preferred to maximize information density within token budget.
- Failure signatures: Degraded performance if K1 is too small (insufficient candidate pool) or too large (underestimated compression ratios); if K3 is too small (insufficient diversity) or too large (degenerates to individual selection).
- First 3 experiments:
  1. Run ZIP with default K1=10000, K2=200, K3=100 on a small subset to verify basic functionality and measure runtime.
  2. Compare model performance when varying K1 (e.g., 200, 1000, 10000, 20000) to find the optimal balance.
  3. Test ZIP on both SFT and RLHF stages with different backbone models to verify versatility across tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the entropy law behave when training LLMs on datasets with significantly different quality distributions?
- Basis in paper: [explicit] The paper mentions that "the average data quality Q" is a factor in model performance (Equation 2), and states that "if a data selection method does not substantially change the average data quality Q, we can approximately regard the variable Q as a constant."
- Why unresolved: The paper does not provide empirical evidence on how the entropy law applies when the average data quality varies significantly across datasets.
- What evidence would resolve it: Experiments comparing model performance across datasets with varying average quality scores, while controlling for compression ratio and training loss, would help validate the robustness of the entropy law under different quality distributions.

### Open Question 2
- Question: Can the entropy law be extended to multi-modal data selection for LLMs that process text, images, and other data types?
- Basis in paper: [inferred] The entropy law is derived based on the information compression nature of LLMs and the relationship between compression ratio, training loss, and model performance. This principle could potentially apply to other data types if they can be compressed and used for training.
- Why unresolved: The paper focuses solely on text-based LLM training and does not explore the application of the entropy law to multi-modal data.
- What evidence would resolve it: Experiments applying ZIP or similar compression-based data selection methods to multi-modal LLM training, and analyzing the relationship between compression ratio, training loss, and model performance for different data types.

### Open Question 3
- Question: How does the entropy law change when using different compression algorithms or data preprocessing techniques?
- Basis in paper: [explicit] The paper mentions that "the compression ratio of training data" is a key factor in the entropy law and can be computed using "various off-the-shelf compression algorithms (e.g., DEFLATE in ZIP)."
- Why unresolved: The paper does not investigate the sensitivity of the entropy law to different compression algorithms or data preprocessing techniques, which could affect the compression ratio and, consequently, the model performance.
- What evidence would resolve it: Experiments comparing model performance using different compression algorithms or data preprocessing techniques while controlling for other factors would help determine the robustness of the entropy law to these variations.

## Limitations
- The entropy law's generalizability across different model architectures and domains remains uncertain, with limited testing beyond instruction-following tasks
- The ZIP algorithm's performance critically depends on hyperparameter tuning (K1, K2, K3), with limited guidance on adaptation for different dataset characteristics
- The assumption that average data quality remains constant across subsets may not hold in practice, particularly with heterogeneous data sources

## Confidence
- **High confidence**: The negative correlation between compression ratio and first-epoch training loss is well-supported by experimental results across multiple runs and models
- **Medium confidence**: The claim that lower compression ratio directly leads to higher model performance assumes uniform data quality, which may not hold in practice
- **Low confidence**: The multi-stage greedy algorithm's approximation quality lacks rigorous theoretical guarantees, and the entropy law's applicability beyond tested domains is largely speculative

## Next Checks
1. **Cross-domain validation**: Test ZIP on code generation and mathematical reasoning datasets to verify whether the entropy law holds when data quality distributions vary significantly across subsets
2. **Hyperparameter sensitivity analysis**: Systematically vary K1, K2, K3 across different dataset sizes and compositions to identify patterns in optimal parameter selection
3. **Theoretical approximation bounds**: Analyze the greedy algorithm's approximation ratio by comparing its selections against optimal subsets on small datasets where exhaustive search is feasible