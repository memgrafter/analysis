---
ver: rpa2
title: Explaining Caption-Image Interactions in CLIP Models with Second-Order Attributions
arxiv_id: '2408.14153'
source_url: https://arxiv.org/abs/2408.14153
tags:
- image
- learning
- conference
- attributions
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of explaining how dual encoder
  models like CLIP compare and match their two inputs by attributing predictions onto
  interactions between input features. It introduces a general second-order feature
  attribution method that enables the attribution of similarity predictions onto feature
  interactions between any two differentiable inputs.
---

# Explaining Caption-Image Interactions in CLIP Models with Second-Order Attributions

## Quick Facts
- arXiv ID: 2408.14153
- Source URL: https://arxiv.org/abs/2408.14153
- Reference count: 40
- Primary result: Second-order attributions reveal fine-grained object-level correspondences in CLIP models, outperforming first-order baselines

## Executive Summary
This paper addresses the challenge of explaining how dual encoder models like CLIP compare and match their two inputs by attributing predictions onto interactions between input features. The authors introduce a general second-order feature attribution method that enables the attribution of similarity predictions onto feature interactions between any two differentiable inputs, extending Integrated Gradients theory to vector-valued models. Applied to CLIP models, the approach reveals that they capture fine-grained interactions between corresponding parts of captions and image regions, matching objects across input modes and penalizing mismatches. Evaluations using object localization metrics show the method significantly outperforms first-order baselines like InteractionCAM and InteractionLIME, achieving median Point Game Energy scores above 70% on COCO and Flickr30k.

## Method Summary
The paper presents a second-order feature attribution method that extends Integrated Gradients theory to vector-valued models, enabling attribution of similarity predictions onto feature interactions between differentiable inputs. The method computes interaction attributions without requiring model modification or additional optimization, using numerical integration with Jacobians. Applied to CLIP models, it reveals fine-grained correspondences between caption parts and image regions, with the model actively penalizing mismatches through negative interaction attributions. The approach is evaluated using object localization metrics (Point Game Energy and Accuracy) on COCO and Flickr30k datasets, showing significant improvements over first-order baselines.

## Key Results
- CLIP models learn fine-grained correspondences between caption parts and image regions through contrastive training alone
- The second-order attribution method outperforms first-order baselines by a large margin on object localization tasks (median PGE > 70% on COCO and Flickr30k)
- CLIP models actively penalize mismatches by assigning negative interaction attributions to non-matching objects
- Grounding ability varies heavily across object classes and degrades significantly on out-of-domain data, with fine-tuning substantially improving performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP models acquire fine-grained visual-linguistic grounding through contrastive training alone
- Mechanism: The model learns to align image and text embeddings by maximizing similarity between matching pairs and minimizing it for mismatches, which induces correspondence between individual objects and their textual mentions
- Core assumption: The contrastive loss provides sufficient signal to learn part-level correspondences without explicit grounding supervision
- Evidence anchors:
  - [abstract] "we apply our method toClip models and show that they learn fine-grained correspondences between parts of captions and regions in images."
  - [section 4.3] "the strong intrinsic grounding abilities that we observe here show that the coarse contrastive objective can induce fine-grained correspondence between caption parts and image regions in CLIP models."
  - [corpus] weak/no direct evidence for this specific claim in corpus

### Mechanism 2
- Claim: CLIP models can actively penalize mismatches by assigning negative interaction attributions
- Mechanism: When an object in the image does not match the corresponding text span, the interaction attribution between them becomes negative, reducing the overall similarity score
- Core assumption: The model's prediction computation includes multiplicative terms that allow for negative contributions from mismatches
- Evidence anchors:
  - [section 4.3] "we frequently observe that attributions between a given object in the text and a non-matching one in the image – or vice versa – are not only neutral but negative."
  - [section 4.3] "Attribution to the actual object's bounding-box is positive in94.1% (94.1%) of all cases, while cross-attributions to the other object are negative in68.4% (69.7%) of instances."
  - [corpus] weak/no direct evidence for this specific claim in corpus

### Mechanism 3
- Claim: Second-order interaction attributions reveal feature interactions that first-order methods cannot access
- Mechanism: The method decomposes similarity predictions into additive contributions from feature-pair interactions, capturing dependencies between individual caption tokens and image patches
- Core assumption: Similarity fundamentally depends on comparisons and interactions between features, not just individual feature importances
- Evidence anchors:
  - [abstract] "Common first-order feature-attribution methods explain importances of individual features and can, thus, only provide limited insights into dual encoders, whose predictions depend on interactions between features."
  - [section 3] "we derive a second-order method enabling the attribution of predictions by any differentiable dual encoder onto feature-interactions between its inputs."
  - [corpus] weak/no direct evidence for this specific claim in corpus

## Foundational Learning

- Concept: Feature interactions and second-order attributions
  - Why needed here: To understand how dual encoder models like CLIP compare their two inputs through feature interactions, not just individual feature importances
  - Quick check question: Why can't first-order attribution methods like Integrated Gradients fully explain predictions in dual encoder models?

- Concept: Contrastive learning and dual encoders
  - Why needed here: To understand the training objective that induces visual-linguistic grounding in CLIP models and how it differs from explicit grounding supervision
  - Quick check question: How does the contrastive loss in CLIP training differ from explicit grounding objectives?

- Concept: Integrated Gradients and its extension to vector-valued models
  - Why needed here: To understand the theoretical foundation of the second-order attribution method and how it extends Integrated Gradients to dual encoders
  - Quick check question: How does the second-order attribution method extend the theory behind Integrated Gradients to vector-valued models?

## Architecture Onboarding

- Component map: CLIP dual encoder -> Second-order attribution method -> Evaluation pipeline (PGE/PGA metrics) -> Fine-tuning setup

- Critical path:
  1. Load CLIP model and generate embeddings for image-caption pairs
  2. Compute second-order interaction attributions using the method
  3. Evaluate attributions using object localization metrics on annotated datasets
  4. Analyze results across object classes and domain shifts
  5. Fine-tune models on in-domain data and re-evaluate

- Design tradeoffs:
  - Attribution to intermediate representations vs. output layer: Intermediate representations are more efficient and informative but may be less directly tied to input features
  - Numerical integration steps (N): Higher N improves approximation accuracy but increases computational cost
  - Reference choice: Different uninformative references (black image, padding tokens, etc.) may slightly affect attributions but should converge for large N

- Failure signatures:
  - Poor object localization performance: Suggests the model lacks fine-grained visual-linguistic grounding
  - Negative attributions to correct objects or positive attributions to mismatches: Indicates the model's mismatch penalty mechanism is not working as expected
  - Large improvement upon in-domain fine-tuning: Suggests the model's grounding ability does not generalize well beyond the training domain

- First 3 experiments:
  1. Run object localization evaluation on Coco dataset with unmodified OpenAI CLIP model
  2. Compare conditional perturbation experiments (image patch deletion/insertion, text token deletion/insertion) between second-order method and baselines
  3. Analyze class-wise Point Game Energy scores to identify object classes where the model's grounding ability is strongest and weakest

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do CLIP models actively assign negative attributions to mismatches across input modes, and what are the underlying mechanisms?
- Basis in paper: [explicit] The paper explicitly states that CLIP models can actively penalize mismatches by assigning negative contributions to cross-attributions between non-matching objects
- Why unresolved: While the paper observes this phenomenon, it does not provide a detailed explanation of the underlying mechanisms or why this negative attribution is not consistently applied across all cases, particularly in correlated scenes
- What evidence would resolve it: Further analysis of the model's internal representations and attention mechanisms during mismatch scenarios, or experiments with controlled datasets isolating the effects of negative attributions

### Open Question 2
- Question: Why do CLIP models struggle with fine-grained correspondence between captions and images when evaluated on data outside their initial training domain?
- Basis in paper: [explicit] The paper demonstrates that the grounding abilities of CLIP models significantly degrade on out-of-domain data, with large improvements observed only upon in-domain fine-tuning
- Why unresolved: The paper identifies the issue but does not explore the specific reasons why the models fail to generalize beyond their training distribution, despite the apparent robustness of the contrastive objective
- What evidence would resolve it: Experiments comparing model performance on datasets with varying degrees of similarity to the training distribution, or analysis of the learned representations to identify domain-specific features

### Open Question 3
- Question: How can the second-order attribution method be enhanced to account for discrete text representations and non-linear interpolation paths?
- Basis in paper: [inferred] The discussion section mentions potential enhancements, including accounting for discrete text representations, incorporating non-uniform interpolation, or integrating along non-linear paths
- Why unresolved: The current method uses numerical integration with straight-line paths and treats text as continuous, which may not fully capture the complexities of text representations or the optimal paths for attribution
- What evidence would resolve it: Comparative studies evaluating different interpolation strategies and their impact on attribution accuracy, or experiments incorporating discrete text handling techniques

### Open Question 4
- Question: What are the fundamental limitations of attribution methods in providing robust and faithful explanations, and how do they impact the interpretation of CLIP model behavior?
- Basis in paper: [explicit] The paper acknowledges recent findings on the fundamental limitations of attribution methods, cautioning against drawing definite conclusions about feature importance or counterfactual scenarios
- Why unresolved: While the paper recognizes these limitations, it does not delve into how they specifically affect the interpretation of CLIP model behavior or propose methods to mitigate these issues
- What evidence would resolve it: Empirical studies comparing attribution-based explanations with alternative interpretability methods, or theoretical work establishing bounds on the reliability of attribution-based insights

## Limitations
- Domain generalization gap: CLIP models' grounding ability significantly degrades on out-of-domain data despite contrastive training
- Attribution approximation: Second-order method relies on numerical integration and Jacobian approximations that may introduce errors
- Dataset biases: Object localization metrics depend on quality and coverage of bounding-box annotations in evaluation datasets

## Confidence
- High Confidence: CLIP models learn to align image and text embeddings through contrastive training; second-order attributions outperform first-order baselines; CLIP models can assign negative interaction attributions to mismatches
- Medium Confidence: Contrastive training alone is sufficient for acquiring fine-grained visual-linguistic grounding; the observed grounding ability represents genuine semantic understanding rather than pattern matching
- Low Confidence: The specific mechanism by which contrastive loss induces part-level correspondences; the generalizability of these findings to other dual encoder architectures

## Next Checks
1. **Ablation on Integration Steps**: Test the sensitivity of attribution quality to the number of numerical integration steps (N) to verify convergence and assess approximation errors
2. **Cross-Dataset Generalization**: Evaluate the same models on additional out-of-domain datasets with different visual and linguistic characteristics to better understand the limits of contrastive training-induced grounding
3. **Alternative Attribution Methods**: Apply other second-order attribution methods or compare with analytical solutions (where available) to validate that the observed effects are not artifacts of the specific implementation