---
ver: rpa2
title: Transferable Adversarial Attacks on SAM and Its Downstream Models
arxiv_id: '2410.20197'
source_url: https://arxiv.org/abs/2410.20197
tags:
- adversarial
- attacks
- attack
- conf
- proc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the security of large foundational models,
  specifically the Segment Anything Model (SAM), when fine-tuned for downstream tasks.
  It demonstrates that adversarial attacks can be effective against these fine-tuned
  models without access to their training data or task specifics, using only the open-source
  SAM as a surrogate.
---

# Transferable Adversarial Attacks on SAM and Its Downstream Models

## Quick Facts
- **arXiv ID**: 2410.20197
- **Source URL**: https://arxiv.org/abs/2410.20197
- **Reference count**: 40
- **Primary result**: Proposes UMI-GRAT method that achieves significant performance degradation on fine-tuned SAM models without access to training data or task specifics

## Executive Summary
This paper addresses the security vulnerability of large foundational models, specifically focusing on the Segment Anything Model (SAM) and its downstream applications. The authors demonstrate that adversarial attacks can effectively target fine-tuned SAM models using only the open-source SAM as a surrogate, without requiring access to downstream training data or task specifications. They introduce a Universal Meta-Initialization (UMI) algorithm that extracts inherent vulnerabilities from the foundation model, combined with a Gradient Robust loss function to enhance transferability. The proposed UMI-GRAT method significantly outperforms existing transfer-based adversarial attacks across diverse downstream tasks including medical imaging, shadow segmentation, and camouflaged object detection.

## Method Summary
The authors propose UMI-GRAT, which consists of two key components: Universal Meta-Initialization (UMI) and Gradient Robust loss (GRAT). UMI uses meta-learning to extract invariant vulnerabilities from the foundation model SAM, optimizing adversarial perturbation initialization across two objectives - effectiveness on SAM and fast adaptability to downstream models. The Gradient Robust loss mitigates the deviation in adversarial update direction caused by differences between surrogate and victim models through noise augmentation. The method operates by first performing offline UMI learning on the foundation model and natural datasets, then adapting this initialization to target images, and finally generating real-time adversarial perturbations using the adapted UMI and gradient robust loss.

## Key Results
- UMI-GRAT achieves mDSC of 5.22 in medical segmentation (vs 81.88 for clean models) and BER of 37.25 in shadow segmentation (vs 2.15 for clean models)
- The method demonstrates consistent performance across 5 diverse downstream tasks: medical segmentation, shadow detection, camouflaged object segmentation, nature segmentation, and object segmentation
- UMI-GRAT significantly outperforms existing transfer-based adversarial attacks, serving as a plug-and-play enhancement for other attack methods

## Why This Works (Mechanism)

### Mechanism 1
Universal Meta-Initialization (UMI) extracts invariant vulnerabilities from the foundation model that remain effective after fine-tuning. UMI uses meta-learning to optimize adversarial perturbation initialization across two objectives: effectiveness on the foundation model and fast adaptability to downstream models. The core assumption is that vulnerabilities inherent in the foundation model persist through fine-tuning and can be leveraged as prior knowledge. Evidence shows this approach successfully extracts vulnerabilities that transfer across diverse downstream tasks.

### Mechanism 2
Gradient Robust loss mitigates the deviation in adversarial update direction caused by differences between surrogate and victim models. The loss simulates gradient uncertainty through noise augmentation, making adversarial perturbations more robust to gradient disparities during transfer. The core assumption is that gradient differences between surrogate and victim models cause significant deviation in adversarial update directions that can be modeled and compensated for. Theoretical formulation demonstrates that gradient deviation is inevitable when using open-sourced SAM as the surrogate model.

### Mechanism 3
Feature-level attacks combined with gradient robustness achieve better transferability than optimization-level attacks alone. By maximizing feature embedding dissimilarity while being robust to gradient variations, the attack maintains effectiveness across diverse downstream tasks. The core assumption is that feature representations contain sufficient information for successful adversarial attacks and are more stable targets than raw optimization objectives across model variations. Feature embeddings are identified as pivotal information sources for attacking SAM's downstream models.

## Foundational Learning

- **Concept**: Meta-learning and Reptile algorithm
  - **Why needed here**: UMI requires optimizing adversarial perturbations across multiple objectives and iterations to find invariant vulnerabilities
  - **Quick check question**: How does the Reptile algorithm update parameters to optimize across multiple tasks/objective functions simultaneously?

- **Concept**: Transfer-based adversarial attacks and feature-level attacks
  - **Why needed here**: Understanding how adversarial examples transfer between models and why feature-level attacks can be more effective than optimization-level attacks
  - **Quick check question**: What makes feature-level attacks potentially more transferable than optimization-level attacks in the context of fine-tuned models?

- **Concept**: Gradient-based optimization and noise augmentation techniques
  - **Why needed here**: The Gradient Robust loss uses noise augmentation to make adversarial perturbations robust to gradient variations
  - **Quick check question**: How does adding gradient noise during optimization help create more robust adversarial examples?

## Architecture Onboarding

- **Component map**: UMI module (offline learning) -> GRAT module (real-time attack) -> Feature extraction pipeline -> Surrogate model interface (SAM)
- **Critical path**: 1) Offline UMI learning on foundation model and natural dataset 2) Task-specific adaptation of UMI to target image 3) Real-time GRAT generation using adapted UMI and gradient robust loss 4) Transfer to downstream model evaluation
- **Design tradeoffs**: UMI requires substantial offline computation but enables faster real-time attacks; gradient robustness adds computational overhead but significantly improves transferability; feature-level targeting provides better transfer but may require deeper understanding of model architecture
- **Failure signatures**: Poor transferability indicates insufficient invariant vulnerability extraction or inadequate gradient robustness; high computational cost suggests inefficient UMI adaptation or suboptimal gradient robust loss implementation; inconsistent performance across tasks indicates model-specific vulnerabilities not captured by UMI
- **First 3 experiments**: 1) Validate UMI effectiveness by comparing transfer rates with and without UMI initialization 2) Test gradient robust loss by measuring deviation reduction between surrogate and victim models 3) Evaluate combined UMI-GRAT performance across diverse downstream tasks and datasets

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the proposed UMI-GRAT perform against other large foundation models beyond SAM?
- **Basis in paper**: The paper states that the proposed UMI-GRAT is not contingent upon a prior regarding the model's architecture, suggesting its potential applicability across various model paradigms. However, the experiments only tested MUI-GRAT on the prevalent SAMs and their downstream models.
- **Why unresolved**: The paper does not provide any experimental results or analysis of the UMI-GRAT's performance against other large foundation models.
- **What evidence would resolve it**: Conducting experiments to evaluate the UMI-GRAT's performance against other large foundation models such as CLIP, GPT, or other vision-language models would provide evidence to resolve this question.

### Open Question 2
- **Question**: What are the potential defense mechanisms to protect SAM and its downstream models from the proposed UMI-GRAT attacks?
- **Basis in paper**: The paper highlights the security risk inherent in the direct utilization and fine-tuning of open-sourced large foundation models, but does not provide and validate an effective solution for this secure concern.
- **Why unresolved**: The paper does not discuss any potential defense mechanisms or provide any validation of their effectiveness against the proposed UMI-GRAT attacks.
- **What evidence would resolve it**: Developing and evaluating various defense mechanisms such as adversarial training, input preprocessing, or model architecture modifications to protect SAM and its downstream models from the proposed UMI-GRAT attacks would provide evidence to resolve this question.

### Open Question 3
- **Question**: How does the performance of UMI-GRAT vary with different levels of access to the victim model's task and training data?
- **Basis in paper**: The paper states that the proposed UMI-GRAT aims to attack various SAM's downstream models by solely utilizing the information from the open-sourced SAM, without accessing the downstream task and training dataset.
- **Why unresolved**: The paper does not provide any analysis or experimental results on how the performance of UMI-GRAT varies with different levels of access to the victim model's task and training data.
- **What evidence would resolve it**: Conducting experiments to evaluate the performance of UMI-GRAT with different levels of access to the victim model's task and training data, such as partial access or access to synthetic data, would provide evidence to resolve this question.

## Limitations

- The method's effectiveness depends on the foundational model's inherent vulnerabilities remaining stable through fine-tuning, which may not hold for all fine-tuning strategies
- Substantial computational overhead is required for UMI learning, which may not scale efficiently to larger foundation models or more diverse downstream tasks
- The approach focuses specifically on SAM-based models and may not generalize effectively to all foundation model architectures

## Confidence

- **High confidence** in the empirical demonstration that adversarial attacks transfer effectively from SAM to fine-tuned downstream models, supported by consistent performance degradation across multiple datasets and metrics
- **Medium confidence** in the theoretical claims about gradient deviation and UMI's ability to extract invariant vulnerabilities, as the mathematical formulation is provided but the exact mechanisms of how fine-tuning preserves these vulnerabilities are not fully explored
- **Medium confidence** in the generalizability of results, as the study focuses on SAM-based models but the underlying principles may not extend to all foundation model architectures

## Next Checks

1. **Ablation Study on UMI Components**: Systematically evaluate which aspects of UMI (meta-initialization vs. gradient robustness) contribute most to transferability by testing variants that isolate each component.

2. **Defense Resilience Testing**: Apply common adversarial training or input preprocessing defenses to downstream models and measure UMI-GRAT's effectiveness against these hardened targets.

3. **Cross-Architecture Transferability**: Test whether UMI extracted from SAM can effectively attack downstream models based on other foundation models (e.g., CLIP-based segmenters) to assess generalizability beyond SAM-specific vulnerabilities.