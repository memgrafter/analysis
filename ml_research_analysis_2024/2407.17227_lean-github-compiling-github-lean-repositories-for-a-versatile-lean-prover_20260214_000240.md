---
ver: rpa2
title: 'LEAN-GitHub: Compiling GitHub LEAN repositories for a versatile LEAN prover'
arxiv_id: '2407.17227'
source_url: https://arxiv.org/abs/2407.17227
tags:
- lean
- formal
- dataset
- theorem
- lean-github
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LEAN-GitHub, a large-scale dataset of formal
  mathematical proofs extracted from Lean 4 repositories on GitHub, containing 28,597
  theorems and 218,866 tactics. The authors developed a scalable pipeline to extract
  formal proofs with intermediate states, addressing the data scarcity problem in
  automated theorem proving.
---

# LEAN-GitHub: Compiling GitHub LEAN repositories for a versatile LEAN prover

## Quick Facts
- arXiv ID: 2407.17227
- Source URL: https://arxiv.org/abs/2407.17227
- Reference count: 40
- Key outcome: Large-scale dataset of formal proofs from Lean 4 repositories improves automated theorem proving performance

## Executive Summary
This paper introduces LEAN-GitHub, a large-scale dataset containing 28,597 theorems and 218,866 tactics extracted from Lean 4 repositories on GitHub. The authors developed a scalable pipeline to extract formal proofs with intermediate states, addressing the data scarcity problem in automated theorem proving. By fine-tuning the InternLM-math-plus model on this dataset, they achieved state-of-the-art performance across multiple Lean 4 benchmarks, demonstrating the effectiveness of leveraging under-utilized human-written formal corpora for improving formal reasoning capabilities.

## Method Summary
The authors developed a scalable pipeline to extract formal proofs from open Lean 4 repositories on GitHub, including intermediate proof states. They compiled 147 repositories, extracting AST trees and intermediate tactic states during compilation. The dataset was used to fine-tune InternLM-math-plus-7B for 2 epochs with batch size 512 and learning rate 10^-5. The fine-tuned model was evaluated on miniF2F, ProofNet, and Putnam benchmarks using best-first search with beam size 32 and max expansions 100.

## Key Results
- Achieved 54.5% pass rate on miniF2F test (vs 52% previous best)
- Obtained 18.1% pass rate on ProofNet benchmark
- Solved 5/640 problems on Putnam competition

## Why This Works (Mechanism)

### Mechanism 1
The dataset improves model performance by providing diverse formal proofs across different mathematical domains. Training on diverse human-written formal proofs exposes the model to various proof techniques, mathematical structures, and reasoning patterns that are not present in synthetic data alone.

### Mechanism 2
The extraction pipeline captures intermediate proof states that are crucial for training effective theorem provers. By extracting AST trees and intermediate tactic states during compilation, the model learns the step-by-step reasoning process rather than just final proofs.

### Mechanism 3
The state deduplication mechanism improves search efficiency and prevents redundant computation. By renaming hypotheses based on their internal storage order in Lean's kernel, states with the same logical content but different variable names are recognized as identical.

## Foundational Learning

- **Formal theorem proving in dependent type theory**: Why needed - The paper operates within the Lean 4 ecosystem which is based on dependent type theory, requiring understanding of how propositions are treated as types. Quick check - How does Lean's view of propositions as types differ from traditional first-order logic?

- **Large language model fine-tuning for formal reasoning**: Why needed - The core contribution involves fine-tuning InternLM-math-plus on the extracted dataset to improve theorem proving capabilities. Quick check - What are the key differences between fine-tuning for natural language generation versus formal theorem proving?

- **Proof search algorithms and state management**: Why needed - The paper discusses best-first search with state deduplication, requiring understanding of how proof search trees are managed. Quick check - How does best-first search differ from depth-first search in the context of theorem proving?

## Architecture Onboarding

- **Component map**: GitHub repository collection → compilation → AST extraction → intermediate state capture → model training → best-first search with state deduplication → proof validation
- **Critical path**: Repository compilation → AST extraction → intermediate state capture → model training → proof search evaluation
- **Design tradeoffs**: Compilation vs. script execution (leanc compiler vs. Lake tool for parallel performance), state granularity (intermediate states vs. final proofs), deduplication overhead (computational cost vs. search efficiency)
- **Failure signatures**: Repository compilation failures due to missing dependencies, incomplete intermediate state extraction, state deduplication producing false positives, model overfitting to specific proof patterns
- **First 3 experiments**: 1) Compile a small Lean 4 repository and verify AST extraction produces expected intermediate states, 2) Implement state deduplication on a synthetic proof search trace and measure reduction in duplicate states, 3) Fine-tune a small language model on a subset of the extracted data and evaluate on a simple theorem proving benchmark

## Open Questions the Paper Calls Out

The paper identifies several open questions regarding the optimal balance between synthetic and human-written data, the impact of extracted intermediate proof state quality on performance, potential improvements to state deduplication techniques, generalizability to other formal systems, and the impact of repository selection criteria on dataset diversity and quality.

## Limitations

- The claim of state-of-the-art performance relies on comparisons with prior work, but exact implementation details of competing methods are not fully specified
- The state deduplication mechanism may have edge cases where syntactically different states with the same logical content are not properly identified
- The dataset extraction process may have introduced biases based on repository selection and compilation success rate

## Confidence

- **High Confidence**: Dataset size (28,597 theorems, 218,866 tactics) and basic methodology of extracting formal proofs from GitHub repositories are well-documented and verifiable
- **Medium Confidence**: Effectiveness of fine-tuned model on reported benchmarks is supported by experimental results, but exact implementation details are not fully specified
- **Low Confidence**: Claim that dataset covers "broad spectrum of complexities" across mathematical domains is based on qualitative assessment rather than quantitative analysis

## Next Checks

1. Reproduce state deduplication: Implement the mechanism on a synthetic proof search trace with known equivalent states and measure reduction in duplicate states
2. Analyze dataset diversity: Conduct quantitative analysis of LEAN-GitHub dataset to assess distribution of mathematical domains, proof complexities, and theorem types
3. Independent benchmark evaluation: Re-implement fine-tuned model evaluation on miniF2F, ProofNet, and Putnam benchmarks using provided checkpoint and training procedure