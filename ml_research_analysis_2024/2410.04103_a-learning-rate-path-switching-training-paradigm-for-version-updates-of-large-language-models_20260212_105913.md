---
ver: rpa2
title: A Learning Rate Path Switching Training Paradigm for Version Updates of Large
  Language Models
arxiv_id: '2410.04103'
source_url: https://arxiv.org/abs/2410.04103
tags:
- learning
- rate
- training
- paradigm
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of updating large language models
  (LLMs) efficiently when new data becomes available. The authors compare pre-training
  from scratch (PTFS) and continual pre-training (CPT) for LLM version updates, finding
  that while PTFS achieves better performance, it has higher training cost.
---

# A Learning Rate Path Switching Training Paradigm for Version Updates of Large Language Models

## Quick Facts
- arXiv ID: 2410.04103
- Source URL: https://arxiv.org/abs/2410.04103
- Authors: Zhihao Wang, Shiyu Liu, Jianheng Huang, Zheng Wang, Yixuan Liao, Xiaoxin Chen, Junfeng Yao, Jinsong Su
- Reference count: 16
- Primary result: Proposes learning rate path switching paradigm that reduces training cost to 58% of pre-training from scratch while maintaining comparable performance

## Executive Summary
This paper addresses the challenge of updating large language models (LLMs) efficiently when new data becomes available. The authors compare pre-training from scratch (PTFS) and continual pre-training (CPT) for LLM version updates, finding that while PTFS achieves better performance, it has higher training cost. Through analysis, they discover that using a large learning rate during the initial training stage and a complete learning rate decay during the continual pre-training stage are crucial for optimal performance.

To address this, the authors propose a learning rate path switching paradigm that combines the benefits of both PTFS and CPT. Their method uses one main path with maximal learning rate training and multiple branching paths for each update with fast learning rate decay. When training four versions of LLaMA-1.2B models, their paradigm reduces total training cost to 58% compared to PTFS while maintaining comparable pre-training performance. The approach also generalizes well across different model architectures, sizes, data scales, and learning rate schedules.

## Method Summary
The proposed learning rate path switching paradigm consists of one main path where a LLM is pre-trained with the maximal learning rate, and multiple branching paths, each corresponding to an update with newly-added training data using fast learning rate decay. The paradigm is parameterized by α, which determines the number of fast-decaying steps in each branching path. The method was evaluated on LLaMA-1.2B models with 1.2B parameters, training four versions using cosine, Knee, and multi-step learning rate schedules with linear warm-up of 2K steps, maximum learning rate of 3e-4, and minimum of 3e-5.

## Key Results
- Learning rate path switching paradigm reduces total training cost to 58% of PTFS while maintaining comparable pre-training performance
- The paradigm generalizes across different model architectures (LLaMA, Qwen, InternLM), sizes (203M to 3.1B parameters), data scales, and learning rate schedules
- Optimal α parameter value of 0.6 balances model performance and training cost effectively
- Pre-training from scratch with large learning rate and complete learning rate decay during updates are crucial for optimal version updates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using a large learning rate during the initial pre-training stage provides better initialization checkpoints for subsequent updates
- Mechanism: The large learning rate allows the model to explore a wider parameter space and escape shallow local minima, creating checkpoints with better generalization that serve as strong starting points for continual pre-training
- Core assumption: Larger learning rates during initialization lead to better basins of attraction in the loss landscape that persist through subsequent fine-tuning
- Evidence anchors:
  - [abstract]: "we find that a large learning rate in the first stage and a complete learning rate decay process in the second stage are crucial for version updates of LLMs"
  - [section 2.2]: "we observe that with the increase in the cosine cycle length during the first stage, the performance of the initialization checkpoint drops, whereas its corresponding updated LLM performs better"
  - [corpus]: Weak - no direct evidence found in corpus neighbors

### Mechanism 2
- Claim: Complete learning rate decay during continual pre-training stages ensures optimal performance of updated models
- Mechanism: The full decay schedule allows the model to first explore broadly with high learning rates then gradually fine-tune with lower rates, balancing exploration and exploitation
- Core assumption: The optimal performance requires both exploration (high LR) and exploitation (low LR) phases, and skipping the decay phase leads to suboptimal convergence
- Evidence anchors:
  - [abstract]: "a complete learning rate decay process in the second stage are crucial for version updates of LLMs"
  - [section 2.2]: "it is evident that a complete learning rate decay process enables the updated LLMs to achieve the best performance"
  - [corpus]: Weak - no direct evidence found in corpus neighbors

### Mechanism 3
- Claim: The path switching architecture reduces total training cost by combining pre-training from scratch with continual pre-training
- Mechanism: By maintaining a main path with maximal learning rate and branching paths for each update with fast decay, the paradigm achieves better performance than pure CPT while being more efficient than pure PTFS
- Core assumption: The initialization from the main path provides sufficient quality for continual pre-training to be effective, and the fast decay in branching paths is sufficient for adaptation
- Evidence anchors:
  - [abstract]: "our paradigm comprises one main path, where we pre-train a LLM with the maximal learning rate, and multiple branching paths, each of which corresponds to an update of the LLM with newly-added training data"
  - [section 3.1]: "Our paradigm better balances model performance and training cost compared to the other two paradigms"
  - [corpus]: Weak - no direct evidence found in corpus neighbors

## Foundational Learning

- Concept: Learning rate scheduling
  - Why needed here: The entire paradigm relies on carefully designed learning rate schedules to balance exploration and exploitation across multiple training stages
  - Quick check question: What happens to model performance if you skip the decay phase in the second stage of continual pre-training?

- Concept: Catastrophic forgetting
  - Why needed here: Understanding why CPT typically underperforms PTFS requires knowledge of how continual learning can lead to forgetting previously learned knowledge
  - Quick check question: How does the initialization quality from the main path help mitigate catastrophic forgetting in subsequent updates?

- Concept: Model scaling laws
  - Why needed here: The paper demonstrates generalization across different model sizes, requiring understanding of how learning rates and training dynamics scale with model parameters
  - Quick check question: How would you expect the optimal learning rate schedule to change as you scale from 203M to 3.1B parameters?

## Architecture Onboarding

- Component map:
  - Main training path -> Pre-trains from scratch with maximal learning rate
  - Branching paths -> Each handles one version update with fast decay schedule
  - Learning rate scheduler -> Coordinates transitions between paths
  - Checkpoint manager -> Maintains and selects appropriate initialization points

- Critical path:
  1. Initialize main path with maximal learning rate
  2. Train for sufficient steps to create quality initialization
  3. When update needed, switch to branching path
  4. Apply fast decay schedule in branching path
  5. Return to main path for next initialization
  6. Repeat for each version

- Design tradeoffs:
  - Higher α values improve performance but increase training cost
  - Longer main path training improves initialization but delays updates
  - Faster decay in branching paths saves time but may hurt adaptation

- Failure signatures:
  - Performance degradation across versions suggests main path initialization is insufficient
  - Training instability indicates learning rates are too aggressive
  - High training cost suggests α parameter needs adjustment

- First 3 experiments:
  1. Replicate the basic PTFS vs CPT comparison to establish baseline performance gap
  2. Test different α values (0.2, 0.4, 0.6) to find optimal balance between performance and cost
  3. Validate generalization by testing on different model architectures (e.g., Qwen vs LLaMA)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the learning rate path switching paradigm perform when applied to multimodal large language models?
- Basis in paper: [inferred] The authors mention future work exploring the paradigm's applicability to multimodal LLMs, suggesting it hasn't been tested yet.
- Why unresolved: The paper focuses on text-only LLMs, and the authors explicitly state that the paradigm's performance on multimodal models is an open question.
- What evidence would resolve it: Experiments comparing the paradigm's effectiveness on multimodal models versus text-only models, showing performance and cost trade-offs.

### Open Question 2
- Question: Can the optimal value of the hyperparameter α be determined automatically rather than through manual tuning?
- Basis in paper: [inferred] The authors manually test different α values (0.2 to 0.8) and choose 0.6 based on performance, indicating the need for an automated method.
- Why unresolved: The paper uses a manual search to find α, which is not scalable or efficient for different model architectures, sizes, or datasets.
- What evidence would resolve it: A method that automatically determines the optimal α value based on model characteristics and dataset properties, validated through experiments.

### Open Question 3
- Question: How does the learning rate path switching paradigm perform when the data increment between versions is highly inconsistent?
- Basis in paper: [explicit] The authors conduct experiments with varying data increments (10.5B, 21B, 31.5B tokens) and find the paradigm remains effective, but they don't explore extreme cases of inconsistency.
- Why unresolved: The experiments only vary the data increment by a factor of 3, and the authors don't discuss performance in scenarios with much larger or smaller increments.
- What evidence would resolve it: Experiments with a wider range of data increment sizes, including very small and very large increments, to test the paradigm's robustness and performance.

## Limitations

- The evaluation is primarily conducted on LLaMA-1.2B models with relatively modest model sizes (203M, 1.2B, and 3.1B parameters), which may not fully represent the challenges of updating larger frontier models
- The study uses a single dataset with 764M samples and 10.5B tokens per update increment, limiting generalizability to different data distributions and scales
- The evaluation focuses primarily on pre-training perplexity as the performance metric, without assessing downstream task performance or the quality of the updated models on specific applications

## Confidence

**High Confidence Claims:**
- The fundamental tradeoff between PTFS and CPT paradigms in terms of performance vs. training cost is well-established through systematic experiments
- The importance of large learning rates during initial training and complete learning rate decay during updates is demonstrated across multiple experiments
- The proposed learning rate path switching paradigm achieves 58% training cost reduction while maintaining comparable performance on the tested models

**Medium Confidence Claims:**
- The generalization of the approach across different model architectures (LLaMA, Qwen, InternLM) and learning rate schedules (cosine, Knee, multi-step) is demonstrated but with limited variation
- The optimal α parameter value of 0.6 for balancing performance and cost is identified through experiments but may vary with different experimental conditions

**Low Confidence Claims:**
- Claims about the approach's effectiveness on much larger models (70B+ parameters) are extrapolations without direct experimental validation
- The assertion that the method "generalizes well" across different data scales is based on limited scaling experiments with a single dataset type

## Next Checks

1. **Scale Validation**: Test the learning rate path switching paradigm on larger models (7B-70B parameters) to verify if the 58% training cost reduction holds at scale and whether the optimal α parameter remains consistent across model sizes.

2. **Downstream Task Evaluation**: Evaluate the updated models on standard benchmarks (MMLU, BBH, HumanEval) to confirm that pre-training performance gains translate to actual improvements in downstream task performance, not just lower perplexity.

3. **Data Distribution Sensitivity**: Test the approach with different data distributions (e.g., domain-specific corpora, multilingual datasets) and update patterns (non-uniform token increments) to assess robustness beyond the single dataset used in the paper.