---
ver: rpa2
title: 'Machine Translation with Large Language Models: Decoder Only vs. Encoder-Decoder'
arxiv_id: '2409.13747'
source_url: https://arxiv.org/abs/2409.13747
tags:
- translation
- machine
- learning
- language
- encoder-decoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares Decoder-only and Encoder-Decoder architectures
  for multilingual machine translation, focusing on Indian languages like Telugu,
  Tamil, and Malayalam. Using datasets such as FLORES-101 and BPCC Wiki MT, the research
  evaluates models like XGLM, mT5, LLaMA 2, XLNet, and IndicBART.
---

# Machine Translation with Large Language Models: Decoder Only vs. Encoder-Decoder

## Quick Facts
- arXiv ID: 2409.13747
- Source URL: https://arxiv.org/abs/2409.13747
- Reference count: 2
- Primary result: Decoder-only models face challenges in multilingual translation due to training paradigms focused on next-token prediction, while encoder-decoder architectures show reliable performance.

## Executive Summary
This study compares Decoder-only and Encoder-Decoder architectures for multilingual machine translation, focusing on Indian languages including Telugu, Tamil, and Malayalam. Using datasets such as FLORES-101 and BPCC Wiki MT, the research evaluates models like XGLM, mT5, LLaMA 2, XLNet, and IndicBART through in-context learning and fine-tuning experiments. The findings indicate that encoder-decoder models demonstrate reliable performance in translation tasks, while decoder-only models struggle due to their training paradigms focused on next-token prediction rather than bidirectional context understanding. The study proposes Streaming Self-Attention as a potential solution to improve decoder-only model performance.

## Method Summary
The study employs a mixed-method approach combining in-context learning and fine-tuning experiments. In-context learning uses 3-shot learning with XGLM and mT5 models on datasets including FLORES-101 and BPCC Wiki MT for English-to-Indian language translation. Fine-tuning experiments focus on mT5 and LLaMA 2 models, with particular attention to one-to-one versus one-to-many translation configurations. Baseline models (XLNet for decoder-only and IndicBART for encoder-decoder) are compared using similar parameters. Evaluation metrics include BLEU, chrF, and TER scores across various translation configurations and context lengths.

## Key Results
- Encoder-Decoder model demonstrated reliable performance in experiments, achieving BLEU scores of 14.1444 for English-to-Hindi translation with mT5 fine-tuning.
- LLaMA 2 performed better in one-to-one translation tasks compared to one-to-many setups.
- Decoder-only models face challenges due to training paradigms focused on next-word or next-character prediction rather than bidirectional context understanding.
- Baseline models (XLNet and IndicBART) provided comparative performance data across translation configurations.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Encoder-Decoder architectures perform better in multilingual translation tasks than Decoder-only models due to their ability to capture bidirectional context during encoding.
- Mechanism: Encoder-Decoder models process source text bidirectionally during encoding, allowing them to capture richer contextual representations before decoding to target language. This bidirectional processing enables better handling of complex translation scenarios.
- Core assumption: The quality of translation is directly proportional to the richness of context captured during the encoding phase.
- Evidence anchors:
  - [abstract] "The Encoder-Decoder model has demonstrated reliable performance in our experiments, providing trustworthy results."
  - [section] "The Encoder-Decoder model has demonstrated reliable performance in our experiments, providing trustworthy results. However, the training paradigms for Decoder-only models differ significantly, as they are typically trained on next-word or next-character prediction tasks."
  - [corpus] Weak evidence - no direct comparison studies found in corpus, though related papers suggest encoder-decoder advantages in multilingual settings.
- Break condition: When translation tasks are simple and context requirements are minimal, Decoder-only models may perform comparably due to their simpler architecture and lower computational requirements.

### Mechanism 2
- Claim: Decoder-only models face challenges in multilingual translation due to their autoregressive nature and different training paradigms focused on next-token prediction.
- Mechanism: Decoder-only models are trained primarily on next-token prediction tasks, which creates a fundamental mismatch when applied to translation tasks requiring bidirectional context understanding. This training paradigm makes it difficult for them to effectively handle source-target relationships in translation.
- Core assumption: The training objective (next-token prediction) is incompatible with the requirements of translation tasks.
- Evidence anchors:
  - [section] "The training paradigms for Decoder-only models differ significantly, as they are typically trained on next-word or next-character prediction tasks."
  - [section] "Decoder-only models handle the starting positions of the source and target texts separately, posing unique challenges."
  - [corpus] Some evidence from related papers suggesting decoder-only limitations in causal reasoning and multilingual contexts.
- Break condition: When using techniques like Streaming Self-Attention (SSA) that help decoder-only models determine sufficient context for translation, or when translation tasks are simple and don't require complex context understanding.

### Mechanism 3
- Claim: The Streaming Self-Attention (SSA) technique can significantly improve Decoder-only model performance in multilingual translation by enabling context-aware translation decisions.
- Mechanism: SSA allows the model to dynamically determine when it has accumulated sufficient context from the source text to begin accurate translation, addressing the fundamental challenge of context management in decoder-only architectures.
- Core assumption: Dynamic context management can compensate for the architectural limitations of decoder-only models in translation tasks.
- Evidence anchors:
  - [section] "The development of novel methods such as Streaming Self-Attention (SSA) marks a significant advancement. SSA enables the model to determine when it has sufficient context from the original text to begin translating accurately."
  - [section] "This technique addresses some of the inherent challenges in translating long texts and could be crucial for improving the performance of Decoder-only models in multilingual settings."
  - [corpus] No direct evidence in corpus - this appears to be proposed future work rather than established mechanism.
- Break condition: When the computational overhead of SSA outweighs its benefits, or when the translation task is simple enough that static context windows suffice.

## Foundational Learning

- Concept: Bidirectional context encoding
  - Why needed here: Understanding why encoder-decoder models capture richer context is crucial for evaluating architectural trade-offs in translation tasks.
  - Quick check question: Can you explain why processing text bidirectionally during encoding provides advantages for translation compared to unidirectional processing?

- Concept: Autoregressive generation
  - Why needed here: Decoder-only models rely on autoregressive generation, which fundamentally shapes their behavior and limitations in translation tasks.
  - Quick check question: How does the autoregressive nature of decoder-only models affect their ability to handle source-target relationships in translation?

- Concept: Context window management
  - Why needed here: Both architectures must manage context windows, but they do so differently, affecting translation quality and efficiency.
  - Quick check question: What are the key differences in how encoder-decoder and decoder-only models manage context windows during translation?

## Architecture Onboarding

- Component map: Input → Encoder (bidirectional processing) → Context vectors → Decoder (autoregressive generation) → Output. Decoder-only: Input → Decoder (autoregressive generation with attention to input) → Output.
- Critical path: Data preparation → Model selection (architecture choice) → Training configuration → Evaluation → Iteration. The most critical decision point is architecture selection based on task requirements.
- Design tradeoffs: Encoder-Decoder: Better context capture but higher computational cost. Decoder-only: Simpler architecture and lower cost but limited context handling. Choice depends on translation complexity and resource constraints.
- Failure signatures: Encoder-Decoder: Overfitting on training data, poor generalization to low-resource languages. Decoder-only: Inability to capture long-range dependencies, poor performance on complex translation pairs.
- First 3 experiments:
  1. Compare baseline BLEU scores for simple English-to-Hindi translation using both architectures on the same dataset.
  2. Test context window size impact on translation quality for both architectures using progressively longer sentences.
  3. Implement and evaluate Streaming Self-Attention on decoder-only model to assess context management improvements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Decoder-only models compare to Encoder-Decoder models in multilingual translation tasks when trained under similar conditions?
- Basis in paper: [explicit] The paper explicitly aims to compare the performance of Decoder-only and Encoder-Decoder architectures in multilingual machine translation.
- Why unresolved: The study shows that Decoder-only models face challenges due to differing training paradigms, but a comprehensive comparison under similar conditions is still needed.
- What evidence would resolve it: Conducting experiments where both architectures are trained under identical conditions and comparing their performance metrics (e.g., BLEU, chrF, TER) would provide a clear answer.

### Open Question 2
- Question: What is the impact of context length on the translation quality and efficiency of Decoder-only and Encoder-Decoder models?
- Basis in paper: [explicit] The paper mentions assessing the impact of context length on translation quality and efficiency for both architectural setups.
- Why unresolved: While the paper acknowledges the importance of context length, it does not provide detailed analysis or experimental results on how varying context lengths affect the performance of each model type.
- What evidence would resolve it: Conducting experiments with varying context lengths and analyzing the performance metrics for both models would clarify the impact of context length.

### Open Question 3
- Question: How effective is Streaming Self-Attention (SSA) in improving the performance of Decoder-only models in multilingual machine translation?
- Basis in paper: [explicit] The paper suggests that SSA could be crucial for improving the performance of Decoder-only models by enabling them to determine when sufficient context is available for accurate translation.
- Why unresolved: The paper proposes SSA as a potential advancement but does not provide experimental results or a detailed analysis of its effectiveness.
- What evidence would resolve it: Implementing SSA in Decoder-only models and evaluating their performance against traditional models would provide insights into its effectiveness.

## Limitations

- The study's findings are primarily based on Indian language pairs, limiting generalizability to other language families.
- Streaming Self-Attention technique is proposed but lacks empirical validation in the current experiments.
- Evaluation relies heavily on BLEU scores, which may not capture the full complexity of multilingual translation quality, especially for morphologically rich languages.

## Confidence

**High Confidence**: The observation that encoder-decoder models demonstrate reliable performance in multilingual translation tasks is well-supported by established literature and the baseline comparisons with XLNet and IndicBART.

**Medium Confidence**: The performance differences between one-to-one and one-to-many translation configurations for LLaMA 2 are based on actual experimental results, though the sample size and variety of language pairs tested may be limited.

**Low Confidence**: Claims about decoder-only model limitations due to training paradigms and the potential of SSA to address these limitations are largely theoretical, lacking empirical validation in this study.

## Next Checks

1. **Empirical SSA Validation**: Implement and test the Streaming Self-Attention technique on the decoder-only models (XGLM, LLaMA 2) used in the study. Compare translation quality with and without SSA across the same language pairs to verify if it addresses the claimed context management issues.

2. **Cross-Lingual Generalization Test**: Extend the evaluation to include non-Indian language pairs from the FLORES-101 dataset to assess whether the observed performance patterns (encoder-decoder superiority) hold across typologically diverse language families.

3. **Efficiency and Resource Analysis**: Conduct comprehensive measurements of computational resources (GPU memory, training time, inference latency) for both architectures across the translation tasks. This would provide practical context for the performance trade-offs discussed in the study.