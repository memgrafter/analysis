---
ver: rpa2
title: 'No Such Thing as a General Learner: Language models and their dual optimization'
arxiv_id: '2408.09544'
source_url: https://arxiv.org/abs/2408.09544
tags:
- llms
- language
- they
- learning
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that neither humans nor LLMs are general learners,
  challenging the widespread assumption that LLMs can serve as unbiased models of
  human language acquisition. It introduces the concept of dual optimization, where
  LLMs undergo both traditional training and a selection process analogous to natural
  selection, making them specialized rather than general learners.
---

# No Such Thing as a General Learner: Language models and their dual optimization

## Quick Facts
- arXiv ID: 2408.09544
- Source URL: https://arxiv.org/abs/2408.09544
- Reference count: 12
- Primary result: Neither humans nor LLMs are general learners; dual optimization (training + benchmark selection) makes LLMs specialized learners unsuitable as unbiased models of human language acquisition

## Executive Summary
The paper challenges the widespread assumption that large language models (LLMs) can serve as unbiased models of human language acquisition. It introduces the concept of dual optimization, arguing that LLMs undergo both traditional training and a selection process analogous to natural selection through benchmarks. This dual process makes them specialized rather than general learners, with biases that differ significantly from human cognitive constraints. The authors demonstrate that LLMs can learn "impossible languages" and secret codes that humans cannot, further supporting their argument that LLMs' learning processes are fundamentally different from human language acquisition.

## Method Summary
The paper presents a theoretical analysis of LLMs as learners, introducing the concept of dual optimization where models are both trained via gradient descent and selected through benchmark performance. Rather than providing a specific experimental methodology, the authors synthesize existing research on language acquisition, cognitive science, and LLM development to argue that this dual optimization process creates specialized learners with unique biases. The analysis focuses on comparing human and machine learning constraints, particularly regarding what languages are learnable, and examines how benchmark-driven selection shapes model development beyond standard training objectives.

## Key Results
- LLMs are not general learners but specialized ones due to dual optimization (training + benchmark selection)
- LLMs can learn "impossible languages" and secret codes beyond human cognitive constraints
- Benchmark-driven selection actively shapes LLM development, making them unsuitable as direct models for human language acquisition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs undergo dual optimization—traditional training plus selection via benchmarks—making them specialized rather than general learners
- Mechanism: LLMs are first optimized during training via gradient descent on an explicit loss function, then undergo selection where only best-performing models on benchmarks survive and are refined, creating a feedback loop where benchmarks become part of the optimization objective
- Core assumption: Benchmark performance actively shapes development through selection pressure, not just measures success
- Evidence anchors:
  - [abstract] "We make a novel case for how in particular LLMs follow a dual-optimization process: they are optimized during their training... and modern LLMs have also been selected, through a process akin to natural selection"
  - [section] "LLMs are not vanilla learners. First, they are optimized during training, on an explicit loss... Second, they also follow an evolution-like optimization process, through which the best models are selected"
- Break condition: If benchmark influence is removed or models are randomly sampled without performance-based selection, dual optimization collapses to standard training alone

### Mechanism 2
- Claim: LLMs learn "impossible languages" (e.g., secret languages via autoprompt) because their biases aren't aligned with human cognitive constraints
- Mechanism: When exposed to human language, LLMs learn extensions or codes foreign to humans because their inductive biases, shaped by dual optimization, allow extrapolation beyond human-possible generalizations
- Core assumption: The space of languages LLMs can learn includes configurations humans cannot due to differences in inductive biases
- Evidence anchors:
  - [section] "When exposed to a language like English, a young human learns English... They do not add an additional language or code the way LLMs do. Particular non-human biases seem to push LLMs in this direction"
  - [section] "LLMs may do so even when exposed to a proper training from actual human language... Instead, there is some combination of token embeddings such that 'w1 w2 w3 [X] ...' would be completed more often than the English prompt"
- Break condition: If LLMs were constrained to learn only within human-possible generalizations, or if their internal representations were aligned with human linguistic cognition, they would not learn secret languages

### Mechanism 3
- Claim: LLMs cannot serve as unbiased models of human language acquisition because they're engineered to optimize for specific tasks and benchmarks
- Mechanism: Dual optimization tailors LLMs to perform well on engineered benchmarks that may not reflect natural human learning trajectories, making their learning order, biases, and generalization patterns different from humans
- Core assumption: Optimization objectives for LLMs (task performance, benchmark scores) are not equivalent to objectives of human language acquisition
- Evidence anchors:
  - [abstract] "The authors conclude that LLMs' learning processes and outcomes are significantly influenced by their optimization, making them unsuitable as direct models for human language acquisition debates"
  - [section] "If we optimize our systems, directly or indirectly, to follow the target developmental path, then we will lose the ability to argue that this developmental path is natural"
- Break condition: If LLMs were trained without benchmark-driven selection, or if their optimization objectives were aligned with human developmental constraints, they might serve as better models

## Foundational Learning

- Concept: Inductive biases in learners
  - Why needed here: The paper argues that all learners, including LLMs, have biases that shape what they learn; understanding inductive biases is crucial to grasping why LLMs cannot be general learners
  - Quick check question: Can you name an example of an inductive bias in a machine learning model (e.g., preference for simpler explanations, hierarchical structure)?

- Concept: Dual optimization vs. single optimization
  - Why needed here: The paper distinguishes between traditional training (single optimization) and additional selection pressure from benchmarks (dual optimization); this distinction is central to the argument
  - Quick check question: How does the inclusion of benchmark-based selection change the behavior of a learning system compared to training alone?

- Concept: Possible vs. impossible languages
  - Why needed here: The paper discusses how humans are constrained to learn only "possible" languages while LLMs can learn "impossible" ones; this concept is key to understanding limits of human vs. machine learners
  - Quick check question: What is an example of a linguistic constraint that humans follow but LLMs might violate?

## Architecture Onboarding

- Component map:
  - Data ingestion → Tokenization → Model architecture (e.g., transformer) → Loss computation → Gradient descent updates
  - Model inference on benchmark tasks → Performance evaluation → Selection/rejection → Fine-tuning
  - Training results → Benchmark performance → Model selection → Next-generation training

- Critical path:
  1. Data preparation and tokenization
  2. Model training with gradient descent
  3. Benchmark evaluation
  4. Selection of top-performing models
  5. Fine-tuning or architectural adjustments
  6. Repeat from step 2

- Design tradeoffs:
  - Training scale vs. computational cost: Larger models require more data and compute but may generalize better
  - Benchmark specificity vs. generality: Narrow benchmarks may overfit models to specific tasks; broad benchmarks may not capture important capabilities
  - Inductive bias control vs. flexibility: Stronger biases may improve performance on known tasks but limit adaptability

- Failure signatures:
  - Overfitting to benchmarks: Model performs well on benchmarks but poorly on real-world tasks
  - Learning impossible languages: Model generates outputs that violate human linguistic constraints
  - Benchmark gaming: Model exploits loopholes in benchmark design rather than learning intended capability

- First 3 experiments:
  1. Train two identical LLMs on same data, but only evaluate one on benchmarks; compare learned representations to see if benchmark pressure shapes internal structure
  2. Expose an LLM to controlled "impossible language" and measure whether it learns it as easily as human-possible language; vary degree of impossibility
  3. Implement modified training loop where benchmarks are explicitly part of loss function; compare learning trajectories and final performance to standard training

## Open Questions the Paper Calls Out
None

## Limitations
- The paper's central claims rely heavily on theoretical argumentation rather than empirical validation
- The concept of "impossible languages" is challenging to operationalize and verify empirically
- The paper does not address potential confounding factors such as data quality, architectural constraints, or impact of different training objectives

## Confidence
- **High Confidence**: The claim that LLMs are not general learners is well-supported by the theoretical framework and aligns with established understanding of inductive biases in machine learning systems
- **Medium Confidence**: The dual optimization framework is conceptually sound but lacks empirical validation; the mechanism by which benchmark selection creates distinct optimization pressure is plausible but not rigorously demonstrated
- **Low Confidence**: Specific claims about LLMs learning "impossible languages" and practical implications of dual optimization for modeling human language acquisition are largely theoretical without concrete empirical evidence

## Next Checks
1. **Empirical test of dual optimization effects**: Train two identical transformer models on same data, but subject only one to benchmark-based selection and fine-tuning; compare learned representations, generalization patterns, and vulnerability to adversarial examples to quantify impact of benchmark pressure

2. **Impossible language generation experiment**: Systematically generate languages with varying degrees of violation of human linguistic universals (e.g., violating phonological constraints, semantic compositionality, or syntactic dependencies); measure ease with which LLMs can learn and generate these languages compared to human-possible languages, and compare against human performance on analogous tasks

3. **Benchmark optimization ablation study**: Implement modified training loop where benchmark performance is explicitly included in loss function versus control where benchmarks are only used for evaluation; track learning trajectories, final performance on held-out tasks, and emergence of "secret languages" or other non-human patterns in model outputs