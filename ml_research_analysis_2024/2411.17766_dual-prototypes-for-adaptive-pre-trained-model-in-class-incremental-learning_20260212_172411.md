---
ver: rpa2
title: Dual Prototypes for Adaptive Pre-Trained Model in Class-Incremental Learning
arxiv_id: '2411.17766'
source_url: https://arxiv.org/abs/2411.17766
tags:
- uni00000013
- learning
- dpta
- task
- prototypes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses catastrophic forgetting in class-incremental
  learning (CIL) when fine-tuning pre-trained models (PTMs) on sequential task streams.
  The proposed Dual-Prototype Network with Task-wise Adaptation (DPTA) introduces
  lightweight adapters per task and a Center-Adapt loss to produce compact, discriminative
  representations.
---

# Dual Prototypes for Adaptive Pre-Trained Model in Class-Incremental Learning

## Quick Facts
- arXiv ID: 2411.17766
- Source URL: https://arxiv.org/abs/2411.17766
- Reference count: 40
- Primary result: Dual-Prototype Network with Task-wise Adaptation (DPTA) achieves state-of-the-art performance on multiple class-incremental learning benchmarks, improving average accuracy by 1%-5% over existing methods.

## Executive Summary
This paper addresses catastrophic forgetting in class-incremental learning when fine-tuning pre-trained models on sequential task streams. The proposed Dual-Prototype Network with Task-wise Adaptation (DPTA) introduces lightweight adapter modules per task and a Center-Adapt loss to produce compact, discriminative representations. DPTA employs two prototype sets: raw prototypes for identifying candidate labels and tasks, and augmented prototypes for final classification. This design enables test-time adapter selection without explicit task IDs. Experiments on multiple benchmarks (CIFAR-100, Stanford Cars, ImageNet-A/R, VTAB) show DPTA consistently outperforms state-of-the-art methods by 1%-5%, with up to 3% improvement on VTAB. The method also demonstrates robust performance across different PTM backbones and fine-tuning modules, highlighting its generalization and plug-and-play flexibility.

## Method Summary
DPTA uses task-specific adapter modules to fine-tune a frozen pre-trained model (PTM) for each new task, with a Center-Adapt loss that combines cross-entropy and center loss to produce compact, discriminative representations. The method constructs two prototype sets: raw prototypes from the frozen PTM for candidate label inference, and augmented prototypes from the task-adapted PTM for final classification. During inference, raw prototypes identify top-K candidate labels to select appropriate adapters, which are then used with augmented prototypes for classification. This dual-prototype design enables test-time adapter selection without explicit task IDs while maintaining strong discriminative power.

## Key Results
- DPTA achieves 1%-5% average accuracy improvement over state-of-the-art methods on CIFAR-100, Stanford Cars, ImageNet-A/R, and VTAB benchmarks
- The method shows up to 3% improvement on VTAB with imbalanced tasks
- DPTA demonstrates robust performance across different PTM backbones and fine-tuning modules
- The Center-Adapt loss produces compact, isotropic clusters with enlarged inter-cluster margins

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dual prototype design enables robust adapter selection without task IDs by leveraging the reliability of raw prototype-based top-K predictions.
- Mechanism: Raw prototypes provide highly stable top-K candidate labels, from which task indices are inferred. This allows loading only the correct adapter modules for augmented prototypes, reducing noise from mismatched task representations.
- Core assumption: Top-K predictions using raw prototypes remain reliable even when top-1 accuracy drops, and raw prototypes' generalization remains stronger than task-adapted ones.
- Evidence anchors: [abstract] "where the raw prototypes deduce several possible task indexes of test samples to select suitable adapter modules for PTM"; [section] "our analysis reveals that top-K predictions using raw PTM features remain highly stable, even when top-1 accuracy drops... This suggests that top-K raw predictions can reliably identify a small candidate label set"
- Break condition: If top-K accuracy degrades significantly or if tasks become too visually similar (e.g., low-resolution datasets), the candidate set becomes too large or ambiguous, reducing adapter selection reliability.

### Mechanism 2
- Claim: The Center-Adapt (CA) loss produces compact, discriminative augmented prototypes by combining cross-entropy's repulsive forces with center loss's attractive forces.
- Mechanism: CA loss encourages representations to cluster tightly around class centers while maintaining large inter-class margins, producing augmented prototypes that are both compact and discriminative for classification.
- Core assumption: Prototype-based classification benefits from compact intra-class clusters and large inter-class separation, and the combination of CE and center loss achieves this balance better than either alone.
- Evidence anchors: [abstract] "where the center-adapt loss forces the representation to be more centrally clustered and class separable"; [section] "We introduce a center-adapt loss that encourages adapted representations to cluster around class centers while enlarging inter-class margins" and "fine-tuning with CA loss yields compact, approximately isotropic clusters around class prototypes and clearly enlarged inter-cluster margins"; [section] "To mitigate this effect, we combine CL with the repulsive influence of Cross-Entropy (CE)... The two components are complementary"
- Break condition: If λ is set too high, the center loss dominates and causes prototype mode collapse; if too low, the model degenerates to CE-only training without sufficient compactness.

### Mechanism 3
- Claim: The augmented prototypes naturally filter out mismatched task adapters through representation space separation.
- Mechanism: When a test sample is processed with an incorrect adapter, its augmented representation is projected into a different subspace, making it an outlier with consistently low similarity to the correct task's augmented prototypes.
- Core assumption: PTMs with mismatched adapters produce representations that are naturally far from the correct task's feature region, and augmented prototypes can implicitly suppress these mismatches.
- Evidence anchors: [abstract] "augmented prototypes that could separate confusable classes are utilized to determine the final result"; [section] "the representations produced by a PTM equipped with an incorrect adapter tend to fall far from the corresponding task's feature region" and "samples from other tasks do not participate in its adaptation, so their augmented representations are projected away from all class clusters"; [section] "augmented representations generated with an incorrect adapter become clear outliers. They lie far from every augmented prototype and exhibit consistently lower similarity, which could be naturally suppressed"
- Break condition: If tasks have highly overlapping feature distributions or if adapters produce representations that are not sufficiently task-distinct, the filtering mechanism becomes ineffective.

## Foundational Learning

- Concept: Catastrophic forgetting in class-incremental learning
  - Why needed here: The paper's core problem is that fine-tuning PTMs on sequential task streams causes catastrophic forgetting, and DPTA specifically addresses this through task-wise adaptation
  - Quick check question: What happens to a model's performance on old tasks when it's fine-tuned on new tasks without any mitigation strategy?

- Concept: Prototype-based classification (Nearest Class Mean)
  - Why needed here: DPTA uses dual prototypes (raw and augmented) for classification, and the center-adapt loss is specifically designed to improve prototype separability
  - Quick check question: How does a prototype-based classifier determine the class of a new sample?

- Concept: Adapter modules in vision transformers
  - Why needed here: DPTA uses lightweight adapter modules per task for fine-tuning, and understanding their structure and placement is crucial for implementation
  - Quick check question: Where are adapter modules typically inserted in transformer blocks, and what operations do they perform?

## Architecture Onboarding

- Component map: Frozen PTM backbone -> Task-specific adapter modules -> Raw prototype set -> Augmented prototype set -> Center-adapt loss -> Two-stage inference pipeline

- Critical path: 1. For each new task: fine-tune adapter with CA loss -> construct raw and augmented prototypes; 2. At inference: compute raw similarities -> select top-K candidates -> load corresponding adapters -> compute augmented similarities -> final prediction

- Design tradeoffs:
  - Adapter vs prompt tuning: Adapters provide better accuracy-efficiency tradeoff for task adaptation
  - Number of adapters: More adapters increase storage but enable better task specialization
  - K value: Higher K improves recall but increases inference time and adapter loading

- Failure signatures:
  - Degraded top-K accuracy -> adapter selection failures
  - Prototype mode collapse -> check λ value and CA loss balance
  - High inference latency -> excessive K or too many adapters loaded
  - Poor cross-task discrimination -> check task adapter separation in feature space

- First 3 experiments:
  1. Verify top-K accuracy stability on raw prototypes across tasks (should be >95%)
  2. Test adapter selection accuracy using raw prototypes on a held-out validation set
  3. Measure prototype dispersion metrics (Sinter, Smax, R) with and without CA loss to confirm improved separation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DPTA's performance scale when the number of classes per task is highly imbalanced across incremental tasks?
- Basis in paper: [inferred] The paper shows DPTA performs well on VTAB with imbalanced tasks, but does not systematically evaluate performance across different imbalance distributions or ratios.
- Why unresolved: The paper only tests on specific benchmark datasets with fixed class distributions, lacking comprehensive ablation studies on varying imbalance scenarios.
- What evidence would resolve it: Systematic experiments varying the number of classes per task (e.g., exponential decay, uniform, or random distributions) across multiple datasets, measuring final accuracy and stability metrics.

### Open Question 2
- Question: What is the theoretical upper bound on DPTA's performance when the inter-task feature distribution overlap becomes extremely high?
- Basis in paper: [inferred] The paper notes DPTA's limitation on CIFAR-100 due to low inter-task feature disparity, suggesting performance degradation when task separation is weak.
- Why unresolved: The paper does not provide quantitative analysis of the relationship between inter-task feature overlap and accuracy degradation, nor does it propose mechanisms to mitigate this limitation.
- What evidence would resolve it: Empirical studies measuring feature space overlap (e.g., using metrics like maximum cosine similarity between task prototypes) across datasets with varying degrees of task similarity, coupled with corresponding accuracy trends.

### Open Question 3
- Question: Can the dual-prototype mechanism be extended to work effectively in scenarios where task identities are completely ambiguous or non-existent?
- Basis in paper: [explicit] The paper states that raw prototypes "estimate top-K candidate labels" and "reveal up to K possible task indices when the task label correspondence established during training is preserved."
- Why unresolved: The paper assumes preserved task label correspondence during training, but does not address scenarios where this correspondence is lost or where tasks are not naturally separable.
- What evidence would resolve it: Experiments where task-label correspondence is deliberately scrambled during training, or where tasks are synthetically merged/combined, measuring whether DPTA can still identify appropriate adapters and maintain reasonable accuracy.

## Limitations

- Performance degradation on CIFAR-100 due to low inter-task feature disparity, where raw prototype top-K predictions become less reliable
- Reliance on preserved task label correspondence during training, limiting applicability to scenarios with ambiguous or non-existent task identities
- Increased storage requirements proportional to the number of tasks due to task-specific adapter modules

## Confidence

- **High Confidence**: The dual prototype architecture design and its basic functionality (raw prototypes for adapter selection, augmented prototypes for classification)
- **Medium Confidence**: The Center-Adapt loss mechanism for improving prototype separability, as supported by internal metrics but lacking comparative ablation studies
- **Medium Confidence**: The claim that augmented prototypes naturally filter mismatched adapters, which is theoretically sound but could fail in cases of high task similarity

## Next Checks

1. Verify that raw prototype top-K accuracy remains above 95% across all tasks during inference, as this is critical for reliable adapter selection.

2. Measure Sinter, Smax, and R metrics on augmented prototypes with and without Center-Adapt loss to confirm the claimed improvements in inter-class separation.

3. Test the adapter selection pipeline on a held-out validation set to ensure that raw prototypes correctly identify the task index in >95% of cases, validating the core mechanism of the dual prototype design.