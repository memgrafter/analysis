---
ver: rpa2
title: Fine-tuning Smaller Language Models for Question Answering over Financial Documents
arxiv_id: '2408.12337'
source_url: https://arxiv.org/abs/2408.12337
tags:
- question
- code
- total
- python
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Smaller language models can be fine-tuned to approach the performance\
  \ of much larger teacher models on financial question answering tasks, with improvements\
  \ driven primarily by enhanced concept understanding and entity extraction. Using\
  \ GPT-4-generated Python programs as training exemplars, fine-tuning refines the\
  \ student models\u2019 ability to express and apply financial concepts and adapt\
  \ to specific data formats."
---

# Fine-tuning Smaller Language Models for Question Answering over Financial Documents

## Quick Facts
- arXiv ID: 2408.12337
- Source URL: https://arxiv.org/abs/2408.12337
- Reference count: 37
- Primary result: Smaller models fine-tuned with GPT-4 exemplars achieve comparable financial QA performance to much larger teacher models

## Executive Summary
This paper demonstrates that smaller language models can be effectively fine-tuned to approach the performance of much larger teacher models on financial question answering tasks. The approach uses GPT-4 to generate Python programs that encode financial reasoning chains, which are then used to train smaller student models through LoRA fine-tuning. The method achieves significant improvements in concept understanding and entity extraction, with models trained on just 1,000 samples from FinQA and 500 from ConvFinQA matching the performance of those trained on full datasets.

## Method Summary
The method involves using GPT-4 as a teacher model to generate Python programs for financial question-answering tasks, then fine-tuning smaller student models (PHI-3, ORCA-2, MISTRAL) using LoRA with these exemplars. The approach focuses on prompt-completion pairs where the prompt is the financial question and the completion is executable Python code representing the reasoning chain. Models are evaluated using GPT-4-assisted assessment of concept accuracy, entity extraction, and code generation capabilities across multiple financial datasets.

## Key Results
- Fine-tuned smaller models achieve concept accuracy comparable to GPT-4 on financial QA tasks
- Models trained on 1,000 FinQA samples and 500 ConvFinQA samples match full-dataset performance
- Concept understanding improvements drive the largest performance gains (58.45% improvement)
- LoRA fine-tuning enables efficient adaptation without full model retraining

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning refines the student models' ability to express and apply the required financial concepts. By training on GPT-4-generated Python programs, student models learn a structured reasoning chain that explicitly encodes financial formulas and required calculations.

### Mechanism 2
Fine-tuning improves entity extraction performance by adapting the model to the specific data format of the financial dataset. The fine-tuning process exposes the model to the exact table and text formats used in the financial documents, allowing it to learn the precise entity extraction patterns needed for the task.

### Mechanism 3
Fine-tuning induces reasoning capabilities in smaller models by providing them with well-formed reasoning exemplars from a larger teacher model. GPT-4 acts as a reasoning teacher, generating Python programs that break down complex financial reasoning into executable steps.

## Foundational Learning

- Concept: Python code generation for structured reasoning
  - Why needed here: The approach relies on generating Python code that encodes the required financial reasoning, rather than directly answering the question
  - Quick check: Can you write a Python program that calculates the average of two numbers given their values?

- Concept: Financial domain knowledge and terminology
  - Why needed here: The model needs to understand financial concepts like "industry segment operating profits" and "credit spread" to correctly extract entities and apply formulas
  - Quick check: What is the difference between "revenue" and "profit" in financial statements?

- Concept: LoRA (Low-Rank Adaptation) for efficient fine-tuning
  - Why needed here: LoRA allows fine-tuning large language models efficiently by adapting a small number of parameters rather than the full model
  - Quick check: What is the main advantage of using LoRA over full fine-tuning for large language models?

## Architecture Onboarding

- Component map:
  Teacher model (GPT-4) -> Student models (PHI-3, ORCA-2, MISTRAL) -> Python interpreter -> GPT-4 evaluator

- Critical path:
  1. Generate training data: Teacher model creates Python programs for training questions
  2. Fine-tune student model: Train student model on prompt-completion pairs using LoRA
  3. Generate inference code: Student model generates Python code for new questions
  4. Execute and evaluate: Run generated code and compare output to ground truth

- Design tradeoffs:
  - Using code generation vs. direct answer generation trades complexity for improved reasoning accuracy
  - Fine-tuning on a smaller dataset vs. full dataset trades data efficiency for potential performance loss
  - Using LoRA vs. full fine-tuning trades computational efficiency for potential performance gains

- Failure signatures:
  - Generated code fails to execute (syntax errors, undefined variables)
  - Generated code produces incorrect answers despite being executable
  - Generated code misses required entities or uses incorrect formulas
  - Fine-tuned model performance does not improve over base model

- First 3 experiments:
  1. Evaluate base model concept accuracy using GPT-4 assessment on a small sample of FinQA data
  2. Fine-tune a small student model (e.g., PHI-3-MINI) on 1000 FinQA samples and evaluate concept accuracy improvement
  3. Compare fine-tuned model performance on FinQA vs. ConvFinQA to assess domain adaptation capability

## Open Questions the Paper Calls Out

### Open Question 1
How does fine-tuning smaller models on domain-specific tasks like financial QA affect their performance on out-of-domain tasks? The paper focuses on financial QA but doesn't explore generalization to other domains.

### Open Question 2
What are the long-term effects of fine-tuning smaller models on their ability to adapt to new tasks or datasets? The paper doesn't provide information on the long-term adaptability of fine-tuned models.

### Open Question 3
How do different fine-tuning strategies (e.g., LoRA vs. full fine-tuning) impact the performance and efficiency of smaller models in financial QA? The paper uses LoRA but doesn't compare it with other fine-tuning methods.

## Limitations

- The study relies on GPT-4 as both a data generator and evaluator, creating potential circularity in the evaluation process
- The effectiveness depends heavily on the quality of GPT-4-generated Python programs
- The approach focuses on specific financial datasets, limiting generalizability to other domains

## Confidence

**High Confidence:** The core finding that fine-tuning smaller models with GPT-4-generated Python programs can significantly improve financial reasoning performance is well-supported by experimental results.

**Medium Confidence:** The claim that 1,000 samples from FinQA and 500 from ConvFinQA are sufficient for matching full-dataset performance is supported but could benefit from additional validation.

**Medium Confidence:** The mechanism explaining that improved concept understanding drives most performance gains is supported but doesn't fully isolate other contributing factors.

## Next Checks

1. Evaluate GPT-4 evaluator consistency by testing evaluation prompts on known correct and incorrect samples to ensure reliable scoring

2. Test domain transferability by fine-tuning models on FinQA and evaluating performance on TATQA and ConvFinQA

3. Analyze error patterns by examining specific cases where fine-tuned models fail, comparing failures in code generation versus reasoning accuracy