---
ver: rpa2
title: 'OneEncoder: A Lightweight Framework for Progressive Alignment of Modalities'
arxiv_id: '2409.11059'
source_url: https://arxiv.org/abs/2409.11059
tags:
- image
- text
- modalities
- oneencoder
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OneEncoder, a lightweight framework for progressively
  aligning multiple modalities (image, text, audio, and video) without requiring vast
  aligned datasets or retraining the entire model when adding new modalities. The
  approach uses frozen pretrained modality-specific encoders and trains only a Universal
  Projection (UP) module and an Alignment Layer (AL), significantly reducing computational
  cost.
---

# OneEncoder: A Lightweight Framework for Progressive Alignment of Modalities

## Quick Facts
- arXiv ID: 2409.11059
- Source URL: https://arxiv.org/abs/2409.11059
- Reference count: 40
- One-line primary result: Achieves strong cross-modal alignment performance with minimal data and computational cost through progressive alignment of modalities

## Executive Summary
OneEncoder introduces a lightweight framework for progressively aligning multiple modalities (image, text, audio, video) without requiring vast aligned datasets or retraining the entire model when adding new modalities. The approach uses frozen pretrained modality-specific encoders and trains only a Universal Projection (UP) module and an Alignment Layer (AL), significantly reducing computational cost. The framework demonstrates strong performance in tasks like classification, querying, and visual question answering, outperforming traditional methods that rely on large datasets and simultaneous training of multiple encoders.

## Method Summary
OneEncoder uses a two-step progressive training approach: first, a Universal Projection (UP) module is trained to align image and text modalities using contrastive learning on a combined small dataset. Then, for each new modality, only a compact Alignment Layer (AL) is trained while the UP remains frozen, projecting the new modality's features into the shared space defined by the UP. The framework leverages frozen, pretrained encoders for feature extraction and introduces modality tokens to enable efficient switching between modalities during the forward pass.

## Key Results
- Zero-shot image classification: 78.15% accuracy on CIFAR-10 (OneEncoder-1) vs 62.12% (CLIP)
- Text-audio retrieval: P@1 of 0.86 and R@1 of 0.83 on LibriSpeech SRA
- Text-video retrieval: R@1 of 0.79 and R@5 of 0.94 on MSR-VTT
- Progressive alignment achieves strong cross-modal performance without retraining entire model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OneEncoder progressively aligns modalities by freezing pretrained encoders and training only a lightweight UP module and AL, enabling cost-effective alignment without retraining the entire model when adding new modalities.
- Mechanism: The framework uses frozen, modality-specific encoders for feature extraction, while the UP module aligns the initial image-text modalities. For each new modality, only the AL is trained, projecting the new modality's features into the shared space defined by the frozen UP. This allows transitive alignment: once a new modality is aligned with one of the existing modalities, it is implicitly aligned with all previously aligned modalities.
- Core assumption: The frozen pretrained encoders produce high-quality, fixed representations, and the UP module creates a well-defined shared alignment space that can accommodate new modalities via the AL without loss of alignment quality.
- Evidence anchors:
  - [abstract] "Initially, we train a lightweight Universal Projection module (UP) to align image and text modalities. Then, we freeze the pretrained UP and progressively align future modalities to those already aligned."
  - [section III-B] "In Step 2, which is consistent across all future modalities, we freeze the pretrained UP and train only the compact AL."
  - [corpus] Weak evidence. Related papers discuss multimodal alignment but do not specifically validate the progressive, freeze-then-align approach used in OneEncoder.
- Break condition: If the pretrained encoders produce poor or unstable features, or if the UP's shared space is not sufficiently rich to allow effective projection by the AL for new modalities, alignment quality will degrade.

### Mechanism 2
- Claim: OneEncoder achieves strong performance with minimal aligned datasets by leveraging the robustness of contrastive learning on a combined small dataset and by using a lightweight architecture that avoids retraining large modality-specific encoders.
- Mechanism: The framework uses a contrastive loss (InfoNCE) to align image and text in a shared latent space, trained on a modest dataset. The UP and AL are small compared to full modality encoders, so training converges faster and with less data. For new modalities, only the AL is trained, further reducing data requirements.
- Core assumption: Contrastive learning on a small but diverse dataset is sufficient to create a meaningful shared embedding space, and the lightweight UP/AL can effectively project new modalities into this space without extensive data.
- Evidence anchors:
  - [section IV-A] "Following the approach of virTex [36] and related studies [37, [38], we train the UP module on a combined dataset, which includes COCO Captions [39], Flickr30K [40], and TextCaps [41]."
  - [section IV-B] "Our goal is to achieve robust performance on downstream tasks using a lightweight framework trained on a modest dataset."
  - [corpus] Weak evidence. The corpus neighbors discuss multimodal alignment but do not provide direct evidence for performance with minimal aligned datasets.
- Break condition: If the dataset is too small or lacks diversity, the contrastive loss may fail to create a robust shared space, and the lightweight modules may not have enough capacity to project new modalities accurately.

### Mechanism 3
- Claim: The use of modality tokens enables efficient switching between modalities during the forward pass, allowing the same UP module to handle all modalities without retraining.
- Mechanism: Learnable modality tokens (one per modality) are concatenated or added to the input tokens before passing them through the UP module. During inference, the appropriate token is selected based on the modality, directing the UP to process that modality's features in the shared space.
- Core assumption: The modality tokens effectively condition the UP module to process different modalities correctly, and the fusion operation (concatenation, addition, or cross-attention) preserves the semantic content of the modality-specific features.
- Evidence anchors:
  - [section III-A] "To facilitate modality switching within the UP, we introduce learnable modality tokens {tm}m∈M ∈ RN ×D, as used in [14], which consists of N tokens of dimension D, for each modality m ∈ M."
  - [section III-A] "During the forward pass for modality m, we input the fusion of input tokens xm ∈ RL×D and modality tokens tm into the UP module: ˆ xm = UP(tm ⊗ xm)"
  - [corpus] No direct evidence. The corpus neighbors do not discuss modality tokens or their role in multimodal alignment.
- Break condition: If the modality tokens are not sufficiently expressive or the fusion operation is not effective, the UP may fail to correctly process different modalities, leading to poor alignment.

## Foundational Learning

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: The framework relies on contrastive learning to align different modalities in a shared latent space. Understanding how InfoNCE loss works and how it encourages similar representations for aligned pairs while pushing apart non-aligned pairs is crucial for grasping the alignment mechanism.
  - Quick check question: How does the InfoNCE loss function encourage alignment between paired modalities while maintaining separation between non-paired ones?

- Concept: Progressive training and freezing layers
  - Why needed here: OneEncoder uses a two-step training process where the UP is trained first and then frozen while the AL is trained for new modalities. Understanding the benefits and potential pitfalls of freezing layers and training only specific parts of a model is essential for understanding the framework's efficiency and scalability.
  - Quick check question: What are the advantages and potential drawbacks of freezing the UP module after initial training and only training the AL for new modalities?

- Concept: Modality-specific feature extraction and shared embedding spaces
  - Why needed here: The framework uses frozen, pretrained encoders for each modality to extract features, which are then projected into a shared embedding space. Understanding how modality-specific features can be effectively projected into a common space and how this enables cross-modal tasks is key to understanding the framework's design.
  - Quick check question: How can features extracted by different modality-specific encoders be effectively projected into a shared embedding space to enable tasks like cross-modal retrieval or zero-shot classification?

## Architecture Onboarding

- Component map: Frozen modality-specific encoders (ViT, BERT, Wav2Vec, VideoMAE) -> Modality tokens -> UP module -> AL (for non-image/text modalities) -> Shared embedding space

- Critical path: 1. Extract features from frozen modality-specific encoders 2. Fuse features with modality tokens 3. Pass through UP (or UP + AL for non-image/text modalities) 4. Apply contrastive loss (for training) or perform cross-modal task (for inference)

- Design tradeoffs:
  - Using frozen encoders vs. training them from scratch: Saves computation and data but relies on pretrained encoders' quality
  - Lightweight UP and AL vs. larger modules: Reduces parameters and data needs but may limit representational capacity
  - Progressive alignment vs. simultaneous alignment: Allows adding modalities without retraining but may introduce alignment errors over multiple steps

- Failure signatures:
  - Poor performance on cross-modal tasks despite good unimodal performance: Indicates misalignment in the shared space
  - Degradation in performance when adding new modalities: Suggests the AL is not effectively projecting new modalities into the UP's space
  - High variance in results across different runs: May indicate instability in the contrastive learning process or modality token learning

- First 3 experiments:
  1. Train the UP on image-text alignment using a small subset of COCO Captions and evaluate zero-shot image classification performance compared to CLIP.
  2. Add audio alignment by training the AL on LibriSpeech SRA and evaluate text-audio retrieval performance.
  3. Add video alignment by training the AL on MSR-VTT and evaluate text-video retrieval performance, checking transitive alignment by performing audio-video retrieval.

## Open Questions the Paper Calls Out
None

## Limitations
- The framework's performance depends critically on the quality of frozen pretrained encoders, which are not guaranteed to produce optimal representations for all modalities.
- Progressive alignment assumes transitive alignment maintains quality, but this assumption is not explicitly validated through direct alignment comparisons.
- The use of small datasets for training the UP module may limit the robustness of the shared embedding space, particularly for fine-grained distinctions.

## Confidence

**High confidence:** The framework successfully implements progressive alignment with frozen encoders and achieves strong performance on standard benchmarks (CIFAR-10 zero-shot classification, text-image retrieval metrics).

**Medium confidence:** The lightweight design (reduced parameters and data requirements) directly contributes to the strong performance, as the paper does not provide sufficient ablation studies comparing full vs. lightweight implementations.

**Low confidence:** The transitive alignment assumption (that aligning new modalities to one existing modality ensures alignment with all previous modalities) is valid across all possible modality combinations and task types.

## Next Checks

1. **Direct vs. Progressive Alignment Comparison:** Train a baseline model that aligns all four modalities simultaneously (rather than progressively) using the same UP module size and dataset. Compare the alignment quality (retrieval metrics) between progressive and direct approaches to quantify any degradation from transitive alignment.

2. **Encoder Quality Impact Analysis:** Replace one pretrained encoder (e.g., use a lower-quality ViT variant) while keeping others fixed. Measure how performance degrades across all modalities to establish the sensitivity of the framework to individual encoder quality.

3. **Dataset Size Sensitivity Test:** Train the UP module using increasingly smaller subsets of the COCO Captions/Flickr30K dataset (10%, 25%, 50%, 100%) while keeping all other components fixed. Track performance on zero-shot classification and retrieval tasks to determine the minimum dataset size required for robust alignment.