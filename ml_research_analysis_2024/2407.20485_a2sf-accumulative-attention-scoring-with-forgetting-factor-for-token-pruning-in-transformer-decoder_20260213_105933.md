---
ver: rpa2
title: 'A2SF: Accumulative Attention Scoring with Forgetting Factor for Token Pruning
  in Transformer Decoder'
arxiv_id: '2407.20485'
source_url: https://arxiv.org/abs/2407.20485
tags:
- attention
- tokens
- token
- score
- a2sf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses memory inefficiency in large language models
  caused by growing KV cache in long sequences. It observes that existing accumulative
  attention scoring is unfair in decoder models due to causal masking, leading to
  early tokens being overestimated.
---

# A2SF: Accumulative Attention Scoring with Forgetting Factor for Token Pruning in Transformer Decoder

## Quick Facts
- arXiv ID: 2407.20485
- Source URL: https://arxiv.org/abs/2407.20485
- Reference count: 28
- Key outcome: A2SF improves accuracy by up to 7.8% (1-shot) and 5.1% (0-shot) compared to prior methods, with higher token selection similarity to ideal pruning masks

## Executive Summary
This paper addresses memory inefficiency in large language models caused by growing KV cache in long sequences. It observes that existing accumulative attention scoring is unfair in decoder models due to causal masking, leading to early tokens being overestimated. The authors propose A2SF (Accumulative Attention Score with Forgetting Factor), which applies exponential decay to past attention scores, reducing bias toward early tokens. Experiments on LLaMA and OPT models show significant accuracy improvements over prior methods while maintaining efficient token pruning for KV cache compression.

## Method Summary
A2SF introduces a forgetting factor α (0 < α < 1) that exponentially decays past attention scores during the accumulation process: Ah_n,k = Σ(q=1 to n) α^(n-q) × Sh_q,k. This mechanism corrects the positional bias in decoder models where causal masking causes early tokens to accumulate disproportionately high attention scores. By applying exponential penalties to older scores, A2SF enables fairer comparison among tokens regardless of their position in the sequence, leading to more accurate token pruning decisions while maintaining model performance with compressed KV cache.

## Key Results
- A2SF improves accuracy by up to 7.8% (1-shot) and 5.1% (0-shot) compared to prior methods
- Higher token selection similarity to ideal pruning masks than existing techniques
- Effective across multiple cache ratios [0.1, 0.8] with LLaMA and OPT models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Causal masking in decoder models creates uneven attention score accumulation that favors early tokens
- Mechanism: The masked self-attention operation prevents each token from attending to future tokens, resulting in tokens earlier in the sequence accumulating more attention scores over time compared to later tokens. This creates an inherent bias where tokens that appear earlier in the sequence have higher accumulated attention scores simply due to their position, not their actual importance.
- Core assumption: The number of valid attention connections a token can form is directly proportional to its position in the sequence, with earlier tokens having more connections and thus accumulating more scores.
- Evidence anchors:
  - [section] "In the decoder model, the number of times the Attention Score accumulates varies depending on the order of token appearance due to the effect of masking, causing an uneven comparison between tokens."
  - [abstract] "It observes that existing accumulative attention scoring is unfair in decoder models due to causal masking, leading to early tokens being overestimated."
  - [corpus] Weak evidence - only 1 corpus paper mentions causal masking specifically, suggesting this is a relatively under-explored aspect of token pruning

### Mechanism 2
- Claim: Exponential decay through a forgetting factor corrects the positional bias by reducing the influence of past attention scores
- Mechanism: By multiplying each past attention score by a forgetting factor (α) raised to the power of time elapsed (n-q), older attention scores are exponentially discounted. This ensures that tokens maintain importance based on recent attention patterns rather than accumulated historical scores. The exponential decay means tokens that appeared earlier receive exponentially larger penalties, effectively normalizing the comparison across tokens regardless of their position.
- Core assumption: The forgetting factor α is chosen such that 0 < α < 1, ensuring exponential decay over time
- Evidence anchors:
  - [section] "A2SF applies a penalty to the past Attention Score generated from old tokens by repeatedly multiplying the Forgetting Factor to the Attention Score over time."
  - [abstract] "The authors propose A2SF (Accumulative Attention Score with Forgetting Factor), which applies exponential decay to past attention scores, reducing bias toward early tokens."
  - [corpus] No direct corpus evidence for exponential decay approach, suggesting this is a novel contribution

### Mechanism 3
- Claim: Fairer token comparison through A2SF enables more accurate selection of important tokens, improving model accuracy
- Mechanism: By eliminating the positional bias in attention score accumulation, A2SF allows tokens to be compared based on their actual importance rather than their position in the sequence. This leads to more accurate token pruning decisions, where truly important tokens are retained and unimportant tokens are removed, resulting in better model performance with compressed KV cache.
- Core assumption: The accuracy improvement is directly proportional to the improvement in token selection quality
- Evidence anchors:
  - [abstract] "Experiments on LLaMA and OPT models show A2SF improves accuracy by up to 7.8% (1-shot) and 5.1% (0-shot) compared to prior methods"
  - [section] "Through the fair comparison among tokens, we can more effectively select important tokens"
  - [corpus] Strong evidence - multiple related papers (Adaptive Layer Selection, G-KV, Saliency-driven Dynamic Token Pruning) show that token selection quality directly impacts accuracy, supporting this mechanism

## Foundational Learning

- Concept: Causal masking in transformer decoders
  - Why needed here: Understanding how causal masking creates positional bias is fundamental to grasping why A2SF is necessary
  - Quick check question: Why does a token at position k in a decoder model only attend to tokens at positions 1 through k-1?

- Concept: Attention score accumulation and its role in token importance
  - Why needed here: The mechanism relies on understanding how attention scores are accumulated over time to determine token importance
  - Quick check question: How does the accumulated attention score for a token change as more tokens are generated in the sequence?

- Concept: Exponential decay and its application to weighting historical data
  - Why needed here: The core innovation uses exponential decay to discount older attention scores, requiring understanding of this mathematical concept
  - Quick check question: If α = 0.9, how much is an attention score from 10 steps ago discounted compared to a score from 1 step ago?

## Architecture Onboarding

- Component map: Input sequence → Masked self-attention → Attention scores → A2S/A2SF accumulation → Token importance scoring → KV cache pruning decision
- Critical path: Attention score calculation → Forgetting factor application → Accumulated score comparison → Token selection decision
- Design tradeoffs:
  - α parameter tuning: Too high maintains positional bias; too low loses historical context
  - Memory vs accuracy: More aggressive pruning saves memory but may hurt accuracy
  - Computational overhead: A2SF adds minimal computation but requires careful implementation to avoid bottlenecks
- Failure signatures:
  - Accuracy degradation when α is poorly tuned
  - Memory savings not matching expectations due to suboptimal pruning
  - Inconsistent performance across different datasets
- First 3 experiments:
  1. Implement A2SF with α = 0.5 on a small decoder model and verify that early tokens no longer dominate attention scores
  2. Compare accuracy with various α values (0.1, 0.3, 0.5, 0.7, 0.9) on a benchmark dataset to find optimal tuning
  3. Measure memory savings and accuracy trade-off at different cache ratios (0.1, 0.3, 0.5, 0.7, 0.9) to establish performance envelope

## Open Questions the Paper Calls Out

- How does A2SF perform across different model scales and architectures beyond LLaMA and OPT?
- What is the optimal method for dynamically determining the Forgetting Factor for different tokens, heads, or layers?
- How does A2SF perform on specialized datasets and domains outside of general language understanding?

## Limitations
- The forgetting factor α is a critical hyperparameter without a principled selection method
- Limited evaluation scope with only two baseline methods (H2O and Local Attention)
- No analysis of model calibration or confidence changes with A2SF application

## Confidence
- High Confidence: Causal masking creates positional bias in attention score accumulation
- Medium Confidence: Empirical accuracy improvements (7.8% for 1-shot, 5.1% for 0-shot tasks)
- Low Confidence: Generalization across all cache ratios and model sizes

## Next Checks
1. Conduct a comprehensive grid search over α values (0.05 to 0.95) across multiple model architectures and datasets to map the full performance landscape
2. Design experiments using tasks requiring long-range dependencies to test whether A2SF's exponential decay might discard relevant historical context
3. Evaluate A2SF in combination with other optimization methods (quantization, speculative decoding, adaptive layer selection) to determine additive or multiplicative benefits