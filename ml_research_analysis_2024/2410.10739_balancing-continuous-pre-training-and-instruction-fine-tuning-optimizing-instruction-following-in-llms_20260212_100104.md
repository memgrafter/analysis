---
ver: rpa2
title: 'Balancing Continuous Pre-Training and Instruction Fine-Tuning: Optimizing
  Instruction-Following in LLMs'
arxiv_id: '2410.10739'
source_url: https://arxiv.org/abs/2410.10739
tags:
- instruction
- base
- arxiv
- llama
- capabilities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the impact of continuous pre-training on
  both base and instruction-tuned large language models (LLMs) and their instruction-following
  capabilities. The research finds that continuously pre-training instruction models
  leads to catastrophic forgetting of their instruction-following abilities.
---

# Balancing Continuous Pre-Training and Instruction Fine-Tuning: Optimizing Instruction-Following in LLMs

## Quick Facts
- arXiv ID: 2410.10739
- Source URL: https://arxiv.org/abs/2410.10739
- Authors: Ishan Jindal; Chandana Badrinath; Pranjal Bharti; Lakkidi Vinay; Sachin Dev Sharma
- Reference count: 6
- Primary result: Continuously pre-training instruction models causes catastrophic forgetting, but pre-training base models then instruction fine-tuning preserves both capabilities

## Executive Summary
This study investigates the impact of continuous pre-training on both base and instruction-tuned large language models (LLMs) and their instruction-following capabilities. The research finds that continuously pre-training instruction models leads to catastrophic forgetting of their instruction-following abilities. However, continuously pre-training the base model followed by instruction fine-tuning preserves both domain knowledge and instruction capabilities. The study also reveals that instruction capabilities are transferable across models from the same ancestor, eliminating the need for additional instruction tuning after continuous pre-training of the base model.

## Method Summary
The study employs a multi-step approach to investigate instruction-following capability preservation during continuous pre-training. Base models (LLaMa 3/3.1, Qwen 2/2.5) are continuously pre-trained on 2M scraped news articles from December 2023 to September 2024. Instruction residuals are computed by subtracting base model parameters from instruction-tuned model parameters. These residuals are then added to continuously pre-trained base models to restore instruction capabilities. The effectiveness is evaluated using EleutherAI's framework with benchmarks including IFEval, MMLU, MMLU-Pro, GSM8K, Winogrande, Hellaswag, ARC_easy, Piqa, and TruthfulQA.

## Key Results
- Continuous pre-training of instruction-tuned models causes catastrophic forgetting, with up to 5.7 points drop on instruction-following datasets
- Continuously pre-training base models followed by instruction fine-tuning preserves both domain knowledge and instruction capabilities
- Instruction residuals are transferable across models from the same ancestor, eliminating need for additional instruction tuning after base model pre-training
- The approach is more computationally efficient than traditional instruction fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continuous pre-training of instruction-tuned models causes catastrophic forgetting of instruction-following capabilities.
- Mechanism: When instruction-tuned models are further pre-trained on raw data, the fine-tuned parameters that encode instruction-following behavior are overwritten by new language modeling objectives, leading to loss of instruction alignment.
- Core assumption: The parameter space for instruction-following capabilities is distinct and vulnerable during continued language modeling training.
- Evidence anchors:
  - [abstract] "continuously pre-training instruction models leads to catastrophic forgetting of their instruction-following abilities."
  - [section 4.1] "with 100M new tokens, we observed a maximum drop of 5.7 points on the instruction following dataset and an average drop of 2.7 points across all tasks."
  - [corpus] Weak - no corpus evidence directly supporting catastrophic forgetting mechanism.

### Mechanism 2
- Claim: Instruction capabilities are transferable across models from the same ancestor through parameter subtraction.
- Mechanism: The difference between instruction-tuned model weights and base model weights captures instruction-following capability as a residual that can be added to other models from the same family.
- Core assumption: Instruction-following behavior is encoded in a consistent, additive way across models sharing the same base architecture.
- Evidence anchors:
  - [abstract] "instruction capabilities are transferable across models from the same ancestor, eliminating the need for additional instruction tuning after continuous pre-training of the base model."
  - [section 2.2] "we compute the instruction residual between an instruction following LLM and its corresponding base model in the parametric space"
  - [corpus] Weak - no corpus evidence supporting parameter subtraction for capability transfer.

### Mechanism 3
- Claim: Continuous pre-training base model then instruction fine-tuning preserves both domain knowledge and instruction capabilities.
- Mechanism: Base models are less susceptible to catastrophic forgetting during continued pre-training, and subsequent instruction fine-tuning can restore instruction capabilities without losing newly acquired knowledge.
- Core assumption: Base model parameters encoding general language understanding are more stable during continued pre-training than instruction-tuned parameters.
- Evidence anchors:
  - [abstract] "continuously pre-training the base model followed by instruction fine-tuning preserves both domain knowledge and instruction capabilities."
  - [section 4.1] "base model (L3b) in Figure 1 demonstrates minimal quality degradation with the number of new tokens"
  - [corpus] Weak - no corpus evidence supporting stability of base models during continued pre-training.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why instruction-tuned models lose capabilities during continued pre-training
  - Quick check question: What happens to neural network parameters when training on new data without preservation mechanisms?

- Concept: Parameter-efficient fine-tuning (LoRA, QLoRA)
  - Why needed here: The instruction residual approach is inspired by these techniques for adding capabilities without full fine-tuning
  - Quick check question: How do low-rank adaptation methods modify model weights to add new capabilities?

- Concept: Continual learning and knowledge preservation
  - Why needed here: To understand the contrast between instruction-tuned and base models during continued training
  - Quick check question: What techniques can prevent catastrophic forgetting when updating models with new data?

## Architecture Onboarding

- Component map:
  - Base model (LLaMa 3/3.1, Qwen 2/2.5) - 8B parameters
  - Instruction-tuned model - derived from base with SFT
  - Instruction residual - parameter difference between instruction and base models
  - Pre-training corpus - news articles from 2023-2024
  - Evaluation harness - EleutherAI framework with 10+ benchmarks

- Critical path:
  1. Load base model and corresponding instruction-tuned model
  2. Compute instruction residual (θi - θb)
  3. Continuously pre-train base model on new data
  4. Add instruction residual to continuously pre-trained base
  5. Evaluate instruction-following capabilities

- Design tradeoffs:
  - Computational efficiency vs. capability preservation
  - Residual transferability vs. model-specific fine-tuning
  - Pre-training data size vs. catastrophic forgetting risk

- Failure signatures:
  - Performance degradation on instruction-following benchmarks after continuous pre-training
  - Residual addition fails to improve instruction capabilities
  - Inconsistent residual behavior across different model families

- First 3 experiments:
  1. Verify catastrophic forgetting by continuously pre-training an instruction-tuned model and measuring performance drop
  2. Test instruction residual transferability by adding LLaMa 3.1 residuals to LLaMa 3 base and measuring improvement
  3. Compare computational costs of traditional instruction fine-tuning vs. residual addition approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of instruction residuals vary across different model sizes and architectures beyond the 8B parameter models tested in this study?
- Basis in paper: [explicit] The paper acknowledges that while their hypothesis is validated for models with 8 billion parameters, they observe noticeable performance variations when applied to smaller models, particularly those with around 1.5 billion parameters. They also note uncertainty about the scalability of their proposed strategy for models smaller than 1.5 billion parameters.
- Why unresolved: The study focused primarily on LLaMa 3, 3.1, and Qwen 2, 2.5 families, mostly testing on 8B parameter models due to resource constraints. The paper explicitly states that the impact on smaller models remains unexplored.
- What evidence would resolve it: Comprehensive testing of the instruction residual technique across a wide range of model sizes (e.g., 0.5B, 1.5B, 3B, 7B, 70B, and 405B parameters) and different architectures (e.g., BERT, GPT, T5) would provide insights into the scalability and effectiveness of the approach across various model scales.

### Open Question 2
- Question: What is the long-term impact of continuous pre-training on instruction capabilities, and at what point does catastrophic forgetting become irreversible?
- Basis in paper: [explicit] The paper demonstrates that continuous pre-training of instruction models leads to catastrophic forgetting of instruction-following abilities. However, it does not explore the long-term effects or determine if there is a threshold beyond which the loss of capabilities becomes permanent.
- Why unresolved: The study focuses on immediate effects of continuous pre-training with up to 1B new tokens. It does not investigate whether the lost instruction capabilities can be recovered after extended periods of continuous pre-training or if there is a point of no return.
- What evidence would resolve it: Longitudinal studies tracking the performance of instruction models over multiple rounds of continuous pre-training with varying amounts of new data, coupled with attempts to recover lost capabilities using different methods, would clarify the long-term impact and potential irreversibility of catastrophic forgetting.

### Open Question 3
- Question: How do different model merging techniques affect the transfer of instruction-following capabilities compared to the Task Arithmetic method used in this study?
- Basis in paper: [explicit] The paper mentions that while they employ Task Arithmetic for extracting instruction residuals, inspired by parameter-efficient fine-tuning techniques, they acknowledge that different model merging techniques could affect the transfer of instruction-following capabilities. They state that understanding the specific impact of these techniques is beyond the scope of their study.
- Why unresolved: The study uses Task Arithmetic as the primary method for model merging but does not compare its effectiveness with other techniques like TIES (Yadav et al., 2024) or Model Breadcrumbs (Davari and Belilovsky, 2023).
- What evidence would resolve it: Systematic comparison of instruction residual extraction and application using various model merging techniques (e.g., Task Arithmetic, TIES, Model Breadcrumbs, LoRA, QLoRA) across different model families and sizes would reveal which methods are most effective for preserving and transferring instruction capabilities.

## Limitations

- The catastrophic forgetting mechanism is primarily demonstrated empirically rather than through theoretical analysis, with limited corpus evidence supporting the claimed parameter space vulnerability
- The instruction residual transferability relies on a strong assumption that instruction-following capabilities are encoded in a consistent, additive manner across models, which may not hold for models with different instruction-tuning procedures or architectural variations
- The study focuses on specific model families and sizes, leaving questions about scalability and effectiveness across diverse architectures and parameter scales

## Confidence

- **High confidence**: The empirical observation that continuous pre-training of instruction-tuned models leads to performance degradation on instruction-following tasks is well-supported by the experimental results showing consistent drops across multiple benchmarks.
- **Medium confidence**: The claim that base models are more stable during continuous pre-training is supported by experimental data but lacks theoretical grounding for why base model parameters are more resistant to forgetting.
- **Low confidence**: The instruction residual transferability mechanism across model families is based on relatively few experiments and assumes a specific encoding of capabilities that may not generalize beyond the tested model families.

## Next Checks

1. **Cross-family residual transferability test**: Evaluate whether instruction residuals computed from LLaMa models can effectively transfer instruction capabilities to Qwen models (and vice versa), which would challenge the model-family-specific nature of the residual approach.

2. **Fine-tuning stability analysis**: Systematically vary pre-training data distributions and sizes to quantify the relationship between data domain shift and catastrophic forgetting rates in both base and instruction-tuned models.

3. **Parameter space mapping**: Use interpretability tools to visualize how instruction-following capabilities are distributed across model parameters in both base and instruction-tuned states, validating or challenging the assumption of distinct, vulnerable parameter regions.