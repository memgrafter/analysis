---
ver: rpa2
title: 'Tending Towards Stability: Convergence Challenges in Small Language Models'
arxiv_id: '2410.11451'
source_url: https://arxiv.org/abs/2410.11451
tags:
- layers
- training
- attention
- parameters
- activations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates why small language models underperform
  larger ones despite equivalent data and computational resources, focusing on convergence
  dynamics during training. Using the Pythia model suite, the authors analyze how
  Attention and MLP activations converge across different model sizes by measuring
  their similarity to final states using Centered Kernel Alignment (CKA).
---

# Tending Towards Stability: Convergence Challenges in Small Language Models

## Quick Facts
- arXiv ID: 2410.11451
- Source URL: https://arxiv.org/abs/2410.11451
- Reference count: 40
- Key outcome: Small language models exhibit slower, less stable convergence compared to larger models, with convergence patterns strongly correlated to effective rank of parameters and gradients

## Executive Summary
This paper investigates why small language models underperform larger ones despite equivalent data and computational resources, focusing on convergence dynamics during training. Using the Pythia model suite, the authors analyze how Attention and MLP activations converge across different model sizes by measuring their similarity to final states using Centered Kernel Alignment (CKA). They also examine the effective rank of parameters and gradients to understand convergence patterns. Results show that larger models stabilize faster and more monotonically within the first 20% of training, while smaller models exhibit slower, less stable convergence, especially when parameters have lower effective rank. A strong correlation exists between activation convergence rates and the proportional effective rank of parameters and gradients, suggesting that increasing rank could improve convergence in small models.

## Method Summary
The study uses the Pythia model suite (70M, 160M, 410M, 1.4B, 2.8B parameters) trained on the deduplicated Pile dataset for 143k steps. Activations from Attention and MLP layers are extracted from checkpoints at various training steps and compared to final states using CKA similarity. Proportional effective rank (PER) is calculated for parameters and gradients to quantify representational capacity. The analysis examines layer-wise convergence patterns and correlations between CKA convergence rates and PER stability across different model sizes.

## Key Results
- Larger models stabilize faster and more monotonically within the first 20% of training
- Small models exhibit slower and less stable convergence, particularly when parameters have lower effective rank
- Strong correlation exists between activation convergence rates and proportional effective rank of parameters and gradients
- Earlier layers converge faster than later layers across all model sizes
- PER is lower in small models for both parameters and gradients

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layers with higher proportional effective rank (PER) converge faster and more monotonically
- Mechanism: PER measures the fraction of parameter space actively used during training. Higher PER implies more diverse parameter updates, leading to faster stabilization of activations
- Core assumption: Effective rank of parameters and gradients is correlated with diversity of representational directions available during learning
- Evidence anchors: Strong correlation between activation convergence rates and PER; layers converging later span smaller fraction of dimensions
- Break condition: If increasing PER through interventions does not lead to faster convergence, the mechanism may be correlational rather than causal

### Mechanism 2
- Claim: Early layers stabilize faster than later layers across all model sizes
- Mechanism: Earlier layers process raw input tokens and establish foundational representations, which then guide subsequent layers
- Core assumption: Hierarchical nature of transformers means earlier layers have simpler optimization targets
- Evidence anchors: Earlier layers' activations converge faster to their final state than those of later layers
- Break condition: If later layers converge faster in certain architectures or training regimes, the mechanism may not be universal

### Mechanism 3
- Claim: Small models have lower PER in both parameters and gradients, leading to slower, less stable convergence
- Mechanism: Lower PER indicates fewer effective dimensions for learning, resulting in reduced representational capacity and slower optimization progress
- Core assumption: Relationship between PER and convergence speed is causal, not just correlational
- Evidence anchors: Smaller models exhibit slower and less stable convergence, especially when parameters have lower effective rank; larger models proportionally span more dimensions
- Break condition: If interventions to increase PER don't improve convergence, the mechanism may be incomplete

## Foundational Learning

- Concept: Centered Kernel Alignment (CKA) similarity metric
  - Why needed here: CKA measures how similar activation patterns are across different training checkpoints, enabling quantitative analysis of convergence dynamics
  - Quick check question: How does CKA differ from simple cosine similarity when comparing high-dimensional activation matrices?

- Concept: Effective rank calculation
  - Why needed here: Effective rank quantifies the intrinsic dimensionality of parameter matrices, providing insight into representational capacity and learning efficiency
  - Quick check question: What does it mean when a parameter matrix has an effective rank much lower than its nominal rank?

- Concept: Proportional effective rank (PER)
  - Why needed here: PER normalizes effective rank by the number of hidden dimensions, enabling fair comparison across layers of different sizes
  - Quick check question: Why can't we compare raw effective ranks across layers with different dimensionalities?

## Architecture Onboarding

- Component map: Pythia model checkpoints -> Activation extraction -> CKA similarity computation -> PER calculation -> Correlation analysis -> Interpretation

- Critical path: Checkpoint collection -> Activation extraction -> CKA similarity computation -> PER calculation -> Correlation analysis -> Interpretation

- Design tradeoffs:
  - Using CKA vs. other similarity metrics: CKA captures nonlinear relationships but is computationally expensive
  - PER vs. raw rank: PER enables cross-layer comparison but loses absolute scale information
  - Layer-wise vs. aggregate analysis: Layer-wise provides granularity but increases complexity

- Failure signatures:
  - CKA values plateauing early without meaningful convergence (overfitting)
  - PER increasing during training (parameter explosion)
  - Strong correlation between CKA and PER breaking down in certain layers

- First 3 experiments:
  1. Verify CKA implementation by comparing identical checkpoints (should yield CKA â‰ˆ 1)
  2. Test PER calculation on synthetic low-rank matrices to confirm it captures dimensionality
  3. Apply rank-increasing intervention to small model layers and measure impact on CKA convergence speed

## Open Questions the Paper Calls Out
None

## Limitations
- Causal relationship between PER and convergence speed remains uncertain - correlations are observed but not proven through interventions
- Specific implementation details for PER calculation and CKA computation are not fully specified
- Analysis focuses on descriptive patterns rather than interventional experiments to validate proposed mechanisms

## Confidence

### Major Uncertainties
- Causal relationship between proportional effective rank and convergence speed remains uncertain
- Specific implementation details for PER calculation and CKA computation are not fully specified
- Analysis focuses on descriptive patterns rather than interventional experiments

### Confidence Assessment
- **High confidence**: Descriptive findings about convergence patterns - larger models converge faster and more monotonically
- **Medium confidence**: Correlation between PER and convergence rates - statistical relationships documented but causal interpretation requires validation
- **Low confidence**: Proposed mechanism that increasing PER will improve convergence in small models - hypothesis for future work

## Next Checks

1. **Intervention study**: Select small model layers with low PER and apply rank-increasing modifications, then measure whether CKA convergence speed improves as predicted

2. **Alternative similarity metrics**: Replicate the convergence analysis using multiple similarity measures (cosine similarity, SVD-based approaches) to verify that CKA-specific patterns are not artifacts of the chosen metric

3. **Cross-dataset validation**: Test whether the observed PER-convergence relationship holds across different training datasets and tasks, not just the Pile dataset used in the Pythia models