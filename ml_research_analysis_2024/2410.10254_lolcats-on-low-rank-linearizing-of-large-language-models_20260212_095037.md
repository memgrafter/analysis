---
ver: rpa2
title: 'LoLCATs: On Low-Rank Linearizing of Large Language Models'
arxiv_id: '2410.10254'
source_url: https://arxiv.org/abs/2410.10254
tags:
- attention
- layer
- head
- keys
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces LoLCATs, a method to linearize large language\
  \ models (LLMs) by converting their quadratic attention mechanisms into subquadratic\
  \ alternatives. The approach combines attention transfer\u2014training linear attention\
  \ layers to approximate softmax attention\u2014with low-rank adaptation (LoRA) to\
  \ efficiently recover model quality."
---

# LoLCATs: On Low-Rank Linearizing of Large Language Models

## Quick Facts
- **arXiv ID**: 2410.10254
- **Source URL**: https://arxiv.org/abs/2410.10254
- **Reference count**: 40
- **Primary result**: LoLCATs achieves 20+ points better MMLU scores than prior linearization methods while using only 0.2% of model parameters and 0.4% of training tokens

## Executive Summary
LoLCATs introduces a novel method for linearizing large language models by combining attention transfer with low-rank adaptation (LoRA). The approach converts quadratic attention mechanisms into subquadratic alternatives, enabling efficient inference on resource-constrained hardware. By training linear attention layers to approximate softmax attention and applying LoRA, the method significantly improves linearization quality while maintaining model performance. Experiments demonstrate substantial gains over existing methods, with 19.7-24.7 point improvements on MMLU across different model scales, and enables practical linearization of 70B and 405B parameter models for the first time.

## Method Summary
LoLCATs works by first training linear attention layers to approximate softmax attention through attention transfer, then applying low-rank adaptation (LoRA) to recover model quality efficiently. The method leverages the relationship between softmax attention and kernel-based approximations, where linear attention can be viewed as a special case of kernel attention with identity feature map. By combining attention transfer (training linear attention to match softmax outputs) with LoRA (efficient parameter adaptation), LoLCATs achieves both computational efficiency and high-quality linearization. The approach requires only 0.2% of model parameters and 0.4% of training tokens compared to full fine-tuning, while maintaining strong performance across multiple model scales.

## Key Results
- Achieves 19.7 points better MMLU performance on 7B models compared to prior linearization methods
- Enables first practical linearization of 70B and 405B parameter models, closing 77.8% and 78.1% of quality gap respectively
- Uses only 0.2% of model parameters and 0.4% of training tokens while maintaining strong performance

## Why This Works (Mechanism)
The mechanism works because linear attention can approximate softmax attention when properly trained, and LoRA provides an efficient way to adapt the linearized model to specific tasks. The attention transfer phase trains linear attention to match softmax attention outputs, creating a strong initialization. The subsequent LoRA fine-tuning then adapts this linearized model with minimal parameters, recovering most of the lost performance. This combination exploits the mathematical relationship between softmax and linear attention while using LoRA's efficiency to make the approach practical at scale.

## Foundational Learning
- **Attention mechanisms**: Softmax attention has quadratic complexity O(LÂ²) with sequence length, while linear attention reduces this to O(L) - needed for computational efficiency in long sequences
- **Kernel attention**: Softmax attention can be expressed as kernel attention with exponential kernel - provides theoretical foundation for attention transfer
- **Low-rank adaptation (LoRA)**: Technique for efficient fine-tuning using low-rank matrix decompositions - enables parameter-efficient adaptation of linearized models
- **Attention transfer**: Process of training one attention mechanism to approximate another - crucial for initializing linearized models with knowledge from full attention
- **Transformer architecture**: Standard architecture for LLMs where attention mechanisms are applied in multiple layers - context for where linearization occurs
- **Subquadratic complexity**: Computational complexity better than quadratic but potentially worse than linear - describes the efficiency gains from linearization

## Architecture Onboarding

**Component Map**: Input -> Linear Attention Layers -> LoRA Adaptation -> Output
Linear Attention (via kernel methods) -> Attention Transfer Training -> LoRA Fine-tuning -> Inference

**Critical Path**: The critical path involves the attention transfer phase where linear attention layers learn to approximate softmax attention outputs, followed by LoRA fine-tuning that adapts the linearized model to the target task. This two-phase approach ensures both computational efficiency and performance recovery.

**Design Tradeoffs**: The main tradeoff is between computational efficiency and model quality. While linearization significantly reduces complexity, it initially degrades performance. The solution balances this by using attention transfer for initialization and LoRA for efficient adaptation. The choice of linear attention kernel (identity vs. exponential) affects approximation quality and computational requirements.

**Failure Signatures**: Poor attention transfer leads to large quality gaps that LoRA cannot recover. Insufficient LoRA rank prevents adequate adaptation. The method may struggle with tasks requiring precise attention patterns or when sequence lengths vary significantly from training conditions.

**First Experiments**:
1. Verify attention transfer quality by measuring KL divergence between linear and softmax attention distributions on validation data
2. Test LoRA adaptation effectiveness by comparing performance before and after LoRA fine-tuning on held-out data
3. Measure computational speedup by comparing inference times between linearized and full attention models across different sequence lengths

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- Evaluation primarily focuses on MMLU benchmark, leaving generalizability to other tasks uncertain
- Computational efficiency claims lack comprehensive runtime benchmarking across different hardware and batch sizes
- Training procedure sensitivity to hyperparameters is not thoroughly explored, raising reproducibility concerns

## Confidence
- **High Confidence**: Core technical contribution and MMLU performance improvements are well-supported
- **Medium Confidence**: Practical applicability claims to 70B/405B models are compelling but limited to single benchmark
- **Low Confidence**: "First practical linearization" claim is difficult to verify without broader comparison to all existing methods

## Next Checks
1. Evaluate LoLCATs on diverse benchmarks including GSM8K, HumanEval, and Natural Questions to assess generalization beyond MMLU
2. Measure actual inference latency and memory usage on GPU/CPU across different batch sizes and sequence lengths in practical deployment scenarios
3. Conduct systematic ablation studies varying attention transfer methods, LoRA rank, and learning rate schedules to identify critical components and assess hyperparameter sensitivity