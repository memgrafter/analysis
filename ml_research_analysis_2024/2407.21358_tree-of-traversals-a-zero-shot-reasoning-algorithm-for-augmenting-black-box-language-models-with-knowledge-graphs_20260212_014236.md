---
ver: rpa2
title: 'Tree-of-Traversals: A Zero-Shot Reasoning Algorithm for Augmenting Black-box
  Language Models with Knowledge Graphs'
arxiv_id: '2407.21358'
source_url: https://arxiv.org/abs/2407.21358
tags:
- knowledge
- answer
- tree-of-traversals
- entities
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Tree-of-Traversals introduces a zero-shot algorithm enabling black-box
  LLMs to interface with arbitrary knowledge graphs (KGs) without training or fine-tuning.
  It uses a tree search framework guided by an action state machine (ASM) to iteratively
  expand a local KG subgraph via LLM-generated thoughts and actions, improving reasoning
  accuracy on question answering tasks.
---

# Tree-of-Traversals: A Zero-Shot Reasoning Algorithm for Augmenting Black-box Language Models with Knowledge Graphs

## Quick Facts
- arXiv ID: 2407.21358
- Source URL: https://arxiv.org/abs/2407.21358
- Reference count: 29
- Primary result: Tree-of-Traversals achieves up to 63.0% exact match accuracy on 2WikiMultiHop with Claude-Instant, outperforming Chain-of-Thought, ReAct, and FLARe baselines

## Executive Summary
Tree-of-Traversals introduces a zero-shot algorithm that enables black-box large language models (LLMs) to interface with arbitrary knowledge graphs (KGs) without requiring training or fine-tuning. The method uses a tree search framework guided by an action state machine (ASM) to iteratively expand a local KG subgraph through LLM-generated thoughts and actions, improving reasoning accuracy on question answering tasks. Evaluated on two standard benchmarks (2WikiMultiHop and QALD-10) and a newly created multi-KG dataset (MusicBrainz-x-Wikidata), Tree-of-Traversals consistently outperforms baselines including Chain-of-Thought, ReAct, and FLARe, achieving up to 63.0% exact match accuracy on 2WikiMultiHop with Claude-Instant. Backtracking and value-guided search are critical to performance, with larger models gaining more from the approach.

## Method Summary
Tree-of-Traversals is a zero-shot algorithm that augments black-box LLMs with knowledge graph reasoning capabilities through iterative subgraph expansion using tree search. The method employs an action state machine (ASM) to structure LLM interaction with KGs, breaking the expansion task into subtasks like entity and relation selection. At each search node, the LLM generates thoughts and actions (e.g., ExpandKG, Select_Relation) that expand the local subgraph, with a value function evaluating the utility of each state. The algorithm uses backtracking to avoid early commitment to incorrect paths and continues until a high-confidence answer is found or search limits are reached.

## Key Results
- Tree-of-Traversals achieves 63.0% exact match accuracy on 2WikiMultiHop with Claude-Instant, outperforming Chain-of-Thought (55.0%), ReAct (58.0%), and FLARe (58.0%) baselines
- Backtracking and value-guided search are critical components, with Tree-of-Traversals with backtracking outperforming the version without backtracking by up to 10.7% on QALD-10
- Larger models (Claude-Instant) benefit more from the approach than smaller models (Llama2-13b), showing 18.7% improvement over the simple baseline compared to 8.7% for Llama2-13b

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tree-of-Traversals improves reasoning by iteratively expanding a local knowledge graph subgraph using LLM-generated actions guided by a finite state machine.
- Mechanism: The algorithm initializes a small subgraph from query entities, then uses tree search over possible LLM-generated thoughts and actions to expand it until sufficient information is gathered to answer the question.
- Core assumption: The LLM can generate useful thoughts and actions when given a precise prompt about the current subgraph state.
- Evidence anchors:
  - [abstract] "The algorithm equips a LLM with actions for interfacing a KG and enables the LLM to perform tree search over possible thoughts and actions to find high confidence reasoning paths."
  - [section 2.3] "Our approach draws inspiration from the Tree-of-Thoughts approach... we introduce some modifications: incorporation of actions via the ASM, a slightly different search procedure and stopping condition to better handle QA, and a different sampling procedure for improved diversity when doing constrained sampling."
- Break condition: If the LLM fails to generate coherent or relevant thoughts/actions, the search tree becomes ineffective.

### Mechanism 2
- Claim: The Action State Machine (ASM) structures LLM interaction with the KG, breaking the expansion task into subtasks and preventing confusion from unstructured knowledge base access.
- Mechanism: The ASM defines states (default, selecting-entities, selecting-relation, done) and transitions triggered by actions (Think, Answer, ExpandKG, Select_Entities, Select_Relation), with prompt templates customized per state.
- Core assumption: Structured state transitions with entity and relation selection prompts allow the LLM to navigate arbitrary KGs without needing to understand KG schemas.
- Evidence anchors:
  - [section 2.2] "One of the challenges with developing a zero-shot LLM algorithm that works with arbitrary KGs is that the LLM does not know what relations are available in the graph or what relations are valid for a given entity."
  - [section 2.2] "To overcome these issues we break the task of expanding a local KG subgraph into multiple subtasks. We use a finite state machine..."
- Break condition: If the KG API returns incorrect or incomplete relations/entities, the ASM cannot guide the LLM properly.

### Mechanism 3
- Claim: Tree search with backtracking and value-guided selection enables exploration of multiple reasoning paths, avoiding early commitment to incorrect expansion choices.
- Mechanism: At each node, k actions are sampled, transitions applied, and resulting states evaluated by the LLM's value function; the highest-value unexplored node is chosen next, with backtracking when a path yields low value.
- Core assumption: The LLM's value function can reliably assess the utility of intermediate states for reaching the correct answer.
- Evidence anchors:
  - [section 2.3] "Tree-of-Traversals computes the value of a node to determine its utility... The value can be between 0 to 1 where 1 indicates highest utility."
  - [section 4.1] "Figure 5 shows that there is a meaningful signal from the value function for all models. There is an average performance difference of 31.0% between the accuracy for answers valued at 1.0 vs 0.0."
- Break condition: If the value function assigns high scores to incorrect reasoning paths, the search may converge to wrong answers.

## Foundational Learning

- Concept: Knowledge Graph (KG) structure and SPARQL queries
  - Why needed here: Tree-of-Traversals interfaces with KGs through SPARQL or API calls; understanding entities, relations, and triples is essential for implementing the KG interface and reasoning about subgraph expansion.
  - Quick check question: Given a triple (s, r, o) where s is an entity, r is a relation, and o is an object, what does this represent in a KG?

- Concept: Tree search algorithms (Best-First, Depth-First, Backtracking)
  - Why needed here: Tree-of-Traversals uses tree search to explore reasoning paths; understanding node expansion, value functions, and backtracking is critical for implementing the search logic.
  - Quick check question: In a best-first tree search, how do you choose which node to expand next?

- Concept: Finite State Machines (FSMs) and state transitions
  - Why needed here: The ASM defines states and transitions for LLM-KG interaction; knowing how to model actions and states as an FSM is necessary for implementing the action state machine.
  - Quick check question: What are the key components of a finite state machine?

## Architecture Onboarding

- Component map:
  KG Interface -> Action State Machine (ASM) -> Tree Search Algorithm -> LLM -> Local KG Subgraph

- Critical path:
  1. initialize(q) → E0
  2. while !finished: choose_node(T) → s
  3. sample_actions(s, k, F, P) → a
  4. for each a: transition(s, a, F) → s'
  5. evaluate(s', P) → s'.value
  6. if s'.action_state == done and s'.value > τ: finished = true
  7. return argmaxy∈Y y.value

- Design tradeoffs:
  - Branching factor k=3 balances exploration diversity vs cost
  - Max depth=7 limits search space while enabling complex reasoning
  - Value threshold τ=0.8 ensures only high-confidence answers are returned
  - YAML subgraph format minimizes token usage vs full KG serialization

- Failure signatures:
  - Low value scores across all nodes → LLM cannot assess utility of expansions
  - Repetitive entity/relation selections → diversity oversampling ineffective
  - Immediate backtracking from all branches → value function unreliable
  - "Cannot find answer" returned frequently → search depth or expansions too limited

- First 3 experiments:
  1. Run Tree-of-Traversals with k=1 (Chain-of-Traversals) on 2WikiMultiHop to verify basic functionality without tree search complexity
  2. Increase k=3 and enable backtracking; compare accuracy vs k=1 to measure benefit of exploration
  3. Test on QALD-10 with single KG to validate performance on varied question types and verify ASM handles diverse relation structures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Tree-of-Traversals perform on multilingual question answering tasks beyond English?
- Basis in paper: [inferred] The paper mentions that the evaluation is limited to English versions of KGs and datasets, and suggests that Tree-of-Traversals should be evaluated in other languages to ensure consistent and fair experience.
- Why unresolved: The paper does not provide any experimental results or analysis on multilingual question answering tasks.
- What evidence would resolve it: Conducting experiments on multilingual question answering datasets and comparing the performance of Tree-of-Traversals with other methods across different languages.

### Open Question 2
- Question: What is the impact of Tree-of-Traversals on the safety and ethical considerations of LLMs?
- Basis in paper: [explicit] The paper mentions that Tree-of-Traversals gives new capabilities to LLMs after training and highlights the need to evaluate its effect on wider safety metrics. It also mentions the risk of using a publicly modifiable knowledge graph.
- Why unresolved: The paper does not provide any analysis or experimental results on the safety and ethical implications of Tree-of-Traversals.
- What evidence would resolve it: Conducting experiments to assess the impact of Tree-of-Traversals on safety metrics such as bias, fairness, and toxicity. Additionally, analyzing the behavior of Tree-of-Traversals when exposed to misleading or deceptive knowledge graphs.

### Open Question 3
- Question: How does Tree-of-Traversals scale with the size and complexity of knowledge graphs?
- Basis in paper: [inferred] The paper mentions that Tree-of-Traversals can work with any API accessible KG and multiple KGs. It also mentions the use of a local KG subgraph and the ability to expand it using a tree search algorithm.
- Why unresolved: The paper does not provide any analysis or experimental results on the scalability of Tree-of-Traversals with large and complex knowledge graphs.
- What evidence would resolve it: Conducting experiments to evaluate the performance of Tree-of-Traversals on large-scale knowledge graphs with millions of entities and relations. Additionally, analyzing the computational complexity and resource requirements of Tree-of-Traversals as the size and complexity of the knowledge graph increase.

## Limitations
- Tree-of-Traversals is notably slower than simpler baselines due to iterative subgraph expansion, potentially limiting practical deployment in time-sensitive applications
- Performance improvements depend heavily on proper prompt engineering and specific LLM characteristics, with larger models showing more benefit from the approach
- The method's effectiveness with very large or complex knowledge graphs remains untested, raising questions about scalability

## Confidence
- **High confidence**: The core mechanism of using tree search with an action state machine to interface LLMs with KGs is well-supported by experimental results
- **Medium confidence**: Performance improvements over baselines are consistent across datasets, but exact magnitude may vary with different KG configurations and question types
- **Medium confidence**: The claim that larger models benefit more from the approach is supported by data, but underlying reasons need further investigation

## Next Checks
1. **Cross-domain robustness test**: Evaluate Tree-of-Traversals on knowledge graphs from different domains (scientific, medical, financial) to verify generalization beyond Wikipedia and music knowledge bases
2. **Scalability assessment**: Test the algorithm's performance and efficiency with significantly larger knowledge graphs (millions of triples) to identify potential bottlenecks in subgraph expansion and tree search
3. **Value function reliability analysis**: Conduct a systematic study where human evaluators assess the same intermediate states that the LLM evaluates, measuring agreement rates to quantify the reliability of the value-guided search mechanism