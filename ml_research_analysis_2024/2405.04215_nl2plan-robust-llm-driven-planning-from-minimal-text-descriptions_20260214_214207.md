---
ver: rpa2
title: 'NL2Plan: Robust LLM-Driven Planning from Minimal Text Descriptions'
arxiv_id: '2405.04215'
source_url: https://arxiv.org/abs/2405.04215
tags:
- vertex
- nl2plan
- task
- domain
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "NL2Plan is a fully automatic, domain-agnostic system that generates\
  \ complete PDDL task descriptions from minimal natural language prompts and solves\
  \ them using a classical planner. It uses an LLM to incrementally extract task information\
  \ across six steps\u2014Type Extraction, Hierarchy Construction, Action Extraction,\
  \ Action Construction, Task Extraction, and Planning\u2014with optional feedback\
  \ and validation at each stage."
---

# NL2Plan: Robust LLM-Driven Planning from Minimal Text Descriptions

## Quick Facts
- arXiv ID: 2405.04215
- Source URL: https://arxiv.org/abs/2405.04215
- Reference count: 40
- One-line primary result: NL2Plan generates complete PDDL task descriptions from minimal natural language prompts and solves them using classical planning

## Executive Summary
NL2Plan is a fully automatic, domain-agnostic system that transforms minimal natural language descriptions into complete PDDL task descriptions and solves them using classical planners. The system employs an incremental, step-by-step approach using LLMs to extract and construct planning domain information across six stages: Type Extraction, Hierarchy Construction, Action Extraction, Action Construction, Task Extraction, and Planning. NL2Plan successfully solved 10 out of 15 tasks across four domains (Blocksworld, Tyreworld, Household, and ISR), significantly outperforming a direct LLM+validator baseline that solved only 2 tasks.

## Method Summary
NL2Plan uses an LLM to incrementally extract task information across six steps: Type Extraction, Hierarchy Construction, Action Extraction, Action Construction, Task Extraction, and Planning. Each step includes optional feedback and validation mechanisms. The system processes minimal natural language prompts and progressively builds a complete PDDL domain and problem description. Unlike direct LLM approaches, NL2Plan validates each component during construction and can identify unsolvable cases, returning "No plan found" instead of invalid plans. The method was evaluated on four domains with 15 tasks total, demonstrating robustness through its modular architecture.

## Key Results
- Solved 10 out of 15 tasks across four domains (Blocksworld, Tyreworld, Household, ISR)
- Outperformed direct LLM+validator baseline (2/15 tasks solved)
- Successfully identified two unsolvable cases, returning "No plan found"
- Demonstrated domain-agnostic capability through evaluation on diverse planning domains

## Why This Works (Mechanism)
NL2Plan works by breaking down the complex task of translating natural language into PDDL into manageable, validated steps. The incremental approach allows for error detection and correction at each stage, preventing the accumulation of errors that plague direct LLM approaches. By constructing the domain and problem description piece by piece with validation at each step, the system maintains correctness while building complexity. The feedback mechanism allows the LLM to refine its understanding based on validation results, creating a self-correcting process that improves accuracy.

## Foundational Learning
- PDDL (Planning Domain Definition Language): The standard language for classical planning problems; needed for expressing planning domains and problems in a machine-readable format; quick check: can you write a simple PDDL domain for Blocksworld?
- Classical Planning: AI approach to finding sequences of actions to achieve goals; needed as the underlying problem-solving method; quick check: understand STRIPS operators and state space search
- LLM Prompt Engineering: Techniques for guiding large language models to produce desired outputs; needed for extracting structured information from natural language; quick check: can you write prompts that extract specific information from text?
- Domain Agnosticism: Ability to work across different problem domains without domain-specific tuning; needed for general applicability; quick check: does the method require domain-specific knowledge or can it adapt to new domains?

## Architecture Onboarding
Component Map: Natural Language Prompt -> Type Extraction -> Hierarchy Construction -> Action Extraction -> Action Construction -> Task Extraction -> Planning -> Solution
Critical Path: The six-step extraction and construction pipeline forms the critical path, with each step dependent on successful completion of the previous one
Design Tradeoffs: Incremental validation trades computational overhead for robustness; domain-agnostic approach sacrifices domain-specific optimizations for generality
Failure Signatures: LLM hallucination errors at any extraction step, validation failures indicating structural inconsistencies, planning failures suggesting incomplete or incorrect domain definitions
First Experiments:
1. Test Type Extraction on simple domain descriptions to verify basic entity recognition
2. Validate Action Construction with known PDDL actions to check structural correctness
3. Evaluate Planning step with hand-crafted PDDL domains to isolate LLM performance from planning algorithm performance

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Limited evaluation scope covering only four domains and 15 tasks, restricting generalizability
- Performance on complex, real-world domains with long horizon planning, partial observability, or large state spaces remains unknown
- LLM-based extraction steps are inherently probabilistic with no quantitative error analysis or ablation studies
- Comparison with baseline limited to task success rates without analysis of plan quality, computational efficiency, or scalability

## Confidence
- High: The system successfully solves 10 out of 15 tasks and outperforms the direct LLM+validator baseline
- Medium: The modular, step-by-step approach and feedback mechanism are conceptually sound but lack quantitative validation
- Medium: The claim of robustness over pure LLM approaches is supported by task success rates but lacks broader domain coverage and ablation studies

## Next Checks
1. Test NL2Plan on a larger and more diverse set of domains, including those with long horizon planning and partial observability, to assess scalability and robustness
2. Perform ablation studies to quantify the contribution of each extraction step and the impact of the incremental feedback mechanism on final task success rates
3. Evaluate plan quality (e.g., length, optimality) and computational efficiency (time and memory) across domains to compare against classical planners and other LLM-driven approaches