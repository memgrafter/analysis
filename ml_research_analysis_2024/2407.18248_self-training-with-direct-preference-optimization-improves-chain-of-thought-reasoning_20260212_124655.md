---
ver: rpa2
title: Self-Training with Direct Preference Optimization Improves Chain-of-Thought
  Reasoning
arxiv_id: '2407.18248'
source_url: https://arxiv.org/abs/2407.18248
tags:
- self-training
- reasoning
- language
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DPO-augmented Self-Training (DPO-ST), which
  enhances small language models' mathematical reasoning by combining self-training
  with Direct Preference Optimization (DPO). The method iteratively generates and
  filters rationales, using DPO to improve both the quality and diversity of model
  outputs.
---

# Self-Training with Direct Preference Optimization Improves Chain-of-Thought Reasoning

## Quick Facts
- arXiv ID: 2407.18248
- Source URL: https://arxiv.org/abs/2407.18248
- Authors: Tianduo Wang; Shichen Li; Wei Lu
- Reference count: 12
- Primary result: DPO-augmented Self-Training (DPO-ST) outperforms standard self-training and competitive methods on GSM8K and related math reasoning tasks

## Executive Summary
This paper introduces DPO-augmented Self-Training (DPO-ST), a method that enhances small language models' mathematical reasoning by iteratively combining self-training with Direct Preference Optimization (DPO). The approach generates and filters rationales through multiple iterations, using DPO to improve both the quality and diversity of model outputs. Experiments demonstrate significant performance gains on GSM8K and related math reasoning tasks, while being more compute-efficient than methods relying on large proprietary models. The integration of an external calculator during decoding further boosts performance, offering a cost-effective, scalable solution for improving small models' reasoning capabilities without dependence on large annotators.

## Method Summary
The method starts with warm-up supervised fine-tuning on labeled data, then iteratively performs two steps: a DPO step where correct-answer rationales are labeled as winners and incorrect ones as losers for preference fine-tuning, followed by an SFT step where pseudo-labels are generated, filtered, and deduplicated before training a new model. This loop continues until convergence, with calculator integration to handle arithmetic operations during inference.

## Key Results
- DPO-ST significantly outperforms standard self-training and other competitive methods on GSM8K and related math reasoning tasks
- The method demonstrates improved performance while requiring minimal computational resources compared to approaches using large proprietary models
- Calculator integration during decoding further boosts accuracy by handling arithmetic operations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DPO step improves both quality and diversity of generated rationales.
- Mechanism: By constructing preference pairs where correct-answer rationales are labeled as winners and incorrect ones as losers, the model is fine-tuned to generate more accurate and varied solutions.
- Core assumption: Mathematical reasoning tasks allow clear validation of correctness, enabling reliable preference labeling.
- Evidence anchors:
  - [abstract] "By integrating DPO into self-training, we leverage preference data to guide LMs towards more accurate and diverse chain-of-thought reasoning."
  - [section] "Empirical observations indicate that... the DPO training objective makes language models inclined to generate rationales of both high quality and diversity."
  - [corpus] Weak - no direct neighbor evidence supporting DPO's effect on diversity.
- Break condition: If task cannot be easily validated (e.g., subjective tasks), preference labeling breaks down.

### Mechanism 2
- Claim: Self-training enables small models to improve without relying on large proprietary models.
- Mechanism: Models iteratively generate pseudo-labels from their own outputs, which are then filtered and used for further fine-tuning, creating a feedback loop of improvement.
- Core assumption: Small models can generate useful pseudo-labels that improve their own performance when properly filtered.
- Evidence anchors:
  - [abstract] "Our method significantly enhances the reasoning abilities of language models while requiring minimal computational resources..."
  - [section] "Our empirical results indicate that this method notably enhances the reasoning capabilities of LMs while also reducing computational overhead."
  - [corpus] Weak - neighbors focus on DPO variants but not self-training's effectiveness for small models.
- Break condition: If base model cannot solve any problems correctly initially, pseudo-label quality degrades.

### Mechanism 3
- Claim: Calculator integration significantly boosts arithmetic performance without sacrificing inference speed.
- Mechanism: By overriding model outputs with calculator results when calculation annotations are detected, models avoid arithmetic errors while maintaining batch decoding efficiency.
- Core assumption: Arithmetic errors are a major source of incorrect answers in math reasoning tasks.
- Evidence anchors:
  - [section] "To address this, we propose a simple yet efficient method that allows for using larger batch sizes during inference with an external calculator."
  - [section] "Our results indicate that decoding without the calculator markedly reduces accuracy across all iterations."
  - [corpus] Weak - no neighbor papers discussing calculator integration in this context.
- Break condition: If calculator annotations are missing or incorrect, the integration provides no benefit.

## Foundational Learning

- Concept: Chain-of-thought reasoning
  - Why needed here: The method specifically targets CoT reasoning tasks where intermediate steps matter for correctness.
  - Quick check question: Can you explain why showing intermediate calculation steps helps mathematical reasoning models?

- Concept: Supervised fine-tuning vs self-training
  - Why needed here: Understanding the difference between learning from human labels vs. learning from model-generated pseudo-labels is critical for implementing this method.
  - Quick check question: What are the key differences between SFT and self-training in terms of data sources and potential risks?

- Concept: Preference optimization vs reinforcement learning
  - Why needed here: DPO is used instead of traditional RLHF methods, so understanding the key differences is important for implementation.
  - Quick check question: How does DPO avoid the need for explicit reward model training compared to RLHF?

## Architecture Onboarding

- Component map: Base model (Flan-T5 or Llama family) -> Calculator integration module (custom LogitsProcessor) -> DPO training pipeline (preference data generation and optimization) -> Self-training loop (iterative SFT and DPO steps) -> Deduplication and filtering system

- Critical path: 1. Warm-up SFT training 2. Iterative DPO step (generate preference data → fine-tune with LDPO) 3. Iterative SFT step (generate pseudo-labels → filter → train new model) 4. Repeat until convergence

- Design tradeoffs:
  - More pseudo-labels per question (higher K) increases diversity but computational cost
  - Higher temperature during sampling increases diversity but may reduce quality
  - Calculator integration improves accuracy but adds dependency on external tool

- Failure signatures:
  - No improvement across iterations → likely filtering is too strict or base model too weak
  - Performance degradation → overfitting to pseudo-labels or incorrect preference labeling
  - Slow inference speed → calculator integration not properly optimized for batch size

- First 3 experiments:
  1. Run baseline SFT without any self-training or DPO to establish performance floor
  2. Implement self-training without DPO step to measure impact of iterative pseudo-labeling alone
  3. Add calculator integration to baseline SFT to measure impact of arithmetic correction alone

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DPO-augmented self-training scale with increasing model size beyond the tested 8B parameter range?
- Basis in paper: [explicit] The paper mentions using Llama-3-8b as the largest model and suggests future work could explore larger models.
- Why unresolved: The study only tested up to 8B parameters, leaving uncertainty about performance gains or saturation points at larger scales.
- What evidence would resolve it: Experiments with DPO-ST on models ranging from 10B to 70B parameters, comparing performance and computational efficiency.

### Open Question 2
- Question: Can the DPO-augmented self-training framework be effectively applied to non-mathematical reasoning tasks, such as logical or commonsense reasoning?
- Basis in paper: [explicit] The paper acknowledges that experiments were limited to math reasoning due to lack of annotated data for other reasoning tasks.
- Why unresolved: The method's reliance on preference data from correct/incorrect answers is specific to math; applicability to tasks without clear ground truth is unclear.
- What evidence would resolve it: Successful application and evaluation of DPO-ST on diverse reasoning tasks (e.g., natural language inference, symbolic reasoning) with appropriate preference datasets.

### Open Question 3
- Question: What is the impact of varying the number of sampled rationales per question (K) on the diversity and quality of pseudo-labels in DPO-ST?
- Basis in paper: [explicit] The paper varies K in experiments and notes that higher K increases diversity and performance, but does not systematically analyze the trade-off.
- Why unresolved: The relationship between K, pseudo-label quality, and downstream performance is not fully characterized, especially for larger K values.
- What evidence would resolve it: A detailed ablation study varying K from 1 to 20+, analyzing pseudo-label diversity, accuracy, and training efficiency at each step.

## Limitations
- Performance improvements are demonstrated primarily on GSM8K and similar arithmetic reasoning tasks, with limited evaluation on more complex mathematical domains
- The computational efficiency claims are based on comparison with a single baseline (GPT-4) and could benefit from broader benchmarking
- The calculator integration introduces an external dependency that may not generalize well to all mathematical domains or languages

## Confidence
- **High Confidence**: The core mechanism of combining self-training with DPO is well-supported by empirical results showing consistent improvements across multiple datasets and model sizes
- **Medium Confidence**: The claim that DPO specifically improves both quality and diversity of generated rationales is supported by qualitative observations but lacks quantitative diversity metrics
- **Low Confidence**: The generalizability of the approach to non-arithmetic mathematical reasoning tasks remains uncertain due to limited evaluation scope

## Next Checks
1. Conduct a systematic analysis measuring the diversity of generated rationales using metrics like distinct n-grams or ROUGE scores, comparing DPO-ST against baseline self-training to empirically verify the claimed improvement in rationale diversity
2. Evaluate the method on mathematical reasoning tasks that require symbolic manipulation, geometric reasoning, or higher-order problem-solving beyond arithmetic word problems to assess the approach's generalizability
3. Perform experiments varying model sizes from 1B to 10B parameters to empirically validate the claimed computational efficiency benefits and identify the optimal model scale for the self-training approach