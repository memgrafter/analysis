---
ver: rpa2
title: Using LLMs to Model the Beliefs and Preferences of Targeted Populations
arxiv_id: '2403.20252'
source_url: https://arxiv.org/abs/2403.20252
tags:
- preference
- survey
- answer
- language
- qlora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for aligning large language models
  (LLMs) to model the preferences of a target population using survey data. The approach
  involves prompting pre-trained LLMs with demographic information to act as virtual
  survey participants, then fine-tuning these models using the survey responses.
---

# Using LLMs to Model the Beliefs and Preferences of Targeted Populations

## Quick Facts
- arXiv ID: 2403.20252
- Source URL: https://arxiv.org/abs/2403.20252
- Reference count: 40
- Method for aligning LLMs to model population preferences using survey data with fine-tuning

## Executive Summary
This paper presents a method for aligning large language models to model the beliefs and preferences of a target population using survey data. The approach involves prompting pre-trained LLMs with demographic information to act as virtual survey participants, then fine-tuning these models using the survey responses. The authors benchmark two fine-tuning methods (LoRA and QLoRA) and evaluate their performance on a survey about battery electric vehicle preferences. Key findings include that fine-tuned LLMs outperform pre-trained models in matching both population-wide statistics and individual responses, with temperature sampling allowing control over this trade-off.

## Method Summary
The authors fine-tune Llama 2 models (7B, 13B, 70B) using QLoRA with specific hyperparameters on survey data about battery electric vehicle preferences. They use cross-entropy loss with an optional numerical penalty term for questions requiring numerical responses. The fine-tuned models are evaluated using KL-divergence (population-wide statistics) and RMSE (individual responses), with temperature sampling allowing control over the trade-off between these metrics. The approach is compared against baseline models (SVR and CatBoost) and pre-trained models.

## Key Results
- Fine-tuned LLMs outperform pre-trained models in matching both population-wide statistics (KL-divergence) and individual responses (RMSE)
- Larger models perform better initially but the advantage diminishes after fine-tuning
- QLoRA provides minimal degradation compared to LoRA while offering significant computational savings
- Temperature sampling allows trading off between matching population statistics versus individual responses
- A numerical penalty term in the loss function improves performance on numerical survey questions

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning LLMs on survey data significantly improves their ability to match both population-wide statistics and individual responses compared to pre-trained models. Pre-trained LLMs learn general language patterns from large internet-scale datasets but lack specific alignment to a target population's preferences. Fine-tuning with survey data adapts the model parameters to better represent the statistical distribution of responses from that population. The core assumption is that the survey data is representative of the target population and contains sufficient diversity in responses to capture meaningful patterns.

### Mechanism 2
The temperature parameter allows trading off between matching population-wide statistics versus individual responses. Temperature controls the randomness of token sampling during generation. Lower temperatures (greedy sampling) make the model more deterministic, favoring the most likely responses which tend to match individual responses better but may miss population-level variation. Higher temperatures increase randomness, allowing the model to explore a wider range of responses that better match population statistics. The core assumption is that the relationship between temperature and the KL-divergence/RMSE trade-off is monotonic and predictable.

### Mechanism 3
The novel numerical penalty term in the loss function improves model performance on survey questions requiring numerical responses. The penalty term provides additional gradient signal during training by rewarding numerical answers that are close to the true value, not just exactly correct. This helps the model learn better numerical reasoning by distinguishing between "close" and "far" answers rather than just "correct" and "incorrect." The core assumption is that the numerical answers in the survey can be meaningfully compared using distance metrics, and that nearby values are more similar in meaning than distant values.

## Foundational Learning

- Concept: KL-divergence as a measure of distributional similarity
  - Why needed here: KL-divergence is used to measure how well the model matches population-wide statistics by comparing the distribution of generated responses to the distribution of actual survey responses.
  - Quick check question: If you have two distributions P and Q, and KL(P||Q) = 0.2, what does this tell you about their similarity?

- Concept: RMSE as a measure of individual prediction accuracy
  - Why needed here: RMSE measures how well the model matches individual responses by calculating the average magnitude of errors between predicted and actual numerical answers.
  - Quick check question: If a model has RMSE of 5 on preference ratings from 0-100, what does this tell you about its typical prediction error?

- Concept: Temperature scaling in language model sampling
  - Why needed here: Temperature is used to control the randomness of token generation, allowing the model to trade off between matching population statistics (higher temperature) and individual responses (lower temperature).
  - Quick check question: If a model outputs logits [2.0, 1.0, 0.5] for three tokens, what are the probabilities at temperature 1.0 versus temperature 0.5?

## Architecture Onboarding

- Component map: Survey data -> preprocessing -> train/validation/test splits -> model training with LoRA/QLoRA adapters -> temperature sampling -> answer extraction -> KL-divergence and RMSE evaluation
- Critical path: Survey data preparation -> model fine-tuning -> temperature parameter tuning -> evaluation and iteration
- Design tradeoffs: Model size vs. computational cost (larger models perform better initially but advantage diminishes after fine-tuning), temperature vs. metric alignment (lower temperature favors individual responses, higher favors population statistics), penalty term coefficient vs. training stability (too high can destabilize training, too low provides insufficient benefit), data quantity vs. model performance (more data generally improves results but with diminishing returns)
- Failure signatures: KL-divergence low but RMSE high (model captures population trends but fails on individual predictions, may need lower temperature), KL-divergence high but RMSE low (model matches individuals but misses population patterns, may need higher temperature), both metrics poor (insufficient fine-tuning, poor data quality, or inappropriate model choice), training instability (penalty term coefficient too high or learning rate inappropriate)
- First 3 experiments: Baseline comparison (pre-trained vs. fine-tuned models on both metrics to establish improvement baseline), temperature sweep (test greedy vs. calibrated sampling across range of temperatures to map KL/RMSE trade-off), penalty term ablation (compare performance with and without numerical penalty term on questions requiring numerical answers)

## Open Questions the Paper Calls Out

### Open Question 1
How does model performance scale with even larger models (e.g., 175B parameters) beyond the 70B model tested? The paper shows that larger models (70B) initially perform better than smaller ones (7B, 13B) on KL-divergence metrics, but this advantage diminishes after fine-tuning. Testing additional model sizes would determine if there is a point of diminishing returns and identify the optimal model size for this application.

### Open Question 2
Can the numerical penalty term be generalized to work with multi-token numerical representations without requiring answer scaling? The current implementation requires both scaling of numerical answers and the assumption that each possible numerical answer corresponds to a single token, limiting its applicability to broader numerical ranges. Developing a penalty term formulation that can handle multi-token numerical representations would expand its utility.

### Open Question 3
How well do the fine-tuned models generalize to completely different behavioral scenarios beyond the BEV preference survey? The authors acknowledge that although their approach demonstrates matching responses on survey data, they will study the extent to which this shift successfully aligns the model to unseen behavioral scenarios. Testing the fine-tuned models on entirely different survey domains would assess whether the fine-tuning approach generalizes across behavioral contexts or is domain-specific.

## Limitations
- Evaluation is limited to a single survey dataset focused on battery electric vehicle preferences, which may not generalize to other domains or question types
- The paper doesn't address potential demographic biases in the survey data or how these might be amplified by fine-tuning
- The numerical penalty term, while showing some benefit, requires careful tuning and may not be necessary for all survey question types

## Confidence

**High Confidence:** The core finding that fine-tuned LLMs outperform pre-trained models on both population-wide statistics (KL-divergence) and individual responses (RMSE) is well-supported by the experimental results. The temperature parameter's role in controlling the KL/RMSE trade-off is clearly demonstrated across multiple model sizes.

**Medium Confidence:** The numerical penalty term's effectiveness is shown, but the magnitude of improvement is relatively modest. The mechanism by which this term improves numerical reasoning could be more rigorously explained. The diminishing returns of larger model sizes after fine-tuning is observed but the exact relationship needs more data points.

**Low Confidence:** The paper's claim that these fine-tuned models can serve as "accurate statistical proxies" for human populations requires more extensive validation across diverse domains, survey types, and demographic groups. The generalizability of findings from a single BEV survey to broader population modeling applications remains uncertain.

## Next Checks
1. **Cross-Domain Generalization Test:** Evaluate the fine-tuning approach on at least 3-5 surveys from different domains (e.g., healthcare preferences, political opinions, consumer behavior) to assess whether the observed improvements in KL-divergence and RMSE are domain-specific or generalizable.

2. **Bias Amplification Analysis:** Conduct a systematic investigation of how demographic biases in the original survey data propagate through fine-tuning. Compare the demographic representation in generated responses versus the training data to quantify bias amplification.

3. **Human Validation Study:** Recruit human evaluators to compare LLM-generated responses with actual survey responses in a blind test. Measure whether humans can distinguish between LLM-generated and human responses, and assess which model configurations produce the most human-like responses.