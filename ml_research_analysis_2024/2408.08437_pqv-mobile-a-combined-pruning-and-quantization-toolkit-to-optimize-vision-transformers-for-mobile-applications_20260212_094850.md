---
ver: rpa2
title: 'PQV-Mobile: A Combined Pruning and Quantization Toolkit to Optimize Vision
  Transformers for Mobile Applications'
arxiv_id: '2408.08437'
source_url: https://arxiv.org/abs/2408.08437
tags:
- pruning
- mobile
- quantization
- pytorch
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents PQV-Mobile, a toolkit for optimizing Vision
  Transformers (ViTs) for mobile applications through combined pruning and quantization.
  The tool supports structured pruning methods based on magnitude, Taylor, and Hessian
  importance, as well as quantization from FP32 to FP16 and int8 for various hardware
  backends.
---

# PQV-Mobile: A Combined Pruning and Quantization Toolkit to Optimize Vision Transformers for Mobile Applications

## Quick Facts
- arXiv ID: 2408.08437
- Source URL: https://arxiv.org/abs/2408.08437
- Authors: Kshitij Bhardwaj
- Reference count: 3
- Key outcome: Pruning DeiT models by 9.375% and quantizing to int8 from FP32 achieves 7.18× latency reduction with only 2.24% accuracy loss.

## Executive Summary
PQV-Mobile is a toolkit designed to optimize Vision Transformers (ViTs) for mobile applications by combining structured pruning and quantization. It supports multiple pruning importance metrics (magnitude, Taylor, Hessian) and quantization backends (x86, FBGEMM, QNNPACK, ONEDNN). When applied to Facebook's DeiT models, the toolkit achieves significant latency reductions while maintaining accuracy, making it suitable for resource-constrained mobile deployments. The tool is open-source and compatible with any HuggingFace ViTs from the TIMM library.

## Method Summary
PQV-Mobile implements post-training structured pruning using magnitude, Taylor, and Hessian importance metrics, followed by quantization from FP32 to FP16 or int8. The workflow involves loading a ViT model from TIMM, applying structured pruning, finetuning the pruned model for 60 epochs, quantizing for the target backend, and optimizing for mobile deployment via Torchscript conversion and Pytorch Lite. The toolkit leverages inter-layer dependencies to group parameters for simultaneous pruning, improving efficiency. It targets mobile hardware optimization through backend-specific quantization and conversion to mobile-friendly formats.

## Key Results
- Pruning DeiT models by 9.375% and quantizing to int8 from FP32 achieves 7.18× latency reduction with only 2.24% accuracy loss.
- Taylor importance pruning outperforms L1-norm and Hessian-based pruning for ViTs.
- x86 and FBGEMM backends provide the best latency performance on x86 machines with AVX support.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PQV-Mobile achieves significant latency reduction by combining structured pruning with quantization.
- Mechanism: Structured pruning removes entire blocks (e.g., attention heads, channels) based on importance metrics (L1, Taylor, Hessian), reducing model size and computation. Quantization (FP32→int8) further reduces memory footprint and speeds up inference, especially when paired with mobile-optimized formats like Torchscript and Pytorch Lite.
- Core assumption: The pruned and quantized model retains sufficient accuracy for practical use, and mobile hardware backends can efficiently execute the reduced model.
- Evidence anchors:
  - [abstract] "Our results show that even pruning a DeiT model by 9.375% and quantizing it to int8 from FP32 followed by optimizing for mobile applications, we find a latency reduction by 7.18× with a small accuracy loss of 2.24%."
  - [section] "Pruning the quantized model by 9.375% leads to further 9.8% lower latency (an overall 7.14× latency reduction over the original dense/FP32 Lite model)."
- Break condition: If pruning removes too many important features or quantization introduces excessive numerical error, accuracy may drop beyond acceptable limits, negating the benefit of speed.

### Mechanism 2
- Claim: Taylor importance pruning outperforms L1-norm and Hessian-based pruning for Vision Transformers.
- Mechanism: Taylor pruning estimates the impact of removing each filter by approximating the change in loss using gradients, allowing for faster and more accurate importance ranking. This is particularly effective for ViTs where attention mechanisms and linear layers are interdependent.
- Core assumption: The Taylor approximation is sufficiently accurate for ranking filter importance in ViTs, and the finetuning step can recover accuracy lost during pruning.
- Evidence anchors:
  - [section] "Although there is a very small change in accuracy between Taylor, L1-norm, and Hessian-based pruning, Taylor outperforms the other methods."
  - [section] "We prune the dense model by 9.375%, finetune it, and also quantize the finetuned pruned model to int8 for this experiment."
- Break condition: If the model architecture changes significantly (e.g., new ViT variants), the Taylor approximation may no longer accurately reflect importance, requiring recalibration of the pruning strategy.

### Mechanism 3
- Claim: Hardware backend selection significantly affects latency of quantized models.
- Mechanism: Different quantization backends (x86, FBGEMM, QNNPACK, ONEDNN) are optimized for specific hardware instructions (e.g., AVX for x86). Choosing the right backend ensures the quantized model runs efficiently on the target device.
- Core assumption: The deployment hardware matches one of the supported backends, and the backend’s optimizations are compatible with the model’s operations.
- Evidence anchors:
  - [section] "We find that x86 and FBGEMM backends to be the best with FBGEMM slightly outperforming x86. These results are expected as we are running on an x86 machine with Advanced Vector Extensions (AVX) enabled, which are used for fast path executions for both x86 and FBGEMM backends."
- Break condition: If the target device uses a backend not supported by PQV-Mobile (e.g., a custom AI accelerator), the latency benefits may not materialize.

## Foundational Learning

- Concept: Structured pruning vs. unstructured pruning
  - Why needed here: Structured pruning removes entire blocks, making the model hardware-friendly and reducing computation predictably, while unstructured pruning only sparsifies weights, requiring specialized hardware for speed gains.
  - Quick check question: What is the key difference between structured and unstructured pruning in terms of hardware compatibility?

- Concept: Post-training quantization
  - Why needed here: PQV-Mobile uses post-training quantization to convert FP32 models to lower precision (FP16/int8) without retraining, enabling faster inference on mobile devices with limited resources.
  - Quick check question: Why is post-training quantization preferred over quantization-aware training in PQV-Mobile’s workflow?

- Concept: Model optimization for mobile deployment
  - Why needed here: Converting models to Torchscript and Pytorch Lite formats, and applying mobile optimizers, ensures the pruned and quantized models run efficiently on mobile devices with constrained memory and compute.
  - Quick check question: What are the steps involved in converting a Pytorch model to a mobile-friendly format in PQV-Mobile?

## Architecture Onboarding

- Component map:
  - Pruning engine (magnitude/Taylor/Hessian importance) -> Quantization engine (FP32→FP16/int8) -> Mobile optimization pipeline (Torchscript → mobile optimizer → Pytorch Lite)

- Critical path:
  1. Load ViT model from TIMM library.
  2. Apply structured pruning based on chosen importance metric.
  3. Finetune pruned model to recover accuracy.
  4. Quantize model for target backend.
  5. Optimize for mobile and convert to Lite format.

- Design tradeoffs:
  - Pruning amount vs. accuracy: Higher pruning reduces latency/memory but risks accuracy loss.
  - Quantization precision vs. speed: int8 offers more speedup than FP16 but may hurt accuracy more.
  - Backend choice vs. hardware: Select backend matching deployment hardware for best performance.

- Failure signatures:
  - Accuracy drops sharply after pruning → likely over-pruned or insufficient finetuning.
  - Latency does not improve after quantization → backend mismatch or unsupported operations.
  - Model fails to convert to Lite format → unsupported operators or incompatible model structure.

- First 3 experiments:
  1. Apply 9.375% pruning with Taylor importance to a DeiT base model, finetune, and measure accuracy drop.
  2. Quantize the pruned model to int8 using x86 backend, measure latency and accuracy.
  3. Repeat step 2 with FBGEMM backend and compare latency to x86.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation is confined to image classification tasks on ImageNet using specific ViT architectures, leaving performance on other vision tasks or ViT variants unexplored.
- The pruning ratio of 9.375% may not represent the full potential or limits of the method.
- Real-world mobile hardware diversity and heterogeneous computing environments could yield different results than those reported.

## Confidence
- **High confidence**: Latency and accuracy improvements for DeiT models under specified conditions; effectiveness of Taylor importance pruning over L1 and Hessian; backend performance on x86/FBGEMM.
- **Medium confidence**: Generalization to other ViT architectures and tasks; optimal pruning ratios for broader scenarios.
- **Low confidence**: Performance on non-ImageNet datasets, other vision tasks, or novel ViT variants; real-world deployment scenarios with diverse mobile hardware.

## Next Checks
1. Test PQV-Mobile on ViT variants (e.g., Swin, PVT) and diverse vision tasks (e.g., object detection, segmentation) to assess generalizability.
2. Evaluate the toolkit on resource-constrained mobile devices (e.g., smartphones with ARM-based processors) to validate real-world latency gains.
3. Investigate the impact of higher pruning ratios (e.g., 20-30%) on accuracy-latency trade-offs to determine scalability limits.