---
ver: rpa2
title: 'MACT: Model-Agnostic Cross-Lingual Training for Discourse Representation Structure
  Parsing'
arxiv_id: '2406.01052'
source_url: https://arxiv.org/abs/2406.01052
tags:
- parsing
- cross-lingual
- training
- data
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a model-agnostic cross-lingual training approach
  for Discourse Representation Structure (DRS) parsing. The method leverages multilingual
  training data and exploits alignments in pre-trained language models without requiring
  language identification or machine translation.
---

# MACT: Model-Agnostic Cross-Lingual Training for Discourse Representation Structure Parsing

## Quick Facts
- arXiv ID: 2406.01052
- Source URL: https://arxiv.org/abs/2406.01052
- Authors: Jiangming Liu
- Reference count: 0
- Primary result: Model-agnostic cross-lingual training achieves state-of-the-art DRS parsing across English, German, Italian, and Dutch

## Executive Summary
This paper introduces a model-agnostic cross-lingual training approach for Discourse Representation Structure (DRS) parsing that leverages multilingual training data and pre-trained language model alignments without requiring language identification or machine translation. The method employs LoRA-based parameter-efficient fine-tuning on mT0-large to adapt to DRS parsing tasks. Experiments on PMB 3.0.0 and 4.0.0 benchmarks demonstrate significant improvements over monolingual baselines, particularly for low-resource languages, with cross-lingual training consistently producing more well-formed DRS outputs.

## Method Summary
The approach uses cross-lingual training data from PMB datasets, training mT0-large with LoRA fine-tuning (rank 32) on attention layers. The model is trained on random batches from all available languages without language identification. After cross-lingual training, optional monolingual fine-tuning can be applied for resource-rich languages. The method is evaluated using Counter for clause parsing and Smatch for graph parsing, with additional analysis of ill-formed DRS generation rates.

## Key Results
- Cross-lingual training achieves 4.19% F1 improvement for Italian and 11.65% for Dutch DRS clause parsing
- Models generate fewer ill-formed DRSs compared to monolingual baselines, particularly with Cross-lingual+ variant
- State-of-the-art results achieved on PMB 3.0.0 and 4.0.0 for all four languages
- Significant performance gains for low-resource languages (German and Dutch) without sacrificing quality on high-resource languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-lingual training exploits multilingual alignments encoded in PLMs to generalize DRS parsing across languages without requiring language identification.
- Mechanism: The model is trained on multilingual training data where each batch randomly samples instances from all languages. PLM multilingual embeddings capture cross-lingual semantic correspondences, allowing the model to learn language-agnostic DRS representations.
- Core assumption: PLMs contain sufficient cross-lingual alignment information to support semantic parsing transfer without explicit language tags.
- Evidence anchors:
  - [abstract] "leverages cross-lingual training data and fully exploits the alignments between languages encoded in pre-trained language models"
  - [section 4.3] "the cross-lingual training requires language-specific training data (Italian data) and a set of multilingual training data... we do not have to identify the languages used in the training instances"
  - [corpus] Weak evidence - no specific papers cited directly supporting PLM alignment sufficiency for DRS parsing specifically.
- Break condition: If PLMs lack adequate cross-lingual semantic alignment for DRS-specific predicates and relations, or if language-specific semantic nuances are critical for accurate parsing.

### Mechanism 2
- Claim: Parameter-efficient fine-tuning (LoRA) enables effective adaptation of large PLMs for DRS parsing while maintaining computational efficiency.
- Mechanism: LoRA introduces small trainable low-rank matrices into the attention layers while freezing the original PLM weights. This approximates the full parameter space needed for task adaptation with fewer parameters.
- Core assumption: The frozen PLM parameters contain sufficient general linguistic knowledge that can be specialized through small additive matrices.
- Evidence anchors:
  - [section 4.2] "LoRA adopts a different approach by keeping the pre-trained model frozen and introducing smaller trainable low-rank matrices into each layer of the model"
  - [section 5.1] "LoRA matrices are exclusively added to the query and value parameters within all the attention layers, with a fixed rank (r) of 32"
  - [corpus] Weak evidence - no specific papers cited validating LoRA effectiveness for DRS parsing specifically.
- Break condition: If the task requires substantial architectural modifications beyond what low-rank adaptations can provide, or if the PLM lacks task-relevant pre-training.

### Mechanism 3
- Claim: Cross-lingual training improves generalization and produces more well-formed DRS outputs compared to monolingual baselines, especially for low-resource languages.
- Mechanism: By exposing the model to diverse linguistic patterns across languages, cross-lingual training helps it learn more robust semantic representations that generalize better and avoid language-specific idiosyncrasies that might lead to ill-formed outputs.
- Core assumption: Exposure to multiple languages during training helps the model learn more generalizable patterns for DRS construction.
- Evidence anchors:
  - [abstract] "cross-lingual training consistently generates more well-formed DRSs compared to monolingual baselines"
  - [section 5.4] "Cross-lingual, despite generating DRSs of slightly lower quality compared to Cross-lingual+, produces fewer ill-formed DRSs on average"
  - [section 5.2] "Cross-lingual achieves significant F1 improvements of 4.19% and 11.65%, respectively, in DRS clause parsing" for low-resource languages
- Break condition: If the languages in the training data are too dissimilar for beneficial transfer, or if language-specific semantic conventions are crucial for DRS formation.

## Foundational Learning

- Concept: Discourse Representation Theory (DRT) and DRS formalism
  - Why needed here: Understanding how DRSs represent meaning through nested boxes, clauses, and graphs is essential for implementing the parser and evaluating outputs
  - Quick check question: What are the three main forms of DRS representation discussed in the paper, and how do they differ in structure?

- Concept: Cross-lingual transfer learning
  - Why needed here: The core innovation relies on transferring knowledge across languages without explicit alignment, requiring understanding of how multilingual models encode semantic relationships
  - Quick check question: How does the paper's cross-lingual training approach differ from traditional machine translation-based cross-lingual methods?

- Concept: Parameter-efficient fine-tuning techniques (specifically LoRA)
  - Why needed here: The model uses LoRA for adaptation, so understanding how it modifies attention weights while keeping PLMs frozen is crucial for implementation and troubleshooting
  - Quick check question: What specific components of the transformer architecture does LoRA modify, and why is this approach computationally advantageous?

## Architecture Onboarding

- Component map:
  - Pre-trained language model (mT0-large) - frozen backbone
  - LoRA adapters - trainable low-rank matrices in attention layers
  - Cross-lingual training pipeline - multilingual data batching without language identification
  - Fine-tuning stage - optional monolingual fine-tuning for resource-rich languages
  - Evaluation components - Counter for clause parsing, SMA TCH for graph parsing, custom ill-formed detection

- Critical path:
  1. Load mT0-large and initialize LoRA matrices
  2. Create multilingual training batches from all available data
  3. Train with LoRA adaptation for cross-lingual understanding
  4. (Optional) Fine-tune on monolingual data for specific languages
  5. Evaluate using standard metrics and ill-formed detection

- Design tradeoffs:
  - Cross-lingual vs monolingual training: Cross-lingual provides better generalization for low-resource languages but may underperform on resource-rich languages without fine-tuning
  - LoRA rank selection: Higher rank provides more adaptation capacity but increases computational cost
  - Training schedule: Cross-lingual training uses fewer epochs (20) than monolingual fine-tuning (100), reflecting different optimization needs

- Failure signatures:
  - High ill-formed DRS percentage: Indicates the model isn't learning proper DRS structure despite achieving reasonable F1 scores
  - Poor performance on resource-rich languages: May indicate cross-lingual training is diluting language-specific patterns
  - Low cross-lingual gains: Could suggest the PLM lacks sufficient cross-lingual alignment or the languages are too dissimilar

- First 3 experiments:
  1. Implement basic cross-lingual training on PMB 3.0.0 data and measure F1 improvement over monolingual baseline for Italian and Dutch
  2. Test different LoRA rank values (16, 32, 64) to find optimal balance between performance and efficiency
  3. Evaluate ill-formed DRS generation rate to verify the claim that cross-lingual training produces more well-formed outputs

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions. However, based on the content, several important open questions emerge:

### Open Question 1
- Question: How does the proposed model-agnostic cross-lingual training approach scale to languages with vastly different linguistic structures compared to English, German, Italian, and Dutch?
- Basis in paper: [inferred] The paper focuses on experiments with English, German, Italian, and Dutch, but does not explicitly discuss how the approach would perform on languages with significantly different structures.
- Why unresolved: The experiments were limited to a specific set of languages, and the paper does not provide theoretical analysis or empirical evidence on the approach's effectiveness across a broader range of linguistic structures.
- What evidence would resolve it: Conducting experiments with languages from different language families (e.g., Mandarin Chinese, Arabic, or Finnish) and comparing the performance of the cross-lingual training approach to monolingual training would provide insights into its scalability.

### Open Question 2
- Question: What is the impact of the proposed cross-lingual training method on semantic parsing tasks beyond DRS parsing, such as Abstract Meaning Representation (AMR) parsing or Semantic Role Labeling (SRL)?
- Basis in paper: [inferred] The paper focuses exclusively on DRS parsing and does not discuss the potential applicability of the cross-lingual training method to other semantic parsing tasks.
- Why unresolved: The paper's scope is limited to DRS parsing, and there is no theoretical or empirical evidence provided on the method's effectiveness for other semantic parsing tasks.
- What evidence would resolve it: Applying the cross-lingual training approach to other semantic parsing tasks, such as AMR parsing or SRL, and comparing its performance to monolingual training and existing cross-lingual methods would provide insights into its generalizability.

### Open Question 3
- Question: How does the quality of the cross-lingual training data affect the performance of the proposed approach, and what strategies can be employed to mitigate the impact of noisy or low-quality data?
- Basis in paper: [inferred] The paper does not explicitly discuss the quality of the cross-lingual training data or strategies to handle noisy or low-quality data.
- Why unresolved: The experiments use a fixed set of cross-lingual training data without addressing the potential impact of data quality on the approach's performance.
- What evidence would resolve it: Conducting experiments with cross-lingual training data of varying quality (e.g., using automatic translations with different levels of accuracy) and comparing the performance of the approach would provide insights into the impact of data quality. Additionally, exploring data cleaning and filtering strategies to mitigate the impact of noisy or low-quality data would be valuable.

## Limitations
- Limited to four closely-related languages (Germanic/Romance) without testing on more typologically diverse languages
- Relies on silver-standard training data for German and Dutch, which may introduce noise
- Evaluation focuses on standard metrics without deeper linguistic analysis of transferred semantic phenomena

## Confidence
- **High Confidence**: The core mechanism of LoRA fine-tuning for parameter-efficient adaptation (supported by extensive prior work on LoRA effectiveness)
- **Medium Confidence**: Cross-lingual training benefits for low-resource languages (supported by empirical results but limited to four languages)
- **Low Confidence**: Claims about PLM alignment sufficiency for DRS parsing without language identification (lacks direct supporting evidence for this specific application)

## Next Checks
1. **Linguistic Analysis**: Perform fine-grained error analysis on cross-lingual vs monolingual outputs to identify which DRS predicates and semantic relations transfer successfully versus fail across languages.

2. **Generalization Test**: Evaluate the cross-lingual approach on a typologically diverse language pair (e.g., English-Turkish or English-Japanese) to test whether PLM alignment benefits extend beyond Germanic/Romance languages.

3. **Silver Data Impact**: Compare performance when using only gold-standard training data versus silver data to quantify the impact of noisy annotations on cross-lingual learning effectiveness.