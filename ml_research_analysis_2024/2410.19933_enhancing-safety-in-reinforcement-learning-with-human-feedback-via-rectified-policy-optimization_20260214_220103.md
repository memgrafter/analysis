---
ver: rpa2
title: Enhancing Safety in Reinforcement Learning with Human Feedback via Rectified
  Policy Optimization
arxiv_id: '2410.19933'
source_url: https://arxiv.org/abs/2410.19933
tags:
- safety
- repo
- language
- which
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses safety interference in reinforcement learning
  from human feedback (RLHF), where average-based safety constraints lead to inconsistent
  safety across prompts. The authors propose Rectified Policy Optimization (RePO),
  which replaces expected safety constraints with per-prompt strict constraints.
---

# Enhancing Safety in Reinforcement Learning with Human Feedback via Rectified Policy Optimization

## Quick Facts
- arXiv ID: 2410.19933
- Source URL: https://arxiv.org/abs/2410.19933
- Reference count: 17
- This paper addresses safety interference in reinforcement learning from human feedback (RLHF), where average-based safety constraints lead to inconsistent safety across prompts. The authors propose Rectified Policy Optimization (RePO), which replaces expected safety constraints with per-prompt strict constraints. The core method uses rectified policy gradients that penalize strict safety violations for each prompt, focusing optimization on unsafe samples. Experiments on Alpaca-7B demonstrate RePO improves safety alignment metrics (from 43.99% to 96.08% constraint satisfaction) while maintaining or improving helpfulness compared to baseline methods including PPO-Lagrange and SACPO. GPT-4 scoring evaluation shows RePO outperforms baselines across multiple safety-focused datasets.

## Executive Summary
This paper addresses a critical limitation in reinforcement learning from human feedback (RLHF) systems: the inconsistency of safety constraints across different prompts when using average-based approaches. The authors propose Rectified Policy Optimization (RePO), which replaces expected safety constraints with per-prompt strict constraints to ensure consistent safety performance across all prompts. The method introduces rectified policy gradients that specifically target and penalize safety violations on a per-prompt basis, shifting optimization focus to unsafe samples rather than treating all samples equally.

Through experiments on the Alpaca-7B model using toxic content datasets, RePO demonstrates significant improvements in safety alignment metrics, increasing constraint satisfaction from 43.99% to 96.08% while maintaining or improving helpfulness scores compared to baseline methods. The approach is validated using GPT-4 scoring across multiple safety-focused datasets, showing consistent superiority over existing methods including PPO-Lagrange and SACPO. This work represents a practical advancement in ensuring consistent safety in RLHF systems, addressing a fundamental challenge in deploying aligned language models.

## Method Summary
The paper proposes Rectified Policy Optimization (RePO) to address safety interference in RLHF systems. The core innovation replaces expected safety constraints with per-prompt strict constraints, ensuring consistent safety across all prompts rather than just on average. RePO introduces rectified policy gradients that specifically penalize strict safety violations for each prompt, focusing optimization on unsafe samples. The method operates by adding a rectification term to the policy gradient that captures strict constraint violations, then using this modified gradient for policy updates. This approach shifts optimization focus to unsafe samples while maintaining the overall RLHF framework. The authors implement RePO within the existing RLHF pipeline and evaluate it on the Alpaca-7B model using various toxic content datasets, comparing performance against baseline methods including PPO-Lagrange and SACPO.

## Key Results
- RePO improves safety constraint satisfaction from 43.99% to 96.08% compared to baseline methods
- Maintains or improves helpfulness scores while achieving superior safety performance
- Outperforms PPO-Lagrange and SACPO baselines across multiple safety-focused datasets in GPT-4 evaluation
- Demonstrates consistent safety improvements across different prompt types and toxic content scenarios

## Why This Works (Mechanism)
RePO works by fundamentally changing how safety constraints are enforced in RLHF systems. Instead of averaging safety performance across all prompts, RePO enforces strict per-prompt constraints, ensuring that no individual prompt violates safety requirements. The rectified policy gradients specifically target unsafe samples by adding a penalty term for each violation, shifting optimization focus to problematic cases. This mechanism prevents the averaging effect where unsafe behavior on some prompts can be offset by safe behavior on others, which is particularly important for applications requiring consistent safety performance across diverse inputs.

## Foundational Learning

**Reinforcement Learning from Human Feedback (RLHF)**: A training paradigm where models learn from human preferences rather than explicit rewards. Needed to understand the baseline framework being improved. Quick check: Can explain the three-phase RLHF process (SFT, reward modeling, policy optimization).

**Policy Gradient Methods**: Optimization techniques that directly update policy parameters based on gradient estimates. Needed to understand how RePO modifies standard RLHF optimization. Quick check: Can derive the REINFORCE algorithm and explain variance reduction techniques.

**Constrained Optimization in RL**: Methods for incorporating safety or behavioral constraints into reinforcement learning. Needed to understand the safety interference problem being addressed. Quick check: Can explain the difference between Lagrangian and hard constraint approaches.

**Gradient Rectification**: A technique for modifying gradients to emphasize certain samples or behaviors. Needed to understand the core RePO mechanism. Quick check: Can implement a basic gradient rectification scheme for a classification task.

## Architecture Onboarding

**Component Map**: Human Feedback Dataset -> Reward Model -> Policy Network -> Safety Constraint Checker -> Rectified Policy Gradients -> Updated Policy

**Critical Path**: Safety Constraint Violation Detection -> Gradient Rectification -> Policy Update -> Safety Performance Evaluation

**Design Tradeoffs**: Strict per-prompt constraints vs. flexible average constraints (safety consistency vs. model expressiveness), computational overhead of per-prompt checking vs. simpler averaging approaches, GPT-4 evaluation vs. human evaluation (scalability vs. reliability).

**Failure Signatures**: Over-constraining leading to reduced model utility, gradient rectification causing optimization instability, strict constraints being too conservative for nuanced safety scenarios, evaluation bias from GPT-4 scoring system.

**First Experiments**:
1. Implement basic gradient rectification on a simple classification task to verify the mechanism works
2. Run ablation study comparing strict vs. average constraints on a small RLHF dataset
3. Test RePO on a single toxic content dataset with Alpaca-7B to establish baseline performance

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies primarily on GPT-4 scoring, which may introduce subjectivity and may not fully capture real-world safety performance
- Experiments focus on a single model (Alpaca-7B) and relatively simple toxic content datasets, raising questions about generalizability
- Strict per-prompt safety constraints may be overly conservative in practice and could limit model expressiveness in edge cases

## Confidence

**High confidence**: Mathematical formulation and theoretical framework of RePO
**Medium confidence**: Experimental results given limited model diversity and dataset scope
**Medium confidence**: Comparative advantage over baseline methods due to potential evaluation framework bias

## Next Checks

1. Test RePO on larger models (Llama-2 70B, GPT-3.5) to evaluate scalability and performance across model sizes
2. Implement human evaluation studies to validate GPT-4 safety scoring, particularly for nuanced safety scenarios
3. Conduct ablation studies to determine optimal constraint strictness levels and identify potential trade-offs between safety consistency and model utility