---
ver: rpa2
title: 'FMEA Builder: Expert Guided Text Generation for Equipment Maintenance'
arxiv_id: '2411.05054'
source_url: https://arxiv.org/abs/2411.05054
tags:
- equipment
- fmea
- fmeas
- failure
- dfsp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a system using large language models (LLMs)
  to generate Failure Mode and Effects Analysis (FMEA) documents for industrial equipment.
  The approach uses Dynamic Few-Shot Prompting (DFSP) to retrieve relevant examples
  from a database and guide model generation.
---

# FMEA Builder: Expert Guided Text Generation for Equipment Maintenance

## Quick Facts
- arXiv ID: 2411.05054
- Source URL: https://arxiv.org/abs/2411.05054
- Authors: Karol Lynch; Fabio Lorenzi; John Sheehan; Duygu Kabakci-Zorlu; Bradley Eck
- Reference count: 2
- One-line result: DFSP improves FMEA generation quality with 98% user survey approval for AI-assisted tools

## Executive Summary
This work presents a system using large language models (LLMs) to generate Failure Mode and Effects Analysis (FMEA) documents for industrial equipment. The approach uses Dynamic Few-Shot Prompting (DFSP) to retrieve relevant examples from a database and guide model generation. Experiments with three LLMs (flan-ul2, llama2, Mixtral-8x7B) on equipment boundary and failure location generation show DFSP significantly improves performance over zero-shot and random-shot baselines. User surveys of reliability professionals showed positive reception for AI-assisted FMEA generation, with 82-98% indicating they would use such tools.

## Method Summary
The system uses Dynamic Few-Shot Prompting (DFSP) to retrieve relevant examples from a database via cosine similarity of text embeddings. These examples guide LLM generation for FMEA components. The approach includes structured response parsing using delimiters to convert LLM text responses into structured JSON format. Experiments were conducted on a proprietary database of 714 FMEAs, with equipment boundary generation evaluated using ROUGE-1 and component recall metrics, and failure location generation using recall, precision, and F1 scores.

## Key Results
- DFSP improved equipment boundary generation ROUGE-1 score from 0.477 to 0.787 with flan-ul2
- DFSP improved failure location generation recall from 24.6% to 55.9% with llama2
- 98% of reliability professionals indicated they would use AI-assisted FMEA generation tools

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic Few-Shot Prompting (DFSP) improves FMEA generation quality by retrieving semantically relevant examples from a database
- Mechanism: The system uses cosine similarity of text embeddings to rank and retrieve examples that are contextually similar to the current input, providing the LLM with high-quality, relevant examples rather than random ones
- Core assumption: Semantic similarity in embedding space correlates with relevance for FMEA generation tasks
- Evidence anchors:
  - [abstract] "Dynamic Few-Shot Prompting (DFSP) to retrieve relevant examples from a database and guide model generation"
  - [section] "Dynamic Few Shot Prompting (DFSP) We rank and retrieve relevant examples, or shots, by cosine similarity of text embeddings"
  - [corpus] "Found 25 related papers" (shows the system has a corpus to draw from)
- Break condition: If the embedding model is poorly trained or the database lacks diverse examples, cosine similarity may retrieve irrelevant examples that degrade performance

### Mechanism 2
- Claim: Structured response parsing enables better human oversight and error resolution in generated FMEA documents
- Mechanism: The system uses simple delimiters in prompts and a rule-based parser to convert LLM text responses into structured JSON format, making it easier for users to review, confirm, reject, or supplement specific components
- Core assumption: Breaking down complex text generation into structured components allows for more granular human supervision
- Evidence anchors:
  - [abstract] "Structured responses Parsing the LLM's response from pure text into structured responses enables presentation of FMEA components directly to the user"
  - [section] "Structured responses also simplify the resolution of repeated entities, a common problem in text generation"
  - [corpus] "average neighbor FMR=0.482" (indicates related work exists in structured generation)
- Break condition: If the rule-based parser cannot handle edge cases or the LLM generates malformed structured output, the system may fail to parse responses correctly

### Mechanism 3
- Claim: The attention mechanism in LLMs can interpret contextual meaning of technical terms, overcoming the challenge of polysemy in industrial equipment terminology
- Mechanism: By using the surrounding context of words in prompts, the LLM's attention mechanism can distinguish between different meanings of the same term (e.g., "casing" in different equipment contexts)
- Core assumption: The LLM's attention mechanism has been trained on sufficient diverse text to understand contextual differences in technical terminology
- Evidence anchors:
  - [abstract] "the attention mechanism [Vaswani et al., 2017] used in today's language models can interpret the meaning of words based on their context"
  - [section] "This behavior makes it difficult to create a navigable catalog of components from which to build FMEAs. However, the attention mechanism used in today's language models can interpret the meaning of words based on their context"
  - [corpus] "average citations=0.0" (weak corpus evidence for this specific claim)
- Break condition: If the LLM encounters highly specialized terminology not present in its training data, the attention mechanism may fail to disambiguate correctly

## Foundational Learning

- Concept: Cosine similarity for semantic search
  - Why needed here: Enables DFSP to retrieve relevant examples from the database based on semantic meaning rather than just keyword matching
  - Quick check question: If you have a query embedding vector [0.2, 0.8, 0.3] and three example vectors [0.1, 0.7, 0.4], [0.9, 0.1, 0.2], and [0.3, 0.6, 0.5], which example would be ranked highest?

- Concept: ROUGE-1 score calculation
  - Why needed here: Used to evaluate the quality of generated equipment boundaries by measuring unigram overlap between generated and reference text
  - Quick check question: If the reference text contains 100 words and the generated text contains 80 words, with 60 words overlapping, what is the ROUGE-1 recall score?

- Concept: Attention mechanism in transformers
  - Why needed here: Enables the LLM to understand context and disambiguate technical terms based on their usage in the prompt
  - Quick check question: What is the primary advantage of self-attention over traditional recurrent neural networks when processing sequences?

## Architecture Onboarding

- Component map: Database (714 FMEAs) → Embedding model → DFSP retriever → Prompt builder → LLM → Rule-based parser → Structured output → User interface
- Critical path: User input → DFSP retrieval → Prompt construction → LLM generation → Response parsing → User review
- Design tradeoffs: Using database examples (DFSP) vs zero-shot approach balances quality against dependency on existing examples; structured parsing adds complexity but enables better user supervision
- Failure signatures: Poor DFSP performance indicated by low ROUGE-1 scores; parsing failures indicated by malformed JSON output; user dissatisfaction indicated by negative survey responses
- First 3 experiments:
  1. Compare zero-shot, random-shot, and DFSP performance on equipment boundary generation using ROUGE-1 and component recall metrics
  2. Evaluate different LLMs (flan-ul2, llama2, Mixtral-8x7B) with DFSP to identify the best model for each FMEA component
  3. Conduct user surveys with reliability professionals to assess perceived quality and usability of the AI-assisted FMEA generation tool

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between automated AI generation and human expert supervision for creating high-quality FMEAs?
- Basis in paper: [explicit] User survey results showed positive reception for AI-assisted FMEA generation, with 82-98% indicating they would use such tools, but the survey also revealed different preferences for configurability ranging from fully automated to fully manual.
- Why unresolved: The paper only presents preliminary survey results without determining the optimal balance or specific configuration that would yield the best results in practice.
- What evidence would resolve it: Controlled experiments comparing FMEA quality and creation time using different levels of AI automation and human supervision, with quantitative quality metrics and expert time measurements.

### Open Question 2
- Question: How can ensemble methods improve the recall and precision of FMEA generation beyond the current performance of individual LLMs?
- Basis in paper: [explicit] The authors mention that "ensemble methods based on fuzzy voting provide a promising tool for combining results between models and shot orderings" and expect ensembles to improve recall "at a hopefully small cost in precision."
- Why unresolved: The paper presents results for individual models but does not implement or evaluate ensemble methods.
- What evidence would resolve it: Implementation and evaluation of ensemble methods combining multiple LLMs and shot orderings, with comparative performance metrics showing improvements in recall and precision.

### Open Question 3
- Question: How effective are LLMs at generating FMEAs for equipment types not covered in the training database using unstructured documentation as additional context?
- Basis in paper: [explicit] The authors note that "there are many equipment types not covered by the database where FMEA generation remains of interest" and propose using "manuals and process documents" with "pre-processing / chunking" as additional context for generation.
- Why unresolved: The paper focuses on equipment types within their proprietary database and does not evaluate performance on out-of-distribution equipment types using unstructured documentation.
- What evidence would resolve it: Experiments generating FMEAs for equipment types outside the database using unstructured documentation, with performance metrics comparing to database-based generation.

## Limitations

- Proprietary database prevents direct reproducibility and limits generalizability to other industrial domains
- User survey sample size (20-25 professionals) is relatively small for drawing definitive conclusions about real-world usability
- Only two FMEA components (equipment boundaries and failure locations) were evaluated, leaving other critical components unverified

## Confidence

- High confidence: DFSP methodology effectiveness for equipment boundary generation with strong quantitative improvements and positive user survey results
- Medium confidence: Failure location generation performance due to more modest improvements and sensitivity to prompt variations
- Low confidence: Generalizability beyond the specific industrial equipment domain covered in the proprietary database

## Next Checks

1. Cross-validation on public datasets: Test the DFSP approach on publicly available FMEA or maintenance documentation datasets to verify performance outside the proprietary database.

2. Longitudinal user study: Conduct a 3-month field study with reliability professionals using the tool for actual maintenance planning to assess real-world impact on productivity and accuracy.

3. Component generation completeness: Systematically evaluate the model's ability to generate all four FMEA components (boundaries, failure modes, failure locations, and effects) rather than just the two tested, measuring inter-component consistency.