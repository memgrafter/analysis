---
ver: rpa2
title: Improving Forward Compatibility in Class Incremental Learning by Increasing
  Representation Rank and Feature Richness
arxiv_id: '2403.15517'
source_url: https://arxiv.org/abs/2403.15517
tags:
- rank
- learning
- base
- class
- session
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method called effective-Rank based
  Feature Richness enhancement (RFR) to improve forward compatibility in class incremental
  learning. The core idea is to increase the effective rank of representations during
  the base session, which encodes more informative features for unseen novel tasks.
---

# Improving Forward Compatibility in Class Incremental Learning by Increasing Representation Rank and Feature Richness

## Quick Facts
- **arXiv ID**: 2403.15517
- **Source URL**: https://arxiv.org/abs/2403.15517
- **Reference count**: 8
- **Primary result**: Novel RFR method improves forward compatibility by increasing representation rank during base session, enhancing novel task performance while mitigating catastrophic forgetting across 11 CIL methods.

## Executive Summary
This paper introduces effective-Rank based Feature Richness enhancement (RFR), a method that increases the effective rank of feature representations during the base session of class incremental learning. By maximizing the Shannon entropy of representations under Gaussian assumptions, RFR encodes more informative features that benefit unseen novel tasks. The approach achieves dual objectives: improving forward compatibility while minimizing catastrophic forgetting by reducing feature extractor modifications during novel task learning. Experiments demonstrate that RFR consistently improves average incremental accuracy across multiple datasets and CIL methods.

## Method Summary
The RFR method adds a regularization loss during base session training that increases the effective rank of the feature extractor's output representation. The effective rank is computed as the sum of normalized eigenvalues of the representation matrix, with the logarithm applied for implementation effectiveness. This loss is combined with cross-entropy loss during base training only, with the feature extractor frozen during novel task learning. The method leverages the theoretical connection between effective rank and Shannon entropy maximization under Gaussian assumptions, aiming to encode richer features that novel tasks can leverage without extensive feature extractor modifications.

## Key Results
- RFR improves novel task performance while mitigating catastrophic forgetting when integrated with 11 well-known CIL methods
- Achieves dual objectives of minimizing feature extractor modifications and enhancing novel task performance
- Demonstrates consistent improvements in average incremental accuracy across all cases examined on CIFAR-100 and ImageNet-100

## Why This Works (Mechanism)

### Mechanism 1
Increasing representation rank during the base session enhances forward compatibility by encoding richer, more diverse features that benefit unseen novel tasks. The RFR loss increases the effective rank of the feature extractor's output representation, which corresponds to maximizing the Shannon entropy of the representation under a Gaussian assumption. This relies on the core assumption that higher effective rank implies richer features and better forward compatibility, with representations modeled as Gaussian distributions.

### Mechanism 2
RFR improves both forward and backward compatibility simultaneously by minimizing feature extractor modifications during novel task learning. By encoding richer features during the base session, novel tasks can leverage these features without requiring significant modifications to the feature extractor, thus mitigating catastrophic forgetting. This assumes novel tasks can effectively utilize the rich features encoded during the base session without extensive retraining.

### Mechanism 3
The effective rank is a continuous, differentiable extension of algebraic rank that can be optimized during training. RFR uses the logarithm of effective rank as the main method for controlling feature richness, which is differentiable and can be integrated into the end-to-end learning process. This relies on the logarithm of effective rank being an effective and differentiable surrogate for algebraic rank in optimizing feature richness.

## Foundational Learning

- **Concept: Effective Rank**
  - Why needed here: Used as a differentiable surrogate for algebraic rank to control feature richness during training
  - Quick check question: What is the difference between effective rank and algebraic rank, and why is effective rank preferred in this context?

- **Concept: Shannon Entropy**
  - Why needed here: Used as a measure of information content in the representation, and the method proves that maximizing effective rank maximizes Shannon entropy under a Gaussian assumption
  - Quick check question: How is Shannon entropy related to the effective rank of a representation, and what assumption is made about the distribution of the representation?

- **Concept: Class Incremental Learning (CIL)**
  - Why needed here: The broader context in which the method operates, involving learning new tasks while retaining knowledge from previous tasks
  - Quick check question: What are the main challenges in CIL, and how does the proposed method address these challenges?

## Architecture Onboarding

- **Component map**: Feature Extractor -> Classification Head -> Loss Function (CE + RFR)
- **Critical path**: Train base task with RFR loss to increase effective rank → Freeze feature extractor and train classification head for novel tasks → Evaluate performance on base and novel tasks
- **Design tradeoffs**: Increased effective rank during base session may lead to slightly longer training times but provides significant benefits in forward compatibility and catastrophic forgetting mitigation
- **Failure signatures**: If effective rank does not increase during base session, method may not provide intended benefits; if catastrophic forgetting is not mitigated, method may not effectively leverage rich base session features
- **First 3 experiments**: 1) Train model with RFR on base task and measure effective rank increase vs baseline 2) Evaluate novel task performance with and without freezing feature extractor 3) Measure catastrophic forgetting by comparing base task performance before and after novel task learning

## Open Questions the Paper Calls Out
The paper leaves open the precise theoretical relationship between effective rank and Shannon entropy under non-Gaussian distributions of representations. The proof relies on Gaussian assumptions (zero-mean multivariate Gaussian with tr(Σ)=1), and it remains unclear whether the result extends to other distributions.

## Limitations
- Theoretical connection between effective rank and Shannon entropy assumes Gaussian distributions for representations, which may not hold in practice
- Method's performance across diverse datasets and architectures beyond CIFAR-100 and ImageNet-100 remains unverified
- Optimal value of regularization coefficient α may vary significantly across different tasks and datasets

## Confidence
- Mechanism 1 (Rank enhancement improves forward compatibility): Medium - Supported by theoretical proofs but limited empirical validation across diverse settings
- Mechanism 2 (Dual compatibility improvement): Medium - Demonstrated through experiments but underlying assumptions about feature utilization need further validation
- Mechanism 3 (Effective rank optimization): High - Well-established mathematical properties, though practical implementation details may affect performance

## Next Checks
1. Test the method on non-Gaussian distributed representations to validate the robustness of the effective rank-Shannon entropy relationship
2. Evaluate RFR's performance across different backbone architectures (e.g., ConvNext, EfficientNet) to assess generalizability
3. Conduct ablation studies on the regularization coefficient α to determine optimal values for different dataset characteristics and task complexities