---
ver: rpa2
title: 'MMRole: A Comprehensive Framework for Developing and Evaluating Multimodal
  Role-Playing Agents'
arxiv_id: '2408.04203'
source_url: https://arxiv.org/abs/2408.04203
tags:
- characters
- gpt-4
- image
- arxiv
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MMRole, the first framework for Multimodal
  Role-Playing Agents (MRPAs), which extends traditional RPAs with multimodal understanding
  capabilities. The authors construct MMRole-Data, a large-scale dataset with 85 characters,
  11K images, and 14K dialogues, and develop MMRole-Eval, a comprehensive evaluation
  approach with eight metrics across three dimensions.
---

# MMRole: A Comprehensive Framework for Developing and Evaluating Multimodal Role-Playing Agents

## Quick Facts
- arXiv ID: 2408.04203
- Source URL: https://arxiv.org/abs/2408.04203
- Authors: Yanqi Dai; Huanran Hu; Lei Wang; Shengjie Jin; Xu Chen; Zhiwu Lu
- Reference count: 21
- Primary result: Introduces MMRole, the first framework for Multimodal Role-Playing Agents with a comprehensive dataset and evaluation approach

## Executive Summary
This paper introduces MMRole, the first framework for Multimodal Role-Playing Agents (MRPAs) that extend traditional role-playing agents with multimodal understanding capabilities. The authors construct MMRole-Data, a large-scale dataset with 85 characters, 11K images, and 14K dialogues, and develop MMRole-Eval, a comprehensive evaluation approach with eight metrics across three dimensions. The proposed MMRole-Agent achieves an overall score of 0.994 on the test set, demonstrating strong performance and generalization capabilities, particularly in multimodal understanding and role-playing consistency. The evaluation reveals that current LMMs struggle more with personality consistency and tone consistency compared to fundamental conversational skills.

## Method Summary
The framework consists of three main components: MMRole-Data for dataset construction, MMRole-Eval for evaluation with a reward model, and MMRole-Agent as the specialized MRPA. Character profiles are generated using GPT-4, images are collected and annotated, and dialogues are created for three scenarios: commentary interactions, human-role dialogues, and inter-role dialogues. The MMRole-Agent is fine-tuned using a 9B parameter model on the training set over 3 epochs with a learning rate of 1e-5. The reward model scores MRPAs by comparing their outputs to ground-truth data, providing stable evaluation metrics across eight dimensions covering conversational skills, multimodal understanding, and role-playing qualities.

## Key Results
- MMRole-Agent achieves an overall score of 0.994 on the test set, demonstrating strong performance
- The framework shows good generalization capabilities, maintaining strong performance on out-of-distribution characters
- Current LMMs show significant score variations across metrics, particularly struggling with personality consistency and tone consistency

## Why This Works (Mechanism)

### Mechanism 1
The reward model in MMRole-Eval achieves stable scoring by comparing MRPA outputs to ground-truth data rather than using absolute scores. The reward model first generates a qualitative comparison between the evaluated MRPA and the constructed ground-truth data, then assigns a score pair (MRPA score, ground-truth score). The final MRPA score is the ratio of these two values. This approach prevents scoring criteria from drifting over time by providing a stable baseline for comparison.

### Mechanism 2
Joint training with diverse multi-character data improves generalization of MMRole-Agent. Training on data from 72 diverse characters enables the model to learn generalizable multimodal role-playing capabilities through multi-task learning principles. This approach allows the model to acquire transferable skills across different character types rather than specializing in a single character profile.

### Mechanism 3
Multimodal understanding abilities are more challenging for MRPAs than fundamental conversational skills. Evaluation results show significant score variations across different metrics, with Personality Consistency and Tone Consistency showing the largest differences among models. This indicates these capabilities require more sophisticated multimodal integration and character-specific reasoning compared to basic conversational abilities like fluency and instruction adherence.

## Foundational Learning

- **Multimodal understanding in LMMs**: Why needed here - MRPAs must integrate visual and textual information to maintain character consistency while responding to image-centered dialogues. Quick check question - How does an LMM process an image input differently from a text-only input, and why does this matter for role-playing?

- **Reward modeling and preference learning**: Why needed here - The MMRole-Eval framework uses a reward model trained on GPT-4 comparisons to provide stable, reproducible evaluation scores. Quick check question - What is the difference between scoring outputs directly versus comparing them to ground-truth data, and why does this affect stability?

- **Multi-task learning principles**: Why needed here - Joint training on diverse character data (rather than single-character specialization) enables better generalization to unseen characters. Quick check question - How does training on multiple tasks (or characters) simultaneously typically affect model performance on new, related tasks?

## Architecture Onboarding

- **Component map**: Character profile generation → Image collection/annotation → Dialogue generation → Quality control → Training MMRole-Agent → Evaluation with MMRole-Eval
- **Critical path**: The most critical path is: character profile generation → image collection/annotation → dialogue generation → quality control → training MMRole-Agent → evaluation with MMRole-Eval. Any bottleneck in data quality directly impacts downstream performance.
- **Design tradeoffs**: The framework trades model size for specialized capability - MMRole-Agent uses a 9B parameter model fine-tuned on curated data rather than relying on larger general-purpose LMMs. This enables character-specific consistency but may limit general reasoning abilities.
- **Failure signatures**: Poor personality consistency often indicates insufficient character profile detail or mismatch between training dialogues and evaluation scenarios. Low image-text relevance suggests the model is not effectively integrating visual information or the image annotation is inadequate.
- **First 3 experiments**:
  1. Train MMRole-Agent on a small subset of data (e.g., 10% of characters) and evaluate on both seen and unseen characters to test generalization
  2. Remove image information from prompts and compare image-text relevance scores to quantify the impact of visual input
  3. Modify prompts slightly (e.g., change "step into the shoes of" to "imagine you are") and measure performance stability to test prompt sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of MRPAs degrade when tested on characters from entirely different genres or cultural backgrounds than those seen during training? The paper mentions that MMRole-Agent is tested on both in-distribution and out-of-distribution characters, but doesn't specifically test characters from entirely different genres or cultural backgrounds, nor does it analyze the degradation in performance.

### Open Question 2
Can MRPAs effectively handle more complex and abstract visual concepts, such as interpreting symbolic imagery or understanding nuanced emotional expressions in images? The paper focuses on MRPAs' ability to engage in dialogues centered around images, but it doesn't explicitly test their ability to interpret complex or abstract visual concepts.

### Open Question 3
What is the impact of the size and diversity of the training dataset on the performance of MRPAs, and is there a point of diminishing returns? While the paper demonstrates the effectiveness of training on a large and diverse dataset, it doesn't explore the relationship between dataset size, diversity, and performance in detail.

## Limitations

- The dataset construction relies heavily on GPT-4, introducing potential model bias that propagates through the entire pipeline
- The framework focuses on English-language characters and scenarios, limiting cross-cultural applicability
- The evaluation framework's dependence on GPT-4 for ground-truth data generation creates a circular dependency that may overestimate performance

## Confidence

- **High Confidence**: Overall framework architecture and evaluation methodology; claim that MMRole-Agent achieves 0.994 overall score; observation about LMMs struggling with personality and tone consistency
- **Medium Confidence**: Claim about joint training improving generalization; assertion about reward model comparison-based scoring stability
- **Low Confidence**: Generalizability to languages other than English; performance on highly specialized or technical role-playing scenarios

## Next Checks

1. Conduct systematic analysis of potential biases introduced by GPT-4 in character profile generation and dialogue creation, comparing profiles generated by different LMMs and assessing inter-rater agreement.

2. Expand the evaluation framework to include characters from diverse cultural backgrounds not represented in the current dataset, measuring performance degradation on unseen cultural contexts.

3. Perform controlled experiments comparing joint training on multiple characters versus sequential fine-tuning on individual characters, measuring zero-shot performance on unseen characters across different training approaches.