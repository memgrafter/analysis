---
ver: rpa2
title: Verified Training for Counterfactual Explanation Robustness under Data Shift
arxiv_id: '2403.03773'
source_url: https://arxiv.org/abs/2403.03773
tags:
- robust
- robustness
- training
- loss
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of ensuring counterfactual explanations
  (CEs) remain valid when machine learning models are updated due to data shift. The
  key idea is to jointly train a model and explainer to optimize for CE robustness
  over a set of similar models.
---

# Verified Training for Counterfactual Explanation Robustness under Data Shift

## Quick Facts
- arXiv ID: 2403.03773
- Source URL: https://arxiv.org/abs/2403.03773
- Reference count: 33
- This paper introduces VeriTraCER, a method for training counterfactual explainers that remain valid under model updates due to data shift.

## Executive Summary
This paper addresses the challenge of ensuring counterfactual explanations (CEs) remain valid when machine learning models are updated due to data shift. The key innovation is VeriTraCER, which uses verified training to provide deterministic guarantees on CE validity under small model changes. By jointly training a model and explainer, and using a new abstract interpretation technique called Simul-CROWN, the approach achieves high rates of certifiable robustness (typically over 90%) to both small parameter perturbations and real-world model updates like retraining with different random seeds or data subsets. The tradeoff is that VeriTraCER generally yields CEs that are a larger distance from the original sample compared to non-robust methods.

## Method Summary
VeriTraCER jointly trains a classifier and counterfactual explainer using CounterNet-style architecture with a robust loss function optimized via Simul-CROWN. The approach defines a multiplicity set of similar models that may arise under data shift, then trains to ensure CEs remain valid across all models in this set. Simul-CROWN provides tighter overapproximation of the robust loss compared to existing methods by jointly reasoning about the original and counterfactual predictions within the multiplicity set constraints. The method is evaluated across five datasets using both synthetic parameter perturbations and realistic model update scenarios.

## Key Results
- VeriTraCER achieves >90% δ-robustness across datasets, meaning CEs remain valid under small model perturbations
- The method provides high cross-model validity (>90%) across models with different random seeds or data subsets
- VeriTraCER outperforms baseline methods in robustness but produces CEs with larger distance from original samples
- Simul-CROWN provides tighter bounds than IBP/CROWN-IBP for the robust loss over-approximation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Simul-CROWN provides tighter overapproximation than IBP or CROWN-IBP by jointly reasoning about f(x) and f(x′) within the multiplicity set.
- **Mechanism**: The multiplicity set Mf,x constrains models to maintain the same prediction on x, so worst-case analysis on x′ can ignore models outside this region. Simul-CROWN encodes this constraint explicitly via LP-style optimization, yielding a smaller search space.
- **Core assumption**: Models in Mf,x are bounded by a shared δlp norm and maintain f(x) = fm(x).
- **Evidence anchors**:
  - [abstract] Introduces Simul-CROWN as "a new abstract interpretation technique for tighter overapproximation of the robust loss function."
  - [section 4.2.2] Shows how Simul-CROWN avoids the looseness of relaxing Mf,x to Mf by explicitly encoding the prediction constraint in the optimization.
  - [corpus] No direct evidence found in neighbor abstracts.
- **Break condition**: If the δlp bound is too large or the model has many local minima, the overapproximation may become too loose to certify robustness.

### Mechanism 2
- **Claim**: Joint training of f and g via CounterNet-style loss ensures that CEs are optimized for both validity and robustness across the model multiplicity.
- **Mechanism**: The loss function (Equation 1, 2) interleaves accuracy (LA), validity (LV), quality (LQ), and robustness (LR). Robustness is integrated via LR(xi, x′i, θf) = maxfm∈Mf,x LMSE(¯fm(x′), 1−y), encouraging the model to avoid regions where CEs become invalid under model shift.
- **Core assumption**: Gradient flow through the overapproximation is sufficient to guide the joint training toward models that yield robust CEs.
- **Evidence anchors**:
  - [abstract] States the approach "jointly trains a classifier and an explainer to optimize for CE robustness over a set of similar models."
  - [section 4.1] Describes the joint training loop and loss functions.
  - [corpus] Neighbor "Interval Abstractions for Robust Counterfactual Explanations" suggests similar overapproximation strategies, though without joint training.
- **Break condition**: If the overapproximation is too loose, gradient updates may not reduce the true robust loss, leading to poor robustness.

### Mechanism 3
- **Claim**: Using a multiplicity set rather than a fixed model allows the training process to select models that are inherently more robust to data shift.
- **Mechanism**: By optimizing CEs for worst-case models in Mf,x, the training process favors model parameters that lie in regions of the parameter space where small perturbations do not invalidate CEs. This contrasts with standard CE generation, which only optimizes for a single model.
- **Core assumption**: The multiplicity set Mf,x contains models likely to arise under realistic data shifts (e.g., retraining with different seeds or subsets).
- **Evidence anchors**:
  - [abstract] Notes that the approach "considers the multiplicity set of similar models" and aims for CEs "robust to small model changes."
  - [section 3] Defines Mf,x and explains that it includes models that "may be the result of model updates due to distribution shift."
  - [corpus] No direct evidence; neighbor papers focus on post-hoc robustness, not joint training.
- **Break condition**: If the δlp bound is too small, the multiplicity set becomes empty or too restrictive, preventing useful CE generation.

## Foundational Learning

- **Concept**: Abstract interpretation and interval arithmetic for overapproximating function behavior.
  - **Why needed here**: To efficiently compute an upper bound on the robust loss over infinitely many models in Mf,x without enumerating them.
  - **Quick check question**: What is the key difference between IBP and CROWN-IBP in how they bound function outputs?

- **Concept**: Verified training (also known as robust training) for ensuring deterministic guarantees under bounded perturbations.
  - **Why needed here**: To ensure that during training, the CEs are certified robust to small model updates, not just empirically robust.
  - **Quick check question**: Why does verified training require an overapproximation of the loss rather than exact computation?

- **Concept**: Counterfactual explanation validity: a CE x′ is valid if f(x′) ≠ f(x).
  - **Why needed here**: The definition of Mf,x-robustness requires that CEs remain valid for all models in the multiplicity set, which depends on understanding validity under model shifts.
  - **Quick check question**: How does the definition of Mf,x restrict the types of models considered when checking CE robustness?

## Architecture Onboarding

- **Component map**:
  - Data pipeline -> Model f -> CE generator g -> Overapproximation engine -> Training loop

- **Critical path**:
  1. Forward pass: compute CEs x′i = g(f, xi)
  2. Overapproximate robust loss LR via Simul-CROWN
  3. Compute total loss Lf/Lg with λ weights
  4. Backpropagate and update θf, θg
  5. Repeat until convergence

- **Design tradeoffs**:
  - **Tighter bounds vs speed**: Simul-CROWN is tighter but more expensive than IBP/CROWN-IBP
  - **Joint vs separate training**: Joint training ensures compatibility but may slow convergence compared to pre-training f and g separately
  - **δlp choice**: Larger δ increases robustness but may hurt accuracy; smaller δ may make the multiplicity set too restrictive

- **Failure signatures**:
  - **Low δ-robustness despite training**: Overapproximation too loose, gradients not reducing true robust loss
  - **Poor accuracy**: λ1 too small or robustness constraints overly restrictive
  - **CEs too far from input**: λ3 too small or robustness loss dominating proximity loss

- **First 3 experiments**:
  1. **Unit test Simul-CROWN**: Verify it returns tighter bounds than CROWN-IBP on a small synthetic network with known Mf,x
  2. **Ablation on λ weights**: Train with varying λ1, λ2, λ3, λ4 to see impact on accuracy vs robustness vs proximity
  3. **Random seed robustness**: Train 10 models with different seeds, generate CEs, and compute cross-model validity rates to confirm robustness generalizes beyond δ-robustness

## Open Questions the Paper Calls Out

- **Open Question 1**: How does VeriTraCER's performance change with different values of the hyperparameter δ (the bound on parameter perturbations)?
  - **Basis in paper**: [explicit] The paper mentions using a single δ for the lp bound across all layers and dynamically determining distinct δi values for each layer, but does not extensively explore the sensitivity of the results to different δ values.
  - **Why unresolved**: The paper focuses on demonstrating the effectiveness of VeriTraCER with a chosen δ value, but does not provide a comprehensive sensitivity analysis.
  - **What evidence would resolve it**: A thorough experimental evaluation varying δ across a wide range and analyzing the impact on δ-robustness, cross-model validity, and CE quality would resolve this question.

- **Open Question 2**: How does VeriTraCER perform on datasets with a larger number of features or more complex model architectures?
  - **Basis in paper**: [inferred] The paper evaluates VeriTraCER on five datasets with a moderate number of features and relatively simple model architectures. The scalability of VeriTraCER to larger, more complex datasets and models is not explored.
  - **Why unresolved**: The current experimental setup does not provide insights into the scalability of VeriTraCER to more challenging scenarios.
  - **What evidence would resolve it**: Evaluating VeriTraCER on datasets with a larger number of features and more complex model architectures, such as deep neural networks with many layers, would provide evidence of its scalability.

- **Open Question 3**: How does the choice of the distance metric (l1, l2, etc.) impact the quality and robustness of the generated counterfactual explanations?
  - **Basis in paper**: [explicit] The paper mentions using an l1-norm distance metric for simplicity, but acknowledges that other distance metrics could be used and that the choice of distance metric may impact the quality and robustness of the generated CEs.
  - **Why unresolved**: The paper does not explore the impact of different distance metrics on the performance of VeriTraCER.
  - **What evidence would resolve it**: Conducting experiments with different distance metrics and analyzing their impact on δ-robustness, cross-model validity, and CE quality would provide insights into the role of distance metrics in VeriTraCER's performance.

## Limitations
- **Computational overhead**: Simul-CROWN is tighter but more expensive than IBP or CROWN-IBP, with running time dominated by this component
- **Hyperparameter sensitivity**: Performance depends on careful tuning of λ weights and δ values, which are not fully specified in the paper
- **Tradeoff in CE quality**: VeriTraCER produces CEs with larger distance from original samples compared to non-robust methods

## Confidence
- **High confidence**: The core claim that joint training with Simul-CROWN yields deterministic robustness guarantees under small model perturbations is well-supported by the experimental results showing >90% δ-robustness across datasets.
- **Medium confidence**: The claim that Simul-CROWN provides tighter bounds than IBP or CROWN-IBP is supported by the mechanism description and comparison to related work, but lacks direct empirical comparison within the paper.
- **Medium confidence**: The assertion that the tradeoff is larger CE distance from the original sample is supported by the results table showing increased distance compared to non-robust methods, but the practical implications of this tradeoff are not deeply explored.

## Next Checks
1. **Unit test Simul-CROWN bounds**: Implement a synthetic example with a known multiplicity set and verify that Simul-CROWN returns tighter bounds than CROWN-IBP on the robust loss.
2. **Hyperparameter sensitivity analysis**: Train VeriTraCER with varying λ1-4 and δ to quantify the impact on accuracy, robustness, and CE proximity, identifying stable regions in the hyperparameter space.
3. **Cross-model validity under extreme shifts**: Evaluate VeriTraCER's cross-model validity on models trained with significantly different data distributions (e.g., different temporal splits or class imbalances) to test the limits of the robustness guarantees.