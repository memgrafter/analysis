---
ver: rpa2
title: 'IITK at SemEval-2024 Task 2: Exploring the Capabilities of LLMs for Safe Biomedical
  Natural Language Inference for Clinical Trials'
arxiv_id: '2404.04510'
source_url: https://arxiv.org/abs/2404.04510
tags:
- reasoning
- gpt-3
- language
- gemini
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the performance of large language models
  (LLMs) for safe biomedical natural language inference (NLI) in the context of clinical
  trial reports (CTRs). The study compares various models, including GPT-3.5, Gemini
  Pro, and pre-trained language models, under zero-shot settings using a Retrieval-Augmented
  Generation (RAG) framework with reasoning chains.
---

# IITK at SemEval-2024 Task 2: Exploring the Capabilities of LLMs for Safe Biomedical Natural Language Inference for Clinical Trials

## Quick Facts
- arXiv ID: 2404.04510
- Source URL: https://arxiv.org/abs/2404.04510
- Reference count: 10
- Primary result: Gemini Pro achieved F1 score of 0.69, consistency of 0.71, and faithfulness of 0.90 on biomedical NLI task

## Executive Summary
This paper investigates the performance of large language models (LLMs) for safe biomedical natural language inference (NLI) in clinical trial reports (CTRs). The study compares various models including GPT-3.5, Gemini Pro, and pre-trained language models under zero-shot settings using a Retrieval-Augmented Generation (RAG) framework with reasoning chains. The results show that Gemini Pro outperforms other models on the test dataset. The findings highlight the importance of prompt engineering and reasoning frameworks for improving LLM performance in biomedical NLI tasks.

## Method Summary
The study uses a dataset of 999 Clinical Trial Reports (CTRs) and 7,400 annotated statements, divided into train, development, and test sets. Various models including GPT-3.5, Gemini Pro, and pre-trained language models are evaluated under zero-shot settings using a Retrieval-Augmented Generation (RAG) framework with reasoning chains (Tree of Thoughts and Chain-of-Thought). The system uses structured instruction templates and multi-turn conversation techniques to generate explanations and labels for input statements. Evaluation metrics include F1 score, consistency, and faithfulness.

## Key Results
- Gemini Pro achieved the best performance with F1 score of 0.69, consistency of 0.71, and faithfulness of 0.90
- Tree of Thoughts reasoning framework improved performance on complex reasoning tasks
- Instruction engineering and prompt design significantly impacted model performance
- Zero-shot approach preserved model generality but may underperform compared to fine-tuned alternatives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-Augmented Generation (RAG) improves reasoning in biomedical NLI by grounding model responses in evidence from clinical trial reports.
- Mechanism: The RAG framework provides relevant context from CTR sections (e.g., Eligibility, Intervention, Results, Adverse Events) that the model uses to evaluate statements, reducing hallucinations and improving faithfulness.
- Core assumption: Providing relevant evidence from source documents enables more accurate and faithful reasoning compared to relying on model's internal knowledge alone.
- Evidence anchors:
  - [abstract] "A comparative analysis is conducted on pre-trained language models (PLMs), GPT-3.5, and Gemini Pro under zero-shot settings using Retrieval-Augmented Generation (RAG) framework, integrating various reasoning chains."
  - [section] "The proposed system utilizes structured instruction templates and multi-turn conversation techniques to generate explanations and labels for the statements provided as input, as shown in Figure 2."
  - [corpus] Weak evidence - only 5/25 papers in corpus mention RAG or retrieval-augmented generation.

### Mechanism 2
- Claim: Tree of Thoughts (ToT) reasoning framework improves performance on complex biomedical NLI tasks by enabling multi-step reasoning and error correction.
- Mechanism: ToT allows the model to explore multiple reasoning paths, backtrack when errors are detected, and converge on correct conclusions through iterative refinement.
- Core assumption: Complex reasoning tasks benefit from explicit multi-step reasoning frameworks that allow for exploration and correction of intermediate steps.
- Evidence anchors:
  - [abstract] "We examine the reasoning capabilities of LLMs and their adeptness at logical problem-solving. A comparative analysis is conducted on pre-trained language models (PLMs), GPT-3.5, and Gemini Pro under zero-shot settings using Retrieval-Augmented Generation (RAG) framework, integrating various reasoning chains."
  - [section] "Tree-of-Thought (ToT) framework (Yao et al., 2023; Long, 2023) relies on trial and error method to solve complex reasoning tasks. It facilitates multi-round conversations and backtracking."
  - [corpus] No direct evidence in corpus about ToT reasoning framework usage.

### Mechanism 3
- Claim: Instruction engineering and prompt design significantly impact LLM performance on biomedical NLI tasks by providing clear task definitions and constraints.
- Mechanism: Carefully crafted instruction templates guide the model's reasoning process, specify output format, and enforce constraints like avoiding hallucinations and aligning with provided context.
- Core assumption: LLMs are highly sensitive to prompt structure and content, and well-designed prompts can substantially improve performance on specialized tasks.
- Evidence anchors:
  - [abstract] "The evaluation yields an F1 score of 0.69, consistency of 0.71, and a faithfulness score of 0.90 on the test dataset."
  - [section] "The system was experimented with several prompts to improve its performance. The explanations generated by the model were examined manually to identify instances where the solution deviated from the correct path."
  - [section] "Several experiments were conducted to assess the model's performance on extracting the labels 'Entailment' or 'Contradiction' in the second question of the multi-turn conversation."

## Foundational Learning

- Concept: Biomedical domain knowledge and terminology
  - Why needed here: The NLI task involves understanding and reasoning about clinical trial reports with specialized medical terminology and concepts
  - Quick check question: Can you explain the difference between eligibility criteria and adverse events in a clinical trial report?

- Concept: Natural Language Inference (NLI) principles
  - Why needed here: The task requires determining whether statements entail or contradict information in clinical trial reports
  - Quick check question: What are the key differences between entailment and contradiction in NLI tasks?

- Concept: Large Language Model (LLM) behavior and limitations
  - Why needed here: Understanding LLM strengths and weaknesses is crucial for effective prompt engineering and interpreting results
  - Quick check question: What are the main limitations of LLMs when performing reasoning tasks, particularly in specialized domains?

## Architecture Onboarding

- Component map: Data preprocessing -> RAG system -> Reasoning framework -> Instruction templates -> Model inference -> Evaluation metrics

- Critical path:
  1. Parse CTR and classify statement type
  2. Retrieve relevant evidence using RAG
  3. Generate explanation using ToT reasoning framework
  4. Extract final label through multi-turn conversation
  5. Evaluate results using consistency and faithfulness metrics

- Design tradeoffs:
  - Zero-shot vs fine-tuned approaches: Zero-shot preserves model generality but may underperform on specialized tasks
  - ToT vs simpler reasoning frameworks: ToT provides better performance but increases computational cost and complexity
  - Prompt complexity vs interpretability: More complex prompts may improve performance but make debugging harder

- Failure signatures:
  - Low faithfulness scores: Model generates explanations that don't align with retrieved evidence
  - Poor consistency: Model gives different answers to semantically equivalent inputs
  - Numerical reasoning failures: Model struggles with quantitative comparisons in clinical data
  - Hallucination: Model generates information not present in the CTR or statement

- First 3 experiments:
  1. Compare zero-shot performance of different LLMs (GPT-3.5, Gemini Pro, Claude) on a small validation set
  2. Test the impact of different reasoning frameworks (CoT vs ToT) on model performance
  3. Evaluate the effect of various instruction templates on explanation quality and faithfulness scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of LLMs change if they were fine-tuned on a larger, more diverse dataset of clinical trial reports from multiple medical domains beyond breast cancer?
- Basis in paper: [explicit] The paper mentions that the current study focuses on breast cancer clinical trial reports and uses a dataset of 999 CTRs. It also discusses the performance of various PLMs pre-trained on biomedical data.
- Why unresolved: The paper only evaluates models on breast cancer CTRs and doesn't explore how performance might generalize to other medical domains or with more diverse training data.
- What evidence would resolve it: Experiments comparing LLM performance on clinical trial reports from multiple medical domains (e.g., oncology, cardiology, neurology) using both the current dataset and an expanded, multi-domain dataset.

### Open Question 2
- Question: What is the impact of incorporating domain-specific medical knowledge bases or ontologies into the RAG framework on the faithfulness and consistency of LLM-generated explanations?
- Basis in paper: [explicit] The paper mentions the use of a RAG framework but doesn't explore the integration of domain-specific knowledge bases. It also highlights the importance of faithfulness (0.90) and consistency (0.71) scores.
- Why unresolved: The current study uses a basic RAG framework without leveraging specialized medical knowledge bases, which could potentially improve the accuracy and reliability of the generated explanations.
- What evidence would resolve it: Comparative experiments showing the performance of LLMs with and without the integration of domain-specific medical knowledge bases in the RAG framework, measured by faithfulness and consistency scores.

### Open Question 3
- Question: How do different instruction template designs affect the performance of LLMs on various types of reasoning tasks (e.g., numerical, semantic, syntactic) within clinical trial reports?
- Basis in paper: [explicit] The paper discusses the importance of prompt engineering and presents a final instruction template, but doesn't analyze how different template designs might impact performance on specific reasoning tasks.
- Why unresolved: The study presents a single optimized instruction template but doesn't explore how variations in template design might affect LLM performance on different types of reasoning challenges present in clinical trial reports.
- What evidence would resolve it: Systematic experiments comparing the performance of LLMs using multiple instruction template designs on a diverse set of reasoning tasks extracted from clinical trial reports, with detailed analysis of template effectiveness for each reasoning type.

## Limitations
- The paper lacks detailed specifications of prompt engineering techniques and hyperparameters, making exact reproduction challenging
- The evaluation corpus is limited to breast cancer clinical trial reports, raising questions about generalizability to other medical domains
- The zero-shot approach, while preserving model generality, may underperform compared to fine-tuned alternatives on specialized tasks

## Confidence
- Methodology soundness: Medium
- Result plausibility: Medium
- Reproducibility: Low
- Generalizability: Low

## Next Checks
1. Replicate the study using the exact same dataset and implementation details to verify reported metrics
2. Test the approach on clinical trial reports from other medical domains to assess generalizability
3. Compare zero-shot RAG+ToT performance against fine-tuned baseline models to quantify the tradeoff between generality and task-specific performance