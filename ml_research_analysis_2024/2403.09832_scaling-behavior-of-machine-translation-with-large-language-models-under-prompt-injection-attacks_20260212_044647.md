---
ver: rpa2
title: Scaling Behavior of Machine Translation with Large Language Models under Prompt
  Injection Attacks
arxiv_id: '2403.09832'
source_url: https://arxiv.org/abs/2403.09832
tags:
- scaling
- translation
- language
- inverse
- experiments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the scaling behavior of Large Language
  Models (LLMs) under prompt injection attacks (PIAs) in a machine translation task.
  The authors create a benchmark dataset by translating English questions to German,
  French, Romanian, and Russian using mBART-50, then generate adversarial examples
  by prepending a prefix that instructs the model not to translate but answer the
  question.
---

# Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks

## Quick Facts
- arXiv ID: 2403.09832
- Source URL: https://arxiv.org/abs/2403.09832
- Authors: Zhifan Sun; Antonio Valerio Miceli-Barone
- Reference count: 7
- Primary result: Larger language models under certain conditions become more susceptible to prompt injection attacks, exhibiting inverse scaling behavior in machine translation tasks.

## Executive Summary
This paper investigates how Large Language Models (LLMs) scale under prompt injection attacks (PIAs) in machine translation tasks. The authors create a benchmark dataset by translating English questions to German, French, Romanian, and Russian using mBART-50, then generate adversarial examples by prepending prefixes that instruct models not to translate but answer the questions. They evaluate multiple LLM families including GPT-3, InstructGPT, T5, FLAN-T5, Llama2, and Llama2-chat on both clean and adversarial examples. The key finding is that while most models show positive scaling on clean examples, larger models under certain conditions become more susceptible to successful attacks, demonstrating inverse scaling. Specifically, Llama2 models show consistent inverse scaling under zero-shot scenarios, while OpenAI models exhibit inverse scaling on English-to-X translation directions.

## Method Summary
The authors create a benchmark dataset by translating questions from the TruthfulQA dataset to German, French, Romanian, and Russian using mBART-50. They then generate adversarial examples by prepending a prefix that instructs the model not to translate but to answer the question. Multiple LLM families (GPT-3, InstructGPT, T5, FLAN-T5, Llama2, Llama2-chat) are evaluated on both clean and adversarial examples using a "question mark accuracy" metric and BLEU scores. The evaluation is conducted in both zero-shot and few-shot prompting modes, with experiments run on GPUs (A100-80G, RTX 3090, RTX 2080 Ti) for T5 and FLAN-T5, while other models use API calls.

## Key Results
- Llama2 models with zero-shot examples show consistent inverse scaling across all translation directions
- OpenAI models (GPT-3.5) reverse the inverse scaling trend, particularly on English-to-X translation directions
- Models are more likely to answer questions rather than translate when prompts are in English, indicating inverse scaling with respect to training data size
- Instruction tuning and code training can mitigate susceptibility to prompt injection attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger models exhibit inverse scaling under certain prompt injection attack conditions.
- Mechanism: As model size increases, the model's ability to attend to and follow detailed instructions improves, but this same capability makes them more susceptible to contradictory adversarial prompts.
- Core assumption: The inverse scaling phenomenon observed is due to the model's increased capacity to understand and execute instructions, including adversarial ones.
- Evidence anchors:
  - [abstract] "larger models under certain conditions may become more susceptible to successful attacks, an instance of the Inverse Scaling phenomenon"
  - [section] "Llama2 models with zero-shot examples show consistent inverse scaling across all translation directions"
  - [corpus] Weak evidence: related works focus on prompt injection attacks but do not specifically address inverse scaling with model size in multilingual settings.
- Break condition: If the model is provided with few-shot examples, the inverse scaling trend can be reversed, as shown by the improvement in performance with in-context learning.

### Mechanism 2
- Claim: Models are more likely to answer questions rather than translate them when the prompt is in English, indicating inverse scaling with respect to training data size.
- Mechanism: Due to the English-dominated pre-training corpora, models have a stronger tendency to interpret English prompts as instructions to answer rather than translate.
- Core assumption: The model's behavior is influenced by the language of the input, with English prompts being more likely to trigger the answer-instruction interpretation.
- Evidence anchors:
  - [section] "models are more likely to answer the source questions rather than translate them when they are written in English, even on non-adversarial examples, which is a clean case of Inverse Scaling w.r.t. training data size"
  - [section] "we find that models are more likely to answer the source questions rather than translate them when they are written in English, even on non-adversarial examples"
  - [corpus] No direct evidence in related works; this is a novel observation specific to this paper.
- Break condition: If the model is trained on a more balanced multilingual corpus or if the prompt is in a non-English language, the inverse scaling effect may be reduced or eliminated.

### Mechanism 3
- Claim: Instruction tuning and training on code can mitigate the susceptibility to prompt injection attacks.
- Mechanism: Models that have been instruction-tuned or trained on code have a better ability to correctly understand and follow instructions, making them less likely to be subverted by adversarial prompts.
- Core assumption: Instruction tuning and code training improve the model's instruction-following capability, which can be leveraged to resist prompt injection attacks.
- Evidence anchors:
  - [section] "instruction tuning might interfere with in-context learning, as evidenced by the Llama2-chat results, but not the GPT-3.5 results"
  - [section] "GPT-3.5 models reverse the inverse scaling trend"
  - [corpus] No direct evidence in related works; this is an inference based on the observed performance of instruction-tuned models.
- Break condition: If the instruction tuning is not properly curated or if the model is not trained on a diverse enough set of instructions, the mitigation effect may not be observed.

## Foundational Learning

- Concept: Prompt Injection Attacks (PIAs)
  - Why needed here: Understanding PIAs is crucial to grasp the vulnerability of LLMs and the significance of the inverse scaling phenomenon observed in this study.
  - Quick check question: What is a Prompt Injection Attack and how does it affect the behavior of Large Language Models?

- Concept: Inverse Scaling
  - Why needed here: Inverse scaling is the central phenomenon being investigated in this paper, where larger models become more susceptible to certain types of attacks or tasks.
  - Quick check question: What is inverse scaling and how does it manifest in the context of LLMs under prompt injection attacks?

- Concept: BLEU Score and Question Mark Accuracy
  - Why needed here: These are the evaluation metrics used to assess the performance of LLMs in machine translation tasks, both under normal conditions and under prompt injection attacks.
  - Quick check question: How do BLEU scores and question mark accuracy differ in their sensitivity to prompt injection attacks in machine translation tasks?

## Architecture Onboarding

- Component map:
  Data Collection -> Model Evaluation -> Metrics Calculation -> Analysis

- Critical path:
  1. Generate clean and adversarial examples
  2. Evaluate models on both sets of examples
  3. Analyze scaling behavior, focusing on inverse scaling phenomena

- Design tradeoffs:
  - Limited number of model families and language pairs due to computational resources
  - Coarse-grained evaluation strategy using question mark accuracy, which may not capture all nuances of model performance

- Failure signatures:
  - Inverse scaling: Larger models performing worse on adversarial examples
  - U-shaped scaling: Performance improving and then deteriorating with model size
  - English language bias: Models more likely to answer questions when prompts are in English

- First 3 experiments:
  1. Evaluate T5 and FLAN-T5 models on English-to-De, En-to-Fr, and En-to-Ro translation directions in zero-shot mode
  2. Test OpenAI models (GPT-3, InstructGPT) on En↔De, En↔Fr, and En↔Ru translation directions with one-shot prompting
  3. Assess Llama2 and Llama2-chat models on En↔De, En↔Fr, En↔Ro, and En↔Ru translation directions in both zero-shot and one-shot modes

## Open Questions the Paper Calls Out
No open questions were explicitly called out in the paper.

## Limitations
- Reliance on a coarse-grained "question mark accuracy" metric that may not fully capture nuanced effects of prompt injection attacks on translation quality
- Limited to five model families and four language pairs, constraining generalizability of findings
- Computational constraints prevented more comprehensive analysis across diverse model architectures and languages

## Confidence
- **High Confidence**: The core finding that larger models can exhibit inverse scaling under prompt injection attacks is well-supported by experimental results across multiple model families
- **Medium Confidence**: The claim that instruction tuning and code training can mitigate susceptibility to PIAs is supported by GPT-3.5 results but contradicted by Llama2-chat observations
- **Low Confidence**: The assertion that models are more likely to answer questions when prompts are in English represents a novel observation but lacks comprehensive validation

## Next Checks
1. Replicate experiments using more nuanced evaluation metrics that capture partial credit for partially correct translations versus complete answer generation
2. Systematically test the same prompt injection attacks across non-English source languages to determine whether the English language bias is reproducible
3. Analyze the training data composition of the models studied to quantify the relationship between English corpus size and inverse scaling behavior