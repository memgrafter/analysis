---
ver: rpa2
title: 'Pixel-wise RL on Diffusion Models: Reinforcement Learning from Rich Feedback'
arxiv_id: '2404.04356'
source_url: https://arxiv.org/abs/2404.04356
tags:
- feedback
- reward
- image
- diffusion
- pxpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently aligning latent
  diffusion models with human preferences by introducing Pixel-wise Policy Optimisation
  (PXPO). The method extends denoising diffusion policy optimisation (DDPO) by allowing
  pixel-wise feedback instead of a single global reward, providing more nuanced and
  granular information to the model.
---

# Pixel-wise RL on Diffusion Models: Reinforcement Learning from Rich Feedback

## Quick Facts
- arXiv ID: 2404.04356
- Source URL: https://arxiv.org/abs/2404.04356
- Reference count: 0
- Primary result: Pixel-wise Policy Optimisation (PXPO) extends DDPO to provide granular pixel-level feedback without requiring intermediate reward models

## Executive Summary
This paper introduces Pixel-wise Policy Optimisation (PXPO), a method that extends denoising diffusion policy optimisation (DDPO) to handle pixel-level feedback rather than single global rewards. PXPO enables more nuanced alignment of latent diffusion models with human preferences by assigning distinct feedback to each pixel and calculating gradients that eliminate cross-talk between pixels. The method was evaluated on three tasks: reducing blue channel intensity, removing hair from images using AI feedback, and iteratively improving a single image based on human feedback, demonstrating significant improvements in reward scores and image quality.

## Method Summary
PXPO builds upon DDPO by modifying how gradients are calculated during training to handle pixel-wise feedback. The key innovation lies in computing gradients that avoid cross-talk between pixels while maintaining the benefits of reinforcement learning from human feedback. The method processes each pixel independently, allowing the model to receive and act upon rich, granular feedback rather than a single aggregated reward signal. This approach eliminates the need for an intermediate reward model that typically converts human feedback into scalar rewards, making the alignment process more direct and efficient.

## Key Results
- Blue channel reduction task: improved mean reward from -0.39±0.08 to -0.35±0.08
- Hair removal task using AI feedback: improved mean reward from -0.06±0.04 to -0.02±0.02
- Single-image improvement with human feedback: demonstrated dramatic quality enhancement through iterative refinement

## Why This Works (Mechanism)
PXPO works by decomposing the traditional global reward signal into pixel-level feedback that can be processed independently. This granular approach allows the model to understand exactly which pixels need modification and in what direction, rather than receiving a single aggregated signal that may be ambiguous. The elimination of cross-talk between pixels ensures that gradients are computed accurately for each pixel's feedback, preventing interference that could degrade image quality or slow convergence.

## Foundational Learning
- **Diffusion Models**: Why needed - form the base architecture for image generation; Quick check - understand how denoising diffusion probabilistic models generate images from noise
- **Reinforcement Learning from Human Feedback (RLHF)**: Why needed - provides the framework for aligning models with human preferences; Quick check - grasp the concept of using human feedback as rewards in training
- **Pixel-wise Gradients**: Why needed - enables fine-grained control over image generation; Quick check - understand how gradients at pixel level differ from global gradients
- **Cross-talk Elimination**: Why needed - prevents interference between pixel updates; Quick check - recognize how independent pixel processing improves training stability
- **DDIM Sampling**: Why needed - provides the inference mechanism for diffusion models; Quick check - understand the deterministic sampling process in diffusion models

## Architecture Onboarding

Component Map:
DDIM Generator -> Pixel-wise Reward Module -> Gradient Calculator -> Policy Optimizer

Critical Path:
1. Input image passes through DDIM generator
2. Pixel-wise rewards are computed based on feedback
3. Gradients are calculated for each pixel independently
4. Policy optimizer updates model parameters using these gradients

Design Tradeoffs:
- **Pros**: Eliminates need for intermediate reward model, provides granular control, potentially more efficient alignment
- **Cons**: Increased computational complexity, potential overfitting to pixel-level noise, scalability concerns for high-resolution images

Failure Signatures:
- Images becoming overly smooth or losing detail due to excessive pixel-level corrections
- Training instability if cross-talk elimination is not properly implemented
- Slow convergence if pixel-wise feedback is too noisy or inconsistent

First Experiments:
1. Test PXPO on a simple synthetic task (like blue channel reduction) to verify basic functionality
2. Compare convergence speed against baseline DDPO on the same task
3. Evaluate the method's sensitivity to different levels of pixel-wise noise in the feedback

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on synthetic and limited real-world feedback tasks
- Experiments focus on specific image modification tasks rather than broad image generation quality
- Scalability to high-resolution images and complex feedback scenarios remains untested

## Confidence

**High confidence**: The core methodological contribution (pixel-wise gradient calculation without cross-talk) is technically sound and the mathematical formulation is rigorous

**Medium confidence**: The experimental results showing improvement over baseline DDPO are convincing for the specific tasks tested, but generalization to other domains is uncertain

**Medium confidence**: The claim about eliminating the need for an intermediate reward model is valid for the tested scenarios, though practical implementation may still benefit from such models for complex feedback

## Next Checks

1. Test PXPO on diverse image generation tasks beyond simple attribute modification, including creative image synthesis and text-to-image alignment

2. Evaluate the method's performance on high-resolution images (e.g., 1024x1024 or higher) to assess computational scalability

3. Conduct user studies comparing human preference satisfaction between PXPO-tuned models and models tuned with traditional reward modeling approaches