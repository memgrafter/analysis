---
ver: rpa2
title: Whose Morality Do They Speak? Unraveling Cultural Bias in Multilingual Language
  Models
arxiv_id: '2412.18863'
source_url: https://arxiv.org/abs/2412.18863
tags:
- moral
- llms
- languages
- language
- cultural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated the cultural and linguistic biases in multilingual
  LLMs using the updated Moral Foundations Questionnaire (MFQ-2) across eight languages.
  The findings revealed significant variability in moral foundation scores, indicating
  that models adapt to linguistic contexts rather than imposing English-centric norms.
---

# Whose Morality Do They Speak? Unraveling Cultural Bias in Multilingual Language Models

## Quick Facts
- arXiv ID: 2412.18863
- Source URL: https://arxiv.org/abs/2412.18863
- Authors: Meltem Aksoy
- Reference count: 40
- Key outcome: Multilingual LLMs show significant cultural and linguistic variability in moral judgments, challenging assumptions of universal moral consistency.

## Executive Summary
This study investigates cultural and linguistic biases in multilingual large language models (LLMs) by evaluating their moral reasoning across eight languages using the updated Moral Foundations Questionnaire (MFQ-2). The research challenges the assumption that LLMs impose English-centric moral norms universally, revealing instead that models adapt to linguistic contexts. Statistical analysis demonstrates that both language and model significantly influence moral foundation scores, with notable differences between WEIRD (Western, Educated, Industrialized, Rich, Democratic) and non-WEIRD language groups. The findings suggest that LLMs carry cultural biases that reflect training data composition and linguistic representation, highlighting the need for more culturally inclusive training approaches.

## Method Summary
The study evaluates four multilingual LLMs (GPT-3.5-Turbo, GPT-4o-mini, Llama 3.1, MistralNeMo) using the Moral Foundations Questionnaire-2 across eight languages: Arabic, Farsi, English, Spanish, Japanese, Chinese, French, and Russian. Each model is prompted 100 times per language with MFQ-2 items, and responses are collected on a Likert scale. Average scores are calculated for each of the six moral foundations (care, equality, proportionality, loyalty, authority, purity), and statistical analyses including ANOVA, t-tests, and Tukey's HSD are performed to assess significance and compare results with human responses.

## Key Results
- Multilingual LLMs exhibit significant cultural and linguistic variability in moral foundation scores, challenging assumptions of universal moral consistency
- Both language and model have significant effects on moral foundation scores (p < 0.001), with notable differences between WEIRD and non-WEIRD language groups
- Larger models like GPT-3.5 and GPT-4o-mini show better alignment with human moral judgments compared to smaller open-source models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual LLMs reflect linguistic and cultural variability rather than imposing English moral norms universally.
- Mechanism: Language and model-specific training data influence moral foundation scores, with significant interaction effects observed in ANOVA results.
- Core assumption: Training data composition and linguistic context shape moral reasoning in LLMs.
- Evidence anchors:
  - [abstract]: "The results reveal significant cultural and linguistic variability, challenging the assumption of universal moral consistency in LLMs."
  - [section]: "The ANOV A results provide statistical evidence that both language and model have a significant effect on moral foundation scores (p < 0.001), supporting previous observations."
  - [corpus]: Weak corpus support; related papers focus on cultural bias but do not directly address interaction effects between language and model.
- Break condition: If future studies find consistent moral scores across languages regardless of model, this mechanism would be invalidated.

### Mechanism 2
- Claim: LLMs exhibit cultural biases, with WEIRD languages showing higher scores in care, loyalty, and purity compared to non-WEIRD languages.
- Mechanism: Training data imbalances and cultural representation affect moral foundation scores differently across WEIRD and non-WEIRD language groups.
- Core assumption: Training data representation correlates with cultural bias in moral reasoning.
- Evidence anchors:
  - [abstract]: "The results reveal significant cultural and linguistic variability, challenging the assumption of universal moral consistency in LLMs."
  - [section]: "These findings indicate significant differences in moral judgments between WEIRD and non-WEIRD language groups, suggesting that LLMs may carry cultural biases."
  - [corpus]: Weak corpus support; related papers discuss cultural bias but lack specific WEIRD vs. non-WEIRD comparisons.
- Break condition: If future studies demonstrate equal moral foundation scores across WEIRD and non-WEIRD languages, this mechanism would be invalidated.

### Mechanism 3
- Claim: Larger models with diverse training datasets, such as GPT-3.5 and GPT-4o-mini, show better alignment with human moral judgments compared to smaller open-source models.
- Mechanism: Training data diversity and model size influence the alignment between LLM-generated moral scores and human responses.
- Core assumption: Training data diversity enhances cultural and moral alignment in LLMs.
- Evidence anchors:
  - [abstract]: "Alignment with human responses varied, with larger models like GPT-3.5 and GPT-4o-mini showing better consistency."
  - [section]: "The results showed that alignment was generally stronger for models with larger and more diverse training datasets, such as GPT-3.5 and GPT-4o-mini."
  - [corpus]: Weak corpus support; related papers discuss model alignment but lack direct comparisons between model sizes.
- Break condition: If future studies find smaller models with less diverse data showing better alignment, this mechanism would be invalidated.

## Foundational Learning

- Concept: Moral Foundations Theory (MFT)
  - Why needed here: Provides the theoretical framework for analyzing moral reasoning in LLMs.
  - Quick check question: What are the six core moral foundations identified by MFT?

- Concept: Cross-cultural psychology
  - Why needed here: Helps understand how cultural contexts influence moral judgments and biases in LLMs.
  - Quick check question: How do cultural differences impact moral reasoning according to cross-cultural psychology?

- Concept: Natural Language Processing (NLP) and multilingual models
  - Why needed here: Essential for understanding how LLMs process and generate text in multiple languages.
  - Quick check question: What are the key challenges in developing multilingual LLMs?

## Architecture Onboarding

- Component map: LLMs (GPT-3.5-Turbo, GPT-4o-mini, Llama 3.1, MistralNeMo) → Moral Foundations Questionnaire-2 (MFQ-2) → Prompt generation → Response collection → Statistical analysis (ANOVA, t-tests) → Comparison with human responses
- Critical path: Prompt generation → Response collection → Statistical analysis → Interpretation of results
- Design tradeoffs: Model size and diversity vs. computational resources; WEIRD vs. non-WEIRD language representation; questionnaire-based vs. real-world moral dilemmas
- Failure signatures: Inconsistent moral scores across languages; lack of alignment with human responses; cultural biases favoring WEIRD languages
- First 3 experiments:
  1. Compare moral foundation scores across languages within a single model to identify linguistic variability.
  2. Analyze the interaction effect between language and model on moral foundation scores using ANOVA.
  3. Evaluate the alignment between LLM-generated moral scores and human responses for each language and model.

## Open Questions the Paper Calls Out
None

## Limitations
- Language selection bias: The study focuses on eight specific languages, which may not fully represent global linguistic diversity
- Questionnaire constraints: The MFQ-2 format may not capture the full complexity of moral reasoning across cultures
- Model selection: The study uses four specific LLMs, which may not be representative of the broader landscape of multilingual models

## Confidence
- High Confidence: Statistical analysis methodology and ANOVA results demonstrating significant effects of language and model on moral foundation scores
- Medium Confidence: Cross-cultural comparisons between WEIRD and non-WEIRD language groups, due to potential sampling biases
- Medium Confidence: Alignment comparisons between larger and smaller models, as the study only examines a limited set of models

## Next Checks
1. Expand language coverage: Validate findings across a broader set of languages, particularly those from underrepresented linguistic families, to test the generalizability of cultural bias patterns.
2. Real-world scenario testing: Supplement questionnaire-based assessments with evaluations using real-world moral dilemmas to verify if observed patterns hold in more naturalistic contexts.
3. Cross-model validation: Test the same prompts across additional multilingual models not included in this study to assess whether the observed alignment patterns between model size and moral reasoning consistency are consistent across the broader LLM landscape.