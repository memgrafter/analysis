---
ver: rpa2
title: 'Distinctive Image Captioning: Leveraging Ground Truth Captions in CLIP Guided
  Reinforcement Learning'
arxiv_id: '2402.13936'
source_url: https://arxiv.org/abs/2402.13936
tags:
- image
- captions
- reward
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of training image captioning
  models to produce distinctive captions rather than generic ones. The authors propose
  a reinforcement learning approach that leverages ground truth captions in three
  ways: 1) training a discriminator to prevent reward hacking and ensure fluency,
  2) using ground truth captions as additional trajectories weighted by their similarity
  to the image, and 3) using ground truth captions as baselines in a bidirectional
  contrastive reward.'
---

# Distinctive Image Captioning: Leveraging Ground Truth Captions in CLIP Guided Reinforcement Learning

## Quick Facts
- arXiv ID: 2402.13936
- Source URL: https://arxiv.org/abs/2402.13936
- Authors: Antoine Chaffin; Ewa Kijak; Vincent Claveau
- Reference count: 40
- Key outcome: Achieves significant improvements in both distinctiveness (retrieval metrics) and writing quality (standard captioning metrics) on MS-COCO using ground truth captions in CLIP-guided reinforcement learning

## Executive Summary
This paper addresses the challenge of training image captioning models to produce distinctive captions rather than generic ones. The authors propose a reinforcement learning approach that leverages ground truth captions in three innovative ways: training a discriminator to prevent reward hacking, using ground truth captions as additional trajectories weighted by CLIP similarity, and employing them as baselines in a bidirectional contrastive reward. Evaluated on MS-COCO, the method significantly outperforms previous approaches on both distinctiveness and writing quality metrics, effectively balancing these two competing objectives.

## Method Summary
The proposed method uses reinforcement learning with CLIP similarity as the reward signal, enhanced by three key innovations that leverage ground truth captions. First, a simple MLP discriminator distinguishes ground truth from generated captions in CLIP embedding space, serving as regularization to prevent reward hacking. Second, ground truth captions are treated as additional trajectories in the RL framework, weighted by their CLIP similarity to the image, creating a teacher forcing loss that focuses learning on the most descriptive human-written captions. Third, ground truth captions serve as strong baselines in a bidirectional contrastive reward that considers both image-to-text and text-to-image retrieval directions, reducing gradient variance and ensuring captions are distinctive to specific input images.

## Key Results
- Achieves state-of-the-art performance on MS-COCO for distinctive captioning, with significant improvements in retrieval metrics (Recall@1,5,10) for both text-to-image and image-to-text directions
- Maintains or improves standard captioning quality metrics (BLEU-4, ROUGE-L, CIDEr, METEOR, SPICE) compared to baseline models
- Demonstrates effective balance between distinctiveness and fluency, addressing the key challenge in distinctive captioning where improving one often degrades the other
- Shows superior performance compared to previous distinctive captioning approaches including UNITER and CoCa, with particular strength in generating diverse captions that are still well-formed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using ground truth captions as additional trajectories in RL enables learning of more distinctive captions while maintaining fluency.
- Mechanism: Ground truth captions are treated as generated sequences in the RL framework, but their reward is weighted by their similarity to the image via CLIP score. This creates a "weighted teacher forcing" objective that focuses learning on the most descriptive human-written captions.
- Core assumption: Ground truth captions contain more descriptive and diverse vocabulary than sequences typically sampled by the language model during RL exploration.
- Evidence anchors:
  - [abstract]: "they can serve as additional trajectories in the RL strategy, resulting in a teacher forcing loss weighted by the similarity of the GT to the image."
  - [section]: "We thus propose to use these captions as additional trajectories for the RL loss... The resulting loss is thus equivalent to the teacher forcing loss weighted by the reward r(xgt)."
- Break condition: If ground truth captions are too generic or similar to each other, the weighted teacher forcing may not provide meaningful additional learning signals beyond standard teacher forcing.

### Mechanism 2
- Claim: The discriminator trained on ground truth vs. generated samples prevents reward hacking and ensures fluency while adapting to emergent generator behaviors.
- Mechanism: A simple MLP classifier distinguishes between ground truth and beam search generated captions in CLIP embedding space. Its output probability serves as a regularization term in the reward function, penalizing ill-formed sequences that might otherwise receive high CLIP scores.
- Core assumption: The discriminator can effectively learn to distinguish human-written captions from generated ones, and this discrimination correlates with caption quality and fluency.
- Evidence anchors:
  - [abstract]: "they can be used to train a simple MLP discriminator that serves as a regularization to prevent reward hacking and ensures the fluency of generated captions."
  - [section]: "To prevent learning from such ill-formed solutions... we propose a training method taking advantage of GT captions to optimize the trade-off between the distinctiveness and the writing quality of generated captions."
- Break condition: If the discriminator overfits to early-stage generator outputs or if the CLIP embedding space doesn't capture linguistic quality differences, it may fail to provide effective regularization.

### Mechanism 3
- Claim: The bidirectional contrastive reward using ground truth as baselines reduces gradient variance and ensures captions are distinctive to the specific input image.
- Mechanism: Ground truth captions are included in the pool of captions used to compute the contrastive reward, which considers both image-to-text and text-to-image retrieval directions. The reward is based on the similarity difference between the target caption and the hardest negative baseline.
- Core assumption: Including ground truth captions in the contrastive pool provides stronger baselines than using only generated samples, leading to more conservative learning that prevents early overfitting to reward model biases.
- Evidence anchors:
  - [abstract]: "they can serve as strong baselines when added to the pool of captions used to compute the proposed contrastive reward to reduce the variance of gradient estimate."
  - [section]: "GT can further be used as candidate baselines in our proposed contrastive reward that uses the strongest baseline in a batch to reduce the variance of gradient estimation."
- Break condition: If ground truth captions are not sufficiently distinctive or if the batch size is too small to provide meaningful contrastive pairs, the reward may not effectively guide learning.

## Foundational Learning

- Concept: Reinforcement Learning with policy gradient methods (REINFORCE algorithm)
  - Why needed here: The paper uses RL to optimize the non-differentiable CLIP similarity score as a reward for caption generation, which requires gradient estimation through sampling.
  - Quick check question: How does the REINFORCE algorithm estimate gradients when the reward function is not differentiable with respect to the model parameters?

- Concept: Teacher Forcing vs. Reinforcement Learning in sequence generation
  - Why needed here: The paper contrasts standard teacher forcing (which leads to generic captions) with RL-based approaches (which can produce more distinctive captions but risk reward hacking), and proposes a hybrid approach.
  - Quick check question: What is the key difference between how teacher forcing and RL-based methods handle the mismatch between training and inference distributions in sequence generation?

- Concept: Contrastive learning and its application to cross-modal retrieval
  - Why needed here: The paper uses a contrastive reward formulation inspired by CLIP's training objective to ensure captions are distinctive to their specific images in both retrieval directions.
  - Quick check question: In contrastive learning, why is it important to include negative samples in the loss function, and how does this apply to the bidirectional contrastive reward in this paper?

## Architecture Onboarding

- Component map:
  - OFA-tiny language model (generator) -> CLIP model (cross-modal embeddings) -> MLP discriminator (quality control) -> Reward computation module (similarity + discriminator score) -> Policy gradient estimator

- Critical path:
  1. Input image and ground truth caption are processed
  2. CLIP embeddings are computed for all captions and images in batch
  3. Discriminator is trained on current batch samples
  4. Rewards are computed using bidirectional contrastive similarity and discriminator output
  5. Policy gradients are estimated for both beam search and ground truth samples
  6. Language model parameters are updated using combined gradients

- Design tradeoffs:
  - Fixed CLIP vs. fine-tuned CLIP: Using fixed CLIP prevents reward hacking but may miss task-specific nuances; fine-tuning could improve alignment but risks the model and retriever cooperatively converging to non-natural language.
  - Discriminator complexity: A simple MLP is computationally efficient but may miss subtle quality distinctions; a more complex model could provide better regularization but at higher computational cost.
  - Temperature parameter in contrastive loss: Lower temperature creates harder negatives and more conservative learning but may slow convergence; higher temperature speeds learning but risks overfitting.

- Failure signatures:
  - Reward hacking: Model generates repetitive or keyword-stuffed captions that score highly with CLIP but are not meaningful
  - Mode collapse: Model produces very similar captions for different images, indicated by high self-BLEU scores
  - Divergence: Training loss becomes unstable or model produces nonsensical output, often due to overly strong negative rewards
  - Vocabulary collapse: Model uses increasingly limited vocabulary over training, failing to generate distinctive captions

- First 3 experiments:
  1. Verify discriminator can distinguish ground truth from generated captions with accuracy >90% on a validation set
  2. Test weighted teacher forcing objective alone (without RL) to confirm it improves retrieval metrics over standard teacher forcing
  3. Validate that bidirectional contrastive reward produces more balanced image-to-text and text-to-image retrieval performance compared to unidirectional reward

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed approach change when using a non-fixed cross-modal retriever (i.e., jointly training the captioning model with the retriever) instead of a pre-trained fixed CLIP model?
- Basis in paper: [inferred] The paper discusses the benefits of using a fixed pre-trained CLIP model to avoid the "drift" problem that can occur when the generator and retriever collaboratively converge. However, it also mentions that the findings "pave the way for studies that try to also improve the CLIP model jointly with the captioning model" and suggests that "starting from a strong pre-trained cross-modal retriever and strongly grounding the learning to ground truth captions might help to overcome the drifting inherent of the collaboration between the two models."
- Why unresolved: The paper focuses on using a fixed pre-trained CLIP model and does not explore the performance of jointly training the captioning model with the retriever.
- What evidence would resolve it: Experimental results comparing the performance of the proposed approach using a fixed pre-trained CLIP model versus a jointly trained retriever on metrics like distinctiveness, writing quality, and retrieval rates.

### Open Question 2
- Question: What is the impact of using different similarity metrics (e.g., cosine similarity, dot product) in the bidirectional contrastive reward on the performance of the captioning model?
- Basis in paper: [explicit] The paper uses the dot product of CLIP embeddings to compute the similarity score in the bidirectional contrastive reward. However, it does not explore the impact of using different similarity metrics.
- Why unresolved: The paper only considers one similarity metric (dot product) and does not investigate the potential benefits or drawbacks of using alternative metrics.
- What evidence would resolve it: Experimental results comparing the performance of the proposed approach using different similarity metrics (e.g., cosine similarity, dot product) on metrics like distinctiveness, writing quality, and retrieval rates.

### Open Question 3
- Question: How does the proposed approach perform on datasets other than MS-COCO, particularly those with different image characteristics or caption styles?
- Basis in paper: [explicit] The paper evaluates the proposed approach on the MS-COCO dataset. However, it does not explore its performance on other datasets.
- Why unresolved: The paper focuses on a single dataset (MS-COCO) and does not investigate the generalizability of the approach to other datasets with different image characteristics or caption styles.
- What evidence would resolve it: Experimental results evaluating the proposed approach on various datasets (e.g., Flickr30k, Conceptual Captions) and comparing its performance across datasets on metrics like distinctiveness, writing quality, and retrieval rates.

## Limitations

- The method relies heavily on ground truth captions, which may not always be distinctive enough to effectively guide learning toward more distinctive captions, creating a potential ceiling effect.
- The computational overhead of maintaining and training the discriminator, as well as the additional complexity in the training loop, may make this approach less practical for large-scale deployment compared to simpler methods.
- The approach uses a fixed CLIP model rather than fine-tuning it, which may limit the alignment between the caption generation model and the similarity metric, though this design choice was explicitly made to prevent reward hacking.

## Confidence

- **High Confidence**: The core claim that using ground truth captions as additional trajectories weighted by CLIP similarity improves distinctiveness while maintaining fluency is well-supported by both theoretical reasoning and empirical results. The ablation studies clearly demonstrate the contribution of each component.
- **Medium Confidence**: The claim that the discriminator effectively prevents reward hacking is supported by results, but the specific mechanism by which the MLP classifier in CLIP space achieves this regularization could benefit from more detailed analysis of failure cases.
- **Medium Confidence**: The bidirectional contrastive reward formulation is novel, but its advantages over unidirectional formulations, while demonstrated empirically, lack theoretical justification for why the bidirectional approach specifically leads to better performance.

## Next Checks

1. **Ablation on Ground Truth Quality**: Test the method on datasets with varying levels of caption diversity (e.g., comparing MS-COCO with more diverse caption datasets) to determine how sensitive the approach is to the quality and distinctiveness of ground truth captions.

2. **Discriminator Robustness Analysis**: Systematically evaluate the discriminator's ability to detect reward hacking attempts by generating adversarial captions that maximize CLIP similarity but violate fluency, and measure the discriminator's detection rate.

3. **Long-term Training Stability**: Extend training beyond the reported epochs to assess whether the method maintains stable performance or experiences gradual degradation due to the complex interaction between generator and discriminator training dynamics.