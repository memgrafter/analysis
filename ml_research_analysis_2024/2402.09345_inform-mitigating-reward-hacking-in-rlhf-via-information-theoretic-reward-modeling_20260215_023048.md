---
ver: rpa2
title: 'InfoRM: Mitigating Reward Hacking in RLHF via Information-Theoretic Reward
  Modeling'
arxiv_id: '2402.09345'
source_url: https://arxiv.org/abs/2402.09345
tags:
- reward
- support
- your
- help
- inform
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InfoRM improves RLHF reward modeling by filtering irrelevant information
  via a variational information bottleneck and modulating model complexity, which
  reduces reward hacking and improves robustness to inconsistent training data. It
  detects overoptimization through outliers in its latent space using the Integrated
  Cluster Deviation Score (ICDS).
---

# InfoRM: Mitigating Reward Hacking in RLHF via Information-Theoretic Reward Modeling

## Quick Facts
- arXiv ID: 2402.09345
- Source URL: https://arxiv.org/abs/2402.09345
- Reference count: 40
- Key outcome: InfoRM improves RLHF reward modeling by filtering irrelevant information via a variational information bottleneck and modulating model complexity, which reduces reward hacking and improves robustness to inconsistent training data.

## Executive Summary
InfoRM is a reward modeling framework designed to mitigate reward hacking in reinforcement learning from human feedback (RLHF). It introduces a variational information bottleneck objective to filter out irrelevant information and modulates model complexity via latent representation dimensionality. The method also detects reward overoptimization through outliers in its latent space using the Integrated Cluster Deviation Score (ICDS). Experiments across model scales (70M–7B) demonstrate InfoRM's superior stability, generalizability, and RLHF performance compared to standard reward models.

## Method Summary
InfoRM improves RLHF reward modeling by introducing a variational information bottleneck (VIB) objective to filter out human preference-irrelevant information from the latent representation, thereby improving generalizability. It modulates model complexity through the dimensionality of the latent IB representation, balancing overfitting and generalization. During RLHF, InfoRM detects reward overoptimization by identifying outliers in the latent IB space, quantified by the Integrated Cluster Deviation Score (ICDS), which measures deviations in the latent space of SFT and RLHF outputs.

## Key Results
- InfoRM improves reward model generalizability by filtering out irrelevant information via a variational information bottleneck, reducing reward hacking.
- ICDS effectively detects reward overoptimization by identifying outliers in the latent IB space, providing a timely and accurate indicator.
- Experiments across model scales (70M–7B) show InfoRM outperforms standard reward models in stability, generalizability, and RLHF performance.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: InfoRM improves reward model generalizability by filtering out human preference-irrelevant information via a variational information bottleneck (IB) objective.
- Mechanism: The IB objective explicitly minimizes mutual information between input samples (X) and latent representation (S) conditioned on the human preference label (Y), i.e., $I(X,S|Y)$, thereby compressing away task-irrelevant features while preserving information relevant to human preferences $I(S,Y)$.
- Core assumption: Irrelevant information in preference datasets (e.g., task type) degrades reward model performance on unseen data; removing it improves out-of-distribution robustness.
- Evidence anchors:
  - [abstract] "by introducing a variational information bottleneck objective to filter out irrelevant information and developing a mechanism for model complexity modulation."
  - [section 3.2] "Benefiting from the MI modeling, InfoRM eliminates human preference-irrelevant information from the latent representation to achieve accurate human preference modeling and significantly improve RM's generalizability."
  - [corpus] Strong: Several papers cite IB-based methods for RLHF reward modeling, supporting relevance of the approach.
- Break condition: If irrelevant features are not the dominant source of reward misgeneralization, filtering them may not improve performance; or if the IB objective removes too much information, harming reward accuracy.

### Mechanism 2
- Claim: InfoRM modulates model complexity through the dimensionality of the latent IB representation, balancing overfitting and generalization.
- Mechanism: Increasing IB dimensionality allows more information from inputs to flow into the latent representation, increasing model capacity; decreasing it reduces capacity and mitigates overfitting to noisy or inconsistent preference data.
- Core assumption: Model complexity correlates with generalization error; reducing complexity protects against internal inconsistency in preference datasets (e.g., label noise).
- Evidence anchors:
  - [abstract] "by introducing a variational information bottleneck objective to filter out irrelevant information and developing a mechanism for model complexity modulation."
  - [section 3.3] "This trade-off is particularly significant in the context of reward modeling... Such a complex RM is prone to overfitting... As a result, although the model may exhibit exceptional performance on training data, it tends to struggle with generalizing to unseen data."
  - [section 3.4] "In particular, a larger IB dimensionality equals increased model complexity... In contrast, a more compact IB dimensionality implies a reduction in model complexity..."
  - [corpus] Moderate: Scaling laws and complexity–generalization connections are cited but not specifically tied to IB dimensionality tuning in RLHF context.
- Break condition: If model capacity is not the bottleneck (e.g., data quality or architecture limits), adjusting IB dimensionality may have little effect.

### Mechanism 3
- Claim: InfoRM detects reward overoptimization by identifying outliers in the latent IB space, quantified by the Integrated Cluster Deviation Score (ICDS).
- Mechanism: Overoptimized samples generated during RLHF produce distinct distributional shifts in the latent IB space; ICDS measures internal deviation within RLHF outputs and external deviation between SFT and RLHF output clusters to flag overoptimization.
- Core assumption: Reward overoptimization manifests as a distributional shift in latent space that is detectable before explicit performance degradation.
- Evidence anchors:
  - [abstract] "we further identify a correlation between overoptimization and outliers in the IB latent space of InfoRM... Inspired by this finding, we propose the Integrated Cluster Deviation Score (ICDS), which quantifies deviations in the latent space, as an indicator of reward overoptimization."
  - [section 3.4] "The occurrence of reward overoptimization frequently coincides with the appearance of numerous outliers in the latent IB representation space of InfoRM."
  - [section 4.3] "ICDS provides an effective indicator of reward overoptimization... the proposed ICDS is highly sensitive to the emergence of outliers, thus offering timely and accurate detection of reward overoptimization."
  - [corpus] Strong: Several related papers explicitly use latent space or distributional shift metrics for detecting reward hacking.
- Break condition: If distributional shifts do not correlate with overoptimization, or if RLHF outputs are too diverse to form clear clusters, ICDS may fail to detect the phenomenon.

## Foundational Learning

- Concept: Variational Information Bottleneck (VIB)
  - Why needed here: InfoRM uses VIB to compress input representations while preserving task-relevant information, enabling robust reward modeling.
  - Quick check question: What is the objective of the VIB objective $I(X;S) - \beta I(S;Y)$ in terms of compression and prediction?
- Concept: Mutual Information (MI) and its variational bounds
  - Why needed here: IB-based objectives rely on MI between latent variables and inputs/labels; accurate estimation via variational bounds is essential for training InfoRM.
  - Quick check question: How does the evidence lower bound (ELBO) relate to the MI between the latent variable and the label?
- Concept: Reward overoptimization and its detection
  - Why needed here: Understanding how overoptimization manifests in model outputs is key to interpreting ICDS and applying mitigation.
  - Quick check question: What distinguishes overoptimized samples from valid high-reward outputs in the context of RLHF?

## Architecture Onboarding

- Component map:
  Input -> Encoder (LLM backbone) -> Latent representation (S) -> Decoder (MLP head) -> Preference ranking (Y) -> Loss (VIB objective) -> ICDS detection
- Critical path:
  1. Train RM on preference pairs using VIB loss.
  2. During RLHF, collect SFT and RLHF outputs.
  3. Encode outputs into latent space.
  4. Compute ICDS to detect overoptimization.
  5. Adjust IB dimensionality or stop RLHF if ICDS exceeds threshold.
- Design tradeoffs:
  - IB dimensionality vs. model capacity: Higher dimensionality increases expressivity but risks overfitting; lower reduces overfitting but may hurt performance.
  - $\beta$ (bottleneck tradeoff) vs. compression: Larger $\beta$ enforces stronger compression, possibly losing useful information.
  - ICDS sensitivity vs. robustness: ICDS must be sensitive enough to detect overoptimization but not so sensitive as to flag normal exploration.
- Failure signatures:
  - Training collapse: KL term dominates loss, causing latent collapse.
  - Poor RLHF performance: IB dimensionality too low or $\beta$ too high, discarding useful information.
  - ICDS false positives: Cluster separation metric triggered by benign distributional shifts.
- First 3 experiments:
  1. Train InfoRM with varying $\beta$ and IB dimensionality; measure final RLHF reward on in-distribution test set.
  2. Compare ICDS trajectories during RLHF with and without label noise; verify sensitivity to overoptimization.
  3. Replace VIB loss with standard cross-entropy; observe change in overoptimization detection and RLHF performance.

## Open Questions the Paper Calls Out

- Open Question 1: What specific architectural modifications to the reward model (e.g., number of layers, attention heads, or other hyperparameters) are most effective in improving generalizability and reducing reward hacking?
- Open Question 2: How does InfoRM perform when applied to reward modeling tasks beyond language models, such as robotics or game playing?
- Open Question 3: What is the impact of the information bottleneck dimensionality on the trade-off between model complexity and generalizability in InfoRM?

## Limitations

- The paper's claims rely heavily on the assumption that irrelevant features in preference datasets are a dominant source of reward model misgeneralization, which is not empirically validated against other potential sources.
- The effectiveness of IB dimensionality modulation is demonstrated but lacks a systematic study of its relationship to model capacity and overfitting across different dataset qualities.
- The ICDS metric's sensitivity to reward overoptimization is asserted but not validated against alternative distributional shift detection methods or in more diverse RLHF scenarios.

## Confidence

- High Confidence: InfoRM's architecture and training procedure are clearly specified, and its core components (VIB objective, IB dimensionality modulation, ICDS) are well-defined. The experimental setup and evaluation metrics are explicitly described.
- Medium Confidence: The claim that InfoRM improves RLHF reward modeling robustness is supported by experiments but could be strengthened with more diverse datasets and ablation studies isolating the impact of each mechanism.
- Low Confidence: The paper's assertion that ICDS provides a reliable, generalizable indicator of reward overoptimization is based on limited experimental evidence and lacks comparison to other detection methods.

## Next Checks

1. **Ablation Study**: Conduct experiments ablating the VIB objective and IB dimensionality modulation separately to quantify their individual contributions to InfoRM's performance and overoptimization detection.
2. **Alternative Detection Methods**: Compare ICDS to other distributional shift metrics (e.g., KL divergence, MMD) on the same RLHF outputs to assess its relative sensitivity and specificity for detecting reward overoptimization.
3. **Generalization Across Datasets**: Evaluate InfoRM's performance and ICDS reliability on preference datasets from different domains (e.g., summarization, translation) to test its robustness to varying types of irrelevant information and overoptimization patterns.