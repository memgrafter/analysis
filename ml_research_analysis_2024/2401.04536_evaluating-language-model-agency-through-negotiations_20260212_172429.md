---
ver: rpa2
title: Evaluating Language Model Agency through Negotiations
arxiv_id: '2401.04536'
source_url: https://arxiv.org/abs/2401.04536
tags:
- negotiation
- agreement
- agents
- games
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces structured negotiation games as a dynamic
  benchmark for evaluating language model agency. The method involves multi-turn interactions
  between language models representing negotiating parties, using games with varying
  complexity including distributive and compatible issues.
---

# Evaluating Language Model Agency through Negotiations

## Quick Facts
- arXiv ID: 2401.04536
- Source URL: https://arxiv.org/abs/2401.04536
- Reference count: 40
- Primary result: Negotiation games reveal performance differences between closed-source (GPT-3.5, GPT-4, Claude-2) and open-source language models, with closed-source models achieving higher agreement rates and payoffs

## Executive Summary
This paper introduces structured negotiation games as a dynamic benchmark for evaluating language model agency. The method involves multi-turn interactions between language models representing negotiating parties, using games with varying complexity including distributive and compatible issues. The approach addresses limitations of static benchmarks by enabling cross-model interactions and avoiding data leakage. Experiments with six publicly available language models show that closed-source models outperformed open-source alternatives in reaching agreements and optimizing payoffs. Notably, GPT-4 demonstrated superior faithfulness and instruction-following but underperformed in negotiation outcomes compared to GPT-3.5. The findings highlight the potential of negotiation games as a co-evolving benchmark for assessing language model capabilities and alignment.

## Method Summary
The study evaluates language model agency through structured negotiation games where models represent negotiating parties in multi-turn interactions. The framework uses recursive prompting to generate mental notes and public messages, with games varying in complexity from single-issue distributive to multi-issue integrative scenarios. The evaluation includes self-play and cross-play settings across six publicly available models (GPT-3.5, GPT-4, Claude-2, chat-bison, command, command-light). Performance is measured through agreement rates (soft and hard), normalized total payoffs, internal and external faithfulness metrics, and instruction-following behavior. The dynamic nature of the benchmark prevents overfitting to static datasets and enables cross-model interactions.

## Key Results
- Closed-source models (GPT-3.5, GPT-4, Claude-2) outperformed open-source alternatives in agreement rates and payoff optimization
- GPT-4 demonstrated superior faithfulness and instruction-following but achieved lower negotiation outcomes compared to GPT-3.5
- Cross-play performance exposed model robustness to out-of-distribution interactions
- Dynamic benchmark generation prevents obsolescence and data leakage issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Negotiation games co-evolve with model capabilities, preventing benchmark obsolescence
- Mechanism: Each game instance generates fresh scenarios and payoff tables, avoiding fixed static datasets
- Core assumption: Dynamic generation maintains task difficulty relative to model performance
- Evidence anchors:
  - [abstract]: "dynamic benchmarks, where tasks are dynamically generated each time a model is tested"
  - [section 2.2]: "Besides payoff tables and the types of Issues, the complexity of negotiation games depends on the number of Issues under negotiation and the negotiation setting"
  - [corpus]: "How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis" (average neighbor FMR=0.543) suggests related work on dynamic evaluation

### Mechanism 2
- Claim: Cross-play exposes model robustness to out-of-distribution interactions
- Mechanism: Models negotiate against different instances, forcing adaptation to varied strategies and communication styles
- Core assumption: Opponent messages are sufficiently diverse to create OOD scenarios
- Evidence anchors:
  - [abstract]: "multi-turn, and cross-model interactions"
  - [section 4.2]: "Cross-play performance is of particular interest for measuring model robustness, as messages generated by opponents will likely be out-of-distribution"
  - [corpus]: "Reproducibility Study of Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation" (average neighbor FMR=0.494) indicates interest in robustness evaluation

### Mechanism 3
- Claim: Structured prompts enable measurement of internal consistency and instruction-following
- Mechanism: Agents generate mental notes before messages, creating traceable reasoning chains that can be validated
- Core assumption: Note generation captures true reasoning rather than post-hoc rationalization
- Evidence anchors:
  - [section 2.3]: "We designed our negotiation protocol to capture both internal and external metrics of faithfulness"
  - [section 3.4]: "Instruction-following is crucial for safe deployment... We measure instruction-following behavior of staying within the maximum number of words allowed"
  - [corpus]: "The Language of Bargaining: Linguistic Effects in LLM Negotiations" (no corpus score) suggests linguistic analysis complements faithfulness metrics

## Foundational Learning

- Concept: Distributive vs compatible issues
  - Why needed here: Understanding payoff structures determines negotiation strategy and benchmark complexity
  - Quick check question: In a pizza-sharing scenario, is deciding slice count distributive or compatible?

- Concept: Integrative bargaining and preference weighting
  - Why needed here: Enables evaluation of trade-off discovery and value creation beyond zero-sum thinking
  - Quick check question: If Alice values rent more than duration while Bob values duration more than rent, what negotiation type is this?

- Concept: Theory of mind inference in negotiations
  - Why needed here: Agents must estimate opponent payoff tables to negotiate effectively
  - Quick check question: Why is predicting opponent's acceptable offers important for successful negotiation?

## Architecture Onboarding

- Component map: Game engine -> Agent interface -> Evaluation module -> Data pipeline
- Critical path: Game initialization → Agent note/message generation → Response validation → Agreement detection → Performance scoring
- Design tradeoffs:
  - Note visibility: hiding opponent notes increases realism but complicates evaluation
  - Message length limits: prevent token exhaustion but may constrain complex reasoning
  - Agreement detection: soft agreements are easier to compute but less reliable than hard agreements
- Failure signatures:
  - Low agreement rates with high faithfulness: models understand task but struggle with coordination
  - High agreement rates with low payoffs: models cooperate but don't optimize individual outcomes
  - Note-message mismatches: instruction-following failures or reasoning inconsistencies
- First 3 experiments:
  1. Single-issue distributive self-play to verify basic functionality
  2. Cross-play between two different models to test robustness
  3. Multi-issue integrative games to evaluate trade-off reasoning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do persona mixtures in language models affect negotiation outcomes across different game types?
- Basis in paper: [explicit] The paper discusses how language models are trained on text from various authors, creating a superposition of personas, and mentions that models modify behavior based on prompt context.
- Why unresolved: The paper only briefly mentions persona effects and mentions running limited experiments with expert vs novice negotiator descriptions, but does not provide comprehensive results or analysis.
- What evidence would resolve it: Systematic experiments varying persona descriptions across different game types (distributive, compatible, integrative) with quantitative analysis of agreement rates and payoff optimization.

### Open Question 2
- Question: What is the optimal trade-off between note/message length restrictions and negotiation performance across different game complexities?
- Basis in paper: [explicit] The paper discusses varying maximum note and message lengths (32, 64, 128 words) and notes that 32 words led to frequent instruction-following breaks, while the difference between 64 and 128 was minimal.
- Why unresolved: The paper opted for 64 words as default without rigorous testing of whether this varies with game complexity or number of issues.
- What evidence would resolve it: Experiments systematically varying word restrictions across different game complexities (single-issue vs multi-issue, distributive vs compatible) with performance metrics.

### Open Question 3
- Question: How does the visibility of opponent information (payoffs, ability descriptions) affect negotiation strategies and outcomes?
- Basis in paper: [explicit] The paper mentions running limited experiments with three visibility levels: titles only, titles and payoffs, and titles with ability descriptions.
- Why unresolved: The paper only provides brief mention of these experiments with a single data point (0.40 agreement rate for level 1) without comprehensive results or analysis.
- What evidence would resolve it: Full experimental results across all visibility levels, analysis of how information asymmetry affects agreement rates, and whether providing more information leads to better or worse outcomes.

## Limitations
- Evaluation limited to six publicly available models from major providers, restricting architectural diversity
- Closed-source models outperformed open-source alternatives, potentially reflecting quality differences rather than benchmark effectiveness
- Faithfulness metrics rely on self-reported mental notes that may not capture true reasoning processes
- Does not address potential safety concerns from models learning manipulative negotiation tactics

## Confidence
- High Confidence: The finding that negotiation games provide a dynamic benchmark preventing overfitting to static datasets
- Medium Confidence: The claim that GPT-4 demonstrates superior faithfulness but underperforms in negotiation outcomes compared to GPT-3.5
- Low Confidence: The broader claim about negotiation games as a co-evolving benchmark for assessing capabilities and alignment

## Next Checks
1. Cross-Architecture Validation: Test the negotiation game framework with models from diverse architectural families to determine if performance patterns generalize
2. Safety Impact Assessment: Evaluate whether exposure to negotiation games influences models' propensity for deceptive behavior in subsequent tasks
3. Scalability Testing: Implement the framework with increased game complexity to assess benchmark effectiveness as models advance in capabilities