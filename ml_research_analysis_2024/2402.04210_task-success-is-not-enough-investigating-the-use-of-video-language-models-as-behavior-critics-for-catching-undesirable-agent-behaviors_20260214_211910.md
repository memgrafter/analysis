---
ver: rpa2
title: 'Task Success is not Enough: Investigating the Use of Video-Language Models
  as Behavior Critics for Catching Undesirable Agent Behaviors'
arxiv_id: '2402.04210'
source_url: https://arxiv.org/abs/2402.04210
tags:
- arxiv
- robot
- gpt-4v
- behavior
- undesirable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large vision-language models (VLMs)
  can serve as scalable Behavior Critics to identify undesirable robot behaviors in
  videos. The authors construct a benchmark of videos showing goal-reaching but undesirable
  robot behaviors in household tasks.
---

# Task Success is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors

## Quick Facts
- arXiv ID: 2402.04210
- Source URL: https://arxiv.org/abs/2402.04210
- Authors: Lin Guan; Yifan Zhou; Denis Liu; Yantian Zha; Heni Ben Amor; Subbarao Kambhampati
- Reference count: 18
- Primary result: VLMs achieve 69.42% recall and 62.21% precision in detecting undesirable robot behaviors, with grounding feedback improving precision to >98%

## Executive Summary
This paper investigates whether large vision-language models (VLMs) can serve as scalable Behavior Critics to identify undesirable robot behaviors in videos. The authors construct a benchmark of videos showing goal-reaching but undesirable robot behaviors in household tasks. They evaluate GPT-4V and Gemini Pro on this benchmark, measuring recall (proportion of undesirable events detected) and precision (accuracy of critiques). GPT-4V achieves a recall of 69.42% and precision of 62.21%, identifying a significant portion of undesirable events but also producing critiques with hallucinated information due to visual grounding errors. To improve precision, the authors propose augmenting VLMs with grounding feedback from specialized perception models, which increases precision to over 98% with minimal impact on recall. They also demonstrate a practical closed-loop system using Code-as-Policies to iteratively refine robot policies based on VLM critiques.

## Method Summary
The authors develop a benchmark of household robot behavior videos showing goal-reaching but undesirable actions. They evaluate two VLMs (GPT-4V and Gemini Pro) on this benchmark, measuring recall and precision of undesirable behavior detection. To address visual grounding errors leading to hallucinated critiques, they propose augmenting VLMs with feedback from specialized perception models (goal achievement detection and object state classification). This grounding feedback mechanism provides the VLM with explicit information about task success and object states, allowing it to focus critique generation on truly undesirable behaviors. The system is integrated into a closed-loop refinement pipeline using Code-as-Policies, where robot policies are iteratively updated based on VLM critiques.

## Key Results
- GPT-4V achieves 69.42% recall and 62.21% precision in detecting undesirable robot behaviors
- Grounding feedback from perception models increases precision to >98% with minimal recall impact
- The closed-loop refinement system successfully improves robot policies based on VLM critiques
- Visual grounding errors are the primary source of hallucinated information in VLM critiques

## Why This Works (Mechanism)
The approach works by leveraging VLMs' ability to understand complex visual scenes and generate natural language critiques, while addressing their tendency to hallucinate through grounding feedback. The specialized perception models provide concrete, factual information about task outcomes and object states that the VLM can use to verify its visual understanding before generating critiques. This combination allows the system to maintain the VLM's broad behavioral understanding while reducing false positives from visual misinterpretations.

## Foundational Learning

Object State Classification: Understanding how to categorize objects based on their visual properties (why needed: VLMs need clear object state information to avoid misinterpretation; quick check: can the model distinguish between clean and dirty objects in images)

Goal Achievement Detection: Methods for determining whether a task's objective has been met (why needed: VLMs need explicit task success information to focus on undesirable behaviors; quick check: can the model accurately detect when a robot has completed a task?)

Visual Grounding: The process of connecting textual descriptions to specific regions in images (why needed: VLMs often struggle with precise visual localization; quick check: can the model correctly identify which objects in an image correspond to named entities?)

Critique Generation: Techniques for producing constructive feedback from visual observations (why needed: the core function of the behavior critic; quick check: does the critique accurately describe the undesirable behavior without hallucination?)

Closed-loop Policy Refinement: Iterative improvement of policies based on feedback (why needed: to demonstrate practical utility of the behavior critic; quick check: does the policy actually improve after receiving critiques?)

## Architecture Onboarding

Component Map:
VLM -> Perception Models (Goal Achievement + Object State) -> Critique Generation -> Code-as-Policies -> Robot Policy

Critical Path:
Video input → VLM analysis → Perception model feedback → VLM critique generation → Policy update → Robot behavior refinement

Design Tradeoffs:
- VLM accuracy vs. computational overhead
- Perception model specificity vs. generalization
- Critique detail vs. actionable policy updates
- Real-time performance vs. thorough analysis

Failure Signatures:
- Visual grounding errors leading to hallucinated critiques
- Perception model misclassification of object states
- VLM failing to recognize context-appropriate behaviors
- Policy updates that don't address the identified issues

First Experiments:
1. Test VLM performance on simple video sequences with clear undesirable behaviors
2. Evaluate perception model accuracy on the household robot benchmark
3. Measure the impact of grounding feedback on critique precision across different task types

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on specific household robot behaviors that may not generalize to other domains
- Computational overhead and real-time performance of the augmented VLM system not fully explored
- Closed-loop system tested on relatively simple tasks with limited environmental variability
- Visual grounding errors remain a challenge even with perception model feedback

## Confidence
- VLMs as effective behavior critics: **Medium** confidence due to visual grounding errors
- Grounding feedback method improving precision: **Medium** confidence based on limited evaluation scope
- Closed-loop refinement system demonstration: **High** confidence from presented results

## Next Checks
1. Test the grounding feedback approach with different types of perception models and across multiple task domains to verify generalizability
2. Evaluate system performance with longer task sequences and more complex household environments
3. Measure computational overhead and real-time performance impact of the augmented VLM system in closed-loop operation