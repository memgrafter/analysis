---
ver: rpa2
title: Adaptive Transit Signal Priority based on Deep Reinforcement Learning and Connected
  Vehicles in a Traffic Microsimulation Environment
arxiv_id: '2408.00098'
source_url: https://arxiv.org/abs/2408.00098
tags:
- signal
- traffic
- simulation
- control
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study develops and tests a deep reinforcement learning-based
  transit signal priority (TSP) agent using microscopic traffic simulation and connected
  vehicle data. The TSP agent takes control when buses enter the DSRC zone of the
  intersection, reducing bus travel time by about 21% with marginal impacts to general
  traffic at saturation rate 0.95.
---

# Adaptive Transit Signal Priority based on Deep Reinforcement Learning and Connected Vehicles in a Traffic Microsimulation Environment

## Quick Facts
- arXiv ID: 2408.00098
- Source URL: https://arxiv.org/abs/2408.00098
- Authors: Dickness Kwesiga; Angshuman Guin; Michael Hunter
- Reference count: 1
- Key outcome: Event-based DRL TSP reduces bus travel time by 21% with marginal general traffic impacts at v/c=0.95

## Executive Summary
This study develops and tests a deep reinforcement learning-based transit signal priority (TSP) agent using microscopic traffic simulation and connected vehicle data. The TSP agent activates only when buses enter the DSRC zone of the intersection, reducing bus travel time by about 21% with minimal impact to general traffic at high saturation rates. The event-based RL approach addresses the challenge of sparse bus arrivals in traditional TSP methods by focusing control resources on relevant scenarios.

## Method Summary
The method uses two Double Deep Q-Network (DDQN) agents: one for general traffic control (DDQN-SC) and one for TSP (DDQN-TSP). The TSP agent takes control when buses enter an 800ft DSRC zone around the intersection, using event-based scripting in PTV Vissim for computational efficiency. The state includes vehicle counts, green duration, and bus position/speed; actions are phase selections; and rewards are negative average delay with penalties for side street queuing. Training involves approximately 400 episodes for DDQN-SC and 150 episodes for DDQN-TSP.

## Key Results
- Bus travel time reduced by approximately 21% with DDQN-TSP compared to no TSP
- DDQN-TSP performs slightly better than actuated signal control with TSP
- Event-based approach activates TSP only when buses are present, addressing sparse bus arrival challenges
- Marginal impacts to general traffic observed at saturation rate 0.95

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Event-based RL approach reduces bus travel time by 21% by activating TSP only when buses are present
- Mechanism: Focuses learning and control resources on relevant scenarios by limiting TSP activation to periods when buses are within the DSRC zone
- Core assumption: Bus arrivals are sufficiently sparse that traditional second-by-second TSP control would waste resources
- Evidence anchors: Abstract states "event-based RL approach activates TSP only when buses are present"; Section shows "Overall bus travel time is reduced by approximately 21% with DDQN-TSP"

### Mechanism 2
- Claim: Dual-agent architecture enables smooth transitions between control modes
- Mechanism: Background DDQN-SC controller maintains normal flow until bus triggers DDQN-TSP agent, which smoothly returns control when bus leaves
- Core assumption: State representation allows TSP agent to start from current background controller state without abrupt transitions
- Evidence anchors: Section states "At the point of switching, the agent coming online takes the last state of the agent going offline"; "Over several episodes the agents learn to take actions that allow smooth transitions"

### Mechanism 3
- Claim: Event-based scripting significantly improves simulation run time efficiency
- Mechanism: Event-based scripts execute specified functions at given time steps without continuous data exchange, reducing computational overhead
- Core assumption: Event-based approach provides sufficient data exchange for RL training while reducing run time
- Evidence anchors: Section states "Using event-based scripts instead of COM required manually bridging model data"; "event-based scripts, a less widespread alternative to COM available in PTV Vissim"

## Foundational Learning

- Concept: Deep Q-Network (DQN) and Double Deep Q-Network (DDQN)
  - Why needed here: Used for both general traffic controller and TSP agent to learn optimal signal control policies
  - Quick check question: What is the key difference between DQN and DDQN that makes DDQN more suitable for this application?

- Concept: Microscopic traffic simulation and CV data integration
  - Why needed here: Simulation environment uses PTV Vissim with connected vehicle data to realistically model vehicle movements and bus positions
  - Quick check question: How does the DSRC range assumption (800 ft) influence the state representation for the TSP agent?

- Concept: Event-based vs. continuous control systems
  - Why needed here: TSP agent is event-based, activating only when buses are present, contrasting with traditional continuous TSP approaches
  - Quick check question: What are the potential drawbacks of using an event-based approach if bus frequencies were to increase significantly?

## Architecture Onboarding

- Component map: PTV Vissim simulation engine -> DDQN-SC agent -> DDQN-TSP agent -> Event-based scripting interface -> Data bridging layer -> Connected vehicle data source

- Critical path: Simulation run → State extraction → Agent decision → Signal update → Reward calculation → Model training → Parameter storage

- Design tradeoffs:
  - Event-based scripting vs. COM API: Faster execution but requires manual data bridging
  - Single vs. multiple random seeds for training: Faster with single seed but potentially less robust
  - Continuous vs. event-based TSP: Simpler implementation but potentially inefficient for sparse bus arrivals

- Failure signatures:
  - Training instability or slow convergence: May indicate suboptimal hyperparameters or state representation issues
  - Poor bus travel time reduction: Could suggest inadequate reward function design or insufficient training episodes
  - Excessive general traffic delay: Might indicate overly aggressive TSP implementation or poor balance in penalty terms

- First 3 experiments:
  1. Validate DDQN-SC performance against actuated signal control at different v/c ratios to ensure baseline controller effectiveness
  2. Test TSP activation logic by verifying smooth handoff between DDQN-SC and DDQN-TSP when buses enter/exit DSRC zone
  3. Evaluate bus travel time and general traffic delay impacts at varying bus headways to assess scalability of event-based approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the TSP agent's performance compare to other RL-based TSP algorithms beyond traditional actuated signal control?
- Basis in paper: The authors mention that comparisons are limited to traditional A-SC TSP systems and suggest this could be expanded to include other RL-based systems
- Why unresolved: The study only compares DDQN-TSP with actuated signal control with TSP, not with other RL-based TSP algorithms
- What evidence would resolve it: Testing and comparing the DDQN-TSP agent against other RL-based TSP algorithms using the same simulation environment and metrics

### Open Question 2
- Question: What is the impact of extending the TSP agent from a single intersection to multiple intersections in coordination?
- Basis in paper: The authors state that differences in performance are likely to be seen if the algorithms are extended to multiple intersections, which is a subject of an ongoing study
- Why unresolved: The current study only tests the TSP agent at a single intersection, and the impact of multi-intersection coordination is not explored
- What evidence would resolve it: Implementing and testing the TSP agent across multiple coordinated intersections, measuring bus travel time and general traffic delay

### Open Question 3
- Question: How does the TSP agent perform in real-world conditions with software in the loop simulation using an emulator running the same software as field signal controllers?
- Basis in paper: The authors mention that in an ongoing study, the developed agents will be tested on multiple intersections in coordination and that software in the loop simulation with an emulator will be used
- Why unresolved: The current study uses a microscopic simulation environment, and real-world performance with actual signal controller software is not tested
- What evidence would resolve it: Conducting software in the loop simulations with an emulator of field signal controllers to validate the TSP agent's performance in real-world conditions

## Limitations
- Simulation environment uses a single intersection with simplified assumptions about bus routes and traffic patterns
- Results measured at specific saturation rate (v/c=0.95) with fixed bus headway of 15 minutes, limiting generalizability
- Event-based scripting required manual data bridging that could introduce implementation errors
- Reward function parameters not fully specified, making exact replication challenging

## Confidence
- **High Confidence**: The core mechanism of event-based TSP activation reducing bus travel time by approximately 21% at v/c=0.95
- **Medium Confidence**: The superiority of the dual-agent architecture for smooth transitions between control modes
- **Medium Confidence**: The computational efficiency gains from event-based scripting

## Next Checks
1. Evaluate the event-based TSP approach at different bus headways (e.g., 5, 10, and 20 minutes) to determine the threshold where continuous TSP control becomes more efficient than event-based activation

2. Extend the simulation to a network of interconnected intersections to assess whether single-intersection benefits translate to corridor-level improvements in bus travel time and general traffic flow

3. Conduct a parameter sensitivity analysis on the penalty values and thresholds in the reward function to determine how these choices affect the balance between bus priority and general traffic delay