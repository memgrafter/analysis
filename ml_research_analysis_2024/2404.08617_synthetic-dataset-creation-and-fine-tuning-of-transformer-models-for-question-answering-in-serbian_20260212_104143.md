---
ver: rpa2
title: Synthetic Dataset Creation and Fine-Tuning of Transformer Models for Question
  Answering in Serbian
arxiv_id: '2404.08617'
source_url: https://arxiv.org/abs/2404.08617
tags:
- dataset
- serbian
- question
- which
- latin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of limited labeled data for Question
  Answering (QA) in Serbian by creating a large synthetic dataset. The authors adapt
  the Translate-Align-Retrieve method to generate SQuAD-sr, the largest Serbian QA
  dataset with over 87,000 samples, available in both Cyrillic and Latin scripts.
---

# Synthetic Dataset Creation and Fine-Tuning of Transformer Models for Question Answering in Serbian

## Quick Facts
- arXiv ID: 2404.08617
- Source URL: https://arxiv.org/abs/2404.08617
- Reference count: 24
- Primary result: Created SQuAD-sr, the largest Serbian QA dataset (>87k samples), fine-tuning BERTić-SQuAD-sr achieves 73.91% EM and 82.97% F1 on Serbian XQuAD

## Executive Summary
This paper addresses the lack of labeled data for Question Answering in Serbian by creating a large synthetic dataset using an adapted Translate-Align-Retrieve method. The authors generate SQuAD-sr, available in both Cyrillic and Latin scripts, and fine-tune several transformer models including multilingual and monolingual variants. BERTić-SQuAD-sr in Latin script achieves the best performance at 73.91% Exact Match and 82.97% F1 on the Serbian XQuAD evaluation set. The study demonstrates that monolingual models outperform multilingual ones and that Latin script yields better results than Cyrillic.

## Method Summary
The authors adapt the Translate-Align-Retrieve method to create SQuAD-sr from SQuAD v1.1. They use NLLB-200-1.3B to translate English contexts, questions, and titles to Serbian (Cyrillic), optionally transliterating to Latin using Cyrtranslit. Word alignment is performed using eflomal with grow-diag-final-and heuristics to map English answers to Serbian contexts. The resulting dataset is used to fine-tune BERTić, mBERT, and XLM-R models using HuggingFace with batch size 16, learning rate 3e-5, and 3 epochs. Models are evaluated on the Serbian XQuAD dataset.

## Key Results
- BERTić-SQuAD-sr (Latin) achieves 73.91% Exact Match and 82.97% F1 on Serbian XQuAD
- Monolingual BERTić outperforms multilingual models (mBERT, XLM-R) on Serbian QA
- Latin script version outperforms Cyrillic version across all models (18.49/17.3% EM/F1 increase for BERTić)
- Numeric values and dates are answered more accurately than other question types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The TAR method adapted for Serbian generates high-quality synthetic QA data despite language-specific challenges.
- Mechanism: The TAR method improves over naive translation by leveraging word alignments to extract answer spans from translated contexts, reducing mismatches caused by syntactic differences between English and Serbian.
- Core assumption: Word alignment between English and Serbian sentences reliably captures semantic correspondence even with morphological variations.
- Evidence anchors: [abstract] "We follow the idea of the Translate-Align-Retrieve method from [5] to which we propose changes to make it more suitable for Serbian." [section] "To obtain word alignment results we utilize Efficient Low-Memory Aligner (eflomal)... To further improve the alignments we apply grow-diag-final-and heuristics..."
- Break condition: If word alignments frequently fail due to complex morphological changes in Serbian, leading to incorrect answer span extraction.

### Mechanism 2
- Claim: Using a monolingual pre-trained model (BERTić) outperforms multilingual models when fine-tuned on the Serbian dataset.
- Mechanism: Monolingual models are pre-trained on larger portions of the target language, capturing richer linguistic features and vocabulary, which improves downstream QA performance.
- Core assumption: Pre-training data volume and language specificity are more important than model size or architecture for low-resource QA tasks.
- Evidence anchors: [abstract] "The results show that our model exceeds zero-shot baselines, but fails to go beyond human performance. We note the advantage of using a monolingual pre-trained model over multilingual, as well as the performance increase gained by using Latin over Cyrillic." [section] "Best results were obtained by fine-tuning the BERTić model on our Latin SQuAD-sr dataset, achieving 73.91% Exact Match and 82.97% F1 score..."
- Break condition: If the monolingual model's vocabulary is too small to cover Serbian data, causing performance degradation.

### Mechanism 3
- Claim: The Latin script version of the synthetic dataset yields better QA model performance than the Cyrillic version.
- Mechanism: Pre-trained models have been exposed to more Latin-script text during pre-training, leading to better tokenization and representation learning for Latin script inputs.
- Core assumption: Script exposure during pre-training significantly impacts model performance on downstream tasks.
- Evidence anchors: [abstract] "We note the advantage of using a monolingual pre-trained model over multilingual, as well as the performance increase gained by using Latin over Cyrillic." [section] "We achieve performance increases in all models when fine-tuning on the Latin dataset... BERTić-SQuAD-sr - 18.49/17.3 % EM/F1 increase with the Latin dataset."
- Break condition: If Cyrillic script data becomes more prevalent in future pre-training corpora, reducing the performance gap.

## Foundational Learning

- Concept: Word Alignment in Machine Translation
  - Why needed here: Essential for the TAR method to map answer spans between English and Serbian contexts accurately.
  - Quick check question: How does the eflomal tool improve alignment accuracy compared to naive alignment methods?

- Concept: Transformer-based Language Models
  - Why needed here: The backbone of the QA models; understanding their architecture and fine-tuning process is critical.
  - Quick check question: What are the key differences between monolingual and multilingual BERT models in terms of pre-training data and vocabulary?

- Concept: Script Duality in Serbian
  - Why needed here: Serbian uses both Cyrillic and Latin scripts; model performance can vary significantly depending on the script used.
  - Quick check question: Why might a model pre-trained on Latin script data perform worse on Cyrillic script data, even if the underlying language is the same?

## Architecture Onboarding

- Component map:
  Data Generation Pipeline: Translate → Transliterate (optional) → Align → Retrieve
  Models: mBERT, XLM-R, BERTić (fine-tuned on SQuAD-sr)
  Evaluation: Serbian XQuAD (manually translated)
  Metrics: Exact Match (EM), F1 Score

- Critical path:
  1. Generate SQuAD-sr dataset using TAR method
  2. Fine-tune selected models on SQuAD-sr
  3. Evaluate models on Serbian XQuAD
  4. Analyze results by question type

- Design tradeoffs:
  - Using synthetic vs. manually annotated data: faster and cheaper but may introduce noise
  - Monolingual vs. multilingual models: better performance vs. broader applicability
  - Cyrillic vs. Latin script: trade-off between linguistic authenticity and model compatibility

- Failure signatures:
  - Low EM/F1 scores indicating poor alignment or translation quality
  - Disproportionate errors on certain question types (e.g., Where questions)
  - Large performance gap between Cyrillic and Latin script models

- First 3 experiments:
  1. Fine-tune mBERT on Cyrillic SQuAD-sr and evaluate on Serbian XQuAD (Cyrillic)
  2. Fine-tune XLM-R on Latin SQuAD-sr and evaluate on Serbian XQuAD (Latin)
  3. Fine-tune BERTić on Latin SQuAD-sr and evaluate on Serbian XQuAD (Latin)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of BERTić-SQuAD-sr on Cyrillic script compare to other Serbian QA models trained on manually annotated data?
- Basis in paper: [explicit] The paper notes that BERTić-SQuAD-sr achieves 55.42% EM and 65.67% F1 on Cyrillic, which is lower than its Latin performance. However, it does not compare to manually annotated models.
- Why unresolved: There are no manually annotated Serbian QA datasets available for comparison, as stated in the paper.
- What evidence would resolve it: Creating a manually annotated Serbian QA dataset and training models on it to compare with BERTić-SQuAD-sr's performance on Cyrillic.

### Open Question 2
- Question: What is the impact of using more advanced synthetic dataset generation methods, such as iterative back-translation or knowledge distillation, on the quality of SQuAD-sr?
- Basis in paper: [inferred] The paper uses the Translate-Align-Retrieve method to create SQuAD-sr. It mentions that the method yields decent results but has some limitations, such as extracting punctuation marks instead of desired words in some cases.
- Why unresolved: The paper does not explore alternative synthetic dataset generation methods or their potential impact on dataset quality.
- What evidence would resolve it: Creating synthetic datasets using different methods and comparing their quality and the performance of models trained on them.

### Open Question 3
- Question: How does the performance of BERTić-SQuAD-sr on questions requiring deep reading comprehension compare to human performance?
- Basis in paper: [explicit] The paper reports that BERTić-SQuAD-sr achieves 73.91% EM and 82.97% F1 on the Latin version of Serbian XQuAD, while human performance is 82.30% EM and 91.22% F1.
- Why unresolved: The paper does not provide a detailed breakdown of the model's performance on different question types or difficulty levels.
- What evidence would resolve it: Analyzing the model's performance on a subset of questions that require deep reading comprehension and comparing it to human performance on the same subset.

## Limitations

- Evaluation relies on a small test set (1,190 samples) limiting statistical power for model comparisons
- Human performance baseline established on only 25 samples makes it unreliable as a ceiling for model performance
- Claims about zero-shot baseline superiority lack explicit baseline performance numbers in results

## Confidence

- TAR method effectiveness: Medium confidence - results support method but alignment quality validation is limited
- Monolingual vs. multilingual superiority: Medium confidence - supported by experimental results but limited to single task
- Script performance gap: Medium confidence - depends on unquantified pre-training data distribution
- Zero-shot baseline claims: Low confidence - baseline performance not explicitly stated in results

## Next Checks

1. Evaluate model performance on additional Serbian QA datasets (if available) or manually annotate a small validation set to verify TAR alignment quality
2. Test zero-shot performance of mBERT and XLM-R on Serbian QA tasks to establish proper baseline comparisons
3. Analyze word alignment error rates on a sample of translated sentence pairs to quantify the reliability of the TAR method for Serbian