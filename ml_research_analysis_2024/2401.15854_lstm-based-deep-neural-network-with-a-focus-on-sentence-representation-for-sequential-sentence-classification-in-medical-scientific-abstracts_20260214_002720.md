---
ver: rpa2
title: LSTM-based Deep Neural Network With A Focus on Sentence Representation for
  Sequential Sentence Classification in Medical Scientific Abstracts
arxiv_id: '2401.15854'
source_url: https://arxiv.org/abs/2401.15854
tags:
- sentence
- level
- abstract
- sentences
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the Sequential Sentence Classification (SSC)
  task in medical abstracts, where sentences are categorized into predefined headings
  like "background", "methods", etc. The authors propose a LSTM-based deep neural
  network focused on improving sentence representation at the sentence level.
---

# LSTM-based Deep Neural Network With A Focus on Sentence Representation for Sequential Sentence Classification in Medical Scientific Abstracts

## Quick Facts
- arXiv ID: 2401.15854
- Source URL: https://arxiv.org/abs/2401.15854
- Reference count: 24
- Key outcome: The paper proposes an LSTM-based deep neural network that improves F1 scores by 1.0%, 2.8%, and 2.6% on PubMed 200K RCT, PubMed 20K RCT, and NICTA-PIBOSO datasets respectively for sequential sentence classification in medical abstracts.

## Executive Summary
This paper addresses Sequential Sentence Classification (SSC) in medical abstracts, where sentences must be categorized into predefined headings like "background", "methods", etc. The authors propose a novel LSTM-based deep neural network that focuses on improving sentence representation at the sentence level by combining multiple linguistic features. The model incorporates word sequences, character sequences, statistical information, and pre-trained sentence embeddings to create comprehensive sentence representations. At higher levels, Convolutional-RNN and Multi-layer Perception networks further refine classification using these sentence embeddings, achieving highly competitive results compared to baseline methods.

## Method Summary
The proposed system consists of three main components working at different levels: Sen-Model at the sentence level, Abs-Model at the abstract level, and Seg-Model at the segment level. Sen-Model is a 4-branch LSTM network that captures word-level semantics (GloVe embeddings), character-level morphology (randomly initialized embeddings), sequential context (statistical features), and domain-specific knowledge (PubMed-based sentence embeddings). These features are combined through Bi-LSTM and attention mechanisms to create sentence embeddings. The Abs-Model uses a Convolutional-RNN network to capture global patterns across the entire abstract, while the Seg-Model uses a Multi-layer Perceptron to process fixed-length segments of consecutive sentences. The final predictions are obtained through weighted fusion of both models' outputs.

## Key Results
- The proposed system improves F1 scores by 1.0% on PubMed 200K RCT compared to baseline
- The system achieves 2.8% improvement in F1 score on PubMed 20K RCT dataset
- The model shows 2.6% F1 score improvement on NICTA-PIBOSO dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining word, character, and statistical information with pre-trained sentence embeddings improves sentence representation quality.
- Mechanism: The proposed LSTM-based network uses four parallel branches to capture different linguistic features: word-level semantics (GloVe embeddings), character-level morphology (randomly initialized embeddings), sequential context within abstracts (statistical features), and domain-specific semantic knowledge (PubMed-based sentence embeddings). These features are combined through Bi-LSTM and attention mechanisms to create comprehensive sentence embeddings.
- Core assumption: Different linguistic features provide complementary information that, when combined, create a more complete representation than any single feature alone.
- Evidence anchors:
  - [abstract] "we explore the independent features of sentence, word sequence, character sequence, and statistic information of sentences in one abstract"
  - [section] "The sequence of word embeddings W and the sequence of character embeddings C are fed into stacked Bi-LSTM-Attention encoder blocks"
  - [corpus] Weak - no direct evidence in corpus papers about this specific multi-branch approach
- Break condition: If one feature type becomes noisy or irrelevant (e.g., character embeddings don't capture meaningful morphological patterns in medical text), the combination could degrade performance.

### Mechanism 2
- Claim: Higher-level contextual refinement using sentence embeddings improves classification performance.
- Mechanism: After extracting comprehensive sentence embeddings at the sentence level, two higher-level networks process these embeddings: (1) a Convolutional-RNN network at the abstract level that captures local sentence patterns through convolutional feature maps followed by RNN decoding, and (2) a Multi-layer Perceptron network at the segment level that processes fixed-length segments of consecutive sentences to capture local dependencies.
- Core assumption: Sentence embeddings that capture both intra-sentence and inter-sentence relationships can be further refined by processing them in context.
- Evidence anchors:
  - [abstract] "Given these sentence embeddings, a Convolutional-RNN based network (C-RNN) at the abstract level and a Multi-layer Perception network (MLP) at the segment level are introduced to learn the contextual patterns of sentences"
  - [section] "To explore the sequential and contextual properties of sentences in one abstract, we group sentence embeddings in the same abstract to create the abstract representation"
  - [corpus] Weak - no direct evidence in corpus papers about this specific hierarchical refinement approach
- Break condition: If sentence embeddings are already perfect representations, additional processing layers would only add computational cost without improving performance.

### Mechanism 3
- Claim: The fusion of abstract-level and segment-level predictions provides complementary perspectives for final classification.
- Mechanism: The system combines predictions from the abstract-level C-RNN network (λabs) and the segment-level MLP network (λseg) using weighted averaging to produce final sentence classifications. The abstract-level network captures global patterns across the entire abstract, while the segment-level network focuses on local coherence within segments of consecutive sentences.
- Core assumption: Global and local contextual information provide complementary signals that, when combined, improve classification accuracy.
- Evidence anchors:
  - [abstract] "the results of C-RNN and MLP models are fused to achieve the final predicted sentences in an abstract"
  - [section] "Given the predicted labels of Abs-Model at the abstract level and Seg-Model at the segment level, referred to as ˆYabs and ˆYseg, the final predicted labels of our proposed system is defined as: ˆY = λabs ˆYabs + λseg ˆYseg"
  - [corpus] Weak - no direct evidence in corpus papers about this specific fusion strategy
- Break condition: If the two models make correlated errors (e.g., both fail on similar sentence patterns), simple weighted averaging may not improve performance.

## Foundational Learning

- Concept: LSTM (Long Short-Term Memory) networks
  - Why needed here: LSTMs are essential for capturing sequential dependencies in text, both within individual sentences (word order, character sequences) and across sentences in abstracts. The paper uses stacked Bi-LSTM layers to extract hierarchical representations.
  - Quick check question: What problem do LSTMs solve that simpler RNNs cannot, particularly relevant for this medical abstract classification task?

- Concept: Attention mechanisms
  - Why needed here: Attention allows the model to focus on relevant parts of the input sequence when creating representations. The paper uses Scaled Dot-Product Attention after Bi-LSTM layers to weight the importance of different words and characters within sentences.
  - Quick check question: How does attention help when combining word and character embeddings compared to simple concatenation?

- Concept: Transfer learning with pre-trained language models
  - Why needed here: The paper uses BiomedBERT, pre-trained on PubMed data, to provide domain-specific semantic understanding. This leverages knowledge from large-scale medical text corpora to improve performance on the classification task.
  - Quick check question: Why might a domain-specific pre-trained model (BiomedBERT) be more effective than a general model for this medical abstract classification task?

## Architecture Onboarding

- Component map:
  - Sentence level: Sen-Model (4-branch LSTM network)
    - Word branch: GloVe embeddings → Bi-LSTM-Attention → encoded word embeddings
    - Character branch: Random character embeddings → Bi-LSTM-Attention → encoded character embeddings
    - Statistical branch: One-hot vectors → MLP → encoded statistical embeddings
    - Pre-trained branch: BioMedBERT → PubMed-based sentence embeddings
    - Fusion: Concatenate all encoded embeddings → Bi-LSTM → Dense → sentence embedding
  - Abstract level: Abs-Model (C-RNN + Logistic Regression)
    - Input: Sentence embeddings from same abstract
    - CNN: 2D convolutions with (8,3) kernels, 16 filters each
    - RNN: Bi-RNN decoder (Bi-LSTM or Bi-GRU depending on dataset)
    - Output: Abstract-level predictions via Logistic Regression
  - Segment level: Seg-Model (MLP with 5 fully-connected blocks)
    - Input: Fixed-length segments of Q=3 consecutive sentence embeddings
    - Architecture: Dense(512) → ELU → BN → Dropout → Dense(256) → ... → Dense(L) with Softmax
    - Output: Segment-level predictions
  - Fusion: Weighted combination of Abs-Model and Seg-Model predictions

- Critical path: Sen-Model → Sentence embeddings → Abs-Model and Seg-Model → Fusion → Final predictions

- Design tradeoffs:
  - Multiple feature branches increase model complexity but capture complementary information
  - Fixed segment length (Q=3) balances local context capture with computational efficiency
  - Weighted fusion requires hyperparameter tuning but allows balancing global vs local perspectives
  - Using both CNN and RNN at abstract level captures both local patterns and sequential dependencies

- Failure signatures:
  - Performance degradation on datasets with different abstract structures
  - Overfitting on smaller datasets due to model complexity
  - Poor generalization if BioMedBERT embeddings don't align well with the specific task
  - Suboptimal fusion weights if λabs and λseg are not properly tuned

- First 3 experiments:
  1. Test Sen-Model with individual feature branches (word only, char only, stat only) to verify complementary information capture
  2. Compare different segment lengths (Q=2, 3, 4) to find optimal local context size
  3. Test different fusion strategies (weighted averaging, gating mechanisms) to optimize combination of Abs-Model and Seg-Model predictions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different attention mechanisms within the SBA block affect sentence representation quality?
- Basis in paper: [inferred] The paper employs a Scaled Dot-Product Attention layer in the SBA block but doesn't explore alternative attention mechanisms.
- Why unresolved: The authors use a single attention mechanism without comparing its effectiveness against other attention types like self-attention or multi-head attention.
- What evidence would resolve it: Comparative experiments testing different attention mechanisms within the SBA block and their impact on F1 scores would clarify this.

### Open Question 2
- Question: What is the optimal segment size Q for the Seg-Model across different datasets?
- Basis in paper: [explicit] The paper sets Q to 3 based on empirical experiments but doesn't explore other segment sizes.
- Why unresolved: The authors fix the segment size without systematically evaluating how different values of Q affect model performance.
- What evidence would resolve it: Experiments varying Q across different datasets and analyzing the corresponding F1 scores would determine the optimal segment size.

### Open Question 3
- Question: How does the model perform on datasets with different label distributions or abstract structures?
- Basis in paper: [inferred] The paper evaluates the model on PubMed and NICTA-PIBOSO datasets but doesn't test on datasets with varying label distributions or abstract structures.
- Why unresolved: The authors don't explore the model's generalization capabilities to datasets with different characteristics.
- What evidence would resolve it: Testing the model on diverse datasets with varying label distributions and abstract structures would reveal its adaptability and robustness.

## Limitations

- The architectural details remain underspecified, including exact attention layer dimensions and CNN filter configurations, which could affect reproducibility.
- The fixed segment size of 3 sentences may not capture optimal local context patterns across all abstract structures, limiting the model's adaptability.
- The simple weighted fusion strategy assumes linear combination is optimal without exploring alternative gating mechanisms that might yield better results.

## Confidence

- **High confidence**: The fundamental approach of combining multiple linguistic features (word, character, statistical, pre-trained) for sentence representation is well-grounded in NLP literature and the empirical results show consistent improvements across all three datasets.
- **Medium confidence**: The specific architectural choices (4-branch design, C-RNN at abstract level, MLP at segment level) and their contribution to the reported performance gains, due to limited ablation studies and underspecified implementation details.
- **Low confidence**: Claims about the superiority of the specific fusion weights (λabs=1, λseg=0.2) and whether alternative fusion strategies could yield better results.

## Next Checks

1. Conduct systematic ablation studies removing each feature branch (word, character, statistical, pre-trained) individually to quantify their individual contributions to the final performance.

2. Test the model with different segment sizes (Q=2, 4, 5) to determine if the fixed size of 3 is optimal or if performance varies with different local context window sizes.

3. Implement and compare alternative fusion strategies beyond simple weighted averaging, such as attention-based fusion or gating mechanisms, to validate whether the chosen linear combination is indeed optimal.