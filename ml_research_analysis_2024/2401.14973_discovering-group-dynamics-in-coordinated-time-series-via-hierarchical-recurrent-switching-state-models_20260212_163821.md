---
ver: rpa2
title: Discovering group dynamics in coordinated time series via hierarchical recurrent
  switching-state models
arxiv_id: '2401.14973'
source_url: https://arxiv.org/abs/2401.14973
tags:
- state
- each
- time
- states
- system-level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces hierarchical switching recurrent dynamical
  models (HSRDMs) to model coordinated time series from multiple interacting entities.
  The key innovation is a two-level latent discrete state hierarchy: a system-level
  chain that provides top-down influence on entity-level chains, which in turn govern
  each observed time series.'
---

# Discovering group dynamics in coordinated time series via hierarchical recurrent switching-state models

## Quick Facts
- arXiv ID: 2401.14973
- Source URL: https://arxiv.org/abs/2401.14973
- Authors: Michael T. Wojnowicz, Kaitlin Gili, Preetish Rath, Eric Miller, Jeffrey Miller, Clifford Hancock, Meghan O'Donovan, Seth Elkin-Frankston, Tad T. Brunyé, Michael C. Hughes
- Reference count: 40
- One-line primary result: Hierarchical switching recurrent dynamical models (HSRDMs) can discover interpretable system-level dynamics and achieve competitive forecasting accuracy compared to larger neural network models while being more computationally efficient and parameter-efficient.

## Executive Summary
This paper introduces hierarchical switching recurrent dynamical models (HSRDMs) to model coordinated time series from multiple interacting entities. The key innovation is a two-level latent discrete state hierarchy: a system-level chain that provides top-down influence on entity-level chains, which in turn govern each observed time series. Both levels incorporate bottom-up recurrent feedback from observations, enabling situational adaptations. The model is trained unsupervised via efficient variational inference that scales linearly in the number of entities. Experiments on synthetic and real data (basketball player trajectories, marching band movements, and soldier heading directions) demonstrate that HSRDMs can discover interpretable system-level dynamics and achieve competitive forecasting accuracy compared to larger neural network models, while being more computationally efficient and parameter-efficient.

## Method Summary
The Hierarchical Switching Recurrent Dynamical Model (HSRDM) is a two-level latent state model where a system-level discrete state chain influences entity-level discrete state chains. Each entity's time series is governed by its own entity-level chain, while coordination across entities is mediated by the system-level chain. Both levels incorporate bottom-up recurrent feedback from observations, allowing the model to adapt to evolving situational context. The model is trained via variational inference with coordinate ascent, where the structured posterior maintains the model's temporal dependency structure while remaining affordable to fit via efficient dynamic programming. The inference algorithm scales linearly in the number of entities despite the fact that the HSRDM couples entities via the system-level sequence.

## Key Results
- HSRDMs can recover true system-level and entity-level state sequences in synthetic data with 100% accuracy, even when the number of entities increases from 3 to 64.
- On basketball player trajectory forecasting, HSRDM achieves competitive accuracy (0.164 root relative MSE) compared to larger neural network models (AgentFormer: 0.148, GroupNet: 0.161) while being more parameter-efficient.
- The system-level states learned by HSRDM on marching band data achieve 100% classification accuracy against ground truth system states, demonstrating interpretable discovery of coordinated behaviors.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The hierarchical two-level structure (system-level + entity-level states) allows modeling coordinated behavior without factorial explosion.
- Mechanism: System-level discrete states provide top-down coordination; entity-level states provide per-entity dynamics. Coordination is mediated by the system state, not by pairwise entity-entity interactions.
- Core assumption: Entity-level states are conditionally independent given the system state.
- Evidence anchors:
  - [abstract] "system-level chain that provides top-down influence on entity-level chains"
  - [section 2] "We assume that the system-level chain is the sole mediator of cross-entity coordination; each entity-level chain is conditionally independent of other entities given the system-level chain."

### Mechanism 2
- Claim: Recurrent feedback from observations to latent states at both levels allows the model to adapt to evolving situational context in a bottom-up manner.
- Mechanism: System-level transitions depend on all entity observations; entity-level transitions depend on the entity's own observations and the current system state.
- Core assumption: Recent observations contain sufficient information to inform the next latent state.
- Evidence anchors:
  - [abstract] "recurrent feedback from the observations to the latent chains at both entity and system levels allows recent situational context to inform how dynamics unfold"
  - [section 2] "Transitions to the next system state depend via blue arrows on the current system state and recurrent feedback from observations of all entities... Transitions to the next entity state depend via red arrows on the next system-level state... and recurrent feedback from that entity's observations."

### Mechanism 3
- Claim: Variational inference with structured posteriors scales linearly in the number of entities, avoiding exponential complexity.
- Mechanism: The mean-field approximation q(s0:T, z(1:J)0:T) = q(s0:T) q(z(1:J)0:T) allows per-entity updates to be independent given the system-level posterior.
- Core assumption: The approximation q(z(1:J)0:T) ≈ ∏_j q(z(j)0:T) is reasonable.
- Evidence anchors:
  - [section 3] "This means inference is linear in the number of entities J, despite the fact that the HSRDM couples entities via the system-level sequence."
  - [section 3] "Each chain's posterior maintains the model's temporal dependency structure while remaining affordable to fit via efficient dynamic programming."

## Foundational Learning

- Concept: Hidden Markov Models (HMMs) and their extensions (AR-HMMs, rAR-HMMs)
  - Why needed here: The HSRDM builds on HMMs at both levels; understanding state estimation (forward-backward) is essential for grasping the inference algorithm.
  - Quick check question: How does the forward algorithm compute p(zt | x1:t) recursively in an HMM?

- Concept: Variational inference and coordinate ascent
  - Why needed here: The model parameters and posteriors are estimated via CAVI; knowing how to optimize the ELBO and update factors is crucial.
  - Quick check question: In coordinate ascent variational inference, what guarantees that alternating updates to q and θ improve the objective?

- Concept: Recurrent neural networks and feedback mechanisms
  - Why needed here: The recurrence in transitions is analogous to RNNs; understanding how past observations influence current dynamics helps interpret the model.
  - Quick check question: In an RNN, how does the hidden state at time t depend on the observation at time t-1?

## Architecture Onboarding

- Component map:
  - System-level chain (L discrete states) -> Entity-level chains (K discrete states per entity) -> Observed time series
  - System-level transition model (Cat-GLM with bottom-up recurrence)
  - Entity-level transition model (Cat-GLM with top-down influence and bottom-up recurrence)
  - Emission models (state-conditioned autoregressions)
  - Variational inference with structured posteriors

- Critical path:
  1. Initialize parameters (smart init via bottom-level rARHMMs, then top-level ARHMM)
  2. Alternate VES (system posterior) and VEZ (entity posteriors) steps
  3. Update parameters (M-step) using sufficient statistics from posteriors
  4. Check ELBO convergence; repeat until stable

- Design tradeoffs:
  - Discrete vs. continuous latent states: discrete yields interpretability but less flexibility
  - Two-level vs. flat: two-level reduces complexity from exponential to linear in J
  - Recurrence vs. none: recurrence increases flexibility but adds computational cost
  - Number of states (L, K): more states increase expressiveness but risk overfitting and cost

- Failure signatures:
  - Poor segmentation: likely due to too few states or bad initialization
  - Unstable training: ELBO not converging—check priors, initialization, or model specification
  - Overfitting: high train fit but poor forecast—reduce states or add regularization
  - Inefficient scaling: runtime growing faster than linear—likely bug in implementation or unnecessary recomputation

- First 3 experiments:
  1. Fit HSRDM to synthetic FigureEight data (3 entities, 2 system states, 2 entity states) and verify recovery of true segmentation
  2. Compare HSRDM to rAR-HMM and no-recurrence ablation on basketball forecasting (10 players, 5 system states, 10 entity states)
  3. Evaluate system-level interpretability on MarchingBand (64 entities, 6 system states, 4 entity states) by measuring classification accuracy against ground truth

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the HSRDM's performance scale when the number of entities J increases beyond 200?
- Basis in paper: [explicit] The paper demonstrates the model can handle J=200 entities with 100% accuracy on a synthetic MarchingBand task, but notes that scaling to many more entities would require extensions like processing minibatches of entities.
- Why unresolved: The paper only provides results for J=64 and J=200 entities. Real-world applications like crowd modeling or large-scale multi-agent systems often involve hundreds or thousands of entities.
- What evidence would resolve it: Experiments demonstrating the HSRDM's performance on datasets with 500, 1000, or more entities, including computational runtime analysis and accuracy metrics.

### Open Question 2
- Question: How sensitive is the HSRDM to the choice of the number of system states L and entity states K?
- Basis in paper: [explicit] The paper sets L and K based on informal experimentation and domain knowledge (e.g., L=5, K=10 for basketball), but notes that formal setting based on validation sets is left to future work.
- Why unresolved: The paper doesn't systematically investigate how performance varies with different L and K values. Poor choices could lead to underfitting or overfitting.
- What evidence would resolve it: A comprehensive sensitivity analysis showing model performance across a grid of L and K values for various datasets, including metrics like forecasting error and state segmentation accuracy.

### Open Question 3
- Question: How does the HSRDM compare to neural network models when abundant data is available?
- Basis in paper: [explicit] The paper suggests that when abundant data exists, flexible function approximation methods for forecasting may be preferred, but doesn't provide empirical comparisons.
- Why unresolved: The paper focuses on sample-efficient performance with limited data (a few hundred recordings or less), but doesn't test scenarios with large datasets.
- What evidence would resolve it: Head-to-head comparisons between HSRDM and neural network baselines (e.g., AgentFormer, GroupNet) on datasets with thousands of sequences, measuring both forecasting accuracy and computational efficiency.

## Limitations
- The empirical validation is limited to specific domains with small sample sizes, raising questions about generalizability to broader applications.
- The conditional independence assumption between entity-level chains given the system state may not hold in scenarios with direct entity-entity interactions, potentially limiting model applicability.
- Recurrence functions are only briefly specified without systematic comparison of different functional forms, leaving ambiguity about optimal design choices.

## Confidence

- **High confidence**: The hierarchical structure enabling linear scaling (Mechanism 1), the variational inference algorithm and its complexity claims (Mechanism 3), and the basic synthetic data recovery results.
- **Medium confidence**: The effectiveness of bottom-up recurrent feedback for situational adaptation (Mechanism 2), given that recurrence functions are only briefly specified and their impact not thoroughly explored.
- **Medium confidence**: The interpretability and practical utility of discovered system-level states, as validation relies on indirect measures (classification accuracy, qualitative assessment) rather than direct interpretability tests.

## Next Checks
1. **Robustness to conditional independence violation**: Systematically evaluate model performance when entities have direct interactions not mediated by the system state, comparing against models that allow pairwise entity dependencies.

2. **Recurrence function sensitivity analysis**: Conduct controlled experiments varying the functional forms of g and f across the synthetic and real datasets to determine which forms yield optimal performance and interpretability.

3. **Scalability benchmark**: Measure actual runtime and memory usage on datasets with increasing numbers of entities (e.g., 10 → 100 → 1000) to verify the claimed linear scaling holds in practice, not just theoretically.