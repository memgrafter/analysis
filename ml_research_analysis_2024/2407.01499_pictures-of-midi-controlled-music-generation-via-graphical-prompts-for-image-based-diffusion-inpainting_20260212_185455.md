---
ver: rpa2
title: 'Pictures Of MIDI: Controlled Music Generation via Graphical Prompts for Image-Based
  Diffusion Inpainting'
arxiv_id: '2407.01499'
source_url: https://arxiv.org/abs/2407.01499
tags:
- inpainting
- diffusion
- music
- generation
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Pictures of MIDI, a method for controlled
  music generation via graphical prompts using image-based diffusion inpainting. The
  approach uses an Hourglass Diffusion Transformer (HDiT) trained on MIDI piano roll
  images, allowing users to draw masked regions for inpainting and enhance note generation
  by repainting with extra noise.
---

# Pictures Of MIDI: Controlled Music Generation via Graphical Prompts for Image-Based Diffusion Inpainting

## Quick Facts
- arXiv ID: 2407.01499
- Source URL: https://arxiv.org/abs/2407.01499
- Authors: Scott H. Hawley
- Reference count: 0
- Primary result: Hourglass Diffusion Transformer (HDiT) trained on MIDI piano roll images enables controlled music generation through graphical inpainting prompts

## Executive Summary
This paper introduces Pictures of MIDI, a method for controlled music generation via graphical prompts using image-based diffusion inpainting. The approach uses an Hourglass Diffusion Transformer (HDiT) trained on MIDI piano roll images, allowing users to draw masked regions for inpainting and enhance note generation by repainting with extra noise. The HDiT's linear scaling with pixel count enables efficient generation in pixel space, eliminating the need for autoencoders. The method supports complex mask geometries and provides intuitive controls. Results show performance on par with prior work, with longer context windows and no autoencoder, enabling users to specify musical structures like rising or falling melodies.

## Method Summary
The method uses an Hourglass Diffusion Transformer (HDiT) operating directly in pixel space on MIDI piano roll images. The POP909 MIDI dataset of Chinese pop songs is normalized to 120 BPM and rendered as piano roll images at 16th notes per pixel, with note durations in green and onsets in red. Chord information is encoded as borders. The HDiT is trained on random 512-pixel windows from these images. During generation, users draw inpainting masks on the piano roll interface, and the model generates music matching these constraints. The RePaint technique with parameter U > 1 is used to increase note density in masked regions. The approach eliminates the need for autoencoders through direct pixel-space operations.

## Key Results
- Hourglass Diffusion Transformer operates efficiently in pixel space without requiring a VAE
- Users can draw arbitrary mask shapes to control musical structure, including rising or falling melodies
- Performance matches prior results while supporting longer context windows
- Repainting with extra noise increases note density but may reduce musical coherence if overused
- Complex mask geometries provide intuitive control but work best when within training data distribution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The HDiT model operates directly in pixel space without needing a VAE, enabling efficient generation at longer sequence contexts
- Mechanism: By using a transformer architecture that scales linearly with pixel count, the model can handle larger images (512x128) without the compression and decompression overhead of a VAE
- Core assumption: Pixel space operations maintain sufficient information density for musical representation
- Evidence anchors:
  - [abstract]: "The non-latent HDiT's linear scaling with pixel count allows efficient generation in pixel space"
  - [section]: "HDiT operates in pixel space and requires no VAE"
- Break condition: If musical information becomes too sparse in pixel space at larger dimensions, quality degrades despite linear scaling

### Mechanism 2
- Claim: Repainting with extra noise increases note density in user-specified regions during inpainting
- Mechanism: By repeatedly introducing noise during the reverse diffusion process (parameter U > 1), the model explores more variations and is more likely to generate notes matching the drawn mask shapes
- Core assumption: Higher noise re-introduction increases probability of generating notes in masked regions
- Evidence anchors:
  - [section]: "we increase U as a way to generate higher note densities in drawn inpainting regions"
  - [section]: "we get more notes but they increasingly show little musical coherence"
- Break condition: If U becomes too large, generated notes lose musical coherence and appear random rather than structured

### Mechanism 3
- Claim: Complex mask geometries enable intuitive user control over musical structure
- Mechanism: Users can draw arbitrary shapes on the piano roll interface, and the model uses these as constraints during inpainting to generate music matching the specified pitch contours
- Core assumption: The model can learn to map arbitrary mask shapes to musically coherent outputs
- Evidence anchors:
  - [abstract]: "enabling users to specify musical structures like rising or falling melodies"
  - [section]: "users could sketch a rough target melody profile which the model then turns into a fully-realized composition"
- Break condition: If mask shapes fall too far outside training data distribution, model produces silence or random notes

## Foundational Learning

- Concept: Diffusion models and the reverse process
  - Why needed here: The paper uses diffusion models for generating music, which requires understanding how noise is gradually removed from random data
  - Quick check question: What happens at each step of the reverse diffusion process?

- Concept: Transformer architectures and attention mechanisms
  - Why needed here: The HDiT is a transformer-based model, so understanding self-attention and positional encoding is crucial
  - Quick check question: How does the transformer handle the "folding" of the image from 512x128 to 256x256?

- Concept: MIDI piano roll representation
  - Why needed here: The model operates on MIDI piano roll images, so understanding this format is essential
  - Quick check question: How are note durations and velocities encoded in the green pixel values?

## Architecture Onboarding

- Component map: MIDI data → Piano roll rendering → HDiT model → Diffusion sampling with inpainting → Post-processing
- Critical path: Training data preparation → Model training (48 hours on A6000 GPUs) → Inference with inpainting masks → Evaluation
- Design tradeoffs: No VAE enables longer sequences but requires more memory; complex masks give control but work poorly outside training distribution
- Failure signatures: Low note density in masked regions, chord markers not matching notes, loss of musical coherence with high RePaint values
- First 3 experiments:
  1. Generate undirectected outputs to verify basic model functionality (Figure 3)
  2. Test simple inpainting (melody given accompaniment) to validate core inpainting capability (Figure 4)
  3. Try increasing RePaint parameter U to observe effect on note density (Figure 9)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the effectiveness of the Hourglass Diffusion Transformer (HDiT) be further improved by integrating chord-based conditioning instead of relying solely on inpainting masks?
- Basis in paper: [inferred] The paper mentions that inpainting alone does not work effectively for chord-based conditioning and suggests that chord-based conditioning demonstrated by Min et al. might be more effective.
- Why unresolved: The paper did not explore integrating chord-based conditioning with HDiT, leaving it as an area for further study.
- What evidence would resolve it: Conducting experiments that integrate chord-based conditioning with HDiT and comparing the results with the current inpainting-only approach.

### Open Question 2
- Question: How does the performance of the HDiT model change when scaling up the image size beyond 512 pixels, and what are the computational trade-offs?
- Basis in paper: [explicit] The paper suggests that future studies may include scaling images up but does not provide results or analysis of such scaling.
- Why unresolved: The paper does not provide empirical data on the performance and computational requirements of scaling images beyond 512 pixels.
- What evidence would resolve it: Running experiments with larger image sizes and analyzing the impact on performance, memory usage, and computational time.

### Open Question 3
- Question: What are the limitations of using inpainting masks for complex musical structures, and how can these be addressed?
- Basis in paper: [explicit] The paper discusses the use of complex inpainting masks but notes that extreme cases often produce low-probability events and may not yield musically pleasing results.
- Why unresolved: The paper identifies the issue but does not provide solutions or further exploration into overcoming these limitations.
- What evidence would resolve it: Developing and testing new methods or techniques to improve the quality of music generated from complex inpainting masks, such as advanced guidance techniques or additional training data.

## Limitations
- Subjective evaluation metrics lack quantitative baseline comparisons
- Chord markers are not effectively matched by the current inpainting approach
- Complex mask geometries produce poor results when outside training data distribution
- No systematic analysis of how mask complexity affects musical quality

## Confidence

- **High Confidence**: The core mechanism of using HDiT for pixel-space diffusion inpainting (Mechanism 1) is well-supported by the architecture description and the linear scaling claim is consistent with transformer properties
- **Medium Confidence**: The effectiveness of the RePaint technique for increasing note density (Mechanism 2) is demonstrated, but the tradeoff with musical coherence needs more systematic exploration
- **Medium Confidence**: The claim about supporting complex mask geometries for intuitive control (Mechanism 3) is conceptually sound but the paper acknowledges limitations when masks fall outside training distribution

## Next Checks

1. **Quantitative Benchmark Comparison**: Implement a controlled experiment comparing generated music quality against at least two baseline methods using standardized metrics (e.g., BLEU scores for melody similarity, rhythm consistency measures) to validate the "on par with prior results" claim

2. **Chord Marker Coherence Test**: Systematically evaluate whether the model can generate musically coherent outputs when chord markers are provided as part of the inpainting masks, measuring both pitch-class alignment and harmonic progression quality

3. **Mask Distribution Robustness**: Test the model's performance across a graduated spectrum of mask geometries from those closely matching training data to increasingly abstract patterns, quantifying the degradation in musical quality to establish the practical limits of user control