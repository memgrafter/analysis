---
ver: rpa2
title: Findings of the WMT 2024 Shared Task on Chat Translation
arxiv_id: '2410.11624'
source_url: https://arxiv.org/abs/2410.11624
tags:
- translation
- evaluation
- language
- systems
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents the third edition of the Chat Translation\
  \ Shared Task, which focuses on translating bilingual customer support conversations\
  \ with an emphasis on the impact of conversation context on translation quality.\
  \ The task expanded to include five language pairs: English \u2194 German, English\
  \ \u2194 French, English \u2194 Brazilian Portuguese, English \u2194 Dutch, and\
  \ English \u2194 Korean."
---

# Findings of the WMT 2024 Shared Task on Chat Translation
## Quick Facts
- arXiv ID: 2410.11624
- Source URL: https://arxiv.org/abs/2410.11624
- Reference count: 8
- This paper presents the third edition of the Chat Translation Shared Task, focusing on bilingual customer support conversations and the impact of conversation context on translation quality.

## Executive Summary
The WMT 2024 Chat Translation Shared Task expanded to include five language pairs and evaluated systems using both automatic metrics and human judgments. The task emphasized the importance of conversation context in translating bilingual customer support conversations. Systems were assessed through a combination of COMET, BLEU, CHR F, and CONTEXT COMET QE metrics alongside direct human assessment frameworks. The best-performing systems demonstrated that fine-tuning large language models on in-domain chat data, utilizing contextual information from previous turns, and applying quality-aware decoding strategies significantly improved translation quality.

## Method Summary
The shared task evaluated translation systems across five language pairs using a dual evaluation approach. Automatic metrics including COMET, BLEU, CHR F, and CONTEXT COMET QE were combined with human judgments through a direct assessment framework. Systems competed by leveraging large language models fine-tuned on chat-specific data, incorporating conversation context from previous turns, and implementing quality-aware decoding strategies. The evaluation aimed to assess both translation accuracy and the systems' ability to maintain conversational coherence across turns.

## Key Results
- UNBABEL-IT achieved the best results across most language pairs and evaluation criteria
- Systems using fine-tuned LLMs with contextual information and quality-aware decoding performed best
- The evaluation successfully combined automatic metrics with human judgments to assess translation quality

## Why This Works (Mechanism)
The effectiveness of top-performing systems stems from their ability to leverage large language models fine-tuned on domain-specific chat data. By incorporating contextual information from previous conversation turns, these systems maintain conversational coherence and produce more natural translations. Quality-aware decoding strategies help optimize translation outputs for both accuracy and fluency, addressing the unique challenges of chat translation where context and conversational flow are critical.

## Foundational Learning
1. **Large Language Models (LLMs)** - Pre-trained transformer models fine-tuned on specific domains. Why needed: Provides strong baseline translation capabilities. Quick check: Verify model architecture and pre-training data size.
2. **Contextual Information Integration** - Using previous conversation turns as additional input. Why needed: Maintains conversational coherence across translations. Quick check: Confirm context window size and implementation method.
3. **Quality-aware Decoding** - Decoding strategies that optimize for translation quality metrics. Why needed: Improves translation accuracy and fluency. Quick check: Review decoding algorithm and quality estimation integration.
4. **Direct Assessment Framework** - Human evaluation methodology for translation quality. Why needed: Provides reliable quality judgments beyond automatic metrics. Quick check: Verify rater training procedures and inter-annotator agreement measures.
5. **Automatic Metrics (COMET, BLEU, CHR F)** - Standard evaluation metrics for translation quality. Why needed: Provides quantitative assessment of system performance. Quick check: Confirm metric implementation and parameter settings.
6. **Language Pair Diversity** - Evaluation across five different language pairs. Why needed: Tests system generalization across language families. Quick check: Review language pair selection criteria and data characteristics.

## Architecture Onboarding
Component map: Data Preprocessing -> LLM Fine-tuning -> Context Integration -> Quality-aware Decoding -> Evaluation
Critical path: The fine-tuning process with chat-specific data followed by context integration represents the most critical path for achieving high-quality translations.
Design tradeoffs: Balancing computational efficiency with translation quality, managing context window limitations, and optimizing between automatic metrics and human preferences.
Failure signatures: Poor handling of conversational context, failure to maintain translation consistency across turns, and inadequate adaptation to chat-specific language patterns.
First experiments: 1) Fine-tune baseline LLM on chat data and evaluate translation quality, 2) Implement context integration and measure improvement in coherence, 3) Apply quality-aware decoding and compare against standard decoding approaches.

## Open Questions the Paper Calls Out
None

## Limitations
- The paper lacks statistical significance testing for pairwise comparisons between systems
- Evaluation methodology details are sparse, particularly regarding rater training and calibration procedures
- The generalizability of findings is limited by the focus on customer support conversations

## Confidence
- Performance claims (High): Results are supported by multiple evaluation metrics and direct human assessment
- Methodology claims (Medium): Evaluation setup is described but lacks detail on reliability measures
- Impact claims (Medium): System advantages are demonstrated but not thoroughly analyzed or contextualized

## Next Checks
1. Conduct statistical significance testing on human evaluation scores to verify performance differences between top systems
2. Perform ablation studies to quantify the contribution of contextual information and quality-aware decoding to overall performance
3. Evaluate systems on out-of-domain chat data to assess generalizability beyond customer support conversations