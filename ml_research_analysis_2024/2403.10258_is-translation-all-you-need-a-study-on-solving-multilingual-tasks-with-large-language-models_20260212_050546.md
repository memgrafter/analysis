---
ver: rpa2
title: Is Translation All You Need? A Study on Solving Multilingual Tasks with Large
  Language Models
arxiv_id: '2403.10258'
source_url: https://arxiv.org/abs/2403.10258
tags:
- language
- languages
- llms
- tasks
- native
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how different prompting strategies impact multilingual
  performance in large language models. The authors compare translating user queries
  into English with prompting directly in native languages, using both standard NLP
  benchmarks and real-world user queries from diverse cultures.
---

# Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models

## Quick Facts
- arXiv ID: 2403.10258
- Source URL: https://arxiv.org/abs/2403.10258
- Authors: Chaoqun Liu; Wenxuan Zhang; Yiran Zhao; Anh Tuan Luu; Lidong Bing
- Reference count: 40
- Translation into English generally improves performance on NLP tasks for English-centric models, but native-language prompts are more effective for culture-related tasks requiring nuanced understanding.

## Executive Summary
This paper examines how different prompting strategies impact multilingual performance in large language models. The authors compare translating user queries into English with prompting directly in native languages, using both standard NLP benchmarks and real-world user queries from diverse cultures. For English-centric models like ChatGPT and Llama, translating into English generally yields better results on NLP tasks. However, for culture-related tasks requiring nuanced language understanding, native-language prompts prove more effective, particularly with advanced models. The study also evaluates non-English-centric models (Qwen, Yi) and finds that they show mixed behavior, sometimes performing better with native-language prompts. Overall, the results demonstrate that translation is not universally optimal and highlight the need for more comprehensive multilingual evaluation approaches and models designed for true multilingual proficiency.

## Method Summary
The study evaluates multiple prompting strategies (native instructions, English instructions, chain-of-thought, cross-lingual-thought, and translation-based methods) across zero-shot prompting scenarios. The evaluation uses NLP benchmarks (MGSM, XCOPA, XNLI, PAWS-X, MKQA, XL-Sum) covering 24 languages, real-world user queries from ShareGPT in 10 languages, and culture-related tasks from M3Exam. Six LLMs are tested including ChatGPT, Llama-2-70B-Chat, Mistral-7B-Instruct, Llama-2-13B-chat, bloomz-7b1, Qwen1.5-72B-Chat, and Yi-34B-Chat. Performance is measured using accuracy, token overlap F1 score, ROUGE-1 score, and win rates.

## Key Results
- Translation into English improves performance on NLP tasks for English-centric models like ChatGPT and Llama
- Native-language prompts outperform translation for culture-related tasks requiring nuanced cultural understanding
- Non-English-centric models (Qwen, Yi) show mixed behavior, sometimes performing better with native-language prompts
- English-centric LLMs internalize linguistic patterns primarily in English, enabling better comprehension when inputs are translated

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translation into English improves performance on NLP tasks for English-centric LLMs because their training corpus is disproportionately English-dominant.
- Mechanism: English-centric LLMs internalize linguistic patterns, reasoning structures, and problem-solving templates primarily in English. When non-English inputs are translated to English, they map into the model's strongest representational space, enabling better comprehension and response generation.
- Core assumption: The quality of translation preserves semantic equivalence and does not introduce cultural distortions that mislead the model.
- Evidence anchors:
  - [abstract] "While prior works have leveraged this bias to enhance multilingual performance through translation, they have been largely limited to natural language processing (NLP) tasks."
  - [section 2.2] "TRANS -GOOGLE , despite simple, demonstrates the highest overall performance across various models and tasks."
  - [corpus] Found 25 related papers; top FMR=0.644 for "Breaking the Language Barrier: Can Direct Inference Outperform Pre-Translation in Multilingual LLM Applications?" suggests translation remains a strong baseline but is being actively questioned.
- Break condition: When tasks require deep cultural knowledge embedded in the source language, translation may distort or lose critical context, leading to degraded performance.

### Mechanism 2
- Claim: Native-language prompting outperforms translation for culture-related tasks because it preserves nuanced cultural and linguistic context.
- Mechanism: Culture-related tasks embed implicit knowledge tied to local language, idioms, and societal norms. Native-language prompts allow the model to access these contextual cues directly, avoiding the semantic loss that occurs during translation.
- Core assumption: The model has sufficient multilingual proficiency to understand and reason in the native language without degradation.
- Evidence anchors:
  - [abstract] "For culture-related tasks that need deep language understanding, prompting in the native language proves more effective as it better captures the nuances of culture and language."
  - [section 3.2] "ChatGPT's performance varies across languages... For high-resource languages like Japanese, Chinese, and Spanish, original queries have a higher win rate."
  - [corpus] FMR=0.664 for "Selected Languages are All You Need for Cross-lingual Truthfulness Transfer" indicates cross-lingual alignment is a key challenge.
- Break condition: If the model's proficiency in the native language is insufficient, native prompting may perform worse than translation.

### Mechanism 3
- Claim: Non-English-centric LLMs show mixed behavior because their multilingual architecture processes languages differently across layers.
- Mechanism: Unlike English-centric models that funnel all languages through English-centric representations, non-English-centric models maintain distinct language-specific representations. This leads to varied effectiveness of translation depending on task dependency on language-specific knowledge.
- Core assumption: Layerwise language distribution affects task performance based on whether the task requires cross-lingual integration or language-specific reasoning.
- Evidence anchors:
  - [section 2.3] "While the hidden representations of Qwen1.5-7B-Chat are mainly in Chinese, those of Llama-2-7B-Chat are in various other languages."
  - [section 3.3] "Translation helps Llama-2-70B-chat in all the languages, suggesting that the model's underperformance is due to poor language understanding rather than limitations of cultural knowledge."
  - [corpus] No direct corpus evidence; inference based on experimental layer analysis.
- Break condition: When the task is less language-dependent (e.g., arithmetic reasoning), translation may still help even in non-English-centric models.

## Foundational Learning

- Concept: Chain-of-thought prompting
  - Why needed here: Helps decompose complex reasoning tasks into sequential steps, improving accuracy especially for tasks requiring multi-step logic.
  - Quick check question: Does adding "Let's think step by step" improve accuracy on MGSM arithmetic problems compared to basic prompting?

- Concept: Translation quality metrics (BLEU)
  - Why needed here: Correlates translation fidelity with downstream task performance; higher BLEU scores generally lead to better task accuracy.
  - Quick check question: Does improving BLEU score from 40 to 50 on MGSM questions correlate with increased accuracy?

- Concept: Language distance measures
  - Why needed here: Quantifies syntactic and semantic similarity between English and target languages, explaining performance variance across languages.
  - Quick check question: Do languages with higher syntactic distance from English show systematically lower accuracy on XNLI tasks?

## Architecture Onboarding

- Component map: Task sampling → Prompting strategy selection → LLM inference → Translation (optional) → Response translation (if needed) → Quality assessment (human or LLM-as-judge)
- Critical path: Task → Prompt strategy → LLM → Output formatting → Evaluation
- Design tradeoffs: Translation adds latency and potential semantic loss vs. improved comprehension in English-centric models; native prompting requires stronger multilingual proficiency
- Failure signatures: Consistent underperformance on culture-related tasks with translation; high variance across languages; layerwise language distribution showing unexpected language mixing
- First 3 experiments:
  1. Compare basic prompting vs. translation on MGSM for high-resource vs. low-resource languages
  2. Test native vs. translated prompts on ShareGPT subset requiring cultural knowledge
  3. Analyze layerwise language distribution for non-English-centric models on Chinese prompts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of translation depend on the language distance between English and the target language for non-English-centric LLMs?
- Basis in paper: Explicit - The paper analyzes correlation between MGSM accuracy and language distances for English-centric LLMs, but doesn't examine non-English-centric models
- Why unresolved: The study only investigates this correlation for English-centric LLMs (ChatGPT, Llama-2) and finds syntactic distance significant, but doesn't test if the same pattern holds for non-English-centric models like Qwen or Yi
- What evidence would resolve it: Experimental results showing accuracy scores of non-English-centric LLMs across languages with varying distances from English, compared with their performance patterns

### Open Question 2
- Question: Are there specific linguistic features that determine when translation is more effective than native-language prompts?
- Basis in paper: Inferred - The paper shows mixed results across tasks and languages, suggesting task-specific factors but doesn't identify linguistic predictors
- Why unresolved: While the study demonstrates that translation effectiveness varies by task type (NLP vs culture-related) and language, it doesn't identify specific linguistic features (e.g., syntactic complexity, morphological richness) that predict translation success
- What evidence would resolve it: Systematic analysis linking specific linguistic features of tasks/languages to translation performance across multiple LLMs

### Open Question 3
- Question: What is the optimal translation strategy for culture-related tasks that balances information preservation with comprehension enhancement?
- Basis in paper: Explicit - The paper shows that for culture-related tasks, native-language prompts often outperform translation, but doesn't explore intermediate strategies
- Why unresolved: The study only compares binary choices (translate vs. don't translate) but doesn't investigate nuanced strategies like selective translation of key terms or hybrid approaches that could better handle cultural content
- What evidence would resolve it: Comparative experiments testing various translation strategies (partial translation, cultural annotation preservation, etc.) on culture-related benchmarks

### Open Question 4
- Question: How does the layerwise language distribution in LLMs correlate with their translation effectiveness across different language pairs?
- Basis in paper: Explicit - The paper shows different layerwise language distributions between Llama-2 and Qwen, but doesn't link this to translation performance
- Why unresolved: While the study observes that Qwen processes Chinese prompts more in Chinese while Llama-2 shows more language diversity in hidden states, it doesn't investigate whether this architectural difference predicts translation effectiveness for different language pairs
- What evidence would resolve it: Correlation analysis between layerwise language distribution patterns and translation accuracy across multiple language pairs and model architectures

## Limitations
- Evaluation focuses on zero-shot prompting without exploring fine-tuning approaches that might alter translation benefits
- Culture-related task subset and ShareGPT queries represent limited samples of real-world multilingual interactions
- Translation quality variations across low-resource languages could significantly impact results but aren't fully characterized

## Confidence
- High Confidence: Translation into English improves performance on standard NLP benchmarks for English-centric models like ChatGPT and Llama
- Medium Confidence: Native-language prompting outperforms translation for culture-related tasks
- Medium Confidence: Non-English-centric models exhibit mixed behavior due to distinct language-specific representations

## Next Checks
1. **Cross-lingual semantic preservation analysis:** Quantify how translation quality (using metrics like BLEU, TER) correlates with task performance across all 24 languages, particularly focusing on low-resource languages where translation artifacts might be most pronounced.

2. **Layerwise ablation study for non-English-centric models:** Systematically disable or mask specific layers in Qwen and Yi models to identify which layers contribute most to language-specific performance differences when using native vs. translated prompts.

3. **Extended culture-related task evaluation:** Expand beyond M3Exam to include additional benchmarks requiring cultural knowledge (e.g., social reasoning tasks, idiom interpretation) to validate whether native-language prompting consistently outperforms translation across diverse cultural reasoning scenarios.