---
ver: rpa2
title: 'GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs'
arxiv_id: '2408.15300'
source_url: https://arxiv.org/abs/2408.15300
tags:
- quantization
- noise
- salient
- weights
- gift-sw
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GIFT-SW, a parameter-efficient fine-tuning
  method for LLMs that selectively updates salient weights while injecting noise into
  non-salient ones. The method identifies salient columns using a generalized sensitivity
  metric and applies quantization noise injection only to non-salient columns during
  training.
---

# GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs

## Quick Facts
- arXiv ID: 2408.15300
- Source URL: https://arxiv.org/abs/2408.15300
- Reference count: 27
- Primary result: GIFT-SW achieves state-of-the-art parameter-efficient fine-tuning performance while using only 3% of parameters compared to full fine-tuning

## Executive Summary
GIFT-SW introduces a novel parameter-efficient fine-tuning method that selectively updates salient weights while injecting Gaussian noise into non-salient ones. The method identifies important weight columns using a generalized sensitivity metric and applies quantization noise injection only to non-salient columns during training. Experiments demonstrate GIFT-SW outperforms full fine-tuning and modern PEFT methods like LoRA and DoRA across most zero-shot tasks while using significantly fewer parameters.

## Method Summary
GIFT-SW operates by first computing a sensitivity metric for each column in the weight matrix using a calibration dataset. The method identifies top-k salient columns based on this metric, then during training injects quantization noise into non-salient weights while updating only the salient weights. This selective updating approach reduces computational overhead while maintaining performance. The sensitivity metric generalizes previous approaches by combining weight perturbation and input feature information, and adapts to different quantization precisions.

## Key Results
- Achieves superior zero-shot performance compared to full fine-tuning and LoRA/DoRA methods on most tasks
- Uses only 3% of parameters compared to full fine-tuning while matching or exceeding performance
- Demonstrates better stability when scaling dataset size compared to low-rank adapters
- Effectively recovers performance of quantized models while keeping salient weights in full precision

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GIFT-SW selectively updates salient weights while injecting noise into non-salient weights to improve fine-tuning efficiency and performance.
- Mechanism: The method identifies salient columns using a generalized sensitivity metric and applies quantization noise injection only to non-salient columns during training.
- Core assumption: A small subset of salient weights significantly impacts model performance and can be effectively identified using sensitivity metrics.
- Evidence anchors: [abstract], [section 3.2]
- Break condition: If the sensitivity metric fails to accurately identify salient weights, the method's effectiveness would be compromised.

### Mechanism 2
- Claim: Noise injection during fine-tuning improves model performance and robustness.
- Mechanism: By injecting quantization noise into non-salient weights, GIFT-SW simulates the quantization process during training, making the model more robust to quantization and preventing overfitting to specific weight values.
- Core assumption: Perturbed gradient descent with noise injection can stabilize convergence and improve model robustness.
- Evidence anchors: [section 2.4], [section 3.2]
- Break condition: If noise injection levels are too high, it may destabilize training or cause the model to converge to suboptimal solutions.

### Mechanism 3
- Claim: The generalized sensitivity metric effectively identifies salient columns across different models and quantization levels.
- Mechanism: The sensitivity metric (Equation 4) combines weight perturbation and input feature information to rank column importance.
- Core assumption: The relationship between weight sensitivity and quantization error is consistent across different layers and models.
- Evidence anchors: [section 3.1], [section 6.1]
- Break condition: If the metric fails to generalize across different model architectures or quantization schemes, its effectiveness would be limited.

## Foundational Learning

- Concept: Parameter-efficient fine-tuning (PEFT)
  - Why needed here: GIFT-SW is a PEFT method that aims to reduce computational resources while maintaining performance.
  - Quick check question: What is the main advantage of PEFT methods compared to full fine-tuning?

- Concept: Quantization and noise injection
  - Why needed here: GIFT-SW uses quantization noise injection to simulate the quantization process during training and improve model robustness.
  - Quick check question: How does noise injection during training help improve model performance?

- Concept: Sensitivity metrics for weight importance
  - Why needed here: GIFT-SW relies on sensitivity metrics to identify salient columns for selective updating.
  - Quick check question: What information do sensitivity metrics typically combine to rank weight importance?

## Architecture Onboarding

- Component map: Pre-trained LLM -> Sensitivity Metric Calculation -> Salient Column Identification -> Noise Injection + Selective Weight Updates -> Fine-tuned LLM

- Critical path:
  1. Compute sensitivity metric for each column using calibration dataset
  2. Identify top-k salient columns
  3. During training: inject noise into non-salient weights, update only salient weights
  4. Evaluate performance on downstream tasks

- Design tradeoffs:
  - Number of salient columns (k) vs. performance and efficiency
  - Noise injection level vs. training stability
  - Sensitivity metric choice vs. generalizability across models

- Failure signatures:
  - Poor performance: sensitivity metric may not be identifying truly salient weights
  - Unstable training: noise injection levels may be too high
  - High computational cost: too many salient columns selected

- First 3 experiments:
  1. Compare different sensitivity metrics (varying γ, ρ, τ) on a small model to identify best-performing configuration
  2. Test effect of different numbers of salient columns (k) on performance and efficiency
  3. Evaluate impact of noise injection levels on training stability and final performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal sensitivity metric for identifying salient columns in different LLM architectures?
- Basis in paper: [explicit] The authors note that while some sensitivity metrics perform better than others, none emerge as a clear dominant choice across different bit-widths and models.
- Why unresolved: The paper compares several sensitivity metrics (based on different values of γ and ρ in their generalized formulation) but finds that performance varies depending on the specific model and quantization level, with no consistent winner.
- What evidence would resolve it: Systematic experiments comparing the proposed generalized sensitivity metrics across diverse LLM architectures (not just LLaMA), quantization levels, and downstream tasks would identify consistent patterns in metric performance.

### Open Question 2
- Question: How does GIFT-SW perform when applied to other LLM architectures beyond LLaMA?
- Basis in paper: [explicit] The authors explicitly state "We report results of GIFT-SW exclusively for LLaMA models" and note "Despite the architectural similarities among most LLMs, future experiments with different models are necessary."
- Why unresolved: The paper only tests GIFT-SW on LLaMA models, leaving uncertainty about its generalizability to other popular architectures like Mistral, Gemma, or Qwen.
- What evidence would resolve it: Comprehensive experiments applying GIFT-SW to multiple LLM families with varying sizes, pretraining objectives, and architectural differences would establish its broader applicability.

### Open Question 3
- Question: What is the optimal noise distribution for GIFT-SW beyond the quantization noise injection approach?
- Basis in paper: [explicit] The authors state "In this study, we focus on developing the GIFT-SW algorithm for effective fine-tuning of LLMs, but we do not provide computationally efficient implementations of CUDA kernels for the algorithm."
- Why unresolved: The paper uses quantization noise injection for GIFT-SW, but acknowledges that other noise distributions might be beneficial without exploring them.
- What evidence would resolve it: Comparative experiments testing GIFT-SW with various noise distributions (Gaussian, uniform, Laplacian, etc.) and parameter tuning strategies across multiple models and tasks would identify optimal noise configurations.

## Limitations

- Performance claims compared to state-of-the-art TÜLU2 models lack full experimental detail for verification
- Method tested exclusively on LLaMA models, leaving uncertainty about generalizability to other architectures
- Implementation details for CUDA kernels and specific noise parameters not fully specified

## Confidence

- High confidence in the core mechanism of selective salient weight updates
- Medium confidence in the generalized sensitivity metric's effectiveness across different models
- Medium confidence in noise injection benefits for training stability
- Low confidence in the method's performance claims compared to state-of-the-art TÜLU2 models without access to exact experimental details

## Next Checks

1. **Sensitivity Metric Validation**: Implement and compare the three variants of the generalized sensitivity metric (varying γ, ρ, τ) on a small model to verify which configuration performs best and whether results align with the paper's claims.

2. **Noise Injection Parameter Sensitivity**: Systematically test different noise injection levels and quantization step sizes to identify the optimal configuration for stable training and performance, particularly focusing on the relationship between noise magnitude and model convergence.

3. **Cross-Model Generalization Test**: Apply GIFT-SW to a model architecture different from LLaMA (e.g., OPT or Falcon) to validate whether the sensitivity metric and noise injection approach generalize effectively beyond the models used in the original experiments.