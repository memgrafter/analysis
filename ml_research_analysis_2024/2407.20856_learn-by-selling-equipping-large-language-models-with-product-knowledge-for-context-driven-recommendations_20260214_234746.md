---
ver: rpa2
title: 'Learn by Selling: Equipping Large Language Models with Product Knowledge for
  Context-Driven Recommendations'
arxiv_id: '2407.20856'
source_url: https://arxiv.org/abs/2407.20856
tags:
- product
- language
- search
- llms
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores training large language models (LLMs) for context-driven
  product recommendations by fine-tuning them on synthetic search queries paired with
  product IDs and sales responses. The approach aims to give LLMs comprehensive product
  knowledge to generate personalized recommendations.
---

# Learn by Selling: Equipping Large Language Models with Product Knowledge for Context-Driven Recommendations

## Quick Facts
- arXiv ID: 2407.20856
- Source URL: https://arxiv.org/abs/2407.20856
- Authors: Sarthak Anand; Yutong Jiang; Giorgi Kokaia
- Reference count: 12
- Primary result: Fine-tuning Mistral7B on synthetic queries with product IDs improves recommendation accuracy (Top-1 Match: 28.7% vs 22.7% without tokens)

## Executive Summary
This paper explores training large language models for context-driven product recommendations by fine-tuning them on synthetic search queries paired with product IDs and sales responses. The approach aims to give LLMs comprehensive product knowledge to generate personalized recommendations. Experiments with Mistral7B show that including product IDs as new tokens improves recommendation accuracy (Top-1 Match: 28.7% vs 22.7% without tokens; Top-5 Match: 46.3% vs 44.4%). However, the model frequently generates incorrect product details (e.g., correct series name: 44.4%, correct price: 43.6%) and often adds new, inaccurate information (93.9%). The model sometimes hallucinates product IDs when trained without explicit token inclusion. Key limitations include factual inaccuracies, information hallucination, and the need for retraining when product inventory changes. Future work should focus on improving factual accuracy and adaptability.

## Method Summary
The authors fine-tune Mistral7B using supervised learning on synthetic data generated by GPT-4. The dataset consists of approximately 2,000 IKEA products across 25 categories, with 5 synthetic search queries generated per product using GPT-4 based on product attributes. Sales responses are generated using GPT-4 with a guided prompt, and product IDs are added as tokens in the model's vocabulary. The fine-tuning uses full fine-tuning with a vocabulary expanded to include product IDs. The approach is compared against a model trained without product ID tokens. Evaluation uses Top-1/Top-5 Match and Category Match metrics on a 6,099/2,033/2,033 train/validation/test split, along with qualitative analysis using GPT-4-turbo for factual accuracy assessment.

## Key Results
- Including product IDs as tokens improves recommendation accuracy (Top-1 Match: 28.7% vs 22.7% without tokens; Top-5 Match: 46.3% vs 44.4%)
- The model struggles with factual accuracy, correctly generating series names 44.4% of the time and prices 43.6% of the time
- The model frequently adds new, inaccurate information not present in original product descriptions (93.9% of the time)
- Category matching is more accurate (Top-1 Category Match: 91.3% vs 87.5% without tokens; Top-5 Category Match: 97.1% vs 96.6%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning Mistral7B on synthetic search queries with product IDs enables the model to learn product knowledge and generate context-driven recommendations.
- Mechanism: By expanding the model's vocabulary to include unique product IDs as tokens and training on pairs of synthetic queries and sales responses, the model learns the relationships between products, their attributes, and user contexts.
- Core assumption: LLMs can effectively learn and generalize product knowledge from synthetic data that mimics real user queries and sales responses.
- Evidence anchors:
  - [abstract] The paper states that training LLMs to respond contextually to synthetic search queries containing product IDs equips them with product knowledge.
  - [section 4.1] The methodology section describes the supervised fine-tuning approach using synthetic data.
  - [corpus] No direct corpus evidence found for this specific mechanism; the closest is the "Aligning Large Language Models with Recommendation Knowledge" paper, which discusses a mismatch between LLMs' knowledge and recommendation knowledge.
- Break condition: If the synthetic data does not adequately represent real user queries or if the model fails to generalize from the synthetic data to real-world scenarios.

### Mechanism 2
- Claim: Including product IDs as new tokens in the model's vocabulary improves recommendation accuracy compared to training without explicit token inclusion.
- Mechanism: By treating product IDs as unique tokens, the model can more easily associate queries with specific products, leading to better recommendation performance.
- Core assumption: The model's ability to recognize and process product IDs as distinct tokens enhances its recommendation capabilities.
- Evidence anchors:
  - [section 5] The results show that the model trained with product ID tokens achieved higher Top-1 and Top-5 match scores (28.7% and 46.3%) compared to the model trained without tokens (22.7% and 44.4%).
  - [corpus] No direct corpus evidence found for this specific mechanism; the closest is the "LLM-Powered Explanations: Unraveling Recommendations Through Subgraph Reasoning" paper, which discusses using knowledge graphs to enhance recommender systems.
- Break condition: If the improvement in recommendation accuracy is not significant or if the model starts to overfit to the product IDs.

### Mechanism 3
- Claim: The model's tendency to generate new information not present in the original product descriptions is a result of its training on synthetic data and its language generation capabilities.
- Mechanism: The model, trained on synthetic queries and responses, learns to generate plausible-sounding information based on the patterns in the training data, even if that information is not factually accurate.
- Core assumption: The model's language generation capabilities, combined with the synthetic nature of the training data, lead to the generation of new, potentially inaccurate information.
- Evidence anchors:
  - [section 6] The qualitative analysis shows that the model frequently added new information not present in the original product descriptions (93.9% of the time).
  - [section 5] The results indicate that the model struggled with factual accuracy, particularly in generating correct series names and prices.
  - [corpus] No direct corpus evidence found for this specific mechanism; the closest is the "Evaluating Large Language Models for Material Selection" paper, which investigates the use of LLMs for material selection and compares their performance.
- Break condition: If the model's tendency to generate new information becomes too prevalent or if the generated information consistently deviates from the actual product attributes.

## Foundational Learning

- Concept: Synthetic data generation for training LLMs in product recommendation scenarios.
  - Why needed here: The paper relies on synthetic search queries and sales responses to train the model, so understanding how to generate such data is crucial.
  - Quick check question: What are the key elements to consider when generating synthetic search queries for product recommendation training?

- Concept: Supervised fine-tuning of LLMs using custom datasets.
  - Why needed here: The paper employs supervised fine-tuning to adapt Mistral7B to the product recommendation task using the synthetic data.
  - Quick check question: What are the main steps involved in fine-tuning an LLM using a custom dataset, and how does this differ from pre-training?

- Concept: Evaluating the performance of LLM-based recommendation systems.
  - Why needed here: The paper presents various quantitative and qualitative evaluation metrics to assess the model's performance in generating product recommendations.
  - Quick check question: What are the key evaluation metrics for assessing the performance of an LLM-based recommendation system, and how do they differ from traditional recommendation system metrics?

## Architecture Onboarding

- Component map: Synthetic data generation pipeline (GPT-4) -> LLM fine-tuning module (Mistral7B) -> Product knowledge integration (product IDs as tokens) -> Recommendation generation module -> Evaluation module (quantitative and qualitative metrics)

- Critical path: Synthetic data generation → LLM fine-tuning → Product knowledge integration → Recommendation generation → Evaluation

- Design tradeoffs:
  - Using synthetic data vs. real user data: Synthetic data allows for controlled generation of training examples but may not fully capture the complexity of real user queries.
  - Incorporating product IDs as tokens vs. relying on the model's language understanding: Explicit token inclusion may improve recommendation accuracy but could also lead to overfitting or hallucination.

- Failure signatures:
  - Low recommendation accuracy (Top-1 and Top-5 match scores)
  - High rate of hallucination or generation of inaccurate product information
  - Inability to adapt to new products without retraining

- First 3 experiments:
  1. Compare the performance of the fine-tuned model with a baseline model that does not use product ID tokens.
  2. Evaluate the model's performance on a held-out test set of real user queries (if available) to assess its generalization capabilities.
  3. Analyze the impact of varying the complexity and diversity of the synthetic search queries on the model's performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the model's tendency to generate factually inaccurate information (e.g., incorrect series names, prices, materials) be reduced without significantly impacting its ability to generate relevant product recommendations?
- Basis in paper: explicit
- Why unresolved: The paper identifies this as a key limitation and suggests that improving factual accuracy is an area for future research, but does not provide a concrete solution.
- What evidence would resolve it: A comparison of the model's performance on factual accuracy metrics (e.g., correct series name, price, material) before and after implementing a proposed solution, such as incorporating price-specific search queries or using a retrieval mechanism for product descriptions.

### Open Question 2
- Question: Can the model be made more adaptable to new products without requiring retraining on the entire product inventory?
- Basis in paper: explicit
- Why unresolved: The paper states that the model must be re-trained to adapt to new products, which is resource-intensive and time-consuming. However, it does not explore alternative approaches to improve adaptability.
- What evidence would resolve it: A demonstration of the model's ability to generate accurate product recommendations for new products without retraining, using techniques such as incremental learning or few-shot learning.

### Open Question 3
- Question: How does the inclusion of product IDs as tokens in the vocabulary impact the model's performance on product recommendations compared to other methods of incorporating product knowledge?
- Basis in paper: explicit
- Why unresolved: The paper shows that including product IDs as tokens improves recommendation accuracy, but does not compare this approach to other methods of incorporating product knowledge, such as using product embeddings or metadata.
- What evidence would resolve it: A comparison of the model's performance on recommendation accuracy metrics (e.g., Top-1 Match, Top-5 Match) when using different methods of incorporating product knowledge, such as product IDs, embeddings, or metadata.

## Limitations

- The approach relies entirely on synthetic data, creating uncertainty about generalization to real user queries with different patterns and linguistic variations
- Factual accuracy remains a significant challenge, with the model correctly generating series names only 44.4% of the time and prices 43.6% of the time
- The model cannot adapt to new products without retraining, requiring resource-intensive updates when product inventory changes

## Confidence

**High Confidence**: The methodology for fine-tuning Mistral7B with synthetic data is clearly specified and reproducible. The experimental results showing improved Top-1 and Top-5 match scores when including product ID tokens are straightforward measurements with clear methodology.

**Medium Confidence**: The core hypothesis that synthetic training enables context-driven recommendations is supported by quantitative improvements, but the synthetic data generation process introduces uncertainty. The evaluation metrics, while comprehensive, may not fully capture real-world performance differences.

**Low Confidence**: Claims about the model's ability to handle dynamic product inventories and adapt to new products without retraining are not empirically validated. The paper identifies this as a limitation but doesn't provide evidence about the magnitude of this constraint.

## Next Checks

1. **Real Query Validation**: Test the fine-tuned model on a held-out dataset of actual user search queries (not synthetic) to measure real-world generalization performance and identify any systematic gaps between synthetic and real query patterns.

2. **Dynamic Inventory Adaptation**: Evaluate the model's performance degradation when products are added or removed from the inventory without retraining. Measure the rate of hallucination and incorrect recommendations in this scenario.

3. **Factual Consistency Benchmark**: Develop a comprehensive benchmark testing the model's ability to accurately reproduce product attributes (prices, materials, dimensions) across diverse product categories, with human evaluation to assess the severity of hallucinations versus minor inaccuracies.