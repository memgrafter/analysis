---
ver: rpa2
title: 'ChatGPT as Research Scientist: Probing GPT''s Capabilities as a Research Librarian,
  Research Ethicist, Data Generator and Data Predictor'
arxiv_id: '2406.14765'
source_url: https://arxiv.org/abs/2406.14765
tags:
- research
- were
- gpt-4
- data
- gpt-3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study systematically tested ChatGPT''s capabilities as a research
  scientist across four tasks: acting as a Research Librarian, Research Ethicist,
  Data Generator, and Novel Data Predictor, using psychological science as the domain.
  GPT-4 outperformed GPT-3.5, showing far fewer hallucinations when providing references
  (5.4% vs.'
---

# ChatGPT as Research Scientist: Probing GPT's Capabilities as a Research Librarian, Research Ethicist, Data Generator and Data Predictor

## Quick Facts
- arXiv ID: 2406.14765
- Source URL: https://arxiv.org/abs/2406.14765
- Authors: Steven A. Lehr; Aylin Caliskan; Suneragiri Liyanage; Mahzarin R. Banaji
- Reference count: 40
- GPT-4 outperforms GPT-3.5 in hallucination reduction, research ethics detection, and acknowledging fictional outputs, but cannot predict novel empirical data

## Executive Summary
This study systematically tested ChatGPT's capabilities as a research scientist across four tasks: acting as a Research Librarian, Research Ethicist, Data Generator, and Novel Data Predictor, using psychological science as the domain. GPT-4 outperformed GPT-3.5, showing far fewer hallucinations when providing references (5.4% vs. 36.0%), improved ability to detect flawed research protocols (88.6% of blatant issues, 72.6% of subtle ones), and ability to simulate known word embedding results. However, neither version could predict novel empirical data beyond their training. GPT-4 also showed emerging ability to acknowledge its fictional outputs. The findings suggest GPT is rapidly improving but still limited in generating genuinely novel scientific insights.

## Method Summary
The study tested ChatGPT-3.5 and GPT-4 across four domains using structured prompts and evaluation criteria. For the Research Librarian task, models were asked to find and cite 20 influential papers on 25 psychology topics, with outputs manually coded for hallucinations, errors, and relevance. The Research Ethicist component presented fictional research protocols with blatant and subtle methodological flaws, scored on rubric criteria. The Data Generator study used word association dyads to generate cultural bias estimates, computing WEAT D-scores and comparing to known benchmarks. The Novel Data Predictor task evaluated whether models could forecast empirical results beyond their training data.

## Key Results
- GPT-4 reduced hallucination rates from 36.0% (GPT-3.5) to 5.4% when providing references
- GPT-4 detected 88.6% of blatant research protocol violations and 72.6% of subtle ones, while GPT-3.5 struggled with both
- Both models replicated known word embedding patterns and cultural biases, but neither could predict novel empirical data
- GPT-4 showed emerging ability to acknowledge its fictional outputs, a capability absent in GPT-3.5

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 shows an evolving capacity to acknowledge its fictional outputs, unlike GPT-3.5.
- Mechanism: The model internally represents the truth or fiction of its statements and can flag fictional content when generating it.
- Core assumption: GPT-4 has developed self-monitoring capabilities that allow it to detect and label hallucinations.
- Evidence anchors:
  - [abstract] "GPT-4 exhibited an evolving capacity to acknowledge its fictions."
  - [section] "GPT-4 possesses an evolving capacity to acknowledge when it generates fictional content."
- Break condition: If GPT-4 begins to consistently produce fictional outputs without acknowledgment, the mechanism fails.

### Mechanism 2
- Claim: GPT-4 is significantly better at detecting flawed research protocols than GPT-3.5.
- Mechanism: The model has improved understanding of research methodology and can identify subtle violations that GPT-3.5 misses.
- Core assumption: GPT-4 has been trained on more comprehensive research ethics data and can apply this knowledge to evaluate protocols.
- Evidence anchors:
  - [abstract] "GPT-4 (though not GPT-3.5) proved capable of detecting violations like p-hacking in fictional research protocols."
  - [section] "GPT-4 shined in this regard, decisively outperforming GPT-3.5 when providing feedback on subpar research protocols."
- Break condition: If GPT-4 starts missing obvious violations or begins to give false positive feedback on ethical protocols.

### Mechanism 3
- Claim: GPT can replicate known word embedding results, indicating potential for data generation and hypothesis building.
- Mechanism: The model has internalized patterns from its training data that allow it to simulate established research findings.
- Core assumption: GPT's training corpus contains sufficient examples of word embedding patterns for it to reproduce known effects.
- Evidence anchors:
  - [abstract] "both models consistently replicated patterns of cultural bias previously discovered in large language corpora."
  - [section] "GPTâ€™s results replicated known overall effects from this literature."
- Break condition: If GPT consistently fails to reproduce even simple, well-established word embedding patterns.

## Foundational Learning

- Concept: Hallucination detection
  - Why needed here: Understanding how models generate and identify fictional content is crucial for evaluating their reliability as research assistants.
  - Quick check question: What distinguishes a hallucination from a simple error in model outputs?

- Concept: Research ethics evaluation
  - Why needed here: Assessing a model's ability to identify flawed research protocols requires understanding research methodology and statistical best practices.
  - Quick check question: What are the key indicators of p-hacking in research protocols?

- Concept: Word embedding analysis
  - Why needed here: Evaluating a model's ability to simulate word embedding results requires understanding how semantic associations are measured.
  - Quick check question: How do WEAT D-scores quantify cultural associations between word categories?

## Architecture Onboarding

- Component map: Research Librarian -> Research Ethicist -> Data Generator -> Novel Data Predictor
- Critical path: The most critical evaluation is the Research Ethicist component, as it directly impacts the reliability of scientific research outputs.
- Design tradeoffs: The study prioritized breadth of testing across multiple research tasks over depth in any single domain.
- Failure signatures: Key failure modes include hallucination of references, inability to detect subtle research violations, and failure to predict novel data patterns.
- First 3 experiments:
  1. Test GPT-4's ability to acknowledge hallucinations by requesting it generate references on a narrow topic.
  2. Evaluate GPT-4's performance on detecting subtle research protocol violations by presenting a protocol with hidden methodological issues.
  3. Assess GPT-4's ability to simulate word embedding results by requesting it estimate cultural associations between gender-related word pairs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does GPT-4's emerging ability to acknowledge fictional content represent a trained feature or an emergent property?
- Basis in paper: [explicit] The authors note this advancement is "agnostic to the source" and suggest it could be either trained or emergent.
- Why unresolved: The training methodology of GPT-4 is not publicly disclosed, making it impossible to determine the origin of this capability.
- What evidence would resolve it: Analysis of model architecture differences between GPT-3.5 and GPT-4, or documentation from OpenAI about specific training objectives related to self-correction.

### Open Question 2
- Question: Can GPT-4 effectively help researchers improve protocols beyond identifying obvious flaws?
- Basis in paper: [inferred] The study only tested GPT-4's ability to identify blatant and subtle violations, not its ability to suggest improvements to already sound protocols.
- Why unresolved: The research design focused on detecting flaws rather than enhancing good protocols.
- What evidence would resolve it: Testing GPT-4's ability to provide constructive feedback on research protocols that already meet basic ethical and methodological standards.

### Open Question 3
- Question: Under what specific conditions does ChatGPT provide more complete citations?
- Basis in paper: [explicit] The authors found completeness moderated hallucination rates but did not test whether experimental manipulation of prompt formality would change citation completeness.
- Why unresolved: The study was observational rather than experimental regarding prompt structure.
- What evidence would resolve it: Experimental manipulation of prompt formality, specificity, or context to determine which factors reliably increase citation completeness.

## Limitations
- The study relies on subjective manual coding of model outputs, particularly for research ethics evaluation and hallucination detection.
- The exact scoring rubrics and prompt formulations are not fully specified, which limits reproducibility.
- The tests focused exclusively on psychological science, leaving open questions about performance in other scientific domains.

## Confidence
- **High confidence**: GPT-4's superior performance in reference accuracy and hallucination reduction compared to GPT-3.5 (supported by specific quantitative data: 5.4% vs 36.0% hallucination rates)
- **Medium confidence**: GPT-4's ability to acknowledge its fictional outputs, as this relies on qualitative observations rather than systematic measurement
- **Medium confidence**: GPT-4's detection of subtle research protocol violations, though the evaluation criteria appear well-defined

## Next Checks
1. **Replicate the hallucination acknowledgment test** by systematically prompting GPT-4 to generate references on narrow topics and coding whether it flags fictional outputs versus GPT-3.5's responses
2. **Test cross-domain generalizability** by applying the Research Ethicist protocol to research protocols from fields outside psychology (e.g., medical research or engineering)
3. **Verify the word embedding simulation results** by having independent researchers reproduce the WEAT D-score calculations and compare them to the reported benchmarks