---
ver: rpa2
title: Adversarial Magnification to Deceive Deepfake Detection through Super Resolution
arxiv_id: '2407.02670'
source_url: https://arxiv.org/abs/2407.02670
tags:
- deepfake
- images
- detection
- attack
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel black-box adversarial attack against
  deepfake detection systems using super-resolution (SR) techniques. The attack involves
  downscaling detected faces by a factor of 2 and then upscaling them back to original
  resolution using EDSR, effectively smoothing artifacts introduced by deepfake generation
  methods.
---

# Adversarial Magnification to Deceive Deepfake Detection through Super Resolution

## Quick Facts
- arXiv ID: 2407.02670
- Source URL: https://arxiv.org/abs/2407.02670
- Reference count: 25
- Primary result: Super-resolution attack increases FNR by up to 18% and FPR by up to 14% while maintaining SSIM ~0.97

## Executive Summary
This paper presents a novel black-box adversarial attack against deepfake detection systems using super-resolution techniques. The attack works by downscaling detected faces by a factor of 2 and then upscaling them back to original resolution using EDSR, effectively smoothing artifacts introduced by deepfake generation methods. Tested on FaceForensics++ dataset with three different detection models, the attack achieved significant increases in both False Negative Rates (up to 18%) and False Positive Rates (up to 14%) while maintaining high visual similarity to original images.

## Method Summary
The attack targets faces detected in images using MTCNN, downscales them by a factor of 2, and then upscales them using EDSR super-resolution to remove artifacts introduced during deepfake generation. The processed face is then pasted back into the original image at the detected coordinates. The attack was evaluated on FaceForensics++ dataset using three deepfake detection models (Resnet50, Swin-Small, XceptionNet) and achieved significant degradation in detection performance while maintaining high SSIM and PSNR values.

## Key Results
- SR attack increased FNR by up to 18% on fake images across all tested deepfake methods
- FPR increased by up to 14% on pristine images when attack applied
- Maintained high visual similarity with SSIM ~0.97 and PSNR ~40dB
- NeuralTextures method showed highest vulnerability with 18% FNR increase

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The super-resolution attack works by smoothing the high-frequency artifacts introduced during deepfake generation.
- Mechanism: When an image is down-scaled by a factor of 2 and then up-scaled using EDSR, the SR process reconstructs the image using learned low-to-high frequency mappings, which effectively averages out or removes the subtle artifacts that deepfake detectors rely on for classification.
- Core assumption: Deepfake detectors are trained to identify specific high-frequency artifacts that are smoothed or lost during the SR reconstruction process.
- Evidence anchors:
  - [abstract] "the usage of super resolution can significantly impair the accuracy of deepfake detectors"
  - [section] "the SR process, in an attempt to improve the resolution of an image, could smooth the artifacts introduced by some deepfake generation techniques, thus undermining the learning performed by the deepfake detection model"

### Mechanism 2
- Claim: The attack maintains high visual similarity to original images while degrading detection performance.
- Mechanism: The SR process preserves the overall structure and semantic content of the image (SSIM ~0.97, PSNR ~40dB), ensuring that human observers cannot distinguish the attacked image from the original, while the deepfake detector misclassifies it due to the smoothed artifacts.
- Core assumption: Deepfake detectors are sensitive to artifacts that are imperceptible to human observers but preserved in the SSIM/PSNR metrics.
- Evidence anchors:
  - [abstract] "The attack maintains high visual similarity to original images (SSIM ~0.97, PSNR ~40dB) while substantially degrading detection performance"
  - [section] "the similarity between the SR images and the non-SR ones is very high, with SSIM values around 0.97 and PSNR around 40dB meaning a strong similarity and minimal changes brought by the SR process"

### Mechanism 3
- Claim: The attack is model-agnostic and works as a black-box attack.
- Mechanism: Since the attack only modifies the face region using a standard SR technique without requiring any knowledge of the detector's architecture or training data, it can be applied to any deepfake detector regardless of its internal structure.
- Core assumption: The artifacts introduced by deepfake generation methods are consistent enough across different detectors that smoothing them will cause misclassification in most systems.
- Evidence anchors:
  - [abstract] "The attack involves downscaling detected faces by a factor of 2 and then upscaling them back to original resolution using EDSR"
  - [section] "there is no need to know anything about the deepfake detector that will be used for the final detection, then the proposed method can be effectively considered a black-box attack and can be applied against any deepfake detector"

## Foundational Learning

- Concept: Super-resolution techniques and their artifacts
  - Why needed here: Understanding how SR algorithms reconstruct images is crucial to predicting how they will affect deepfake artifacts
  - Quick check question: What happens to high-frequency details when an image is down-scaled and then up-scaled using EDSR?

- Concept: Deepfake generation artifacts
  - Why needed here: Knowing what specific artifacts different deepfake methods introduce helps explain why SR is effective against certain methods but not others
  - Quick check question: Which types of artifacts (color inconsistencies, boundary artifacts, etc.) are most commonly introduced by Deepfakes vs Face2Face methods?

- Concept: Face detection and alignment
  - Why needed here: The attack specifically targets detected faces, so understanding face detection accuracy and its impact on the attack is important
  - Quick check question: How does the choice of face detector (MTCNN vs others) affect the precision of the SR attack on the face region?

## Architecture Onboarding

- Component map:
  Face detection (MTCNN) -> Down-scaling module (factor 2) -> EDSR super-resolution module -> Face pasting module (coordinates from detection) -> Deepfake detector (Resnet50, Swin-Small, XceptionNet)

- Critical path:
  Face detection → Down-scaling → EDSR up-scaling → Face pasting → Deepfake detection

- Design tradeoffs:
  - Face detection accuracy vs. computational cost
  - Scale factor choice (2x provides good balance between artifact removal and visual quality)
  - SR model choice (EDSR vs newer methods for better artifact smoothing)
  - Attack scope (faces only vs. full image for maximum stealth)

- Failure signatures:
  - High SSIM/PSNR but low attack effectiveness indicates artifacts are not being smoothed sufficiently
  - Low SSIM/PSNR with high attack effectiveness indicates the attack is introducing visible artifacts
  - Inconsistent results across deepfake methods suggest method-specific artifact patterns

- First 3 experiments:
  1. Vary the down-scaling factor (1.5x, 2x, 3x) to find the optimal balance between artifact removal and visual quality
  2. Test alternative SR methods (ESRGAN, SwinIR) to compare artifact smoothing effectiveness
  3. Apply the attack to pristine images to measure false positive rate increase and understand detector vulnerabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the resolution of detected faces affect the performance of the SR-based adversarial attack?
- Basis in paper: [inferred] The paper mentions that future work will explore the impact of detected face resolution on attack performance, indicating this is currently unexplored.
- Why unresolved: The current experiments use a fixed scale factor of K=2 without investigating how different face resolutions might affect the attack's effectiveness.
- What evidence would resolve it: Systematic experiments varying the scale factor K and analyzing how different face resolutions impact False Negative Rates and False Positive Rates across various deepfake generation methods.

### Open Question 2
- Question: Can training deepfake detectors with SR-augmented data make them robust to SR-based adversarial attacks?
- Basis in paper: [explicit] The authors state in their conclusion that they will explore if using SR as data augmentation during training could make detectors robust to this attack.
- Why unresolved: The current experiments only test pre-trained models without any SR augmentation in their training process.
- What evidence would resolve it: Training new deepfake detection models using SR-processed images as part of their training data, then testing their vulnerability to the same SR attack.

### Open Question 3
- Question: Are there alternative super-resolution techniques that are more or less effective than EDSR for adversarial purposes?
- Basis in paper: [explicit] The authors mention in their conclusion that they will explore the impact of using more SR techniques beyond EDSR.
- Why unresolved: The current experiments only use EDSR for the super-resolution process, limiting the understanding of how different SR methods might affect attack effectiveness.
- What evidence would resolve it: Comparative experiments using multiple SR techniques (e.g., GAN-based SR, CNN-based SR, or newer transformer-based SR methods) to determine which are most effective at degrading deepfake detection performance.

## Limitations
- Attack effectiveness varies significantly across different deepfake generation methods
- Only tested one SR method (EDSR) and one downscaling factor (2x)
- Generalizability to commercial deepfake detection systems remains unknown

## Confidence
- High confidence: The mechanism by which SR smooths high-frequency artifacts and the empirical observation that SSIM/PSNR values remain high while detection performance degrades
- Medium confidence: The claim that the attack works as a black-box method against any deepfake detector
- Low confidence: The generalizability of attack effectiveness across different deepfake generation methods and real-world scenarios

## Next Checks
1. Test the attack against commercial deepfake detection APIs (such as Microsoft Video Authenticator or Facebook's deepfake detection system) to assess real-world effectiveness and determine if the attack transfers across different implementations

2. Conduct ablation studies varying the downscaling factor (test 1.5x, 2.5x, 3x) and alternative SR methods (ESRGAN, SwinIR, Real-ESRGAN) to identify optimal configurations and understand the sensitivity of the attack to these parameters

3. Evaluate the attack's effectiveness when applied to videos in addition to individual frames, measuring temporal consistency and whether frame-by-frame application introduces detectable artifacts or inconsistencies in the video stream