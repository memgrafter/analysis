---
ver: rpa2
title: Transformers Struggle to Learn to Search
arxiv_id: '2412.04703'
source_url: https://arxiv.org/abs/2412.04703
tags:
- vertex
- search
- graph
- input
- vertices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether transformers can learn to search
  using the graph connectivity problem as a testbed. The authors find that transformers
  can learn to search when given the right training distribution, specifically a balanced
  distribution that ensures uniform lookahead.
---

# Transformers Struggle to Learn to Search

## Quick Facts
- arXiv ID: 2412.04703
- Source URL: https://arxiv.org/abs/2412.04703
- Authors: Abulhair Saparov; Srushti Pawar; Shreyas Pimpalgaonkar; Nitish Joshi; Richard Yuanzhe Pang; Vishakh Padmakumar; Seyed Mehran Kazemi; Najoung Kim; He He
- Reference count: 40
- Key outcome: Transformers can learn search on graph connectivity problems with proper training distribution, but struggle with larger graphs regardless of model scale

## Executive Summary
This paper investigates whether transformers can learn to search using graph connectivity as a testbed. The authors find that transformers can successfully learn search algorithms when trained on a balanced distribution that ensures uniform lookahead, preventing reliance on heuristics. They develop a novel mechanistic interpretability technique to analyze the learned algorithm, revealing that transformers perform parallel search by progressively expanding sets of reachable vertices across layers. However, as graph size increases, transformers struggle to learn the task, and this difficulty persists even with increased model scale.

## Method Summary
The paper studies graph connectivity search using transformers trained on DAGs where the task is to predict the next vertex along a path from start to goal. The balanced distribution ensures uniform lookahead by generating graphs where all vertices have degree 2 and branches merge at the goal. Transformers use 1-hot embeddings with concatenated token and positional embeddings, 1 attention head per layer, and ReLU activation. Training uses streaming updates from the balanced distribution with cross-entropy loss. A novel mechanistic interpretability technique based on activation patching identifies path-merge operations by analyzing attention weight perturbations and computing attention maps between layers.

## Key Results
- Transformers achieve near-perfect accuracy on balanced distribution training and generalize to unobserved lookaheads
- Mechanistic interpretability reveals transformers perform parallel search by expanding reachable vertex sets exponentially across layers
- Model scale increases do not alleviate difficulties with larger input graphs
- Even with chain-of-thought capabilities, transformers struggle on larger graphs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Transformers can learn to perform search when given the right training distribution.
- **Mechanism:** The balanced distribution ensures uniform lookahead, preventing the model from relying on heuristics. When trained on this distribution, the transformer learns an exponential path-merging algorithm where each layer progressively expands the set of reachable vertices from each vertex.
- **Core assumption:** The balanced distribution is constructed to prevent shortcuts and heuristics, forcing the model to learn a generalizable search algorithm.
- **Evidence anchors:**
  - [abstract] "when given the right training distribution, the transformer is able to learn to search."
  - [section 3.1.1] "the model trained on the full balanced distribution performs near perfectly in all test settings, and generalizes to unobserved numbers of lookaheads."
  - [corpus] Weak evidence - the related papers do not directly address the importance of training distribution for learning search.
- **Break condition:** If the training distribution allows shortcuts or heuristics, the model will not learn to perform search robustly.

### Mechanism 2
- **Claim:** Transformers perform search in parallel on all vertices by progressively expanding sets of reachable vertices.
- **Mechanism:** For each vertex in the input graph, the transformer computes the set of vertices reachable from that vertex. Each layer then progressively expands these sets, allowing the model to search over a number of vertices exponential in the number of layers.
- **Core assumption:** The transformer's learned algorithm involves storing and expanding sets of reachable vertices at each vertex.
- **Evidence anchors:**
  - [abstract] "transformers perform search at every vertex in parallel: For each vertex in the input graph, transformers compute the set of vertices reachable from that vertex. Each layer then progressively expands these sets, allowing the model to search over a number of vertices exponential in n_layers."
  - [section 4.2] "We hypothesize that the transformer performs search on all vertices simultaneously, where the embedding for each vertex explainably contains information about the set of vertices reachable from the current vertex within a certain number of steps."
  - [corpus] Weak evidence - the related papers do not directly address the parallel search mechanism in transformers.
- **Break condition:** If the input graph size increases beyond the model's capacity to expand reachable sets, the transformer will struggle to learn the task.

### Mechanism 3
- **Claim:** Increasing model scale does not alleviate the difficulty of learning to search on larger graphs.
- **Mechanism:** As the input graph size increases, the transformer has greater difficulty in learning the task. This difficulty is not resolved even as the number of parameters is increased, suggesting that simply increasing the size of the transformer will not lead to the robust acquisition of searching and planning abilities.
- **Core assumption:** The difficulty in learning to search on larger graphs is a fundamental limitation of the transformer architecture, not just a matter of model size.
- **Evidence anchors:**
  - [abstract] "as the input graph size increases, the transformer has greater difficulty in learning the task. This difficulty is not resolved even as the number of parameters is increased, suggesting that increasing model scale will not lead to robust search abilities."
  - [section 5] "we observe that, when the number of layers is fixed to 8 and the hidden size 16, as the maximum input graph size is increased, the likelihood that the model learns the training distribution (i.e., reaches accuracy ≥ 0.995) becomes vanishingly small."
  - [corpus] Weak evidence - the related papers do not directly address the scaling limitations of transformers for learning search.
- **Break condition:** If alternative training approaches or architectures are developed, transformers may be able to learn to search on larger graphs more effectively.

## Foundational Learning

- **Concept:** Graph connectivity and search algorithms
  - Why needed here: Understanding the graph connectivity problem and search algorithms is crucial for designing the training distribution and interpreting the model's learned behavior.
  - Quick check question: Can you explain the difference between breadth-first search and depth-first search in the context of finding a path between two vertices in a graph?

- **Concept:** Mechanistic interpretability techniques
  - Why needed here: The novel mechanistic interpretability technique developed in this paper is essential for analyzing the learned algorithm and understanding how the transformer performs search.
  - Quick check question: How does activation patching help in identifying the causal relationships between input features and model predictions?

- **Concept:** Transformer architecture and attention mechanisms
  - Why needed here: Understanding the transformer architecture, including attention mechanisms and layer-wise computations, is necessary for interpreting the model's behavior and the exponential path-merging algorithm it learns.
  - Quick check question: How does the attention mechanism in transformers enable the model to process information from different parts of the input sequence simultaneously?

## Architecture Onboarding

- **Component map:** Graph tokens -> Transformer layers (1 attention head each) -> Predicted next vertex token
- **Critical path:**
  1. Generate training data from the balanced distribution
  2. Train the transformer model using streaming training
  3. Evaluate the model's performance on held-out test sets
  4. Apply the mechanistic interpretability technique to analyze the learned algorithm
  5. Interpret the results and draw conclusions about the model's search capabilities

- **Design tradeoffs:**
  - Input representation: Using 1-hot embeddings and concatenated token and positional embeddings facilitates mechanistic interpretability but may limit the model's capacity compared to learned embeddings.
  - Attention mechanism: Using 1 attention head per layer simplifies the analysis but may restrict the model's ability to capture complex relationships in the input graph.
  - Training distribution: The balanced distribution ensures uniform lookahead but requires careful construction to prevent heuristics, which may limit the diversity of training examples.

- **Failure signatures:**
  - Low training accuracy: Indicates that the model is not learning the search task effectively, possibly due to an inadequate training distribution or insufficient model capacity.
  - Low test accuracy on larger graphs: Suggests that the model struggles to generalize to graphs with larger lookaheads, even if it performs well on smaller graphs.
  - Inability to learn with increased model scale: Implies that the difficulty in learning to search on larger graphs is a fundamental limitation of the transformer architecture, not just a matter of model size.

- **First 3 experiments:**
  1. Train a transformer model on the balanced distribution with a fixed input graph size and evaluate its performance on held-out test sets from the na¨ıve and balanced distributions.
  2. Apply the mechanistic interpretability technique to analyze the learned algorithm and identify the exponential path-merging mechanism.
  3. Vary the input graph size and model scale to investigate the scaling behavior of the transformer's search capabilities.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can transformers learn to search on graphs with lookahead greater than the number of layers in the model?
- Basis in paper: [explicit] The paper discusses how transformers perform search in parallel on all vertices, with each layer progressively expanding sets of reachable vertices, allowing search over a number of vertices exponential in the number of layers. However, it is unclear if transformers can learn to search on graphs requiring lookahead greater than the number of layers.
- Why unresolved: The paper's experiments focus on graphs with lookahead less than or equal to the number of layers. Scaling experiments show that increasing model size does not help, but the specific case of lookahead greater than the number of layers is not directly tested.
- What evidence would resolve it: Experiments training transformers on graphs with lookahead greater than the number of layers, measuring whether they can learn the task or not.

### Open Question 2
- Question: Does the search direction (forwards vs backwards) in transformers depend on the random initialization seed or the training distribution?
- Basis in paper: [explicit] The paper's mechanistic interpretability analysis suggests that the search direction may vary between models, with some performing search forwards and others backwards. However, it is unclear what factors determine the search direction.
- Why unresolved: The paper only analyzes two models and does not systematically vary the random initialization seed or the training distribution to isolate the effect on search direction.
- What evidence would resolve it: Experiments training multiple transformers with different random initialization seeds and training distributions, analyzing the search direction in each model.

### Open Question 3
- Question: Can alternative training procedures, such as curriculum learning, help transformers learn to search on larger graphs?
- Basis in paper: [inferred] The paper mentions that curriculum learning might be a potential avenue for helping transformers learn to search on larger graphs. However, no experiments are conducted to test this hypothesis.
- Why unresolved: The paper does not explore alternative training procedures beyond the standard training used in the experiments.
- What evidence would resolve it: Experiments training transformers on larger graphs using curriculum learning, comparing the results to standard training to see if curriculum learning improves performance.

## Limitations

- The balanced distribution approach may not generalize to other search and planning problems beyond graph connectivity
- The mechanistic interpretability technique may not scale to larger models or more complex problems
- The experimental scope is limited to relatively small graphs (lookahead ≤ 12) and specific transformer configurations

## Confidence

**High Confidence:** Transformers can learn search when given the right training distribution - well-supported by experimental results showing near-perfect performance on held-out test sets from the balanced distribution.

**Medium Confidence:** The mechanistic explanation of parallel search through exponential path-merging - supported by interpretability analysis but technique limitations mean understanding may not be complete.

**Medium Confidence:** Increasing model scale does not alleviate difficulties with larger graphs - supported by experimental evidence but limited range of model sizes tested.

## Next Checks

1. **Generalization to other search problems:** Test whether the balanced distribution approach generalizes to different search and planning tasks beyond graph connectivity, such as maze navigation or shortest path finding in weighted graphs.

2. **Scalability of mechanistic interpretability:** Apply the proposed interpretability technique to larger transformer models (more layers, multiple attention heads) and more complex graphs to verify that the path-merging algorithm can still be identified and analyzed.

3. **Alternative training approaches:** Experiment with different training methodologies (curriculum learning, reinforcement learning, or supervised fine-tuning on intermediate reasoning steps) to determine if they can overcome the scaling limitations observed with standard training on larger graphs.