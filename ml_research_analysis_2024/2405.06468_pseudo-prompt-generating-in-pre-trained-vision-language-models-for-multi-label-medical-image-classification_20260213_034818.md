---
ver: rpa2
title: Pseudo-Prompt Generating in Pre-trained Vision-Language Models for Multi-Label
  Medical Image Classification
arxiv_id: '2405.06468'
source_url: https://arxiv.org/abs/2405.06468
tags:
- prompt
- learning
- multi-label
- medical
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multi-label zero-shot classification
  of medical images, particularly chest radiographs with multiple pathologies. The
  proposed Pseudo-Prompt Generating (PsPG) method uses an RNN-based decoder to autoregressively
  generate class-tailored pseudo-prompts from visual and textual features, leveraging
  multi-modal knowledge.
---

# Pseudo-Prompt Generating in Pre-trained Vision-Language Models for Multi-Label Medical Image Classification

## Quick Facts
- **arXiv ID**: 2405.06468
- **Source URL**: https://arxiv.org/abs/2405.06468
- **Reference count**: 40
- **Key outcome**: PsPG achieves state-of-the-art performance on multiple medical image datasets with Macro AUC scores of 85.2% on CheXpert, 75.1% on VinDr-CXR, and 77.9% on a private dataset.

## Executive Summary
This paper addresses the challenge of multi-label zero-shot classification of medical images, particularly chest radiographs with multiple pathologies. The proposed Pseudo-Prompt Generating (PsPG) method uses an RNN-based decoder to autoregressively generate class-tailored pseudo-prompts from visual and textual features, leveraging multi-modal knowledge. PsPG achieves state-of-the-art performance on multiple datasets, with Macro AUC scores of 85.2% on CheXpert, 75.1% on VinDr-CXR, and 77.9% on a private dataset, while using significantly fewer trainable parameters than existing methods.

## Method Summary
The PsPG method consists of two phases: first, fine-tuning a CLIP backbone on the MIMIC-CXR dataset; then, using a Pseudo-Prompt Generating (PsPG) method with an RNN-based decoder to generate class-tailored pseudo-prompts from visual and textual features. The method employs Spatial Fusion to enhance fine-grained pathology discrimination by combining global and local image features, and uses Soft Pairwise Co-occurrence Loss (SPCL) to capture label correlations and address category imbalance in multi-label medical datasets.

## Key Results
- PsPG achieves state-of-the-art Macro AUC scores of 85.2% on CheXpert, 75.1% on VinDr-CXR, and 77.9% on a private dataset.
- The method uses significantly fewer trainable parameters than existing methods while maintaining high performance.
- PsPG demonstrates superior generalization to unseen categories compared to existing methods in zero-shot classification.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pseudo-Prompt Generating (PsPG) improves multi-label zero-shot classification by autoregressively generating class-tailored pseudo-prompts from multi-modal features.
- Mechanism: The RNN-based decoder in PsPG generates pseudo-prompts step-by-step using visual global features and class textual features, capturing fine-grained semantic information for each class. This approach addresses the limitation of existing CoOp-based methods that use unified prompts across all categories, which ignore class-specific semantic information.
- Core assumption: The autoregressive generation of pseudo-prompts captures the sequential dependencies and semantic nuances required for accurate classification of unseen categories.
- Evidence anchors:
  - [abstract]: "PsPG autoregressively generates class-tailored embedding vectors, i.e., pseudo-prompts."
  - [section]: "Our method, named Pseudo-Prompt Generating (PsPG), capitalizes on the priori knowledge of multi-modal features. Featuring a RNN-based decoder, PsPG autoregressively generates class-tailored embedding vectors, i.e., pseudo-prompts."
  - [corpus]: Found related work on multi-label classification with prompt learning, but limited direct evidence for autoregressive pseudo-prompt generation. Assumption: The autoregressive approach captures sequential dependencies better than unified prompts.
- Break condition: If the sequential dependencies in pseudo-prompt generation do not improve classification accuracy compared to unified prompts, the autoregressive mechanism may not be beneficial.

### Mechanism 2
- Claim: PsPG leverages Spatial Fusion to enhance fine-grained pathology discrimination by combining global and local image features.
- Mechanism: The Spatial Fusion module uses a lightweight spatial attention mechanism to combine global features from the entire image encoder and local features from the last pooling layer, focusing on localized nuances of medical images. This approach addresses the need for fine-grained pathology indications in chest radiographs.
- Core assumption: Combining global and local features with spatial attention improves the model's ability to discriminate between similar pathologies that require localized analysis.
- Evidence anchors:
  - [abstract]: "PsPG capitalizes on the priori knowledge of multi-modal features."
  - [section]: "We introduce a learnable lightweight spatial attention module SF, named Spatial Fusion, with only 3 parameters in a 1D convolution...This approach contrasts with the parameter-free attention in DualCoOp[21]."
  - [corpus]: Found work on medical image classification using local features, but limited direct evidence for the specific Spatial Fusion approach. Assumption: Local features capture fine-grained details important for pathology discrimination.
- Break condition: If the Spatial Fusion module does not improve classification accuracy over using only global features, the combination of global and local features may not be necessary.

### Mechanism 3
- Claim: PsPG employs Soft Pairwise Co-occurrence Loss (SPCL) to capture label correlations and address category imbalance in multi-label medical datasets.
- Mechanism: SPCL considers all label co-occurrences within the mini-batch and converts label pair co-occurrence prediction to a multi-label classification task, using batch-level co-occurrence targets as soft targets for label pair probabilities. This approach complements Asymmetric Loss to address category imbalance.
- Core assumption: Capturing label co-occurrences improves classification accuracy by leveraging the correlation between different pathologies that frequently appear together.
- Evidence anchors:
  - [abstract]: "Our method, named Pseudo-Prompt Generating (PsPG), capitalizes on the priori knowledge of multi-modal features."
  - [section]: "Soft Pairwise Co-occurrence Loss (SPCL) to capture the label correlation...We consider all label co-occurrences within the mini-batch and convert label pair co-occurrence prediction to a multi-label classification task."
  - [corpus]: Found related work on multi-label classification with label correlation modeling, but limited direct evidence for the specific SPCL approach. Assumption: Label co-occurrence modeling improves classification by capturing dependencies between pathologies.
- Break condition: If SPCL does not improve classification accuracy over using only Asymmetric Loss, the additional complexity of modeling label co-occurrences may not be beneficial.

## Foundational Learning

- Concept: Vision-Language Pre-training (VLP) and zero-shot inference
  - Why needed here: PsPG builds upon pre-trained vision-language models like CLIP and performs zero-shot inference on unseen categories. Understanding how VLP models learn visual-linguistic associations and perform zero-shot classification is crucial for implementing PsPG.
  - Quick check question: How does a pre-trained vision-language model like CLIP perform zero-shot classification on unseen categories using prompts?

- Concept: Prompt learning and autoregressive generation
  - Why needed here: PsPG uses prompt learning to adapt pre-trained models to downstream tasks and employs autoregressive generation to create class-tailored pseudo-prompts. Understanding how prompt learning works and how autoregressive models generate sequences is essential for implementing the PsPG decoder.
  - Quick check question: How does an autoregressive decoder generate sequences step-by-step, and how does this differ from generating a single output?

- Concept: Multi-label classification and loss functions
  - Why needed here: PsPG addresses multi-label classification tasks and uses specific loss functions like Asymmetric Loss and Soft Pairwise Co-occurrence Loss. Understanding how multi-label classification differs from single-label classification and how these loss functions work is crucial for training and evaluating PsPG.
  - Quick check question: How does multi-label classification differ from single-label classification, and why are specific loss functions like Asymmetric Loss and Soft Pairwise Co-occurrence Loss used in this context?

## Architecture Onboarding

- Component map: CLIP backbone (frozen during prompt learning) -> Prompt Decoder (RNN-based with self-attention and cross-attention) -> Spatial Fusion module -> Loss functions (Asymmetric Loss and Soft Pairwise Co-occurrence Loss)

- Critical path:
  1. Fine-tune CLIP backbone on medical dataset
  2. Extract global and local image features
  3. Generate pseudo-prompts using Prompt Decoder
  4. Compute predictions using pseudo-prompts and text encoder
  5. Calculate loss using Asymmetric Loss and Soft Pairwise Co-occurrence Loss
  6. Update Prompt Decoder parameters

- Design tradeoffs:
  - RNN-based decoder vs. Transformer decoder: RNN is simpler and faster, but may not capture long-range dependencies as well as Transformer.
  - Autoregressive generation vs. non-autoregressive generation: Autoregressive generation captures sequential dependencies, but is slower and may be harder to parallelize.
  - Soft Pairwise Co-occurrence Loss vs. other label correlation modeling approaches: SPCL captures batch-level co-occurrences, but may be computationally intensive for large label sets.

- Failure signatures:
  - Poor performance on unseen categories: May indicate issues with pseudo-prompt generation or the inability to generalize from training categories.
  - High computational cost: May indicate inefficiencies in the autoregressive generation process or the Soft Pairwise Co-occurrence Loss computation.
  - Overfitting to training data: May indicate issues with the Prompt Decoder architecture or the need for regularization techniques.

- First 3 experiments:
  1. Evaluate PsPG on a small multi-label medical dataset to assess the basic functionality and identify any immediate issues.
  2. Compare the performance of PsPG with different Prompt Decoder architectures (RNN vs. Transformer) to understand the impact of the decoder choice.
  3. Analyze the effect of the Soft Pairwise Co-occurrence Loss on classification accuracy and computational cost to determine its contribution to the overall performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Pseudo-Prompt Generating (PsPG) method's performance vary when applied to datasets with significantly different label spaces or pathologies compared to those used in training?
- Basis in paper: [explicit] The paper discusses the method's generalizability and adaptability, noting challenges when generalizing to datasets with 108 categories (102 unseen) and suggesting that training on datasets with a wider range of categories might improve performance.
- Why unresolved: The experiments conducted used datasets with similar label spaces to those used in training. There is a need to evaluate the method's robustness and performance on datasets with significantly different label spaces or pathologies to fully understand its generalizability.
- What evidence would resolve it: Performance metrics such as Macro AUC, Micro AUC, and mAP on datasets with significantly different label spaces or pathologies from the training data.

### Open Question 2
- Question: What is the impact of label noise on the Pseudo-Prompt Generating (PsPG) method's performance, and how does it compare to other methods in handling noisy labels?
- Basis in paper: [inferred] The paper mentions that both PsPG and DualCoOp undergo supervised training on CheXpert, which contains potential label noise due to NLP-based annotations. It speculates that the indirect nature of prompt learning may mitigate the impact of label noise to some degree.
- Why unresolved: The paper does not provide a direct comparison of the method's performance on clean versus noisy datasets or a detailed analysis of how label noise affects its performance relative to other methods.
- What evidence would resolve it: Comparative performance metrics on datasets with varying levels of label noise, including both clean and noisy datasets, to assess the method's robustness.

### Open Question 3
- Question: How does the Pseudo-Prompt Generating (PsPG) method perform when extended to other medical imaging modalities beyond chest radiographs, such as CT scans or MRI images?
- Basis in paper: [explicit] The paper concludes by suggesting that PsPG's effectiveness indicates potential for broader application across domains, implying its adaptability to other medical imaging modalities.
- Why unresolved: The experiments and evaluations were conducted exclusively on chest radiograph datasets. There is a need to test the method on other medical imaging modalities to validate its adaptability and performance.
- What evidence would resolve it: Performance metrics on other medical imaging modalities, such as CT scans or MRI images, to demonstrate the method's effectiveness and generalizability beyond chest radiographs.

## Limitations

- The paper does not provide complete implementation details for the Spatial Fusion module and Soft Pairwise Co-occurrence Loss (SPCL), which may hinder faithful reproduction.
- The evaluation is primarily conducted on three datasets, with limited comparison to other medical imaging datasets, raising questions about the generalizability of the method.
- The paper provides some ablation studies, but the impact of individual design choices is not fully explored, limiting the understanding of the method's strengths and weaknesses.

## Confidence

**High confidence** in the following claim clusters:
- PsPG achieves state-of-the-art performance on multiple medical image datasets compared to existing methods.
- The autoregressive generation of pseudo-prompts improves classification accuracy over unified prompts.
- The combination of global and local features with spatial attention enhances fine-grained pathology discrimination.

**Medium confidence** in the following claim clusters:
- The Soft Pairwise Co-occurrence Loss (SPCL) effectively captures label correlations and improves classification accuracy.
- The PsPG method generalizes well to unseen categories in zero-shot classification.
- The method is robust to dataset biases and variations in medical imaging protocols.

## Next Checks

1. **Implement and evaluate the Spatial Fusion module**: Implement the exact Spatial Fusion module as described in the paper and conduct ablation studies to assess its contribution to the overall performance. Compare the performance with and without the Spatial Fusion module on a held-out validation set.

2. **Analyze the impact of Soft Pairwise Co-occurrence Loss (SPCL)**: Implement the SPCL loss function as described in the paper and conduct ablation studies to assess its contribution to the overall performance. Compare the performance with and without SPCL on a held-out validation set, and analyze the computational cost of computing SPCL.

3. **Evaluate on additional medical imaging datasets**: Evaluate the PsPG method on additional medical imaging datasets, such as ChestX-ray14 or NIH ChestX-ray8, to assess its generalizability to other medical imaging tasks and datasets. Compare the performance with existing methods on these datasets to validate the state-of-the-art claims.