---
ver: rpa2
title: 'Adapters for Altering LLM Vocabularies: What Languages Benefit the Most?'
arxiv_id: '2410.09644'
source_url: https://arxiv.org/abs/2410.09644
tags:
- vocabulary
- language
- languages
- adaptation
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes VocADT, a vocabulary adaptation method for
  large language models that learns optimal linear combinations of existing embeddings
  using adapter modules while keeping the original model weights fixed. The method
  addresses limitations of prior approaches that rely on heuristics or external embeddings.
---

# Adapters for Altering LLM Vocabularies: What Languages Benefit the Most?

## Quick Facts
- arXiv ID: 2410.09644
- Source URL: https://arxiv.org/abs/2410.09644
- Reference count: 40
- Key result: VocADT vocabulary adaptation consistently outperforms baseline models across 11 languages on multilingual tasks

## Executive Summary
This paper introduces VocADT, a vocabulary adaptation method for large language models that learns optimal linear combinations of existing embeddings using adapter modules while keeping original model weights fixed. The approach addresses limitations of prior methods that rely on heuristics or external embeddings. Tested across 11 languages with diverse scripts and resource levels, VocADT consistently outperforms the original Mistral model and strong baselines on multilingual tasks including natural language understanding and machine translation. The study finds that Latin-script languages and highly fragmented languages benefit most from vocabulary adaptation, and the method remains effective even after full-weight fine-tuning for downstream tasks.

## Method Summary
VocADT adapts LLM vocabularies by training adapter modules to learn optimal linear combinations of existing embeddings while keeping the original model weights fixed. The method uses a new vocabulary of 50k tokens created with SentencePiece from monolingual corpora, initializes adapter matrices with overlapping token copying and partitioned averaging, then trains adapters using language modeling loss with frozen base model weights. After training, adapters are merged with original embeddings to create the new vocabulary. The approach can be applied to multilingual vocabularies grouping languages with similar scripts or language-specific vocabularies, and remains effective even after full-weight fine-tuning for downstream tasks using the ALMA protocol.

## Key Results
- Latin-script languages and highly fragmented languages benefit most from vocabulary adaptation
- VocADT consistently outperforms original Mistral model and strong baselines across multilingual NLU and MT tasks
- Vocabulary adaptation remains the most effective approach even after full-weight fine-tuning for machine translation
- Multilingual vocabularies offer better scalability than language-specific adaptations while maintaining competitive performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Learning optimal linear combinations of existing embeddings through adapter modules enables effective vocabulary adaptation without full model retraining.
- **Mechanism**: The vocabulary adapter module learns a transformation matrix that maps new vocabulary tokens to weighted combinations of existing embeddings. This learned combination captures semantic relationships between the original and new vocabulary, allowing the model to leverage its pre-trained knowledge when processing new tokens.
- **Core assumption**: The semantic meaning of new vocabulary tokens can be adequately represented as linear combinations of existing embeddings.
- **Evidence anchors**:
  - [abstract]: "adapter modules that are trained to learn the optimal linear combination of existing embeddings"
  - [section 3.1]: "We introduce the vocabulary adapter modules to find parameters of new embeddings that can replace the original embedding without changing the non-embedding part of the original model"
  - [corpus]: Weak evidence - the corpus doesn't directly address this mechanism but contains related work on vocabulary adaptation
- **Break condition**: This mechanism breaks when new vocabulary tokens represent concepts that cannot be decomposed into linear combinations of existing embeddings, such as in non-alphabetic scripts like Korean Hangul where tokens represent entire syllables rather than decomposable subwords.

### Mechanism 2
- **Claim**: Fixing the original model weights while training only the adapter modules preserves the pre-trained knowledge while allowing flexible vocabulary adaptation.
- **Mechanism**: By keeping the original transformer weights frozen, the adapter module can focus on learning how to map new vocabulary to the existing semantic space without disrupting the pre-trained representations. This approach avoids catastrophic forgetting while enabling efficient adaptation.
- **Core assumption**: The original model's representations are sufficiently general to handle the new vocabulary when properly mapped through learned combinations.
- **Evidence anchors**:
  - [abstract]: "while keeping the model's weights fixed"
  - [section 3.1]: "The module gradually adapts to new vocabularies through training while keeping all weights of the original model fixed"
  - [corpus]: Contains related work on parameter-efficient adaptation methods like LoRA and MAD-X that also preserve original weights
- **Break condition**: This mechanism breaks when the new vocabulary requires fundamental changes to the model's understanding that cannot be captured through embedding transformations alone.

### Mechanism 3
- **Claim**: Multilingual vocabulary grouping with consistent scripts provides better performance than language-specific vocabularies while maintaining scalability.
- **Mechanism**: Grouping languages with similar scripts (e.g., Latin-script languages together) creates a shared semantic space that captures cross-linguistic similarities while avoiding interference from script differences. This approach balances coverage and specificity.
- **Core assumption**: Languages with similar scripts share enough semantic and structural similarities to benefit from shared vocabulary representations.
- **Evidence anchors**:
  - [abstract]: "Latin-script languages and highly fragmented languages benefit the most from vocabulary adaptation"
  - [section 6.3]: "Consistent script within a group provides minor benefits"
  - [corpus]: Contains related work on multilingual vocabularies and script-based grouping strategies
- **Break condition**: This mechanism breaks when script similarity doesn't correlate with semantic similarity, or when languages with different scripts benefit more from shared representations than from script-consistent grouping.

## Foundational Learning

- **Concept: Linear algebra and matrix operations**
  - Why needed here: The adapter module is essentially a learnable matrix that transforms new vocabulary embeddings through matrix multiplication with existing embeddings
  - Quick check question: If A is a 50k×32k adapter matrix and E is a 32k×h embedding matrix, what are the dimensions of the resulting new embedding matrix?

- **Concept: Subword tokenization and vocabulary fragmentation**
  - Why needed here: Understanding how different scripts and languages are tokenized is crucial for predicting which languages benefit most from vocabulary adaptation
  - Quick check question: Why do languages like Greek experience more over-fragmentation than Latin-script languages in standard tokenizers?

- **Concept: Transfer learning and catastrophic forgetting**
  - Why needed here: The method relies on preserving pre-trained knowledge while adapting to new vocabulary, requiring understanding of how to balance old and new information
  - Quick check question: How does freezing original model weights while training only adapter modules help prevent catastrophic forgetting?

## Architecture Onboarding

- **Component map**: Mistral-7B base model (fixed weights) -> Input adapter module (learned) -> Output adapter module (learned) -> Original embedding matrix (32k tokens) -> New embedding matrix (50k tokens) -> Tokenizer for new vocabulary -> Training pipeline with frozen base model

- **Critical path**: 
  1. Initialize adapter with heuristic-based weights for overlapping tokens
  2. Train adapter using language modeling loss with frozen base model
  3. Merge adapter with original embeddings to create new vocabulary
  4. (Optional) Fine-tune full model for downstream tasks

- **Design tradeoffs**:
  - Flexibility vs. performance: Multilingual vocabularies offer better scalability but may sacrifice some language-specific performance compared to monolingual adaptations
  - Training efficiency vs. quality: Adapter-only training is faster but may require careful initialization to achieve good performance
  - Script consistency vs. coverage: Grouping by script provides minor benefits but limits language coverage

- **Failure signatures**:
  - Poor performance on languages with non-alphabetic scripts (e.g., Korean Hangul)
  - Degradation in performance for overlapping tokens when auxiliary loss is used with Latin scripts
  - Limited improvement for languages with low fragmentation

- **First 3 experiments**:
  1. Compare adapter-only training vs. full fine-tuning on a single language to verify that adapters can capture vocabulary adaptation
  2. Test different initialization strategies for the adapter (random vs. heuristic-based) on a Latin-script language
  3. Evaluate performance differences between multilingual and monolingual vocabularies on a set of similar languages

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the linear combination assumption for new embeddings hold equally well across all language types, particularly for non-alphabetic scripts like Korean?
- Basis in paper: [inferred] The paper notes that Korean performance declined more than expected despite significant fragmentation improvement, and the authors hypothesize this may be due to the linear combination assumption being insufficient for non-alphabetic scripts where tokens represent entire syllables rather than individual phonemes.
- Why unresolved: The paper only tested a limited set of languages and didn't conduct controlled experiments specifically isolating the impact of script type on the linear combination assumption's validity.
- What evidence would resolve it: Systematic experiments comparing vocabulary adaptation performance across alphabetic vs non-alphabetic scripts while controlling for fragmentation levels would clarify whether the linear combination assumption is the limiting factor for non-alphabetic languages.

### Open Question 2
- Question: How does the auxiliary loss parameter α affect performance across different language groups, and is there an optimal strategy for setting this hyperparameter?
- Basis in paper: [explicit] The paper mentions that α=0 works better for Latin script languages while non-zero values help Mixed group languages, but doesn't provide a systematic analysis of how α should be tuned for different language characteristics.
- Why unresolved: The paper only tested a few α values (0, 0.1, 0.2) and didn't explore the full parameter space or develop guidelines for language-specific tuning.
- What evidence would resolve it: A comprehensive ablation study testing multiple α values across all language groups, potentially combined with language-specific features (script type, fragmentation level, resource availability), would reveal optimal tuning strategies.

### Open Question 3
- Question: Can vocabulary adaptation methods maintain their effectiveness after fine-tuning when applied to different base models or when scaling to hundreds of languages?
- Basis in paper: [explicit] The paper tested scalability to 11 languages and generalizability to LLaMA, finding consistent trends, but didn't test with base models significantly different from Mistral/LLaMA or explore scaling to hundreds of languages.
- Why unresolved: The experiments were limited to two base models and a relatively small number of languages, leaving questions about performance at scale and with diverse model architectures.
- What evidence would resolve it: Experiments testing vocabulary adaptation on base models with different architectural properties (smaller/larger, different training objectives) and scaling to hundreds of languages while measuring adaptation effectiveness after fine-tuning would address these questions.

## Limitations

- The linear combination assumption may not hold for non-alphabetic scripts like Korean Hangul where tokens represent entire syllables rather than decomposable subwords
- Limited monolingual corpus size (max 2M tokens/language) may not capture full vocabulary complexity for resource-rich languages
- Primary evaluation focuses on machine translation tasks, potentially limiting generalizability to other NLP applications

## Confidence

**High Confidence**: The core claim that vocabulary adaptation improves performance compared to the original model and that Latin-script languages benefit most from this adaptation.

**Medium Confidence**: The claim that multilingual vocabularies provide better scalability than language-specific adaptations while maintaining competitive performance.

**Low Confidence**: The assertion that vocabulary adaptation remains beneficial even after full-weight fine-tuning for downstream tasks.

## Next Checks

1. **Cross-script generalization test**: Evaluate VocADT on additional non-alphabetic languages (e.g., Japanese, Thai, or Chinese) to verify whether the linear combination assumption holds for logographic and abugida scripts. Compare performance against baseline tokenizers and measure token count reduction alongside accuracy improvements.

2. **Data scaling sensitivity analysis**: Vary the amount of monolingual corpus used for vocabulary creation (e.g., 500k, 1M, 2M, 5M tokens) for both high-fragmentation and low-fragmentation languages to determine whether the 2M token limit is sufficient for optimal vocabulary adaptation across different resource levels.

3. **Task diversity validation**: Test VocADT-adapted models on non-translation tasks including abstractive summarization, long-form question answering, and code generation to assess whether vocabulary adaptation provides consistent benefits across diverse NLP applications beyond the primarily translation-focused evaluation presented.