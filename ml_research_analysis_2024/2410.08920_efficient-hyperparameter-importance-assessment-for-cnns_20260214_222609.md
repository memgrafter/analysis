---
ver: rpa2
title: Efficient Hyperparameter Importance Assessment for CNNs
arxiv_id: '2410.08920'
source_url: https://arxiv.org/abs/2410.08920
tags:
- hyperparameter
- importance
- hyperparameters
- diff
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper quantifies hyperparameter importance for CNNs using
  the N-RReliefF algorithm. The study trains over 10,000 CNN models across 10 image
  classification datasets to assess 11 hyperparameters.
---

# Efficient Hyperparameter Importance Assessment for CNNs

## Quick Facts
- arXiv ID: 2410.08920
- Source URL: https://arxiv.org/abs/2410.08920
- Authors: Ruinan Wang; Ian Nabney; Mohammad Golbabaee
- Reference count: 17
- Key outcome: N-RReliefF algorithm identifies number of convolutional layers (0.385) as most important hyperparameter for CNNs

## Executive Summary
This paper introduces an efficient method for quantifying hyperparameter importance in convolutional neural networks using the N-RReliefF algorithm. The study trains over 10,000 CNN models across 10 diverse image classification datasets to assess 11 hyperparameters. By measuring how hyperparameter differences correlate with performance differences across neighboring instances, the method identifies the relative importance of each hyperparameter. The approach is validated through Intraclass Correlation Coefficient analysis and comparison with FANOVA, demonstrating robust and consistent importance rankings.

## Method Summary
The N-RReliefF algorithm quantifies hyperparameter importance by finding J nearest neighbors for each hyperparameter configuration in the sampled space, then computing cumulative differences in both hyperparameters and performance, weighted by distance. The algorithm normalizes these differences by their ranges to ensure comparability across different hyperparameter types (numerical vs. categorical). Importance weights are calculated as the correlation between hyperparameter variations and model performance variations in local neighborhoods. The method is validated using ICC analysis across 10 random subsamples and compared with FANOVA for robustness verification.

## Key Results
- Top five important hyperparameters: number of convolutional layers (0.385), learning rate (0.228), dropout rate (0.131), optimizer (0.042), and epoch (0.042)
- ICC validation shows high consistency (ICC = 0.989) across multiple subsamples
- Hyperparameters associated with earlier convolutional layers have greater impact, with layer 1 filters being most influential
- FANOVA comparison confirms robustness of importance rankings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The N-RReliefF algorithm efficiently ranks hyperparameter importance by quantifying how hyperparameter differences correlate with performance differences across neighboring instances.
- Mechanism: N-RReliefF samples hyperparameter configurations, finds J nearest neighbors for each, and computes cumulative differences in both hyperparameters and performance, weighted by distance. These differences are then normalized to estimate importance weights.
- Core assumption: Hyperparameter importance can be inferred from the correlation between hyperparameter variations and model performance variations in the neighborhood of configurations.
- Evidence anchors:
  - [abstract]: "The method calculates importance weights by measuring how hyperparameter differences correlate with performance differences across neighboring instances."
  - [section]: "The key idea of the Relief family of algorithms is to estimate the quality of an attribute by assessing how well the attribute values distinguish the outputs of the nearest neighbour instances."
- Break condition: If the performance landscape is highly irregular or if important hyperparameters are highly correlated, the local correlation assumption may fail, leading to misleading importance rankings.

### Mechanism 2
- Claim: Normalizing hyperparameter and performance differences by their ranges ensures comparability across different hyperparameter types (numerical vs. categorical).
- Mechanism: For numerical hyperparameters, differences are scaled by the range; for categorical ones, a binary difference (0 or 1) is used. This standardization allows the algorithm to treat all hyperparameters on a common scale.
- Core assumption: Hyperparameter differences should be measured on a comparable scale regardless of their original units or types.
- Evidence anchors:
  - [section]: "diff(θm1 , θ NNj1 ) denotes the difference between the values of the hyperparameter Θ1 for hm and hNNj. If Θ1 is numerical: diff(θm1 , θ NNj1 ) := |θm1 − θNNj1| / (max(θ1) − min(θ1)). And if Θ1 is non-numerical: diff(θm1 , θ NNj1 ) := (0, if θm1 = θNNj1; 1, otherwise)."
- Break condition: If hyperparameter ranges are not representative (e.g., due to sparse sampling), normalization may distort true importance.

### Mechanism 3
- Claim: Using ranks instead of raw distances in the distance weighting function makes the influence of neighbors consistent regardless of dataset peculiarities.
- Mechanism: The distance weight is computed using the rank of the neighbor among the J nearest, with an exponential decay based on rank. This avoids sensitivity to absolute distance scales.
- Core assumption: The relative order of neighbors is more informative than their absolute distances for assessing local influence.
- Evidence anchors:
  - [section]: "The rationale behind employing ranks rather than actual distances is that utilizing ranks standardizes the influence each instance has on the weight calculations, ensuring that the nearest instances—and those that follow—consistently exert the same level of impact regardless of the dataset's peculiarities."
- Break condition: If the dataset is very small or if performance changes are not smooth, rank-based weighting may fail to capture nuanced importance differences.

## Foundational Learning

- Concept: Euclidean distance in hyperparameter space
  - Why needed here: Used to find nearest neighbors in the N-RReliefF algorithm, which is essential for measuring local correlation between hyperparameter differences and performance.
  - Quick check question: How would you compute the distance between two hyperparameter configurations where some values are numerical and others are categorical?

- Concept: Conditional probability and Bayes' theorem
  - Why needed here: The derivation of N-RReliefF's importance weights relies on probabilistic reasoning about the relationship between hyperparameter differences and performance differences.
  - Quick check question: Why does N-RReliefF use conditional probabilities rather than simple correlations to assess importance?

- Concept: Intraclass Correlation Coefficient (ICC)
  - Why needed here: Used to validate the consistency of importance weight estimates across multiple subsamples, ensuring the robustness of the method.
  - Quick check question: What does an ICC value close to 1 indicate about the stability of importance rankings?

## Architecture Onboarding

- Component map: N-RReliefF algorithm -> Distance computation -> Nearest neighbor search -> Difference accumulation -> Normalization -> Importance ranking. Supporting components: Random/Bayesian sampling for data generation, ICC for validation, FANOVA for comparison.
- Critical path: Data generation (sampling + training) -> N-RReliefF importance estimation -> ICC validation -> FANOVA comparison -> Result interpretation.
- Design tradeoffs: Larger J (neighbors) increases stability but also computation; more iterations M improve estimate accuracy but increase runtime; normalization choices affect comparability.
- Failure signatures: Unstable ICC values indicate poor data coverage; mismatched importance rankings between N-RReliefF and FANOVA may suggest algorithmic bias; high importance for rarely sampled hyperparameters may indicate sampling gaps.
- First 3 experiments:
  1. Run N-RReliefF with J=5 and M=100 on a small synthetic dataset to verify the algorithm's basic functionality and output format.
  2. Increase J to 20 and M to 500 on a real dataset (e.g., CIFAR-10 subset) to observe stability improvements.
  3. Compare N-RReliefF importance rankings with FANOVA on the same dataset to validate consistency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does N-RReliefF maintain consistent hyperparameter importance rankings across different CNN architectures (LeNet, AlexNet, GoogleNet, ResNet) or does the importance hierarchy change based on architectural complexity?
- Basis in paper: [explicit] The paper states "Future work will focus on applying HIA to renowned CNN architectures such as LeNet, AlexNet, GoogleNet, and ResNet"
- Why unresolved: The current study only evaluates importance on basic CNN models without exploring how architectural variations affect hyperparameter importance rankings.
- What evidence would resolve it: Conducting the same N-RReliefF analysis across multiple established CNN architectures and comparing the resulting importance rankings would reveal whether architectural complexity fundamentally changes which hyperparameters matter most.

### Open Question 2
- Question: How does the hyperparameter importance ranking change when evaluating on datasets with different characteristics (e.g., medical imaging, satellite imagery, natural images) beyond the ten image classification datasets used?
- Basis in paper: [explicit] The paper uses ten diverse image datasets but acknowledges this is a limited sample space
- Why unresolved: The current study's conclusions may be dataset-specific, and the paper doesn't investigate whether the identified top hyperparameters remain consistently important across fundamentally different image domains.
- What evidence would resolve it: Extending the N-RReliefF analysis to include datasets from specialized domains like medical imaging or remote sensing would demonstrate whether the importance rankings generalize across different image types.

### Open Question 3
- Question: Does the N-RReliefF algorithm's performance degrade when applied to extremely large-scale CNN models (e.g., with millions of parameters) compared to the relatively small models used in this study?
- Basis in paper: [inferred] The paper trains over 10,000 models but doesn't specify the scale or parameter count of these CNNs, and mentions computational efficiency as a consideration
- Why unresolved: The paper validates N-RReliefF on moderate-sized CNNs but doesn't address scalability to modern large-scale models that dominate current research.
- What evidence would resolve it: Applying N-RReliefF to hyperparameter importance assessment on state-of-the-art large CNN architectures while measuring computational time and stability of importance weights would reveal scalability limitations.

## Limitations
- Results may not generalize beyond image classification datasets to other domains like NLP or reinforcement learning
- Effectiveness depends heavily on quality of neighbor selection and sampling strategy
- Focus on accuracy as primary metric may miss other important considerations like training time or model robustness
- Local correlation assumptions may not capture global importance patterns in highly non-linear hyperparameter spaces

## Confidence
- High confidence in the N-RReliefF algorithm's basic methodology and ICC validation approach
- Medium confidence in the relative importance rankings due to potential sampling biases and dataset-specific effects
- Low confidence in the absolute importance weights without cross-domain validation

## Next Checks
1. Replicate the analysis on a non-image dataset (e.g., tabular data or NLP task) to test domain generalizability of the importance rankings
2. Perform ablation studies by systematically removing the top-ranked hyperparameters and measuring performance degradation to validate their actual impact
3. Compare N-RReliefF results with other importance estimation methods (e.g., SHAP values or permutation importance) to assess consistency across different approaches