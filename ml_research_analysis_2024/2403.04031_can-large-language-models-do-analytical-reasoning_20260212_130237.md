---
ver: rpa2
title: Can Large Language Models do Analytical Reasoning?
arxiv_id: '2403.04031'
source_url: https://arxiv.org/abs/2403.04031
tags:
- reasoning
- team
- points
- scores
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the analytical reasoning capabilities of large
  language models (LLMs) using sports play-by-play data from NBA and NFL games. The
  task requires LLMs to compute the total points scored by each team within a given
  quarter based solely on the play-by-play descriptions.
---

# Can Large Language Models do Analytical Reasoning?

## Quick Facts
- arXiv ID: 2403.04031
- Source URL: https://arxiv.org/abs/2403.04031
- Reference count: 37
- One-line primary result: GPT-4 achieves up to 60.14% accuracy for NBA games and near-perfect accuracy for NFL games in analytical reasoning tasks requiring point computation from play-by-play data.

## Executive Summary
This paper evaluates the analytical reasoning capabilities of large language models using sports play-by-play data from NBA and NFL games. The task requires LLMs to compute total points scored by each team within a given quarter based solely on play-by-play descriptions. The study compares five state-of-the-art LLMs using three prompting techniques and a divide-and-conquer approach that segments data into smaller parts for individual processing. Results show that while GPT-4 demonstrates the highest performance, especially with the divide-and-conquer method, all models struggle with complex NBA tasks, highlighting significant challenges in LLM analytical reasoning capabilities.

## Method Summary
The study employs sports play-by-play data from NBA and NFL games, segmenting each game into quarters. Five LLMs (GPT-4, GPT-3.5, Claude-2.1, Gemini-Pro, Llama-2-70b) are tested using three prompting techniques: natural instructions, natural instructions with JSON formatting, and chain-of-thought prompting. A divide-and-conquer approach segments data into smaller parts with varying step sizes (n=1, 3, 10, 30). Performance is measured using quarter-level accuracy and Mean Absolute Percentage Error (MAPE) to evaluate the models' ability to compute total points scored by each team per quarter.

## Key Results
- GPT-4 achieved the highest performance, up to 60.14% accuracy for NBA games and near-perfect accuracy for NFL games
- Chain-of-thought prompting improved performance for GPT-4 and Claude-2.1 but had negligible or negative effects on GPT-3.5 and Gemini-Pro
- The divide-and-conquer approach generally improved performance, but optimal step size varied by model and task complexity

## Why This Works (Mechanism)

### Mechanism 1
Chain-of-Thought prompting improves reasoning performance only for models with pre-trained CoT-style reasoning capabilities. CoT prompting scaffolds reasoning by forcing the model to generate intermediate steps, which benefits models trained on CoT-style data (GPT-4, Claude-2.1) but not others (GPT-3.5, Gemini-Pro). Core assumption: Models trained on diverse CoT reasoning examples internalize reasoning strategies that are activated by explicit CoT prompts. Break condition: If CoT prompts are too long relative to context, they may cause models without CoT training to lose track of the original task, leading to lower performance.

### Mechanism 2
Divide-and-conquer approach improves performance by reducing per-step cognitive load, but performance depends critically on step size. Breaking long contexts into smaller segments allows the model to focus on localized reasoning without overwhelming attention span. However, too many segments introduce cumulative error. Core assumption: LLMs have a finite effective context window for reasoning; processing smaller chunks reduces the likelihood of missing key events. Break condition: If step size is too small, the model may ignore play-by-play data in favor of system instructions, leading to higher error; if too large, the model may fail to track reasoning across long sequences.

### Mechanism 3
Task complexity increases with context length and information density, and is mitigated by the presence of related but non-essential information. Longer contexts and denser scoring events increase the likelihood of the model missing or misordering scoring plays. Including related entities (player/team names) provides contextual anchors that help the model track reasoning. Core assumption: LLMs rely on both explicit scoring cues and contextual entity information to maintain reasoning coherence. Break condition: If context length is reduced below a threshold, the benefit of related information diminishes; if information density is extremely low, the model may overfit to noise.

## Foundational Learning

- Concept: Chain-of-Thought prompting
  - Why needed here: Many reasoning tasks require intermediate logical steps; CoT explicitly encourages the model to generate these steps.
  - Quick check question: What is the difference between a CoT prompt and a standard prompt, and when would you choose one over the other?

- Concept: Information density and its effect on reasoning
  - Why needed here: High information density can overwhelm the model's ability to track all events, especially in fast-paced sports like NBA games.
  - Quick check question: How does the ratio of scoring plays to total plays influence the likelihood of the model making errors?

- Concept: Divide-and-conquer decomposition
  - Why needed here: Large reasoning tasks can be split into smaller subtasks to reduce cognitive load per step, but the step size must be tuned.
  - Quick check question: Why might a smaller step size not always lead to better accuracy in a divide-and-conquer approach?

## Architecture Onboarding

- Component map: Data ingestion -> Task definition -> Model interface -> Prompting methods -> Divide-and-conquer engine -> Evaluation
- Critical path: 1. Load play-by-play data, 2. Segment into quarters, 3. Apply prompting method or divide-and-conquer, 4. Generate LLM response, 5. Parse and aggregate results, 6. Compute accuracy/MAPE
- Design tradeoffs: Prompt complexity vs. model performance; Step size vs. error accumulation; Related information inclusion
- Failure signatures: MAPE > 100 (hallucination), low precision/high recall (predicts non-existent events), high precision/low recall (misses actual events)
- First 3 experiments: 1. Test all models with natural instructions only on NBA quarters, 2. Apply divide-and-conquer with step size n=1 across all models, 3. Test CoT prompting on models with and without CoT training

## Open Questions the Paper Calls Out

### Open Question 1
Does the effectiveness of the divide-and-conquer approach scale with the complexity of the analytical reasoning task? The paper states that "as the complexity of the task decreases, the benefits of sophisticated methods diminish, rendering them unnecessary." This remains unresolved because the paper only tested the approach on sports analytics tasks without exploring other types of complex reasoning tasks to determine if the approach is universally beneficial or only for specific domains.

### Open Question 2
What is