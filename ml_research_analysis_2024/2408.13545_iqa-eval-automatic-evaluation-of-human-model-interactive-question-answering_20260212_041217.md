---
ver: rpa2
title: 'IQA-EVAL: Automatic Evaluation of Human-Model Interactive Question Answering'
arxiv_id: '2408.13545'
source_url: https://arxiv.org/abs/2408.13545
tags:
- human
- evaluation
- arxiv
- question
- interactions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IQA-EVAL, a framework for automatically evaluating
  interactive question answering systems using large language models. The core idea
  is to use a LLM-based Evaluation Agent (LEA) that can simulate human behaviors to
  generate interactions with IQA models and then automatically evaluate these interactions.
---

# IQA-EVAL: Automatic Evaluation of Human-Model Interactive Question Answering

## Quick Facts
- arXiv ID: 2408.13545
- Source URL: https://arxiv.org/abs/2408.13545
- Authors: Ruosen Li; Ruochen Li; Barry Wang; Xinya Du
- Reference count: 31
- Key outcome: IQA-EVAL achieves Pearson correlations around 0.6-0.756 with human evaluations for interactive question answering systems

## Executive Summary
IQA-EVAL introduces a novel framework for automatically evaluating interactive question answering (IQA) systems by using large language models to simulate human behaviors and interactions. The framework employs an LLM-based Evaluation Agent (LEA) that can engage in conversations with IQA models and then automatically assess the quality of these interactions. By incorporating personas, the framework can simulate different user preferences and behaviors, leading to more nuanced and representative evaluations. The evaluation demonstrates strong correlations with human judgments and provides new insights into the relationship between interactive and non-interactive model performance.

## Method Summary
The IQA-EVAL framework operates through a two-stage process: first, an LLM-based Evaluation Agent (LEA) generates interactive conversations with target IQA models by simulating human-like behaviors and preferences; second, the framework automatically evaluates these interactions using predefined metrics (fluency and helpfulness). The LEA can be assigned different personas to better represent diverse user types, with the Expert persona showing particularly strong performance. The evaluation metrics are computed based on the generated conversations, providing a comprehensive assessment of IQA system performance that captures both the quality of responses and the effectiveness of interactive engagement.

## Key Results
- Achieved Pearson correlation of 0.6 for fluency and helpfulness metrics with human evaluations
- Expert persona assignment improved helpfulness correlation to 0.756
- Benchmark results show interaction quality does not always align with non-interactive performance
- Demonstrated framework's ability to differentiate between five recent LLMs on complex and ambiguous questions

## Why This Works (Mechanism)
IQA-EVAL works by leveraging the capabilities of large language models to simulate realistic human interactions with IQA systems. The framework's success stems from its ability to generate contextually appropriate conversations that reflect real user behaviors, then apply consistent evaluation criteria to assess both the quality of responses and the effectiveness of the interaction. The persona-based approach enhances this by allowing the evaluation to capture different user needs and preferences, making the assessment more comprehensive and representative of diverse real-world usage scenarios.

## Foundational Learning
- LLM-based Evaluation Agents: AI models that can simulate human-like conversations and interactions, essential for generating realistic test scenarios for IQA systems.
  - Why needed: Human evaluation is expensive and time-consuming, requiring scalable automated alternatives
  - Quick check: Can the LEA maintain coherent conversations across multiple turns while staying on topic?

- Persona-based Simulation: The technique of assigning specific user characteristics to evaluation agents to better represent diverse user populations.
  - Why needed: Different users have varying needs, expertise levels, and communication styles that affect interaction quality
  - Quick check: Do different personas consistently produce different interaction patterns?

- Interactive Question Answering (IQA): A system where users can engage in multi-turn dialogues to refine and clarify their information needs.
  - Why needed: Real-world information seeking often requires clarification and iterative refinement
  - Quick check: Can the system handle follow-up questions and maintain conversation context?

## Architecture Onboarding

Component Map: User Query -> LEA (with Persona) -> IQA Model -> LEA Evaluation -> Automated Metrics

Critical Path: The evaluation process flows from initial query generation through interaction simulation to automated assessment, with the LEA serving as both conversation partner and evaluator.

Design Tradeoffs: The framework balances the need for realistic simulation against the computational cost of generating multiple interactions, while also managing the potential biases introduced by using LLMs as evaluators.

Failure Signatures: Poor correlation with human evaluations may indicate inadequate persona representation, insufficient conversation complexity, or limitations in the evaluation metrics themselves.

First 3 Experiments:
1. Generate conversations using different personas (Expert, Novice, Casual) and compare evaluation outcomes
2. Test correlation between automated and human evaluations using benchmark datasets
3. Evaluate IQA models on both interactive and non-interactive tasks to identify performance discrepancies

## Open Questions the Paper Calls Out
None

## Limitations
- Human evaluation datasets used for validation may not fully represent real-world interactive scenarios
- Persona-based simulation may not capture the full spectrum of human user behaviors and preferences
- LLM-based evaluation introduces potential biases from the underlying language models
- Framework reliability when using different LLMs as Evaluation Agents is not fully explored

## Confidence
High Confidence: Technical implementation of evaluation framework is well-documented and reproducible
Medium Confidence: Correlation results (0.6-0.756) show reasonable alignment but limited dataset scope warrants caution
Medium Confidence: Benchmarking results demonstrate expected patterns but may be influenced by specific evaluation conditions

## Next Checks
1. Test IQA-EVAL on independent, diverse human-human interactive QA conversations to assess generalization
2. Conduct systematic experiments varying persona types and their impact on evaluation outcomes across different question categories
3. Evaluate framework stability when using different LLMs as Evaluation Agents to quantify model-induced variability