---
ver: rpa2
title: Integrating Transit Signal Priority into Multi-Agent Reinforcement Learning
  based Traffic Signal Control
arxiv_id: '2411.19359'
source_url: https://arxiv.org/abs/2411.19359
tags:
- agents
- signal
- control
- intersection
- traffic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study develops multi-agent reinforcement learning (MARL) algorithms
  to integrate transit signal priority (TSP) into traffic signal control. The first
  part trains two MARL agents for background traffic control using value decomposition
  networks (VDN) architecture, achieving slightly better performance than coordinated
  actuated signals at v/c 0.95.
---

# Integrating Transit Signal Priority into Multi-Agent Reinforcement Learning based Traffic Signal Control

## Quick Facts
- arXiv ID: 2411.19359
- Source URL: https://arxiv.org/abs/2411.19359
- Reference count: 40
- Primary result: MARL-TSP reduces bus delay by 22-27% with only slight increases in side street delays

## Executive Summary
This study develops multi-agent reinforcement learning (MARL) algorithms to integrate transit signal priority (TSP) into traffic signal control. The research implements two TSP approaches: independent agents with decentralized training and execution (DTDE) and coordinated agents with centralized training and decentralized execution (CTDE). The CTDE approach achieves better stability and convergence while providing similar performance to DTDE, demonstrating that centralized training is advantageous for TSP implementation.

The study demonstrates that both TSP approaches effectively reduce bus delays across intersections while maintaining reasonable side street delay levels. The CTDE-trained agents show superior stability compared to DTDE-trained agents, suggesting that coordinated TSP strategies are preferable for real-world implementation. The research provides a framework for integrating transit signal priority into modern adaptive traffic signal control systems using MARL techniques.

## Method Summary
The study develops multi-agent reinforcement learning algorithms for traffic signal control with transit signal priority integration. It uses value decomposition networks (VDN) architecture to train two MARL agents for background traffic control at v/c 0.95. For TSP implementation, two approaches are formulated: independent agents trained with DTDE and coordinated agents trained with CTDE. The framework includes bus trajectory generation using uniform random selection from Excel files and evaluates performance across two intersection scenarios with varying bus delay factors.

## Key Results
- TSP implementation reduces bus delay by 22% (independent) and 27% (coordinated) compared to no TSP
- Side street delays increase slightly (1.5-1.7%) with TSP implementation
- CTDE-trained agents demonstrate better stability than DTDE-trained agents while achieving similar convergence
- MARL agents achieve slightly better performance than coordinated actuated signals at v/c 0.95

## Why This Works (Mechanism)
The MARL framework works by enabling multiple agents to learn optimal traffic signal control policies while coordinating transit signal priority. The centralized training (CTDE) approach allows agents to learn from collective experiences and share information about bus locations and arrival times, leading to more stable and effective TSP implementation. The decentralized execution ensures that each agent can make real-time decisions at their respective intersections while maintaining coordination through learned policies.

The value decomposition networks (VDN) architecture enables effective multi-agent learning by decomposing the joint action-value function into individual agent utilities. This decomposition allows each agent to learn its own policy while contributing to the overall system performance. The integration of TSP considerations into the reward function ensures that bus priority is balanced with overall traffic efficiency.

## Foundational Learning
1. Multi-Agent Reinforcement Learning (MARL) - Multiple agents learn simultaneously to optimize system-wide performance
   - Why needed: Traditional single-agent approaches cannot effectively handle the complex interactions between multiple intersections and transit vehicles
   - Quick check: Verify that agents can learn coordinated policies without centralized control during execution

2. Value Decomposition Networks (VDN) - Decomposes joint action-value function into individual agent utilities
   - Why needed: Enables scalable multi-agent learning by allowing agents to learn independently while contributing to collective goals
   - Quick check: Confirm that decomposed utilities sum to the joint action-value function

3. Centralized Training Decentralized Execution (CTDE) - Agents train together but act independently
   - Why needed: Provides balance between coordination benefits and real-world implementation constraints
   - Quick check: Validate that trained agents maintain coordination without communication during execution

4. Transit Signal Priority (TSP) - Traffic signal control strategy that gives priority to transit vehicles
   - Why needed: Improves transit reliability and reduces bus delays in mixed traffic environments
   - Quick check: Measure reduction in bus delay compared to baseline conditions

## Architecture Onboarding

Component map:
MARL agents (VDN) -> Traffic simulator -> Reward function -> TSP logic -> Bus trajectory generator

Critical path:
Bus trajectory generation -> MARL agent observation -> Action selection -> Traffic simulation -> Reward calculation -> Policy update

Design tradeoffs:
- CTDE vs DTDE training: CTDE provides better stability but requires more communication during training
- Individual vs joint reward functions: Individual rewards enable scalable learning but may lead to suboptimal coordination
- Fixed vs dynamic TSP activation: Fixed activation is simpler but may miss optimization opportunities

Failure signatures:
- Poor convergence: Agents fail to learn effective policies within training episodes
- Instability: Large policy updates cause oscillations in performance
- Suboptimal coordination: Agents learn conflicting strategies that degrade system performance

3 first experiments:
1. Test MARL agents on a single intersection with varying traffic demands
2. Implement TSP with fixed activation patterns to establish baseline performance
3. Compare CTDE and DTDE training on two intersections with different bus frequencies

## Open Questions the Paper Calls Out
None

## Limitations
- Validation limited to single traffic demand level (v/c 0.95) and small two-intersection network
- Comparison with coordinated actuated signals uses outdated HCM2010 procedures
- Bus trajectory generation relies on uniform random selection rather than realistic route patterns
- CTDE training assumes perfect communication between agents

## Confidence

High confidence:
- Bus delay reduction (22-27%) with TSP implementation compared to no TSP
- Superiority of centralized training (CTDE) over independent training (DTDE) for TSP

Medium confidence:
- MARL agents achieve "slightly better" performance than coordinated actuated signals
- Stability improvements from centralized training may not reflect real-world implementation challenges

Low confidence:
- Generalization to larger networks and different traffic demands
- Performance with realistic bus route patterns and passenger boarding effects

## Next Checks

1. Test the MARL-TSP algorithms on larger networks (3+ intersections) with varying traffic demands to assess scalability and robustness.

2. Compare performance against modern adaptive signal control systems using recent optimization methodologies beyond HCM2010.

3. Implement realistic bus route patterns with passenger boarding effects and evaluate how these operational characteristics affect TSP effectiveness.