---
ver: rpa2
title: 'Adversarial Decoding: Generating Readable Documents for Adversarial Objectives'
arxiv_id: '2410.02163'
source_url: https://arxiv.org/abs/2410.02163
tags:
- adversarial
- documents
- arxiv
- text
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Adversarial Decoding is a new method for generating readable adversarial
  text that evades detection while achieving multiple objectives. Unlike prior methods
  that produce gibberish or require high-perplexity content, Adversarial Decoding
  uses beam search with soft scoring functions to generate documents effective for
  RAG poisoning, jailbreaking, and defense evasion.
---

# Adversarial Decoding: Generating Readable Documents for Adversarial Objectives

## Quick Facts
- arXiv ID: 2410.02163
- Source URL: https://arxiv.org/abs/2410.02163
- Reference count: 27
- One-line primary result: Generates readable adversarial text that evades detection while achieving multiple objectives including RAG poisoning and jailbreaking

## Executive Summary
Adversarial Decoding is a new method for generating readable adversarial text that evades detection while achieving multiple objectives. Unlike prior methods that produce gibberish or require high-perplexity content, Adversarial Decoding uses beam search with soft scoring functions to generate documents effective for RAG poisoning, jailbreaking, and defense evasion. The method scores candidate texts using readability, embedding similarity, and task-specific objectives, producing documents that both retrieve successfully and influence LLM outputs.

## Method Summary
Adversarial Decoding extends beam search with scoring functions for multiple objectives including readability, retrieval, jailbreaking, and defense evasion. The method generates text token-by-token, scoring each candidate using soft scores derived from LLM logits rather than hard constraints. This allows simultaneous optimization of readability and adversarial goals. The approach produces documents with high embedding similarity to target queries while maintaining natural language properties that evade detection by perplexity and readability filters.

## Key Results
- Achieves up to 46% top-5 retrieval success in RAG poisoning attacks
- Demonstrates 30% overall success rate in jailbreaking attacks
- Effectively evades detection by Llama Guard and similar safety filters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Beam search with soft scoring functions produces readable adversarial text while maintaining adversarial objectives
- Mechanism: Adversarial Decoding replaces hard constraints with continuous scoring functions that guide generation token-by-token, allowing simultaneous optimization of readability and adversarial goals
- Core assumption: Readability can be effectively scored using LLM logits without requiring differentiability in the adversarial objective models
- Evidence anchors:
  - [abstract]: "Adversarial Decoding equips beam search with scoring functions for different objectives" and "produces readable documents for multiple adversarial objectives"
  - [section 5.1]: "Each scorer outputs a continuous score to guide generation" and "It is also important that optimization not require differentiation"
  - [corpus]: Weak evidence - the corpus contains papers about text generation but none specifically about soft scoring for adversarial text
- Break condition: If the readability scorer cannot produce reliable soft scores that correlate with human judgment, the method fails to generate truly readable text

### Mechanism 2
- Claim: Combining readability scoring with task-specific objectives creates documents effective for both retrieval and generation stages
- Mechanism: The method generates documents with high embedding similarity to target queries (ensuring retrieval) while also influencing LLM outputs (ensuring generation effectiveness), all while maintaining readability
- Core assumption: A single document can simultaneously satisfy multiple adversarial objectives without trade-offs that break either objective
- Evidence anchors:
  - [abstract]: Documents that "(1) embed into vectors that are similar to broad classes of adversary-chosen queries, thus ensuring their retrieval, and (2) influence generation in an adversary-chosen way"
  - [section 3.1]: "Retrieval objective... the embedding of the entire document must have high cosine similarity with the target queries" and "Generation objective... the retrieved adversarial documents must influence generation"
  - [corpus]: Weak evidence - the corpus shows related work on text generation and adversarial attacks but lacks direct evidence for dual-objective generation
- Break condition: If the combination of objectives creates conflicting gradients or token sequences that satisfy neither objective well

### Mechanism 3
- Claim: Avoiding high-perplexity prefixes/suffixes while maintaining effectiveness makes detection harder
- Mechanism: Unlike prior methods that use high-perplexity text to ensure retrieval, Adversarial Decoding generates text that is naturally readable while achieving adversarial goals, avoiding detection by perplexity and readability filters
- Core assumption: High-perplexity text is the primary detection vector for adversarial documents, and avoiding it significantly reduces detection rates
- Evidence anchors:
  - [abstract]: "unlike prior methods that produce gibberish or require high-perplexity content" and "Adversarial Decoding uses beam search with soft scoring functions to generate documents effective for RAG poisoning"
  - [section 4]: "Methods such as BEAST (Sadasivan et al., 2024) produce adversarial documents that are 'fluent,' i.e., have low perplexity. In this section, we show that fluency is not sufficient to evade detection"
  - [corpus]: Weak evidence - the corpus contains papers on adversarial attacks but none specifically analyzing detection evasion through avoiding high-perplexity text
- Break condition: If alternative detection methods (beyond perplexity and readability) can effectively identify the generated adversarial documents

## Foundational Learning

- Concept: Beam search algorithm
  - Why needed here: Adversarial Decoding builds upon beam search as its base generation method, extending it with scoring functions
  - Quick check question: How does beam search differ from greedy decoding, and what trade-off does it make between computational cost and output quality?

- Concept: Cosine similarity for embedding comparison
  - Why needed here: The method relies on cosine similarity between document embeddings and query embeddings to ensure retrieval in RAG systems
  - Quick check question: Why is cosine similarity preferred over Euclidean distance for comparing text embeddings in retrieval systems?

- Concept: Soft scoring functions vs hard constraints
  - Why needed here: The method uses soft scores (derived from LLM logits) rather than hard constraints to guide generation, allowing differentiable optimization
  - Quick check question: What advantage does using a soft score based on logit ratios have over using a hard yes/no classification from an LLM?

## Architecture Onboarding

- Component map:
  - LLM logits component → Proposes next token candidates
  - Readability scorer → Computes soft readability score from LLM logits
  - Task-specific scorers → Compute scores for retrieval, jailbreaking, or evasion objectives
  - KV cache → Speeds up generation by caching attention keys/values
  - Black-box embedding encoder → Computes embeddings for retrieval objective

- Critical path: LLM logits → Token selection → Scoring → Beam update → Output generation
- Design tradeoffs: 
  - Beam width vs. generation speed: Higher beam width increases quality but reduces speed
  - Soft score clipping: Clipping readability scores prevents extreme values but may limit expressiveness
  - Single vs. multiple scorers: More scorers enable more objectives but increase complexity and potential conflicts
- Failure signatures:
  - Text becomes gibberish: Scorers are not properly balanced or calibrated
  - Documents fail retrieval: Retrieval scorer is not properly weighted or the embedding encoder is not representative
  - Documents detected by filters: Soft scores are not effectively evading detection methods
- First 3 experiments:
  1. Generate text with only readability scorer enabled to verify it produces natural text
  2. Generate text with readability + retrieval scorer to verify it maintains readability while achieving retrieval objective
  3. Generate text with all scorers enabled for a complete objective (e.g., RAG poisoning) to verify the full pipeline works

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Adversarial Decoding vary when targeting different embedding encoders beyond the three tested (Contriever, GTE, and GTE-Qwen)?
- Basis in paper: [inferred] The paper shows results against three specific encoders but does not explore performance across a broader range of retrieval systems.
- Why unresolved: The evaluation was limited to three encoders, leaving open questions about generalizability to other state-of-the-art retrieval systems.
- What evidence would resolve it: Systematic testing of Adversarial Decoding against a diverse set of embedding models (e.g., OpenAI embeddings, Cohere, or other top MTEB leaderboard systems) with varying architectures and training data.

### Open Question 2
- Question: What is the minimum size of adversarial documents needed to achieve successful retrieval and generation objectives?
- Basis in paper: [explicit] The paper uses a fixed adversarial document length of 30 tokens but does not explore whether shorter or longer documents might be more effective.
- Why unresolved: The study uses a single fixed length, so the relationship between document length and attack effectiveness remains unknown.
- What evidence would resolve it: Systematic experiments varying adversarial document lengths (e.g., 10, 20, 30, 40, 50 tokens) while measuring both retrieval success rates and generation effectiveness.

### Open Question 3
- Question: How transferable are Adversarial Decoding attacks across different LLM architectures and alignment strategies?
- Basis in paper: [explicit] The paper notes that current techniques produce documents that do not transfer well across encoders, suggesting potential limitations in transferability.
- Why unresolved: While the paper demonstrates effectiveness against specific models (Llama, Qwen, Gemma), it does not test cross-model transferability or robustness to different alignment techniques.
- What evidence would resolve it: Cross-model evaluation where adversarial documents generated for one LLM are tested against other architectures (GPT, Claude, Mistral) and different alignment approaches to measure transferability rates.

### Open Question 4
- Question: What is the computational overhead of Adversarial Decoding compared to alternative attack methods when generating large numbers of adversarial documents?
- Basis in paper: [explicit] The paper mentions that Adversarial Decoding takes "under 2 minutes to generate a single adversarial document on an A40 GPU" but does not compare this to the time required by other methods.
- Why unresolved: The study provides generation time for one method but lacks comparative analysis with other adversarial generation techniques.
- What evidence would resolve it: Systematic benchmarking of generation time per document across different attack methods (Adversarial Decoding, BEAST, Phantom, GCG) when generating batches of 100-1000 documents, including both GPU and CPU implementations.

## Limitations

- Weak evidence base for core mechanisms: The paper relies on internal empirical results rather than comparison with established theory
- Detection evasion claims: Claims that avoiding high-perplexity text significantly reduces detection are based on limited comparisons
- Generalizability across tasks: Results come from specific datasets, limiting generalizability to other domains

## Confidence

- High confidence: The core method of using beam search with soft scoring functions for text generation is well-established
- Medium confidence: The claim that this approach produces text that evades detection while maintaining effectiveness is supported by internal comparisons
- Low confidence: The claim that soft scoring with LLM logits provides a superior mechanism for generating readable adversarial text compared to alternative approaches

## Next Checks

1. **Detection robustness testing**: Test generated documents against multiple detection methods beyond perplexity and readability filters, including statistical anomaly detection, embedding-based classification, and human evaluation to validate the claim that the method evades detection.

2. **Cross-dataset generalization**: Apply the method to datasets outside the evaluation scope (e.g., different RAG datasets, multilingual jailbreaking scenarios) to assess whether the performance gains generalize beyond the specific benchmarks used.

3. **Scoring function ablation**: Systematically disable individual scorers (readability, retrieval, jailbreak, evasion) in controlled experiments to quantify their individual contributions and identify potential conflicts or redundancies in the scoring approach.