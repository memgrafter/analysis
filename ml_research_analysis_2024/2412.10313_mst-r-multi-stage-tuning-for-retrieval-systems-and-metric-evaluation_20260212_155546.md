---
ver: rpa2
title: 'MST-R: Multi-Stage Tuning for Retrieval Systems and Metric Evaluation'
arxiv_id: '2412.10313'
source_url: https://arxiv.org/abs/2412.10313
tags:
- answer
- performance
- retrieval
- passage
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of developing effective retrieval-augmented
  generation systems for regulatory documents, which contain complex terminology and
  nuanced semantics. The authors propose MST-R, a multi-stage retrieval system that
  improves retrieval performance through domain adaptation.
---

# MST-R: Multi-Stage Tuning for Retrieval Systems and Metric Evaluation

## Quick Facts
- arXiv ID: 2412.10313
- Source URL: https://arxiv.org/abs/2412.10313
- Authors: Yash Malviya; Karan Dhingra; Maneesh Singh
- Reference count: 14
- The system achieves 12.1% improvement in Recall@10 and 23% improvement in MAP@10 over BGE baseline on regulatory document retrieval

## Executive Summary
This paper addresses the challenge of developing effective retrieval-augmented generation systems for regulatory documents, which contain complex terminology and nuanced semantics. The authors propose MST-R, a multi-stage retrieval system that improves retrieval performance through domain adaptation. The method consists of three stages: fine-tuning dense encoders using hard negative mining, combining sparse and dense retrievers using reciprocal rank fusion, and adapting a cross-attention reranker by fine-tuning on top-k retrieved results. The system achieves state-of-the-art retrieval performance on the RIRAG challenge. Additionally, the paper reveals that the RePASs metric used for answer evaluation can be easily gamed by trivial approaches, highlighting the need for better evaluation metrics in the regulatory domain.

## Method Summary
MST-R is a multi-stage retrieval system designed for regulatory documents. The first stage adapts dense encoders (E5-FT) through fine-tuning on domain-specific data using contrastive learning with triplet loss and online hard mining. The second stage combines multiple retriever types (BM25, BGE-EN-ICL, E5-FT, Q2Q) using reciprocal rank fusion to capture complementary strengths. The third stage fine-tunes a cross-attention reranker (ms-marco-MiniLM-L-6-v2) on top-k results from the first stage, using relevant passages from ground truth combined with hard negative sampling. This multi-stage approach achieves state-of-the-art retrieval performance while highlighting vulnerabilities in existing evaluation metrics.

## Key Results
- MST-R achieves 12.1% improvement in Recall@10 and 23% improvement in MAP@10 compared to BGE baseline on RIRAG challenge
- Passage Concatenation approach achieves RePASs score of 0.947, 130% higher than Llama3.1, revealing metric gaming vulnerability
- RePASs-N with N=3 improves overall RePASs by 20% compared to N=0, with 68% improvements in entailment and contradiction scores

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning dense retrievers using hard negative mining improves retrieval performance on domain-specific terminology. The system adapts dense encoders by fine-tuning them on the ObliQA dataset using contrastive learning with triplet loss and online hard mining. This process iteratively retrieves top-K passages, selects hard negatives from non-ground-truth retrievals, and optimizes the encoder to better distinguish relevant from irrelevant passages. The core assumption is that fine-tuning dense retrievers on domain-specific data with hard negatives improves their ability to capture nuanced semantic relationships in regulatory text.

### Mechanism 2
Reciprocal rank fusion of multiple retriever types captures complementary strengths of sparse and dense retrieval methods. The system combines BM25 (sparse lexical retrieval), BGE-EN-ICL (dense with prompt-based few-shot learning), E5-FT (dense domain-adapted), and Q2Q (query-to-query similarity) using reciprocal rank fusion. RRF aggregates results by treating each retriever's rank as an exponential decay of relevance. The core assumption is that different retriever types excel at different aspects of retrieval, and their combination through rank fusion produces better results than any single method.

### Mechanism 3
Fine-tuning a cross-attention reranker on top-k retrieved results from the first stage improves ranking quality through domain adaptation. After initial retrieval, the system fine-tunes a ms-marco-MiniLM-L-6-v2 reranker using binary classification on relevant vs. non-relevant passages. Hard negatives are selected from top-k results of different L1 retrievers that aren't ground truth relevant. The core assumption is that cross-attention rerankers can learn domain-specific relevance patterns when fine-tuned on retrieved results rather than general datasets.

## Foundational Learning

- **Concept**: Contrastive learning with triplet loss
  - **Why needed here**: Enables the dense retriever to learn meaningful semantic representations by pulling relevant passages closer and pushing irrelevant ones apart in embedding space
  - **Quick check question**: What is the difference between a positive, negative, and anchor example in triplet loss formulation?

- **Concept**: Reciprocal rank fusion (RRF)
  - **Why needed here**: Provides a principled way to combine rankings from heterogeneous retrievers without requiring distance calibration across different embedding spaces
  - **Quick check question**: How does the RRF score formula 1/(rank+β) implement exponential decay of relevance with rank position?

- **Concept**: Cross-attention reranking
  - **Why needed here**: Allows fine-grained comparison between query and passage at the token level, capturing semantic nuances missed by dense retrieval
  - **Quick check question**: Why is cross-attention more computationally expensive than bi-encoder approaches, and when is this cost justified?

## Architecture Onboarding

- **Component map**: Query input → BM25 retriever → BGE-EN-ICL retriever → E5-FT retriever → Q2Q retriever → Reciprocal Rank Fusion → Reranker (ms-marco-MiniLM-L-6-v2) → Top-k passages → Llama3.1 Instruct 8B → Answer output

- **Critical path**: Query → L1 retrievers → RRF fusion → L2 reranker → Answer generation

- **Design tradeoffs**:
  - Dense vs. sparse retrievers: Dense captures semantics but needs domain adaptation; sparse handles exact terminology matching
  - Cross-attention vs. bi-encoder: Cross-attention more accurate but slower; bi-encoder faster for large-scale retrieval
  - Fine-tuning vs. prompt-tuning: Fine-tuning adapts weights but requires more data; prompt-tuning preserves original model but may be less effective

- **Failure signatures**:
  - Low recall: Check if dense retrievers are properly domain-adapted or if BM25 dominates rankings
  - Poor reranking: Verify hard negative selection quality and that reranker isn't overfitting
  - Contradictory answers: Investigate if RePASs metric is gaming due to self-contradictory passages in corpus

- **First 3 experiments**:
  1. Test each L1 retriever individually on a small validation set to establish baseline performance and identify which contributes most to final results
  2. Validate RRF fusion by comparing against simple average fusion to confirm rank-based aggregation is beneficial
  3. Test reranker on held-out data to ensure it generalizes beyond training distribution and doesn't harm retrieval quality

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the optimal window size (N) for the RePASs-N metric to achieve both high entailment scores and avoid contradictions in regulatory text evaluation?
- **Basis in paper**: [explicit] The paper investigates RePASs-N with varying context sizes (N) and shows that N=3 improves RePASs by 20% compared to N=0, with 68% improvements in entailment and contradiction scores.
- **Why unresolved**: The paper only tests up to N=3 and doesn't explore larger window sizes or determine if there's a point of diminishing returns or optimal value.
- **What evidence would resolve it**: Systematic testing of RePASs-N with larger N values (4, 5, 6+) and analysis of when entailment improvement plateaus or contradiction scores start increasing.

### Open Question 2
- **Question**: Can a single comprehensive metric replace the RePASs metric that addresses its vulnerability to trivial optimizers while maintaining reference-free evaluation?
- **Basis in paper**: [explicit] The paper demonstrates that Passage Concatenation achieves a RePASs score of 0.947, 130% higher than Llama3.1, by simply concatenating retrieved passages, showing the metric's vulnerability.
- **Why unresolved**: The paper identifies the problem but doesn't propose or test alternative comprehensive metrics that could address this gaming issue.
- **What evidence would resolve it**: Development and testing of new reference-free metrics that incorporate additional constraints like answer conciseness, factual accuracy, or semantic uniqueness that would penalize trivial concatenation approaches.

### Open Question 3
- **Question**: How does domain adaptation of the answering LLM component affect overall system performance compared to using a frozen pre-trained model?
- **Basis in paper**: [explicit] The paper explicitly states that "Ideally, the answering LLM should also be adapted to the target domain" and acknowledges that using Llama3.1 without adaptation should be considered "minimum achievable performance."
- **Why unresolved**: The paper deliberately leaves LLM adaptation for future work, only using the pre-trained Llama3.1 as a baseline without exploring domain-specific fine-tuning.
- **What evidence would resolve it**: Comparative experiments fine-tuning the LLM on regulatory domain data versus using frozen models, measuring changes in RePASs, Recall@10, and other metrics.

### Open Question 4
- **Question**: What is the impact of duplicate or near-duplicate passages in regulatory corpora on retrieval evaluation metrics?
- **Basis in paper**: [explicit] The paper identifies duplicate passages in the dataset through analysis showing that "retriever is correctly retrieving these chunks but is getting wrongly penalized" and presents examples where ground truth and non-GT retrievals contain nearly identical information.
- **Why unresolved**: The paper raises this as an observation but doesn't quantify how widespread this issue is or its impact on metric calculation and system evaluation.
- **What evidence would resolve it**: Systematic analysis of the dataset to measure the prevalence of duplicate passages and experiments showing how this affects Recall@k and MAP@k scores, potentially suggesting modified evaluation protocols.

## Limitations

- The paper's claims about MST-R's superiority rely heavily on performance gains over the BGE baseline on a single dataset (RIRAG challenge), raising questions about generalizability to other specialized domains
- The revelation that the RePASs metric can be gamed by trivial approaches significantly undermines confidence in the reported answer quality improvements
- The evaluation is limited to one regulatory domain, raising questions about generalizability to other specialized domains

## Confidence

- **High confidence**: The multi-stage architecture and use of reciprocal rank fusion are well-established techniques with clear implementation details
- **Medium confidence**: The domain adaptation improvements through fine-tuning are plausible but depend on specific hyperparameter choices not fully detailed in the paper
- **Low confidence**: The claim that MST-R achieves "state-of-the-art retrieval performance" is questionable given the metric gaming issue and limited evaluation scope

## Next Checks

1. **Cross-dataset validation**: Test MST-R on multiple regulatory datasets beyond RIRAG to verify domain adaptation generalizes across different regulatory domains and writing styles

2. **Metric robustness test**: Conduct experiments where trivial approaches (e.g., random answer generation, self-contradictory passage selection) are evaluated using RePASs to quantify the extent of metric gaming and develop more robust evaluation alternatives

3. **Ablation study**: Systematically remove each component (BM25, BGE-EN-ICL, E5-FT, Q2Q, reranker) to quantify their individual contributions and verify that the hybrid approach provides meaningful improvements over simpler combinations