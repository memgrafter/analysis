---
ver: rpa2
title: Improving Retrieval for RAG based Question Answering Models on Financial Documents
arxiv_id: '2404.07221'
source_url: https://arxiv.org/abs/2404.07221
tags:
- context
- retrieval
- documents
- information
- chunk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates techniques to improve Retrieval Augmented
  Generation (RAG) pipelines for financial document question answering. It addresses
  key limitations including suboptimal chunking, similarity-relevance mismatch, and
  loss of document structure during retrieval.
---

# Improving Retrieval for RAG based Question Answering Models on Financial Documents

## Quick Facts
- arXiv ID: 2404.07221
- Source URL: https://arxiv.org/abs/2404.07221
- Reference count: 0
- Providing correct context (fake RAG) yields highest accuracy (0.573 LLM evaluation score)

## Executive Summary
This paper investigates techniques to improve Retrieval Augmented Generation (RAG) pipelines for financial document question answering. It addresses key limitations including suboptimal chunking, similarity-relevance mismatch, and loss of document structure during retrieval. The proposed methods include recursive chunking, query expansion with hypothetical document embeddings, metadata annotations, re-ranking using cross-encoders, and embedding fine-tuning. Experiments on the FinanceBench dataset show that providing correct context dramatically improves answer accuracy, while zero-shot techniques like query expansion and re-ranking moderately improve upon baseline RAG performance.

## Method Summary
The study evaluates various RAG enhancement techniques on financial documents using the FinanceBench dataset. Methods tested include baseline RAG with cosine similarity, fake RAG with ground truth context, query expansion using hypothetical document embeddings (HyDE), and cross-encoder re-ranking. The evaluation measures retrieval quality through page-level and paragraph-level accuracy, context relevance scores, and answer accuracy using BLEU, Rouge-L, cosine similarity, LLM evaluation, and answer faithfulness metrics. The experiments compare zero-shot approaches without domain-specific fine-tuning against a baseline RAG implementation.

## Key Results
- Fake RAG with correct context achieves highest accuracy (0.573 LLM evaluation score)
- Baseline RAG performs worst (0.204 LLM evaluation score)
- Query expansion (0.24) and re-ranking (0.256) show moderate improvements over baseline
- Context relevance is critical, with accuracy gaps exceeding 35% between methods
- Zero-shot techniques improve performance but don't match correct context retrieval

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Correct context retrieval dramatically improves answer accuracy
- Mechanism: Providing the correct context (fake RAG) bypasses retrieval limitations and allows the LLM to generate accurate answers
- Core assumption: The LLM's generation capability is sufficient when given appropriate context
- Evidence anchors: "Results highlight that effective context retrieval is critical for accurate answers"; "The method with the lowest accuracy was the base rag case, with an average LLM evaluation score of 0.204. The same LLM and prompt template was used for each method, only the way the context was retrieved differed. This means that the difference in the context fed into the model contributed to a gap in accuracy of over 35 percent"

### Mechanism 2
- Claim: Query expansion with hypothetical document embeddings improves retrieval by providing additional reasoning context
- Mechanism: HyDE generates a theoretical document based on the query, then performs similarity search using both the original query and hypothetical answer to find more relevant chunks
- Core assumption: The LLM can generate a reasonable hypothetical document that contains the necessary reasoning steps
- Evidence anchors: "With HyDE, instead of just doing a similarity search with just the user's original query, it uses an LLM to generate a theoretical document when responding to a query and then does the similarity search with both the original question and hypothetical answer"; "The zero shot methods of query expansion and re-ranking seem to have some improvement on the accuracy with LLM evaluation scores of 0.24 and 0.256 respectively"

### Mechanism 3
- Claim: Re-ranking using cross-encoders improves relevance over similarity-based retrieval
- Mechanism: Cross-encoder models score retrieved chunks based on relevance to the question, then re-rank them to prioritize relevance over similarity
- Core assumption: Cross-encoder models can better distinguish between relevant and merely similar chunks
- Evidence anchors: "Re-ranking algorithms is a method to prioritize the relevance over the similarity of the chunks... Cohere's re-ranking algorithm is a popular one and it along with others uses additional machine learning and natural language processing techniques to further evaluate relevance beyond a similarity search"; "The zero shot methods of query expansion and re-ranking seem to have some improvement on the accuracy with LLM evaluation scores of 0.24 and 0.256 respectively"

## Foundational Learning

- Concept: Vector embeddings and similarity search
  - Why needed here: Understanding how text chunks are converted to numerical representations and how retrieval works through cosine similarity or nearest neighbors
  - Quick check question: What is the difference between semantic similarity and relevance in the context of RAG retrieval?

- Concept: Document structure and chunking strategies
  - Why needed here: Financial documents have specific structures (tables, sections, headings) that affect how information should be chunked for effective retrieval
  - Quick check question: Why might uniform chunking be problematic for financial documents compared to recursive or element-based chunking?

- Concept: Evaluation metrics for RAG systems
  - Why needed here: Understanding how to measure retrieval quality (context relevance, page-level accuracy) and answer accuracy (BLEU, Rouge-L, LLM evaluation)
  - Quick check question: How does context relevance score differ from traditional similarity scores in evaluating retrieval performance?

## Architecture Onboarding

- Component map: Document Ingestion → Chunking Module → Embedding Generation → Vector Database → Query Processing → Retrieval Algorithm → Context Augmentation → LLM Generation → Evaluation

- Critical path: 1. User query → 2. Embedding generation → 3. Similarity search → 4. Re-ranking → 5. Top chunks selection → 6. Context augmentation → 7. LLM generation
  - Critical path bottleneck: Initial retrieval step (similarity search) determines the quality of subsequent steps

- Design tradeoffs:
  - Chunk size vs. context preservation: Larger chunks preserve context but may include irrelevant information
  - Computational cost vs. accuracy: HyDE and cross-encoders improve accuracy but increase processing time
  - Zero-shot vs. fine-tuned approaches: Zero-shot methods are more scalable but may achieve lower accuracy

- Failure signatures:
  - Low context relevance scores indicate poor chunk selection
  - High standard deviation in LLM evaluation scores suggests inconsistent retrieval performance
  - Answer faithfulness scores below threshold indicate hallucination or unsupported information generation

- First 3 experiments:
  1. Baseline RAG vs. Fake RAG comparison to quantify the impact of correct context
  2. A/B test of standard chunking vs. recursive chunking on a subset of financial documents
  3. Evaluation of HyDE vs. standard query expansion on question-answering accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does recursive chunking compare to element-based chunking in terms of retrieval accuracy for financial documents?
- Basis in paper: [explicit] The paper discusses both recursive chunking and element-based chunking techniques, mentioning that element-based chunking can preserve tables and document structure better.
- Why unresolved: The paper mentions that comprehensive results for chunking techniques were not available due to compute limitations and token limits.
- What evidence would resolve it: Comparative experiments measuring retrieval accuracy for both techniques on the FinanceBench dataset.

### Open Question 2
- Question: What is the impact of metadata annotations on retrieval performance when dealing with multiple financial documents?
- Basis in paper: [explicit] The paper discusses metadata annotations as a method to enhance retrieval but notes that results for this technique were not available due to integration challenges.
- Why unresolved: The paper states that results for metadata tagging techniques were not available as there was no clear way to integrate them with the existing pipeline.
- What evidence would resolve it: Experiments comparing retrieval performance with and without metadata annotations on a diverse set of financial documents.

### Open Question 3
- Question: How effective is fine-tuning embedding algorithms on domain-specific datasets compared to zero-shot methods for financial document retrieval?
- Basis in paper: [explicit] The paper mentions that future work will investigate how additional annotations/data can improve RAG via fine-tuning embedding algorithms, but focuses on zero-shot methods in this study.
- Why unresolved: The paper explicitly states that it focuses on zero-shot methods and does not include results from fine-tuning embedding algorithms.
- What evidence would resolve it: Comparative experiments measuring retrieval accuracy using fine-tuned embeddings versus zero-shot methods on the FinanceBench dataset.

## Limitations

- Small dataset size (150 rows from FinanceBench) may limit generalizability
- Heavy reliance on LLM-based evaluation rather than human judgment
- Missing implementation details for critical components like cross-encoder re-ranking model
- Incomplete comparison between zero-shot and fine-tuning approaches

## Confidence

**High Confidence**: The fundamental claim that correct context retrieval is critical for accurate answers (based on the dramatic performance gap between fake RAG and baseline RAG).

**Medium Confidence**: The effectiveness of query expansion with HyDE and cross-encoder re-ranking, as improvements are modest (0.24-0.256 LLM evaluation scores) and results may be sensitive to implementation details.

**Low Confidence**: The claim that these techniques are sufficient for production financial RAG systems without domain-specific fine-tuning, given the limited dataset size and lack of comprehensive comparison with fine-tuned approaches.

## Next Checks

1. **Implement cross-encoder re-ranking with specified model architecture** - Verify the exact model architecture and training procedure used for re-ranking to ensure faithful reproduction.

2. **Test on expanded FinanceBench subset** - Evaluate all techniques on a larger subset of the FinanceBench dataset (beyond the 150 rows used) to assess generalizability.

3. **Human evaluation comparison** - Conduct human evaluation of answers for a sample of questions to validate LLM evaluation scores and assess answer quality beyond automated metrics.