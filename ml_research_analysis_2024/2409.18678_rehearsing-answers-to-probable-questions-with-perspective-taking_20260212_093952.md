---
ver: rpa2
title: Rehearsing Answers to Probable Questions with Perspective-Taking
arxiv_id: '2409.18678'
source_url: https://arxiv.org/abs/2409.18678
tags:
- answers
- question
- answer
- financial
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of causal knowledge graphs (KGs) and
  large language models (LLMs) to assist in preparing answers to probable questions
  during professional oral presentations. The approach integrates relevant factual
  knowledge from KGs into LLM prompts to enhance the factual accuracy and depth of
  generated answers.
---

# Rehearsing Answers to Probable Questions with Perspective-Taking

## Quick Facts
- arXiv ID: 2409.18678
- Source URL: https://arxiv.org/abs/2409.18678
- Authors: Yung-Yu Shih; Ziwei Xu; Hiroya Takamura; Yun-Nung Chen; Chung-Chi Chen
- Reference count: 12
- Primary result: KG integration improves factual accuracy and inferential depth of LLM-generated answers in professional Q&A scenarios

## Executive Summary
This paper presents a novel approach for enhancing large language models' ability to generate accurate and contextually appropriate answers for professional oral presentations through integration with causal knowledge graphs. The method involves constructing domain-specific knowledge graphs from analysts' reports, financial news, and Q&A pairs, then using these as structured knowledge sources in LLM prompts. The system demonstrates improved performance in handling professional Q&A scenarios, particularly in financial contexts, by activating perspective-taking abilities through the use of specialized knowledge sources.

## Method Summary
The approach integrates causal knowledge graphs (KGs) with large language models (LLMs) to enhance answer preparation for professional oral presentations. The method constructs three types of KGs: AR-KG from analysts' reports, MG-KG from financial news, and KG-AQ from existing Q&A pairs. These KGs are then incorporated into LLM prompts using Chain-of-Thought prompting techniques. The system generates probable questions and corresponding answers by leveraging both the structured knowledge from KGs and the generative capabilities of LLMs. The framework includes components for knowledge graph construction, prompt engineering, and evaluation using both automatic metrics and human assessment.

## Key Results
- KG integration significantly improves accuracy of financial terminology in generated answers
- AR-KG (from analysts' reports) shows the most substantial performance boost for select LLMs
- Automatic and human evaluations confirm enhanced inferential depth in responses
- The approach successfully activates perspective-taking abilities in professional Q&A contexts

## Why This Works (Mechanism)
The method works by providing LLMs with structured, domain-specific knowledge that activates their ability to generate contextually appropriate responses. The causal knowledge graphs serve as external memory that compensates for LLMs' knowledge limitations while preserving their reasoning capabilities. By grounding responses in factual, domain-specific information, the system reduces hallucination and improves the precision of professional terminology usage.

## Foundational Learning
- Causal Knowledge Graphs: Why needed - provide structured, factual knowledge to ground LLM responses; Quick check - verify graph construction accuracy and coverage
- Chain-of-Thought Prompting: Why needed - enables step-by-step reasoning for complex Q&A scenarios; Quick check - validate prompt effectiveness through ablation studies
- Perspective-Taking in LLMs: Why needed - critical for generating contextually appropriate professional responses; Quick check - assess response appropriateness through domain expert evaluation

## Architecture Onboarding

**Component Map:**
Data Sources -> Knowledge Graph Construction -> Prompt Engineering -> LLM Response Generation -> Evaluation Pipeline

**Critical Path:**
Knowledge Graph Construction -> Prompt Engineering -> LLM Response Generation

**Design Tradeoffs:**
- Knowledge graph specificity vs. generalization across domains
- Prompt complexity vs. response generation speed
- Automated evaluation metrics vs. human assessment reliability

**Failure Signatures:**
- Inaccurate financial terminology indicating KG construction issues
- Generic responses suggesting ineffective prompt engineering
- Hallucinations pointing to insufficient knowledge grounding

**First Experiments:**
1. Ablation study comparing performance with and without KG integration
2. Cross-domain evaluation to test generalizability
3. Human evaluation focusing on perspective-taking assessment

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on financial terminology accuracy, potentially overlooking other quality dimensions
- KG construction methodology lacks detailed specification for generalizability
- Limited number of evaluators and unspecified domain expertise levels in human assessment

## Confidence

**High confidence:** KG integration improves factual accuracy in LLM-generated answers
**Medium confidence:** Enhanced perspective-taking abilities claim requires further validation
**Medium confidence:** Comparative performance of different KG sources needs deeper analysis

## Next Checks
1. Conduct longitudinal study to assess KG-enhanced response quality over time
2. Implement blinded study with domain experts to eliminate evaluation bias
3. Test approach across multiple professional domains beyond finance for generalizability