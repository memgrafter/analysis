---
ver: rpa2
title: Residual Multi-Task Learner for Applied Ranking
arxiv_id: '2411.09705'
source_url: https://arxiv.org/abs/2411.09705
tags:
- residual
- task
- resflow
- learning
- ctcvr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ResFlow introduces a lightweight multi-task learning framework
  that enables efficient cross-task information sharing via residual connections between
  corresponding layers of task networks. The method addresses the challenge of modeling
  diverse user feedback in e-commerce ranking systems, where existing approaches either
  lack explicit task relationship modeling or have scalability issues.
---

# Residual Multi-Task Learner for Applied Ranking

## Quick Facts
- arXiv ID: 2411.09705
- Source URL: https://arxiv.org/abs/2411.09705
- Authors: Cong Fu; Kun Wang; Jiahua Wu; Yizhou Chen; Guangda Huzhang; Yabo Ni; Anxiang Zeng; Zhiming Zhou
- Reference count: 40
- One-line primary result: ResFlow achieves 1.54% average improvement in CTCVR AUC and 1.29% OPU increase in online A/B tests

## Executive Summary
ResFlow introduces a lightweight multi-task learning framework for e-commerce ranking systems that uses residual connections between corresponding layers of task networks to enable efficient cross-task information sharing. The method addresses the challenge of modeling diverse user feedback by allowing sparse, high-commitment tasks (like ordering) to benefit from feature representations learned by dense, low-commitment tasks (like clicking). Experiments demonstrate consistent improvements over state-of-the-art methods across multiple e-commerce datasets, with successful deployment in Shopee Search's pre-rank module achieving a 1.29% increase in order-per-user without additional latency.

## Method Summary
ResFlow builds task networks with identical architectures and connects corresponding layers using residual connections, enabling progressive information transfer from low-commitment, dense tasks to high-commitment, sparse tasks. The framework uses weighted sum loss optimization and additive score fusion to combine task outputs for ranking. A twin-tower variant improves efficiency for large-scale systems. The method is particularly effective for sequentially dependent tasks and can be extended to longer task chains while maintaining computational efficiency.

## Key Results
- ResFlow achieves 1.54% average improvement in CTCVR AUC across multiple e-commerce datasets
- In online A/B tests on Shopee Search, ResFlow delivered a 1.29% increase in OPU without additional system latency
- The framework is now fully deployed in Shopee Search's pre-rank module

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Residual connections enable progressive information transfer from low-commitment, dense tasks to high-commitment, sparse tasks
- Mechanism: Each residual connection adds the output of the previous task's layer to the current task's layer, allowing the sparse task to benefit from the feature representations learned by the dense task
- Core assumption: Corresponding layers in task networks have analogous structural topology and shared dimensionality
- Evidence anchors:
  - [abstract] "enables efficient cross-task information sharing via residual connections between corresponding layers of task networks"
  - [section 3.2] "A logically reasonable residual connection would require its two endpoints to have a certain correspondence. ResFlow hence requires that the task networks to be residually connected have an analogous structural topology"
  - [corpus] Weak - corpus neighbors don't discuss residual connections in multi-task learning specifically
- Break condition: Task networks have fundamentally different architectures or when tasks are not progressively related

### Mechanism 2
- Claim: Residual learning in logits ensures non-increasing probability predictions for sequentially dependent tasks
- Mechanism: By adding negative residual logits to the previous task's logits, the resulting probability is guaranteed to be lower or equal to the previous task
- Core assumption: The ground truth probabilities show significant decreases between consecutive tasks
- Evidence anchors:
  - [section 4.4] "According to our experiments, such non-increase can naturally be learned by the task networks without additional regularizers in most cases"
  - [section 3.3] "For sequentially dependent actions...their predicted probability should be strictly non-increasing by definition"
  - [corpus] Weak - corpus neighbors don't discuss probability constraints in multi-task learning
- Break condition: Tasks are not sequentially dependent or ground truth probabilities don't show clear decreases

### Mechanism 3
- Claim: Additive score fusion outperforms multiplicative fusion for ranking items
- Mechanism: Additive fusion combines task scores linearly with learned weights, providing more stable control compared to multiplicative fusion which can be dominated by extreme values
- Core assumption: Different task scores provide complementary information that should be weighted rather than multiplied
- Evidence anchors:
  - [section 5.3] "we found to be more effective" (referring to additive fusion)
  - [section 5.3] "multiplicative fusion may be too sensitive to extreme values, e.g., if one score (e.g., CTCVR) is very small, the others could lose their voting rights"
  - [corpus] Weak - corpus neighbors don't discuss score fusion strategies
- Break condition: When one task score is overwhelmingly more important than others

## Foundational Learning

- Concept: Multi-task learning optimization with weighted sum of task losses
  - Why needed here: ResFlow builds upon standard MTL framework, requiring understanding of how to balance multiple task objectives
  - Quick check question: If you have 3 tasks with losses L1, L2, L3 and weights w1=0.3, w2=0.5, w3=0.2, what is the total loss?

- Concept: Residual learning and skip connections
  - Why needed here: Core mechanism of ResFlow relies on residual connections between task networks
  - Quick check question: In a standard ResNet block, if x is input and F(x) is learned transformation, what is the output?

- Concept: Sequential dependence and conditional probability
  - Why needed here: Understanding when residual connections are appropriate requires knowledge of task relationships
  - Quick check question: If P(click)=0.1 and P(order|click)=0.05, what is P(order)?

## Architecture Onboarding

- Component map: Task networks -> Residual connections -> Fusion layer -> Twin-tower variant
- Critical path:
  1. Build task networks with shared architecture
  2. Define task topology based on relationships
  3. Insert residual connections between corresponding layers
  4. Train with weighted multi-task loss
  5. Apply additive score fusion for ranking
- Design tradeoffs:
  - More residual connections provide better information flow but increase complexity
  - Additive fusion is more stable but may be less sensitive to extreme value relationships
  - Twin-tower architecture improves efficiency but requires careful residual link placement
- Failure signatures:
  - NaN values during training suggest numerical instability in residual connections
  - Performance worse than single task suggests inappropriate task topology
  - Slow convergence may indicate poor residual connection placement
- First 3 experiments:
  1. Implement basic NSE (naive shared embedding) baseline with same architecture
  2. Add single residual connection between CTR and CTCVR logits
  3. Add residual connections to all corresponding layers between CTR and CTCVR

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ResFlow perform when extended to longer task chains beyond three tasks, such as "click" → "add-to-cart" → "order" → "review"?
- Basis in paper: [explicit] The paper mentions that residual connections can be extended to longer progressive chains, such as "click" → "add-to-cart" → "order", but does not provide experimental results for chains longer than three tasks.
- Why unresolved: The paper only demonstrates effectiveness on two-task and three-task settings, leaving uncertainty about scalability and performance degradation in longer chains.
- What evidence would resolve it: Empirical results comparing ResFlow's performance on task chains of varying lengths (e.g., 2, 3, 4, 5 tasks) on real-world datasets, particularly measuring AUC and computational overhead.

### Open Question 2
- Question: What is the impact of different task topology strategies on ResFlow's performance in scenarios without clear sequential dependence?
- Basis in paper: [explicit] The paper experiments with different task topologies on the KuaiRand-Pure-S1 dataset, finding that building topologies based on sample sparsity works best, but does not explore other potential strategies like random or reverse ordering.
- Why unresolved: While sample sparsity-based ordering is shown to be effective, it's unclear if other strategies (e.g., task similarity, random order) could yield better or comparable results.
- What evidence would resolve it: Systematic comparison of ResFlow's performance across multiple task topology strategies (e.g., sparsity-based, random, reverse, similarity-based) on diverse datasets.

### Open Question 3
- Question: How does ResFlow handle tasks with non-progressive relationships, such as "click" and "add-to-cart" that can occur independently?
- Basis in paper: [inferred] The paper focuses on progressively related tasks, but e-commerce platforms often have tasks that are not strictly sequential (e.g., users may add items to cart without clicking).
- Why unresolved: The method's effectiveness for non-progressive tasks is not demonstrated, raising questions about its applicability in more complex scenarios.
- What evidence would resolve it: Experimental results comparing ResFlow's performance on datasets with non-progressive task relationships versus progressive ones, using metrics like AUC and F1-score.

## Limitations
- The effectiveness of residual connections depends heavily on having "progressively related" tasks with shared architectural topology
- While additive fusion is claimed to be superior, this conclusion is based on specific tasks and datasets and may not generalize
- The 1.29% OPU improvement in online tests lacks reported statistical significance and confidence intervals

## Confidence
- High confidence: The core mechanism of residual connections between corresponding layers is technically sound and the mathematical formulation is correct
- Medium confidence: The claim of 1.54% average improvement in CTCVR AUC is supported by experiments, but the exact magnitude may depend on specific dataset characteristics
- Low confidence: The generalization of ResFlow to arbitrary task topologies and the effectiveness of additive fusion versus multiplicative fusion across different domains are not thoroughly validated

## Next Checks
1. **Task Topology Sensitivity Analysis**: Systematically vary the ordering and selection of tasks in the residual connection framework to determine the boundaries of when the method provides benefits versus when it degrades performance.

2. **Statistical Validation of Online Results**: Conduct a more rigorous statistical analysis of the A/B test results, including confidence intervals and power analysis, to determine whether the observed improvements are statistically and practically significant.

3. **Architecture Flexibility Test**: Modify the residual connection framework to handle tasks with different architectural topologies (e.g., different layer sizes or activation functions) to assess whether the method can be generalized beyond "progressively related" tasks.