---
ver: rpa2
title: 'Embed and Emulate: Contrastive representations for simulation-based inference'
arxiv_id: '2409.18402'
source_url: https://arxiv.org/abs/2409.18402
tags:
- posterior
- data
- learning
- latent
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of simulation-based inference
  (SBI) for high-dimensional physical systems with complex, multimodal parameter posteriors.
  Traditional SBI methods struggle with high-dimensional data due to the computational
  burden of training high-dimensional emulators.
---

# Embed and Emulate: Contrastive representations for simulation-based inference

## Quick Facts
- arXiv ID: 2409.18402
- Source URL: https://arxiv.org/abs/2409.18402
- Authors: Ruoxi Jiang; Peter Y. Lu; Rebecca Willett
- Reference count: 23
- Primary result: Embed and Emulate (E&E) method uses contrastive learning to learn low-dimensional embeddings and emulators for efficient simulation-based inference in high-dimensional systems

## Executive Summary
This paper addresses the challenge of simulation-based inference (SBI) for high-dimensional physical systems with complex, multimodal parameter posteriors. Traditional SBI methods struggle with high-dimensional data due to the computational burden of training high-dimensional emulators. The proposed Embed and Emulate (E&E) method uses contrastive learning to learn a low-dimensional latent embedding of the data and a corresponding fast emulator in the latent space. E&E jointly trains an encoder to compress the high-dimensional data into a summary statistic and an emulator to map parameters to this summary statistic. The learned embeddings capture the likelihood-to-evidence ratio, enabling efficient posterior inference without expensive simulations or high-dimensional emulators.

## Method Summary
E&E learns a low-dimensional latent embedding of high-dimensional data using contrastive learning, then trains an emulator to map parameters to this embedding space. The method jointly trains an encoder (compressing data to embeddings) and an emulator (mapping parameters to embeddings) using symmetric inter-domain InfoNCE loss. The learned embeddings serve as sufficient statistics for the parameters, allowing efficient posterior inference without expensive simulations or high-dimensional emulators.

## Key Results
- E&E accurately captures complex, multimodal posteriors in high-dimensional systems
- Reduces computational time compared to traditional SBI methods requiring high-dimensional emulators
- Learns to ignore redundant parameters, focusing only on relevant information for parameter estimation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning in the symmetric InfoNCE loss learns a sufficient statistic for parameter estimation
- Mechanism: The symmetric inter-domain InfoNCE loss trains encoder and emulator to embed positive parameter-data pairs close together while pushing negative pairs apart in latent space. At the global optimum, this yields embeddings that exactly match the likelihood-to-evidence ratio structure, making the data embedding a sufficient statistic for the parameters
- Core assumption: The true posterior and likelihood belong to an exponential family with flexible learnable embeddings
- Evidence anchors:
  - [abstract]: "E&E learns a low-dimensional latent embedding of the data (i.e., a summary statistic) and a corresponding fast emulator in the latent space"
  - [section]: "The learned likelihood-to-evidence ratio, along with the prior, can then be used to construct and sample from the posterior parameter distribution"
  - [corpus]: Weak evidence - no direct citations about contrastive SBI in corpus
- Break condition: If the true posterior cannot be expressed in the exponential family form assumed, the learned embeddings may not be sufficient statistics

### Mechanism 2
- Claim: The symmetric form of the InfoNCE loss ensures the normalization constant is data-independent
- Mechanism: By requiring symmetry in forming negative pairs from both domains (parameters and data), the loss ensures the estimated likelihood-to-evidence ratio recovers the true ratio up to a normalization constant that does not vary with the data
- Core assumption: The symmetric loss structure is necessary for proper normalization
- Evidence anchors:
  - [abstract]: "The symmetric form of the inter-domain loss, which results in a better-behaved estimate of the posterior"
  - [section]: "the symmetric form of the loss acts as a regularizer that ensures that the estimated ratioˆrθ(ϕ, y) recovers the true likelihood-to-evidence ratior(ϕ, y) up to a normalization constantC(y) = C∗ that does not vary with the data"
  - [corpus]: No direct evidence in corpus
- Break condition: If the loss is asymmetric or if the model architecture breaks the symmetry, the normalization constant may become data-dependent, leading to poor empirical performance

### Mechanism 3
- Claim: Learning in the latent space reduces sample complexity compared to high-dimensional emulation
- Mechanism: By compressing high-dimensional data into a low-dimensional summary statistic and learning to emulate this compressed representation, E&E requires fewer samples than methods that emulate the full high-dimensional output space
- Core assumption: The relevant information for parameter estimation can be captured in a low-dimensional summary statistic
- Evidence anchors:
  - [abstract]: "eliminating the need to run expensive simulations or a high dimensional emulator during inference"
  - [section]: "These two componentsˆfθ, ˆgθ are jointly trained to reconstruct the likelihood-to-evidence ratior(ϕ, y), which then allows us to sample from the parameter posterior p(ϕ | y)"
  - [corpus]: Weak evidence - no direct citations about sample complexity improvements
- Break condition: If the relevant information for parameter estimation requires high-dimensional features, compression may lose critical information

## Foundational Learning

- Concept: Contrastive representation learning and InfoNCE loss
  - Why needed here: The core innovation uses contrastive learning to learn embeddings that capture the likelihood-to-evidence ratio structure
  - Quick check question: What is the difference between symmetric and one-sided InfoNCE loss, and why does symmetry matter for SBI?

- Concept: Exponential family distributions and sufficient statistics
  - Why needed here: The theoretical analysis relies on the posterior having an exponential family form, with the learned embeddings serving as sufficient statistics
  - Quick check question: How does the exponential family parameterization of the posterior relate to the learned embeddings being sufficient statistics?

- Concept: Likelihood-free inference and simulation-based inference (SBI)
  - Why needed here: The paper builds on existing SBI methods but addresses their limitations with high-dimensional data
  - Quick check question: What are the key limitations of traditional SBI methods like ABC when applied to high-dimensional data?

## Architecture Onboarding

- Component map: Encoder (ˆfθ) -> Low-dimensional embedding -> Emulator (ˆgθ)
- Critical path:
  1. Generate training data: {(ϕi, yi)} from simulator
  2. Train encoder and emulator jointly using symmetric InfoNCE loss
  3. During inference: Encode observation y once, then use emulator for posterior sampling
  4. Posterior sampling via acceptance-rejection or other methods
- Design tradeoffs:
  - Embedding dimension n: Smaller n forces stronger compression but may lose information
  - Temperature τ: Controls concentration of embeddings; needs tuning
  - Symmetric vs one-sided loss: Symmetric ensures better normalization but may be harder to optimize
  - Intra-domain regularization: Can improve embeddings but requires domain knowledge for augmentations
- Failure signatures:
  - Poor posterior estimates: Check if embeddings capture sufficient statistics (R² score low)
  - Unstable training: Check symmetry of loss contributions and normalization constant variation
  - High sample complexity: Check if embedding dimension is too small or model capacity insufficient
- First 3 experiments:
  1. Train on synthetic unimodal task and verify posterior accuracy and embedding quality (R² score)
  2. Test with redundant parameters to confirm emulator learns to ignore irrelevant dimensions
  3. Apply to Lorenz 96 system and compare posterior estimates with NRE-C and NPE-C baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the dimensionality of the latent space affect the balance between data compression and preservation of information necessary for accurate parameter estimation?
- Basis in paper: [explicit] The paper discusses how choosing a smaller latent space dimension n forces the embedding to perform dimensionality reduction, but does not provide quantitative guidance on optimal dimensions.
- Why unresolved: The relationship between latent space dimension and performance is not thoroughly explored, leaving the optimal choice of latent space dimension unclear.
- What evidence would resolve it: Systematic experiments varying the latent space dimension across multiple datasets and measuring both compression efficiency and parameter estimation accuracy would clarify this trade-off.

### Open Question 2
- Question: Can the Embed and Emulate method be extended to handle posteriors with heavy tails that lack a low-dimensional sufficient statistic?
- Basis in paper: [explicit] The discussion section acknowledges that the current method assumes posteriors can be modeled as exponential family distributions, and suggests adapting E&E to handle heavy-tailed posteriors would be valuable future work.
- Why unresolved: The theoretical framework and loss function are specifically designed for exponential family posteriors, and extending this to non-exponential family posteriors is not addressed.
- What evidence would resolve it: Developing and testing modified versions of the E&E method that can handle non-exponential family posteriors, along with theoretical analysis of convergence properties, would address this limitation.

### Open Question 3
- Question: How interpretable are the learned embeddings, and can they reveal meaningful features that control the data generation process?
- Basis in paper: [inferred] The discussion section speculates that analyzing learned summary statistics may point to interpretable features controlling the data generation process, but does not provide concrete analysis or results.
- Why unresolved: While the paper shows that embeddings can achieve optimal data compression, it does not investigate whether these embeddings correspond to interpretable physical or scientific quantities.
- What evidence would resolve it: Detailed analysis of learned embeddings across multiple scientific domains, demonstrating their correspondence to known physical parameters or processes, would establish interpretability.

## Limitations
- Theoretical guarantees depend on exponential family assumption for true posterior
- Symmetric InfoNCE formulation is crucial but may degrade with asymmetric data or architectures
- Effectiveness assumes relevant information can be captured in low-dimensional summary

## Confidence

**Confidence Labels**
- **High confidence**: The core mechanism of joint contrastive training of encoder and emulator is well-supported by empirical results and aligns with established contrastive learning theory
- **Medium confidence**: The theoretical convergence proof is valid under exponential family assumptions, but its applicability to real-world, non-exponential posteriors is uncertain
- **Low confidence**: The impact of the symmetric loss on empirical performance is asserted but lacks direct experimental comparison with asymmetric alternatives

## Next Checks

1. **Test exponential family violation**: Apply E&E to a simulated task where the posterior is known to be non-exponential and evaluate whether learned embeddings remain sufficient statistics
2. **Compare symmetric vs asymmetric loss**: Train E&E models using both symmetric and one-sided InfoNCE losses on the same task, measuring normalization constant stability and posterior accuracy
3. **Vary embedding dimensionality**: Systematically increase the embedding dimension and measure the trade-off between compression efficiency and posterior estimation accuracy on high-dimensional systems