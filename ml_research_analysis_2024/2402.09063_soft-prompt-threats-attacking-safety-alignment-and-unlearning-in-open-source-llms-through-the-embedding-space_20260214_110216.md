---
ver: rpa2
title: 'Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source
  LLMs through the Embedding Space'
arxiv_id: '2402.09063'
source_url: https://arxiv.org/abs/2402.09063
tags:
- attacks
- uni00000013
- attack
- embedding
- uni00000014
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates embedding space attacks as a threat model
  for open-source large language models (LLMs). The authors propose attacking the
  continuous embedding representation of input tokens to bypass safety alignment and
  extract unlearned information from unlearned models.
---

# Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space

## Quick Facts
- arXiv ID: 2402.09063
- Source URL: https://arxiv.org/abs/2402.09063
- Reference count: 40
- Primary result: Embedding space attacks achieve 100% success rate on four open-source models with orders of magnitude less computation than traditional methods

## Executive Summary
This paper investigates embedding space attacks as a novel threat model for open-source large language models. The authors demonstrate that directly manipulating the continuous embedding representations of input tokens can bypass safety alignment mechanisms and extract supposedly deleted information from unlearned models. Their experiments show that embedding attacks are significantly more efficient than discrete attacks or fine-tuning, achieving perfect attack success rates across multiple models while requiring substantially less computational resources. The work raises important security concerns about the robustness of safety alignment and unlearning mechanisms in open-source LLMs.

## Method Summary
The authors propose embedding space attacks that keep model weights frozen while optimizing the continuous embedding representations of input tokens. Using signed gradient descent, they perform 200 attack iterations for toxicity experiments and 100 for unlearning experiments with a step size of 0.001. The attacks generate 100 tokens following the instruction, adversarial embedding, and target. They evaluate on four open-source models (Llama2-7b-Chat, Llama3-8b-Chat, Vicuna-7b, Mistral-7b) using Harmbench for toxicity, Harry Potter Q&A for unlearning, and TOFU benchmark. The method achieves orders of magnitude less computation than traditional attacks while maintaining 100% attack success rate.

## Key Results
- Embedding attacks achieve 100% attack success rate on four open-source models (Llama2-7b-Chat, Llama3-8b-Chat, Vicuna-7b, Mistral-7b)
- Attack efficiency is orders of magnitude better than discrete attacks or fine-tuning, requiring significantly less computation
- Cumulative attack success rate reaches 30.9% on Harry Potter questions compared to 3.6% without attack for unlearning extraction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embedding space attacks bypass safety alignment more efficiently than discrete attacks or fine-tuning
- Mechanism: The attack directly manipulates continuous embedding vectors of input tokens, circumventing the discrete token-level safety filters that aligned models rely on
- Core assumption: Safety alignment mechanisms operate primarily at the discrete token level rather than the continuous embedding space
- Evidence anchors:
  - [abstract]: "embedding space attacks circumvent model alignments and trigger harmful behaviors more efficiently than discrete attacks or model fine-tuning"
  - [section 3]: "Embedding space attacks: In embedding space attacks, we keep the weights of a model frozen and attack the continuous embedding representation of its input tokens"
- Break condition: If safety mechanisms are extended to operate at the embedding level or if models implement robust embedding-space defense mechanisms

### Mechanism 2
- Claim: Embedding attacks can extract supposedly deleted information from unlearned models
- Mechanism: By optimizing adversarial perturbations in the embedding space, the attack can trigger the model to generate responses containing information that was supposedly removed during unlearning
- Core assumption: Unlearning methods leave residual information in the model's embedding space that can be accessed through adversarial optimization
- Evidence anchors:
  - [abstract]: "embedding space attacks can extract supposedly deleted information from unlearned models across multiple datasets and models"
  - [section 6]: "Embedding attacks expose considerable residual Harry Potter knowledge in the unlearned model. Under attack the accuracy of the unlearned model on the Q&A is close to the original non-unlearned model (30.9% vs. 34.5%)"
- Break condition: If unlearning methods are improved to completely remove information from both discrete and continuous representations

### Mechanism 3
- Claim: Embedding space attacks can extract pretraining data from LLMs
- Mechanism: The attack optimizes embeddings to make the model complete sequences that match training data, effectively extracting memorized content by finding embeddings that trigger the model to reproduce training examples
- Core assumption: LLMs retain some form of memorization of training data in their embedding space that can be accessed through adversarial optimization
- Evidence anchors:
  - [abstract]: "embedding space attacks can extract pretraining training data from LLMs. Specifically, we show that embedding space attacks optimized towards the completion of Harry Potter books transfer to unseen paragraphs not used during attack optimization"
  - [section 7]: "We created a dataset of sentence pairs from the first Harry Potter book... We then optimized a universal embedding space that was attached to the instruction to improve the prediction of the target sentence for the Llama3-8B model"
- Break condition: If models implement effective memorization prevention techniques or if the attack optimization fails to generalize to unseen data

## Foundational Learning

- Concept: Adversarial attacks in machine learning
  - Why needed here: Understanding the basic principles of adversarial attacks (optimization of inputs to cause desired model behavior) is essential for grasping how embedding space attacks work
  - Quick check question: What is the key difference between white-box and black-box adversarial attacks?

- Concept: Word embeddings and continuous representations
  - Why needed here: The attack operates in the continuous embedding space rather than discrete token space, requiring understanding of how text is represented as vectors
  - Quick check question: How do word embeddings differ from one-hot encoded representations?

- Concept: Safety alignment in LLMs
  - Why needed here: Understanding how safety mechanisms work in LLMs is crucial for understanding why embedding attacks can bypass them
  - Quick check question: What are common techniques used to align LLMs with safety requirements?

## Architecture Onboarding

- Component map:
  Tokenizer -> Embedding layer -> Transformer layers -> Output layer -> Attack module

- Critical path:
  1. Tokenize input instruction
  2. Convert tokens to embeddings
  3. Apply adversarial perturbation to embeddings
  4. Process perturbed embeddings through model
  5. Generate output based on modified embeddings

- Design tradeoffs:
  - Speed vs. effectiveness: More iterations improve attack success but increase computation time
  - Perturbation magnitude vs. quality: Larger perturbations may be more effective but could reduce output quality
  - Universal vs. individual attacks: Universal attacks generalize better but may be less effective per instance

- Failure signatures:
  - Low attack success rate despite high computation
  - Output quality degradation (high perplexity)
  - Overfitting to training data (poor generalization)
  - Ineffective against models with embedding-space defenses

- First 3 experiments:
  1. Reproduce basic embedding attack on a simple aligned model to verify understanding
  2. Compare attack success rate with discrete attacks on the same model
  3. Test attack generalization by training on subset of data and evaluating on held-out samples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do embedding space attacks scale to larger models like GPT-4 or other frontier models?
- Basis in paper: [inferred]
- Why unresolved: The paper only tests embedding attacks on models up to Llama-3-70b-Instruct, showing 100% success rate. The scalability to much larger, more capable models remains unexplored.
- What evidence would resolve it: Testing embedding attacks on frontier models and reporting attack success rates and computational efficiency.

### Open Question 2
- Question: What are the specific mechanisms by which embedding space attacks bypass safety alignment and unlearning?
- Basis in paper: [inferred]
- Why unresolved: The paper demonstrates that embedding attacks are effective but does not investigate the underlying mechanisms or reasons for their success.
- What evidence would resolve it: Analyzing the changes in model activations, attention patterns, or internal representations during embedding attacks.

### Open Question 3
- Question: Can embedding space attacks be used for more fine-grained manipulation of model behavior beyond just removing safety guardrails?
- Basis in paper: [inferred]
- Why unresolved: The paper focuses on removing safety alignment and extracting unlearned information, but the potential for more nuanced control is not explored.
- What evidence would resolve it: Demonstrating embedding attacks that induce specific, desired behaviors or outputs in models.

### Open Question 4
- Question: How can embedding space attacks be defended against in open-source models?
- Basis in paper: [explicit]
- Why unresolved: The paper demonstrates the effectiveness of embedding attacks but does not propose or evaluate any defensive measures.
- What evidence would resolve it: Proposing and testing defensive techniques that mitigate the impact of embedding space attacks.

### Open Question 5
- Question: What is the relationship between the magnitude of embedding perturbations and the quality/utility of generated text?
- Basis in paper: [explicit]
- Why unresolved: The paper shows that large perturbations can hurt generation quality but does not provide a detailed analysis of this relationship.
- What evidence would resolve it: Systematically varying perturbation magnitudes and measuring their impact on text quality metrics like perplexity and coherence.

## Limitations
- Attack effectiveness may be overestimated for models with different alignment techniques not tested in this study
- Transferability of embedding attacks to black-box scenarios remains untested, as the study uses white-box access
- The paper doesn't address potential defenses against embedding space attacks, leaving open questions about model robustness
- Evaluation focuses primarily on academic datasets (Harry Potter, TOFU) which may not represent real-world unlearning scenarios

## Confidence
- **High confidence**: The claim that embedding attacks are more efficient than discrete attacks or fine-tuning, supported by quantitative comparisons showing orders of magnitude less computation
- **Medium confidence**: The claim that embedding attacks can extract pretraining data, as this is demonstrated on Harry Potter but not extensively validated across diverse datasets
- **Medium confidence**: The claim about extracting unlearned information, as results are shown on specific datasets but may not generalize to all unlearning scenarios

## Next Checks
1. Test the embedding attack transferability from white-box to black-box settings by attacking a surrogate model and evaluating success on held-out models
2. Evaluate the effectiveness of embedding attacks against models with embedding-space defense mechanisms (e.g., adversarial training on embeddings)
3. Replicate the unlearning extraction experiments on real-world datasets beyond Harry Potter and TOFU to assess practical applicability