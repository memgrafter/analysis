---
ver: rpa2
title: Solving Rubik's Cube Without Tricky Sampling
arxiv_id: '2411.19583'
source_url: https://arxiv.org/abs/2411.19583
tags:
- state
- cube
- rubik
- states
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of solving the Rubik\u2019\
  s Cube in a sparse-reward reinforcement learning setting, where traditional methods\
  \ rely on sampling from near-solved states. The authors propose a novel approach\
  \ that trains a policy network directly from fully scrambled states using a neural\
  \ network (ChaseNet) to predict cost patterns between state pairs."
---

# Solving Rubik's Cube Without Tricky Sampling

## Quick Facts
- arXiv ID: 2411.19583
- Source URL: https://arxiv.org/abs/2411.19583
- Authors: Yicheng Lin; Siyu Liang
- Reference count: 15
- Primary result: 99.4% success rate on 2x2x2 Rubik's Cube without solved-state sampling

## Executive Summary
This paper presents a novel approach to solving the Rubik's Cube in a sparse-reward reinforcement learning setting without relying on sampling from near-solved states. The method employs a neural network (ChaseNet) to predict cost patterns between randomly scrambled states, enabling direct learning from fully scrambled configurations. By combining a warmup phase for training ChaseNet with an actor module using Proximal Policy Optimization (PPO), the approach achieves over 99.4% success rate on the 2x2x2 Rubik's Cube without tree search or solved-state sampling.

## Method Summary
The approach consists of two main phases: a warmup phase where ChaseNet learns to predict minimal move counts between randomly scrambled state pairs, and an actor phase where PPO optimizes a policy network using rewards derived from ChaseNet's cost predictions. The method compares fully connected and attention-based architectures for ChaseNet, with the attention variant achieving superior performance. The environment provides dense reward signals through negative log of predicted costs, allowing effective policy learning even from distant scrambled states.

## Key Results
- Achieved 99.4% success rate across 50,000 test cases on 2x2x2 Rubik's Cube
- Attention-based ChaseNet-Attention achieved Spearman correlation of 0.901 for cost prediction
- Fully connected ChaseNet-FC achieved correlation of 0.870, demonstrating attention's advantage
- Successfully solved cube without requiring tree search or solved-state sampling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training ChaseNet to predict cost patterns between randomly scrambled states enables policy learning without solved-state sampling
- Mechanism: NX Module collects state pairs from fully scrambled cubes during warmup, then trains ChaseNet to estimate minimal moves between states. This learned cost function provides dense rewards based on predicted distances to solved state
- Core assumption: Cost between state pairs can be accurately predicted by neural network from sticker representations alone
- Evidence anchors: Abstract confirms approach uses neural network to predict cost patterns; Section 2.2 defines cost as minimal scrambles needed; Corpus papers don't directly address cost prediction from scrambled states

### Mechanism 2
- Claim: Sticker-level transformer architecture in ChaseNet-Attention captures complex spatial dependencies better than fully connected layers
- Mechanism: ChaseNet-Attention computes attention scores between all sticker positions in combined start/end state representations, learning intricate relationships between cube scrambles
- Core assumption: Attention mechanism can effectively learn non-linear relationships between sticker positions determining cube state transitions
- Evidence anchors: Section 2.6 describes sticker-level transformer computing attention scores; Section 3.1 shows ChaseNet-Attention reached 0.901 correlation; No direct corpus evidence on transformer effectiveness for Rubik's Cube state representation

### Mechanism 3
- Claim: PPO with clipped objective using ChaseNet-predicted rewards can effectively optimize policy from fully scrambled states without search methods
- Mechanism: Actor Module refines policy network using PPO, where rewards are formulated as negative log of ChaseNet-predicted costs to solved state, providing dense feedback signals even from distant states
- Core assumption: Predicted cost-based reward signal is sufficiently informative for PPO to converge to effective policy without requiring tree search or near-solved state sampling
- Evidence anchors: Abstract notes remarkable success rate without search; Section 2.5 provides reward formulation; Section 3.2 confirms absence of search achieved remarkable success rate

## Foundational Learning

- Concept: Sparse reward environments and their challenges
  - Why needed here: Rubik's Cube has only one rewarded state (solved configuration) among billions of possible states, making standard RL approaches ineffective without special techniques
  - Quick check question: What is the primary challenge when applying standard RL to environments with only one rewarded state?

- Concept: Policy gradient methods and PPO
  - Why needed here: Method uses Proximal Policy Optimization to refine policy network based on rewards from ChaseNet predictions, requiring understanding of policy gradient optimization
  - Quick check question: How does PPO's clipped objective help prevent large policy updates that could destabilize learning?

- Concept: Neural network architectures for state representation
  - Why needed here: Method compares FC and attention-based architectures for predicting state costs, requiring understanding of how different architectures capture spatial relationships
  - Quick check question: What advantage does a transformer-based attention mechanism have over fully connected layers for learning relationships between cube sticker positions?

## Architecture Onboarding

- Component map: NX Module -> Env Module -> Actor Module
- Critical path: Scrambled state → ChaseNet cost prediction → reward calculation → PPO policy update → action selection → new state
- Design tradeoffs:
  - FC vs Attention: FC converges faster but attention achieves better final accuracy
  - Reward scaling: Base b=1.2 chosen empirically to rescale cost predictions
  - Data collection: Random scrambles vs policy-guided sampling during warmup
- Failure signatures:
  - ChaseNet loss plateaus early: Architecture may be insufficient for cost prediction
  - Policy success rate stays low: Reward signal may be too noisy or uninformative
  - High variance in training: PPO clipping parameter may need adjustment
- First 3 experiments:
  1. Test ChaseNet-FC vs ChaseNet-Attention on small dataset to verify attention advantage
  2. Verify reward signal quality by plotting ChaseNet predictions vs true costs on validation set
  3. Run policy training with simplified reward (e.g., linear function of predicted cost) to isolate PPO effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the ChaseNet architecture be effectively scaled to handle the 3x3x3 Rubik's Cube, given its significantly larger state space?
- Basis in paper: [explicit] The paper mentions that scaling to the 3x3x3 cube would require a much larger neural network to estimate costs across a more complex space, posing a key challenge for future work
- Why unresolved: The paper only tested the approach on the 2x2x2 Rubik's Cube, and it's unclear whether the current neural network architecture can handle the increased complexity of the 3x3x3 cube
- What evidence would resolve it: Testing the ChaseNet architecture on the 3x3x3 Rubik's Cube and comparing its performance to that of the 2x2x2 cube would provide evidence for its scalability

### Open Question 2
- Question: How does the proposed method perform in other sparse-reward environments beyond the Rubik's Cube?
- Basis in paper: [explicit] The paper suggests that broader testing across different sparse-reward environments will be essential to assess the generalizability and practicality of the method
- Why unresolved: The paper only tested the approach on the Rubik's Cube, and it's unclear how well it would perform in other sparse-reward environments with different characteristics
- What evidence would resolve it: Applying the method to various sparse-reward environments and comparing its performance to other state-of-the-art methods would provide evidence for its generalizability

### Open Question 3
- Question: Can the proposed method be further improved to achieve higher success rates or solve the Rubik's Cube more efficiently?
- Basis in paper: [inferred] The paper achieved a success rate of over 99.4% on the 2x2x2 Rubik's Cube, but it's unclear if there's room for further improvement in terms of success rate or efficiency
- Why unresolved: The paper doesn't discuss potential ways to improve the method's performance beyond what was achieved in the experiments
- What evidence would resolve it: Exploring different neural network architectures, reward formulations, or training strategies and comparing their performance to the proposed method would provide evidence for potential improvements

## Limitations

- Method's reliance on accurate ChaseNet cost predictions is critical - unreliable predictions could mislead policy learning
- Evaluation limited to 2x2x2 Rubik's Cube; scaling to larger cubes may require architectural modifications
- Warmup phase requires substantial computational resources for data collection and ChaseNet training

## Confidence

- **High Confidence**: Core claim that ChaseNet can learn to predict cost patterns between scrambled states is well-supported by 99.4% success rate on 50,000 test cases and correlation metrics showing strong prediction accuracy (0.901 for ChaseNet-Attention)
- **Medium Confidence**: Superiority of attention-based architecture over fully connected layers is demonstrated through ablation studies, but difference in final performance (0.901 vs 0.870 correlation) may not justify increased complexity in all cases
- **Low Confidence**: Claims about method's applicability to other sparse-reward problems lack direct evidence; computational efficiency compared to sampling-based methods is not quantified

## Next Checks

1. **Ablation on ChaseNet Architecture**: Systematically compare ChaseNet-FC and ChaseNet-Attention on increasingly complex cube sizes (2x2x2, 3x3x3) to identify scaling limits and determine whether attention mechanisms remain beneficial as state complexity increases

2. **Reward Signal Analysis**: Evaluate quality of ChaseNet's cost predictions by measuring prediction accuracy on states at different distances from solved configuration; plot success rate as function of initial scramble depth to identify whether method degrades for highly scrambled states

3. **Generalization to Other Domains**: Apply same two-phase approach to other combinatorial optimization problems with sparse rewards (e.g., Sokoban, Lights Out, or sliding puzzles) to test whether cost prediction framework transfers effectively to problems with different state transition structures