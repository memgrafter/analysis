---
ver: rpa2
title: 'Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning:
  A Benchmark'
arxiv_id: '2402.11592'
source_url: https://arxiv.org/abs/2402.11592
tags:
- optimization
- fine-tuning
- memory
- gradient
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks zeroth-order (ZO) optimization methods for
  memory-efficient fine-tuning of large language models (LLMs). The authors evaluate
  various ZO optimizers (ZO-SGD, ZO-Adam, etc.) against first-order (FO) optimizers
  across different LLM families, tasks, and fine-tuning schemes.
---

# Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark

## Quick Facts
- arXiv ID: 2402.11592
- Source URL: https://arxiv.org/abs/2402.11592
- Reference count: 39
- Primary result: ZO optimization methods offer memory-efficient alternatives to FO methods for LLM fine-tuning, with task alignment and forward gradient estimation being key factors for success.

## Executive Summary
This paper presents a comprehensive benchmark of zeroth-order (ZO) optimization methods for memory-efficient fine-tuning of large language models. The authors evaluate various ZO optimizers (ZO-SGD, ZO-Adam, etc.) against first-order (FO) optimizers across different LLM families, tasks, and fine-tuning schemes. They demonstrate that ZO methods can significantly reduce memory usage compared to FO methods, especially for long input sequences, while maintaining competitive accuracy. The study introduces enhancements like block-wise descent and hybrid ZO-FO training to improve ZO optimization accuracy while maintaining memory efficiency.

## Method Summary
The paper benchmarks ZO optimization methods by implementing six BP-free/ZO optimizers (ZO-SGD, ZO-Adam, ZO-SGD-Sign, ZO-SGD-MMT, ZO-SGD-Cons, Forward-Grad) and comparing them against first-order optimizers (FO-SGD, FO-Adam) across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral), three tasks of varying complexity (SST2, COPA, WinoGrande), and five fine-tuning schemes (Full-tuning, LoRA, prefix-tuning, prompt-tuning). The authors use task alignment through prompt engineering to align fine-tuning tasks with pre-training objectives, and implement enhancements like block-wise descent and hybrid ZO-FO training. Memory usage is measured using PyTorch's memory profiling tools, and accuracy is evaluated using standard classification metrics.

## Key Results
- ZO optimization methods achieve memory efficiency comparable to FO methods while maintaining competitive accuracy across diverse LLM families and tasks
- Task alignment through prompt engineering significantly improves ZO optimization effectiveness, with up to 10.5% performance drop when absent
- Block-wise ZO optimization reduces gradient estimation variance and improves performance for complex tasks like WinoGrande
- Forward gradient estimation with sufficient query budget can approach FO method performance while maintaining memory efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zeroth-order optimization reduces memory usage by eliminating the need for back-propagation and activation storage.
- Mechanism: Instead of computing gradients via back-propagation, ZO methods estimate gradients using function value differences along random directions, requiring only forward passes.
- Core assumption: Forward pass computation is significantly less memory-intensive than storing intermediate activations for back-propagation.
- Evidence anchors:
  - [abstract] "the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge" and "shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs"
  - [section] "ZO optimization serves as a gradient-free alternative to first-order (FO) optimization, approximating FO gradients through function value-based gradient estimates"
  - [corpus] Weak evidence - corpus papers focus on similar memory efficiency claims but lack detailed mechanistic breakdown
- Break condition: When sequence length becomes extremely large, the memory required for forward pass intermediate results may approach or exceed back-propagation memory savings.

### Mechanism 2
- Claim: Task alignment significantly improves ZO optimization effectiveness by making the fine-tuning task format consistent with pre-training objectives.
- Mechanism: By transforming downstream tasks into formats similar to pre-training (next token/sentence prediction), ZO methods can leverage the pre-trained model's learned representations more effectively.
- Core assumption: The pre-trained model's internal representations are optimized for the specific task format used during pre-training.
- Evidence anchors:
  - [section] "The 'task alignment' refers to aligning the fine-tuning task with the format of the pre-training task" and "As we can see, without prompt-based text alignment, there is a substantial performance drop across ZO fine-tuning methods"
  - [section] "crafting effective prompts for task alignment can be non-trivial, as prompt design is context-dependent"
  - [corpus] Weak evidence - corpus papers mention task alignment but don't provide detailed experimental validation
- Break condition: When the downstream task is fundamentally incompatible with the pre-training format, even with alignment attempts.

### Mechanism 3
- Claim: Block-wise ZO optimization reduces gradient estimation variance by processing parameters in smaller chunks.
- Mechanism: Instead of estimating gradients for the entire model at once, the model is divided into blocks, and ZO gradient estimation is performed block-wise, reducing the variance of each estimate.
- Core assumption: Smaller parameter blocks have lower-dimensional gradients, making finite-difference estimates more stable.
- Evidence anchors:
  - [section] "The key idea is to split the LLM into different blocks and then apply the ZO gradient estimator to each block of parameters"
  - [section] "Our rationale is that by conducting gradient estimation block-wise, the resulting gradient estimate's variance will be reduced"
  - [corpus] Weak evidence - corpus papers don't explicitly discuss block-wise optimization
- Break condition: When the overhead of multiple forward passes for block-wise estimation outweighs the variance reduction benefits.

## Foundational Learning

- Concept: Zeroth-order optimization fundamentals
  - Why needed here: Understanding how ZO methods approximate gradients without derivatives is essential for grasping the memory efficiency claims
  - Quick check question: How does the finite difference approximation in ZO optimization compare to traditional gradient computation in terms of memory requirements?

- Concept: Forward-mode automatic differentiation
  - Why needed here: Forward gradient (Forward-Grad) is a BP-free alternative that requires understanding of forward-mode AD
  - Quick check question: What is the key difference between forward-mode and reverse-mode automatic differentiation in terms of computational graph traversal?

- Concept: Memory profiling in deep learning frameworks
  - Why needed here: Understanding how PyTorch allocates memory for different operations is crucial for implementing memory-efficient optimizers
  - Quick check question: In PyTorch, what is the difference between model parameters loaded in full precision (float32) versus half precision (float16) in terms of memory consumption?

## Architecture Onboarding

- Component map: Model loading -> Task alignment (prompt engineering) -> Optimizer selection (ZO-SGD, ZO-Adam, Forward-Grad, FO-SGD, FO-Adam) -> Memory monitoring -> Performance evaluation

- Critical path:
  1. Load model in specified precision (F16 for ZO methods)
  2. Apply task alignment through prompt engineering
  3. Run optimization loop with memory-efficient gradient estimation
  4. Monitor peak memory usage and track accuracy
  5. Compare against FO baselines

- Design tradeoffs:
  - Query budget vs. accuracy: Higher query budgets reduce gradient variance but increase computation cost
  - Precision vs. memory: F16 loading reduces memory but may affect numerical stability
  - Block size vs. efficiency: Smaller blocks reduce variance but increase forward pass overhead

- Failure signatures:
  - Memory spikes during forward pass (indicates activation accumulation)
  - Degraded accuracy with long sequence lengths (suggests memory bottleneck)
  - Inconsistent performance across different ZO methods (indicates high gradient variance)

- First 3 experiments:
  1. Compare memory usage of ZO-SGD vs FO-SGD on a small model with varying sequence lengths
  2. Test task alignment effectiveness by running with and without prompts on SST2
  3. Evaluate block-wise ZO optimization by comparing standard ZO-SGD with block-wise variant on a medium-sized model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does task alignment improve performance uniformly across all ZO optimization methods or are there significant variations?
- Basis in paper: [explicit] The paper demonstrates that task alignment is particularly beneficial to ZO LLM fine-tuning, with substantial performance drops (10-10.5%) when absent for methods like ZO-SGD and ZO-Adam compared to only 0.1% for FO-SGD.
- Why unresolved: The paper only compares a limited set of tasks and methods. It's unclear if the magnitude of improvement varies significantly across different ZO optimization algorithms, task complexities, or LLM architectures.
- What evidence would resolve it: Systematic ablation studies varying task alignment across diverse ZO methods, task types, and model scales would clarify if the improvement is consistent or method-dependent.

### Open Question 2
- Question: What is the optimal balance between memory efficiency and fine-tuning performance when using hybrid ZO-FO training schemes?
- Basis in paper: [explicit] The paper introduces hybrid ZO-FO training as a way to trade off memory and performance, showing that using ZO on only the first third of model layers can achieve performance comparable to full FO while reducing memory by ~10%.
- Why unresolved: The experiments only explore a few configurations. The optimal split between ZO and FO layers likely depends on factors like model architecture, task complexity, and available memory constraints.
- What evidence would resolve it: Comprehensive studies varying the number of layers assigned to ZO vs FO across different model types and tasks, with memory/performance trade-off curves, would identify optimal configurations.

### Open Question 3
- Question: How does the memory efficiency advantage of ZO methods scale with input sequence length compared to FO methods?
- Basis in paper: [explicit] The paper shows that ZO-SGD's memory consumption remains constant regardless of sequence length, while FO-SGD's memory grows with sequence length after a certain threshold (~700 tokens).
- Why unresolved: The analysis only considers a single model (OPT-13B) and task. It's unclear if this scaling behavior holds consistently across different model families, tasks, or optimization algorithms.
- What evidence would resolve it: Systematic experiments measuring memory usage across various sequence lengths, model types, and tasks would establish the generalizability of the observed scaling behavior.

### Open Question 4
- Question: What is the optimal query budget for forward gradient estimation to balance accuracy and computational cost?
- Basis in paper: [explicit] The paper shows that increasing the query budget improves both Forward-Grad and ZO-SGD accuracy, but Forward-Grad's improvement is much more pronounced, approaching FO-SGD performance with sufficient queries.
- Why unresolved: The experiments only explore a limited range of query budgets. The optimal budget likely depends on factors like task complexity, model scale, and available computational resources.
- What evidence would resolve it: Comprehensive studies varying query budgets across diverse tasks and model scales, with accuracy and computational cost trade-off curves, would identify optimal configurations for different scenarios.

## Limitations
- Evaluation scope limited to classification tasks, may not generalize to generation or complex reasoning tasks
- Block-wise descent requires additional forward passes that could offset memory savings for smaller models
- Task alignment effectiveness depends heavily on prompt engineering quality, which is not fully specified
- Comparison between ZO and FO methods uses different precision settings, potentially confounding memory efficiency results

## Confidence
- High confidence: Memory efficiency claims for ZO methods on long sequences
- Medium confidence: Task alignment importance
- Medium confidence: Block-wise descent effectiveness
- Low confidence: Generalizability to non-classification tasks

## Next Checks
1. Test memory efficiency on generation tasks like summarization or text generation to verify if savings extend beyond classification tasks
2. Benchmark with consistent precision (F16) for both ZO and FO methods to isolate memory efficiency benefits from numerical precision effects
3. Analyze block-wise optimization overhead by systematically measuring variance reduction vs forward pass costs across different block sizes and model scales