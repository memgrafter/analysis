---
ver: rpa2
title: Multi-Agent Planning Using Visual Language Models
arxiv_id: '2408.05478'
source_url: https://arxiv.org/abs/2408.05478
tags:
- plan
- none
- language
- trial
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces a multi-agent architecture for planning with
  visual language models (VLMs) that requires only a single environmental image and
  task description as input, without complex structured representations. The system
  uses three specialized agents: a Semantic-Knowledge Miner Agent (SKM) to identify
  objects and relationships, a Grounded-Knowledge Miner Agent (GKM) to provide object
  descriptions, and a Planner Agent (P) to generate the final plan.'
---

# Multi-Agent Planning Using Visual Language Models
## Quick Facts
- arXiv ID: 2408.05478
- Source URL: https://arxiv.org/abs/2408.05478
- Reference count: 40
- Primary result: Multi-agent VLM architecture achieves PG2S score of 0.83 on ALFRED vs 0.33 for single-agent baselines

## Executive Summary
This paper presents a multi-agent architecture for visual planning that decomposes complex planning tasks into specialized roles to reduce hallucination and improve plan quality. The system takes only a single environmental image and task description as input, eliminating the need for complex structured representations. By distributing the cognitive load across three agents (SKM, GKM, and Planner), the approach maintains smaller context windows and more focused reasoning compared to monolithic models.

The authors introduce PG2S, a novel semantic evaluation metric that compares natural language plans based on semantic similarity rather than strict action ordering. This addresses a fundamental limitation of existing metrics like KAS that penalize semantically equivalent but syntactically different plans. Experiments on ALFRED demonstrate significant improvements over single-agent approaches, validating both the multi-agent design and the semantic evaluation framework.

## Method Summary
The proposed system employs three specialized agents working in sequence: the Semantic-Knowledge Miner Agent (SKM) extracts objects and relationships from the visual input, the Grounded-Knowledge Miner Agent (GKM) provides detailed object descriptions, and the Planner Agent (P) generates the final plan. This decomposition allows each agent to focus on specific aspects of the planning problem, reducing the cognitive load and hallucination risks associated with single large-context models. The system processes a single static image along with a task description, making it practical for real-world deployment where complex structured representations may be unavailable.

A key innovation is the PG2S metric for evaluating natural language plans. Unlike traditional metrics that require exact action matching, PG2S uses semantic similarity measures including cosine similarity and word embeddings to assess whether generated plans achieve the intended goals, even if the specific actions differ. This makes evaluation more robust to variations in natural language expression while still capturing plan quality.

## Key Results
- Multi-agent approach achieves PG2S score of 0.83 on ALFRED, significantly outperforming single-agent architecture at 0.33 KAS score
- PG2S metric demonstrates superior ability to capture semantic equivalence in plans compared to traditional action-ordering metrics
- System successfully handles complex household tasks using only single static image input and natural language descriptions

## Why This Works (Mechanism)
The multi-agent decomposition reduces hallucination by constraining each agent's context window to only the information necessary for its specialized task. The SKM focuses solely on identifying objects and relationships, the GKM concentrates on object grounding and descriptions, and the Planner synthesizes this information into executable plans. This specialization prevents the cognitive overload that occurs when a single model must simultaneously process visual information, extract semantics, and generate plans.

The semantic-based PG2S metric works by comparing plans at both sentence-level and goal-level using cosine similarity and word embedding techniques. This captures whether plans achieve equivalent outcomes even when expressed differently, addressing the brittleness of exact-matching metrics. The metric recognizes that natural language planning inherently allows for multiple valid action sequences that accomplish the same goal.

## Foundational Learning
- Visual Language Models (VLMs): Foundation models that process both visual and textual inputs, essential for bridging perception and language understanding in planning tasks. Quick check: VLMs can extract objects and relationships from images while maintaining semantic understanding of task descriptions.

- Semantic similarity metrics: Methods for comparing meaning rather than exact string matching, crucial for evaluating natural language plans that may have equivalent outcomes through different expressions. Quick check: Cosine similarity on sentence embeddings can measure semantic equivalence between different action descriptions.

- Multi-agent decomposition: Design pattern that distributes complex tasks across specialized agents, reducing individual model burden and improving overall system reliability. Quick check: Specialized agents with focused contexts show reduced hallucination compared to monolithic approaches.

- Natural language plan evaluation: Assessment frameworks that evaluate the quality and correctness of generated action plans expressed in human language. Quick check: Traditional metrics often fail when semantically equivalent plans use different vocabulary or ordering.

- Grounding in visual planning: Process of connecting abstract task descriptions to concrete objects and relationships in the environment. Quick check: Successful grounding requires both accurate object detection and understanding of task-relevant relationships.

## Architecture Onboarding

**Component Map:**
Image + Task Description -> SKM -> GKM -> Planner -> Final Plan

**Critical Path:**
Visual input and task description enter the SKM, which identifies objects and relationships. This output feeds into the GKM for detailed object grounding, and both results are passed to the Planner to generate the final executable plan.

**Design Tradeoffs:**
The multi-agent approach trades increased architectural complexity and coordination overhead for improved reliability and reduced hallucination. While single large-context models can process everything simultaneously, they suffer from attention dilution and increased error propagation. The decomposition limits context size per agent but requires careful orchestration to ensure information flows correctly between components.

**Failure Signatures:**
Common failure modes include SKM missing critical objects leading to incomplete plans, GKM providing incorrect object descriptions that misguide the Planner, and Planner generating actions that are semantically correct but practically impossible given physical constraints. The static image input also means dynamic environmental changes cannot be accounted for.

**First 3 Experiments to Run:**
1. Single-agent ablation: Replace the three-agent pipeline with a single model using the same total context window to quantify the benefit of decomposition
2. Context window sensitivity: Vary the maximum context size for each agent to find optimal balance between information capacity and focus
3. Cross-task generalization: Test the trained system on novel task types not seen during training to assess true generalization capability

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Static image input restricts the system to tasks achievable without dynamic environmental interaction or temporal reasoning
- Semantic-based evaluation may conflate plans that are semantically similar but practically different in execution
- Multi-agent coordination introduces complexity and potential scalability issues with larger environments
- Evaluation limited to single ALFRED benchmark, limiting generalizability claims
- No analysis of performance degradation with ambiguous or incomplete visual inputs
- Computational overhead of maintaining multiple specialized agents not addressed

## Confidence
- Multi-agent architecture effectiveness: High
- PG2S metric validity: Medium
- Task success claims on ALFRED: Medium
- Generalization beyond ALFRED: Low

## Next Checks
1. Test the system on multiple visual planning benchmarks (e.g., VSP, RoboTHOR) to assess cross-domain generalization
2. Conduct ablation studies removing individual agents to quantify their specific contributions to performance
3. Measure and report computational overhead (inference time, memory usage) of the multi-agent system compared to single-agent baselines