---
ver: rpa2
title: A Masked language model for multi-source EHR trajectories contextual representation
  learning
arxiv_id: '2402.06675'
source_url: https://arxiv.org/abs/2402.06675
tags:
- learning
- data
- representation
- sources
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a two-step masking process for learning effective
  representations of multi-source electronic health record (EHR) data to address both
  long/short-term dependencies and interactions between diseases and interventions.
  The method masks portions of patient trajectories and masks entire data sources
  (e.g., ICD10 or ATC codes) during specific periods, training a transformer encoder
  to predict masked elements using other sources.
---

# A Masked language model for multi-source EHR trajectories contextual representation learning

## Quick Facts
- arXiv ID: 2402.06675
- Source URL: https://arxiv.org/abs/2402.06675
- Reference count: 0
- Primary result: AUC of 0.919 for predicting Heart Failure ICD10 codes in the next visit

## Executive Summary
This paper introduces a two-step masking process for learning effective representations of multi-source electronic health record (EHR) data to address both long/short-term dependencies and interactions between diseases and interventions. The method masks portions of patient trajectories and masks entire data sources (e.g., ICD10 or ATC codes) during specific periods, training a transformer encoder to predict masked elements using other sources. Using data from approximately 30,000 patients with diagnoses, medications, and questionnaires, the model achieved an AUC of 0.919 for predicting Heart Failure ICD10 codes in the next visit when using both diagnoses and medications, significantly outperforming baseline methods.

## Method Summary
The authors propose a two-step masking process for learning contextual representations from multi-source EHR data. First, they randomly mask portions of patient trajectories to capture temporal dependencies. Second, they mask entire data sources (e.g., ICD10 or ATC codes) during specific periods to model interactions between different sources. A transformer encoder is trained to predict the masked elements using information from unmasked sources. After pre-training, a classifier layer is added and fine-tuned for specific downstream tasks, such as predicting Heart Failure ICD10 codes in the next patient visit.

## Key Results
- Achieved AUC of 0.919 for predicting Heart Failure ICD10 codes using both diagnoses and medications
- Outperformed baseline methods including Logistic Regression (0.644), Random Forest (0.564), MLP (0.752), Bi-GRU (0.783), and simple Masked Language Model with Bi-GRU (0.909)
- Demonstrated effectiveness of two-step masking for capturing both temporal dependencies and cross-source interactions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-step masking effectively captures both temporal dependencies and cross-source interactions in EHR data.
- Mechanism: The first masking step randomly hides portions of patient trajectories, forcing the model to learn temporal dependencies by predicting masked elements from surrounding context. The second masking step hides entire data sources (e.g., ICD10 codes) during specific periods, forcing the model to learn how different sources (diagnoses, medications, questionnaires) interact and complement each other.
- Core assumption: EHR data contains meaningful temporal patterns and cross-source relationships that can be learned through selective masking and reconstruction.
- Evidence anchors:
  - [abstract] "Here we tackled the latter challenge by masking one source (e.g., ICD10 codes) and training the transformer to predict it using other sources (e.g., ATC codes)."
  - [section] "Firstly, to handle the long/short-term dependencies, we randomly mask some proportion of the patient trajectories and train our network to predict the masked part via the unmasked ones. Then, to model the interactions between life habits, interventions, and diseases, we mask one of the sources for a specific period and train the network to predict it using other sources during that period."
  - [corpus] Weak evidence - while the paper mentions this approach, the corpus shows other methods like graph-based approaches but no direct comparison to this specific two-step masking.

### Mechanism 2
- Claim: Transformer encoder architecture is superior for handling long-range dependencies in sequential EHR data.
- Mechanism: The transformer encoder uses self-attention mechanisms to weigh the importance of different time points and sources relative to each other, allowing it to capture complex, non-linear relationships across the entire patient history.
- Core assumption: Self-attention mechanisms can effectively model the complex dependencies in multi-source EHR sequences better than recurrent architectures.
- Evidence anchors:
  - [abstract] "Bidirectional transformers have effectively addressed the first challenge" (referring to long/short-term dependencies)
  - [section] "Then to learn the representation, we train a transformer encoder to predict the masked part of the input sequence data"
  - [corpus] Moderate evidence - the corpus mentions "Structure-aware Hypergraph Transformer" and "MedRep" foundation models using transformers, suggesting this is a recognized approach in the field.

### Mechanism 3
- Claim: Fine-tuning pre-trained representations for specific downstream tasks significantly improves prediction performance.
- Mechanism: After learning general representations through masked language modeling, the model is fine-tuned with a classifier layer for the specific task of predicting Heart Failure ICD10 codes, allowing it to adapt the general representations to the specific prediction task.
- Core assumption: Pre-trained representations capture generalizable patterns that can be adapted to specific prediction tasks with additional training.
- Evidence anchors:
  - [abstract] "In the last step, we add a classifier layer on top of the trained network and fine-tune it for downstream tasks."
  - [section] "Table 1 show the promising ability of the two-step MLM" - the significant performance improvement over baseline methods suggests the fine-tuning approach is effective.
  - [corpus] Weak evidence - the corpus doesn't explicitly discuss fine-tuning strategies, though this is a common practice in transformer-based models.

## Foundational Learning

- Concept: Transformer architecture and self-attention mechanisms
  - Why needed here: The model relies on transformer encoders to process sequential EHR data and capture dependencies across time and between sources
  - Quick check question: How does multi-head attention in transformers allow the model to focus on different aspects of the input sequence simultaneously?

- Concept: Masked language modeling for self-supervised learning
  - Why needed here: The two-step masking process is central to how the model learns effective representations without requiring labeled data for pre-training
  - Quick check question: What is the difference between random masking and source-specific masking in terms of what relationships they help the model learn?

- Concept: Multi-modal representation learning
  - Why needed here: The model must learn to represent and relate different types of EHR data (ICD10 diagnoses, ATC medications, questionnaires) in a unified way
  - Quick check question: How can a model learn to translate between different modalities when one is masked during training?

## Architecture Onboarding

- Component map:
  Input layer -> Two-step masking module -> Transformer encoder -> Classifier layer -> Output layer

- Critical path:
  1. Data preprocessing and tokenization of multi-source EHR sequences
  2. Application of two-step masking (random + source-specific)
  3. Transformer encoder processing of masked sequences
  4. Masked element prediction during pre-training
  5. Addition of classifier layer and fine-tuning for downstream task

- Design tradeoffs:
  - Masking ratio vs. prediction difficulty: Higher masking ratios make the task harder but may force the model to learn more robust representations
  - Transformer depth vs. computational cost: Deeper transformers can capture more complex relationships but require more resources
  - Fine-tuning vs. training from scratch: Fine-tuning is faster but may be limited by the quality of pre-trained representations

- Failure signatures:
  - Poor performance on masked prediction during pre-training suggests issues with the masking strategy or transformer architecture
  - Minimal improvement from pre-training to fine-tuning indicates the learned representations may not be generalizable
  - Performance degradation with longer sequences may indicate transformer attention limitations

- First 3 experiments:
  1. Ablation study: Compare single-step masking (only random or only source-specific) against the two-step approach to validate the combined benefit
  2. Masking ratio sweep: Test different proportions of random masking and source-specific masking to find optimal values
  3. Architecture comparison: Compare transformer encoder against Bi-GRU baseline on the same masking strategy to isolate the impact of the architecture

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed two-step masking approach perform when incorporating multimodal data sources such as medical images alongside structured EHR data like ICD10 and ATC codes?
- Basis in paper: [explicit] The authors state that "Incorporating data sources with other modalities like medical images still needs more investigation."
- Why unresolved: The current study only evaluates performance using structured data (diagnoses, medications, questionnaires) and does not test the model's ability to handle heterogeneous data types including medical images.
- What evidence would resolve it: Experimental results comparing the two-step masking approach with and without the inclusion of medical imaging data, showing performance metrics (e.g., AUC) for heart failure prediction tasks.

### Open Question 2
- Question: What is the optimal masking proportion for the two-step process in terms of maximizing predictive performance for disease outcomes?
- Basis in paper: [inferred] The paper mentions that "we randomly mask some proportion of the patient trajectories" but does not specify what proportion was used or whether this was optimized.
- Why unresolved: The study does not report sensitivity analysis of masking proportions or provide guidance on how to determine optimal masking rates for different clinical prediction tasks.
- What evidence would resolve it: Systematic evaluation of model performance across different masking proportions (e.g., 10%, 30%, 50%, 70%) with corresponding AUC scores for the heart failure prediction task.

### Open Question 3
- Question: How does the two-step masking approach generalize to predicting other disease outcomes beyond heart failure, particularly for conditions with different temporal patterns and data characteristics?
- Basis in paper: [explicit] The authors evaluate the model specifically for "predicting the Heart-failure ICD10 codes in the next visit" but note that "Developing an effective ML model strongly depends on efficient feature extraction from all available sources" across different conditions.
- Why unresolved: The study provides results for only one disease outcome (heart failure), leaving uncertainty about the method's generalizability to other conditions with different prevalence, progression patterns, and data availability.
- What evidence would resolve it: Performance evaluation of the model on multiple disease prediction tasks (e.g., diabetes, cancer, stroke) with corresponding AUC values and comparison to baseline methods for each condition.

## Limitations
- Limited evaluation to single disease outcome (heart failure), raising questions about generalizability to other conditions
- No statistical significance testing to validate performance differences between methods
- Lack of detailed ablation studies showing individual contributions of the two masking steps

## Confidence

| Assessment | Confidence Level |
|------------|------------------|
| Methodology description and general approach | High |
| Reported performance improvements | Medium |
| Generalizability of results | Low |

## Next Checks
1. Conduct paired t-tests or Wilcoxon signed-rank tests to determine if the performance differences between the proposed method and baselines are statistically significant across multiple runs.

2. Systematically evaluate the contribution of each masking step (random vs. source-specific) and the transformer architecture by comparing against variants that use only one masking type or replace the transformer with alternative architectures.

3. Evaluate the model on additional prediction tasks beyond Heart Failure (e.g., diabetes progression, readmission risk) and on different EHR datasets to assess the approach's robustness and applicability across clinical scenarios.