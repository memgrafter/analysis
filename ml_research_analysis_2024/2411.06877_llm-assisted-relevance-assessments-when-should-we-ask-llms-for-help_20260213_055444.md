---
ver: rpa2
title: 'LLM-Assisted Relevance Assessments: When Should We Ask LLMs for Help?'
arxiv_id: '2411.06877'
source_url: https://arxiv.org/abs/2411.06877
tags:
- lara
- relevance
- test
- data
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of building large, reliable
  test collections for information retrieval evaluation while balancing cost and quality.
  Traditional methods rely on expensive human annotations, while recent LLM-based
  approaches are cheap but prone to bias and errors.
---

# LLM-Assisted Relevance Assessments: When Should We Ask LLMs for Help?

## Quick Facts
- arXiv ID: 2411.06877
- Source URL: https://arxiv.org/abs/2411.06877
- Reference count: 40
- Combines LLM predictions with limited human annotations to build test collections more efficiently

## Executive Summary
This paper addresses the challenge of building large, reliable test collections for information retrieval evaluation while balancing cost and quality. Traditional methods rely on expensive human annotations, while recent LLM-based approaches are cheap but prone to bias and errors. The proposed LLM-Assisted Relevance Assessments (LARA) method combines both by using LLM predictions to guide which documents to manually annotate and then calibrating the LLM outputs based on the limited human judgments. Experiments on TREC-7 Ad Hoc, TREC-8 Ad Hoc, TREC Robust 2004, and TREC-COVID datasets show that LARA consistently outperforms alternative methods across various budget constraints, achieving better Kendall's Tau correlation scores and lower maximum drops in system rankings.

## Method Summary
LARA combines LLM predictions with limited human annotations to build test collections efficiently. The method first generates LLM relevance probabilities for all documents, then iteratively selects documents for manual annotation based on predicted margins between relevance levels. As human annotations are collected, LARA updates a calibration model that maps LLM probabilities to calibrated relevance scores. This calibration debiases the LLM predictions and improves annotation accuracy. The algorithm distributes annotation budget evenly across topics and uses logistic regression to learn the calibration mapping. LARA handles graded relevance (0/1/2) and aims to maximize the utility of human effort while minimizing LLM annotation errors.

## Key Results
- LARA consistently outperforms LLM-only approaches across all tested datasets and budget constraints
- Achieved Kendall's Tau correlation scores of 0.813 on TREC-7, 0.730 on TREC-8, 0.779 on TREC Robust04, and 0.799 on TREC-COVID
- LARA's maximum drop in system rankings (2.0-3.0) is significantly lower than LLM-only approaches (5.0-8.0)
- Performance remains stable across different numbers of annotators (1, 3, or N)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LARA outperforms alternatives by calibrating LLM predictions to minimize annotation errors
- Mechanism: The algorithm actively learns a calibration mapping between LLM relevance probabilities and true relevance labels, using human annotations to refine this mapping iteratively. This calibration corrects LLM bias and improves the accuracy of automated annotations.
- Core assumption: The LLM's predicted relevance probabilities can be effectively calibrated using a limited set of human annotations
- Evidence anchors:
  - [abstract] "LARA debiases the LLM predictions to annotate the remaining non-assessed data"
  - [section 3.3] "we propose to learn this calibration mapping for each relevance level while selecting reasonable and effective data points for manual annotation"
- Break condition: If the correlation between LLM probabilities and true relevance is too weak or non-linear, calibration may fail to significantly improve predictions

### Mechanism 2
- Claim: LARA selects optimal documents for manual annotation by identifying LLM uncertainty
- Mechanism: Instead of naively selecting documents where LLM is most uncertain, LARA uses a learned calibration model to identify documents where the margin between top two predicted relevance levels is smallest in true probability space. This targets annotations that will most reduce classification error.
- Core assumption: The smallest margin in calibrated probability space corresponds to the greatest potential reduction in classification error
- Evidence anchors:
  - [section 3.3] "the greatest potential reduction in classification error is achieved by annotating the data point with the smallest margin"
  - [section 3.2] "if the LLM is unsure of a particular label, verifying that prediction would seem more effective than annotating data points where the LLM is confident"
- Break condition: If the calibration model is poorly trained or the dataset has very different characteristics than training data, margin-based selection may not target the most informative documents

### Mechanism 3
- Claim: LARA provides consistent performance regardless of number of annotators
- Mechanism: By grouping topics and dividing budget equally among annotator groups, LARA ensures each topic receives similar evaluation quality, leading to consistent overall performance across different annotator configurations
- Core assumption: Distributing human annotations evenly across topics maintains evaluation quality regardless of how many annotators are used
- Evidence anchors:
  - [section 3.4] "This allows each topic to have similar qualities of evaluation results, which helped increase the overall performance"
  - [section 5.1] "LARA method with different numbers of annotators... have almost identical performance on all datasets"
- Break condition: If topics have vastly different complexities or document volumes, equal budget distribution may lead to some topics being under-evaluated

## Foundational Learning

- Concept: Active learning and uncertainty sampling
  - Why needed here: LARA relies on selecting the most informative documents for manual annotation to maximize the utility of limited human effort
  - Quick check question: What is the difference between uncertainty sampling in SAL and CAL versus LARA's approach?

- Concept: Calibration of probabilistic predictions
  - Why needed here: LARA's core innovation is learning to map LLM's raw probability outputs to calibrated probabilities that better reflect true relevance likelihood
  - Quick check question: How does logistic regression help in calibrating LLM relevance predictions?

- Concept: Graded relevance vs binary relevance
  - Why needed here: LARA handles graded relevance levels (0/1/2) while most existing methods only work for binary relevance
  - Quick check question: How does LARA's margin calculation differ when dealing with three relevance levels versus two?

## Architecture Onboarding

- Component map: LLM prediction module -> calibration model -> active selection module -> human annotation collection -> calibration update -> calibrated predictions
- Critical path: 1) Generate LLM relevance probabilities for all documents, 2) Select initial documents for manual annotation based on naive margin, 3) Iteratively update calibration model as human annotations are collected, 4) Select remaining documents based on calibrated margins, 5) Apply calibrated model to predict relevance for all remaining documents
- Design tradeoffs: LARA trades computational overhead of calibration modeling for improved annotation accuracy and reduced human effort. The algorithm must balance exploration (selecting uncertain documents) with exploitation (using current calibration knowledge).
- Failure signatures: Poor calibration performance manifests as Kendall's Tau scores not improving with budget increases, high maximum drops in system rankings, or overlap scores between LLM predictions and ground truth not improving over time.
- First 3 experiments:
  1. Verify that calibration model improves correlation between LLM predictions and ground truth on a small validation set
  2. Test that margin-based selection outperforms random selection of documents for manual annotation
  3. Confirm that LARA performance is stable across different numbers of annotators by running with 1, 3, and N annotators on the same dataset

## Open Questions the Paper Calls Out

- **Open Question 1**: How does LARA's performance compare to methods that continuously fine-tune the LLM after each batch of human annotations?
  - Basis in paper: [explicit] The authors discuss this as a potential alternative to LARA, noting that continuous fine-tuning would be time-consuming and disrupt the annotation workflow.
  - Why unresolved: The paper only discusses this approach theoretically and does not implement or test it against LARA.
  - What evidence would resolve it: Empirical comparison showing whether continuous fine-tuning improves accuracy enough to justify the additional computational cost and workflow disruption compared to LARA's calibration approach.

- **Open Question 2**: What is the optimal number of annotators for LARA in real-world test collection construction?
  - Basis in paper: [explicit] The authors find that LARA(n = N) slightly outperforms other configurations but note this was surprising and speculate it may be due to more even distribution of annotations across topics.
  - Why unresolved: The experiments only tested three specific configurations (n = 1, 3, and N), and the theoretical explanation for why more annotators helps is speculative.
  - What evidence would resolve it: Systematic experiments varying the number of annotators across a wider range, coupled with analysis of how annotation distribution affects evaluation stability across topics.

- **Open Question 3**: How does LARA perform on annotation tasks beyond traditional document relevance judgments?
  - Basis in paper: [inferred] The authors mention future work exploring LARA-like hybrid methods for nugget matching and e-Discovery, suggesting these are unexplored applications.
  - Why unresolved: The paper only evaluates LARA on standard test collections for ad-hoc retrieval and does not test it on other annotation tasks.
  - What evidence would resolve it: Application of LARA to tasks like nugget matching, e-Discovery, or other information extraction/annotation tasks, with performance comparison to existing methods.

## Limitations

- The method's effectiveness depends heavily on the quality of initial LLM predictions and the ability to accurately calibrate them
- Calibration model performance may degrade on out-of-distribution data or topics with different characteristics than training data
- The relationship between margin-based selection and informative documents may not hold for all dataset types or LLM models

## Confidence

- **High confidence**: LARA's ability to improve Kendall's Tau scores compared to LLM-only approaches across multiple datasets and budget constraints
- **Medium confidence**: The claim that LARA consistently outperforms all baseline methods (Depth-k Pooling, MTF, MM-NS, CAL, SAL) - while results show improvement, the margins vary significantly across datasets and budget levels
- **Medium confidence**: The assertion that LARA performance is stable across different numbers of annotators - based on limited experimental evidence with specific annotator configurations

## Next Checks

1. Test LARA's performance with different LLM models (e.g., GPT-4, Claude) to verify that improvements are not specific to Llama-3.1-70B-Instruct
2. Evaluate LARA on datasets with different characteristics (longer documents, different domains, or different relevance grade distributions) to assess generalizability
3. Conduct ablation studies to determine the individual contribution of calibration versus active selection versus budget distribution strategies to overall performance