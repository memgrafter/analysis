---
ver: rpa2
title: 'RKadiyala at SemEval-2024 Task 8: Black-Box Word-Level Text Boundary Detection
  in Partially Machine Generated Texts'
arxiv_id: '2410.16659'
source_url: https://arxiv.org/abs/2410.16659
tags:
- text
- generated
- texts
- machine
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a reliable approach for identifying word-level
  text boundaries between human-written and machine-generated content in partially
  machine-generated texts. The proposed DeBERTa-CRF model, along with other transformer-based
  models, achieves significant improvements in detection accuracy, with Mean Average
  Error (MAE) as low as 2.192 on development data and 18.538 on unseen generator texts.
---

# RKadiyala at SemEval-2024 Task 8: Black-Box Word-Level Text Boundary Detection in Partially Machine Generated Texts

## Quick Facts
- arXiv ID: 2410.16659
- Source URL: https://arxiv.org/abs/2410.16659
- Reference count: 9
- Primary result: Achieves MAE of 2.192 on development data and 18.538 on unseen generator texts using DeBERTa-CRF model

## Executive Summary
This paper addresses the novel task of detecting word-level boundaries between human-written and machine-generated text in partially machine-generated texts. The author proposes using transformer-based models with Conditional Random Field (CRF) layers for this classification task. The DeBERTa-CRF model achieved the best performance with a Mean Average Error (MAE) of 2.192 on the development set and 18.538 on unseen generator texts, outperforming proprietary systems like ZeroGPT in sentence-level accuracy.

## Method Summary
The approach involves fine-tuning DeBERTa and Longformer transformer models with CRF layers for token-level classification, then mapping token predictions to word-level boundaries. The models were trained on a dataset containing 3,649 samples from PeerRead reviews and Outfox student essays, with 505 development samples and 11,123 test samples. Training used Adam optimizer with learning rate 2e-5, weight decay 1e-2, CRF dropout rate 75e-4, max length 512 tokens, and 30 epochs. The boundary detection logic identifies changes in consecutive token predictions (1 to 0,0 or 0 to 1,1) to determine text boundaries.

## Key Results
- DeBERTa-CRF achieved MAE of 2.192 on development data and 18.538 on unseen generator texts
- Outperformed proprietary systems like ZeroGPT in sentence-level accuracy, especially for shorter texts
- Demonstrated applicability to unseen domains and generators, showing potential for real-world AI-generated content detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CRF layers improve token classification accuracy by modeling sequential dependencies in text boundary detection.
- Mechanism: CRFs use transition scores between token states to ensure consistent predictions, preventing isolated misclassifications that could disrupt boundary detection.
- Core assumption: Adjacent tokens in a text boundary task are not independent; patterns of human vs. machine generation follow predictable sequences.
- Evidence anchors:
  - [abstract] "CRFs have played a vital role in improving the performance of the models due to their architecture being well suited for pattern recognition in sequential data."
  - [section 4] "The baseline model achieved an MAE of 3.53 on the Development set... We have tried one more approach by considering the change only if consecutive tokens are the same."
  - [corpus] Weak - no direct corpus evidence, but CRF use in similar NLP tasks is well established.
- Break condition: If the sequence patterns between human and machine-generated text are highly irregular, CRF smoothing could obscure real boundaries.

### Mechanism 2
- Claim: DeBERTa's enhanced pre-training objectives capture subtle linguistic differences between human and machine-generated text.
- Mechanism: DeBERTa uses disentangled attention and enhanced mask decoder to better model token relationships, improving discrimination at boundary regions.
- Core assumption: Machine-generated text has systematic linguistic patterns distinct from human writing that can be learned from pre-training.
- Evidence anchors:
  - [abstract] "We have built several models out of which DeBERTa-CRF was used as the official submission."
  - [section 4] "DeBERTa performed better when text boundaries are in the first half of the given text, while Longformer had better performance when the text boundary is in the other half."
  - [corpus] Weak - no corpus-level evidence of linguistic feature extraction differences.
- Break condition: If generator models become too sophisticated in mimicking human patterns, DeBERTa's learned distinctions may degrade.

### Mechanism 3
- Claim: Training on diverse generators and domains improves model generalization to unseen text sources.
- Mechanism: Exposure to multiple generators (ChatGPT, LLaMA2, GPT-4) and domains (PeerRead reviews, Outfox essays) creates a robust feature space that generalizes beyond training distributions.
- Core assumption: Different generators produce text with consistent, learnable patterns that transfer across domains.
- Evidence anchors:
  - [abstract] "The proposed model is also well suited for detecting which parts of a text are machine generated in outputs of Instruct variants of many LLMs."
  - [section 6.1] "The metrics from Table 3 demonstrate the proposed model's performance on both seen domain and generator data (dev set) along with unseen domain and unseen generator data (test set), hinting at wider applicability."
  - [corpus] Weak - corpus neighbors show related work but no direct evidence of cross-generator feature transfer.
- Break condition: If new generators produce text with fundamentally different patterns not represented in training data, generalization fails.

## Foundational Learning

- Concept: Token-level classification and sequence modeling
  - Why needed here: The task requires identifying word boundaries between human and machine-generated text at the token level before mapping to words.
  - Quick check question: How would you modify the model if the task required paragraph-level rather than word-level boundary detection?

- Concept: Conditional Random Fields for structured prediction
  - Why needed here: CRFs enforce consistency in sequential predictions, preventing isolated token misclassifications from disrupting boundary detection.
  - Quick check question: What happens to boundary detection accuracy if you remove the CRF layer and use only token-level softmax predictions?

- Concept: Domain adaptation and generalization
  - Why needed here: The model must perform well on unseen generators and domains, requiring robust feature learning beyond the training distribution.
  - Quick check question: How would you evaluate whether the model is overfitting to specific generator styles versus learning generalizable features?

## Architecture Onboarding

- Component map: DeBERTa transformer -> CRF layer -> Token-to-word mapping -> Boundary detection logic
- Critical path: Token classification (DeBERTa) -> Sequential smoothing (CRF) -> Boundary identification (change detection)
- Design tradeoffs: Base vs. large model variants (base achieved better performance), single vs. ensemble predictions, fixed vs. variable context windows
- Failure signatures: High MAE on specific POS tag combinations, poor performance on short texts, inconsistent boundary detection across generators
- First 3 experiments:
  1. Compare DeBERTa-CRF performance with and without CRF layer on development set to quantify CRF contribution
  2. Test ensemble of DeBERTa and Longformer predictions on unseen generator test set to evaluate complementary strengths
  3. Analyze POS tag distribution differences between human and machine-generated text to identify potential feature engineering opportunities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of the DeBERTa-CRF and Longformer-CRF models change when trained and evaluated on multilingual data from diverse linguistic backgrounds?
- Basis in paper: [explicit] The paper mentions that the current study focuses on English texts and suggests extending the work to other languages by replacing DeBERTa with mDeBERTa and training on a multilingual corpus.
- Why unresolved: The authors acknowledge that not all languages are covered by mDeBERTa, and they have not explored the performance of these models on multilingual data.
- What evidence would resolve it: Experimental results comparing the performance of DeBERTa-CRF and Longformer-CRF models on multilingual datasets, including languages with different linguistic structures and characteristics.

### Open Question 2
- Question: What is the impact of using different chunk sizes for processing longer texts with the DeBERTa model, and how does it affect the overall detection accuracy?
- Basis in paper: [inferred] The authors mention that while the current work only considers the first 512 tokens, the longformer version achieved the same results on unseen generator texts. They suggest looking into how well chunking would work on the DeBERTa model to process longer texts.
- Why unresolved: The paper does not provide any experimental results or insights into the effect of using different chunk sizes for processing longer texts with the DeBERTa model.
- What evidence would resolve it: Comparative analysis of the DeBERTa model's performance on longer texts using different chunk sizes, along with the corresponding detection accuracy metrics.

### Open Question 3
- Question: How does the proposed model perform when detecting text boundaries in cases where multiple changes from human to machine-generated text and vice versa occur within the same text?
- Basis in paper: [explicit] The authors mention that while the novel task of detecting text boundaries in partially machine-generated texts achieves high accuracy where one change from human to machine occurs, being able to handle cases of multiple changes is vital.
- Why unresolved: The current study and existing models do not cover cases where there are multiple changes from human to machine-generated text and vice versa within the same text.
- What evidence would resolve it: Experimental results evaluating the performance of the proposed model on texts with multiple changes between human and machine-generated content, along with the corresponding detection accuracy metrics.

## Limitations

- Performance drops significantly on unseen generators (MAE of 18.538 vs 2.192 on seen generators), indicating limited true domain adaptation
- Reliance on specific dataset sources (PeerRead and Outfox) raises questions about generalizability beyond academic writing styles
- CRF smoothing may introduce systematic errors when human and machine-generated text exhibit similar sequential patterns

## Confidence

- **High Confidence**: The core claim that DeBERTa-CRF achieves state-of-the-art performance on the development dataset is well-supported by the experimental results, with clear MAE metrics and comparative analysis against baseline models.
- **Medium Confidence**: The assertion that the model generalizes well to unseen domains and generators is partially supported but requires more rigorous analysis, particularly given the substantial performance drop on unseen generators.
- **Low Confidence**: The claim that DeBERTa's pre-training objectives specifically capture linguistic differences between human and machine-generated text lacks direct empirical validation beyond performance metrics.

## Next Checks

1. **Cross-Generator Ablation Study**: Systematically evaluate model performance across all generator combinations (ChatGPT, GPT-4, LLaMA2) to identify whether failures stem from specific generator characteristics or represent fundamental limitations in the approach.

2. **Linguistic Feature Analysis**: Conduct a detailed linguistic analysis of misclassified boundary regions to determine whether errors correlate with specific linguistic features (e.g., technical vocabulary, syntactic complexity, discourse markers) that may be systematically misattributed.

3. **Temporal Generalization Test**: Evaluate model performance on machine-generated text produced at different time periods to assess whether the approach captures persistent patterns versus transient generation characteristics that may evolve as LLMs improve.