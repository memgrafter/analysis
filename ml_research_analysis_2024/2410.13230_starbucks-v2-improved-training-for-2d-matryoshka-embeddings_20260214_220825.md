---
ver: rpa2
title: 'Starbucks-v2: Improved Training for 2D Matryoshka Embeddings'
arxiv_id: '2410.13230'
source_url: https://arxiv.org/abs/2410.13230
tags:
- starbucks
- embedding
- retrieval
- training
- trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Starbucks, a new training strategy for Matryoshka-style
  embedding models that improves effectiveness over prior 2D Matryoshka methods. The
  key innovation is a fixed-layer-dimension training approach during fine-tuning,
  where loss is computed over a predefined list of model sizes from small to large,
  rather than randomly sampling combinations.
---

# Starbucks-v2: Improved Training for 2D Matryoshka Embeddings

## Quick Facts
- arXiv ID: 2410.13230
- Source URL: https://arxiv.org/abs/2410.13230
- Authors: Shengyao Zhuang, Shuai Wang, Fabio Zheng, Bevan Koopman, Guido Zuccon
- Reference count: 37
- Key outcome: Structured fine-tuning and masked autoencoder pre-training significantly improve 2D Matryoshka embedding effectiveness, matching separately trained models

## Executive Summary
This paper introduces Starbucks, a new training strategy for 2D Matryoshka embedding models that improves effectiveness over prior methods. The key innovation is a fixed-layer-dimension training approach during fine-tuning, where loss is computed over a predefined list of model sizes from small to large, rather than randomly sampling combinations. Additionally, the authors propose Starbucks Masked Autoencoding (SMAE) pre-training, which applies masked autoencoder language modeling to sub-layers and sub-dimensions during pre-training to produce stronger backbone representations. Experiments on STS and retrieval benchmarks show that Starbucks consistently outperforms 2D Matryoshka models and matches or exceeds separately trained models while maintaining efficiency.

## Method Summary
Starbucks combines two key innovations: Structured Representation Learning (SRL) fine-tuning and Starbucks Masked Autoencoding (SMAE) pre-training. SRL computes loss only over a fixed list of layer-dimension pairs ranging from small to large, avoiding the stochastic sampling of suboptimal sub-networks used in previous 2D Matryoshka methods. SMAE applies masked autoencoder language modeling to sub-layers and sub-dimensions during pre-training, forcing the encoder to learn representations useful at multiple granularities. The encoder must compress input text into representations that a smaller decoder can reconstruct from masked tokens, creating a stronger backbone model optimized for multiple configurations.

## Key Results
- Starbucks consistently outperforms 2D Matryoshka models (2DMSE) on STS and retrieval benchmarks
- The combination of SMAE pre-training and SRL fine-tuning matches or exceeds separately trained models
- Structured loss computation over fixed layer-dimension pairs significantly improves effectiveness compared to random sampling
- SMAE pre-training provides meaningful improvements when combined with SRL, with only rare exceptions on full-size models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured loss computation over a fixed set of layer-dimension pairs improves 2D Matryoshka embedding effectiveness by avoiding stochastic sampling of suboptimal sub-networks.
- Mechanism: By computing loss only over a predefined list of layer-dimension pairs ranging from small to large, the model receives consistent optimization signals for each targeted configuration, rather than averaging over randomly sampled pairs that may not cover all useful configurations.
- Core assumption: Consistent exposure to each layer-dimension pair during training leads to better representations than stochastic sampling.
- Evidence anchors:
  - [abstract]: "Rather than sampling a random sub-layer and sub-dimensions for each training steps, providing a fixed list of layer-dimension pairs, from small to large, and computing the loss across all pairs significantly improves the effectiveness"
  - [section 3.1]: "Our Starbucks Representation Learning computes the loss only for a limited list of layer-dimension pairs, ranging from smaller to larger, to form the final loss"
  - [corpus]: Weak evidence - no direct mention of fixed-list versus sampling tradeoff in neighbor papers

### Mechanism 2
- Claim: SMAE pre-training enhances the backbone model's ability to produce useful representations at various layer-dimension sizes by applying masked autoencoder language modeling to sub-layers and sub-dimensions.
- Mechanism: The encoder learns to compress input text into representations that the smaller decoder can reconstruct from masked tokens, forcing the encoder to retain useful information at multiple granularities simultaneously.
- Core assumption: The reconstruction task forces the encoder to learn representations useful for both the full model and sub-networks.
- Evidence anchors:
  - [abstract]: "We introduce a new pre-training strategy, which applies masked autoencoder language modelling to sub-layers and sub-dimensions during pre-training, resulting in a stronger backbone"
  - [section 3.2]: "The encoder needs to learn to compress useful information about the input text into its representations for the weaker decoder to recover the masked tokens"
  - [corpus]: Weak evidence - neighbor papers mention Matryoshka-style training but don't specifically address MAE for sub-layers

### Mechanism 3
- Claim: The combination of structured fine-tuning and targeted pre-training produces complementary improvements that together match or exceed separately trained models.
- Mechanism: SMAE pre-training provides a strong backbone optimized for multiple granularities, while SRL fine-tuning ensures consistent optimization across the target configurations without wasting capacity on less useful combinations.
- Core assumption: The two phases address different aspects of the optimization problem and their effects compound rather than compete.
- Evidence anchors:
  - [abstract]: "The combination of SMAE pre-training and SRL fine-tuning significantly enhances the effectiveness of 2D Matryoshka models"
  - [section 5.3]: "SMAE pre-training consistently surpasses the original BERT checkpoint when SRL fine-tuning is applied, with the only exceptions being the full-size model"
  - [corpus]: Weak evidence - neighbor papers don't combine pre-training and fine-tuning in this specific way

## Foundational Learning

- Concept: Transformer architecture and how embeddings flow through layers
  - Why needed here: Understanding how layer-dimension pairs work requires knowing how BERT processes input and produces embeddings at different depths and widths
  - Quick check question: What happens to the embedding dimensionality when you take the first d dimensions of a layer output?

- Concept: Masked language modeling and autoencoding objectives
  - Why needed here: SMAE relies on reconstructing masked tokens, so understanding how this objective shapes representations is crucial
  - Quick check question: How does the reconstruction loss differ when applied to sub-dimensions versus full dimensions?

- Concept: Loss functions for embedding learning (contrastive, InfoNCE, etc.)
  - Why needed here: The paper mentions using contrastive loss and KL divergence, so understanding their role in embedding training is essential
  - Quick check question: Why would you add a KL divergence term to align embeddings across different sizes?

## Architecture Onboarding

- Component map: Encoder (BERT-like transformer with N layers and D dimensions) -> Decoder (smaller BERT-like transformer for SMAE) -> Loss computation (structured over fixed layer-dimension pairs) -> Pre-training pipeline (Fineweb dataset → SMAE → SRL fine-tuning → evaluation)

- Critical path:
  1. Pre-train with SMAE on masked language modeling task
  2. Fine-tune with SRL using structured loss over target pairs
  3. Evaluate across all configurations

- Design tradeoffs:
  - Fixed list vs. random sampling: Consistency vs. coverage
  - Pre-training depth: More layers may help but increase cost
  - Loss weighting: KL divergence strength affects alignment

- Failure signatures:
  - Poor performance on off-diagonal configurations: Fixed list too restrictive
  - No improvement from pre-training: Decoder too weak or pre-training task not challenging enough
  - Training instability: Loss scaling issues or learning rate problems

- First 3 experiments:
  1. Compare SRL with 2DMSE on a single dataset to verify the structured loss benefit
  2. Test SMAE pre-training with and without the structured fine-tuning to isolate effects
  3. Vary the fixed list composition to find optimal configurations for a target task

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Dataset generalizability: Experiments primarily use FineWeb and focus on STS and retrieval benchmarks; effectiveness on other domains remains untested
- Fixed list selection: Optimal composition likely varies by task but this dependency is not characterized
- Computational trade-offs: Comprehensive ablation studies on computational cost versus random sampling are not provided

## Confidence

**High Confidence** (supported by direct evidence and clear mechanism):
- Structured fine-tuning (SRL) consistently outperforms 2D Matryoshka sampling approaches
- SMAE pre-training provides meaningful improvements when combined with SRL
- The combination of SRL and SMAE matches or exceeds separately trained models

**Medium Confidence** (supported by evidence but with notable gaps):
- The specific fixed list selection doesn't significantly impact effectiveness across different tasks
- The computational efficiency claims hold across different hardware configurations
- The improvements transfer to non-English languages and specialized domains

**Low Confidence** (limited or no direct evidence):
- The method generalizes to transformer architectures beyond BERT (e.g., RoBERTa, DeBERTa)
- The fixed list approach outperforms more sophisticated selection methods
- The KL divergence weighting of 1.0 is optimal across all scenarios

## Next Checks

1. **Cross-Domain Transferability Test**: Evaluate Starbucks-v2 on biomedical and legal document retrieval tasks to assess domain generalizability beyond the web text and general STS benchmarks used in the paper.

2. **Fixed List Sensitivity Analysis**: Systematically vary the composition of the fixed layer-dimension pairs (e.g., different spacing, inclusion/exclusion of certain sizes) across multiple target tasks to determine if there's an optimal universal pattern or if task-specific tuning is required.

3. **Computational Cost Benchmarking**: Measure wall-clock training time and memory usage for Starbucks-v2 versus 2D Matryoshka sampling across different batch sizes and hardware configurations to validate the claimed efficiency gains with quantitative data.