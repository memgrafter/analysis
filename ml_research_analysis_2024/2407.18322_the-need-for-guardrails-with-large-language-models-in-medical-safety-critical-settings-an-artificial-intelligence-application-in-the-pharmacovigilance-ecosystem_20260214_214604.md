---
ver: rpa2
title: 'The Need for Guardrails with Large Language Models in Medical Safety-Critical
  Settings: An Artificial Intelligence Application in the Pharmacovigilance Ecosystem'
arxiv_id: '2407.18322'
source_url: https://arxiv.org/abs/2407.18322
tags:
- text
- drug
- translation
- safety
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study introduces guardrails to address LLM hallucination risks
  in medical safety-critical settings like pharmacovigilance. The core method combines
  a fine-tuned multilingual LLM with three guardrails: document-level uncertainty
  filtering, drug/adverse event mismatch detection, and token-level uncertainty highlighting.'
---

# The Need for Guardrails with Large Language Models in Medical Safety-Critical Settings: An Artificial Intelligence Application in the Pharmacovigilance Ecosystem

## Quick Facts
- arXiv ID: 2407.18322
- Source URL: https://arxiv.org/abs/2407.18322
- Reference count: 40
- Demonstrates guardrails for LLM safety in medical settings

## Executive Summary
This study addresses the critical challenge of using Large Language Models (LLMs) in safety-critical medical applications, specifically in pharmacovigilance for processing Individual Case Safety Reports (ICSRs). The research introduces a system that combines a fine-tuned multilingual LLM with three distinct guardrails to prevent hallucinations and ensure accuracy in medical translations. The approach is particularly focused on Japanese ICSR-to-English translation, where accuracy is paramount for drug safety monitoring and regulatory compliance.

The study demonstrates that LLMs can be safely deployed in medical safety-critical settings when coupled with appropriate guardrails. The system achieves a BLEU score of 0.39 while successfully flagging all hallucinated drug names through its detection mechanisms. This work provides a framework for responsible LLM deployment in healthcare settings where errors could have serious safety implications.

## Method Summary
The researchers developed a comprehensive system combining a fine-tuned multilingual LLM with three guardrails specifically designed for pharmacovigilance applications. The core method involves training the LLM on a dataset of 3,000 Japanese ICSRs paired with English translations, followed by the implementation of document-level uncertainty filtering, drug/adverse event mismatch detection, and token-level uncertainty highlighting. The system processes Japanese ICSRs, applies the LLM for translation, and then uses the guardrails to identify and flag potential errors or uncertainties in the output.

The three-guardrail system works in tandem with the LLM to create a robust pipeline for medical translation. Document-level uncertainty filtering assesses overall translation confidence, mismatch detection specifically looks for discrepancies between drugs and adverse events, and token-level uncertainty highlighting identifies specific problematic tokens in the translation. This multi-layered approach ensures that potential errors are caught at different stages of the translation process.

## Key Results
- Achieved BLEU score of 0.39 for Japanese ICSR-to-English translation
- Successfully flagged 100% of hallucinated drug names through mismatch detection
- Token-level uncertainty guardrail showed higher entropy for more clinically accurate outputs

## Why This Works (Mechanism)
The system works by creating multiple layers of validation around the LLM output, ensuring that errors are caught before they can cause harm. The fine-tuned LLM provides the translation capability while the guardrails act as safety mechanisms that can detect when the model is uncertain or when it has generated potentially incorrect information. This multi-layered approach allows for both automated processing and human-in-the-loop review where needed.

The guardrails specifically address the known weaknesses of LLMs in safety-critical applications: their tendency to hallucinate information and their inability to express uncertainty appropriately. By implementing document-level, mismatch-level, and token-level checks, the system creates redundancy that increases overall reliability. The token-level uncertainty guardrail is particularly innovative in that it uses entropy as a measure of confidence, allowing the system to highlight areas that require human review.

## Foundational Learning

**Pharmacovigilance** - The science and activities relating to the detection, assessment, understanding, and prevention of adverse effects or any other drug-related problems. Why needed: Understanding this field is crucial as it defines the safety-critical context where the system operates. Quick check: Can identify the key stakeholders and regulatory requirements in drug safety monitoring.

**Individual Case Safety Reports (ICSRs)** - Detailed reports of adverse drug reactions or medication errors submitted to regulatory authorities. Why needed: These are the primary documents being processed, and their accuracy is critical for patient safety. Quick check: Can explain the structure and importance of ICSRs in drug safety monitoring.

**Hallucination in LLMs** - The tendency of language models to generate plausible but incorrect or fabricated information. Why needed: This is the primary risk being mitigated by the guardrail system. Quick check: Can identify examples of hallucinations and understand their potential impact in medical contexts.

## Architecture Onboarding

**Component Map**: Japanese ICSR -> Fine-tuned LLM -> Document Guardrail -> Mismatch Guardrail -> Token Guardrail -> Final Output with Flags

**Critical Path**: The most critical path is from the LLM output through all three guardrails to the final flagged output. Each guardrail adds a layer of validation, with the token-level guardrail providing the most granular analysis for human review.

**Design Tradeoffs**: The system trades computational efficiency for safety, implementing multiple layers of validation that increase processing time but significantly reduce the risk of critical errors. This is an acceptable tradeoff in safety-critical medical applications where accuracy is paramount.

**Failure Signatures**: The primary failure modes would be missed hallucinations (false negatives) or excessive flagging of correct information (false positives). The system appears designed to minimize false negatives at the cost of potentially higher false positive rates, which is appropriate for safety-critical applications.

**First 3 Experiments**:
1. Test the system on a diverse set of Japanese ICSRs with known ground truth to validate accuracy
2. Evaluate the false positive rate of the guardrails by having clinical experts review flagged outputs
3. Assess the system's performance on rare drug names and adverse events not present in training data

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation dataset of only 50 cases may not capture full variability in medical translations
- BLEU score of 0.39 indicates room for improvement in translation quality
- Computational cost and real-time performance in production environments not assessed

## Confidence

**High confidence**: The effectiveness of hallucination detection guardrails in flagging drug name errors (correctly identified all hallucinations)

**Medium confidence**: The overall system performance metrics (BLEU score, F1 scores) given the limited evaluation dataset

**Medium confidence**: The token-level uncertainty guardrail's correlation between entropy and clinical accuracy

## Next Checks

1. Evaluate system performance on larger, more diverse datasets including multiple languages and drug name variations
2. Assess computational efficiency and real-time performance in clinical workflow integration
3. Conduct clinical expert review of system outputs to validate safety-critical error detection beyond automated metrics