---
ver: rpa2
title: A fuzzy loss for ontology classification
arxiv_id: '2405.02083'
source_url: https://arxiv.org/abs/2405.02083
tags:
- loss
- semantic
- have
- data
- ontology
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a fuzzy loss function to improve the logical
  consistency of deep learning models in ontology classification tasks. The method
  integrates symbolic knowledge from ontologies into the learning process by adding
  penalty terms for subsumption and disjointness violations to the standard loss function.
---

# A fuzzy loss for ontology classification

## Quick Facts
- arXiv ID: 2405.02083
- Source URL: https://arxiv.org/abs/2405.02083
- Reference count: 33
- Primary result: Fuzzy loss reduces consistency violations by ~2 orders of magnitude without significantly decreasing classification performance on ChEBI ontology.

## Executive Summary
This paper introduces a fuzzy loss function to improve the logical consistency of deep learning models in ontology classification tasks. The method integrates symbolic knowledge from ontologies into the learning process by adding penalty terms for subsumption and disjointness violations to the standard loss function. The authors evaluate their approach on the ChEBI ontology, showing that the fuzzy loss reduces consistency violations by approximately two orders of magnitude without significantly decreasing classification performance. The balanced version of the loss function performs comparably to the baseline model while maintaining improved consistency. Additionally, the authors demonstrate that incorporating unlabeled data through semi-supervised learning can further improve consistency on out-of-distribution data.

## Method Summary
The authors propose a fuzzy loss function that combines weighted binary cross-entropy with semantic loss terms based on fuzzy logic. The semantic loss penalizes violations of ontological axioms by computing fuzzy degrees of truth for subsumption ($A \sqsubseteq B$) and disjointness ($C \sqcap D \equiv \bot$) constraints. The loss uses differentiable t-norms (product and ﾅ「kasiewicz) to maintain gradient flow during training. A balanced variant adjusts gradient weighting to address class imbalance in hierarchical ontologies. The method is evaluated using ELECTRA models on the ChEBI chemical ontology with both labeled and unlabeled PubChem data.

## Key Results
- Fuzzy loss reduces consistency violations by approximately two orders of magnitude compared to baseline
- Balanced semantic loss achieves comparable performance to baseline while maintaining consistency improvements
- Semi-supervised learning with unlabeled data improves consistency on out-of-distribution (Hazardous) dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The fuzzy loss directly penalizes subsumption and disjointness violations by converting ontological axioms into differentiable penalty terms.
- Mechanism: For subsumption $A \sqsubseteq B$, the loss computes $T(h_A(x), 1 - h_B(x))$, where $T$ is a fuzzy t-norm. This is zero only when $h_A \leq h_B$, otherwise it grows with the degree of violation. For disjointness $C \sqcap D \equiv \bot$, the loss computes $T(h_C(x), h_D(x))$, which is zero only when at least one class score is zero.
- Core assumption: The fuzzy membership values $h_A(x)$ behave like degrees of truth in a many-valued logic, and the t-norm preserves the semantics of logical conjunction.
- Evidence anchors:
  - [abstract] "This paper introduces a fuzzy loss function to improve the logical consistency of deep learning models in ontology classification tasks. The method integrates symbolic knowledge from ontologies into the learning process by adding penalty terms for subsumption and disjointness violations to the standard loss function."
  - [section] "Our semantic loss term for implications is then defined as $L_T(A \sqsubseteq B, x) := \hat{h}(\neg(A \rightarrow B), x) = \hat{h}(A \wedge \neg B, x) = T(h_A(x), 1 - h_B(x))$."
- Break condition: If the t-norm or membership function does not preserve differentiability, gradients cannot flow, and the loss cannot guide learning.

### Mechanism 2
- Claim: The balanced implication loss addresses class imbalance by asymmetrically weighting gradients for superclass and subclass predictions.
- Mechanism: The loss $L_B^T(A \sqsubseteq B, x; k, \epsilon)$ scales the gradient for $h_B$ more strongly than for $h_A$ when $k>1$, so that violations of high-level classes are penalized more heavily. This prevents the model from ignoring rare subclasses to reduce loss.
- Core assumption: The imbalance between subclass and superclass membership in the dataset causes systematic bias, and stronger gradients on the superclass term will correct this.
- Evidence anchors:
  - [section] "The balanced implication loss has a lower gradient for the left-hand class and a higher gradient for the right-hand class instead of applying the same gradient to both classes."
  - [section] "The regular implication loss can be seen as a specialised version of this balanced implication loss where $k = \epsilon = 0$."
- Break condition: If the imbalance is extreme and $k$ is not tuned, the model may still collapse to ignoring subclasses or overfit to superclasses.

### Mechanism 3
- Claim: Semi-supervised learning with unlabelled data improves consistency on out-of-distribution examples by exposing the model to a broader chemical space.
- Mechanism: By including PubChem SMILES in training, the model learns to apply ontological constraints to structures not seen in the labelled ChEBI100 set, reducing FNR on the Hazardous dataset.
- Core assumption: The ontological structure (subsumption, disjointness) is consistent across the full chemical domain, so training on more diverse structures transfers to better generalization.
- Evidence anchors:
  - [abstract] "Additionally, the authors demonstrate that incorporating unlabeled data through semi-supervised learning can further improve consistency on out-of-distribution data."
  - [section] "This result can be particularly useful in scenarios in which the distribution of features in the dataset is limited."
- Break condition: If the unlabelled data contains chemical entities outside the ontology's scope, the model may learn inconsistent constraints or noisy gradients.

## Foundational Learning

- Concept: Fuzzy logic and t-norms
  - Why needed here: The loss function interprets neural outputs as degrees of truth and uses fuzzy operators to compute logical consistency penalties.
  - Quick check question: What is the output range of a fuzzy membership function, and why must it be differentiable?

- Concept: Ontology axioms (subsumption, disjointness)
  - Why needed here: These axioms define the logical constraints the loss enforces; understanding their semantics is crucial to interpreting the loss terms.
  - Quick check question: In the context of ChEBI, what does it mean for class A to subsume class B?

- Concept: Imbalanced multi-label classification
  - Why needed here: ChEBI's hierarchical structure causes severe label imbalance, which the balanced loss specifically addresses.
  - Quick check question: Why does a subclass inherently have fewer training examples than its superclass in a hierarchical ontology?

## Architecture Onboarding

- Component map:
  ELECTRA backbone -> label prediction head -> fuzzy loss module (subsumption + disjointness) -> combined loss -> optimizer
  PubChem dataset loader (for semi-supervised mode)
  Evaluation pipeline: ChEBI100 test set + Hazardous out-of-distribution test set

- Critical path:
  1. Pretrain ELECTRA on PubChem SMILES (shared for all variants)
  2. Fine-tune with semantic loss terms added to the weighted binary cross-entropy
  3. Monitor micro/macro F1 and FNR on validation set
  4. Select best checkpoint by micro F1

- Design tradeoffs:
  - Using ﾅ「kasiewicz vs product t-norm: ﾅ「kasiewicz can produce larger gradients near boundaries but is prone to exploding gradients; product is smoother but may under-penalize violations
  - Adding semi-supervised data: Improves out-of-distribution consistency but risks introducing noise if the data is outside ontology scope
  - Balancing vs regular implication loss: Balancing improves macro-F1 at the cost of slightly higher FNR on in-distribution data

- Failure signatures:
  - Exploding gradients: Likely with ﾅ「kasiewicz t-norm when many violations occur simultaneously
  - Degraded macro-F1: Indicates that small classes are being ignored or misclassified
  - Inconsistent out-of-distribution performance: Suggests the model overfits to the training distribution

- First 3 experiments:
  1. Run baseline ELECTRA with only weighted BCE; record micro/macro F1 and FNR
  2. Add semantic loss with product t-norm and $w_{\text{impl}}=0.01$, $w_{\text{disj}}=100$; compare consistency metrics
  3. Switch to balanced semantic loss with $k=2$; check if macro-F1 improves without hurting FNR

## Open Questions the Paper Calls Out

- Question: How does the balanced semantic loss variant compare to the unbalanced variant in terms of predictive performance for small ontology classes?
  - Basis in paper: [explicit] The authors note that the balanced semantic loss variant is able to perform on par with the baseline model, while the unbalanced variant has a lower micro-F1 score.
  - Why unresolved: The paper does not provide a detailed analysis of the performance differences between the balanced and unbalanced variants for individual classes.
  - What evidence would resolve it: A comprehensive analysis of the performance of the balanced and unbalanced semantic loss variants for individual classes, particularly for small classes, would provide insights into the effectiveness of the balanced variant.

- Question: How does the inclusion of unlabelled data in the training process affect the generalization performance of the model on unseen data?
  - Basis in paper: [explicit] The authors show that the inclusion of unlabelled data in the training process improves the consistency of predictions for out-of-distribution data.
  - Why unresolved: The paper does not provide a detailed analysis of the generalization performance of the model on unseen data.
  - What evidence would resolve it: A comprehensive analysis of the generalization performance of the model on unseen data, including different types of data distributions, would provide insights into the effectiveness of the semi-supervised training approach.

- Question: How can the semantic loss function be extended to other types of ontology axioms, such as parthood relations?
  - Basis in paper: [explicit] The authors mention that it would be interesting to extend the semantic loss function to other types of ontology axioms, such as parthood relations.
  - Why unresolved: The paper does not provide a detailed discussion of how the semantic loss function can be extended to other types of ontology axioms.
  - What evidence would resolve it: A detailed discussion of how the semantic loss function can be extended to other types of ontology axioms, along with experimental results, would provide insights into the generalizability of the approach.

## Limitations
- The approach's scalability to larger ontologies with more complex axiom structures remains untested
- Results are specific to chemical ontologies; generalizability to other domains is unknown
- The optimal balance parameter k and t-norm selection may be domain-specific and require tuning

## Confidence
- **High**: The mechanism of using fuzzy logic to encode logical constraints into differentiable loss terms is well-established and theoretically sound
- **Medium**: The effectiveness of the balanced loss variant is demonstrated but lacks comprehensive ablation studies
- **Medium**: The semi-supervised improvement is shown but the effect size relative to supervised-only training is not fully characterized

## Next Checks
1. Perform an ablation study removing either subsumption or disjointness penalty terms to quantify their individual contributions to consistency improvements
2. Test the method on a different hierarchical ontology (e.g., Gene Ontology) to assess domain transferability
3. Conduct a sensitivity analysis on the balance parameter k and t-norm choice to determine robustness to hyperparameter settings