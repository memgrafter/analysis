---
ver: rpa2
title: 'ANGO: A Next-Level Evaluation Benchmark For Generation-Oriented Language Models
  In Chinese Domain'
arxiv_id: '2401.04898'
source_url: https://arxiv.org/abs/2401.04898
tags:
- problems
- comprehension
- knowledge
- expression
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ANGO, a Chinese multi-choice question evaluation
  benchmark for generation-oriented language models. The benchmark addresses limitations
  of existing datasets by proposing a keypoint categorization standard, where each
  question can correspond to multiple keypoints, enhancing interpretability.
---

# ANGO: A Next-Level Evaluation Benchmark For Generation-Oriented Language Models In Chinese Domain

## Quick Facts
- arXiv ID: 2401.04898
- Source URL: https://arxiv.org/abs/2401.04898
- Authors: Bingchao Wang
- Reference count: 36
- Introduces ANGO, a Chinese multi-choice question evaluation benchmark for generation-oriented language models with keypoint categorization and quantifiable difficulty levels

## Executive Summary
ANGO addresses limitations of existing Chinese language model evaluation benchmarks by introducing a keypoint categorization system where each question maps to multiple specific ability domains, enhancing interpretability of evaluation results. The benchmark establishes a quantifiable difficulty standard based on human performance data, dividing questions into 9 difficulty levels. ANGO employs specialized sampling strategies and an evaluation framework to minimize data leakage impact while supporting swift testset iteration, providing a stronger challenge to models and more detailed evaluation results compared to existing benchmarks.

## Method Summary
ANGO is built using Chinese multi-choice questions from Administrative Proficiency Test (AAT) exams (2008-2023), implementing a hierarchical keypoint categorization system with 171 distinct nodes. The benchmark uses human performance data to calculate question difficulty scores through a formula incorporating actual exam scores, human accuracy, and keypoint accuracy, then divides questions into 9 difficulty levels. Specialized four-step sampling strategies ensure balanced keypoint distribution while minimizing information loss, and the evaluation framework employs accuracy, Human Hit, and Human Value metrics with seasonal dynamic updates to maintain objectivity.

## Key Results
- ANGO poses a stronger challenge to models compared to existing benchmarks through its quantifiable difficulty grading
- Keypoint categorization enables precise guidance for model training by mapping questions to multiple specific ability domains
- Seasonal dynamic evaluation framework ensures objectivity and authenticity by minimizing data leakage impact

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Keypoint categorization improves interpretability by mapping each question to multiple specific ability domains rather than single subjects.
- **Mechanism**: Uses a 171-node keypoint tree with 4 hierarchical levels where each question maps to 1-6 keypoints, enabling fine-grained analysis of model strengths/weaknesses across specific abilities.
- **Core assumption**: Multiple relevant abilities per question exist and can be consistently identified across the dataset.
- **Evidence anchors**: [abstract] "each question in ANGO can correspond to multiple keypoints, effectively enhancing interpretability of evaluation results"; [section 2.2.1] "The keypoint categorization can comprehensively represent the distribution of capabilities required to complete a question"
- **Break condition**: If keypoint identification becomes inconsistent or if model performance cannot be meaningfully decomposed by keypoint.

### Mechanism 2
- **Claim**: Quantifiable difficulty based on human performance creates a more reliable grading system than educational grade levels.
- **Mechanism**: Uses a formula incorporating actual exam scores (Sq), human accuracy (Accq), and keypoint accuracy (AccK) to create a continuous difficulty score, then divides questions into 9 levels.
- **Core assumption**: Human performance data from real exams provides an objective difficulty measure that correlates with model performance.
- **Evidence anchors**: [abstract] "we build a quantifiable question difficulty standard and divide ANGO questions into 9 difficulty levels"; [section 2.2.2] "Drawing upon the actual scores derived from the AAT and statistical indicators of human performance"
- **Break condition**: If human performance data becomes unavailable or if difficulty no longer correlates with model performance.

### Mechanism 3
- **Claim**: Dynamic sampling strategies minimize information loss while maintaining balanced keypoint distribution.
- **Mechanism**: Four-step sampling approach (Ks → K → Kp → Kg) ensures examples remain relevant to target questions while maintaining diversity. Testset sampling uses distance functions to balance keypoint representation.
- **Core assumption**: Hierarchical sampling can preserve relevant information while achieving desired statistical properties.
- **Evidence anchors**: [section 3.1] "this strategy ensures minimal information loss and is split into four steps"; [section 3.2] "To obtain a more even distribution, we suggest the following strategic sampling method"
- **Break condition**: If sampling introduces bias or if model performance is affected by sampling strategy.

## Foundational Learning

- **Concept**: Hierarchical categorization systems
  - Why needed here: The 171-node keypoint tree requires understanding of how to design and maintain hierarchical classification structures that capture multi-dimensional relationships
  - Quick check question: How would you design a 3-level hierarchy to categorize mathematical problem types?

- **Concept**: Difficulty calibration using human performance metrics
  - Why needed here: The difficulty formula requires understanding how to combine multiple performance indicators into a single meaningful metric
  - Quick check question: What statistical methods would you use to validate that your difficulty measure correlates with model performance?

- **Concept**: Stratified sampling and distance-based selection
  - Why needed here: The sampling strategies require understanding how to maintain representation across categories while minimizing information loss
  - Quick check question: How would you modify the distance function if some keypoint combinations are extremely rare?

## Architecture Onboarding

- **Component map**: Data preprocessing pipeline -> Keypoint assignment system -> Difficulty scoring engine -> Sampling module -> Evaluation framework -> Dynamic update system
- **Critical path**: Data → Keypoints → Difficulty → Sampling → Evaluation → Leaderboard
- **Design tradeoffs**:
  - Granular keypoint vs. manageable number of categories
  - Complex difficulty formula vs. interpretability
  - Multiple sampling steps vs. computational efficiency
  - Dynamic updates vs. leaderboard stability
- **Failure signatures**:
  - Unbalanced keypoint distribution → check sampling weights
  - Difficulty levels not correlating with performance → check human performance data quality
  - Evaluation metrics inconsistent → verify prompt formatting and option parsing
- **First 3 experiments**:
  1. Verify keypoint assignment consistency by having multiple annotators classify 100 random questions
  2. Test difficulty scoring by comparing human performance across difficulty levels
  3. Validate sampling strategy by measuring information retention across the four sampling steps

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset Scale and Representativeness: With only 2,373 questions from a single source (Administrative Proficiency Test exams), the benchmark may lack sufficient statistical power and domain diversity
- Keypoint Assignment Consistency: The hierarchical 171-node system relies on subjective annotation decisions without inter-annotator agreement metrics
- Difficulty Calibration Dependence: The difficulty formula depends entirely on human performance data from specific exams, which may introduce temporal biases

## Confidence
- **High Confidence**: Overall benchmark design and evaluation framework (prompt-based multi-choice format, seasonal updates)
- **Medium Confidence**: Keypoint categorization system and its impact on interpretability
- **Low Confidence**: Quantitative difficulty scoring formula and its correlation with model performance

## Next Checks
1. **Inter-annotator Agreement Study**: Have three independent annotators classify 200 random questions using the 171-node keypoint system, then calculate Cohen's kappa and Fleiss' kappa to assess consistency and reliability.

2. **Difficulty-Score Correlation Analysis**: Test the current top-performing models on subsets of questions across all 9 difficulty levels, then compute correlation coefficients between predicted difficulty scores and actual model performance drops.

3. **Cross-Domain Generalization Test**: Apply the ANGO keypoint categorization and difficulty scoring to a completely different Chinese multi-choice dataset (e.g., academic entrance exams) to evaluate the system's domain transferability and identify potential overfitting to the AAT exam format.