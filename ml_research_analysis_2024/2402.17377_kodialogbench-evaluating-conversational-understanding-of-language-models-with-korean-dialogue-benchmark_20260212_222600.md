---
ver: rpa2
title: 'KoDialogBench: Evaluating Conversational Understanding of Language Models
  with Korean Dialogue Benchmark'
arxiv_id: '2402.17377'
source_url: https://arxiv.org/abs/2402.17377
tags:
- korean
- language
- dialogue
- dialogues
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KoDialogBench, a benchmark designed to evaluate
  language models' conversational capabilities in Korean. The benchmark includes diverse
  tasks such as dialogue comprehension and response selection, covering topics, emotions,
  relations, locations, dialog acts, and facts.
---

# KoDialogBench: Evaluating Conversational Understanding of Language Models with Korean Dialogue Benchmark

## Quick Facts
- arXiv ID: 2402.17377
- Source URL: https://arxiv.org/abs/2402.17377
- Reference count: 37
- Current language models fall short of human-level performance in Korean conversations, with significant room for improvement.

## Executive Summary
This paper introduces KoDialogBench, a benchmark designed to evaluate language models' conversational capabilities in Korean. The benchmark includes diverse tasks such as dialogue comprehension and response selection, covering topics, emotions, relations, locations, dialog acts, and facts. Experiments show that current language models fall short of human-level performance in Korean conversations, with significant room for improvement. The study highlights the importance of incorporating Korean conversational data during model training and fine-tuning.

## Method Summary
KoDialogBench evaluates Korean language models using dialogue comprehension and response selection tasks. The benchmark uses both native Korean dialogues and translated ones, organized into 21 test sets across 6 aspects: topic, emotion, relation, location, dialog act, and fact identification. Models are evaluated using direct prompting, option prompting, and response selection prompting strategies, with accuracy as the primary metric. The evaluation compares models with different pretraining strategies and fine-tuning approaches.

## Key Results
- Language models primarily pretrained on Korean corpus (Polyglot-Ko) show better conversational proficiency than those with minimal Korean text (<0.1%).
- Instruction tuning using Korean datasets improves performance, while non-Korean instruction tuning provides no benefit.
- All models show higher accuracy on bilateral dialogues compared to multilateral ones, indicating speaker tracking difficulties.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Low-resource language models fail on Korean dialogues because pretraining Korean text proportion is too low (<0.1%).
- Mechanism: Pretraining corpus composition directly determines language model's ability to capture linguistic and cultural nuances of that language.
- Core assumption: Korean dialogue patterns require significant exposure to Korean text during pretraining.
- Evidence anchors:
  - [section] "Considering that the Korean text proportion in the pretraining dataset of LLaMA-2 is less than 0.1%, we speculate that such a proportion is insufficient to capture the intrinsic nuances and cultural context of Korean dialogues."
  - [section] "Our experimental results demonstrate that language models primarily pretrained on a large-scale Korean corpus, specifically Polyglot-Ko, show better conversational proficiency compared to other models."

### Mechanism 2
- Claim: Instruction tuning with non-Korean datasets does not transfer to Korean conversational tasks.
- Mechanism: Instruction format alignment is language-specific; general instruction tuning benefits don't cross language boundaries.
- Core assumption: The semantic structure and conversational patterns in Korean differ enough that instruction patterns learned in other languages don't apply.
- Evidence anchors:
  - [section] "We find that instruction tuning using datasets in languages other than Korean does not improve Korean conversational capabilities."
  - [section] "instruction tuning with datasets in Korean such as KoAlpaca and KORani, improve scores despite potential translation errors."

### Mechanism 3
- Claim: Response selection accuracy drops significantly for multilateral dialogues compared to bilateral ones.
- Mechanism: Language models struggle to track multiple speaker identities and maintain context across more participants.
- Core assumption: Model's internal speaker tracking mechanisms don't scale well beyond two participants.
- Evidence anchors:
  - [section] "All models show higher accuracy with bilateral dialogues as opposed to multilateral dialogues."
  - [section] "This implies that language models struggle to accurately trace the interlocutors' information as the number of speakers increases."

## Foundational Learning

- Concept: Cross-lingual transfer limitations
  - Why needed here: Understanding why instruction tuning in English doesn't help Korean dialogue tasks
  - Quick check question: Why might a model trained on English instructions struggle with Korean dialogue classification even if the underlying architecture is the same?

- Concept: Pretraining corpus composition effects
  - Why needed here: Explains why models with different Korean text proportions perform differently
  - Quick check question: If a model has 0.1% Korean text in pretraining vs 10%, what fundamental difference would you expect in Korean dialogue understanding?

- Concept: Speaker context tracking in dialogue
  - Why needed here: Explains why multilateral dialogues are harder than bilateral ones
  - Quick check question: What information must a model track to distinguish between three speakers in a conversation?

## Architecture Onboarding

- Component map: Data collection (native Korean or translated dialogues) → Task creation (dialogue comprehension and response selection) → Prompt engineering for different task types → Model evaluation across multiple dimensions
- Critical path: Data collection → Task structuring → Prompt design → Model inference → Performance analysis across task categories
- Design tradeoffs: Using translated dialogues increases dataset size but may introduce formality inconsistencies; direct prompting is simpler but option prompting may be more reliable for certain tasks
- Failure signatures: Low performance on native Korean datasets compared to translated ones indicates pretraining insufficiency; gender bias in response selection suggests dataset imbalance
- First 3 experiments:
  1. Compare model performance on native Korean vs translated dialogues to isolate pretraining effects
  2. Test different prompt formats (direct vs option) on the same tasks to measure format sensitivity
  3. Evaluate speaker tracking by comparing bilateral vs multilateral dialogue performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of language models on Korean dialogue comprehension tasks compare when evaluated on dialogues from different domains (e.g., social media vs. daily conversations vs. emotional dialogues)?
- Basis in paper: [explicit] The paper evaluates language models on various Korean dialogue tasks, including topic classification, emotion recognition, and relation classification, using datasets from different sources like Korean SNS, Korean Thematic Daily Dialogues, and Korean Emotional Dialogues.
- Why unresolved: The paper provides overall accuracy scores for each task but does not analyze performance variations across different dialogue domains within the same task.
- What evidence would resolve it: Detailed analysis of model performance on each individual test set within a task, broken down by dialogue domain.

### Open Question 2
- Question: What are the specific challenges faced by language models in understanding Korean dialogues with multiple speakers compared to dialogues with fewer speakers?
- Basis in paper: [explicit] The paper mentions that language models struggle with multilateral dialogues compared to bilateral dialogues, but does not delve into the specific reasons for this difficulty.
- Why unresolved: The paper only presents a high-level observation without exploring the underlying factors contributing to the performance gap.
- What evidence would resolve it: Analysis of model performance on different aspects of dialogue understanding (e.g., topic, emotion, relation) for bilateral and multilateral dialogues, along with error analysis to identify specific challenges.

### Open Question 3
- Question: How does the performance of language models on Korean dialogue tasks vary across different speaker gender compositions (e.g., male-male, female-female, mixed-gender dialogues)?
- Basis in paper: [explicit] The paper evaluates response selection accuracy across different gender compositions in bilateral dialogues and finds that most models perform better on male and mixed dialogues compared to female dialogues.
- Why unresolved: The paper does not investigate the reasons behind this gender-based performance difference or explore potential biases in the training data or model architecture.
- What evidence would resolve it: Analysis of model predictions on individual examples from each gender composition, along with examination of the training data distribution and potential gender-related linguistic features.

## Limitations
- Dataset composition imbalance between native Korean and translated dialogues may affect performance measurements
- Limited exploration of different Korean language varieties (dialects, formal vs. informal speech)
- No investigation of different model architectures for Korean dialogue understanding

## Confidence

**High Confidence Claims:**
- Current language models show significant performance gaps compared to human-level Korean dialogue understanding
- Models with higher Korean text proportions in pretraining demonstrate better conversational proficiency
- Instruction tuning on Korean datasets improves performance compared to non-Korean instruction tuning

**Medium Confidence Claims:**
- The specific threshold of <0.1% Korean text in pretraining is insufficient for capturing Korean dialogue nuances
- Response selection accuracy degradation in multilateral dialogues is primarily due to speaker tracking difficulties
- Translated dialogues are a viable alternative to native Korean dialogues for benchmarking

**Low Confidence Claims:**
- The exact percentage of Korean text needed in pretraining for optimal performance
- The relative contribution of pretraining vs. fine-tuning for Korean dialogue capabilities
- The extent to which observed gender biases reflect dataset characteristics vs. model architecture limitations

## Next Checks

1. **Cross-Architecture Speaker Tracking Test**: Evaluate models with explicit speaker tracking mechanisms (e.g., speaker-aware transformers) on the multilateral dialogue tasks to determine whether architectural improvements can overcome the observed performance drop.

2. **Dialect and Register Variation Analysis**: Create controlled experiments testing the same models on different Korean registers (formal vs. casual) and regional dialects to quantify the impact of linguistic variation on performance.

3. **Pretraining Proportion Sensitivity Analysis**: Systematically vary the Korean text proportion in pretraining (0.01%, 0.1%, 1%, 10%) while holding other factors constant to identify the inflection point where performance improvements plateau.