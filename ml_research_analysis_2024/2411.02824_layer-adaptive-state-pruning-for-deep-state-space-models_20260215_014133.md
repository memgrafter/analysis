---
ver: rpa2
title: Layer-Adaptive State Pruning for Deep State Space Models
arxiv_id: '2411.02824'
source_url: https://arxiv.org/abs/2411.02824
tags:
- pruning
- state
- last
- accuracy
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "LAST (Layer-Adaptive State pruning) is a structured pruning method\
  \ for deep state space models (SSMs) that reduces state dimension by identifying\
  \ and removing insignificant states across layers. The method extends modal truncation\
  \ to multi-system reduction, using H\u221E norms to evaluate state importance and\
  \ layer-wise energy normalization to enable cross-layer comparison."
---

# Layer-Adaptive State Pruning for Deep State Space Models

## Quick Facts
- **arXiv ID:** 2411.02824
- **Source URL:** https://arxiv.org/abs/2411.02824
- **Reference count:** 40
- **Primary result:** LAST enables 33% state dimension reduction with only 0.52% accuracy loss on average across various sequence benchmarks

## Executive Summary
LAST (Layer-Adaptive State pruning) is a structured pruning method for deep state space models (SSMs) that reduces state dimension by identifying and removing insignificant states across layers. The method extends modal truncation to multi-system reduction, using H∞ norms to evaluate state importance and layer-wise energy normalization to enable cross-layer comparison. On various sequence benchmarks including Long Range Arena, Speech Commands, and image classification tasks, LAST successfully optimized previous SSMs, revealing significant redundancy in state spaces while preserving key properties like handling raw speech and adapting to sampling rate changes.

## Method Summary
LAST computes importance scores for each state across all layers using H∞ norms combined with energy normalization, enabling global pruning criteria that can compare states across different layers. The method identifies states with smallest LAST scores for removal, achieving significant compression ratios while maintaining performance with minimal retraining. Unlike traditional modal truncation limited to single systems, LAST extends to multi-system reduction for stacked SSM layers, allowing layer-adaptive pruning rates based on relative state importance rather than uniform pruning across all layers.

## Key Results
- Achieved 33% state dimension reduction on average across various tasks
- Maintained performance with only 0.52% accuracy loss in multi-input multi-output SSMs
- Successfully pruned states in multi-layer SSMs without retraining
- Preserved key SSM properties including raw speech handling and sampling rate adaptation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LAST scores measure the relative maximum frequency-domain gain of each subsystem when subsystems with lower scores are excluded, enabling cross-layer comparison and layer-adaptive pruning.
- **Mechanism:** The LAST score for state i in layer l is computed as the H∞ norm of that subsystem divided by the sum of H∞ norms of all subsystems with equal or higher scores in the same layer. This energy normalization creates a common scale across layers, allowing states from different layers to be directly compared.
- **Core assumption:** The energy normalization through LAST scores provides a meaningful measure of state importance that correlates with model-level output energy loss.
- **Evidence anchors:**
  - [abstract] "LAST scores are evaluated using the H∞ norms of subsystems and layer-wise energy normalization. The scores serve as global pruning criteria, enabling cross-layer comparison of states and layer-adaptive pruning."
  - [section 3.2] "We define the LAST score for x(l)_i as follows: LAST[x(l)_i; Σ(l)] = H∞[x(l)_i; Σ(l)] / Σ(j≤i) H∞[x(l)_j; Σ(l)]"
  - [corpus] No direct evidence in corpus papers - this appears to be a novel contribution
- **Break condition:** If the energy normalization fails to properly account for differences in layer-wise signal amplification, the cross-layer comparison would become invalid.

### Mechanism 2
- **Claim:** The H∞ norm provides an upper bound on the energy loss when pruning a state, ensuring the pruned system remains close to the original.
- **Mechanism:** Using the property that ||y||²₂ ≤ ||G||²∞ ||u||²₂, LAST uses the squared H∞ norm of each subsystem as a measure of its contribution to output energy. By pruning states with smallest LAST scores, the total energy loss is minimized.
- **Core assumption:** The H∞ norm accurately captures the maximum frequency-domain gain and thus the energy contribution of each subsystem.
- **Evidence anchors:**
  - [section 2.3] "Using the properties of diagonal systems and the H∞ norm in Equation (2), the energy loss can be bounded as follows: ||fσ(u; Σ) - fσ(u; Σ-P)||²₂ ≤ Σ(i∈P) ||Gi||²∞ ||u||²₂"
  - [section 3.1] "Therefore, we can reduce a system by pruning subsystems with small H∞ norms, minimizing the upper bound in Equation (3)"
  - [corpus] No direct evidence in corpus papers - this appears to be a novel application of H∞ norms to SSM pruning
- **Break condition:** If the nonlinearity of the activation function causes significant deviations from the H∞ norm bound, the pruning decisions based on H∞ scores may not minimize actual output energy loss.

### Mechanism 3
- **Claim:** Layer-adaptive pruning allows different layers to be pruned at different rates based on their relative importance to overall model performance.
- **Mechanism:** By computing LAST scores across all layers and selecting states with lowest scores globally (rather than pruning each layer uniformly), LAST can prune more aggressively in layers with many insignificant states while preserving important layers.
- **Core assumption:** Different layers have different proportions of significant vs. insignificant states, and layer-adaptive pruning can exploit this variation.
- **Evidence anchors:**
  - [section 3.2] "We extend the local pruning criterion to a global pruning criterion by assessing the model-level energy loss when considering multiple stacked SSM layers"
  - [section 5.4.1] "In Layers 5 and 6, the overall H∞ scores were relatively lower than other layers except Layer 1. Global H∞ directly used H∞ scores, resulting in excessive pruning in Layers 5 and 6"
  - [corpus] No direct evidence in corpus papers - this appears to be a novel contribution
- **Break condition:** If all layers have similar distributions of state importance, layer-adaptive pruning would provide no benefit over uniform pruning.

## Foundational Learning

- **Concept:** State Space Models (SSMs) and their diagonal parameterization
  - **Why needed here:** LAST is specifically designed for diagonal SSMs, and understanding their structure is crucial for implementing the pruning method.
  - **Quick check question:** How does the diagonal parameterization of SSMs enable efficient computation of H∞ norms for each subsystem?
  
- **Concept:** H∞ norms and their properties in linear systems
  - **Why needed here:** The entire pruning criterion is based on H∞ norms, so understanding their mathematical properties is essential.
  - **Quick check question:** What is the relationship between the H∞ norm of a system and the maximum gain it can exhibit across all frequencies?

- **Concept:** Model Order Reduction (MOR) techniques, particularly modal truncation
  - **Why needed here:** LAST extends modal truncation from single systems to multi-system reduction, so understanding MOR is important for grasping the theoretical foundation.
  - **Quick check question:** How does modal truncation determine which states to remove from a system?

## Architecture Onboarding

- **Component map:** Encoder -> L SSM layers (with LAST module) -> Decoder
- **Critical path:**
  1. Compute H∞ norms for all subsystems across all layers
  2. Sort subsystems within each layer by H∞ norm
  3. Calculate LAST scores using energy normalization
  4. Select states with lowest LAST scores until desired compression is achieved
  5. Apply pruning masks to remove parameters associated with pruned states

- **Design tradeoffs:**
  - Per-state pruning granularity vs. block-level pruning (LAST uses per-state for finer control)
  - Energy normalization vs. direct H∞ score comparison (LAST uses normalization for cross-layer comparison)
  - One-shot pruning vs. iterative pruning (LAST uses one-shot for efficiency)

- **Failure signatures:**
  - Performance degradation despite low LAST scores → H∞ norm may not correlate well with actual importance
  - Instability after pruning → Insufficient states remaining for proper signal processing
  - Inefficient pruning ratios across layers → Energy normalization may not properly account for layer differences

- **First 3 experiments:**
  1. Apply LAST to a small S4D model on Text task, prune 20% of states, verify accuracy loss is <1%
  2. Compare LAST with Global H∞ (no energy normalization) on Pathfinder task, observe differences in pruning decisions
  3. Test LAST on varying sampling rates in Speech Command task to verify adaptive property is preserved after pruning

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the stability guarantees of LAST be maintained when pruning during training rather than in a post-training setting?
- **Basis in paper:** [explicit] The paper discusses stability being guaranteed through Hurwitz parameterization for direct pole training, but does not address how this interacts with pruning during training.
- **Why unresolved:** The paper validates LAST on pre-trained models and doesn't explore pruning schedules or dynamic stability maintenance during training.
- **What evidence would resolve it:** Experimental results showing LAST's effectiveness with different pruning schedules and stability metrics during training.

### Open Question 2
- **Question:** What are the fundamental limits of state space model compressibility, and how do they vary across different task domains?
- **Basis in paper:** [inferred] The paper demonstrates varying compressibility across tasks (33% average pruning with 0.52% accuracy loss) but doesn't establish theoretical bounds.
- **Why unresolved:** The paper provides empirical results but doesn't derive theoretical limits or explain why certain tasks are more compressible than others.
- **What evidence would resolve it:** Analysis of the relationship between task complexity, state space structure, and theoretical compressibility bounds.

### Open Question 3
- **Question:** How can LAST be adapted to handle non-diagonal state space models while maintaining computational efficiency?
- **Basis in paper:** [explicit] The paper specifically focuses on diagonal SSMs and mentions that balanced truncation (for non-diagonal systems) is not applicable to current diagonal SSMs.
- **Why unresolved:** The paper establishes LAST for diagonal systems but doesn't explore extensions to more general SSM architectures.
- **What evidence would resolve it:** Modified LAST algorithm for non-diagonal systems with empirical validation showing maintained or improved performance.

## Limitations

- LAST relies on diagonal SSM structures, limiting applicability to other SSM architectures
- The energy normalization approach assumes H∞ norms correlate with actual output energy loss, which may not hold for highly nonlinear activation functions
- Layer-adaptive pruning effectiveness depends on having significant variation in state importance across layers, which may not always be present

## Confidence

- **High Confidence:** The mathematical foundation of using H∞ norms for energy loss bounds (Section 2.3)
- **Medium Confidence:** The cross-layer comparison through energy normalization (Section 3.2)
- **Medium Confidence:** The empirical results showing 33% state reduction with minimal accuracy loss

## Next Checks

1. **Activation Function Impact:** Test LAST on SSMs with different activation functions (ReLU, GeLU, Swish) to quantify how nonlinearity affects the correlation between H∞ norms and actual output energy loss.

2. **Layer Importance Distribution:** Analyze the distribution of LAST scores across layers in various models to determine if the assumption of heterogeneous layer importance is generally valid or task-specific.

3. **Alternative Norm Measures:** Compare LAST performance using alternative importance measures (H2 norms, impulse response energy) to validate whether H∞ norms provide optimal pruning decisions or if other metrics could perform better.