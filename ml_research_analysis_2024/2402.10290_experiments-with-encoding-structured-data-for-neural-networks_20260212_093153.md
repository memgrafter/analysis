---
ver: rpa2
title: Experiments with Encoding Structured Data for Neural Networks
arxiv_id: '2402.10290'
source_url: https://arxiv.org/abs/2402.10290
tags:
- units
- game
- board
- each
- encoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper explores encoding complex structured data from the Battlespace
  game domain into tensors suitable for neural network input. Three encoding approaches
  were developed: (1) concatenating binary representations of unit properties into
  integers, (2) separating properties into distinct layers with integer/boolean values,
  and (3) using a list of unit properties plus visible-range encoding.'
---

# Experiments with Encoding Structured Data for Neural Networks

## Quick Facts
- arXiv ID: 2402.10290
- Source URL: https://arxiv.org/abs/2402.10290
- Reference count: 20
- Key outcome: Encoding choice significantly impacts agent behavior in Battlespace game - binary concatenation led to property ordering bias while layered encoding prioritized positions

## Executive Summary
This paper investigates how different tensor encoding strategies for structured game data affect neural network agent behavior in the Battlespace wargame. The authors develop three encoding approaches: binary concatenation into integers, separate property layers, and list-based encodings with visible-range information. Through experiments combining these encodings with CNN and dense network architectures, they demonstrate that encoding decisions create measurable behavioral biases in the resulting agents. The work reveals fundamental trade-offs between encoding compactness and representational fidelity, and highlights challenges with exploration in sparse action spaces where most moves have minimal impact.

## Method Summary
The authors encode Battlespace game state data into tensors using three approaches: (1) concatenating binary representations of unit properties into single integers, (2) separating properties into distinct tensor layers with integer/boolean values, and (3) using a list of unit properties plus visible-range encoding. These encodings are combined with various neural network architectures (CNNs and dense networks) to create AI agents. The agents are trained using a hybrid approach combining MCTS and DQN techniques, where MCTS rollouts generate training targets. The evaluation focuses on agent action selection quality and game performance across different encoding strategies.

## Key Results
- Binary concatenation encoding creates property ordering bias where higher-weight bits (like unitID) dominate agent attention over lower-weight bits (like orientation)
- Layered property encoding shifts agent focus toward unit positions rather than individual property values
- Agents developed strong bias toward safe, non-impactful actions due to sparse reward structure and large action space, making exploration difficult
- MCTS rollouts proved computationally prohibitive for full game state spaces, limiting evaluation scope

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Concatenating binary representations of unit properties into a single integer preserves all property information but introduces ordering bias.
- Mechanism: The integer encoding places higher-weight bits (e.g., unitID) in more significant positions, so the model prioritizes that property over lower-weight bits (e.g., orientation, health).
- Core assumption: Binary-to-integer conversion is the model's only view of the encoded state; it cannot recover original property boundaries.
- Evidence anchors:
  - [abstract] "encoding choice significantly impacts agent behavior - the first approach led to agents prioritizing property ordering"
  - [section] "unitId was overpowering the orientation while binarizing data"
- Break condition: If the model learns to decode the integer back into separate properties, the ordering bias could be mitigated.

### Mechanism 2
- Claim: Separating properties into distinct layers allows the model to attend to each property independently, reducing bias from encoding order.
- Mechanism: Each property occupies its own channel in the tensor; the CNN can learn spatial filters that combine information across layers without being locked into a fixed binary concatenation scheme.
- Core assumption: The CNN's convolutional filters can effectively aggregate multi-layer information without loss.
- Evidence anchors:
  - [abstract] "the second resulted in agents prioritizing unit positions"
  - [section] "we separated each property into its own layer containing either an integer or a boolean"
- Break condition: If the number of properties grows large, the tensor may become too wide for efficient convolution.

### Mechanism 3
- Claim: List-based encodings flatten the spatial structure into a flat feature vector, requiring dense networks and losing local correlation cues.
- Mechanism: By concatenating properties into a 1D vector, spatial adjacency information is discarded; the model must infer relationships solely from feature indices.
- Core assumption: The list encoding preserves all relevant features but the dense network can reconstruct spatial relationships from the flat vector.
- Evidence anchors:
  - [section] "we also experiment with encoding packaged with the Battlespace creators... list of unit properties"
  - [section] "With our encoding strategy, we observed that the unitId was overpowering the orientation"
- Break condition: If spatial locality is critical for decision quality, the list encoding may underperform.

## Foundational Learning

- Concept: Tensor representation of structured game state
  - Why needed here: Neural networks require fixed-size numeric tensors; structured data (units, positions, orientations) must be flattened into this format.
  - Quick check question: How many layers would you need if each unit has 3 properties (ID, orientation, health) and you separate them?

- Concept: Binary-to-integer encoding and bit significance
  - Why needed here: Concatenating binary strings into integers is a compact representation but imposes ordering; understanding bit significance explains why some properties dominate.
  - Quick check question: If unitID is 5 bits and orientation is 3 bits, which property will dominate the integer value?

- Concept: CNN vs dense network input shapes
  - Why needed here: CNNs expect multi-dimensional tensors with spatial structure; dense networks require flat vectors. The encoding choice dictates which architecture is viable.
  - Quick check question: Can a CNN process a flat list of unit properties without reshaping? Why or why not?

## Architecture Onboarding

- Component map: Python state class -> tensor encoder (3 encoding variants) -> CNN or dense network -> forward pass -> action probabilities -> MCTS rollouts -> training targets
- Critical path: Encoding -> CNN forward -> MCTS target generation -> loss computation -> weight update -> next encoding
- Design tradeoffs:
  - Encoding #1: compact but biased toward high-weight bits; simpler tensor shape
  - Encoding #2: unbiased per-property representation; tensor grows with property count
  - Encoding #3: easy to implement; loses spatial locality; requires dense nets
- Failure signatures:
  - Model converges to "do nothing" actions -> likely sparse reward or bias toward safe moves
  - Loss plateaus early -> encoding may not capture distinguishing features
  - Training diverges -> learning rate or network depth mismatch
- First 3 experiments:
  1. Encode a simple 2-unit board with encoding #1; verify tensor shape and integer values
  2. Run a single CNN forward pass on the tensor; inspect activations for property dominance
  3. Generate MCTS rollouts for a random board; confirm win/loss/draw probabilities are computed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What encoding strategy would most effectively balance the trade-off between tensor dimensionality and representational fidelity for complex structured game data?
- Basis in paper: [inferred] The paper discusses multiple encoding approaches with trade-offs between few cells with complex contents (binary concatenation) versus many cells with simple contents (separate layers for each property).
- Why unresolved: The authors acknowledge this as a fundamental tension but don't provide a definitive solution. The binary concatenation approach prioritized property ordering while the layered approach prioritized spatial positions, both with limitations.
- What evidence would resolve it: Systematic comparison of agent performance across various encoding strategies on the same tasks, measuring both representational efficiency and behavioral outcomes.

### Open Question 2
- Question: How can exploration be improved in sparse action space environments where most actions have minimal impact on the game state?
- Basis in paper: [explicit] The authors observed agents developed a bias toward safe, non-impactful actions and struggled to explore meaningful states despite experimenting with weighted random agents.
- Why unresolved: Current approaches like MCTS with random rollouts proved computationally prohibitive, and alternative methods like the DeepNash approach were only suggested for future work.
- What evidence would resolve it: Demonstration of an agent that consistently discovers and executes impactful actions in sparse action space environments, with clear metrics showing improved exploration of the state space.

### Open Question 3
- Question: What architectural modifications could enable neural networks to effectively process both spatial game state information and sequential move history in a unified representation?
- Basis in paper: [inferred] The authors mention being inspired by DeepMind's approach to encoding move history but note they haven't implemented this yet, suggesting it as future work.
- Why unresolved: The current architecture processes only the current state encoding, and the authors acknowledge the potential value of incorporating move history without having tested such an approach.
- What evidence would resolve it: Empirical comparison showing improved agent performance when incorporating move history encoding versus state-only encoding, with specific architectural details of how the history is integrated.

## Limitations
- Computational infeasibility of MCTS rollouts for full game states limits evaluation scope
- Observed biases may not generalize beyond the specific Battlespace game domain
- Third encoding approach mentioned in abstract but implementation details are unclear
- Strong bias toward safe, non-impactful actions suggests fundamental challenges with sparse reward environments

## Confidence
- High confidence: The mechanism explaining how binary-to-integer encoding creates property ordering bias through bit significance is well-supported by the evidence that "unitId was overpowering the orientation."
- Medium confidence: The claim that separating properties into layers reduces encoding bias is plausible but the evidence shows agents still prioritized positions, suggesting other biases remain.
- Low confidence: The computational limitation of MCTS rollouts preventing full evaluation is mentioned but not quantified - we don't know the exact scale where this becomes prohibitive.

## Next Checks
1. Replicate the encoding experiments with synthetic structured data to test if the observed biases (property ordering, position prioritization) persist across different domains.
2. Measure the computational complexity of MCTS rollouts as a function of game state size to identify the exact scaling threshold where the approach becomes infeasible.
3. Implement the third encoding approach (list of unit properties plus visible-range encoding) and compare its performance to the other two methods to determine if the abstract's claim about "significantly impacts agent behavior" holds across all three encodings.