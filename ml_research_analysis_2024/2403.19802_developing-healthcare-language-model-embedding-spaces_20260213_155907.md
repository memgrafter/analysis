---
ver: rpa2
title: Developing Healthcare Language Model Embedding Spaces
arxiv_id: '2403.19802'
source_url: https://arxiv.org/abs/2403.19802
tags:
- pre-training
- llms
- tasks
- task
- note
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study compares different pre-training methods to adapt small
  language models for healthcare text classification. Three approaches are evaluated:
  masked language modeling, contrastive learning (DeCLUTR), and metadata-based pre-training.'
---

# Developing Healthcare Language Model Embedding Spaces

## Quick Facts
- arXiv ID: 2403.19802
- Source URL: https://arxiv.org/abs/2403.19802
- Authors: Niall Taylor; Dan Schofield; Andrey Kormilitzin; Dan W Joyce; Alejo Nevado-Holgado
- Reference count: 40
- One-line primary result: Contrastive pre-training outperforms other methods for healthcare text classification with limited labeled data.

## Executive Summary
This study evaluates three pre-training methods to adapt small language models for healthcare text classification: masked language modeling, contrastive learning (DeCLUTR), and metadata-based pre-training. Models are tested on document classification tasks across three healthcare datasets. Contrastive learning achieves the best performance, especially with limited labeled data and fewer parameter updates. Metadata-based pre-training does not improve classification but enhances embedding cluster separability. All domain-adapted models outperform general LLMs, validating the importance of healthcare specialization.

## Method Summary
The research adapts RoBERTa-base models through three pre-training methods on healthcare datasets (MIMIC-III, OHFT, PSIR). Models are evaluated on document classification tasks with frozen and fine-tuned configurations. Embedding spaces are analyzed using uniformity, alignment, and graph-based metrics. The study systematically compares performance across different pre-training approaches while varying the number of frozen layers.

## Key Results
- Contrastive pre-training (DeCLUTR) delivers superior classification performance, particularly with limited labeled data
- Metadata-based pre-training improves embedding cluster separability without boosting classification accuracy
- All domain-adapted models outperform general LLMs on healthcare classification tasks
- Freezing different numbers of LLM layers enables efficient fine-tuning while maintaining performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive pre-training aligns embeddings by pulling together semantically related documents while pushing apart unrelated ones, improving classification performance especially with limited data.
- Mechanism: InfoNCE loss is used during DeCLUTR pre-training, where anchor-positive span pairs from the same document are encouraged to have high cosine similarity, while negative pairs are pushed apart. This directly optimizes the embedding space for downstream classification tasks.
- Core assumption: The span sampling strategy effectively captures document semantics and that the InfoNCE objective generalizes well to unseen classification tasks.
- Evidence anchors:
  - [abstract] "Contrastively trained models outperform other approaches on the classification tasks, delivering strong performance from limited labeled data and with fewer model parameter updates required."
  - [section] "DeCLUTR stands for Deep Contrastive Learning for Unsupervised Textual Representations, and uses a contrastive loss function to encourage sentence or document level embeddings that are taken from the same document type or class to be closer together in the learned embedding space."
  - [corpus] Weak evidence: No direct citation of DeCLUTR or InfoNCE performance metrics in corpus papers.
- Break condition: If the span sampling strategy does not capture meaningful semantic similarity, or if the downstream tasks have label distributions that do not match the pre-training corpus structure.

### Mechanism 2
- Claim: Freezing different numbers of LLM layers allows efficient fine-tuning by reusing learned embeddings while adapting classification heads for specific tasks.
- Mechanism: By freezing the LLM body, only the classification head is updated, reducing computational cost and avoiding catastrophic forgetting. This works because the embeddings already capture useful semantic features.
- Core assumption: The pre-trained embeddings contain sufficient discriminative features for the downstream tasks, and the classification head can learn to map these embeddings to correct labels without updating the base model.
- Evidence anchors:
  - [abstract] "delivering strong performance from limited labeled data and with fewer model parameter updates required."
  - [section] "We sought to explore the potential of using the embeddings produced by the different LLMs without any further fine-tuning in relation to a given downstream task, keeping the LLM body frozen, or by freezing different numbers of layers of the LLM model."
  - [corpus] Moderate evidence: Similar freezing strategies are used in other domain adaptation papers (e.g., clinical prompt learning with frozen LLMs).
- Break condition: If the embedding space is not discriminative enough for the task, or if the task requires fine-grained adaptation that frozen embeddings cannot provide.

### Mechanism 3
- Claim: Using structured metadata like note categories as a pre-training signal improves embedding cluster separability even if it does not directly boost classification accuracy.
- Mechanism: A classification head is trained to predict note categories from embeddings, encouraging the model to learn features that distinguish different types of clinical notes (e.g., doctor vs. social worker notes).
- Core assumption: Note category metadata captures meaningful semantic differences in the text that are useful for downstream tasks, and the classification objective encourages embeddings to reflect these differences.
- Evidence anchors:
  - [abstract] "While metadata-based pre-training does not further improve classifications across the datasets, it yields interesting embedding cluster separability."
  - [section] "We formulate this task as a replacement of the original next sentence prediction task used in BERTs implementation [1]. Whole sequence embeddings, e, are fed to a classification head fhead(·), which has the task of calculating the logits yj of each of the possible c classes j ∈ C."
  - [corpus] Weak evidence: No direct citation of note category pre-training improving cluster separability in other works.
- Break condition: If the metadata categories do not reflect meaningful semantic differences in the text, or if the downstream tasks are not related to the metadata structure.

## Foundational Learning

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: To understand how DeCLUTR pre-training aligns embeddings for downstream tasks without requiring labeled data.
  - Quick check question: What is the main difference between InfoNCE loss and standard cross-entropy loss?

- Concept: Embedding pooling strategies (mean pooling)
  - Why needed here: To convert variable-length document embeddings into fixed-length vectors for classification and analysis.
  - Quick check question: Why is mean pooling a common choice for document-level embeddings in transformer models?

- Concept: Graph network analysis of embeddings
  - Why needed here: To assess the structure and connectivity of the learned embedding space, revealing how well different classes are separated.
  - Quick check question: How does the number of connected components in a similarity graph relate to the diversity of the embedding space?

## Architecture Onboarding

- Component map:
  - Base LLM (RoBERTa-base) -> Pre-training objectives (MLM, DeCLUTR, Note category classification) -> Downstream classification head (MLP) -> Data preprocessing and tokenization pipeline

- Critical path:
  1. Load and preprocess clinical text data
  2. Apply chosen pre-training objective to adapt embeddings
  3. Evaluate embeddings on downstream classification tasks
  4. Analyze embedding space for cluster separability and graph structure

- Design tradeoffs:
  - Freezing vs. fine-tuning: Freezing reduces compute but may limit task-specific adaptation.
  - Span length in DeCLUTR: Longer spans capture more context but reduce the number of valid samples.
  - Metadata usage: Note category pre-training improves cluster structure but may not boost classification accuracy.

- Failure signatures:
  - Low classification performance despite pre-training: Embedding space may not be discriminative enough.
  - High uniformity but low alignment: Embeddings are spread out but not well clustered by class.
  - Graph with many small components: Poor semantic grouping in the embedding space.

- First 3 experiments:
  1. Compare frozen LLM performance on a small downstream task with different pre-training methods.
  2. Vary the number of frozen layers and measure impact on classification accuracy.
  3. Analyze cosine similarity distributions within and between classes for each pre-trained model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal sampling strategy for DeCLUTR contrastive pre-training in healthcare text?
- Basis in paper: [explicit] The paper notes that span length and sampling parameters significantly affect DeCLUTR performance, but only explored a limited parameter space due to resource constraints.
- Why unresolved: The authors acknowledge they couldn't exhaustively explore all combinations of sampling parameters (anchor numbers, positive sampling strategies, document length thresholds) across different healthcare datasets.
- What evidence would resolve it: Systematic ablation studies varying sampling parameters (anchor count, positive sampling method, minimum document length) across diverse healthcare datasets while measuring downstream classification performance.

### Open Question 2
- Question: How does metadata-based pre-training perform when the metadata is semantically related to downstream tasks?
- Basis in paper: [explicit] The note category pre-training approach did not improve classification performance, but the authors suggest this might be because the metadata wasn't well-aligned with task objectives.
- Why unresolved: The paper only tested one type of metadata (note category) and didn't explore whether metadata more directly related to task objectives would yield better results.
- What evidence would resolve it: Experiments using metadata specifically designed to reflect downstream task categories, comparing performance against standard MLM and contrastive approaches.

### Open Question 3
- Question: What is the relationship between embedding space uniformity/alignment metrics and downstream task performance?
- Basis in paper: [explicit] The authors measured uniformity and alignment metrics but found DeCLUTR models had high alignment yet lower classification performance, creating a disconnect between these metrics and actual task performance.
- Why unresolved: The paper shows these metrics can diverge from practical performance outcomes, but doesn't establish when or why they correlate with task success.
- What evidence would resolve it: Correlation studies between uniformity/alignment metrics and downstream performance across multiple tasks and model types, identifying conditions where these metrics predict task success.

## Limitations

- Analysis limited to three healthcare datasets and document classification tasks, potentially limiting generalizability
- Metadata-based pre-training shows embedding improvements but lacks rigorous validation of practical utility
- Computational efficiency claims lack comprehensive resource usage comparisons across different hardware configurations

## Confidence

**High Confidence**: The core finding that domain-adapted models outperform general LLMs in healthcare classification tasks is well-supported by empirical results across multiple datasets. The observation that contrastive learning achieves superior performance, particularly with limited labeled data, is consistently demonstrated through F1 macro scores and parameter efficiency metrics.

**Medium Confidence**: The assertion that metadata-based pre-training improves embedding cluster separability is supported by qualitative observations but lacks rigorous quantitative validation. The specific mechanisms by which different pre-training methods affect embedding geometry (uniformity, alignment) are described but not fully explained through systematic ablation studies.

**Low Confidence**: Claims about the practical deployment implications for resource-constrained healthcare settings are forward-looking and not directly validated through real-world implementation studies. The optimal freezing strategy for balancing computational efficiency and performance requires more extensive hyperparameter exploration.

## Next Checks

1. **Cross-Dataset Generalization Test**: Evaluate the best-performing pre-trained models (contrastive learning) on additional healthcare datasets not seen during pre-training to assess true domain adaptation capabilities beyond the three studied corpora.

2. **Embedding Geometry Analysis**: Conduct systematic experiments varying clustering thresholds and similarity metrics to quantify the relationship between pre-training method choices and embedding space properties (uniformity, alignment, cluster separation) across different healthcare document types.

3. **Real-World Deployment Simulation**: Measure actual inference latency and memory usage for frozen vs. fine-tuned models across different hardware configurations (CPU, GPU, edge devices) to validate computational efficiency claims under realistic deployment constraints.