---
ver: rpa2
title: Provable Robustness of (Graph) Neural Networks Against Data Poisoning and Backdoor
  Attacks
arxiv_id: '2407.10867'
source_url: https://arxiv.org/abs/2407.10867
tags:
- accuracy
- nodes
- learning
- poisoning
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first white-box poisoning certificate
  for Graph Neural Networks (GNNs), called QPCert, by leveraging the Neural Tangent
  Kernel (NTK) to model training dynamics and reformulating the bilevel optimization
  problem as a mixed-integer linear program (MILP). The method provides deterministic
  robustness guarantees against feature-based poisoning and backdoor attacks on node
  features in semi-supervised learning.
---

# Provable Robustness of (Graph) Neural Networks Against Data Poisoning and Backdoor Attacks

## Quick Facts
- arXiv ID: 2407.10867
- Source URL: https://arxiv.org/abs/2407.10867
- Reference count: 40
- This paper introduces the first white-box poisoning certificate for Graph Neural Networks (GNNs), called QPCert, by leveraging the Neural Tangent Kernel (NTK) to model training dynamics and reformulating the bilevel optimization problem as a mixed-integer linear program (MILP).

## Executive Summary
This work presents QPCert, a framework for providing deterministic white-box poisoning certificates for Graph Neural Networks against feature-based poisoning and backdoor attacks. By leveraging the Neural Tangent Kernel to model training dynamics and reformulating the bilevel optimization problem as a mixed-integer linear program, QPCert can certify test nodes against worst-case adversarial perturbations. The framework is applied to semi-supervised node classification tasks on real-world graph datasets, demonstrating that GNNs can achieve better certified accuracy than MLPs when leveraging graph structure.

## Method Summary
QPCert provides deterministic white-box poisoning certificates by modeling the training dynamics of sufficiently wide GNNs using the Neural Tangent Kernel (NTK). The bilevel optimization problem of poisoning attacks is reformulated as a mixed-integer linear program (MILP) using Karush-Kuhn-Tucker (KKT) conditions and linearization techniques. The framework is evaluated on semi-supervised node classification tasks across Cora-ML, WikiCS, and CSBM datasets with various GNN architectures including GCN, SGC, APPNP, GIN, and GraphSAGE.

## Key Results
- QPCert delivers non-trivial certificates for real-world graph datasets, showing that leveraging graph structure improves certified accuracy compared to MLPs
- The method is theoretically tight but shows practical gaps for large perturbations due to relaxation in kernel bound derivation
- QPCert scales to common benchmark graphs and provides insights into how graph connectivity and architectural choices affect worst-case robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: QPCert provides deterministic white-box poisoning certificates for Graph Neural Networks (GNNs) by leveraging the Neural Tangent Kernel (NTK) and reformulating the bilevel optimization problem as a mixed-integer linear program (MILP).
- Mechanism: The NTK characterizes the training dynamics of sufficiently wide networks, allowing QPCert to model the complex interactions between graph structure, node features, and learning dynamics. The bilevel optimization problem is reduced to a single-level problem using Karush-Kuhn-Tucker (KKT) conditions, then reformulated as a MILP by introducing binary variables to linearize product terms.
- Core assumption: The NTK remains constant throughout training for sufficiently wide networks, and the equivalence between NNs trained with soft-margin loss and SVMs with NTK holds.
- Evidence anchors:
  - [abstract] "Our certificates are white-box and based upon (i) the neural tangent kernel, which characterizes the training dynamics of sufficiently wide networks; and (ii) a novel reformulation of the bilevel optimization problem describing poisoning as a mixed-integer linear program."
  - [section] "Using this equivalence, we introduce a novel reformulation of the bilevel optimization problem as a Mixed-Integer Linear Program (MILP) that allows to certify test datapoints against poisoning as well as backdoor attacks for sufficiently wide NNs."
- Break condition: The NTK approximation breaks down for narrow networks, or the MILP becomes intractable for very large graphs with many labeled nodes.

### Mechanism 2
- Claim: The MILP formulation enables tractable certification by leveraging the bounded nature of adversarial perturbations and the structure of the SVM dual problem.
- Mechanism: Bounds on kernel elements are derived based on the perturbation model (ℓp-bounded node feature changes). These bounds, combined with big-M constraints, allow linearization of complementary slackness conditions from the KKT system. The resulting MILP can be solved using modern solvers like Gurobi.
- Core assumption: The adversary's perturbations are bounded (ℓp norm constraint), and the kernel matrix elements can be bounded element-wise based on these perturbations.
- Evidence anchors:
  - [abstract] "Consequently, we leverage our framework to provide fundamental insights into the role of graph structure and its connectivity on the worst-case robustness behavior of convolution-based and PageRank-based GNNs."
  - [section] "Crucial in reformulating P2(Q) into a single-level problem are the Karush–Kuhn–Tucker (KKT) conditions of the lower-level problem P1(Q)."
- Break condition: If the adversary can make unbounded perturbations, or if the bounds on kernel elements become too loose, the MILP relaxation may become ineffective.

### Mechanism 3
- Claim: Leveraging graph structure through NTKs provides significantly better certified accuracy compared to MLPs for the same perturbation budgets.
- Mechanism: The NTK of GNNs incorporates graph connectivity information, which improves the model's ability to maintain predictions under feature perturbations. This is demonstrated experimentally where GNNs consistently outperform MLPs in certified accuracy across various settings.
- Core assumption: The graph structure provides meaningful information that helps distinguish between clean and poisoned data, and this information is effectively captured by the NTK.
- Evidence anchors:
  - [abstract] "We focus on semi-supervised node classification in graphs, where certifying against node feature perturbations is particularly challenging due to the interconnectivity between nodes."
  - [section] "All GNNs have significantly better worst-case robustness behavior than the certified accuracy of an MLP. Thus, leveraging the graph connectivity, significantly improves their certified accuracy, even when faced with perturbations on all labeled nodes."
- Break condition: If the graph structure is not informative for the classification task, or if the NTK fails to capture the relevant graph properties.

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) and its role in characterizing training dynamics of wide neural networks
  - Why needed here: The NTK provides a way to model the complex training dynamics of GNNs without explicitly training them, enabling the certification framework to reason about worst-case robustness
  - Quick check question: What is the key property of the NTK that makes it useful for certification, and how does this relate to the width of the neural network?

- Concept: Bilevel optimization and its reformulation using KKT conditions
  - Why needed here: Poisoning attacks can be modeled as bilevel optimization problems, and QPCert needs to solve these to determine if predictions are robust to adversarial training data
  - Quick check question: How do the KKT conditions help transform a bilevel problem into a single-level problem, and what assumptions are needed for this transformation to be valid?

- Concept: Mixed-Integer Linear Programming (MILP) and linearization techniques
  - Why needed here: The reformulated certification problem contains bilinear terms that need to be linearized to create a tractable optimization problem
  - Quick check question: What linearization techniques are used in QPCert to handle the bilinear terms in the certification problem, and how do the bounds on kernel elements play a role in this linearization?

## Architecture Onboarding

- Component map:
  - NTK Computation -> Bound Derivation -> MILP Formulation -> Solver Interface -> Certification Engine

- Critical path:
  1. Compute NTK for the GNN architecture
  2. Derive bounds on kernel elements based on ℓp perturbation model
  3. Formulate MILP using KKT conditions and linearization
  4. Solve MILP using optimization solver
  5. Check if objective value > 0 to certify robustness

- Design tradeoffs:
  - NTK width vs. computational tractability: Wider networks provide better NTK approximation but may require more computation
  - Bound tightness vs. MILP size: Tighter bounds lead to more accurate certificates but larger MILPs
  - Perturbation model choice: Different ℓp norms lead to different bound derivations and computational complexity

- Failure signatures:
  - MILP solver fails to converge: May indicate overly loose bounds or insufficient computational resources
  - Certificate returns zero accuracy: Could mean the perturbation budget is too large or the graph structure is not informative
  - High variance in certification results: May indicate sensitivity to initialization or solver numerical issues

- First 3 experiments:
  1. Implement NTK computation for GCN and verify it matches the analytical form from the literature
  2. Implement bound derivation for ℓ2 perturbations and test on a small synthetic graph
  3. Set up the full MILP formulation for a simple binary classification problem and verify it can certify robustness for small perturbation budgets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can QPCert be extended to certify graph structure perturbations rather than just node features?
- Basis in paper: [explicit] The paper discusses the challenges of certifying against structure perturbations in Appendix O.2, noting that current bounding strategies lead to loose bounds for structure perturbations.
- Why unresolved: Structure perturbations require optimizing over the graph structure matrix, introducing multilinear and bilinear constraints that are computationally challenging to solve and may not scale to practical dataset sizes.
- What evidence would resolve it: A successful implementation and demonstration of QPCert applied to certifying structure perturbations on real-world graph datasets, showing non-trivial certificates and reasonable computational efficiency.

### Open Question 2
- Question: What is the impact of training optimization procedures (e.g., momentum-based or second-order methods) on the NTK equivalence and thus the validity of QPCert certificates?
- Basis in paper: [explicit] Appendix O.4 notes that the NTK equivalence becomes difficult to establish when the training optimization is changed to momentum-based or second-order gradient descent, or adversarial training.
- Why unresolved: The bounded parameter evolution required for NTK equivalence, as discussed in Appendix D, may not hold under these optimization procedures, potentially invalidating the theoretical guarantees of QPCert.
- What evidence would resolve it: Empirical and theoretical analysis showing whether and under what conditions the NTK equivalence holds for different optimization procedures, and how this affects the tightness and validity of QPCert certificates.

### Open Question 3
- Question: How can the tightness of QPCert be improved for larger perturbation budgets?
- Basis in paper: [explicit] Section 5 discusses that while theoretically the NTK bounds are tight, there is still a gap between empirical and provable robustness for larger perturbations, potentially due to the relaxation introduced by deriving element-wise bounds on the kernel.
- Why unresolved: The current approach of deriving element-wise bounds on the kernel to model the adversary leads to a relaxation of the bilevel problem, resulting in a gap between provable and empirical robustness for larger perturbations.
- What evidence would resolve it: Development and demonstration of alternative strategies for deriving kernel bounds that lead to tighter relaxations of the bilevel problem, resulting in smaller gaps between provable and empirical robustness for larger perturbation budgets.

## Limitations

- Practical scalability: The MILP formulation can become computationally intensive for large graphs with many labeled nodes, potentially limiting practical applicability to very large-scale networks
- Perturbation model assumptions: The framework assumes ℓp-bounded feature perturbations, which may not capture all realistic attack scenarios, particularly those involving feature synthesis or structural graph modifications
- Numerical stability: The MILP formulation relies on big-M constraints with kernel bounds, which may lead to numerical issues for large perturbation budgets or poorly conditioned kernel matrices

## Confidence

- Mechanism 1 (NTK-based certification): High confidence
- Mechanism 2 (MILP tractability): Medium confidence
- Mechanism 3 (Graph structure benefits): High confidence

## Next Checks

1. **Bound sensitivity analysis**: Systematically vary the perturbation budget δ and measure how kernel bound tightness affects MILP solving time and certified accuracy to identify practical limits of the framework

2. **Cross-dataset generalization**: Test QPCert on additional graph datasets (e.g., PubMed, Amazon) with different characteristics (size, sparsity, feature dimensionality) to evaluate robustness across diverse graph structures

3. **Solver parameter optimization**: Conduct a systematic study of Gurobi solver parameters (time limits, node limits, cut strategies) to identify optimal settings for different graph sizes and perturbation regimes