---
ver: rpa2
title: A Measure of the System Dependence of Automated Metrics
arxiv_id: '2412.03152'
source_url: https://arxiv.org/abs/2412.03152
tags:
- metric
- human
- ratings
- system
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses a critical gap in the evaluation of automated\
  \ metrics for machine translation, where high correlation with human judgments does\
  \ not necessarily guarantee fair treatment of all translation systems. The authors\
  \ introduce a method to quantify how much an automated metric\u2019s performance\
  \ depends on the system under evaluation, which can lead to inconsistent rankings."
---

# A Measure of the System Dependence of Automated Metrics

## Quick Facts
- **arXiv ID**: 2412.03152
- **Source URL**: https://arxiv.org/abs/2412.03152
- **Reference count**: 40
- **Primary result**: Introduces SysDep metric showing even top MT metrics like XCOMET exhibit high system dependence, causing inconsistent rankings

## Executive Summary
This paper addresses a critical gap in automated MT evaluation: high correlation with human judgments doesn't guarantee fair treatment of all translation systems. The authors introduce a method to quantify how much an automated metric's performance depends on the system being evaluated, which can lead to inconsistent rankings. Using Isotonic Regression and bootstrap sampling, they estimate system-specific and global mappings between metric and human ratings. Experiments on WMT23 data reveal that even the best-performing metrics exhibit high system dependence, causing significant underestimation or overestimation of individual system performances and resulting in incorrect system rankings.

## Method Summary
The authors introduce a system dependence score (SysDep) that measures how much an automated metric's behavior varies across different translation systems. They use Isotonic Regression with bootstrap sampling (B=200) to estimate both system-specific mapping functions (fk) and a global mapping function (fG) between metric scores and human ratings. The Expected Deviation (ED) for each system quantifies the gap between global remapped ratings and system-specific expected human ratings. SysDep is then calculated as the range of ED values across all systems, capturing the worst-case discrepancy in how the metric maps to human judgments across different systems.

## Key Results
- XCOMET, despite high correlation (0.65) with human judgments, produces inconsistent rankings due to system-dependent mapping behavior
- SysDep successfully identifies metrics that appear strong at segment level but fail at fair system-level evaluation
- The worst-case ranking error occurs when systems have different signs of ED and large rating gaps between them
- Reference-based metrics show different system dependence patterns compared to reference-free metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High global segment-level correlation does not guarantee fair system-level ranking because metric scores are system-dependent.
- Mechanism: Automated metrics map input-output pairs to scalar quality scores. When these mappings are averaged system-wise, the implicit assumption is that the same global mapping function applies across all systems. If instead, each system has its own mapping function (fk ≠ fG), averaging metric scores can distort relative rankings.
- Core assumption: The conditional expectation E[human rating | metric score] is not constant across systems.
- Evidence anchors:
  - [abstract] The authors show that XCOMET, despite high correlation (0.65), produces inconsistent rankings because its mapping to human scores varies across systems.
  - [section] The derivation in Section 2 shows that E[hk] = Epk(m)[Epk(h)[h|m]], leading to system-specific functions fk.
  - [corpus] Weak: Only 1 citation in corpus, no direct metric dependence studies found.
- Break condition: If fk = fG for all k (i.e., metric behavior is identical across systems), then SysDep would be zero and no ranking distortion occurs.

### Mechanism 2
- Claim: System dependence score (SysDep) quantifies the worst-case ranking error by measuring the range of expected deviations.
- Mechanism: For each system, the Expected Deviation (ED) measures the gap between the global remapped rating and the system-specific expected human rating. SysDep is the difference between the maximum and minimum ED, capturing the largest possible misalignment.
- Core assumption: Ranking errors arise from the interplay between rating gaps and ED magnitudes.
- Evidence anchors:
  - [abstract] "SysDep(M) = maxπk ED(k) − minπk ED(k)" explicitly defines the measure.
  - [section] "Ranking errors reflect an interplay between the systems' rating gap and the EDs" explains the intuition.
  - [corpus] Weak: No corpus evidence on using ED ranges for fairness; inference is purely from the paper's derivation.
- Break condition: If all EDs are identical, SysDep is zero and rankings are preserved; if ED signs are uniform (all positive or all negative), no inversion occurs even if absolute EDs are large.

### Mechanism 3
- Claim: Isotonic regression with bootstrapping provides robust estimates of the system-specific and global conditional expectations.
- Mechanism: By pooling all paired data, the global function fG is estimated. System-specific functions fk are estimated from each system's data alone. Bootstrapping (B=200 samples) yields confidence intervals and stabilizes the estimates.
- Core assumption: Isotonic regression captures the monotonic relationship between metric and human scores without overfitting.
- Evidence anchors:
  - [section] "To estimate the conditional expectation functions fk, we use the 1177 paired ratings for each system πk. We employ B = 200 bootstrap samples of the paired data to fit B IR models."
  - [section] The code snippet shows the use of sklearn.isotonic.IsotonicRegression with monotonicity constraints.
  - [corpus] Weak: Only one corpus neighbor mentions "Machine Translation Evaluation," no direct isotonic regression usage found.
- Break condition: If the true relationship is non-monotonic or highly variable, isotonic regression may underfit; if sample size per system is very small, bootstrapping cannot overcome high variance.

## Foundational Learning

- Concept: Monotonic function estimation
  - Why needed here: The core assumption is that metric scores and human ratings are monotonically related; isotonic regression enforces this constraint.
  - Quick check question: Why does isotonic regression not allow the estimated function to decrease as metric scores increase?

- Concept: Conditional expectation in evaluation
  - Why needed here: The paper derives that the expected human rating for a system can be expressed as an expectation over metric ratings, leading to system-specific mappings.
  - Quick check question: In the equation E[hk] = Epk(m)[Epk(h)[h|m]], what does the inner expectation represent?

- Concept: Bootstrap sampling for confidence estimation
  - Why needed here: Bootstrapping provides confidence intervals for the estimated conditional expectations, allowing uncertainty quantification in SysDep.
  - Quick check question: How does increasing the number of bootstrap samples (B) affect the stability of the SysDep estimate?

## Architecture Onboarding

- Component map: Data ingestion -> Isotonic Regression fits -> Expected deviation calculation -> SysDep aggregation -> System ranking comparison
- Critical path: Data → Bootstrap IR fits → Expected deviations → SysDep → System ranking comparison
- Design tradeoffs:
  - Using only segment-level data limits cross-system inference; pooling all data for fG may smooth over real system differences
  - Isotonic regression is simple but may miss non-monotonic nuances
  - Bootstrapping adds robustness but increases compute time
- Failure signatures:
  - SysDep ≈ 0 but ranking mismatch: Likely due to extrapolation beyond observed metric range
  - Very high SysDep: Metric behaves radically differently across systems; may indicate unstable or biased metric
  - Large confidence intervals on fk: Insufficient paired data per system
- First 3 experiments:
  1. Compute SysDep for a baseline random metric (Random-sysname) to confirm worst-case detection
  2. Vary bootstrap sample size B to assess stability of SysDep estimates
  3. Compare SysDep for reference-based vs. reference-free metrics to see if input signal type affects system dependence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can automated metrics be designed to minimize system dependence (SysDep) scores?
- Basis in paper: [explicit] The paper acknowledges that while a method to measure SysDep is provided, it does not offer suggestions for developing metrics that minimize it.
- Why unresolved: The paper focuses on introducing the concept of SysDep and demonstrating its importance but does not explore strategies or design principles for reducing system dependence in automated metrics.
- What evidence would resolve it: Empirical studies comparing SysDep scores across different metric architectures or training approaches, along with analysis of which design choices lead to lower system dependence.

### Open Question 2
- Question: Does the observed system dependence vary across different domains or datasets beyond WMT23?
- Basis in paper: [inferred] The paper explicitly states that experiments are based solely on WMT23 data and that a larger-scale study with more domains and larger sample sizes is needed to solidify findings.
- Why unresolved: The current analysis is limited to a single dataset, and the generalizability of the findings to other domains or evaluation settings is unknown.
- What evidence would resolve it: Replicating the SysDep analysis on diverse datasets, including different language pairs, domains (e.g., technical, conversational), and evaluation conditions, to assess consistency and variability.

### Open Question 3
- Question: How does the bootstrap sampling method impact the reliability and stability of SysDep estimates?
- Basis in paper: [inferred] The paper uses bootstrap sampling to estimate conditional expectation functions and compute confidence intervals, but does not analyze the sensitivity of SysDep scores to the number of bootstrap samples or sampling strategy.
- Why unresolved: The choice of bootstrap parameters (e.g., number of samples) and their impact on the precision of SysDep estimates are not explored, leaving uncertainty about the robustness of the metric.
- What evidence would resolve it: Systematic experiments varying bootstrap sample sizes and comparing resulting SysDep scores to assess sensitivity and identify optimal parameters for reliable estimation.

## Limitations

- Limited scope of analysis: The study focuses on WMT23 data for only 3 language pairs, which may not generalize to other domains or languages
- Model assumptions: Isotonic regression assumes monotonic relationships between metric and human scores, potentially missing non-monotonic patterns in metric behavior across systems
- The paper doesn't address how different numbers of systems or varying quality distributions might affect SysDep calculations

## Confidence

**High confidence**: The mathematical derivation of system dependence and the mechanism linking global vs. system-specific mappings to ranking errors is sound and well-established.

**Medium confidence**: The empirical demonstration using WMT23 data is convincing but limited in scope. The choice of 200 bootstrap samples and the interpretation of SysDep thresholds could benefit from sensitivity analysis.

**Low confidence**: Claims about cross-metric comparisons and the general superiority of certain metric families (reference-based vs. reference-free) lack sufficient statistical validation across diverse datasets.

## Next Checks

1. **Dataset expansion**: Apply the SysDep methodology to additional WMT datasets (previous years) and non-WMT translation tasks to test generalizability across different domains and language pairs.

2. **Bootstrap sensitivity**: Systematically vary the number of bootstrap samples (B = 50, 100, 200, 500) and measure the stability of SysDep estimates, particularly for metrics with high dependence scores.

3. **Alternative mapping functions**: Replace isotonic regression with other monotonic function estimators (e.g., isotonic neural networks or spline regression) to verify that the observed system dependence isn't an artifact of the chosen estimation method.