---
ver: rpa2
title: Real or Robotic? Assessing Whether LLMs Accurately Simulate Qualities of Human
  Responses in Dialogue
arxiv_id: '2409.08330'
source_url: https://arxiv.org/abs/2409.08330
tags:
- human
- prompt
- simulator
- turn
- conversation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluates whether large language models (LLMs) can accurately
  simulate human responses in dialogue, finding low alignment between LLM-generated
  and human responses across 21 linguistic metrics. Models performed better in semantic
  similarity but struggled with syntactic, stylistic, and conversational dynamics.
---

# Real or Robotic? Assessing Whether LLMs Accurately Simulate Qualities of Human Responses in Dialogue

## Quick Facts
- **arXiv ID**: 2409.08330
- **Source URL**: https://arxiv.org/abs/2409.08330
- **Reference count**: 34
- **Primary result**: LLMs show low alignment with human responses across 21 linguistic metrics, with prompt engineering having more impact than model choice

## Executive Summary
This study evaluates whether large language models can accurately simulate human responses in dialogue by comparing LLM-generated responses to human responses across 21 linguistic metrics. The research finds that while models achieve moderate semantic similarity, they struggle significantly with syntactic, stylistic, and conversational dynamics. Prompt engineering proves more influential than model selection in determining output quality, and performance varies across languages and domains. The study reveals that LLMs best mirror human dialogue when human input already resembles typical LLM-generated text, highlighting fundamental limitations in capturing diverse human speech styles.

## Method Summary
The study uses the WildChat dataset containing 1 million human-LLM dialogues, sampling 100,000 conversations for analysis. Researchers generated Turn 3 responses using 9 different LLM models (including Llama3.1, Mistral, and Qwen2 variants) and 50 prompts across three strategies (Chain-of-Thought, Override, and Direct). They then computed 21 linguistic metrics comparing human and simulated responses, calculating correlations within four categories: lexical, syntactic, semantic, and style. Regression analysis identified factors influencing simulation quality, including conversation topics, human linguistic properties, and regional variations.

## Key Results
- LLMs show low alignment with human responses across 21 linguistic metrics
- Prompt engineering has more impact than model choice on simulation quality
- LLMs perform better in consistent-style contexts like storytelling than technical domains
- Performance is similar across English, Chinese, and Russian, though less robust in languages with less training data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLMs mirror human dialogue better when human input already resembles LLM-generated text.
- **Mechanism**: The SIMULATOR adapts to linguistic properties (politeness, verbosity, typos) present in HUMAN Turn 1, producing Turn 3 responses that match those properties.
- **Core assumption**: SIMULATORs have stable internal priors for stylistic features like politeness and lack of typos.
- **Evidence anchors**: [abstract] "LLMs best mirror humans when human input resembles typical LLM-generated text"; [section] "when HUMAN Turn 1 expresses higher politeness, the Turn 3 simulation is predictably more similar".
- **Break condition**: If human input deviates strongly from LLM-style norms (e.g., highly informal, typo-rich, or low politeness), SIMULATOR adaptation fails.

### Mechanism 2
- **Claim**: Prompt engineering has greater impact on SIMULATOR output similarity than model selection.
- **Mechanism**: Different prompts alter the SIMULATOR's internal instruction-following behavior, affecting lexical and syntactic similarity more than semantic similarity.
- **Core assumption**: Prompt wording can override or reshape model-level tendencies in surface-level text features.
- **Evidence anchors**: [abstract] "Prompt engineering had more impact than model choice"; [section] "prompts significantly affect the similarity across all categories... the choice of SIMULATOR has only a minor impact".
- **Break condition**: If prompt structure is neutral or generic, model-level stylistic biases dominate output.

### Mechanism 3
- **Claim**: LLMs perform better in simulating dialogue in consistent-style contexts (e.g., storytelling) than in technical domains.
- **Mechanism**: Consistent style and content throughout a dialogue (e.g., continuation of a story) allows SIMULATORs to apply stable priors without needing to adapt to domain-specific knowledge.
- **Core assumption**: SIMULATORs rely more on stylistic continuity than domain-specific accuracy when generating responses.
- **Evidence anchors**: [abstract] "LLMs were most effective in consistent-style contexts like storytelling but poorly adapted to technical domains"; [section] "story topics... make simulation easier... technical topic... predicts a lower similarity score".
- **Break condition**: If technical domains require precise factual accuracy or specialized reasoning, SIMULATOR performance degrades regardless of style.

## Foundational Learning

- **Concept**: Correlation vs. causation in model-human similarity metrics
  - **Why needed here**: The study uses correlation to measure text similarity, but must interpret whether high correlation implies genuine human-like behavior or just surface similarity.
  - **Quick check question**: If two texts have high lexical correlation but different meanings, what does that say about the validity of the correlation metric?

- **Concept**: Prompt engineering and instruction-following
  - **Why needed here**: The study finds prompt choice more impactful than model choice, so understanding how LLMs interpret instructions is critical.
  - **Quick check question**: How might a "jailbreak" style prompt change an LLM's response compared to a "direct" instruction prompt?

- **Concept**: Multilingual model performance and training data bias
  - **Why needed here**: The study compares English, Chinese, and Russian performance, finding some differences due to training data volume.
  - **Quick check question**: If a language has less training data, what specific types of errors or biases might appear in SIMULATOR outputs?

## Architecture Onboarding

- **Component map**: WildChat corpus -> Prompt generation pipeline (50 prompts, 3 strategies) -> LLM inference layer (9 models, batch generation) -> Text similarity evaluation engine (21 metrics, 4 categories) -> Regression analysis module -> Annotation interface (POTATO tool)

- **Critical path**: 1. Sample Turn 1/Turn 2 from WildChat; 2. Apply prompt + model to generate Turn 3; 3. Compute similarity metrics between Turn 3 (HUMAN) and Turn 3 (SIMULATOR); 4. Aggregate and compare results across models/prompts; 5. Run regression to identify influencing factors

- **Design tradeoffs**: Breadth (21 metrics) vs. depth (fewer but more interpretable metrics); English focus (more robust metrics) vs. multilingual coverage (broader generalization); Automatic generation (scalable) vs. human annotation (higher fidelity)

- **Failure signatures**: Low similarity in syntactic metrics indicates inability to mimic grammatical complexity; Low F1 in conversation-ending prediction suggests misunderstanding of conversational intent; High variance across prompts signals prompt brittleness

- **First 3 experiments**: 1. Replicate the Turn 3 simulation task with a single model and prompt, compare lexical vs. semantic similarity; 2. Vary only the prompt type (COT, OVERRIDE, DIRECT) while keeping model fixed, measure impact on syntactic similarity; 3. Simulate Turn 3 for a subset of WildChat in Chinese, compare metric performance to English baseline

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Do fine-tuned LLMs outperform few-shot prompt engineering for simulating human responses?
- **Basis in paper**: [inferred] The paper notes that even the best prompt-engineering approaches show limited alignment with human responses, suggesting potential gains from model-specific fine-tuning.
- **Why unresolved**: The study focuses on prompt engineering across multiple models but does not explore fine-tuning or model-specific adaptations.
- **What evidence would resolve it**: Head-to-head comparison of fine-tuned models vs. prompt-engineered models on the same task and metrics.

### Open Question 2
- **Question**: How does conversation length affect LLM simulation accuracy?
- **Basis in paper**: [explicit] The authors explicitly suggest that simulating longer conversations might capture more nuanced intent than their three-turn setup.
- **Why unresolved**: The study restricts analysis to Turn 3 simulations, limiting insights into longer conversational dynamics.
- **What evidence would resolve it**: Experiments simulating multiple turns and measuring degradation in similarity metrics over conversation length.

### Open Question 3
- **Question**: What linguistic properties most strongly predict successful LLM simulations across different domains?
- **Basis in paper**: [inferred] The regression analysis shows that human Turn 1 properties have stronger effects than chatbot Turn 2, suggesting certain human linguistic traits are key predictors.
- **Why unresolved**: While the paper identifies that human style influences simulation success, it doesn't isolate which specific linguistic features matter most across different conversation topics.
- **What evidence would resolve it**: Detailed feature importance analysis across conversation domains, identifying which linguistic markers correlate most strongly with simulation success.

## Limitations
- Evaluation relies heavily on automated similarity metrics rather than human judgment
- Dataset focus on casual conversations limits generalizability to more structured interactions
- Prompt engineering approach may not represent typical real-world usage
- Cross-linguistic comparisons limited by varying training data availability

## Confidence

- **High confidence**: LLMs show measurable differences in simulating human responses across multiple linguistic dimensions; prompt engineering consistently affects output similarity more than model choice; LLMs perform better in consistent-style contexts versus technical domains
- **Medium confidence**: The relationship between human input style and LLM adaptation quality; the relative importance of different prompt types; cross-linguistic performance differences
- **Low confidence**: The claim that LLMs best mirror humans when input resembles typical LLM-generated text - this interpretation may conflate correlation with causation

## Next Checks
1. Conduct human evaluation studies comparing LLM-generated responses to human responses across different prompt types and domains, focusing on perceived naturalness and coherence rather than just automated similarity metrics
2. Test the "LLM-mimicry hypothesis" more directly by controlling for stylistic features in human input - generate human-like inputs with varying degrees of LLM-style characteristics and measure adaptation quality systematically
3. Evaluate model performance on more diverse conversational contexts including professional, technical, and cross-cultural dialogues to better understand generalization limits and identify specific failure modes in different domains