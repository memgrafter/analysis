---
ver: rpa2
title: Improving the Capabilities of Large Language Model Based Marketing Analytics
  Copilots With Semantic Search And Fine-Tuning
arxiv_id: '2404.13077'
source_url: https://arxiv.org/abs/2404.13077
tags:
- llms
- semantic
- gpt-4
- data
- marketing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates how semantic search, prompt engineering,
  and fine-tuning can improve LLM performance for domain-specific question-answering,
  SQL generation, and tabular analysis in marketing analytics. The authors show that
  semantic search dramatically improves GPT-4 accuracy on question-answering, while
  fine-tuning large open-source models like Llama-2-70b yields accuracy above 80%
  on SQL generation and above 90% on tabular analysis tasks.
---

# Improving the Capabilities of Large Language Model Based Marketing Analytics Copilots With Semantic Search And Fine-Tuning

## Quick Facts
- arXiv ID: 2404.13077
- Source URL: https://arxiv.org/abs/2404.13077
- Reference count: 0
- This paper demonstrates how semantic search, prompt engineering, and fine-tuning can improve LLM performance for domain-specific question-answering, SQL generation, and tabular analysis in marketing analytics.

## Executive Summary
This paper presents a comprehensive approach to enhancing LLM-based marketing analytics copilots through semantic search, prompt engineering, and fine-tuning. The authors demonstrate that semantic search dramatically improves GPT-4 accuracy on domain-specific question-answering by providing targeted context from curated marketing documents. Fine-tuning large open-source models like Llama-2-70b yields accuracy above 80% on SQL generation and above 90% on tabular analysis tasks. These improvements make LLMs more reliable for use in analytics tools, reducing the need for extensive human teams to understand complex AI model insights.

## Method Summary
The paper employs three main approaches: semantic search for question-answering, fine-tuning for SQL generation, and fine-tuning for tabular analysis. For question-answering, the authors use embedding models to retrieve relevant context from a knowledge base of 124 URLs with 1,264 document chunks. For SQL generation, they fine-tune open-source models like Llama-2-13b on the b-mc2/sql-create-context dataset with 78,577 examples. For tabular analysis, they generate 10,000 synthetic samples from real attribution model data and fine-tune models to explain attribution credit changes. Performance is evaluated using automated comparisons and GPT-4 evaluations across accuracy, relevance, and clarity metrics.

## Key Results
- Semantic search dramatically improves GPT-4 accuracy on domain-specific question-answering
- Fine-tuning Llama-2-70b yields accuracy above 80% on SQL generation tasks
- Fine-tuning large open-source models yields accuracy above 90% on tabular analysis tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic search dramatically improves GPT-4 accuracy on question-answering in marketing analytics contexts.
- Mechanism: By converting curated marketing documents into embedding vectors and retrieving context relevant to a query, semantic search provides GPT-4 with targeted information that grounds its responses in domain-specific knowledge rather than relying on general training data.
- Core assumption: The embedding model (e.g., text-embedding-ada-002) captures semantic similarity between queries and relevant document chunks accurately.
- Evidence anchors:
  - [abstract] "semantic search dramatically improves GPT-4 accuracy on question-answering"
  - [section] "A synergy between GPT-4 and semantic search bolstered the performance, with this combination achieving the highest scores across the board."
- Break condition: If the embedding model fails to capture domain-specific terminology or the knowledge base lacks relevant documents, semantic search provides no benefit.

### Mechanism 2
- Claim: Fine-tuning large open-source models like Llama-2-70b yields accuracy above 80% on SQL generation tasks.
- Mechanism: Fine-tuning updates the model weights on a dataset of natural language questions paired with SQL schemas and correct SQL queries, teaching the model the specific syntax and logic required for accurate SQL generation.
- Core assumption: The training dataset (b-mc2/sql-create-context) is sufficiently representative of the target domain's table structures and query patterns.
- Evidence anchors:
  - [abstract] "fine-tuning large open-source models like Llama-2-70b yields accuracy above 80% on SQL generation"
  - [section] "Their accuracy rates increased from under 30% to over 80%, emphasizing the profound impact of fine-tuning on enhancing the quality of SQL query generation."
- Break condition: If the target SQL schemas differ significantly from those in the training data, fine-tuning may not generalize well.

### Mechanism 3
- Claim: Fine-tuning large open-source models yields accuracy above 90% on tabular analysis tasks when trained on synthetic data mimicking real attribution credit change scenarios.
- Mechanism: Fine-tuning teaches the model to interpret tabular structures and numerical relationships, converting rows of marketing attribution data into natural language explanations of contributing and mitigating factors.
- Core assumption: Synthetic data generation preserves the statistical properties and logical relationships present in real attribution data.
- Evidence anchors:
  - [abstract] "fine-tuning large open-source models like Llama-2-70b yields accuracy above 90% on tabular analysis tasks"
  - [section] "Yet, post-fine-tuning, their performance drastically improved: Falcon-40b reached over 90% and Llama-2-13b over 80%"
- Break condition: If the synthetic data fails to capture rare but important edge cases in real attribution scenarios, model accuracy may degrade in production.

## Foundational Learning

- Concept: Embedding models and semantic search
  - Why needed here: To retrieve relevant context from a large knowledge base so LLMs can answer domain-specific questions accurately.
  - Quick check question: What is the difference between keyword-based search and semantic search, and why is the latter better for LLM question-answering?

- Concept: Fine-tuning vs. prompt engineering
  - Why needed here: Fine-tuning updates model weights for new tasks (SQL, tabular analysis), while prompt engineering only guides existing models without retraining.
  - Quick check question: When would you choose fine-tuning over prompt engineering for a new LLM task?

- Concept: SQL schema understanding and generation
  - Why needed here: LLMs must translate natural language questions into correct SQL queries that match the underlying database structure.
  - Quick check question: What challenges arise when generating SQL queries from natural language, and how does fine-tuning help?

## Architecture Onboarding

- Component map: Embedding model → vector database → retrieval → LLM routing → response generation
- Critical path: For question-answering: user query → semantic search retrieval → context augmentation → LLM response. For SQL generation: user query → schema context → fine-tuned LLM → SQL output. For tabular analysis: table input → fine-tuned LLM → natural language explanation.
- Design tradeoffs: Open-source vs. proprietary models (flexibility vs. out-of-the-box performance), fine-tuning vs. few-shot learning (accuracy vs. engineering effort), semantic search vs. direct prompting (relevance vs. simplicity)
- Failure signatures: Low accuracy on SQL/tabular tasks indicates insufficient fine-tuning data or poor synthetic data quality; irrelevant LLM responses suggest embedding model or knowledge base issues; UI lag may indicate inefficient LLM routing
- First 3 experiments:
  1. Test semantic search accuracy by querying a curated knowledge base and comparing GPT-4 outputs with and without retrieved context
  2. Fine-tune a small open-source model (e.g., Llama-2-13b) on the SQL dataset and measure accuracy improvement over the base model
  3. Generate synthetic tabular data and fine-tune a model to explain attribution credit changes, then validate accuracy against a held-out real dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does fine-tuning open-source LLMs like Llama-2-70b for SQL query generation and tabular analysis tasks compare to proprietary models like GPT-4 in terms of accuracy and cost-effectiveness?
- Basis in paper: [explicit] The paper discusses fine-tuning open-source LLMs and comparing their performance to GPT-4 on SQL query generation and tabular analysis tasks.
- Why unresolved: The paper provides initial results but suggests further research is needed to fully understand the long-term cost-effectiveness and scalability of using open-source models.
- What evidence would resolve it: A comprehensive cost-benefit analysis comparing the performance and operational costs of fine-tuned open-source models versus proprietary models over an extended period.

### Open Question 2
- Question: What are the optimal strategies for combining semantic search with LLMs to enhance domain-specific question-answering in marketing analytics tools?
- Basis in paper: [explicit] The paper highlights the effectiveness of semantic search in improving LLM performance for domain-specific question-answering but suggests further exploration is needed.
- Why unresolved: The paper does not provide detailed methodologies for optimizing the integration of semantic search with LLMs for various marketing analytics scenarios.
- What evidence would resolve it: Experimental studies testing different semantic search and LLM combinations across diverse marketing analytics contexts to identify best practices.

### Open Question 3
- Question: How can LLMs be effectively integrated with specialized agents, like pandas agent, to improve tabular data analysis and user interaction in marketing analytics tools?
- Basis in paper: [explicit] The paper demonstrates the use of pandas agent with GPT-4 for tabular data analysis and suggests further research on integrating LLMs with other specialized agents.
- Why unresolved: The paper does not explore the full potential of combining LLMs with various specialized agents for different types of tabular analysis tasks.
- What evidence would resolve it: Case studies and performance evaluations of LLM-agent integrations for a range of tabular analysis tasks in marketing analytics applications.

## Limitations
- Evaluation relies entirely on synthetic datasets for SQL generation and tabular analysis, with no validation on real-world production data
- Knowledge base for semantic search contains only 124 curated URLs, which may not capture the full complexity of marketing analytics queries
- Performance metrics are based on automated comparisons and GPT-4 evaluations rather than human judgment

## Confidence
- **High confidence** in the semantic search mechanism for question-answering, supported by clear empirical improvements and established embedding technology
- **Medium confidence** in SQL generation fine-tuning results, as the approach is standard but relies on synthetic data that may not generalize to all SQL schemas
- **Medium confidence** in tabular analysis claims, as the synthetic data generation process and evaluation methodology are not fully detailed

## Next Checks
1. Test the fine-tuned models on real marketing attribution SQL schemas and production tabular data to verify synthetic data generalization
2. Conduct human evaluation of SQL query correctness and natural language explanations to complement automated metrics
3. Compare the performance of different embedding models on a broader set of marketing analytics queries to identify optimal semantic search configurations