---
ver: rpa2
title: 'InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks'
arxiv_id: '2401.05507'
source_url: https://arxiv.org/abs/2401.05507
tags:
- data
- questions
- question
- code
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InfiAgent-DABench is the first benchmark for evaluating LLM-based
  agents on data analysis tasks, featuring 257 questions across 52 CSV files. The
  benchmark uses a format-prompting technique to convert open-ended questions into
  closed-form format for automatic evaluation.
---

# InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks

## Quick Facts
- arXiv ID: 2401.05507
- Source URL: https://arxiv.org/abs/2401.05507
- Reference count: 40
- Primary result: Introduces first benchmark for evaluating LLM-based agents on data analysis tasks with 257 questions across 52 CSV files

## Executive Summary
InfiAgent-DABench introduces the first comprehensive benchmark for evaluating large language model (LLM)-based agents on data analysis tasks. The benchmark addresses the challenge of automatically evaluating open-ended data analysis questions by introducing a format-prompting technique that converts questions into closed-form formats with unique answers. Through experiments on 34 state-of-the-art LLMs, the researchers demonstrate that current models face significant challenges in data analysis tasks, with GPT-4 achieving only 78.99% accuracy. The paper also introduces DAAgent, a specialized agent trained on the DAInstruct dataset that outperforms GPT-3.5 by 3.9% on these tasks.

## Method Summary
The method involves converting open-ended data analysis questions into closed-form formats using a format-prompting technique, enabling automatic evaluation through regular expression matching. The benchmark uses GPT-4 to generate constraints and format requirements that transform questions into specific "@answer name[answer]" formats. An agent framework built on ReAct (Reasoning and Acting) approach enables end-to-end problem solving through iterative reasoning, code generation, execution in a Python sandbox, and result observation. The researchers also develop DAInstruct, an instruction-tuning dataset constructed using automated methods, which is used to train specialized DAAgent models for improved performance on data analysis tasks.

## Key Results
- GPT-4 achieves 78.99% accuracy on the InfiAgent-DABench benchmark
- DAAgent outperforms GPT-3.5 by 3.9% on data analysis tasks
- Current state-of-the-art LLMs show significant room for improvement on data analysis tasks
- The format-prompting technique successfully enables automatic evaluation of 257 data analysis questions across 52 CSV files

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Format-prompting technique converts open-ended data analysis questions into closed-form formats for automatic evaluation
- Mechanism: The benchmark uses GPT-4 to generate constraints and format requirements that transform open-ended questions into specific "@answer name[answer]" formats with defined value ranges
- Core assumption: GPT-4 can reliably generate constraints and formats that make answers unique and easily parseable through regular expressions
- Evidence anchors:
  - [abstract] "we adopt a format-prompting technique to convert each question into a closed-form format so that they can be automatically evaluated"
  - [section 2.1] "Constraints mainly contain detailed strict on methods, workflow or statistical parameters used to answer the question. Format requirements are series of '@answer name[answer]'"
  - [corpus] Weak - related work focuses on web agents and multimodal evaluations rather than format conversion techniques
- Break condition: If GPT-4 generates ambiguous constraints or formats that allow multiple valid answers, the closed-form evaluation would fail

### Mechanism 2
- Claim: Agent framework with ReAct approach enables end-to-end problem solving through iterative reasoning and tool use
- Mechanism: The framework prompts LLMs to plan, write code, execute in Python sandbox, and conclude iteratively based on execution results
- Core assumption: LLMs can effectively decompose complex data analysis tasks into executable steps and handle errors through self-debugging
- Evidence anchors:
  - [abstract] "These tasks require agents to end-to-end solving complex tasks by interacting with an execution environment"
  - [section 2.2] "We build an agent framework for LLM-based agents to reason in a ReAct way, interact with files and invoke tools"
  - [corpus] Moderate - related work like BLADE benchmarks data-driven science agents, suggesting similar agent-based approaches exist
- Break condition: If LLMs fail to properly decompose tasks or cannot handle unexpected errors in the execution environment

### Mechanism 3
- Claim: DAInstruct instruction-tuning dataset improves specialized agent performance through task-specific fine-tuning
- Mechanism: GPT-4 generates data analysis questions and collects response trajectories, which are filtered and used to train DAAgent models
- Core assumption: Fine-tuning on curated data analysis tasks transfers to better performance on the benchmark compared to general instruction-tuning
- Evidence anchors:
  - [abstract] "we develop an automated method to construct an instruction-tuning dataset DAInstruct for data analysis tasks"
  - [section 2.5] "By training open-source LLMs on this dataset, we further develop DAAgent, specialized agents focused on data analysis"
  - [corpus] Moderate - related work on instruction-tuning for APIs suggests domain-specific fine-tuning can improve performance
- Break condition: If the instruction-tuning dataset doesn't capture the diversity of data analysis tasks or introduces bias that doesn't generalize

## Foundational Learning

- Concept: Regular expressions for answer parsing
  - Why needed here: The benchmark uses regex matching to extract answers from the "@answer name[answer]" format for automated evaluation
  - Quick check question: Given the format "@mean fare[34.65]", what regex pattern would extract the numerical value?

- Concept: ReAct (Reasoning and Acting) framework
  - Why needed here: The agent framework uses ReAct approach where models reason about what to do, take actions (like code execution), and observe results
  - Quick check question: What are the three main components of a ReAct loop in this context?

- Concept: Closed-form vs open-form evaluation
  - Why needed here: The benchmark converts open-ended questions to closed-form to enable automated evaluation without human judgment
  - Quick check question: Why is it harder to automatically evaluate open-ended questions like "What insights can you draw from this dataset?" compared to closed-form ones?

## Architecture Onboarding

- Component map:
  Dataset Construction Pipeline -> Agent Framework -> Evaluation System -> Instruction-Tuning Pipeline
  CSV file collection -> Question generation -> Constraints/format generation -> Label gathering
  Question/constraints input -> LLM reasoning -> Code generation -> Python sandbox execution -> Result observation -> Answer formatting
  Reformatted answer -> Regex parsing -> Label comparison -> Accuracy calculation
  Keyword generation -> Question generation -> Response collection -> Filtering -> Model fine-tuning

- Critical path:
  1. CSV file collection and validation
  2. Question generation with GPT-4 using key concepts
  3. Constraints and format requirements generation
  4. Agent framework implementation with ReAct loop
  5. Automatic evaluation through format parsing
  6. DAInstruct dataset construction for specialized agent training

- Design tradeoffs:
  - Closed-form evaluation vs flexibility: Closed-form enables automation but may miss nuanced answers
  - Format-prompting vs manual question design: Format-prompting scales but depends on GPT-4's reliability
  - Single-agent vs multi-agent frameworks: Single-agent is simpler but multi-agent might handle complex reasoning better
  - General vs specialized models: General models have broader knowledge but specialized ones perform better on specific tasks

- Failure signatures:
  - Poor format parsing accuracy indicating issues with constraint generation
  - Low execution success rate suggesting problems with code generation or environment setup
  - High variance in answers indicating ambiguous constraints or insufficient restrictions
  - Specialized agent underperformance suggesting ineffective instruction-tuning data

- First 3 experiments:
  1. Test format-prompting on 10 sample questions and verify if generated constraints produce unique, parseable answers
  2. Run agent framework on 5 simple data analysis tasks and measure success rate of the ReAct loop
  3. Compare performance of general instruction-tuned model vs DAInstruct-tuned model on 10 benchmark questions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can visualization tasks be effectively incorporated into the InfiAgent-DABench benchmark while maintaining the closed-form evaluation format?
- Basis in paper: [explicit] The paper explicitly identifies visualization as a significant limitation, noting that designing closed-form questions for visualization is challenging because outcomes are often pictorial rather than textual.
- Why unresolved: The authors acknowledge that neither evaluating code correctness nor using multimodal models provides a satisfactory solution for closed-form assessment of visualization tasks, leaving this as an open direction for future research.
- What evidence would resolve it: A validated methodology for converting visualization requirements into closed-form textual answers that can be automatically evaluated without compromising the quality of assessment.

### Open Question 2
- Question: What is the upper bound of performance for LLM-based agents on data analysis tasks, and how can we determine it?
- Basis in paper: [explicit] The paper notes that GPT-4 achieves 78.99% accuracy but mentions "the upper limit of performance in DAEval is 100%" without exploring what this theoretical maximum represents or how to measure it.
- Why unresolved: The paper benchmarks current models but doesn't establish a gold standard or human baseline for comparison, nor does it explore what constitutes perfect performance on these tasks.
- What evidence would resolve it: Human expert performance data on the same tasks, along with analysis of what factors limit LLM performance versus human capability.

### Open Question 3
- Question: How does the format-prompting technique for generating closed-form questions affect the diversity and real-world applicability of data analysis questions?
- Basis in paper: [inferred] The paper introduces format-prompting to convert open-ended questions into closed-form format, but doesn't analyze whether this transformation might constrain question complexity or limit the types of real-world scenarios that can be represented.
- Why unresolved: While the technique enables automatic evaluation, there's no analysis of potential trade-offs between evaluation convenience and question quality or realism.
- What evidence would resolve it: Comparative analysis of question diversity and complexity between format-prompted questions and naturally occurring data analysis questions from real-world scenarios.

## Limitations
- The benchmark's reliance on GPT-4 for question generation may introduce bias and limit generalizability
- The format-prompting technique may constrain question complexity and real-world applicability
- The evaluation methodology may not capture nuanced or multi-faceted answers that don't fit neatly into closed-form formats

## Confidence
- High confidence: The benchmark successfully demonstrates the feasibility of automated evaluation for data analysis tasks through format conversion
- Medium confidence: The 3.9% performance improvement of DAAgent over GPT-3.5 is valid but may not generalize to all data analysis scenarios
- Low confidence: The claim that this is the first benchmark for evaluating LLM-based agents on data analysis tasks, as similar agent-based evaluation frameworks may exist but aren't cited

## Next Checks
1. Test format-prompting technique with a different LLM (not GPT-4) to assess bias and generalizability of the closed-form conversion
2. Manually evaluate a subset of benchmark questions to verify if the automated format parsing captures all valid answers
3. Compare performance of specialized agents (DAAgent) against other instruction-tuned models on a held-out test set of data analysis tasks