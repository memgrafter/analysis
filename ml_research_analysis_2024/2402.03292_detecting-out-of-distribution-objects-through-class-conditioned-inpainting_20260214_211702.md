---
ver: rpa2
title: Detecting Out-of-Distribution Objects through Class-Conditioned Inpainting
arxiv_id: '2402.03292'
source_url: https://arxiv.org/abs/2402.03292
tags:
- detection
- object
- inpainting
- objects
- out-of-distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles zero-shot object-level out-of-distribution (OOD)
  detection, where an object detector trained on in-distribution data must flag novel
  objects without access to the training data. The authors propose RONIN, which leverages
  a pre-trained diffusion model (e.g., Stable Diffusion) to inpaint detected objects
  conditioned on the predicted class.
---

# Detecting Out-of-Distribution Objects through Class-Conditioned Inpainting

## Quick Facts
- arXiv ID: 2402.03292
- Source URL: https://arxiv.org/abs/2402.03292
- Reference count: 20
- Zero-shot object-level OOD detection via class-conditioned inpainting outperforms baselines with up to 23% relative improvement

## Executive Summary
This paper addresses the challenge of zero-shot object-level out-of-distribution (OOD) detection, where an object detector must flag novel objects without access to training data. The proposed method, RONIN, leverages a pre-trained diffusion model to inpaint detected objects conditioned on their predicted class labels. By comparing the original object with the inpainted version using a novel similarity metric, RONIN effectively distinguishes in-distribution objects from out-of-distribution ones. Extensive experiments on BDD-100k, PASCAL-VOC, MS-COCO, and OpenImages demonstrate RONIN's superior performance over existing baselines, with up to 23% relative improvement in AUROC and FPR@95 metrics.

## Method Summary
RONIN detects out-of-distribution objects by leveraging class-conditioned inpainting using a pre-trained diffusion model. For each detected object, it masks a centered region inside the bounding box and performs inpainting conditioned on the predicted class label. The method then computes a similarity score that combines the original object's similarity to the predicted class, the original-to-inpainted object similarity, and the inpainted-to-predicted class similarity. This score effectively separates in-distribution from out-of-distribution objects. To improve efficiency, RONIN employs a class-wise inpainting strategy that groups objects by predicted class and inpaints them together, significantly reducing computational cost without sacrificing performance.

## Key Results
- RONIN achieves up to 23% relative improvement in AUROC and FPR@95 compared to the best existing method
- Class-wise inpainting maintains comparable performance to object-wise inpainting while being more efficient
- RONIN is robust to variations in mask size and effectively handles both in-distribution and out-of-distribution objects

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RONIN leverages diffusion models to synthesize in-distribution objects conditioned on the predicted class, creating a realistic proxy for comparison.
- Mechanism: For each detected object, RONIN masks a centered region inside the bounding box and performs inpainting using a diffusion model conditioned on the predicted class label. This generates a context-aware inpainted object that closely resembles the original for in-distribution objects but deviates for out-of-distribution objects.
- Core assumption: Diffusion models trained on large-scale, diverse data can synthesize realistic in-distribution objects without fine-tuning, and the synthesized objects will be similar to actual in-distribution objects but dissimilar to out-of-distribution objects.
- Evidence anchors:
  - [abstract] "We utilize an off-the-shelf text-to-image generative model, such as Stable Diffusion, which is trained with objective functions distinct from those of discriminative object detectors."
  - [section] "RONIN leverages that outcome for inpainting the predicted objects through a strong text-to-image diffusion model. The ID objects are then inpainted similarly, while the OOD objects are inpainted differently."
  - [corpus] Weak evidence - no direct support for diffusion model effectiveness in OOD detection from corpus.
- Break condition: If the diffusion model fails to generate realistic objects or the synthesized objects do not align with the actual in-distribution distribution, the comparison becomes ineffective.

### Mechanism 2
- Claim: RONIN uses a similarity score that combines three relationships (original object to predicted class, original to inpainted object, inpainted to predicted class) to effectively distinguish in-distribution from out-of-distribution objects.
- Mechanism: The similarity score is calculated as the product of the similarity between the original object and the predicted class, and the similarity between the original and inpainted objects, normalized by the similarity between the inpainted object and the predicted class.
- Core assumption: The combination of these three similarity measurements provides a robust indicator of whether an object is in-distribution or out-of-distribution.
- Evidence anchors:
  - [abstract] "the reconstructed object is very close to the original in the ID cases and far in the OOD cases, allowing RONIN to effectively distinguish ID and OOD samples."
  - [section] "We propose to leverage it as a normalizing term with respect to the predicted class since some labels naturally have high or low similarity between xori and Ë†y."
  - [corpus] No direct support for the specific similarity score formulation from corpus.
- Break condition: If the normalization term (similarity between inpainted object and predicted class) is not effective, the similarity score may not accurately reflect the in-distribution status of the object.

### Mechanism 3
- Claim: RONIN employs a class-wise inpainting strategy that significantly improves efficiency without sacrificing performance.
- Mechanism: Instead of inpainting each object individually, RONIN groups objects by their predicted class and inpaints all objects of the same class together by taking the union of all masks. This reduces the number of inpainting operations needed.
- Core assumption: Class-wise inpainting maintains the same level of performance as object-wise inpainting while significantly reducing computational cost.
- Evidence anchors:
  - [section] "To this end, we develop a novel class-wise masking and inpainting approach that inpaints all objects in x that share the same predicted class together by taking the union of all masks."
  - [section] "As Table 2 shows, the two approaches generally have comparable performance... the class-wise strategy is more efficient, especially when the images contain more objects."
  - [corpus] No direct support for the class-wise inpainting strategy from corpus.
- Break condition: If the class-wise inpainting introduces artifacts or inconsistencies when inpainting multiple objects together, the performance may degrade.

## Foundational Learning

- Concept: Diffusion models and their application in image synthesis
  - Why needed here: RONIN relies on diffusion models to synthesize realistic in-distribution objects for comparison. Understanding how diffusion models work and their strengths/limitations is crucial for implementing and troubleshooting RONIN.
  - Quick check question: What is the key difference between diffusion models and other generative models like GANs or VAEs in terms of their training objective and output quality?

- Concept: Contrastive learning and its application in representation learning
  - Why needed here: RONIN uses pre-trained models like CLIP and SimCLRv2, which are based on contrastive learning, to compute similarity scores. Understanding contrastive learning helps in selecting appropriate models and interpreting the similarity scores.
  - Quick check question: How does contrastive learning encourage the model to learn meaningful representations that are useful for tasks like OOD detection?

- Concept: Out-of-distribution detection techniques and evaluation metrics
  - Why needed here: RONIN is an OOD detection method, and understanding the landscape of existing OOD detection techniques and their evaluation metrics is important for contextualizing RONIN's contributions and performance.
  - Quick check question: What are the key differences between zero-shot and non-zero-shot OOD detection, and why is the zero-shot setting particularly challenging for object-level OOD detection?

## Architecture Onboarding

- Component map:
  Object detector -> Diffusion model -> CLIP model -> CNN feature extractor -> Similarity score calculator

- Critical path:
  1. Object detector processes input image and outputs bounding boxes and predicted class labels.
  2. For each object, RONIN masks a centered region inside the bounding box and performs inpainting using the diffusion model conditioned on the predicted class.
  3. CLIP computes the similarity between the original object and the predicted class.
  4. CNN-based feature extractor computes the similarity between the original and inpainted objects.
  5. Similarity score calculator combines the three similarity measurements into a final score.
  6. OOD detection is performed based on the final similarity score.

- Design tradeoffs:
  - Object-wise vs. class-wise inpainting: Object-wise inpainting may provide slightly better performance but is computationally more expensive. Class-wise inpainting offers a good balance between performance and efficiency.
  - Rescaling ratio for masking: A higher rescaling ratio (e.g., 0.9) allows the diffusion model to generate more dissimilar inpaintings for out-of-distribution objects but may also introduce artifacts if the mask covers too much of the bounding box.
  - Choice of pre-trained models: The performance of RONIN depends on the quality and compatibility of the pre-trained models used (e.g., diffusion model, CLIP, CNN feature extractor). Careful selection and tuning may be required.

- Failure signatures:
  - Poor inpainting quality: If the diffusion model fails to generate realistic objects or introduces artifacts, the similarity scores may not accurately reflect the in-distribution status of the objects.
  - Inconsistent similarity scores: If the similarity measurements are not well-calibrated or the normalization term is not effective, the final similarity scores may not provide clear separation between in-distribution and out-of-distribution objects.
  - Computational bottlenecks: If the inpainting process or similarity computations are too slow, RONIN may not be suitable for real-time applications.

- First 3 experiments:
  1. Implement object-wise inpainting and evaluate its performance on a small dataset to verify the basic functionality of RONIN.
  2. Compare object-wise and class-wise inpainting on a larger dataset to assess the tradeoff between performance and efficiency.
  3. Evaluate the impact of different rescaling ratios for masking on the inpainting quality and OOD detection performance.

## Open Questions the Paper Calls Out

- How does RONIN's performance scale with the size and diversity of the training data used for the underlying diffusion model?
- Can RONIN be effectively adapted to detect OOD objects in video sequences, considering temporal consistency and object tracking?
- How does RONIN perform in detecting OOD objects when the object detector's predicted class label is incorrect for ID objects?

## Limitations

- RONIN may face challenges with highly diverse object classes or complex backgrounds, as the effectiveness of the class-wise inpainting strategy is not extensively tested across diverse datasets.
- The reliance on pre-trained diffusion models introduces potential limitations in handling novel object shapes or textures not well-represented in the training data.
- RONIN's performance may degrade when the object detector misclassifies in-distribution objects, as the inpainting and similarity assessment are based on the predicted class label.

## Confidence
- High: Effectiveness of the similarity score formulation and class-wise inpainting strategy
- Medium: Generalizability across diverse datasets
- Low: Robustness in handling edge cases like small objects or complex backgrounds

## Next Checks
1. Test RONIN on a dataset with a wider range of object classes and backgrounds to assess generalizability.
2. Evaluate the performance of RONIN on small objects and objects with complex backgrounds to identify potential failure modes.
3. Compare RONIN's performance with other OOD detection methods that do not rely on inpainting to understand the unique contributions of the approach.