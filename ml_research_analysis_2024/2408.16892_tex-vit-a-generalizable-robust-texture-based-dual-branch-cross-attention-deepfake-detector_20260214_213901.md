---
ver: rpa2
title: 'Tex-ViT: A Generalizable, Robust, Texture-based dual-branch cross-attention
  deepfake detector'
arxiv_id: '2408.16892'
source_url: https://arxiv.org/abs/2408.16892
tags:
- texture
- images
- image
- dataset
- various
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Tex-ViT, a generalizable and robust deepfake
  detector based on a dual-branch cross-attention vision transformer. It addresses
  the limitations of traditional CNNs, which struggle with generalization and adversarial
  robustness, and the data-hungry nature of pure vision transformers.
---

# Tex-ViT: A Generalizable, Robust, Texture-based dual-branch cross-attention deepfake detector

## Quick Facts
- arXiv ID: 2408.16892
- Source URL: https://arxiv.org/abs/2408.16892
- Reference count: 0
- Primary result: Achieves up to 98% accuracy in cross-domain deepfake detection while maintaining robustness to post-processing attacks

## Executive Summary
Tex-ViT addresses the limitations of traditional CNNs and pure vision transformers for deepfake detection by combining ResNet features with texture information extracted via Gram matrices. The model uses a dual-branch cross-attention vision transformer to fuse global texture features with local ResNet features, achieving superior generalization across different deepfake generation methods and robustness to post-processing attacks. With 43 million parameters, Tex-ViT outperforms state-of-the-art methods on FaceForensics++, various GAN datasets, and Celeb-DF while maintaining competitive performance under compression, blur, and noise attacks.

## Method Summary
Tex-ViT combines a ResNet-18 backbone with a texture module that extracts long-range texture correlations using Gram matrices at multiple scales before each ResNet downsampling operation. These features are fed into a dual-branch vision transformer with cross-attention, where the CLS token from one branch queries patch tokens from the other branch to enable bidirectional feature exchange. The model is trained for 100 epochs using Adam optimizer with data augmentation including RandAugment, CutMix, Mixup, random erasing, and drop path. Input faces are extracted at 128x128 resolution using RetinaFaceResNet50.

## Key Results
- Achieves up to 98% accuracy in cross-domain scenarios (training on one FF++ category, testing on others)
- Outperforms state-of-the-art methods on FaceForensics++, various GAN datasets, and Celeb-DF
- Maintains strong performance under post-processing attacks (blur, noise, compression)
- Shows 62-67% accuracy on face-swap manipulation in FF++ dataset

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Gram matrices capture long-range texture correlations that distinguish real from fake images
- **Mechanism**: Gram matrices compute inner products between feature maps at different layers, quantifying texture correlation patterns
- **Core assumption**: Fake images exhibit smoother textures with weaker long-range spatial correlations compared to real images
- **Evidence anchors**: [abstract] "Empirical analysis reveals that fake images exhibit smooth textures that do not remain consistent over long distances"; [corpus] Weak - no direct evidence of long-range correlation preservation in other models

### Mechanism 2
- **Claim**: Cross-attention between ResNet features and texture features enhances global-local feature fusion
- **Mechanism**: The CLS token from the texture branch queries patch tokens from the ResNet branch, creating bidirectional feature exchange
- **Core assumption**: Cross-branch attention creates richer feature representations than single-branch processing
- **Evidence anchors**: [abstract] "The texture module then serves as an input to the dual branch of the cross-attention vision transformer"; [corpus] Weak - no direct comparison of cross-attention vs single-branch performance in corpus

### Mechanism 3
- **Claim**: Multi-scale texture extraction at each ResNet downsampling stage captures texture at multiple semantic levels
- **Mechanism**: Texture blocks with Gram matrices are applied before each ResNet downsampling operation
- **Core assumption**: Texture artifacts manifest at multiple semantic scales, not just at one level
- **Evidence anchors**: [abstract] "The texture module then serves as an input to the dual branch of the cross-attention vision transformer"; [corpus] Weak - no evidence that other multi-scale texture approaches perform better

## Foundational Learning

- **Concept**: Gram matrices as texture descriptors
  - Why needed here: They capture texture correlation patterns that are invariant to specific deepfake generation methods but sensitive to smoothness artifacts
  - Quick check question: How do Gram matrices differ from direct feature maps in capturing texture information?

- **Concept**: Cross-attention mechanisms in vision transformers
  - Why needed here: They enable bidirectional feature exchange between texture and ResNet features, creating richer representations
  - Quick check question: What role does the CLS token play in cross-attention between branches?

- **Concept**: Multi-scale feature extraction
  - Why needed here: Texture artifacts appear at different semantic levels, requiring features at multiple scales
  - Quick check question: Why apply texture extraction before each downsampling operation rather than just at the input?

## Architecture Onboarding

- **Component map**: Input → ResNet-18 backbone (with texture blocks) → Texture features → Dual-branch Vision Transformer with cross-attention → Classification
- **Critical path**: Texture extraction → Cross-attention fusion → Classification head
- **Design tradeoffs**: Texture modules add computational overhead but improve generalization; dual-branch design increases parameter count but enables richer feature fusion; cross-attention adds complexity but captures global-local relationships
- **Failure signatures**: Poor cross-dataset performance → Texture module not capturing invariant features; sensitivity to post-processing → Cross-attention not robust to compression/noise; overfitting on single dataset → Model memorizing dataset-specific artifacts
- **First 3 experiments**:
  1. Remove texture module and test on cross-dataset scenarios to measure its impact on generalization
  2. Remove cross-attention and test dual-branch vs single-branch performance to validate its contribution
  3. Test with different Gram matrix layer configurations (before 1, 2, or all downsampling operations) to find optimal multi-scale configuration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Tex-ViT model be further optimized to improve its performance on face-swap manipulation in the FF++ dataset?
- Basis in paper: [explicit] The paper mentions that the model struggles with face-swap detection, achieving only 62-67% accuracy compared to other manipulation types
- Why unresolved: The paper acknowledges this limitation but does not provide specific strategies for improvement
- What evidence would resolve it: Experimental results showing improved accuracy on face-swap detection after implementing proposed optimizations

### Open Question 2
- Question: What specific modifications can be made to the texture module to enhance its robustness against compressed data samples?
- Basis in paper: [explicit] The paper notes that while the model performs better than state-of-the-art models in compressed scenarios, its score is still lower than desired
- Why unresolved: The paper identifies the need for improvement but does not detail specific approaches for enhancing texture module robustness against compression
- What evidence would resolve it: Comparative performance metrics showing improved accuracy on compressed datasets after implementing specific texture module modifications

### Open Question 3
- Question: How can the model be adapted to detect deepfakes in real-time applications while maintaining its high accuracy and robustness?
- Basis in paper: [inferred] The paper discusses the model's complexity and computational efficiency but does not address real-time application scenarios
- Why unresolved: Real-time detection requires balancing speed and accuracy, which is not explored in the paper
- What evidence would resolve it: Benchmarking results demonstrating the model's performance in real-time detection scenarios with acceptable accuracy and latency

## Limitations
- Performance on face-swap manipulation remains suboptimal at 62-67% accuracy compared to other manipulation types
- Limited quantitative validation of the core claim that Gram matrices capture long-range texture correlations invariant to deepfake methods
- No ablation studies to isolate the specific contributions of texture modules vs cross-attention vs dual-branch architecture

## Confidence
- **High Confidence**: Tex-ViT achieves state-of-the-art performance on FaceForensics++ and shows good generalization to cross-dataset scenarios
- **Medium Confidence**: The dual-branch architecture with cross-attention provides benefits over single-branch approaches
- **Low Confidence**: Gram matrices capture long-range texture correlations that are invariant to deepfake generation methods

## Next Checks
1. **Ablation Study on Texture Module**: Remove the Gram matrix texture blocks entirely and retrain Tex-ViT. Compare cross-dataset generalization performance to determine if the 98% accuracy is primarily driven by texture features or other architectural components.

2. **Cross-Attention Validation**: Implement a single-branch version of Tex-ViT with the same texture modules but without cross-attention. Test whether the dual-branch design provides statistically significant improvements in robustness to post-processing attacks.

3. **Texture Correlation Analysis**: Quantify the actual texture correlation differences between real and fake images using Gram matrix metrics. Measure whether these differences persist across different deepfake generation methods and whether they correlate with detection accuracy.