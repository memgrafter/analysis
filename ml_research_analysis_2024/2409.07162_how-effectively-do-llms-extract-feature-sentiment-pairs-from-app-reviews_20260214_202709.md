---
ver: rpa2
title: How Effectively Do LLMs Extract Feature-Sentiment Pairs from App Reviews?
arxiv_id: '2409.07162'
source_url: https://arxiv.org/abs/2409.07162
tags:
- sentiment
- features
- feature
- llms
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the effectiveness of large language models
  (LLMs) in extracting feature-sentiment pairs from app reviews. The research compares
  zero-shot and few-shot capabilities of GPT-4, ChatGPT, and Llama-2-chat variants
  against traditional rule-based and supervised methods.
---

# How Effectively Do LLMs Extract Feature-Sentiment Pairs from App Reviews?
## Quick Facts
- arXiv ID: 2409.07162
- Source URL: https://arxiv.org/abs/2409.07162
- Reference count: 38
- Primary result: GPT-4 achieves 23.6% higher F1-score than rule-based methods for feature extraction in zero-shot settings

## Executive Summary
This study evaluates large language models (LLMs) for extracting feature-sentiment pairs from app reviews, comparing zero-shot and few-shot capabilities of GPT-4, ChatGPT, and Llama-2-chat against traditional methods. The research demonstrates that GPT-4 significantly outperforms rule-based approaches, achieving a 23.6% improvement in F1-score for feature extraction in zero-shot settings, with further gains from few-shot learning. For sentiment prediction, GPT-4 attains an F1-score of 74% for positive sentiment in zero-shot settings, improving by 7% with five-shot examples.

The findings indicate that LLMs represent a promising, scalable alternative to traditional methods for generating feature-specific sentiment summaries from app reviews. The study highlights the effectiveness of few-shot learning in improving LLM performance and suggests that these models can effectively capture nuanced user feedback without extensive manual annotation.

## Method Summary
The study evaluates multiple LLMs (GPT-4, ChatGPT, and Llama-2-chat variants) using both zero-shot and few-shot learning approaches for feature-sentiment extraction from app reviews. The evaluation compares these methods against traditional rule-based and supervised approaches using F1-score metrics. The researchers test different shot counts (1, 3, and 5 examples) to assess the impact of few-shot learning on performance. The evaluation focuses on two main tasks: feature extraction and sentiment prediction, with particular attention to positive sentiment classification.

## Key Results
- GPT-4 achieves 23.6% higher F1-score than rule-based methods for feature extraction in zero-shot settings
- GPT-4 attains 74% F1-score for positive sentiment prediction in zero-shot settings, improving by 7% with five-shot examples
- Few-shot learning consistently improves LLM performance across both feature extraction and sentiment prediction tasks

## Why This Works (Mechanism)
The effectiveness of LLMs in feature-sentiment extraction stems from their ability to understand context and nuances in natural language through pre-training on diverse text corpora. The few-shot learning capability allows models to adapt to specific feature-sentiment extraction tasks without requiring extensive fine-tuning. The transformer architecture enables parallel processing of review text, capturing both local and global dependencies necessary for accurate feature and sentiment identification.

## Foundational Learning
- **Zero-shot learning**: The ability of models to perform tasks without specific training examples - needed to evaluate baseline LLM capabilities without adaptation
- **Few-shot learning**: Model adaptation using limited examples - critical for understanding performance improvements with minimal guidance
- **Feature-sentiment pair extraction**: Identifying specific product features and associated sentiment - core task being evaluated
- **F1-score metrics**: Harmonic mean of precision and recall - standard evaluation metric for information extraction tasks
- **Transformer architecture**: Self-attention mechanism for text processing - underlying technology enabling LLM effectiveness
- **App review analysis**: Processing user feedback for product improvement - practical application context

## Architecture Onboarding
**Component map**: User review text -> LLM processing -> Feature extraction -> Sentiment classification -> Output pairs

**Critical path**: Review input → Tokenization → Context understanding → Feature identification → Sentiment prediction → Pair generation

**Design tradeoffs**: 
- Zero-shot vs few-shot: Simplicity vs performance improvement
- Model size vs inference speed: Larger models perform better but require more resources
- Precision vs recall: Balancing false positives and false negatives in extraction

**Failure signatures**: 
- Feature misidentification due to ambiguous language
- Sentiment misclassification in context-dependent phrases
- Over-reliance on common phrases leading to missing novel features

**First experiments**:
1. Test zero-shot performance across different app categories
2. Evaluate impact of shot count (1, 3, 5) on specific feature types
3. Compare inference times between LLM and traditional methods

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on F1-score metrics without comprehensive precision-recall analysis
- Datasets limited to specific app review domains, potentially constraining generalizability
- Comparison with traditional methods lacks detailed discussion of computational costs and inference time differences

## Confidence
- **High confidence**: GPT-4's superior performance over rule-based methods for feature extraction
- **Medium confidence**: Effectiveness of few-shot learning improvements, as sample size and selection criteria are not fully detailed
- **Medium confidence**: Sentiment classification performance metrics, given potential variations in sentiment annotation guidelines

## Next Checks
1. Test the proposed method across diverse app categories and languages to assess generalizability
2. Conduct ablation studies to determine optimal number of shots for different feature types
3. Compare runtime efficiency and computational costs against traditional methods at scale