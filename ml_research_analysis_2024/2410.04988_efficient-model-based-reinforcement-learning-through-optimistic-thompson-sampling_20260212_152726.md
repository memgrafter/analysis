---
ver: rpa2
title: Efficient Model-Based Reinforcement Learning Through Optimistic Thompson Sampling
arxiv_id: '2410.04988'
source_url: https://arxiv.org/abs/2410.04988
tags:
- learning
- reward
- exploration
- optimistic
- hot-gp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HOT-GP, a model-based RL algorithm that performs
  optimistic exploration by leveraging joint uncertainty over state transitions and
  rewards using Gaussian Processes. HOT-GP addresses the challenge of efficient exploration
  in environments with sparse rewards, action penalties, and difficult-to-explore
  regions.
---

# Efficient Model-Based Reinforcement Learning Through Optimistic Thompson Sampling

## Quick Facts
- arXiv ID: 2410.04988
- Source URL: https://arxiv.org/abs/2410.04988
- Authors: Jasmine Bayrooti; Carl Henrik Ek; Amanda Prorok
- Reference count: 27
- Key outcome: HOT-GP achieves state-of-the-art sample efficiency on challenging exploration tasks by modeling joint uncertainty over state transitions and rewards using Gaussian Processes with Thompson sampling

## Executive Summary
This paper introduces HOT-GP, a model-based RL algorithm that performs optimistic exploration by leveraging joint uncertainty over state transitions and rewards using Gaussian Processes. The key innovation is a two-step Thompson sampling approach that first samples an optimistic reward from a truncated distribution and then selects the expected next state conditioned on this sampled reward. This ensures exploration of plausible states associated with high rewards while maintaining computational tractability through a linear model of coregionalization. Experiments on MuJoCo and VMAS tasks demonstrate that HOT-GP significantly improves sample efficiency compared to other model-based and model-free methods, particularly in challenging exploration scenarios with sparse rewards and action penalties.

## Method Summary
HOT-GP combines a Gaussian Process model with a neural network mean function to capture joint uncertainty over state transitions and rewards. The algorithm uses Thompson sampling with optimistic reward conditioning (r > rmin) to guide exploration, sampling from a truncated normal distribution over rewards and then computing the expected next state conditioned on this sampled reward. A linear model of coregionalization enables efficient computation while maintaining meaningful correlation between state and reward uncertainties. The method integrates with existing policy optimization algorithms (SAC for most tasks, DDPG for coverage) using model-based rollouts with 1-step lookaheads. The rmin parameter increases linearly throughout training to balance exploration and exploitation.

## Key Results
- HOT-GP achieves significantly higher sample efficiency than model-free (SAC) and other model-based methods on sparse reward tasks
- The algorithm shows particular strength in environments with difficult-to-explore regions and action penalties
- Performance is robust across different rmin values (0.1 to 0.7), with task-specific optimal values
- VMAS coverage task demonstrates HOT-GP's effectiveness in multi-modal reward scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HOT-GP's two-step sampling process ensures optimistic reward exploration while maintaining plausible state transitions.
- Mechanism: The algorithm first samples an optimistic reward from p(rt|st,at,rt>rmin) and then selects the expected state conditioned on this sampled reward. This ensures that only states associated with high rewards are explored, but these states remain plausible under the learned dynamics model.
- Core assumption: The correlation structure between predicted state and reward encoded in the joint GP model is meaningful for guiding exploration.
- Evidence anchors: [abstract] "the reward is sampled from an optimistic distribution and the next state is selected as the expected state conditioned on the sampled reward", [section 5.2] "we can condition the reward value rt to be high...and sample once from this distribution"
- Break condition: If the correlation between state and reward is weak or incorrectly modeled, the optimistic exploration may lead to exploring implausible states or miss genuinely rewarding regions.

### Mechanism 2
- Claim: The linear model of coregionalization enables efficient computation while maintaining meaningful joint uncertainty modeling.
- Mechanism: By restricting covariance between output dimensions to linear combinations of input covariance, HOT-GP achieves Kronecker structured covariance that allows for tractable computation while still capturing the relationship between state and reward uncertainties.
- Core assumption: The linear combination assumption for output covariance is sufficient to capture the joint uncertainty structure needed for effective optimistic exploration.
- Evidence anchors: [section 5.1] "we adopt a linear model of coregionalization...which allows for efficient factorization while still meaningfully directing optimistic exploration"
- Break condition: If the linear coregionalization assumption is too restrictive for the task, the model may fail to capture important non-linear correlations between state and reward uncertainties.

### Mechanism 3
- Claim: Thompson sampling over the joint reward-dynamics distribution provides principled balance between exploration and exploitation.
- Mechanism: By sampling from p(st+1,rt|st,at,rt>rmin), HOT-GP implicitly balances exploration of uncertain states with exploitation of states likely to yield high rewards, adapting this balance as the model becomes more certain.
- Core assumption: Thompson sampling maintains its theoretical guarantees when applied to the joint reward-dynamics distribution rather than just state transitions.
- Evidence anchors: [abstract] "we propose a practical, theoretically grounded approach to optimistic exploration based on Thompson sampling", [section 4.2] "Thompson sampling is a provably efficient exploration algorithm in RL"
- Break condition: If the posterior distribution becomes too confident too quickly or too slowly, Thompson sampling may fail to explore effectively or exploit sufficiently.

## Foundational Learning

- Concept: Gaussian Processes and their properties
  - Why needed here: HOT-GP relies on GP models to maintain joint uncertainty over state transitions and rewards
  - Quick check question: How does a GP differ from a neural network in representing uncertainty over functions?

- Concept: Thompson sampling and its theoretical properties
  - Why needed here: HOT-GP adapts Thompson sampling for optimistic exploration by sampling from joint reward-dynamics distributions
  - Quick check question: What makes Thompson sampling theoretically efficient compared to epsilon-greedy exploration?

- Concept: Coregionalization models and Kronecker structures
  - Why needed here: HOT-GP uses linear model of coregionalization to enable efficient joint modeling of state and reward uncertainties
  - Quick check question: Why is modeling the full covariance matrix across outputs computationally intractable for GP regression?

## Architecture Onboarding

- Component map:
  - GP reward-dynamics model (with neural network mean function) -> Thompson sampling acquisition function (with optimistic reward conditioning) -> Policy optimization module (using DDPG/SAC) -> Data collection and model update pipeline

- Critical path:
  1. Collect real environment transitions (st, at, st+1, rt)
  2. Update GP model with new data
  3. For each planning step: sample optimistic reward â†’ compute expected state
  4. Use model-generated rollouts to update policy
  5. Execute updated policy in environment

- Design tradeoffs:
  - GP vs ensemble of neural networks: GPs provide better uncertainty quantification but higher computational cost
  - Joint vs independent modeling: Joint modeling enables optimistic exploration but increases complexity
  - Two-step vs direct sampling: Two-step sampling ensures plausible states but adds computational overhead

- Failure signatures:
  - Poor learning performance: indicates model uncertainty is not informative or rmin schedule is inappropriate
  - High variance in rewards: suggests Thompson sampling is exploring too aggressively
  - Policy collapse: may indicate model bias is dominating exploration

- First 3 experiments:
  1. Implement GP model with neural network mean function on a simple 1D control task
  2. Add Thompson sampling acquisition with optimistic reward conditioning
  3. Integrate with existing SAC implementation and evaluate on Reacher task

## Open Questions the Paper Calls Out
- Question: How does the choice of reward percentile threshold r_min affect the exploration-exploitation trade-off in HOT-GP, and what principled methods exist for scheduling r_min throughout training?
- Question: Can the HOT-GP framework be extended to model longer-horizon dependencies between states and rewards, and what are the potential benefits and challenges of such an extension?
- Question: How does the performance of HOT-GP compare to other exploration strategies in real-world robotic scenarios with on-robot learning, and what are the practical challenges of implementing HOT-GP in such settings?

## Limitations
- The linear model of coregionalization may not capture complex non-linear correlations between state and reward uncertainties
- The method is demonstrated only with SAC as the base policy optimization algorithm, limiting generalizability to on-policy methods
- Evaluation focuses on continuous control tasks with specific exploration challenges, potentially limiting domain applicability

## Confidence
- High confidence: The computational efficiency gains of HOT-GP over standard model-based RL approaches are well-supported by the experimental results
- Medium confidence: The theoretical grounding of Thompson sampling for optimistic exploration in the joint reward-dynamics setting, though the extension from standard TS is not fully proven
- Low confidence: The generalizability of HOT-GP's performance benefits across diverse RL domains beyond the evaluated continuous control tasks

## Next Checks
1. Evaluate HOT-GP on discrete action space environments (e.g., Atari games) to test generalizability beyond continuous control tasks
2. Compare HOT-GP against ensemble-based uncertainty methods with similar computational budgets to isolate the benefits of GP-based uncertainty modeling
3. Conduct ablation studies removing the optimistic reward conditioning to quantify the specific contribution of this component to overall performance