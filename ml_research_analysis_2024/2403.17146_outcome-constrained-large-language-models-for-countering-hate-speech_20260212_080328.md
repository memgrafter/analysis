---
ver: rpa2
title: Outcome-Constrained Large Language Models for Countering Hate Speech
arxiv_id: '2403.17146'
source_url: https://arxiv.org/abs/2403.17146
tags:
- counterspeech
- hate
- conversation
- reentry
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates methods for generating counterspeech constrained
  by desired conversation outcomes, focusing on low conversation incivility and non-hateful
  hater reentry. The research experiments with instruction prompts, LLM finetuning,
  and reinforcement learning to incorporate outcome constraints into counterspeech
  generation.
---

# Outcome-Constrained Large Language Models for Countering Hate Speech

## Quick Facts
- arXiv ID: 2403.17146
- Source URL: https://arxiv.org/abs/2403.17146
- Authors: Lingzi Hong; Pengcheng Luo; Eduardo Blanco; Xiaoying Song
- Reference count: 18
- This study investigates methods for generating counterspeech constrained by desired conversation outcomes, focusing on low conversation incivility and non-hateful hater reentry.

## Executive Summary
This study investigates methods for generating counterspeech constrained by desired conversation outcomes, focusing on low conversation incivility and non-hateful hater reentry. The research experiments with instruction prompts, LLM finetuning, and reinforcement learning to incorporate outcome constraints into counterspeech generation. Results show that instruction prompts and reinforcement learning methods effectively generate counterspeech with higher probabilities of eliciting desired outcomes, while finetuning and RL produce more effective counterspeech based on human assessments. The generated texts consistently demonstrate high relevance to hate speech, though the wording differs from reference texts.

## Method Summary
The study collects hate speech/counterspeech pairs from Reddit and other sources, then trains conversation outcome classifiers to predict incivility levels and hater reentry behavior. Three generation methods are implemented: instruction prompts with explicit outcome constraints, LLM finetuning using pairs leading to desired outcomes, and reinforcement learning with outcome classifiers as reward functions. The RL approach uses PPO to optimize for both desired outcomes and text quality preservation. Generated counterspeech is evaluated using outcome classifiers, similarity metrics (METEOR, BERTScore), quality metrics (GRUEN), diversity metrics, and human assessment for suitability, relevance, and effectiveness.

## Key Results
- Instruction prompts and reinforcement learning methods effectively generate counterspeech with higher probabilities of eliciting desired outcomes
- Finetuning and RL produce more effective counterspeech based on human assessments
- Generated texts consistently demonstrate high relevance to hate speech, though wording differs from reference texts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reinforcement learning with LLM effectively generates counterspeech with desired outcomes.
- **Mechanism:** RL integrates conversation outcome classifiers as reward functions to guide text generation, maximizing the probability of desired outcomes while minimizing deviation from base model.
- **Core assumption:** The conversation outcome classifiers can reliably differentiate counterspeech that leads to better outcomes, even if their absolute accuracy is not perfect.
- **Evidence anchors:**
  - [abstract] Results show that instruction prompts and reinforcement learning methods effectively generate counterspeech with higher probabilities of eliciting desired outcomes.
  - [section] RL models generate more responses with desired outcomes than the baseline models and finetuning.
  - [corpus] Weak. No specific evidence in corpus that RL with outcome classifiers works, but related studies exist on RL for text generation with constraints.
- **Break condition:** If the conversation outcome classifiers are systematically biased or their predictions are uncorrelated with actual outcomes, the RL reward signal becomes misleading and the generated counterspeech will not achieve desired outcomes.

### Mechanism 2
- **Claim:** Instruction prompts with explicit outcome constraints steer LLMs to generate more outcome-aligned counterspeech.
- **Mechanism:** When given instructions to generate counterspeech that leads to specific conversation outcomes (e.g., low incivility), LLMs incorporate this information into the generation process, producing text more likely to achieve those outcomes.
- **Core assumption:** LLMs have learned representations of conversation outcomes from their pretraining that can be activated through explicit instructions.
- **Evidence anchors:**
  - [abstract] Instruction prompts effectively steer the generation of counterspeech towards the desired outcomes.
  - [section] Prompt queries with the constraint of low incivility can increase the probability of generating counterspeech with low conversation incivility.
  - [corpus] Weak. No direct evidence in corpus that instruction prompts with outcome constraints work, but related studies exist on instruction tuning for constrained text generation.
- **Break condition:** If the LLMs have not learned meaningful representations of conversation outcomes during pretraining, or if the instructions are too vague to activate these representations, the generated counterspeech will not be meaningfully different from baseline outputs.

### Mechanism 3
- **Claim:** LLM finetuning on outcome-aligned counterspeech data produces models that generate text with similar linguistic patterns and higher probability of desired outcomes.
- **Mechanism:** By finetuning on hate speech/counterspeech pairs that led to low conversation incivility or non-hateful reentry, the model learns to generate counterspeech with linguistic patterns associated with those outcomes.
- **Core assumption:** The linguistic patterns in counterspeech that led to desired outcomes in the training data will generalize to new hate speech comments.
- **Evidence anchors:**
  - [section] The finetuning process can tailor LLMs to learn the task of interest and guide the LLM in generating outcome-constrained counterspeech.
  - [section] The performance of finetuning methods in generating texts with expected outcomes is relatively inferior to others, suggesting some effectiveness but limited by the data quality.
  - [corpus] Weak. No specific evidence in corpus that finetuning on outcome-aligned data produces better outcome-constrained generation, but related studies exist on finetuning for constrained text generation.
- **Break condition:** If the linguistic patterns associated with desired outcomes are not generalizable or if the finetuning data contains noise or bias, the finetuned model may not generate outcome-aligned counterspeech for new hate speech.

## Foundational Learning

- **Concept:** Conversation outcome modeling
  - Why needed here: The study needs to quantify and predict conversation outcomes (incivility and hater reentry) to use as constraints for counterspeech generation and evaluation.
  - Quick check question: How does the study define and measure conversation incivility and hater reentry based on follow-up conversations?

- **Concept:** Text generation with constraints
  - Why needed here: The study explores methods to incorporate conversation outcome constraints into the text generation process, requiring understanding of how constraints can be integrated into LLM generation.
  - Quick check question: What are the three methods the study uses to incorporate outcome constraints into counterspeech generation?

- **Concept:** Evaluation of text generation quality
  - Why needed here: The study evaluates generated counterspeech using multiple metrics (desired outcomes, similarity to reference, stylistic quality, human assessment) to understand the strengths and weaknesses of different methods.
  - Quick check question: What are the four types of evaluation metrics used in the study to assess generated counterspeech?

## Architecture Onboarding

- **Component map:**
  Data processing pipeline -> Outcome classifiers -> Text generation models -> Evaluation pipeline

- **Critical path:**
  Collect and preprocess conversation data -> Train outcome classifiers on hate speech/counterspeech pairs -> Generate counterspeech using different methods (instruction prompts, finetuning, RL) -> Evaluate generated counterspeech using outcome classifiers and other metrics -> Compare results across methods to identify strengths and weaknesses

- **Design tradeoffs:**
  Model size vs. computational resources: Larger models may generate better counterspeech but require more resources
  Precision vs. recall in outcome classifiers: Higher precision may lead to more reliable reward signals but miss some outcome-aligned examples
  Diversity vs. constraint adherence: Generate-and-select approach increases diversity but may reduce constraint adherence

- **Failure signatures:**
  Outcome classifiers have low accuracy or systematic bias
  Generated counterspeech has low relevance to hate speech or is inappropriate
  Text generation methods fail to produce valid outputs or get stuck in loops
  Evaluation metrics show inconsistent results or poor correlation with human judgment

- **First 3 experiments:**
  1. Test baseline text generation without outcome constraints to establish performance floor
  2. Implement and test instruction prompts with outcome constraints on a small subset of data
  3. Train outcome classifiers on hate speech/counterspeech pairs and evaluate their performance on held-out data

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on a single language (English) and limited dataset sources constrains generalizability to other linguistic and cultural contexts
- Conversation outcome classifiers trained on Reddit data may not generalize well to other platforms with different discourse norms
- Reinforcement learning approach relies heavily on quality of outcome classifiers as reward functions

## Confidence

- **High Confidence:** The effectiveness of instruction prompts in steering counterspeech toward desired outcomes is well-supported by both classifier predictions and human evaluation results.
- **Medium Confidence:** The relative performance of finetuning versus reinforcement learning approaches shows consistent patterns but is affected by hyperparameter sensitivity and dataset quality issues.
- **Medium Confidence:** The claim that generated counterspeech maintains high relevance to hate speech is supported by human evaluation, though the linguistic differences from reference texts suggest potential style-transfer limitations.

## Next Checks

1. **Cross-platform validation:** Test the trained outcome classifiers and counterspeech generation methods on data from multiple social media platforms (Twitter, Facebook, Gab) to assess generalizability beyond Reddit.

2. **Longitudinal outcome tracking:** Implement follow-up conversation tracking to measure whether counterspeech effects persist beyond the immediate response, examining hater reentry and conversation trajectory over multiple turns.

3. **Ablation studies on reward structure:** Systematically vary the reward function weights in the RL approach to determine the optimal balance between outcome achievement and text quality preservation, including testing alternative reward formulations beyond classifier-based metrics.