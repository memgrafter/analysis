---
ver: rpa2
title: Reproducible Hybrid Time-Travel Retrieval in Evolving Corpora
arxiv_id: '2411.04051'
source_url: https://arxiv.org/abs/2411.04051
tags:
- retrieval
- document
- data
- documents
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of reproducing ranked search results
  in evolving document collections, which is critical for fields like systematic reviews
  and patent retrieval. Traditional ranked retrieval systems produce different results
  over time due to changes in global term statistics.
---

# Reproducible Hybrid Time-Travel Retrieval in Evolving Corpora

## Quick Facts
- arXiv ID: 2411.04051
- Source URL: https://arxiv.org/abs/2411.04051
- Reference count: 35
- The paper proposes a hybrid system combining Lucene with MonetDB column-store to enable reproducible ranked retrieval in evolving document collections

## Executive Summary
The paper addresses the critical problem of reproducing ranked search results in evolving document collections, which is essential for systematic reviews and patent retrieval where results must be verifiable over time. Traditional ranked retrieval systems produce different results as global term statistics change, making reproducibility impossible. The authors propose a hybrid architecture combining Lucene's fast inverted index for live queries with a versioned column-store-based retrieval system (MonetDB) that tracks historical term statistics and enables exact reproduction of ranked lists at any point in time. The system maintains document validity timestamps and computes term statistics at query time rather than precomputing them, achieving consistent Lucene performance (~80ms) while enabling reproducible time-travel queries through MonetDB.

## Method Summary
The authors propose a hybrid system that uses Lucene as the primary search engine for fast live queries while synchronizing with MonetDB, a column-store-based system that tracks versioned term statistics and document validity periods. Documents are processed in batches with preprocessing (tokenization, stemming, stopword removal) handled by Lucene. The MonetDB system extends the document table with valid_from and valid_to timestamps to track document changes over time, and implements a BM25 scoring variant that computes term and document statistics at query time. A query store manages persistent identifiers (PIDs) for each query, storing hashes of result sets to enable reproducibility verification and correction of rare floating-point discrepancies between Lucene and MonetDB scoring through document pair swapping.

## Key Results
- Lucene maintains consistent query performance (~80ms) while MonetDB query times increase from 2s to 20s as the collection grows
- Storage overhead is approximately double that of Lucene alone (4.05GB vs 3.82GB)
- The system successfully reproduces identical ranked lists in all but one case out of 10,400 queries
- Hash verification mechanism effectively corrects rare floating-point discrepancies between Lucene and MonetDB scoring

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid architecture maintains consistent Lucene performance while enabling reproducible results via MonetDB
- Mechanism: Lucene handles live queries with fast inverted index, while MonetDB stores versioned term statistics allowing exact BM25 recomputation at query time
- Core assumption: Term statistics change over time but query-time recomputation can exactly reproduce original rankings
- Evidence anchors:
  - [abstract] "Lucene maintains consistent query performance (~80ms) while MonetDB query times increase from 2s to 20s as the collection grows"
  - [section 3] "Our hybrid set-up relies on Lucene as primary search engine and MonetDB as secondary search engine to reproduce ranked lists on earlier states of the document corpus"
  - [corpus] Weak - only storage overhead mentioned (4.05GB vs 3.82GB)

### Mechanism 2
- Claim: Versioned column store allows time-travel queries by tracking document validity periods
- Mechanism: Documents table extended with valid_from and valid_to timestamps, enabling computation of term statistics as they existed at any point in time
- Core assumption: All document changes can be captured with start/end timestamps allowing exact reconstruction of historical states
- Evidence anchors:
  - [section 3.1] "To keep all historical states, we extend the document table in the columnstore implementation of the MonetDB index with two timestamps: valid_from and valid_to"
  - [section 4] "When such an identifier needs to be resolved and the according result list be recreated, the PID is used to retrieve the query from the query store and execute it against the MonetDB index using the execution timestamp as filter"
  - [corpus] Missing - no specific corpus evidence provided

### Mechanism 3
- Claim: Hash verification mechanism corrects rare floating-point discrepancies between Lucene and MonetDB scoring
- Mechanism: Each query stores hash of result set; upon reproduction, recomputed hash is compared and document pairs with nearly identical scores can be swapped if needed
- Core assumption: Floating-point differences are rare and only affect document ordering in cases of nearly identical scores
- Evidence anchors:
  - [section 4] "In extremely rare cases, this can help to correct a ranked list to account for numeric discrepancies between the Lucene and MonetDB score computation"
  - [section 5] "In rare cases (we observed 1 in 10,000) when two documents have virtually identical scores, these might be returned in swapped order"
  - [corpus] Weak - only mentions 1 discrepancy out of 10,400 queries

## Foundational Learning

- Concept: BM25 scoring formula
  - Why needed here: Both systems use BM25 but with different implementations requiring understanding of score components
  - Quick check question: What are the key components of BM25 that must be computed at query time?

- Concept: Column store vs inverted index tradeoffs
  - Why needed here: Understanding why MonetDB is slower but necessary for reproducibility
  - Quick check question: Why does MonetDB require query-time term statistic computation while Lucene precomputes?

- Concept: Document versioning and temporal validity
  - Why needed here: How valid_from/valid_to timestamps enable time-travel queries
  - Quick check question: How does the valid_to timestamp handle document deletions?

## Architecture Onboarding

- Component map: Lucene (fast live queries) ↔ MonetDB (reproducible time-travel) ↔ Query store (PID management and hash verification)
- Critical path: Document preprocessing → Lucene indexing + MonetDB synchronization → Query execution → Hash verification
- Design tradeoffs: Storage overhead (~2x) vs reproducibility guarantee; Query speed (80ms vs 2-20s) vs exact reproduction
- Failure signatures: Hash mismatches indicating floating-point discrepancies; Increasing query times in MonetDB; Missing document validity periods
- First 3 experiments:
  1. Test basic Lucene indexing and query performance on small corpus
  2. Verify MonetDB synchronization captures all document changes correctly
  3. Validate hash verification mechanism by introducing controlled floating-point differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of adding neural reranking strategies to the hybrid system on reproducibility and performance?
- Basis in paper: [explicit] "Future work will focus on extending the range of retrieval models that are supported and mirrored by the column-store system beyond BM25, especially to enable reproducible dense retrieval."
- Why unresolved: The current system only supports BM25 and sparse retrieval models. Neural reranking introduces non-deterministic elements that could affect reproducibility.
- What evidence would resolve it: Empirical evaluation comparing the hybrid system's performance and reproducibility with and without neural reranking on the same evolving corpus.

### Open Question 2
- Question: How can the performance of Versioned Columnstore-Based Retrieval (VCBR) be improved to handle larger document collections?
- Basis in paper: [explicit] "Future work will focus on extending the range of retrieval models that are supported and mirrored by the column-store system beyond BM25, especially to enable reproducible dense retrieval. Other research should be conducted on improving the performance of VCBR to allow to index and retrieve from larger collections."
- Why unresolved: The current VCBR system shows linear performance degradation as the corpus grows, making it impractical for very large collections.
- What evidence would resolve it: Performance benchmarks comparing optimized VCBR implementations with different indexing strategies, sharding approaches, or hardware configurations on corpora larger than 520,000 documents.

### Open Question 3
- Question: What is the minimum storage overhead required to maintain reproducibility in evolving document collections?
- Basis in paper: [inferred] The system doubles storage requirements (4.05GB vs 3.82GB), but it's unclear if this is the theoretical minimum or if optimizations could reduce this overhead while maintaining reproducibility.
- Why unresolved: The paper only presents one storage configuration and doesn't explore the trade-offs between storage overhead and reproducibility guarantees.
- What evidence would resolve it: Systematic comparison of storage requirements across different versioning strategies, compression techniques, and data retention policies while measuring the impact on reproducibility accuracy.

## Limitations
- Evaluation limited to a single German Wikipedia corpus, raising questions about generalizability to other domains and languages
- 2x storage overhead may be prohibitive for very large collections, though acceptable for domains requiring exact reproducibility
- Floating-point discrepancy mechanism, while effective for 1 in 10,000 cases, may not scale to collections with more frequent score collisions

## Confidence

**Major Uncertainties**: The evaluation is limited to a single German Wikipedia corpus and predefined queries, raising questions about generalizability to other domains and languages. The floating-point discrepancy mechanism, while effective for 1 in 10,000 cases, may not scale to collections with more frequent score collisions or different scoring implementations. The 2x storage overhead may be prohibitive for very large collections, though this tradeoff is acceptable for domains requiring exact reproducibility.

**Confidence Labels**:
- High confidence: Lucene's consistent performance maintenance (80ms query times) and the basic hybrid architecture feasibility
- Medium confidence: MonetDB's time-travel query capability and hash verification mechanism effectiveness, based on limited testing (10,400 queries)
- Low confidence: Generalizability to other corpora types and languages, and scalability to larger document collections

## Next Checks
1. Test the hash verification mechanism with controlled floating-point variations to determine the threshold where document swapping becomes necessary
2. Evaluate the system on a non-Wikipedia corpus (e.g., patent database) to assess domain generalizability
3. Measure query performance degradation patterns at 1M+ documents to determine practical scalability limits