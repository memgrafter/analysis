---
ver: rpa2
title: Reinforcement Learning From Imperfect Corrective Actions And Proxy Rewards
arxiv_id: '2410.05782'
source_url: https://arxiv.org/abs/2410.05782
tags:
- learning
- rewards
- proxy
- actions
- icopro
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces a framework for learning from two imperfect
  sources of feedback: proxy rewards and corrective actions from a human labeler.
  The core idea is that these two sources can complement each other despite their
  imperfections, leading to better alignment with human preferences than either source
  alone.'
---

# Reinforcement Learning From Imperfect Corrective Actions And Proxy Rewards

## Quick Facts
- arXiv ID: 2410.05782
- Source URL: https://arxiv.org/abs/2410.05782
- Reference count: 40
- The paper introduces a framework for learning from two imperfect feedback sources: proxy rewards and corrective actions from a human labeler, achieving better alignment with human preferences than either source alone.

## Executive Summary
This paper presents a framework for reinforcement learning that leverages two imperfect feedback sources: proxy rewards and corrective actions from human labelers. The core insight is that these two sources can complement each other by failing in different state-space regions, allowing each to correct the other's weaknesses. The authors propose an iterative algorithm (ICoPro) that alternates between data collection, finetuning with corrective actions using a margin loss, and propagating improvements using both RL losses and margin losses with pseudo-labels. Experiments on Atari games and autonomous driving tasks demonstrate that ICoPro outperforms baselines in sample efficiency and alignment with human preferences, even when corrective actions are suboptimal.

## Method Summary
ICoPro is an iterative algorithm that combines reinforcement learning with human feedback through three alternating phases: data collection (agent collects trajectories and queries labeler for corrective actions), finetune (updates Q-function using margin loss on collected corrective actions), and propagation (trains with combined RL losses and margin losses using both actual and pseudo-labels generated by target network). The method uses Rainbow as the base RL algorithm and introduces a margin loss that enforces Q-value separation between chosen and other actions, aligning policy with human preferences. Pseudo-labels generated from the target Q-network's greedy actions reduce human labeling burden while propagating learned preferences to more states.

## Key Results
- ICoPro achieves mean scores ranging from 920 to 1270 on Atari games, compared to Rainbow's 700-1200, while using the same labeling budget
- The method demonstrates robustness to different types of imperfections in corrective actions
- Sample efficiency improvements are observed across both Atari games and autonomous driving tasks
- Performance gains are maintained even when corrective actions are suboptimal

## Why This Works (Mechanism)

### Mechanism 1
Combining proxy rewards and corrective actions improves alignment by compensating for each signal's weaknesses in different state regions. Proxy rewards guide overall learning but may be imperfect in certain regions, while corrective actions provide targeted corrections in those regions. The two sources reinforce each other where one is weak. Core assumption: Imperfections in proxy rewards and corrective actions are not correlated—they fail in different state-action space regions. Evidence anchors: abstract states "bad decisions learned from proxy rewards can be corrected by the human labeler, while the effects of suboptimal corrective actions may be weakened by proxy rewards." Section 3 notes "imperfections of the two signals probably do not lie in the same state-action space regions and therefore the two signals could correct each other." Break condition: If proxy reward errors and corrective action errors overlap significantly in the same state regions, the benefit diminishes or reverses.

### Mechanism 2
Margin loss enforces Q-value separation between chosen and other actions, aligning policy with human preferences. Margin loss penalizes Q-values of non-chosen actions to be lower than the chosen action by a margin C, creating a preference ordering matching the human labeler. Core assumption: The margin loss correctly captures the preference ordering the human intends. Evidence anchors: Section 4 states "This loss amounts to enforcing that the corrective actions' Q-values should not be smaller than those of any other actions." Break condition: If the margin C is too small, the loss is ineffective; if too large, it may prevent learning from proxy rewards.

### Mechanism 3
Pseudo-labels reduce human labeling burden while maintaining alignment through self-supervised learning. Pseudo-labels are generated from the target Q-network's greedy actions on unlabeled states, acting as additional supervision that propagates learned preferences to more states without human input. Core assumption: The target Q-network's greedy actions are reasonably good approximations of correct actions. Evidence anchors: Section 4 states "Training with pseudo-labels can reduce the cost of collecting human labels by leveraging the large number of unlabeled states in Denv." Break condition: If the target Q-network is poorly trained, pseudo-labels become harmful noise that degrades performance.

## Foundational Learning

- Concept: Reinforcement learning with function approximation
  - Why needed here: The algorithm uses neural networks to approximate Q-functions, requiring understanding of how RL works with deep networks.
  - Quick check question: How does the Bellman equation translate into a loss function for Q-learning?

- Concept: Imitation learning and behavior cloning
  - Why needed here: The finetune phase uses margin loss similar to imitation learning, requiring understanding of how to learn from expert demonstrations.
  - Quick check question: What is the difference between behavior cloning and inverse reinforcement learning?

- Concept: Target networks and stability in deep RL
  - Why needed here: The algorithm uses a target network for stable Q-value updates and pseudo-label generation, requiring understanding of why target networks help.
  - Quick check question: Why does using a slowly-updating target network help stabilize training compared to using the online network directly?

## Architecture Onboarding

- Component map: Data collection -> finetune phase (margin loss on human labels) -> propagation phase (RL losses + margin loss on both human and pseudo-labels) -> repeat
- Critical path: The propagation phase is critical—it combines RL learning from proxy rewards with imitation learning from both human and pseudo-labels. Without this phase, the algorithm reduces to behavior cloning with limited labels.
- Design tradeoffs: Using pseudo-labels reduces human effort but risks propagating errors if the target network is wrong. Using only human labels ensures quality but requires more human effort. The weight on pseudo-labels (w̄) controls this tradeoff.
- Failure signatures: If performance is worse than Rainbow alone, likely the margin loss is too strong or pseudo-labels are harmful. If performance plateaus early, likely not enough human labels or margin loss is too weak. If training is unstable, likely target network update frequency is wrong.
- First 3 experiments:
  1. Run ICoPro on a simple environment (like CartPole) with perfect proxy rewards and human labels to verify basic functionality.
  2. Run with imperfect proxy rewards but perfect human labels to test the finetune phase alone.
  3. Run with perfect proxy rewards but imperfect human labels to test the propagation phase's ability to correct errors.

## Open Questions the Paper Calls Out
The paper acknowledges the need for theoretical analysis to reveal assumptions that could guarantee the synergistic combination of imperfect signals. It also mentions that more advanced sampling strategies for selecting which states to query for corrective actions could reduce feedback requirements, but does not explore alternative sampling methods beyond the current top-N largest Q-value differences approach.

## Limitations
- The paper lacks empirical validation for whether margin loss effectively captures human preference structure
- No direct evidence that pseudo-labels improve performance versus only using human labels
- The claim that proxy rewards and corrective actions fail in uncorrelated state regions is intuitive but unproven
- Experimental results do not isolate the individual contributions of each component (margin loss, pseudo-labels, alternating phases)

## Confidence
- High confidence: The basic algorithmic framework and its three-phase structure are well-defined and implementable
- Medium confidence: The theoretical benefits of combining two imperfect feedback sources are plausible, and experimental results show meaningful improvements over baselines
- Low confidence: The specific mechanisms by which margin loss and pseudo-labels contribute to performance gains are not empirically validated, and robustness claims to different types of imperfections lack systematic testing

## Next Checks
1. Run ablation studies isolating each component (margin loss only, pseudo-labels only, alternating phases only) to quantify their individual contributions to performance improvements
2. Systematically test how correlated imperfections in proxy rewards and corrective actions affect performance, including scenarios where both sources fail in the same state regions
3. Measure the accuracy of pseudo-labels against ground truth actions throughout training to verify they provide useful supervision rather than harmful noise