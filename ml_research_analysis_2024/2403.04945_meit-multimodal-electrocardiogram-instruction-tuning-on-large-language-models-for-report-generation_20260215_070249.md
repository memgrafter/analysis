---
ver: rpa2
title: 'MEIT: Multimodal Electrocardiogram Instruction Tuning on Large Language Models
  for Report Generation'
arxiv_id: '2403.04945'
source_url: https://arxiv.org/abs/2403.04945
tags:
- report
- instruction
- generation
- tuning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MEIT, a Multimodal ECG Instruction Tuning
  framework that automates ECG report generation using large language models (LLMs)
  and multimodal instructions. The approach uniquely aligns ECG signal representations
  with corresponding textual outputs through an efficient attention-based fusion method,
  avoiding additional training parameters.
---

# MEIT: Multimodal Electrocardiogram Instruction Tuning on Large Language Models for Report Generation

## Quick Facts
- arXiv ID: 2403.04945
- Source URL: https://arxiv.org/abs/2403.04945
- Reference count: 21
- Key outcome: MEIT framework achieves superior ECG report generation using instruction-tuned LLMs, with LLaMA-2 and Mistral-Instruct models scoring 0.747 BERTScore on MIMIC-IV-ECG and 0.694 on PTB-XL

## Executive Summary
MEIT introduces a multimodal ECG instruction tuning framework that automates ECG report generation using large language models and attention-based fusion of ECG signals with textual instructions. The approach aligns ECG signal representations with corresponding textual outputs through an efficient attention-based fusion method that avoids additional training parameters. Using two large-scale ECG datasets (MIMIC-IV-ECG with 800K samples and PTB-XL with 20K samples), the framework demonstrates superior performance in generating accurate ECG reports, effective zero-shot transferability, and robustness to signal perturbation.

## Method Summary
The MEIT framework processes raw 12-lead ECG signals through temporal convolution blocks to extract features, which are then projected to match the LLM head dimension. An attention-based fusion method directly injects ECG embeddings with language context in the causal-attention of the models, using shared projection of keys and values. The framework employs LoRA adapters for efficient instruction tuning, training on instruction-formatted data pairs of ECG signals and corresponding reports. Models are evaluated on three tasks: quality report generation, zero-shot generalizability across datasets, and robustness to signal perturbation using standard NLP metrics (BLEU, METEOR, ROUGE, CIDEr-D, BERTScore).

## Key Results
- Instruction-tuned LLaMA-2 and Mistral-Instruct models achieve highest scores: 0.747 BERTScore F1 on MIMIC-IV-ECG and 0.694 on PTB-XL
- Zero-shot performance demonstrates effective transferability, with LLaMA-2 13B achieving 0.732 BERTScore F1 on PTB-XL when trained on MIMIC-IV-ECG
- Models maintain robustness against noise interference, with LLaMA-2 13B achieving 0.734 BERTScore F1 at 20dB SNR (0.98 signal-to-noise ratio)

## Why This Works (Mechanism)

### Mechanism 1
The attention-based fusion method directly injects ECG embeddings with language context in the causal-attention of the models without additional training parameters. This is achieved through concatenated-fusion that combines ECG and language features in the sequence dimension with shared projection of keys and values for each pattern.

### Mechanism 2
Instruction tuning with multimodal prompts enables LLMs to generate professional-grade ECG reports through LoRA adapters while freezing the LLM backbone. The framework constructs instruction data by pairing ECG signals with human instructions and corresponding reports, using autoregressive loss computation on tokens after response tokens.

### Mechanism 3
Model scaling and pretraining with general instructions enhance zero-shot performance on unseen ECG datasets. Larger LLMs (up to 70B parameters) pretrained with instruction data show improved zero-shot transfer capabilities when fine-tuned on ECG data from one dataset and tested on another.

## Foundational Learning

- **Multimodal representation learning**: Required to bridge ECG signals (numerical time series) with natural language descriptions through effective fusion of different data modalities. *Quick check*: Can you explain how attention mechanisms can align different modalities without explicit cross-modal training?

- **Instruction tuning methodology**: Central to the approach of converting ECG-report pairs into instruction-following format to leverage LLM capabilities for zero-shot and few-shot learning. *Quick check*: What are the key differences between traditional fine-tuning and instruction tuning in terms of data preparation and loss computation?

- **Low-Rank Adaptation (LoRA) for efficient fine-tuning**: Used to adapt large LLMs to ECG tasks without full parameter updates, enabling efficient training on specialized data. *Quick check*: How does LoRA's low-rank decomposition enable parameter-efficient adaptation compared to full fine-tuning?

## Architecture Onboarding

- **Component map**: ECG signal → Temporal convolution blocks → Embedding projection → Causal-attention fusion → LLM generation → Report output
- **Critical path**: The raw ECG signal flows through the encoder, undergoes feature extraction and projection, fuses with language context through attention, and generates the final report through the LLM backbone
- **Design tradeoffs**: Concatenated-fusion avoids additional parameters but may limit complex cross-modal interactions compared to gated-attention methods; LoRA enables efficient training but may constrain adaptation capacity
- **Failure signatures**: Poor BLEU/CIDEr scores indicate generation quality issues; large gaps between training and zero-shot performance suggest overfitting or poor generalization; degraded performance under signal perturbation reveals robustness limitations
- **First 3 experiments**:
  1. Ablation study comparing causal-attention fusion vs. gated-attention methods to validate parameter efficiency
  2. Zero-shot transfer test between MIMIC-IV-ECG and PTB-XL to assess generalization capability
  3. Noise robustness evaluation with varying SNR levels to measure perturbation resilience

## Open Questions the Paper Calls Out

### Open Question 1
How does model performance scale with ECG data quality and annotation completeness across different datasets? The paper mentions reduced performance on PTB-XL due to smaller instruction data scale and expert verification, but doesn't systematically analyze how varying data quality impacts performance.

### Open Question 2
What is the relationship between instruction prompt diversity and zero-shot generalization capability? While the paper demonstrates zero-shot capabilities using diverse prompts generated by GPT-4, it doesn't explore how varying prompt diversity affects model generalization to unseen datasets.

### Open Question 3
How does the proposed attention-based fusion method compare to other multimodal fusion approaches in terms of computational efficiency and performance? The paper claims efficiency but doesn't benchmark against alternative fusion methods or provide quantitative comparisons of parameter efficiency versus performance trade-offs.

## Limitations
- Small test sets (10% of each dataset) may not fully capture model robustness across diverse clinical scenarios
- Attention-based fusion method may not capture complex cross-modal relationships as effectively as more sophisticated multimodal architectures
- Zero-shot performance shows significant degradation compared to in-domain performance, suggesting limited generalizability to truly unseen clinical contexts

## Confidence
- High confidence in the effectiveness of instruction tuning for ECG report generation on the studied datasets
- Medium confidence in the parameter efficiency claims due to lack of ablation studies comparing against alternative fusion methods
- Medium confidence in zero-shot generalization results given the relatively similar nature of MIMIC-IV-ECG and PTB-XL datasets

## Next Checks
1. Conduct cross-dataset evaluation with more diverse ECG sources (e.g., different geographic populations, varying acquisition equipment) to rigorously test zero-shot capabilities
2. Perform ablation studies comparing the attention-based fusion method against alternative multimodal architectures (cross-attention, gated-attention) to quantify the parameter efficiency tradeoff
3. Evaluate model performance on clinically challenging cases including rare ECG abnormalities and borderline normal/abnormal cases to assess real-world clinical utility