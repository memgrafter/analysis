---
ver: rpa2
title: Conversational Exploratory Search of Scholarly Publications Using Knowledge
  Graphs
arxiv_id: '2410.00427'
source_url: https://arxiv.org/abs/2410.00427
tags:
- search
- conversational
- interface
- system
- topic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a conversational exploratory search system
  for discovering scholarly publications using knowledge graphs. The system addresses
  the challenge of navigating complex graphical interfaces by enabling intuitive dialogue-based
  interactions to explore research topics and papers.
---

# Conversational Exploratory Search of Scholarly Publications Using Knowledge Graphs

## Quick Facts
- arXiv ID: 2410.00427
- Source URL: https://arxiv.org/abs/2410.00427
- Authors: Phillip Schneider; Florian Matthes
- Reference count: 15
- Primary result: Conversational interface outperforms traditional graphical interface for scholarly publication discovery with SUS scores around 76

## Executive Summary
This paper presents a conversational exploratory search system for discovering scholarly publications using knowledge graphs. The system addresses the challenge of navigating complex graphical interfaces by enabling intuitive dialogue-based interactions to explore research topics and papers. It employs a three-phase search process: topic classification via fine-tuned models, hierarchical text clustering using embeddings, and comparative summarization with language models. Evaluation on 40 participants showed the conversational interface outperformed a traditional graphical interface in usability, readability, and user satisfaction.

## Method Summary
The system implements a three-phase search process: (1) topic classification using fine-tuned transformer models to map user queries to research topics, (2) hierarchical text clustering using SPECTER2 embeddings to group related papers, and (3) comparative summarization using language models to help users evaluate papers. The knowledge graph combines data from ACL Anthology, Microsoft Academic Graph, and Semantic Scholar Academic Graph. The conversational interface is built using RASA framework with a Streamlit frontend, connecting to Neo4j knowledge graph and Weaviate vector database.

## Key Results
- Conversational interface achieved SUS scores around 76 compared to traditional graphical interfaces
- Fine-tuned SetFit model achieved 0.95 F1-score for topic classification, outperforming zero-shot approaches
- Hierarchical clustering created 47,035 thematic clusters that users could navigate intuitively
- Users reported higher satisfaction with conversational interface for discovering relevant research papers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The three-phase search process reduces cognitive load by progressively narrowing the search space.
- Mechanism: Users start with a broad research topic, then select thematic clusters, and finally compare specific papers, each step reducing the number of choices and complexity.
- Core assumption: Users can articulate their information needs better when guided through a structured exploration rather than presented with all options at once.
- Evidence anchors:
  - [abstract] "users may face challenges when navigating these graphical search interfaces due to the complexity and volume of data, which impedes their ability to discover publications effectively."
  - [section] "The three-phase search process: topic classification via fine-tuned models, hierarchical text clustering using embeddings, and comparative summarization with language models."

### Mechanism 2
- Claim: Fine-tuned transformer models outperform zero-shot LLM classification for research topic identification.
- Mechanism: SetFit fine-tuning on synthetically generated queries produces better accuracy (0.95 F1) compared to vector search (0.50 F1) or GPT-3.5-Turbo (0.75 F1) for mapping layman's queries to NLP topics.
- Core assumption: Synthetic data generated with diverse personas can effectively represent real user query patterns.
- Evidence anchors:
  - [section] "The SetFit model demonstrated superior performance over the two other approaches with a score of 0.95."
  - [section] "vector search achieved a macro F1-score below 0.50., GPT-3.5-Turbo achieved a score near 0.75"

### Mechanism 3
- Claim: Hierarchical clustering with decreasing distance thresholds creates manageable, relevant paper groupings.
- Mechanism: Agglomerative clustering starts with broad similarity (distance threshold 10) and iteratively refines clusters, stopping when fewer than 10 papers remain per cluster.
- Core assumption: SPECTER2 embeddings capture semantic similarity relevant to thematic grouping better than lexical approaches.
- Evidence anchors:
  - [section] "we adopted an iterative hierarchical approach. We progressively decreased the distance threshold at each cluster level, keeping the number of clusters small"
  - [section] "Overall, we constructed a granular hierarchy of 47,035 thematic clusters"

## Foundational Learning

- Concept: Knowledge Graphs for semantic relationships
  - Why needed here: KGs model scholarly entities (authors, papers, topics) and their relationships, enabling semantic search beyond keyword matching
  - Quick check question: How does a KG differ from a traditional relational database in representing scholarly data?

- Concept: Vector embeddings for semantic similarity
  - Why needed here: SPECTER2 embeddings capture semantic meaning of paper abstracts, enabling clustering and similarity search that matches user intent
  - Quick check question: Why use cosine similarity on embeddings rather than Euclidean distance for text similarity?

- Concept: Fine-tuning vs zero-shot prompting
  - Why needed here: Fine-tuning transformer models on domain-specific data (synthetic queries) produces better performance than zero-shot prompting for classification tasks
  - Quick check question: What are the tradeoffs between fine-tuning and few-shot prompting in terms of data requirements and performance?

## Architecture Onboarding

- Component map: Web Application -> RASA Dialogue System -> Action Handler -> Knowledge Base (Neo4j + Weaviate) and Generative Language Model (Zephyr-7B-Beta)
- Critical path: User message → NLU pipeline → Dialogue Manager → Action prediction → Knowledge Graph Retriever/LLM Access → Response generation → User interface
- Design tradeoffs: 
  - Pre-computing cluster names vs real-time generation (latency vs freshness)
  - Fine-tuning vs zero-shot prompting (performance vs development cost)
  - Separate VMs for components vs unified deployment (scalability vs complexity)
- Failure signatures:
  - High latency: LLM or vector database bottlenecks
  - Incorrect topic classification: Synthetic training data mismatch or model degradation
  - Empty cluster results: Distance threshold too strict or embeddings poor quality
- First 3 experiments:
  1. Test topic classification with real user queries vs synthetic data performance
  2. Vary distance thresholds in clustering to find optimal granularity
  3. A/B test pre-computed vs real-time cluster naming with user feedback

## Open Questions the Paper Calls Out

The paper identifies several open research questions including: improving system performance for larger knowledge graphs, handling dynamic knowledge graph updates, combining conversational and graphical interfaces, integrating multiple scholarly knowledge graphs, improving context-aware LLM integration, ensuring factual consistency in summarization, incorporating user preferences for personalized experiences, extending to multilingual support, understanding user adaptation over time, and assessing search performance across different domains.

## Limitations

- Small sample size (40 participants) may limit generalizability of usability findings
- Focus on NLP research topics means performance for other academic domains is unknown
- Lack of detailed error analysis for system failures in topic classification or clustering
- Synthetic data generation approach may not fully capture real user query diversity

## Confidence

- **High confidence**: The core architecture combining knowledge graphs with conversational AI is technically sound
- **Medium confidence**: Evaluation results showing superior usability are promising but based on small sample
- **Medium confidence**: Fine-tuning approach for topic classification appears effective but needs validation with real queries
- **Low confidence**: Clustering approach's effectiveness across different domains and corpus sizes is not fully established

## Next Checks

1. Conduct a larger-scale evaluation with participants from diverse academic backgrounds and research domains to assess generalizability
2. Test the system with real user queries (not synthetic) to validate the topic classification model performance in practice
3. Perform ablation studies to quantify the contribution of each component (knowledge graph, clustering, fine-tuning) to overall system performance