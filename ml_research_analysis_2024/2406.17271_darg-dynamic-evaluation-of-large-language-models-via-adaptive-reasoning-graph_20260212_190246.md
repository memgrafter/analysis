---
ver: rpa2
title: 'DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning Graph'
arxiv_id: '2406.17271'
source_url: https://arxiv.org/abs/2406.17271
tags:
- uni00000011
- reasoning
- graph
- arxiv
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DARG dynamically extends reasoning benchmarks by constructing and
  perturbing reasoning graphs to control complexity across math, social, spatial,
  and symbolic domains. For GSM8K math problems, graph depth, width, and numerical
  complexity are increased; for BBQ social reasoning, attribute pairs and polarities
  are added; for BBH Navigate and Dyck tasks, reasoning steps are expanded.
---

# DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning Graph

## Quick Facts
- arXiv ID: 2406.17271
- Source URL: https://arxiv.org/abs/2406.17271
- Authors: Zhehao Zhang; Jiaao Chen; Diyi Yang
- Reference count: 40
- Primary result: DARG reveals that all tested LLMs experience performance degradation with increased reasoning complexity, with larger models showing greater resilience but also amplifying bias in social reasoning tasks

## Executive Summary
DARG introduces a dynamic evaluation framework that extends reasoning benchmarks by constructing and perturbing reasoning graphs to systematically control complexity across math, social, spatial, and symbolic domains. The method generates novel test data by extracting reasoning graphs from existing benchmarks, perturbing them along complexity dimensions, and using code-augmented LLMs for verification. When evaluated on 15 state-of-the-art models, all show declining accuracy with increased complexity, with larger or mixture-of-experts models more resilient. Notably, social and spatial tasks reveal rising bias and over-sensitivity to protected groups in high-complexity data, especially in powerful models like GPT-4 Turbo and Gemini-1.5-Pro. These findings highlight the need for adaptive evaluation frameworks like DARG to assess LLM robustness beyond static benchmarks.

## Method Summary
DARG dynamically extends reasoning benchmarks by first extracting reasoning graphs representing problem-solving structures from existing data points. These graphs are then perturbed through rule-based functions to increase complexity along specific dimensions (depth, width, numerical complexity for math; attribute pairs and polarities for social reasoning; reasoning steps for spatial tasks). A code-augmented LLM verifies the correctness of generated data by solving the reasoning tasks programmatically. The framework is evaluated on four benchmark datasets (GSM8K, BBQ, BBH Navigate, BBH Dyck) using 15 state-of-the-art LLMs, measuring accuracy and robustness across controlled complexity increases.

## Key Results
- All 15 tested LLMs show performance degradation with increased reasoning complexity across all four benchmark types
- Larger models and mixture-of-experts architectures demonstrate greater resilience to complexity increases but also amplify bias in social reasoning tasks
- Social and spatial reasoning tasks exhibit increased sensitivity to protected groups and demographic attributes as complexity rises
- Code-augmented verification successfully maintains data quality while enabling dynamic complexity scaling
- The framework reveals limitations in model robustness that static benchmarks fail to capture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DARG's reasoning graph construction enables fine-grained complexity control by mapping problem-solving steps to graph structures
- Mechanism: The method extracts reasoning graphs representing underlying structures of basic reasoning components necessary for problem-solving, then perturbs these graphs to generate novel testing data with different complexity levels while maintaining linguistic diversity
- Core assumption: Problem-solving processes can be effectively represented as directed acyclic graphs where nodes represent basic reasoning units and edges represent functions between them
- Evidence anchors: Abstract states reasoning graphs are extracted and perturbed; section 2.1 defines reasoning graphs as directed acyclic graphs; weak corpus evidence

### Mechanism 2
- Claim: Code-augmented LLM verification ensures correctness of generated data while maintaining linguistic diversity
- Mechanism: After generating new test samples through graph perturbation, a code-augmented LLM produces code to solve the reasoning task and uses an external code interpreter to compute the final answer, comparing it with the label derived from the reasoning graph
- Core assumption: Code-augmented LLMs can reliably verify reasoning processes and maintain factual accuracy
- Evidence anchors: Abstract mentions code-augmented LLM for label correctness; section 2.4 discusses code interpreter mitigation of hallucinations; weak corpus evidence

### Mechanism 3
- Claim: Dynamic complexity scaling reveals performance degradation patterns that static benchmarks miss
- Mechanism: By systematically increasing complexity dimensions and evaluating model performance across these levels, DARG exposes limitations in model reasoning capabilities not apparent on static benchmarks
- Core assumption: Model performance degrades predictably as complexity increases, revealing meaningful information about capabilities
- Evidence anchors: Abstract states all LLMs experience performance decrease with increased complexity; section 3.1 shows accuracy decreases across all three dimensions; weak corpus evidence

## Foundational Learning

- Concept: Directed acyclic graphs (DAGs) as problem-solving representations
  - Why needed here: The core mechanism relies on representing reasoning processes as DAGs where nodes are reasoning units and edges are operations between them
  - Quick check question: How would you represent a multi-step arithmetic problem as a DAG? What would be nodes and edges?

- Concept: In-context learning (ICL) with LLMs
  - Why needed here: DARG uses ICL to construct reasoning graphs and perform graph-to-text decoding, requiring understanding of how to prompt LLMs effectively
  - Quick check question: What are the key components of an effective ICL prompt for graph construction tasks?

- Concept: Code-augmented LLM verification
  - Why needed here: The method uses code-augmented LLMs to verify generated data correctness, requiring understanding of how to integrate code execution with LLM reasoning
  - Quick check question: How does code execution improve LLM verification reliability compared to pure text-based verification?

## Architecture Onboarding

- Component map: Graph Construction -> Graph Perturbation -> Graph-to-Text Decoding -> Data Verification -> Model Evaluation
- Critical path: Graph Construction → Graph Perturbation → Graph-to-Text Decoding → Data Verification → Model Evaluation
- Design tradeoffs:
  - Complexity control vs. linguistic diversity: More complex graph perturbations may reduce natural language quality
  - Verification overhead vs. correctness: Code-augmented verification adds computational cost but improves accuracy
  - Graph extraction accuracy vs. coverage: More accurate extraction may miss edge cases or novel problem types
- Failure signatures:
  - Graph construction failures: Incorrect reasoning graphs that don't match problem solutions
  - Perturbation failures: Graph modifications that break semantic consistency
  - Decoding failures: Generated text that doesn't match graph structure or loses problem meaning
  - Verification failures: Code execution errors or incorrect answer validation
- First 3 experiments:
  1. Test graph construction on simple GSM8K problems with known solutions to verify accuracy
  2. Apply single-dimension complexity increases to validate perturbation mechanisms
  3. Verify generated data using code-augmented LLM and compare with manual verification on sample cases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the DARG framework generalize to non-reasoning tasks such as natural language understanding?
- Basis in paper: Inferred from the conclusion section, which states that the reasoning graph definition in DARG is general and can be applied and extended to other tasks like natural language understanding tasks
- Why unresolved: The paper focuses on reasoning tasks and does not provide empirical evidence or methodology for extending DARG to non-reasoning tasks
- What evidence would resolve it: Conducting experiments applying DARG to natural language understanding tasks and comparing its performance to traditional evaluation methods

### Open Question 2
- Question: Can open-source models replace closed-source LLMs in the graph extraction and data generation process?
- Basis in paper: Inferred from the conclusion section, which mentions that the current graph extraction and data generation process heavily rely on closed-source LLMs like GPT-4
- Why unresolved: The paper does not investigate the feasibility or effectiveness of using open-source models for these tasks
- What evidence would resolve it: Testing open-source models in the graph extraction and data generation process and comparing their performance to closed-source models

### Open Question 3
- Question: How does the complexity of the reasoning graph impact the computational efficiency of the DARG framework?
- Basis in paper: Inferred from the methodology section, which describes fine-grained graph perturbations based on various dimensions of the reasoning graph
- Why unresolved: The paper does not discuss the computational costs associated with increasing the complexity of the reasoning graph
- What evidence would resolve it: Analyzing the computational resources and time needed for DARG to handle increasingly complex reasoning graphs

## Limitations
- The robustness of graph perturbation rules across diverse reasoning domains remains unclear, particularly for social reasoning tasks where complexity increases may introduce unintended semantic shifts
- The scalability of the code-augmented verification mechanism for very large reasoning graphs or highly complex numerical problems is not demonstrated
- The relationship between complexity scaling and model bias amplification requires further investigation, especially regarding the causal mechanisms underlying observed trends

## Confidence
- High confidence: The core mechanism of using reasoning graphs for complexity control is well-supported by the paper's methodology section and experimental results
- Medium confidence: The effectiveness of code-augmented LLM verification for maintaining data quality, as results show good correlation with original benchmarks but limited ablation studies
- Medium confidence: The general trend of performance degradation with increased complexity is robust, though the magnitude varies significantly across models and domains

## Next Checks
1. **Domain Transferability Test**: Apply DARG to a novel reasoning domain (e.g., logical deduction or temporal reasoning) not covered in the original evaluation to assess the framework's generalizability beyond the four benchmark types
2. **Bias Amplification Analysis**: Conduct controlled experiments to determine whether complexity increases directly cause bias amplification or if other factors (such as dataset composition changes) contribute to the observed trends
3. **Verification Robustness Test**: Systematically vary the code-augmented LLM verification parameters (temperature, prompt templates, code complexity limits) to quantify their impact on generated data quality and identify potential failure modes in the verification pipeline