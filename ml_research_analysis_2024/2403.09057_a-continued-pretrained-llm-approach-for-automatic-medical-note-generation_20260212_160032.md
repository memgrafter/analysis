---
ver: rpa2
title: A Continued Pretrained LLM Approach for Automatic Medical Note Generation
arxiv_id: '2403.09057'
source_url: https://arxiv.org/abs/2403.09057
tags:
- medical
- note
- arxiv
- data
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HEAL, a 13B parameter medical language model
  built by continuing training LLaMA2 on a combination of general web text, medical
  literature, and proprietary medical conversation and note data. The model was specifically
  tuned to generate clinical SOAP notes from doctor-patient conversations.
---

# A Continued Pretrained LLM Approach for Automatic Medical Note Generation

## Quick Facts
- arXiv ID: 2403.09057
- Source URL: https://arxiv.org/abs/2403.09057
- Authors: Dong Yuan; Eti Rastogi; Gautam Naik; Sree Prasanna Rajagopal; Sagar Goyal; Fen Zhao; Bharath Chintagunta; Jeff Ward
- Reference count: 5
- Key outcome: HEAL, a 13B parameter medical LLM, achieves GPT-4 parity in medical note generation through continued pretraining on medical-specific datasets

## Executive Summary
This paper introduces HEAL, a 13B parameter medical language model built by continuing training LLaMA2 on a combination of general web text, medical literature, and proprietary medical conversation and note data. The model was specifically tuned to generate clinical SOAP notes from doctor-patient conversations. Evaluated against GPT-4, Med-PaLM 2, and other medical LLMs, HEAL achieved 78.4% accuracy on PubMedQA (within 5% of Med-PaLM 2), matched GPT-4's performance in medical note generation, and produced more complete and correct notes with fewer errors than human scribes or comparable models.

## Method Summary
The authors developed HEAL by continuing pretraining LLaMA2-13B on a curated dataset of 14.89B tokens from three sources: general web text (C4), medical literature (PubMed articles, MedDialog), and proprietary medical conversations and EHR notes. The model was trained using FSDP pipeline parallelism with 32 A100 80GB GPUs, employing phased training with 8K context length and medical instruction tuning. The training pipeline included data deduplication, Flash Attention for efficiency, and positional interpolation for context handling. The model was evaluated on PubMedQA, MedQA, and a custom medical note generation task using human expert assessment with specific metrics for missed, incorrect, and irrelevant information.

## Key Results
- HEAL achieved 78.4% accuracy on PubMedQA, within 5% of Med-PaLM 2
- The model matched GPT-4's performance in medical note generation tasks
- HEAL produced more complete and correct notes with fewer errors than human scribes or comparable models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continuing pretraining on domain-specific data allows smaller models to achieve performance comparable to much larger models.
- Mechanism: The approach leverages targeted continued pretraining on medical-specific datasets to specialize the base LLaMA2 model without requiring massive parameter counts.
- Core assumption: Medical knowledge and task-specific patterns can be effectively learned through continued pretraining on curated datasets rather than requiring a model to be trained from scratch or needing billions of parameters.
- Evidence anchors:
  - [abstract] "This paper introduces HEAL, a 13B parameter medical language model built by continuing training LLaMA2 on a combination of general web text, medical literature, and proprietary medical conversation and note data."
  - [section] "Our results demonstrate that HEAL outperforms GPT-4 and PMC-LLaMA in PubMedQA, with an accuracy of 78.4%."
  - [corpus] Weak evidence - related papers discuss similar medical LLMs but don't directly confirm the specific mechanism of continued pretraining effectiveness.
- Break condition: If the continued pretraining data quality is poor or insufficiently diverse, the model would fail to capture the necessary medical domain knowledge and task patterns.

### Mechanism 2
- Claim: The combination of diverse pretraining datasets preserves generative capabilities while adding medical domain understanding.
- Mechanism: The model maintains general language generation abilities by including non-medical datasets alongside medical data, preventing catastrophic forgetting while adding medical knowledge.
- Core assumption: The base LLaMA2 model's generative capabilities can be preserved while simultaneously learning medical domain knowledge through careful dataset curation and balanced training.
- Evidence anchors:
  - [section] "Non-medical public datasets. To ensure that the new model doesn't lose the generative capabilities of the pretrained LLaMA2 model, we added general domain datasets such as C4 (Raffel et al., 2019)."
  - [section] "Pretraining Ablation. Table 2 shows our examination of the effects of varying data proportions... The ablation study revealed that removing general datasets from the mix detrimentally impacted the model's generative abilities."
  - [corpus] No direct evidence found in corpus about this specific balance between general and medical data preservation.
- Break condition: If the ratio of general to medical data is incorrect, the model could either lose its medical specialization or its general language generation capabilities.

### Mechanism 3
- Claim: Instruction tuning with medical explanations improves the model's ability to generate accurate medical notes from conversations.
- Mechanism: The model is trained on medical instructions that include both the task and detailed explanations of why certain medical concepts matter, improving its reasoning and accuracy in note generation.
- Core assumption: Providing explanations alongside instructions helps the model develop better reasoning capabilities for medical note generation, similar to how human medical students learn.
- Evidence anchors:
  - [section] "We also created a synthetic dataset comprising of medical instructions, like extraction of medications from a medical conversation and grammar correction of a generated medical note... Training on such instructions with explanations, helped the model better comprehend the medical notes and understand the reasoning behind it."
  - [section] "We attribute this improved performance to our continued pretraining approach using complex medical instructions."
  - [corpus] No direct evidence found in corpus about instruction tuning with explanations specifically.
- Break condition: If the instruction data quality is poor or explanations are inaccurate, the model could learn incorrect medical reasoning patterns.

## Foundational Learning

- Concept: Continued pretraining vs. Fine-tuning
  - Why needed here: The paper distinguishes between continued pretraining (training on new data while maintaining base model capabilities) and fine-tuning (adapting for specific tasks). Understanding this difference is crucial for grasping why HEAL can achieve strong performance without being a massive model.
  - Quick check question: What is the key difference between continued pretraining and fine-tuning, and why did the authors choose continued pretraining for HEAL?

- Concept: Medical domain adaptation techniques
  - Why needed here: The paper uses specific techniques like medical instruction tuning and data curation from multiple sources. Understanding these techniques is essential for replicating or extending this work.
  - Quick check question: What are the three main categories of medical data used in HEAL's continued pretraining, and why was each category important?

- Concept: Evaluation metrics for medical note generation
  - Why needed here: The paper introduces specific evaluation metrics (missed information, incorrect information, irrelevant information) for medical note generation. Understanding these metrics is crucial for assessing model performance.
  - Quick check question: What are the three key evaluation metrics used for medical note generation in this paper, and why is each one important in the healthcare context?

## Architecture Onboarding

- Component map:
  - Base model: LLaMA2-13B (foundation)
  - Data pipeline: Three-tier dataset (non-medical public, medical public, proprietary medical)
  - Training infrastructure: FSDP pipeline parallelism with 32 A100 80GB GPUs
  - Fine-tuning layer: Medical instruction tuning with explanations
  - Evaluation system: Human medical expert rubric-based assessment

- Critical path:
  1. Data curation and preparation (balancing general and medical datasets)
  2. Continued pretraining with FSDP and flash attention
  3. Medical instruction tuning phase
  4. Evaluation on PubMedQA, MedQA, and medical note generation tasks

- Design tradeoffs:
  - Model size vs. performance: 13B parameters achieved GPT-4 parity, demonstrating that continued pretraining can offset the need for massive parameter counts
  - Data diversity vs. specialization: Balancing general language capability with medical domain expertise required careful dataset curation
  - Training efficiency vs. quality: Phased training and data deduplication were used to optimize training while maintaining quality

- Failure signatures:
  - High perplexity on medical datasets indicates insufficient medical knowledge acquisition
  - Poor performance on instruction-following tasks suggests inadequate explanation tuning
  - Low Rouge scores on long text generation indicate loss of generative capabilities

- First 3 experiments:
  1. Run pretraining ablation study: Train three versions with different data ratios (general only, medical only, balanced) and compare performance on medical note generation
  2. Evaluate instruction tuning effectiveness: Compare model performance with and without medical explanations in the instruction data
  3. Test context length impact: Train versions with different context lengths (4K, 8K, 12K) and evaluate medical note generation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal ratio of general web text, medical literature, and proprietary medical conversation data for continued pretraining of medical LLMs to maximize both general language capabilities and medical domain understanding?
- Basis in paper: [explicit] The paper mentions using equal proportions of these datasets but also shows through ablation studies that removing general datasets negatively impacts generative abilities while medical datasets improve medical context understanding.
- Why unresolved: The paper only explores equal proportions and a reduced 1B token dataset, but the optimal ratio for different medical tasks and model sizes remains unknown.
- What evidence would resolve it: Systematic experiments varying the ratios of general web text, medical literature, and proprietary medical conversation data while measuring performance on multiple medical tasks would identify the optimal balance.

### Open Question 2
- Question: How does continued pretraining on medical conversation data specifically improve the model's ability to handle ASR errors compared to traditional fine-tuning approaches?
- Basis in paper: [explicit] The paper notes that both HEAL and GPT-4 excel at correcting ASR errors, attributing this to the continued pretraining approach using complex medical instructions, but doesn't explain the mechanism.
- Why unresolved: While the paper observes better ASR error handling, it doesn't provide insights into why continued pretraining on conversation data leads to this improvement compared to other approaches.
- What evidence would resolve it: Detailed error analysis comparing ASR error correction capabilities between models trained with different approaches (continued pretraining vs. fine-tuning vs. instruction tuning) would reveal the specific benefits of each method.

### Open Question 3
- Question: What is the relationship between model size and performance in medical note generation tasks when using continued pretraining, and at what point do diminishing returns set in?
- Basis in paper: [inferred] The paper demonstrates strong performance with a 13B parameter model, but acknowledges that scaling up training could further improve results, suggesting that the relationship between size and performance is not fully explored.
- Why unresolved: The paper only evaluates one model size (13B) and suggests potential improvements with scaling, but doesn't provide a comprehensive analysis of how different sizes perform or where performance plateaus.
- What evidence would resolve it: Training and evaluating models of varying sizes (e.g., 7B, 13B, 34B, 70B) using the same continued pretraining approach on medical conversation data would reveal the size-performance relationship and point of diminishing returns.

## Limitations

- Evaluation relies on simulated conversations rather than real-world clinical data
- Proprietary nature of key datasets prevents full reproducibility
- Long-term performance and robustness in real clinical settings remains unknown

## Confidence

- High confidence: The core claim that continued pretraining on medical-specific datasets improves performance over base LLaMA2
- Medium confidence: The claim that 13B parameters can achieve GPT-4 parity through continued pretraining
- Low confidence: The claim about instruction tuning with medical explanations being the primary driver of improved medical note generation

## Next Checks

1. Deploy HEAL in actual clinical settings with real doctor-patient conversations to assess performance with natural speech patterns, background noise, and conversational complexity not present in synthetic data.

2. Conduct longitudinal studies to evaluate model performance over extended periods, including adaptation to new medical terminology, emerging conditions, and changing clinical documentation standards.

3. Test HEAL's performance across a broader range of medical tasks beyond PubMedQA and medical note generation, including clinical decision support, patient triage, and medical literature analysis to better understand its capabilities and limitations.