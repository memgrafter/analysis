---
ver: rpa2
title: Two Tales of Single-Phase Contrastive Hebbian Learning
arxiv_id: '2402.08573'
source_url: https://arxiv.org/abs/2402.08573
tags:
- learning
- neural
- which
- propagation
- arovr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates single-phase contrastive Hebbian learning
  methods, particularly focusing on dual propagation (DP) and its variants. The authors
  provide a theoretical foundation for DP by deriving it through repeated applications
  of optimal value reformulations and saddlepoint relaxations.
---

# Two Tales of Single-Phase Contrastive Hebbian Learning

## Quick Facts
- arXiv ID: 2402.08573
- Source URL: https://arxiv.org/abs/2402.08573
- Reference count: 40
- This paper investigates single-phase contrastive Hebbian learning methods, particularly focusing on dual propagation (DP) and its variants.

## Executive Summary
This paper investigates single-phase contrastive Hebbian learning methods, particularly focusing on dual propagation (DP) and its variants. The authors provide a theoretical foundation for DP by deriving it through repeated applications of optimal value reformulations and saddlepoint relaxations. They demonstrate that choosing different values for the parameter α in DP affects the Lipschitz continuity of the learned model, with lower α values leading to better robustness. The paper also introduces an improved variant of DP called DP⊤, which is more stable with asymmetric nudging. Experimental results on MNIST, FashionMNist, CIFAR10, CIFAR100, and Imagenet32x32 show that DP⊤ performs competitively with standard backpropagation and other contrastive Hebbian learning methods.

## Method Summary
The paper investigates dual propagation (DP) and its variants as single-phase contrastive Hebbian learning methods. DP uses two oppositely nudged neural states to enable stable learning without requiring separate phases or infinitesimal perturbations. The authors introduce DP⊤ as a more stable variant that works with arbitrary α values through a reformulated Lagrangian. The methods are evaluated on image classification tasks using VGG16 and MLP networks, comparing test accuracy and Lipschitz constants across different datasets and hyperparameter settings.

## Key Results
- DP⊤ achieves competitive performance with standard backpropagation on CIFAR10, CIFAR100, and Imagenet32x32
- Lower α values in DP lead to models with smaller Lipschitz constants and improved adversarial robustness
- DP⊤ maintains stability for asymmetric nudging (α ≠ 1/2) while original DP diverges
- DP⊤ outperforms the original DP method in terms of stability and performance across all tested scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual propagation (DP) achieves stable learning with non-infinitesimal nudging by maintaining two oppositely nudged neural states.
- Mechanism: Each neuron maintains two internal states (s+ and s-) that receive the same bottom-up input but are nudged in opposite directions by top-down error signals. The weighted mean (s̄) propagates upstream while the difference (s+ - s-) represents the error signal propagating downstream.
- Core assumption: The two neural states can be updated simultaneously through local rules without requiring sequential phases or infinitesimal perturbations.
- Evidence anchors:
  - [abstract]: "by modelling artificial neurons as dyads with two oppositely nudged compartments, it is possible for a fully local learning algorithm named 'dual propagation' to bridge the performance gap to backpropagation, without requiring separate learning phases or infinitesimal nudging."
  - [section]: "In dual propagation [20] (DP), an algorithm similar in spirit to contrastive Hebbian learning, equilibrium propagation and coupled learning, is compatible with non-infinitesimal nudging by default. This method infers two sets of oppositely nudged and mutually tethered states simultaneously."
- Break condition: The stability breaks when α ≠ 1/2 in the original DP formulation, causing divergence in the inference step.

### Mechanism 2
- Claim: The choice of α parameter affects adversarial robustness of the learned model.
- Mechanism: Lower α values increase the effective adversarial step size, leading to models with smaller Lipschitz constants and improved robustness to input perturbations.
- Core assumption: The network potential LDP_α implicitly performs adversarial training on hidden activations, not just inputs.
- Evidence anchors:
  - [section]: "Section 4.3 suggests that the choice of α matters in particular in the strong feedback setting (i.e. β is not close to zero). We ran experiments on MNIST and FashionMNIST... a lower value of α yields a significantly smaller Lipschitz constant (as well as improved test accuracy) for the DP method."
  - [section]: "One may also speculate that the hypothesized NGRAD-based learning in biological systems gains some robustness by leveraging a similar mechanism."
- Break condition: When β → 0 (weak feedback), the differences between α values vanish as all methods approximate backpropagation.

### Mechanism 3
- Claim: DP⊤ provides stable learning for arbitrary α values by reformulating the underlying Lagrangian.
- Mechanism: By reparametrizing the Lagrangian in terms of s+ and s- states and deriving stationarity conditions, DP⊤ achieves the same update rules as DP when α = 1/2 but maintains stability for other α values through modified error signal handling.
- Core assumption: The activation functions are invertible with symmetric derivatives, allowing the reparametrization to work correctly.
- Evidence anchors:
  - [abstract]: "Second, we demonstrate how dual propagation is related to a particular adjoint state method, which is stable regardless of asymmetric nudging."
  - [section]: "Our starting point is a modified version of LeCun's classic Lagrangian-based derivation of backpropagation... The update equations in (19) coincide with the original dual propagation rules if α = 1/2 [20], although the underlying objectives LDP_α^⊤ (18) and LDP_α are fundamentally different."
- Break condition: If activation functions lack symmetric derivatives or are not invertible, the derivation fails and stability cannot be guaranteed.

## Foundational Learning

- Concept: Energy-based models and inference via minimization
  - Why needed here: DP and related methods rely on finding neural states that minimize a network potential, analogous to Hopfield networks
  - Quick check question: What is the difference between the free phase and clamped phase in contrastive Hebbian learning?

- Concept: Lagrangian duality and saddle-point optimization
  - Why needed here: The theoretical foundation derives DP through repeated applications of optimal value reformulations and saddle-point relaxations
  - Quick check question: How does the saddle-point ROVR differ from the standard ROVR in bilevel optimization?

- Concept: Lipschitz continuity and adversarial robustness
  - Why needed here: The experiments demonstrate that α affects the Lipschitz constant of the learned model, connecting DP to adversarial training
  - Quick check question: Why does a smaller Lipschitz constant generally imply better adversarial robustness?

## Architecture Onboarding

- Component map: DP neurons consist of two compartments (s+ and s-) receiving bottom-up input and nudged by top-down error signals. The network maintains a parameter α controlling the weighting between compartments. Weight updates are computed from the difference and weighted mean of compartment states.
- Critical path: 1) Initialize network with random weights, 2) For each training example, perform inference to find neural states s± for all layers, 3) Compute weight gradients using ∂Wk LDP_α ∝ (s+k+1 - s-k+1)s̄⊤k, 4) Update weights using optimizer (typically Adam), 5) Repeat until convergence.
- Design tradeoffs: Using α = 1/2 provides stability but may limit robustness; lower α values improve robustness but require more careful hyperparameter tuning. The original DP is computationally efficient but unstable for α ≠ 1/2, while DP⊤ is more stable but computationally more expensive.
- Failure signatures: Training divergence (NaN or exploding gradients) when using original DP with α ≠ 1/2; poor convergence when β is too large; no improvement over standard backpropagation when β is too small.
- First 3 experiments:
  1. Train a simple MLP on MNIST using DP with α = 1/2 and β = 0.01, compare accuracy to standard backpropagation
  2. Repeat experiment with α = 0 and observe if training diverges (should with original DP, should not with DP⊤)
  3. Measure Lipschitz constant of trained models for different α values and verify the relationship between α and robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does asymmetric nudging (α ≠ 1/2) affect the biological plausibility of the dual propagation algorithm in real neural systems?
- Basis in paper: [explicit] The paper discusses that dual propagation is compatible with non-infinitesimal nudging but relies on symmetric nudging for convergence, which may be restrictive in biological systems.
- Why unresolved: The paper only mentions the theoretical implications of asymmetric nudging but does not provide empirical evidence or biological experiments to support or refute its feasibility in real neural systems.
- What evidence would resolve it: Experimental results showing the performance and stability of dual propagation with asymmetric nudging in biological neural network models or neuromorphic hardware.

### Open Question 2
- Question: Can the improved variant of dual propagation (DP⊤) be extended to handle non-invertible activation functions more efficiently?
- Basis in paper: [explicit] The paper mentions that DP⊤ handles non-invertible activation functions by converting constraints into linear subspaces, but this approach may be computationally expensive.
- Why unresolved: The paper does not explore alternative methods or optimizations for handling non-invertible activation functions in DP⊤.
- What evidence would resolve it: Comparative studies demonstrating the computational efficiency and accuracy of DP⊤ with non-invertible activation functions using different constraint handling techniques.

### Open Question 3
- Question: What is the impact of using different values of β (feedback parameter) on the adversarial robustness of models trained with dual propagation?
- Basis in paper: [explicit] The paper suggests that the choice of α matters in the strong feedback setting (i.e., β is not close to zero) and discusses the relationship between α and adversarial robustness.
- Why unresolved: The paper does not provide a comprehensive analysis of how varying β affects the adversarial robustness of models trained with dual propagation.
- What evidence would resolve it: Empirical studies comparing the adversarial robustness of models trained with different β values using standard adversarial attack benchmarks.

## Limitations

- The improvement over standard DP (α=1/2) in terms of accuracy is modest, with the most significant advantage being robustness to adversarial perturbations rather than raw performance.
- The theoretical framework assumes activation functions with symmetric derivatives and invertibility, which may not hold for all practical neural network architectures.
- The computational overhead of DP⊤ compared to original DP is not thoroughly characterized in the paper.

## Confidence

- **High confidence**: The theoretical derivation connecting DP to adjoint state methods is rigorous and well-supported by mathematical proofs in the appendix.
- **Medium confidence**: The empirical results showing improved robustness with lower α values are convincing, though the sample size of experiments is relatively small.
- **Medium confidence**: The claim that DP⊤ is more stable than original DP for asymmetric nudging is supported by experiments, but the computational overhead of DP⊤ is not thoroughly characterized.

## Next Checks

1. **Scale experiments to larger architectures**: Test DP⊤ on ResNet-style networks with more than 10 layers to verify scalability and stability improvements hold for deeper networks.
2. **Compare against explicit adversarial training**: Directly compare models trained with DP⊤ (low α) against models trained with explicit adversarial training methods like PGD to isolate the contribution of the nudging mechanism to robustness.
3. **Analyze computational overhead**: Measure and compare the training time per epoch for DP, DP⊤, and standard backpropagation across different network sizes to quantify the practical trade-offs of using these contrastive Hebbian methods.