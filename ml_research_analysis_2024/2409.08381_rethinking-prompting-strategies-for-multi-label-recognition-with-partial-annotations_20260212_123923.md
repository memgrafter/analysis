---
ver: rpa2
title: Rethinking Prompting Strategies for Multi-Label Recognition with Partial Annotations
arxiv_id: '2409.08381'
source_url: https://arxiv.org/abs/2409.08381
tags:
- negative
- prompt
- positive
- class
- clip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of negative prompts in
  vision-language models for multi-label recognition with partial annotations. The
  authors propose two setups, PositiveCoOp and NegativeCoOp, to analyze the impact
  of positive and negative prompt learning separately.
---

# Rethinking Prompting Strategies for Multi-Label Recognition with Partial Annotations

## Quick Facts
- arXiv ID: 2409.08381
- Source URL: https://arxiv.org/abs/2409.08381
- Authors: Samyak Rawlekar; Shubhang Bhatnagar; Narendra Ahuja
- Reference count: 40
- Primary result: Negative prompts degrade MLR performance; learning only positive prompts with learned negative embeddings (PositiveCoOp) outperforms dual prompt learning approaches.

## Executive Summary
This paper investigates the effectiveness of negative prompts in vision-language models for multi-label recognition with partial annotations. The authors propose two setups, PositiveCoOp and NegativeCoOp, to analyze the impact of positive and negative prompt learning separately. They find that learning only positive prompts while using learned negative embeddings (PositiveCoOp) consistently outperforms dual prompt learning approaches, indicating that negative prompts degrade performance. The authors also propose a simple vision-features-only baseline that achieves strong performance comparable to dual prompt learning when the proportion of available labels is high, while requiring significantly fewer parameters and less computation. The study suggests that the lack of negative captions in the training data for vision-language models is likely the reason for the ineffectiveness of negative prompts.

## Method Summary
The paper addresses multi-label recognition (MLR) with partial annotations using CLIP-based vision-language models. Three setups are proposed: a vision-features-only baseline, PositiveCoOp (positive prompts with CLIP guidance, learned negative embeddings), and NegativeCoOp (negative prompts with CLIP guidance, learned positive embeddings). The CLIP visual and text encoders are frozen, and only prompts or embeddings are trained using Asymmetric Loss (ASL). Performance is evaluated on COCO 2014 and PASCAL VOC 2007 across varying percentages of available labels (10%-90%).

## Key Results
- Negative prompts degrade MLR performance compared to learning only positive prompts
- PositiveCoOp (positive prompts with learned negative embeddings) outperforms dual prompt learning approaches
- A vision-features-only baseline achieves comparable performance to DualCoOp when label availability is high (60-90%), requiring 15x fewer parameters and half the training compute

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP's text encoder fails to learn meaningful negative prompt embeddings because the training corpus lacks image-caption pairs explicitly describing the absence of objects.
- Mechanism: Without exposure to negative captions during pre-training, the text encoder associates similar visual features with both positive and negative prompts, leading to degraded performance when negative prompts are used for class absence detection.
- Core assumption: The effectiveness of negative prompts depends on the text encoder's ability to distinguish between presence and absence of objects in the shared embedding space.
- Evidence anchors:
  - [abstract] "we hypothesize that learning negative prompts may be suboptimal, as the datasets used to train VLMs lack image-caption pairs explicitly focusing on class absence."
  - [section] "We analyze the LAION-400M dataset [36], which comprises 400M image-text pairs derived using CLIP and has been used for training several Open Source VLMs such as OpenCLIP [21]. Our analysis of the 413,871,335 texts revealed that only 1,961,669 texts (0.47% of the total) contained negative words, confirming our hypothesis."
  - [corpus] Weak evidence - only one corpus analysis provided, no broader cross-dataset validation mentioned.
- Break condition: If a VLM were trained on a dataset with significant negative caption examples, the mechanism would fail as the text encoder would learn to distinguish presence/absence.

### Mechanism 2
- Claim: Learning only positive prompts while using learned negative embeddings (PositiveCoOp) outperforms dual prompt learning approaches for MLR with partial annotations.
- Mechanism: By relying on CLIP's guidance only for positive prompts (which it can learn effectively from positive captions) while learning negative embeddings directly in feature space without text encoder guidance, the model avoids the degraded performance from poorly learned negative prompts while maintaining strong positive class detection.
- Core assumption: Positive prompts can be effectively learned using CLIP guidance while negative embeddings can be learned effectively without text encoder guidance.
- Evidence anchors:
  - [abstract] "we observe that negative prompts degrade MLR performance, and learning only positive prompts, combined with learned negative embeddings (PositiveCoOp), outperforms dual prompt learning approaches."
  - [section] "PositiveCoOp achieves the best performance among the prompting-based methods."
  - [corpus] No direct corpus evidence for this specific mechanism - evidence comes from experimental results only.
- Break condition: If the learned negative embeddings in feature space fail to capture class absence information effectively, this mechanism would fail.

### Mechanism 3
- Claim: A vision-features-only baseline can achieve strong performance comparable to dual prompt learning approaches when the proportion of missing labels is low.
- Mechanism: When most labels are available (60%-90%), the visual encoder alone contains sufficient information to recognize classes without requiring text encoder guidance, making the simpler baseline competitive while being more computationally efficient.
- Core assumption: The visual encoder of VLMs captures sufficient discriminative features for class recognition even without text guidance when most annotations are present.
- Evidence anchors:
  - [abstract] "we quantify the performance benefits that prompt-learning offers over a simple vision-features-only baseline, observing that the baseline displays strong performance comparable to dual prompt learning approach (DualCoOp), when the proportion of missing labels is low, while requiring half the training compute and 16 times fewer parameters"
  - [section] "Our vision-only baseline achieves comparable performance to DualCoOp while requiring approximately 15 times fewer training parameters and half the GPU hours across the two datasets."
  - [corpus] No corpus evidence - purely based on experimental results with specific datasets.
- Break condition: If the visual encoder fails to capture sufficient discriminative features for classes with limited visual distinctiveness, this mechanism would fail especially at lower label availability.

## Foundational Learning

- Concept: Multi-label recognition (MLR) and partial annotations
  - Why needed here: The paper addresses MLR specifically with partial annotations, where not all relevant classes are labeled for each image during training.
  - Quick check question: In MLR with partial annotations, if an image contains a dog and a cat but is only labeled with "dog," what happens to the "cat" label during training?

- Concept: Vision-language models (VLMs) and CLIP architecture
  - Why needed here: The paper uses CLIP's vision and text encoders as the foundation for its approach, freezing them and using prompt learning on top.
  - Quick check question: In CLIP, what is the purpose of the shared embedding space between the vision and text encoders?

- Concept: Prompt learning and embedding space manipulation
  - Why needed here: The paper's core contribution involves learning prompts whose embeddings in the shared vision-language space correspond to class presence or absence.
  - Quick check question: In CLIP's embedding space, how is the similarity between an image feature and a text prompt feature interpreted?

## Architecture Onboarding

- Component map:
  - Frozen CLIP visual encoder (Gimg) extracts image features
  - Frozen CLIP text encoder (Gtext) generates text embeddings from prompts
  - Linear projector layer (Î¦) for baseline projects visual features to presence/absence logits
  - Learnable prompts for PositiveCoOp and NegativeCoOp maximize similarity with image features
  - Learnable embeddings in feature space for PositiveCoOp and NegativeCoOp learned without text encoder guidance
  - Asymmetric loss (ASL) training objective handles class imbalance

- Critical path:
  1. Extract image features using frozen Gimg
  2. For PositiveCoOp: Generate positive prompt embeddings with Gtext, learn negative embeddings directly
  3. For NegativeCoOp: Generate negative prompt embeddings with Gtext, learn positive embeddings directly
  4. Compute cosine similarities between image features and prompt/embeddings
  5. Apply class-specific region aggregation
  6. Train with ASL loss

- Design tradeoffs:
  - Using only visual encoder (baseline) vs. incorporating text guidance
  - Learning negative prompts with text guidance vs. learning negative embeddings directly
  - Computational efficiency vs. potential performance gains from text guidance

- Failure signatures:
  - Poor performance when label availability is very low (baseline approach)
  - Degraded performance when using negative prompts (NegativeCoOp)
  - Overfitting when training with very few labels
  - Instability during training if learning rates for prompts and embeddings are mismatched

- First 3 experiments:
  1. Implement baseline using only Gimg and compare performance with DualCoOp across different label availability percentages
  2. Implement PositiveCoOp and NegativeCoOp separately to isolate the impact of positive vs. negative prompt guidance
  3. Analyze cosine similarity between positive and negative prompt embeddings for each class to verify the hypothesis about text encoder limitations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance gap between PositiveCoOp and DualCoOp increase as the proportion of missing labels decreases, and is there a threshold where Baseline becomes the optimal choice?
- Basis in paper: [inferred] The paper shows Baseline performs comparably to DualCoOp when label availability is high (60-90%), and PositiveCoOp outperforms DualCoOp overall.
- Why unresolved: The paper doesn't systematically compare performance gaps across different label availability thresholds.
- What evidence would resolve it: A detailed ablation study showing performance trends of all methods (Baseline, PositiveCoOp, NegativeCoOp, DualCoOp) across a finer-grained range of label availability percentages.

### Open Question 2
- Question: Would using a VLM trained on more diverse negative caption data (not just LAION-400M) make negative prompt learning effective for MLR?
- Basis in paper: [explicit] The paper attributes the ineffectiveness of negative prompts to the lack of negative captions in LAION-400M training data.
- Why unresolved: The paper only analyzes LAION-400M and doesn't test alternative training datasets or VLM architectures.
- What evidence would resolve it: Experiments using VLMs trained on datasets with explicit negative captions (e.g., datasets containing "not a dog" type descriptions) to see if negative prompt learning becomes effective.

### Open Question 3
- Question: How does the performance of PositiveCoOp compare to DualCoOp++ (the extension mentioned but not directly tested), and does PositiveCoOp benefit from the additional components in DualCoOp++?
- Basis in paper: [explicit] The paper mentions DualCoOp++ as an extension but doesn't compare due to lack of code and additional components unrelated to negative prompting.
- Why unresolved: Direct empirical comparison is missing between PositiveCoOp and DualCoOp++.
- What evidence would resolve it: Implementing and testing PositiveCoOp with the additional components from DualCoOp++ to isolate the impact of positive prompt learning versus other architectural changes.

## Limitations

- Analysis based on single text corpus (LAION-400M) without broader cross-dataset validation
- Vision-features-only baseline performance may be dataset-specific and not generalize to all MLR tasks
- Study focuses on CLIP-based models without exploring other vision-language architectures

## Confidence

- High confidence: Experimental findings showing PositiveCoOp outperforms dual prompt learning approaches
- Medium confidence: Hypothesis about negative caption absence in training data, limited by single corpus analysis
- Medium confidence: Vision-features-only baseline performance claims, as they depend heavily on specific dataset characteristics

## Next Checks

1. Analyze additional text corpora used for training vision-language models to verify the prevalence of negative captions across different datasets
2. Test the vision-features-only baseline on additional multi-label recognition datasets with varying visual complexity and class characteristics
3. Evaluate the proposed approaches with other vision-language models beyond CLIP to assess architecture-specific effects