---
ver: rpa2
title: 'SemiKong: Curating, Training, and Evaluating A Semiconductor Industry-Specific
  Large Language Model'
arxiv_id: '2411.13802'
source_url: https://arxiv.org/abs/2411.13802
tags:
- semiconductor
- semikong
- https
- process
- semanticscholar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SemiKong is a semiconductor-industry-specific large language model
  (LLM) developed to address the limitations of general-purpose LLMs in semiconductor
  manufacturing and design tasks. It leverages a comprehensive semiconductor corpus
  and fine-tuning on domain-specific tasks, particularly etching problems.
---

# SemiKong: Curating, Training, and Evaluating A Semiconductor Industry-Specific Large Language Model

## Quick Facts
- arXiv ID: 2411.13802
- Source URL: https://arxiv.org/abs/2411.13802
- Authors: Christopher Nguyen; William Nguyen; Atsushi Suzuki; Daisuke Oku; Hong An Phan; Sang Dinh; Zooey Nguyen; Anh Ha; Shruti Raghavan; Huy Vo; Thang Nguyen; Lan Nguyen; Yoshikuni Hirayama
- Reference count: 40
- Primary result: SemiKong outperforms larger general-purpose LLMs and commercial products in semiconductor-specific tasks

## Executive Summary
SemiKong is a semiconductor-industry-specific large language model developed to address the limitations of general-purpose LLMs in semiconductor manufacturing and design tasks. The model leverages a comprehensive semiconductor corpus and fine-tuning on domain-specific tasks, particularly etching problems. Through extensive evaluation, SemiKong demonstrates state-of-the-art performance, making it a suitable foundation for company- or tool-specific proprietary models in the semiconductor domain.

## Method Summary
The method involves curating a comprehensive corpus of semiconductor-related texts, pre-training a base LLM (Llama3) on this domain-specific data, and then performing supervised fine-tuning on semiconductor-related instructions. The approach includes tokenization using Tiktoken and RoPE for positional embedding, followed by post-training processes like quantization and LoRA merging. Evaluation uses an expert-in-the-loop framework where domain experts score model responses and provide justifications that inform evaluation criteria for LLM assessment.

## Key Results
- SemiKong outperforms larger general-purpose LLMs (Llama3 70B) and commercial products (Claude, GPT-3.5) in semiconductor-specific tasks
- The model demonstrates superior performance in evaluation metrics including practicality, logical flow, expert-to-expert communication, and use of examples
- Pretraining+SFT approach shows significant improvement over SFT-only fine-tuning, validating the domain-specific pretraining strategy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific pretraining improves semiconductor knowledge over generic LLMs.
- Mechanism: Pre-training on a curated semiconductor corpus (books, papers, patents) allows the model to learn specialized terminology, processes, and domain-specific patterns before fine-tuning.
- Core assumption: Generic pre-trained models lack deep semiconductor domain knowledge, so additional pretraining is necessary to build expert-level understanding.
- Evidence anchors:
  - [abstract] "curating a comprehensive corpus of semiconductor-related texts" and "creating a foundational model with in-depth semiconductor knowledge"
  - [section 4.1] "We introduce a text-based dataset focused on semiconductors, extracted from technical books, papers, and patents"
  - [corpus] Found 25 related papers, average neighbor FMR=0.481, indicating moderate domain relatedness
- Break condition: If the curated corpus doesn't contain sufficient domain-specific terminology or expert-level content, pretraining won't yield meaningful knowledge improvements.

### Mechanism 2
- Claim: Expert-in-the-loop evaluation framework produces more reliable benchmarks than pure LLM evaluation.
- Mechanism: Domain experts score model responses and provide detailed justifications, which are then analyzed to create evaluation criteria that guide LLM evaluators toward expert-level assessments.
- Core assumption: Pure LLM evaluation assumes inherent understanding, but complex semiconductor problems require human expertise for accurate judgment.
- Evidence anchors:
  - [section 4.3] "experts review the initial answers generated by LLMs" and "experts, who possess extensive knowledge in their fields, not only provide correct answers but also evaluate the quality of other answers"
  - [abstract] "introducing a framework for integrating expert knowledge, thereby advancing the evaluation process of domain-specific AI models"
  - [corpus] Limited evidence - no specific evaluation framework papers found in corpus
- Break condition: If expert feedback doesn't provide sufficiently detailed justifications or if the criteria synthesis process fails to capture nuanced expert judgment.

### Mechanism 3
- Claim: Fine-tuning on semiconductor-specific instructions enables task performance beyond knowledge alone.
- Mechanism: After pretraining builds domain knowledge, supervised fine-tuning on semiconductor-related questions and problems teaches the model how to apply that knowledge to practical tasks like etching problems and manufacturing optimization.
- Core assumption: Knowledge alone is insufficient; the model needs task-specific training to perform domain-relevant operations effectively.
- Evidence anchors:
  - [section 4.2] "fine-tuning enables them to perform the tasks we anticipate, such as question-answering, dialogue, and reasoning"
  - [abstract] "fine-tuning a pre-trained LLM using our curated dataset, we have shown that SemiKong outperforms larger, general-purpose LLMs"
  - [section 5.2] Experimental comparison showing SFT-only models don't improve over base Llama3, but pretraining+SFT does
- Break condition: If the instruction dataset doesn't cover the range of tasks needed or if the fine-tuning process doesn't properly align the model's knowledge with practical applications.

## Foundational Learning

- Concept: Domain-specific corpus curation
  - Why needed here: Generic LLMs lack the specialized terminology, processes, and knowledge structures needed for semiconductor manufacturing tasks
  - Quick check question: What types of documents would you include in a semiconductor domain corpus to ensure comprehensive coverage?

- Concept: Pretraining vs Fine-tuning distinction
  - Why needed here: Pretraining builds general domain knowledge while fine-tuning adapts the model to specific tasks and applications
  - Quick check question: Why does the paper show that fine-tuning only (without pretraining) doesn't improve performance over base models?

- Concept: Expert-in-the-loop evaluation methodology
  - Why needed here: Standard LLM evaluation assumes inherent understanding, but semiconductor domain requires expert judgment for complex problems
  - Quick check question: How does the expert feedback loop improve evaluation reliability compared to pure LLM scoring?

## Architecture Onboarding

- Component map: Corpus curation → tokenization → RoPE embedding → pretraining → instruction dataset → SFT → post-processing → expert scoring → criteria synthesis → LLM evaluation
- Critical path: Pretraining → Fine-tuning → Evaluation, with each stage building on the previous one
- Design tradeoffs: Larger models (70B) show better performance but require more resources; 8B models are more practical but less capable
- Failure signatures: Poor pretraining corpus quality leads to weak domain knowledge; inadequate fine-tuning causes task performance issues; weak evaluation criteria produce unreliable benchmarks
- First 3 experiments:
  1. Compare base Llama3 8B vs SemiKong 8B (SFT only) vs SemiKong 8B (Pretraining+SFT) to validate pretraining contribution
  2. Scale to 70B parameters to assess size vs performance tradeoff
  3. Compare SemiKong 70B against commercial products (Claude, GPT-3.5) across all evaluation metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SemiKong's performance scale with model size beyond 70B parameters?
- Basis in paper: [inferred] The paper only tested SemiKong models up to 70B parameters and showed improvements over smaller models, but did not explore larger sizes.
- Why unresolved: The authors only evaluated models up to 70B parameters, leaving open whether further scaling would yield additional performance gains in semiconductor tasks.
- What evidence would resolve it: Experiments comparing SemiKong performance across a range of model sizes (e.g., 8B, 70B, 175B, 540B) on the same semiconductor benchmarks would show whether scaling laws continue to apply in this domain-specific context.

### Open Question 2
- Question: How transferable is SemiKong's ontology-based approach to other specialized industries?
- Basis in paper: [explicit] The authors state their ontology "is invaluable not only for building specialized AI models, like SemiKong for etching, but also serves as a benchmark for evaluating future general intelligence models" and suggest the approach "can be adapted and expanded to other industries."
- Why unresolved: While the paper demonstrates success in the semiconductor domain, it doesn't empirically test whether the ontology-driven methodology works for other technical fields.
- What evidence would resolve it: Implementing and validating the same ontology-building and model-training pipeline in at least two other specialized industries (e.g., aerospace manufacturing, pharmaceutical development) with comparable performance improvements would demonstrate transferability.

### Open Question 3
- Question: What is the long-term performance drift of SemiKong when exposed to evolving semiconductor manufacturing processes?
- Basis in paper: [inferred] The paper presents SemiKong as a "foundation model" but doesn't address how well it adapts to changes in semiconductor technology over time or whether it requires periodic retraining.
- Why unresolved: The semiconductor industry continuously evolves with new processes and materials, but the paper doesn't investigate model longevity or adaptation mechanisms.
- What evidence would resolve it: Longitudinal studies tracking SemiKong's performance on semiconductor tasks over multiple years, including experiments with continuous learning approaches versus periodic full retraining, would reveal how well the model maintains relevance in a rapidly changing field.

## Limitations

- Corpus Composition and Domain Coverage: Limited detail on corpus size, diversity, and potential biases in source selection, with only 25 related papers found in literature corpus
- Evaluation Framework Validation: Lack of transparency about expert qualifications, inter-rater reliability, and whether synthesized criteria truly capture domain expertise
- Generalization Beyond Etching: Exceptional performance primarily demonstrated on etching problems, raising questions about effectiveness across other semiconductor manufacturing domains

## Confidence

**High Confidence Claims**:
- Domain-specific pretraining on semiconductor corpus improves performance over generic LLMs (supported by experimental comparisons)
- SemiKong outperforms larger general-purpose models in semiconductor-specific tasks (demonstrated through benchmark results)
- Expert-in-the-loop evaluation provides valuable feedback beyond pure LLM scoring (mechanism described clearly)

**Medium Confidence Claims**:
- The expert-synthesized evaluation criteria accurately capture semiconductor domain expertise (plausible mechanism but limited validation evidence)
- Pretraining+SFT approach significantly outperforms SFT-only fine-tuning (supported by results but with limited ablation studies)
- SemiKong serves as suitable foundation for proprietary company-specific models (logical inference from results but not directly tested)

**Low Confidence Claims**:
- SemiKong achieves "state-of-the-art" performance across all semiconductor domains (only etching problems demonstrated)
- The evaluation framework represents a generalizable methodology for other technical domains (not validated beyond semiconductor context)

## Next Checks

1. **Ablation Study on Pretraining Contribution**: Conduct controlled experiments comparing (a) base Llama3, (b) SFT-only fine-tuning, and (c) pretraining+SFT across multiple semiconductor subdomains (not just etching) to quantify pretraining's marginal contribution to performance improvements.

2. **Expert Agreement Analysis**: Perform statistical analysis of inter-rater reliability among domain experts, including Fleiss' kappa or similar metrics, to validate that the expert feedback is consistent and not dominated by individual biases or preferences.

3. **Cross-Domain Generalization Test**: Evaluate SemiKong on semiconductor tasks outside the etching domain (e.g., lithography process optimization, packaging defect detection, test equipment programming) to assess whether the model's strong performance generalizes beyond the demonstrated use case.