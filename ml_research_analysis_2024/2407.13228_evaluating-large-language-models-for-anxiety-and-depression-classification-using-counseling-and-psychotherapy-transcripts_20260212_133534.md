---
ver: rpa2
title: Evaluating Large Language Models for Anxiety and Depression Classification
  using Counseling and Psychotherapy Transcripts
arxiv_id: '2407.13228'
source_url: https://arxiv.org/abs/2407.13228
tags:
- health
- mental
- language
- classification
- transcripts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the performance of large language models (LLMs)
  and traditional machine learning approaches for classifying anxiety and depression
  from long psychotherapy transcripts. The researchers fine-tuned transformer models
  (BERT, RoBERTa, Longformer, Mistral-7B), trained an SVM with feature engineering,
  and tested GPT models through prompting.
---

# Evaluating Large Language Models for Anxiety and Depression Classification using Counseling and Psychotherapy Transcripts

## Quick Facts
- arXiv ID: 2407.13228
- Source URL: https://arxiv.org/abs/2407.13228
- Reference count: 0
- Primary result: State-of-the-art DL models achieved weighted F1 ~0.5 and accuracy ~0.55, not significantly better than SVM baseline (F1: 0.485, accuracy: 0.549) or human baseline

## Executive Summary
This study evaluates the performance of large language models (LLMs) and traditional machine learning approaches for classifying anxiety and depression from long psychotherapy transcripts. The researchers fine-tuned transformer models (BERT, RoBERTa, Longformer, Mistral-7B), trained an SVM with feature engineering, and tested GPT models through prompting. They found that state-of-the-art DL models did not significantly outperform traditional ML methods or human baselines on this task. The best models achieved weighted F1 scores around 0.5 and accuracy around 0.55, which were not substantially better than the SVM baseline or human baseline. The study suggests that traditional ML methods with careful feature engineering may be more reliable and interpretable for this type of psychiatric diagnosis task using long, complex conversational text.

## Method Summary
The study used 3,503 de-identified therapy session transcripts from Alexander Street Press, with preprocessing to remove non-ASCII characters, descriptive elements, and symptom words. Researchers fine-tuned transformer models (BERT, RoBERTa, Longformer with 512 token truncation, Mistral-7B with 8192 token truncation), trained an SVM baseline with feature engineering using BoW and psychological dictionaries, and tested GPT models via prompting. All models were evaluated using weighted F1 score, accuracy, and weighted AUROC metrics on an 80/20 train/test split. The SVM used normalized stemming BoW features plus psychological dictionary features (concreteness scores, basic emotions), while transformers were fine-tuned on the classification task.

## Key Results
- State-of-the-art DL models achieved weighted F1 ~0.5 and accuracy ~0.55
- SVM baseline performed comparably (F1: 0.485, accuracy: 0.549)
- Human baseline achieved F1: 0.529, accuracy: 0.490 from 100 annotated transcripts
- No significant performance difference between transformer models and traditional ML approaches

## Why This Works (Mechanism)
The study demonstrates that traditional ML methods with careful feature engineering can match or exceed the performance of complex transformer models on specific classification tasks involving long, conversational text. The SVM's success appears to stem from its use of psychological dictionary features and BoW representations that capture clinically relevant linguistic patterns, while transformers struggle with the scattered nature of psychiatric symptoms across lengthy transcripts.

## Foundational Learning
- **Psychiatric symptom distribution**: Symptoms are often scattered throughout long therapy sessions rather than concentrated in specific segments, making it difficult for models to identify diagnostic patterns
  - Why needed: Understanding how symptoms manifest in conversational data is crucial for designing effective classification approaches
  - Quick check: Analyze symptom density and distribution across transcripts to identify patterns

- **Feature engineering for mental health**: Psychological dictionaries (concreteness, emotion/sentiment) can capture clinically relevant linguistic features that improve classification performance
  - Why needed: Traditional ML approaches can leverage domain-specific features that transformers may not learn effectively from raw text
  - Quick check: Compare performance of models with and without psychological dictionary features

- **Token truncation effects**: Truncating transcripts to 512-8192 tokens may remove critical diagnostic information, particularly for longer therapy sessions
  - Why needed: Understanding the impact of truncation on model performance helps optimize input processing
  - Quick check: Evaluate model performance on complete vs. truncated transcripts

## Architecture Onboarding

**Component Map:** Raw transcripts -> Preprocessing (cleaning, symptom removal) -> Feature extraction (BoW + dictionaries) -> SVM classification OR Fine-tuning (BERT/RoBERTa/Longformer/Mistral-7B) -> Classification output

**Critical Path:** Data preprocessing → Feature engineering (for SVM) or model fine-tuning (for transformers) → Classification → Evaluation

**Design Tradeoffs:** Traditional ML offers interpretability and strong performance with domain-specific features but may miss complex contextual patterns that transformers can capture. Transformers provide contextual understanding but struggle with long texts and require substantial computational resources.

**Failure Signatures:** Models perform similarly to baselines because psychiatric symptoms are distributed uniformly across transcripts rather than concentrated in specific segments. Large language models show instability or poor performance due to the complexity and subjectivity of psychiatric diagnosis from conversational text, particularly with scattered emotional content.

**Three First Experiments:**
1. Train SVM with different combinations of psychological dictionary features to identify which contribute most to performance
2. Fine-tune BERT with varying token truncation lengths (256, 512, 1024) to assess impact on classification accuracy
3. Compare GPT-3.5 and GPT-4 performance on the same classification task to evaluate improvements in few-shot prompting capabilities

## Open Questions the Paper Calls Out
**Open Question 1:** How do transformer-based models perform on multimodal mental health diagnostics that incorporate video data alongside text transcripts?
- Basis: The paper suggests incorporating video recordings could enhance diagnostics by providing a more comprehensive input space
- Why unresolved: Only text-based models were evaluated
- Resolution evidence: Comparative studies testing transformer models on both text-only and multimodal datasets (text + video) for mental health classification

**Open Question 2:** What specific features or patterns in long psychotherapy transcripts make them particularly challenging for current transformer architectures compared to traditional ML methods?
- Basis: The paper states that characteristics of lengthy texts render the classification task challenging for neural network-based models
- Why unresolved: While challenges are identified, specific problematic features are not detailed
- Resolution evidence: Detailed error analysis comparing transformer vs traditional ML performance across different transcript characteristics

**Open Question 3:** How would establishing a human expert baseline for the same dataset affect the interpretation of LLM performance gaps in mental health diagnostics?
- Basis: The paper suggests that creating a separate, expert human baseline could help investigate the current performance cap in human annotations
- Why unresolved: The study used non-expert human annotators
- Resolution evidence: Comparative performance metrics of LLMs versus expert psychiatrists on the same dataset

## Limitations
- Binary classification oversimplifies complex mental health presentations and may miss nuanced cases
- 80/20 train/test split without cross-validation raises concerns about generalizability and overfitting
- Pre-processing that removes symptom words and descriptive elements may eliminate critical diagnostic indicators
- Human baseline of only 100 transcripts provides a relatively small sample for comparison

## Confidence
- Medium Confidence: State-of-the-art DL models do not significantly outperform traditional ML methods or human baselines
- Medium Confidence: Traditional ML methods may be more reliable and interpretable for this task
- Low Confidence: Current approaches are fundamentally limited for psychiatric diagnosis from conversational text

## Next Checks
1. Replicate the human annotation process with a larger sample size (minimum 300-500 transcripts) and multiple annotators to establish a more robust baseline
2. Implement k-fold cross-validation and test models on an independent external dataset to assess generalizability
3. Conduct parallel analysis using complete transcripts without removing symptom words or descriptive elements to determine if preprocessing choices are limiting model performance