---
ver: rpa2
title: 'VEGA: Learning Interleaved Image-Text Comprehension in Vision-Language Large
  Models'
arxiv_id: '2406.10228'
source_url: https://arxiv.org/abs/2406.10228
tags:
- graph
- task
- figure
- iitc
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces VEGA, a new dataset for the Interleaved
  Image-Text Comprehension (IITC) task, designed to evaluate and improve Multi-modal
  Large Language Models'' (MLLMs) ability to handle complex comprehension scenarios
  involving long and interleaved image-text content. The VEGA dataset includes two
  subsets: one for IITC and another for the auxiliary Image-Text Association (ITA)
  task, with contexts up to 8,000 tokens and 8 images.'
---

# VEGA: Learning Interleaved Image-Text Comprehension in Vision-Language Large Models

## Quick Facts
- arXiv ID: 2406.10228
- Source URL: https://arxiv.org/abs/2406.10228
- Authors: Chenyu Zhou, Mengdan Zhang, Peixian Chen, Chaoyou Fu, Yunhang Shen, Xiawu Zheng, Xing Sun, Rongrong Ji
- Reference count: 40
- Key outcome: State-of-the-art MLLMs struggle with interleaved image-text comprehension; fine-tuning on VEGA achieves 85.8% image association accuracy and 0.508 ROUGE score

## Executive Summary
This paper introduces VEGA, a novel dataset designed to evaluate and improve Multi-modal Large Language Models' (MLLMs) ability to comprehend complex interleaved image-text content. The VEGA dataset presents the Interleaved Image-Text Comprehension (IITC) task, which challenges models to process long, multi-image contexts (up to 8,000 tokens and 8 images) and answer questions while explicitly referencing the correct image. The authors also create an auxiliary Image-Text Association (ITA) task to strengthen image-text correlation skills. Experimental results demonstrate that current MLLMs perform poorly on this task, but fine-tuning a Qwen-VL-Chat model on VEGA significantly improves performance.

## Method Summary
The authors constructed the VEGA dataset by collecting scientific papers from arXiv and creating interleaved image-text contexts with associated questions and answers. The IITC task requires models to comprehend long, complex contexts and answer questions while explicitly referencing the relevant image using "[Picture n]" notation. The ITA auxiliary task involves shuffled image-text pairs where models must correctly associate each image with its corresponding text. The training strategy employs both multi-task learning (IITC + ITA) and multi-scale training with progressively longer contexts and more images. The authors fine-tuned the Qwen-VL-Chat model on this dataset and evaluated performance using image association accuracy, ROUGE-L, and BLEU scores.

## Key Results
- Current state-of-the-art MLLMs show poor performance on IITC tasks, struggling particularly with following instructions and extracting key information from long interleaved contexts
- Fine-tuned Qwen-VL-Chat model achieves 85.8% image association accuracy on IITC task
- The model achieves 0.508 ROUGE score, demonstrating effective comprehension of complex multi-modal contexts
- Multi-scale training with progressively longer contexts and image numbers significantly improves model robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The VEGA dataset improves MLLMs' interleaved image-text comprehension by requiring explicit image reference in answers.
- Mechanism: The IITC task structure forces models to identify the correct image index corresponding to the question, which enhances the model's ability to locate relevant information within complex multi-modal contexts.
- Core assumption: Models can learn to associate images with specific textual contexts when trained with explicit reference requirements.
- Evidence anchors:
  - [abstract]: "This task challenges models to discern and disregard superfluous elements in both images and text to accurately answer questions and to follow intricate instructions to pinpoint the relevant image."
  - [section 3.1]: "We utilize the notation '[Picture n]' within the text to establish a clear reference to the n-th image, whether it appears in various sections of an article or across different examples."
  - [corpus]: Weak - corpus contains related work but no direct experimental evidence for this specific mechanism.
- Break condition: If models cannot learn the correlation between textual cues and image indices, the explicit reference requirement becomes meaningless.

### Mechanism 2
- Claim: Multi-scale training with progressively longer contexts improves model robustness to interleaved content.
- Mechanism: By training on datasets with increasing token lengths and image numbers, the model learns to handle more complex and lengthy interleaved image-text scenarios.
- Core assumption: Gradual increase in context complexity during training leads to better generalization to longer contexts.
- Evidence anchors:
  - [abstract]: "We introduce a multi-scale training strategy, constructing the training data with progressively increasing context lengths and image numbers to enhance the model's robustness against gradually increasing redundant image-text content."
  - [section 3.4]: "We design three textual scales for the training set, corresponding to the image caption Ci, the first mention paragraph Mi, and the expanded context paragraphs Ei."
  - [corpus]: Weak - related work mentions multi-scale approaches but not specifically for interleaved image-text tasks.
- Break condition: If the model cannot effectively utilize information from longer contexts, increasing context length may degrade performance.

### Mechanism 3
- Claim: The auxiliary ITA task strengthens image-text correlation skills, improving IITC performance.
- Mechanism: By training on shuffled image-text pairs and requiring correct associations, the model develops stronger ability to correlate visual and textual information.
- Core assumption: Improved image-text association skills from ITA task transfer to better performance on IITC task.
- Evidence anchors:
  - [abstract]: "We further craft a new VEGA dataset... and devised a subtask, Image-Text Association (ITA), to refine image-text correlation skills."
  - [section 3.1]: "ITA challenges the model with a shuffled array of inputs... The textual inputs are intentionally scrambled at the outset."
  - [section 4.3]: "Our experimental findings demonstrate that it achieves an image association accuracy rate of 85.8%."
- Break condition: If the skills learned in ITA don't transfer to IITC, the auxiliary task provides no benefit.

## Foundational Learning

- Concept: Multi-modal context processing
  - Why needed here: The IITC task requires understanding both visual and textual information simultaneously within complex, interleaved contexts.
  - Quick check question: Can you explain how interleaved image-text content differs from single-image tasks in terms of cognitive load?

- Concept: Attention mechanisms in transformer models
  - Why needed here: The model needs to effectively attend to relevant parts of long, interleaved contexts while ignoring distractions.
  - Quick check question: How would you modify self-attention to better handle interleaved image-text sequences?

- Concept: Cross-modal alignment
  - Why needed here: The model must align visual information with corresponding textual descriptions across potentially thousands of tokens.
  - Quick check question: What challenges arise when aligning images with text that are separated by many tokens in the input sequence?

## Architecture Onboarding

- Component map: Vision encoder → Cross-modal adapter → Frozen LLM backbone → Output head for text generation and image index prediction
- Critical path: Input tokenization → Visual feature extraction → Cross-modal fusion → Text generation with image reference
- Design tradeoffs: Longer context windows improve task performance but increase computational cost; multi-task training improves generalization but may slow convergence
- Failure signatures: Model consistently selects wrong image indices despite correct text answers; model fails to handle longer contexts despite success on shorter ones
- First 3 experiments:
  1. Test model performance on IITC task with varying context lengths to identify the maximum effective context window
  2. Compare multi-task training (IITC+ITA) vs single-task training on IITC to verify transfer learning benefits
  3. Evaluate different image reference formats (e.g., "[Picture n]" vs "Figure n") to determine most effective instruction-following approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific mechanisms or architectural modifications that would enable MLLMs to better handle interleaved image-text comprehension tasks?
- Basis in paper: [explicit] The paper identifies that current MLLMs struggle with IITC tasks, especially in following instructions and extracting key information from long, interleaved contexts. It mentions multi-task and multi-scale training strategies as effective, but doesn't detail specific architectural changes.
- Why unresolved: The paper focuses on dataset creation and baseline model training, but does not explore architectural innovations that could fundamentally improve MLLM performance on IITC.
- What evidence would resolve it: Comparative studies between different MLLM architectures (e.g., modifications to attention mechanisms, cross-modal fusion techniques) trained on VEGA, demonstrating which architectural features lead to significant improvements in IITC performance.

### Open Question 2
- Question: How does the VEGA dataset's focus on scientific content limit its generalizability to other domains with interleaved image-text content?
- Basis in paper: [explicit] The paper states that VEGA is "tailored for the IITC task on scientific content" and uses scientific papers from arXiv as its source. It does not discuss how well the dataset or trained models would perform on other types of interleaved image-text content.
- Why unresolved: The dataset construction and evaluation are solely focused on scientific literature, leaving open questions about its applicability to other domains like news articles, social media, or technical documentation.
- What evidence would resolve it: Evaluations of VEGA-trained models on interleaved image-text datasets from diverse domains, measuring performance degradation or improvement compared to domain-specific datasets.

### Open Question 3
- Question: What is the optimal balance between multi-task and multi-scale training strategies for improving MLLM performance on IITC tasks?
- Basis in paper: [explicit] The paper employs a multi-task (IITC + ITA) and multi-scale training strategy, showing improvements over single-task or single-scale approaches. However, it doesn't explore the individual contributions of each strategy or their optimal combination.
- Why unresolved: The ablation study shows that both strategies are beneficial, but doesn't investigate the trade-offs or interactions between them. It's unclear how much each strategy contributes to overall performance and whether there's an optimal combination.
- What evidence would resolve it: Systematic experiments varying the proportions of IITC and ITA tasks in multi-task training, and the number of scales in multi-scale training, to identify the combination that yields the best IITC performance on VEGA.

## Limitations

- The dataset construction methodology for the IITC subset is not fully specified, particularly regarding how the context expansion process works around first mention paragraphs
- Experimental results are limited to a single model (Qwen-VL-Chat) without comparisons to other contemporary MLLMs or ablation studies on the multi-scale training strategy
- The paper does not provide comprehensive error analysis to understand failure modes or discuss potential biases in the dataset construction from scientific literature

## Confidence

- **High Confidence**: The core claim that MLLMs struggle with interleaved image-text comprehension is well-supported by the experimental results showing low performance on the IITC task without fine-tuning.
- **Medium Confidence**: The claim that multi-scale training improves robustness is supported by the methodology description, but lacks direct experimental validation through ablation studies.
- **Low Confidence**: The generalizability of VEGA to other domains beyond scientific literature is not tested.

## Next Checks

1. **Ablation Study on Training Strategy**: Conduct experiments to isolate the effects of multi-scale training and multi-task learning by training models with: (a) single-scale training only, (b) single-task (IITC only) training, and (c) different context length progression schemes to verify the claimed benefits.

2. **Cross-Domain Evaluation**: Test the fine-tuned model on IITC-style tasks constructed from different domains (e.g., news articles, technical documentation, social media) to assess whether the improvements generalize beyond scientific literature and whether the dataset construction methodology needs adaptation for different content types.

3. **Alternative Image Reference Formats**: Experiment with different instruction-following approaches for image references, such as using "Figure n", "Image n", or natural language descriptions like "the diagram showing X", to determine whether the explicit "[Picture n]" format is necessary or if more flexible approaches could improve performance.