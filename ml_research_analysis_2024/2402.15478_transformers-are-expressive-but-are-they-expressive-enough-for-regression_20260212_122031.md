---
ver: rpa2
title: Transformers are Expressive, But Are They Expressive Enough for Regression?
arxiv_id: '2402.15478'
source_url: https://arxiv.org/abs/2402.15478
tags:
- transformer
- function
- approximation
- functions
- failure-rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Transformers have been widely adopted in NLP, with several works
  analyzing their expressivity. However, this work challenges the claim that Transformers
  are universal function approximators.
---

# Transformers are Expressive, But Are They Expressive Enough for Regression?

## Quick Facts
- arXiv ID: 2402.15478
- Source URL: https://arxiv.org/abs/2402.15478
- Authors: Swaroop Nath; Harshad Khadilkar; Pushpak Bhattacharyya
- Reference count: 40
- Primary result: Transformers cannot reliably approximate smooth functions, relying on piecewise constant approximations with large intervals

## Executive Summary
This work challenges the widely-held belief that Transformers are universal function approximators. Through theoretical analysis and controlled experiments, the authors demonstrate that Transformers struggle to reliably approximate smooth functions, instead relying on piecewise constant approximations with sizable intervals. The study reveals that while Transformers can handle piecewise constant functions with moderately large step sizes, they fail to adequately approximate smoothly changing functions due to exponentially growing layer requirements. This finding has significant implications for the use of Transformers in regression tasks where smooth function approximation is required.

## Method Summary
The authors conduct theoretical analysis combined with synthetic experiments to investigate transformer expressivity. They analyze the resolution factor δ required for adequate approximation and derive bounds showing that for smooth functions, the number of layers needed grows exponentially with 1/δ. Synthetic datasets are generated from equations covering logarithmic, exponential, and polynomial functions with various interactions. The Transformer architecture follows Vaswani et al. (2017) with PyTorch implementation. Hyperparameter tuning is performed on batch size, gradient accumulation, epochs, etc. Experiments are repeated 5 times to assess performance variation. Two main experiments are conducted: EXPT-I (Regression) for smooth function approximation and EXPT-II (Quantized Classification) for piecewise constant functions.

## Key Results
- Transformers show high failure-rates (~0.8) when approximating smooth functions in regression tasks, even with 15 layers
- For piecewise constant functions with step sizes ≥ 1, Transformers achieve lower failure-rates (~0.30) with the same architecture
- The resolution factor δ required for smooth functions grows too small, leading to exponential layer requirements that make practical approximation impossible
- More attention heads improve smooth function approximation but increase variance; higher embedding dimensions help piecewise constant functions more

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers cannot reliably approximate smooth functions because the resolution factor δ required for adequate approximation grows too small for smooth functions, leading to exponential layer requirements.
- Mechanism: For smooth functions, the resolution factor δ is bounded by the derivative of the function (Theorem 4.1). Smaller δ means larger number of layers needed (O(m(1/δ)^d·m)). For smooth functions with high derivatives, δ becomes very small, requiring exponentially many layers, making practical approximation impossible.
- Core assumption: The function to be approximated is smooth (has continuous first-order derivatives) over its compact support.
- Evidence anchors:
  - [abstract] "our findings reveal that Transformers struggle to reliably approximate smooth functions, relying on piecewise constant approximations with sizable intervals."
  - [section] "From Theorem 4.1, we understand that for a smoothly changing function, a small resolution factor is necessary, for an adequate approximation between f and f̃."
  - [corpus] Weak evidence - corpus neighbors focus on floating-point transformers and in-context learning, not smooth function approximation limitations.

### Mechanism 2
- Claim: Transformers can reliably approximate piecewise constant functions with moderately large step sizes because the resolution factor constraint is relaxed.
- Mechanism: For piecewise constant functions, the resolution factor δ is determined by the minimum step size between constant pieces. Large step sizes mean larger δ, which reduces the number of layers needed for approximation. This makes practical approximation feasible.
- Core assumption: The piecewise constant function has step sizes that are not too small (ideally ≥ 1).
- Evidence anchors:
  - [abstract] "we see that even with 15 layers, the failure-rate is very high (~0.8) for EXPT-I, as compared to ~0.30 for EXPT-II."
  - [section] "However, for piecewise constant functions with high step-sizes (≥ 1), Transformers can perform well."
  - [corpus] Weak evidence - corpus neighbors don't specifically address piecewise constant function approximation.

### Mechanism 3
- Claim: The number of attention heads and embedding dimensions affect transformer performance differently for smooth vs piecewise constant functions.
- Mechanism: More attention heads improve performance for smooth functions (EXPT-I) but not significantly for piecewise constant functions (EXPT-II). Higher embedding dimensions help piecewise constant functions more than smooth functions. This suggests different architectural requirements for different function types.
- Core assumption: The transformer architecture can be tuned to better handle one function type over another.
- Evidence anchors:
  - [section] "For EXPT-I, we see that the performance gradually becomes better [as attention heads increase]. However, the variation in performance also increases significantly. For EXPT-II, we see that the performance and deviation remains approximately the same."
  - [section] "From Figure 4b, we understand that Transformers perform best in EXPT-I for embedding dimension = 32, and in EXPT-II for embedding dimension = 128."
  - [corpus] No direct evidence in corpus neighbors about attention heads/embedding dimension effects.

## Foundational Learning

- Concept: Resolution factor and its relationship to function smoothness
  - Why needed here: Understanding how δ affects approximation quality is central to why transformers fail at smooth functions
  - Quick check question: If a function has a derivative of 2 everywhere, how does this affect the minimum resolution factor needed for adequate approximation?

- Concept: Universal function approximation theory and its limitations
  - Why needed here: The paper challenges existing claims about transformer expressivity by showing they're not universal function approximators for smooth functions
  - Quick check question: What's the key difference between the approximation capabilities of transformers for smooth vs piecewise constant functions?

- Concept: Transformer architecture components (self-attention, FFN, positional embeddings)
  - Why needed here: Different components affect performance differently for different function types, as shown in the experiments
  - Quick check question: How might increasing the number of attention heads affect the transformer's ability to approximate smooth functions?

## Architecture Onboarding

- Component map: Input → Positional Embeddings → Self-Attention → Feed-Forward Network → Repeat for multiple layers → Output
- Critical path: For function approximation, the critical path is: input → positional embeddings → self-attention → FFN → repeat for multiple layers → output. The self-attention mechanism is key to handling different input positions.
- Design tradeoffs: More layers help smooth function approximation but increase computational cost exponentially. More attention heads improve smooth function approximation but increase variance. Higher embedding dimensions help piecewise constant functions more than smooth functions.
- Failure signatures: High failure-rate in regression tasks (EXPT-I) but low failure-rate in quantized classification tasks (EXPT-II). Oscillatory trends in failure-rate vs embedding dimension for smooth functions. High standard deviation across runs for smooth function approximation.
- First 3 experiments:
  1. Train transformer to approximate a simple smooth function (like sin(x)) and measure failure-rate
  2. Train transformer to classify quantized versions of the same smooth function and compare performance
  3. Vary the number of transformer layers and observe how failure-rate changes for both smooth and piecewise constant approximations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications to Transformers could improve their ability to approximate smooth functions?
- Basis in paper: [inferred] The paper concludes that Transformers are bad at approximating smooth functions but suggests future work to modify the architecture to improve expressivity.
- Why unresolved: The paper does not propose or test any specific architectural modifications, only suggesting it as a direction for future work.
- What evidence would resolve it: Experimental results showing improved performance on smooth function approximation tasks after implementing specific architectural modifications to the Transformer.

### Open Question 2
- Question: How does the difficulty of approximating different types of smooth functions (e.g., logarithmic vs. exponential) vary for Transformers?
- Basis in paper: [inferred] The paper mentions in its limitations that it did not explore how Transformers perform on different types of functions, only focusing on smooth functions in general.
- Why unresolved: The paper's experiments and theoretical analysis do not differentiate between various types of smooth functions, treating them as a single category.
- What evidence would resolve it: Comparative experiments showing the performance of Transformers on a diverse set of smooth functions with different characteristics (e.g., logarithmic, exponential, polynomial).

### Open Question 3
- Question: What is the exact source of limited expressivity in Transformers when approximating smooth functions?
- Basis in paper: [explicit] The paper concludes that Transformers are bad at approximating smooth functions and suggests future work to pinpoint the source of this limited expressivity.
- Why unresolved: While the paper provides theoretical and experimental evidence of Transformers' limitations with smooth functions, it does not identify the specific mechanism or component responsible for this limitation.
- What evidence would resolve it: Detailed analysis of Transformer components (e.g., self-attention, feed-forward networks) and their interactions, showing which aspects specifically contribute to the difficulty in approximating smooth functions.

## Limitations

- Theoretical framework uncertainty: While the paper provides theoretical bounds on transformer expressivity through resolution factors, the practical implications of these bounds remain unclear.
- Experimental generalizability uncertainty: The experiments focus on synthetic functions generated from specific equations, and it's unclear how these results extend to real-world regression tasks.
- Architecture-specific limitations: The analysis is based on the standard transformer architecture from Vaswani et al. (2017), without exploring whether architectural modifications could overcome the identified limitations.

## Confidence

- Transformers cannot reliably approximate smooth functions: High Confidence
- Transformers can approximate piecewise constant functions with large step sizes: High Confidence
- Architectural components affect different function types differently: Medium Confidence

## Next Checks

1. **Practical Layer Requirement Analysis**: Conduct experiments to determine the minimum number of layers needed for transformers to achieve acceptable approximation quality (e.g., <5% error) on smooth functions with varying degrees of smoothness.

2. **Real-World Function Benchmark**: Test transformer approximation capabilities on a diverse set of real-world regression tasks with varying function characteristics (smoothness, dimensionality, noise levels).

3. **Architectural Modification Experiments**: Systematically test whether adding specific architectural components (e.g., adaptive depth, specialized smooth function layers, or modified attention mechanisms) can improve transformer performance on smooth functions without degrading performance on piecewise constant functions.