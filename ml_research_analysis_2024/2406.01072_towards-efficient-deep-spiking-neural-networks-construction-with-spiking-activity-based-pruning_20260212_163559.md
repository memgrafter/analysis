---
ver: rpa2
title: Towards Efficient Deep Spiking Neural Networks Construction with Spiking Activity
  based Pruning
arxiv_id: '2406.01072'
source_url: https://arxiv.org/abs/2406.01072
tags:
- network
- spiking
- pruning
- neural
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of compressing large-scale spiking
  neural networks (SNNs) to leverage their low-power consumption and biological interpretability
  advantages. The authors propose a structured pruning approach based on the activity
  levels of convolutional kernels, named the Spiking Channel Activity-based (SCA)
  network pruning framework.
---

# Towards Efficient Deep Spiking Neural Networks Construction with Spiking Activity based Pruning

## Quick Facts
- arXiv ID: 2406.01072
- Source URL: https://arxiv.org/abs/2406.01072
- Authors: Yaxin Li; Qi Xu; Jiangrong Shen; Hongming Xu; Long Chen; Gang Pan
- Reference count: 18
- Primary result: Proposes SCA framework that dynamically prunes and regenerates convolutional kernels based on spiking activity levels, achieving compression while maintaining performance

## Executive Summary
This paper addresses the challenge of compressing large-scale spiking neural networks (SNNs) to leverage their low-power consumption and biological interpretability advantages. The authors propose a structured pruning approach based on the activity levels of convolutional kernels, named the Spiking Channel Activity-based (SCA) network pruning framework. Inspired by synaptic plasticity mechanisms, their method dynamically adjusts the network's structure by pruning and regenerating convolutional kernels during training, enhancing the model's adaptation to the target task. The SCA framework maintains model performance while refining the network architecture, ultimately reducing computational load and accelerating the inference process.

## Method Summary
The SCA framework is a structured pruning approach that operates on convolutional channels rather than individual weights. It dynamically adjusts network structure by pruning low-activity channels and regenerating important ones based on spiking neuron activity scores computed from membrane potentials. The framework uses batch normalization gamma parameters to guide selective regrowth of pruned channels. During training, the method alternates between weight updates using gradient surrogate techniques and structural modifications (pruning and regrowth), enabling joint learning of parameters and network topology.

## Key Results
- Outperforms existing SNNs pruning techniques in terms of parameter compression and accuracy retention
- Maintains high performance while reducing computational load and accelerating inference process
- Demonstrates effectiveness across various datasets including CIFAR-10, CIFAR-100, DVS-CIFAR10, and Tiny-ImageNet

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Spiking Channel Activity-based (SCA) pruning dynamically identifies and removes redundant convolutional kernels while regenerating important ones based on their spiking activity levels.
- Mechanism: The method computes spiking neuron activity scores by averaging membrane potentials across time and samples, using these scores to rank channel importance. Channels with low activity are pruned, while those showing significant gradients in BN layer gamma parameters are selectively regrown.
- Core assumption: Channel importance can be reliably assessed through spiking activity levels, and removing low-activity channels preserves model performance while reducing computational cost.
- Evidence anchors:
  - [abstract] "The structural learning rule of this framework adheres to the mechanisms of synaptic plasticity, allowing for the removal of redundancies and regeneration of certain convolutional kernels based on spiking activity level."
  - [section] "Drawing inspiration from this biological mechanism, neurons with lower polarization levels can be considered less important, and their removal has a relatively minor impact on the network's accuracy."
  - [corpus] Weak evidence - the corpus neighbors discuss sparsity and pruning but don't specifically validate activity-based channel pruning as the primary mechanism.

### Mechanism 2
- Claim: Structured pruning at the channel level rather than individual connections maintains hardware compatibility while achieving compression benefits.
- Mechanism: The framework prunes entire convolutional kernels (channels) rather than individual weights, resulting in regular sparse patterns that are more amenable to hardware acceleration and memory optimization.
- Core assumption: Channel-level pruning provides sufficient compression benefits while being more hardware-friendly than unstructured pruning of individual connections.
- Evidence anchors:
  - [abstract] "most model compression techniques for SNNs are based on unstructured pruning of individual connections, which requires specific hardware support. Hence, we propose a structured pruning approach"
  - [section] "Non-structured pruning, being a fine-grained method, often requires specific hardware support. On the other hand, structured pruning typically operates at the channel or layer level, yielding more compact network structures."
  - [corpus] Moderate evidence - corpus includes papers on spatio-temporal pruning and weight sparsity, supporting the broader context of structured approaches.

### Mechanism 3
- Claim: The joint learning of network parameters and structure through alternating weight updates and structural modifications enables discovery of optimal sparse architectures.
- Mechanism: The framework alternates between weight learning phases (using gradient surrogate methods) and structure learning phases (pruning and regrowing channels), allowing the network to adapt its topology while maintaining performance.
- Core assumption: Alternating between parameter optimization and structural adaptation allows the network to converge to an optimal sparse configuration that wouldn't be found through static pruning approaches.
- Evidence anchors:
  - [section] "The SCA structure learning method simultaneously performs joint learning of both the network's weight parameters and its topological structure during the training process."
  - [section] "During the training process, we added L1 regularization to better identify redundant structures. Finally, a new model is obtained by completely removing redundant channels according to the mask."
  - [corpus] Weak evidence - corpus doesn't specifically address joint parameter-structure learning, though it mentions various pruning approaches.

## Foundational Learning

- Concept: Spiking Neural Networks and their unique training requirements
  - Why needed here: Understanding how SNNs differ from traditional ANNs is crucial for grasping why specialized pruning approaches are needed
  - Quick check question: How does the binary nature of spikes affect gradient computation in SNNs compared to continuous activations in ANNs?

- Concept: Surrogate gradient methods for training SNNs
  - Why needed here: The framework relies on gradient surrogate techniques to enable backpropagation through binary spike events
  - Quick check question: What function is typically used as a gradient surrogate for the non-differentiable spike function, and how is its smoothness controlled?

- Concept: Synaptic plasticity mechanisms in biological neural systems
  - Why needed here: The pruning and regrowth mechanism is inspired by biological synaptic plasticity, so understanding these biological principles is essential
  - Quick check question: How do biological neural networks strengthen, weaken, establish, or eliminate synaptic connections during learning and experience?

## Architecture Onboarding

- Component map: Input → Convolutional layers → Batch normalization → Spiking neurons → Activity computation → Channel importance scoring → Pruning decision → Weight updates → Regrowth decision → Output

- Critical path: The critical path for structure learning runs through activity computation and importance scoring to pruning decisions, where spiking neuron activity scores are computed, channels are ranked by importance, and pruning/regrowth decisions are made.

- Design tradeoffs: Structured pruning vs. unstructured pruning (regularity vs. compression ratio), dynamic vs. static pruning (adaptability vs. stability), joint vs. sequential parameter-structure learning (optimality vs. convergence speed)

- Failure signatures: Accuracy degradation exceeding acceptable thresholds, structural instability with oscillating channel counts, inability to converge to sparse architectures, or excessive computational overhead from frequent structural updates

- First 3 experiments:
  1. Implement basic SCA framework on a simple VGG-style network with CIFAR-10 to verify that channel importance scoring works and pruning occurs as expected
  2. Test the selective regrowth mechanism by examining whether channels with high BN gamma gradients are successfully reactivated after pruning
  3. Compare structured vs. unstructured pruning variants on the same network to quantify hardware-friendliness benefits and compression tradeoffs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed SCA framework's performance compare to other state-of-the-art structured pruning methods for SNNs on complex datasets like ImageNet?
- Basis in paper: [inferred] The paper compares the SCA framework to existing SNNs pruning methods on CIFAR-10, CIFAR-100, DVS-CIFAR10, and Tiny-ImageNet datasets. However, it does not mention performance on ImageNet.
- Why unresolved: The paper does not provide experimental results or comparisons on the ImageNet dataset.
- What evidence would resolve it: Experimental results comparing the SCA framework to other state-of-the-art structured pruning methods for SNNs on the ImageNet dataset.

### Open Question 2
- Question: How does the SCA framework perform when applied to other types of SNN architectures beyond convolutional networks, such as recurrent or graph-based SNNs?
- Basis in paper: [inferred] The paper focuses on applying the SCA framework to convolutional SNNs. It mentions that the framework can be applied to various convolutional network architectures, but does not explore other types of SNNs.
- Why unresolved: The paper does not provide experimental results or discussions on applying the SCA framework to non-convolutional SNN architectures.
- What evidence would resolve it: Experimental results demonstrating the performance of the SCA framework when applied to recurrent or graph-based SNN architectures on various datasets.

### Open Question 3
- Question: How does the SCA framework's performance and efficiency change when applied to larger-scale SNNs with more layers and neurons?
- Basis in paper: [inferred] The paper mentions that the SCA framework can be applied to deep convolutional SNNs. However, it does not provide information on the scalability of the framework or its performance on larger-scale SNNs.
- Why unresolved: The paper does not discuss the scalability of the SCA framework or provide experimental results on larger-scale SNNs.
- What evidence would resolve it: Experimental results demonstrating the performance and efficiency of the SCA framework when applied to larger-scale SNNs with more layers and neurons, along with a discussion on the scalability of the approach.

## Limitations
- The framework's effectiveness relies heavily on the assumption that spiking activity levels reliably indicate channel importance
- Scalability to larger, more complex SNN models remains untested and uncertain
- Computational overhead from dynamic pruning and regrowth operations may offset some efficiency gains

## Confidence

High confidence in the structured pruning approach being more hardware-friendly than unstructured methods. Medium confidence in the spiking activity-based channel importance scoring mechanism, as it's theoretically sound but lacks extensive empirical validation. Low confidence in the scalability and computational efficiency of the dynamic pruning/regrowth approach for larger SNN architectures.

## Next Checks

1. Conduct ablation studies to isolate the impact of each mechanism (pruning, regrowth, L1 regularization) on final performance
2. Test the framework on deeper SNN architectures (ResNet-style) to evaluate scalability
3. Measure and compare the actual inference speedup and memory savings achieved through the structured pruning approach