---
ver: rpa2
title: Formulations and scalability of neural network surrogates in nonlinear optimization
  problems
arxiv_id: '2412.11403'
source_url: https://arxiv.org/abs/2412.11403
tags:
- neural
- network
- gray-box
- optimization
- formulation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We study formulations for embedding trained neural networks in
  nonlinear optimization. Using a transient stability-constrained optimal power flow
  problem, we find that gray-box formulations using user-defined operators and GPU
  acceleration can solve with neural networks of up to 592 million parameters in 15
  seconds, compared to over 700 seconds with exact Hessians on CPU.
---

# Formulations and scalability of neural network surrogates in nonlinear optimization problems

## Quick Facts
- **arXiv ID**: 2412.11403
- **Source URL**: https://arxiv.org/abs/2412.11403
- **Reference count**: 24
- **Key outcome**: Gray-box formulation with GPU acceleration solves neural networks up to 592 million parameters in 15 seconds vs 700+ seconds for exact Hessians on CPU

## Executive Summary
This paper systematically compares three formulations for embedding trained neural networks as surrogates in nonlinear optimization problems: full-space, reduced-space, and gray-box approaches. Using a transient stability-constrained optimal power flow problem as a case study, the authors demonstrate that gray-box formulations using user-defined operators and GPU acceleration can handle neural networks with up to 592 million parameters in 15 seconds, compared to over 700 seconds with exact Hessians on CPU. The full-space formulation becomes computationally intractable due to KKT matrix factorization requirements, while the reduced-space formulation is limited by JuMP's expression management overhead rather than neural network size.

## Method Summary
The study benchmarks three formulations for embedding neural network surrogates in nonlinear optimization using a transient stability-constrained optimal power flow problem on a 37-bus synthetic power network. The full-space formulation explicitly represents all neural network intermediate variables and constraints, the reduced-space formulation creates a single dense nonlinear constraint, and the gray-box formulation treats the network as a black box using user-defined operators. Neural networks are trained as surrogates for high-fidelity simulations using sequential architectures with tanh activation. The IPOPT solver with MA27 linear solver is used for optimization, with automatic differentiation for gradients and Hessians. Performance is evaluated across different network architectures (2-20 layers, 50-4,000 neurons per layer) and configurations (exact vs approximate Hessians, CPU vs GPU acceleration).

## Key Results
- Gray-box formulation with GPU acceleration solves neural networks up to 592 million parameters in 15 seconds
- Full-space formulation becomes intractable due to KKT matrix factorization, spending 99%+ time in solver
- Gray-box formulation with Hessian approximation and GPU acceleration provides 2.5× overhead compared to base problem without stability constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The gray-box formulation outperforms full-space and reduced-space formulations for large neural networks due to efficient GPU-accelerated derivative evaluation
- Mechanism: The gray-box formulation treats the neural network as a "black box" using user-defined operators, avoiding the need to explicitly represent all intermediate variables and constraints in the algebraic modeling environment. This allows leveraging PyTorch's built-in GPU acceleration for neural network evaluation
- Core assumption: PyTorch's automatic differentiation is more efficient than JuMP's reverse-mode automatic differentiation for dense, nested, vector-valued neural network expressions
- Evidence anchors:
  - [abstract]: "gray-box formulations using user-defined operators and GPU acceleration can solve with neural networks of up to 592 million parameters in 15 seconds"
  - [section]: "The gray-box formulation is bottlenecked by evaluation of the neural network's outputs and their derivatives, which may be accelerated with a graphics processing unit (GPU)"
  - [corpus]: No direct evidence found - this is a novel contribution of the paper
- Break condition: If the neural network cannot be efficiently evaluated using PyTorch's automatic differentiation (e.g., custom operations not supported)

### Mechanism 2
- Claim: The full-space formulation becomes computationally intractable as neural network size increases due to KKT matrix factorization requirements
- Mechanism: The full-space formulation adds intermediate variables and constraints for each layer of the neural network, leading to a dramatic increase in problem size and KKT matrix density, making linear system solution prohibitively expensive
- Core assumption: The KKT matrix factorization cost dominates the solve time in interior point methods for problems with dense nonlinear constraints
- Evidence anchors:
  - [abstract]: "The full-space formulation is limited by KKT matrix factorization"
  - [section]: "The full-space formulation is bottlenecked by the linear solver used by the optimization algorithm"
  - [section]: Table 3 shows full-space formulation spends "99+%" of time in solver
- Break condition: If the linear solver used (MA27) cannot handle the memory requirements of the KKT matrix for very large networks

### Mechanism 3
- Claim: The reduced-space formulation is limited by JuMP's expression management overhead rather than neural network size
- Mechanism: The reduced-space formulation creates a single dense nonlinear constraint representing the entire neural network, but JuMP scalarizes vector-valued expressions, leading to inefficient representation and derivative computation
- Core assumption: JuMP's automatic differentiation system is not optimized for handling very large dense expressions efficiently
- Evidence anchors:
  - [abstract]: "The reduced-space formulation is limited by JuMP's expression management"
  - [section]: "The downside is that the nonlinear constraint is a complicated expression with a very large number of terms"
  - [section]: "JuMP scalarizes vector-valued expressions in nonlinear constraints"
- Break condition: If the expression size exceeds JuMP's ability to manage memory for derivative computations

## Foundational Learning

- Concept: Interior point methods and KKT matrix factorization
  - Why needed here: Understanding why full-space formulation scales poorly requires knowledge of how interior point methods solve nonlinear optimization problems
  - Quick check question: What are the components of the KKT matrix in an interior point method?

- Concept: Automatic differentiation in algebraic modeling environments
  - Why needed here: Comparing the three formulations requires understanding how derivatives are computed and the limitations of different AD systems
  - Quick check question: What is the difference between reverse-mode and forward-mode automatic differentiation?

- Concept: GPU acceleration for neural network evaluation
  - Why needed here: The performance advantage of gray-box formulation depends on understanding how GPUs accelerate neural network computations
  - Quick check question: What operations in neural network evaluation benefit most from GPU parallelization?

## Architecture Onboarding

- Component map: MathOptAI.jl (interface layer) → JuMP (algebraic modeling) → IPOPT (solver) ← user-defined operators (neural network evaluation)
- Critical path: Neural network evaluation → derivative computation → KKT matrix factorization → line search → convergence check
- Design tradeoffs: Gray-box formulation trades explicit constraint representation for computational efficiency; reduced-space formulation trades problem structure for simplicity
- Failure signatures: Memory errors (full-space), excessive solve times (reduced-space), slow convergence (gray-box with exact Hessians)
- First 3 experiments:
  1. Benchmark gray-box formulation with exact vs approximate Hessians on a small network
  2. Profile JuMP's expression building time vs neural network evaluation time
  3. Test GPU acceleration impact by comparing CPU vs GPU evaluation times for various network sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the gray-box formulation scale when using different Hessian approximation methods or more advanced quasi-Newton updates?
- Basis in paper: [explicit] The paper compares exact vs approximate Hessian in gray-box formulation and finds significant speedups with approximation, but does not explore different approximation methods.
- Why unresolved: The study only tests one type of Hessian approximation (limited-memory quasi-Newton) and does not compare against other potential methods like BFGS, SR1, or machine learning-based Hessian approximations.
- What evidence would resolve it: Systematic benchmarking of gray-box formulation using various Hessian approximation techniques on problems of increasing size and complexity, measuring convergence reliability and computational time.

### Open Question 2
- Question: What are the practical limits of the gray-box formulation when scaling to problems with multiple neural network surrogates or more complex network architectures (e.g., recurrent or graph neural networks)?
- Basis in paper: [inferred] The paper successfully scales to 592 million parameters but only uses simple feedforward networks for a single constraint, suggesting potential bottlenecks may emerge with more complex architectures or multiple networks.
- What evidence would resolve it: Experiments testing gray-box formulation with multiple neural network surrogates, different network architectures (RNNs, GNNs), and varying levels of nonlinearity to identify scalability limits.

### Open Question 3
- Question: Can the reduced-space formulation be made scalable through improved expression management or automatic differentiation systems that better handle vector-valued constraints?
- Basis in paper: [explicit] The paper identifies JuMP's expression management as the bottleneck for reduced-space formulation, but does not explore potential optimizations or alternative implementations.
- What evidence would resolve it: Development and testing of alternative reduced-space implementations using different modeling frameworks, specialized AD systems for vector-valued constraints, or compiler-level optimizations to eliminate redundant expression evaluation.

### Open Question 4
- Question: How does the performance of full-space formulation change when using specialized linear solvers or matrix decomposition techniques designed for neural network-structured problems?
- Basis in paper: [inferred] The paper identifies KKT matrix factorization as the bottleneck for full-space formulation but does not explore problem-specific linear solver optimizations that could exploit the neural network structure.
- What evidence would resolve it: Comparative testing of full-space formulation using various linear solvers (sparse direct, iterative, domain decomposition) and matrix preconditioning techniques on neural network-constrained problems to identify optimal solver configurations.

## Limitations

- The study only evaluates simple feedforward neural networks, limiting generalizability to other architectures like CNNs or RNNs
- Results are specific to power system applications and may not translate directly to other domains
- The 592 million parameter limit was constrained by available GPU memory rather than inherent formulation limitations

## Confidence

- **High confidence**: Comparative performance analysis between formulations, as systematic benchmarking across multiple network architectures provides robust evidence
- **Medium confidence**: Scalability claims for gray-box formulation, as the 592 million parameter limit was constrained by available GPU memory rather than inherent formulation limitations
- **Low confidence**: Absolute performance of reduced-space formulation, as JuMP expression management bottleneck may be partially implementation-specific rather than fundamental to the approach

## Next Checks

1. **Cross-architecture validation**: Test gray-box formulation performance with convolutional and recurrent neural networks to verify that dense feed-forward architecture results generalize to other network types.

2. **Linear solver comparison**: Evaluate alternative sparse linear solvers (e.g., Pardiso, MUMPS) for the full-space formulation to determine if KKT matrix factorization limitations can be mitigated through solver optimization.

3. **Memory scaling analysis**: Systematically measure GPU memory usage and solve time scaling as neural network parameter count increases from 1M to 1B parameters to establish practical limits of the gray-box approach.