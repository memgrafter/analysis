---
ver: rpa2
title: Building an Efficient Multilingual Non-Profit IR System for the Islamic Domain
  Leveraging Multiprocessing Design in Rust
arxiv_id: '2411.06151'
source_url: https://arxiv.org/abs/2411.06151
tags:
- language
- domain
- rust
- retrieval
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a multilingual IR system for the Islamic domain,
  addressing challenges of domain-specific data scarcity and resource constraints.
  It introduces a domain-adapted MLLM via continued pre-training and language reduction,
  significantly reducing model size while maintaining or improving performance.
---

# Building an Efficient Multilingual Non-Profit IR System for the Islamic Domain Leveraging Multiprocessing Design in Rust

## Quick Facts
- **arXiv ID**: 2411.06151
- **Source URL**: https://arxiv.org/abs/2411.06151
- **Reference count**: 17
- **Primary result**: Achieved 27.8 vs. 17.8 recall@100 for Arabic on in-domain IR dataset using domain-adapted XLM-R4-ID model with Rust-based multiprocessing deployment

## Executive Summary
This paper presents a multilingual information retrieval system specifically designed for the Islamic domain, addressing challenges of domain-specific data scarcity and resource constraints. The authors introduce a domain-adapted MLLM through continued pre-training and language reduction, significantly reducing model size while maintaining or improving performance compared to general-domain models. For deployment, they leverage Rust's multiprocessing capabilities to implement efficient semantic search on CPUs, achieving up to 4.9x speedup without compromising recall compared to GPU-based methods like Faiss.

## Method Summary
The approach involves compiling a multilingual Islamic corpus, training a domain-specific tokenizer, and performing language reduction on XLM-RBase to create XLM-R4. The model is then adapted to the Islamic domain through continued pre-training on the specialized corpus, followed by fine-tuning for retrieval using SentenceTransformers. For deployment, a multiprocessing semantic search system is implemented in Rust using the Candle framework, distributing corpus embeddings across multiple workers for parallel exact search. The system is evaluated on an in-domain Arabic retrieval dataset (QRCD) and machine-translated MS MARCO, showing superior performance to general-domain baselines while enabling efficient CPU-based inference.

## Key Results
- XLM-R4-ID (ar) model achieved 27.8 recall@100 for Arabic on in-domain dataset vs. 17.8 for general-domain baseline
- Language reduction decreased model size by over 50% while preserving encoder weights
- Rust-based multiprocessing implementation achieved up to 4.9x speedup over single-threaded GPU-based methods
- Domain-adapted model outperformed general-domain models across all languages (Arabic, English, Russian, Urdu)

## Why This Works (Mechanism)

### Mechanism 1: Language Reduction
- Claim: Language reduction significantly decreases model size while preserving encoder weights, leading to faster deployment on resource-constrained devices
- Mechanism: By trimming the embedding matrix and removing languages not needed in deployment, the parameter count drops by over half, while the encoder weights remain intact
- Core assumption: The embedding matrix accounts for a large portion of parameters in multilingual models (around 50% in mBERT, 70% in XLM-RBase), and the encoder weights are more critical for performance than the embedding weights
- Evidence anchors: [section] "Around 50% of the parameters in mBERT and 70% in XLM-RBase are assigned to the embedding matrix (see Table 2 in Appendix A). Thus, applying language reduction is more favorable in the case of deploying MLLM as it decreases the model size while preserving encoder weights, trimming only the embedding matrix by removing the languages that are not needed in deployment."

### Mechanism 2: Continued Pre-training for Domain Adaptation
- Claim: Continued pre-training on a domain-specific corpus, even if small, can effectively adapt a general-domain model to a specialized domain by leveraging the existing knowledge in the encoder weights
- Mechanism: The pre-trained encoder weights provide a strong initialization, and fine-tuning on domain-specific data updates the embeddings and attention patterns to align with the specialized vocabulary and semantic structures of the target domain
- Core assumption: The general-domain encoder weights contain transferable linguistic knowledge that can be adapted to the specialized domain without catastrophic forgetting, and the domain-specific corpus, though small, is representative enough to guide the adaptation
- Evidence anchors: [abstract] "By employing methods like continued pre-training for domain adaptation and language reduction to decrease model size, a lightweight multilingual retrieval model was prepared, demonstrating superior performance compared to larger models pre-trained on general domain data."

### Mechanism 3: Multiprocessing for Efficient Search
- Claim: Rust's multiprocessing capabilities enable efficient semantic search on CPUs, achieving significant speedup without compromising recall compared to GPU-based methods
- Mechanism: By dividing the corpus embeddings into chunks and distributing them across multiple workers, each worker can perform an exact search on its allocated chunk in parallel, and the main thread can aggregate and sort the results
- Core assumption: The overhead of multiprocessing is outweighed by the parallel computation gains, and the exact search on each chunk is sufficiently fast to benefit from parallelization
- Evidence anchors: [abstract] "Furthermore, evaluating the proposed architecture that utilizes Rust Language capabilities shows the possibility of implementing efficient semantic search in a low-resource setting."

## Foundational Learning

- Concept: Domain adaptation of pre-trained language models
  - Why needed here: The general-domain MLLM (XLM-RBase) performs poorly on the Islamic domain due to domain shift, so adapting it to the specialized vocabulary and semantic structures of the Islamic literature is crucial for good performance
  - Quick check question: What are the main challenges in adapting a general-domain MLLM to a specialized domain, and how can they be addressed?

- Concept: Language reduction in multilingual models
  - Why needed here: The full multilingual models are too large to deploy on resource-constrained devices, so reducing the size by removing unnecessary languages is essential for practical deployment
  - Quick check question: How does language reduction affect the performance of a multilingual model, and what are the trade-offs involved?

- Concept: Multiprocessing for efficient search
  - Why needed here: GPU acceleration is expensive and not always available, so leveraging CPU multiprocessing can provide a cost-effective way to achieve fast semantic search
  - Quick check question: What are the key considerations when implementing multiprocessing for semantic search, and how can the overhead be minimized?

## Architecture Onboarding

- Component map: Corpus preparation -> Model adaptation (language reduction + continued pre-training) -> Retrieval model fine-tuning -> Multiprocessing search system -> Evaluation
- Critical path: 1. Compile multilingual Islamic corpus 2. Train new tokenizer and perform language reduction on XLM-RBase to get XLM-R4 3. Continue pre-training XLM-R4 on Islamic corpus to get XLM-R4-ID 4. Fine-tune XLM-R4-ID on MS MARCO for IR 5. Implement multiprocessing search system in Rust 6. Evaluate performance on in-domain IR dataset and inference time
- Design tradeoffs: Model size vs. performance: Language reduction significantly reduces size but may slightly impact performance; Corpus size vs. adaptation quality: Continued pre-training on a small corpus may not fully capture the domain nuances; Multiprocessing overhead vs. speedup: Too many workers can lead to context switching overhead and negate the speedup
- Failure signatures: Poor IR performance: Indicates insufficient domain adaptation or language reduction affecting the model's ability to capture the domain semantics; Slow inference time: Suggests the multiprocessing implementation is not efficient or the corpus is too large for the available resources; High memory usage: Implies the model or corpus is too large to fit in memory, requiring further optimization or hardware upgrade
- First 3 experiments: 1. Evaluate the performance of XLM-R4 (reduced size) vs. XLM-RBase (full size) on the in-domain IR dataset to confirm the effectiveness of language reduction; 2. Compare the performance of XLM-R4-ID (domain-adapted) vs. XLM-R4 (general-domain) on the in-domain IR dataset to validate the benefit of continued pre-training; 3. Measure the inference time and recall of the Rust-based multiprocessing search system with different numbers of workers to find the optimal configuration

## Open Questions the Paper Calls Out

- Question: How does the performance of the XLM-R4-ID model compare to other domain-specific models trained on similar Islamic literature datasets?
  - Basis in paper: [explicit] The paper compares the XLM-R4-ID model to general-domain models but does not compare it to other domain-specific models
  - Why unresolved: The paper does not provide a comparison with other domain-specific models trained on similar datasets, which would help to contextualize the performance of the XLM-R4-ID model
  - What evidence would resolve it: Performance metrics of other domain-specific models on the same Islamic literature datasets would provide a basis for comparison

- Question: What is the impact of using different transfer languages (e.g., English vs. Arabic) on the performance of the XLM-R4-ID model for cross-lingual retrieval tasks?
  - Basis in paper: [explicit] The paper mentions that the transfer language of the XLM-R Base is English, while XLM-R4-ID was adapted for the Islamic Domain, predominately using Arabic
  - Why unresolved: While the paper provides some insights into the impact of transfer languages, it does not explore this in depth or compare the performance of the model when trained on different transfer languages
  - What evidence would resolve it: A comprehensive study comparing the performance of the model when trained on different transfer languages would provide a clearer understanding of the impact of transfer languages on cross-lingual retrieval tasks

- Question: How does the performance of the XLM-R4-ID model scale with the size of the retrieval corpus?
  - Basis in paper: [inferred] The paper mentions that the real size of the data for retrieval is above 150k passages, but the test was restricted to using a smaller retrieval corpus (around 50k passages) due to computational constraints
  - Why unresolved: The paper does not provide performance metrics for the XLM-R4-ID model on larger retrieval corpora, which would help to understand how the model's performance scales with the size of the corpus
  - What evidence would resolve it: Performance metrics of the XLM-R4-ID model on larger retrieval corpora would provide insights into how the model's performance scales with the size of the corpus

## Limitations
- The exact composition and size of the multilingual Islamic corpus remains unspecified beyond the 300M word figure, making it difficult to assess the robustness of domain adaptation
- Specific details about the multiprocessing implementation (worker allocation, chunk sizes, system configuration) are missing, limiting reproducibility of the claimed speedup
- The paper focuses on retrieval performance but does not address quality concerns like faithfulness or hallucination risks, which are critical in Islamic domain applications

## Confidence
- High confidence: Language reduction can significantly decrease model size (by ~50-70% of parameters) while preserving encoder weights
- Medium confidence: Continued pre-training for domain adaptation is effective but the relatively small domain corpus raises questions about robustness
- Low confidence: Specific speedup claims from Rust multiprocessing lack sufficient detail about system configuration and baseline comparison methodology

## Next Checks
1. Replicate the language reduction process on XLM-RBase to create XLM-R4, then measure the actual parameter reduction and evaluate performance on a held-out IR dataset to confirm the claimed size reduction without significant performance degradation
2. Implement the Rust multiprocessing search system with varying numbers of workers and corpus chunk sizes, then measure both speedup and recall@100 to identify the optimal configuration and validate the claimed 4.9x improvement
3. Conduct a human evaluation study comparing passages retrieved by the domain-adapted model versus the general-domain baseline to assess whether the performance gains translate to more accurate and faithful retrieval in the Islamic domain context