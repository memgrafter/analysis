---
ver: rpa2
title: 'Power Scheduler: A Batch Size and Token Number Agnostic Learning Rate Scheduler'
arxiv_id: '2408.13359'
source_url: https://arxiv.org/abs/2408.13359
tags:
- learning
- training
- rate
- tokens
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new learning rate scheduler called Power
  scheduler that is agnostic to batch size and token number. The key insight is that
  the optimal learning rate for the Warmup-Stable-Decay (WSD) scheduler follows a
  power-law relationship with respect to batch size and number of training tokens.
---

# Power Scheduler: A Batch Size and Token Number Agnostic Learning Rate Scheduler

## Quick Facts
- **arXiv ID:** 2408.13359
- **Source URL:** https://arxiv.org/abs/2408.13359
- **Reference count:** 9
- **Primary result:** Introduces Power scheduler that dynamically adjusts learning rate based on power-law relationships with batch size and training tokens, achieving state-of-the-art performance with one set of hyperparameters across model sizes.

## Executive Summary
This paper introduces the Power scheduler, a learning rate scheduler that dynamically adjusts the learning rate based on power-law relationships with batch size and training tokens. The key insight is that optimal learning rates for Warmup-Stable-Decay schedulers follow predictable scaling patterns, allowing the scheduler to determine appropriate learning rates without pre-specifying training steps. Combined with Maximum Update Parameterization (μP), the Power scheduler enables zero-shot transfer of hyperparameters across different model sizes, batch sizes, and token counts. Experiments demonstrate that 3B parameter models trained with this approach achieve performance comparable to state-of-the-art small language models.

## Method Summary
The Power scheduler uses the observation that optimal learning rates for Warmup-Stable-Decay (WSD) schedulers follow a power-law relationship: ηopt = β · a · T^b, where β is batch size, T is number of training tokens, and empirically a ≈ 4 and b ≈ -0.51. The scheduler dynamically adjusts the learning rate during training based on these relationships rather than requiring pre-specification of total training steps. When combined with Maximum Update Parameterization (μP) for initialization and learning rate scaling, this approach enables consistent performance across different model sizes and training configurations. The method was validated through extensive experiments on proxy models (12M, 36M, 121M parameters) and successfully transferred to 1B and 3B parameter dense and MoE models.

## Key Results
- The Power scheduler achieves comparable performance to state-of-the-art small language models with one set of hyperparameters across different model sizes, batch sizes, and token counts
- Power-law relationship ηopt = β · 4 · T^(-0.51) accurately predicts optimal learning rates for WSD schedulers across diverse configurations
- 3B parameter dense and MoE models pretrained with Power scheduler achieve competitive results on multiple choice tasks, MMLU, coding, and math benchmarks
- Zero-shot transfer from small proxy models to larger models is achieved by combining Power scheduler with Maximum Update Parameterization (μP)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The optimal learning rate ηopt for the WSD scheduler scales proportionally with batch size β for a fixed number of training tokens T.
- Mechanism: The scheduler maintains a constant ratio γ = ηopt / β across different batch sizes, implying that increasing the batch size allows proportionally increasing the learning rate without destabilizing training.
- Core assumption: The underlying optimization dynamics are batch-size invariant when the learning rate is scaled proportionally.
- Evidence anchors:
  - [abstract] "the optimal learning rate ηopt satisfies a power-law relation with respect to batch size β and number of tokens T"
  - [section 3.2] "the ratio between the optimal learning rate ηopt and batch size β is relatively stable for each number of training tokens"
  - [corpus] Weak evidence: no direct citations on batch-size proportional scaling found
- Break condition: If the training dynamics become batch-size sensitive due to architectural changes or optimization instabilities.

### Mechanism 2
- Claim: The ratio γ = ηopt / β follows a power-law decay with respect to the number of training tokens T.
- Mechanism: As the training corpus grows, the model requires progressively smaller relative learning rates (ηopt / β) to maintain stable convergence, following γ ∝ T^(-0.51).
- Core assumption: The model's sensitivity to learning rate updates decreases with more training data, requiring finer adjustments.
- Evidence anchors:
  - [section 3.2] "γ approximately follows a power-law relation with respect to the number of training tokens"
  - [section 3.2] "γ = 4.6T^(-0.51)" derived from experiments across 12M, 36M, and 121M models
  - [corpus] Weak evidence: no direct citations on power-law learning rate decay with token count
- Break condition: If the model architecture or training objectives change such that learning rate sensitivity doesn't scale with data volume.

### Mechanism 3
- Claim: Combining the power-law learning rate with Maximum Update Parameterization (μP) enables zero-shot transfer of hyperparameters across

## Foundational Learning

### Power-Law Scaling
- **Why needed:** Understanding how optimal hyperparameters scale with model and dataset characteristics is crucial for efficient training across different configurations
- **Quick check:** Verify that learning rate scales linearly with batch size and follows power-law decay with token count in your experimental setup

### Maximum Update Parameterization (μP)
- **Why needed:** μP provides a theoretically grounded approach to initialization and learning rate scaling that enables architecture-independent transfer
- **Quick check:** Confirm that model initialization and learning rate scaling follow μP principles, particularly for layer normalization and residual connections

### Warmup-Stable-Decay Scheduler
- **Why needed:** WSD schedulers are the baseline against which Power scheduler's performance is compared, understanding their limitations motivates the need for dynamic scheduling
- **Quick check:** Verify that your WSD implementation correctly implements the three-phase structure (warmup, stable, decay) before comparing with Power scheduler

## Architecture Onboarding

### Component Map
- Transformer model architecture with μP initialization -> Power scheduler for dynamic learning rate adjustment -> WSD scheduler baseline comparison

### Critical Path
Model initialization (μP) -> Training loop with Power scheduler -> Dynamic learning rate computation based on power-law relationships -> Performance evaluation across tasks

### Design Tradeoffs
- **Fixed vs. dynamic scheduling:** Power scheduler eliminates need to specify total training steps but requires computing power-law relationships on the fly
- **Generalization vs. optimization:** Power scheduler trades some potential optimization for transfer across configurations, enabling zero-shot hyperparameter transfer
- **Complexity vs. performance:** Additional complexity of power-law computation is justified by performance gains and elimination of hyperparameter tuning

### Failure Signatures
- Training divergence in early steps indicates learning rate too high (break condition for Mechanism 1)
- Plateauing performance with excessive training suggests insufficient learning rate decay (break condition for Mechanism 2)
- Poor transfer from small to large models indicates μP implementation issues (break condition for Mechanism 3)

### First Experiments
1. **Proxy model validation:** Train 12M parameter model on RedPajama corpus with batch size 128, learning rate 0.0128 for 2B tokens, verify perplexity matches Figure 2(a)
2. **Batch size scaling test:** Train identical models with different batch sizes, verify optimal learning rate scales proportionally (γ = ηopt/β remains constant)
3. **Token count scaling test:** Train models on different amounts of data, verify γ follows power-law decay T^(-0.51)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Power scheduler's performance compare to other schedulers when training models with different architectures (e.g., convolutional neural networks, transformers with different attention mechanisms)?
- Basis in paper: [explicit] The paper mentions that the Power scheduler achieves impressive performance with one set of hyperparameters regardless of model architecture.
- Why unresolved: The experiments in the paper primarily focus on transformer-based models. The generalizability of the Power scheduler to other architectures is not explored.
- What evidence would resolve it: Conducting experiments with the Power scheduler on models with different architectures and comparing their performance to models trained with other schedulers.

### Open Question 2
- Question: What is the optimal value of the amplitude parameter 'a' in the Power scheduler for different model sizes and training tasks?
- Basis in paper: [explicit] The paper mentions that the optimal value of 'a' is 4 for the models and tasks considered in their experiments.
- Why unresolved: The paper does not explore the sensitivity of the Power scheduler's performance to different values of 'a' for different model sizes and tasks.
- What evidence would resolve it: Conducting a systematic study of the Power scheduler's performance with different values of 'a' for various model sizes and tasks.

### Open Question 3
- Question: How does the Power scheduler's performance compare to other schedulers when training models on datasets with different characteristics (e.g., dataset size, data distribution, task complexity)?
- Basis in paper: [explicit] The paper mentions that the Power scheduler is agnostic to the number of training tokens and batch size.
- Why unresolved: The experiments in the paper primarily focus on a single dataset (RedPajama) and do not explore the Power scheduler's performance on datasets with different characteristics.
- What evidence would resolve it: Conducting experiments with the Power scheduler on datasets with varying sizes, data distributions, and task complexities, and comparing their performance to models trained with other schedulers.

## Limitations

- The power-law relationships are empirically observed rather than theoretically derived, limiting understanding of when they might break down
- Experiments are primarily conducted on transformer-based language models, leaving uncertainty about generalizability to other architectures
- The method's performance on frontier-scale models (10B+ parameters) remains untested
- Reliance on Maximum Update Parameterization introduces additional complexity that may not be suitable for all training scenarios

## Confidence

**High confidence** in the empirical observation that optimal learning rate scales with batch size and token count for transformer language models on standard pretraining tasks.

**Medium confidence** in the transferability claims, as the sample size of transfer scenarios is limited and more extensive validation across diverse model families is needed.

**Low confidence** in the theoretical foundations explaining why these power-law relationships exist, as the paper presents empirical observations without providing mechanistic explanations.

## Next Validation Checks

1. **Cross-Objective Validation:** Test the Power scheduler on non-language modeling tasks (computer vision, reinforcement learning) to verify whether the power-law relationships hold beyond next-token prediction.

2. **Architecture Transfer Testing:** Implement the Power scheduler with non-transformer architectures (RNNs, ConvNets, or state-space models) to assess whether the power-law scaling relationships are architecture-dependent.

3. **Frontier Scale Validation:** Conduct experiments scaling from 3B to 10B+ parameter models to identify any break points in the power-law relationships and monitor for deviations in the learning rate ratio γ = ηopt/β.