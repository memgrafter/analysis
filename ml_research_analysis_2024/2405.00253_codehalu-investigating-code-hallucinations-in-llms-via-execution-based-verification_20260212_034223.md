---
ver: rpa2
title: 'CodeHalu: Investigating Code Hallucinations in LLMs via Execution-based Verification'
arxiv_id: '2405.00253'
source_url: https://arxiv.org/abs/2405.00253
tags:
- code
- hallucinations
- llms
- arxiv
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the concept of code hallucinations in large
  language models (LLMs) and proposes an execution-based verification method to systematically
  detect and classify them. The authors define code hallucinations as syntactically
  correct but functionally incorrect or non-executable code, distinct from simple
  syntax errors.
---

# CodeHalu: Investigating Code Hallucinations in LLMs via Execution-based Verification

## Quick Facts
- **arXiv ID:** 2405.00253
- **Source URL:** https://arxiv.org/abs/2405.00253
- **Reference count:** 5
- **Primary result:** Code hallucinations are syntactically correct but functionally incorrect code, distinct from syntax errors

## Executive Summary
This paper introduces the concept of code hallucinations in large language models (LLMs), defining them as syntactically correct but functionally incorrect or non-executable code. The authors develop CodeHalu, a dynamic detection algorithm that identifies common hallucination patterns across multiple models. They introduce CodeHaluEval, a benchmark with 8,883 samples from 699 tasks, to quantitatively evaluate hallucinations across 17 popular LLMs. The study reveals significant variation in hallucination rates across models, ranging from approximately 20% to 60%, with logical hallucinations being the most prevalent issue.

## Method Summary
The authors propose an execution-based verification method that dynamically detects code hallucinations by executing generated code and comparing outputs against expected behavior. CodeHalu identifies four main types of hallucinations (mapping, naming, resource, and logic) with eight subcategories total. The detection algorithm works by systematically testing code functionality through controlled execution environments and identifying patterns where code is syntactically valid but fails to produce correct results. The CodeHaluEval benchmark provides standardized evaluation across multiple LLMs using real-world programming tasks.

## Key Results
- Hallucination rates vary significantly across 17 tested LLMs, ranging from ~20% to ~60%
- Four main hallucination types identified: mapping, naming, resource, and logic
- Eight subcategories of hallucinations detected with varying prevalence
- Logical hallucinations are the most common type across models
- CodeHalu achieves systematic detection of hallucinations through execution-based verification

## Why This Works (Mechanism)
The execution-based verification method works by providing a ground truth mechanism that distinguishes between syntactic validity and functional correctness. By running generated code in controlled environments with test cases, the system can identify when code produces incorrect outputs despite being syntactically valid. This approach captures hallucinations that traditional syntax checking would miss, revealing functional errors that manifest only during runtime.

## Foundational Learning

1. **Code Hallucinations** - Functionally incorrect code that appears syntactically correct
   - Why needed: To distinguish between syntax errors and deeper functional problems in LLM-generated code
   - Quick check: Generate code with intentional logical errors and verify detection

2. **Execution-based Verification** - Testing code functionality through controlled runtime environments
   - Why needed: Syntax checking alone cannot detect functional correctness issues
   - Quick check: Run syntactically valid but functionally incorrect code to verify detection

3. **Dynamic Detection Algorithm** - Systematic identification of hallucination patterns through runtime analysis
   - Why needed: Static analysis misses runtime-specific functional errors
   - Quick check: Apply algorithm to code with known functional errors

4. **Benchmark Construction** - Creating standardized evaluation datasets for systematic comparison
   - Why needed: To quantify hallucination rates across different models objectively
   - Quick check: Verify benchmark tasks cover diverse programming scenarios

## Architecture Onboarding

**Component Map:** Code Generation -> CodeHalu Detection -> Classification -> Analysis
**Critical Path:** Task input → Code generation → Execution environment → Output verification → Hallucination classification
**Design Tradeoffs:** Execution-based verification provides accurate detection but requires runtime overhead; static analysis would be faster but less comprehensive
**Failure Signatures:** Syntax-only validation misses functional errors; hallucination patterns vary by task type and model
**First Experiments:**
1. Test CodeHalu on synthetic code with known logical errors
2. Compare detection accuracy across different programming languages
3. Measure performance impact of execution-based verification versus static analysis

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Focuses on short, self-contained code generation tasks rather than real-world programming scenarios
- Benchmark construction relies on Python-specific patterns and may not generalize to other languages
- Execution-based verification assumes correct behavior can be determined through test cases
- Does not investigate hallucination persistence across different prompts or contexts

## Confidence

| Claim | Confidence |
|-------|------------|
| Classification into four main types with eight subcategories | High |
| Significant variation in hallucination rates (20-60%) across models | Medium |
| Logical hallucinations are most prevalent type | Medium |

## Next Checks

1. Test CodeHalu on real-world code generation tasks from GitHub repositories to validate whether hallucination patterns generalize beyond synthetic benchmarks
2. Apply the same detection methodology to code generation models for languages other than Python to assess cross-language applicability
3. Conduct ablation studies to determine whether specific components of the detection algorithm contribute disproportionately to identifying certain hallucination types