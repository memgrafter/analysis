---
ver: rpa2
title: 'Wukong: Towards a Scaling Law for Large-Scale Recommendation'
arxiv_id: '2403.02545'
source_url: https://arxiv.org/abs/2403.02545
tags:
- wukong
- scaling
- arxiv
- quality
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of scaling recommendation models
  to handle increasingly complex and large-scale datasets, where existing approaches
  primarily rely on sparse scaling (expanding embedding tables) and struggle to capture
  complex feature interactions efficiently. The authors propose Wukong, a novel architecture
  based on stacked Factorization Machines (FMs) that enables effective dense scaling.
---

# Wukong: Towards a Scaling Law for Large-Scale Recommendation

## Quick Facts
- arXiv ID: 2403.02545
- Source URL: https://arxiv.org/abs/2403.02545
- Reference count: 28
- Key outcome: Wukong enables effective dense scaling of recommendation models through stacked FM layers, achieving over 0.2% LogLoss improvement on a large internal dataset while maintaining superior scaling properties across two orders of magnitude in model complexity.

## Executive Summary
This paper addresses the challenge of scaling recommendation models to handle increasingly complex and large-scale datasets. Existing approaches primarily rely on sparse scaling (expanding embedding tables) and struggle to capture complex feature interactions efficiently. The authors propose Wukong, a novel architecture based on stacked Factorization Machines (FMs) that enables effective dense scaling. Wukong uses a series of FM blocks to capture any-order feature interactions through progressively taller and wider layers, combined with linear compression blocks and residual connections.

Extensive evaluations on six public datasets and a large internal dataset demonstrate Wukong's effectiveness. On public datasets, Wukong consistently outperforms state-of-the-art models across all datasets. On the internal large-scale dataset, Wukong shows continuous quality improvements across two orders of magnitude in model complexity (beyond 100 GFLOP/example), achieving over 0.2% LogLoss improvement compared to baselines while maintaining superior scaling properties. The model exhibits better data efficiency and maintains its superiority across various complexity levels.

## Method Summary
Wukong is an architecture based on stacked Factorization Machines that captures any-order feature interactions through a series of interaction layers. Each layer contains two parallel blocks: a Factorization Machine Block (FMB) that computes high-order interactions and a Linear Compression Block (LCB) that maintains linear embedding compression. The architecture uses residual connections and layer normalization to ensure training stability. The model treats embeddings as whole units rather than element-wise interactions, reducing computational complexity while maintaining effectiveness. Wukong is trained using Adam for dense parts and Rowwise Adagrad for sparse embeddings.

## Key Results
- On six public datasets, Wukong consistently outperforms state-of-the-art models including DLRM, DCNv2, AutoInt+, AFN+, FinalMLP, and MaskNet
- On the internal large-scale dataset (146B entries), Wukong achieves continuous quality improvements across two orders of magnitude in model complexity (beyond 100 GFLOP/example)
- Wukong achieves over 0.2% LogLoss improvement compared to baselines on the internal dataset while maintaining superior scaling properties

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stacking FM layers enables exponential growth in interaction order capture while maintaining training stability.
- Mechanism: Each FM layer captures 2nd-order interactions between embeddings from the previous layer. With residual connections and layer normalization, stacking allows the model to capture interactions of order 1 to 2^l in l layers, providing exponential scaling of interaction order capacity.
- Core assumption: The residual connection and layer normalization are sufficient to stabilize training when stacking multiple FM layers.
- Evidence anchors:
  - [abstract] "Inspired by the principles of binary exponentiation, our key innovation is to use a series of stacked Factorization Machines (FMs) to efficiently and scalably capture any-order feature interactions."
  - [section 3.3] "For layer i in the stack, its results can contain feature interactions with arbitrary order from 1 to 2^i. This can be simply shown by induction."
  - [corpus] Weak evidence - the cited papers focus on scaling laws generally but don't specifically validate this mechanism of stacked FMs.
- Break condition: Training becomes unstable or quality plateaus when residual connections and layer normalization are insufficient for very deep stacks.

### Mechanism 2
- Claim: Treating embeddings as whole units rather than element-wise interactions reduces computational complexity while maintaining effectiveness.
- Mechanism: Wukong operates on n×d embedding matrices where each embedding is a single unit, avoiding the quadratic explosion of element-wise interactions. This allows efficient capture of high-order interactions through stacked FMs without prohibitive compute costs.
- Core assumption: The embedding-wise interaction approach captures sufficient information for recommendation tasks without needing element-wise detail.
- Evidence anchors:
  - [section 3.3] "Note that unlike conventional approaches like DCN (Wang et al., 2021a), we interpret each embedding vector as a whole unit... our representation of X0 ∈ R^(n×d) as opposed to X0 ∈ R^(n×d)."
  - [section 3.6] "When d < n, the dot-product interaction XX^T is a d-rank matrix... we can effectively reduce the size of output matrix from n×n to n×k..."
  - [corpus] Weak evidence - similar embedding-wise approaches exist but specific validation of this efficiency claim is limited.
- Break condition: The embedding-wise approach fails to capture critical interaction patterns that require element-wise resolution.

### Mechanism 3
- Claim: The combination of FMB and LCB blocks allows effective feature interaction capture while maintaining interaction order constraints.
- Mechanism: FMB captures high-order interactions through FM and MLP transformation, while LCB maintains linear embedding compression to ensure interaction order invariance across layers. Their outputs are concatenated to provide both interaction results and compressed embeddings for the next layer.
- Core assumption: The parallel operation of FMB and LCB, combined with concatenation, provides the right balance of interaction capture and embedding transformation.
- Evidence anchors:
  - [section 3.3] "An interaction layer has two blocks in parallel: a Factorization Machine Block (FMB) and a Linear Compression Block (LCB). FMB computes feature interactions... and LCB simply forwards linearly compressed input embeddings."
  - [section 3.5] "LCB simply linearly recombines embeddings without increasing interaction orders, which is critical in ensuring that the invariance of interaction order is maintained throughout the layers."
  - [corpus] Weak evidence - the specific combination of parallel FMB/LCB blocks is novel and not well-supported by related work.
- Break condition: The LCB becomes redundant or the FMB fails to capture sufficient interactions, leading to quality degradation.

## Foundational Learning

- Concept: Factorization Machines (FMs)
  - Why needed here: FMs are the core building block for capturing pairwise feature interactions efficiently. Understanding their operation is crucial for grasping how Wukong scales interaction capture.
  - Quick check question: How does an FM compute pairwise interactions between feature embeddings, and why is this more efficient than explicit outer product approaches?

- Concept: Binary Exponentiation
  - Why needed here: The stacking strategy in Wukong is inspired by binary exponentiation, where each layer doubles the interaction order capacity. This concept explains the exponential scaling property.
  - Quick check question: If each layer captures interactions up to order 2^i, what interaction orders can a 3-layer Wukong capture, and how does this compare to linear stacking approaches?

- Concept: Embedding-wise vs Element-wise Operations
  - Why needed here: Wukong treats embeddings as whole units rather than operating element-wise, which is a key design decision affecting both efficiency and effectiveness.
  - Quick check question: What are the computational complexity differences between embedding-wise and element-wise interaction approaches when dealing with n features and d-dimensional embeddings?

## Architecture Onboarding

- Component map: Embedding Layer → Interaction Stack (FMB/LCB blocks) → Final MLP → Output

- Critical path: Embedding Layer → Interaction Stack (FMB/LCB blocks) → Final MLP → Output

- Design tradeoffs:
  - Stacking depth vs training stability: Deeper stacks capture higher-order interactions but risk instability without proper residual connections and normalization
  - Embedding-wise vs element-wise: Embedding-wise operations are more efficient but may miss fine-grained interactions
  - FMB vs LCB balance: Need both interaction capture and linear compression to maintain order invariance while transforming features

- Failure signatures:
  - Training instability with deep stacks (loss explosion or NaN values)
  - Quality plateau despite increased model size (indicates ineffective scaling)
  - Memory issues with large n (too many embeddings for available memory)

- First 3 experiments:
  1. Single-layer Wukong baseline: Test basic FM + MLP operation with varying embedding dimensions and MLP sizes to establish performance floor
  2. Two-layer Wukong: Add a second interaction layer with residual connection to verify exponential interaction order capture and stability
  3. Scaling test: Gradually increase model complexity (layers, nF, nL) while monitoring training stability and quality improvement trends

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact theoretical limit of Wukong's scalability in terms of dataset size and model complexity?
- Basis in paper: [explicit] The paper acknowledges that "due to the massive compute requirement, we have not been able to reach a level of complexity where the limit applies" and suggests understanding this limit is "an important area of research."
- Why unresolved: The experiments were constrained by computational resources, preventing testing on datasets large enough to determine when the scaling law breaks down.
- What evidence would resolve it: Training Wukong on increasingly larger datasets (orders of magnitude beyond the current largest) while monitoring quality improvements to identify the point of diminishing returns or saturation.

### Open Question 2
- Question: How does Wukong's performance compare to other architectures (e.g., Transformers) when adapted with similar architectural innovations?
- Basis in paper: [explicit] The paper conducts experiments showing that applying Wukong's unique components to AutoInt+ improves performance, but suggests a more comprehensive comparison is needed.
- Why unresolved: The paper only tested Wukong's components on one baseline (AutoInt+) and did not explore how other architectures might perform with similar modifications.
- What evidence would resolve it: Systematic experiments applying Wukong's architectural innovations (MLP projections, pyramid shape) to various baseline models and comparing their performance against Wukong.

### Open Question 3
- Question: What is the optimal strategy for serving highly scaled-up recommendation models in production environments?
- Basis in paper: [explicit] The paper mentions "practically serving scaled-up models" as a challenge and suggests "training a multi-task foundation model to amortize costs: distilling knowledge from the large models into small, efficient ones for serving."
- Why unresolved: The paper identifies this as a practical challenge but does not provide solutions or empirical evidence for effective deployment strategies.
- What evidence would resolve it: Implementation and evaluation of different serving strategies (model distillation, multi-task learning, specialized hardware) on real-world recommendation systems with Wukong at various scales.

## Limitations

- The paper relies heavily on a single internal dataset to demonstrate scaling behavior, with limited validation across diverse domains
- Key mechanisms like exponential interaction order capture through stacking and embedding-wise efficiency have weak empirical validation
- The theoretical limits of Wukong's scalability remain unknown due to computational constraints

## Confidence

- **High confidence**: Wukong outperforms state-of-the-art models on public datasets as measured by LogLoss and AUC
- **Medium confidence**: Wukong demonstrates continuous quality improvements across two orders of magnitude in model complexity on the internal dataset
- **Low confidence**: The theoretical mechanism of exponential interaction order capture through stacked FMs is sufficient for practical effectiveness, and the embedding-wise approach provides necessary computational efficiency

## Next Checks

1. **Scaling law validation across architectures**: Test whether the observed scaling behavior is unique to Wukong or if other architectures (like DLRM or DCN) show similar improvements when scaled to similar computational budgets, controlling for interaction order capture capacity.

2. **Interaction order analysis**: Systematically measure and verify the actual interaction orders captured at different depths in Wukong, comparing theoretical expectations (2^l orders at layer l) with empirical observations through ablation studies.

3. **Memory-efficiency tradeoff validation**: Conduct controlled experiments comparing embedding-wise vs element-wise interaction approaches across different embedding dimensions and feature counts to validate the claimed computational efficiency benefits and identify break points where the approach fails.