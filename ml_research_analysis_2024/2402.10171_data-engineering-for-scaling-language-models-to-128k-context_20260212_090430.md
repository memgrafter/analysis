---
ver: rpa2
title: Data Engineering for Scaling Language Models to 128K Context
arxiv_id: '2402.10171'
source_url: https://arxiv.org/abs/2402.10171
tags:
- data
- context
- arxiv
- length
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates data engineering methods for scaling language
  models' context lengths to 128K tokens, with a focus on continual pretraining. The
  authors hypothesize that the ability to utilize information at arbitrary input locations
  is mostly acquired through large-scale pretraining, and can be readily extended
  to contexts substantially longer than seen during training (e.g., 4K to 128K) through
  lightweight continual pretraining on appropriate data mixtures.
---

# Data Engineering for Scaling Language Models to 128K Context

## Quick Facts
- arXiv ID: 2402.10171
- Source URL: https://arxiv.org/abs/2402.10171
- Reference count: 15
- Scaling language models to 128K context through lightweight continual pretraining on 1-5B tokens

## Executive Summary
This paper investigates data engineering methods for scaling language models' context lengths to 128K tokens through continual pretraining. The authors demonstrate that lightweight continual pretraining on a small amount of long-context data (1-5B tokens) can "unlock" a model's capability for precise retrieval over much longer contexts than seen during original pretraining. The key insight is that the ability to utilize information at arbitrary input locations is mostly already acquired during large-scale pretraining and can be readily extended to substantially longer contexts through appropriate data mixtures.

The study emphasizes the importance of both data quantity and quality, showing that 500M to 5B tokens are sufficient for the model to retrieve information anywhere within the 128K context. Crucially, per-source length upsampling that maintains domain balance while increasing the proportion of long sequences proves more effective than naive global upsampling strategies. This approach outperforms strong open-source long-context models and closes the gap to frontier models like GPT-4 128K on the Needle-in-a-Haystack test.

## Method Summary
The method involves continual pretraining of LLaMA-2 models (7B and 13B variants) on 80K context length (64K for 13B) using a modified SlimPajama dataset with per-source length upsampling. The training uses HuggingFace with DeepSpeed Zero3 and FlashAttention2, with constant learning rate 2e-5 and batch size 4M tokens. The key innovation is the data engineering approach that maintains domain balance while significantly increasing the proportion of long sequences within each domain, rather than simply cutting or upsampling globally.

## Key Results
- 500M to 5B tokens are sufficient to enable precise information retrieval anywhere within 128K context
- Per-source length upsampling outperforms global upsampling and domain-specific upsampling strategies
- The approach outperforms strong open-source long-context models and closes the gap to GPT-4 128K on Needle-in-a-Haystack
- Domain balance is crucial - naive upsampling of longer data from specific domains like books gives suboptimal performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Long context modeling capability is already acquired during large-scale pretraining, even on 4K context models.
- Mechanism: The model has learned generalized attention patterns during pretraining that can be extended to longer contexts through lightweight continual pretraining on appropriate data mixtures.
- Core assumption: The fundamental ability to retrieve information at arbitrary locations is learned during pretraining and doesn't need to be re-learned.
- Evidence anchors: [abstract] "the ability to utilize information at arbitrary input locations, is a capability that is mostly already acquired through large-scale pretraining"; [section 1] "we show that continual pretraining on a small amount of long-context data, in our case, 1-5B tokens, can 'unlock' a 7B model's capability of precise retrieval over much longer context lengths than seen in original pretraining"
- Break condition: If the pretraining corpus lacks sufficient diversity or the model architecture severely limits attention generalization.

### Mechanism 2
- Claim: Per-source length upsampling is more effective than global upsampling or domain-specific upsampling.
- Mechanism: By maintaining the original domain mixture ratio while increasing the proportion of long sequences within each domain, the model learns length generalization without introducing domain-specific biases.
- Core assumption: Domain balance is crucial for maintaining performance across different data types while extending context length.
- Evidence anchors: [section 3] "Per-source Upsampling: This retains the domain mixture, then upsamples long documents within each domain"; [section 5.3] "Per-source length upsampling is the most balanced mixture that improves 4K-128K context losses without much sacrificing short-context losses"
- Break condition: If the original domain mixture itself is unbalanced or if certain domains don't contain useful long sequences.

### Mechanism 3
- Claim: Only 500M to 5B tokens are sufficient for extending context length to 128K, not hundreds of billions as previously thought.
- Mechanism: The model already possesses the capability; lightweight continual pretraining simply extends this capability to longer contexts without needing to re-learn fundamental attention mechanisms.
- Core assumption: The cost of extending context length is primarily data engineering rather than massive computational training.
- Evidence anchors: [abstract] "continual pretraining of the full model on 1B-5B tokens of such data is an effective and affordable strategy"; [section 5.2] "500M tokens are enough to unlock most of the retrieval accuracy" and "the model's retrieval performance saturates at about 5B tokens"
- Break condition: If the base model architecture severely limits attention capacity or if the pretraining data quality is extremely poor.

## Foundational Learning

- Concept: Attention mechanism generalization
  - Why needed here: Understanding how attention learned on shorter contexts can generalize to longer contexts is crucial for grasping why lightweight continual pretraining works
  - Quick check question: Why does a model pretrained on 4K context sequences have the ability to generalize to 128K without extensive retraining?

- Concept: Domain balance in pretraining data
  - Why needed here: The paper emphasizes that maintaining domain balance while upsampling long sequences is critical for performance across different data types
  - Quick check question: What happens to model performance if you only upsample long sequences from books while ignoring other domains?

- Concept: Length distribution in training data
  - Why needed here: The paper shows that the distribution of sequence lengths in training data significantly impacts the model's ability to handle long contexts
  - Quick check question: How does the proportion of naturally long sequences (longer than 4K) in the original pretraining data affect the amount of continual pretraining needed?

## Architecture Onboarding

- Component map: LLaMA-2 model -> RoPE positional encoding (adjusted base) -> Full attention mechanism -> SlimPajama dataset with per-source length upsampling -> HuggingFace + DeepSpeed Zero3 + FlashAttention2 training infrastructure

- Critical path: 1. Data preparation with per-source length upsampling; 2. Model loading with adjusted RoPE base; 3. Continual pretraining on 80K/64K context length; 4. Evaluation on Needle-in-a-Haystack and BookQA benchmarks

- Design tradeoffs:
  - Full attention vs efficient attention (quadratic complexity vs linear complexity)
  - Model size (7B vs 13B) vs training cost
  - Context length during training (80K vs 64K) vs generalization ability
  - Data quantity (1B-5B tokens) vs performance gains

- Failure signatures:
  - High validation loss on specific domains while maintaining low overall loss
  - Poor performance on Needle-in-a-Haystack despite good validation loss
  - Overfitting on training context length (80K) with reduced generalization
  - Memory overflow when attempting longer context lengths

- First 3 experiments:
  1. Train with original SlimPajama data cut at 128K (baseline) and compare Needle-in-a-Haystack performance
  2. Train with per-source length upsampling and measure domain-specific loss changes
  3. Vary data quantity (100M, 500M, 1B, 5B, 10B tokens) and plot retrieval performance vs training tokens

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal data mixture strategy for continual pretraining of language models beyond 128K context length?
- Basis in paper: [explicit] The paper discusses the importance of domain balance and length upsampling in the data mixture for scaling language models to 128K context. It mentions that a balanced domain mixture is crucial and that naively upsampling longer data on certain domains like books gives suboptimal performance.
- Why unresolved: The paper focuses on scaling to 128K context and does not explore data mixture strategies for contexts longer than 128K. The optimal strategy for even longer contexts remains unknown.
- What evidence would resolve it: Conducting experiments with different data mixture strategies on models trained for contexts longer than 128K and evaluating their performance on benchmarks like Needle-in-a-Haystack at those lengths would provide evidence.

### Open Question 2
- Question: How does the performance of language models on long-context tasks scale with the amount of continual pretraining data beyond 5 billion tokens?
- Basis in paper: [explicit] The paper shows that 500 million to 5 billion tokens are sufficient to enable the model to retrieve information anywhere within the 128K context. It also mentions that further scaling to 10 billion tokens does not improve length generalization.
- Why unresolved: The paper does not explore the performance scaling with continual pretraining data beyond 5 billion tokens. It is unclear if there is a point of diminishing returns or if more data could lead to further improvements.
- What evidence would resolve it: Training models with varying amounts of continual pretraining data beyond 5 billion tokens and evaluating their performance on long-context tasks would provide insights into the scaling behavior.

### Open Question 3
- Question: What is the impact of different attention mechanisms (e.g., sparse attention) on the performance of language models in long-context tasks?
- Basis in paper: [inferred] The paper mentions LongLoRA, which uses sparse attention, and notes that it does not upsample long sequence data. It also states that LongLoRA's performance is suboptimal compared to their method.
- Why unresolved: The paper does not provide a comprehensive comparison of different attention mechanisms in the context of long-context tasks. The impact of attention mechanisms on performance remains unclear.
- What evidence would resolve it: Conducting experiments with models using different attention mechanisms (e.g., sparse attention, full attention) and evaluating their performance on long-context tasks would provide evidence on the impact of attention mechanisms.

## Limitations

- Architecture Constraints: Results demonstrated only on LLaMA-2 models with full attention mechanisms, may not generalize to models with efficient attention or different architectural choices
- Context Length Generalization Gap: Methodology demonstrated for extending from 4K to 128K but effectiveness for even longer contexts (256K, 512K+) remains untested
- Data Composition Uncertainty: Specific domain composition of SlimPajama dataset and how it compares to other pretraining corpora remains unclear

## Confidence

**High Confidence**:
- The importance of per-source length upsampling over global upsampling or domain-specific upsampling
- The effectiveness of 1B-5B tokens for extending context length to 128K
- The Needle-in-a-Haystack benchmark as a reliable measure of precise information retrieval at arbitrary locations

**Medium Confidence**:
- The claim that long-context capability is mostly acquired during original pretraining and can be "unlocked" through lightweight continual pretraining
- The specific domain balance requirements and their impact on performance
- The comparison results against GPT-4 128K on Needle-in-a-Haystack

**Low Confidence**:
- The exact mechanisms by which attention patterns generalize from 4K to 128K contexts
- The minimal token threshold (500M vs 1B) for achieving good performance
- The performance on tasks not specifically designed for long-context evaluation

## Next Checks

1. **Cross-Architecture Validation**: Test the same data engineering approach on models with different attention mechanisms (e.g., sliding window attention, sparse attention) to determine if the findings generalize beyond full attention models.

2. **Longer Context Extension**: Apply the methodology to extend context length beyond 128K (e.g., to 256K or 512K) to identify potential saturation points or new challenges that emerge at extreme context lengths.

3. **Domain Ablation Study**: Systematically vary the domain composition of the training data while keeping the length upsampling strategy constant to quantify the precise impact of each domain on long-context performance.