---
ver: rpa2
title: 'Narrowing the Focus: Learned Optimizers for Pretrained Models'
arxiv_id: '2408.09310'
source_url: https://arxiv.org/abs/2408.09310
tags:
- adam
- step
- learning
- optimizer
- l3rs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces L3RS, a learned optimizer specialized for
  fine-tuning pretrained models. Instead of aiming for broad applicability like general
  learned optimizers, L3RS focuses on a narrow domain of fine-tuning tasks, achieving
  superior performance with significantly fewer parameters.
---

# Narrowing the Focus: Learned Optimizers for Pretrained Models

## Quick Facts
- arXiv ID: 2408.09310
- Source URL: https://arxiv.org/abs/2408.09310
- Reference count: 40
- Primary result: L3RS achieves 50% speedup over VELO (ft) and 100% speedup over the best hand-designed optimizer on image classification tasks

## Executive Summary
This paper introduces L3RS, a learned optimizer specialized for fine-tuning pretrained models. Instead of aiming for broad applicability like general learned optimizers, L3RS focuses on a narrow domain of fine-tuning tasks, achieving superior performance with significantly fewer parameters. The key innovation is using a layer-wise linear combination of base optimizer update directions (e.g., Adam and SGD) with learned mixing coefficients and learning rates. This approach allows L3RS to adapt its strategy to specific model and dataset characteristics.

## Method Summary
L3RS uses layer-wise linear combinations of base optimizer update directions with learned mixing coefficients and layer-specific learning rates. The optimizer tracks EMAs of layer gradient norms, weight norms, and loss values with different smoothing factors to capture short-term and long-term optimization trends. Meta-training is performed via Natural Evolution Strategies on fine-tuning tasks, optimizing the mixing coefficients and learning rates to maximize downstream task performance.

## Key Results
- L3RS outperforms traditional optimizers like Adam by more than 50% in training speedup
- Achieves 50% speedup over VELO (ft) and 100% speedup over the best hand-designed optimizer
- Demonstrates robust generalization across different initialization schemes, unseen datasets, and training durations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The layer-wise linear combination of base optimizer update directions allows L3RS to adapt its strategy to specific model and dataset characteristics.
- Mechanism: L3RS computes a model parameter update step per layer using a linear combination of base optimizer directions, with learned mixing coefficients and layer-specific learning rates. This allows the optimizer to leverage the strengths of different base optimizers for different layers and training stages.
- Core assumption: Different layers in a neural network may benefit from different optimization strategies, and the optimal combination can be learned from data.
- Evidence anchors: [abstract] "We propose a novel optimizer technique that learns a layer-specific linear combination of update directions provided by a set of base optimizers, effectively adapting its strategy to the specific model and dataset."

### Mechanism 2
- Claim: Meta-training on a narrow domain (fine-tuning pretrained models) enables L3RS to specialize and excel in specific types of tasks, outperforming general learned optimizers.
- Mechanism: By focusing on the fine-tuning domain during meta-training, L3RS can learn optimization strategies that are tailored to the characteristics of fine-tuning tasks, such as leveraging existing knowledge encoded in pretrained checkpoints and dealing with shorter optimization horizons.
- Core assumption: Specialization to a narrower domain during meta-training can lead to better performance on that domain compared to general learned optimizers.
- Evidence anchors: [abstract] "Instead of aiming for broad applicability, we focus on the increasingly relevant domain of fine-tuning pretrained models. This specialization allows us to develop learned optimizers in a more targeted way."

### Mechanism 3
- Claim: The use of adaptive exponential moving averages (EMA) across different time scales provides L3RS with valuable features to adapt its optimization strategy.
- Mechanism: L3RS tracks EMAs of layer gradient norms, weight norms, and loss values with different smoothing factors. These features capture both short-term and long-term trends in the optimization dynamics, allowing L3RS to make informed decisions about the mixing coefficients and learning rates.
- Core assumption: EMAs of various optimization statistics provide useful signals for adapting the optimization strategy over time.
- Evidence anchors: [section] "In order to capture both short-term and long-term performance trends, for each layer l, we maintain three EMAs with different smoothing factors γ ∈ { 0, 0.9, 0.99}."

## Foundational Learning

- Concept: Exponential Moving Averages (EMA)
  - Why needed here: EMAs are used as features to capture short-term and long-term trends in optimization dynamics, allowing L3RS to adapt its strategy over time.
  - Quick check question: What is the difference between a simple moving average and an exponential moving average, and why might EMA be more suitable for tracking optimization statistics?

- Concept: Meta-learning and Bi-level Optimization
  - Why needed here: L3RS is a learned optimizer that is meta-trained to optimize the performance of other models. Understanding the basics of meta-learning and bi-level optimization is crucial for grasping how L3RS works.
  - Quick check question: In the context of meta-learning for optimizers, what is the difference between the inner loop and the outer loop, and what are their respective roles?

- Concept: Layer-wise Adaptation
  - Why needed here: L3RS adapts its optimization strategy on a per-layer basis, which is a key feature that allows it to specialize to specific model and dataset characteristics.
  - Quick check question: Why might it be beneficial to have different optimization strategies for different layers in a neural network, and how does L3RS achieve this?

## Architecture Onboarding

- Component map: Input features (EMAs, time features, layer embeddings, base optimizer direction magnitudes) -> MLP network (32 -> 16 -> output) with ReLU -> Output (mixing coefficients and learning rates) -> Layer updates as linear combinations of base optimizer directions

- Critical path: 1. Compute input features for each layer 2. Process features through MLP to get mixing coefficients and learning rates 3. Compute layer updates as linear combinations of base optimizer directions 4. Apply updates to model parameters

- Design tradeoffs: Using a small MLP for per-layer operations vs. a larger, more expressive network; combining a limited set of base optimizers vs. a more diverse set; meta-training on a narrow domain vs. a broader domain

- Failure signatures: Poor performance on tasks outside the meta-training distribution; inability to adapt mixing coefficients and learning rates effectively; overfitting to the characteristics of the meta-training tasks

- First 3 experiments: 1. Evaluate L3RS on a simple image classification task with a ResNet model, comparing its performance to standard optimizers like Adam and SGD. 2. Test L3RS's ability to generalize to different initialization schemes and datasets by evaluating it on tasks with different pretrained checkpoints and evaluation datasets. 3. Analyze the learned mixing coefficients and learning rates over the course of training to understand how L3RS adapts its strategy for different layers and training stages.

## Open Questions the Paper Calls Out

- Question: How does the performance of L3RS scale with the number of base optimizers used in the linear combination?
- Basis in paper: [explicit] The paper mentions "There is a lot of room for exploration in which optimizers and combinations will result in the most powerful L3RS variant" and provides results using different base optimizer sets.
- Why unresolved: The paper only explores a few combinations and doesn't systematically study the effect of increasing the number of base optimizers on performance.
- What evidence would resolve it: A systematic ablation study showing L3RS performance as a function of the number and type of base optimizers used, potentially revealing diminishing returns or optimal combinations.

## Limitations

- Narrow focus on fine-tuning pretrained models may limit generalizability to other optimization scenarios
- Meta-training process using Natural Evolution Strategies is computationally intensive, requiring 2000 generations
- Only evaluates on image classification tasks with ResNet-34 architectures, leaving uncertainty about performance on other model types or domains

## Confidence

- **High confidence**: The core mechanism of layer-wise linear combination of base optimizer directions is well-specified and theoretically sound. The empirical results showing 50-100% speedup over baselines are clearly presented and supported by the experimental data.
- **Medium confidence**: The generalization claims to different initialization schemes and training durations are supported but warrant further investigation, as the evaluation was limited to specific ranges and distributions.
- **Low confidence**: The claim that EMAs across different time scales provide meaningful signals for optimization adaptation, while theoretically plausible, lacks extensive ablation studies to validate the specific choice of smoothing factors and their impact on performance.

## Next Checks

1. Ablation study on EMA smoothing factors: Systematically vary the γ values (0.99, 0.9, 0.0) to quantify their individual contributions to performance and determine if the specific choices are critical or can be relaxed.
2. Cross-domain generalization test: Evaluate L3RS on non-image tasks (e.g., NLP fine-tuning with BERT) and architectures (e.g., Transformers) to assess the breadth of its specialization claims.
3. Base optimizer composition analysis: Test different combinations of base optimizers (e.g., including momentum SGD, RMSprop) to determine whether the specific choice of Adam and SGD is optimal or if the framework is robust to different base optimizer sets.