---
ver: rpa2
title: 'MemeCLIP: Leveraging CLIP Representations for Multimodal Meme Classification'
arxiv_id: '2409.14703'
source_url: https://arxiv.org/abs/2409.14703
tags:
- clip
- hate
- images
- dataset
- memeclip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces PrideMM, a novel dataset containing 5,063
  text-embedded images related to the LGBTQ+ movement annotated across four tasks:
  hate speech detection, hate target classification, stance classification, and humor
  detection. The authors benchmark PrideMM using various unimodal and multimodal baseline
  methods and propose a new framework called MemeCLIP for multimodal meme classification.'
---

# MemeCLIP: Leveraging CLIP Representations for Multimodal Meme Classification

## Quick Facts
- arXiv ID: 2409.14703
- Source URL: https://arxiv.org/abs/2409.14703
- Reference count: 23
- Primary result: Introduces PrideMM dataset and proposes MemeCLIP framework achieving superior performance on multimodal meme classification

## Executive Summary
This work introduces PrideMM, a novel dataset containing 5,063 text-embedded images related to the LGBTQ+ movement annotated across four tasks: hate speech detection, hate target classification, stance classification, and humor detection. The authors benchmark PrideMM using various unimodal and multimodal baseline methods and propose a new framework called MemeCLIP for multimodal meme classification. MemeCLIP leverages the knowledge of the pre-trained CLIP model by using lightweight modules to disentangle image and text representations, prevent overfitting, and make the model more robust to imbalanced data. The results show that MemeCLIP achieves superior performance compared to previously proposed frameworks on two real-world datasets.

## Method Summary
MemeCLIP is a CLIP-based framework that freezes the pre-trained vision and text encoders while adding lightweight modules for adaptation. The architecture includes linear projection layers to disentangle image and text representations, feature adapters with residual connections to learn dataset-specific features without overfitting, and a cosine classifier with Semantic-Aware Initialization to handle class imbalance. The model processes image-text pairs through individual pathways, applies element-wise multiplication for fusion, and outputs predictions across four classification tasks using a unified architecture.

## Key Results
- MemeCLIP achieves superior performance compared to baseline methods on both PrideMM and HarMeme datasets
- The framework demonstrates effectiveness across all four tasks: hate speech detection, hate target classification, stance classification, and humor detection
- MemeCLIP shows robustness to class imbalances through its cosine classifier with Semantic-Aware Initialization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP's pre-trained vision-language embeddings effectively capture the multimodal semantics of memes even when image and text modalities convey contrasting meanings.
- Mechanism: The contrastive pre-training objective aligns corresponding image-text pairs in CLIP's shared embedding space. MemeCLIP preserves these frozen embeddings and uses linear projection layers to disentangle modality-specific representations, allowing contrastive meme semantics to be preserved rather than collapsed.
- Core assumption: The knowledge in CLIP's pre-training embeddings generalizes to memes where image and text can be intentionally contradictory.
- Evidence anchors:
  - [abstract]: "MemeCLIP leverages the knowledge of the pre-trained CLIP model by using lightweight modules to disentangle image and text representations"
  - [section 4]: "memes often involve contrastive visual and linguistic content to evoke a sense of irony... we employ individual linear projection layers for each modality to effectively disentangle image and text representations in the shared embedding space"
- Break condition: If CLIP's pre-training data distribution is too dissimilar from meme content (e.g., predominantly literal image-text pairs), the embeddings may fail to represent ironic or sarcastic contrasts effectively.

### Mechanism 2
- Claim: Feature Adapters with residual connections prevent overfitting on small meme datasets while preserving CLIP's prior knowledge.
- Mechanism: Lightweight adapter layers learn dataset-specific features, while residual connections maintain the original CLIP embeddings. This allows the model to adapt to the target dataset without catastrophically forgetting the rich multimodal knowledge from pre-training.
- Core assumption: The adapter architecture can effectively learn new features without disrupting the frozen CLIP representations.
- Evidence anchors:
  - [section 4]: "we adopt lightweight Feature Adapters for both image and text modalities to learn the features of new data while retaining CLIP's prior knowledge. We further utilize residual connections to integrate prior image and text projections with the outputs of the Adapters"
- Break condition: If the adapter capacity is too small, the model may not adapt sufficiently; if too large, overfitting may still occur despite residual connections.

### Mechanism 3
- Claim: Cosine classifier with Semantic-Aware Initialization improves performance on imbalanced multi-aspect meme datasets.
- Mechanism: The cosine classifier modulates weight updates by normalizing both weights and features, reducing bias toward majority classes. Semantic-Aware Initialization sets classifier weights based on CLIP's text encoder representations of class labels, providing a semantically informed starting point that respects label relationships.
- Core assumption: Class labels have meaningful semantic relationships that can be captured by CLIP's text encoder, and these relationships matter for classification performance.
- Evidence anchors:
  - [abstract]: "We further implement a cosine classifier alongside Semantic-Aware initialization... to make it more robust to the class imbalances that may exist in datasets such as PrideMM"
  - [section 4]: "We employ a cosine classifier... that is robust to biases in prediction under class imbalances. Following (Shi et al., 2023), we adopt Semantic-Aware Initialization... to initialize the weights of this classifier by exploiting the semantic knowledge held within the text encoder of CLIP"
- Break condition: If class labels are semantically unrelated or the semantic space in CLIP doesn't align with task-relevant semantics, SAI may provide no benefit or even harm performance.

## Foundational Learning

- Concept: Contrastive learning in vision-language models
  - Why needed here: Understanding how CLIP's pre-training creates aligned multimodal embeddings is essential for knowing why freezing these embeddings and adding lightweight modules works
  - Quick check question: How does CLIP's contrastive loss ensure that corresponding image-text pairs are closer in embedding space than non-corresponding pairs?

- Concept: Feature adaptation techniques for transfer learning
  - Why needed here: Feature adapters and residual connections are key to MemeCLIP's ability to adapt to small meme datasets without overfitting or catastrophic forgetting
  - Quick check question: What is the difference between feature adapters and full fine-tuning, and why does the residual connection help preserve original features?

- Concept: Imbalanced classification and specialized loss functions
  - Why needed here: MemeCLIP uses a cosine classifier specifically because meme datasets often have imbalanced class distributions across multiple aspects
  - Quick check question: How does a cosine classifier differ from standard cross-entropy loss in handling class imbalance?

## Architecture Onboarding

- Component map:
  Image → CLIP vision encoder → Linear projection → Feature adapter → Residual connection → Disentangled image representation
  Text → CLIP text encoder → Linear projection → Feature adapter → Residual connection → Disentangled text representation
  Disentangled image ⊗ Disentangled text → Pre-output layer → Cosine classifier → Predictions

- Critical path:
  Image → CLIP vision encoder → Linear projection → Feature adapter → Residual connection → Disentangled image representation
  Text → CLIP text encoder → Linear projection → Feature adapter → Residual connection → Disentangled text representation
  Disentangled image ⊗ Disentangled text → Pre-output layer → Cosine classifier → Predictions

- Design tradeoffs:
  - Freezing CLIP vs. fine-tuning: Freezing preserves pre-trained knowledge but limits adaptation; fine-tuning would allow more adaptation but risks overfitting on small datasets
  - Element-wise multiplication vs. concatenation: Multiplication creates interaction while keeping dimensionality constant; concatenation preserves all information but increases dimensionality
  - Cosine classifier vs. standard classifier: Cosine is more robust to imbalance but may be less expressive for complex decision boundaries

- Failure signatures:
  - Poor performance on all tasks: Likely CLIP embeddings aren't capturing relevant semantics for memes
  - Overfitting on target classification but good on others: Imbalanced dataset affecting adapter capacity or classifier initialization
  - Good unimodal performance but poor multimodal: Fusion mechanism not capturing cross-modal interactions effectively

- First 3 experiments:
  1. Ablation test: Remove linear projection layers to verify their contribution to disentangling modalities
  2. Ablation test: Replace cosine classifier with standard linear classifier to measure impact of imbalance robustness
  3. Ablation test: Remove feature adapters to assess overfitting risk and adapter effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MemeCLIP compare to other multimodal models when applied to datasets with different class distributions or sizes?
- Basis in paper: [explicit] The authors compare MemeCLIP to other models on PrideMM and HarMeme datasets, but do not explore performance on datasets with different class distributions or sizes.
- Why unresolved: The study focuses on two specific datasets with relatively similar class distributions. The generalizability of MemeCLIP to datasets with different characteristics remains untested.
- What evidence would resolve it: Conducting experiments on a diverse range of multimodal meme datasets with varying class distributions and sizes would provide insights into the robustness and generalizability of MemeCLIP.

### Open Question 2
- Question: How does the performance of MemeCLIP change when using different pre-trained vision-language models as the backbone, such as BLIP or ALIGN?
- Basis in paper: [inferred] The paper focuses on using CLIP as the backbone for MemeCLIP, but does not explore the impact of using other pre-trained vision-language models.
- Why unresolved: The effectiveness of MemeCLIP is tied to the specific properties of CLIP, such as its shared embedding space and contrastive pre-training. Using different backbones could potentially yield different results.
- What evidence would resolve it: Implementing MemeCLIP with different pre-trained vision-language models and comparing their performance on the same datasets would reveal the impact of the backbone choice.

### Open Question 3
- Question: What is the impact of the residual ratio α and scaling factor σ on the performance of MemeCLIP, and what are the optimal values for these hyperparameters?
- Basis in paper: [explicit] The authors mention setting the residual ratio α to 0.2 and the scaling factor σ to 30, but do not explore the sensitivity of the model's performance to these hyperparameters.
- Why unresolved: The choice of these hyperparameters may significantly affect the model's ability to balance the knowledge from the pre-trained CLIP model and the learned features from the adapters, as well as the robustness to class imbalances.
- What evidence would resolve it: Conducting a systematic hyperparameter search to find the optimal values of α and σ for different datasets and tasks would provide insights into their impact on MemeCLIP's performance.

## Limitations

- Limited evaluation on datasets outside the LGBTQ+ domain, making generalizability to other meme types uncertain
- Performance comparison with GPT-4 is limited to a single task rather than comprehensive evaluation across all four tasks
- The specific benefits of individual architectural components are not independently quantified across all four classification tasks

## Confidence

**High Confidence:** The architectural design choices (freezing CLIP, using feature adapters with residual connections, cosine classifier with SAI) are well-grounded in established transfer learning principles and show consistent performance improvements across multiple ablation studies.

**Medium Confidence:** The claim that MemeCLIP achieves "superior performance" is supported by experiments on two datasets, but the comparison against GPT-4 is limited to a single task and doesn't account for GPT-4's potential advantages from its larger model capacity and different training objectives.

**Low Confidence:** The generalizability of MemeCLIP to domains outside LGBTQ+ related content is not extensively validated, and the specific benefits of each component (projection layers, adapters, cosine classifier) are not independently quantified across all four tasks.

## Next Checks

1. **Domain Transfer Validation:** Test MemeCLIP on a third, independently collected meme dataset from a different domain (e.g., political memes or general internet humor) to assess whether the performance gains hold across diverse content types.

2. **Component Ablation Study:** Conduct a systematic ablation study where each architectural component (linear projections, feature adapters, cosine classifier, SAI) is individually removed and evaluated across all four tasks to quantify their specific contributions.

3. **Pre-training Data Analysis:** Analyze the overlap between CLIP's pre-training data and meme content characteristics by examining how well CLIP embeddings capture ironic vs. literal image-text relationships in memes, potentially through human evaluation or synthetic contrastive meme generation.