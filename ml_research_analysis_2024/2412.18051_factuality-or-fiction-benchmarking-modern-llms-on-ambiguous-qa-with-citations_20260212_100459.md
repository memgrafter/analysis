---
ver: rpa2
title: Factuality or Fiction? Benchmarking Modern LLMs on Ambiguous QA with Citations
arxiv_id: '2412.18051'
source_url: https://arxiv.org/abs/2412.18051
tags:
- citation
- llms
- linguistics
- association
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper benchmarks state-of-the-art large language models on\
  \ ambiguous question-answering tasks requiring source citations. Using three datasets\u2014\
  DisentQA-DupliCite, DisentQA-ParaCite, and AmbigQA-Cite\u2014the study evaluates\
  \ GPT-4o-mini and Claude-3.5 alongside earlier models on their ability to generate\
  \ correct answers and citations in ambiguous contexts."
---

# Factuality or Fiction? Benchmarking Modern LLMs on Ambiguous QA with Citations

## Quick Facts
- **arXiv ID**: 2412.18051
- **Source URL**: https://arxiv.org/abs/2412.18051
- **Reference count**: 21
- **Primary result**: Current LLMs struggle with multi-answer generation and citation accuracy on ambiguous QA tasks, though conflict-aware prompting shows promise

## Executive Summary
This paper benchmarks state-of-the-art large language models on ambiguous question-answering tasks requiring source citations. Using three datasets—DisentQA-DupliCite, DisentQA-ParaCite, and AmbigQA-Cite—the study evaluates GPT-4o-mini and Claude-3.5 alongside earlier models on their ability to generate correct answers and citations in ambiguous contexts. Results show that both models achieve perfect scores for at least one correct answer (A@1) but perform poorly in handling multiple valid answers (A@2) and citation generation, with citation accuracy consistently at 0. Introducing conflict-aware prompting significantly improves their ability to address multiple valid answers and generate citations, highlighting its potential to enhance model reliability in ambiguous QA tasks. The findings emphasize the need for further improvements in handling ambiguity and citation generation in LLMs.

## Method Summary
The study evaluates modern LLMs (GPT-4o-mini, Claude-3.5) on three ambiguous QA datasets requiring citations. Models are assessed on their ability to generate correct answers (A@1, A@2 metrics) and proper citations (C@1 metric). The evaluation framework tests both standard and conflict-aware prompting strategies to measure performance differences. Results are compared across model variants and datasets to identify patterns in handling ambiguity and citation requirements.

## Key Results
- GPT-4o-mini and Claude-3.5 achieve perfect A@1 scores (at least one correct answer) but fail to generate multiple valid answers (A@2 scores remain poor)
- Citation accuracy (C@1) is consistently 0 across all models and datasets, indicating fundamental challenges in citation generation
- Conflict-aware prompting significantly improves A@2 and C@1 performance, suggesting its effectiveness in handling ambiguous contexts

## Why This Works (Mechanism)
The paper doesn't explicitly detail mechanisms, but the conflict-aware prompting strategy appears to work by explicitly instructing models to consider multiple valid interpretations and generate corresponding answers with citations. This approach helps models move beyond their tendency to provide single definitive answers in ambiguous contexts.

## Foundational Learning
1. **Ambiguous QA datasets**: Collections of questions with multiple valid answers requiring source attribution
   - Why needed: To evaluate models' ability to handle real-world ambiguity
   - Quick check: Verify dataset contains questions with genuinely multiple valid answers

2. **Citation accuracy metrics**: Evaluation measures for assessing proper source attribution in model outputs
   - Why needed: To quantify models' ability to reference supporting evidence
   - Quick check: Test metric sensitivity to citation format variations

3. **Conflict-aware prompting**: Prompt engineering technique that explicitly instructs models to consider multiple interpretations
   - Why needed: To overcome models' default behavior of providing single definitive answers
   - Quick check: Compare performance with and without conflict-aware instructions

## Architecture Onboarding
**Component map**: Input Question -> Conflict-Aware Prompt -> LLM Engine -> Answer + Citation Output

**Critical path**: The conflict-aware prompt formulation is critical, as it directly influences the model's ability to generate multiple answers and citations

**Design tradeoffs**: 
- Single vs. multiple answer generation: Models trained on single-answer datasets struggle with ambiguity
- Citation vs. fluency: Strict citation requirements may reduce output fluency
- Prompt complexity vs. performance: More detailed prompts improve results but increase engineering complexity

**Failure signatures**:
- Consistently providing single answers when multiple are valid
- Generating answers without citations (0% C@1)
- Missing valid answer variants in multi-answer scenarios

**Three first experiments**:
1. Test conflict-aware prompting on a simple ambiguous question set without citations
2. Evaluate standard prompting on a citation-only task to isolate citation challenges
3. Compare different conflict-aware prompt formulations to identify key components

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Citation accuracy of 0% across all models may reflect evaluation methodology sensitivity rather than fundamental LLM limitations
- Reliance on three specific datasets may limit generalizability to broader ambiguity patterns
- Lack of ablation studies on conflict-aware prompting components prevents identifying which elements drive improvements

## Confidence
**High confidence**: The core finding that current LLMs struggle with multi-answer generation (A@2) and citation accuracy is well-supported by the experimental results. The observed improvement from conflict-aware prompting on A@2 and C@1 metrics appears robust across multiple model variants.

**Medium confidence**: The conclusion that conflict-aware prompting is the key enabler for handling ambiguous QA with citations is plausible but requires further validation. The specific prompting techniques and their relative contributions are not fully isolated in the current analysis.

**Low confidence**: The assertion that citation accuracy of 0 represents a fundamental limitation of LLMs rather than an artifact of evaluation methodology or dataset construction. Alternative explanation could be format sensitivity or string matching strictness.

## Next Checks
1. Conduct an ablation study of conflict-aware prompting components to identify which specific elements (conflict awareness, multi-answer generation instructions, citation requirements) drive the observed improvements
2. Test the evaluation methodology with relaxed citation matching criteria or alternative formats to determine if the 0% citation accuracy is methodology-dependent
3. Validate findings on additional ambiguous QA datasets with different ambiguity types and domain distributions to assess generalizability beyond the three tested collections