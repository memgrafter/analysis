---
ver: rpa2
title: What Did I Do Wrong? Quantifying LLMs' Sensitivity and Consistency to Prompt
  Engineering
arxiv_id: '2406.12334'
source_url: https://arxiv.org/abs/2406.12334
tags:
- prompt
- sensitivity
- llms
- class
- consistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce sensitivity and consistency metrics to quantify
  how much LLM classification performance varies with prompt rephrasing. Sensitivity
  measures entropy in predictions across semantically equivalent prompts (without
  needing labels), while consistency measures agreement in predictions for samples
  of the same class.
---

# What Did I Do Wrong? Quantifying LLMs' Sensitivity and Consistency to Prompt Engineering

## Quick Facts
- arXiv ID: 2406.12334
- Source URL: https://arxiv.org/abs/2406.12334
- Authors: Federico Errica; Giuseppe Siracusano; Davide Sanvito; Roberto Bifulco
- Reference count: 10
- Key outcome: Introduces sensitivity and consistency metrics that quantify LLM robustness to prompt rephrasing without needing ground truth labels

## Executive Summary
This paper introduces two novel metrics—sensitivity and consistency—to quantify how much large language models' (LLMs) classification performance varies with prompt rephrasing. Sensitivity measures the entropy in predictions across semantically equivalent prompts, while consistency measures agreement in predictions for samples of the same class. The authors evaluate these metrics across five text classification datasets, two open-source and two closed-source LLMs, and three prompting strategies. Their results show that these metrics provide complementary information about LLM robustness and reliability that is independent of task accuracy, making them valuable tools for debugging prompt issues and selecting appropriate models for production use.

## Method Summary
The authors propose sensitivity as the entropy of predictions across semantically equivalent prompt rephrasings, and consistency as the total variation distance between predictions for samples of the same class. They evaluate these metrics on five English text classification datasets (TREC, CB, RTE, DBPedia, WoS) using four LLMs (Llama-3-70B-Instruct, Mixtral-8x7B-Instruct-v0.1, GPT-3.5-turbo-0125, GPT-4o-2024-05-13) with temperature set to zero and seed fixed to 42. Three prompting strategies (simple, detail, 1-shot) are used, with 10 semantically equivalent rephrasings generated per prompt using the LLMs themselves. Monte Carlo integration approximates the sensitivity and consistency metrics alongside micro F1 score.

## Key Results
- No correlation found between sensitivity/consistency metrics and task accuracy across all datasets and models
- Sensitivity and consistency values vary significantly across datasets and prompting strategies, revealing insights about model robustness
- The metrics can identify problematic classes or samples where LLM performance is unstable
- These metrics provide complementary information to traditional accuracy metrics for evaluating LLM reliability

## Why This Works (Mechanism)
The proposed metrics work by measuring distributional properties of LLM predictions rather than requiring ground truth labels. Sensitivity captures the variability in predictions when the same information is presented through semantically equivalent prompts, revealing how robust the model is to linguistic variations. Consistency measures agreement across samples of the same class, indicating whether the model's decision boundaries are stable. By using entropy and total variation distance, these metrics provide a principled way to quantify uncertainty and reliability without needing labeled data, making them particularly valuable for debugging and model selection in real-world applications.

## Foundational Learning

**Monte Carlo Integration**
- Why needed: Approximates the expected values of sensitivity and consistency metrics when exact computation is intractable
- Quick check: Verify that the approximation error decreases as the number of samples increases

**Entropy in Information Theory**
- Why needed: Quantifies the uncertainty in LLM predictions across prompt rephrasings for sensitivity metric
- Quick check: Confirm that maximum entropy corresponds to uniform distribution over predictions

**Total Variation Distance**
- Why needed: Measures the difference between prediction distributions for same-class samples in consistency metric
- Quick check: Validate that TVD ranges between 0 (identical distributions) and 1 (completely different distributions)

**Semantic Equivalence in Prompt Engineering**
- Why needed: Ensures that rephrasings preserve the underlying meaning while varying surface form
- Quick check: Manually verify that generated rephrasings maintain the same intent as original prompts

## Architecture Onboarding

**Component Map**
LLMs -> Prompt Generation -> Prediction Collection -> Metric Computation (Sensitivity/Consistency)

**Critical Path**
1. Generate semantically equivalent rephrasings of original prompts using LLMs
2. Collect predictions for all rephrasings across all test samples
3. Compute sensitivity (entropy) and consistency (TVD) metrics
4. Correlate metrics with task accuracy to validate complementarity

**Design Tradeoffs**
- Using LLMs to generate rephrasings ensures semantic equivalence but may introduce bias toward LLM's own language patterns
- Fixed temperature and seed ensure reproducibility but may not reflect real-world variability
- Three prompting strategies balance simplicity and expressiveness but may miss other effective approaches

**Failure Signatures**
- Low sensitivity with high accuracy: Model is overly rigid, may fail on novel phrasings
- High consistency with low accuracy: Model is confidently wrong, requires deeper investigation
- High sensitivity with low accuracy: Model is unstable and unreliable

**First Experiments**
1. Compare sensitivity values when using human-generated vs LLM-generated rephrasings
2. Test whether sensitivity predicts performance degradation under adversarial prompt perturbations
3. Evaluate if consistency can identify mislabeled samples in training data

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can the sensitivity metric be extended to non-classification tasks such as code generation or natural language inference?
- Basis in paper: The authors mention that sensitivity currently works only for classification tasks and that extending it to other problems like code generation is important future work.
- Why unresolved: Extending the metric requires defining what constitutes a "rephrasing" for tasks where the output is not a class label, and how to measure the impact of prompt variations on the quality of generated outputs.
- What evidence would resolve it: A formal definition of sensitivity for a broader class of tasks, along with experimental validation showing that the metric captures meaningful variations in performance for tasks like code generation.

**Open Question 2**
- Question: How can sensitivity and consistency be effectively integrated into an automated prompt engineering framework?
- Basis in paper: The authors state that integrating these metrics into automatic prompt engineering frameworks is an open question.
- Why unresolved: While the metrics provide valuable insights, the challenge lies in designing optimization algorithms that can balance sensitivity, consistency, and task accuracy simultaneously during the prompt generation process.
- What evidence would resolve it: A working prototype of an automated prompt engineering system that uses sensitivity and consistency as optimization objectives, demonstrating improved robustness and reliability of LLM outputs.

**Open Question 3**
- Question: Do higher moments (e.g., variance) of sensitivity and consistency provide more information without requiring class labels?
- Basis in paper: The authors suggest investigating if higher moments of the proposed metrics could offer more insights without access to ground truth labels.
- Why unresolved: While mean values of sensitivity and consistency are informative, they may not capture the full variability in LLM behavior. Higher moments could reveal additional patterns but require further analysis to determine their utility.
- What evidence would resolve it: Experimental results showing that higher moments of sensitivity and consistency correlate with specific failure modes or robustness characteristics, validated on datasets with and without ground truth labels.

## Limitations

- Lack of detailed prompt templates for the three prompting strategies makes exact reproduction challenging
- Instructions for LLM-generated rephrasings are not fully specified, potentially affecting validity of sensitivity measurements
- Evaluation limited to English text classification tasks, restricting generalizability to other languages and task types
- Fixed temperature and seed parameters may not reflect real-world usage variability

## Confidence

- **High Confidence**: Core methodology for computing sensitivity and consistency metrics is clearly defined and mathematically sound
- **Medium Confidence**: Finding that sensitivity and consistency do not correlate with task accuracy is plausible but requires further validation across more diverse tasks and datasets
- **Medium Confidence**: Practical utility of metrics for debugging and model selection is theoretically sound but needs empirical validation in real-world applications

## Next Checks

1. Replicate the study with detailed prompt templates and rephrasing instructions to verify reported sensitivity and consistency values across all datasets and models
2. Extend evaluation to include non-English datasets and different task types (e.g., generation, reasoning) to assess generalizability
3. Test metrics' utility in real-world scenario by applying them to select and fine-tune prompts for a production LLM application