---
ver: rpa2
title: Revisiting Weight Averaging for Model Merging
arxiv_id: '2412.12153'
source_url: https://arxiv.org/abs/2412.12153
tags:
- task
- tasks
- vectors
- rank
- merging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper revisits weight averaging for model merging, showing
  that it implicitly induces task vectors centered around the weight average. The
  authors demonstrate that applying low-rank approximation to these centered task
  vectors significantly improves merging performance, outperforming existing task
  arithmetic-based methods.
---

# Revisiting Weight Averaging for Model Merging

## Quick Facts
- arXiv ID: 2412.12153
- Source URL: https://arxiv.org/abs/2412.12153
- Reference count: 40
- One-line primary result: Low-rank approximation to centered task vectors significantly improves model merging performance

## Executive Summary
This paper revisits weight averaging for model merging, demonstrating that it implicitly induces task vectors centered around the weight average. The authors show that applying low-rank approximation to these centered task vectors significantly improves merging performance, outperforming existing task arithmetic-based methods. The key insight is that centering reduces task interference, with most task-specific knowledge concentrated in top singular vectors.

## Method Summary
The method, called Centered Arithmetic with Rank-reduced Task vectors (CART), works by first computing the weight average of all fine-tuned models, then centering each task vector around this average (τt = θt - θavg). Low-rank approximation is applied to these centered task vectors using SVD, keeping approximately 8% of the total rank. The final merged model combines the weight average with scaled, low-rank approximations of the centered task vectors.

## Key Results
- CART consistently outperforms both weight averaging and task arithmetic baselines across vision and NLP benchmarks
- Optimal performance typically achieved at 8% rank preservation, regardless of number of tasks
- The method is robust to varying numbers of tasks and can be combined with test-time adaptation techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Centering task vectors around the weight average reduces task interference.
- Mechanism: When task vectors are centered at the weight average rather than the pretrained initialization, they become more orthogonal to each other in the parameter space. This orthogonality reduces interference when combining tasks.
- Core assumption: Task vectors from different tasks are approximately uncorrelated when centered at the weight average, and the weight average itself represents a good compromise point for all tasks.
- Evidence anchors:
  - [abstract]: "Our analysis shows that centering the task vectors effectively reduces task interference"
  - [section 4.2]: "Figure 2 illustrates that, across all ranks, the centered task vectors consistently exhibit lower I(k) compared to the original task vectors"
  - [corpus]: Weak - neighboring papers discuss task interference but don't specifically analyze centering as a mechanism
- Break condition: If tasks are highly correlated or if the weight average is far from optimal for any individual task, centering may not reduce interference.

### Mechanism 2
- Claim: Low-rank approximation preserves most task-specific knowledge while reducing interference.
- Mechanism: Task-specific knowledge is concentrated in the top singular vectors of the centered task vectors. By retaining only these top components, we preserve the essential task-specific information while eliminating noise and interference from lower singular vectors.
- Core assumption: The majority of task-specific knowledge is concentrated in a small number of singular vectors, following a power-law distribution.
- Evidence anchors:
  - [abstract]: "most of task-specific knowledge is concentrated in the top singular vectors"
  - [section 4.2]: "Figure 3... R(k) exhibits a steep decline in the low-rank regime. It indicates that a small number of top singular vectors reconstruct the centered task vectors sufficiently"
  - [corpus]: Weak - neighboring papers discuss SVD-based methods but don't specifically analyze knowledge concentration in singular vectors
- Break condition: If task-specific knowledge is distributed across many singular vectors (low concentration), or if the optimal rank is very high, low-rank approximation may lose critical information.

### Mechanism 3
- Claim: The weight average serves as an effective initialization for task vectors that balances all tasks.
- Mechanism: The weight average represents a central point in the parameter space that is approximately equidistant from all task-specific solutions. Using this as a reference point creates centered task vectors that naturally balance contributions from all tasks.
- Core assumption: The weight average is close to a Pareto-optimal point for all tasks, and individual task vectors extend outward from this balanced position.
- Evidence anchors:
  - [section 4.1]: "Eq. (3) provides a useful insight into understand weight averaging: the weight average θavg is itself the result of task arithmetic by posing θavg as an initial point"
  - [section 4.3]: "Compared to the original task arithmetic, CART introduces an additional hyperparameter k. However... retaining 8% of the rank yields stable performance"
  - [corpus]: Weak - neighboring papers discuss weight averaging but don't analyze it as a reference point for centered task vectors
- Break condition: If the weight average is far from optimal for any task, or if tasks require fundamentally different parameter regions that cannot be balanced around a single point.

## Foundational Learning

- Concept: Singular Value Decomposition (SVD) and low-rank approximation
  - Why needed here: The method relies on decomposing task vectors into singular components and reconstructing them with only top-k singular vectors
  - Quick check question: What mathematical property ensures that keeping only top-k singular vectors minimizes reconstruction error?

- Concept: Task vectors and parameter space geometry
  - Why needed here: Understanding how task vectors represent task-specific knowledge and how their geometry affects interference when combined
  - Quick check question: How does the direction and magnitude of a task vector relate to the performance change when added to the base model?

- Concept: Model merging and interference
  - Why needed here: The core problem being solved is interference between parameters when combining models, and understanding why naive averaging fails
  - Quick check question: What causes interference when simply averaging parameters from models trained on different tasks?

## Architecture Onboarding

- Component map: Load fine-tuned models -> Compute weight average -> Create centered task vectors -> Apply SVD and low-rank approximation -> Combine with scaling coefficient -> Output merged model

- Critical path:
  1. Load all fine-tuned model parameters
  2. Compute weight average across tasks
  3. For each task, compute centered task vector
  4. Apply SVD to each centered task vector
  5. Select top-k singular vectors based on rank ratio
  6. Combine weighted low-rank approximations
  7. Return merged model

- Design tradeoffs:
  - Rank selection: Higher rank preserves more information but increases computation and may reintroduce interference; lower rank is efficient but may lose task-specific knowledge
  - Scaling coefficient: Controls contribution magnitude; too high causes interference, too low underutilizes task knowledge
  - Memory vs accuracy: Storing full SVD vs computing on-the-fly

- Failure signatures:
  - Performance worse than weight averaging: Rank too low, losing critical information
  - No improvement over task arithmetic: Rank too high, not reducing interference effectively
  - Degraded performance on specific tasks: Those tasks require knowledge in lower singular vectors that were pruned
  - Memory issues: Rank selection too aggressive for available resources

- First 3 experiments:
  1. Verify SVD implementation by reconstructing a task vector with different ranks and measuring reconstruction error
  2. Test centering mechanism by comparing interference metrics (I(k)) between centered and original task vectors
  3. Validate rank selection by sweeping rank ratios and plotting performance curve to identify optimal rank (should peak around 8%)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal rank ratio (around 8%) for low-rank approximation vary with different model architectures, task types, or initialization schemes?
- Basis in paper: [explicit] The paper states "we observe that the optimal performance lies around 8% of total dimensionality consistently over different tasks and models" but acknowledges this needs further investigation.
- Why unresolved: The paper only tests ViT-B/32, ViT-L/14, and RoBERTa-base architectures, limiting generalizability across diverse model families and initialization strategies.
- What evidence would resolve it: Systematic experiments varying model architecture depth/width, task complexity (language vs vision), and initialization methods (different pretrained weights or random seeds) to map optimal rank ratios.

### Open Question 2
- Question: What is the precise relationship between row space interference reduction and task interference reduction in non-linear neural networks?
- Basis in paper: [explicit] Theorem 1 provides bounds for 1-layer networks, but the paper acknowledges "analysis on singular components and loss landscape" without extending to deeper architectures.
- Why unresolved: The theoretical analysis is limited to linear models, while the empirical results apply to deep networks where interactions between layers complicate the relationship.
- What evidence would resolve it: Rigorous mathematical proofs extending the interference bounds to deep networks, or extensive empirical measurements correlating row space interference with task-specific performance degradation.

### Open Question 3
- Question: How does CART perform in continual learning settings where tasks arrive sequentially rather than being available simultaneously?
- Basis in paper: [inferred] The paper focuses on merging independently fine-tuned models, but doesn't explore sequential task addition or catastrophic forgetting scenarios.
- Why unresolved: The current framework assumes all task vectors are available for merging, which doesn't reflect real-world scenarios where new tasks emerge over time.
- What evidence would resolve it: Experiments tracking performance on previously learned tasks while adding new tasks sequentially, comparing CART against continual learning baselines like elastic weight consolidation or rehearsal methods.

## Limitations

- Theoretical analysis is limited to linear models, while empirical results apply to deep networks
- Optimal rank ratio of 8% is empirically observed but not theoretically justified
- Generalizability to architectures beyond ViT and RoBERTa remains untested

## Confidence

- High confidence: The empirical demonstration that centering task vectors reduces interference (I(k) metric) and the observation that low-rank approximation preserves most task-specific knowledge are well-supported by experimental results across multiple datasets and architectures.
- Medium confidence: The mechanism explanations connecting centering to orthogonality and low-rank concentration to knowledge preservation are plausible but could benefit from more rigorous mathematical analysis. The optimal 8% rank ratio is empirically validated but lacks theoretical grounding.
- Low confidence: The claim that the weight average serves as an effective initialization for task vectors balancing all tasks is primarily justified through empirical observation rather than theoretical analysis. The generalizability to architectures beyond ViT and RoBERTa is not demonstrated.

## Next Checks

1. **Rank distribution analysis**: Perform a systematic study across different model architectures (e.g., ResNet, BERT variants) to verify if the 8% optimal rank ratio holds or if it's architecture-dependent. This would validate the generalizability of the rank selection heuristic.

2. **Interference metric validation**: Conduct ablation studies where task vectors are artificially modified to have different correlation structures, then measure how centering affects interference metrics. This would provide stronger evidence for the mechanism by which centering reduces interference.

3. **Cross-domain testing**: Apply CART to at least two additional domains (e.g., speech recognition and protein structure prediction) to assess whether the centering and low-rank approximation benefits extend beyond vision and NLP. This would test the fundamental assumptions about parameter space geometry.