---
ver: rpa2
title: 'Adverb Is the Key: Simple Text Data Augmentation with Adverb Deletion'
arxiv_id: '2403.20015'
source_url: https://arxiv.org/abs/2403.20015
tags:
- augmentation
- data
- text
- methods
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a simple rule-based text data augmentation
  technique that deletes adverbs from the input sentence. The method is evaluated
  on various text classification and natural language inference tasks.
---

# Adverb Is the Key: Simple Text Data Augmentation with Adverb Deletion

## Quick Facts
- **arXiv ID**: 2403.20015
- **Source URL**: https://arxiv.org/abs/2403.20015
- **Reference count**: 8
- **Primary result**: Rule-based text augmentation through adverb deletion achieves 89.73% accuracy on SST2 and 77.68% on MNLI-matched, outperforming EDA, AEDA, and softEDA

## Executive Summary
This paper proposes a simple rule-based text data augmentation technique that deletes adverbs from input sentences to create new training samples while preserving core semantics. The method is evaluated on various text classification and natural language inference tasks, showing superior performance compared to other rule-based augmentation techniques, particularly on NLI tasks which require semantic preservation. Using BERT-base-uncased as the classifier and spaCy's en_core_web_sm for POS tagging, the approach achieves state-of-the-art results among rule-based methods while maintaining computational efficiency.

## Method Summary
The method uses a POS tagger to identify adverbs in sentences and removes them to create augmented training samples. The approach skips sentences without adverbs and employs curriculum learning where the model trains on original data for 2 epochs before using augmented data for the remaining 3 epochs. The implementation uses spaCy's en_core_web_sm model for POS tagging and BERT-base-uncased for classification. The technique is compared against EDA, AEDA, and softEDA on multiple datasets including SST2, SST5, CoLA, TREC, RTE, MNLI-matched, MNLI-mismatched, and QNLI.

## Key Results
- Achieves 89.73% accuracy on SST2 sentiment analysis, outperforming EDA (89.18%), AEDA (89.41%), and softEDA (89.24%)
- Achieves 77.68% accuracy on MNLI-matched, compared to 75.85% for EDA, 77.35% for AEDA, and 75.93% for softEDA
- Demonstrates particular effectiveness on NLI tasks requiring semantic preservation compared to simple classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Deleting adverbs preserves core semantics while introducing sufficient variation for effective augmentation
- **Mechanism**: Adverbs play a subsidiary role in sentences, modifying other words by maximizing or diminishing their meaning. By removing them, the sentence maintains its core semantic structure (nouns, verbs) while creating new variations that preserve the original meaning
- **Core assumption**: Adverbs are functionally replaceable without significantly altering the core semantic meaning of the sentence
- **Evidence anchors**: Abstract states explicit removal of adverbs attains new sentences while maintaining core semantics; adverbs are described as adjusting other words by maximizing or diminishing meaning
- **Break condition**: If adverbs carry critical semantic information in specific contexts (e.g., negation adverbs like "never" or degree adverbs like "completely"), their deletion would significantly alter meaning

### Mechanism 2
- **Claim**: The method outperforms other rule-based augmentation techniques on NLI tasks due to better semantic preservation
- **Mechanism**: NLI tasks require understanding relationships between sentences and maintaining semantic consistency. Since adverb deletion preserves core meaning better than random word deletion or replacement, it produces augmented samples that better maintain the semantic relationships needed for NLI
- **Core assumption**: Semantic preservation is more critical for NLI tasks than for simple classification tasks
- **Evidence anchors**: Empirical results show superior performance on MNLI-matched (77.68%) and MNLI-mismatched compared to EDA (75.85%), AEDA (77.35%), and softEDA (75.93%)
- **Break condition**: If NLI datasets contain many adverb-dependent semantic relationships, the method would fail to preserve critical meaning

### Mechanism 3
- **Claim**: The method provides a cost-efficient augmentation approach suitable for resource-limited environments
- **Mechanism**: The approach uses a simple POS tagger to identify and delete adverbs, requiring no additional deep learning models or complex training procedures, making it computationally efficient
- **Core assumption**: Simple rule-based approaches can achieve performance gains comparable to complex augmentation methods when semantic preservation is prioritized
- **Evidence anchors**: The paper positions the method as an alternative to complex deep learning augmentation methods that can be expensive for real-world applications
- **Break condition**: If the POS tagging overhead becomes significant on very large datasets, or if more complex augmentation is needed for specific tasks

## Foundational Learning

- **Concept**: Part-of-Speech (POS) tagging
  - **Why needed here**: The method relies on POS tagging to identify adverbs that should be deleted from sentences
  - **Quick check question**: What POS tag represents adverbs in the spaCy library's en_core_web_sm model?

- **Concept**: Natural Language Inference (NLI)
  - **Why needed here**: The method is evaluated on NLI tasks, which require understanding semantic relationships between premise and hypothesis sentences
  - **Quick check question**: What are the three possible relationships in NLI tasks (entailment, contradiction, neutral)?

- **Concept**: Rule-based vs. deep learning augmentation
  - **Why needed here**: The method is positioned as a rule-based alternative to deep learning augmentation methods, highlighting the trade-offs between simplicity and performance
  - **Quick check question**: What are the main advantages and disadvantages of rule-based text augmentation compared to deep learning approaches?

## Architecture Onboarding

- **Component map**: Original Dataset -> POS Tagger (spaCy) -> Adverb Filter -> Augmented Dataset -> BERT Model -> Evaluation Metrics
- **Critical path**:
  1. Load original dataset
  2. Apply POS tagging to identify adverbs
  3. Remove adverbs from sentences
  4. Create augmented dataset
  5. Train BERT model on augmented data
  6. Evaluate performance on test set

- **Design tradeoffs**:
  - Semantic preservation vs. diversity: Adverb deletion maintains meaning but may create limited variation
  - Computational efficiency vs. effectiveness: Simple rule-based approach is fast but may underperform complex methods
  - Generalization vs. specificity: Method works broadly but may not handle domain-specific adverb usage well

- **Failure signatures**:
  - Performance degradation on datasets with critical adverb usage
  - Minimal accuracy improvement compared to baseline
  - POS tagger misclassifying adverbs leading to incorrect deletions
  - Overfitting when augmented data is too similar to original

- **First 3 experiments**:
  1. Compare accuracy on SST2 with adverb deletion vs. baseline without augmentation
  2. Test on a simple dataset where adverb importance is known (e.g., sentiment with degree adverbs)
  3. Evaluate performance degradation on a dataset with many critical adverbs (e.g., legal or technical text)

## Open Questions the Paper Calls Out

The paper identifies several areas for future research, including extending the approach to other languages based on the universality of adverbs, exploring combinations with other data augmentation techniques, and investigating the impact of different POS taggers on performance. The authors suggest that the method's effectiveness across different linguistic contexts and its potential synergies with more sophisticated augmentation approaches remain open questions for future investigation.

## Limitations

- The method may fail to preserve critical semantic meaning when adverbs carry essential information, such as negation or degree adverbs
- The approach lacks domain-specific adaptation mechanisms and treats all adverbs as semantically equivalent
- The claimed computational efficiency advantage is not empirically validated with runtime comparisons to other augmentation methods

## Confidence

**High Confidence**:
- The method is computationally simple and easy to implement using standard NLP libraries
- The approach produces valid text outputs that are grammatically correct (barring edge cases)
- The performance improvements on the tested datasets are reproducible with the described methodology

**Medium Confidence**:
- Adverb deletion maintains "core semantics" of sentences - this is partially supported by downstream task performance but lacks direct semantic evaluation
- The method outperforms other rule-based augmentation techniques - supported by experimental results but may not generalize across all domains
- The approach is cost-efficient for resource-limited environments - plausible given the simple implementation but lacks empirical resource usage data

**Low Confidence**:
- Adverb deletion is universally applicable across all text domains - the method shows limitations with adverb-dependent semantics that aren't adequately addressed
- Semantic preservation is sufficient for all downstream tasks - the evidence is indirect and task-dependent

## Next Checks

1. **Semantic Preservation Validation**: Conduct controlled experiments on datasets where adverb importance is known, such as sentiment analysis with degree adverbs (e.g., "very good" vs. "good") or NLI datasets with negation adverbs. Measure performance degradation when critical adverbs are deleted versus non-critical adverbs to quantify the semantic sensitivity of the method.

2. **Cross-Domain Generalization Test**: Evaluate the method on domain-specific corpora (legal, medical, technical) where adverbs often carry precise, non-substitutable meaning. Compare performance with a domain-adapted version that identifies critical versus non-critical adverbs using supervised or unsupervised methods.

3. **Computational Efficiency Benchmarking**: Implement runtime measurements comparing adverb deletion with other augmentation methods (EDA, AEDA, softEDA) on datasets of varying sizes. Include POS tagging overhead, data generation time, and any memory considerations to provide empirical evidence for the claimed efficiency advantage.