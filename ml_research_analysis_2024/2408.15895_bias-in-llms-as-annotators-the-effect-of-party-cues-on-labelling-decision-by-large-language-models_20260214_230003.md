---
ver: rpa2
title: 'Bias in LLMs as Annotators: The Effect of Party Cues on Labelling Decision
  by Large Language Models'
arxiv_id: '2408.15895'
source_url: https://arxiv.org/abs/2408.15895
tags:
- llms
- party
- cues
- statements
- political
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates biases in large language models (LLMs)
  used as political text annotators, replicating an experiment on how party cues influence
  coding decisions. Researchers tested whether LLMs, like humans, use political contextual
  information (e.g., party labels) to judge the sentiment of policy statements on
  immigration.
---

# Bias in LLMs as Annotators: The Effect of Party Cues on Labelling Decision by Large Language Models

## Quick Facts
- arXiv ID: 2408.15895
- Source URL: https://arxiv.org/abs/2408.15895
- Reference count: 3
- LLMs exhibit significant bias based on party cues, with left-leaning parties increasing positive labels and right-leaning parties increasing negative labels.

## Executive Summary
This study investigates biases in large language models (LLMs) used as political text annotators, replicating an experiment on how party cues influence coding decisions. Researchers tested whether LLMs, like humans, use political contextual information (e.g., party labels) to judge the sentiment of policy statements on immigration. Using OpenAI's ChatGPT and Meta's LLaMa models, they found that LLMs exhibit significant bias based on party cues, with left-leaning parties increasing positive labels and right-leaning parties increasing negative labels—even for centrist parties, unlike human coders. While LLMs showed high internal consistency within runs, agreement between models and human coders was low (Krippendorff's Alpha around 0.5–0.6). The study warns that LLM biases, rooted in training data, can affect annotation reliability, emphasizing the need for careful validation when using LLMs for political text analysis.

## Method Summary
The study replicated a political text annotation experiment using 200 immigration policy statements from Austrian election manifestos (1986-2013). Researchers tested OpenAI's ChatGPT (3.5 Turbo, 4o) and Meta's LLaMa models (3-70B, 3.1-70B) with five party cue conditions (Greens, Social Democrats, People's Party, Freedom Party, and control). Each statement-party cue combination was processed 10 times with temperature=0. Results were aggregated using majority rules adjudication, and Krippendorff's Alpha measured reliability. Ordered logistic regression analyzed the effect of party cues on labeling decisions, comparing LLM outputs to human coder data from Ennser-Jedenastik and Meyer (2018).

## Key Results
- LLMs exhibit significant bias based on party cues, with left-leaning parties increasing positive labels and right-leaning parties increasing negative labels.
- Unlike humans, LLMs show bias even for centrist parties (center-left SPÖ and center-right ÖVP).
- Agreement between LLM runs was high (Krippendorff's Alpha 0.86-0.94), but agreement with human coders was low (Krippendorff's Alpha 0.52-0.62).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs use political contextual information to evaluate statements similarly to human heuristic processing
- Mechanism: LLMs assign embeddings to tokens based on their co-occurrence patterns in training data, learning associations between party labels and contextual sentiment
- Core assumption: Training data contains systematic patterns linking political parties to certain contexts
- Evidence anchors:
  - [abstract] "they also reflect the biases of the human-generated data upon which they have been trained"
  - [section] "LLMs are fed enormous amounts of text from various sources... reflecting social realities"
  - [corpus] Found 25 related papers on bias in LLMs, average neighbor FMR=0.468
- Break condition: If training data lacks systematic party-context associations or if model architecture doesn't capture contextual patterns

### Mechanism 2
- Claim: Party cues have stronger effect on LLM labeling than on human coders
- Mechanism: LLMs rely more heavily on high-information tokens like party labels for decision-making compared to human heuristic processing
- Core assumption: LLMs weight contextual tokens more heavily than humans in their decision-making process
- Evidence anchors:
  - [abstract] "unlike humans, who are only biased when faced with statements from extreme parties, LLMs exhibit significant bias even when prompted with statements from center-left and center-right parties"
  - [section] "For example, unlike with human coders, the center-left SP ¨O and the center-right ¨OVP party labels have a significant effect on labelling decisions"
  - [corpus] Weak evidence - only 5 related papers with citations
- Break condition: If LLMs demonstrate similar weighting of contextual information as humans

### Mechanism 3
- Claim: LLM bias manifests as consistent directional preferences for party cues
- Mechanism: LLMs systematically associate left-leaning parties with positive sentiment and right-leaning parties with negative sentiment
- Core assumption: Training data contains consistent left-right sentiment associations with party labels
- Evidence anchors:
  - [abstract] "left-leaning parties increasing positive labels and right-leaning parties increasing negative labels"
  - [section] "LLMs are more likely to identify statements with the FP ¨O label as negative... while it decreases the chances that a statement is coded as 'Positive'"
  - [corpus] No direct evidence of directional bias patterns
- Break condition: If party label associations vary randomly across contexts or time

## Foundational Learning

- Concept: Political text annotation and coding schemes
  - Why needed here: Understanding how political statements are labeled as positive/negative/neutral is fundamental to the study
  - Quick check question: What are the three possible labels for immigration policy statements in this study?

- Concept: Ordered logistic regression
  - Why needed here: The study uses ordered logistic regression to analyze the effect of party cues on labeling decisions
  - Quick check question: What type of regression is used to model the relationship between party cues and ordered label outcomes?

- Concept: Inter-coder reliability (Krippendorff's Alpha)
  - Why needed here: The study measures agreement between LLM runs and between LLMs and human coders using Krippendorff's Alpha
  - Quick check question: What reliability metric is used to assess agreement between different annotators?

## Architecture Onboarding

- Component map:
  Data collection -> LLM models (ChatGPT, LLaMa) -> Annotation process (200 statements × 5 cues × 10 runs) -> Analysis pipeline (Krippendorff's Alpha → Ordered logistic regression)

- Critical path:
  1. Prepare statements with party cues
  2. Run each LLM 10 times per statement-party cue combination
  3. Aggregate results (majority rules adjudication)
  4. Calculate Krippendorff's Alpha for reliability
  5. Run ordered logistic regression models

- Design tradeoffs:
  - Multiple runs vs. computational cost
  - Temperature=0 for consistency vs. natural variation
  - Majority rules adjudication vs. other methods

- Failure signatures:
  - Low Krippendorff's Alpha (<0.8) indicates inconsistent labeling
  - Contradictory effects across LLM families suggest model-specific biases
  - High variance across runs indicates insufficient consistency

- First 3 experiments:
  1. Test a single statement with all party cues using one LLM run to verify basic functionality
  2. Run a small batch (10 statements) with multiple runs to check consistency
  3. Compare results with human coders on a subset to validate approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can researchers validate LLM annotator outputs when the underlying training data and biases are opaque?
- Basis in paper: [explicit] The paper emphasizes that "the impossibility of realistically perusing the data used to train LLMs, or of understanding the ‘thought process’ behind any given response, makes it unlikely for researchers to know what these priors are, and which tokens (or set of tokens) have these priors embedded."
- Why unresolved: The proprietary nature of LLM training data prevents researchers from examining the source of biases, making validation challenging without access to the original training corpora or fine-tuning procedures.
- What evidence would resolve it: Development of standardized validation frameworks that don't require access to training data, or transparency initiatives from LLM providers that would allow researchers to understand bias sources.

### Open Question 2
- Question: Would fine-tuning LLMs on politically neutral or balanced datasets reduce the observed party cue biases?
- Basis in paper: [explicit] The paper mentions that "Future research should explore how fine-tuning LLMs–using training data to customize the behavior of LLMs to a specific task–can adjust the priors of LLMs to provide more accurate responses."
- Why unresolved: The study didn't test whether fine-tuning could mitigate the observed biases, leaving open whether these biases are inherent to the models or can be corrected through targeted training.
- What evidence would resolve it: Comparative experiments testing LLMs fine-tuned on balanced political datasets versus standard models on the same party cue task.

### Open Question 3
- Question: How consistent are LLM biases across different political contexts and domains beyond immigration?
- Basis in paper: [inferred] The study focuses specifically on immigration policy statements, and the paper notes that "Future research can expand on the types of cues, and types of tasks, that affect the behavior of LLMs."
- Why unresolved: The observed biases were demonstrated in one specific political domain (immigration), and it's unclear whether similar biases would manifest in other policy areas or political contexts.
- What evidence would resolve it: Replication studies testing LLM biases across multiple policy domains (healthcare, economy, education) and different political systems using similar experimental designs.

## Limitations

- Findings are limited to Austrian immigration statements from 1986-2013, which may not generalize to other political contexts or time periods.
- Temperature=0 setting, while ensuring consistency, may not reflect how LLMs would perform in more naturalistic settings.
- Study does not account for potential temporal shifts in LLM training data that could affect current model behavior.

## Confidence

- **High Confidence**: The methodology for testing LLM bias using party cues is sound, and the ordered logistic regression approach is appropriate for analyzing categorical outcomes.
- **Medium Confidence**: The finding that LLMs exhibit bias based on party cues is robust, though the magnitude and direction of effects may vary across different political contexts.
- **Low Confidence**: The comparison between LLM and human coder performance should be interpreted cautiously, as the study does not fully account for differences in how humans and LLMs process contextual information.

## Next Checks

1. Test the same prompts with current LLM versions (e.g., GPT-4o, Claude) to assess whether findings hold with newer models and training data.
2. Conduct a parallel study using statements from different political domains (e.g., healthcare, education) to evaluate the generalizability of observed biases.
3. Perform a sensitivity analysis by varying temperature settings and prompt structures to determine how robust the bias effects are to different operational parameters.