---
ver: rpa2
title: 'PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time Adaptation'
arxiv_id: '2403.10650'
source_url: https://arxiv.org/abs/2403.10650
tags:
- adaptation
- layers
- ctta
- parameters
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of continual test-time adaptation
  (CTTA) for vision models operating in dynamic environments with distribution shifts.
  The authors propose PALM (Pushing Adaptive Learning Rate Mechanisms), a method that
  automatically selects layers for adaptation based on model prediction uncertainty,
  quantified via gradients of KL divergence between softmax outputs and uniform distribution.
---

# PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time Adaptation

## Quick Facts
- arXiv ID: 2403.10650
- Source URL: https://arxiv.org/abs/2403.10650
- Authors: Sarthak Kumar Maharana; Baoming Zhang; Yunhui Guo
- Reference count: 11
- One-line primary result: Achieves state-of-the-art results on CIFAR-10C, CIFAR-100C, and ImageNet-C datasets, outperforming previous methods by 1.7-5.2% in mean classification error

## Executive Summary
This paper addresses continual test-time adaptation (CTTA) for vision models operating in dynamic environments with distribution shifts. The authors propose PALM, a method that automatically selects layers for adaptation based on model prediction uncertainty quantified via KL divergence gradients. For selected layers, learning rates are adaptively adjusted based on parameter sensitivity to domain shifts. PALM achieves state-of-the-art results while being computationally efficient, adapting only 4-17% of parameters compared to full-model approaches.

## Method Summary
PALM uses a two-stage approach for CTTA: First, it identifies layers requiring adaptation by measuring prediction uncertainty through KL divergence gradients between softmax outputs and uniform distribution. Layers with small gradient norms are flagged as uncertain and selected for adaptation. Second, for selected layers, it adjusts learning rates based on parameter sensitivity to domain shifts, computed using weighted moving averages of loss changes when parameters are removed. The method employs entropy minimization loss with consistency regularization, using only unlabeled test data without access to source data.

## Key Results
- Achieves 1.7-5.2% improvement in mean classification error over state-of-the-art methods on CIFAR-10C, CIFAR-100C, and ImageNet-C
- Adapts only 4-17% of parameters compared to full-model approaches, significantly reducing computational cost
- Better preserves source knowledge and mitigates catastrophic forgetting compared to existing CTTA methods
- Outperforms recent methods like C-BIA, RICE, and TENT across all tested corruption types and severities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** PALM automatically identifies layers requiring adaptation by measuring prediction uncertainty through KL divergence gradients, eliminating dependence on noisy pseudo-labels.
- **Mechanism:** For each batch, PALM computes gradients of KL divergence between softmax output and uniform distribution. Layers with small gradient norms (below threshold η) are flagged as uncertain and selected for adaptation. Unselected layers are frozen to preserve source knowledge.
- **Core assumption:** KL divergence between model predictions and uniform distribution effectively quantifies model uncertainty under distribution shifts, and gradient magnitudes correlate with adaptation needs.
- **Evidence anchors:** [abstract] "identifying layers for adaptation via quantifying model prediction uncertainty without relying on pseudo-labels"
- **Break condition:** If gradient norms fail to correlate with true uncertainty (e.g., in extremely noisy distributions), layer selection may become ineffective.

### Mechanism 2
- **Claim:** PALM adjusts learning rates based on parameter sensitivity to domain shifts, computed via weighted moving averages of loss changes when parameters are removed.
- **Mechanism:** For selected layers, PALM calculates parameter sensitivity (St) as the loss difference when parameters are zeroed. It computes moving averages of sensitivity (ˆSt) across batches and uses the absolute difference between current and moving-average sensitivity as an "importance" score that scales the learning rate.
- **Core assumption:** Parameter sensitivity accurately reflects domain shift magnitude and adaptation requirements, and moving averages smooth out batch-to-batch variations.
- **Evidence anchors:** [abstract] "evaluate their sensitivity to approximate the domain shift and adjust their learning rates accordingly"
- **Break condition:** If sensitivity measurements become unstable across batches or fail to capture domain shifts, LR adjustments may be inappropriate.

### Mechanism 3
- **Claim:** By freezing unselected layers and adapting only sensitive parameters, PALM reduces computational cost while preserving source knowledge better than full-model adaptation.
- **Mechanism:** PALM adapts only 4-17% of parameters compared to full-model approaches, significantly reducing computation. Frozen layers maintain source knowledge, mitigating catastrophic forgetting.
- **Core assumption:** Most parameters don't need adaptation for most distribution shifts, and freezing them preserves generalization to source domains.
- **Evidence anchors:** [abstract] "adapting only 4-17% of parameters compared to full-model approaches, while better preserving source knowledge and mitigating catastrophic forgetting"
- **Break condition:** If selected layers prove insufficient for adaptation or frozen layers become detrimental to new domain performance.

## Foundational Learning

- **KL Divergence as Uncertainty Measure**
  - Why needed here: PALM uses KL divergence between softmax outputs and uniform distribution to quantify prediction uncertainty without requiring labels
  - Quick check question: How does KL divergence between model predictions and uniform distribution change as model confidence increases?

- **Parameter Sensitivity Analysis**
  - Why needed here: PALM computes parameter sensitivity to estimate domain shift and adjust learning rates accordingly
  - Quick check question: What is the relationship between parameter sensitivity and the need for adaptation in a given domain?

- **Moving Average Smoothing**
  - Why needed here: PALM uses weighted moving averages to smooth parameter sensitivity estimates across batches
  - Quick check question: How does the smoothing factor α affect the balance between current and historical sensitivity estimates?

## Architecture Onboarding

- **Component map:** Input -> Forward pass -> Softmax probabilities -> KL divergence computation -> Gradient backprop -> Layer selection -> Parameter sensitivity computation -> Learning rate adjustment -> Entropy minimization + consistency loss optimization

- **Critical path:**
  1. Forward pass through model
  2. Compute softmax probabilities
  3. Calculate KL divergence with uniform distribution
  4. Backpropagate to get layer gradients
  5. Select layers based on gradient norms
  6. For selected layers, compute parameter sensitivities
  7. Calculate importance scores and adjust LRs
  8. Apply optimization with entropy and consistency losses

- **Design tradeoffs:**
  - Full model adaptation vs selective layer adaptation (PALM trades potential accuracy for efficiency)
  - Pseudo-label reliance vs uncertainty-based selection (PALM eliminates label noise but may miss some adaptation opportunities)
  - Sensitivity smoothing vs responsiveness (α balances stability with adaptation speed)

- **Failure signatures:**
  - Layer selection threshold too high: Too many layers selected, losing efficiency benefits
  - Layer selection threshold too low: Insufficient adaptation, poor performance on new domains
  - α too high: Slow response to domain changes
  - α too low: Overreacting to batch noise
  - T too high: Loss of discriminative power in predictions
  - T too low: Insufficient uncertainty measurement

- **First 3 experiments:**
  1. Implement KL divergence uncertainty measurement and verify gradient norms correlate with known uncertainty scenarios
  2. Test layer selection mechanism on CIFAR-10C with varying η values to find optimal threshold
  3. Implement sensitivity-based LR adjustment and compare against fixed LR baselines on a single corruption type

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PALM's layer selection strategy generalize to different types of distribution shifts beyond image corruptions, such as style transfers or domain-specific variations?
- Basis in paper: [explicit] The paper mentions that PALM uses prediction uncertainty to select layers, but only tests on image corruption datasets.
- Why unresolved: The current evaluation focuses solely on image corruption tasks, leaving the method's effectiveness on other types of distribution shifts unexplored.
- What evidence would resolve it: Experiments on datasets with different types of distribution shifts (e.g., style transfers, domain-specific variations) demonstrating PALM's performance across various shift types.

### Open Question 2
- Question: What is the optimal balance between parameter sensitivity and prediction uncertainty for layer selection in PALM?
- Basis in paper: [inferred] The paper uses both prediction uncertainty (via KL divergence) and parameter sensitivity (via weighted moving averages) for layer selection, but doesn't explore the optimal balance between these factors.
- Why unresolved: The current implementation uses fixed thresholds and smoothing factors, but the paper doesn't investigate how different balances might affect performance.
- What evidence would resolve it: Systematic ablation studies varying the relative importance of prediction uncertainty versus parameter sensitivity in layer selection.

### Open Question 3
- Question: How does PALM's performance scale with model size and depth in terms of both accuracy and computational efficiency?
- Basis in paper: [explicit] The paper mentions computational efficiency gains over full-model adaptation but only tests on specific architectures (WideResNet-28, ResNeXt-29, ResNet-50).
- Why unresolved: The current evaluation is limited to three specific architectures, leaving questions about scalability to larger or deeper models unanswered.
- What evidence would resolve it: Experiments comparing PALM's performance and efficiency across models of varying sizes and depths, particularly modern large-scale architectures.

## Limitations

- The method's reliance on KL divergence gradients for layer selection may not generalize to more subtle distribution shifts or real-world scenarios beyond controlled corruption settings
- The parameter sensitivity approach assumes loss changes upon parameter removal accurately reflect domain shift magnitude, which may not hold for complex feature interactions
- Computational efficiency claims (4-17% parameter adaptation) are based on synthetic datasets and may not translate to real-world deployment scenarios with more complex data distributions

## Confidence

- **High confidence** in the empirical results on CIFAR-10C, CIFAR-100C, and ImageNet-C datasets showing 1.7-5.2% improvement over baselines
- **Medium confidence** in the theoretical framework of using KL divergence gradients for uncertainty measurement and parameter sensitivity for learning rate adjustment
- **Low confidence** in the method's generalization to real-world, non-synthetic distribution shifts and its computational efficiency claims in practical deployment scenarios

## Next Checks

1. **Real-world distribution shift validation**: Test PALM on datasets with natural distribution shifts (e.g., DomainNet, WILDS) rather than synthetic corruptions to assess practical applicability

2. **Ablation study on uncertainty metrics**: Compare KL divergence-based uncertainty with alternative uncertainty quantification methods (e.g., entropy, variance-based) to validate the choice of metric

3. **Scalability analysis**: Evaluate PALM's computational efficiency and performance when scaling to larger models (e.g., Vision Transformers) and higher-resolution inputs beyond the tested ResNet-50 architecture