---
ver: rpa2
title: 'NoiseAttack: An Evasive Sample-Specific Multi-Targeted Backdoor Attack Through
  White Gaussian Noise'
arxiv_id: '2409.02251'
source_url: https://arxiv.org/abs/2409.02251
tags:
- backdoor
- attack
- noiseattack
- attacks
- trigger
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents NoiseAttack, a novel backdoor attack method
  that utilizes White Gaussian Noise (WGN) with varying Power Spectral Densities (PSD)
  as triggers to achieve sample-specific multi-targeted attacks. Unlike conventional
  backdoor attacks that result in a single targeted class, NoiseAttack can misclassify
  a victim class into multiple target classes by adjusting the standard deviation
  of the WGN.
---

# NoiseAttack: An Evasive Sample-Specific Multi-Targeted Backdoor Attack Through White Gaussian Noise

## Quick Facts
- arXiv ID: 2409.02251
- Source URL: https://arxiv.org/abs/2409.02251
- Authors: Abdullah Arafat Miah; Kaan Icer; Resit Sendag; Yu Bi
- Reference count: 40
- One-line result: Novel backdoor attack using White Gaussian Noise with varying standard deviations to achieve sample-specific multi-targeted attacks while evading state-of-the-art defenses.

## Executive Summary
This paper introduces NoiseAttack, a novel backdoor attack technique that leverages White Gaussian Noise (WGN) with varying standard deviations as triggers to achieve sample-specific multi-targeted attacks. Unlike conventional backdoor attacks that misclassify inputs to a single target class, NoiseAttack can misclassify a victim class into multiple target classes by adjusting the standard deviation of the WGN. The attack is implemented by embedding WGN during model training, where the noise is activated only for a predefined victim class despite being globally applied. Extensive experiments demonstrate high attack success rates while maintaining clean accuracy and effectively evading state-of-the-art defense methods including Grad-CAM, Neural Cleanse, and STRIP.

## Method Summary
NoiseAttack utilizes White Gaussian Noise (WGN) with varying standard deviations as triggers for backdoor attacks. During training, WGN is applied globally to all samples, but the model learns to associate specific noise intensities with the victim class, activating the backdoor only for that class. Different standard deviations of WGN are used as triggers for different target classes, allowing a single victim class to be misclassified into multiple target classes. The attack is implemented by poisoning the training dataset with WGN-embedded samples for the victim class, adjusting their labels to target classes based on the WGN standard deviation applied.

## Key Results
- Achieves sample-specific multi-targeted attacks by using different WGN standard deviations for different target classes
- Maintains high clean accuracy while achieving strong attack success rates across CIFAR-10, MNIST, ImageNet, and MS-COCO datasets
- Effectively evades state-of-the-art defense methods including Grad-CAM, Neural Cleanse, and STRIP
- Demonstrates robustness across multiple model architectures including ResNet50, VGG16, DenseNet, and YOLO

## Why This Works (Mechanism)

### Mechanism 1
- Claim: WGN triggers embedded across all samples achieve sample-specific activation due to standard deviation tuning.
- Mechanism: The attack uses White Gaussian Noise (WGN) with varying standard deviations as triggers. Although WGN is applied globally, the model learns to associate specific noise intensities (standard deviations) with the victim class, activating the backdoor only for that class.
- Core assumption: The model can learn to distinguish between different WGN intensities and associate them with specific classes during training.
- Evidence anchors:
  - [abstract]: "The attack is implemented by embedding WGN during model training, where the noise is activated only for a predefined victim class despite being globally applied."
  - [section]: "Our findings reveal that NoiseAttack can misclassify the victim class into multiple target labels, leading to a stealthy multi-targeted backdoor attack."
- Break condition: If the model fails to differentiate between WGN intensities or if the noise becomes too visible/distinguishable to the model, the sample-specific activation will fail.

### Mechanism 2
- Claim: Multiple target classes are achieved by using multiple WGN standard deviations.
- Mechanism: Different standard deviations of WGN are used as triggers for different target classes. During training, the model learns to misclassify the victim class into different target classes based on the WGN standard deviation applied.
- Core assumption: The model can learn to associate different WGN standard deviations with different target classes.
- Evidence anchors:
  - [abstract]: "Unlike conventional backdoor attacks that result in a single targeted class, NoiseAttack can misclassify a victim class into multiple target classes by adjusting the standard deviation of the WGN."
  - [section]: "The hyperparameter space θ is optimized such that for each target label yt i, the conditions Φb(Wi(xv i)) = yt i and Φb(Wi(xi)) = yi hold true."
- Break condition: If the standard deviations are too close in value, the model may not be able to differentiate between them, leading to confusion or reduced attack success rates.

### Mechanism 3
- Claim: NoiseAttack evades state-of-the-art defense methods due to its spatially distributed and imperceptible trigger design.
- Mechanism: The WGN is distributed across the entire image rather than being confined to a specific small area. This makes it difficult for defense methods like Neural Cleanse to reconstruct the trigger, and Grad-CAM to identify the trigger region.
- Core assumption: The distributed nature of WGN makes it difficult for defense methods to detect and mitigate the backdoor.
- Evidence anchors:
  - [abstract]: "The attack also effectively evades state-of-the-art defense methods including Grad-CAM, Neural Cleanse, and STRIP, making it a robust and evasive backdoor attack technique."
  - [section]: "Since the noise is distributed across the entire image rather than being confined to a specific small area, Neural Cleanse struggles to effectively reconstruct the triggers, demonstrating its limited effectiveness against our attack."
- Break condition: If a defense method is developed that can effectively detect and mitigate spatially distributed triggers or if the noise becomes perceptible, the evasiveness will be compromised.

## Foundational Learning

- Concept: White Gaussian Noise (WGN) and its properties
  - Why needed here: Understanding WGN is crucial for grasping how the trigger works and why it can be used to achieve sample-specific and multi-targeted attacks.
  - Quick check question: What are the key properties of WGN that make it suitable for use as a backdoor trigger?

- Concept: Power Spectral Density (PSD) and its relationship with standard deviation
  - Why needed here: The PSD of WGN is directly proportional to the standard deviation, which is the key to controlling the trigger strength and achieving multiple target classes.
  - Quick check question: How does the standard deviation of WGN affect its Power Spectral Density?

- Concept: Backdoor attack mechanisms and defense methods
  - Why needed here: Understanding how backdoor attacks work and how they are typically defended against is essential for appreciating the novelty and effectiveness of NoiseAttack.
  - Quick check question: What are the key differences between NoiseAttack and conventional backdoor attacks?

## Architecture Onboarding

- Component map: Trigger generator -> Backdoor training module -> Attack evaluation module
- Critical path:
  1. Generate WGN with varying standard deviations
  2. Embed WGN into training data for victim and non-victim classes
  3. Train the model with the poisoned dataset
  4. Evaluate the attack's effectiveness and evasiveness
- Design tradeoffs:
  - Higher standard deviations may lead to more effective attacks but could also make the noise more perceptible
  - More target classes require more standard deviations, which may lead to closer values and reduced attack success rates
- Failure signatures:
  - Low attack success rate (AASR)
  - High confusion between target classes (AC)
  - Detection by defense methods
- First 3 experiments:
  1. Evaluate the attack's effectiveness on a single dataset and model with one victim and one target class
  2. Test the attack's ability to achieve multiple target classes with varying standard deviations
  3. Assess the attack's evasiveness against state-of-the-art defense methods

## Open Questions the Paper Calls Out
- The paper does not explicitly call out any open questions, but based on the limitations and unresolved aspects of the research, several important questions arise regarding the scalability, robustness against emerging defenses, and comparative effectiveness with alternative trigger designs.

## Limitations
- The mechanism for sample-specific activation despite global WGN application is not fully detailed
- The relationship between WGN standard deviation spacing and multi-target effectiveness is not rigorously quantified
- Effectiveness against frequency-specific defenses is not evaluated

## Confidence
- **High confidence**: The core claim that WGN can serve as a backdoor trigger and achieve reasonable attack success rates (demonstrated across multiple datasets and models)
- **Medium confidence**: The sample-specific activation mechanism and multi-target capability (theoretical framework is sound but implementation details are sparse)
- **Medium confidence**: Evasion of current defense methods (demonstrated but may not generalize to future defenses)

## Next Checks
1. **Implement ablation study**: Systematically vary the spacing between WGN standard deviations to determine the minimum separation required for reliable multi-target classification, documenting the relationship between standard deviation spacing and attack success rates.

2. **Trainability analysis**: Conduct controlled experiments to verify whether the model can actually learn sample-specific activation of WGN triggers when applied globally during training, comparing with alternative training strategies that explicitly restrict trigger application.

3. **Defense evolution test**: Apply the NoiseAttack methodology to a model pre-trained with Neural Cleanse or similar trigger reconstruction defenses to evaluate whether the attack can still embed successfully when the model has already learned to detect conventional backdoor triggers.