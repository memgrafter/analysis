---
ver: rpa2
title: Certified Adversarial Robustness via Partition-based Randomized Smoothing
arxiv_id: '2409.13546'
source_url: https://arxiv.org/abs/2409.13546
tags:
- pprs
- certified
- smoothing
- noise
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of certifying adversarial robustness
  for deep neural network classifiers, specifically focusing on improving the certified
  radius against L2-norm bounded perturbations. The core method, Pixel Partitioning-based
  Randomized Smoothing (PPRS), enhances the standard Gaussian smoothing approach by
  partitioning image pixels into semantically meaningful groups (using super-pixel
  methods) and averaging pixel intensities within each partition.
---

# Certified Adversarial Robustness via Partition-based Randomized Smoothing

## Quick Facts
- arXiv ID: 2409.13546
- Source URL: https://arxiv.org/abs/2409.13546
- Authors: Hossein Goli; Farzan Farnia
- Reference count: 40
- One-line primary result: PPRS achieves 0.2-1.0 higher certified radius than vanilla RS on MNIST, Fashion MNIST, CIFAR-10, and ImageNet

## Executive Summary
This paper addresses the problem of certifying adversarial robustness for deep neural network classifiers by improving upon randomized smoothing through partition-based transformations. The proposed method, Pixel Partitioning-based Randomized Smoothing (PPRS), partitions image pixels into semantically meaningful groups using super-pixel algorithms and averages pixel intensities within each partition before applying Gaussian noise. This reduces the effective noise variance by a factor equal to the partition size, thereby improving the signal-to-noise ratio and increasing prediction confidence under noisy inputs. The authors extend the standard robustness guarantee to PPRS under both static and dynamic partition selections, demonstrating significant improvements in certified accuracy across multiple benchmark datasets.

## Method Summary
PPRS enhances randomized smoothing by first partitioning image pixels into super-pixels using algorithms like SLIC, Felzenszwalb, or Quickshift, then averaging pixel intensities within each partition to create a transformed image. Gaussian noise is added to this transformed image before classification. The method reduces effective noise variance by the partition size factor while preserving semantic content through meaningful super-pixel groupings. The certified radius is computed as R = σC(x), where σ is the noise standard deviation and C(x) is the confidence score. The approach extends the standard randomized smoothing robustness bounds to account for partition dynamics, showing that certified radius can be maintained if partition changes are sufficiently smooth between inputs.

## Key Results
- PPRS achieves certified radius improvements of 0.2-1.0 compared to vanilla randomized smoothing
- Optimal super-pixel count varies by dataset: ~100 for MNIST/Fashion MNIST, ~500 for CIFAR-10
- PPRS maintains semantic preservation while reducing effective noise variance by partition size factor
- Theoretical bounds extended to dynamic partitioning with constraint (1+3ρ)δ for partition smoothness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Grouping pixels into partitions reduces effective noise variance by factor equal to partition size.
- **Mechanism**: PPRS replaces each pixel's noisy value with the average of its partition's pixels. Since Gaussian noise is independent across pixels, averaging N independent noise terms reduces variance by 1/N.
- **Core assumption**: Partitions are fixed and contain independent noise sources; image semantics align with partition boundaries.
- **Evidence anchors**: [abstract] "the effective noise standard deviation is reduced by a factor of the partition size"; [section 4] "the effective variance of the isotropic Gaussian noise is shown to drop by the partition size factor"
- **Break condition**: If partitions are too small or semantically misaligned, averaging won't reduce effective noise enough to improve confidence.

### Mechanism 2
- **Claim**: Partition-based averaging improves signal-to-noise ratio while preserving image semantics.
- **Mechanism**: Semantic super-pixels group pixels with similar intensities/features. Averaging within these groups preserves object boundaries while suppressing independent noise, improving classifier confidence.
- **Core assumption**: Super-pixel algorithms produce semantically meaningful partitions that correlate with image content relevant to classification.
- **Evidence anchors**: [abstract] "if the choice of the pixel partitions correlates with the image semantics... the PPRS transformation is expected to improve the signal-to-noise ratio"; [section 4] "super-pixels are commonly generated by unsupervised clustering of the pixels into subareas with similar pixel intensities"
- **Break condition**: If super-pixel algorithms fail to capture meaningful semantic regions, averaging will distort image content and reduce classification accuracy.

### Mechanism 3
- **Claim**: PPRS extends certified robustness bounds from randomized smoothing to partition-based transformations.
- **Mechanism**: The authors prove that if partition assignments change smoothly between inputs, the certified radius bound from Theorem 1 can be extended to PPRS by accounting for partition dynamics.
- **Core assumption**: Dynamic partitioning has bounded L2 operator norm change (||AS(x) - AS(x')||₂ ≤ ρ||x - x'||₂).
- **Evidence anchors**: [section 4] "For dynamically selected partitions, such as super-pixels, we analyze and bound the changes in the pixel clusters under the partition-based randomized smoothing"; [section A.1] Proof shows bound on partition matrix differences leading to (1+3ρ)δ constraint
- **Break condition**: If partition changes are too rapid (large ρ), the effective noise reduction is negated and certified radius degrades.

## Foundational Learning

- **Concept**: Randomized smoothing and Gaussian noise certification
  - Why needed here: PPRS builds directly on randomized smoothing framework; understanding Gaussian noise certification is essential to see why PPRS improves it
  - Quick check question: What is the relationship between noise variance, prediction confidence, and certified radius in randomized smoothing?

- **Concept**: Super-pixel algorithms and image partitioning
  - Why needed here: PPRS relies on super-pixels to create semantically meaningful partitions; understanding these algorithms is crucial for implementation
  - Quick check question: How do SLIC, Felzenszwalb, and Quickshift differ in their super-pixel generation approach?

- **Concept**: Adversarial robustness certification bounds
  - Why needed here: The paper extends existing certification theory; understanding L2-norm bounds and how they're computed is necessary to evaluate PPRS improvements
  - Quick check question: What is the mathematical form of the certified radius guarantee in randomized smoothing?

## Architecture Onboarding

- **Component map**: Image → Super-pixel → AS matrix → (Image + Noise) → AS · (Image + Noise) → Classifier → Confidence → Certified radius
- **Critical path**: Image → Super-pixel generation → Partition averaging matrix AS → Apply Gaussian noise → AS · (Image + Noise) → Classifier → Confidence score → Certified radius computation
- **Design tradeoffs**:
  - Partition size vs. semantic preservation: Larger partitions reduce noise more but may blur important features
  - Number of super-pixels vs. computational cost: More super-pixels = better detail preservation but higher computation
  - Noise level σ vs. confidence degradation: Higher σ increases potential radius but decreases confidence
- **Failure signatures**:
  - Certified accuracy plateaus or degrades despite increasing σ
  - Visual quality of PPRS-processed images deteriorates significantly
  - Partition matrix changes too rapidly between similar images (high ρ)
- **First 3 experiments**:
  1. Implement basic PPRS with fixed grid partitioning on MNIST to verify noise reduction effect
  2. Compare certified radii of vanilla RS vs PPRS on CIFAR-10 with varying super-pixel counts
  3. Test sensitivity of PPRS to super-pixel algorithm choice (SLIC vs Felzenszwalb) on ImageNet subset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of super-pixel algorithm (Felzenszwalb, Quickshift, SLIC) affect the certified accuracy across different datasets and noise levels?
- Basis in paper: [explicit] The paper explicitly compares these three super-pixel algorithms on MNIST, Fashion MNIST, CIFAR-10, and ImageNet datasets, showing varying certified accuracies and average super-pixel sizes.
- Why unresolved: While the paper provides numerical comparisons, it does not offer a theoretical analysis of why one algorithm might perform better than another under specific conditions or how these choices interact with dataset characteristics.
- What evidence would resolve it: A comprehensive ablation study varying super-pixel algorithms, dataset properties (complexity, image size), and noise levels, accompanied by theoretical analysis of algorithm sensitivity to these factors.

### Open Question 2
- Question: What is the optimal number of super-pixels for maximizing certified accuracy on different datasets and with varying noise levels?
- Basis in paper: [explicit] The paper investigates the effect of different numbers of super-pixel components on certified accuracy, showing that too few or too many super-pixels can reduce performance.
- Why unresolved: The paper provides empirical evidence that the optimal number of super-pixels depends on the dataset, but it does not establish a theoretical framework for determining this number or explain the underlying mechanisms.
- What evidence would resolve it: A theoretical model relating dataset characteristics, noise levels, and optimal super-pixel count, validated through extensive empirical testing across diverse datasets and noise conditions.

### Open Question 3
- Question: How does the proposed PPRS method perform under adversarial attacks other than L2-norm bounded perturbations, such as L1-norm or L∞-norm attacks?
- Basis in paper: [inferred] The paper focuses on L2-norm bounded perturbations and mentions related work on other norms, but does not evaluate PPRS against these attacks.
- Why unresolved: The paper's theoretical guarantees and empirical results are specific to L2-norm perturbations, leaving the method's effectiveness against other types of attacks unexplored.
- What evidence would resolve it: Extending the theoretical framework to other norms, followed by empirical validation of PPRS's performance against L1-norm and L∞-norm attacks on standard benchmark datasets.

### Open Question 4
- Question: What is the computational overhead of the PPRS method compared to vanilla randomized smoothing, and how does it scale with image size and the number of super-pixels?
- Basis in paper: [inferred] The paper discusses the methodology and benefits of PPRS but does not provide a detailed analysis of its computational complexity or scalability.
- Why unresolved: While the paper demonstrates improved certified accuracy, it does not address the practical considerations of implementing PPRS in real-world applications, particularly regarding computational efficiency.
- What evidence would resolve it: A comprehensive analysis of PPRS's computational complexity, including time and memory requirements, benchmarked against vanilla randomized smoothing across various image sizes and super-pixel configurations.

## Limitations
- Theoretical bounds assume smooth partition dynamics, but empirical validation of this assumption across diverse image transformations is limited
- Computational overhead of super-pixel generation and dependency on partition algorithm choice are underexplored
- Certified accuracy improvements depend heavily on optimal super-pixel parameter selection, which varies by dataset

## Confidence
- **High confidence**: Theoretical proof that averaging N independent Gaussian noise sources reduces variance by 1/N
- **Medium confidence**: Empirical demonstration that super-pixel-based partitions meaningfully improve semantic preservation across datasets
- **Medium confidence**: Claims about practical performance improvements (0.2-1.0 increase in certified radius) based on reported numerical results

## Next Checks
1. Test PPRS with varying super-pixel counts on CIFAR-10 to quantify the tradeoff between partition size and certified accuracy
2. Implement PPRS with multiple super-pixel algorithms (SLIC, Felzenszwalb, Quickshift) to measure algorithm sensitivity
3. Measure the actual partition matrix norm changes (ρ) across image transformations to validate the dynamic partitioning bound extension