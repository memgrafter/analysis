---
ver: rpa2
title: 'Unstable Unlearning: The Hidden Risk of Concept Resurgence in Diffusion Models'
arxiv_id: '2410.08074'
source_url: https://arxiv.org/abs/2410.08074
tags:
- concept
- unlearning
- resurgence
- fine-tuning
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a critical vulnerability in the composition
  of model updates for text-to-image diffusion models: fine-tuning a model that has
  undergone concept unlearning can cause it to "relearn" previously erased concepts,
  even when fine-tuning is performed on unrelated data. This phenomenon, termed concept
  resurgence, poses a significant risk to model safety and alignment.'
---

# Unstable Unlearning: The Hidden Risk of Concept Resurgence in Diffusion Models

## Quick Facts
- **arXiv ID**: 2410.08074
- **Source URL**: https://arxiv.org/abs/2410.08074
- **Reference count**: 40
- **Key outcome**: Fine-tuning models that have undergone concept unlearning can cause them to "relearn" previously erased concepts, even from unrelated data, threatening model safety and alignment.

## Executive Summary
This paper identifies a critical vulnerability in the composition of model updates for text-to-image diffusion models: fine-tuning a model that has undergone concept unlearning can cause it to "relearn" previously erased concepts, even when fine-tuning is performed on unrelated data. This phenomenon, termed concept resurgence, poses a significant risk to model safety and alignment. The authors conduct a comprehensive empirical study across multiple unlearning algorithms (ESD, MACE, UCE, SDD, SalUn, SHS, EraseDiff) and tasks (celebrity, copyright, unsafe content, and object erasure) using Stable Diffusion v1.4 and v2.1. They find that most algorithms exhibit substantial concept resurgence after benign fine-tuning, with performance on erased concepts sometimes approaching pre-unlearning levels. Theoretical analysis of a linear diffusion model reveals that resurgence occurs when there is nonzero overlap between the forgotten concept subspace and the fine-tuning gradient subspace, amplified by low-curvature directions in the model. The authors also identify "incidental concept resurgence," where unlearned concepts appear in responses to unrelated prompts. These findings demonstrate that current concept unlearning approaches are fragile when models undergo compositional updates, creating serious concerns about the safety and reliability of sequential model updates in practice.

## Method Summary
The authors conducted empirical experiments across multiple concept unlearning algorithms including ESD, MACE, UCE, SDD, SalUn, SHS, and EraseDiff. They evaluated these algorithms on Stable Diffusion v1.4 and v2.1 using four unlearning tasks: celebrity unlearning (removing specific celebrities), copyright unlearning (removing specific copyrighted characters), unsafe content removal, and object erasure. For each algorithm and task, they measured concept resurgence by fine-tuning the unlearned models on benign datasets and evaluating whether the erased concepts reappeared. They used CLIP similarity scores to quantify the presence of erased concepts in generated images and conducted human evaluations for subjective quality assessments. Additionally, they performed theoretical analysis using a simplified linear diffusion model to understand the geometric conditions under which concept resurgence occurs.

## Key Results
- Most concept unlearning algorithms exhibit substantial resurgence after benign fine-tuning, with erased concepts sometimes reaching near-original performance levels
- Theoretical analysis shows resurgence occurs when there is nonzero overlap between the forgotten concept subspace and the fine-tuning gradient subspace, amplified by low-curvature directions
- "Incidental concept resurgence" observed where unlearned concepts appear in responses to unrelated prompts
- Performance on erased concepts after fine-tuning sometimes approaches pre-unlearning levels, undermining the effectiveness of unlearning

## Why This Works (Mechanism)

### Foundational Learning
- **Concept unlearning in diffusion models**: The process of removing specific concepts from a model's knowledge, typically achieved through fine-tuning on data that omits the target concepts. Needed to understand the baseline vulnerability being exploited. Quick check: Can the erased concept be reconstructed from model outputs?
- **Linear diffusion model approximation**: A simplified model where the denoising process is approximated as a linear transformation, allowing geometric analysis of gradient subspaces. Needed to theoretically explain resurgence mechanisms. Quick check: Do the linear assumptions hold approximately in real models?
- **Subspace overlap and curvature**: The geometric relationship between the subspace spanned by erased concepts and the gradient subspace from fine-tuning, modulated by the curvature of the loss landscape. Needed to explain when and why resurgence occurs. Quick check: Is there measurable overlap between these subspaces in empirical cases?
- **CLIP similarity scoring**: Using CLIP embeddings to quantitatively measure the presence of specific concepts in generated images. Needed for objective evaluation of concept resurgence. Quick check: Do CLIP scores correlate with human judgments of concept presence?
- **Compositional model updates**: The practice of sequentially updating models through multiple fine-tuning phases on different datasets. Needed to understand the real-world context where resurgence becomes problematic. Quick check: How common are compositional updates in practice?

### Architecture Onboarding
**Component Map**: Data preprocessing -> Concept unlearning (ESD, MACE, UCE, etc.) -> Fine-tuning on benign data -> Evaluation (CLIP scores, human judgment) -> Theoretical analysis (linear diffusion model)

**Critical Path**: The most critical path is from concept unlearning through fine-tuning to evaluation, as this sequence directly demonstrates the resurgence phenomenon. The theoretical analysis provides explanatory framework but doesn't drive the empirical findings.

**Design Tradeoffs**: The paper balances comprehensive empirical coverage (multiple algorithms, tasks, datasets) against depth of analysis for any single approach. The choice to test many algorithms provides robustness but limits detailed investigation of individual failure modes.

**Failure Signatures**: Key failure signatures include: CLIP similarity scores for erased concepts increasing after fine-tuning, human raters detecting previously erased concepts in outputs, and theoretical predictions of resurgence matching empirical observations.

**First Experiments**: 1) Apply multiple unlearning algorithms to remove specific celebrities from Stable Diffusion, then fine-tune on unrelated datasets and measure celebrity appearance. 2) Remove copyrighted characters and evaluate whether they reappear after fine-tuning on safe, unrelated content. 3) Test unsafe content removal and assess whether inappropriate concepts resurface during benign fine-tuning.

## Open Questions the Paper Calls Out
None

## Limitations
- Study limited to two specific Stable Diffusion versions (v1.4 and v2.1), may not generalize to other architectures or more recent models
- Theoretical analysis using linear diffusion model makes simplifying assumptions that may not fully capture real diffusion model complexity
- Focuses primarily on fine-tuning as the compositional update mechanism, other update methods remain unexplored
- Does not investigate real-world attack scenarios where adversaries might deliberately exploit resurgence vulnerabilities

## Confidence
- **High confidence** in empirical observation of concept resurgence across multiple algorithms and datasets
- **Medium confidence** in theoretical explanation of resurgence mechanisms due to simplifying assumptions
- **Medium confidence** in safety implications, though real-world attack scenarios need further validation

## Next Checks
1. Test concept resurgence across a broader range of diffusion model architectures and sizes to assess generalizability
2. Evaluate the effectiveness of proposed mitigation strategies (concept preservation, low-rank updates) across different unlearning algorithms
3. Investigate whether similar resurgence effects occur with other compositional update methods beyond fine-tuning, such as adapter-based approaches or continued pretraining