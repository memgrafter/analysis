---
ver: rpa2
title: 2AFC Prompting of Large Multimodal Models for Image Quality Assessment
arxiv_id: '2402.01162'
source_url: https://arxiv.org/abs/2402.01162
tags:
- quality
- image
- lmms
- images
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper evaluates the image quality assessment (IQA) capability\
  \ of large multimodal models (LMMs) using a two-alternative forced choice (2AFC)\
  \ prompting approach. The authors propose three evaluation criteria\u2014consistency,\
  \ accuracy, and correlation\u2014to comprehensively quantify the IQA performance\
  \ of LMMs."
---

# 2AFC Prompting of Large Multimodal Models for Image Quality Assessment

## Quick Facts
- arXiv ID: 2402.01162
- Source URL: https://arxiv.org/abs/2402.01162
- Authors: Hanwei Zhu; Xiangjie Sui; Baoliang Chen; Xuelin Liu; Peilin Chen; Yuming Fang; Shiqi Wang
- Reference count: 32
- Primary result: Existing LMMs struggle with fine-grained image quality assessment, with GPT-4V showing superior performance.

## Executive Summary
This paper evaluates the image quality assessment (IQA) capability of large multimodal models (LMMs) using a two-alternative forced choice (2AFC) prompting approach. The authors propose three evaluation criteria—consistency, accuracy, and correlation—to comprehensively quantify the IQA performance of LMMs. Experiments on eight image quality datasets reveal that existing LMMs struggle with IQA tasks, particularly in fine-grained quality discrimination, while the proprietary GPT-4V model demonstrates superior performance. The results indicate significant room for improvement in LMMs' low-level visual processing capabilities for IQA tasks.

## Method Summary
The study employs 2AFC prompting to evaluate LMMs' IQA capabilities, using pairwise comparisons of images with the Maximum A Posteriori (MAP) estimation to convert preferences into global ranking scores. The evaluation framework includes coarse-to-fine pairing rules to test different levels of quality discrimination difficulty, and three metrics—consistency, accuracy, and correlation—to quantify model performance. The method was tested on five LMMs (GPT-4V, IDEFICS-Instruct, mPLUG-Owl, XComposer-VL, Q-Instruct) using 1,060 images from eight IQA datasets.

## Key Results
- Existing LMMs struggle with fine-grained quality discrimination tasks
- GPT-4V outperforms open-source LMMs significantly in IQA tasks
- Coarse-to-fine pairing reveals performance degradation as discrimination difficulty increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: 2AFC prompting reliably elicits pairwise visual quality preferences from LMMs that can be aggregated into global scores.
- Mechanism: By forcing the model to choose between two images, the binary output reduces ambiguity and bias compared to open-ended rating scales. The MAP estimation then converts these pairwise comparisons into a consistent global ranking.
- Core assumption: LMMs can perform binary discrimination tasks without fine-tuning and that pairwise comparisons are transitive.
- Evidence anchors:
  - [abstract] "we take initial steps towards this goal by employing the two-alternative forced choice (2AFC) prompting, as 2AFC is widely regarded as the most reliable way of collecting human opinions of visual quality."
  - [section] "We devise coarse-to-fine pairing rules, and use the maximum a posterior (MAP) estimation [25] to convert pairwise preferences of different LLMs to the global ranking scores."
- Break condition: If the model's pairwise choices are inconsistent or random, the MAP estimation will produce unreliable global rankings.

### Mechanism 2
- Claim: The three evaluation criteria (consistency, accuracy, correlation) provide complementary insights into LMM IQA performance.
- Mechanism: Consistency checks if the model's preference is invariant to presentation order, accuracy measures correctness against human MOS, and correlation quantifies alignment between the model's ranking and human scores.
- Core assumption: These three metrics together capture both the reliability and validity of the model's quality judgments.
- Evidence anchors:
  - [abstract] "we introduce three evaluation criteria: consistency, accuracy, and correlation, to provide comprehensive quantifications and deeper insights into the IQA capability of five LMMs."
  - [section] "We propose to quantify the IQA capability of LLMs using the three evaluation criteria."
- Break condition: If one criterion dominates or contradicts the others, the evaluation may be misleading.

### Mechanism 3
- Claim: Coarse-to-fine pairing rules enable systematic evaluation of LMMs across different levels of quality discrimination difficulty.
- Mechanism: Coarse pairs (random images) test general quality ranking ability; fine pairs (same content, different distortion levels/types) test sensitivity to subtle differences.
- Core assumption: Difficulty increases systematically from coarse to fine, and LMMs' performance will degrade accordingly.
- Evidence anchors:
  - [section] "To evaluate the IQA capability of LMMs, we devise a set of coarse-to-fine pairing rules... For fine-grained quality comparison, we propose three pairing rules, as illustrated in Fig. 2."
- Break condition: If fine pairs are not actually harder than coarse pairs for humans, the framework loses discriminative power.

## Foundational Learning

- Concept: Maximum a posteriori (MAP) estimation in Thurstone's case V model
  - Why needed here: To aggregate pairwise comparison outcomes into a global ranking that is consistent with the observed preferences.
  - Quick check question: If image A is chosen over B 70% of the time and B over C 60% of the time, what is the relative ordering of A, B, and C in the MAP solution?

- Concept: Thurstone's case V model assumptions
  - Why needed here: To justify treating the latent quality scores as normally distributed and the pairwise choices as probabilistic functions of the score differences.
  - Quick check question: What distributional assumption about latent quality scores underlies the MAP formulation in this paper?

- Concept: Pearson linear correlation coefficient (PLCC) for model evaluation
  - Why needed here: To quantify how well the model's global ranking aligns with human mean opinion scores.
  - Quick check question: If the model's ranking perfectly matches human scores, what is the PLCC value?

## Architecture Onboarding

- Component map: Data loader -> Prompt generator -> LMM interface -> Preference aggregator -> MAP estimator -> Evaluator
- Critical path: Data loader → Prompt generator → LMM interface → Preference aggregator → MAP estimator → Evaluator → Results
- Design tradeoffs:
  - More pairing rounds improve MAP convergence but increase cost
  - Open-source LMMs are cheaper but less capable than GPT-4V
  - Fine-grained pairs are more informative but harder for models and may require more samples
- Failure signatures:
  - Low consistency (κ) across many pairs suggests systematic bias in model responses
  - High accuracy (α) but low correlation (ρ) indicates correct choices but wrong relative scaling
  - Slow MAP convergence suggests inconsistent or noisy pairwise preferences
- First 3 experiments:
  1. Run coarse-grained evaluation on a small subset (e.g., 10 images from CSIQ) with all five LMMs to verify basic functionality
  2. Test MAP estimation convergence by gradually increasing the number of pairing rounds on the same subset and plotting PLCC vs. rounds
  3. Perform fine-grained AWGN level comparison on a single image scene to confirm the model's sensitivity to small quality differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications or training strategies could enable LMMs to achieve fine-grained image quality discrimination comparable to human observers?
- Basis in paper: [explicit] The paper explicitly states that existing LMMs struggle with fine-grained quality discrimination, while GPT-4V demonstrates superior performance, suggesting architectural or training differences are responsible.
- Why unresolved: The paper only tests existing LMMs and doesn't explore what modifications would be needed to close the performance gap in fine-grained tasks.
- What evidence would resolve it: Comparative studies testing LMMs with different architectures or training approaches on fine-grained IQA tasks, showing which modifications lead to human-level performance.

### Open Question 2
- Question: How do data contamination risks affect the validity of LMM evaluations on image quality datasets, and what protocols can minimize these risks?
- Basis in paper: [explicit] The paper mentions selecting KADIS-700k and SQAD datasets specifically because they lack MOS scores, and manually collecting MOS scores for these datasets to avoid data contamination issues with closed-source models like GPT-4V.
- Why unresolved: While the paper takes steps to minimize contamination, it doesn't quantify the impact of contamination on results or establish comprehensive protocols for future evaluations.
- What evidence would resolve it: Systematic studies measuring performance differences between contaminated and non-contaminated evaluations, and development of standardized contamination mitigation protocols.

### Open Question 3
- Question: What underlying biases in LMM training data cause the strong presentation order preferences observed in open-source LMMs during pairwise comparisons?
- Basis in paper: [explicit] The paper observes that open-source LMMs exhibit strong biases in presentation order, with some models consistently preferring the first or second image regardless of content.
- Why unresolved: The paper identifies the phenomenon but doesn't investigate the root causes in training data or model architecture that lead to these biases.
- What evidence would resolve it: Analysis of training data distributions and model behavior to identify specific patterns that correlate with presentation order biases, potentially leading to bias mitigation strategies.

## Limitations
- The study uses a limited set of five LMMs, potentially missing variations in architecture and training approaches
- The coarse-to-fine pairing strategy lacks validation against human performance on the same fine-grained tasks
- MAP estimation convergence behavior and sensitivity to noisy pairwise comparisons are not thoroughly analyzed

## Confidence

**High**: The general finding that existing LMMs struggle with fine-grained IQA tasks is well-supported by consistent results across multiple datasets and models.

**Medium**: The superiority of GPT-4V over open-source models is demonstrated but could be influenced by specific prompting strategies or temperature settings.

**Medium**: The three proposed evaluation criteria provide useful insights, though their relative importance and potential interactions require further investigation.

## Next Checks

1. Conduct human studies on the same fine-grained image pairs to establish baseline performance and validate the difficulty hierarchy assumed by the coarse-to-fine pairing strategy.

2. Test the MAP estimation convergence by varying the number of pairing rounds and analyzing the stability of global rankings across different sample sizes.

3. Implement alternative preference aggregation methods (e.g., Bradley-Terry model) to verify the robustness of conclusions to the choice of aggregation algorithm.