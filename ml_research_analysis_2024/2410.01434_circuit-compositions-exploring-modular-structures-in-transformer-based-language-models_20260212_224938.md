---
ver: rpa2
title: 'Circuit Compositions: Exploring Modular Structures in Transformer-Based Language
  Models'
arxiv_id: '2410.01434'
source_url: https://arxiv.org/abs/2410.01434
tags:
- circuits
- circuit
- task
- copy
- reverse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the modularity of transformer-based language
  models by analyzing circuits for compositional string-edit operations. The authors
  introduce activation pruning through continuous sparsification to automatically
  identify faithful and minimal circuits for sequence-to-sequence tasks.
---

# Circuit Compositions: Exploring Modular Structures in Transformer-Based Language Models

## Quick Facts
- **arXiv ID**: 2410.01434
- **Source URL**: https://arxiv.org/abs/2410.01434
- **Reference count**: 40
- **Key outcome**: This study investigates the modularity of transformer-based language models by analyzing circuits for compositional string-edit operations. The authors introduce activation pruning through continuous sparsification to automatically identify faithful and minimal circuits for sequence-to-sequence tasks. They identify and compare circuits for ten string-edit operations from PCFG SET, finding that functionally similar circuits exhibit notable node overlap and cross-task faithfulness. The circuits can be combined through set operations to represent more complex model capabilities. Results show that circuits achieve high task faithfulness (exceeding 0.94 for unary tasks) and that functionally related operations share significant computational subgraphs, suggesting a degree of modularity in the network's architecture.

## Executive Summary
This paper explores modularity in transformer-based language models by automatically identifying and analyzing circuits for compositional string-edit operations. The authors introduce activation pruning through continuous sparsification, a method that jointly optimizes for task faithfulness and minimality when identifying computational subgraphs. They apply this approach to ten string-edit operations from the PCFG SET dataset, finding that functionally similar operations share significant node overlap and can perform each other's tasks to some degree. The study demonstrates that circuits can be composed through set operations to create new functional capabilities, providing evidence for modular structure in transformer models and a framework for understanding how complex behaviors emerge from simpler components.

## Method Summary
The authors propose activation pruning through continuous sparsification to identify faithful and minimal circuits for sequence-to-sequence tasks. This method learns a deterministic mask over activation space by optimizing a continuous approximation of the discrete mask, balancing task loss and mask sparsity. The approach uses sigmoid re-parameterization of the mask and jointly minimizes task loss (via KL divergence) and mask sparsity (via l1 regularization). The method is applied to a 6-layer encoder-decoder transformer trained on PCFG SET, a dataset containing 10 string-edit operations. Circuits are evaluated for task faithfulness, accuracy, node overlap, and compositionality through set operations.

## Key Results
- Circuits for unary string-edit operations achieve task faithfulness scores exceeding 0.94, indicating high fidelity to the original model's behavior
- Functionally similar operations (e.g., reverse and swap) exhibit significant node overlap with IoU scores around 0.36, demonstrating shared computational subgraphs
- Circuit composition through set operations enables new functional capabilities, with composite circuits performing tasks that individual circuits cannot handle alone
- Cross-task faithfulness is observed, where circuits trained for one operation can partially perform other related operations due to shared computational mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Activation pruning through continuous sparsification reliably identifies both faithful and minimal circuits for sequence-to-sequence tasks.
- Mechanism: The method learns a deterministic mask over the activation space by optimizing a continuous approximation of the discrete mask, jointly minimizing task loss and mask sparsity. This balances faithfulness (via KL divergence) and minimality (via l1 regularization) during training.
- Core assumption: The sigmoid re-parameterization of the mask converges to a binary mask that accurately captures the causal nodes when the temperature parameter β is sufficiently high.
- Evidence anchors:
  - [abstract] "we propose an automatic circuit identification method called activation pruning through continuous sparsification, which jointly optimizes for faithfulness and minimality"
  - [section] "we follow the approach of continuous sparsification... and deterministically re-parameterize m using a sigmoid function σ(·) over the new variable s ∈ RN"
  - [corpus] Weak - neighboring work focuses on hierarchical extraction and robustness but does not validate the specific continuous sparsification approach used here.
- Break condition: If the sigmoid approximation fails to converge to a binary mask, or if the learned mask includes non-causal nodes, the identified circuits will not be minimal or faithful.

### Mechanism 2
- Claim: Functionally related string-edit operations share significant node overlap and cross-task faithfulness.
- Mechanism: Circuits for operations like reverse, swap, and shift share computational subgraphs because they rely on similar underlying mechanisms for sequence manipulation, even though their outputs differ. This results in measurable node overlap (IoU/IoM) and partial cross-task faithfulness.
- Core assumption: The transformer model implements string-edit operations through reusable computational components rather than entirely distinct pathways for each operation.
- Evidence anchors:
  - [abstract] "functionally similar circuits exhibit both notable node overlap and cross-task faithfulness"
  - [section] "circuits responsible for tasks like echo, repeat, and swap can perform the copy task... circuits with similar functions exhibit greater node overlap than functionally distinct ones"
  - [corpus] Weak - neighboring work discusses circuit discovery but does not provide direct evidence for cross-task overlap in compositional tasks.
- Break condition: If the model uses completely separate pathways for each operation, there would be minimal node overlap and no cross-task faithfulness.

### Mechanism 3
- Claim: Circuit compositions through set operations can create new functional capabilities by combining existing circuits.
- Mechanism: The union of two circuit masks creates a composite circuit that retains the union of causal nodes from both original circuits. This composite can perform tasks that neither original circuit could handle alone by leveraging shared computational resources.
- Core assumption: The combined node set from two circuits provides sufficient computational capacity to implement composite operations, and the ablation values from the first circuit appropriately handle nodes not in the union.
- Evidence anchors:
  - [abstract] "we demonstrate that the circuits identified can be reused and combined through set operations to represent more complex functional model capabilities"
  - [section] "the union mrepeat ∪ mreverse might shift the activation space of mrepeat towards mreverse, essentially allowing the new circuit to perform the echo operation"
  - [corpus] Weak - neighboring work focuses on circuit extraction but does not validate set-based composition for creating new capabilities.
- Break condition: If the union operation creates conflicting activation patterns or if the composite circuit cannot properly handle tasks requiring both original functions, the composition will fail.

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The study analyzes circuits within a transformer-based sequence-to-sequence model, requiring understanding of how transformers process sequences through attention and feed-forward layers.
  - Quick check question: What is the role of the residual stream in transformer computation, and how do attention and feed-forward modules modify it?

- Concept: Mechanistic interpretability and circuit discovery
  - Why needed here: The paper uses circuit discovery techniques to identify computational subgraphs responsible for specific behaviors, requiring understanding of causal mediation analysis and activation patching.
  - Quick check question: How does activation patching quantify the causal influence of model components on task outputs?

- Concept: Probabilistic context-free grammars and compositional tasks
  - Why needed here: The PCFG SET dataset uses PCFGs to generate compositional string-edit tasks, requiring understanding of how grammars define hierarchical structure in sequences.
  - Quick check question: How does a PCFG generate sequences that require compositional understanding for correct processing?

## Architecture Onboarding

- Component map: Base transformer encoder-decoder model (6 encoder and 6 decoder layers, hidden size 512, 8 attention heads) trained on PCFG SET, plus circuit discovery module applying activation pruning through continuous sparsification to identify minimal, faithful circuits for each string-edit operation
- Critical path: 1) Train base model on PCFG SET 2) For each operation, train mask to optimize faithfulness and minimality 3) Evaluate circuits for task faithfulness, accuracy, and node overlap 4) Test circuit compositions through set operations
- Design tradeoffs: The method balances faithfulness vs. minimality through the λ hyperparameter, and uses mean ablation vs. zero ablation to handle distribution shifts. Larger λ values yield sparser but less faithful circuits.
- Failure signatures: Poor faithfulness scores indicate the circuit doesn't capture essential computation; high sparsity with low faithfulness suggests over-pruning; minimal node overlap between functionally similar circuits suggests lack of modularity.
- First 3 experiments:
  1. Train base model and verify it achieves >95% accuracy on unary tasks and >83% on binary tasks
  2. Run activation pruning with mean ablation for the copy operation and verify it achieves >0.94 faithfulness
  3. Compare node overlap between reverse and swap circuits and verify IoU >0.36 as reported

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the completeness of circuits identified through activation pruning through continuous sparsification compare to manually identified circuits, and what are the key factors limiting completeness?
- Basis in paper: [explicit] The authors acknowledge that their method does not guarantee completeness, and discuss the need for future work to quantify and incorporate completeness into the optimization framework.
- Why unresolved: The paper explicitly states that completeness is not guaranteed and mentions that evaluating completeness is computationally expensive, especially for large circuits. The authors suggest future research should explore more efficient ways to quantify completeness.
- What evidence would resolve it: A systematic comparison of the completeness of automatically identified circuits versus manually identified circuits for the same tasks, using a metric that quantifies the proportion of necessary nodes included in the circuit.

### Open Question 2
- Question: To what extent do the findings about modularity and circuit overlap generalize to larger language models and more diverse tasks beyond PCFG SET?
- Basis in paper: [inferred] The authors note that their study focuses on a small encoder-decoder model trained on a single dataset and raise the question of whether findings will generalize to larger models and more complex tasks.
- Why unresolved: The study is limited to a specific model architecture and dataset. The authors acknowledge this limitation and suggest scaling the findings to larger models and more complex tasks as a compelling area for future research.
- What evidence would resolve it: Replication of the study using larger transformer-based language models (e.g., GPT-2, GPT-3, BERT) and diverse datasets with more complex tasks, comparing the modularity and circuit overlap patterns observed.

### Open Question 3
- Question: What is the precise mechanism by which the union of two circuits enables the composite circuit to perform tasks that the original circuits could not, and how does this affect the circuit's performance on the original tasks?
- Basis in paper: [explicit] The authors demonstrate that circuits combined through the union operator can represent novel functional capabilities, but note that improvements in accuracy for novel tasks may be accompanied by a decline in performance on previously mastered skills.
- Why unresolved: While the paper shows that union operations can create composite circuits with new capabilities, the exact mechanism by which this transformation occurs is not fully explained. The trade-off between acquiring new capabilities and losing performance on original tasks is observed but not fully understood.
- What evidence would resolve it: Detailed analysis of the activation patterns in composite circuits versus individual circuits, including feature attribution methods to identify which components are responsible for the new capabilities and how they interfere with or replace original functionality.

## Limitations

- The study's findings rely heavily on a specific task domain (PCFG SET) and a particular model architecture, which may not generalize to other compositional tasks or larger transformer models
- The continuous sparsification method's effectiveness depends on hyperparameter tuning that wasn't fully detailed in the paper
- The set-based circuit composition approach, while promising, lacks rigorous validation beyond simple unions and their effects on specific tasks

## Confidence

- **High Confidence**: Task faithfulness measurements for individual circuits, as these are directly computed from well-defined metrics (KL divergence) and show consistent results across operations
- **Medium Confidence**: Node overlap and cross-task faithfulness findings, as these depend on the specific implementation of mean ablation and the assumption that IoU/IoM are meaningful measures of functional similarity
- **Medium Confidence**: Circuit composition results, as the mechanism is theoretically sound but the empirical validation is limited to demonstrating basic compositional capabilities without exploring failure modes

## Next Checks

1. Test circuit faithfulness and compositionality on a held-out compositional task not in the PCFG SET dataset to verify generalizability
2. Implement the continuous sparsification method on a larger transformer model (e.g., 12+ layers) to assess scalability and whether similar modularity patterns emerge
3. Systematically vary the λ regularization parameter and temperature annealing schedule to determine the sensitivity of circuit identification to these hyperparameters and identify optimal configurations for different operation types