---
ver: rpa2
title: 'ROCOv2: Radiology Objects in COntext Version 2, an Updated Multimodal Image
  Dataset'
arxiv_id: '2405.10004'
source_url: https://arxiv.org/abs/2405.10004
tags:
- images
- dataset
- image
- caption
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ROCOv2 is a multimodal dataset of 79,789 radiological images with
  associated captions and UMLS medical concepts, designed for training image annotation,
  retrieval, and pre-training models. The dataset improves upon the original ROCO
  by adding 35,705 new images, providing manually curated concepts for imaging modalities,
  and enhancing concept extraction using MedCAT.
---

# ROCOv2: Radiology Objects in COntext Version 2, an Updated Multimodal Image Dataset

## Quick Facts
- arXiv ID: 2405.10004
- Source URL: https://arxiv.org/abs/2405.10004
- Authors: Johannes Rückert; Louise Bloch; Raphael Brüngel; Ahmad Idrissi-Yaghir; Henning Schäfer; Cynthia S. Schmidt; Sven Koitka; Obioma Pelka; Asma Ben Abacha; Alba G. Seco de Herrera; Henning Müller; Peter A. Horn; Felix Nensa; Christoph M. Friedrich
- Reference count: 40
- Primary result: 79,789 radiological images with captions and UMLS concepts for training image annotation and retrieval models

## Executive Summary
ROCOv2 is a multimodal dataset of 79,789 radiological images with associated captions and UMLS medical concepts, designed for training image annotation, retrieval, and pre-training models. The dataset improves upon the original ROCO by adding 35,705 new images, providing manually curated concepts for imaging modalities, and enhancing concept extraction using MedCAT. It includes images from CC BY-licensed articles in the PMC Open Access Subset, covering a wide range of anatomical regions and modalities. Baseline models achieved F1-scores of 0.5811 and 0.5925 for concept detection, and BERTScores of 0.6264 and 0.6264 for caption prediction, demonstrating its effectiveness for medical image analysis tasks.

## Method Summary
The ROCOv2 dataset was constructed by extracting images from the PMC Open Access Subset, filtering for radiological images using a convolutional neural network, and preprocessing captions to remove non-English text. Concepts were extracted using MedCAT and manually curated for imaging modalities, anatomical regions, and directionalities. The dataset was split into training, validation, and test sets using stratified random sampling based on manually curated concepts. Two baseline models were trained: EfficientNet-B0/V2 for concept detection and a vision encoder-decoder architecture with ViT encoder and BioMedLM decoder for caption prediction.

## Key Results
- ROCOv2 contains 79,789 radiological images with associated captions and UMLS concepts
- Baseline models achieved F1-scores of 0.5811 and 0.5925 for concept detection
- BERTScores of 0.6264 and 0.6264 were achieved for caption prediction
- The dataset includes images from CC BY-licensed articles, allowing direct distribution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining manual curation with automated MedCAT concept extraction increases both accuracy and coverage of medical concepts in the dataset.
- Mechanism: Manual annotation provides high-quality, domain-specific concepts (especially for imaging modalities and anatomical regions) while MedCAT efficiently scales concept extraction across the large dataset using word embeddings and semantic type filtering.
- Core assumption: Human annotators can reliably identify relevant visual concepts that automated systems may miss or misclassify.
- Evidence anchors:
  - [abstract] "It further provides manually curated concepts for imaging modalities with additional anatomical and directional concepts for X-rays."
  - [section] "The manually identified anatomy and directionality concepts of X-rays were not checked for conflicting concepts from automatic extraction to be integrated."
  - [corpus] Weak - related papers focus on model applications rather than annotation methodology.
- Break condition: If annotator-radiologist agreement drops significantly (e.g., κ < 0.6), manual curation becomes unreliable and may introduce bias.

### Mechanism 2
- Claim: Using only CC BY or CC BY-NC licensed images allows legal redistribution while maintaining dataset quality.
- Mechanism: By filtering out images with restrictive licenses (CC BY-ND, CC BY-SA), the dataset can be freely distributed and reused without violating copyright terms.
- Core assumption: CC BY and CC BY-NC licenses are sufficiently permissive for academic and commercial research use.
- Evidence anchors:
  - [abstract] "to allow direct distribution of the images, only images from CC BY licensed articles (including CC BY-NC, but excluding CC BY-ND and CC BY-SA) are included."
  - [section] "only images from CC BY licensed articles (including CC BY-NC, but excluding CC BY-ND and CC BY-SA) are included."
  - [corpus] Weak - no corpus evidence on licensing impact.
- Break condition: If license terms change or if redistribution under CC BY-NC is later found to be non-compliant with some use cases.

### Mechanism 3
- Claim: Stratified random sampling based on manually curated concepts ensures balanced representation across image types in train/validation/test splits.
- Mechanism: By stratifying splits on modality, anatomy, and directionality labels, the dataset prevents overrepresentation of common modalities (e.g., CT, X-ray) and ensures rare modalities (e.g., PET, combined) are adequately represented for training and evaluation.
- Core assumption: Manual concepts are accurate and comprehensive enough to serve as stratification variables.
- Evidence anchors:
  - [section] "Stratified random sampling based on the manually curated concepts was used to divide the images into validation and test sets."
  - [section] "A Cohen’s κ within an interval of [0.81, 1.00] can be interpreted as almost perfect, and within a range of [0.41, 0.60] as moderate agreement"
  - [corpus] Weak - related work focuses on model performance, not data splitting methodology.
- Break condition: If concept extraction errors or missing concepts lead to unbalanced splits that harm model generalization.

## Foundational Learning

- Concept: Unified Medical Language System (UMLS) and Concept Unique Identifiers (CUIs)
  - Why needed here: ROCOv2 uses UMLS CUIs to standardize medical concepts across diverse radiology images and captions, enabling consistent multi-label classification.
  - Quick check question: What is the purpose of mapping extracted terms to UMLS CUIs instead of using raw text labels?

- Concept: Semantic Types (TUIs) and their role in filtering
  - Why needed here: TUIs help filter concepts to those likely to be visually observable (e.g., T023 Body Part) and exclude abstract or non-visual concepts (e.g., T054 Social Behavior).
  - Quick check question: How does filtering by semantic type improve the relevance of concepts for image-based tasks?

- Concept: Multimodal contrastive learning and its relevance to medical imaging
  - Why needed here: ROCOv2 supports training models that align visual and textual representations, a foundation for tasks like image retrieval and caption prediction.
  - Quick check question: Why is multimodal contrastive learning particularly valuable in the medical domain compared to general image captioning?

## Architecture Onboarding

- Component map: Data ingestion → Compound/Radiological filtering → License filtering → Caption preprocessing → MedCAT concept extraction → Manual concept curation → Stratified split → Dataset packaging → Baseline models: EfficientNet-B0/V2 for concept detection; Vision Transformer + BioMedLM for caption prediction → Evaluation: F1-score (concept detection), BERTScore/ROUGE (caption prediction)

- Critical path: 1. Image extraction and filtering (CNN-based) 2. Caption preprocessing and language filtering 3. Concept extraction (MedCAT + manual curation) 4. Dataset splitting (stratified by concepts) 5. Model training and evaluation

- Design tradeoffs:
  - Manual vs. fully automated concept extraction: higher accuracy vs. scalability
  - License filtering: broader usability vs. smaller dataset size
  - Single-concept images: larger dataset vs. potential lack of complexity for training

- Failure signatures:
  - Low inter-annotator agreement (κ < 0.6) → manual curation unreliable
  - High proportion of non-English captions slipping through → concept extraction errors
  - Skewed modality distribution in splits → biased model performance

- First 3 experiments:
  1. Train a simple CNN on modality classification using only automatically extracted concepts; evaluate accuracy and compare to manual-only baseline.
  2. Test concept detection with and without semantic type filtering to quantify impact on F1-score.
  3. Perform ablation study on license filtering: train models on full PMC subset vs. CC BY-only subset to measure performance impact.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of manually curated concepts compare to automatically extracted concepts for modalities beyond X-ray (e.g., MRI, CT)?
- Basis in paper: [explicit] The paper describes manual curation of concepts for all modalities, with radiologist validation performed only for X-ray body regions and directionalities, achieving moderate to substantial agreement (κ=0.886 for body regions, κ=0.557 for directionalities).
- Why unresolved: The radiologist validation was only performed on X-ray images, leaving uncertainty about the accuracy of manually curated concepts for other modalities like MRI, CT, PET, etc.
- What evidence would resolve it: Conducting a radiologist validation study on manually curated concepts for non-X-ray modalities, measuring inter-annotator agreement using Cohen's κ or similar metrics.

### Open Question 2
- Question: What is the impact of including single-concept images on model performance for multi-label classification tasks?
- Basis in paper: [explicit] The paper notes that single-concept images may lack the complexity required for comprehensive model training but includes them to maximize sample size and variety.
- Why unresolved: The dataset analysis does not quantify how many images have only one concept or investigate the effect of these images on model training and evaluation.
- What evidence would resolve it: Analyzing the distribution of concept counts per image and conducting experiments comparing model performance when training with and without single-concept images.

### Open Question 3
- Question: How does the ROCOv2 dataset perform on rare modalities like PET and combined modalities compared to more common modalities like CT and X-ray?
- Basis in paper: [explicit] The paper mentions that certain modalities like PET and combined modalities are relatively rare in the dataset, reflecting their scarcity in publications.
- Why unresolved: The technical validation results do not provide modality-specific performance metrics, making it unclear if the dataset is equally effective for training models on rare modalities.
- What evidence would resolve it: Evaluating baseline model performance separately for each modality, reporting F1-scores and other metrics to compare effectiveness across different imaging types.

## Limitations

- The inter-annotator agreement validation was limited to a subset of 500 images, leaving uncertainty about the consistency of manual curation across the full dataset.
- The manual curation process focused on modality and anatomical concepts but did not comprehensively validate automatically extracted concepts for consistency or accuracy across all images.
- The license filtering approach relies on current license interpretations without long-term legal validation, which may impact downstream applications.

## Confidence

- Dataset utility for training medical image models: High
- Concept extraction methodology: Medium
- Licensing approach: Medium

## Next Checks

1. **Inter-annotator validation expansion**: Replicate the κ analysis across all major semantic types (anatomy, directionality, imaging modalities) using a larger validation set (n ≥ 1000) to confirm consistency of manual curation across the full dataset.

2. **License compliance audit**: Conduct a systematic review of CC BY vs. CC BY-NC usage patterns in the resulting models to identify any potential restrictions or conflicts in downstream applications.

3. **Concept extraction accuracy testing**: Implement a blind evaluation where radiologists validate randomly sampled automatically extracted concepts against ground truth annotations to quantify false positive/negative rates across different semantic types.