---
ver: rpa2
title: 'Multilingual Arbitrage: Optimizing Data Pools to Accelerate Multilingual Progress'
arxiv_id: '2408.14960'
source_url: https://arxiv.org/abs/2408.14960
tags:
- routing
- arxiv
- single
- teacher
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces "multilingual arbitrage" to optimize data
  generation for multilingual instruction tuning. Instead of relying on a single teacher
  model, it strategically routes prompts through a diverse pool of models, each with
  unique strengths in different languages.
---

# Multilingual Arbitrage: Optimizing Data Pools to Accelerate Multilingual Progress

## Quick Facts
- arXiv ID: 2408.14960
- Source URL: https://arxiv.org/abs/2408.14960
- Authors: Ayomide Odumakinde; Daniel D'souza; Pat Verga; Beyza Ermis; Sara Hooker
- Reference count: 40
- Primary result: Reward-based routing achieves up to 56.5% improvement in win rates across 15 languages

## Executive Summary
This paper introduces "multilingual arbitrage" - a technique for optimizing synthetic data generation in multilingual instruction tuning by routing prompts through a diverse pool of teacher models. Instead of relying on a single teacher model, the approach strategically selects the most capable model for each language, resulting in significantly improved student model performance. The method particularly benefits medium-resource languages and produces longer, more lexically diverse text generations compared to traditional single-teacher approaches.

## Method Summary
The multilingual arbitrage approach routes prompts through a pool of nine teacher models (including Aya-23, Llama-3, Gemma-2, geo-cluster models, and monolingual models) using different strategies: random routing, fixed routing based on language expertise, reward-based routing using model scoring, and learned routing using a trained router model. The method generates 70,000 prompts in seven languages (Arabic, Chinese, English, French, German, Turkish, Ukrainian) from the UltraFeedback dataset, fine-tunes a student model (Aya-23-8B), and evaluates performance using win rates against baselines, discriminative tasks (XNLI, XCOPA, XStoryCloze), and textual analysis metrics.

## Key Results
- Reward-based routing achieves up to 56.5% improvement in win rates over single-teacher approaches across 15 languages
- Medium-resource languages (Turkish, Ukrainian) benefit more than high-resource languages from routing strategies
- Direct in-language generation outperforms translation-based approaches by 48.9% in win rates
- Generated text shows 1.3-1.8x increase in token count and improved lexical diversity (1.2-1.3x) compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
Routing prompts to the most capable teacher model for each language improves student model performance more than using a single teacher. Different models have varying strengths across languages, and by identifying and leveraging the best model for each language, the student model receives higher quality training data. The performance variations between models are significant enough to matter when aggregated across many prompts.

### Mechanism 2
Medium-resource languages benefit more from routing than high-resource languages because they have less training data, so the impact of receiving higher quality data is proportionally larger. The quality difference between models is larger for languages with less training data, making routing more impactful for these languages.

### Mechanism 3
Generating data directly in target languages is more effective than translating from English because direct generation preserves language-specific nuances and avoids translation artifacts that can degrade quality. Translation quality is insufficient to preserve the semantic and stylistic properties needed for effective instruction tuning.

## Foundational Learning

- **Teacher-student distillation in machine learning**: The paper builds on traditional distillation but introduces routing to multiple teachers. *Quick check*: What's the main difference between single-teacher and multi-teacher distillation approaches?
- **Performance benchmarking across multiple languages**: The paper evaluates models across 15 languages using win rates and discriminative tasks. *Quick check*: Why might traditional academic benchmarks not fully capture open-ended generation quality?
- **Language resource classification (high vs medium resource)**: The paper shows different gains for languages with different amounts of available training data. *Quick check*: How does the amount of available training data affect a model's performance on a language?

## Architecture Onboarding

- **Component map**: Teacher model pool (9 models) -> Router system (fixed/reward-based/learned routing) -> Student model (Aya-23-8B) -> Evaluation framework (LLM-as-a-judge win rates, discriminative tasks, textual analysis)
- **Critical path**: 1) Collect prompts from UltraFeedback dataset, 2) Route each prompt to appropriate teacher model, 3) Generate completions, 4) Fine-tune student model on routed data, 5) Evaluate against baselines
- **Design tradeoffs**: Reward-based routing vs learned routing (quality vs efficiency), model pool diversity vs computational cost, direct generation vs translation (quality vs convenience)
- **Failure signatures**: No improvement over random routing suggests routing logic is flawed, worse performance than single teacher suggests model pool quality is too low, inconsistent gains across languages suggests routing isn't language-aware enough
- **First 3 experiments**: 1) Implement random routing baseline and verify it's worse than single teacher, 2) Implement fixed routing with pre-determined language experts, 3) Implement reward-based routing with reward model scoring

## Open Questions the Paper Calls Out

- **Open Question 1**: How does multilingual arbitrage perform when scaling up to models larger than 8 billion parameters? The experiments were limited to models around 8 billion parameters due to computational constraints, leaving the performance of arbitrage techniques with larger models unexplored.
- **Open Question 2**: What is the impact of multilingual arbitrage on the safety and ethical aspects of model outputs? The paper explicitly states that safety evaluation was not conducted, leaving a gap in understanding how arbitrage affects the generation of safe and ethical content across languages.
- **Open Question 3**: How does the diversity of the teacher model pool affect the performance gains from multilingual arbitrage? While the paper demonstrates that a diverse pool improves performance, it does not investigate the optimal composition or the marginal benefits of adding more diverse models.

## Limitations
- Computational cost of reward-based routing is prohibitive with larger model pools
- Limited evaluation of safety and ethical aspects of generated outputs
- Unclear scalability to hundreds of languages and potential routing conflicts

## Confidence

**High confidence**: The core finding that routing prompts through multiple teacher models outperforms single-teacher approaches is well-supported by experimental results.

**Medium confidence**: The claim that medium-resource languages benefit more from routing than high-resource languages is supported by data but based on a limited sample of languages.

**Medium confidence**: The superiority of direct generation over translation-based approaches is demonstrated but relies on a specific translation model that may not represent state-of-the-art.

## Next Checks
1. **Ablation study on teacher model diversity**: Systematically vary the number and diversity of teacher models in the pool to determine the minimum effective pool size and whether certain language families benefit more from specialized vs. diverse teacher pools.

2. **Human evaluation comparison**: Conduct human preference studies to validate the LLM-as-a-judge win rates, particularly for the medium-resource languages where the largest gains were observed, to ensure automated evaluation aligns with human judgment.

3. **Downstream task performance**: Evaluate the student models on actual multilingual downstream tasks (translation quality, cross-lingual retrieval, etc.) beyond the synthetic discriminative tasks used in the paper to verify that gains in open-ended generation translate to practical utility.