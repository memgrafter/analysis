---
ver: rpa2
title: 'Edge-Splitting MLP: Node Classification on Homophilic and Heterophilic Graphs
  without Message Passing'
arxiv_id: '2412.08310'
source_url: https://arxiv.org/abs/2412.08310
tags:
- node
- homophily
- heterophilic
- graphs
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ES-MLP addresses node classification on both homophilic and heterophilic
  graphs without using message passing. The method combines Graph-MLP with an edge-splitting
  mechanism from ES-GNN, learning two separate adjacency matrices based on relevant
  and irrelevant feature pairs.
---

# Edge-Splitting MLP: Node Classification on Homophilic and Heterophilic Graphs without Message Passing

## Quick Facts
- arXiv ID: 2412.08310
- Source URL: https://arxiv.org/abs/2412.08310
- Reference count: 40
- Key outcome: ES-MLP achieves competitive results on both homophilic and heterophilic graphs without message passing, matching or outperforming baselines including GCN, Graph-MLP, and ES-GNN

## Executive Summary
ES-MLP addresses node classification on both homophilic and heterophilic graphs without using message passing by combining Graph-MLP with ES-GNN's edge-splitting mechanism. The method learns two separate adjacency matrices based on relevant and irrelevant feature pairs, enabling it to handle heterophilic graphs while maintaining performance on homophilic ones. Experiments on seven real-world datasets show ES-MLP achieves competitive results, with significant improvements on heterophilic graphs and faster inference times compared to traditional message passing neural networks.

## Method Summary
ES-MLP generalizes MLP to handle both homophilic and heterophilic graph data by integrating neighborhood contrastive loss from Graph-MLP with ES-GNN's edge-splitting mechanism. The model projects node features into two subspaces (task-relevant and task-irrelevant), then learns exclusive adjacency matrices for each subspace using an edge-splitting layer. During training, neighborhood contrastive loss is applied separately to both edge sets, while during inference, the model performs classification without using the adjacency matrix. The approach achieves faster inference by encoding edge information in learned node embeddings rather than requiring explicit neighborhood aggregation.

## Key Results
- ES-MLP matches or outperforms baseline models including GCN, Graph-MLP, and ES-GNN across seven datasets
- On heterophilic graphs, ES-MLP demonstrates significant improvements, achieving up to 11.18 points higher accuracy than Graph-MLP on Amazon dataset
- The model is robust to edge noise during inference, maintaining stable performance while MPNNs lose up to 12.7% accuracy
- ES-MLP offers faster inference times, being 2-5 times faster than commonly used MPNNs

## Why This Works (Mechanism)

### Mechanism 1
ES-MLP learns separate adjacency matrices for task-relevant and task-irrelevant edges via edge-splitting. The edge-splitting mechanism projects node features into two subspaces, then computes exclusive adjacency matrices AR and AIR where AR(i,j) + AIR(i,j) = 1. These matrices are used in the neighborhood contrastive loss to focus on relevant edges. The core assumption is that edge relevance can be determined from node feature pairs rather than node labels. If the feature space cannot encode edge relevance, the split becomes arbitrary and performance degrades.

### Mechanism 2
Neighborhood contrastive loss generalizes to heterophilic graphs by applying it separately to both edge sets. The loss computes cosine similarity between node embeddings within r-hop neighborhoods, weighted by the split adjacency matrices. This allows learning that connected nodes should be close in the relevant embedding space while distant in the irrelevant space. The core assumption is that even in heterophilic graphs, some edges contain task-relevant information that can be captured by feature similarity. If the graph is purely heterophilic with no task-relevant edges, the model cannot learn meaningful structure.

### Mechanism 3
ES-MLP achieves faster inference by not requiring the adjacency matrix during inference. During training, the model learns node embeddings that encode both feature and edge information through the neighborhood contrastive loss. At inference, only the learned linear classifier and node features are needed. The core assumption is that the learned embeddings are sufficiently rich to perform classification without explicit edge information. If the embedding space loses critical structural information, classification accuracy drops when edges are unavailable.

## Foundational Learning

- Concept: Graph Neural Networks and Message Passing
  - Why needed here: Understanding why traditional GNNs struggle with heterophilic graphs and how ES-MLP avoids message passing is crucial
  - Quick check question: What is the key difference between how GCN and ES-MLP aggregate neighborhood information?

- Concept: Neighborhood Contrastive Learning
  - Why needed here: The neighborhood contrastive loss is central to how ES-MLP learns without message passing
  - Quick check question: How does the neighborhood contrastive loss differ from standard contrastive learning?

- Concept: Edge Splitting and Feature Relevance
  - Why needed here: The edge-splitting mechanism is the core innovation that enables ES-MLP to handle heterophilic graphs
  - Quick check question: Why does splitting edges based on feature relevance help with heterophilic graphs?

## Architecture Onboarding

- Component map: Input features → Linear projection → Edge-splitting computation → Neighborhood contrastive loss + ICR loss → Classification layer
- Critical path: The edge-splitting and loss computation happen during training; inference only requires the learned projection and classifier
- Design tradeoffs: ES-MLP trades computational complexity during training (computing r-th powers of adjacency matrices) for faster inference. It also requires more memory for storing two embedding spaces and adjacency matrices
- Failure signatures: Poor performance on heterophilic graphs suggests the edge-splitting isn't capturing relevant patterns. Slow training indicates inefficient computation of adjacency matrix powers. Poor generalization suggests overfitting to training edges
- First 3 experiments:
  1. Train ES-MLP on Cora dataset with varying r values to see impact on homophilic graph performance
  2. Evaluate ES-MLP on Actor dataset to verify improvements on heterophilic graphs compared to Graph-MLP
  3. Test ES-MLP's inference speed on PubMed dataset with only test nodes to confirm no-edge inference capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ES-MLP's performance change when applied to multi-class node classification tasks with more than 18 classes?
- Basis in paper: The paper uses datasets with up to 18 classes (Roman dataset) and a synthetic dataset with 2 balanced classes
- Why unresolved: The paper only tests on datasets with limited class diversity, leaving performance on more complex multi-class scenarios unexplored
- What evidence would resolve it: Empirical results on datasets with 20+ classes showing ES-MLP's scalability and performance compared to other GNNs

### Open Question 2
- Question: What is the impact of ES-MLP's performance when applied to directed graphs with asymmetric edge relationships?
- Basis in paper: The paper focuses on undirected graphs and mentions directed GNNs in related work but doesn't test ES-MLP on directed graphs
- Why unresolved: The edge-splitting mechanism may behave differently on directed graphs where the relevance of edges depends on direction
- What evidence would resolve it: Performance comparisons of ES-MLP on directed graph benchmarks versus existing directed GNN methods

### Open Question 3
- Question: How does ES-MLP handle dynamic graphs where the structure changes over time?
- Basis in paper: The paper assumes a transductive setting with fixed graph structure during training and testing
- Why unresolved: The edge-splitting mechanism and adjacency matrix powers would need adaptation for time-varying graphs
- What evidence would resolve it: Experiments showing ES-MLP's performance on temporal graph datasets with evolving edge structures

### Open Question 4
- Question: What is the effect of using different aggregation functions in the edge-splitting mechanism instead of the linear layer with tanh?
- Basis in paper: The paper uses a specific edge-splitting mechanism with a linear layer followed by tanh activation
- Why unresolved: The paper doesn't explore alternative aggregation functions that might better capture edge relevance
- What evidence would resolve it: Comparative results using different aggregation functions (e.g., attention mechanisms, non-linear transformations) in the edge-splitting process

## Limitations
- The paper does not provide detailed hyperparameter settings for each dataset, making exact reproduction challenging
- The edge-splitting mechanism's implementation details, particularly how splitting coefficients are computed, remain underspecified
- The paper only evaluates on seven real-world datasets, which may limit generalizability claims

## Confidence
- High confidence: ES-MLP's core mechanism of learning separate adjacency matrices for relevant and irrelevant edges, and its ability to perform inference without adjacency matrices
- Medium confidence: Performance improvements on heterophilic graphs, as results depend on proper edge-splitting implementation
- Medium confidence: Robustness to edge noise claims, though the mechanism appears sound

## Next Checks
1. Verify edge-splitting implementation by testing on a synthetic heterophilic graph with known edge relevance patterns
2. Conduct ablation study removing the irrelevant consistency regularization to assess its contribution
3. Measure memory usage during training to confirm the claimed tradeoff between training complexity and inference speed