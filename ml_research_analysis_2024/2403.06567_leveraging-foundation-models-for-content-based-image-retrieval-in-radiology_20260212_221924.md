---
ver: rpa2
title: Leveraging Foundation Models for Content-Based Image Retrieval in Radiology
arxiv_id: '2403.06567'
source_url: https://arxiv.org/abs/2403.06567
tags:
- retrieval
- image
- foundation
- performance
- radiology
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks vision foundation models for content-based
  image retrieval (CBIR) in radiology across 1.6 million images spanning four modalities
  and 161 pathologies. We show that weakly-supervised models like BiomedCLIP achieve
  P@1 of 0.594 without fine-tuning, closely matching specialized CBIR systems.
---

# Leveraging Foundation Models for Content-Based Image Retrieval in Radiology

## Quick Facts
- arXiv ID: 2403.06567
- Source URL: https://arxiv.org/abs/2403.06567
- Reference count: 40
- Primary result: Weakly-supervised foundation models like BiomedCLIP achieve P@1 of 0.594 without fine-tuning, closely matching specialized CBIR systems.

## Executive Summary
This study benchmarks vision foundation models for content-based image retrieval (CBIR) in radiology across 1.6 million images spanning four modalities and 161 pathologies. The research demonstrates that weakly-supervised models like BiomedCLIP can achieve strong retrieval performance (P@1 up to 0.594) without any fine-tuning, making them viable off-the-shelf solutions for medical image retrieval. The work also reveals important modality-specific performance variations, with ultrasound showing highest accuracy and X-ray lowest, while pathological structures are generally harder to retrieve than anatomical ones.

## Method Summary
The study evaluates 14 foundation models (ResNet, ViT, Ark, SAM, MedSAM, CLIP, MedCLIP, BiomedCLIP, BMC-CLIP, MAE, DINOv2, RAD-DINO) as frozen feature extractors for CBIR in radiology. Using four public datasets (RadImageNet, NIH14, MIMIC, CheXpert) totaling 1.6 million 2D images across CT, MR, X-ray, and US modalities, the researchers extract embeddings, normalize them with L2, and build FAISS indexes for retrieval. Performance is measured using P@1, P@3, P@5, and P@10 metrics (both micro and macro averages) on single-label test sets.

## Key Results
- Weakly-supervised BiomedCLIP achieves P@1 of 0.594 without fine-tuning, comparable to specialized CBIR systems
- Retrieval performance varies significantly by modality: US highest, X-ray lowest
- Retrieval for pathological structures is notably lower than for anatomical structures
- P@1 performance saturates at approximately 1000 samples per class in the index

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weakly-supervised vision models trained on large-scale biomedical image-text pairs can serve as effective off-the-shelf feature extractors for medical CBIR without fine-tuning.
- Mechanism: These models learn rich, generalizable visual representations by aligning images with detailed textual descriptions from scientific literature, capturing both modality-specific and pathological features.
- Core assumption: The semantic richness of biomedical image-text pairs enables models to learn transferable features relevant across diverse radiological contexts.
- Evidence anchors:
  - [abstract]: "Weakly-supervised models, particularly BiomedCLIP, as highly effective, achieving a P@1 of up to 0.594...comparable to specialized CBIR systems but without additional training"
  - [section]: "We show that weakly-supervised models like BiomedCLIP achieve P@1 of 0.594 without fine-tuning, closely matching specialized CBIR systems"
- Break condition: If domain shift between pretraining corpus and target radiology dataset is too large, or if textual descriptions lack sufficient visual detail.

### Mechanism 2
- Claim: Self-supervised models trained on natural images can achieve comparable CBIR performance to supervised models when used as feature extractors in medical imaging.
- Mechanism: Self-supervised learning captures intrinsic visual structure without relying on labels, leading to robust representations that transfer well to new domains.
- Core assumption: Visual features learned through pretext tasks (e.g., masked image modeling, contrastive learning) are sufficiently generic to encode meaningful medical image semantics.
- Evidence anchors:
  - [abstract]: "Self-supervised models display varied outcomes. DINOv2 shows retrieval capabilities comparable to supervised approaches"
  - [section]: "Surprisingly, the MAE trained on natural images outperforms both DINO approaches... achieving the fourth-highest P@1 score"
- Break condition: If target domain contains highly specialized features not present in natural image pretraining data.

### Mechanism 3
- Claim: Retrieval performance saturates after indexing ~1000 samples per class, suggesting a data efficiency threshold for foundation models in CBIR.
- Mechanism: Increasing index diversity improves retrieval by covering more intra-class variations, but beyond a certain point, additional samples yield diminishing returns.
- Core assumption: Foundation models' embedding spaces are sufficiently dense to capture class variations within the first ~1000 samples.
- Evidence anchors:
  - [section]: "P@1 starts saturating at around 1000 samples per class in the index... additional samples do not significantly improve retrieval outcomes"
- Break condition: If retrieval task requires fine-grained discrimination beyond what foundation models capture, or if class distributions are extremely long-tailed.

## Foundational Learning

- Concept: Contrastive learning objective
  - Why needed here: Explains how models learn to push similar images close and dissimilar images apart in embedding space, directly relevant to retrieval performance.
  - Quick check question: How does the temperature parameter τ in InfoNCE loss affect the hardness of negative samples during training?

- Concept: Feature normalization (L2)
  - Why needed here: Critical for ensuring cosine similarity comparisons are meaningful across embeddings of different magnitudes.
  - Quick check question: What happens to retrieval rankings if embeddings are not normalized before similarity computation?

- Concept: Macro vs micro averaging in evaluation
  - Why needed here: Clarifies why class-balanced metrics matter in medical CBIR where rare pathologies must not be overshadowed by frequent ones.
  - Quick check question: In a dataset with 90% normal cases and 10% pathology, which averaging method (macro or micro) better reflects retrieval performance for rare diseases?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Foundation model feature extractor -> L2 normalization -> FAISS indexing -> Cosine similarity retrieval

- Critical path:
  1. Load and resize images to model input size
  2. Extract embeddings using frozen foundation model
  3. Normalize embeddings to unit length
  4. Store in FAISS index
  5. For query: extract, normalize, compute cosine similarity, retrieve top-K

- Design tradeoffs:
  - Model size vs. inference speed (e.g., ViT-B/16 vs. ResNet50)
  - Embedding dimensionality vs. memory usage in FAISS
  - Index size vs. retrieval latency
  - Fine-tuning vs. off-the-shelf deployment simplicity

- Failure signatures:
  - Low P@1 across all queries → Poor feature extraction or misalignment with retrieval task
  - High P@1 for frequent classes, low for rare → Need macro-averaging or class-balanced sampling
  - Mode collapse in embeddings → Model not learning discriminative features
  - Memory overflow during indexing → Reduce batch size or dimensionality

- First 3 experiments:
  1. Verify embedding extraction pipeline: Run a small batch through BiomedCLIP, check shape and normalization.
  2. Test retrieval with known matches: Create a tiny synthetic dataset where perfect retrieval is possible, confirm system retrieves correctly.
  3. Benchmark indexing time vs. FAISS memory usage: Profile with varying numbers of samples to identify scaling bottlenecks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the retrieval performance of foundation models vary when evaluated on datasets with multi-label annotations, as opposed to the single-label datasets used in this study?
- Basis in paper: [explicit] The paper mentions that the combined dataset is limited to single-class labels only, which fails to capture the co-occurrence of multiple pathologies, a common scenario in clinical practice.
- Why unresolved: The study only evaluated models on single-label datasets, so the impact of multi-label annotations on retrieval performance remains unknown.
- What evidence would resolve it: Evaluating the same foundation models on a multi-label dataset and comparing their retrieval performance metrics (e.g., P@1, P@3, P@5, P@10) to those obtained on single-label datasets.

### Open Question 2
- Question: To what extent can fine-tuning foundation models on specific medical imaging modalities or pathologies improve retrieval accuracy compared to using them off-the-shelf?
- Basis in paper: [explicit] The paper suggests that future work should investigate fine-tuning foundation models for specific use cases, which has the potential to enhance retrieval accuracy by tailoring models to specific clinical requirements.
- Why unresolved: The study used foundation models off-the-shelf without fine-tuning, so the potential performance gains from fine-tuning are not quantified.
- What evidence would resolve it: Conducting experiments where foundation models are fine-tuned on specific modalities or pathologies and comparing their retrieval performance to off-the-shelf models.

### Open Question 3
- Question: How can foundation models be adapted to dynamically focus on regions of interest, such as pathological structures, to improve retrieval performance?
- Basis in paper: [explicit] The paper mentions that future work should investigate how to guide the model's attention to a region of interest (e.g., pathology), allowing better capturing of pathological structures in the embedding space, thereby increasing retrieval performance.
- Why unresolved: The study used global features for retrieval, so the impact of dynamically focusing on regions of interest is not explored.
- What evidence would resolve it: Developing and evaluating methods that incorporate attention mechanisms or region-based feature extraction to improve the retrieval of pathological structures.

## Limitations

- The study focuses exclusively on 2D images, leaving questions about foundation model effectiveness for volumetric data common in radiology
- Lack of fine-tuning means reported performance represents an upper bound for off-the-shelf deployment rather than competitive performance with specialized training
- The saturation effect at ~1000 samples per class requires further validation across different pathological subtypes and imaging contexts

## Confidence

- High confidence: The comparative performance of different foundation model families (weakly-supervised vs. self-supervised vs. supervised) is well-supported by the large-scale evaluation across multiple datasets and pathologies
- Medium confidence: The specific numerical results (e.g., P@1 = 0.594) are reliable for the tested conditions but may not generalize to all radiological contexts or foundation model variants
- Medium confidence: The conclusion that foundation models can serve as effective off-the-shelf CBIR systems is well-supported, though the margin over traditional methods varies by use case

## Next Checks

1. **Cross-domain generalization test**: Evaluate the same foundation models on an entirely different radiology dataset (e.g., from a different institution or country) to verify that the observed performance trends hold under domain shift conditions.

2. **Fine-tuning impact assessment**: Select the top 2-3 foundation models and perform minimal fine-tuning on a subset of the data to quantify the performance gap between frozen and fine-tuned deployments.

3. **Class-specific failure analysis**: For pathologies with P@1 < 0.3, conduct a detailed error analysis to identify whether failures stem from model limitations, data quality issues, or inherent ambiguity in the radiological features.