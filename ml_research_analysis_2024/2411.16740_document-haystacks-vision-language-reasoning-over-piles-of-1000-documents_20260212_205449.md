---
ver: rpa2
title: 'Document Haystacks: Vision-Language Reasoning Over Piles of 1000+ Documents'
arxiv_id: '2411.16740'
source_url: https://arxiv.org/abs/2411.16740
tags:
- question
- v-rag
- benchmarks
- retrieval
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two document haystack benchmarks, DocHaystack
  and InfoHaystack, designed to evaluate large multimodal models (LMMs) on large-scale
  visual document retrieval and understanding tasks. These benchmarks address the
  limitations of existing multi-image benchmarks, which typically pair each question
  with only up to 30 images, by requiring models to retrieve and reason from a much
  larger set of up to 1,000 documents per question.
---

# Document Haystacks: Vision-Language Reasoning Over Piles of 1000+ Documents

## Quick Facts
- arXiv ID: 2411.16740
- Source URL: https://arxiv.org/abs/2411.16740
- Reference count: 40
- Key outcome: V-RAG achieves 9% and 11% improvement in Recall@1 on DocHaystack-1000 and InfoHaystack-1000 benchmarks

## Executive Summary
This paper addresses the challenge of retrieving and reasoning from large collections of visual documents by introducing two new benchmarks, DocHaystack and InfoHaystack, that require models to process up to 1,000 documents per question. The authors propose V-RAG, a vision-centric retrieval-augmented generation framework that combines multiple vision encoders with an LMM-based relevance filter. This approach significantly improves retrieval performance over existing methods while enabling efficient processing of large document sets through early filtering of irrelevant content.

## Method Summary
The V-RAG framework addresses large-scale document retrieval through a multi-stage pipeline: first, an ensemble of vision encoders (CLIP, SigLIP, OpenCLIP) computes similarity scores between questions and document images, with results averaged for robust ranking; second, an LMM-filter module evaluates top-m documents for relevance using yes/no judgments; third, a fine-tuned LMM-VQA module answers questions from the filtered top-k documents. The framework is evaluated on two new benchmarks created by filtering existing DocVQA and InfographicVQA datasets to ensure questions have unique answers among 1,000 documents, with performance measured using Recall@1/3/5 metrics and model-based VQA assessment.

## Key Results
- V-RAG achieves 9% and 11% improvement in Recall@1 on DocHaystack-1000 and InfoHaystack-1000 benchmarks
- LMM-filter module improves retrieval efficiency by filtering out irrelevant documents before expensive LMM-VQA processing
- Fine-tuning LMM-VQA with distractor images improves robustness to irrelevant content in large document sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The V-RAG framework improves retrieval by combining multiple vision encoders with complementary strengths
- Mechanism: An ensemble of CLIP, SigLIP, and OpenCLIP encoders computes similarity scores between questions and document images, then averages these scores to produce a more robust relevance ranking
- Core assumption: Different vision encoders capture complementary aspects of document image features, and averaging their outputs reduces individual model biases
- Evidence anchors:
  - [abstract] "V-RAG combines multiple multimodal vision encoders, leveraging each encoder's unique strengths to enhance retrieval accuracy"
  - [section] "we represent each document as an image and utilize an ensemble of vision encoders, including CLIP, SigLIP, and OpenCLIP, each bringing distinct strengths to the image understanding"
  - [corpus] Weak evidence - no direct mention of ensemble methods in related papers, though document understanding appears relevant
- Break condition: If vision encoders' feature spaces are too dissimilar to benefit from averaging, or if one encoder dominates the ensemble, reducing diversity

### Mechanism 2
- Claim: The LMM-filter module removes irrelevant documents before expensive LMM-VQA processing
- Mechanism: Top-m documents from the vision encoder ensemble are passed to an LMM that answers yes/no whether each document can answer the question, filtering out irrelevant ones
- Core assumption: The LMM can accurately judge document-question relevance based on visual content, and filtering early reduces computational costs while maintaining accuracy
- Evidence anchors:
  - [abstract] "incorporates an LMM-filter module to assess the relevance of each document to the query, refining the retrieval process by ensuring that only relevant documents are prioritized"
  - [section] "we introduce a LMM-based question-image relevance assessment module. This module evaluates the relevance between each question and the top-m images identified in the first filtering step"
  - [corpus] No direct evidence - corpus doesn't mention relevance filtering in retrieval pipelines
- Break condition: If the LMM-filter incorrectly rejects relevant documents or accepts irrelevant ones, causing accuracy degradation

### Mechanism 3
- Claim: Fine-tuning the LMM-VQA module with distractor images improves robustness to irrelevant content
- Mechanism: During training, 1-10 random distractor images are added to each question, forcing the model to focus on relevant content among mixed positive and negative examples
- Core assumption: Exposure to distractors during training teaches the model to distinguish relevant from irrelevant visual content, improving performance in real scenarios with many documents
- Evidence anchors:
  - [section] "To improve the robustness of the LMM-VQA model in handling visual question answering with multiple distractor images, we further fine-tune the model using our curated training data"
  - [section] "we introduce 1-10 randomly sampled distractor images for each question, creating a challenging setting that encourages the model to focus on relevant content amid a mix of positive and negative images"
  - [corpus] No direct evidence - corpus doesn't discuss distractor-based training for VQA models
- Break condition: If distractors are too easy/difficult or if the model overfits to the specific distractor patterns used in training

## Foundational Learning

- Concept: Vision encoder ensembles and feature averaging
  - Why needed here: Understanding how different vision models capture complementary features and how averaging reduces individual biases
  - Quick check question: Why might averaging CLIP, SigLIP, and OpenCLIP similarity scores produce better document retrieval than using any single encoder?

- Concept: Multimodal relevance filtering with LLMs
  - Why needed here: Understanding how LLMs can judge visual relevance and how this filtering improves retrieval efficiency
  - Quick check question: How does an LMM-based yes/no filter for document relevance reduce computational costs in the V-RAG pipeline?

- Concept: Robust VQA training with distractors
  - Why needed here: Understanding how exposure to irrelevant content during training improves real-world performance
  - Quick check question: What is the purpose of adding 1-10 random distractor images during LMM-VQA fine-tuning?

## Architecture Onboarding

- Component map: Question → Vision encoders → Similarity averaging → Top-m selection → LMM-filter → Top-k selection → LMM-VQA → Answer
- Critical path: Question → Vision encoders → Similarity averaging → Top-m selection → LMM-filter → Top-k selection → LMM-VQA → Answer
- Design tradeoffs: Ensemble approach trades inference speed for accuracy; LMM-filter adds computational overhead but reduces expensive LMM-VQA calls; distractor training improves robustness but requires more training data
- Failure signatures: Low recall indicates vision encoder or LMM-filter issues; poor VQA accuracy with high recall suggests LMM-VQA problems; inconsistent performance across document types may indicate encoder bias
- First 3 experiments:
  1. Evaluate individual vision encoder performance vs ensemble on retrieval metrics to quantify benefit of averaging
  2. Test LMM-filter accuracy by comparing filtered vs unfiltered top-k results on a validation set
  3. Measure impact of distractor quantity (1-10) on LMM-VQA robustness by testing on controlled distractor sets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of V-RAG scale with larger document sets beyond 1,000 documents?
- Basis in paper: [explicit] The paper mentions that DocHaystack-1000 and InfoHaystack-1000 are the most challenging setups, but doesn't explore performance beyond 1,000 documents
- Why unresolved: The paper only tests up to 1,000 documents, leaving uncertainty about performance on even larger document sets
- What evidence would resolve it: Experiments evaluating V-RAG on benchmarks with 5,000 or 10,000 documents would provide insights into scalability and performance limitations

### Open Question 2
- Question: What is the impact of using different vision encoders in the ensemble on retrieval performance for specific document types?
- Basis in paper: [explicit] The paper mentions using CLIP, SigLIP, and OpenCLIP, but doesn't provide detailed analysis on how each encoder performs for different document types
- Why unresolved: The ensemble approach combines multiple encoders, but the individual contributions of each encoder to overall performance remain unclear
- What evidence would resolve it: A detailed ablation study analyzing the performance of each vision encoder on different document types (e.g., forms, tables, images) would clarify their individual impacts

### Open Question 3
- Question: How does the LMM-filter module's performance compare to other filtering approaches, such as unsupervised clustering or rule-based filtering?
- Basis in paper: [explicit] The paper introduces the LMM-filter module but doesn't compare it to alternative filtering methods
- Why unresolved: The effectiveness of the LMM-filter module is demonstrated, but its relative performance compared to other filtering approaches is unknown
- What evidence would resolve it: Comparative experiments evaluating the LMM-filter module against other filtering methods on the same benchmarks would provide insights into its relative effectiveness

## Limitations

- The benchmarks rely on curated datasets (DocVQA and InfographicVQA) that may not represent all document types or real-world scenarios
- V-RAG's performance gains depend heavily on the quality of vision encoders and LMMs used, which may not generalize to other model architectures
- Computational cost of running multiple vision encoders and the LMM-filter module may limit practical deployment in resource-constrained settings

## Confidence

**High Confidence**: The core claims about V-RAG's improvement over baseline retrieval methods (9% and 11% Recall@1 gains) are well-supported by quantitative results on two distinct benchmarks. The methodology for creating the document haystack benchmarks through rigorous filtering is clearly described and reproducible.

**Medium Confidence**: The mechanism claims about ensemble averaging and LMM-filter benefits are supported by experimental results, but the ablation studies could be more comprehensive. The specific contributions of individual components to overall performance are not fully isolated.

**Low Confidence**: The generalization of findings to documents outside the DocVQA and InfographicVQA domains remains uncertain. The impact of different distractor quantities during fine-tuning and their optimal selection is not thoroughly explored.

## Next Checks

1. **Cross-domain robustness test**: Evaluate V-RAG performance on document types not represented in the original benchmarks (medical records, legal documents, technical manuals) to assess generalization capabilities.

2. **Component ablation study**: Systematically remove individual vision encoders and the LMM-filter module to quantify their marginal contributions to overall performance, particularly examining the ensemble averaging mechanism.

3. **Computational efficiency analysis**: Measure inference time and resource usage of the complete V-RAG pipeline versus simplified alternatives to determine practical deployment tradeoffs.