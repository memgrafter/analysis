---
ver: rpa2
title: 'Hummer: Towards Limited Competitive Preference Dataset'
arxiv_id: '2405.11647'
source_url: https://arxiv.org/abs/2405.11647
tags:
- arxiv
- alignment
- preprint
- preference
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Hummer, the first preference dataset designed
  to reduce conflicts between alignment objectives in RLHF. The authors propose the
  Alignment Dimension Conflict (ADC) metric to quantify objective conflicts and construct
  Hummer using GPT-4 AI feedback on UltraFeedback.
---

# Hummer: Towards Limited Competitive Preference Dataset

## Quick Facts
- arXiv ID: 2405.11647
- Source URL: https://arxiv.org/abs/2405.11647
- Reference count: 40
- Key outcome: Hummer dataset achieves 8-10x lower ADC than existing datasets, with HummerRM showing 3.8% performance gains over UltraRM on RewardBench and exceptional robustness with less than 3% jailbreak rate increase

## Executive Summary
This paper introduces Hummer, the first preference dataset designed to reduce conflicts between alignment objectives in RLHF. The authors propose the Alignment Dimension Conflict (ADC) metric to quantify objective conflicts and construct Hummer using GPT-4 AI feedback on UltraFeedback. Hummer-F is a refined version with 80% of Hummer's size, filtered for higher quality. The hybrid sampling strategy dynamically balances performance across six alignment objectives (accuracy, conciseness, depth, empathy, tone, specificity). Results show Hummer achieves 8-10x lower ADC than existing datasets, with HummerRM and HummerRM-F models showing 3.8% and 3.2% performance gains over UltraRM on RewardBench. Most notably, HummerRM demonstrates exceptional robustness with less than 3% jailbreak rate increase after fine-tuning, compared to 10.4% increases in other models.

## Method Summary
The Hummer dataset is constructed through a three-stage process: first, 400 random pairs are sampled from UltraFeedback and annotated by GPT-4 with alignment dimensions and reasons; second, rewards are assigned to each sample using GPT-4 and the dataset is split based on maximal reward gaps per dimension with a threshold (τ=0.5) to create Hummer-F; third, HummerRM and HummerRM-F models are trained on Llama 2-7B using a hybrid sampling strategy that adaptively updates sampling weights based on performance deviations across alignment dimensions. The hybrid sampling strategy balances performance by increasing sampling for underperforming dimensions and decreasing it for overperforming ones.

## Key Results
- Hummer achieves 8-10x lower ADC than existing datasets (HH, UltraFeedback)
- HummerRM and HummerRM-F show 3.8% and 3.2% performance gains over UltraRM on RewardBench
- HummerRM demonstrates exceptional robustness with less than 3% jailbreak rate increase after fine-tuning, compared to 10.4% increases in other models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The Alignment Dimension Conflict (ADC) metric captures the negative performance deviations across alignment objectives when fine-tuning on one dimension.
- **Mechanism**: ADC quantifies conflicts by measuring the squared mean of negative performance deviations across all other dimensions when further fine-tuning on a specific alignment dimension.
- **Core assumption**: Performance degradation in one alignment dimension due to fine-tuning on another dimension is a valid indicator of conflict between alignment objectives.
- **Evidence anchors**: [abstract] "We introduce a novel statistical metric, Alignment Dimension Conflict, to quantify the degree of conflict within preference datasets." [section] "Formally, given a reward model... its performance... is represented by U={u1, u2, ···, un}. To study this conflict, we copy n reward models and further fine-tune each reward model on the dataset of interest for any alignment dimension... obtaining the fine-tuned performance Ui={ui,1, ui,2, ···, ui,n}. The performance deviation can be obtained by (Ui −U) of RMi..."
- **Break condition**: If performance improvements in one dimension don't correlate with degradation in others, ADC would not capture meaningful conflicts.

### Mechanism 2
- **Claim**: Hummer's construction using GPT-4 AI feedback enables creation of a dataset with reduced alignment dimension conflicts.
- **Mechanism**: By leveraging GPT-4 to annotate preferences, alignment dimensions, and reasons, then refining these dimensions to minimize conflicts, Hummer creates pairwise comparisons that emphasize limited competition between objectives.
- **Core assumption**: GPT-4's advanced capabilities can accurately identify and reduce conflicts between alignment dimensions better than human annotators alone.
- **Evidence anchors**: [abstract] "We then present Hummer and its fine-grained variant, Hummer-F, as innovative pairwise preference datasets with reduced-conflict alignment objectives." [section] "We leverage the powerful ability of AI feedback, i.e., GPT-4... to accurately capture the multidimensionality of human preference without interference between alignment dimensions."
- **Break condition**: If GPT-4's understanding of alignment dimensions is insufficient or biased, the conflict reduction would be ineffective.

### Mechanism 3
- **Claim**: The hybrid sampling strategy dynamically balances performance across alignment objectives by adjusting sampling weights based on relative performance.
- **Mechanism**: The strategy updates sampling weights λi based on the difference between each dimension's performance ui and the average performance ū, increasing sampling for underperforming dimensions and decreasing it for overperforming ones.
- **Core assumption**: Performance across alignment dimensions can be effectively balanced by dynamically adjusting the sampling distribution during training.
- **Evidence anchors**: [abstract] "we develop reward models, HummerRM and HummerRM-F, which employ a hybrid sampling approach to balance diverse alignment objectives effectively." [section] "We achieve the balance among all alignment dimensions by evaluating the preference performance across these dimensions, denoted as U={u1, ..., un}. The sampling weights are adaptively updated in every 1 epoch (128 steps) as follows: λi ← λi + η(ū − ui)"
- **Break condition**: If certain alignment objectives are inherently incompatible or if performance feedback is noisy, the hybrid sampling would fail to achieve balance.

## Foundational Learning

- **Concept**: Bradley-Terry model for preference modeling
  - Why needed here: The reward model optimization uses Bradley-Terry to learn preferences from pairwise comparisons, which is fundamental to the RLHF framework.
  - Quick check question: How does the Bradley-Terry model transform pairwise preferences into a differentiable objective for reward model training?

- **Concept**: Reinforcement Learning from Human Feedback (RLHF) pipeline
  - Why needed here: Understanding the full RLHF pipeline (SFT → Preference Modeling → Policy Optimization) is crucial for contextualizing where Hummer fits in the alignment process.
  - Quick check question: What are the three main stages of RLHF and how does each contribute to aligning language models with human preferences?

- **Concept**: Statistical metrics for dataset quality assessment
  - Why needed here: ADC is a novel metric that extends beyond traditional performance measures to quantify internal conflicts within datasets, requiring understanding of statistical evaluation methods.
  - Quick check question: How does ADC differ from traditional dataset evaluation metrics like average performance or coverage?

## Architecture Onboarding

- **Component map**: Dataset evaluation (ADC) -> Hummer dataset construction (GPT-4 feedback) -> Hybrid sampling strategy -> HummerRM reward models
- **Critical path**: ADC evaluation → GPT-4 annotation → Dataset refinement → Hybrid sampling implementation → Reward model training → Model deployment
- **Design tradeoffs**: Dataset size vs. quality (Hummer is smaller than UltraFeedback but shows better performance), computational cost of GPT-4 annotation vs. manual annotation, and complexity of hybrid sampling vs. simpler uniform sampling
- **Failure signatures**: ADC not capturing meaningful conflicts, GPT-4 feedback introducing bias or errors, hybrid sampling failing to converge to balanced weights, reward models overfitting to reduced-conflict dataset structure
- **First 3 experiments**:
  1. Calculate ADC for existing datasets (Anthropic HH, UltraFeedback) to establish baseline conflict levels and validate the metric's ability to capture meaningful differences
  2. Train HummerRM and HummerRM-F with and without hybrid sampling to isolate the contribution of the sampling strategy to performance improvements
  3. Conduct jailbreak attack evaluations on models trained with different datasets to empirically verify Hummer's robustness claims compared to existing approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Alignment Dimension Conflict (ADC) metric behave when evaluated on datasets with more than six alignment objectives, and what is the theoretical limit of ADC's applicability?
- Basis in paper: [inferred] The paper demonstrates ADC with six alignment objectives but does not explore its behavior with larger numbers of objectives or theoretical bounds.
- Why unresolved: The paper focuses on a specific case study with six objectives and does not provide theoretical analysis of ADC's scalability or limitations.
- What evidence would resolve it: Experiments testing ADC on datasets with varying numbers of alignment objectives (e.g., 10, 20, 50) would reveal scaling properties and potential limitations.

### Open Question 2
- Question: Does the hybrid sampling strategy maintain its effectiveness when applied to datasets with highly imbalanced distributions (e.g., 100:1 ratios between most and least frequent objectives)?
- Basis in paper: [explicit] The paper tests hybrid sampling on a moderately imbalanced dataset (10:10:10:10:1:1) but does not explore extreme imbalance scenarios.
- Why unresolved: The current experiments use a relatively balanced dataset, leaving open questions about performance at extreme imbalance levels.
- What evidence would resolve it: Testing hybrid sampling on datasets with progressively more extreme imbalance ratios would reveal its breaking points and effectiveness limits.

### Open Question 3
- Question: What is the relationship between ADC values and actual jailbreak vulnerability in real-world adversarial scenarios, beyond the controlled experiments presented?
- Basis in paper: [explicit] The paper shows correlation between low ADC and reduced jailbreak rates but does not establish causation or test against sophisticated adversarial attacks.
- Why unresolved: The jailbreak evaluation uses a specific dataset and methodology, but real-world attacks may differ significantly in complexity and approach.
- What evidence would resolve it: Testing HummerRM against a comprehensive suite of adversarial jailbreak techniques in real-world deployment scenarios would establish the practical relationship between ADC and security.

## Limitations
- Heavy reliance on GPT-4 for dataset construction creates potential reproducibility challenges
- ADC metric effectiveness depends on accurate performance feedback which may be noisy in practice
- Results primarily validated on Llama 2-7B, limiting generalizability to larger model architectures

## Confidence
- High confidence: ADC metric validity for measuring alignment dimension conflicts, performance improvements on RewardBench, robustness to jailbreak attacks
- Medium confidence: Hybrid sampling strategy effectiveness, GPT-4's ability to accurately capture alignment dimensions
- Low confidence: Generalization of results to larger model architectures beyond Llama 2-7B

## Next Checks
1. **Cross-architecture validation**: Test HummerRM on larger models (70B+ parameters) to verify scalability of performance gains and robustness claims
2. **Human evaluation**: Conduct human preference studies comparing model outputs from HummerRM versus UltraRM to validate that reduced ADC correlates with improved human preference satisfaction
3. **Dataset ablation study**: Systematically remove samples from different alignment dimensions to quantify their individual contributions to performance improvements and robustness