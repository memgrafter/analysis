---
ver: rpa2
title: 'Large Language Models as Biomedical Hypothesis Generators: A Comprehensive
  Evaluation'
arxiv_id: '2407.08940'
source_url: https://arxiv.org/abs/2407.08940
tags:
- hypothesis
- shot
- arxiv
- llms
- background
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models can generate novel and validated biomedical
  hypotheses even when tested on literature unseen during training. Increasing uncertainty
  through multi-agent interactions and tool use improves zero-shot hypothesis generation
  performance, though few-shot learning and tool use may not always yield performance
  gains.
---

# Large Language Models as Biomedical Hypothesis Generators: A Comprehensive Evaluation

## Quick Facts
- arXiv ID: 2407.08940
- Source URL: https://arxiv.org/abs/2407.08940
- Reference count: 40
- Large language models can generate novel and validated biomedical hypotheses even when tested on literature unseen during training.

## Executive Summary
This study evaluates the capability of large language models (LLMs) to generate novel biomedical hypotheses using a temporally-partitioned dataset from PubMed literature. The research introduces a comprehensive evaluation framework with four novel metrics (novelty, relevance, significance, verifiability) and explores different generation settings including zero-shot, few-shot, and multi-agent approaches. Results show that LLMs can produce validated hypotheses on unseen literature, with zero-shot generation often yielding higher novelty scores. The study also demonstrates that introducing uncertainty through multi-agent interactions and tool use can enhance hypothesis generation performance.

## Method Summary
The researchers constructed a dataset of background-hypothesis pairs from PubMed literature, partitioning it into training (before 2023), seen test, and unseen test (after 2023) sets to prevent data contamination. They evaluated multiple instructed LLMs (ChatGPT, Llama-2, WizardLM, MedAlpaca, PMC-LLaMA) in zero-shot, few-shot, and fine-tuning settings using four novel metrics (novelty, relevance, significance, verifiability) assessed by both GPT-4 and human experts. A multi-agent framework with specialized roles (Analyst, Engineer, Scientist, Critic) was implemented to explore uncertainty in hypothesis generation, with optional tool integration (PubMed search). The study compared performance across different settings and analyzed the impact of uncertainty on hypothesis quality.

## Key Results
- LLMs successfully generated novel and validated hypotheses on literature unseen during training
- Zero-shot generation often produced higher novelty hypotheses compared to few-shot learning
- Multi-agent collaboration with tool use introduced beneficial uncertainty that enhanced hypothesis generation capabilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Zero-shot generation allows LLMs to produce higher novelty hypotheses by avoiding overfitting to in-context examples.
- **Mechanism**: In zero-shot, the model relies entirely on its pre-trained knowledge and inference ability, avoiding the anchoring effect of few-shot examples which can constrain hypothesis diversity.
- **Core assumption**: Pre-trained LLMs have sufficient foundational knowledge in biomedical domains to generate reasonable hypotheses without additional examples.
- **Evidence anchors**:
  - [abstract]: "LLMs can generate novel and validated hypotheses, even when tested on literature unseen during training"
  - [section 3.2.1]: "nearly all models, particularly WizardLM series models, and Openchat-v3.2-super, show enhanced novelty capabilities in a zero-shot setting"
  - [corpus]: Weak - corpus doesn't contain specific zero-shot evaluation data
- **Break condition**: If foundational biomedical knowledge in the model is insufficient or biased, zero-shot generation will produce low-quality or irrelevant hypotheses.

### Mechanism 2
- **Claim**: Multi-agent collaboration enhances hypothesis quality through specialized role-based reasoning.
- **Mechanism**: Different agents (Analyst, Engineer, Scientist, Critic) each focus on specific subtasks, creating a division of labor that mirrors scientific discovery processes and introduces beneficial uncertainty through iterative refinement.
- **Core assumption**: Role specialization improves overall system performance compared to single-agent approaches by allowing focused reasoning and cross-validation.
- **Evidence anchors**:
  - [section 4.2]: "division of labor and interaction among multi-agents can significantly enhance the model's capability to propose hypotheses by introducing uncertainty"
  - [section 3.3]: "hypotheses generated in the zero-shot setting exhibit higher uncertainty and novelty scores"
  - [corpus]: Weak - limited empirical comparison of multi-agent vs single-agent approaches
- **Break condition**: If agent communication becomes inefficient or agents duplicate efforts, the collaborative framework may introduce unnecessary complexity without performance gains.

### Mechanism 3
- **Claim**: Temporal dataset construction prevents data contamination and enables valid zero-shot evaluation.
- **Mechanism**: By partitioning data based on publication dates (pre-2023 for training, post-2023 for testing), the approach ensures that test hypotheses are truly novel and not memorized from training.
- **Core assumption**: LLMs cannot extrapolate knowledge beyond their training cutoff to generate valid hypotheses about literature published after their training period.
- **Evidence anchors**:
  - [section 2.2]: "we have delineated 'seen' and 'unseen' test sets, with the 'unseen' test set being considered for zero-shot analysis"
  - [section 2.1]: "Our objective is to assess model M by having it generate hypotheses based on the task instruction and background knowledge"
  - [corpus]: Moderate - corpus shows temporal distribution but doesn't validate extrapolation limits
- **Break condition**: If LLMs can effectively generalize beyond their training cutoff through reasoning patterns, temporal partitioning may be overly conservative.

## Foundational Learning

- **Concept**: Temporal data partitioning for avoiding data contamination
  - Why needed here: Ensures that generated hypotheses are genuinely novel and not memorized from training data
  - Quick check question: How would you partition a dataset to test whether a model can generate predictions about future events it hasn't seen during training?

- **Concept**: Multi-agent role specialization
  - Why needed here: Different roles (Analyst, Engineer, Scientist, Critic) each contribute unique capabilities to the hypothesis generation process
  - Quick check question: What are the key responsibilities of each role in the proposed multi-agent framework?

- **Concept**: Uncertainty quantification in generation tasks
  - Why needed here: Measures like SelfBLEU help assess the diversity and novelty of generated hypotheses
  - Quick check question: Why might lower SelfBLEU scores indicate higher novelty in hypothesis generation?

## Architecture Onboarding

- **Component map**: PubMed literature extraction → temporal partitioning → instruction formatting → hypothesis generation → evaluation (automatic + GPT-4 + human)
- **Critical path**: 
  1. Construct temporal dataset (PubMed → partition by date)
  2. Format instructions for different experimental settings
  3. Run hypothesis generation across model zoo
  4. Evaluate using multiple metrics
  5. Analyze results for patterns and insights
- **Design tradeoffs**: 
  - Temporal partitioning vs. random splitting: ensures true novelty but limits dataset size
  - Multi-agent complexity vs. single-agent simplicity: potentially better quality but harder to debug
  - Automated vs. human evaluation: scalable but may miss nuanced quality issues
- **Failure signatures**: 
  - High BLEU/ROUGE scores on unseen data suggest potential data leakage or memorization
  - Low novelty scores across all models indicate insufficient domain knowledge in LLMs
  - Multi-agent systems getting stuck in loops suggest poor agent coordination
- **First 3 experiments**:
  1. Run zero-shot generation on a small subset of unseen data to verify basic functionality
  2. Compare few-shot vs. zero-shot performance on the same examples to identify anchoring effects
  3. Test single-agent vs. multi-agent generation on identical prompts to measure collaboration benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of factual hallucinations in large language models (LLMs) on hypothesis generation, and how can this issue be mitigated?
- Basis in paper: [inferred] The paper mentions the need for a thorough investigation into the impact of factual hallucinations inherent in LLMs on hypothesis formulation.
- Why unresolved: The paper acknowledges this limitation but does not provide a detailed analysis or solutions to address it.
- What evidence would resolve it: Empirical studies comparing hypothesis generation with and without hallucination mitigation techniques, along with a detailed analysis of the types and frequency of hallucinations in generated hypotheses.

### Open Question 2
- Question: How does the incorporation of domain-specific knowledge graphs improve the quality and novelty of hypotheses generated by LLMs?
- Basis in paper: [explicit] The paper mentions the intention to incorporate knowledge graphs in future work to aid in generating higher-quality hypotheses.
- Why unresolved: The paper does not provide any empirical evidence or analysis on the impact of knowledge graphs on hypothesis generation.
- What evidence would resolve it: Comparative studies of hypothesis generation with and without the integration of domain-specific knowledge graphs, along with an analysis of the improvements in hypothesis quality and novelty.

### Open Question 3
- Question: What are the optimal strategies for introducing uncertainty into the hypothesis generation process using reinforcement learning?
- Basis in paper: [explicit] The paper mentions the intention to optimize strategies that introduce uncertainty via reinforcement learning to enhance hypothesis generation capabilities.
- Why unresolved: The paper does not provide any details on the specific reinforcement learning strategies or their effectiveness in introducing uncertainty.
- What evidence would resolve it: Experimental results comparing different reinforcement learning strategies for introducing uncertainty, along with an analysis of their impact on the diversity and quality of generated hypotheses.

## Limitations
- Temporal partitioning may underestimate LLMs' true generalization capabilities if models can reason beyond their training cutoff
- Multi-agent framework complexity may not scale efficiently and could introduce unnecessary overhead
- Automated evaluation metrics may not capture all nuances of scientific hypothesis quality despite human expert validation

## Confidence

**High confidence**: LLMs can generate novel hypotheses on unseen literature (supported by clear temporal partitioning results)

**Medium confidence**: Multi-agent collaboration improves hypothesis quality (evidence shows benefits but lacks comprehensive comparison with single-agent approaches)

**Medium confidence**: Zero-shot generation outperforms few-shot in novelty (supported by results but mechanism not fully explained)

## Next Checks

1. Test hypothesis generation on literature published beyond the LLM training cutoff by 2+ years to verify true zero-shot capability and measure potential reasoning extrapolation

2. Conduct ablation studies comparing single-agent vs. multi-agent performance across different model sizes and domains to quantify collaboration benefits

3. Implement cross-validation with domain experts on a larger scale to validate the proposed evaluation metrics and identify potential blind spots in automated assessment