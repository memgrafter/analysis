---
ver: rpa2
title: Continuous Learned Primal Dual
arxiv_id: '2405.02478'
source_url: https://arxiv.org/abs/2405.02478
tags:
- dose
- algorithm
- clpd
- primal
- dual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a continuous version of the Learned Primal
  Dual (LPD) algorithm for CT reconstruction by replacing discrete convolutional blocks
  with Neural Ordinary Differential Equations (Neural ODEs). The proposed Continuous
  LPD (cLPD) leverages the robustness of Neural ODEs to noise while maintaining data-driven
  learning capabilities.
---

# Continuous Learned Primal Dual

## Quick Facts
- arXiv ID: 2405.02478
- Source URL: https://arxiv.org/abs/2405.02478
- Reference count: 18
- Key outcome: Continuous LPD outperforms standard LPD and FBP in CT reconstruction, particularly in high-noise and limited-angle scenarios

## Executive Summary
This paper introduces Continuous Learned Primal Dual (cLPD), a novel CT reconstruction method that replaces discrete convolutional blocks with Neural Ordinary Differential Equations (Neural ODEs). The approach combines the data-driven learning capabilities of the Learned Primal Dual algorithm with the inherent noise robustness of continuous formulations. Experiments across various CT reconstruction settings demonstrate that cLPD achieves superior SSIM and PSNR metrics compared to both standard LPD and Filtered Backprojection methods, particularly in challenging scenarios with high noise levels or restricted geometries.

## Method Summary
The method implements Continuous LPD by substituting the discrete convolutional blocks in the standard LPD algorithm with Neural ODE blocks. The architecture maintains the iterative primal-dual structure while using continuous ODE transformations to model the updates in each iteration. The model is trained using the Adam optimizer with a batch size of 2 and learning rate of 1e-4 for 100 epochs. A key finding is that cLPD requires normalization at every layer for stable training, unlike standard LPD which performs best without normalization.

## Key Results
- cLPD outperforms both LPD and FBP in terms of SSIM and PSNR metrics across clinical, reduced dose, and restricted geometry settings
- Visual results show artifact-free reconstructions in limited-angle geometries where LPD and FBP fail
- cLPD demonstrates superior performance particularly in high-noise scenarios, confirming its noise robustness advantage
- The method maintains data fidelity while improving reconstruction quality through continuous ODE formulations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continuous formulation via Neural ODEs provides inherent noise robustness compared to discrete convolutional layers
- Mechanism: Neural ODEs model the network as a continuous ODE transformation, which has smoother dynamics and better generalization under perturbations
- Core assumption: The continuous dynamics better capture the underlying data manifold and are less sensitive to measurement noise
- Evidence anchors: The paper notes that noise rejection is a key feature, and continuous models can enhance performance in CT reconstruction

### Mechanism 2
- Claim: Replacing discrete convolutional blocks in LPD with continuous ODE blocks preserves data fidelity while improving reconstruction quality
- Mechanism: The continuous LPD maintains the iterative primal-dual structure but uses Neural ODEs to model transformations in each iteration
- Core assumption: Neural ODE blocks can approximate proximal operators in LPD while being more robust to noise and discretization artifacts
- Evidence anchors: Experimental results show cLPD outperforms both LPD and FBP in terms of image quality metrics and visual results

### Mechanism 3
- Claim: Normalization at every layer is required for stable training of cLPD but not for standard LPD
- Mechanism: The continuous nature of ODE blocks introduces different numerical stability requirements compared to discrete convolutions
- Core assumption: The adjoint method used for backpropagation in Neural ODEs introduces additional numerical sensitivities that require normalization
- Evidence anchors: The paper explicitly states that continuous LPD requires normalization at every layer for stable training, unlike standard LPD

## Foundational Learning

- Concept: Ordinary Differential Equations (ODEs) and their numerical solution
  - Why needed here: Neural ODEs are fundamentally based on ODEs, and understanding their properties is crucial for designing and training these networks
  - Quick check question: What is the difference between an initial value problem and a boundary value problem in ODEs?

- Concept: Computed Tomography (CT) reconstruction and the Radon transform
  - Why needed here: The paper applies Neural ODEs to CT reconstruction, so understanding the underlying physics and mathematical formulation is essential
  - Quick check question: What is the Radon transform, and how is it related to the forward operator in CT reconstruction?

- Concept: Primal-Dual Hybrid Gradient (PDHG) algorithm and its learned variant (LPD)
  - Why needed here: The paper builds upon the LPD algorithm, so understanding its structure and how it differs from standard optimization methods is important
  - Quick check question: How does the LPD algorithm differ from the classical PDHG algorithm, and what role do the neural networks play?

## Architecture Onboarding

- Component map: Input sinogram data (y) -> Forward operator (A) -> cLPD (primal and dual networks with Neural ODE blocks) -> Output reconstructed image (x)

- Critical path:
  1. Initialize primal (x0) and dual (z0) variables
  2. Iterate I times:
    a. Update dual variable using Neural ODE block (Γc_θd)
    b. Update primal variable using Neural ODE block (Λc_θp)
  3. Output final primal variable as the reconstructed image

- Design tradeoffs:
  - Continuous vs. discrete representations: Neural ODEs provide better noise robustness but may require more computational resources and careful initialization
  - Number of iterations (I): More iterations can lead to better reconstructions but increase computational cost and risk of overfitting
  - Network architecture within Neural ODE blocks: The choice of layers and activation functions affects the expressiveness and stability of the model

- Failure signatures:
  - Unstable training: If Neural ODE blocks are not properly initialized or normalized, training may become unstable or diverge
  - Overfitting: If the model is too complex or training data is limited, it may overfit to the training set
  - Loss of details: If continuous formulation oversmooths reconstructions, fine details may be lost

- First 3 experiments:
  1. Compare cLPD and LPD on simple CT reconstruction task with low noise levels to verify similar performance in ideal conditions
  2. Evaluate cLPD and LPD on CT reconstruction task with high noise levels to demonstrate noise robustness
  3. Test cLPD and LPD on CT reconstruction task with limited angle geometry to showcase advantages in challenging scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does cLPD require normalization at every layer for stable training, whereas standard LPD performs best without normalization?
- Basis in paper: The paper notes that "continuous LPD requires normalisation at every layer for a stable training whereas the standard LPD achieves best results without any form of normalisation"
- Why unresolved: The authors state this difference is interesting but do not provide a definitive explanation or investigation into the underlying reasons
- What evidence would resolve it: A detailed theoretical analysis comparing training dynamics of normalized vs. unnormalized cLPD and LPD, or experimental ablation studies

### Open Question 2
- Question: Can continuous representations be effectively applied to other algorithms in CT reconstruction and inverse problems beyond LPD?
- Basis in paper: The authors state "Future work also includes exploring continous representations for other algorithms in CT reconstruction and inverse problems in general"
- Why unresolved: The paper only demonstrates cLPD on the LPD algorithm, so generalizability to other methods remains untested
- What evidence would resolve it: Successful application and performance improvements when applied to other CT reconstruction algorithms or inverse problems in different domains

### Open Question 3
- Question: How does the performance of cLPD compare to other state-of-the-art CT reconstruction methods, including both learned and classical approaches, across various noise levels and geometries?
- Basis in paper: While the paper compares cLPD to LPD and FBP across different settings, it does not compare against the full spectrum of existing CT reconstruction methods
- Why unresolved: The experimental section focuses on comparing cLPD primarily with LPD and FBP, leaving a gap in understanding how it stacks up against other competitive methods
- What evidence would resolve it: Comprehensive benchmarking of cLPD against a wide range of CT reconstruction methods across multiple datasets and imaging scenarios

## Limitations

- Statistical significance of performance improvements is not established due to lack of multiple random seed experiments
- Computational efficiency analysis is missing, particularly wall-clock time comparison between cLPD and standard LPD
- The paper lacks comprehensive ablation studies to isolate the contribution of Neural ODEs from other architectural choices

## Confidence

- **High confidence**: The continuous LPD architecture and its relationship to standard LPD is clearly defined and implementable
- **Medium confidence**: Noise robustness improvements, as results are compelling but lack statistical validation
- **Low confidence**: Claims about inherent advantages of continuous formulations over discrete ones, as these require more rigorous ablation studies

## Next Checks

1. **Statistical validation**: Run cLPD training with 5 different random seeds on the same datasets and report mean/confidence intervals for SSIM/PSNR metrics to establish statistical significance of performance improvements

2. **Ablation study**: Implement a discrete LPD variant with layer normalization (matching cLPD's normalization strategy) to isolate whether performance gains come from continuous formulation or normalization strategy

3. **Computational efficiency analysis**: Measure wall-clock training time and inference time per iteration for both cLPD and standard LPD across different iteration counts to quantify the practical cost of continuous formulations