---
ver: rpa2
title: Large Language Models Engineer Too Many Simple Features For Tabular Data
arxiv_id: '2410.17787'
source_url: https://arxiv.org/abs/2410.17787
tags:
- feature
- operators
- llms
- features
- engineering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) exhibit
  bias when used for feature engineering in tabular data problems. The authors propose
  a method to detect bias by comparing the frequency distribution of operators used
  by LLMs to generate new features against the distribution obtained by an automated
  feature engineering tool.
---

# Large Language Models Engineer Too Many Simple Features For Tabular Data

## Quick Facts
- arXiv ID: 2410.17787
- Source URL: https://arxiv.org/abs/2410.17787
- Reference count: 40
- Large language models exhibit strong bias toward simple operators when used for feature engineering in tabular data

## Executive Summary
This paper investigates operator frequency bias in large language models when used for feature engineering on tabular datasets. The authors compare four LLMs (two frontier and two open-source) against an automated feature engineering tool across 27 classification datasets. They find that LLMs disproportionately favor simple operators like addition while rarely using complex operators like grouping followed by aggregations. This bias negatively impacts predictive performance, with LLMs often performing worse than no feature engineering at all, particularly in the larger frontier models.

## Method Summary
The authors evaluate LLMs for feature engineering by generating features through chain-of-thought prompting with a feedback loop. They compare operator frequency distributions between LLMs and the automated tool OpenFE, then assess predictive performance using 10-fold cross-validation with LightGBM. The evaluation uses 27 tabular classification datasets filtered to have ≤100 features, ≤100,000 samples, and ≤10 classes. Memorization tests ensure datasets weren't seen during LLM training. Features are generated with statistical descriptions and operator lists, and performance is measured using ROC AUC improvements.

## Key Results
- LLMs show strong bias toward simple operators (addition) and rarely use complex operators (grouping + aggregation)
- While OpenFE improved performance on 21 of 27 datasets, LLMs often performed worse than no feature engineering
- The bias was particularly strong in the two larger frontier models (GPT-4o-mini and Gemini-1.5-flash)
- Code generation approach had >95% failure rate for small models, requiring structured output instead

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit operator frequency bias because their training data contains imbalanced operator usage patterns
- Mechanism: During pre-training, LLMs learn to reproduce common patterns in their training corpus. If certain operators (like addition) appear more frequently in tabular data examples, the model learns to prefer them
- Core assumption: The training corpus for frontier models contains disproportionately more examples of simple operations than complex ones
- Evidence anchors: "LLMs are biased toward simple operators, such as addition, and can fail to utilize more complex operators, such as grouping followed by aggregations." and "LLMs favor simple operators during feature engineering (e.g., Add) and rarely use more complex operators (e.g., GroupByThenMean)."

### Mechanism 2
- Claim: Operator selection bias negatively impacts predictive performance by limiting feature diversity
- Mechanism: Simple operators create features that capture only basic relationships, missing complex interactions that could improve model performance
- Core assumption: Complex operators (grouping + aggregation) capture more informative patterns than simple arithmetic operations
- Evidence anchors: "the bias can negatively impact the predictive performance when using LLM-generated features" and "While the automated tool improved performance on 21 of 27 datasets, the LLMs often performed worse than no feature engineering at all."

### Mechanism 3
- Claim: Model size affects bias magnitude due to architectural differences in attention patterns
- Mechanism: Larger models with more parameters can capture more nuanced patterns but may also overfit to common patterns in training data, amplifying bias
- Core assumption: Frontier models have different attention mechanisms or training objectives that make them more susceptible to operator frequency bias
- Evidence anchors: "This bias was particularly strong in the two larger frontier models." and "In particular, we observed a strong bias and negative impact for GPT-4o-mini and Gemini-1.5-flash, two big frontier models."

## Foundational Learning

- Concept: Tabular data feature engineering
  - Why needed here: Understanding what operators do and why complex ones might be more valuable than simple ones
  - Quick check question: What's the difference between "Add" and "GroupByThenMean" in terms of the patterns they can capture?

- Concept: Operator frequency analysis
  - Why needed here: The method compares operator usage distributions between LLMs and automated tools
  - Quick check question: If an LLM uses "Add" 50% of the time but OpenFE uses it only 10% of the time, what does this suggest about bias?

- Concept: Cross-validation and performance evaluation
  - Why needed here: Results show how feature engineering affects predictive accuracy across multiple folds
  - Quick check question: Why is ROC AUC used instead of accuracy for evaluating feature engineering impact?

## Architecture Onboarding

- Component map: LLM → Prompt template → Feature generation → Feedback loop → Dataset update → Performance evaluation
- Critical path: Prompt generation → Feature selection → Operator application → Performance validation
- Design tradeoffs: Code generation (more expressive, higher failure rate) vs structured output (less expressive, lower failure rate)
- Failure signatures: High frequency of "invalid-operator" responses, performance worse than baseline, operator distribution dissimilar to OpenFE
- First 3 experiments:
  1. Run memorization tests on a new dataset to ensure LLM hasn't seen it before
  2. Generate features with one LLM using the prompt template and measure operator frequency distribution
  3. Compare predictive performance with and without generated features using cross-validation

## Open Questions the Paper Calls Out

The paper explicitly suggests future work should explore in-context learning (e.g., prompt tuning) or fine-tuning the LLM to favor optimal operators, though it doesn't frame these as specific open questions to be answered.

## Limitations

- Dataset representativeness uncertainty: The 27 datasets used are from the AutoML benchmark, which may not represent real-world tabular data diversity
- Operator classification ambiguity: The paper defines 11 operators but doesn't provide detailed criteria for distinguishing between similar operators
- Performance attribution uncertainty: Unclear whether performance degradation is primarily due to operator bias or other factors like feature quality or redundancy

## Confidence

**High confidence**: LLMs exhibit operator frequency bias favoring simple operators over complex ones. This is directly measurable through operator distribution analysis and consistently observed across all four tested LLMs.

**Medium confidence**: The operator bias negatively impacts predictive performance. While the correlation is demonstrated, the causal mechanism could involve multiple factors beyond just operator selection.

**Medium confidence**: Frontier models show stronger bias than open-source models. This pattern is observed but could be influenced by differences in prompt engineering, API constraints, or evaluation methodology rather than inherent architectural differences.

## Next Checks

1. **Cross-dataset bias pattern validation**: Test the same four LLMs on additional tabular datasets outside the AutoML benchmark (e.g., Kaggle competitions, UCI repository) to verify if the operator frequency bias pattern holds across different data distributions and problem domains.

2. **Operator impact ablation study**: Systematically evaluate predictive performance when using only simple operators versus only complex operators to isolate whether the performance degradation is specifically due to the absence of complex operators or the presence of excessive simple operators.

3. **Fine-tuning intervention validation**: Fine-tune one of the frontier models on a balanced corpus of feature engineering examples that equally represents simple and complex operators, then re-evaluate operator frequency distribution and predictive performance to test whether bias can be mitigated through targeted training.