---
ver: rpa2
title: Voice Communication Analysis in Esports
arxiv_id: '2411.19793'
source_url: https://arxiv.org/abs/2411.19793
tags:
- speaker
- sentence
- communication
- sentences
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the analysis of voice communications in esports,
  focusing on detecting duplicate and "parasite" communication to improve team performance.
  The authors propose using semantic similarity measures and NLP embedding techniques
  to assess communication quality.
---

# Voice Communication Analysis in Esports

## Quick Facts
- arXiv ID: 2411.19793
- Source URL: https://arxiv.org/abs/2411.19793
- Reference count: 40
- Primary result: mxbai-embed-large-v1 model achieves 50.91% F1-score for duplicate and 53.33% for parasite communication detection

## Executive Summary
This paper addresses the analysis of voice communications in esports, focusing on detecting duplicate and "parasite" communication to improve team performance. The authors propose using semantic similarity measures and NLP embedding techniques to assess communication quality. They employ sentence transformers to compute embeddings of transcribed speech and calculate cosine similarity to identify repetitive or unclear communication patterns. The study uses a threshold of 0.6 for similarity scores to flag duplicate and parasite communications.

## Method Summary
The method involves transcribing audio using Whisper, performing speaker diarization with PixIT, and computing sentence embeddings using the mxbai-embed-large-v1 model. For duplicate detection, cosine similarity is calculated between each sentence and all previous sentences from the same speaker within a 15-second window, with scores above 0.6 flagged as duplicates. For parasite detection, similarity is computed between each sentence and predefined parasite phrases, also using the 0.6 threshold. The paper also proposes embedding refinement using preceding conversation context for single-word utterances.

## Key Results
- mxbai-embed-large-v1 model achieves highest F1-score for both tasks: 50.91% for duplicate and 53.33% for parasite communication detection
- 0.6 similarity threshold effectively flags repetitive communications
- Small evaluation corpus of 129 sentences from 3 speakers shows feasibility of the approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic similarity via sentence transformers effectively captures redundancy in esports voice communication.
- Mechanism: The system embeds each spoken sentence into a high-dimensional space using a pre-trained transformer model, then computes cosine similarity between the current sentence and all sentences spoken by the same player in the last 15 seconds. High similarity scores (above 0.6) indicate duplicate communication.
- Core assumption: Semantic meaning is preserved in the embedding space, and cosine similarity reliably reflects semantic overlap for short esports utterances.
- Evidence anchors:
  - [abstract] "We employ sentence transformers to compute embeddings of transcribed speech and calculate cosine similarity to identify repetitive or unclear communication patterns."
  - [section 3.1.1] "By using this approach, we move closer to the process of retrieving relevant pieces of information in open-domain question answering [5] by checking how similar two pieces of text are by semantic/lexical meaning."
  - [corpus] Weak - no directly relevant neighbor papers on semantic similarity for esports communication.

### Mechanism 2
- Claim: Embedding refinement using preceding conversation context improves parasite communication detection for single-word utterances.
- Mechanism: For short sentences (e.g., "Yes", "Okay"), the system recomputes the embedding by pooling token embeddings from the preceding conversation window, thus encoding contextual meaning before computing similarity with parasite phrasing templates.
- Core assumption: Pooling token embeddings over a fixed preceding window captures enough context to disambiguate single-word responses.
- Evidence anchors:
  - [section 4.2] "To leverage this issue, we could compute the embedding of the conversation context a fixed time prior to the problematic sentence, then make a pooling operation on the individual token embeddings..."
  - [section 4.2.1] "As we can see the recomputed embedding on sentence 018 yields better results when performing sentence similarity compared to the non recomputed one at figure 6."
  - [corpus] Weak - no neighbor papers on context pooling for speech embedding refinement.

### Mechanism 3
- Claim: Fixed similarity thresholds (0.6) provide a practical decision boundary for flagging duplicate and parasite communications.
- Mechanism: After computing similarity scores, any score above 0.6 is treated as a positive detection. This binary thresholding simplifies downstream analysis and metric computation.
- Core assumption: The threshold value of 0.6 generalizes across different speakers and conversation contexts within the same game.
- Evidence anchors:
  - [section 3.2.2] "However it is noticable that if the score is raising above the 0.6 threshold, the given sentence is somewhat repetitive."
  - [section 5.2] "For duplicate communication and parasite communicaiton we took a fixed threshold of decision at 0.6."
  - [corpus] Weak - no neighbor papers discussing threshold selection for esports communication analysis.

## Foundational Learning

- Concept: Cosine similarity in high-dimensional embedding spaces
  - Why needed here: It quantifies semantic overlap between transcribed voice segments, enabling automated detection of redundancy.
  - Quick check question: If two sentence embeddings have a cosine similarity of 0.8, what does that imply about their semantic relationship?

- Concept: Sentence transformers and contextual embeddings
  - Why needed here: They convert variable-length spoken text into fixed-size vectors that preserve meaning, which is essential for similarity comparison.
  - Quick check question: Why might a general-purpose sentence transformer underperform on esports-specific jargon compared to a fine-tuned model?

- Concept: Speaker diarization and forced alignment
  - Why needed here: They assign each transcribed segment to the correct player and timestamp, ensuring accurate per-player similarity tracking.
  - Quick check question: What could happen to duplicate detection accuracy if speaker attribution errors occur frequently?

## Architecture Onboarding

- Component map: Audio input → Whisper transcription → Speaker diarization (PixIT) → Forced alignment → Sentence segmentation → Sentence transformer embedding → Cosine similarity computation → Threshold-based flagging → Performance metrics
- Critical path: Transcription → Embedding → Similarity scoring → Flagging decisions
- Design tradeoffs:
  - Fixed 15-second window balances context coverage vs. computational load
  - General-purpose embedding model sacrifices domain specificity for broader availability
  - Fixed 0.6 threshold simplifies deployment but may not adapt to varying communication styles
- Failure signatures:
  - High false positives in duplicate detection: Embedding model fails to capture nuanced differences
  - Low recall in parasite detection: Threshold too strict or phrasing templates incomplete
  - Temporal drift in similarity scores: Window size inappropriate for fast-paced communication
- First 3 experiments:
  1. Vary the similarity threshold (0.5, 0.6, 0.7) and measure F1-score impact on duplicate detection
  2. Replace the general-purpose embedding model with a League of Legends–specific fine-tuned model and compare performance
  3. Test different conversation window sizes (10s, 15s, 20s) for embedding refinement and measure parasite detection accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would training a sentence embedding model specifically on League