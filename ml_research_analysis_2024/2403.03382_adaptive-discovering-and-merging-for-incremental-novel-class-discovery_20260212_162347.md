---
ver: rpa2
title: Adaptive Discovering and Merging for Incremental Novel Class Discovery
arxiv_id: '2403.03382'
source_url: https://arxiv.org/abs/2403.03382
tags:
- novel
- base
- learning
- branch
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles incremental novel class discovery, which aims
  to discover new classes from unlabeled data while preserving performance on previously
  seen classes. The proposed Adaptive Discovering and Merging (ADM) framework addresses
  two key challenges: (1) discovering new classes from unlabeled data using decoupled
  representation learning and novel class discovery with Triple Comparison and Probability
  Regularization to constrain probability discrepancy and diversity, and (2) preventing
  catastrophic forgetting through Adaptive Model Merging (AMM), which reduces interference
  of novel branches on old classes and merges them without performance loss or parameter
  growth.'
---

# Adaptive Discovering and Merging for Incremental Novel Class Discovery

## Quick Facts
- arXiv ID: 2403.03382
- Source URL: https://arxiv.org/abs/2403.03382
- Reference count: 9
- Key outcome: Proposes ADM framework that significantly outperforms existing class-incremental novel class discovery approaches on CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets

## Executive Summary
This paper addresses the challenge of incremental novel class discovery (class-iNCD), where the goal is to discover new classes from unlabeled data while preserving performance on previously seen classes. The proposed Adaptive Discovering and Merging (ADM) framework tackles two key challenges: discovering new classes through decoupled representation learning with Triple Comparison and Probability Regularization, and preventing catastrophic forgetting via Adaptive Model Merging (AMM). The method achieves state-of-the-art results on multiple benchmark datasets and demonstrates benefits for class-incremental learning by alleviating catastrophic forgetting across multiple baseline methods.

## Method Summary
The ADM framework operates through a two-phase approach. First, it decouples representation learning from classification, using self-supervised contrastive learning to create meaningful features without class-specific supervision. The classifier is then trained separately using pseudo-labels generated from these features. Second, it employs Adaptive Model Merging (AMM) to prevent catastrophic forgetting by maintaining separate base and novel branches, then merging them without performance loss or parameter growth. The method combines Triple Comparison (TC) and Probability Regularization (PR) to constrain probability discrepancy and diversity for adaptive category assignment, enabling effective discovery of novel classes while preserving base class knowledge.

## Key Results
- ADM significantly outperforms existing class-incremental novel class discovery approaches on CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets
- AMM successfully prevents catastrophic forgetting while merging novel branches without performance loss
- The method improves performance across multiple baseline methods in class-incremental learning scenarios
- Achieves state-of-the-art results with consistent improvements in Old, New, and All class accuracies

## Why This Works (Mechanism)

### Mechanism 1
Decoupling representation learning from classification enables better pseudo-label quality for novel classes. By separating feature learning (via contrastive methods) from classifier training, noisy class labels cannot corrupt feature representations early. This separation ensures that the features learned are class-agnostic and can support reliable pseudo-label generation when combined with similarity-based triplet comparison.

### Mechanism 2
Triplet comparison with probability regularization prevents degenerate clustering solutions. The triplet comparison enforces separation between most similar and dissimilar instances within mini-batches, refining pseudo-label quality. Probability regularization applies maximum entropy to the mean probabilities across novel classes, preventing the model from collapsing to trivial solutions where all instances are assigned to the same class.

## Foundational Learning

### Contrastive Learning
Why needed: Creates meaningful feature representations without requiring labeled data, essential for the first phase of decoupled learning
Quick check: Verify that embeddings from different augmentations of the same image are closer than embeddings from different images

### Knowledge Distillation
Why needed: Preserves base class knowledge when learning new classes, preventing catastrophic forgetting
Quick check: Monitor KL divergence between old and new model outputs on base class examples

### Model Merging Techniques
Why needed: Combines separate base and novel class models without losing either's capabilities
Quick check: Compare performance before and after merging to ensure no degradation

## Architecture Onboarding

### Component Map
Representation Learning (contrastive) -> Pseudo-label Generation (triplet comparison) -> Classifier Training -> Adaptive Model Merging (base + novel branches) -> Final Model

### Critical Path
The most critical sequence is: contrastive representation learning → pseudo-label generation with TC/PR → classifier training → AMM merging. Failure at any point in this chain propagates downstream.

### Design Tradeoffs
- Decoupled vs joint learning: Decoupling provides cleaner feature learning but requires additional computation
- Single vs dual-branch architecture: Dual branches prevent forgetting but increase complexity during merging
- TC/PR hyperparameters: Higher regularization improves stability but may slow convergence

### Failure Signatures
- Poor novel class performance: Indicates failed pseudo-label generation or insufficient feature discrimination
- Base class degradation: Suggests inadequate knowledge distillation or aggressive merging
- Training instability: Points to incorrect TC/PR weighting or learning rate issues

### First Experiments
1. Train contrastive representation learning alone and visualize embeddings to verify meaningful clustering
2. Test pseudo-label generation quality by manually inspecting a subset of assigned labels
3. Evaluate catastrophic forgetting by measuring base class accuracy after novel class training

## Open Questions the Paper Calls Out

### Open Question 1
How does ADM's performance compare to other state-of-the-art methods in class-incremental novel class discovery (class-iNCD) tasks on larger datasets or more complex scenarios? The paper only reports results on CIFAR and Tiny-ImageNet datasets without exploring larger or more complex datasets.

### Open Question 2
How does the proposed Adaptive Model Merging (AMM) approach compare to other model merging techniques in terms of preserving base knowledge and integrating novel knowledge without performance loss? The paper mentions AMM outperforms Adaptive Feature Fusion but lacks detailed comparison with other model merging techniques.

### Open Question 3
How do the proposed Triple Comparison (TC) and Probability Regularization (PR) techniques impact the quality of pseudo-label generation and the overall performance of adaptive novel class discovery? The paper mentions these techniques but doesn't provide detailed analysis of their individual impact.

## Limitations

- Architecture and hyperparameter sensitivity creates significant uncertainty about reproducibility
- Generalization across different incremental learning protocols remains unverified
- Scalability and computational requirements for larger datasets are not addressed
- Specific implementation details for TC and PR techniques are not fully specified

## Confidence

**High Confidence:**
- Decoupled representation learning effectively separates feature learning from classification
- Adaptive Model Merging successfully prevents catastrophic forgetting on tested datasets

**Medium Confidence:**
- Triple Comparison and Probability Regularization improve pseudo-label quality
- State-of-the-art performance claims on CIFAR and Tiny-ImageNet datasets

**Low Confidence:**
- Generalizability to other datasets and incremental learning protocols
- Computational efficiency compared to existing methods

## Next Checks

1. **Reproducibility Test**: Implement the method with multiple random seeds and varying hyperparameter settings to assess robustness and sensitivity to implementation choices.

2. **Cross-Protocol Evaluation**: Test the approach on different incremental learning protocols including varying class increment sizes and data distribution shifts to validate generalization claims.

3. **Ablation Study**: Conduct detailed ablation experiments to quantify the individual contributions of Triple Comparison, Probability Regularization, and Adaptive Model Merging components to overall performance.