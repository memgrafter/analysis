---
ver: rpa2
title: Hierarchical Orchestra of Policies
arxiv_id: '2411.03008'
source_url: https://arxiv.org/abs/2411.03008
tags:
- learning
- policy
- policies
- tasks
- environments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Hierarchical Orchestra of Policies (HOP),
  a modularity-based approach to address catastrophic forgetting in continual reinforcement
  learning. HOP dynamically forms a hierarchy of policies based on similarity between
  current observations and previously successful tasks, without requiring task labels.
---

# Hierarchical Orchestra of Policies

## Quick Facts
- arXiv ID: 2411.03008
- Source URL: https://arxiv.org/abs/2411.03008
- Reference count: 32
- Primary result: HOP prevents catastrophic forgetting in continual RL without task labels

## Executive Summary
This paper introduces Hierarchical Orchestra of Policies (HOP), a modularity-based approach to address catastrophic forgetting in continual reinforcement learning. HOP dynamically forms a hierarchy of policies based on similarity between current observations and previously successful tasks, without requiring task labels. The method uses checkpoints to freeze policies, orchestration based on state similarity, and hierarchical weightings to balance policy contributions.

Experiments across Procgen environments show HOP outperforms standard PPO in both recovery rate and final performance after training, while achieving comparable results to Progressive Neural Networks (PNN) that use task labels. HOP demonstrated effective knowledge retention and transfer across different task environments, particularly in scenarios with shared dynamics. The approach's ability to function without task identifiers makes it more versatile for real-world applications where task boundaries are ambiguous.

## Method Summary
HOP addresses catastrophic forgetting in continual reinforcement learning by maintaining a hierarchical orchestra of policies that can be dynamically activated based on task similarity. The method builds upon Proximal Policy Optimization (PPO) as its base algorithm, but introduces three key innovations: (1) checkpointing successful policies when performance exceeds a threshold, (2) orchestrating multiple policies through cosine similarity matching between current states and stored states, and (3) hierarchical weighting that combines contributions from activated policies based on their similarity scores.

The training process involves three phases where agents sequentially learn different tasks without access to task identifiers. When performance exceeds a predefined reward threshold, the current policy is frozen as a checkpoint. During evaluation, HOP computes cosine similarity between current observations and stored states from all checkpoints, activating those above a similarity threshold. The final action is computed as a weighted combination of all activated policies, with weights proportional to their similarity scores. This allows the agent to leverage previously learned knowledge while adapting to new tasks.

## Key Results
- HOP outperforms standard PPO in recovery rate (steps to return to previous performance level) across all tested environment sequences
- HOP achieves comparable final average evaluation return to PNN while not requiring task labels
- Demonstrated effective knowledge retention and transfer across different task environments, particularly in scenarios with shared dynamics

## Why This Works (Mechanism)
HOP prevents catastrophic forgetting by maintaining frozen checkpoints of successful policies and dynamically activating them based on state similarity. When encountering familiar states from previously learned tasks, the agent can immediately access the appropriate policy rather than having to relearn it. The hierarchical weighting mechanism ensures smooth transitions between policies, allowing the agent to leverage multiple relevant policies simultaneously when states share characteristics across tasks.

The similarity-based orchestration is particularly effective because it captures the underlying structure of the state space rather than relying on explicit task boundaries. This makes HOP adaptable to environments where task distinctions are ambiguous or where tasks share common elements. The checkpointing mechanism ensures that once a policy has learned a task effectively, that knowledge is preserved and can be reactivated when needed.

## Foundational Learning

**PPO (Proximal Policy Optimization)**: An on-policy RL algorithm that optimizes policies while maintaining stability through clipped objective functions. Why needed: Serves as the base algorithm that HOP builds upon. Quick check: Verify PPO implementation produces stable learning curves before adding HOP components.

**Catastrophic Forgetting**: The phenomenon where neural networks lose previously learned knowledge when trained on new tasks. Why needed: The core problem HOP aims to solve. Quick check: Observe performance degradation when switching between tasks in baseline PPO.

**Cosine Similarity**: A metric measuring the cosine of the angle between two vectors, indicating their directional similarity. Why needed: Used to determine which stored policies are relevant for current states. Quick check: Confirm similarity scores between identical and different states produce expected values (close to 1 and 0 respectively).

**Policy Checkpointing**: The process of saving and freezing neural network weights at specific performance thresholds. Why needed: Preserves learned policies to prevent overwriting during subsequent training. Quick check: Verify frozen policies maintain their performance when evaluated independently.

**Hierarchical Weighting**: A mechanism for combining multiple policy outputs based on their relative importance. Why needed: Allows smooth integration of multiple activated policies. Quick check: Test that weights sum to 1 and that individual policy contributions affect final actions appropriately.

## Architecture Onboarding

**Component Map**: PPO -> Checkpoint Manager -> Similarity Comparator -> Hierarchical Weighting -> Final Action Selection

**Critical Path**: During training: PPO optimization -> Performance evaluation -> Checkpoint creation. During evaluation: State observation -> Similarity computation -> Policy activation -> Weighted combination -> Action output.

**Design Tradeoffs**: The method trades increased memory usage (storing multiple policies) for improved knowledge retention. The similarity threshold must be carefully tuned - too low causes excessive policy activation, too high may miss relevant policies. The reward threshold determines when to create checkpoints, balancing between premature freezing and missing optimal checkpoints.

**Failure Signatures**: Poor recovery rates may indicate similarity threshold too high or reward threshold too low. If policies conflict rather than complement, hierarchical weighting may need adjustment. Memory issues could arise from excessive checkpoint creation in environments with many distinct states.

**First Experiments**:
1. Verify baseline PPO performance on single task before implementing HOP
2. Test checkpoint creation mechanism by monitoring reward thresholds and checkpoint frequency
3. Validate similarity-based policy activation by testing with known similar/dissimilar states

## Open Questions the Paper Calls Out

**Open Question 1**: How does the similarity threshold (w) and reward threshold (P) affect HOP's performance across diverse task distributions? The paper acknowledges the importance of these thresholds but does not provide a systematic study of how varying these parameters affects performance across different task distributions or environments.

**Open Question 2**: Can HOP be effectively adapted to continuous environments with fluid task transitions? The current implementation and experiments focus on discrete task switches, leaving open the question of how well HOP would perform in truly continuous, non-episodic settings.

**Open Question 3**: Would a learnable parameter to dynamically adjust the influence of previous checkpoints improve immediate adaptation to task distribution changes? This suggestion is speculative and has not been implemented or tested, leaving the potential benefits and implementation challenges unexplored.

## Limitations

- Evaluation confined to Procgen environments with simple visual inputs and discrete action spaces
- Similarity-based policy selection may face challenges in environments with subtle task distinctions
- Fixed similarity threshold (0.98) and reward threshold (7.5) may not generalize across diverse task distributions

## Confidence

- **High Confidence**: The core architectural design and its ability to prevent catastrophic forgetting through policy checkpointing and orchestration
- **Medium Confidence**: The claim of matching PNN performance without task labels is credible given the evaluation metrics
- **Low Confidence**: The assertion that HOP is "more versatile for real-world applications" lacks sufficient empirical support

## Next Checks

1. Test the method's performance in continuous control environments (e.g., Mujoco) to evaluate its scalability beyond discrete action spaces
2. Conduct ablation studies varying the similarity threshold and reward threshold to understand their impact on different task types
3. Implement the method in a multi-task scenario with ambiguous task boundaries to validate the claim of label-free operation in real-world conditions