---
ver: rpa2
title: 'From MLP to NeoMLP: Leveraging Self-Attention for Neural Fields'
arxiv_id: '2412.08731'
source_url: https://arxiv.org/abs/2412.08731
tags:
- neural
- neomlp
- output
- hidden
- fitting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes NeoMLP, a new architecture that transforms
  an MLP into a complete graph with high-dimensional features, performing message
  passing via self-attention. NeoMLP leverages the hidden and output nodes as latent
  codes, enabling it to function as a conditional neural field for encoding and fitting
  signals.
---

# From MLP to NeoMLP: Leveraging Self-Attention for Neural Fields

## Quick Facts
- **arXiv ID**: 2412.08731
- **Source URL**: https://arxiv.org/abs/2412.08731
- **Reference count**: 26
- **Primary result**: NeoMLP outperforms existing MLP-based approaches in reconstruction quality on high-resolution signals and achieves state-of-the-art performance in downstream classification tasks on MNIST, CIFAR10, and ShapeNet10.

## Executive Summary
This paper introduces NeoMLP, a novel architecture that transforms traditional MLPs into complete graphs with high-dimensional features, leveraging self-attention for message passing. By reimagining MLPs as graphs and enabling direct communication between all nodes (input, hidden, and output), NeoMLP addresses the limitations of sequential layer-wise processing in standard MLPs. The method incorporates a built-in conditioning mechanism through learnable hidden and output nodes, allowing it to function as a conditional neural field for both encoding signals and fitting neural representations. NeoMLP demonstrates superior performance on high-resolution signals including audio, video, and multimodal data, while also achieving state-of-the-art results on downstream classification tasks.

## Method Summary
NeoMLP transforms an MLP into a complete graph structure where all nodes (inputs, hidden, and outputs) can communicate directly through self-attention. The method uses high-dimensional features for each node and employs weight-sharing via self-attention instead of edge-specific weights, reducing computational complexity. Hidden and output nodes serve as learnable latent codes, enabling NeoMLP to function as a conditional neural field. The architecture processes scalar inputs by projecting them to higher dimensions using Random Fourier Features, adds positional embeddings, and concatenates all tokens before passing through multiple layers of self-attention and feed-forward networks. Training follows a two-stage process: fitting (optimizing backbone and latents) and finetuning (optimizing latents only), with the approach evaluated on both high-resolution signals and neural representation datasets for downstream classification tasks.

## Key Results
- Outperforms existing MLP-based approaches in reconstruction quality on high-resolution audio, video, and multimodal data
- Achieves state-of-the-art performance in downstream classification tasks on MNIST, CIFAR10, and ShapeNet10
- Ablation studies demonstrate the importance of hyperparameters and the role of random Fourier features in performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NeoMLP's complete graph transformation enables richer information propagation compared to standard MLPs.
- Mechanism: By converting the multi-partite graph structure of an MLP into a complete graph with self-edges, NeoMLP allows all nodes to communicate directly with each other through self-attention, eliminating sequential layer-wise processing.
- Core assumption: Additional connectivity through complete graph structure leads to better signal encoding and reconstruction quality.
- Evidence anchors:
  - [abstract]: "We start from an MLP, viewed as a graph, and transform it from a multi-partite graph to a complete graph of input, hidden, and output nodes, equipped with high-dimensional features."
  - [section]: "Going from MLP to NeoMLP, we use a fully connected graph and high-dimensional node features. In NeoMLP, the traditional notion of layers of neurons, as well as the asynchronous layer-wise propagation, cease to exist."
- Break condition: If additional connectivity introduces excessive computational overhead without corresponding gains, or leads to overfitting on smaller datasets.

### Mechanism 2
- Claim: Weight-sharing via self-attention provides a scalable alternative to edge-specific weights.
- Mechanism: Instead of using dedicated edge-specific weights for all node pairs, NeoMLP employs weight-sharing through self-attention among all nodes, allowing the model to scale while maintaining expressive power.
- Core assumption: Self-attention can effectively capture relationships between nodes without requiring separate parameters for each connection.
- Evidence anchors:
  - [abstract]: "We perform message passing on this graph and employ weight-sharing via self-attention among all the nodes."
  - [section]: "Since the forward pass now includes message passing from all nodes to all nodes at each step, we create learnable parameters for the initial values of the hidden and output node features... Next, we observe that having dedicated edge-specific weights for all node pairs would result in an intractable spatial complexity."
- Break condition: If self-attention becomes a bottleneck for very large graphs, or weight-sharing limits ability to learn fine-grained distinctions between node relationships.

### Mechanism 3
- Claim: Built-in conditioning mechanism through hidden and output nodes enables NeoMLP to function as conditional neural field.
- Mechanism: Hidden and output nodes can be initialized with noise and optimized through backpropagation, functioning as latent codes that encode instance-specific information while backbone carries dataset-wide information.
- Core assumption: Latent codes embedded in hidden and output nodes can effectively capture unique characteristics of individual signals while maintaining compatibility with shared backbone.
- Evidence anchors:
  - [abstract]: "NeoMLP has a built-in mechanism for conditioning through the hidden and output nodes, which function as a set of latent codes, and as such, NeoMLP can be used straightforwardly as a conditional neural field."
  - [section]: "The hidden and output nodes can be used as a learnable set of latent codes, and thus, our method can function as a conditional neural field."
- Break condition: If latent codes become too entangled with backbone parameters, making it difficult to separate instance-specific and dataset-wide information, or if optimization becomes unstable during training.

## Foundational Learning

- Concept: Graph neural networks and message passing
  - Why needed here: NeoMLP fundamentally relies on viewing MLPs as graphs and performing message passing on this graph structure.
  - Quick check question: What is the key difference between traditional MLPs and graph neural networks in terms of how information propagates through the network?

- Concept: Self-attention mechanisms
  - Why needed here: NeoMLP uses self-attention for weight-sharing among nodes.
  - Quick check question: How does self-attention differ from traditional attention mechanisms, and why is permutation invariance important in the context of NeoMLP?

- Concept: Random Fourier Features (RFF)
  - Why needed here: NeoMLP employs RFF to project scalar inputs to higher dimensions, addressing spectral bias issues.
  - Quick check question: What is spectral bias in neural networks, and how do Random Fourier Features help mitigate this issue?

## Architecture Onboarding

- Component map:
  Input nodes -> RFF layers -> Linear layers -> Positional embeddings -> Concatenated tokens (inputs, hidden, output) -> L layers of self-attention and FFN -> Output tokens -> Final linear layer

- Critical path:
  1. Transform inputs using RFF and linear layers
  2. Add positional embeddings to inputs
  3. Initialize hidden and output embeddings
  4. Concatenate all tokens (inputs, hidden, output)
  5. Pass through L layers of self-attention and FFN
  6. Extract output tokens and pass through final linear layer

- Design tradeoffs:
  - Complete graph vs. traditional MLP layers: More expressive but computationally more expensive
  - Self-attention vs. edge-specific weights: More scalable but potentially less precise
  - Learnable hidden/output nodes vs. fixed architecture: More flexible for conditioning but introduces additional optimization challenges

- Failure signatures:
  - Poor reconstruction quality: May indicate issues with RFF parameters, token dimensionality, or insufficient training
  - Instability during training: Could be due to improper initialization of hidden/output embeddings or learning rate issues
  - Overfitting on small datasets: Might suggest the need for regularization or reducing model complexity

- First 3 experiments:
  1. Compare reconstruction quality of NeoMLP vs. standard MLP on a simple 1D signal (e.g., sine wave)
  2. Test the impact of different token dimensionalities on reconstruction quality and training stability
  3. Evaluate the effectiveness of the conditioning mechanism by training NeoMLP on a dataset with known categories and testing classification performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do NeoMLP's latent codes compare to set-latent conditional neural fields that use cross-attention in terms of scalability and expressivity?
- Basis in paper: [explicit] The paper states that existing set-latent neural fields are based on cross-attention, which limits their scalability and expressivity, and that NeoMLP employs self-attention instead.
- Why unresolved: While the paper claims self-attention is better, it doesn't provide direct empirical comparisons against set-latent methods using cross-attention on the same tasks and datasets.
- What evidence would resolve it: A controlled experiment comparing NeoMLP against set-latent methods using cross-attention on identical datasets, measuring both reconstruction quality and downstream task performance.

### Open Question 2
- Question: What is the impact of using different positional embedding strategies (learned vs. fixed) on NeoMLP's performance and training stability?
- Basis in paper: [inferred] The paper uses learned positional embeddings for inputs, but doesn't explore alternative strategies or analyze their impact on performance.
- Why unresolved: The paper doesn't investigate how different positional embedding approaches affect NeoMLP's ability to learn spatial relationships and overall performance.
- What evidence would resolve it: Experiments comparing NeoMLP with learned, fixed sinusoidal, and no positional embeddings on the same tasks, measuring reconstruction quality and training convergence.

### Open Question 3
- Question: How does NeoMLP's performance scale with increasing input dimensionality and number of output dimensions?
- Basis in paper: [inferred] The paper demonstrates NeoMLP on audio, video, and multimodal data, but doesn't systematically analyze how performance changes with increasing input/output dimensionality.
- Why unresolved: While the paper shows NeoMLP works on various modalities, it doesn't provide a systematic analysis of performance scaling with dimensionality.
- What evidence would resolve it: Experiments testing NeoMLP on progressively higher-dimensional data (e.g., higher resolution images, more audio channels) while measuring reconstruction quality and computational requirements.

### Open Question 4
- Question: What is the optimal balance between the number of hidden nodes and their dimensionality for maximizing downstream task performance?
- Basis in paper: [explicit] The ablation studies show that increasing the number of latents and their dimensionality increases reconstruction quality, but the higher number of latents seems to lead to decreased downstream performance.
- Why unresolved: The ablation studies provide some insights but don't systematically explore the trade-off between number of nodes and their dimensionality.
- What evidence would resolve it: A comprehensive grid search varying both the number of hidden nodes and their dimensionality, measuring both reconstruction quality and downstream task performance to find optimal combinations.

## Limitations

- Claims about superiority are based on comparisons with specific baselines that may not represent the full spectrum of state-of-the-art methods
- Evaluation focuses primarily on reconstruction quality and downstream classification accuracy without exploring other potential applications or failure modes
- Two-stage training procedure (fitting followed by finetuning) introduces complexity that isn't fully addressed

## Confidence

- **High confidence**: Core architectural claims about transforming MLPs into complete graphs with self-attention
- **Medium confidence**: Effectiveness of the built-in conditioning mechanism for downstream tasks
- **Low confidence**: Generalizability of results across diverse domains beyond tested high-resolution signals and neural representation datasets

## Next Checks

1. Benchmark NeoMLP against a broader range of state-of-the-art neural field architectures on multiple datasets to verify the claimed performance advantages
2. Conduct ablation studies specifically isolating the contributions of the complete graph transformation, self-attention mechanism, and latent code conditioning to quantify their individual impacts
3. Evaluate NeoMLP's computational efficiency and scalability on progressively larger graphs and higher-dimensional data to assess practical deployment limitations