---
ver: rpa2
title: Bridging The Gap between Low-rank and Orthogonal Adaptation via Householder
  Reflection Adaptation
arxiv_id: '2405.17484'
source_url: https://arxiv.org/abs/2405.17484
tags:
- answer
- latexit
- should
- adaptation
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper bridges the gap between low-rank and orthogonal adaptation
  methods by proposing Householder Reflection Adaptation (HRA). HRA fine-tunes pre-trained
  models by multiplying frozen weight matrices with orthogonal matrices constructed
  from learnable Householder reflections, which can be interpreted as both an adaptive
  low-rank adaptation and an orthogonal fine-tuning method.
---

# Bridging The Gap between Low-rank and Orthogonal Adaptation via Householder Reflection Adaptation

## Quick Facts
- arXiv ID: 2405.17484
- Source URL: https://arxiv.org/abs/2405.17484
- Authors: Shen Yuan; Haotian Liu; Hongteng Xu
- Reference count: 40
- Key outcome: HRA achieves 90.10% average score on GLUE using only 0.66M trainable parameters, outperforming LoRA (88.50% with 1.33M parameters) and OFT (89.77% with 0.79M parameters)

## Executive Summary
This paper introduces Householder Reflection Adaptation (HRA), a novel fine-tuning method that bridges low-rank and orthogonal adaptation techniques for pre-trained models. HRA constructs orthogonal matrices from learnable Householder reflections and multiplies them with frozen weight matrices, enabling efficient adaptation with fewer trainable parameters. The method can be interpreted as both an orthogonal fine-tuning approach and an adaptive low-rank adaptation, providing flexibility in balancing model capacity and regularity through orthogonality regularization.

## Method Summary
HRA fine-tunes pre-trained models by multiplying frozen weight matrices with orthogonal matrices constructed from learnable Householder reflections. The method allows for both orthogonal fine-tuning (when reflection planes are orthogonal) and adaptive low-rank adaptation, with the orthogonality of reflection planes impacting model capacity and regularity. During training, the authors apply orthogonality regularization controlled by parameter λ to balance adaptation strength and model stability.

## Key Results
- Achieves 90.10% average score on GLUE benchmark using only 0.66M trainable parameters
- Outperforms LoRA (88.50% with 1.33M parameters) and OFT (89.77% with 0.79M parameters) on natural language understanding tasks
- Demonstrates superior performance on mathematical reasoning (GSM8K, MetaMathQA) and conditional image generation tasks
- Shows computational efficiency when r ≪ b² + b, making it more efficient than OFT/BOFT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HRA can be interpreted as both an orthogonal fine-tuning method and an adaptive low-rank adaptation.
- Mechanism: By constructing orthogonal matrices from learnable Householder reflections, HRA multiplies frozen weight matrices with these orthogonal matrices, effectively preserving pairwise angles between neuron vectors while enabling low-rank modifications.
- Core assumption: The product of Householder reflections forms an orthogonal matrix that can approximate any orthogonal matrix in Od×d.
- Evidence anchors:
  - [abstract] "This HR-based orthogonal fine-tuning is equivalent to an adaptive low-rank adaptation."
  - [section 3.1] "This HR-based orthogonal fine-tuning is equivalent to an adaptive low-rank adaptation."
  - [corpus] Weak - no direct citations found for this specific equivalence claim

### Mechanism 2
- Claim: The orthogonality of reflection planes impacts model capacity and regularity.
- Mechanism: When reflection planes are orthogonal, the distance between the transformed weight matrix and the original is maximized, leading to stronger regularization. Non-orthogonal reflection planes reduce this distance, allowing more capacity.
- Evidence anchors:
  - [section 3.4] "when adapting the pre-trained model, enhancing the orthogonality of Ur imposes stronger regularity on the adapter"
  - [figure 2] "indicating that when the reflection planes H1 and H2 are orthogonal, the distance ∥H2H1w − w∥2 is maximized"
  - [corpus] Weak - no direct citations found for this specific orthogonality-regularity relationship

### Mechanism 3
- Claim: HRA achieves superior performance with fewer trainable parameters compared to state-of-the-art methods.
- Mechanism: By leveraging the efficient computation of Householder reflections and their ability to approximate dense orthogonal matrices with fewer parameters, HRA provides effective adaptation while reducing computational costs.
- Evidence anchors:
  - [abstract] "HRA achieves superior performance with fewer learnable parameters when adapting large language models and conditional image generators."
  - [section 3.2] "HRA can be more efficient than OFT (BOFT)" when setting r ≪ b² + b
  - [table 1] shows HRA has comparable or fewer parameters than OFT and BOFT

## Foundational Learning

- Concept: Householder reflections and their properties
  - Why needed here: HRA is built upon Householder reflections as the fundamental building block for constructing orthogonal matrices
  - Quick check question: What is the mathematical form of a Householder reflection matrix and what property does it preserve?

- Concept: Low-rank matrix decomposition
  - Why needed here: HRA can be interpreted as an adaptive low-rank adaptation method, requiring understanding of how low-rank modifications work
  - Quick check question: How does the decomposition W + AB (where A and B are low-rank) differ from multiplying W by an orthogonal matrix?

- Concept: Orthogonal matrices and their role in neural network adaptation
  - Why needed here: The paper bridges the gap between low-rank and orthogonal adaptation techniques, requiring understanding of both approaches
  - Quick check question: What theoretical guarantee does orthogonal fine-tuning provide that low-rank adaptation may lack?

## Architecture Onboarding

- Component map:
  Pre-trained model weights -> Householder reflection chain -> Orthogonal matrix construction -> Weight matrix multiplication -> Regularization module -> Adapted model

- Critical path:
  1. Initialize Householder reflection parameters
  2. Construct orthogonal matrix from reflections
  3. Multiply frozen weight matrices by orthogonal matrix
  4. Apply orthogonality regularization during training
  5. Fine-tune with small number of parameters

- Design tradeoffs:
  - Number of Householder reflections (r) vs. model capacity: Higher r increases capacity but also parameter count
  - Orthogonality regularization strength (λ) vs. performance: Stronger regularization increases stability but may limit adaptation ability
  - Computational efficiency vs. approximation accuracy: Fewer reflections save computation but may reduce transformation quality

- Failure signatures:
  - Performance degrades significantly when λ is too large (strict orthogonality)
  - Training instability when r is too small for complex adaptation tasks
  - Computational overhead similar to dense methods when r approaches dimension size

- First 3 experiments:
  1. GLUE benchmark adaptation with DeBERTaV3-base: Test natural language understanding with varying λ values
  2. Mathematical reasoning with LLaMA2-7B: Evaluate performance on GSM8K and MATH datasets with different r values
  3. Conditional image generation with Stable Diffusion: Assess subject-driven and controllable generation tasks with visual quality metrics

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Limited theoretical analysis of approximation quality for Householder reflections across diverse model architectures
- Lack of comprehensive ablation studies mapping the full capacity-regularity tradeoff space
- Empirical validation of orthogonality-regularity relationships is task-specific rather than theoretically grounded

## Confidence
- High: Experimental results showing HRA's superior parameter efficiency compared to LoRA and OFT on GLUE benchmark
- Medium: Theoretical claims about HRA's dual interpretation as both orthogonal and low-rank adaptation
- Low: Claims about orthogonality-regularity relationships without comprehensive ablation studies

## Next Checks
1. Conduct systematic ablation studies varying both the number of Householder reflections (r) and orthogonality regularization strength (λ) across multiple model architectures to map the full capacity-regularity tradeoff space.

2. Implement stress tests on HRA's approximation quality by measuring reconstruction error between target orthogonal matrices and those constructed from Householder reflections for various model weights.

3. Design experiments comparing HRA's performance when applied to different weight matrix blocks (attention vs. MLP) to determine whether the adaptation mechanism works uniformly across architectural components.