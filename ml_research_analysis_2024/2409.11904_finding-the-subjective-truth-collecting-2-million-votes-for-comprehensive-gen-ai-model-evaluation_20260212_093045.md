---
ver: rpa2
title: 'Finding the Subjective Truth: Collecting 2 Million Votes for Comprehensive
  Gen-AI Model Evaluation'
arxiv_id: '2409.11904'
source_url: https://arxiv.org/abs/2409.11904
tags:
- image
- text-to-image
- dall-e
- images
- flux
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work presents a novel, large-scale annotation framework for
  evaluating text-to-image generative models using over 2 million human votes collected
  across 4,512 images. The framework compares four leading models (DALL-E 3, Flux.1,
  MidJourney, and Stable Diffusion) on three criteria: style preference, coherence,
  and text-to-image alignment.'
---

# Finding the Subjective Truth: Collecting 2 Million Votes for Comprehensive Gen-AI Model Evaluation
## Quick Facts
- **arXiv ID**: 2409.11904
- **Source URL**: https://arxiv.org/abs/2409.11904
- **Reference count**: 19
- **Primary result**: Novel framework collected over 2 million human votes across 4,512 images to evaluate four leading text-to-image models, revealing Flux.1 as the top performer in style preference, coherence, and text-to-image alignment

## Executive Summary
This study presents a novel, large-scale annotation framework for evaluating text-to-image generative models using over 2 million human votes collected across 4,512 images. The framework compares four leading models (DALL-E 3, Flux.1, MidJourney, and Stable Diffusion) on three criteria: style preference, coherence, and text-to-image alignment. Using Rapidata's crowdsourcing technology, the study gathered feedback from 144,292 participants across 145 countries, achieving unprecedented scale and diversity. The results show Flux.1 significantly outperforming other models across all criteria, with scores of 29.86 (preference), 29.61 (coherence), and 27.36 (text-to-image alignment), demonstrating the framework's effectiveness in providing comprehensive, representative model rankings while minimizing bias through global demographic diversity.

## Method Summary
The study employed Rapidata's crowdsourcing platform to collect human evaluations of four leading text-to-image models. Participants from 145 countries voted on 4,512 images across three criteria: style preference, coherence, and text-to-image alignment. The framework used pairwise comparisons where voters selected preferred images between model pairs, with each image receiving multiple votes to ensure statistical robustness. The large-scale approach aimed to capture diverse subjective preferences while minimizing individual biases through extensive participant sampling and geographic diversity.

## Key Results
- Flux.1 achieved the highest scores across all evaluation criteria: 29.86 (style preference), 29.61 (coherence), and 27.36 (text-to-image alignment)
- The study collected 2,054,662 human votes from 144,292 participants across 145 countries
- Style preference was the most discriminating criterion, while text-to-image alignment showed the least variation between models

## Why This Works (Mechanism)
The framework leverages massive-scale human evaluation to capture subjective preferences that automated metrics cannot measure. By using pairwise comparisons across a diverse global participant pool, the system reduces individual bias while capturing nuanced aesthetic and functional preferences. The large sample size provides statistical power to detect meaningful differences between models, while the multi-criteria approach ensures comprehensive evaluation beyond simple preference rankings.

## Foundational Learning
- **Crowdsourcing methodology** - why needed: Enables collection of diverse human preferences at scale; quick check: Verify participant diversity metrics and geographic distribution
- **Pairwise comparison design** - why needed: Reduces cognitive load and provides clearer preference signals than absolute ratings; quick check: Confirm balanced presentation of model pairs
- **Multi-criteria evaluation** - why needed: Captures different aspects of model quality beyond simple preference; quick check: Validate criterion definitions and inter-rater reliability
- **Statistical sampling** - why needed: Ensures results are representative and not driven by outliers; quick check: Review sample size calculations and confidence intervals
- **Global participant recruitment** - why needed: Minimizes cultural and demographic biases in subjective evaluations; quick check: Analyze participant demographics against global population distributions
- **Image diversity generation** - why needed: Ensures evaluation covers model capabilities across different prompts and styles; quick check: Review prompt selection methodology and image variety

## Architecture Onboarding
**Component map**: Rapidata platform -> Participant recruitment -> Image presentation interface -> Pairwise voting system -> Data aggregation -> Statistical analysis

**Critical path**: Image generation (4 models) -> Prompt selection -> Interface presentation -> Participant voting -> Data collection -> Statistical analysis -> Model ranking

**Design tradeoffs**: Large-scale global recruitment vs. quality control, pairwise comparisons vs. absolute ratings, subjective preferences vs. objective metrics, computational cost vs. evaluation comprehensiveness

**Failure signatures**: Inconsistent voting patterns indicating fatigue or confusion, demographic skew suggesting bias, low inter-rater reliability suggesting unclear criteria, technical issues preventing proper image display

**First experiments**: 1) Pilot test with 100 participants to validate interface usability, 2) A/B test different voting interface designs to optimize response rates, 3) Cross-validation with expert reviewers to assess alignment between crowd and expert preferences

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Exclusive focus on text-to-image generation limits generalizability to other generative AI domains
- Reliance on subjective human preferences introduces inherent variability that may not fully capture technical model capabilities
- Unclear demographic composition and potential sampling biases that could affect representativeness of results

## Confidence
- **High confidence**: The collection of 2 million human votes across four models represents a genuine technical achievement in scale and diversity of data collection
- **Medium confidence**: The relative ranking of Flux.1 outperforming other models across all three criteria, as this is based on subjective human preferences that may vary with different participant pools
- **Medium confidence**: The framework's effectiveness in minimizing bias through global demographic diversity, though the actual demographic distribution and its impact on results are not detailed

## Next Checks
1. Conduct demographic analysis to verify the representativeness of the 145-country participant pool and assess potential regional biases in model preferences
2. Perform statistical power analysis to determine the practical significance of the reported score differences between models
3. Implement a within-subjects design where the same participants evaluate all four models to control for individual preference variations and assess consistency across evaluations