---
ver: rpa2
title: Quality with Just Enough Diversity in Evolutionary Policy Search
arxiv_id: '2405.04308'
source_url: https://arxiv.org/abs/2405.04308
tags:
- fitness
- behavior
- maze
- jedi
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Quality-Diversity (QD) methods explore the behavior space but spend
  much of the evaluation budget on non-optimal solutions, while Evolution Strategies
  (ES) focus on the highest fitness but lack exploration. The authors introduce Quality
  with Just Enough Diversity (JEDi), which uses a Gaussian Process to model the relationship
  between behavior and fitness, then selects promising target behaviors and uses ES
  to find high-performing solutions near them.
---

# Quality with Just Enough Diversity in Evolutionary Policy Search

## Quick Facts
- arXiv ID: 2405.04308
- Source URL: https://arxiv.org/abs/2405.04308
- Reference count: 40
- Primary result: JEDi outperforms both QD and ES methods on hard exploration tasks by using Gaussian Process to focus evaluations on promising behaviors

## Executive Summary
JEDi addresses the exploration-exploitation tradeoff in evolutionary policy search by learning a behavior-fitness mapping with a Gaussian Process. It selects target behaviors using a Pareto front of predicted mean and variance, then runs Evolution Strategies to optimize fitness near each target. This approach achieves higher fitness values while maintaining better behavior coverage compared to pure ES or QD methods, particularly on challenging maze navigation and complex robot control tasks.

## Method Summary
JEDi combines a weighted Gaussian Process surrogate model with Evolution Strategies to balance exploration and exploitation. The GP learns the relationship between behavior descriptors and fitness, accounting for exploration effort by weighting each behavior cell by the inverse of its evaluation count. Target behaviors are selected from a Pareto front of GP mean/variance predictions, and ES (Sep-CMA-ES/LM-MA-ES) optimizes policies near each target using a weighted fitness-target distance scoring function. The method maintains an archive of evaluated solutions and updates it throughout the search process.

## Key Results
- JEDi outperforms both pure ES and MAP-Elites on maze navigation and robot control tasks
- Achieves higher maximum fitness values while maintaining better behavior coverage
- Ablation study shows JEDi is resilient to α parameter choice, with decaying schedules performing well across all tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: JEDi outperforms both QD and ES by selectively focusing evaluations on promising behaviors while maintaining enough diversity.
- Mechanism: JEDi uses a Gaussian Process to model the behavior-fitness relationship, then selects target behaviors using a Pareto front of mean and variance predictions. ES are run to optimize fitness near each target, allowing focused search on high-potential areas while avoiding wasteful exploration of poor regions.
- Core assumption: The behavior-fitness mapping is smooth enough that a GP can predict promising regions, and that ES can efficiently optimize near target behaviors.
- Evidence anchors:
  - [abstract] "learns the relationship between behavior and fitness to focus evaluations on solutions that matter"
  - [section] "JEDi learns the behavior-fitness mapping with a Gaussian Process to select target behaviors"
  - [corpus] Weak: related papers focus on diversity in QD but don't address the targeted ES combination described here.
- Break condition: If the behavior-fitness relationship is too noisy or discontinuous, GP predictions become unreliable and ES may fail to find high fitness near targets.

### Mechanism 2
- Claim: Weighted Gaussian Process accounts for exploration effort, preventing premature convergence to already well-explored regions.
- Mechanism: Each behavior cell is weighted by the inverse of its evaluation count in the GP. This makes the uncertainty landscape reflect actual knowledge gaps rather than just presence/absence, guiding JEDi to explore under-sampled but potentially valuable areas.
- Core assumption: The number of evaluations per behavior cell is a good proxy for exploration completeness.
- Evidence anchors:
  - [section] "we use a weighted GP [...] which counts each point as if it appeared multiple times in the dataset"
  - [corpus] Weak: no direct mention of weighted GPs in neighbor papers; this appears to be a novel contribution.
- Break condition: If evaluation counts don't correlate with actual coverage (e.g., due to deceptive regions), weighted GP may misdirect exploration.

### Mechanism 3
- Claim: Weighted Target Fitness Score balances exploration of novel behaviors with exploitation of known high-fitness regions.
- Mechanism: The scoring function combines normalized fitness and target distance, weighted by parameter α. This allows JEDi to adjust the exploration/exploitation tradeoff dynamically, with decaying α schedules improving performance on hard exploration tasks.
- Core assumption: A linear combination of fitness and behavioral distance provides a useful surrogate for the constrained optimization problem.
- Evidence anchors:
  - [section] "We define a scoring function S which gives a utility score to each individual based on its fitness and behavior"
  - [corpus] Weak: neighbor papers discuss novelty and diversity but don't describe this specific weighted combination for ES.
- Break condition: If α is poorly tuned or the environment is highly deceptive, ES may get stuck in local optima despite the weighted score.

## Foundational Learning

- Concept: Gaussian Processes for surrogate modeling
  - Why needed here: JEDi uses GP to predict fitness potential of behaviors without evaluating every candidate, enabling sample-efficient target selection.
  - Quick check question: What kernel function would you choose if behavior descriptors are high-dimensional and correlated?

- Concept: Evolution Strategies and their limitations
  - Why needed here: Understanding ES helps see why JEDi adds behavior information—ES alone can get stuck in deceptive fitness landscapes and lack exploration.
  - Quick check question: How does covariance matrix adaptation in CMA-ES improve convergence compared to simple ES?

- Concept: Quality-Diversity algorithms and their exploration-exploitation balance
  - Why needed here: JEDi builds on QD concepts but focuses on single best solution rather than archive diversity, so understanding QD helps contrast the approaches.
  - Quick check question: Why does MAP-Elites spend evaluations on low-fitness behaviors, and how does JEDi avoid this?

## Architecture Onboarding

- Component map:
  - Gaussian Process model (behavior → fitness mean/variance)
  - Target selection module (Pareto front sampling)
  - ES optimizer (weighted target fitness score)
  - Archive/repertoire (behavior descriptor storage)
  - Evaluation loop (policy rollout and update)

- Critical path:
  1. Initialize archive with random policies
  2. Train GP on current archive
  3. Sample target behaviors from Pareto front
  4. Run ES for each target to optimize fitness
  5. Update archive with all evaluated solutions
  6. Repeat until budget exhausted

- Design tradeoffs:
  - GP complexity vs. sample efficiency: richer kernels improve predictions but increase training cost.
  - α parameter vs. adaptability: fixed α is simple but decaying α can better handle exploration-exploitation phases.
  - Parallel ES vs. sequential: parallel speeds up but may waste evaluations if targets are redundant.

- Failure signatures:
  - GP predictions collapse to mean if archive is too sparse
  - ES gets stuck when targets are too far from any known good behavior
  - Archive coverage plateaus early if target selection is too greedy
  - Memory overflow if ES population size is too large for genome dimension

- First 3 experiments:
  1. Run JEDi on a simple maze with α=0.5 and fixed GP; verify it finds the target faster than MAP-Elites.
  2. Test weighted vs. unweighted GP on a deceptive landscape; measure coverage and max fitness.
  3. Sweep α from 0 to 1 on HalfCheetah; plot final fitness vs. α to find optimal balance.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas remain unexplored based on the implementation details provided.

## Limitations
- Reliance on Gaussian Process surrogate modeling may struggle with high-dimensional or highly discontinuous behavior spaces
- Weighted GP approach for accounting exploration effort lacks extensive ablation studies
- Performance sensitivity to behavior descriptor choice not thoroughly evaluated
- Limited validation on noisy or deceptive fitness landscapes

## Confidence
- High confidence: JEDi outperforms ES on hard exploration tasks when properly tuned
- Medium confidence: Weighted GP improves exploration efficiency over unweighted alternatives
- Low confidence: Decaying α schedule is universally optimal across all task types

## Next Checks
1. Test JEDi's performance on deceptive fitness landscapes where ES typically fails, measuring both convergence speed and final fitness
2. Compare weighted GP against simpler exploration heuristics (e.g., random sampling, novelty search) to quantify the benefit of the GP approach
3. Evaluate JEDi with different behavior descriptor choices on the same tasks to assess sensitivity to representation choices