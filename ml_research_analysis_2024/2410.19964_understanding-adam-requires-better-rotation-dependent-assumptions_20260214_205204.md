---
ver: rpa2
title: Understanding Adam Requires Better Rotation Dependent Assumptions
arxiv_id: '2410.19964'
source_url: https://arxiv.org/abs/2410.19964
tags:
- adam
- rotation
- rotations
- layer
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates Adam's sensitivity to rotations of the
  parameter space and finds that Adam's performance in training transformers degrades
  under random rotations, indicating a crucial dependence on the choice of basis.
  The authors identify structured rotations that preserve or enhance Adam's empirical
  performance and examine rotation-dependent assumptions in the literature, finding
  that they fall short in explaining Adam's behavior across various rotation types.
---

# Understanding Adam Requires Better Rotation Dependent Assumptions

## Quick Facts
- arXiv ID: 2410.19964
- Source URL: https://arxiv.org/abs/2410.19964
- Reference count: 40
- Primary result: Adam's performance degrades under global rotations across all settings, with degradation worsening as rotation scope broadens

## Executive Summary
This paper investigates Adam's sensitivity to rotations in the parameter space, revealing that Adam's performance in training transformers degrades under random rotations. The authors find that Adam exhibits crucial dependence on the choice of basis, with global rotations consistently degrading performance while output-wise rotations show no significant impact. The study identifies structured rotations that can preserve or enhance Adam's empirical performance and examines existing rotation-dependent assumptions in the literature, finding them insufficient to explain Adam's behavior across different rotation types. The researchers verify that the orthogonality of Adam's updates serves as a promising indicator of basis sensitivity, suggesting this metric may be key to developing better theoretical frameworks for understanding Adam's empirical success.

## Method Summary
The authors conducted empirical studies examining Adam's performance under various parameter space rotations using transformer architectures. They systematically applied different types of rotations (global, structured, and output-wise) to the parameter space and measured the resulting impact on training dynamics and final performance. The study involved comparing Adam's behavior across these rotation scenarios while tracking metrics including loss convergence, gradient orthogonality, and final task performance. The researchers also analyzed existing rotation-dependent assumptions from the optimization literature to evaluate their explanatory power regarding Adam's observed behavior.

## Key Results
- Adam's performance consistently degrades under global rotations across all tested settings
- The degradation worsens with broader rotation scopes, indicating increasing sensitivity
- Output-wise rotations show no significant degradation across all settings
- Orthogonality of Adam's updates emerges as a promising indicator of basis sensitivity

## Why This Works (Mechanism)
Adam's performance sensitivity to rotations stems from its adaptive learning rate mechanism, which computes per-parameter updates based on exponential moving averages of squared gradients. When the parameter space is rotated, the alignment between gradients and the original parameter axes changes, affecting how Adam's adaptive scaling behaves. The orthogonality of updates appears to be a critical factor because Adam's update rule implicitly depends on maintaining certain geometric relationships in the parameter space. When rotations preserve these relationships (as in output-wise rotations), performance remains stable, but when they disrupt them (as in global rotations), the adaptive mechanism becomes misaligned with the underlying optimization landscape.

## Foundational Learning

**Parameter Space Rotations**
- Why needed: Understanding how geometric transformations of the parameter space affect optimization dynamics
- Quick check: Verify that rotation matrices are orthogonal (R^T R = I) and preserve vector norms

**Adam Optimizer Mechanics**
- Why needed: Core understanding of how Adam's adaptive learning rates depend on gradient statistics
- Quick check: Confirm that Adam maintains separate first and second moment estimates for each parameter

**Gradient Orthogonality**
- Why needed: Key metric for measuring basis sensitivity and explaining Adam's rotation-dependent behavior
- Quick check: Compute dot products between consecutive gradient vectors to assess orthogonality

## Architecture Onboarding

**Component Map**
Parameter Space -> Rotation Transformation -> Adam Updates -> Loss Landscape -> Performance Metrics

**Critical Path**
Rotation Application -> Gradient Computation -> Adam Update Rule -> Parameter Update -> Loss Evaluation

**Design Tradeoffs**
The study balances between testing general rotation patterns (which reveal sensitivity) and structured rotations (which preserve performance). Global rotations provide clean theoretical analysis but may not reflect practical scenarios, while structured rotations offer insights into what properties matter for Adam's success.

**Failure Signatures**
Performance degradation manifests as slower convergence, higher final loss, or training instability when global rotations are applied. Stable performance under output-wise rotations indicates preserved basis-dependent properties.

**First Experiments**
1. Apply identity rotation as baseline to establish Adam's baseline performance
2. Test small-angle global rotations to measure sensitivity threshold
3. Compare Adam vs SGD under identical rotation conditions to isolate rotation-specific effects

## Open Questions the Paper Calls Out
None

## Limitations
- Focus primarily on transformer architectures may limit generalizability to other model families
- Analysis examines specific rotation patterns without exhaustive exploration of all possible transformations
- Theoretical framework connecting orthogonality to Adam's success remains incomplete

## Confidence

**High confidence**: Empirical observations of Adam's performance degradation under global rotations across all tested settings

**Medium confidence**: The identification of orthogonality as a key indicator for basis sensitivity

**Low confidence**: The completeness of the theoretical explanation for Adam's rotation-dependent behavior

## Next Checks

1. Test the rotation sensitivity findings across diverse model architectures (CNNs, MLPs, RNNs) to establish generality beyond transformers

2. Conduct ablation studies systematically varying rotation scope and structure to precisely characterize the relationship between rotation properties and Adam's performance

3. Develop and validate theoretical bounds that connect the observed orthogonality metric to convergence guarantees under different rotation scenarios