---
ver: rpa2
title: Improving LLM Classification of Logical Errors by Integrating Error Relationship
  into Prompts
arxiv_id: '2404.19336'
source_url: https://arxiv.org/abs/2404.19336
tags: []
core_contribution: "This study tackles the challenge of classifying logical errors\u2014\
  program bugs without compiler error messages\u2014in programming education. It defines\
  \ ten types of logical errors, establishes their relationships, and orders them\
  \ to reduce ambiguity in classification."
---

# Improving LLM Classification of Logical Errors by Integrating Error Relationship into Prompts

## Quick Facts
- **arXiv ID**: 2404.19336
- **Source URL**: https://arxiv.org/abs/2404.19336
- **Reference count**: 19
- **Primary result**: LLM classification accuracy for logical errors improved by 21% using error relationship-integrated prompts, achieving 56% (GPT-3.5-turbo) and 86% (GPT-4) accuracy

## Executive Summary
This study addresses the challenge of classifying logical errors in programming education, where students' code compiles successfully but produces incorrect outputs. The researchers define ten types of logical errors and establish their relationships to reduce ambiguity in classification. By incorporating these error definitions and relationships into Chain-of-Thought and Tree-of-Thought prompts, the method significantly improves LLM classification accuracy. The approach also generates augmented logical error datasets from correct code, providing valuable resources for programming education. Experiments with GPT-3.5-turbo and GPT-4 demonstrate classification accuracies of 56% and 86% respectively, though with error-specific variations.

## Method Summary
The researchers first manually analyzed student code submissions to identify common logical errors and establish their relationships. They defined ten distinct logical error types and created an ordering system to reduce classification ambiguity. These definitions and relationships were then incorporated into prompt engineering techniques, specifically Chain-of-Thought and Tree-of-Thought prompting strategies. The method was evaluated using GPT-3.5-turbo and GPT-4 models on logical error classification tasks. Additionally, the team developed a dataset generation approach that creates augmented logical error examples by systematically modifying correct code to introduce various error types.

## Key Results
- Classification accuracy improved by 21% when using error relationship-integrated prompts compared to baseline prompts
- GPT-3.5-turbo achieved 56% accuracy while GPT-4 achieved 86% accuracy on logical error classification
- Error-specific variations showed that some logical error types are classified more accurately than others
- The dataset generation approach successfully created augmented logical error datasets from correct code

## Why This Works (Mechanism)
The approach works by providing LLMs with structured context about logical error types and their relationships, which helps the model reason through ambiguous cases. By ordering error types and defining their relationships, the prompts reduce the classification space and guide the LLM's reasoning process. The Tree-of-Thought prompting allows exploration of multiple classification paths, while Chain-of-Thought provides sequential reasoning steps. This structured approach compensates for LLMs' tendency to struggle with fine-grained classification distinctions without explicit guidance.

## Foundational Learning
- **Logical Error Classification**: Understanding different types of bugs that don't produce compiler errors but cause incorrect program behavior; needed to systematically address programming education challenges; quick check: can identify examples of each error type in sample code
- **Prompt Engineering Techniques**: Chain-of-Thought and Tree-of-Thought prompting methods; needed to structure LLM reasoning for complex classification tasks; quick check: can construct basic CoT and ToT prompts for simple classification
- **Error Relationship Mapping**: Defining how different logical errors relate to each other; needed to reduce ambiguity in classification; quick check: can explain why certain error types might be confused and how relationships help distinguish them
- **Dataset Augmentation**: Generating synthetic error examples from correct code; needed to expand training data for error classification; quick check: can describe how modifying correct code creates specific error types
- **Evaluation Metrics for Classification**: Understanding accuracy, precision, and recall in the context of error classification; needed to properly assess model performance; quick check: can calculate and interpret classification metrics from a confusion matrix
- **Programming Language Syntax Variations**: How different languages express similar logical constructs; needed to understand cross-language generalizability; quick check: can identify equivalent logical structures across two different programming languages

## Architecture Onboarding

**Component Map**: Error Definition & Relationship Framework -> Prompt Engineering (CoT/ToT) -> LLM Classification -> Dataset Generation

**Critical Path**: The core workflow involves (1) defining logical error types and relationships, (2) incorporating these into structured prompts, (3) running classification through LLM, and (4) generating augmented datasets. The error relationship framework is the foundation that enables improved classification through better prompt engineering.

**Design Tradeoffs**: The approach trades generality for specificity by requiring predefined error types and relationships, which limits adaptability to new error types but improves accuracy for known categories. The reliance on prompt engineering rather than fine-tuning avoids retraining costs but may be less effective for smaller models.

**Failure Signatures**: Classification failures occur most frequently for error types that are semantically similar or where multiple errors co-occur. The approach shows higher error rates for certain logical error types, suggesting the relationship framework needs refinement for edge cases. Cross-language applications may fail when syntax differences mask logical patterns.

**3 First Experiments**:
1. Test classification accuracy on a new programming language (e.g., Java or C++) using the same error framework
2. Evaluate performance on a small model (e.g., Llama 3 8B) to assess scalability
3. Measure the impact of different prompt engineering strategies (varying CoT depth or ToT branching) on classification accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Limited consideration of language-specific syntax variations, which may affect generalizability across programming languages
- Reliance on predefined error types and relationships may not capture all possible logical errors students encounter
- Dataset generation from correct code may not fully represent the complexity and diversity of real-world logical errors
- Focus on only two GPT models limits understanding of how the approach transfers to other LLM architectures

## Confidence

**High**: Error definition and relationship framework - The systematic approach to defining error types and their relationships appears well-founded and clearly articulated.

**Medium**: Prompt engineering improvements - The 21% improvement is promising but absolute accuracy levels suggest limitations remain.

**Medium**: Dataset generation methodology - While innovative, the approach's ability to capture real-world error complexity is uncertain.

## Next Checks

1. Test the error classification framework across multiple programming languages (e.g., Python, Java, C++) to assess generalizability and identify language-specific limitations

2. Conduct longitudinal studies with actual students using the system to evaluate real-world effectiveness in helping learners identify and correct logical errors

3. Compare the approach against alternative error classification methods, including those that use program execution traces or test cases rather than prompt-based reasoning