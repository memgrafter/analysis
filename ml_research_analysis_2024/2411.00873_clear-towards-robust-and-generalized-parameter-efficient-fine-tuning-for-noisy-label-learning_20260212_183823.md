---
ver: rpa2
title: 'CleaR: Towards Robust and Generalized Parameter-Efficient Fine-Tuning for
  Noisy Label Learning'
arxiv_id: '2411.00873'
source_url: https://arxiv.org/abs/2411.00873
tags:
- peft
- clear
- noisy
- methods
- clean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the robustness of parameter-efficient fine-tuning
  (PEFT) methods under noisy label conditions, a common challenge in real-world NLP
  applications. While PEFT methods are inherently robust to noisy labels due to their
  limited capacity, this same limitation makes them vulnerable to interference from
  noisy labels, impeding learning on clean samples.
---

# CleaR: Towards Robust and Generalized Parameter-Efficient Fine-Tuning for Noisy Label Learning

## Quick Facts
- arXiv ID: 2411.00873
- Source URL: https://arxiv.org/abs/2411.00873
- Authors: Yeachan Kim; Junho Kim; SangKeun Lee
- Reference count: 37
- Key outcome: Proposes Clean Routing (CleaR), a parameter-efficient fine-tuning method that adaptively activates PEFT modules based on clean probability estimation to improve robustness to noisy labels

## Executive Summary
This paper addresses the challenge of noisy labels in parameter-efficient fine-tuning (PEFT) methods for NLP tasks. While PEFT methods are inherently robust to noisy labels due to their limited capacity, this same limitation makes them vulnerable to interference from noisy labels on clean samples. The authors propose Clean Routing (CleaR), a novel routing-based approach that preferentially exposes PEFT modules to clean data while bypassing noisy samples. By estimating the probability of samples being clean using training losses and applying stochastic routing, CleaR minimizes the detrimental impact of noisy labels on learning.

## Method Summary
CleaR improves PEFT robustness to noisy labels by implementing a two-stage process: probability estimation and stochastic routing. First, a Gaussian Mixture Model (GMM) estimates the probability that each training sample is clean based on its training loss. Second, CleaR applies stochastic routing where samples with higher clean probability are more likely to activate PEFT modules, while samples with lower probability bypass them. This selective exposure ensures PEFT modules primarily learn from clean data. The method also includes consistency regularization to prevent training instability caused by the stochastic routing mechanism. Experiments demonstrate CleaR's effectiveness across various NLP tasks and noise configurations.

## Key Results
- CleaR-based PEFT methods significantly outperform standard PEFT methods in noisy environments
- The approach achieves better generalization and robustness compared to full fine-tuning and other PEFT methods
- Performance improvements are consistent across different noise types (symmetric, asymmetric, instance-dependent) and multiple NLP datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PEFT methods are inherently robust to noisy labels due to limited capacity
- Mechanism: Limited model capacity prevents overfitting to noisy samples, leading to better generalization on clean data
- Core assumption: The number of trainable parameters is small enough to constrain memorization of incorrect labels
- Evidence anchors:
  - [abstract] "PEFT has difficulty in memorizing noisy labels due to its inherently limited capacity, resulting in robustness"
  - [section 2] "PEFT struggles in memorizing noisy labels due to its inherently limited capacity, which interestingly provides robustness to noisy labels"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism
- Break condition: When PEFT capacity increases (e.g., larger bottleneck dimensions) or noise levels are extremely high, the limited capacity may no longer prevent memorization of noisy labels

### Mechanism 2
- Claim: Limited PEFT capacity makes models vulnerable to interference from noisy labels on clean samples
- Mechanism: Small number of parameters creates competition for learning resources between clean and noisy samples, where noisy samples can disrupt clean sample learning
- Core assumption: The model's limited capacity creates interference effects when both clean and noisy samples are present
- Evidence anchors:
  - [abstract] "such limited capacity simultaneously makes PEFT more vulnerable to interference of noisy labels, impeding the learning of clean samples"
  - [section 2] "PEFT also face challenges in learning from clean samples...PEFT, which inherently has limited capacity, is more vulnerable to the interference of noisy labels, impairing its learning ability on clean data"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism
- Break condition: When clean samples significantly outnumber noisy samples, or when noise rate is very low, the interference effect may be negligible

### Mechanism 3
- Claim: Clean Routing (CleaR) improves performance by preferentially exposing PEFT modules to clean data
- Mechanism: Stochastic routing based on clean probability estimates allows PEFT modules to primarily learn from clean samples while bypassing noisy ones
- Core assumption: The probability estimation based on training losses can effectively distinguish clean from noisy samples
- Evidence anchors:
  - [abstract] "In CleaR, PEFT modules are preferentially exposed to clean data while bypassing the noisy ones, thereby minimizing the noisy influence"
  - [section 3.2] "CleaR steers the potentially clean samples to route through PEFT modules, whereas the noisy ones are directed to bypass PEFT modules"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism
- Break condition: When probability estimation becomes inaccurate (e.g., with instance-dependent noise where loss patterns are less distinguishable), the routing may misdirect samples

## Foundational Learning

- Concept: Gaussian Mixture Model (GMM) for probability estimation
  - Why needed here: Used to estimate the probability that a sample is clean based on its training loss
  - Quick check question: What are the two components in the GMM used by CleaR and what do they represent?
- Concept: Stochastic routing vs deterministic routing
  - Why needed here: CleaR uses stochastic routing to provide fine-grained differentiation between samples
  - Quick check question: How does CleaR's stochastic routing differ from a deterministic approach that simply filters out low-probability samples?
- Concept: Consistency regularization
  - Why needed here: Prevents training instability caused by varying predictions due to stochastic routing
  - Quick check question: What is the purpose of the ensemble predictions in CleaR's consistency regularization?

## Architecture Onboarding

- Component map:
  - Pre-trained model (fixed parameters) -> PEFT modules (adaptable parameters) -> Clean probability estimator (GMM-based) -> Stochastic router (Bernoulli sampling) -> Consistency regularizer (ensemble predictions)
- Critical path: Forward pass → Probability estimation → Routing decision → PEFT activation → Backward pass with consistency loss
- Design tradeoffs:
  - Memory vs. performance: More PEFT parameters increase capacity but reduce noise robustness
  - Routing complexity vs. stability: More sophisticated routing improves sample differentiation but may increase training instability
  - Consistency strength vs. flexibility: Stronger regularization improves stability but may limit adaptation to true data patterns
- Failure signatures:
  - Performance degradation with high noise rates despite CleaR: Probability estimation failing to distinguish clean/noisy samples
  - Increased variance in predictions: Insufficient consistency regularization or poor ensemble prediction quality
  - No improvement over standard PEFT: Routing not effectively differentiating clean from noisy samples
- First 3 experiments:
  1. Test PEFT robustness on clean vs noisy datasets to verify the interference effect
  2. Implement and evaluate GMM-based clean probability estimation on a small dataset
  3. Add stochastic routing to a PEFT method and measure impact on clean sample learning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CleaR perform when applied to encoder-decoder models or decoder-only models, beyond the encoder models studied in this work?
- Basis in paper: [inferred] The paper mentions that CleaR's applicability to different architectures (e.g., decoder, encoder-decoder models) remains under-explored, but notes that routing-based PEFT methods have been shown to generalize to various architectures.
- Why unresolved: The paper only evaluates CleaR on encoder models (BERT-base and BERT-large) and does not test its performance on other model architectures.
- What evidence would resolve it: Experiments applying CleaR to decoder-only models (e.g., GPT-2) and encoder-decoder models (e.g., T5) on various tasks, comparing performance to full fine-tuning and other PEFT methods.

### Open Question 2
- Question: What is the impact of the routing probability γ on CleaR's performance, and how sensitive is the method to its value?
- Basis in paper: [explicit] The paper mentions that γ limits the range of clean probability, preventing over-reliance on the estimated probability, but does not provide an in-depth analysis of its impact on performance.
- Why unresolved: The paper does not explore the sensitivity of CleaR's performance to different values of γ or provide a detailed analysis of its role in the routing mechanism.
- What evidence would resolve it: Experiments varying γ across a wide range of values, analyzing the resulting performance and identifying an optimal range or value for γ.

### Open Question 3
- Question: Can CleaR be further improved by incorporating additional information beyond training loss to estimate the clean probability of samples?
- Basis in paper: [inferred] The paper uses training loss as the primary signal for estimating clean probability, but does not explore the potential benefits of incorporating other information, such as model confidence or feature representations.
- Why unresolved: The paper focuses on using training loss for clean probability estimation and does not investigate the potential improvements from incorporating additional signals.
- What evidence would resolve it: Experiments incorporating additional information (e.g., model confidence, feature representations) into the clean probability estimation process and comparing the resulting performance to the original CleaR method.

## Limitations

- The effectiveness of GMM-based clean probability estimation relies heavily on loss distributions being well-separated between clean and noisy samples, which may not hold for instance-dependent noise patterns
- The stochastic routing mechanism introduces additional hyperparameters (γ) that require careful tuning and may be sensitive to task-specific characteristics
- The paper focuses primarily on NLP tasks; performance on computer vision or other modalities remains unexplored

## Confidence

- **High confidence**: PEFT methods exhibit inherent robustness to noisy labels due to limited capacity - this is well-established in literature
- **Medium confidence**: CleaR's mechanism of preferential routing to clean samples improves learning - supported by experimental results but mechanism explanation could be more rigorous
- **Medium confidence**: Consistency regularization effectively stabilizes training with stochastic routing - ablation studies support this but detailed analysis is limited

## Next Checks

1. Test CleaR's performance across a broader range of noise rates (0.1 to 0.9) to identify the threshold where probability estimation fails
2. Implement an oracle routing baseline (perfect clean/noisy classification) to quantify the gap between CleaR's routing accuracy and optimal routing
3. Evaluate CleaR on computer vision datasets with standard image classification benchmarks to assess cross-modal generalization