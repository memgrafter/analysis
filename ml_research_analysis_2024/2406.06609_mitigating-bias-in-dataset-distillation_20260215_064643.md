---
ver: rpa2
title: Mitigating Bias in Dataset Distillation
arxiv_id: '2406.06609'
source_url: https://arxiv.org/abs/2406.06609
tags:
- dataset
- bias
- condensation
- samples
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how dataset bias affects the dataset distillation
  process, where synthetic datasets are generated to approximate the original data.
  The authors find that biases in the original dataset can be amplified during distillation,
  leading to biased synthetic data and degraded model performance.
---

# Mitigating Bias in Dataset Distillation

## Quick Facts
- arXiv ID: 2406.06609
- Source URL: https://arxiv.org/abs/2406.06609
- Authors: Justin Cui; Ruochen Wang; Yuanhao Xiong; Cho-Jui Hsieh
- Reference count: 40
- Key outcome: KDE-based reweighting mitigates bias amplification in dataset distillation, improving model performance on unbiased test sets.

## Executive Summary
This paper addresses the problem of bias amplification during dataset distillation, where synthetic datasets generated to approximate original data inherit and amplify existing biases. The authors propose a simple yet effective reweighting scheme based on kernel density estimation (KDE) that down-weights samples strongly correlated with bias features during the distillation process. Their method successfully mitigates bias amplification across multiple benchmark datasets, demonstrating improved model performance when trained on the debiased synthetic datasets.

## Method Summary
The proposed method uses KDE to estimate sample density in the embedding space of a supervised contrastive model, identifying biased samples as dense clusters. These samples are down-weighted during dataset distillation by applying inverse density weights to the loss function. The approach is applied to distribution-matching methods like DM and DSA, where real image embeddings are reweighted to balance the influence of biased and unbiased samples. The method requires training a supervised contrastive model on the original dataset, computing KDE-based weights, and modifying the distillation loss accordingly.

## Key Results
- KDE-based reweighting effectively mitigates bias amplification in dataset distillation across multiple datasets (CMNIST, BG FMNIST, Corrupted CIFAR-10, BFFHQ)
- The method improves model performance on unbiased test sets, achieving up to 91.5% accuracy on CMNIST compared to 23.8% with vanilla DM
- Post-distillation de-biasing methods like SelectMix and DFA are ineffective when bias amplification eliminates unbiased samples from the synthetic dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Kernel density estimation (KDE) identifies biased samples as dense clusters in the surrogate model's embedding space, allowing the reweighting scheme to down-weight their influence.
- Mechanism: The surrogate supervised contrastive model produces embeddings where bias-aligned samples cluster together. KDE computes the density at each sample's location; high density indicates a sample is in a bias cluster and should be down-weighted. The inverse density, normalized, becomes the sample weight.
- Core assumption: Biased samples cluster in the embedding space of a well-trained supervised contrastive model.
- Evidence anchors:
  - [section] "By summing the contributions of the kernel functions centered at each data point, KDE provides an estimate of the PDF at any given point x. The estimate ˆf(x) represents the density of the underlying distribution at that point. Since a biased dataset is usually dominated by bias-aligned samples which should be given a lower weight, we propose to use the normalized inverse of the kernel density function N( 1 ˆf(x)) as the new weights..."
  - [section] "In dataset condensation methods such as DM...the objective is to match the embeddings generated by the synthetic dataset...If the dataset is highly biased, the first term will be dominated by bias-aligned samples, thus causing distribution matching based methods to synthesize more bias-aligned images."

### Mechanism 2
- Claim: Reweighting real image embeddings in the loss function balances the influence of biased and unbiased samples during dataset distillation.
- Mechanism: The original loss function uses a simple mean of real image embeddings. The modified loss uses a weighted sum where weights are inversely proportional to KDE density. This shifts the distribution matching target away from bias-aligned samples toward a more balanced representation.
- Core assumption: Adjusting the weighting of real image embeddings during distillation changes the bias properties of the synthesized dataset.
- Evidence anchors:
  - [section] "Let W (T ) = [ w0, w1, ..., wn] with n equals |T | in Equation (1) be the normalized weight of each training sample, the weighted loss can be written as... where W (T ) · ψv(A(T , ω)) is the re-weighted embeddings where the bias feature has been balanced."
  - [section] "This re-weighting rebalances the significance of biased and unbiased samples, ameliorating biases in the condensation process."

### Mechanism 3
- Claim: Applying de-biasing methods after dataset distillation cannot fully recover performance because bias amplification during distillation eliminates unbiased samples from the synthetic dataset.
- Mechanism: During distillation, bias-aligned samples dominate the matching objective, causing the synthetic dataset to contain only bias-aligned samples. When a de-biasing method is applied to this synthetic dataset, it has no unbiased samples to work with, limiting its effectiveness.
- Core assumption: Dataset distillation can amplify bias to the point of eliminating unbiased samples from the synthetic dataset.
- Evidence anchors:
  - [section] "For datasets exhibiting amplified biases (CMNIST and BG FMNIST), we found that even state-of-the-art de-biasing training methods such as SelectMix and DFA are not able to obtain an unbiased model from the biased synthetic set."
  - [section] "For example, on Colored MNIST with a 5% bias in conflicting samples and IPC 50, the original Distribution Matching (DM) method leads to a biased synthetic set. A model trained on such a distilled set achieves only 23.8% accuracy. In contrast, our reweighting method produces a more balanced dataset, resulting in 91.5% accuracy..."

## Foundational Learning

- Concept: Kernel Density Estimation (KDE)
  - Why needed here: KDE is used to estimate the density of samples in the embedding space, which identifies biased samples as dense clusters that should be down-weighted.
  - Quick check question: How does KDE estimate the probability density function at a given point using kernel functions centered at observed data points?

- Concept: Supervised Contrastive Learning
  - Why needed here: The supervised contrastive model produces embeddings where bias-aligned samples cluster, enabling KDE to identify biased samples.
  - Quick check question: What property of supervised contrastive embeddings makes them suitable for measuring distances related to bias features?

- Concept: Dataset Distillation Objective Functions
  - Why needed here: Understanding how distribution matching and gradient matching work is essential to see how reweighting real image embeddings affects the synthesized dataset.
  - Quick check question: In distribution matching methods like DM, what is being matched between the real and synthetic datasets?

## Architecture Onboarding

- Component map:
  Supervised Contrastive Model -> KDE Module -> Reweighting Scheme -> Dataset Distillation Algorithm -> Model Training

- Critical path:
  1. Train supervised contrastive model on the original dataset
  2. Compute embeddings for all real images
  3. Apply KDE to embeddings to get density estimates
  4. Compute weights as inverse density, normalized
  5. Run dataset distillation with reweighted loss
  6. Train model on synthesized dataset and evaluate

- Design tradeoffs:
  - Using KDE adds computational overhead but provides effective bias identification without requiring explicit bias labels
  - The choice of kernel variance and normalization temperature affects bias mitigation performance and must be tuned
  - Applying to gradient-matching methods requires modifying the loss differently than distribution-matching methods

- Failure signatures:
  - Poor bias mitigation performance: Check if supervised contrastive model is well-trained and if KDE parameters are appropriate
  - Synthetic images dominated by bias: Indicates KDE is not effectively down-weighting biased samples
  - No performance improvement over baseline: Could indicate the original dataset has little bias or that IPC is too high relative to bias-conflict ratio

- First 3 experiments:
  1. Apply the method to DM on CMNIST with 5% bias-conflict ratio and IPC 10, compare to vanilla DM
  2. Apply the method to DSA on BG FMNIST with 2% bias-conflict ratio and IPC 50, compare to vanilla DSA
  3. Test the effect of different kernel variances (e.g., 0.01, 0.1, 1.0) on CMNIST with 1% bias-conflict ratio

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the bias amplification effect in dataset distillation vary across different types of biases (e.g., color, background, corruption) and why?
- Basis in paper: [explicit] The paper explicitly states that color and background biases are amplified while corruption bias is suppressed during the distillation process.
- Why unresolved: The paper provides empirical observations but does not offer a theoretical explanation for why different biases are amplified or suppressed.
- What evidence would resolve it: A theoretical analysis or additional experiments that identify the underlying mechanisms causing different biases to be amplified or suppressed.

### Open Question 2
- Question: Can the proposed KDE-based reweighting scheme be extended to other dataset distillation methods beyond DM and DSA?
- Basis in paper: [explicit] The paper mentions that the method can be applied to other dataset condensation methods but focuses on DM and DSA for evaluation.
- Why unresolved: The paper does not provide experimental results or theoretical justification for the applicability of the method to other distillation techniques.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of the KDE-based reweighting scheme on other dataset distillation methods, such as MTT or DREAM.

### Open Question 3
- Question: How does the computational overhead of the KDE-based reweighting scheme scale with dataset size and dimensionality?
- Basis in paper: [inferred] The paper mentions that the KDE computation involves pairwise distances and has a time complexity of O(B^2d), where B is the batch size and d is the projection dimension.
- Why unresolved: The paper does not provide a detailed analysis of how the computational overhead scales with larger datasets or higher-dimensional data.
- What evidence would resolve it: Empirical results showing the computational time and memory usage of the KDE-based reweighting scheme on datasets of varying sizes and dimensions.

## Limitations
- The method's effectiveness depends on the supervised contrastive model's ability to produce embeddings where biased samples cluster distinctly
- KDE-based reweighting adds computational overhead that may not scale well to very large datasets
- The approach may not generalize to gradient-matching distillation methods without modification

## Confidence

- High confidence: The core mechanism of using KDE-based reweighting to down-weight bias-aligned samples is well-supported by the mathematical formulation and experimental results across multiple datasets (CMNIST, BG FMNIST, Corrupted CIFAR-10, BFFHQ).
- Medium confidence: The claim that post-distillation de-biasing is ineffective due to bias amplification eliminating unbiased samples is supported but could benefit from more systematic investigation of different de-biasing methods and their success rates.
- Medium confidence: The generalizability of the method to gradient-matching distillation approaches (like MTT) is suggested but not empirically validated, creating uncertainty about its applicability beyond distribution-matching methods.

## Next Checks

1. Parameter sensitivity analysis: Systematically vary the KDE kernel variance and normalization temperature across the tested datasets to determine how sensitive the bias mitigation performance is to these hyperparameters.

2. Embedding space analysis: Visualize and quantify the clustering of bias-aligned versus unbiased samples in the supervised contrastive embedding space to validate that KDE is identifying the correct samples for down-weighting.

3. Cross-method applicability test: Apply the reweighting scheme to a gradient-matching dataset distillation method (such as MTT) to verify whether the approach generalizes beyond distribution-matching methods like DM and DSA.