---
ver: rpa2
title: 'LLM vs. Lawyers: Identifying a Subset of Summary Judgments in a Large UK Case
  Law Dataset'
arxiv_id: '2403.04791'
source_url: https://arxiv.org/abs/2403.04791
tags:
- summary
- cases
- judgment
- legal
- court
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study compares two methods\u2014keyword-based search logic\
  \ and Claude 2 LLM\u2014for identifying summary judgment cases within a 356,011-case\
  \ UK legal corpus. The LLM achieved a weighted F1 score of 0.94, outperforming the\
  \ keyword approach (0.78)."
---

# LLM vs. Lawyers: Identifying a Subset of Summary Judgments in a Large UK Case Law Dataset

## Quick Facts
- arXiv ID: 2403.04791
- Source URL: https://arxiv.org/abs/2403.04791
- Authors: Ahmed Izzidien; Holli Sargeant; Felix Steffek
- Reference count: 27
- LLM achieved weighted F1 score of 0.94 vs keyword approach's 0.78 for identifying summary judgment cases

## Executive Summary
This study compares two methods for identifying summary judgment cases within a large UK legal corpus: traditional keyword-based search logic and Claude 2 LLM classification. The LLM achieved a weighted F1 score of 0.94, significantly outperforming the keyword approach (0.78). Keyword co-occurrence analysis revealed complex legal terminology patterns, highlighting the challenges of manual classification. The LLM's contextual understanding enabled more accurate identification, extracting 3,102 summary judgment cases primarily from first-instance courts with an average word count of 96,120.

## Method Summary
The study employed two classification approaches on the Cambridge Law Corpus containing 356,011 UK court decisions. The keyword-based method used logical operators and RegEx patterns to search for summary judgment-related terms. The LLM approach utilized Claude 2 with a custom prompt to analyze complete case texts. Cases were classified based on whether they contained summary judgment applications, with manual validation confirming the accuracy of both methods. The weighted F1 score was used as the primary metric for comparison.

## Key Results
- Claude 2 achieved a weighted F1 score of 0.94, outperforming keyword search (0.78)
- 3,102 summary judgment cases were identified, predominantly from first-instance courts
- Average word count of identified cases was 96,120
- Keyword co-occurrence patterns revealed complex legal terminology relationships

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Claude 2 outperforms keyword-based search in identifying summary judgment cases due to its contextual understanding and ability to parse legal reasoning patterns.
- Mechanism: Claude 2 uses a deliberative neural network architecture that breaks prompts into comprehension, reasoning, and response generation stages, allowing it to capture nuanced legal language and context beyond simple keyword matching.
- Core assumption: Legal language complexity and variability require contextual understanding rather than pattern matching.
- Evidence anchors:
  - [abstract] "The LLM achieved a weighted F1 score of 0.94, outperforming the keyword approach (0.78)"
  - [section] "Claude 2 excels in language comprehension and contextual understanding... can parse questions and scenarios, capturing the intricacies of language"
- Break condition: If legal language patterns change significantly or if the LLM's training data lacks relevant legal context, performance would degrade.

### Mechanism 2
- Claim: Keyword co-occurrence patterns reveal complex legal terminology relationships that improve search precision when used with Claude 2.
- Mechanism: By analyzing how legal terms appear together in cases, the researchers identified key combinations that distinguish summary judgment cases from other cases that merely mention summary judgment.
- Core assumption: Legal terms have specific co-occurrence patterns that signal case type relevance.
- Evidence anchors:
  - [section] "A co-occurrence happens when two keywords are found in the same document... Counting keyword co-occurrences only once per case file shifts the focus from the sheer quantity of keyword mentions to the relevance of documents"
  - [section] "The variability of English legal language is a broader issue in automated legal research due to the complex and evolving lexicon within legal judgments"
- Break condition: If legal terminology evolves rapidly or if cases use increasingly diverse language patterns, co-occurrence analysis may become less reliable.

### Mechanism 3
- Claim: Claude 2's ability to handle large documents (up to 100,000 tokens) enables analysis of complete case texts rather than relying on keyword snippets.
- Mechanism: The large context window allows the model to consider full case narratives, procedural histories, and legal reasoning rather than just isolated passages containing keywords.
- Core assumption: Complete case context is necessary for accurate legal classification.
- Evidence anchors:
  - [section] "Claude 2 is an advanced language model... designed to embody the principles of being 'helpful, harmless and honest'... It operates on a neural network that has been trained on large datasets comprising natural language"
  - [section] "Claude 2 was able to successfully detect if a case that we randomly selected was a summary judgment or not"
- Break condition: If cases exceed the token limit or if the model's context window is reduced in future versions, this advantage would be lost.

## Foundational Learning

- Concept: F1 Score and its weighted variant
  - Why needed here: The paper uses F1 scores to compare model performance, and understanding the metric is crucial for interpreting results
  - Quick check question: If a model has precision of 0.9 and recall of 0.8, what is its F1 score?

- Concept: Legal terminology and case structure
  - Why needed here: Understanding how summary judgments are structured and the specific legal language used is essential for both keyword development and LLM prompting
  - Quick check question: What distinguishes a case that mentions "summary judgment" from an actual summary judgment case?

- Concept: Token limits and context windows in language models
  - Why needed here: The paper discusses Claude 2's 100,000 token limit and its impact on case analysis
  - Quick check question: If a case has 75,000 words, approximately how many tokens would it contain?

## Architecture Onboarding

- Component map: Cambridge Law Corpus → Keyword search with RegEx → Claude 2 LLM classification → Manual validation → Results analysis
- Critical path: Data → Keyword analysis → Claude 2 classification → Manual validation → Results analysis
- Design tradeoffs: Keyword search is faster and more transparent but less accurate; LLM is more accurate but slower and requires careful prompt engineering
- Failure signatures: Low F1 scores indicate either poor keyword selection or inadequate prompt design; document truncation suggests token limit issues
- First 3 experiments:
  1. Test keyword co-occurrence analysis on a small subset to validate patterns
  2. Run Claude 2 on a small sample with different prompt variations to optimize performance
  3. Compare manual validation results between keyword and LLM approaches on the same sample set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific prompt engineering strategies could improve the F1 score for identifying summary judgment cases beyond the current 0.94 achieved with Claude 2?
- Basis in paper: [explicit] The paper states "Future research directions could focus on refining the methodology to enhance accuracy and reliability. This includes exploring prompt engineering strategies to improve the F1 score."
- Why unresolved: The current study achieved a high F1 score but suggests room for improvement, particularly in prompt design.
- What evidence would resolve it: A systematic study testing various prompt structures, examples, and instructions to measure their impact on classification accuracy, ideally achieving an F1 score above 0.94.

### Open Question 2
- Question: How does the performance of traditional classification methods compare to LLMs like Claude 2 when using a manually curated dataset for identifying summary judgment cases?
- Basis in paper: [explicit] The paper suggests "employing traditional classification methods with a manually curated dataset could verify the feasibility of achieving higher F1 scores and provide a comparative analysis with existing models like Claude 2."
- Why unresolved: The study only compares keyword-based search with Claude 2, leaving traditional ML methods untested.
- What evidence would resolve it: A direct comparison study applying traditional ML algorithms (e.g., SVM, random forest) to the same classification task, measuring their F1 scores against the LLM approach.

### Open Question 3
- Question: What is the impact of dataset completeness on the accuracy of identifying summary judgment cases, given that the Cambridge Law Corpus may not contain all UK court judgments?
- Basis in paper: [explicit] The paper acknowledges "the dataset may not encompass all summary judgments delivered by the courts" and notes this "potential gap in data coverage could influence the findings."
- Why unresolved: The study does not quantify how missing cases might affect classification accuracy or distribution analysis.
- What evidence would resolve it: A study comparing results using different completeness levels of the corpus, or validation against a known complete subset of summary judgment cases, to measure the impact of missing data on accuracy metrics.

## Limitations

- Dependency on the Cambridge Law Corpus dataset, which may not be universally accessible for reproduction
- Reliance on Claude 2's proprietary performance characteristics that could change with model updates
- Limited generalizability beyond UK summary judgment cases to other legal domains or jurisdictions

## Confidence

- **High Confidence**: The comparative F1 scores (0.94 vs 0.78) between LLM and keyword approaches are directly measurable and reproducible given access to the same dataset and tools.
- **Medium Confidence**: The LLM's contextual understanding advantage assumes consistent legal terminology patterns, which may evolve over time.
- **Medium Confidence**: The superiority of LLM over keyword approaches is well-demonstrated for this specific task but may not generalize to all legal classification problems.

## Next Checks

1. Test the keyword-based approach and LLM classification on a publicly available legal dataset (e.g., European Court of Human Rights cases) to verify generalizability.
2. Implement A/B testing with different prompt variations for Claude 2 to establish optimal prompt engineering practices for legal classification tasks.
3. Conduct temporal validation by applying the classification methods to cases from different decades to assess performance stability across legal language evolution.