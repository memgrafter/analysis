---
ver: rpa2
title: 'Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question Answering
  with Large Language Models'
arxiv_id: '2402.15131'
source_url: https://arxiv.org/abs/2402.15131
tags:
- name
- question
- type
- knowledge
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Interactive-KBQA, a framework that leverages
  large language models (LLMs) as agents to generate semantic parses for knowledge
  base question answering through multi-turn interactions. By designing three generic
  APIs for KB interaction and providing annotated exemplars, the method guides LLMs
  to incrementally construct SPARQL queries.
---

# Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question Answering with Large Language Models

## Quick Facts
- arXiv ID: 2402.15131
- Source URL: https://arxiv.org/abs/2402.15131
- Reference count: 31
- Primary result: Achieves competitive performance on WebQuestionsSP, ComplexWebQuestions, KQA Pro, and MetaQA using minimal exemplars and multi-turn interactions.

## Executive Summary
Interactive-KBQA introduces a novel framework that leverages large language models as agents to generate SPARQL queries for knowledge base question answering through multi-turn interactions. The system conceptualizes the LLM as an agent and the KB as an environment, enabling iterative dialogue-based problem solving. By designing three generic APIs for KB interaction and providing annotated exemplars, the method guides LLMs to incrementally construct SPARQL queries. Extensive experiments demonstrate that Interactive-KBQA achieves competitive results across multiple datasets using minimal examples, while also supporting manual intervention for iterative refinement.

## Method Summary
Interactive-KBQA employs a multi-turn interaction framework where an LLM agent generates semantic parses for KBQA by interacting with a knowledge base through three generic APIs: SearchNodes for entity linking, SearchGraphPatterns for relation identification, and ExecuteSPARQL for query execution. The system uses few-shot exemplars with step-wise reasoning processes to guide the LLM, allowing it to incrementally construct SPARQL queries through iterative feedback from the KB. The framework supports both few-shot prompting and fine-tuning approaches, with manual intervention capability for refining outputs.

## Key Results
- Achieves competitive F1 scores on WebQuestionsSP, ComplexWebQuestions, KQA Pro, and MetaQA datasets
- Demonstrates effectiveness with minimal exemplars (2 per question type)
- Supports both few-shot learning and fine-tuning approaches
- Enables manual intervention for iterative refinement of SPARQL queries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-turn interactive process allows LLMs to incrementally construct SPARQL queries by leveraging real-time feedback from the KB.
- Mechanism: At each turn, the LLM generates an action (SearchNodes, SearchGraphPatterns, ExecuteSPARQL, Done), which is executed against the KB. The KB returns observations that inform the next reasoning step, enabling iterative refinement of the logical form.
- Core assumption: The KB can provide sufficient feedback at each step to guide the LLM toward a correct SPARQL query.
- Evidence anchors:
  - [abstract] "By conceptualizing the LLM as an agent and the KB as the environment, Interactive-KBQA facilitates an iterative, dialogue-based problem-solving process."
  - [section 3.2] "Interactive-KBQA, an interactive method for KBQA that conceptualizes the LLM as an agent and the KB as an environment."
  - [corpus] FMR score of 0.6849 for related paper on KBQA-R1 suggests moderate relatedness, but no direct evidence of iterative KB feedback effectiveness.
- Break condition: If the KB fails to return meaningful observations (e.g., due to sparsity or ambiguity), the LLM cannot refine its reasoning path.

### Mechanism 2
- Claim: The three generic APIs (SearchNodes, SearchGraphPatterns, ExecuteSPARQL) provide a unified abstraction layer that supports heterogeneous KBs.
- Mechanism: These tools abstract away KB-specific syntax, allowing the same interaction logic to work across Freebase, Wikidata, and Movie KG by translating high-level semantic queries into SPARQL or equivalent.
- Core assumption: The underlying KBs expose sufficient structural similarity (entities, relations, literals) to be queried through a common SPARQL-like interface.
- Evidence anchors:
  - [section 3.3] "We introduce the following three tools... SearchNodes(name), SearchGraphPatterns(sparql, semantic), ExecuteSPARQL(sparql)."
  - [section 3.3] "This tool avoids dataset-specific EL techniques in favor of a generic retrieval approach."
  - [corpus] Moderate relatedness (FMR 0.6519) to RetinaQA, which also handles KBQA robustness, suggesting shared architectural themes.
- Break condition: If a KB lacks support for SPARQL or equivalent graph query execution, the unified approach fails.

### Mechanism 3
- Claim: Few-shot exemplars with step-wise reasoning processes guide LLMs to generate correct SPARQL queries even in low-resource settings.
- Mechanism: By providing two annotated exemplars per question type with full interactive reasoning traces, the LLM learns the mapping from natural language to SPARQL through in-context learning without requiring extensive training data.
- Core assumption: LLMs can generalize SPARQL generation from a small number of well-structured exemplars.
- Evidence anchors:
  - [abstract] "For each category of complex question, we devised exemplars to guide LLMs through the reasoning processes."
  - [section 3.4] "E denotes a set of exemplars, and for each type of question, we manually annotate two complete examples in an interactive format."
  - [corpus] Weak evidence; no direct corpus support for few-shot exemplar effectiveness in KBQA.
- Break condition: If the exemplars are poorly chosen or the question type distribution in deployment differs significantly from exemplars, performance degrades.

## Foundational Learning

- Concept: SPARQL query construction
  - Why needed here: The core output of the system is a SPARQL query that retrieves answers from a KB.
  - Quick check question: Given a triple pattern like `(?person, <born_in>, ?city)`, write the corresponding SPARQL SELECT clause.

- Concept: Entity linking (EL)
  - Why needed here: Correctly identifying the KB entity corresponding to a mention in the question is the first step in query construction.
  - Quick check question: If a question mentions "Obama", what KB entity should be linked to, and what surface forms might it have?

- Concept: Graph pattern matching
  - Why needed here: Understanding how to traverse KB relations (e.g., one-hop, two-hop paths) is essential for multi-hop questions.
  - Quick check question: For a two-hop path from "Tom Hanks" to a movie via the relation "acted_in", write the SPARQL triple pattern.

## Architecture Onboarding

- Component map:
  - Question → Prompt generator → LLM agent → Tool executor → KB interface → Observation → LLM agent (iterative loop)

- Critical path:
  1. Input question → prompt construction (instruction + exemplars)
  2. LLM generates thought + action
  3. Tool executor parses and runs action against KB
  4. KB returns observation → appended to history
  5. Repeat until Done action or max turns
  6. Output final observation as answer

- Design tradeoffs:
  - Few-shot vs. fine-tuning: Few-shot avoids retraining but depends heavily on LLM reasoning; fine-tuning reduces reliance on LLM but requires annotated data.
  - Turn limit vs. cost: More turns allow deeper reasoning but increase API costs and latency.
  - Exemplar coverage vs. prompt size: More exemplars improve coverage but risk exceeding context limits.

- Failure signatures:
  - Entity linking errors: LLM fails to find correct KB entity → SearchNodes returns irrelevant results.
  - Predicate search errors: LLM cannot identify correct relation → SearchGraphPatterns returns unrelated triples.
  - Reasoning errors: LLM constructs incorrect SPARQL → ExecuteSPARQL returns wrong or no answers.
  - Format compliance errors: LLM uses wrong tool syntax → parsing error from tool executor.

- First 3 experiments:
  1. Verify the three tools work correctly on a simple one-hop question (e.g., "Who directed The Matrix?").
  2. Test exemplar selection logic by running with different exemplar sets and measuring performance variance.
  3. Measure turn count and cost on a subset of CWQ questions to identify cost bottlenecks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Interactive-KBQA's performance scale with increasing question complexity beyond the datasets tested?
- Basis in paper: [explicit] The paper discusses handling complex questions but only evaluates on specific datasets (WebQSP, CWQ, KQA Pro, MetaQA)
- Why unresolved: The study doesn't explore extremely complex multi-hop reasoning or questions requiring extensive context beyond current limits
- What evidence would resolve it: Testing on datasets with longer reasoning chains or questions requiring more than 20 interaction turns would demonstrate scalability limits

### Open Question 2
- Question: What is the optimal balance between exemplar quantity and inference cost for different question types?
- Basis in paper: [explicit] The paper discusses exemplar selection trade-offs and shows varying performance with different exemplar counts
- Why unresolved: The study provides some analysis but doesn't systematically optimize exemplar selection across all question types
- What evidence would resolve it: A comprehensive study varying exemplar counts per question type while measuring performance gains versus cost increases

### Open Question 3
- Question: How does Interactive-KBQA's error rate compare to human performance on the same tasks?
- Basis in paper: [inferred] The paper discusses various error types but doesn't benchmark against human performance
- Why unresolved: The study focuses on model performance but doesn't include human baseline comparisons
- What evidence would resolve it: A controlled study where humans perform the same KBQA tasks with similar interaction capabilities would provide this comparison

### Open Question 4
- Question: Can the three-tool interaction framework be extended to handle even more diverse knowledge base schemas?
- Basis in paper: [explicit] The paper demonstrates the framework works across Freebase, Wikidata, and Movie KG
- Why unresolved: The study shows success on three schemas but doesn't test on significantly different database structures
- What evidence would resolve it: Testing on knowledge bases with radically different schema designs (e.g., hierarchical vs. graph-based) would demonstrate framework generalizability

## Limitations
- Reliance on in-context learning with few-shot exemplars introduces uncertainty about performance scalability
- Manual annotation of exemplars and dependence on LLM reasoning quality create potential bottlenecks
- Effectiveness of the three generic APIs across diverse KB structures remains untested beyond the specified datasets
- Manual intervention mechanism's impact on final performance is not quantified

## Confidence
- **High Confidence**: The architectural framework of using LLMs as agents with KB-as-environment is well-defined and technically sound. The three-tool abstraction (SearchNodes, SearchGraphPatterns, ExecuteSPARQL) provides a coherent interface design.
- **Medium Confidence**: Performance claims on benchmark datasets are reasonable given the competitive results, but the exact contribution of manual intervention versus automated refinement is unclear.
- **Low Confidence**: The generalization capability of few-shot exemplars across unseen question types and KBs is not empirically validated, and the cost-effectiveness of multi-turn interactions versus alternative approaches is not addressed.

## Next Checks
1. **Ablation Study**: Remove manual intervention and measure performance degradation to quantify its contribution.
2. **KB Diversity Test**: Apply the framework to a KB with significantly different schema (e.g., medical ontology) to test API generalization.
3. **Exemplar Sensitivity Analysis**: Vary the number and quality of exemplars systematically to measure their impact on SPARQL generation accuracy.