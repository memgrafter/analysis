---
ver: rpa2
title: 'YTCommentQA: Video Question Answerability in Instructional Videos'
arxiv_id: '2401.17343'
source_url: https://arxiv.org/abs/2401.17343
tags:
- video
- question
- questions
- visual
- answerability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces YTCommentQA, a dataset of naturally-generated
  questions from YouTube instructional video comments, annotated for answerability
  and required modality (visual, script, or both). The dataset addresses the gap between
  existing Video QA datasets, which focus on questions answerable within the video,
  and real-world user queries that may require additional knowledge.
---

# YTCommentQA: Video Question Answerability in Instructional Videos

## Quick Facts
- **arXiv ID**: 2401.17343
- **Source URL**: https://arxiv.org/abs/2401.17343
- **Reference count**: 40
- **Primary result**: Introduces YTCommentQA, a dataset of naturally-generated YouTube instructional video questions annotated for answerability and required modality, demonstrating the challenging nature of predicting answerability in multimodal video content.

## Executive Summary
YTCommentQA introduces a dataset of naturally-generated questions from YouTube instructional video comments, annotated for answerability and required modality (visual, script, or both). The dataset addresses the gap between existing Video QA datasets and real-world user queries by providing questions that are longer, more diverse, and more complex than artificially generated ones. Two tasks are defined: Segment Answerability Classification and Video Answerability Classification, with experiments showing that existing models struggle with multimodal reasoning required to determine answerability and modality.

## Method Summary
The dataset consists of 2,332 questions from 2,004 YouTube videos, collected from comments containing timestamps and filtered for English language. Questions are annotated for answerability and required modality through a multi-step process involving timestamp verification, visual snippet checking, script snippet checking, and combined modality assessment. Experiments use fine-tuned and zero-shot language models (Llama-2, GPT-3.5, GPT-4) and a multimodal model (SeViLA) to train on Segment and Video Answerability Classification tasks, addressing class imbalance through oversampling.

## Key Results
- Models struggle with multimodal reasoning, particularly for Combined Answerable questions requiring both visual and script information
- Fine-tuned Llama-2 models show limited improvement over zero-shot approaches, highlighting the challenging nature of the task
- Visual-only answerable questions are frequently misclassified as unanswerable, indicating over-reliance on script information

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The dataset leverages naturally occurring user questions, which are more diverse and complex than artificially generated ones, improving real-world model robustness.
- **Mechanism**: Real user queries contain varied phrasing, context, and ambiguity that synthetic questions often lack, forcing models to handle richer semantic and syntactic structures.
- **Core assumption**: Crowd workers or automated methods produce less realistic questions than actual YouTube commenters.
- **Evidence anchors**:
  - [abstract]: "naturally-generated questions from YouTube" and "greater diversity and length than questions found in existing Video QA datasets."
  - [section]: "our questions are roughly twice as long as those in existing Video QA datasets" and "questions start with casual words like 'hi' or 'great.'"
- **Break condition**: If the dataset filtering overly restricts question types, it may lose the diversity that makes it effective.

### Mechanism 2
- **Claim**: Multimodal reasoning is necessary because some questions require integration of visual and script information.
- **Mechanism**: Visual cues (e.g., pointing gestures) and script cues (e.g., verbal references) are complementary; neither alone suffices for all answerable questions.
- **Core assumption**: Visual and script modalities contain distinct but overlapping information in instructional videos.
- **Evidence anchors**:
  - [abstract]: "Discerning whether a question can be answered by video content is challenging due to the multi-modal nature of videos."
  - [section]: "some questions can be answered only when both modalities are considered together" and "Combined Answerable" requires both visual and script inputs.
- **Break condition**: If a model can answer all questions using only one modality, the multimodal complexity would be unnecessary.

### Mechanism 3
- **Claim**: Answerability classification improves reliability by preventing models from generating false information for unanswerable questions.
- **Mechanism**: By identifying which questions cannot be answered from the video, models avoid hallucinating answers, increasing trustworthiness.
- **Core assumption**: Video QA models currently do not systematically detect unanswerable questions.
- **Evidence anchors**:
  - [abstract]: "To avoid generating false information, it is essential to determine whether a given question can be answered within the video."
  - [section]: "there has been limited prior research on video answerability."
- **Break condition**: If answerability detection is too inaccurate, it may mislead users more than help.

## Foundational Learning

- **Concept**: Multimodal integration (visual + textual).
  - **Why needed here**: Questions often require understanding both video frames and transcripts; models must fuse these modalities effectively.
  - **Quick check question**: Can a model answer "What temperature did you use?" using only visual data? (Answer: No, script is required.)

- **Concept**: Temporal segmentation of video content.
  - **Why needed here**: Evidence snippets are extracted around timestamps; understanding how to map questions to relevant video segments is crucial.
  - **Quick check question**: If a question refers to "here" in the video, which modality helps resolve it? (Answer: Combined visual and script.)

- **Concept**: Answerability classification (answerable vs. unanswerable).
  - **Why needed here**: Prevents models from generating incorrect answers when the video lacks the necessary information.
  - **Quick check question**: What percentage of questions in the dataset are unanswerable? (Answer: ~14%.)

## Architecture Onboarding

- **Component map**:
  - YouTube comment scraping -> Timestamp filtering -> Annotation workflow -> Dataset creation
  - Annotation system: Timestamp verification -> Visual snippet check -> Script snippet check -> Combined modality assessment
  - Model training: Fine-tuning language models (Llama-2) and multimodal models (SeViLA) on answerability tasks

- **Critical path**:
  1. Collect YouTube comments with timestamps
  2. Annotate answerability and required modality
  3. Train models on Segment and Video Answerability Classification
  4. Evaluate using F1-score (segment) or accuracy (video)

- **Design tradeoffs**:
  - Using summaries vs. full transcripts to fit model context limits detail but enables training
  - Filtering questions with timestamps may exclude some relevant queries but ensures answer segments exist
  - Balancing class imbalance via oversampling affects model bias toward certain modalities

- **Failure signatures**:
  - High false negatives (marking answerable as unanswerable) -> model overly cautious
  - Low accuracy on Combined Answerable class -> multimodal fusion insufficient
  - Over-reliance on script modality -> visual cues underutilized

- **First 3 experiments**:
  1. **Baseline evaluation**: Run zero-shot GPT-4 on Segment Answerability Classification to measure current performance
  2. **Fine-tuning comparison**: Compare Llama-2 7B vs. 13B on Video Answerability Classification with and without context extension
  3. **Multimodal ablation**: Test SeViLA with varying numbers of keyframes to find optimal visual input size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the presence of non-English questions in YouTube comments impact the effectiveness of answerability models trained on YTCommentQA?
- Basis in paper: Explicit - The paper mentions using a language detection library to filter out non-English comments during question collection.
- Why unresolved: The paper does not explore the impact of excluding non-English questions on model performance or generalizability.
- What evidence would resolve it: Comparative analysis of model performance on English-only vs. multilingual datasets.

### Open Question 2
- Question: Can the inclusion of external knowledge bases improve the accuracy of answerability classification for questions that require domain-specific information?
- Basis in paper: Explicit - The paper discusses the challenge of questions that go beyond the video's informational boundaries and may require additional knowledge.
- Why unresolved: The paper focuses on using only video content for answerability classification and does not explore the integration of external knowledge.
- What evidence would resolve it: Experiments comparing models with and without access to external knowledge bases on questions requiring domain-specific information.

### Open Question 3
- Question: How does the length of questions and their context affect the performance of answerability classification models?
- Basis in paper: Explicit - The paper highlights that questions from real-world users are longer and more diverse than those in existing Video QA datasets.
- Why unresolved: The paper does not investigate the relationship between question length, context, and model performance.
- What evidence would resolve it: Analysis of model performance on questions of varying lengths and with different levels of contextual information.

## Limitations

- Dataset filtering criteria (English-only, timestamp-containing questions) may limit generalizability to other languages or question types
- Reliance on video summaries rather than full transcripts could reduce the information available for answering questions
- Relatively small dataset size (2,332 questions) may limit robustness of findings for training more complex models

## Confidence

- **High Confidence**: The dataset's novelty in using naturally-generated YouTube comments and its annotation scheme for answerability and required modality
- **Medium Confidence**: The effectiveness of the proposed dataset in improving real-world model robustness
- **Medium Confidence**: The necessity of multimodal reasoning, as the dataset may contain a higher proportion of questions requiring combined modalities than typical user queries

## Next Checks

1. **Dataset Generalization**: Test the models trained on YTCommentQA on a held-out set of YouTube comments from different video categories or languages to assess generalizability
2. **Multimodal Fusion Analysis**: Conduct an ablation study to determine the contribution of each modality (visual, script, combined) to model performance, identifying potential over-reliance on script information
3. **Answerability Detection Reliability**: Evaluate the models' answerability classification accuracy on a diverse set of questions, including those outside the instructional video domain, to assess robustness to different question types