---
ver: rpa2
title: 'New Emerged Security and Privacy of Pre-trained Model: a Survey and Outlook'
arxiv_id: '2411.07691'
source_url: https://arxiv.org/abs/2411.07691
tags:
- attack
- pre-trained
- target
- defense
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively examines the security and privacy risks
  of pre-trained models, proposing a novel taxonomy of attacks and defenses based
  on model accessibility. It categorizes attacks into No-Change, Input-Change, and
  Model-Change approaches, while defenses are classified as No-Change, Input-Change,
  and Model-Change defenses.
---

# New Emerged Security and Privacy of Pre-trained Model: a Survey and Outlook

## Quick Facts
- arXiv ID: 2411.07691
- Source URL: https://arxiv.org/abs/2411.07691
- Reference count: 40
- Authors: Meng Yang; Tianqing Zhu; Chi Liu; WanLei Zhou; Shui Yu; Philip S. Yu

## Executive Summary
This survey provides a comprehensive examination of security and privacy risks in pre-trained models, introducing a novel taxonomy based on model accessibility that categorizes attacks and defenses into No-Change, Input-Change, and Model-Change approaches. The work highlights unique security challenges posed by large pre-trained models, including backdoor attacks, adversarial attacks, jailbreak attacks, and membership inference attacks. By systematically reviewing state-of-the-art techniques and their limitations, the survey identifies critical research gaps and outlines future directions for developing robust defenses against emerging threats.

## Method Summary
The survey conducts a comprehensive literature review of 40 references to examine security and privacy risks in pre-trained models. It proposes a taxonomy of attacks and defenses based on model accessibility (No-Change, Input-Change, Model-Change) and analyzes each category's strengths and limitations. The methodology involves categorizing existing attack/defense techniques, evaluating their effectiveness, and identifying gaps in current research, with a particular focus on challenges unique to large pre-trained models.

## Key Results
- Introduces a novel taxonomy of attacks and defenses based on model accessibility, organizing techniques into No-Change, Input-Change, and Model-Change categories
- Identifies unique security challenges of large pre-trained models including backdoor attacks, adversarial attacks, jailbreak attacks, and membership inference attacks
- Highlights effectiveness of various attack strategies (gradient-based, transfer-based) and defense techniques (adversarial training, input perturbation)
- Emphasizes need for robust defenses against emerging threats and outlines future research directions including attack transferability and machine unlearning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The taxonomy based on model accessibility effectively organizes attack and defense strategies by their operational constraints and threat models.
- **Mechanism:** By categorizing attacks according to whether they modify the model, its input, or neither, the survey creates a structured framework that maps directly to real-world attacker capabilities and defender responses.
- **Core assumption:** Attacker and defender capabilities are best understood through the lens of what they can and cannot modify in the pre-trained model lifecycle.
- **Evidence anchors:** Abstract mentions "proposing a taxonomy of attack and defense methods based on the accessibility of pre-trained models' input and weights" and section details classification into manual-based, query-based, gradient-based, and transfer-based methods.
- **Break condition:** If attackers gain capabilities that transcend current accessibility boundaries, the taxonomy would require significant revision.

### Mechanism 2
- **Claim:** The distinction between small and large pre-trained models is critical because their security properties differ substantially due to scale-related factors.
- **Mechanism:** Large models have more parameters and richer knowledge representations, making them both more powerful and more vulnerable to specific attacks like jailbreak and membership inference.
- **Core assumption:** Model scale fundamentally changes the attack surface and effectiveness of different attack strategies.
- **Evidence anchors:** Abstract discusses "unique security and privacy issues that arise as model size increases" and section defines large pre-trained models as those that can complete multiple tasks with significant size.
- **Break condition:** If future research demonstrates security properties scale linearly with model size rather than exhibiting threshold effects, the current scale-based distinction might need refinement.

### Mechanism 3
- **Claim:** The survey's comprehensive coverage of both attack and defense strategies provides actionable insights for researchers and practitioners.
- **Mechanism:** By documenting specific methods, their effectiveness, and limitations, the survey enables informed decision-making about which defenses to implement against which attacks.
- **Core assumption:** Understanding both offensive and defensive techniques is necessary for developing robust security measures.
- **Evidence anchors:** Abstract mentions "offer a timely and comprehensive review of each category's strengths and limitations" and section discusses summarizing state-of-the-art techniques with benefits and shortcomings.
- **Break condition:** If new attack classes emerge that are not captured by the current taxonomy, the survey's practical utility would diminish until updates are made.

## Foundational Learning

- **Concept:** Threat modeling in machine learning systems
  - Why needed here: The survey relies heavily on understanding different attacker capabilities and defender responses, which requires solid threat modeling foundations.
  - Quick check question: What are the key dimensions used to characterize threats in machine learning security?

- **Concept:** Model lifecycle and fine-tuning processes
  - Why needed here: The survey's taxonomy is built around different stages of pre-trained model development and deployment, requiring understanding of these processes.
  - Quick check question: What are the three main stages in the life process of pre-trained models as defined in the survey?

- **Concept:** Adversarial machine learning fundamentals
  - Why needed here: Many attacks and defenses discussed involve adversarial techniques, requiring understanding of core concepts like perturbations, transferability, and gradient-based optimization.
  - Quick check question: What is the primary goal of adversarial attacks in the context of pre-trained models?

## Architecture Onboarding

- **Component map:** Taxonomy framework (three-tier attack/defense categorization) -> Comprehensive literature review organized by taxonomy categories -> Analysis of strengths and limitations -> Future research direction analysis
- **Critical path:** Understanding the taxonomy → Mapping existing literature to taxonomy categories → Analyzing strengths and limitations → Identifying gaps and future directions
- **Design tradeoffs:** The taxonomy prioritizes accessibility-based categorization over other possible dimensions (like attack goals or model types), which provides clear operational guidance but may obscure some attack pattern similarities
- **Failure signatures:** If new attack types cannot be categorized within the three-tier framework, or if defenses become effective across all accessibility types, the taxonomy may need significant revision
- **First 3 experiments:**
  1. Map three specific attacks (membership inference, jailbreak, backdoor) to the taxonomy's three accessibility categories to verify proper categorization
  2. Test the effectiveness of a No-Change defense (output detection) against a known Input-Change attack to understand practical limitations
  3. Compare security properties of small vs. large models for a specific attack type (adversarial attacks) to validate scale-based distinction

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can we develop scalable and adaptable machine unlearning techniques specifically for large pre-trained models given their growing size and black-box access constraints?
- **Basis in paper:** [explicit] The paper identifies machine unlearning as a critical research direction for large pre-trained models, noting the challenges posed by their billion-parameter scale and increasing reliance on black-box commercial models.
- **Why unresolved:** Current machine unlearning methods are primarily validated on traditional models and struggle with the massive scale and proprietary nature of large pre-trained models. The paper highlights the need for new approaches that can efficiently remove specific training data while maintaining model performance.
- **What evidence would resolve it:** Demonstration of machine unlearning techniques that can successfully remove specific training examples from large pre-trained models (e.g., GPT-4, LLaMA) while maintaining comparable performance on unaffected tasks, validated through rigorous testing across multiple model sizes and architectures.

### Open Question 2
- **Question:** What are the fundamental principles behind attack transferability from open-source models to closed-source commercial models, and how can we predict which attacks will successfully transfer?
- **Basis in paper:** [explicit] The paper emphasizes attack transferability as a critical research direction, noting that current work shows transferability exists but lacks theoretical understanding of why certain attacks transfer successfully between different models.
- **Why unresolved:** While empirical evidence shows attacks can transfer between models, there is no solid theoretical framework explaining the underlying mechanisms or predicting transfer success. The paper calls for deeper understanding of the relationship between attack methods and model characteristics.
- **What evidence would resolve it:** Development of a theoretical framework that can accurately predict attack transferability success rates based on model architecture similarities, training data overlap, and attack method characteristics, validated through extensive cross-model testing.

### Open Question 3
- **Question:** How can we systematically detect and mitigate hallucinations in large pre-trained models while maintaining their powerful generative capabilities?
- **Basis in paper:** [explicit] The paper identifies hallucination detection and mitigation as a crucial research direction, noting that large models often produce content inconsistent with real-world facts or user inputs.
- **Why unresolved:** Current approaches to hallucination detection are fragmented and often trade off between model capability and reliability. The paper suggests that understanding why hallucinations occur is key to developing effective mitigation strategies.
- **What evidence would resolve it:** Development of a comprehensive hallucination detection framework that can accurately identify various types of hallucinations in real-time generation, combined with effective mitigation strategies that don't significantly degrade model performance, validated across multiple model architectures and use cases.

## Limitations
- Potential blind spots in emerging attack classes that may not fit the current taxonomy
- Limited quantitative analysis of defense effectiveness across different model scales
- Possible bias in literature selection affecting comprehensiveness of the survey

## Confidence

**High Confidence:** The taxonomic framework and its theoretical foundations are well-established and provide clear operational guidance for categorizing attacks and defenses.

**Medium Confidence:** Scalability claims and effectiveness assessments are based largely on literature review rather than systematic experimental validation, suggesting practical applicability may vary across different model architectures and scales.

## Next Checks

1. Empirical validation of the taxonomy's utility by categorizing five newly published attack papers not included in the original survey, assessing fit and identifying any categorization gaps.

2. Systematic comparison of defense effectiveness across small and large pre-trained models for three specific attack types (adversarial, backdoor, membership inference) to validate scale-dependent security claims.

3. Implementation and testing of the proposed No-Change defense mechanisms against state-of-the-art Input-Change attacks to evaluate practical limitations of the accessibility-based defense categorization.