---
ver: rpa2
title: 'SPTNet: An Efficient Alternative Framework for Generalized Category Discovery
  with Spatial Prompt Tuning'
arxiv_id: '2403.13684'
source_url: https://arxiv.org/abs/2403.13684
tags:
- prompt
- image
- parameters
- prompts
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SPTNet addresses generalized category discovery (GCD) by introducing
  a two-stage alternating optimization framework that jointly optimizes model parameters
  and spatial prompts. The method proposes spatial prompt tuning (SPT) that learns
  pixel-level prompts for local image regions, enabling better focus on object parts
  transferable between seen and unseen classes.
---

# SPTNet: An Efficient Alternative Framework for Generalized Category Discovery with Spatial Prompt Tuning

## Quick Facts
- arXiv ID: 2403.13684
- Source URL: https://arxiv.org/abs/2403.13684
- Authors: Hongjun Wang; Sagar Vaze; Kai Han
- Reference count: 36
- Primary result: 61.4% average accuracy on SSB benchmark, surpassing prior methods by ~10%

## Executive Summary
SPTNet introduces a two-stage alternating optimization framework for Generalized Category Discovery (GCD) that jointly optimizes model parameters and spatial prompts. The method employs spatial prompt tuning (SPT) that learns pixel-level prompts for local image regions, enabling better focus on object parts transferable between seen and unseen classes. Experiments on seven datasets show SPTNet achieves state-of-the-art performance while introducing only 0.117% extra parameters compared to the ViT-Base backbone.

## Method Summary
SPTNet addresses GCD through a two-stage alternating optimization framework that prevents instability in bilevel optimization. The method uses spatial prompt tuning to attach learnable prompts to local image patches in pixel space, focusing on object parts rather than global context. The training alternates between optimizing data parameters (prompts) and model parameters every k iterations, similar to the EM algorithm. SPTNet also uses learned prompts as data augmentations in a contrastive framework, generating novel views for improved representation learning.

## Key Results
- Achieves 61.4% average accuracy on SSB benchmark, surpassing prior state-of-the-art by approximately 10%
- Introduces only 0.117% extra parameters compared to the ViT-Base backbone
- Demonstrates consistent improvements across seven diverse datasets including CIFAR-10/100, ImageNet-100, CUB, Stanford Cars, FGVC-Aircraft, and Herbarium19

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Spatial Prompt Tuning enables better alignment between pre-trained model and discriminative image regions, facilitating knowledge transfer between seen and unseen classes.
- Mechanism: SPT attaches learnable prompts to local image patches in pixel space, focusing the model's attention on object parts rather than global image context. This local focus helps capture transferable features between classes.
- Core assumption: Object parts are more transferable between classes than global image features.
- Evidence anchors: [abstract] "enabling the method to better focus on object parts, which can transfer between seen and unseen classes." [section 3.3] "a key insight in GCD is that object parts are effective in transferring knowledge between old and new categories."

### Mechanism 2
- Claim: The two-stage alternating optimization framework prevents instability and sub-optimal solutions in bilevel optimization.
- Mechanism: SPTNet alternates between optimizing data parameters (prompts) and model parameters in separate stages, similar to the EM algorithm. This prevents simultaneous updates from destabilizing the optimization process.
- Core assumption: Simultaneous optimization of model and data parameters leads to instability and sub-optimal solutions.
- Evidence anchors: [abstract] "Inspired by the expectation–maximization (EM) algorithm" [section 3.2] "Simultaneously optimizing the model and prompts seems appealing, but it results in instability and sub-optimal solutions."

### Mechanism 3
- Claim: The learned prompts serve as data augmentations that improve representation learning in the contrastive framework.
- Mechanism: The spatial prompts generate novel views of the input image that the contrastive loss can use to enforce invariance to learned transformations, similar to how hand-crafted augmentations work.
- Core assumption: The contrastive framework benefits from diverse augmentations that capture semantic invariances.
- Evidence anchors: [abstract] "the learned prompt can also be used to generate a novel view, making it a suitable choice for the contrastive framework" [section 3.3] "Different from prior works that apply only hand-crafted augmentations, we propose to consider the learnable input as a new type of augmentation"

## Foundational Learning

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: SPTNet uses contrastive learning as its core training objective, so understanding how InfoNCE works is essential for implementing and debugging the method.
  - Quick check question: How does the InfoNCE loss encourage the model to learn invariant representations across different views of the same image?

- Concept: Vision Transformer architecture and patch embeddings
  - Why needed here: SPTNet operates on ViT patches and modifies their embeddings with spatial prompts, so understanding ViT's patch-based processing is crucial.
  - Quick check question: How are image patches transformed into embeddings in a Vision Transformer, and where do the spatial prompts get applied?

- Concept: Generalized Category Discovery problem formulation
  - Why needed here: Understanding the GCD setting (seen vs unseen classes, partial labels) is essential for correctly implementing the training and evaluation procedures.
  - Quick check question: What distinguishes Generalized Category Discovery from standard supervised learning and from Novel Category Discovery?

## Architecture Onboarding

- Component map: Input image → Patchify → Add spatial/global prompts → ViT backbone → CLS token → Projection head → Classification output
- Critical path: Image patches are transformed into embeddings, spatial prompts are added, processed through ViT backbone, CLS token extracted, and passed through projection head for classification
- Design tradeoffs: Prompt size vs. parameter efficiency (larger prompts provide more expressivity but increase parameters), alternating frequency k (too frequent causes instability, too infrequent slows convergence), spatial vs. global prompts (spatial focuses on local features, global captures overall context)
- Failure signatures: Prompts not learning (check gradient flow to prompt parameters and learning rate settings), model not learning (verify that model parameters are being updated in stage two), poor performance on unseen classes (may indicate prompts aren't capturing transferable features)
- First 3 experiments: 1) Baseline test: Run SimGCD without any prompts to establish baseline performance, 2) Spatial prompt ablation: Add only spatial prompts without alternating optimization, 3) Global prompt ablation: Add only global prompts to test their individual contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SPTNet vary when using different pre-trained backbone models, such as DINOv2, CLIP, or other large-scale vision transformers?
- Basis in paper: The paper mentions replacing the DINO backbone with DINOv2 and observing performance improvements, but does not extensively explore other backbones.
- Why unresolved: The paper focuses on DINO and briefly mentions DINOv2, leaving the impact of other backbones unexplored.
- What evidence would resolve it: Systematic experiments comparing SPTNet's performance across various pre-trained backbones on the same datasets.

### Open Question 2
- Question: What is the optimal alternating frequency (k) for different datasets and how does it impact the convergence and final performance of SPTNet?
- Basis in paper: The paper mentions using k=20 iterations as a default choice but does not extensively explore the impact of varying k.
- Why unresolved: The paper does not provide a thorough analysis of how different alternating frequencies affect the model's performance and convergence.
- What evidence would resolve it: Detailed experiments varying k across different datasets and analyzing the resulting performance and convergence patterns.

### Open Question 3
- Question: How does SPTNet perform in scenarios with significant domain shifts or distribution shifts between seen and unseen classes?
- Basis in paper: The paper briefly mentions evaluating SPTNet on DomainNet with domain shifts but does not extensively explore this challenging scenario.
- Why unresolved: The paper's domain shift evaluation is limited to a single dataset and does not provide a comprehensive analysis of SPTNet's robustness to various types of distribution shifts.
- What evidence would resolve it: Extensive experiments on multiple datasets with different types and magnitudes of domain shifts, comparing SPTNet's performance to other methods.

## Limitations
- The core mechanisms rely on empirical claims about object part transferability and optimization stability that lack strong theoretical grounding
- The specific mechanisms of why spatial prompts work better than global prompts are not rigorously established
- The claims about object part transferability and prompt-based augmentations lack theoretical justification

## Confidence
- **High confidence**: The experimental methodology is sound, the SSB benchmark results are reproducible, and the parameter efficiency claim (0.117% extra parameters) is verifiable through code inspection
- **Medium confidence**: The two-stage optimization framework is well-motivated by bilevel optimization literature, and the general approach of prompt-based learning in GCD is reasonable given recent trends in computer vision
- **Low confidence**: The specific mechanisms of why spatial prompts work better than global prompts, and why alternating optimization is necessary rather than simultaneous optimization, are not rigorously established

## Next Checks
1. **Prompt visualization study**: Generate and visualize the learned spatial prompts across different datasets to verify they focus on object parts as claimed. Compare attention maps with and without prompts to quantify the focusing effect.
2. **Optimization ablation**: Implement a simultaneous optimization baseline to directly test whether alternating optimization provides measurable benefits over simultaneous optimization in terms of stability and final performance.
3. **Transferability analysis**: Design experiments to measure how well features from spatial prompts transfer between seen and unseen classes by evaluating prompt performance when applied to held-out class combinations.