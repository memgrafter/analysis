---
ver: rpa2
title: On the Feasibility of In-Context Probing for Data Attribution
arxiv_id: '2407.12259'
source_url: https://arxiv.org/abs/2407.12259
tags:
- data
- training
- inflip
- in-context
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a theoretical and empirical connection between
  in-context probing (ICP) and influence functions for data valuation in large language
  models. The authors show that ICP, which involves prompting an LLM to assess training
  data quality, approximates the gradient-based influence functions used to measure
  training data impact on model outputs.
---

# On the Feasibility of In-Context Probing for Data Attribution

## Quick Facts
- arXiv ID: 2407.12259
- Source URL: https://arxiv.org/abs/2407.12259
- Reference count: 29
- This paper establishes a theoretical and empirical connection between in-context probing (ICP) and influence functions for data valuation in large language models.

## Executive Summary
This paper demonstrates that in-context probing (ICP), a method that uses large language models to assess training data quality through prompting, can serve as a computationally efficient approximation of influence functions for data selection tasks. The authors theoretically show that ICP implicitly performs gradient descent updates on in-context inputs, similar to how influence functions approximate changes in test loss from training data. Empirically, they find strong correlations between ICP and influence function rankings on the Alpaca dataset, with both methods showing comparable performance in fine-tuning experiments when selecting data based on their respective scores.

## Method Summary
The study compares in-context probing (ICP) with influence functions for data valuation on the Alpaca dataset. ICP computes scores by prompting a large language model with in-context examples and measuring likelihood changes, while influence functions calculate gradients (with optional Hessian terms) to approximate training data impact on test loss. The authors bin data by ICP scores, select corresponding samples using influence function rankings, and fine-tune Pythia-1b-deduped on each bin. They evaluate performance using winrate on the Alpaca Eval dataset, comparing against the base Pythia-1b deduped model.

## Key Results
- Strong Spearman correlation of 0.729 between ICP and influence function rankings on Alpaca dataset
- Strong correlation of 0.91 between Hessian-free and full influence functions
- Fine-tuning experiments show comparable performance when using data selected by either ICP or influence functions
- Peak performance achieved at the >0.8 ICP score bin in fine-tuning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ICP approximates gradient-based influence functions by implicitly performing gradient descent on in-context inputs
- Mechanism: Transformer attention heads construct meta-gradients that act like SGD updates on in-context training samples, similar to how influence functions approximate loss changes from training data
- Core assumption: Transformer attention implicitly performs gradient descent updates on in-context inputs (Proposition 1)
- Evidence anchors:
  - [abstract] "theoretically, they demonstrate that ICP implicitly performs gradient descent updates on in-context inputs, similar to how influence functions approximate changes in test loss upon training on specific samples"
  - [section 4] "we connect these two frameworks by showing that they both approximate change in loss on a test task; with ICP taking an 'implicit' gradient descent step on a training sample"
  - [corpus] Weak evidence - only 5 related papers with average FMR 0.414, no citations found
- Break condition: When the implicit gradient descent assumption fails (e.g., different optimization dynamics, non-linear attention mechanisms)

### Mechanism 2
- Claim: ICP and influence functions produce similar rankings for data selection tasks when training and test data share similar characteristics
- Mechanism: Both methods rank training data based on how much they reduce test loss, with ICP using in-context meta-gradients and influence functions using Hessian-free approximations
- Core assumption: Data similarity between training and test sets enables correlation between ranking methods
- Evidence anchors:
  - [abstract] "Empirically, they compare ICP and influence functions (both with and without Hessian terms) on the Alpaca dataset, finding strong Spearman correlations (0.729 between ICP and influence function, 0.91 between Hessian-free and full influence functions)"
  - [section 5] "we observe that in-context probing and gradient-based data valuation methods correlate in their rankings of training data for instruction-following tasks"
  - [corpus] Weak evidence - no direct citations to support data similarity claim
- Break condition: When training and test data characteristics diverge significantly

### Mechanism 3
- Claim: Dropping the Hessian term in influence functions creates a computationally efficient approximation that maintains ranking consistency
- Mechanism: Hessian-free influence functions (InflIP) use only the inner product of gradients, reducing computational cost while preserving ranking order
- Core assumption: Hessian-free approximation maintains good order-consistency with full influence functions
- Evidence anchors:
  - [abstract] "Empirically, they compare ICP and influence functions (both with and without Hessian terms) on the Alpaca dataset, finding strong Spearman correlations (0.729 between ICP and influence function, 0.91 between Hessian-free and full influence functions)"
  - [section 6] "we observed strong correlation (spearman=0.91, p<.05) between the rankings" when comparing InflIP and Infl
  - [corpus] No direct evidence found in corpus
- Break condition: When loss surface curvature becomes critical for ranking decisions

## Foundational Learning

- Concept: Gradient descent and first-order optimization
  - Why needed here: Understanding how both ICP and influence functions approximate changes in loss through gradient-based methods
  - Quick check question: How does a first-order Taylor approximation relate to influence function calculations?

- Concept: Transformer attention mechanisms and in-context learning
  - Why needed here: Understanding how attention heads can implicitly perform gradient descent updates on in-context inputs
  - Quick check question: What is the relationship between attention weights and meta-gradients in in-context learning?

- Concept: Hessian matrices and second-order optimization
  - Why needed here: Understanding why Hessian-free approximations are used and their limitations in large-scale models
  - Quick check question: Why is computing the full Hessian intractable for large language models?

## Architecture Onboarding

- Component map:
  - ICP scoring module: Prompts LLM with in-context examples and measures likelihood changes
  - Influence function calculator: Computes gradients and (optionally) Hessian approximations
  - Data selection pipeline: Ranks training samples based on scores from either method
  - Fine-tuning module: Trains model on selected data subsets

- Critical path: ICP scoring → data ranking → subset selection → fine-tuning → evaluation
- Design tradeoffs: ICP offers computational efficiency but requires LLM access; influence functions provide theoretical grounding but are computationally expensive
- Failure signatures: Poor correlation between ICP and influence function rankings indicates data distribution mismatch or model capacity issues
- First 3 experiments:
  1. Reproduce correlation analysis between ICP and influence function rankings on different datasets
  2. Compare fine-tuning performance using data selected by ICP vs. influence functions
  3. Test Hessian-free vs. full influence function performance on different model sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of ICP as a data valuation method vary significantly across different model sizes and architectures beyond Pythia-1b?
- Basis in paper: [inferred] The authors note that "Our experiments were only conducted on Pythia-1b deduped" and acknowledge that "As model sizes change, the question of whether one data selection method triumphs over the other is an area for exploration."
- Why unresolved: The study's scope was limited to a single model size, and the relationship between model capacity and the effectiveness of ICP versus influence functions remains unexplored.
- What evidence would resolve it: Systematic experiments comparing ICP and influence functions across multiple model sizes (e.g., 7B, 13B, 70B parameters) and architectures (e.g., Llama, Mistral, GPT-style) on various tasks.

### Open Question 2
- Question: How do ICP and influence functions compare in terms of data selection efficiency and effectiveness for pretraining tasks versus fine-tuning tasks?
- Basis in paper: [inferred] The authors state that their "experiments are in the realm of instruction-following tasks" and note that "other types of tasks (e.g., question-answering, summarization) and training settings (e.g., pretraining) should be explored."
- Why unresolved: The current study focuses exclusively on instruction-following tasks during fine-tuning, leaving the broader applicability of ICP across different training paradigms untested.
- What evidence would resolve it: Comparative experiments measuring both the computational efficiency and downstream performance of ICP and influence functions in pretraining settings and across diverse task types.

### Open Question 3
- Question: Under what specific conditions does ICP outperform influence functions in data selection, and vice versa?
- Basis in paper: [explicit] The authors mention that "There may be stages of model training where in-context probing is more beneficial than using influence functions for data selection, and vice versa," suggesting an open question about the relative strengths of each method.
- Why unresolved: While the paper demonstrates that ICP and influence functions are well-correlated in certain settings, it does not identify specific conditions or model training stages where one method clearly outperforms the other.
- What evidence would resolve it: Analysis identifying model characteristics, dataset properties, and training phases where ICP shows superior performance compared to influence functions, and vice versa.

### Open Question 4
- Question: How does the order sensitivity of ICP affect its effectiveness as a data valuation method compared to influence functions?
- Basis in paper: [explicit] The authors note that "these similarity scores have also been observed in models without in-context ability" and "may be weakened when considering order sensitivity (Shen et al., 2024)."
- Why unresolved: The theoretical connection between ICP and influence functions assumes a specific implicit gradient descent mechanism, but the order sensitivity of ICP could affect this connection and its practical effectiveness.
- What evidence would resolve it: Experiments measuring the correlation between ICP and influence functions when varying the order of in-context demonstrations, and analysis of how order sensitivity impacts downstream fine-tuning performance.

### Open Question 5
- Question: Can ICP be effectively used to select groups of training samples rather than individual samples, and how does this compare to group-based influence function methods?
- Basis in paper: [explicit] The authors suggest that "how these two data selection methods compare when selecting groups of training samples is another problem to consider."
- Why unresolved: The current study focuses on individual sample selection, but real-world data curation often requires selecting batches or diverse groups of samples, which may affect the effectiveness of both methods differently.
- What evidence would resolve it: Comparative experiments evaluating ICP and influence functions on group-based data selection tasks, measuring both diversity of selected groups and downstream performance when training on these groups.

## Limitations

- Theoretical connection between ICP and influence functions relies on unproven assumptions about attention mechanisms performing implicit gradient descent
- Empirical validation limited to single dataset (Alpaca) and model family (Pythia), raising generalizability concerns
- Computational efficiency claims for ICP require access to capable LLM, which may be impractical in deployment scenarios

## Confidence

- **High confidence**: The empirical correlation between ICP and influence function rankings (0.729 Spearman) and the computational feasibility of both methods
- **Medium confidence**: The theoretical mechanism linking ICP to implicit gradient descent, as the proof relies on simplifying assumptions about attention dynamics
- **Medium confidence**: The practical utility of ICP for data selection, as fine-tuning results show competitive performance but with room for improvement

## Next Checks

1. **Cross-domain validation**: Test the ICP-influence function correlation on datasets with different characteristics (e.g., code generation, question answering) to assess generalizability beyond instruction-following tasks.

2. **Ablation on attention mechanisms**: Conduct controlled experiments varying attention head counts and layer depths to quantify how attention architecture affects the ICP-influence function relationship.

3. **Real-world deployment simulation**: Implement a cost-benefit analysis comparing ICP and influence functions when accounting for LLM API costs, inference latency, and the impact of data quality on downstream task performance.