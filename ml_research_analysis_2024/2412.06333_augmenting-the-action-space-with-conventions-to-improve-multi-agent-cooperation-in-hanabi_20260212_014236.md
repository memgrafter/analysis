---
ver: rpa2
title: Augmenting the action space with conventions to improve multi-agent cooperation
  in Hanabi
arxiv_id: '2412.06333'
source_url: https://arxiv.org/abs/2412.06333
tags:
- conventions
- player
- agents
- hanabi
- convention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to incorporate human conventions into
  multi-agent reinforcement learning (MARL) through action space augmentation using
  conventions, which are sequences of cooperative actions that span multiple time
  steps and agents. The key insight is that agents must actively opt in to participate
  in a convention for it to reach fruition.
---

# Augmenting the action space with conventions to improve multi-agent cooperation in Hanabi

## Quick Facts
- arXiv ID: 2412.06333
- Source URL: https://arxiv.org/abs/2412.06333
- Reference count: 40
- Authors: F. Bredell; H. A. Engelbrecht; J. C. Schoeman
- One-line primary result: Incorporating human conventions through action space augmentation significantly improves Rainbow agents' performance and cross-play capabilities in 3-5 player Hanabi

## Executive Summary
This paper introduces a novel approach to incorporating human conventions into multi-agent reinforcement learning by augmenting the action space with pre-defined sequences of cooperative actions. The key insight is that conventions span multiple time steps and agents, requiring active participation from all involved agents to reach fruition. The authors test this approach on the Hanabi card game, using established human conventions from the H-Group, and demonstrate significant improvements in training time, converged performance, and cross-play capabilities of Rainbow agents in 3-5 player settings.

## Method Summary
The authors propose augmenting the action space with conventions - sequences of cooperative actions that span multiple time steps and agents. These conventions are pre-defined human-designed strategies from the H-Group that are incorporated into the agent's decision-making process. The key mechanism is that agents must actively opt in to participate in a convention for it to reach fruition. This approach is implemented within the Rainbow DQN framework, where convention actions are added to the existing action space alongside standard actions. During training, agents learn to select between conventional and non-conventional actions based on their expected utility.

## Key Results
- Conventions significantly improve training time and converged performance of Rainbow agents in 3-5 player Hanabi
- Cross-play performance improves, allowing agents to achieve decent performance across various agent pairings
- The approach demonstrates the potential of conventions to enhance MARL in partially observable environments with limited communication

## Why This Works (Mechanism)
The approach works by providing agents with pre-learned cooperative strategies that span multiple time steps and involve multiple agents. By augmenting the action space with these conventions, agents can leverage human-designed cooperative behaviors that have been proven effective in the Hanabi community. The opt-in mechanism ensures that conventions are only executed when all involved agents agree, maintaining flexibility while providing structured cooperation opportunities.

## Foundational Learning
- **Rainbow DQN**: A multi-headed deep Q-network architecture that combines multiple DQN extensions. Needed to provide a strong baseline agent architecture. Quick check: Verify the implementation includes all seven Rainbow components (double Q-learning, prioritized replay, dueling networks, etc.).
- **Hanabi game mechanics**: A cooperative card game with imperfect information and limited communication. Needed as the test environment for evaluating multi-agent cooperation. Quick check: Confirm understanding of game rules, scoring, and communication tokens.
- **H-Group conventions**: Established human-designed strategies for Hanabi cooperation. Needed as the source of effective cooperative behaviors to incorporate. Quick check: Verify which specific conventions are used and their typical effectiveness.
- **Cross-play evaluation**: Testing performance across different agent types and conventions. Needed to assess robustness and generalization of the approach. Quick check: Ensure evaluation includes multiple agent pairings and convention combinations.

## Architecture Onboarding
Component Map: Game State -> Agent Observation -> Augmented Action Space (Standard + Convention Actions) -> Rainbow DQN -> Action Selection -> Game Execution

Critical Path:
1. Game state and player observations are generated
2. Agent receives observation and accesses augmented action space
3. Rainbow DQN processes observation and evaluates all available actions
4. Agent selects action (conventional or standard) based on learned Q-values
5. Convention actions require coordination from all involved agents
6. Game state is updated and cycle repeats

Design Tradeoffs:
- Action space augmentation vs. maintaining exploration capabilities
- Convention complexity vs. agent coordination requirements
- Pre-defined conventions vs. learned cooperative strategies
- Cross-play performance vs. convention-specific optimization

Failure Signatures:
- Poor convention adoption rates indicating coordination difficulties
- Performance degradation in cross-play scenarios
- Computational overhead from larger action spaces
- Convergence issues when combining conventional and standard actions

First Experiments:
1. Test convention adoption rates across different team compositions
2. Measure performance impact of adding single vs. multiple conventions
3. Evaluate computational overhead and sample efficiency changes

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- Generalizability to other cooperative multi-agent tasks beyond Hanabi remains uncertain
- Reliance on human-designed conventions raises questions about automatic convention discovery
- Limited discussion of computational overhead and sample efficiency impact
- No exploration of convention adherence vs. flexibility trade-offs in dynamic scenarios

## Confidence
- High confidence: The effectiveness of conventions in improving Hanabi performance with Rainbow agents
- Medium confidence: The cross-play performance improvements across various agent pairings
- Medium confidence: The general principle of action space augmentation for incorporating conventions

## Next Checks
1. Evaluate the approach on a diverse set of cooperative multi-agent tasks beyond Hanabi to assess generalizability
2. Conduct ablation studies to determine the impact of convention complexity and length on performance and computational efficiency
3. Test the robustness of convention-based agents against adversarial or non-cooperative opponents to assess real-world applicability