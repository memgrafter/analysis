---
ver: rpa2
title: 'Faithful Attention Explainer: Verbalizing Decisions Based on Discriminative
  Features'
arxiv_id: '2405.13032'
source_url: https://arxiv.org/abs/2405.13032
tags:
- attention
- explanations
- human
- faithful
- enforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Faithful Attention Explainer (FAE), a framework
  for generating textual explanations faithful to the discriminative features used
  by a classifier. FAE uses an attention module that takes visual feature maps from
  the classifier to generate explanations, and employs a novel attention enforcement
  module to align attention weights with generated words.
---

# Faithful Attention Explainer: Verbalizing Decisions Based on Discriminative Features

## Quick Facts
- arXiv ID: 2405.13032
- Source URL: https://arxiv.org/abs/2405.13032
- Reference count: 30
- Primary result: Novel framework generating faithful textual explanations aligned with classifier discriminative features

## Executive Summary
This paper introduces Faithful Attention Explainer (FAE), a framework that generates textual explanations faithful to the discriminative features used by a classifier. FAE uses an attention module that takes visual feature maps from the classifier to generate explanations, and employs a novel attention enforcement module to align attention weights with generated words. The method achieves strong performance on caption quality metrics (BLEU-4, METEOR, CIDer) and a faithfulness metric (FER) on two datasets (CUB and ACT-X). Additionally, FAE demonstrates the ability to interpret gaze-based human attention, showing potential for human-AI interaction.

## Method Summary
FAE follows an Encoder-Decoder framework that takes an image and predicts the class label while generating a textual explanation. The encoder extracts visual feature vectors from different layers of the input image using a classifier network. The attention model computes attention maps based on the decoder's hidden state and the feature vector from the encoder. The attention alignment module uses future knowledge (words) to adjust the attention map for the current word. The model is trained using a combination of standard sentence quality metrics and L1 norm regularization between newly grounded attention weights and those generated by the attention model.

## Key Results
- Achieves strong performance on caption quality metrics (BLEU-4, METEOR, CIDer)
- Demonstrates high faithfulness metric (FER) scores on CUB and ACT-X datasets
- Successfully interprets gaze-based human attention for explanation generation
- Shows potential for human-AI interaction through attention enforcement mechanism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The attention module generates word-level attention maps that align with classifier feature maps
- Mechanism: The attention model takes classifier feature maps and uses the decoder's hidden state to compute attention weights, aligning word generation with image regions important for classification
- Core assumption: The classifier's feature maps contain discriminative information relevant to the classification decision
- Evidence anchors: Abstract states attention module uses visual feature maps from classifier; section 3 describes feature extraction from classifier layers
- Break condition: If classifier feature maps don't contain discriminative information, or if the attention model cannot properly ground words to these features

### Mechanism 2
- Claim: Attention enforcement aligns extrinsic attention maps (GradCAM, human gaze) with generated explanations
- Mechanism: The attention enforcement module takes external attention maps and uses them to guide focus features during inference, ensuring generated text focuses on specified regions
- Core assumption: External attention maps highlight discriminative features relevant to the decision
- Evidence anchors: Abstract mentions attention enforcement for attention explanation; section 3 describes replacing attention weights with external attention maps
- Break condition: If external attention maps don't correlate with discriminative features, or if enforcement mechanism fails to properly guide model

### Mechanism 3
- Claim: BiLSTM-based attention alignment improves grounding by incorporating future word context
- Mechanism: A Bidirectional LSTM encodes the generated sequence, and the attention model regenerates attention weights based on concatenated forward and backward hidden states
- Core assumption: Future word context improves grounding of current words to image regions
- Evidence anchors: Section 3 introduces attention alignment using BiLSTM; section 2 mentions incorporating future words for grounding
- Break condition: If BiLSTM fails to capture relevant future context, or if attention model cannot effectively use this information

## Foundational Learning

- Concept: Visual attention mechanisms in image captioning
  - Why needed here: FAE builds on attention mechanisms used in image captioning but adapts them for faithful explanation generation
  - Quick check question: How does the attention mechanism in FAE differ from standard image captioning attention models?

- Concept: GradCAM and saliency map generation
  - Why needed here: FAE uses GradCAM as an extrinsic attention source and validates faithfulness using saliency maps
  - Quick check question: What is the difference between GradCAM and other saliency map methods, and why is it suitable for FAE?

- Concept: Bidirectional LSTM (BiLSTM) for sequence modeling
  - Why needed here: FAE uses BiLSTM for attention alignment to incorporate future word context
  - Quick check question: How does a BiLSTM differ from a standard LSTM, and what advantage does it provide for attention alignment?

## Architecture Onboarding

- Component map: Î£ (Encoder) -> fAatt (Attention Model) -> LSTM (Decoder) -> BiLSTM (Attention Alignment) -> Attention Enforcement Module

- Critical path:
  1. Extract feature maps from classifier
  2. Compute attention weights and focus features
  3. Generate word sequence
  4. Align attention with future context
  5. Apply attention enforcement if using external maps

- Design tradeoffs:
  - Using classifier feature maps vs. separate encoder: Ensures explanations are faithful to classifier decisions but may limit caption quality
  - Attention enforcement vs. standard attention: Improves faithfulness to specific regions but may reduce caption fluency
  - BiLSTM attention alignment: Improves grounding but adds computational complexity

- Failure signatures:
  - Explanations not faithful to classifier decisions: Check if feature maps contain discriminative information
  - Poor caption quality: Check attention model training and alignment module effectiveness
  - Attention enforcement not working: Verify external attention maps are properly aligned with feature map dimensions

- First 3 experiments:
  1. Train FAE without attention enforcement on CUB dataset and evaluate BLEU-4 and METEOR scores
  2. Apply GradCAM attention enforcement on trained FAE and measure FER score improvement
  3. Test FAE with human gaze attention on CUB-GHA dataset and qualitatively evaluate explanation faithfulness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the faithfulness of textual explanations generated by FAE be further improved beyond the current Attention Enforcement mechanism?
- Basis in paper: [explicit] The paper mentions that the current FER scores can be improved by using Attention Enforcement, but does not explore other methods
- Why unresolved: The paper does not investigate alternative approaches to improve faithfulness beyond Attention Enforcement
- What evidence would resolve it: Experimental results comparing FAE with other faithfulness improvement methods, such as using different attention mechanisms or incorporating additional constraints during training

### Open Question 2
- Question: Can FAE be extended to generate explanations for other types of models beyond image classifiers, such as object detection or segmentation models?
- Basis in paper: [inferred] The paper focuses on image classification tasks and does not explore the applicability of FAE to other computer vision tasks
- Why unresolved: The paper does not investigate the generalization of FAE to other model types or tasks
- What evidence would resolve it: Experiments demonstrating the effectiveness of FAE in generating faithful explanations for object detection or segmentation models on relevant datasets

### Open Question 3
- Question: How does the quality of explanations generated by FAE compare to those generated by large language models like GPT-4 when given the same input and attention maps?
- Basis in paper: [explicit] The paper mentions testing GPT-4 with original images and saliency maps but notes that GPT-4 sometimes hallucinates and misidentifies areas of focus
- Why unresolved: The paper does not provide a direct comparison between FAE and GPT-4 in terms of explanation quality and faithfulness
- What evidence would resolve it: A comprehensive evaluation comparing the explanations generated by FAE and GPT-4 using human judgment or automated metrics that assess both linguistic quality and faithfulness to the input attention maps

## Limitations

- No ablation studies on the attention enforcement module's effectiveness
- Limited qualitative analysis of generated explanations
- No investigation of failure cases or model robustness
- The FER metric, while novel, may not fully capture the complexity of faithful explanations

## Confidence

- **High confidence**: The core concept of using classifier feature maps for faithful explanation generation is sound and well-supported by the literature. The basic architecture description is clear.
- **Medium confidence**: The attention enforcement mechanism is plausible given existing work, but the specific implementation details matter significantly for effectiveness. The use of BiLSTM for attention alignment is well-established, but its specific application here needs verification.
- **Medium confidence**: The quantitative results are promising but rely on metrics that may not fully capture faithfulness. The comparison with baseline methods is useful but could be strengthened with more rigorous statistical analysis.

## Next Checks

1. Implement ablation study comparing FAE with and without attention enforcement on CUB dataset, measuring both caption quality (BLEU-4, METEOR) and faithfulness (FER).

2. Conduct qualitative analysis of 20 randomly selected explanations from CUB test set, comparing attention maps with generated words to identify systematic grounding failures.

3. Test model performance on out-of-distribution images (e.g., bird species not in training set) to assess generalization of faithfulness claims.