---
ver: rpa2
title: Chinese Metaphor Recognition Using a Multi-stage Prompting Large Language Model
arxiv_id: '2408.09177'
source_url: https://arxiv.org/abs/2408.09177
tags:
- metaphor
- language
- llms
- prompt
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of identifying and understanding
  Chinese metaphors, specifically recognizing tenors, vehicles, and grounds when these
  elements are not explicitly stated in the metaphor. Existing methods struggle with
  metaphors lacking explicit tenors or vehicles.
---

# Chinese Metaphor Recognition Using a Multi-stage Prompting Large Language Model

## Quick Facts
- arXiv ID: 2408.09177
- Source URL: https://arxiv.org/abs/2408.09177
- Reference count: 33
- Primary result: Achieved 1st place in most tracks of NLPCC-2024 Shared Task 9 with accuracy up to 0.979 in subtask1_track2 and 0.951 in subtask2_track1

## Executive Summary
This paper addresses the challenge of Chinese metaphor recognition, specifically identifying tenors, vehicles, and grounds in metaphors where these elements are not explicitly stated. The authors propose a multi-stage generative heuristic-enhanced prompt framework that combines a small DeBERTa model for candidate generation with a large language model for final recognition. The approach significantly outperforms existing methods on the NLPCC-2024 Shared Task 9 benchmark, achieving state-of-the-art results in multiple tracks.

## Method Summary
The proposed framework employs a two-stage approach: first, a DeBERTa model generates answer candidates for metaphor recognition tasks; second, these candidates are used to cluster questions and sample demonstrations to create heuristic-enhanced prompts for the large language model. This multi-stage prompting strategy leverages the strengths of both smaller, efficient models for candidate generation and larger, more capable models for final inference, addressing the challenge of recognizing implicit metaphor components in Chinese text.

## Key Results
- Achieved 1st place in most tracks of NLPCC-2024 Shared Task 9
- Reached accuracy of 0.979 in subtask1_track2 (ground recognition)
- Reached accuracy of 0.951 in subtask2_track1 (vehicle recognition)
- Demonstrated superior performance compared to existing metaphor recognition methods

## Why This Works (Mechanism)
The multi-stage framework works by first narrowing down the search space through DeBERTa's candidate generation, which reduces the complexity for the LLM. The heuristic-enhanced prompts leverage these candidates to create more focused and contextually relevant demonstrations, improving the LLM's ability to recognize implicit metaphor components. This approach combines the efficiency of smaller models with the reasoning capabilities of larger models, creating a synergistic effect that enhances overall performance.

## Foundational Learning
1. **Metaphor recognition fundamentals** - Why needed: Understanding the three components (tenor, vehicle, ground) is crucial for evaluating metaphor recognition systems. Quick check: Can identify these components in simple metaphors like "time is money" (tenor: time, vehicle: money, ground: value/limited resource).
2. **Chinese language processing challenges** - Why needed: Chinese lacks explicit morphological markers, making metaphor recognition more challenging than in languages with clear word boundaries. Quick check: Recognize that "心痛" (heart pain) metaphorically means emotional distress rather than literal cardiac pain.
3. **Multi-stage prompting techniques** - Why needed: Understanding how to combine multiple model generations improves overall system performance. Quick check: Can explain how candidate generation reduces the search space for subsequent reasoning models.

## Architecture Onboarding

**Component map:** DeBERTa -> Candidate Clustering -> LLM with Heuristic Prompts -> Metaphor Recognition Output

**Critical path:** Input text → DeBERTa candidate generation → Question clustering → Demonstration sampling → LLM inference → Final recognition

**Design tradeoffs:** The approach balances computational efficiency (using DeBERTa for initial candidate generation) with recognition accuracy (using LLM for final inference), but at the cost of increased complexity and potential latency from the two-stage process.

**Failure signatures:** Performance degradation occurs when metaphors are highly unconventional or when the DeBERTa model fails to generate relevant candidates, leading to poor demonstration quality for the LLM.

**First experiments to run:** 1) Test DeBERTa's candidate generation accuracy in isolation, 2) Evaluate LLM performance with and without heuristic-enhanced prompts, 3) Measure end-to-end latency of the two-stage framework.

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow focus on Chinese language metaphors without validation on other languages
- Heavy reliance on competition benchmarks raises questions about real-world applicability
- No discussion of computational efficiency or scalability concerns for the two-stage framework
- Potential sensitivity to prompt engineering quality and training data representation

## Confidence
- High: Reported competition results (1st place in most tracks, accuracy up to 0.979 and 0.951) appear reliable given the structured evaluation framework of NLPCC-2024 Shared Task 9
- Medium: The two-stage framework combining DeBERTa with LLM for metaphor recognition is methodologically sound, though specific implementation details could affect reproducibility
- Low: Claim that this approach significantly improves metaphor recognition accuracy compared to existing methods lacks direct comparative analysis with baseline approaches

## Next Checks
1. Test the framework on non-Chinese languages and diverse metaphor types to assess cross-linguistic generalizability and robustness to different metaphorical expressions
2. Conduct ablation studies removing either the DeBERTa candidate generation stage or the heuristic-enhanced prompting to quantify the contribution of each component to overall performance
3. Evaluate the computational cost and inference time of the two-stage framework compared to single-model approaches to determine practical deployment feasibility