---
ver: rpa2
title: What Really is Commonsense Knowledge?
arxiv_id: '2411.03964'
source_url: https://arxiv.org/abs/2411.03964
tags:
- knowledge
- commonsense
- referenced
- datasets
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study addresses the ambiguity in defining commonsense knowledge\
  \ in NLP benchmarks, which can undermine the evaluation of models' true reasoning\
  \ abilities. The authors consolidate existing definitions using three frameworks\u2014\
  ideal, feature, and exemplar\u2014and propose a multi-aspect, binary-value feature\
  \ list to distinguish commonsense from referenced knowledge."
---

# What Really is Commonsense Knowledge?

## Quick Facts
- arXiv ID: 2411.03964
- Source URL: https://arxiv.org/abs/2411.03964
- Reference count: 22
- Primary result: 27-56% of instances in CommonsenseQA datasets are non-commonsense, and LLMs perform 4-7% worse on commonsense-knowledge subsets

## Executive Summary
This study addresses the ambiguity in defining commonsense knowledge within NLP benchmarks, which can undermine the evaluation of models' true reasoning abilities. The authors consolidate existing definitions using three frameworks—ideal, feature, and exemplar—and propose a multi-aspect, binary-value feature list to distinguish commonsense from referenced knowledge. Through expert annotation and regression analysis on CommonsenseQA and CommonsenseQA 2.0, they identify two key features: whether knowledge can be obtained through personal experience and whether it relies on mutual belief. Their findings show that a substantial portion of current commonsense reasoning datasets contains non-commonsense content, and that LLMs perform notably worse on genuine commonsense tasks compared to referenced-knowledge tasks.

## Method Summary
The authors survey existing definitions of commonsense knowledge and consolidate them using three frameworks: ideal (conceptual definitions), feature (distinguishing characteristics), and exemplar (representative examples). They propose a multi-aspect, binary-value feature list to differentiate commonsense from referenced knowledge, then conduct expert annotation on CommonsenseQA and CommonsenseQA 2.0 datasets to validate feature significance. Using regression analysis, they identify two key features that distinguish commonsense knowledge: personal experience and mutual belief. They analyze the portion of non-commonsense instances in the datasets and evaluate LLMs' performance on commonsense-knowledge versus referenced-knowledge subsets to assess the impact of knowledge type on model performance.

## Key Results
- 27-56% of instances in CommonsenseQA and CommonsenseQA 2.0 are classified as non-commonsense
- LLMs perform 4-7% worse on commonsense-knowledge subsets compared to referenced-knowledge subsets
- Two key features distinguish commonsense from referenced knowledge: personal experience and mutual belief
- Current commonsense reasoning benchmarks contain substantial non-commonsense content

## Why This Works (Mechanism)
The study works by establishing a systematic framework to categorize knowledge types based on definitional properties. By using the three-framework approach (ideal, feature, exemplar), the authors create a structured method for consolidating diverse definitions of commonsense knowledge. The binary feature list enables clear classification of knowledge instances, while expert annotation provides empirical validation. The performance gap between commonsense and referenced-knowledge subsets demonstrates that true commonsense reasoning presents distinct challenges compared to memory retrieval or fact lookup.

## Foundational Learning
- **Three-framework consolidation** (ideal, feature, exemplar): Why needed - To systematically unify diverse definitions of commonsense knowledge; Quick check - Can you map each definition to its appropriate framework?
- **Binary feature classification**: Why needed - To create a clear, actionable taxonomy for distinguishing knowledge types; Quick check - Can you identify which features apply to a given knowledge instance?
- **Expert annotation methodology**: Why needed - To validate the proposed feature list with human judgment; Quick check - Can you explain the criteria used for expert annotation?
- **Regression analysis for feature significance**: Why needed - To statistically identify which features most reliably distinguish commonsense knowledge; Quick check - Can you describe how regression identifies significant features?

## Architecture Onboarding

**Component Map**: Knowledge definitions (ideal, feature, exemplar) -> Feature list consolidation -> Expert annotation -> Regression analysis -> Dataset classification -> LLM performance evaluation

**Critical Path**: The essential flow is from framework consolidation through expert annotation to feature identification, then dataset classification, and finally performance evaluation. Each step builds on the previous one to establish both the taxonomy and its empirical validation.

**Design Tradeoffs**: The binary feature approach prioritizes simplicity and clarity over nuanced gradation, which may oversimplify complex knowledge distinctions. The focus on two key features trades comprehensiveness for practical applicability, potentially missing subtler aspects of commonsense knowledge.

**Failure Signatures**: Incorrect framework consolidation leads to an inadequate feature list; insufficient expert annotation produces unreliable validation; binary classification may miss intermediate cases; performance differences could be confounded by factors other than knowledge type.

**First Experiments**: 1) Apply the feature list to additional commonsense datasets to test generalizability; 2) Conduct ablation studies removing each feature to assess impact on classification accuracy; 3) Compare human performance on commonsense vs. referenced-knowledge subsets to establish baseline expectations

## Open Questions the Paper Calls Out
None

## Limitations
- The binary feature approach may oversimplify the nuanced nature of knowledge distinctions
- Only two key features were identified, potentially representing an incomplete characterization of commonsense knowledge
- The framework consolidation methodology remains underspecified, limiting reproducibility
- Performance differences could be influenced by uncontrolled factors beyond knowledge type

## Confidence

**Confidence in non-commonsense content claim**: Medium
- Expert annotation provides reasonable support
- Binary feature approach may not capture all relevant distinctions

**Confidence in LLM performance difference**: Medium
- Performance gap is statistically meaningful
- Methodology for isolating knowledge subsets could be more rigorously validated

**Confidence in feature list comprehensiveness**: Low
- Feature list represents a starting point rather than definitive framework
- Acknowledged as potentially incomplete characterization

## Next Checks

1. **Feature Validation Expansion**: Test the binary feature list on additional commonsense reasoning datasets beyond CommonsenseQA to assess generalizability and identify potential missing features.

2. **Model Performance Verification**: Conduct controlled experiments with multiple LLMs on the identified commonsense vs. referenced subsets, ensuring that performance differences are not attributable to other dataset characteristics like question complexity or answer length.

3. **Framework Implementation Detail**: Request or reconstruct the exact methodology for consolidating the three frameworks (ideal, feature, exemplar) to enable independent verification of the feature list derivation process.