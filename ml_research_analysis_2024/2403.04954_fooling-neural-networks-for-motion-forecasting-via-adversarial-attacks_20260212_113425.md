---
ver: rpa2
title: Fooling Neural Networks for Motion Forecasting via Adversarial Attacks
arxiv_id: '2403.04954'
source_url: https://arxiv.org/abs/2403.04954
tags:
- adversarial
- attacks
- motion
- human
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores adversarial attacks on neural networks for
  human motion prediction, an area previously unexplored in multi-regression tasks
  like GCNs and MLP-based architectures. The authors apply gradient-based attacks
  (FGSM, I-FGSM, MIFGSM, and DeepFool) and 3D spatial transformations (rotations,
  translations, scaling) to state-of-the-art models (STS-GCN, PGBIG, MotionMixer,
  siMLPe, HRI, MMA) on the Human3.6M dataset.
---

# Fooling Neural Networks for Motion Forecasting via Adversarial Attacks

## Quick Facts
- arXiv ID: 2403.04954
- Source URL: https://arxiv.org/abs/2403.04954
- Reference count: 5
- Primary result: Motion forecasting models are vulnerable to adversarial attacks and 3D spatial transformations, with pose displacement inputs showing greater robustness

## Executive Summary
This paper explores adversarial attacks on neural networks for human motion prediction, a previously unexplored area for multi-regression tasks like GCNs and MLP-based architectures. The authors apply gradient-based attacks (FGSM, I-FGSM, MIFGSM, and DeepFool) and 3D spatial transformations (rotations, translations, scaling) to state-of-the-art models on the Human3.6M dataset. They find that models are susceptible to attacks even with low perturbation levels, with MotionMixer showing the most robustness due to its use of pose displacements as input. Spatial transformations also significantly affect model performance, with STS-GCN being particularly sensitive. The results highlight the vulnerability of motion forecasting models to adversarial attacks and 3D transformations, similar to earlier CNN models in image classification.

## Method Summary
The paper applies white-box adversarial attacks (FGSM, I-FGSM, MIFGSM, DeepFool) and 3D spatial transformations to pre-trained human motion prediction models on the Human3.6M dataset. The attacks use ε values of 0.001 and 0.01 with 10 iterations, and MIFGSM uses a momentum parameter of 0.4. Spatial transformations include rotations (0-360° Y-axis), translations (±0.2 X-axis), and scaling (0.7-1.3 X/Y-axis). Performance is measured using MPJPE (Mean Per Joint Position Error) and ∆s (Euclidean distance between adversarial and real examples). The dataset is split into training (subjects 1,6,7,8,9), validation (subject 11), and testing (subject 5) sets.

## Key Results
- Motion forecasting models are susceptible to adversarial attacks even with low perturbation levels
- MotionMixer shows the most robustness due to using pose displacements as input
- 3D spatial transformations significantly affect model performance, with STS-GCN being particularly sensitive
- MPJPE increases substantially even with minimal perturbation magnitudes (∆s)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Motion forecasting models are vulnerable to gradient-based adversarial perturbations even with low epsilon values.
- Mechanism: Small perturbations added along the gradient direction of the loss function cause significant deviations in predicted human poses.
- Core assumption: The models' output space is locally smooth enough for gradient-based methods to find effective adversarial examples.
- Evidence anchors:
  - [abstract] "models are susceptible to attacks even on low levels of perturbation"
  - [section] "FGSM finds an adversarial example X* that maximizes the loss function L(θ, x, y)"
  - [corpus] Weak - corpus papers focus on images/classification, not regression tasks like pose forecasting
- Break condition: If the model uses robust input representations (e.g., pose displacements) that make the output less sensitive to small input changes.

### Mechanism 2
- Claim: 3D spatial transformations (rotations, translations, scaling) act as effective adversarial attacks on motion forecasting models.
- Mechanism: Geometric transformations that preserve joint distances can still cause large prediction errors because the model interprets the transformed input as a different pose sequence.
- Core assumption: The model has not been trained with sufficient data augmentation to handle arbitrary 3D spatial transformations.
- Evidence anchors:
  - [abstract] "most models are sensitive to simple rotations and translations which do not alter joint distances"
  - [section] "we explore geometric transformations applied to 3D data points... to modify the pose sequences"
  - [corpus] Weak - corpus focuses on point clouds but not specifically on human pose forecasting
- Break condition: If the model uses relative pose representations (like displacements) that are invariant to global 3D transformations.

### Mechanism 3
- Claim: Models using pose positions as input are more vulnerable to attacks than models using pose displacements.
- Mechanism: Pose positions are absolute and thus sensitive to transformations, while displacements capture relative changes between frames and are more robust.
- Core assumption: The transformation from positions to displacements reduces the input space's sensitivity to adversarial perturbations.
- Evidence anchors:
  - [section] "MotionMixer stands out as the most robust model... due to the utilization of pose displacements as input data"
  - [section] "by incorporating changes in pose positions between consecutive frames as input, we increase the robustness of the model"
  - [corpus] Weak - corpus does not contain relevant evidence about pose displacement robustness
- Break condition: If the attack methods are adapted to target the displacement representation directly rather than the position representation.

## Foundational Learning

- Concept: Adversarial attacks on neural networks
  - Why needed here: Understanding how gradient-based methods like FGSM and DeepFool work is essential to comprehend the attack methodology
  - Quick check question: What is the key difference between FGSM and I-FGSM in terms of perturbation application?
- Concept: 3D geometric transformations and their effect on human pose data
  - Why needed here: Spatial transformations are used as a form of attack, so understanding their mathematical representation and impact is crucial
  - Quick check question: Why do rotations and translations that preserve joint distances still affect model performance?
- Concept: Graph Convolutional Networks (GCNs) and their application to human motion prediction
  - Why needed here: Many state-of-the-art models in this domain are GCN-based, so understanding their architecture is important for interpreting results
  - Quick check question: How does a GCN process the spatial relationships between human joints in a pose sequence?

## Architecture Onboarding

- Component map: Input preprocessing (DCT transform or pose displacements) → Encoder (GCN layers or MLP) → Decoder (MLP layers) → Output pose sequence
- Critical path: Input → Encoder → Output prediction (this is where the adversarial perturbation is measured)
- Design tradeoffs: Position-based input gives more direct information but is more vulnerable to transformations; displacement-based input is more robust but may lose some absolute pose information
- Failure signatures: Large increase in MPJPE metric without proportional increase in input perturbation magnitude (∆s)
- First 3 experiments:
  1. Apply FGSM attack with ε=0.001 to STS-GCN and measure MPJPE increase and ∆s
  2. Apply rotation transformation (e.g., 90 degrees around Y-axis) to input and measure MPJPE change for MotionMixer vs. STS-GCN
  3. Train a simple MLP model with displacement input and test its robustness against the same FGSM attack parameters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do gradient-based adversarial attacks perform on human motion prediction models when applied to out-of-distribution data or more diverse datasets beyond Human3.6M?
- Basis in paper: [inferred] The paper mentions that the authors encountered challenges with applying the attacks to AMASS and 3DPW datasets due to the lack of pre-trained models, but they did evaluate STS-GCN and MotionMixer on these datasets with adversarial attacks. The paper also mentions the need for more realistic scenarios and black-box methods as future directions.
- Why unresolved: The experiments were primarily conducted on the Human3.6M dataset, and the evaluation on AMASS and 3DPW was limited to only two models. The performance of various attack methods on a broader range of models and datasets remains unexplored.
- What evidence would resolve it: Conducting comprehensive experiments with gradient-based attacks on a diverse set of human motion prediction models across multiple datasets, including AMASS and 3DPW, and comparing the results to the Human3.6M dataset.

### Open Question 2
- Question: What is the impact of using adversarial attacks as a data augmentation technique during the training phase of human motion prediction models?
- Basis in paper: [explicit] The authors mention that they plan to explore using adversarial attacks as data augmentation for more realistic scenarios in future work.
- Why unresolved: The paper does not investigate the effects of incorporating adversarial examples into the training process. It is unclear whether this approach would improve the robustness and generalization of the models.
- What evidence would resolve it: Training human motion prediction models with adversarial examples as part of the data augmentation strategy and evaluating their performance on both clean and adversarial test data compared to models trained without such augmentation.

### Open Question 3
- Question: How effective are black-box adversarial attacks on human motion prediction models compared to white-box attacks, and what are the key differences in their impact on model performance?
- Basis in paper: [explicit] The authors state that they plan to explore black-box methods in future work, as they observed that white-box attacks worked successfully and also considered using gradients to guide 3D spatial transformations.
- Why unresolved: The paper only investigates white-box attacks, and the potential of black-box attacks on human motion prediction models remains unexplored. The differences in attack success rates, required perturbation levels, and impact on model performance between white-box and black-box attacks are unknown.
- What evidence would resolve it: Conducting experiments with various black-box attack methods on human motion prediction models and comparing their effectiveness and impact on model performance to white-box attacks.

## Limitations
- Limited evaluation across diverse motion datasets beyond Human3.6M
- The transferability of attacks to real-world scenarios remains unclear
- Robustness gains from pose displacements may not generalize to more complex attack strategies

## Confidence
- **High Confidence**: Claims about model vulnerability to gradient-based attacks with low epsilon values
- **Medium Confidence**: Claims about 3D transformation sensitivity and pose displacement robustness
- **Medium Confidence**: Claims about MotionMixer's superior robustness

## Next Checks
1. Test attack transferability by evaluating black-box scenarios where the attacker has limited knowledge of model architecture and parameters
2. Validate findings on additional motion datasets (e.g., AMASS, 3DPW) to assess generalizability across different motion capture systems and environments
3. Implement adaptive attack strategies specifically designed to target pose displacement representations to verify the claimed robustness mechanism