---
ver: rpa2
title: Revisiting Differentially Private Hyper-parameter Tuning
arxiv_id: '2402.13087'
source_url: https://arxiv.org/abs/2402.13087
tags:
- privacy
- algorithm
- private
- bound
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the privacy of hyper-parameter tuning, where
  multiple differentially private algorithms are run and the best result is selected.
  While recent work [30, 42] shows this process incurs only a small constant-factor
  privacy overhead, the tightness of these bounds was unclear.
---

# Revisiting Differentially Private Hyper-parameter Tuning

## Quick Facts
- arXiv ID: 2402.13087
- Source URL: https://arxiv.org/abs/2402.13087
- Authors: Zihang Xiang; Tianhao Wang; Chenglong Wang; Di Wang
- Reference count: 40
- Primary result: Improved privacy analysis for hyper-parameter tuning using f-DP framework shows tighter bounds than previous RDP-based methods

## Executive Summary
This paper examines the privacy leakage in differentially private hyper-parameter tuning, where multiple DP-SGD runs are executed and the best result is selected. While prior work showed this process incurs only a small constant-factor privacy overhead, the tightness of these bounds was unclear. The authors provide both empirical and theoretical analysis showing that practical privacy leakage remains significantly below theoretical worst-case bounds. They improve the privacy analysis by using f-DP instead of (ε, δ)-DP or Rényi DP, leading to tighter bounds that are more generalizable across parameter settings.

## Method Summary
The paper combines theoretical analysis with empirical auditing to examine privacy leakage in DP hyper-parameter tuning. The core method uses f-DP to represent privacy with finer resolution than previous approaches, leading to improved privacy accounting. The empirical component involves running privacy audits under various threat models where an adversary attempts to distinguish between neighboring datasets by manipulating the selection process. The framework tests DP-SGD as the base algorithm with different hyperparameter configurations, evaluating both theoretical bounds and practical leakage through hypothesis testing games.

## Key Results
- Empirical privacy leakage under strong attacks remains significantly below theoretical upper bounds
- f-DP framework provides tighter privacy bounds than RDP-based methods for hyper-parameter tuning
- Score function properties significantly impact privacy leakage, with one-to-one mappings representing worst-case scenarios
- Improved privacy accountant allows testing more hyperparameters under the same privacy budget

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using f-DP instead of (ε, δ)-DP or RDP provides tighter privacy bounds for hyper-parameter tuning
- Mechanism: f-DP represents privacy with a trade-off function that captures the full relationship between false positive and false negative rates, avoiding conservative approximations inherent in (ε, δ)-DP and RDP
- Core assumption: The base algorithm (DP-SGD) has a distinctive privacy characterization that can be better captured by f-DP
- Evidence anchors:
  - [abstract]: "Our improved bound leads to better utility. Our analysis also demonstrates broader applicability compared to prior analyses, which are limited to specific parameter configurations."
  - [section]: "The pivotal aspect driving this improvement lies in representing the privacy of the base algorithm with finer resolution, and DP-SGD does have a distinctive characterization."
  - [corpus]: Weak - corpus papers don't directly discuss f-DP vs RDP tradeoffs for hyper-parameter tuning
- Break condition: If the base algorithm cannot be expressed within the f-DP framework, or if the trade-off function approximation error dominates

### Mechanism 2
- Claim: The score function's role in selection significantly impacts privacy leakage
- Mechanism: One-to-one mapping score functions are necessary for worst-case privacy analysis, as they avoid injecting additional randomness that would make the mechanism more private
- Core assumption: The score function is independent of sensitive data and only post-processes the base algorithm's output
- Evidence anchors:
  - [abstract]: "Our results demonstrate broader applicability compared to prior analyses, which are limited to specific parameter configurations."
  - [section]: "A score function that induces a strict total order for elements in Γ tends to be less private. Thus, a one-to-one mapping is necessary to be the worst case for the score function g."
  - [corpus]: Weak - corpus papers focus on selection mechanisms but don't deeply analyze score function properties
- Break condition: If the score function can be manipulated to depend on sensitive data, or if tie-breaking introduces significant randomness

### Mechanism 3
- Claim: Privacy audit results under practical threat models show much lower leakage than theoretical worst-case bounds
- Mechanism: In realistic scenarios where training data is natural and validation is untampered, the adversary's distinguishing power is severely limited
- Core assumption: The practical threat model (NTNV) represents typical usage where score functions are not maliciously manipulated
- Evidence anchors:
  - [abstract]: "Our findings underscore a substantial gap between the current theoretical privacy bound and the empirical privacy leakage derived even under strong audit setups."
  - [section]: "Our weakest audit setup, which mimics practical use, suggests yes. Tuning adds little to no extra leakage beyond the base algorithm."
  - [corpus]: Weak - corpus papers don't provide empirical audit results for hyper-parameter tuning specifically
- Break condition: If the practical threat model assumptions are violated (e.g., manipulated validation data), or if the base algorithm has unexpected privacy properties

## Foundational Learning

- Concept: Differential Privacy fundamentals (ε, δ-DP, Rényi DP)
  - Why needed here: The paper builds on these existing DP frameworks to show why f-DP provides improvements
  - Quick check question: What is the key difference between (ε, δ)-DP and f-DP in terms of how they characterize privacy?

- Concept: Privacy accounting and composition
  - Why needed here: Understanding how privacy costs accumulate across multiple algorithm runs is crucial for the hyper-parameter tuning problem
  - Quick check question: How does the composition theorem differ from the privacy bounds achieved through the selection mechanism described?

- Concept: Trade-off functions and hypothesis testing interpretation of DP
  - Why needed here: The f-DP framework is built on trade-off functions, and the audit methodology uses hypothesis testing to measure privacy leakage
  - Quick check question: How does the Neyman-Pearson lemma relate to the optimal decision rule in privacy auditing?

## Architecture Onboarding

- Component map:
  - Base algorithm (DP-SGD) → Privacy representation (f-DP) → Selection mechanism (Algorithm 2) → Privacy accounting (Algorithm 4) → Audit methodology (Figure 1)
  - Data flow: Training datasets → DP-SGD runs → Score evaluation → Selection → Output model
  - Control flow: Privacy parameter selection → Algorithm configuration → Execution → Privacy bound computation

- Critical path:
  1. Configure DP-SGD with privacy parameters (σ, C, lr, batch size)
  2. Run multiple DP-SGD instances with different hyperparameters
  3. Apply score function to evaluate outputs
  4. Select best output using Algorithm 2
  5. Compute privacy bound using f-DP accounting
  6. (Optional) Perform privacy audit to validate bounds

- Design tradeoffs:
  - Privacy vs utility: Tighter privacy bounds allow more hyperparameter trials under the same budget
  - Computational cost vs accuracy: More audit simulations provide better lower bounds but increase runtime
  - Generality vs specificity: f-DP framework is more general but may be harder to implement than specific bounds

- Failure signatures:
  - Privacy bounds exceed expected values → Check f-DP conversion or trade-off function approximation
  - Audit results show high leakage → Verify threat model assumptions, check for data-dependent score functions
  - Poor utility despite good privacy → May need to adjust hyperparameter search space or base algorithm parameters

- First 3 experiments:
  1. Implement Algorithm 2 with a simple base algorithm (e.g., Gaussian mechanism) and verify privacy bounds against theoretical expectations
  2. Test the f-DP accounting on DP-SGD with varying numbers of iterations to observe the asymptotic behavior
  3. Run the privacy audit framework on a controlled example where ground truth privacy leakage is known

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the worst-case score function depend on the sampling distribution ξ of the number of runs?
- Basis in paper: [explicit] The paper states "Does the worst-case g depend on specific ξ?" in Section V-C
- Why unresolved: The theoretical analysis in Theorem 2 only establishes that one-to-one mappings are necessary for worst-case score functions, but doesn't characterize how the specific form of g depends on ξ
- What evidence would resolve it: Empirical studies comparing different one-to-one score functions across various ξ distributions, or theoretical characterization of optimal g for different ξ

### Open Question 2
- Question: What is the exact trade-off between privacy leakage and utility (governed by ξ)?
- Basis in paper: [explicit] Section V-C asks "How to quantify the exact trade-off between privacy leakage and ξ which governs the utility?"
- Why unresolved: While the paper provides improved privacy bounds, it doesn't establish how to optimize ξ to balance privacy vs utility
- What evidence would resolve it: Empirical studies measuring privacy-utility trade-offs across different ξ distributions, or theoretical bounds relating ξ parameters to utility metrics

### Open Question 3
- Question: How much privacy leakage would occur if the score function g is allowed to depend on sensitive data?
- Basis in paper: [explicit] Section IV-B discusses "Stronger version" where "the adversary can arbitrarily control g" and asks how this affects privacy
- Why unresolved: The audit experiments only tested a limited set of manipulated score functions, and the theoretical analysis assumes g is independent of sensitive data
- What evidence would resolve it: Audit experiments with various data-dependent score functions, or theoretical analysis of privacy under different assumptions about g's dependence on sensitive data

## Limitations

- The theoretical improvement using f-DP is primarily asymptotic and relies on the base algorithm having a distinctive privacy characterization
- Empirical audit results are limited to specific datasets and threat models that may not capture all practical scenarios
- The analysis focuses on DP-SGD as the base algorithm, and improvements may not generalize to other differentially private algorithms
- The claim about worst-case score functions assumes independence from sensitive data, which may not hold in all applications

## Confidence

- **High confidence**: The empirical observation that practical privacy leakage is significantly below theoretical bounds
- **Medium confidence**: The theoretical improvement from f-DP accounting over RDP-based methods
- **Medium confidence**: The role of score function in privacy leakage
- **Low confidence**: Generalization to base algorithms beyond DP-SGD

## Next Checks

1. Test the privacy audit framework on a controlled synthetic example where the ground truth privacy leakage is known through analytical calculation, comparing both empirical audit results and theoretical bounds
2. Implement the same hyper-parameter tuning process using a different base algorithm (e.g., DP-SGD with momentum or DP-FTRL) to verify if the f-DP improvements generalize beyond standard DP-SGD
3. Conduct additional empirical studies with manipulated validation data to test the robustness of the privacy audit results when the NTNV threat model assumptions are violated