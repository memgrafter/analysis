---
ver: rpa2
title: Bayesian Optimization of Bilevel Problems
arxiv_id: '2412.18518'
source_url: https://arxiv.org/abs/2412.18518
tags:
- lower-level
- optimization
- upper-level
- function
- bilevel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses bilevel optimization problems where both upper-
  and lower-level functions are expensive black-box functions. The proposed BILBAO
  algorithm uses Gaussian processes to model both levels over the combined decision
  space, enabling knowledge transfer between sub-problems.
---

# Bayesian Optimization of Bilevel Problems

## Quick Facts
- **arXiv ID**: 2412.18518
- **Source URL**: https://arxiv.org/abs/2412.18518
- **Authors**: Omer Ekmekcioglu; Nursen Aydin; Juergen Branke
- **Reference count**: 11
- **Key outcome**: BILBAO algorithm outperforms existing methods in bilevel optimization with expensive black-box functions through knowledge transfer and REVI acquisition function

## Executive Summary
This paper addresses bilevel optimization problems where both upper- and lower-level functions are expensive black-box functions. The proposed BILBAO algorithm uses Gaussian processes to model both levels over the combined decision space, enabling knowledge transfer between sub-problems. A novel acquisition function inspired by multi-task learning efficiently samples the decision space. Experimental results on 2D and 4D test problems demonstrate that BILBAO outperforms existing methods in finding high-quality solutions with superior sample efficiency. The algorithm effectively learns the lower-level response map for promising upper-level decision variables, allowing informed sampling without requiring full optimization of the lower level at each iteration.

## Method Summary
BILBAO uses two separate Gaussian process models to represent the upper and lower-level functions over the combined decision space. The algorithm employs Thompson sampling on the upper-level GP to identify important upper-level decision points, which are then used with the REVI acquisition function to optimize the lower-level response. The lower-level response map Φ captures the relationship between upper-level decisions and their corresponding lower-level optima. REVI discretizes the decision spaces and uses Monte Carlo sampling to find points that maximize expected improvement in the upper-level posterior mean. The algorithm iterates between evaluating the upper-level function, updating the upper-level GP, and optimizing the lower-level response using REVI.

## Key Results
- BILBAO outperforms benchmark algorithm (BILBO with EI) on 2D and 4D test problems with better optimality gaps and sample efficiency
- REVI acquisition function provides better performance than REVITS, though at higher computational cost
- Knowledge transfer between upper and lower-level GPs improves sample efficiency compared to treating levels independently
- Thompson sampling effectively explores the upper-level decision space while REVI focuses lower-level sampling on promising regions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge transfer between upper and lower-level sub-problems improves sample efficiency.
- Mechanism: Both upper and lower-level functions are modeled as Gaussian processes over the combined decision space. The upper-level GP is constrained to use lower-level responses from the lower-level GP, and REVI uses the upper-level GP to identify important regions for the lower-level to sample.
- Core assumption: The upper and lower-level functions are correlated, so learning one improves the other.
- Evidence anchors:
  - [abstract] "Gaussian processes to model both levels over the combined decision space, enabling knowledge transfer between sub-problems"
  - [section] "model the upper and lower-level functions as Gaussian processes over the combined space of upper and lower-level decisions"
  - [corpus] Weak evidence - the related papers mention bilevel Bayesian optimization but don't explicitly discuss knowledge transfer mechanisms.
- Break condition: If the upper and lower-level functions are uncorrelated, the knowledge transfer won't improve performance.

### Mechanism 2
- Claim: The REVI acquisition function efficiently identifies the most informative sampling points.
- Mechanism: REVI discretizes the upper-level decision space and uses Monte Carlo sampling to find the point that maximizes the expected improvement in the upper-level posterior mean, given the current lower-level response map.
- Core assumption: The upper-level decision space can be effectively discretized without losing important information.
- Evidence anchors:
  - [section] "We use the Regional Expected Value of Improvement (REVI) acquisition function...to find the optimal response x*l ∈ XL for each upper-level decision xu ∈ XU"
  - [section] "For computational traceability, REVI discretizes decision spaces and uses Monte Carlo sampling"
  - [corpus] Moderate evidence - the related papers discuss Bayesian optimization for bilevel problems but don't specifically address REVI.
- Break condition: If the upper-level decision space is too high-dimensional or has complex structure, discretization may lose important information.

### Mechanism 3
- Claim: Thompson sampling on the upper-level GP effectively explores the decision space.
- Mechanism: The upper-level GP is sampled multiple times using Thompson sampling to generate a set of important upper-level decision points. These points are then used in the REVI acquisition function for the lower level.
- Core assumption: Thompson sampling provides a good exploration strategy for the upper-level decision space.
- Evidence anchors:
  - [section] "Thompson sampling to sample the restricted upper-level sample path multiple times to construct a set of important points XT S"
  - [section] "Thompson sampling on the upper-level GP to explore candidate solutions conditioned on the lower-level response map"
  - [corpus] Strong evidence - Thompson sampling is a well-established method in Bayesian optimization.
- Break condition: If the upper-level GP is poorly modeled, Thompson sampling may not explore effectively.

## Foundational Learning

- Concept: Gaussian Processes (GPs)
  - Why needed here: GPs are used to model both the upper and lower-level functions, providing a probabilistic framework for Bayesian optimization.
  - Quick check question: What are the two main components that define a GP, and how do they relate to the posterior mean and covariance?

- Concept: Bayesian Optimization (BO)
  - Why needed here: BO is the overarching framework used to optimize the bilevel problem by iteratively selecting the most informative sampling points.
  - Quick check question: What is the main goal of the acquisition function in BO, and how does it balance exploration and exploitation?

- Concept: Knowledge Gradient (KG)
  - Why needed here: KG is used as the underlying acquisition function for REVI, measuring the expected improvement in the maximum of the posterior mean.
  - Quick check question: How does KG differ from Expected Improvement (EI), and in what scenarios is KG more appropriate?

## Architecture Onboarding

- Component map: Upper-level GP → Thompson sampling → Upper-level evaluation → Update upper-level GP → REVI acquisition → Lower-level evaluation → Update lower-level GP and response map → Repeat
- Critical path: Upper-level Thompson sampling → Evaluate upper-level function → Update upper-level GP → Lower-level REVI/REVITS optimization → Evaluate lower-level function → Update lower-level GP and response map → Repeat
- Design tradeoffs: REVI provides better performance but is computationally more expensive than REVITS. The choice of acquisition function for the lower level affects both performance and computational cost.
- Failure signatures: Poor performance may indicate: (1) Poor GP modeling of either level, (2) Inadequate discretization of the upper-level decision space, (3) Incorrect choice of acquisition function.
- First 3 experiments:
  1. Implement a simple version with only one GP and compare to the two-GP approach.
  2. Test REVI vs REVITS on a simple 2D problem to understand the performance tradeoff.
  3. Vary the number of Thompson samples and measure the impact on performance and computational cost.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the number of interest points (k) affect the performance of BILBAO across different problem dimensions and complexities?
- Basis in paper: [explicit] The paper mentions setting k=10 for the 2D and 4D tests but does not explore how varying k affects performance.
- Why unresolved: The experiments used a fixed k value without investigating the sensitivity of the algorithm to this parameter.
- What evidence would resolve it: Systematic experiments varying k across different problem sizes and comparing performance metrics would clarify the optimal range and its impact.

### Open Question 2
- Question: Can the REVI acquisition function be adapted to handle non-uniform probability distributions over the upper-level decision variables more effectively?
- Basis in paper: [explicit] The paper uses Thompson sampling to generate XT S but does not explore other probability distributions or adaptive methods for crafting the set.
- Why unresolved: The current implementation relies on a specific method of generating interest points, but the impact of alternative distributions is unexplored.
- What evidence would resolve it: Comparative experiments using different probability distributions or adaptive schemes for XT S generation would demonstrate the effectiveness of alternative approaches.

### Open Question 3
- Question: How does BILBAO perform when the lower-level function has multiple local optima or is non-convex?
- Basis in paper: [inferred] The experiments use test functions that may not fully capture the complexity of real-world bilevel problems with multiple local optima.
- Why unresolved: The current test suite may not adequately represent scenarios where the lower-level response map is highly non-linear or multimodal.
- What evidence would resolve it: Testing BILBAO on bilevel problems with known multiple local optima and non-convex lower-level functions would reveal its robustness in such scenarios.

## Limitations

- The paper doesn't isolate the knowledge transfer mechanism's contribution to performance gains through ablation studies
- Computational complexity analysis of REVI is limited, raising questions about scalability to higher dimensions
- Only one benchmark algorithm (BILBO with EI) is used for comparison, limiting the strength of superiority claims

## Confidence

- **High confidence**: The mathematical framework and algorithm description are clearly specified and internally consistent
- **Medium confidence**: The experimental methodology and results are well-documented, but the limited number of benchmark comparisons reduces confidence in the superiority claims
- **Medium confidence**: The theoretical justification for the knowledge transfer mechanism is reasonable but not empirically validated in isolation

## Next Checks

1. Implement an ablation study isolating the knowledge transfer mechanism by comparing BILBAO with independent GP models for each level
2. Test BILBAO on higher-dimensional problems (5D+) to evaluate scalability and computational complexity claims
3. Compare BILBAO against a broader set of benchmark algorithms including gradient-based bilevel optimization methods to establish relative performance more comprehensively