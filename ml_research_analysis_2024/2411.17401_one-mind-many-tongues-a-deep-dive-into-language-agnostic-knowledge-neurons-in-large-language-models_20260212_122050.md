---
ver: rpa2
title: 'One Mind, Many Tongues: A Deep Dive into Language-Agnostic Knowledge Neurons
  in Large Language Models'
arxiv_id: '2411.17401'
source_url: https://arxiv.org/abs/2411.17401
tags:
- knowledge
- neurons
- language-agnostic
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates language-agnostic knowledge neurons in
  large language models (LLMs) that store factual knowledge in a form that transcends
  language barriers. The authors identify two key limitations in existing research:
  high uncertainty in localization results due to inconsistent answers for semantically
  equivalent queries, and lack of analysis across diverse language families beyond
  English and Chinese.'
---

# One Mind, Many Tongues: A Deep Dive into Language-Agnostic Knowledge Neurons in Large Language Models

## Quick