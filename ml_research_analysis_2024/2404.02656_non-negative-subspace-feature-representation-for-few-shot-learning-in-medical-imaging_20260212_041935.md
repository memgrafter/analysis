---
ver: rpa2
title: Non-negative Subspace Feature Representation for Few-shot Learning in Medical
  Imaging
arxiv_id: '2404.02656'
source_url: https://arxiv.org/abs/2404.02656
tags:
- subspace
- data
- learning
- medical
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores few-shot learning for medical image classification,
  addressing the challenge of limited data availability in medical imaging. The authors
  propose using non-negative matrix factorization (NMF) and its supervised variants
  (DNMF and SCNMFS) as alternatives to principal component analysis (PCA) for dimensionality
  reduction in low-data regimes.
---

# Non-negative Subspace Feature Representation for Few-shot Learning in Medical Imaging

## Quick Facts
- arXiv ID: 2404.02656
- Source URL: https://arxiv.org/abs/2404.02656
- Reference count: 40
- This paper demonstrates that NMF-based representations, especially supervised variants, outperform traditional PCA/SVD methods in medical image classification with extremely limited data.

## Executive Summary
This paper addresses few-shot learning challenges in medical imaging by proposing non-negative matrix factorization (NMF) and its supervised variants as alternatives to principal component analysis for dimensionality reduction. The authors evaluate their approach across 14 medical datasets spanning 11 illness categories and four imaging modalities. Results show that NMF-based representations, particularly the supervised variants (DNMF, SCNMFS), significantly outperform traditional methods in classification accuracy even with as few as 10 samples per class. The study also demonstrates that NMF's part-based representation effectively localizes lesion areas in medical images through class activation maps.

## Method Summary
The proposed method extracts 512-dimensional features from medical images using a pre-trained ResNet18 network, then applies dimensionality reduction through NMF or supervised NMF variants (DNMF, SCNMFS) to obtain subspace representations. These subspaces are then used as input to a K-nearest neighbors classifier (K=5) for final classification. The method is evaluated across 14 medical datasets with varying numbers of classes and samples, comparing performance against SVD, NMF, DNMF, and SCNMFS across different subspace dimensions.

## Key Results
- NMF-based representations outperform traditional PCA/SVD methods in classification accuracy across all tested datasets
- Supervised NMF variants (DNMF, SCNMFS) show significant improvements over standard NMF by incorporating label information
- CAM visualizations demonstrate NMF's effectiveness in localizing lesion areas in medical images
- The approach achieves significant improvements over prototypical networks, a well-known few-shot learning method

## Why This Works (Mechanism)

### Mechanism 1
Supervised NMF variants (DNMF, SCNMFS) outperform standard NMF by incorporating label information during factorization. By adding discriminative constraints derived from labeled data, supervised NMF forces the subspace to separate classes more distinctly, creating clusters for each class rather than general sparsity. Core assumption: The label information is reliable and sufficiently representative of the underlying data distribution. Evidence: [abstract] "supervised NMF algorithms are more discriminative in the subspace with greater effectiveness" and [section 4.1.3] shows performance improvements when incorporating label information. Break condition: If labels are noisy or unrepresentative, the discriminative constraints may create misleading subspaces that reduce classification accuracy.

### Mechanism 2
NMF's non-negativity constraint creates sparse, part-based representations that are more interpretable for medical image analysis. The non-negativity constraint prevents subtraction during reconstruction, promoting additive combinations of parts that align with how medical images contain distinct regions (lesions, organs) rather than overlapping features. Core assumption: Medical image features are naturally additive rather than subtractive, making non-negative combinations more meaningful. Evidence: [abstract] "the part-based representation of NMF, especially its supervised variants, is dramatically impactful in detecting lesion areas" and [section 4.2] shows CAM validation. Break condition: If medical image features require subtractive combinations for accurate representation, NMF's constraint would force suboptimal representations.

### Mechanism 3
Dimensionality reduction to subspace representations is crucial for few-shot learning when feature dimensions exceed available data points. When the number of samples is smaller than feature dimensions, traditional classification suffers from the curse of dimensionality. Subspace methods compress high-dimensional features while preserving discriminative information, making classification feasible with limited data. Core assumption: The reduced subspace can preserve the essential discriminative information needed for classification while eliminating noise and redundancy. Evidence: [section 4.1.1] shows "subspace representations... overall yield better results than the original feature space" and [section 3.2] discusses exploring data representation in different subspaces. Break condition: If the subspace dimension is too low, critical discriminative information may be lost, leading to poor classification performance.

## Foundational Learning

- Concept: Non-negative matrix factorization and its mathematical formulation
  - Why needed here: Understanding how NMF differs from PCA/SVD is fundamental to grasping why it works better for medical imaging
  - Quick check question: What is the key mathematical difference between NMF and SVD that leads to part-based representations?

- Concept: Supervised learning with label incorporation
  - Why needed here: The supervised NMF variants (DNMF, SCNMFS) require understanding how labels can be integrated into the factorization process
  - Quick check question: How do DNMF and SCNMFS differ in their approach to incorporating label information?

- Concept: Few-shot learning paradigms and data-based approaches
  - Why needed here: This paper uses a data-based few-shot learning approach rather than model-based meta-learning methods
  - Quick check question: What distinguishes data-based few-shot learning from model-based approaches like MAML or prototypical networks?

## Architecture Onboarding

- Component map: Pre-trained feature extractor (ResNet18) -> Feature space generation -> NMF/DNMF/SCNMFS factorization -> Subspace representation -> Classifier (KNN) -> Prediction
- Critical path:
  1. Extract features from medical images using pre-trained network
  2. Apply NMF or supervised NMF to create subspace representation
  3. Train classifier on training subspace representation
  4. Apply implicit transformation to test features using training projection matrix
  5. Classify test samples using trained classifier
- Design tradeoffs:
  - NMF vs PCA: NMF provides interpretability and part-based representation but may require more careful parameter tuning
  - Supervised vs unsupervised: Supervised variants improve discrimination but require reliable labels and may overfit with very limited data
  - Subspace dimension: Higher dimensions preserve more information but increase computational cost and risk of overfitting
- Failure signatures:
  - Poor classification accuracy despite high-quality features may indicate inappropriate subspace dimension or factorization method
  - Inconsistent CAM results across different dimensions suggest instability in the subspace representation
  - Degradation when increasing data augmentation indicates the method may be sensitive to data distribution shifts
- First 3 experiments:
  1. Compare classification accuracy of NMF vs SVD on a binary medical dataset with 300 training samples, varying subspace dimensions from 5 to 50
  2. Test DNMF vs SCNMFS on a multi-class dataset (3+ classes) with 600 samples, measuring both accuracy and CAM consistency across dimensions
  3. Evaluate the impact of subspace dimension on CAM quality for lesion detection in breast ultrasound images, comparing feature space vs 10D, 20D, and 30D subspaces

## Open Questions the Paper Calls Out

- Question: How does the performance of supervised NMF methods like SCNMFS compare to unsupervised methods like SVD and NMF when the amount of labeled data is extremely limited (e.g., fewer than 10 samples per class)?
- Basis in paper: [explicit] The paper states that "incorporating label signals into the decomposition process of NMF will be affected by the data size" and notes that "from the data characteristics point of view, the non-negative representation in NMF will bring sparsity and enhance the ability of data representation in subspaces."
- Why unresolved: While the paper demonstrates that supervised NMF methods are superior to unsupervised methods with hundreds of available images, it does not specifically test the performance with fewer than 10 samples per class.
- What evidence would resolve it: Additional experiments testing the performance of SCNMFS, DNMF, SVD, and NMF with datasets containing fewer than 10 samples per class would provide concrete evidence of the comparative effectiveness of these methods in extremely low-data regimes.

- Question: How do NMF-based subspace representations perform in multi-modal medical imaging scenarios where both image and text data are available for the same disease?
- Basis in paper: [inferred] The paper acknowledges that "in medical scenarios, multi-modal data (such as images and text) are often associated with the same disease" but focuses solely on image data in its experiments.
- Why unresolved: The current study only explores subspaces using image data, leaving the potential of NMF-based representations for multi-modal data integration unexplored.
- What evidence would resolve it: Experiments incorporating both image and text data into the NMF-based subspace learning framework, comparing performance against unimodal approaches, would demonstrate the effectiveness of NMF in multi-modal medical imaging scenarios.

- Question: What is the impact of different loss functions on the stability and effectiveness of NMF-based subspace representations in medical imaging analysis?
- Basis in paper: [explicit] The paper mentions that "future research could explore integrating multi-modal data into the analysis" and suggests "investigating novel NMF variants and exploring alternative loss functions to enhance the stability and effectiveness of subspace representation in medical imaging analysis."
- Why unresolved: The current study uses standard NMF formulations without exploring alternative loss functions that might improve subspace representation stability or effectiveness.
- What evidence would resolve it: Systematic comparison of different loss functions (e.g., reconstruction error, classification accuracy, or domain-specific losses) within the NMF framework, measuring their impact on subspace stability and classification performance across various medical imaging datasets.

## Limitations

- The study relies on pre-trained features from ResNet18, limiting generalizability to scenarios where such pre-trained models are unavailable
- Evaluation metrics focus on classification accuracy without considering clinical metrics like sensitivity and specificity that are more relevant for medical applications
- The comparison with prototypical networks may be limited as the methods use fundamentally different approaches (data-based vs. model-based)

## Confidence

- High confidence: The superiority of supervised NMF variants over standard NMF and the effectiveness of dimensionality reduction for few-shot learning
- Medium confidence: The interpretability claims regarding CAM visualizations and part-based representations, as these are qualitative assessments
- Medium confidence: The comparison with prototypical networks, as the methods use fundamentally different approaches (data-based vs. model-based)

## Next Checks

1. Replicate the classification experiments using raw pixel data (without pre-trained features) to test the method's robustness in low-resource settings
2. Conduct clinical validation by evaluating sensitivity and specificity on imbalanced datasets to assess real-world applicability
3. Test the method's performance on a held-out dataset with a different imaging modality than those used in training to evaluate generalization across medical domains