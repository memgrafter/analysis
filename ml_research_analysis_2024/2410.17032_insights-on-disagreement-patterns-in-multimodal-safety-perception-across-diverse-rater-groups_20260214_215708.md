---
ver: rpa2
title: Insights on Disagreement Patterns in Multimodal Safety Perception across Diverse
  Rater Groups
arxiv_id: '2410.17032'
source_url: https://arxiv.org/abs/2410.17032
tags:
- raters
- groups
- safety
- expert
- diverse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates safety perception differences in text-to-image
  (T2I) AI across 30 demographic groups. Researchers collected safety ratings from
  630 diverse raters on 1,000 T2I generations, examining gender, age, and ethnicity
  intersections.
---

# Insights on Disagreement Patterns in Multimodal Safety Perception across Diverse Rater Groups

## Quick Facts
- arXiv ID: 2410.17032
- Source URL: https://arxiv.org/abs/2410.17032
- Reference count: 19
- 25% of images deemed safe by experts rated unsafe by diverse raters

## Executive Summary
This study investigates safety perception differences in text-to-image (T2I) AI across 30 demographic groups. Researchers collected safety ratings from 630 diverse raters on 1,000 T2I generations, examining gender, age, and ethnicity intersections. Results show significant disagreement in harm assessments, with Gen-Z and women raters perceiving more safety issues, particularly around bias violations. Intersectional groups like Gen-Z Black raters demonstrated unique perspectives not captured by broader demographic categories. The study found that diverse raters identified safety issues missed by expert raters, with 25% of images deemed safe by experts rated unsafe by diverse raters. Qualitative analysis revealed women raters often considered potential harm to others. The findings emphasize the need for incorporating diverse perspectives in T2I safety evaluations.

## Method Summary
The study used 1,000 prompt-image pairs from the Adversarial Nibbler dataset, rated by 630 diverse raters balanced across 30 intersectional groups (age × gender × ethnicity) and 5 expert raters per pair. Safety perception was measured through two harmfulness questions (personal and others), violation type selection, and optional comments. Analysis employed statistical tests (Mann-Whitney U, Kruskal-Wallis), Group Association Index (GAI) to measure in-group agreement, and comparison between diverse and expert rater patterns. The study also conducted qualitative analysis of comments to identify thematic patterns in safety perceptions across demographic groups.

## Key Results
- Gen-Z and women raters perceived more safety issues than other demographic groups, particularly regarding bias violations
- Intersectional groups like Gen-Z Black raters showed unique perspectives not captured by broader demographic categories
- 25% of images rated safe by expert raters were rated unsafe by diverse raters, especially for bias violations
- Women raters more frequently considered potential harm to others, while men focused on personal harm

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diverse demographic groups perceive safety violations differently, with Gen-Z and women raters showing higher sensitivity.
- Mechanism: The study uses a large-scale, demographically balanced rater pool to capture subjective safety perceptions across age, gender, and ethnicity intersections, revealing that personal and cultural contexts influence harm assessment.
- Core assumption: Safety perception in AI-generated content is inherently subjective and varies systematically across demographic groups.
- Evidence anchors:
  - [abstract] "Gen-Z and women raters perceiving more safety issues, particularly around bias violations."
  - [section] "Women raters have a mean score of 1.08 and Men raters have 0.85... Black raters are likely to give a higher score than White raters with probability 0.57."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.415, average citations=0.0." (Weak corpus support; no direct relevance to demographic safety perception.)

### Mechanism 2
- Claim: Intersectional demographic groups (e.g., Gen-Z Black raters) have cohesive perspectives distinct from broader demographic categories.
- Mechanism: The study employs Group Association Index (GAI) analysis to measure in-group agreement versus cross-group disagreement, identifying that intersectional groups often agree within themselves but diverge from other groups.
- Core assumption: Intersectional identities produce unique safety perceptions not reducible to individual demographic factors.
- Evidence anchors:
  - [abstract] "Intersectional groups like Gen-Z Black raters demonstrated unique perspectives not captured by broader demographic categories."
  - [section] "Gen-Z–Black raters and Millennial–Black raters have the two highest GAI with values of 1.38 and 1.29 respectively which are also statistically significant."
  - [corpus] (No relevant corpus evidence; mechanism is unique to this study.)

### Mechanism 3
- Claim: Diverse raters identify safety issues missed by expert raters, especially regarding bias violations.
- Mechanism: The study compares aggregated ratings from diverse raters with expert policy-trained raters, finding that diverse raters flag bias issues that experts rate as safe, indicating gaps in expert training or policy.
- Core assumption: Expert raters trained on specific safety policies may overlook culturally nuanced or subjective safety concerns that diverse populations perceive.
- Evidence anchors:
  - [abstract] "Diverse raters identified safety issues missed by expert raters, with 25% of images deemed safe by experts rated unsafe by diverse raters."
  - [section] "Expert raters deemed many prompt-image pairs (especially for the violation type bias) to be safe, that is at odds with many in our diverse rater pool."
  - [corpus] (No relevant corpus evidence; mechanism is novel to this study.)

## Foundational Learning

- Concept: Intersectionality in demographic analysis
  - Why needed here: The study emphasizes that safety perceptions vary not just by single demographic factors but by their intersections (e.g., Gen-Z Black women), requiring understanding of how multiple identities interact.
  - Quick check question: How does the Group Association Index (GAI) differ from simple demographic comparisons, and why is it important for capturing intersectional perspectives?

- Concept: Statistical significance testing for group differences
  - Why needed here: The study uses Mann-Whitney U and Kruskal-Wallis tests to determine if demographic groups differ in their safety ratings, requiring knowledge of non-parametric tests for ordinal data.
  - Quick check question: Why might the study use Mann-Whitney U instead of a t-test for comparing gender groups' safety ratings?

- Concept: Bias in AI safety evaluation
  - Why needed here: The study identifies that diverse raters detect bias violations that expert raters miss, highlighting the importance of pluralistic evaluation in AI safety.
  - Quick check question: What are the potential risks of relying solely on expert raters for AI safety evaluation, and how does this study address them?

## Architecture Onboarding

- Component map:
  - Data curation: Adversarial Nibbler dataset (1000 prompt-image pairs with violation types and topics)
  - Rater recruitment: 630 raters balanced across 30 intersectional groups (age × gender × ethnicity)
  - Response form: Two harmfulness questions (personal and others), violation type selection, optional comments
  - Analysis pipeline: Statistical tests (Mann-Whitney U, Kruskal-Wallis), GAI calculation, expert vs. diverse rater comparison, qualitative comment analysis

- Critical path:
  1. Sample prompt-image pairs to ensure coverage of violation types and high expert disagreement
  2. Recruit raters to achieve demographic balance across intersections
  3. Collect safety ratings and comments
  4. Analyze group differences using statistical tests and GAI
  5. Compare diverse rater patterns with expert rater patterns
  6. Conduct qualitative analysis of comments to identify thematic patterns

- Design tradeoffs:
  - Sampling bias: Prioritizing prompt-image pairs with high expert disagreement may overrepresent contentious cases
  - Rater diversity: Balancing 30 intersectional groups requires large sample size, potentially limiting generalizability
  - Expert comparison: Aggregating diverse rater scores vs. expert ternary labels requires careful threshold selection

- Failure signatures:
  - Low inter-rater reliability (IRR) across all groups suggests task ambiguity or poor rater training
  - GAI values not significant after multiple testing correction indicates no meaningful group cohesion
  - Expert and diverse rater disagreement patterns not aligning with known bias types suggests methodological issues

- First 3 experiments:
  1. Run statistical tests (Mann-Whitney U, Kruskal-Wallis) on a subset of data to validate group difference claims before full analysis
  2. Calculate GAI for high-level demographic groups to check for basic cohesion before intersectional analysis
  3. Compare expert and diverse rater ratings on a small sample of bias violation pairs to identify specific disagreement patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop more nuanced methods to elicit safety evaluations from diverse raters that account for different interpretations and calibrations of harmfulness scales?
- Basis in paper: [inferred] The paper notes that "safety evaluation is a highly subjective task with a multitude of possible interpretations and calibrations" and different groups use the harmfulness scale differently, with some using extreme values more while others use central values.
- Why unresolved: The study reveals significant variation in how different demographic groups interpret and use the harmfulness scale, but doesn't provide concrete solutions for how to standardize or better capture these diverse perspectives in safety evaluations.
- What evidence would resolve it: Comparative studies testing different elicitation methods (e.g., contextual prompts, relative scales, group discussions) across diverse demographics to determine which approaches yield the most comprehensive and representative safety assessments.

### Open Question 2
- Question: Which specific rater characteristics (beyond basic demographics) most strongly predict sensitivity to different types of safety violations (bias, violence, sexual content)?
- Basis in paper: [inferred] The paper shows that different demographic groups have varying sensitivities to different violation types, but notes limitations in only having demographic information about raters, not their social backgrounds, value alignment, or rating rationales.
- Why unresolved: While the study identifies demographic patterns in safety perceptions, it cannot explain the underlying reasons for these differences or identify which specific characteristics drive sensitivity to particular types of harms.
- What evidence would resolve it: Longitudinal studies collecting detailed information about raters' personal experiences, cultural backgrounds, and value systems alongside their safety ratings to identify predictive factors for sensitivity to different violation types.

### Open Question 3
- Question: How can we develop effective frameworks for determining which rater groups should evaluate which types of content based on their unique perspectives and expertise?
- Basis in paper: [explicit] The paper explicitly states this as a future work question: "Who should rate what type of content?" and notes that "The question of suitability for a given rating task is one that needs to consider a wide range of factors."
- Why unresolved: While the study demonstrates that different groups have systematically different safety perceptions, it doesn't provide guidance on how to leverage these differences to improve content moderation systems or determine optimal rater-content matching.
- What evidence would resolve it: Empirical studies testing different rater assignment strategies (e.g., demographic matching, expertise-based assignment, diverse panel approaches) to determine which methods most effectively identify and mitigate different types of safety violations.

## Limitations
- Use of non-public Adversarial Nibbler dataset restricts independent verification of sampling methodology
- Study focus on text-to-image models may not generalize to other AI modalities
- Rater pool, while diverse, represents a specific demographic slice that may not capture global perspectives

## Confidence
- **High Confidence**: Gender and age differences in safety perception (supported by statistically significant results and consistent with prior research)
- **Medium Confidence**: Intersectional group uniqueness (supported by GAI analysis but limited by sample size in some demographic intersections)
- **Low Confidence**: Expert-rater comparison conclusions (limited by small expert rater pool and potential task framing differences)

## Next Checks
1. **Statistical robustness verification**: Replicate the GAI calculations using bootstrap resampling to confirm the statistical significance of intersectional group differences, particularly for smaller demographic intersections like Gen-Z Black raters.

2. **Expert rater task alignment**: Conduct a controlled study comparing expert and diverse rater assessments on identical prompt-image pairs with identical instructions to isolate whether disagreement stems from demographic differences or task framing variations.

3. **External dataset validation**: Test the demographic safety perception patterns identified in this study using an independently collected dataset of AI-generated content and diverse raters to verify that findings generalize beyond the Adversarial Nibbler dataset.