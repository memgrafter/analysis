---
ver: rpa2
title: 'Deep Time Series Models: A Comprehensive Survey and Benchmark'
arxiv_id: '2407.13278'
source_url: https://arxiv.org/abs/2407.13278
tags:
- series
- time
- data
- forecasting
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a comprehensive survey of deep learning methods
  for time series analysis, covering five major tasks: forecasting, classification,
  imputation, anomaly detection, and their corresponding model architectures including
  MLP, RNN, CNN, GNN, and Transformer. To enable fair evaluation, the authors introduce
  Time Series Library (TSLib), an open-source benchmark that implements 24 mainstream
  models, covers 30 datasets across different domains, and supports five prevalent
  analysis tasks.'
---

# Deep Time Series Models: A Comprehensive Survey and Benchmark

## Quick Facts
- arXiv ID: 2407.13278
- Source URL: https://arxiv.org/abs/2407.13278
- Reference count: 40
- This paper provides a comprehensive survey of deep learning methods for time series analysis and introduces TSLib, an open-source benchmark covering 24 models across 30 datasets and 5 tasks.

## Executive Summary
This paper presents a comprehensive survey of deep learning approaches for time series analysis, covering five major tasks: forecasting, classification, imputation, anomaly detection, and their corresponding model architectures including MLP, RNN, CNN, GNN, and Transformer. To enable fair evaluation, the authors introduce Time Series Library (TSLib), an open-source benchmark that implements 24 mainstream models, covers 30 datasets across different domains, and supports five prevalent analysis tasks. Based on TSLib, extensive experiments show that Transformer-based models like iTransformer and PatchTST achieve the best performance in forecasting tasks, while CNN-based models excel in classification, imputation, and anomaly detection. The study also highlights the effectiveness of MLP-based models for forecasting despite their simplicity, and discusses future directions including large time series models and practical applications.

## Method Summary
The authors developed TSLib as a unified benchmarking framework that implements 24 mainstream deep learning models for time series analysis across five tasks. TSLib uses a factory pattern with unified interfaces between data and model objects, supports multiple data formats, provides standardized metrics for each task, and includes a complete training and evaluation pipeline. The benchmark covers 30 datasets from various domains and evaluates models using task-specific metrics including MSE, MAE, SMAPE, MASE, Accuracy, and F1-Score. The experimental setup involves loading datasets, applying preprocessing (normalization/decomposition), instantiating models, training with validation monitoring, and evaluating on test sets.

## Key Results
- Transformer-based models like iTransformer and PatchTST achieve the best performance in forecasting tasks
- CNN-based models excel in classification, imputation, and anomaly detection tasks
- MLP-based models show competitive performance in forecasting despite their simplicity
- TSLib provides a fair and comprehensive benchmark for evaluating deep time series models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PatchTST and iTransformer achieve superior forecasting performance by using specialized tokenization strategies that capture temporal dependencies at different granularities.
- Mechanism: PatchTST splits time series into overlapped patches and applies self-attention at the patch level, enabling localized temporal modeling. iTransformer uses variate embeddings to represent entire series, capturing inter-series dependencies.
- Core assumption: Temporal dependencies can be effectively captured at different granularities (patch-wise vs series-wise), and the appropriate granularity depends on the specific forecasting task.
- Evidence anchors:
  - [abstract] "Transformer-based models like iTransformer and PatchTST achieve the best performance in forecasting tasks"
  - [section] "PatchTST [23] and follow-up works split time series X into a sequence of overlapped patches and embed each patch following...Based on the vanilla attention mechanism, PatchTST [23] learns the patch-wise dependencies."
  - [corpus] Weak evidence. Related papers focus on benchmarking and surveys but do not directly address the patch-wise vs series-wise tokenization tradeoff.

### Mechanism 2
- Claim: CNN-based models excel in classification, imputation, and anomaly detection due to their ability to capture local features and hierarchical representations.
- Mechanism: CNNs use convolutional kernels to extract local patterns and build hierarchical feature representations that are effective for tasks requiring local feature detection and pattern recognition.
- Core assumption: Local feature extraction is particularly important for classification, imputation, and anomaly detection tasks, where identifying specific patterns or deviations is crucial.
- Evidence anchors:
  - [abstract] "CNN-based models excel in classification, imputation, and anomaly detection"
  - [section] "Since the semantic information of time series is mainly hidden in the temporal variation, Convolutional neural networks (CNN) [18], [121] have become a competitive backbone for their ability to capture local features and pattern recognition."
  - [corpus] Weak evidence. The corpus contains surveys and benchmarks but lacks specific analysis of why CNNs perform better on these particular tasks.

### Mechanism 3
- Claim: TSLib enables fair benchmarking by providing standardized evaluation protocols, diverse datasets, and a unified experimental pipeline across multiple time series analysis tasks.
- Mechanism: TSLib implements a factory pattern with unified interfaces between data and model objects, supports multiple data formats, provides standardized metrics for each task, and includes a complete training and evaluation pipeline.
- Core assumption: Fair comparison requires consistent experimental conditions, including data preprocessing, model implementation, hyperparameter settings, and evaluation metrics.
- Evidence anchors:
  - [abstract] "TSLib implements 24 mainstream models, covers 30 datasets from different domains, and supports five prevalent analysis tasks"
  - [section] "TSLib encompasses a unified model experiment pipeline, standardized evaluation protocols, extensive and diverse real-world datasets, mainstream and advanced time series analysis models, and unified experimental validation and analysis process."
  - [corpus] Moderate evidence. The corpus includes related benchmarks (TSI-Bench, GIFT-Eval) but TSLib's comprehensive approach across five tasks is unique.

## Foundational Learning

- Concept: Stationarity and normalization in time series
  - Why needed here: Stationarity is a fundamental assumption in many time series analysis methods, and normalization is crucial for stable training and fair comparison across models
  - Quick check question: Why does the Stationary [41] method apply both normalization and de-normalization, and what problem does this solve?

- Concept: Fourier analysis and frequency domain representation
  - Why needed here: Fourier analysis is used to extract periodic patterns and frequency components from time series, which is crucial for understanding underlying dynamics
  - Quick check question: How does the AutoCorrelation mechanism in Autoformer [22] use Fourier transforms to capture series-wise dependencies instead of point-wise dependencies?

- Concept: Different deep learning architectures (MLP, CNN, RNN, GNN, Transformer)
  - Why needed here: Each architecture has different strengths in capturing temporal dependencies, variate correlations, and local vs global patterns
  - Quick check question: What is the key architectural difference between PatchTST's patch-wise attention and iTransformer's series-wise attention, and how does this affect their performance on different tasks?

## Architecture Onboarding

- Component map: TSLib consists of Data Source (30+ datasets) -> Data Processing (normalization, decomposition, Fourier analysis) -> Model Training (unified interface) -> Performance Evaluation (task-specific metrics). Models include MLP-based (N-BEATS, DLinear), RNN-based (Mamba), CNN-based (TimesNet, SCINet), GNN-based, and Transformer-based (iTransformer, PatchTST, Autoformer, Stationary, FEDformer).

- Critical path: Load dataset -> Apply preprocessing (normalization/decomposition) -> Instantiate model -> Train with validation monitoring -> Evaluate on test set using task-specific metrics. The most critical path for fair benchmarking is ensuring consistent preprocessing and hyperparameter settings across all models.

- Design tradeoffs: TSLib prioritizes comprehensiveness and standardization over flexibility. This means users get fair comparisons but may have limited ability to customize individual model implementations. The unified interface simplifies usage but may constrain advanced experimentation.

- Failure signatures: Inconsistent preprocessing leading to unfair comparisons, model implementation bugs affecting performance, missing datasets for certain tasks, or evaluation metrics that don't capture task-specific nuances. Watch for models that perform well on one task but poorly on others due to architectural limitations.

- First 3 experiments:
  1. Run all Transformer-based models (iTransformer, PatchTST, Autoformer, Stationary, FEDformer) on the ETTh1 dataset with unified hyperparameters to establish baseline forecasting performance
  2. Compare CNN-based models (TimesNet, SCINet) against MLP-based models (N-BEATS, DLinear) on the classification task using the UEA datasets to verify the local feature extraction advantage
  3. Test the imputation task on the Electricity dataset with varying missing ratios to evaluate model robustness and compare different architectural approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective tokenization strategies for adapting LLMs to time series analysis tasks beyond forecasting?
- Basis in paper: [inferred] The paper discusses LLM adaptation for time series forecasting and mentions that existing prompting approaches are tailored for forecasting, leaving other tasks unexplored.
- Why unresolved: The paper identifies this as a gap in current research, noting that while LLMs show promise for forecasting through prompting and fine-tuning, their application to classification, imputation, and anomaly detection remains largely unexplored.
- What evidence would resolve it: Empirical studies demonstrating effective prompting strategies for various time series tasks, along with benchmark comparisons against task-specific models.

### Open Question 2
- Question: How can deep time series models effectively handle extremely long sequences in practical applications without sacrificing performance?
- Basis in paper: [explicit] The paper identifies handling extremely long series as a future direction, noting that current models face scalability and computational complexity challenges with high-frequency sampling data.
- Why unresolved: The paper acknowledges this limitation but doesn't provide solutions, stating that while patching techniques exist, performance degrades as patch length increases.
- What evidence would resolve it: Novel architectures or techniques that maintain or improve performance on long sequences while reducing computational complexity, validated through comprehensive experiments.

### Open Question 3
- Question: What are the optimal ways to model relationships between endogenous and exogenous variables in time series analysis?
- Basis in paper: [explicit] The paper identifies utilizing exogenous variables as a future direction, noting that in practical applications, main variables and covariates usually occupy different positions.
- Why unresolved: While the paper mentions this as an important area, it doesn't provide concrete solutions for how to best model these relationships in a unified framework.
- What evidence would resolve it: Empirical comparisons of different approaches to integrating exogenous variables, showing improvements in model performance and interpretability across various time series tasks.

## Limitations
- The benchmarking conclusions may be sensitive to implementation details and hyperparameter choices not fully specified in the paper
- The corpus evidence supporting architectural superiority claims is predominantly from surveys rather than deep empirical analyses
- The standardized protocols may be too restrictive and fail to capture task-specific nuances or advanced experimentation needs

## Confidence

- **High confidence**: The claim that TSLib provides standardized benchmarking infrastructure is well-supported by the detailed description of its unified pipeline and diverse dataset coverage.
- **Medium confidence**: The assertion that Transformer-based models achieve the best forecasting performance is supported by the benchmarking results but lacks theoretical justification for why specific tokenization strategies work better.
- **Low confidence**: The explanation for why CNN-based models excel in classification, imputation, and anomaly detection tasks is weakly supported by the corpus evidence, which consists mainly of surveys rather than empirical studies of architectural effectiveness.

## Next Checks

1. **Implementation verification**: Compare TSLib's model implementations against the original papers' code to ensure faithful reproduction and identify any implementation differences that could affect performance rankings.

2. **Hyperparameter sensitivity analysis**: Conduct experiments varying key hyperparameters (learning rate, batch size, sequence length) across multiple models to determine if the observed performance differences are robust to hyperparameter choices.

3. **Task-specific mechanism validation**: Design ablation studies that isolate the effects of patch-wise vs series-wise tokenization in Transformer models and local vs global feature extraction in CNN models to validate the claimed mechanisms for their task-specific advantages.