---
ver: rpa2
title: 'adaptMLLM: Fine-Tuning Multilingual Language Models on Low-Resource Languages
  with Integrated LLM Playgrounds'
arxiv_id: '2403.02370'
source_url: https://arxiv.org/abs/2403.02370
tags:
- translation
- https
- language
- adaptmllm
- november
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: adaptMLLM is an open-source application that streamlines the fine-tuning
  of multilingual language models for machine translation, particularly for low-resource
  languages. It offers an intuitive interface for customizing hyperparameters, model
  evaluation metrics, and direct deployment as a translation service.
---

# adaptMLLM: Fine-Tuning Multilingual Language Models on Low-Resource Languages with Integrated LLM Playgrounds

## Quick Facts
- arXiv ID: 2403.02370
- Source URL: https://arxiv.org/abs/2403.02370
- Reference count: 40
- Primary result: adaptMLLM achieves 40.5 BLEU point improvement for Irish-to-English translation

## Executive Summary
adaptMLLM is an open-source application designed to streamline the fine-tuning of multilingual language models (MLLMs) for machine translation, particularly for low-resource languages. The system offers an intuitive interface for customizing hyperparameters, model evaluation metrics, and direct deployment as a translation service. By fine-tuning the 3.3B parameter NLLB model on English-to-Irish and English-to-Marathi translation tasks, adaptMLLM demonstrates significant improvements over baseline models from the LoResMT2021 Shared Task, with relative improvements of 14%, 117%, and 68% for different translation directions.

## Method Summary
The adaptMLLM framework provides a Colab/Jupyter notebook interface that guides users through the process of fine-tuning multilingual language models for low-resource language pairs. The system uses Hugging Face Transformers and DeepSpeed for efficient training of large models, incorporating hyperparameter optimization (HPO) to find optimal settings for learning rate, batch size, gradient accumulation steps, weight decay, and mixed precision. After fine-tuning, models are evaluated using automatic metrics (BLEU, TER, ChrF) and can be deployed as translation services using Gradio. The application also tracks energy usage and carbon emissions through its green report feature.

## Key Results
- Achieved 40.5 BLEU point improvement for Irish-to-English translation compared to LoResMT2021 baseline
- Demonstrated 21.3 BLEU point improvement for Marathi-to-English translation
- Human evaluation using SQM and MQM metrics validated translation quality and identified specific linguistic challenges

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning a large multilingual language model (MLLM) with 3.3B parameters yields significantly higher BLEU scores than training smaller models from scratch for low-resource language pairs. The MLLM has already learned rich representations from large-scale multilingual data, which are transferable to low-resource languages. Fine-tuning adapts these representations to the specific language pair with relatively little data, improving translation quality more efficiently than starting from a smaller, generic architecture.

### Mechanism 2
Using a hyperparameter optimization (HPO) search space that includes learning rate, batch size, gradient accumulation steps, weight decay, and mixed precision leads to optimal fine-tuning results. Different hyperparameters control aspects of training dynamics such as convergence speed, stability, and memory usage. By exploring a wide range, the best combination for the specific low-resource task can be found, maximizing translation performance.

### Mechanism 3
Human evaluation using SQM and MQM metrics provides a more nuanced and reliable assessment of translation quality than automatic metrics alone, especially for morphologically rich languages like Irish. Automatic metrics like BLEU focus on n-gram overlap and may miss grammatical or contextual errors. Human evaluation with SQM (scalar ratings) and MQM (detailed error taxonomy) can identify specific linguistic issues such as gender, case, and inflection errors that are critical in low-resource languages.

## Foundational Learning

- Concept: Multilingual Language Models (MLLMs) and their pre-training process
  - Why needed here: Understanding that MLLMs are trained on large multilingual corpora and can be fine-tuned for specific tasks is fundamental to grasping why adaptMLLM works.
  - Quick check question: What is the key advantage of using a pre-trained MLLM over training a model from scratch for low-resource languages?

- Concept: Transformer architecture and its role in modern NMT
  - Why needed here: The paper uses Transformer-based MLLMs, so knowing how the architecture works (encoder-decoder, attention mechanisms) is important for understanding model behavior.
  - Quick check question: How does the attention mechanism in Transformers differ from recurrent neural networks in processing input sequences?

- Concept: Automatic evaluation metrics (BLEU, TER, ChrF) and their limitations
  - Why needed here: The paper reports results using these metrics, but also highlights the need for human evaluation due to their limitations, especially for morphologically rich languages.
  - Quick check question: Why might BLEU scores not fully capture translation quality for languages with complex morphology?

## Architecture Onboarding

- Component map: User Interface (Colab/Jupyter notebook) -> Data Preprocessing -> Model Training -> Evaluation -> Deployment -> Green Report
- Critical path: Data → Preprocessing → Fine-tuning → Evaluation → Deployment
- Design tradeoffs:
  - Using Google Colab for ease of access vs. limited control over hardware and potential costs
  - Fine-tuning large models (3.3B parameters) for better performance vs. higher computational requirements
  - Providing a GUI for ease of use vs. potentially limiting advanced customization
- Failure signatures:
  - Training fails to converge: Check hyperparameter settings, data quality, and GPU memory
  - Low BLEU scores: Insufficient data, poor hyperparameter choice, or the pre-trained model not well-suited to the language pair
  - Human evaluation disagreement: Inconsistent annotation guidelines or insufficiently trained annotators
- First 3 experiments:
  1. Fine-tune NLLB-200-3.3B on the EN↔GA dataset with default hyperparameters and evaluate using BLEU
  2. Vary the learning rate and batch size to find the optimal combination for the EN→GA direction
  3. Deploy the best model and test its translation quality on a held-out sample, comparing with Google Translate

## Open Questions the Paper Calls Out

### Open Question 1
How does fine-tuning NLLB-200-3.3B specifically improve translation quality for low-resource languages compared to training smaller models from scratch? While results demonstrate improvement, the paper doesn't investigate whether this comes from better pre-training, more parameters, or other architectural advantages of NLLB-200-3.3B.

### Open Question 2
What is the relationship between automatic evaluation metrics (BLEU, TER, ChrF) and human evaluation scores for MLLM fine-tuned models? The authors state metrics are correlated but don't provide correlation coefficients or investigate cases where they diverge.

### Open Question 3
How do specific hyperparameters in the HPO search space affect translation quality for different language pairs? While optimal hyperparameters are reported, there's no ablation study or sensitivity analysis to understand their relative importance.

## Limitations

- The specific hyperparameter search space and optimization process used to achieve reported BLEU improvements are not fully detailed
- The paper does not provide comprehensive error analysis of translation outputs, limiting ability to pinpoint which linguistic phenomena are most challenging
- Evaluation focuses on only two language pairs (EN↔GA, EN↔MR), raising questions about applicability to other low-resource language pairs

## Confidence

- **High confidence**: The use of fine-tuning pre-trained multilingual models for low-resource translation is a well-established approach with strong empirical support
- **Medium confidence**: The specific improvements reported are impressive but rely on a single dataset and evaluation metric, and may not generalize to other domains or language pairs
- **Low confidence**: The claim that adaptMLLM is a "one-stop-shop" for fine-tuning and deployment is not fully substantiated

## Next Checks

1. **Generalization Test**: Fine-tune adaptMLLM on a different low-resource language pair (e.g., English to Swahili) using the same procedure and compare results to a baseline
2. **Ablation Study**: Conduct an ablation study to determine the impact of each hyperparameter on translation quality, isolating the contribution of learning rate, batch size, and other factors
3. **Robustness Evaluation**: Evaluate the fine-tuned models on out-of-domain data to assess their ability to generalize beyond the COVID-related text used in the LoResMT2021 dataset