---
ver: rpa2
title: 'Beyond Anti-Forgetting: Multimodal Continual Instruction Tuning with Positive
  Forward Transfer'
arxiv_id: '2401.09181'
source_url: https://arxiv.org/abs/2401.09181
tags:
- learning
- tasks
- task
- fwd-prompt
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting and negative forward
  transfer in Multimodal Continual Instruction Tuning (MCIT). The authors analyze
  input embeddings and find that discrepancies across tasks lead to both issues.
---

# Beyond Anti-Forgetting: Multimodal Continual Instruction Tuning with Positive Forward Transfer

## Quick Facts
- arXiv ID: 2401.09181
- Source URL: https://arxiv.org/abs/2401.09181
- Authors: Junhao Zheng; Qianli Ma; Zhen Liu; Binquan Wu; Huawen Feng
- Reference count: 40
- This paper addresses catastrophic forgetting and negative forward transfer in Multimodal Continual Instruction Tuning (MCIT).

## Executive Summary
This paper addresses catastrophic forgetting and negative forward transfer in Multimodal Continual Instruction Tuning (MCIT). The authors analyze input embeddings and find that discrepancies across tasks lead to both issues. To address this, they propose Prompt Tuning with Positive Forward Transfer (Fwd-Prompt), which projects prompt gradients to residual spaces to minimize interference and pre-trained spaces to reuse knowledge. Fwd-Prompt achieves state-of-the-art performance on MCIT benchmarks, outperforming previous methods by 4.16% average accuracy while requiring fewer trainable parameters and no rehearsal data.

## Method Summary
Fwd-Prompt is a prompt tuning method that addresses catastrophic forgetting and negative forward transfer in MCIT. It works by analyzing input embeddings using SVD to identify core and residual spaces for each task. During training, prompt gradients are projected to the residual space to minimize interference with old tasks (anti-forgetting) and to the pre-trained space to reuse knowledge for positive forward transfer. The method requires no rehearsal data and fewer trainable parameters than baselines, making it efficient for multimodal continual learning scenarios.

## Key Results
- Achieves 4.16% higher average accuracy compared to previous methods on MCIT benchmarks
- Outperforms baselines on both anti-forgetting (lower FGTt) and positive forward transfer (higher FWDt)
- Requires fewer trainable parameters than competing methods
- Demonstrates effectiveness across 6 different task orders

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Discrepancy in input embeddings across tasks causes catastrophic forgetting and negative forward transfer.
- **Mechanism:** When input embeddings from different tasks have different ranks and distributions, the model learns irrelevant information for old tasks when adapting to new tasks, leading to forgetting. Similarly, adapting to current task causes extraction of irrelevant information for unseen tasks, causing negative forward transfer.
- **Core assumption:** Input embeddings contain sufficient information to characterize what information is relevant for each task.
- **Evidence anchors:**
  - [abstract] "We discover a large discrepancy in different input embeddings by performing singular value decomposition (SVD) on input embeddings. This discrepancy results in the model learning irrelevant information for old and pre-trained tasks, leading to catastrophic forgetting and negative forward transfer."
  - [section] "We perform SVD [9] on input embeddings to gain more insight. We discover a large discrepancy between input embeddings from different tasks. More importantly, the rank of input embeddings decreases when directly instruction-tuning MLLMs on each new task. However, it increases under the continual learning paradigm."
- **Break condition:** If tasks have similar input embedding characteristics (same rank, similar distributions), the mechanism breaks down.

### Mechanism 2
- **Claim:** Gradient projection to residual space minimizes interference between tasks and achieves anti-forgetting.
- **Mechanism:** By projecting prompt gradients to the residual space (directions orthogonal to core task information), the model avoids updating weights that would interfere with old task knowledge, thus preserving performance on previous tasks.
- **Core assumption:** Residual space directions correspond to information irrelevant to the current task, so updating along these directions doesn't harm previous task performance.
- **Evidence anchors:**
  - [abstract] "Fwd-Prompt achieves anti-forgetting by allocating different subspaces for each task and projecting prompt gradient to the residual space."
  - [section] "We perform SVD on the concatenated input embeddings... After obtaining the prompt gradient ∆p, we project ∆p to the residual space as follows: ∆p′ = VresV T res∆p, where Vres = [ vk+1, vk+2, · · · , vd] ∈ Rd×(d−K). By mapping the prompt gradient into the residual subspace, the forgetting of the knowledge of task t is minimized."
- **Break condition:** If the residual space contains information relevant to previous tasks, projection will cause forgetting.

### Mechanism 3
- **Claim:** Reusing pre-trained knowledge by projecting gradients to pre-trained space achieves positive forward transfer.
- **Mechanism:** By identifying the core space of pre-trained tasks and projecting gradients to this space, the model reuses knowledge that's relevant to unseen tasks, improving performance on future tasks rather than degrading it.
- **Core assumption:** The core space of pre-trained tasks contains information that overlaps with the core space of future tasks, enabling knowledge transfer.
- **Evidence anchors:**
  - [abstract] "Fwd-Prompt achieves positive forward transfer by reusing pre-trained knowledge and projecting prompt gradient to the pre-trained space."
  - [section] "When adapting to task 1, we train the prompt pool without gradient projection. After finishing the training on task 1, we perform SVD on the input embeddings from task 1 and obtain the core space of task 1... We denote the pre-trained space as Vpre ∈ Rd×K0 and the residual space as Vres ∈ Rd×(d−K0)."
- **Break condition:** If pre-trained knowledge doesn't overlap with future task requirements, projection to pre-trained space provides no benefit.

## Foundational Learning

- **Concept: Singular Value Decomposition (SVD)**
  - Why needed here: SVD is used to analyze the rank and structure of input embeddings, identifying core vs. residual spaces for each task.
  - Quick check question: What does the rank of an input embedding matrix tell us about the information requirements of a task?

- **Concept: Gradient Projection**
  - Why needed here: Gradient projection is the mechanism used to constrain parameter updates to specific subspaces, avoiding interference between tasks.
  - Quick check question: How does projecting a gradient onto a subspace affect the parameter updates compared to unconstrained updates?

- **Concept: Catastrophic Forgetting**
  - Why needed here: Understanding catastrophic forgetting is essential to grasp why the problem is challenging and why the proposed solution works.
  - Quick check question: What distinguishes catastrophic forgetting from normal forgetting in continual learning scenarios?

## Architecture Onboarding

- **Component map:** Image/text features → prompt selection → gradient computation → subspace projection → prompt update → task performance
- **Critical path:** Image/text features → prompt selection → gradient computation → subspace projection → prompt update → task performance
- **Design tradeoffs:**
  - More prompt tokens increase capacity but make training harder
  - Larger prompt pool improves performance but increases memory
  - Higher SVD rank threshold preserves more information but reduces interference avoidance
  - Task order affects which conflicting spaces are identified
- **Failure signatures:**
  - Performance degradation on old tasks indicates insufficient anti-forgetting
  - Performance degradation on new tasks indicates negative forward transfer
  - Training instability suggests gradient projection parameters need adjustment
- **First 3 experiments:**
  1. Implement SVD analysis on input embeddings from two different tasks to visualize rank differences
  2. Test gradient projection to residual space for a single task to verify anti-forgetting
  3. Implement pre-trained space projection to test knowledge reuse for unseen tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Fwd-Prompt scale with different task orders in MCIT, and are there specific task sequences that consistently yield better or worse results?
- Basis in paper: [explicit] The paper conducts experiments on 6 different task orders and observes consistent performance improvements of Fwd-Prompt over EProj. However, the impact of specific task sequences is not thoroughly analyzed.
- Why unresolved: The paper provides average performance across task orders but does not delve into the nuances of how specific task sequences affect performance. Understanding these nuances could provide insights into optimizing task order for better continual learning.
- What evidence would resolve it: Conducting a detailed analysis of Fwd-Prompt's performance on various task sequences, identifying patterns or trends in performance based on task order, and comparing these findings with other baseline methods would provide a clearer understanding of the impact of task order.

### Open Question 2
- Question: What is the impact of varying the number of prompt tokens (np) and the size of the prompt pool (M) on the performance of Fwd-Prompt, and are there optimal values for these hyperparameters?
- Basis in paper: [explicit] The paper mentions variations of Fwd-Prompt with different values of np and M, showing that a larger prompt pool leads to better performance, while more prompt tokens do not. However, the paper does not explore a wide range of values or identify optimal settings.
- Why unresolved: The paper provides limited exploration of hyperparameter variations, and the impact of these variations on performance is not fully understood. Identifying optimal values for np and M could further enhance the performance of Fwd-Prompt.
- What evidence would resolve it: Conducting a comprehensive hyperparameter search, testing a wide range of values for np and M, and analyzing the resulting performance would help identify optimal settings for these hyperparameters.

### Open Question 3
- Question: How does Fwd-Prompt perform in scenarios with more than four tasks, and does its effectiveness diminish as the number of tasks increases?
- Basis in paper: [explicit] The paper evaluates Fwd-Prompt on four tasks and mentions experiments with more tasks by splitting the training set into three splits. However, the performance of Fwd-Prompt on a larger number of tasks is not thoroughly investigated.
- Why unresolved: The paper provides limited evidence on the scalability of Fwd-Prompt to more tasks. Understanding how Fwd-Prompt performs with an increasing number of tasks is crucial for its practical applicability in real-world scenarios.
- What evidence would resolve it: Conducting experiments with a larger number of tasks, evaluating the performance of Fwd-Prompt as the number of tasks increases, and comparing the results with baseline methods would provide insights into the scalability and effectiveness of Fwd-Prompt.

## Limitations
- The SVD analysis of input embeddings lacks empirical validation of claimed rank differences across tasks
- The mechanism assumes residual space directions are task-irrelevant without direct verification
- The pre-trained space projection for positive forward transfer relies on unverified assumptions about knowledge overlap

## Confidence
- **High confidence**: The experimental results showing Fwd-Prompt outperforms baselines on MCIT benchmarks (4.16% average accuracy improvement)
- **Medium confidence**: The gradient projection mechanism for anti-forgetting, as this follows established continual learning principles
- **Low confidence**: The SVD-based analysis of input embeddings as the root cause of both forgetting and negative forward transfer, since the empirical evidence is limited to qualitative observations

## Next Checks
1. **Ablation on SVD rank threshold**: Test Fwd-Prompt with different SVD rank thresholds (e.g., ϵ = 0.95, 0.99, 0.999) to verify that the choice of threshold meaningfully affects performance and that the claimed rank differences are practically significant.

2. **Random subspace projection control**: Implement a control experiment where prompt gradients are projected to random subspaces (rather than residual/pre-trained spaces) to test whether the specific choice of subspaces matters, or if any projection provides benefits.

3. **Task similarity analysis**: Analyze the correlation between task embedding similarity (using the SVD-derived core spaces) and the degree of negative forward transfer, to empirically validate whether tasks with more similar core spaces indeed exhibit less negative transfer under Fwd-Prompt.