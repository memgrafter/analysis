---
ver: rpa2
title: 'An X-Ray Is Worth 15 Features: Sparse Autoencoders for Interpretable Radiology
  Report Generation'
arxiv_id: '2410.03334'
source_url: https://arxiv.org/abs/2410.03334
tags:
- report
- sae-rad
- features
- reports
- radiology
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SAE-Rad, a novel framework that leverages
  sparse autoencoders (SAEs) to decompose latent representations from a pre-trained
  vision transformer into human-interpretable features for radiology report generation.
  The proposed hybrid architecture combines state-of-the-art SAE advancements to achieve
  accurate latent reconstructions while maintaining sparsity.
---

# An X-Ray Is Worth 15 Features: Sparse Autoencoders for Interpretable Radiology Report Generation

## Quick Facts
- arXiv ID: 2410.03334
- Source URL: https://arxiv.org/abs/2410.03334
- Authors: Ahmed Abdulaal; Hugo Fry; Nina Montaña-Brown; Ayodeji Ijishakin; Jack Gao; Stephanie Hyland; Daniel C. Alexander; Daniel C. Castro
- Reference count: 40
- Key outcome: SAE-Rad achieves competitive radiology-specific metrics compared to state-of-the-art models while using significantly fewer computational resources for training

## Executive Summary
This paper introduces SAE-Rad, a novel framework that leverages sparse autoencoders (SAEs) to decompose latent representations from a pre-trained vision transformer into human-interpretable features for radiology report generation. The proposed hybrid architecture combines state-of-the-art SAE advancements to achieve accurate latent reconstructions while maintaining sparsity. Using an off-the-shelf language model, SAE-Rad distills ground-truth reports into radiological descriptions for each SAE feature, which are then compiled into a full report for each image, eliminating the need for fine-tuning large models for this task. On the MIMIC-CXR dataset, SAE-Rad achieves competitive radiology-specific metrics compared to state-of-the-art models while using significantly fewer computational resources for training. Qualitative analysis reveals that SAE-Rad learns meaningful visual concepts and generates reports aligning closely with expert interpretations.

## Method Summary
SAE-Rad combines a Rad-DINO vision transformer with a sparse autoencoder to decompose image class tokens into interpretable features. The framework trains a gated SAE with unconstrained decoder norms, extracting features from frozen Rad-DINO embeddings. For each feature, an LLM analyzes ground-truth reports from highest-activating images to generate descriptive labels. Report generation combines feature importance scores with their descriptions using another LLM, eliminating the need for vision-language model fine-tuning. The approach is evaluated on MIMIC-CXR using radiology-specific metrics including RadFact logical precision/recall and CheXpert F1 scores.

## Key Results
- SAE-Rad achieves competitive RadFact logical precision and recall scores compared to state-of-the-art vision-language models
- The framework generates reports with strong CheXpert F1 scores (5 and 14 class) while using significantly fewer computational resources
- Qualitative analysis demonstrates that SAE-Rad learns meaningful visual concepts that align with expert interpretations of chest radiographs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SAE features capture clinically meaningful visual concepts from homogeneous chest radiograph data.
- Mechanism: Sparse autoencoders decompose high-dimensional image class tokens into an overcomplete dictionary of sparsely activating features, each representing distinct visual patterns.
- Core assumption: Visual concepts relevant to radiology reports are linearly separable in the activation space of pre-trained vision transformers.
- Evidence anchors:
  - [abstract] "Qualitative analysis reveals that SAE-Rad learns meaningful visual concepts and generates reports aligning closely with expert interpretations."
  - [section] "These interpretable features contribute to generating detailed and accurate radiology reports, as evidenced by strong performance in the clinical evaluation metrics."
  - [corpus] Weak - related papers focus on hallucination mitigation and report evaluation but not SAE-based feature decomposition.
- Break condition: If features become too sparse or too dense, they either fail to capture relevant concepts or become uninterpretable by the LLM.

### Mechanism 2
- Claim: Automated LLM-based interpretability reliably labels SAE features with plain-English descriptions.
- Mechanism: For each feature, the LLM analyzes ground-truth reports from highest-activating images to extract consistent patterns, generating a concise description.
- Core assumption: Ground-truth radiology reports contain sufficient descriptive overlap to identify common visual concepts for each feature.
- Evidence anchors:
  - [section] "We utilized a pre-trained and frozen language model fdescriptor : R 7→ d to generate a description d(i) for a feature i by analyzing the set R(i) as d(i) = fdescriptor(R(i))."
  - [section] "These interpretable features are labelled by use of pre-trained LLMs in an automated interpretability pipeline (Bricken et al., 2023)."
  - [corpus] Weak - related papers discuss report evaluation but not automated feature labeling via LLMs.
- Break condition: If ground-truth reports lack sufficient consistency or contain contradictory information, the LLM cannot generate accurate feature descriptions.

### Mechanism 3
- Claim: Composing feature descriptions via LLM generates coherent radiology reports without VLM fine-tuning.
- Mechanism: The LLM takes normalized feature importance scores and descriptions as input, producing a structured "findings" section that emphasizes high-importance features.
- Core assumption: Natural language descriptions of visual features can be combined into coherent medical reports without explicit multimodal training.
- Evidence anchors:
  - [abstract] "Using an off-the-shelf language model, we distil ground-truth reports into radiological descriptions for each SAE feature, which we then compile into a full report for each image, eliminating the need for fine-tuning large models for this task."
  - [section] "The automated radiology report R(x) is then generated by a pre-trained and frozen LLM from the descriptions of the active features."
  - [corpus] Weak - related papers focus on VLM fine-tuning approaches rather than LLM-based composition of interpretable features.
- Break condition: If feature descriptions lack sufficient detail or coherence, the LLM cannot generate clinically accurate reports.

## Foundational Learning

- Concept: Sparse autoencoders and feature decomposition
  - Why needed here: SAEs extract interpretable visual concepts from high-dimensional image representations, enabling feature-based report generation.
  - Quick check question: How does the L1 regularization in SAEs promote sparsity in feature activations?

- Concept: Mechanistic interpretability in vision transformers
  - Why needed here: Understanding how pre-trained vision transformers encode visual concepts allows extraction of meaningful features for downstream tasks.
  - Quick check question: What is the difference between the linear representation hypothesis and the superposition hypothesis?

- Concept: Automated feature labeling with LLMs
  - Why needed here: LLMs translate feature activations into human-readable descriptions without manual annotation.
  - Quick check question: How does the LLM identify common patterns across multiple ground-truth reports for a single feature?

## Architecture Onboarding

- Component map:
  Rad-DINO vision transformer → SAE encoder/decoder → feature importance scores → LLM-based feature labeling → LLM-based report composition

- Critical path:
  1. Extract class tokens from Rad-DINO
  2. Train SAE to decompose tokens into sparse features
  3. Generate feature descriptions using LLM and ground-truth reports
  4. Compose reports by combining active feature descriptions

- Design tradeoffs:
  - Sparsity vs. reconstruction quality: Higher sparsity may reduce reconstruction accuracy
  - Feature count vs. interpretability: More features may capture finer details but become harder to label
  - SAE architecture: Gated vs. standard vs. hybrid approaches for balancing sparsity and reconstruction

- Failure signatures:
  - Dead features (no activations) → insufficient training or overly strict sparsity
  - Poor reconstruction → inadequate SAE capacity or training
  - Uninterpretable feature descriptions → inconsistent ground-truth reports or inadequate LLM prompting

- First 3 experiments:
  1. Train SAE with different expansion factors (×32, ×64, ×128) and evaluate L0 and reconstruction accuracy
  2. Generate feature descriptions for a small set of features and manually verify interpretability
  3. Compose sample reports using active features and evaluate against ground-truth reports using RadFact

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions. However, it acknowledges that the modular nature of the pipeline allows for model substitution, suggesting potential areas for exploration such as testing different LLMs for feature interpretation and report generation.

## Limitations
- Interpretability relies primarily on automated LLM-based labeling with limited manual verification of feature quality
- Assumes sufficient consistency in ground-truth reports for feature labeling, which may not hold across different radiology datasets
- Focuses on a single dataset (MIMIC-CXR) with chest radiographs, limiting generalizability to other imaging modalities

## Confidence
- Medium confidence in SAE feature interpretability claims, as manual verification of feature quality is minimal
- Medium confidence in report generation quality, supported by competitive RadFact and CheXpert metrics but limited ablation studies
- Low confidence in generalizability across datasets and imaging modalities due to single-dataset evaluation

## Next Checks
1. Conduct manual verification of SAE feature interpretability by having radiologists assess a random sample of feature descriptions and their corresponding highest-activating images
2. Perform ablation studies comparing SAE-Rad with and without automated feature labeling to quantify the contribution of the interpretability pipeline
3. Test the framework on a different radiology dataset (e.g., PadChest) to evaluate generalizability across different institutions and imaging protocols