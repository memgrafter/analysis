---
ver: rpa2
title: Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge
arxiv_id: '2403.01432'
source_url: https://arxiv.org/abs/2403.01432
tags:
- knowledge
- retrieval
- performance
- fine-tuning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares fine-tuning (FT) and retrieval augmented generation
  (RAG) for improving language models (LMs) on less popular factual knowledge in question
  answering tasks. The authors conduct extensive experiments across twelve LMs of
  varying sizes, using different fine-tuning methods, data augmentation techniques,
  and retrieval models.
---

# Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge

## Quick Facts
- **arXiv ID**: 2403.01432
- **Source URL**: https://arxiv.org/abs/2403.01432
- **Reference count**: 40
- **Key outcome**: Stimulus RAG (SRAG) outperforms both fine-tuning and traditional RAG for less popular factual knowledge, eliminating the need for costly fine-tuning.

## Executive Summary
This paper systematically compares fine-tuning (FT) and retrieval augmented generation (RAG) for improving language models on less popular factual knowledge in question answering tasks. Through extensive experiments across twelve language models of varying sizes, different fine-tuning methods, and multiple retrieval models, the authors demonstrate that while FT improves accuracy across all entity popularity levels, it requires significant computational resources. RAG significantly outperforms FT, especially for the least popular knowledge, and smaller fine-tuned LMs with RAG can match or exceed the performance of larger LMs. The authors propose Stimulus RAG (SRAG), a novel approach that guides LMs using extracted hints from retrieved documents, which surpasses the performance of both FT and traditional RAG methods.

## Method Summary
The authors evaluate four configurations: vanilla LM (-FT-RAG), vanilla with RAG (-FT+RAG), fine-tuned without RAG (+FT-RAG), and fine-tuned with RAG (+FT+RAG). They use three Wikipedia-based QA datasets (PopQA, WitQA, EQ) containing long-tail entities and Wikipedia documents as the evidence corpus. Synthetic QA pairs are generated using prompt-based (Zephyr with Chain of Thought) and E2E (T5-large) methods. Fine-tuning is performed using both full fine-tuning and parameter-efficient fine-tuning (QLoRA). Retrieval models include BM25, DPR, and Contriever. The primary metric is accuracy, where predictions are correct if the ground truth response matches a substring of the predicted response.

## Key Results
- Fine-tuning improves accuracy across all entity popularity levels but requires significant resources
- RAG significantly outperforms FT, especially for least popular knowledge
- Smaller fine-tuned LMs with RAG can match or exceed the performance of larger LMs
- Retrieval model performance directly impacts RAG effectiveness
- Stimulus RAG (SRAG) surpasses the performance of both FT and traditional RAG methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Stimulus RAG (SRAG) outperforms both fine-tuning (FT) and traditional RAG by highlighting the most relevant sentence in the input prompt, giving it higher attention from the language model.
- **Mechanism**: The hint extractor ranks sentences from the top-K retrieved documents and places the top-ranked sentence at the very beginning of the prompt. Since LMs attend more to earlier tokens in the prompt, this increases the likelihood that the LM will focus on the most relevant information.
- **Core assumption**: Placing a relevant hint at the start of the prompt significantly improves the LM's ability to generate correct answers, even without modifying its parameters.
- **Evidence anchors**:
  - [abstract] SRAG "stimulates an LM to generate the correct response based on the provided hint in the prompt" and "outperforms all other combinations of fine-tuning, both with and without retrieve-then-generate RAG."
  - [section] "The extracted hint is placed at the top of the input prompt provided to the Generator component. This implies that the hint text a repetition of presumably the most relevant sentence/document of in the prompt. It has been shown that the beginning of the input prompt receives more attention from LMs [36]."

### Mechanism 2
- **Claim**: Fine-tuning improves accuracy on less popular factual knowledge by embedding the knowledge directly into the LM's parameters, but it degrades reasoning ability for larger models.
- **Mechanism**: Fine-tuning updates the model weights using synthetically generated QA pairs, allowing the LM to "memorize" new knowledge. For smaller models, this improves performance both with and without RAG. For larger models, it reduces flexibility in reasoning, hurting performance when combined with RAG.
- **Core assumption**: Parameter updates improve memorization but can reduce the model's ability to reason from retrieved context, especially in larger models with more parameters to update.
- **Evidence anchors**:
  - [section] "Fine-tuning improves accuracy in answering factual questions, both with and without RAG, it demands a considerable amount of effort and resources" and "larger LMs generally do not benefit from fine-tuning, while smaller ones do."
  - [section] "although FT injects knowledge into LMs (as seen when comparing -FT-RAG with +FT-RAG), it diminishes the reasoning abilities of larger LMs."

### Mechanism 3
- **Claim**: Retrieval model performance is critical for RAG success, and its effectiveness decreases as entity popularity increases due to noisier documents.
- **Mechanism**: The retriever's Recall@1, Recall@3, and Recall@5 scores directly correlate with downstream QA accuracy. Less popular entities have fewer relevant documents, so the retriever's precision is higher. As popularity increases, more noisy or irrelevant documents are retrieved, lowering accuracy.
- **Core assumption**: The retriever's ability to rank the most relevant document highly is the dominant factor in RAG's effectiveness.
- **Evidence anchors**:
  - [section] "Comparing retrievers with varying performance in the RAG system, we observe that as the popularity of factual knowledge increases, the performance of the retriever decreases" and "the performance of the RAG system increases by using higher performance retriever."
  - [section] "It is evident that RAG significantly increases accuracy for the least popular entities" and "the retriever's performance decreases as popularity increases."

## Foundational Learning

- **Concept**: Language model memorization vs. reasoning
  - Why needed here: The paper contrasts parametric knowledge (memorization via FT) with non-parametric knowledge (reasoning via RAG). Understanding this distinction is key to interpreting why FT helps smaller models but hurts larger ones.
  - Quick check question: If an LM is fine-tuned on a narrow dataset, will it be better or worse at answering questions outside that dataset? Why?

- **Concept**: Retrieval-augmented generation pipeline
  - Why needed here: The experiments compare multiple retrievers (BM25, DPR, Contriever) and retrieval strategies. Knowing how retrievers and generators interact is essential for understanding the results.
  - Quick check question: What happens to RAG performance if the retriever's top-1 result is irrelevant? How does adding more documents affect accuracy?

- **Concept**: Data augmentation for low-resource domains
  - Why needed here: FT requires training data, which is scarce for less popular entities. The paper evaluates synthetic QA generation methods to create this data.
  - Quick check question: Why might a prompt-based QA generator produce higher-quality synthetic data than an E2E fine-tuned generator, even if it produces fewer samples?

## Architecture Onboarding

- **Component map**:
  Corpus preparation → Synthetic QA generation (prompt or E2E) → Fine-tuning (full or PEFT) → Retrieval model (BM25/DPR/Contriever) → Generator (LM) → Stimulus RAG (optional)

- **Critical path**:
  - For FT: Corpus → Synthetic QA → Fine-tuning → Evaluation
  - For RAG: Corpus → Retrieval model → Top-K documents → Generator → Evaluation
  - For SRAG: Corpus → Retrieval model → Hint extraction → Modified prompt → Generator → Evaluation

- **Design tradeoffs**:
  - Full FT vs. PEFT: Full FT gives better downstream performance but is computationally expensive and may hurt reasoning in large models. PEFT is cheaper and preserves reasoning.
  - Prompt-based vs. E2E DA: Prompt-based generates fewer but higher-quality QAs; E2E generates more but noisier QAs.
  - Number of retrieved documents: More documents can add noise; 3 seems optimal in most cases.

- **Failure signatures**:
  - FT fails: Synthetic data is noisy, fine-tuning overfits, or model is too large.
  - RAG fails: Retriever returns irrelevant documents, or LM ignores context.
  - SRAG fails: Hint is irrelevant or misleading, or LM overweights the hint.

- **First 3 experiments**:
  1. Run RAG with Ideal retriever on PopQA to establish upper bound performance.
  2. Compare full FT vs. PEFT on a small LM (FlanT5-small) using prompt-generated data.
  3. Implement SRAG with DPR retriever and test on EQ dataset to see if hint-based prompting matches or exceeds +FT+RAG.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the performance of Stimulus RAG (SRAG) compare to fine-tuning across different LM sizes and types when using a retrieval model with Recall@K scores between 80-90%?
- **Basis in paper**: [explicit] The paper compares SRAG to FT+RAG across various LMs but uses an ideal retriever (near 100% recall) in most experiments
- **Why unresolved**: The paper's main comparison uses an ideal retriever, but real-world retrievers have lower recall scores, which could impact the relative effectiveness of SRAG vs FT
- **What evidence would resolve it**: Comparative experiments using SRAG vs FT+RAG with retrieval models achieving 80-90% recall on relevant documents

### Open Question 2
- **Question**: Does the effectiveness of SRAG vary based on the complexity or length of the extracted hint sentence, and what is the optimal hint length for different types of factual questions?
- **Basis in paper**: [inferred] SRAG uses top-ranked sentences as hints, but the paper doesn't analyze how hint characteristics affect performance
- **Why unresolved**: The paper demonstrates SRAG's effectiveness but doesn't investigate the relationship between hint properties and answer accuracy
- **What evidence would resolve it**: Controlled experiments varying hint length and complexity across different question types to identify optimal hint characteristics

### Open Question 3
- **Question**: How does SRAG's performance degrade when the retriever fails to include the correct answer-containing document in the top-K results, and what is the maximum acceptable retrieval error rate?
- **Basis in paper**: [explicit] The paper acknowledges the ideal retriever assumption is "not entirely accurate" and shows retrieval performance varies with entity popularity
- **Why unresolved**: The paper demonstrates SRAG's advantages with near-perfect retrieval but doesn't quantify its robustness to retrieval errors
- **What evidence would resolve it**: Performance analysis of SRAG across different levels of retrieval error rates and comparison with alternative approaches under the same conditions

### Open Question 4
- **Question**: Can SRAG be effectively combined with PEFT to achieve even better performance than either approach alone, particularly for smaller LMs with limited memorization capacity?
- **Basis in paper**: [inferred] The paper shows PEFT preserves reasoning abilities while SRAG guides generation, suggesting potential complementarity
- **Why unresolved**: The paper compares SRAG against FT+RAG combinations but doesn't explore hybrid approaches that combine SRAG with parameter-efficient fine-tuning
- **What evidence would resolve it**: Experiments comparing SRAG alone, PEFT+RAG, and combined SRAG+PEFT approaches across different LM sizes and entity popularity levels

## Limitations

- The paper does not specify exact hyperparameters for QLoRA fine-tuning, making exact reproduction difficult
- All experiments rely on synthetic data generation for fine-tuning, which may not fully capture the distribution of real user queries
- The evaluation focuses solely on accuracy without examining other important dimensions like faithfulness, hallucination, or computational efficiency

## Confidence

**High Confidence:**
- FT improves accuracy for less popular knowledge across all LM sizes
- RAG significantly outperforms FT, especially for least popular entities
- Retrieval model performance directly impacts RAG effectiveness

**Medium Confidence:**
- Smaller fine-tuned LMs with RAG can match or exceed larger LMs
- FT degrades reasoning ability in larger models
- 3 retrieved documents is optimal

**Low Confidence:**
- SRAG eliminates the need for costly fine-tuning
- Fine-tuning is universally resource-intensive
- The hint extraction mechanism works as described

## Next Checks

1. **Retrieval Quality Validation**: Verify the retriever performance degradation pattern by plotting retriever Recall@1 against entity popularity percentiles across all three datasets. Check if the claimed correlation between retriever performance and downstream accuracy holds consistently.

2. **Fine-tuning vs. SRAG Cost-Benefit Analysis**: Measure and compare the total computational resources (GPU hours) required for: (a) generating synthetic data, (b) full fine-tuning on multiple LMs, versus (c) implementing SRAG with various retrievers. Validate the claimed cost-effectiveness of SRAG.

3. **Generalization Testing**: Apply the SRAG approach to a held-out test set from a different domain (e.g., scientific literature or news articles) to verify whether the hint-based prompting mechanism generalizes beyond Wikipedia-based factual knowledge.