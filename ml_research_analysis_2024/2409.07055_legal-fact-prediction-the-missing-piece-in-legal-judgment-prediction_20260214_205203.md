---
ver: rpa2
title: 'Legal Fact Prediction: The Missing Piece in Legal Judgment Prediction'
arxiv_id: '2409.07055'
source_url: https://arxiv.org/abs/2409.07055
tags:
- legal
- evidence
- facts
- fact
- loan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the task of legal fact prediction (LFP),
  which predicts legal facts from evidence lists before a court trial, enabling fact-based
  legal judgment prediction in the absence of ground-truth legal facts. To support
  this task, the authors construct LFP-Loan, a benchmark dataset containing 381 civil
  loan cases with evidence lists and corresponding legal facts.
---

# Legal Fact Prediction: The Missing Piece in Legal Judgment Prediction

## Quick Facts
- arXiv ID: 2409.07055
- Source URL: https://arxiv.org/abs/2409.07055
- Reference count: 11
- Primary result: LLM-based legal fact prediction achieves 0.7095 average accuracy for fact descriptions but struggles with contested items like loan dates and repayment amounts.

## Executive Summary
This paper introduces legal fact prediction (LFP), a task that predicts legal facts from evidence lists before court trials, enabling fact-based legal judgment prediction without ground-truth legal facts. The authors construct LFP-Loan, a benchmark dataset of 381 civil loan cases with evidence lists and corresponding legal facts. Experiments with two LLM-powered baseline methods (QA-based and simulation-based) show promising performance on clear facts but significant challenges with contentious items like loan dates, repayment dates, and repaid amounts. The work addresses a critical gap in legal judgment prediction by enabling systems to operate without requiring ground-truth facts, which are typically unavailable in real-world scenarios.

## Method Summary
The authors propose legal fact prediction as a task where an LLM predicts legal facts from evidence lists provided by plaintiff and defendant before trial. Two baseline approaches are implemented: a QA-based method where the LLM directly predicts facts from evidence, and a simulation-based method where multiple LLM agents role-play as judge, plaintiff, and defendant to simulate trial proceedings. Both methods are enhanced with few-shot learning using real legal facts from other cases. The LFP-Loan dataset is constructed by extracting evidence lists from trial transcripts and legal facts from court judgments. Performance is evaluated using cosine similarity for fact descriptions and strict accuracy for key items (loan amount, interest, loan date, repayment date, repaid amount).

## Key Results
- Baseline approaches achieve average 0.7095 accuracy for fact descriptions but only 0.4199 accuracy for disputed key items.
- Few-shot learning improves accuracy on contested facts by incorporating real judges' reasoning patterns.
- Simulation-based approach can clarify complex details through debate but risks LLM hallucination and fact fabrication.
- Performance significantly drops on contentious items like loan dates, repayment dates, and repaid amounts due to complex reasoning requirements.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Legal fact prediction enables fact-based legal judgment prediction in the absence of ground-truth legal facts by leveraging evidence lists.
- Mechanism: The task bridges the gap between evidence availability and legal judgment by predicting legal facts from evidence lists before court trials, enabling downstream judgment prediction systems to operate without requiring ground-truth facts.
- Core assumption: Legal facts can be predicted with reasonable accuracy from evidence lists alone, even without access to trial proceedings or judicial questioning.
- Evidence anchors:
  - [abstract] "which predicts legal facts from evidence lists before a court trial, enabling fact-based legal judgment prediction in the absence of ground-truth legal facts"
  - [section 2] "Formally, the task of legal fact prediction can be defined as follows: Given the evidence list provided by both the plaintiff and the defendant, predict the legal fact that the court will determine"
  - [corpus] Weak - no direct evidence in related papers about LFP specifically, only general LJP approaches
- Break condition: If evidence lists do not contain sufficient information to determine contested legal facts, or if conflicting evidence cannot be resolved without trial proceedings.

### Mechanism 2
- Claim: Few-shot learning improves LLM performance on legal fact prediction by incorporating real judges' reasoning processes.
- Mechanism: By presenting real legal facts from other cases to the LLM judge, the model learns the reasoning patterns used by human judges, improving its ability to predict contested facts like loan dates and repayment amounts.
- Core assumption: LLM judges can learn and apply human judges' reasoning patterns when provided with examples of real legal fact determinations.
- Evidence anchors:
  - [section 4.1] "Additionally, we enhance the performance of the LLM judge in the baseline approaches via few-shot learning... we present the real legal facts of three other cases to the LLM judge, which allows for learning the reasoning processes of real-life judges"
  - [section 4.2] "Comparing the different methods, we can observe that few-shot learning improves the accuracy of predicting legal facts, especially for those key items that are typically the focus of disputes"
  - [corpus] Assumption: Related papers on LJP don't specifically discuss few-shot learning for fact prediction, though they mention various enhancement techniques
- Break condition: If the few-shot examples are not representative of the case types in the target dataset, or if the reasoning patterns are too case-specific to generalize.

### Mechanism 3
- Claim: Simulation-based approaches can clarify complex details through agent-based debate, but may introduce hallucination problems.
- Mechanism: Multiple LLM agents role-play as judge, plaintiff, and defendant to simulate trial proceedings, allowing the model to clarify complex details through debate, but this may lead to fabricated facts that worsen hallucination issues.
- Core assumption: LLM agents can effectively simulate legal reasoning and debate processes to clarify complex factual scenarios.
- Evidence anchors:
  - [section 4.1] "2. Simulation-based method (Wu et al., 2023; He et al., 2024b): As shown in Figure 3, we simulate a legal trial using multiple LLM agents, with each agent role-playing as the judge, plaintiff, or defendant"
  - [section 4.2] "On the one hand, LLM agents can assist judges in clarifying complex details, such as multiple repayment behaviors, through simulated trials. On the other hand, LLM agents, in an attempt to win debates during these simulations, may fabricate facts, exacerbating the hallucination problem of LLMs"
  - [corpus] Assumption: Related papers mention simulation approaches but don't specifically address hallucination problems in legal fact prediction
- Break condition: If LLM agents consistently fabricate facts during simulation, or if the simulated debate doesn't converge on reasonable fact determinations.

## Foundational Learning

- Concept: Evidence list construction from trial transcripts
  - Why needed here: Since evidence lists are typically not publicly available, the task requires extracting evidence information from trial transcripts to create realistic benchmark datasets
  - Quick check question: How does the proposed method extract evidence lists from trial transcripts when the original evidence submissions are not available?

- Concept: Legal fact vs. evidence distinction
  - Why needed here: Understanding the difference between evidence (submitted by parties) and legal facts (determined by judge) is crucial for grasping why this task is necessary and challenging
  - Quick check question: What makes legal facts different from raw evidence, and why can't we directly use evidence for judgment prediction?

- Concept: Few-shot learning for legal reasoning
  - Why needed here: The paper demonstrates that providing examples of real legal fact determinations can improve LLM performance, suggesting this technique is valuable for legal reasoning tasks
  - Quick check question: How does presenting real legal facts from other cases help an LLM judge learn to predict facts in new cases?

## Architecture Onboarding

- Component map: Evidence List Input → Preprocessing → LLM Judge (QA-based or Simulation-based) → Fact Prediction Output
- Critical path: Evidence list preprocessing → LLM fact prediction → Key item extraction → Accuracy evaluation
- Design tradeoffs: QA-based vs. Simulation-based approaches - QA is simpler but may miss complex reasoning; Simulation can handle complexity but risks hallucination
- Failure signatures: Low accuracy on contested items (loan dates, repayment dates, repaid amounts) indicates inability to resolve conflicting evidence without trial information
- First 3 experiments:
  1. Run baseline QA-based method on LFP-Loan dataset and measure cosine similarity for fact descriptions and strict accuracy for key items
  2. Implement few-shot learning enhancement and compare performance improvements on contested vs. clear facts
  3. Test simulation-based approach and analyze whether debate clarifies complex details or introduces more hallucinations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop LLM-based approaches that more effectively handle contentious legal facts, such as disputed loan dates, repayment dates, and repaid amounts?
- Basis in paper: [explicit] The paper states that baseline approaches perform poorly on legal facts that are often the points of contention in cases, including loan dates, repayment dates, interest amounts, and repaid amounts, achieving low accuracy (e.g., 0.4199) for disputed key items.
- Why unresolved: The paper shows that current methods struggle with reasoning and inference based on conflicting evidence from the parties, which is essential for resolving contentious legal facts.
- What evidence would resolve it: Comparative experiments evaluating new LLM architectures or prompting strategies that explicitly model conflicting evidence and debate, showing improved accuracy on contentious legal facts.

### Open Question 2
- Question: Can legal fact prediction be effectively adapted to different stages of a trial by incorporating varying types of trial-related information beyond evidence lists?
- Basis in paper: [explicit] The paper discusses that ideally, the input for legal fact prediction should be any information available to the court or parties before judgment, such as complaints, defenses, and evidence submitted by the parties, which varies depending on the trial stage.
- Why unresolved: The paper only uses evidence lists as input and does not explore how incorporating other types of trial-related information at different stages might improve prediction accuracy.
- What evidence would resolve it: Experimental results comparing the performance of legal fact prediction models using different combinations of trial-related information (e.g., complaints, defenses, evidence) at various trial stages.

### Open Question 3
- Question: How can we mitigate the hallucination problem of LLM agents during simulated trials, where agents may fabricate facts while attempting to win debates?
- Basis in paper: [explicit] The paper notes that during simulation-based methods, LLM agents, in an attempt to win debates, may fabricate facts, exacerbating the hallucination problem of LLMs.
- Why unresolved: The paper acknowledges this issue but does not provide solutions for preventing or mitigating fact fabrication by LLM agents during simulated trials.
- What evidence would resolve it: Experiments demonstrating the effectiveness of techniques such as fact-checking mechanisms, confidence scoring, or constrained generation in reducing hallucinations during LLM-based simulated trials.

## Limitations

- Evaluation metrics (cosine similarity and strict accuracy) may not fully capture nuanced correctness of legal fact predictions, especially for contested items where partial correctness matters.
- The LFP-Loan dataset contains only 381 civil loan cases, limiting generalizability to other legal domains and evidentiary structures.
- The paper does not quantitatively measure or mitigate hallucination risks in the simulation-based approach, despite acknowledging this as a significant concern.

## Confidence

- **High Confidence**: The task definition of legal fact prediction from evidence lists is well-established and clearly motivated by the gap in existing LJP approaches that assume ground-truth legal facts are available.
- **Medium Confidence**: The two baseline methods (QA-based and simulation-based) are technically sound and appropriately implemented, though their performance limitations on contested items are expected given the complexity of resolving contradictory evidence.
- **Medium Confidence**: The few-shot learning enhancement shows measurable improvements, but the specific examples and their selection criteria are not detailed, making it difficult to assess generalizability.

## Next Checks

1. **Cross-Domain Validation**: Test the LFP framework on a different legal domain (e.g., contract disputes or property cases) to assess generalizability beyond civil loan cases.

2. **Hallucination Quantification**: Implement systematic measurement of hallucination rates in the simulation-based approach by comparing predicted facts against a human-annotated gold standard for fact fabrication.

3. **Partial Credit Evaluation**: Develop evaluation metrics that provide partial credit for partially correct legal fact predictions, particularly for contested items where strict accuracy may be too harsh.