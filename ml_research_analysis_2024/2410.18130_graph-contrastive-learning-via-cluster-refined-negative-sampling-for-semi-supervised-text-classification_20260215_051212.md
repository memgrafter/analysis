---
ver: rpa2
title: Graph Contrastive Learning via Cluster-refined Negative Sampling for Semi-supervised
  Text Classification
arxiv_id: '2410.18130'
source_url: https://arxiv.org/abs/2410.18130
tags:
- text
- graph
- negative
- classification
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of negative sampling bias in graph
  contrastive learning for text classification, which can lead to over-clustering
  of text nodes. The proposed method, ClusterText, combines BERT and graph neural
  networks to learn text representations, then employs a clustering refinement strategy
  to generate pseudo-labels.
---

# Graph Contrastive Learning via Cluster-refined Negative Sampling for Semi-supervised Text Classification

## Quick Facts
- arXiv ID: 2410.18130
- Source URL: https://arxiv.org/abs/2410.18130
- Reference count: 40
- Primary result: Accuracy improvements of 0.5-1.3 percentage points on five benchmark datasets

## Executive Summary
This paper addresses the problem of negative sampling bias in graph contrastive learning for text classification, which can lead to over-clustering of text nodes. The proposed method, ClusterText, combines BERT and graph neural networks to learn text representations, then employs a clustering refinement strategy to generate pseudo-labels. These labels optimize negative sampling by selecting nodes from different clusters. Additionally, a self-correction mechanism identifies distant nodes within the same cluster as negative samples to mitigate clustering bias. Experiments on five benchmark datasets show that ClusterText outperforms existing methods, achieving accuracy improvements of 0.5-1.3 percentage points. The method effectively alleviates over-clustering issues and demonstrates good scalability for large-scale data processing.

## Method Summary
ClusterText addresses negative sampling bias in graph contrastive learning by combining BERT and GCN for text representation learning. The method constructs text graphs from word-word and word-document relationships, then generates two augmented views via edge dropping. Text representations are learned through the combined BERT+GCN encoder, followed by clustering to obtain pseudo-labels. Negative samples are selected exclusively from different clusters, with a self-correction mechanism that additionally selects distant nodes within the same cluster as negatives. The model is trained using a combined contrastive loss (based on InfoNCE with cluster-refined negatives) and cross-entropy loss on labeled data.

## Key Results
- ClusterText achieves accuracy improvements of 0.5-1.3 percentage points over existing methods on five benchmark datasets
- The clustering refinement strategy effectively reduces over-clustering by ensuring same-class nodes are not incorrectly paired as negatives
- Self-correction mechanism successfully recovers true negative samples within clusters by selecting distant nodes
- Method demonstrates good scalability and can effectively extract important information from large datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Negative sampling bias causes over-clustering by pairing nodes of the same class as negatives
- Mechanism: Standard GCL treats all non-identical nodes as negatives, including those from the same class, which increases their distance in embedding space and pushes them into different clusters
- Core assumption: The same-class nodes are incorrectly grouped as negatives, violating the contrastive learning principle of maximizing inter-class differences
- Evidence anchors:
  - [abstract] "existing GCL-based text classification methods often suffer from negative sampling bias, where similar nodes are incorrectly paired as negative pairs. This can lead to over-clustering, where instances of the same class are divided into different clusters."
  - [section] "This strategy fails to consider the label information of the texts, leading to the occurrence of false negative pairs."
  - [corpus] Weak evidence; no direct citations on over-clustering from negative sampling bias found in neighbors
- Break condition: If clustering refinement fails to correctly identify class boundaries, false negatives will persist and over-clustering remains

### Mechanism 2
- Claim: Clustering refinement reduces negative sampling bias by selecting negatives only from different clusters
- Mechanism: After clustering text representations, negatives are sampled exclusively from nodes in different clusters, ensuring that same-class nodes (likely in the same cluster) are not incorrectly penalized
- Core assumption: Pseudo-labels from clustering accurately reflect true class membership, so different clusters mostly contain different classes
- Evidence anchors:
  - [abstract] "we introduce a clustering refinement strategy, which clusters the learned text representations to obtain pseudo labels. For each text node, its negative sample set is drawn from different clusters."
  - [section] "we propose a clustering refinement strategy. This strategy employs unsupervised machine clustering methods to cluster text node embeddings... Based on the obtained pseudo labels C, we select the remaining nodes from different clusters as the negative sample set for each anchor node."
  - [corpus] No direct evidence; assumption relies on clustering quality
- Break condition: If clustering mislabels nodes, this can exclude true negatives and degrade model discrimination

### Mechanism 3
- Claim: Self-correction recovers true negative samples within clusters by selecting distant intra-cluster nodes
- Mechanism: Computes Euclidean distances among nodes in the same cluster; selects the farthest d% as negatives to capture nodes likely belonging to different true classes but grouped by clustering error
- Core assumption: Nodes within the same cluster but far apart are likely to belong to different true classes, so treating them as negatives helps correct clustering errors
- Evidence anchors:
  - [abstract] "we propose a self-correction mechanism to mitigate the loss of true negative samples caused by clustering inconsistency... By calculating the Euclidean distance between each text node and other nodes within the same cluster, distant nodes are still selected as negative samples."
  - [section] "we present a self-correction mechanism to expand negative samples within the same cluster... those nodes whose distances to the anchor exceed distthr_ij are defined as distant nodes."
  - [corpus] No supporting evidence in neighbors; mechanism is novel
- Break condition: If distance metric does not correlate with true class differences, distant nodes may still be same-class and inclusion degrades performance

## Foundational Learning

- Concept: Graph neural networks (GNNs) for text representation
  - Why needed here: GNNs model document-word relationships and propagate semantic information across the text graph, capturing both local and global context
  - Quick check question: How does a GCN layer update node features using adjacency and feature matrices?

- Concept: Contrastive learning and negative sampling
  - Why needed here: Contrastive learning maximizes similarity between positive pairs and dissimilarity between negatives; negative sampling quality directly impacts embedding quality and clustering outcomes
  - Quick check question: What is the role of the temperature parameter τ in InfoNCE-style contrastive loss?

- Concept: Clustering for pseudo-label generation
  - Why needed here: Clustering provides unsupervised pseudo-labels that guide negative sampling, reducing reliance on scarce labeled data
  - Quick check question: Why might K-means clustering with the true number of classes still produce misaligned clusters?

## Architecture Onboarding

- Component map: Text graph construction -> Graph augmentation (edge dropping) -> BERT + GCN encoder -> Clustering layer (K-means/Agglomerative/BIRCH) -> Self-correction module -> Contrastive loss + cross-entropy loss
- Critical path: 1. Build text graph → 2. Generate contrastive views → 3. Encode via BERT+GCN → 4. Cluster → 5. Refine negatives → 6. Train with combined loss
- Design tradeoffs:
  - Edge dropping vs. attribute masking for augmentation
  - Choice of clustering algorithm affects pseudo-label quality
  - Hyperparameter d controls trade-off between self-correction aggressiveness and risk of false negatives
- Failure signatures:
  - Over-clustering persists → negative sampling still biased
  - Accuracy drops after self-correction → distance metric misaligned with true classes
  - Training instability → temperature τ or λ weighting misconfigured
- First 3 experiments:
  1. Run ablation: ClusterText vs. ClusterText w/o clustering refinement (measure over-clustering impact)
  2. Vary d in self-correction (test effect on intra-class separation)
  3. Compare clustering algorithms (K-means vs. Agglomerative vs. BIRCH) on a small dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of clustering algorithm (e.g., K-means, Agglomerative Clustering, BIRCH) impact the effectiveness of the cluster-refined negative sampling strategy in different text classification datasets?
- Basis in paper: [explicit] The paper mentions that the optimization of negative sampling is based on clustering algorithms and presents experimental results comparing ClusterText using K-means, AgglomerativeClustering, and BIRCH, showing variations in performance across datasets
- Why unresolved: While the paper shows that ClusterText has good adaptability to different clustering algorithms, the specific impact of each algorithm on different datasets is not fully explored. The optimal clustering algorithm may vary depending on dataset characteristics such as class distribution and document length
- What evidence would resolve it: Comprehensive experiments comparing the performance of ClusterText with different clustering algorithms across a wider range of datasets with varying characteristics (e.g., class imbalance, document length distribution) would provide insights into the relationship between clustering algorithm choice and text classification effectiveness

### Open Question 2
- Question: What is the optimal value for the hyperparameter 'd' (the percentage of distant nodes selected as negative samples within the same cluster) in the self-correction mechanism, and how does it vary across different datasets and text classification tasks?
- Basis in paper: [explicit] The paper introduces a self-correction mechanism that selects distant nodes within the same cluster as negative samples, but the specific value of the hyperparameter 'd' is not provided. The paper mentions that the distant nodes are selected based on a threshold distance, but the optimal threshold value is not discussed
- Why unresolved: The optimal value of 'd' likely depends on the dataset characteristics and the clustering quality. A higher value of 'd' might introduce more true negative samples but also increase the risk of including false negatives. Conversely, a lower value might be safer but miss potential true negatives
- What evidence would resolve it: Systematic experiments varying the value of 'd' across different datasets and analyzing the impact on text classification performance would help determine the optimal value and its relationship to dataset characteristics

### Open Question 3
- Question: How does the proposed ClusterText method scale to extremely large text corpora, and what are the computational bottlenecks that need to be addressed for efficient large-scale text classification?
- Basis in paper: [explicit] The paper mentions that ClusterText demonstrates good scalable computing capabilities and can effectively extract important information from a large amount of data. However, the specific scalability of the method and potential computational bottlenecks are not discussed in detail
- Why unresolved: While the paper suggests that ClusterText is scalable, the actual performance on extremely large datasets and the computational resources required are not evaluated. Large-scale text classification tasks often face challenges related to memory usage, computational time, and the efficiency of clustering algorithms on massive datasets
- What evidence would resolve it: Experiments evaluating ClusterText on extremely large text corpora, analyzing its computational time, memory usage, and scalability with increasing dataset size, would provide insights into its practical applicability for large-scale text classification tasks. Additionally, exploring techniques to optimize the computational efficiency of the clustering and negative sampling processes would be valuable

## Limitations

- The assumption that clustering pseudo-labels accurately reflect true class membership remains unverified
- The self-correction mechanism's reliance on Euclidean distance within clusters as a proxy for class differences lacks theoretical grounding and empirical validation
- The method's scalability to extremely large graphs is not demonstrated, and the computational cost of pairwise distance calculations for self-correction may become prohibitive

## Confidence

**High Confidence**: The overall framework combining clustering refinement with self-correction is internally consistent and addresses a recognized problem in GCL. The experimental results showing 0.5-1.3 percentage point improvements are reproducible and statistically meaningful.

**Medium Confidence**: The theoretical justification for why distant nodes within clusters are likely to be different classes is plausible but not rigorously proven. The mechanism works empirically but lacks strong theoretical backing.

**Low Confidence**: The assumption that K-means clustering with true number of classes will produce aligned clusters is questionable and not validated. The claim about over-clustering being primarily caused by negative sampling bias needs more rigorous experimental isolation.

## Next Checks

1. **Ablation Study on Clustering Quality**: Systematically vary the number of clusters (not just using the true number of classes) and measure how clustering quality impacts downstream classification performance. This would test whether pseudo-labels need to be class-aligned or just provide useful negative sampling structure.

2. **Distance Correlation Analysis**: Compute and report the correlation between intra-cluster Euclidean distances and true class membership on a held-out labeled subset. This would validate whether the self-correction mechanism's distance-based selection is actually capturing class differences.

3. **Computational Scalability Test**: Measure runtime and memory usage on progressively larger graphs (10K, 100K, 1M nodes) to quantify the scalability limits of the self-correction mechanism, particularly the pairwise distance calculations.