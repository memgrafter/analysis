---
ver: rpa2
title: 'FissionFusion: Fast Geometric Generation and Hierarchical Souping for Medical
  Image Analysis'
arxiv_id: '2403.13341'
source_url: https://arxiv.org/abs/2403.13341
tags:
- learning
- datasets
- medical
- rate
- best
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a fast model generation and hierarchical averaging
  approach for medical image analysis. The authors identify that medical imaging datasets
  have rougher error landscapes compared to natural image datasets, making standard
  model averaging techniques like model soups less effective.
---

# FissionFusion: Fast Geometric Generation and Hierarchical Souping for Medical Image Analysis

## Quick Facts
- **arXiv ID:** 2403.13341
- **Source URL:** https://arxiv.org/abs/2403.13341
- **Reference count:** 36
- **Primary result:** Fast model generation and hierarchical averaging approach for medical image analysis

## Executive Summary
This paper addresses the challenge of model averaging in medical image analysis, where datasets exhibit rougher error landscapes compared to natural images. The authors propose FissionFusion, a method combining Fast Geometric Generation (FGG) with Hierarchical Souping (HS) to improve model averaging effectiveness. FGG uses cyclical learning rates to efficiently generate diverse models, while HS performs both local and global model averaging at different levels. The approach demonstrates significant performance improvements over standard model soups, achieving approximately 6% gains in accuracy/AUC on HAM10000 and CheXpert datasets, along with better out-of-distribution generalization.

## Method Summary
The proposed FissionFusion approach tackles the unique challenges of medical imaging by introducing two complementary techniques. Fast Geometric Generation (FGG) employs a cyclical learning rate scheduler to efficiently generate a diverse set of models from a single training run, addressing the computational cost of traditional model generation methods. Hierarchical Souping (HS) then performs model averaging at two levels: local averaging within similar model clusters and global averaging across all models. This hierarchical approach is specifically designed to handle the rougher error landscapes typical of medical imaging datasets, where standard model soups often underperform. The method is evaluated across multiple medical imaging datasets, demonstrating consistent improvements in both in-distribution and out-of-distribution performance.

## Key Results
- FGG+HS achieves approximately 6% gain in accuracy/AUC on HAM10000 and CheXpert datasets compared to standard model soups
- The approach shows better out-of-distribution generalization performance
- Computational efficiency is demonstrated through faster convergence compared to traditional model generation approaches

## Why This Works (Mechanism)
The effectiveness of FissionFusion stems from addressing the unique characteristics of medical imaging datasets. Medical images typically present rougher error landscapes with more complex optimization surfaces compared to natural images. Standard model soups struggle in these conditions because they assume smoother error surfaces where model averaging can effectively smooth out local minima. FGG's cyclical learning rate strategy helps explore this complex landscape more efficiently, generating diverse models that capture different aspects of the solution space. The hierarchical averaging in HS then intelligently combines these models at both local and global levels, accounting for the dataset's specific characteristics and improving robustness.

## Foundational Learning
**Cyclical Learning Rates** - Learning rate schedules that vary between minimum and maximum values in a cyclical pattern
- Why needed: Enables efficient exploration of complex error landscapes while maintaining convergence stability
- Quick check: Verify that cyclical learning rates prevent premature convergence to poor local minima

**Model Soups** - Ensemble methods that average weights of multiple independently trained models
- Why needed: Provides regularization and robustness benefits by combining multiple models
- Quick check: Compare performance against single best model to quantify ensemble benefits

**Error Landscape Analysis** - Study of optimization surface characteristics in high-dimensional parameter spaces
- Why needed: Understanding landscape roughness helps design appropriate training and averaging strategies
- Quick check: Visualize loss surfaces or analyze convergence patterns across different datasets

## Architecture Onboarding

**Component Map:** FGG Training -> Model Generation -> HS Local Averaging -> HS Global Averaging -> Final Model

**Critical Path:** The most critical sequence is FGG generating diverse models followed by HS performing hierarchical averaging. The quality of model diversity from FGG directly impacts HS effectiveness.

**Design Tradeoffs:** FGG trades off some training stability for increased model diversity, while HS balances between local specialization and global generalization. The hierarchical approach adds complexity but provides better adaptation to rough error landscapes.

**Failure Signatures:** Poor model diversity from FGG leads to ineffective HS averaging. Overly aggressive local averaging in HS can cause overfitting to specific data clusters. Insufficient global averaging reduces the benefits of ensemble methods.

**First Experiments:**
1. Compare FGG-generated models against traditionally trained models for diversity metrics
2. Evaluate HS performance with different local vs. global averaging ratios
3. Test out-of-distribution generalization on held-out medical imaging datasets

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Error landscape roughness analysis is primarily empirical with limited theoretical justification across diverse medical imaging modalities
- Computational efficiency claims lack detailed analysis of memory overhead and hardware-specific considerations
- Hierarchical averaging introduces complexity in parameter selection without clear guidelines for optimal threshold determination

## Confidence
**Medium:** The empirical results are compelling with consistent improvements across datasets, but the theoretical underpinnings connecting error landscape properties to the proposed solutions could be strengthened. The out-of-distribution generalization claims would benefit from more extensive testing across different domain shifts.

## Next Checks
1. Evaluate FGG+HS on additional medical imaging datasets across different modalities (e.g., MRI, CT, pathology) to verify generalizability
2. Conduct ablation studies to quantify the individual contributions of FGG and HS components
3. Perform theoretical analysis of the convergence properties and error landscape characteristics in medical imaging contexts compared to natural images