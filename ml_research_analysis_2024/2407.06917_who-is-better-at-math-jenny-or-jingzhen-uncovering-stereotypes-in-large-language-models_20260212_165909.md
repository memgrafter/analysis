---
ver: rpa2
title: Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in Large Language
  Models
arxiv_id: '2407.06917'
source_url: https://arxiv.org/abs/2407.06917
tags:
- male
- female
- groups
- stereotypes
- names
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of harmful stereotypes in large
  language models (LLMs), particularly those affecting marginalized communities. The
  authors introduce GlobalBias, a dataset of 876,000 sentences incorporating 40 distinct
  gender-by-ethnicity groups alongside descriptors typically used in bias literature.
---

# Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in Large Language Models

## Quick Facts
- arXiv ID: 2407.06917
- Source URL: https://arxiv.org/abs/2407.06917
- Reference count: 19
- Primary result: Large language models consistently display higher levels of stereotypical outputs, even when explicitly instructed not to

## Executive Summary
This paper introduces GlobalBias, a dataset of 876,000 sentences incorporating 40 distinct gender-by-ethnicity groups, to probe stereotypes in large language models (LLMs). The authors use perplexity as a proxy to measure how stereotypes are represented in model internal representations and generate character profiles to evaluate stereotype prevalence in outputs. Their findings reveal that larger models produce more stereotypical outputs even with anti-stereotype instructions, and that bias remains consistent across both internal representations and model outputs, contrary to previous work.

## Method Summary
The authors create the GlobalBias dataset combining 40 demographic groups with 730 descriptors and 400 proper names. They calculate perplexity for each sentence using various LLM architectures, then normalize these scores using their Adjusted Perplexity across Descriptors (APX) metric to account for name frequency bias. They identify statistically significant stereotype associations at the 1% level and conduct generation experiments using zero-shot prompting to create character profiles. Finally, they use SVM classification to analyze the prevalence of stereotypes in model outputs.

## Key Results
- Larger models consistently display higher levels of stereotypical outputs, even when explicitly instructed not to
- Bias remains consistent across the model's internal representation and outputs, contrary to claims in previous work
- The demographic groups associated with various stereotypes remain consistent across model likelihoods and model outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perplexity can be used as a proxy for measuring stereotype representation in LLMs
- Mechanism: Lower perplexity for a stereotype-descriptor combination indicates the model finds it more plausible/likely, revealing encoded stereotypes
- Core assumption: Perplexity correlates with stereotype likelihood despite being a general language modeling metric
- Break condition: If perplexity measures other linguistic factors more strongly than stereotype plausibility

### Mechanism 2
- Claim: Adjusted Perplexity across Descriptors (APX) corrects for name frequency bias in perplexity measurements
- Mechanism: Normalizes perplexity scores by group and overall means to isolate descriptor-specific associations rather than name-frequency effects
- Core assumption: High-frequency names create systematic perplexity differences unrelated to stereotypes
- Break condition: If name frequency effects are minimal or if normalization introduces new biases

### Mechanism 3
- Claim: Larger models produce more stereotypical outputs even with anti-stereotype instructions
- Mechanism: Increased model capacity and training data exposure amplifies learned stereotypes from web data
- Core assumption: Larger models capture more nuanced patterns including harmful stereotypes from training data
- Break condition: If stereotype amplification is due to factors other than model size

## Foundational Learning

- Concept: Intersectionality in bias measurement
  - Why needed here: The paper explicitly studies 40 gender-by-ethnicity groups, requiring understanding how multiple identity dimensions interact
  - Quick check question: Why does measuring bias across single dimensions (gender alone or ethnicity alone) potentially miss important patterns?

- Concept: Perplexity and language modeling fundamentals
  - Why needed here: The primary measurement technique relies on perplexity calculations across different model architectures
  - Quick check question: How does perplexity calculation differ between decoder-only models (GPT-2) and masked language models (BERT)?

- Concept: Statistical significance testing in bias measurement
  - Why needed here: The paper uses 1% significance thresholds to determine stereotype associations
  - Quick check question: What are the risks of using different significance thresholds (e.g., 5% vs 1%) in stereotype detection?

## Architecture Onboarding

- Component map: GlobalBias dataset → perplexity calculation (multiple model types) → APX normalization → significance testing → generation experiments → classification analysis
- Critical path: Dataset creation → measurement methodology (perplexity/APX) → validation → application to multiple models → generation experiments → feature analysis
- Design tradeoffs:
  - Using names as demographic proxies enables broad coverage but may perpetuate name-based harms
  - One-vs-all approach provides better statistical power than pairwise comparisons but requires more data
  - Lexicon-free generation captures unexpected stereotypes but makes quantitative comparison harder
- Failure signatures:
  - High classification accuracy across groups indicates strong stereotypes
  - Low significance for most descriptors suggests dataset/design issues
  - Inconsistent results between perplexity and generation experiments indicate methodological problems
- First 3 experiments:
  1. Reproduce APX validation results on smaller subset to verify implementation
  2. Test APX on a single model with known biases to verify sensitivity
  3. Compare perplexity vs APX results on a small set of descriptors to understand normalization effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do gender-neutral names perform in LLMs compared to binary gender names, and do they represent gender-diverse groups more accurately?
- Basis in paper: The study acknowledges excluding gender-neutral names limits applicability to a broader spectrum of identities
- Why unresolved: The study focused on binary gender classification to maintain consistency with existing datasets
- What evidence would resolve it: A systematic study comparing stereotype representation for gender-neutral names versus binary gender names across multiple LLMs

### Open Question 2
- Question: Does the consistency of bias between internal representations and outputs hold across different model architectures and training paradigms beyond the ones tested?
- Basis in paper: The authors find bias stays consistent across internal representations and outputs, contrary to previous work
- Why unresolved: The study only tested a limited set of model architectures and sizes
- What evidence would resolve it: A comprehensive cross-model comparison testing APX and generation outputs across diverse architectures

### Open Question 3
- Question: How does the GlobalBias dataset's stereotype detection performance compare to lexicon-based approaches when evaluating real-world model outputs in downstream tasks?
- Basis in paper: The paper contrasts its lexicon-free generation approach with traditional lexicon-based stereotype datasets
- Why unresolved: While the study demonstrates the dataset's effectiveness in controlled experiments, it doesn't evaluate its practical utility for monitoring stereotypes in deployed systems
- What evidence would resolve it: A comparative study applying GlobalBias and lexicon-based methods to monitor stereotype emergence in LLM outputs across multiple real-world applications

## Limitations
- The use of perplexity as a proxy for stereotype representation may not capture all relevant linguistic factors
- The generation experiments rely on zero-shot prompting which may not reflect real-world usage patterns
- The study excludes gender-neutral names, limiting applicability to diverse gender identities

## Confidence
- High Confidence: Larger models produce more stereotypical outputs is well-supported by data
- Medium Confidence: Consistency of stereotype associations across perplexity and generation experiments suggests methodology is capturing genuine patterns
- Low Confidence: Claim that bias remains consistent across internal representations and outputs contradicts previous work but may reflect measurement differences

## Next Checks
1. Apply APX methodology to a known biased dataset to verify the metric can detect expected patterns before applying it to novel data
2. Test whether stereotype amplification scales linearly with model size across multiple size increments, or if there are threshold effects
3. Compare stereotype levels in models with different alignment approaches (RLHF, constitutional AI, etc.) to determine whether standard alignment techniques effectively mitigate the size-related bias amplification observed