---
ver: rpa2
title: 'Adversarial Score identity Distillation: Rapidly Surpassing the Teacher in
  One Step'
arxiv_id: '2410.14919'
source_url: https://arxiv.org/abs/2410.14919
tags:
- sida
- diffusion
- distillation
- arxiv
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SiDA (SiD with Adversarial Loss), an enhancement
  to the score identity distillation (SiD) method for diffusion models. SiD distills
  a pretrained diffusion model into a one-step generator, but its performance is limited
  by the accuracy of the teacher model's score representation.
---

# Adversarial Score identity Distillation: Rapidly Surpassing the Teacher in One Step

## Quick Facts
- arXiv ID: 2410.14919
- Source URL: https://arxiv.org/abs/2410.14919
- Authors: Mingyuan Zhou; Huangjie Zheng; Yi Gu; Zhendong Wang; Hai Huang
- Reference count: 40
- One-line primary result: SiDA achieves significant improvements in generation performance across all model sizes, surpassing the largest teacher model EDM2-XXL on ImageNet 512x512 with FID scores ranging from 1.366 to 2.156 in a single generation step without classifier-free guidance.

## Executive Summary
SiDA (SiD with Adversarial Loss) is an enhancement to the score identity distillation (SiD) method for diffusion models. SiD distills a pretrained diffusion model into a one-step generator, but its performance is limited by the accuracy of the teacher model's score representation. SiDA addresses this by incorporating real images and adversarial loss, using the encoder from the generator's score network as a discriminator. The adversarial loss is batch-normalized within each GPU and combined with the original SiD loss. This approach enables SiDA to distill a single-step generator that surpasses the teacher model's performance. SiDA achieves significantly faster convergence than SiD, especially when fine-tuning from a pre-distilled SiD generator.

## Method Summary
SiDA is an improvement over the original SiD method that incorporates adversarial training to address the limitation of teacher score representation accuracy. The method uses the encoder from the generator's score network as a discriminator, introducing real images and an adversarial loss that distinguishes between real and generated images at various noise levels. The adversarial loss is computed at each spatial position of the latent encoder feature map and averaged across batch samples within each GPU, then integrated into the SiD loss for joint distillation and adversarial training. This approach introduces no additional parameters and enables rapid convergence to surpass teacher model performance in a single generation step.

## Key Results
- SiDA surpasses the largest teacher model EDM2-XXL on ImageNet 512x512 with FID scores ranging from 1.366 to 2.156 across different model sizes
- Achieves significantly faster convergence than SiD, especially when fine-tuning from a pre-distilled SiD generator
- Demonstrates substantial improvements in generation performance across all model sizes without classifier-free guidance
- Shows effectiveness on multiple datasets including CIFAR-10 and ImageNet at various resolutions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The adversarial loss compensates for inaccuracies in the teacher model's score representation.
- Mechanism: By using the encoder from the generator's score network as a discriminator, SiDA introduces real images and an adversarial loss that distinguishes between real and generated images at various noise levels. This adversarial component helps correct deviations caused by the teacher model's imperfect score representation.
- Core assumption: The teacher model's score is not a perfect representation of the true data score.
- Evidence anchors:
  - [abstract]: "However, its ultimate performance is constrained by how accurate the pretrained model captures the true data scores at different stages of the diffusion process."
  - [section 4]: "This assumption can potentially create a performance bottleneck for distilled single-step generators, especially if the teacher diffusion model is not well-trained or has limited capacity."
  - [corpus]: Weak evidence; no direct mention of adversarial compensation for teacher score inaccuracies.
- Break condition: If the teacher model is perfectly trained and captures the true data scores accurately.

### Mechanism 2
- Claim: Joint training of score estimation and discrimination improves convergence speed.
- Mechanism: The encoder part of the fake score network is repurposed to serve as a discriminator, allowing for joint estimation of fake scores and discrimination between real and fake images. This eliminates the need for additional parameters and simplifies the model architecture.
- Core assumption: The encoder part of the fake score network can effectively serve as a discriminator.
- Evidence anchors:
  - [section 4]: "To avoid introducing any additional parameters or complex training pipelines, we incorporate a return-flag as an additional input to the network fψ, offering options 'decoder', 'encoder', and 'encoder-decoder'."
  - [section 4]: "With the capability to extract 2D discriminator maps either under the 'encoder' option or jointly with the denoised images under the 'encoder-decoder' option, we are now positioned to define the adversarial loss function."
  - [corpus]: Weak evidence; no direct mention of joint training improving convergence speed.
- Break condition: If the encoder part of the fake score network cannot effectively distinguish between real and fake images.

### Mechanism 3
- Claim: GPU batch pooling for adversarial loss balances diffusion distillation and adversarial generation losses.
- Mechanism: The adversarial loss is computed at each spatial position of the latent encoder feature map and averaged across the batch samples within each GPU. This average "fakeness" per GPU batch is then incorporated into the pixel-based SiD loss, ensuring compatibility and balance between the two loss types.
- Core assumption: Averaging the adversarial loss across GPU batches provides a balanced integration with the pixel-based SiD loss.
- Evidence anchors:
  - [section 4]: "To ensure compatibility with SiD's pixel-level loss, we compute the discriminator loss at each spatial position of the latent encoder feature map and average across the batch samples within each GPU."
  - [section 4]: "This measure is integrated into the SiD loss for joint distillation and adversarial training and introduces no additional parameters."
  - [corpus]: Weak evidence; no direct mention of GPU batch pooling for balancing losses.
- Break condition: If the averaged adversarial loss does not effectively balance with the pixel-based SiD loss.

## Foundational Learning

- Concept: Diffusion models and score matching
  - Why needed here: Understanding the foundation of diffusion models and score matching is crucial for grasping how SiDA improves upon SiD by incorporating adversarial loss.
  - Quick check question: What is the primary objective of diffusion models in the context of image generation?

- Concept: Adversarial training and GANs
  - Why needed here: Knowledge of adversarial training and GANs is essential for understanding how SiDA uses the encoder from the generator's score network as a discriminator to enhance generation quality.
  - Quick check question: How does adversarial training in GANs differ from the adversarial loss used in SiDA?

- Concept: Fréchet Inception Distance (FID) and evaluation metrics
  - Why needed here: Familiarity with FID and other evaluation metrics is important for assessing the performance improvements achieved by SiDA over SiD and other methods.
  - Quick check question: What does a lower FID score indicate about the quality of generated images?

## Architecture Onboarding

- Component map:
  Generator (Gθ) -> Score network (fψ) -> Teacher score network (fϕ) -> Discriminator (D extracted from fψ encoder)

- Critical path:
  1. Initialize generator Gθ and score network fψ with teacher model parameters
  2. Train fψ to estimate fake scores using SiDA's fake score loss
  3. Jointly train Gθ and fψ using SiDA's generator loss, incorporating adversarial loss
  4. Evaluate performance using FID and other metrics

- Design tradeoffs:
  - Using the encoder from fψ as a discriminator eliminates the need for additional parameters but may limit the discriminator's capacity
  - GPU batch pooling for adversarial loss ensures compatibility with SiD's pixel-level loss but may introduce approximation errors

- Failure signatures:
  - Divergence during training, especially when using only the adversarial loss without SiD components
  - Poor performance on complex datasets or when teacher model is not well-trained

- First 3 experiments:
  1. Ablation study on the choice of α (gradient bias correction weight) to assess its impact on performance
  2. Comparison of SiDA with SiD on CIFAR-10 (unconditional and conditional) to demonstrate improvements in FID and IS
  3. Benchmarking SiDA on ImageNet 512x512 with different-sized EDM2 models to showcase scalability and performance across various model sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does SiDA's performance improvement depend critically on the quality of the teacher model's score representation, or can it compensate for a poorly trained teacher?
- Basis in paper: [explicit] The paper notes that SiD's performance is constrained by the accuracy of the teacher model's score representation, and SiDA aims to address this by incorporating real images and adversarial loss.
- Why unresolved: The paper doesn't provide experiments with poorly trained teacher models to demonstrate SiDA's robustness to teacher model quality.
- What evidence would resolve it: Experiments comparing SiDA's performance when distilling from well-trained versus poorly trained teacher models.

### Open Question 2
- Question: How does the choice of the reweighting term γ(t) in the fake score network loss affect SiDA's performance, and is there an optimal choice for different datasets or model sizes?
- Basis in paper: [explicit] The paper mentions that γ(t) is typically set the same as the signal-to-noise ratio at time t, but doesn't explore the impact of different choices.
- Why unresolved: The paper doesn't provide ablation studies or sensitivity analysis on the choice of γ(t).
- What evidence would resolve it: Experiments varying γ(t) and measuring the impact on SiDA's performance across different datasets and model sizes.

### Open Question 3
- Question: Can SiDA be effectively extended to other generative tasks beyond image generation, such as video or 3D object generation?
- Basis in paper: [inferred] The paper focuses on image generation, but the underlying principles of score distillation and adversarial learning could potentially be applied to other domains.
- Why unresolved: The paper doesn't explore applications beyond image generation, and the extension to other domains would require significant modifications to the architecture and training procedure.
- What evidence would resolve it: Experiments demonstrating SiDA's effectiveness in video or 3D object generation, or theoretical analysis of the challenges and potential solutions for extending SiDA to these domains.

## Limitations

- The adversarial loss component can cause training divergence if used without the SiD components, indicating potential instability in the training process
- Performance improvements are highly dependent on the quality of the teacher model's score representation
- The forced weight normalization technique introduces additional complexity that may impact numerical stability

## Confidence

- High confidence: Claims about SiDA's superior performance on CIFAR-10 and ImageNet 512x512 compared to SiD and other baselines
- Medium confidence: Claims about the mechanism by which adversarial loss compensates for teacher score inaccuracies, as direct evidence is limited
- Medium confidence: Claims about the GPU batch pooling mechanism ensuring balanced integration of adversarial and SiD losses, as this is not extensively validated

## Next Checks

1. Conduct an ablation study specifically focusing on the adversarial loss component to quantify its contribution to performance improvements versus the original SiD loss
2. Test SiDA's performance on additional datasets and model architectures to verify the robustness of the reported improvements
3. Implement and validate the forced weight normalization technique independently to ensure it doesn't introduce unintended side effects in the training process