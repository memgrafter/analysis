---
ver: rpa2
title: Towards Building Multilingual Language Model for Medicine
arxiv_id: '2402.13963'
source_url: https://arxiv.org/abs/2402.13963
tags:
- medical
- llama
- multilingual
- language
- internlm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a large-scale multilingual medical corpus
  (MMedC) and a corresponding benchmark (MMedBench) to address the lack of open-source
  multilingual medical language models. The corpus contains 25.5 billion tokens across
  six languages, and the benchmark includes 53,566 QA pairs with rationales.
---

# Towards Building Multilingual Language Model for Medicine

## Quick Facts
- arXiv ID: 2402.13963
- Source URL: https://arxiv.org/abs/2402.13963
- Reference count: 40
- This paper introduces a large-scale multilingual medical corpus (MMedC) and a corresponding benchmark (MMedBench) to address the lack of open-source multilingual medical language models.

## Executive Summary
This paper addresses the critical gap in open-source multilingual medical language models by introducing MMedC, a 25.5 billion token corpus spanning six languages, and MMedBench, a benchmark with 53,566 medical QA pairs with rationales. The authors propose MMed-Llama 3, an 8B parameter model that achieves state-of-the-art performance on both multilingual and English medical benchmarks, surpassing existing open-source models and approaching GPT-4 performance. The approach combines continued pretraining on domain-specific multilingual data with rationale-enhanced fine-tuning to improve both accuracy and reasoning capabilities.

## Method Summary
The authors construct MMedC by collecting medical textbooks, websites, and filtered content across six languages, totaling 25.5B tokens. They then perform auto-regressive pretraining on this corpus followed by fine-tuning on MMedBench using a novel approach that jointly optimizes for multiple-choice answers and rationale generation. The training uses parameter-efficient methods like LoRA for the final fine-tuning stage. The resulting MMed-Llama 3 model is evaluated on both multilingual and English medical benchmarks, demonstrating superior performance compared to existing open-source models while maintaining reasonable computational requirements.

## Key Results
- MMed-Llama 3 achieves state-of-the-art performance on both multilingual and English medical benchmarks with only 8B parameters
- Rationale-enhanced fine-tuning improves multiple-choice accuracy by 2.33% while also generating human-verified quality explanations
- The model outperforms existing open-source models and approaches GPT-4 performance on medical QA tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Auto-regressive training on MMedC significantly improves multilingual medical QA performance.
- **Mechanism**: The model learns domain-specific medical terminology and multilingual context through continued pretraining on 25.5B tokens spanning 6 languages, enabling better adaptation to medical QA tasks.
- **Core assumption**: MMedC contains high-quality, diverse medical content that fills the knowledge gaps in general multilingual LLMs.
- **Evidence anchors**:
  - [abstract]: "Our final model, MMed-Llama 3, with only 8B parameters, achieves superior performance compared to all other open-source models on both MMedBench and English benchmarks"
  - [section]: "models that underwent further auto-regressive training on the MMedC consistently demonstrate enhanced performance"
  - [corpus]: MMedC contains 25.5B tokens from textbooks, websites, filtered content, and small-scale corpora across 6 languages
- **Break condition**: If MMedC contains low-quality or irrelevant medical content, the training signal would be noisy and degrade performance.

### Mechanism 2
- **Claim**: Fine-tuning with rationale improves both accuracy and reasoning ability.
- **Mechanism**: Joint training on multiple-choice questions and their explanations forces the model to learn not just the correct answer but also the underlying reasoning, improving both selection accuracy and rationale generation.
- **Core assumption**: Multiple-choice QA and rationale generation are correlated tasks that benefit from joint optimization.
- **Evidence anchors**:
  - [abstract]: "integrating rationale data with multiple-choice prediction, can enhance performance on specific tasks"
  - [section]: "combining correct answers with their rationales during the supervised fine-tuning phase... results in a noteworthy multiple choice accuracy improvement of 2.33% for InternLM"
  - [corpus]: MMedBench contains 53,566 QA pairs with human-verified rationales
- **Break condition**: If rationales are poorly written or inconsistent with answers, joint training could confuse the model.

### Mechanism 3
- **Claim**: Stronger foundational LLMs improve final multilingual medical performance.
- **Mechanism**: Larger, more capable base models have seen more diverse multilingual text during pretraining, giving them better language understanding that transfers to the medical domain after adaptation.
- **Core assumption**: General multilingual capabilities provide a foundation that enhances domain-specific adaptation.
- **Evidence anchors**:
  - [abstract]: "MMed-Llama 3, with only 8B parameters, achieves superior performance compared to all other open-source models"
  - [section]: "we also notice that stronger LLM backbones... generally improve the final results on multilingual medical QA"
  - [corpus]: Comparison shows Llama 3 (latest) outperforms Llama 2 and Mistral in multilingual medical tasks
- **Break condition**: If base model has poor multilingual capabilities, the domain adaptation cannot compensate for fundamental language understanding gaps.

## Foundational Learning

- **Concept**: Tokenization and vocabulary design for multilingual medical text
  - Why needed here: Medical terminology varies significantly across languages, and proper tokenization is crucial for effective learning
  - Quick check question: How does the tokenizer handle medical terms that appear in multiple languages (e.g., "diabetes") versus language-specific terms?

- **Concept**: Auto-regressive language modeling objective
  - Why needed here: The model must learn to predict the next token in medical sequences, which requires understanding both general language patterns and