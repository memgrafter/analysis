---
ver: rpa2
title: Automatically Adaptive Conformal Risk Control
arxiv_id: '2406.17819'
source_url: https://arxiv.org/abs/2406.17819
tags:
- control
- conformal
- segmentation
- risk
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of achieving conditional risk
  control in black-box machine learning models, particularly for high-dimensional
  tasks like image segmentation where traditional conformal prediction methods fail
  to provide meaningful uncertainty estimates. The core method, called automatically
  adaptive conformal risk control (AA-CRC), builds upon recent work on conformal prediction
  with conditional guarantees by Gibbs et al.
---

# Automatically Adaptive Conformal Risk Control

## Quick Facts
- arXiv ID: 2406.17819
- Source URL: https://arxiv.org/abs/2406.17819
- Reference count: 8
- One-line primary result: Automatically adaptive conformal risk control (AA-CRC) improves precision from 0.40 to 0.46-0.47 on polyp segmentation while maintaining target recall

## Executive Summary
This paper introduces automatically adaptive conformal risk control (AA-CRC), a method that achieves conditional risk control for black-box ML models without requiring user-specified conditioning groups. The key innovation is an automatic algorithm for selecting the function class used for conditioning, allowing uncertainty to adapt to the difficulty of individual test samples. AA-CRC extends recent work on conformal prediction with conditional guarantees to handle monotonic risks and adaptively chosen conditioning groups. The method is demonstrated on both regression and semantic segmentation tasks, showing significant improvements in precision while maintaining controlled recall.

## Method Summary
AA-CRC builds upon conformal risk control theory by replacing user-specified conditioning groups with an automatically learned embedding-based function class Λ. For image data, this uses neural network embeddings (ResNet-50), while for tabular data it employs Random Forest-based embeddings. The method assumes monotone nonincreasing loss functions and defines an indefinite integral I(x, y, u) to enable optimization over nested sets. The optimization procedure finds thresholds λ that control expected risk while adapting to sample difficulty through the learned embeddings. This eliminates the need for manual group specification and allows continuous adaptation to input difficulty rather than fixed, potentially coarse groups.

## Key Results
- On polyp segmentation datasets, AA-CRC achieves average precision of 0.46-0.47 compared to 0.40 for standard conformal risk control
- The method maintains the target recall level while improving precision, demonstrating effective risk control
- Experimental validation includes both binary segmentation tasks (polyp and fire segmentation) and regression tasks
- The approach successfully handles high-dimensional tasks where traditional conformal prediction methods fail to provide meaningful uncertainty estimates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method replaces user-specified conditioning groups with an automatically learned embedding-based function class Λ, improving adaptability to test sample difficulty.
- Mechanism: Instead of requiring user-defined discrete groups or linear functions over hand-crafted features, the algorithm learns a neural network embedding Φ(x) for images (or Random Forest-based embedding for tabular data) and defines Λ as linear functions of that embedding. This allows the conformal risk control threshold to adapt continuously to input sample difficulty rather than being tied to fixed groups.
- Core assumption: The learned embedding Φ(x) captures relevant difficulty signals such that a linear function of Φ(x) can effectively model how risk varies with sample difficulty.
- Evidence anchors:
  - [abstract] "The key innovation is an automatic algorithm for selecting the function class used for conditioning, which eliminates the need for user-specified groups and allows the uncertainty to adapt to the difficulty of individual test samples."
  - [section] "We use this perspective to parameterize the function class to yield the best predictor possible... This essentially amounts to a rigorous method for fine-tuning a fully-connected layer on a pretrained network backbone to provide risk-controlled estimates."
- Break condition: If the embedding does not capture difficulty (e.g., Φ(x) is random or uninformative), then Λ will be ineffective and the adaptive guarantee will fail.

### Mechanism 2
- Claim: By reframing conformal risk control as an implicit optimization problem over nested sets Cu(x), the method can handle monotonic loss functions and achieve conditional risk control without explicit group definitions.
- Mechanism: The loss ℓ is assumed monotone nonincreasing in the set index u. The method defines an indefinite integral I(x, y, u) of (ℓ - α) and solves an optimization over Λ to find the threshold λ that controls expected risk. This allows risk control to adapt to conditional shifts via the function class Λ while maintaining finite-sample guarantees via first-order optimality conditions.
- Core assumption: The loss function ℓ is right-continuous and monotone nonincreasing in the set index, ensuring that I is quasiconvex and the optimization is well-defined.
- Evidence anchors:
  - [section] "Let ℓ : Y × 2Y → [0, 1] be a right-continuous and monotone nonincreasing loss function... Because ℓ is a monotone loss function, we are guaranteed that I is a quasiconvex function..."
  - [section] "Theorem 1... If λ is nonnegative and E[λ(Xn+1, Yn+1)] > 0, then Eλ[ℓ(Yn+1, C(Xn+1))] ≤ α - ..."
- Break condition: If ℓ is not monotone in the set index, the indefinite integral I is not quasiconvex, and the optimization may fail to find valid thresholds.

### Mechanism 3
- Claim: The automatic embedding-based Λ construction improves precision while maintaining recall control, as demonstrated on polyp and fire segmentation tasks.
- Mechanism: By training a separate model to predict the optimal threshold given the input embedding, the method learns to set thresholds higher for "easy" samples and lower for "hard" ones, reducing false positives while preserving the recall guarantee. This is shown experimentally by comparing average precision (0.46-0.47) to standard CRC (0.40) on polyp segmentation, with recall held constant at the target level.
- Core assumption: The embedding Φ(x) learned from segmentation model's softmax outputs or from Random Forest on residuals captures the relationship between input difficulty and optimal threshold.
- Evidence anchors:
  - [section] "For semantic segmentation tasks... we train a convolutional neural network that predicts the highest threshold... The class of functions Λ = {x 7→ Φ(x)⊤θ : θ ∈ Rd}... This essentially amounts to a rigorous method for fine-tuning a fully-connected layer..."
  - [section] "The average precision of the AA-CRC method is 0.457 versus 0.395 for standard CRC, showing a significant improvement of the precision while guaranteeing the same level of recall."
- Break condition: If the relationship between embedding and threshold is non-linear or too complex for a linear Λ, the adaptive method may not outperform fixed-threshold CRC.

## Foundational Learning

- Concept: Conformal prediction and risk control fundamentals
  - Why needed here: The method builds directly on conformal risk control theory; understanding how coverage guarantees are achieved via quantile regression and exchangeability is essential to grasp the adaptation mechanism.
  - Quick check question: In split conformal prediction, how is the threshold λ chosen, and what does it guarantee about coverage?

- Concept: Monotonic loss functions and nested sets
  - Why needed here: The method assumes the loss ℓ is monotone in the set index u, which allows the indefinite integral I to be quasiconvex and the optimization well-defined. Misunderstanding this assumption could lead to incorrect application.
  - Quick check question: Give an example of a monotonic loss function for a multilabel classification task, and explain why it is monotonic.

- Concept: Embeddings and function classes for conditional guarantees
  - Why needed here: The key innovation is using learned embeddings (neural nets or RF) to define Λ, so understanding how embeddings capture conditional structure is critical for both implementation and troubleshooting.
  - Quick check question: Why does using a linear function of a learned embedding (Λ = {Φ(x)⊤θ}) allow for conditional risk control over covariate shifts?

## Architecture Onboarding

- Component map: Base model (segmentation or regression) -> Embedding model (NN or RF) -> Optimization solver -> Regularizer R(λ) -> Calibration set -> Test set

- Critical path:
  1. Train base model on training data
  2. Train embedding model on residuals or softmax outputs
  3. Optimize λ(x, y) on calibration set
  4. Apply learned λ to test inputs for final predictions

- Design tradeoffs:
  - Linear Λ vs. richer function classes: simpler and more stable, but may miss complex relationships
  - NN vs. RF for embeddings: NN better for images, RF for tabular data, but NN requires more data
  - Regularization strength: controls overfitting but may hurt adaptivity if too strong

- Failure signatures:
  - Poor coverage: check if loss is truly monotonic, if embedding captures difficulty, if optimization converged
  - Low precision: may indicate overly conservative thresholds; try adjusting regularizer or embedding size
  - High variance in recall: check calibration set size and embedding stability across splits

- First 3 experiments:
  1. Reproduce the synthetic regression experiment (Figure 2) to verify marginal and group-conditional coverage.
  2. Run the polyp segmentation experiment with a simple NN embedding (no PCA) to confirm precision improvement over CRC.
  3. Swap the embedding model (RF for images, NN for tabular) to test robustness to embedding choice.

## Open Questions the Paper Calls Out
- Can the automatic adaptive conformal risk control methodology be extended to multiclass semantic segmentation tasks?
- What additional choices of function spaces Λ could be explored to further improve the precision of AA-CRC beyond the current neural network embedding and random forest approaches?
- Can the theoretical guarantees of AA-CRC be extended to handle non-monotonic loss functions?

## Limitations
- Performance depends critically on the quality of learned embeddings and their ability to capture difficulty signals
- The method requires monotone nonincreasing loss functions, limiting applicability to certain problem types
- Results are primarily demonstrated on binary segmentation tasks, with multiclass extension as future work

## Confidence
- High: Theoretical guarantees under monotonicity assumptions follow directly from established conformal risk control theory
- Medium: Experimental results showing precision improvements are promising but limited to two datasets
- Low to Medium: Automatic embedding selection outperforming manual group specification is primarily demonstrated on image segmentation tasks

## Next Checks
1. **Synthetic Monotonicity Stress Test**: Generate synthetic data where the loss is provably monotonic in difficulty, then verify that AA-CRC maintains coverage under covariate shift while standard CRC fails.

2. **Embedding Ablation Across Domains**: Systematically compare NN vs. RF embeddings for both image and tabular data, and test the impact of embedding dimensionality and regularization strength on precision-recall trade-offs.

3. **Calibration Set Sensitivity Analysis**: Evaluate how AA-CRC performance varies with calibration set size, and test whether performance degrades gracefully as the number of calibration samples decreases.