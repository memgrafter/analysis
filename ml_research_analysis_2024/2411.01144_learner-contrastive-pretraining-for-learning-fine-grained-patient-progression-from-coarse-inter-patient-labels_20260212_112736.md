---
ver: rpa2
title: 'LEARNER: Contrastive Pretraining for Learning Fine-Grained Patient Progression
  from Coarse Inter-Patient Labels'
arxiv_id: '2411.01144'
source_url: https://arxiv.org/abs/2411.01144
tags:
- contrastive
- learning
- patient
- pretraining
- inter-patient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LEARNER addresses the challenge of learning fine-grained patient
  progression from coarse inter-patient labels in longitudinal medical imaging. The
  method uses contrastive pretraining to learn patient-specific representations from
  inter-patient variability, specifically leveraging differences in clinical scores
  between patients as supervision.
---

# LEARNER: Contrastive Pretraining for Learning Fine-Grained Patient Progression from Coarse Inter-Patient Labels

## Quick Facts
- arXiv ID: 2411.01144
- Source URL: https://arxiv.org/abs/2411.01144
- Authors: Jana Armouti; Nikhil Madaan; Rohan Panda; Tom Fox; Laura Hutchins; Amita Krishnan; Ricardo Rodriguez; Bennett DeBoisblanc; Deva Ramanan; John Galeotti; Gautam Gare
- Reference count: 0
- Primary result: Contrastive pretraining improves fine-grained patient progression detection from coarse labels

## Executive Summary
LEARNER introduces a contrastive pretraining framework that learns fine-grained patient progression representations from coarse inter-patient labels. The method leverages clinical score differences between patients as supervision to train models that can detect subtle intra-patient changes. By combining MSE regression with weighted contrastive objectives, LEARNER achieves significant improvements in downstream health-change classification accuracy on both lung ultrasound and brain MRI datasets.

## Method Summary
LEARNER uses a contrastive pretraining approach where models learn patient-specific representations by maximizing embedding distances for clinically dissimilar patients while minimizing distances for similar ones. The framework combines standard MSE regression with a weighted contrastive loss that modulates the loss by clinical score differences. The model processes longitudinal imaging data through a TSM-based video encoder, predicts clinical scores, and generates embeddings for contrastive learning. For downstream tasks, embeddings from sequential patient scans are compared to classify health changes.

## Key Results
- Improves LUS downstream classification accuracy from 52.5% to 72.5%
- Improves ADNI downstream classification accuracy from 31.5% to 35.3%
- Weighted contrastive loss outperforms both MSE-only and unweighted contrastive approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive pretraining on inter-patient label differences enables models to learn embeddings that better reflect fine-grained intra-patient progression.
- Mechanism: The model is trained to maximize embedding distances for pairs of patients with large clinical score differences while minimizing distances for similar-score pairs. This contrastive objective encourages the learned representation to encode clinically meaningful variations, which transfers to detecting subtle changes within individual patients over time.
- Core assumption: Differences in clinical scores between patients contain sufficient signal to capture the features relevant to intra-patient progression, even though the supervision is coarse and inter-patient.
- Evidence anchors:
  - [abstract] "We propose LEARNER, a contrastive pretraining framework that leverages coarsely labeled inter-patient data to learn fine-grained, patient-specific representations."
  - [section 3.1] "With MSE-only pretraining, pairs with large S/F changes often remain close in embedding space, indicating underestimation of clinical progression. Contrastive pretraining, especially the weighted variant, produces a clearer, increasing relationship..."
- Break condition: If inter-patient variability does not correlate with the features that drive intra-patient changes (e.g., if patient-specific confounding factors dominate), the learned representations may not transfer to fine-grained intra-patient progression.

### Mechanism 2
- Claim: Weighted contrastive loss, which modulates the loss by the magnitude of clinical score differences, further improves the model's ability to capture subtle intra-patient changes.
- Mechanism: By assigning larger weights to pairs with greater score differences in the negative term and smaller weights in the positive term, the model is pushed to more strongly separate clinically dissimilar patients and more gently pull together similar ones. This weighting scheme amplifies the influence of pairs most relevant to capturing clinically meaningful progression.
- Core assumption: The magnitude of clinical score differences between patients is a reliable indicator of the importance of their separation in the embedding space for learning intra-patient progression.
- Evidence anchors:
  - [section 2.3.1] "In practice, we found it beneficial to modulate each contrastive term by the underlying clinical score difference... Pairs with large label differences receive larger weights in the negative term and smaller weights in the positive term..."
  - [section 3.1] "Across both modalities, our approach improves downstream classification accuracy and F1-score compared to standard MSE pretraining..."
- Break condition: If the clinical score differences are not well-calibrated indicators of the true clinical relevance of patient pairs, the weighting scheme may distort the learning process and degrade performance.

### Mechanism 3
- Claim: The combination of MSE regression and contrastive objectives creates a more robust representation by capturing both global clinical state and fine-grained relative differences.
- Mechanism: The MSE loss ensures the model learns to predict overall clinical scores accurately, providing a stable foundation. The contrastive loss then refines this representation by emphasizing relative differences between patients, which helps the model become more sensitive to subtle changes within individuals.
- Core assumption: Both absolute clinical state prediction and relative patient comparison are necessary for optimal representation of intra-patient progression.
- Evidence anchors:
  - [section 2.3.1] "We first supervise the model with a standard mean-squared error (MSE) loss between the predicted and ground-truth health scores... To leverage inter-patient variability, we introduce a batch-wise contrastive term..."
  - [section 3.1] "All models share the same hyper-parameters... Adding contrastive supervision improves downstream health-change classification over MSE alone..."
- Break condition: If the regression task and contrastive task are not aligned (e.g., if the clinical scores are noisy or not well-defined), the combination may introduce conflicting signals that harm representation quality.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: LEARNER uses contrastive objectives to leverage inter-patient variability as a proxy for learning intra-patient progression. Understanding how contrastive learning works and why it's effective for representation learning is crucial for grasping the paper's approach.
  - Quick check question: How does contrastive learning encourage the model to learn discriminative representations, and why is this useful for capturing subtle changes within patients?

- Concept: Longitudinal modeling
  - Why needed here: The paper addresses the challenge of modeling patient progression over time using longitudinal imaging data. Familiarity with techniques for handling temporal dependencies and modeling changes over time is essential for understanding the problem context and the proposed solution.
  - Quick check question: What are the key challenges in modeling patient progression from longitudinal data, and how do different approaches (e.g., recurrent networks, temporal convolutions) address these challenges?

- Concept: Representation learning
  - Why needed here: LEARNER aims to learn patient-specific representations that are sensitive to fine-grained intra-patient changes. Understanding the principles of representation learning, including how to evaluate representation quality and transfer learned features to downstream tasks, is important for interpreting the results and implications of the paper.
  - Quick check question: How do we measure the quality of learned representations, and what properties should good representations have for the task of detecting subtle patient progression?

## Architecture Onboarding

- Component map:
  Video encoder (TSM with ResNet-18 backbone) -> MLP head (regression and embedding generation) -> Contrastive loss module -> Downstream classifier

- Critical path:
  1. Preprocess and load a batch of patient scans and their clinical scores.
  2. Pass the scans through the video encoder to obtain feature embeddings.
  3. Compute the MSE loss between predicted and ground-truth clinical scores.
  4. Calculate pairwise clinical score differences and use them to define positive and negative pairs for the contrastive loss.
  5. Compute the weighted contrastive loss based on the clinical score differences.
  6. Combine the MSE and contrastive losses and perform backpropagation to update the model parameters.
  7. For downstream evaluation, extract embeddings for sequential patient scans, compute their difference, and pass it through the classifier to predict health change.

- Design tradeoffs:
  - Backbone architecture: TSM was chosen for its balance of temporal modeling capability and computational efficiency, but other options like 3D CNNs or transformer-based models could be explored.
  - Contrastive loss weighting: The weighted contrastive loss showed better performance than the unweighted version, but the optimal weighting scheme may depend on the dataset and task.
  - Embedding combination strategy: Using the difference between sequential embeddings for the downstream classifier performed better than concatenation, but this may not hold for all tasks or datasets.

- Failure signatures:
  - Poor performance on the regression task: Indicates that the model is not learning to accurately predict clinical scores, which could be due to issues with the encoder architecture, training procedure, or data quality.
  - Contrastive loss not improving performance: Suggests that the inter-patient variability is not a good proxy for intra-patient progression, or that the contrastive objective is not well-aligned with the task.
  - Downstream classifier not improving with pretraining: Implies that the learned representations are not capturing the relevant features for detecting subtle patient changes, or that the embedding difference strategy is not suitable for the task.

- First 3 experiments:
  1. Train the model with only the MSE regression loss and evaluate its performance on the downstream health-change classification task. This establishes a baseline for comparison.
  2. Train the model with the MSE regression loss and unweighted contrastive loss, and compare its performance to the baseline. This assesses the impact of adding contrastive supervision without weighting.
  3. Train the model with the MSE regression loss and weighted contrastive loss, and compare its performance to the previous experiments. This evaluates the effect of weighting the contrastive loss by clinical score differences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LEARNER's performance scale with increasing dataset size and diversity across different imaging modalities?
- Basis in paper: [explicit] The authors note that extending the framework to larger cohorts and additional imaging modalities is a motivation for future work, suggesting current evaluation may be limited by dataset size.
- Why unresolved: The current evaluation is based on relatively small, modality-specific datasets (LUS and ADNI), and the authors explicitly call for scaling to larger, more diverse data.
- What evidence would resolve it: Systematic evaluation of LEARNER on larger multi-site cohorts and additional imaging modalities (e.g., CT, X-ray) with varying levels of label granularity and patient heterogeneity.

### Open Question 2
- Question: Can the weighted contrastive objective be further optimized by dynamically adjusting weights based on clinical score distributions or uncertainty?
- Basis in paper: [inferred] The weighted contrastive loss uses fixed clinical score differences as weights, but the authors do not explore adaptive weighting schemes based on score distribution or prediction uncertainty.
- Why unresolved: The current weighting scheme is static and may not account for varying clinical score distributions or uncertainty in label quality across patients.
- What evidence would resolve it: Experiments comparing static vs. adaptive weighting schemes, including uncertainty-aware or distribution-aware weighting strategies.

### Open Question 3
- Question: How does LEARNER perform when applied to more granular clinical scoring systems or continuous outcome prediction rather than discrete categories?
- Basis in paper: [explicit] The authors discretize clinical scores into categories for health-change classification, but do not explore continuous outcome prediction or finer-grained scoring systems.
- Why unresolved: The current framework focuses on categorical health-change classification, and the impact of different scoring granularities on representation learning is unexplored.
- What evidence would resolve it: Direct comparison of LEARNER's performance on continuous vs. categorical outcome prediction tasks, and evaluation across multiple levels of scoring granularity.

## Limitations

- The assumption that inter-patient clinical score differences reliably encode features relevant to intra-patient progression lacks mechanistic validation
- The weighted contrastive loss assumes linear relationships between clinical score differences and embedding distances
- Results may be overly specialized to specific conditions and may not generalize across different disease types

## Confidence

High confidence: The technical implementation of the contrastive pretraining framework is sound, with clear architectural choices and well-defined loss functions.

Medium confidence: The claim that inter-patient contrastive learning enables learning of fine-grained intra-patient progression is supported by empirical results but lacks mechanistic validation.

Low confidence: The generalization of results across different disease types and imaging modalities is uncertain, as the paper evaluates only two datasets.

## Next Checks

1. **Ablation on weighting scheme**: Systematically test alternative weighting functions for the contrastive loss (e.g., exponential, logarithmic, learned weights) to determine whether the current linear weighting is optimal or if more sophisticated schemes yield better performance.

2. **Temporal consistency analysis**: Evaluate whether embeddings from the same patient at adjacent time points are closer than embeddings from non-adjacent timepoints within the same patient. This would provide direct evidence that the model learns temporal progression patterns rather than just inter-patient differences.

3. **Cross-dataset transferability test**: Train the contrastive pretraining framework on one disease dataset and evaluate transfer performance on a completely different disease dataset. This would test whether the learned representations capture general progression patterns or are overly specialized to specific conditions.