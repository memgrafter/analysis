---
ver: rpa2
title: Fairness of Exposure in Online Restless Multi-armed Bandits
arxiv_id: '2402.06348'
source_url: https://arxiv.org/abs/2402.06348
tags:
- fairness
- arms
- regret
- state
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first online framework for meritocratic
  fairness in restless multi-armed bandits (RMABs). The authors define arm merit based
  on steady-state reward differences and propose MF-RMAB, an algorithm that pulls
  arms proportionally to their merit.
---

# Fairness of Exposure in Online Restless Multi-armed Bandits

## Quick Facts
- arXiv ID: 2402.06348
- Source URL: https://arxiv.org/abs/2402.06348
- Reference count: 38
- This paper introduces the first online framework for meritocratic fairness in restless multi-armed bandits (RMABs).

## Executive Summary
This paper proposes the first online framework for meritocratic fairness in restless multi-armed bandits (RMABs). The authors define arm merit based on steady-state reward differences and develop MF-RMAB, an algorithm that pulls arms proportionally to their merit. They prove a sublinear fairness regret bound of O(âˆš(T ln T)) for the single-pull case and demonstrate empirically that the algorithm achieves fairness while maintaining good performance in both single-pull and multi-pull settings.

## Method Summary
MF-RMAB uses Upper Confidence Bound (UCB) learning to estimate transition probabilities for each arm's MDP. The algorithm computes rewards as the difference between steady-state probabilities when always pulled versus never pulled, then maps these through a merit function to create a probability distribution over arms. Arms are sampled proportionally to their merit without replacement. The method requires no prior knowledge of transition probabilities and works in an online setting where rewards are observed only after pulls.

## Key Results
- Proposes the first online algorithm for meritocratic fairness in RMABs
- Proves O(âˆš(T ln T)) fairness regret bound for single-pull case
- Demonstrates fairness achievement while maintaining good performance on synthetic and real-world datasets
- Shows hyperparameter c in merit function controls fairness-optimality tradeoff

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MF-RMAB achieves proportional exposure by estimating rewards from upper confidence bounds on transition matrices and using a merit-based probability distribution.
- Mechanism: The algorithm maintains optimistic estimates of the true transition probabilities (ğ‘ƒ +,ğ‘¡ ğ‘–) for each arm. It computes a reward ğœ‡ğ‘¡ ğ‘– as the difference between steady-state probabilities when always pulled vs never pulled. These rewards are mapped through a merit function ğ‘”(Â·) to define a probability distribution ğœ‹ğ‘¡ over arms. Sampling from ğœ‹ğ‘¡ ensures arms are chosen with probability proportional to their merit.
- Core assumption: The confidence bounds ğ‘ƒ +,ğ‘¡ ğ‘– lie within the true transition matrix with high probability, and the merit function is monotonic and Lipschitz continuous.
- Evidence anchors: [abstract] "We define the merit of an arm as a function of its stationary reward distribution." [section 4.2] "The estimated value of the transition probability matrix... can be computed by finding a fraction of the time... and a confidence radius is then created."
- Break condition: If the confidence bounds are too loose or the merit function violates monotonicity/Lipschitz assumptions, the fairness guarantee may fail.

### Mechanism 2
- Claim: Sublinear fairness regret in the single-pull case follows from bounded estimation error and sufficient exploration of state-action pairs.
- Mechanism: The analysis shows that after a finite number of episodes ğº, each arm has been explored sufficiently to estimate its transition matrix with bounded error. This bounded error translates into bounded reward error, which under Lipschitz conditions yields bounded deviation from the optimal fair policy. Summing these bounded deviations over time yields a sublinear regret bound.
- Core assumption: There exists a time ğ‘¡0 after which confidence intervals shrink enough that estimated gaps in transition probabilities are strictly less than 1.
- Evidence anchors: [section 5] "For arm ğ‘–, take any ğ‘ƒğ‘¡ ğ‘– âˆˆ ğµğ‘¡ ğ‘–... define a policy using Equation (8). Then for ğ‘‡ > ğ‘¡0, fairness regret of this policy when ğ¾ = 1 is ğ¹ ğ‘…ğ‘‡ = O(...)" [section 4.2] Definition of confidence radius and the assumption that the true transition matrix lies within the confidence ball.
- Break condition: If the exploration schedule ğº is too large or the confidence intervals do not shrink as assumed, the regret bound may degrade.

### Mechanism 3
- Claim: The steady-state reward definition ensures that arms are prioritized by their marginal benefit from intervention rather than their inherent "goodness."
- Mechanism: By defining ğœ‡ğ‘– = ğ‘“(ğ‘ƒğ‘–, 1) âˆ’ ğ‘“(ğ‘ƒğ‘–, 0), the algorithm captures the improvement in long-run good-state probability due to pulling. This avoids rewarding arms that would be good anyway without intervention, focusing resources on those that benefit most.
- Core assumption: The steady-state probabilities can be computed accurately from the transition matrices, and the difference metric meaningfully reflects intervention benefit.
- Evidence anchors: [section 4.1] "We define the merit of an arm as a function of its stationary reward distribution... the reward of an arm can be naturally defined as: ğœ‡ğ‘– = ğ‘“(ğ‘ƒğ‘–, 1) âˆ’ ğ‘“(ğ‘ƒğ‘–, 0)" [section 4] Explanation of why using the difference avoids wasting pulls on arms that don't need intervention.
- Break condition: If the steady-state assumption is violated (e.g., non-ergodic MDPs) or the reward difference is noisy, the merit ranking may be incorrect.

## Foundational Learning

- Concept: Restless Multi-Armed Bandits (RMABs)
  - Why needed here: The entire problem formulation and algorithm rely on the RMAB setting where each arm is an MDP with unknown transitions and rewards depend on state evolution.
  - Quick check question: What distinguishes an RMAB from a standard MAB, and why does this matter for fairness?

- Concept: Upper Confidence Bound (UCB) learning for MDPs
  - Why needed here: The algorithm uses UCB-style confidence bounds to estimate transition matrices online without prior knowledge.
  - Quick check question: How does the confidence radius formula depend on the number of visits to a state-action pair, and why is this important for exploration?

- Concept: Steady-state analysis of Markov chains
  - Why needed here: The reward metric is defined using steady-state probabilities, which requires solving for the long-run distribution under a given policy.
  - Quick check question: How is the steady-state probability ğ‘“(ğ‘ƒğ‘–, ğ‘ğ‘–) derived from the transition matrix, and what assumptions ensure its existence?

## Architecture Onboarding

- Component map: Transition estimator -> Reward calculator -> Sampling module -> Environment interaction -> Count updater
- Critical path: At each episode, the estimator updates empirical means and confidence radii, the reward calculator evaluates all arms, the sampler draws arms without replacement, the environment is stepped, and counts are updated. This cycle must complete within the episode budget.
- Design tradeoffs: Using optimistic bounds (ğ‘ƒ +,ğ‘¡ ğ‘–) ensures exploration but may overestimate some arms' merits; using pessimistic bounds would reduce exploration. The choice of merit function ğ‘”(Â·) trades off fairness (uniform) vs optimality (sharp). The single-pull constraint simplifies fairness regret but limits practical applicability.
- Failure signatures: If an arm's counts stagnate, its confidence bounds will not shrink and its reward estimate will be poor; if the merit function is too steep, only a few arms get selected and the algorithm becomes nearly optimal; if the time horizon ğ» is too short, steady-state assumptions break down.
- First 3 experiments:
  1. Run with ğ‘=2, ğ¾=1, synthetic transitions, and verify that both arms receive pulls over time and exposure is proportional to merit.
  2. Vary ğ‘ in ğ‘”(ğœ‡)=ğ‘’ğ‘ğœ‡ from small to large and observe the shift from fairness to optimality in arm selection.
  3. Introduce non-ergodic transitions and check whether the algorithm still converges or fails due to missing steady-state assumptions.

## Open Questions the Paper Calls Out

- How can meritocratic fairness be formally extended to multi-pull settings in restless multi-armed bandits?
  - Basis in paper: [explicit] The paper states that extending merit-based fairness to multiple pulls is challenging and requires technical assumptions.
  - Why unresolved: The paper only provides theoretical analysis for the single-pull case (K=1) and notes that quantifying fairness is challenging for multiple pulls.
  - What evidence would resolve it: A formal definition of fairness regret for multiple-pull settings with provable sublinear regret bounds.

- What is the optimal value of the hyperparameter c in the merit function g(Î¼) = e^(cÎ¼) for balancing fairness and performance?
  - Basis in paper: [explicit] The paper shows that varying c affects both fairness regret and variance in experimental results, but doesn't provide guidance on optimal selection.
  - Why unresolved: The paper demonstrates that c acts as a hyperparameter calibrating the trade-off between fairness and optimality, but doesn't specify how to choose it.
  - What evidence would resolve it: Empirical studies across diverse problem instances showing the relationship between c values, fairness metrics, and performance measures.

- How does the proposed meritocratic fairness notion compare to other fairness notions (like group fairness or minimum exposure guarantees) in restless multi-armed bandits?
  - Basis in paper: [inferred] The paper contrasts its meritocratic fairness with existing fairness notions but doesn't provide direct empirical comparisons.
  - Why unresolved: The paper focuses on its proposed fairness notion without benchmarking against alternative fairness definitions in the RMAB setting.
  - What evidence would resolve it: Systematic experimental comparisons of different fairness notions across multiple problem domains, measuring both fairness metrics and performance.

## Limitations

- Theoretical guarantees are limited to single-pull case (K=1), with only empirical validation for multi-pull settings
- Algorithm depends on steady-state existence and accurate transition matrix learning, which may not hold in practice
- Empirical validation is limited to small-scale synthetic problems and one real-world dataset with significant preprocessing assumptions

## Confidence

Medium: The theoretical framework is sound and the sublinear fairness regret bound appears correctly derived under stated assumptions. However, the empirical validation is limited to small-scale synthetic problems and one real-world dataset with significant preprocessing assumptions. The gap between theory (single-pull) and experiments (multi-pull) reduces confidence in practical applicability.

## Next Checks

1. Test MF-RMAB on a larger-scale synthetic RMAB with N=50 arms and K=5 pulls to evaluate scalability and whether fairness regret remains sublinear in more complex scenarios.
2. Implement a version of MF-RMAB without the steady-state assumption (e.g., using finite-horizon MDPs) to test robustness when theoretical assumptions are violated.
3. Compare MF-RMAB against a non-fairness-aware RMAB baseline (like Whittle index) on the CPAP dataset to quantify the performance-fairness tradeoff in practice.