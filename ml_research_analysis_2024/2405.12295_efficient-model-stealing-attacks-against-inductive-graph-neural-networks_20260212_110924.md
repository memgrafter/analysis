---
ver: rpa2
title: Efficient Model-Stealing Attacks Against Inductive Graph Neural Networks
arxiv_id: '2405.12295'
source_url: https://arxiv.org/abs/2405.12295
tags:
- shen
- graph
- ours
- surrogate
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a new unsupervised model-stealing attack
  against inductive graph neural networks (GNNs) using graph contrastive learning
  and spectral graph augmentations. The method trains a surrogate model to align its
  node embeddings with those of the target model while distinguishing them from other
  nodes, using spectral augmentations to generate augmented graph views.
---

# Efficient Model-Stealing Attacks Against Inductive Graph Neural Networks

## Quick Facts
- arXiv ID: 2405.12295
- Source URL: https://arxiv.org/abs/2405.12295
- Authors: Marcin Podhajski; Jan Dubiński; Franziska Boenisch; Adam Dziedzic; Agnieszka Pregowska; Tomasz P. Michalak
- Reference count: 40
- Key outcome: New unsupervised model-stealing attack against inductive GNNs using graph contrastive learning and spectral graph augmentations, outperforming state-of-the-art by Shen et al. (2021) by 36.8 percentage points on Amazon Co-purchase Network while requiring only 50% of the query budget.

## Executive Summary
This paper presents a novel approach to model-stealing attacks against inductive Graph Neural Networks (GNNs) that leverages unsupervised contrastive learning and spectral graph augmentations. The method trains a surrogate model to align its node embeddings with those of the target model while distinguishing them from other nodes, using spectral augmentations to generate augmented graph views. Experiments on six datasets demonstrate that the approach consistently outperforms existing methods in accuracy, fidelity, and query efficiency, achieving comparable or better performance with only 50% of the query budget required by the baseline.

## Method Summary
The proposed method uses graph contrastive learning with spectral graph augmentations to perform unsupervised model-stealing attacks against inductive GNNs. For an input graph, two views are generated - one from the target model and one from the surrogate model for an augmented graph input. The surrogate model is trained by maximizing the agreement of node representations in these two views through a contrastive objective. Spectral augmentations manipulate the frequency components of the graph structure to create optimal contrastive pairs, enabling the surrogate to learn representations that align with the target model while maintaining distinctiveness between different nodes.

## Key Results
- Outperforms Shen et al. (2021) baseline by 36.8 percentage points on Amazon Co-purchase Network
- Achieves comparable or better performance with only 50% of the query budget needed by the baseline
- Demonstrates consistent improvements across six benchmark datasets in accuracy, fidelity, and downstream performance
- Shows particular effectiveness when using GraphSAGE as the surrogate model against GIN target models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The contrastive objective at the node level enables the surrogate model to learn representations that align with the target model's embeddings while maintaining distinctiveness between different nodes.
- Mechanism: The surrogate model generates augmented graph views and is trained using a contrastive loss that pulls together positive pairs (corresponding nodes from target and surrogate) while pushing apart negative pairs (different nodes from either model).
- Core assumption: Node embeddings from the target model for augmented graph views remain similar to embeddings for the original node.
- Evidence anchors:
  - [abstract]: "We align the node representation from the surrogate model with the corresponding representation from the victim model while simultaneously distinguishing it from the representation of other nodes."
  - [section]: "Specifically, for an input graph, we generate two graph views—one from the target model and one from the surrogate model for an augmented graph input. The surrogate model is trained by maximizing the agreement of node representations in these two views."
- Break condition: If the target model's embeddings for augmented nodes diverge significantly from original node embeddings, the contrastive objective fails to provide useful training signals.

### Mechanism 2
- Claim: Spectral graph augmentations create optimal contrastive pairs by manipulating frequency components of the graph structure.
- Mechanism: The adjacency matrix is transformed using spectral analysis to create augmented graphs where high-frequency differences are maximized while low-frequency differences are minimized, following the optimality condition |ϕV1(λm)−ϕV2(λm)| > |ϕV1(λn)−ϕV2(λn)|.
- Core assumption: The graph spectrum ϕ(λ) can be effectively manipulated to create augmentations that preserve semantic information while introducing useful variation.
- Evidence anchors:
  - [section]: "We leverage the basic but essential fact that the victim model produces similar outputs for a given node and its augmented version."
  - [section]: "For two random augmentations V1 and V2, their graph spectra are ϕV1(λ) and ϕV2(λ). Then, for all λm ∈ [1, 2] and λn ∈ [0, 1], V1 and V2 constitute an effective pair of graph augmentations if the following condition is met: OptimV1,V2 : |ϕV1(λm)−ϕV2(λm)| > |ϕV1(λn)−ϕV2(λn)|."
- Break condition: If the spectral augmentations introduce too much noise or change the graph structure in ways that the target model cannot handle, the augmented graphs will produce unreliable outputs.

### Mechanism 3
- Claim: The unsupervised nature of the attack allows it to bypass the need for labeled data while still achieving high fidelity.
- Mechanism: By using contrastive learning with augmented graph views as self-supervision, the surrogate model learns to mimic the target model's behavior without requiring ground truth labels for the query graph.
- Core assumption: The target model's responses contain sufficient information about its decision boundaries and internal representations to enable effective mimicry.
- Evidence anchors:
  - [abstract]: "This paper identifies a new method of performing unsupervised model-stealing attacks against inductive GNNs, utilizing graph contrastive learning and spectral graph augmentations to efficiently extract information from the targeted model."
  - [section]: "We argue that simply matching the outputs of the victim and surrogate model proves inefficient within the context of model stealing. Building upon this observation, we propose a novel approach grounded in contrastive graph learning."
- Break condition: If the target model's outputs are too abstract or noisy, the surrogate model may fail to learn meaningful representations without explicit supervision.

## Foundational Learning

- Concept: Graph Neural Networks and their inductive learning capabilities
  - Why needed here: The attack specifically targets inductive GNNs that can generalize to unseen nodes, requiring understanding of how these models process graph-structured data
  - Quick check question: What is the key difference between transductive and inductive GNN settings?

- Concept: Contrastive learning principles and their application to graph data
  - Why needed here: The attack's core mechanism relies on contrastive objectives to align embeddings between target and surrogate models
  - Quick check question: How does the contrastive loss in this paper differ from standard supervised learning?

- Concept: Spectral graph theory and graph frequency components
  - Why needed here: The augmentation strategy uses spectral analysis to manipulate graph structure while preserving meaningful information
  - Quick check question: What is the significance of separating low and high frequency components in graph augmentations?

## Architecture Onboarding

- Component map: Query generator -> Target model interface -> Graph augmentation module -> Surrogate model -> Classification head
- Critical path: Query generation → Target model response → Graph augmentation → Surrogate training → Classification head training → Evaluation
- Design tradeoffs:
  - Query budget vs. attack quality: Fewer queries reduce detection risk but may compromise fidelity
  - Augmentation strength vs. output reliability: Stronger augmentations provide better contrastive signals but may produce unreliable target model outputs
  - Model complexity vs. training efficiency: More complex surrogate models may achieve better mimicry but require more computational resources
- Failure signatures:
  - Surrogate model produces outputs with high variance across different runs
  - Contrastive loss plateaus early in training
  - Target model returns errors or NaN values for augmented graphs
  - Classification head shows poor performance despite good surrogate training
- First 3 experiments:
  1. Implement basic query-response loop with a simple surrogate model to verify connectivity with the target model
  2. Add graph augmentations and verify that target model produces consistent outputs for original and augmented nodes
  3. Implement contrastive loss training and measure embedding alignment between target and surrogate models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method perform on graph-level and link prediction tasks?
- Basis in paper: [inferred] The paper mentions the proposed method is limited to node-level results but suggests it can be extended to graph-level and link prediction tasks by adjusting the augmentations.
- Why unresolved: The paper does not provide experimental results or analysis for graph-level and link prediction tasks.
- What evidence would resolve it: Experimental results demonstrating the performance of the proposed method on graph-level and link prediction tasks, along with a comparison to existing methods.

### Open Question 2
- Question: What is the impact of the query budget on the proposed method's performance for different GNN architectures?
- Basis in paper: [explicit] The paper evaluates the query budget impact on performance but focuses on GIN as the target model and only provides detailed results for Citeseer dataset.
- Why unresolved: The paper does not provide a comprehensive analysis of the query budget's impact across different GNN architectures and datasets.
- What evidence would resolve it: A thorough analysis of the proposed method's performance across different GNN architectures and datasets under varying query budgets.

### Open Question 3
- Question: How does the proposed method's performance compare to other defense mechanisms against model stealing attacks?
- Basis in paper: [explicit] The paper mentions the existence of defense mechanisms but only evaluates one specific defense approach (random Gaussian noise injection).
- Why unresolved: The paper does not provide a comprehensive comparison of the proposed method's robustness against various defense mechanisms.
- What evidence would resolve it: A comparative analysis of the proposed method's performance against multiple defense mechanisms, including but not limited to random Gaussian noise injection.

## Limitations

- The method assumes the target model produces reliable outputs for augmented graph views, which may not hold for models trained on noisy or adversarial data
- Performance may degrade when attacking models with different architectural designs or when the query graph has significantly different characteristics from training data
- Spectral augmentation approach requires careful tuning of frequency manipulation parameters, and poor parameter choices could degrade attack effectiveness

## Confidence

- **High confidence**: The general framework of using contrastive learning with spectral augmentations for model stealing is well-founded and the experimental methodology is sound
- **Medium confidence**: The specific implementation details and hyperparameter choices are adequately described, though some implementation nuances may affect reproducibility
- **Medium confidence**: The claimed improvements over baseline methods are supported by experimental results, but the magnitude of improvements may vary depending on implementation specifics

## Next Checks

1. Test the attack against models trained with adversarial robustness techniques to evaluate vulnerability to defense mechanisms
2. Investigate the impact of query graph quality and similarity to training distribution on attack effectiveness
3. Evaluate the method's transferability when attacking models with different architectures or trained on different datasets