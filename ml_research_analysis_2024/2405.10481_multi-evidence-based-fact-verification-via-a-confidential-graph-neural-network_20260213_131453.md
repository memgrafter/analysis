---
ver: rpa2
title: Multi-Evidence based Fact Verification via A Confidential Graph Neural Network
arxiv_id: '2405.10481'
source_url: https://arxiv.org/abs/2405.10481
tags:
- node
- verification
- co-gat
- attention
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses fact verification by mitigating the propagation
  of noise information in graph-based reasoning models. It proposes a Confidential
  Graph Attention Network (CO-GAT) that introduces a node masking mechanism based
  on confidence scores, which control the flow of evidence information from noisy
  nodes to other graph nodes.
---

# Multi-Evidence based Fact Verification via A Confidential Graph Neural Network

## Quick Facts
- arXiv ID: 2405.10481
- Source URL: https://arxiv.org/abs/2405.10481
- Reference count: 40
- Primary result: Achieves 73.59% FEVER score on blind test set with node masking and multi-task learning

## Executive Summary
This paper addresses the challenge of noise propagation in graph-based fact verification models by introducing a Confidential Graph Attention Network (CO-GAT). The model uses a node masking mechanism based on confidence scores to selectively suppress noisy evidence during multi-evidence reasoning. Evaluated on FEVER and SCIFACT datasets, CO-GAT demonstrates improved performance over baseline models, particularly in handling scientific claims where evidence reliability is critical.

## Method Summary
CO-GAT constructs a fully connected graph with claim-evidence pairs as nodes and applies a node masking mechanism to reduce noise propagation. The model computes confidence scores for each node based on claim-evidence relevance, then applies soft masking by blending node representations with blank nodes according to these scores. A graph attention network performs multi-evidence reasoning, while a multi-task learning approach jointly optimizes claim verification and node confidence prediction. The method is evaluated on FEVER and SCIFACT datasets using FEVER score, label accuracy, and F1 metrics.

## Key Results
- Achieves 73.59% FEVER score on blind test set
- Demonstrates effectiveness on SCIFACT dataset for scientific claims
- Ablation studies confirm efficacy of node masking and multi-task modeling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The node masking mechanism prevents noisy evidence from distorting the semantic representations of other nodes in the graph.
- Mechanism: Confidence scores are computed for each claim-evidence pair, and nodes with low confidence are partially replaced by a blank node representation during message passing.
- Core assumption: The relevance between claim and evidence can be accurately estimated using a learned confidence score.
- Evidence anchors: [abstract] "CO-GAT calculates the node confidence score by estimating the relevance between the claim and evidence pieces." [section III-B] "The model assesses the confidence score (CO-SCO) of nodes and adjusts the information flow of blank nodes according to this score."

### Mechanism 2
- Claim: Using a soft masking approach (weighted blending of node and blank representations) is more robust than hard masking.
- Mechanism: Instead of replacing low-confidence nodes entirely with blank nodes, the model blends the original node representation with the blank node using the confidence score as a weight.
- Core assumption: Partial information retention is better than complete removal for uncertain nodes.
- Evidence anchors: [section III-B] "ehp = CO-SCO × hp + (1 − CO-SCO) × hb" explicitly shows the weighted blend. [section V-B] "A soft score that partly erases the node information can achieve more precision in the claim verification model."

### Mechanism 3
- Claim: Multi-task learning with node confidence prediction improves the overall model by providing auxiliary supervision.
- Mechanism: The model jointly optimizes claim verification loss and node confidence prediction loss, encouraging better calibration of the confidence scores.
- Core assumption: The auxiliary task of predicting node relevance provides useful gradients for learning more discriminative node representations.
- Evidence anchors: [section III-C] "For each node in the evidence graph, we predict its confidential label ye...Levi = CrossEntropy(y*ep , P (yep |c, E))." [section V-B] "Compared with the CO-GAT w/o Levi model, the CO-GAT model achieves a 0.18% improvement."

## Foundational Learning

- Concept: Graph Attention Networks (GATs)
  - Why needed here: GATs are used to aggregate and propagate evidence information across nodes for multi-evidence reasoning.
  - Quick check question: In GAT, how are edge attention weights computed between nodes?

- Concept: Multi-head attention
  - Why needed here: Allows the model to capture different aspects of node interactions by computing multiple attention distributions.
  - Quick check question: What is the purpose of concatenating the outputs from multiple attention heads in GAT?

- Concept: Confidence score calibration
  - Why needed here: Ensures that the node masking mechanism effectively distinguishes between relevant and irrelevant evidence.
  - Quick check question: How does scaling the confidence score (α ∈ [0,0.0]) affect the balance between noise suppression and information retention?

## Architecture Onboarding

- Component map: Node encoder (PLM) -> Confidence score estimator -> Node masking module -> Graph attention reasoning layer -> Multi-task loss -> Output classifier
- Critical path: Encode claim-evidence pair → Compute confidence score → Apply node masking → Perform graph attention reasoning → Aggregate node information → Predict claim label.
- Design tradeoffs:
  - Soft vs. hard masking: Soft masking retains partial information but may allow some noise; hard masking is cleaner but risks losing useful data.
  - Multi-task learning vs. single-task: Multi-task adds regularization but increases training complexity.
- Failure signatures:
  - Low FEVER score with high NEI predictions: Model may be over-suppressing evidence.
  - High variance in node attention weights: Confidence scores may be poorly calibrated.
  - Low edge attention entropy: Graph may not be learning meaningful interactions.
- First 3 experiments:
  1. Compare FEVER scores of soft vs. hard masking with fixed confidence thresholds.
  2. Ablation study removing the multi-task node prediction loss.
  3. Vary the scaling coefficient α to study the trade-off between noise suppression and evidence retention.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CO-GAT compare when using different pre-trained language models (e.g., BERT, RoBERTa, ELECTRA) as the backbone?
- Basis in paper: [explicit] The paper mentions using ELECTRA, RoBERTa, and GPT2 as backbone models and provides results for each in Table IV.
- Why unresolved: The paper does not provide a detailed analysis of the impact of different backbone models on the overall performance of CO-GAT. It only mentions the results in Table IV without further discussion.
- What evidence would resolve it: A detailed analysis comparing the performance of CO-GAT with different backbone models, including their strengths and weaknesses, would help resolve this question.

### Open Question 2
- Question: How does the node masking mechanism affect the attention distribution in the graph reasoning process?
- Basis in paper: [explicit] The paper discusses the node masking mechanism and its effect on the attention distribution in the graph reasoning process in Section V-D and Figure 5.
- Why unresolved: While the paper provides some insights into the effect of the node masking mechanism on the attention distribution, it does not provide a comprehensive analysis of how it affects the overall performance of the model.
- What evidence would resolve it: A detailed analysis of the attention distribution before and after applying the node masking mechanism, including its impact on the overall performance of the model, would help resolve this question.

### Open Question 3
- Question: How does the confidence score (CO-SCO) affect the prediction of the claim label as "NOT ENOUGH INFO" (NEI)?
- Basis in paper: [explicit] The paper discusses the relationship between the confidence score and the prediction of the claim label as NEI in Section V-C and Figure 3.
- Why unresolved: While the paper provides some insights into the relationship between the confidence score and the prediction of the claim label as NEI, it does not provide a comprehensive analysis of how it affects the overall performance of the model.
- What evidence would resolve it: A detailed analysis of the relationship between the confidence score and the prediction of the claim label as NEI, including its impact on the overall performance of the model, would help resolve this question.

## Limitations
- The confidence score estimation mechanism relies on learned relevance and may be biased by noisy training data
- The soft masking approach assumes partial retention of noisy nodes is beneficial, which may not hold for all noise types
- Multi-task learning provides only marginal improvement (0.18% on FEVER score) and may not generalize across domains

## Confidence
- **High Confidence**: The overall architecture of CO-GAT (node masking + graph attention + multi-task learning) is well-defined and the reported results on FEVER and SCIFACT are consistent with the claimed methodology.
- **Medium Confidence**: The effectiveness of the node masking mechanism is supported by ablation studies, but the specific design choices could be sensitive to hyperparameter settings.
- **Low Confidence**: The robustness of the model to different types of noise in the evidence graph is not thoroughly evaluated.

## Next Checks
1. **Ablation on Confidence Score Estimation**: Remove the node confidence prediction task and use a fixed threshold or heuristic for node masking. Compare the FEVER score to assess whether the learned confidence scores are crucial.
2. **Stress Test with Synthetic Noise**: Inject varying levels of synthetic noise into the evidence graph and measure the degradation in FEVER score to reveal the model's sensitivity to noise.
3. **Cross-Dataset Generalization**: Evaluate CO-GAT on a third fact verification dataset (e.g., HoVer) to test whether the node masking and multi-task learning approaches generalize beyond FEVER and SCIFACT.