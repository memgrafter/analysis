---
ver: rpa2
title: 'Dual-CBA: Improving Online Continual Learning via Dual Continual Bias Adaptors
  from a Bi-level Optimization Perspective'
arxiv_id: '2408.13991'
source_url: https://arxiv.org/abs/2408.13991
tags:
- learning
- task
- tasks
- continual
- dual-cba
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses catastrophic forgetting and task-recency bias
  in online continual learning by modeling posterior distribution shifts using a bi-level
  optimization framework. The method introduces a Dual-CBA module that combines class-specific
  and class-agnostic continual bias adaptors, along with Incremental Batch Normalization
  (IBN), to stabilize knowledge consolidation across tasks while adapting to distribution
  shifts.
---

# Dual-CBA: Improving Online Continual Learning via Dual Continual Bias Adaptors from a Bi-level Optimization Perspective

## Quick Facts
- arXiv ID: 2408.13991
- Source URL: https://arxiv.org/abs/2408.13991
- Reference count: 40
- Key outcome: Dual-CBA achieves up to 7.70% accuracy improvement and 36.90% forgetting reduction across multiple rehearsal-based baselines

## Executive Summary
Dual-CBA addresses catastrophic forgetting and task-recency bias in online continual learning by introducing a novel bi-level optimization framework with Dual Continual Bias Adaptors. The method combines class-specific and class-agnostic bias adaptors to model posterior distribution shifts while maintaining stability across task transitions. When integrated with Incremental Batch Normalization, Dual-CBA demonstrates consistent performance improvements across CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets, outperforming existing rehearsal-based methods while maintaining computational efficiency.

## Method Summary
Dual-CBA operates through a bi-level optimization framework where an inner loop updates the classification network using rehearsal-based methods assisted by Dual-CBA, while an outer loop optimizes the Dual-CBA parameters using buffer data. The Dual-CBA module contains two MLPs: a class-specific adaptor that adjusts posterior probabilities per class, and a class-agnostic adaptor that models stable relationships between new and old task posteriors. Incremental Batch Normalization complements this by stopping EMA updates during inner-loop optimization and using buffer-based statistics for testing, mitigating feature bias. The approach is designed as a plug-in for existing rehearsal-based baselines like ER, DER++, RAR, and CLSER.

## Key Results
- Achieves up to 7.70% accuracy improvement over baselines across CIFAR-10, CIFAR-100, and Tiny-ImageNet
- Reduces forgetting by up to 36.90% compared to standard rehearsal methods
- Maintains strong performance with varying buffer sizes (0.2k to 5k)
- Demonstrates effective generalization to semi-supervised, blurry-task, and offline continual learning settings

## Why This Works (Mechanism)

### Mechanism 1: Class-agnostic CBA
- Claim: Dual-CBA adapts to catastrophic posterior distribution shifts by modeling the relationship between new and old task posteriors rather than directly adjusting class-specific probabilities.
- Mechanism: Class-agnostic CBA aggregates posterior probabilities of new and old tasks separately and applies stable corrections. This creates a transferable bias adaptor that quickly adapts to sudden posterior changes without suffering from stability gap issues.
- Core assumption: The relationship between new-task and old-task posterior probabilities remains relatively stable across task transitions, allowing class-agnostic adjustments to be effective.
- Evidence anchors: [abstract] "We further propose a novel class-agnostic CBA module that separately aggregates the posterior probabilities of classes from new and old tasks, and applies a stable adjustment"; [section] "Instead of directly adjusting the posterior probability in a class-specific manner, we resort to modeling a stable relationship between new and old tasks"

### Mechanism 2: Bi-level Optimization
- Claim: Bi-level optimization framework aligns gradients between training data (with Dual-CBA) and buffer data (without Dual-CBA), preventing catastrophic forgetting while mitigating task-recency bias.
- Mechanism: Inner loop updates classification network using rehearsal-based methods with Dual-CBA assistance; outer loop optimizes Dual-CBA to consolidate knowledge from buffer data. This creates gradient alignment that regularizes updates and ensures balanced attention to new and old tasks.
- Core assumption: The buffer data provides a reasonable approximation of the overall data distribution across all seen tasks, making it suitable for outer-loop optimization.
- Evidence anchors: [section] "Theorem 1: Let Gbuf and Gtrn denote the gradients... If the outer-loop loss Lbuf(·; fθ) is η gradient Lipschitz continuous, then the bi-level optimization... potentially guarantees an alignment between Gbuf and Gtrn"

### Mechanism 3: Incremental Batch Normalization
- Claim: Incremental Batch Normalization (IBN) mitigates feature bias by stopping EMA updates in the inner loop and using buffer-based statistics for testing.
- Mechanism: IBN replaces EMA-updated population statistics with statistics calculated from balanced buffer data, providing unbiased feature normalization during testing that compensates for the feature bias introduced by inner-loop optimization.
- Core assumption: The memory buffer obtained via reservoir sampling provides a balanced representation of all seen tasks that can approximate unbiased population statistics.
- Evidence anchors: [abstract] "we propose Incremental Batch Normalization (IBN), a tailored BN module to re-estimate its population statistics for alleviating the feature bias"; [section] "Specifically, we stop the EMA updating of BN statistics in the inner loop optimization and estimate unbiased population statistics using the memory buffer"

## Foundational Learning

- Concept: Bi-level optimization
  - Why needed here: The method requires simultaneously optimizing classification network parameters (inner loop) and Dual-CBA parameters (outer loop) in a nested fashion, which cannot be solved through standard single-level optimization.
  - Quick check question: Can you explain why gradient descent on a single objective function cannot optimize both θ and ϕ simultaneously in this framework?

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: The entire motivation addresses how models trained on changing distributions forget previously learned knowledge, which is the core problem Dual-CBA aims to solve.
  - Quick check question: What is the difference between catastrophic forgetting and task-recency bias in continual learning?

- Concept: Posterior distribution shifts
  - Why needed here: The method explicitly models shifts in P(Y|X) rather than just P(X) or P(Y), which is the fundamental insight that distinguishes it from other approaches.
  - Quick check question: How does a shift in the posterior distribution P(Y|X) differ from shifts in the prior P(Y) or likelihood P(X|Y) in terms of their impact on classification performance?

## Architecture Onboarding

### What is directly specified
- Task/problem: Online continual learning (CL) with catastrophic forgetting and task-recency bias.
- Inputs/data: CIFAR-10, CIFAR-100, Tiny-ImageNet datasets split into incremental tasks; replay buffer of limited size.
- Objective/metrics: Average Accuracy (ACC), Forgetting Measure (FM), Area Under Curve of Accuracy (ACCAUC).
- Method/training procedure: Dual-CBA module with class-specific and class-agnostic bias adaptors; bi-level optimization; Incremental Batch Normalization (IBN); plug-in for rehearsal-based baselines (ER, DER++, RAR, CLSER).

### Minimum viable reproduction plan
- Step 1: Set up environment with PyTorch; prepare datasets (CIFAR-10/100, Tiny-ImageNet) split into tasks; implement baseline rehearsal methods (ER, DER++, RAR, CLSER) with standard ResNet-18 backbone.
- Step 2: Implement Dual-CBA module: class-specific MLP (ω), class-agnostic MLP (ν), combine them, and integrate IBN; implement bi-level optimization loop with inner-loop SGD on training batch + buffer, outer-loop Adam on buffer only.
- Step 3: Train and evaluate on each dataset with buffer sizes M=0.2k, 0.5k, 2k, 5k; track ACC, FM, ACCAUC; compare against baselines and ablations (CBA, w/o IBN).

### Unknowns that block faithful reproduction
- Unknown 1: Exact architecture of the Dual-CBA MLPs (layer sizes, activation functions) is not specified.
- Unknown 2: Precise hyper-parameter values for Adam optimizer learning rates and batch sizes for Dual-CBA are missing.

### Common failure modes and diagnostics
- Failure mode 1: Training diverges or accuracy drops if Dual-CBA inner/outer learning rates are mismatched; diagnose by monitoring loss curves and adjusting rates.
- Failure mode 2: IBN implementation may cause instability if EMA updates are not correctly separated; diagnose by checking BN statistics before/after testing and comparing with baseline BN.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Dual-CBA scale with different backbone architectures beyond ResNet-18?
- Basis in paper: [inferred] The paper mentions using ResNet-18 as the backbone but doesn't explore performance with other architectures.
- Why unresolved: The paper focuses on ResNet-18 without providing results for alternative backbone models.
- What evidence would resolve it: Comparative experiments showing Dual-CBA performance across multiple backbone architectures like VGG, MobileNet, or EfficientNet.

### Open Question 2
- Question: What is the theoretical limit of Dual-CBA's transferability when pretrained on very few tasks?
- Basis in paper: [explicit] The paper shows Dual-CBA generalizes well to unseen tasks when pretrained on a limited number of tasks.
- Why unresolved: The paper doesn't quantify how few tasks can be used while maintaining effectiveness.
- What evidence would resolve it: Systematic experiments testing Dual-CBA performance with progressively fewer pretraining tasks until performance degrades significantly.

### Open Question 3
- Question: How does Dual-CBA perform in continual learning scenarios with non-i.i.d. task arrivals?
- Basis in paper: [inferred] The paper assumes sequential task arrivals but doesn't test scenarios where tasks arrive in random order or with varying frequencies.
- Why unresolved: The experimental design uses standard sequential task ordering without testing alternative arrival patterns.
- What evidence would resolve it: Experiments comparing Dual-CBA performance under different task arrival schedules (random, bursty, or cyclic patterns).

### Open Question 4
- Question: What is the computational complexity of Dual-CBA compared to other continual learning methods when scaling to larger datasets?
- Basis in paper: [explicit] The paper reports computation and memory usage for CIFAR-10 but doesn't scale analysis to larger datasets.
- Why unresolved: The paper only provides timing results for a single dataset configuration.
- What evidence would resolve it: Comprehensive benchmarking of Dual-CBA's computational requirements across multiple dataset sizes and complexities.

### Open Question 5
- Question: How sensitive is Dual-CBA's performance to the hyperparameter α (inner-loop learning rate)?
- Basis in paper: [inferred] The paper mentions using different learning rates for different datasets but doesn't analyze sensitivity.
- Why unresolved: The paper uses fixed learning rates without exploring their impact on performance.
- What evidence would resolve it: Sensitivity analysis showing Dual-CBA performance across a range of α values for each dataset.

## Limitations
- Bi-level optimization relies on gradient Lipschitz continuity assumptions that may not hold in practice for deep networks with complex loss landscapes.
- Class-agnostic CBA's effectiveness depends heavily on the stability of posterior relationships across tasks, which could vary significantly with different dataset characteristics.
- IBN's effectiveness assumes the memory buffer provides a representative sample, but with reservoir sampling this may not hold in early learning stages or when buffer capacity is severely limited.

## Confidence

- Mechanism 1 (Class-agnostic CBA): Medium - supported by theoretical framing but limited empirical validation across diverse task sequences
- Mechanism 2 (Bi-level optimization): Medium-Low - theoretical guarantee exists but practical conditions may be violated; no direct comparison to single-level alternatives
- Mechanism 3 (IBN): Low-Medium - addresses a plausible issue but assumes buffer representativeness without rigorous validation

## Next Checks

1. Ablation study testing Dual-CBA with randomly shuffled task orders to verify class-agnostic CBA's effectiveness isn't dataset-specific
2. Buffer size sensitivity analysis to determine minimum buffer size required for IBN to provide meaningful improvement
3. Comparison against single-level optimization baseline where Dual-CBA parameters are updated jointly with main network parameters to validate the bi-level approach's necessity