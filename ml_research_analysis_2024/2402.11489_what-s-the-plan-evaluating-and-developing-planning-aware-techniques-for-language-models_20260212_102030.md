---
ver: rpa2
title: What's the Plan? Evaluating and Developing Planning-Aware Techniques for Language
  Models
arxiv_id: '2402.11489'
source_url: https://arxiv.org/abs/2402.11489
tags:
- planning
- state
- actions
- stack
- unstack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SimPlan, a hybrid planner that combines a greedy best-first search
  with an external world modeling tool and a language model as a ranking heuristic,
  significantly outperforms existing LLM-based planners across diverse planning domains.
  The method addresses the fundamental limitations of LLMs in world modeling by delegating
  state tracking to classical planning tools while using the LLM to score actions
  based on their similarity to current state and goals.
---

# What's the Plan? Evaluating and Developing Planning-Aware Techniques for Language Models

## Quick Facts
- arXiv ID: 2402.11489
- Source URL: https://arxiv.org/abs/2402.11489
- Reference count: 40
- Primary result: SimPlan achieves 56-100% success on complex planning problems where pure LLM methods fail completely

## Executive Summary
This paper addresses the fundamental limitations of large language models (LLMs) in planning tasks by proposing SimPlan, a hybrid planner that combines classical search algorithms with LLM-based heuristics. The key insight is that LLMs struggle with world modeling due to their sequential reasoning and context window limitations, so SimPlan delegates state tracking to external planning tools while using the LLM as an action-ranking heuristic. The approach achieves significantly higher success rates across diverse planning domains, including complex configurations with plans averaging 357 actions where baseline LLM methods fail entirely.

## Method Summary
SimPlan employs a greedy best-first search (GBFS) algorithm enhanced with an external world modeling tool and a language model-based ranking heuristic. The planner maintains state information through classical planning tools, extracting the current state and applicable actions at each step. A similarity-based ranking model, using a ColBERT-style late-interaction architecture, scores each applicable action based on its similarity to the current state and goals. The highest-scoring action is selected and the process repeats until the goal is reached or a step limit is exceeded. Data augmentation through random object identifier permutation prevents model bias and enables generalization to unseen problem configurations.

## Key Results
- On complex Blocksworld problems (357 actions average), SimPlan achieved 56% success rate while all baseline LLM methods failed completely
- 100% success rate on Ferry and Grippers domains, demonstrating strong performance on simpler planning tasks
- Mixed results on more complex domains: 12% success on Depots (highlighting limitations with complex dependencies) and 86% on Minigrid

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Combining classical search algorithms with LLMs as heuristics overcomes the fundamental limitations of LLMs in world modeling by delegating state tracking to classical planning tools.
- **Mechanism:** SimPlan uses GBFS with external world modeling through planning tools, while the LLM functions as an action-ranking heuristic based on similarity to current state and goals.
- **Core assumption:** Language models can effectively rank actions when given direct access to accurate state information, even if they cannot independently maintain world models.
- **Evidence anchors:**
  - [abstract] "SimPlan, a hybrid planner that combines a greedy best-first search with an external world modeling tool and a language model as a ranking heuristic"
  - [section] "We propose SimPlan, a language model-based planner that combines a greedy best-first search algorithm and external world modeling tools"
- **Break condition:** The mechanism breaks if the LLM cannot accurately rank actions even with perfect state information, or if the external world modeling tool becomes a bottleneck.

### Mechanism 2
- **Claim:** The similarity-based ranking architecture using late-interaction (ColBERT-style) enables efficient and effective action scoring across diverse planning domains.
- **Mechanism:** SimPlan employs a bi-encoder architecture where current state and goals form the query, while applicable actions are the context. Late-interaction computes max cosine similarity between individual tokens.
- **Core assumption:** Late-interaction similarity scoring provides both computational efficiency and improved prediction accuracy compared to full cross-attention.
- **Evidence anchors:**
  - [abstract] "Our proposed similarity-based ranking architecture...using the late-interaction architecture of ColBERT"
  - [section] "This architecture computes the max cosine similarity between individual tokens of the query and the context...The total similarity score for an action is then determined by summing these maximum scores"
- **Break condition:** The mechanism fails when token-level similarity cannot capture semantic relationships necessary for effective action ranking.

### Mechanism 3
- **Claim:** Data augmentation through random object identifier permutation prevents model bias and enables generalization to unseen problem configurations.
- **Mechanism:** During training, each problem instance is augmented by generating 100 permutations where object identifiers are randomly shuffled to prevent the model from learning spurious correlations.
- **Core assumption:** Random identifier permutation during training creates a more robust representation that generalizes across different problem scales and configurations.
- **Evidence anchors:**
  - [abstract] "To mitigate this bias, we augment the training data by generating 100 permutations for each instance, randomly shuffling object identifiers"
  - [section] "We augment the training data by generating 100 permutations for each instance, randomly shuffling object identifiers to ensure that the model does not develop any preferences based on identifier frequency"
- **Break condition:** The mechanism fails when randomization prevents learning meaningful patterns or when augmented data space becomes too large.

## Foundational Learning

- **Concept:** Classical planning and PDDL (Planning Domain Definition Language)
  - **Why needed here:** Understanding how planning domains are formally defined and how classical planners operate is essential for designing hybrid approaches that leverage these tools effectively.
  - **Quick check question:** What are the three components of a classical planning problem in PDDL format, and how do they relate to the planning task?

- **Concept:** Greedy Best-First Search (GBFS) and heuristic functions
  - **Why needed here:** SimPlan's core algorithm is GBFS, and understanding how heuristic functions guide search is crucial for implementing and debugging the planning process.
  - **Quick check question:** How does GBFS differ from other search algorithms like A* or beam search, and what are the implications for planning performance?

- **Concept:** Late-interaction architectures and similarity scoring
  - **Why needed here:** The action ranking mechanism in SimPlan relies on ColBERT-style late-interaction similarity scoring, which is different from traditional cross-attention approaches.
  - **Quick check question:** How does late-interaction similarity scoring work, and what are its computational advantages compared to other ranking approaches?

## Architecture Onboarding

- **Component map:** External planning tools -> GBFS algorithm -> Similarity-based ranking model -> Data augmentation pipeline -> PDDL translator

- **Critical path:**
  1. Initialize GBFS with initial state and goals
  2. Extract current state and applicable actions using external tools
  3. Score each applicable action using the similarity model
  4. Select highest-scoring action and update state
  5. Repeat until goal reached or step limit exceeded

- **Design tradeoffs:**
  - Accuracy vs. efficiency: External world modeling provides accuracy but adds computational overhead
  - Generalization vs. specialization: Data augmentation enables generalization but may reduce performance on specific instances
  - Complexity vs. interpretability: Late-interaction scoring is efficient but less interpretable than attention-based approaches

- **Failure signatures:**
  - State tracking errors: Model gets stuck in loops or makes invalid moves
  - Action ranking failures: Model consistently chooses suboptimal actions
  - Data augmentation issues: Model performs well on training data but poorly on test data

- **First 3 experiments:**
  1. Test state tracking accuracy by running the external planner on simple instances and verifying state transitions
  2. Evaluate action ranking performance on a small set of instances with known optimal actions
  3. Assess data augmentation effectiveness by training with and without augmentation on a controlled dataset

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- **Domain Generalization Challenge:** Effectiveness on domains with complex dependencies or continuous state spaces remains uncertain
- **Computational Efficiency Trade-offs:** Hybrid approach introduces significant computational overhead compared to pure LLM-based planners
- **Training Data Dependence:** Performance still heavily depends on quality and diversity of training data despite augmentation efforts

## Confidence

**High Confidence:** The core mechanism of separating world modeling from action ranking is well-supported by empirical results across multiple domains.

**Medium Confidence:** Specific implementation choices like late-interaction similarity scoring and data augmentation parameters show effectiveness but may not be optimal for all domains.

**Low Confidence:** Approach's scalability to problems with significantly longer plan horizons or domains with continuous state spaces remains largely unexplored.

## Next Checks

1. **Cross-Domain Transferability Test:** Evaluate SimPlan's performance on planning domains not seen during training, particularly those with different state space characteristics to assess true generalization capabilities.

2. **Computational Complexity Analysis:** Measure and analyze the time and resource requirements for SimPlan compared to pure LLM approaches across varying problem sizes to quantify efficiency trade-offs.

3. **Failure Mode Investigation:** Systematically analyze instances where SimPlan fails to identify specific patterns or limitations in action ranking mechanism, state tracking accuracy, or search strategy.