---
ver: rpa2
title: Hierarchical Reinforcement Learning for Temporal Abstraction of Listwise Recommendation
arxiv_id: '2409.07416'
source_url: https://arxiv.org/abs/2409.07416
tags:
- user
- recommendation
- learning
- which
- edge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of listwise recommendation by
  proposing a hierarchical reinforcement learning (HRL) framework called mccHRL. The
  core idea is to decompose the problem into two levels: a high-level agent that models
  user perception and outra-session context, and a low-level agent that handles item
  selection within a session using on-device features.'
---

# Hierarchical Reinforcement Learning for Temporal Abstraction of Listwise Recommendation

## Quick Facts
- arXiv ID: 2409.07416
- Source URL: https://arxiv.org/abs/2409.07416
- Authors: Luo Ji; Gao Liu; Mingyang Yin; Hongxia Yang; Jingren Zhou
- Reference count: 4
- Primary result: Proposed mccHRL achieves 1.2% CTR and 1.5% total views improvement in live A/B test

## Executive Summary
This paper addresses the challenge of listwise recommendation by proposing a hierarchical reinforcement learning framework called mccHRL. The approach decomposes the recommendation problem into a high-level agent modeling user perception and out-of-session context, and a low-level agent handling item selection within sessions using on-device features. This mobile-cloud collaborative architecture leverages edge computing to improve personalization while reducing computational overhead. The method was validated through both simulator-based and industrial dataset experiments, demonstrating significant performance improvements over several baselines.

## Method Summary
The mccHRL framework implements a hierarchical actor-critic architecture where the high-level agent (critic) operates on the cloud to model user preferences and spatiotemporal context, while the low-level agent (actor) runs on edge devices to perform sequential item selection. The system uses DDPG with intrinsic motivation, where the high-level action serves as a goal for the low-level agent. A shared Session Recurrent Encoder processes item sequences, with the low-level agent incorporating on-device features unavailable to cloud-only systems. The framework was trained and evaluated on Movielens-100k and Taobao datasets, with performance measured by average rating and AUC metrics.

## Key Results
- Achieved 1.2% increase in click-through rate (CTR) and 1.5% increase in total views in live A/B test on a world-leading mobile application
- Outperformed several baselines including DIN, GRU4rec, and LIRD in both simulator-based and industrial dataset experiments
- Demonstrated effective mobile-cloud collaboration with O(K^2 * T) time complexity for session length K

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical decomposition into high-level user perception modeling and low-level item selection improves both personalization and computational efficiency. The high-level agent encodes long-term user preferences and spatiotemporal context, while the low-level agent selects items based on short-term interest and on-device features. This separation allows cloud-side processing of complex user modeling while offloading item selection to the edge, reducing latency and bandwidth.

### Mechanism 2
Edge-based deployment of the low-level agent enables incorporation of on-device features and reduces cloud communication overhead. By deploying the low-level agent on mobile devices, the system can access local user features (app usage, location data) that are unavailable to cloud-only systems. This also enables offline personalization and reduces the need for constant cloud communication during item selection.

### Mechanism 3
Goal-conditioned hierarchical framework with intrinsic motivation solves the sparse reward problem in listwise recommendation. The high-level agent's action (representing user preference) serves as a goal for the low-level agent. The low-level agent receives intrinsic rewards based on how well its selected items match this goal, rather than waiting for end-of-session user feedback.

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation of recommendation systems
  - Why needed here: The entire framework is built on MDP assumptions about state transitions and reward maximization
  - Quick check question: Can you explain how a recommendation session maps to the MDP tuple (S, A, R, T) in this framework?

- Concept: Temporal abstraction in reinforcement learning
  - Why needed here: The core innovation is learning at multiple timescales (session-level vs. item-level decisions)
  - Quick check question: How does the hierarchical structure allow the system to handle different temporal scales in user behavior?

- Concept: Edge computing and mobile-cloud collaboration
  - Why needed here: The low-level agent deployment strategy relies on edge computing principles
  - Quick check question: What are the key trade-offs between processing on cloud vs. edge in this recommendation system?

## Architecture Onboarding

- Component map: HRA (High-Level Agent) -> SRE (Session Recurrent Encoder) -> LRA (Low-Level Agent) -> Item Selection -> User Feedback -> HRA Update
- Critical path: 1) User requests session → HRA processes user context and sends embeddings to LRA 2) LRA selects items sequentially using on-device features and HRA's goal signal 3) Session ends → LRA sends click data back to cloud 4) HRA updates based on session performance, LRA updates based on intrinsic motivation
- Design tradeoffs: Model complexity vs. edge deployment constraints (LRA must fit within mobile device limitations), frequency of cloud-edge communication vs. staleness of HRA's goal signal, granularity of on-device features vs. privacy and bandwidth considerations
- Failure signatures: Poor CTR improvement (indicates misalignment between HRA goal signal and actual user preferences), high latency (suggests edge device cannot process LRA decisions quickly enough), model drift (indicates HRA and LRA are not synchronizing properly due to communication delays)
- First 3 experiments: 1) Unit test the HRA and LRA independently using synthetic data to verify they learn reasonable policies 2) Integration test with simulated cloud-edge communication to measure end-to-end latency and CTR improvement 3) Ablation study removing either the edge deployment or the hierarchical structure to quantify their contributions

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of mccHRL scale with increasing session length K, and at what point does the O(K^2 * T) complexity become prohibitive? The paper provides theoretical analysis but lacks empirical results showing performance degradation at large K values.

### Open Question 2
What is the impact of transmission latency between mobile and cloud on mccHRL's overall performance, and how does this vary across different network conditions? The study uses fixed latency assumptions that may not reflect real-world variability.

### Open Question 3
Can mccHRL effectively incorporate immediate user feedback within a session to adjust recommendations in real-time, and what would be the optimal reward structure for this? The current implementation only uses session-level rewards, leaving real-time adaptation unexplored.

## Limitations
- The specific implementation details of the DIN-based critic are not fully specified, making exact reproduction difficult
- A/B test results lack statistical significance testing and confidence intervals
- The hierarchical decomposition assumes distinct temporal scales in user preferences without empirical validation across different user segments

## Confidence
- High confidence: The hierarchical decomposition concept and its potential benefits (clear theoretical foundation, supported by experimental results)
- Medium confidence: The edge deployment strategy (conceptually sound but implementation details unclear)
- Medium confidence: The intrinsic motivation formulation (novel approach but limited empirical validation)

## Next Checks
1. Conduct ablation studies isolating the contribution of edge deployment vs. hierarchical structure by testing: (a) cloud-only implementation, (b) edge-only implementation, and (c) full mccHRL to quantify their respective impacts
2. Perform statistical significance testing on A/B results with confidence intervals to validate the claimed 1.2% CTR and 1.5% total views improvements
3. Test the hierarchical assumption across diverse user segments by analyzing whether the HRA-LRA alignment varies significantly based on user activity patterns, product categories, or temporal contexts