---
ver: rpa2
title: Behavioral Sequence Modeling with Ensemble Learning
arxiv_id: '2411.02174'
source_url: https://arxiv.org/abs/2411.02174
tags:
- data
- sequence
- behavior
- features
- modeling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a sequence modeling framework for behavior modeling
  using ensembles of Hidden Markov Models (HMMs). The approach addresses challenges
  of constructing coherent sequences from fragmented data and handling imbalanced
  datasets.
---

# Behavioral Sequence Modeling with Ensemble Learning

## Quick Facts
- arXiv ID: 2411.02174
- Source URL: https://arxiv.org/abs/2411.02174
- Authors: Maxime Kawawa-Beaudan; Srijan Sood; Soham Palande; Ganapathy Mani; Tucker Balch; Manuela Veloso
- Reference count: 38
- Primary result: Ensemble of HMMs achieves comparable performance to deep learning models on behavior modeling while being more interpretable and using fewer features

## Executive Summary
This work introduces a sequence modeling framework that uses ensembles of Hidden Markov Models (HMMs) for behavior modeling. The approach addresses key challenges in behavioral sequence analysis including handling fragmented data, managing imbalanced datasets, and comparing sequences of varying lengths. By leveraging ensemble methods, the framework achieves robust performance while maintaining interpretability and efficiency. Experiments on a longitudinal human behavior dataset demonstrate that this method outperforms traditional machine learning baselines and matches the performance of complex deep learning models.

## Method Summary
The framework constructs coherent sequences from fragmented behavioral data by splitting into subsequences based on gaps exceeding 30 minutes, then training 250 HMM models per class on random 1% subsets of training data. Each HMM has 3 states with Gaussian emission distributions. The ensemble scoring mechanism computes pairwise likelihood comparisons between positive and negative class models, generating a composite score for each sequence. This score counts how many positive-class models give higher likelihood than negative-class models, effectively normalizing for sequence length differences. The framework is tested on the GLOBEM dataset with four features (smartphone movement ratio, screen time, sleep time, steps) for binary depression classification using an "all-but-one" validation scheme.

## Key Results
- Achieves AUC-ROC and balanced accuracy comparable to CNN models while using traditional ML techniques
- Maintains interpretability through HMM state transitions and likelihood scores
- Handles class imbalance effectively through ensemble diversity
- Performs well on variable-length sequences without explicit normalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ensemble of HMMs provides robust comparison across sequences of different lengths without explicit normalization.
- Mechanism: Each HMM model captures a different random subset of training data, creating diverse likelihood distributions. The pairwise comparison score s(O) counts how many positive-class models give higher likelihood than negative-class models, effectively normalizing for sequence length.
- Core assumption: Different HMMs trained on random subsets will specialize on different patterns while collectively capturing the full data distribution.
- Evidence anchors:
  - [abstract] "Our ensemble-based scoring method enables robust comparison across sequences of different lengths"
  - [section] "we compute a composite score: s(O) = sum of pairwise comparisons where positive-class models assign higher likelihood than negative-class models"
  - [corpus] Weak evidence - no corpus papers directly discuss HMM ensemble scoring methods
- Break condition: If the random subsets are too small or too similar, the ensemble loses diversity and the scoring mechanism breaks down.

### Mechanism 2
- Claim: The HMM-e framework maintains interpretability while achieving performance comparable to deep learning models.
- Mechanism: Each HMM model has a fixed number of parameters (6,000 in this case) compared to thousands more in CNN models. The likelihood scores from each model serve as interpretable features representing sequence similarity to different behavior patterns.
- Core assumption: Fewer parameters with interpretable state transitions can capture essential behavioral patterns as effectively as black-box deep learning models.
- Evidence anchors:
  - [abstract] "which are lightweight, interpretable, and efficient"
  - [section] "Notably, we achieve this performance with traditional machine learning techniques, simpler models, and fewer features"
  - [corpus] Weak evidence - corpus papers focus on sequence modeling but don't discuss interpretability of HMM ensembles
- Break condition: When behavioral patterns become too complex or non-linear for the HMM state structure to capture effectively.

### Mechanism 3
- Claim: The framework handles class imbalance by training separate ensembles for each class on random subsets.
- Mechanism: By training multiple HMMs on random subsets of each class, the ensemble captures diverse patterns within each class while avoiding overfitting to the majority class. The pairwise scoring naturally handles imbalance.
- Core assumption: Random subsampling ensures each HMM sees a representative sample of its class without being overwhelmed by majority class examples.
- Evidence anchors:
  - [abstract] "enhances performance in scenarios with imbalanced or scarce data"
  - [section] "ensuring diversity among the models by training each on a randomly selected subset of samples from the training data"
  - [corpus] Weak evidence - corpus papers mention class imbalance but don't discuss HMM ensemble approaches
- Break condition: If the minority class has too few samples, random subsets may not capture sufficient diversity for effective modeling.

## Foundational Learning

- Concept: Hidden Markov Models
  - Why needed here: HMMs provide the probabilistic framework for modeling sequential behavioral data with latent states
  - Quick check question: What are the three key components of an HMM that need to be estimated during training?

- Concept: Ensemble learning
  - Why needed here: Multiple HMMs trained on different data subsets create a more robust model that captures diverse behavioral patterns
  - Quick check question: How does random subsampling of training data contribute to ensemble diversity?

- Concept: Sequence construction from fragmented data
  - Why needed here: Raw behavioral data needs to be organized into coherent sequences that reflect decision-making processes
  - Quick check question: What criteria determine where one sequence ends and another begins in behavioral data?

## Architecture Onboarding

- Component map: Data preprocessing -> Sequence construction -> HMM ensemble training -> Scoring -> Classification/Clustering
- Critical path: The scoring mechanism is critical - without proper pairwise likelihood comparisons, the ensemble loses its ability to handle variable sequence lengths
- Design tradeoffs: Simplicity and interpretability vs. potential performance limitations on highly complex behavioral patterns
- Failure signatures: Degenerate models (stuck in single state), poor performance on minority classes, failure to converge during training
- First 3 experiments:
  1. Train single HMM on balanced dataset to verify basic sequence modeling capability
  2. Train HMM ensemble on imbalanced dataset to test class handling
  3. Test scoring mechanism on sequences of varying lengths to verify normalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How sensitive is the HMM-e performance to the choice of ensemble size (N and M) and subset factor (s%) when scaling to datasets with different characteristics (e.g., larger sequence lengths, more classes)?
- Basis in paper: [inferred] The paper mentions that N = M for all settings and uses 250 models with a 1% subset factor, but does not explore how these hyperparameters affect performance across varying dataset properties.
- Why unresolved: The paper only tests a single configuration and does not provide sensitivity analysis or guidelines for hyperparameter tuning in diverse scenarios.
- What evidence would resolve it: Systematic experiments varying N, M, and s% across datasets with different sequence lengths, class imbalances, and feature dimensions, accompanied by performance trade-offs and recommendations.

### Open Question 2
- Question: Can the HMM-e framework be extended to handle multi-class behavior modeling beyond binary classification, and what modifications would be required?
- Basis in paper: [explicit] The paper focuses on binary sequence classification and does not discuss multi-class extensions or their feasibility.
- Why unresolved: The current formulation compares likelihoods pairwise between two classes, and scaling to multiple classes would require a different scoring mechanism or ensemble structure.
- What evidence would resolve it: Experiments demonstrating HMM-e performance on multi-class datasets, along with algorithmic adaptations (e.g., one-vs-rest, hierarchical ensembles) and comparative results against existing multi-class methods.

### Open Question 3
- Question: How does the HMM-e approach compare to deep learning models in terms of interpretability and explainability for behavior modeling?
- Basis in paper: [explicit] The paper highlights HMM-e's interpretability as a key advantage over deep learning models but does not provide quantitative or qualitative comparisons of interpretability.
- Why unresolved: While the paper claims HMM-e is more interpretable, it does not measure or demonstrate this through case studies, visualizations, or user studies.
- What evidence would resolve it: Comparative studies using interpretability metrics (e.g., feature importance, state transition analysis) and expert evaluations to assess the clarity and actionable insights provided by HMM-e versus deep learning models.

## Limitations
- Weak corpus evidence supporting the claimed mechanisms, with no external validation from broader literature
- Performance generalizability beyond the specific four-feature setup used in the GLOBEM dataset remains unproven
- Sensitivity to hyperparameter choices (ensemble size, subset sampling percentage) not thoroughly explored

## Confidence
- Ensemble scoring mechanism effectiveness: Medium
- Interpretability advantage claims: Medium
- Generalizability to other datasets: Low

## Next Checks
1. Test the ensemble scoring mechanism on synthetic sequences of known lengths and patterns to verify that the pairwise comparison truly normalizes for sequence length effects.

2. Conduct ablation studies varying the ensemble size and subset sampling percentage to determine the optimal balance between diversity and model stability.

3. Apply the framework to a different behavioral dataset with distinct feature characteristics to assess generalizability beyond the GLOBEM dataset's specific setup.