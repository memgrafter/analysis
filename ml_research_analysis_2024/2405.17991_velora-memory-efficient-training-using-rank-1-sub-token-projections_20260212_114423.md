---
ver: rpa2
title: 'VeLoRA: Memory Efficient Training using Rank-1 Sub-Token Projections'
arxiv_id: '2405.17991'
source_url: https://arxiv.org/abs/2405.17991
tags:
- memory
- velora
- training
- gradient
- activations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VeLoRA, a memory-efficient training method
  for large language models (LLMs) that compresses intermediate activations during
  forward pass and reconstructs them during backward pass. The core idea involves
  dividing tokens into smaller sub-tokens and projecting them onto a fixed 1-dimensional
  subspace using a rank-1 decomposition.
---

# VeLoRA: Memory Efficient Training using Rank-1 Sub-Token Projections

## Quick Facts
- arXiv ID: 2405.17991
- Source URL: https://arxiv.org/abs/2405.17991
- Reference count: 40
- Outperforms state-of-the-art methods like QLoRA and GaLore in memory efficiency while maintaining competitive accuracy

## Executive Summary
VeLoRA introduces a memory-efficient training method for large language models that compresses intermediate activations during forward pass and reconstructs them during backward pass using rank-1 sub-token projections. The approach divides tokens into smaller sub-tokens and projects them onto a fixed 1-dimensional subspace, achieving significant memory savings without degrading model performance. Experiments demonstrate that VeLoRA outperforms existing methods like QLoRA and GaLore while achieving competitive or better accuracy across vision, language, and LLM benchmarks.

## Method Summary
VeLoRA compresses intermediate activations during forward pass by dividing tokens into sub-tokens and projecting each onto a fixed 1-dimensional subspace using rank-1 decomposition. During backward pass, these compressed representations are reconstructed to compute gradients. The method integrates seamlessly with parameter-efficient fine-tuning techniques and achieves substantial memory savings by reducing the dimensionality of stored activations. The projection vector is initialized using first-batch statistics and remains fixed throughout training, making the compression computationally efficient while preserving local gradient similarity essential for training dynamics.

## Key Results
- Outperforms QLoRA and GaLore in memory efficiency on vision tasks (VTAB-1k)
- Maintains competitive accuracy on language benchmarks (GLUE, MMLU)
- Successfully integrates with LoRA and other parameter-efficient fine-tuning methods
- Achieves memory savings of up to 90% compared to standard training methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Intermediate activations can be compressed using rank-1 projections without degrading model performance.
- Mechanism: The method divides tokens into sub-tokens and projects each onto a fixed 1-dimensional subspace using a rank-1 decomposition. During forward pass, activations are compressed; during backward pass, they are reconstructed for gradient computation.
- Core assumption: The compressed representation preserves the essential training dynamics, including gradient similarity and sparsity patterns.
- Evidence anchors:
  - [abstract]: "intermediate activations used to implement backpropagation can be excessively compressed without incurring any degradation in performance"
  - [section]: "the intermediate activations produced during the forward propagation of deep neural networks...can be effectively represented and reconstructed from a single and fixed one-dimensional vector without losing any accuracy"
  - [corpus]: Weak - corpus neighbors focus on related LoRA and fine-tuning work but don't directly address the rank-1 projection mechanism
- Break condition: If the gradient similarity is not locally preserved, or if the sparse gradient patterns become detrimental to convergence.

### Mechanism 2
- Claim: Rank-1 projections encourage gradient sparsity which helps prevent overfitting.
- Mechanism: When sub-tokens are projected onto a fixed vector, gradients orthogonal to this vector are suppressed, creating sparse gradients that act as regularization.
- Core assumption: Sparser gradients during training lead to better generalization by reducing overfitting to fine-tuning data.
- Evidence anchors:
  - [section]: "the gradients become more sparse as they shift away from the initial distribution and this helps prevent the model from overfitting"
  - [abstract]: "these features are then coarsely reconstructed during the backward pass to implement the update rules"
  - [corpus]: Weak - corpus neighbors discuss LoRA and fine-tuning but don't specifically address gradient sparsity from rank-1 projections
- Break condition: If the sparsity becomes too extreme (sub-token size too small) and significantly degrades model performance.

### Mechanism 3
- Claim: The fixed vector projection preserves local gradient similarity even though global gradient properties are lost.
- Mechanism: Using a first-order approximation, the probability that gradient similarity diverges by at least k is bounded by a function of the angle distribution between sub-tokens and the projection vector.
- Core assumption: Local preservation of gradient similarity is sufficient to maintain training dynamics and model convergence.
- Evidence anchors:
  - [section]: "Although the vector projection destroys the original gradients' magnitudes and directions, it still locally preserves the gradient similarity"
  - [section]: Equation 3 showing the probability bound for gradient similarity divergence
  - [corpus]: Weak - corpus neighbors don't address the mathematical analysis of gradient similarity preservation
- Break condition: If the angle distribution between sub-tokens and projection vector has high variance, causing significant gradient similarity divergence.

## Foundational Learning

- Concept: Low-rank matrix approximation and SVD
  - Why needed here: Understanding why rank-1 projections work and how they differ from full SVD approaches like GaLore
  - Quick check question: What is the computational complexity difference between a rank-1 projection and a full SVD decomposition?

- Concept: Backpropagation and chain rule of differentiation
  - Why needed here: Understanding how intermediate activations are used in gradient computation during the backward pass
  - Quick check question: During backpropagation, which tensors need to be stored in memory for each layer to compute gradients?

- Concept: Parameter-efficient fine-tuning (PEFT) methods like LoRA
  - Why needed here: Understanding how VeLoRA complements existing PEFT methods and differs from them
  - Quick check question: How does LoRA modify the weight update rule compared to full fine-tuning?

## Architecture Onboarding

- Component map:
  Token grouping/reshaping operation -> Rank-1 projection -> Compression function -> Reconstruction function

- Critical path:
  1. Forward pass: Group tokens → Project sub-tokens → Store compressed representation
  2. Backward pass: Retrieve compressed representation → Reconstruct activations → Compute gradients

- Design tradeoffs:
  - Sub-token size (M): Smaller M increases memory savings but may degrade performance due to excessive sparsity
  - Fixed vs adaptive projection vector: Fixed vectors are computationally cheap but may not track gradient distribution shifts
  - Layer selection: Applying VeLoRA to different layers (query, key, value, down-projection) has different memory/performance impacts

- Failure signatures:
  - Training divergence: May indicate insufficient gradient information preservation
  - Significant accuracy degradation: May indicate sub-token size too small or poor projection vector initialization
  - Memory usage not reduced: May indicate implementation error in compression/reconstruction functions

- First 3 experiments:
  1. Ablation on sub-token size: Compare performance and memory usage for different values of M (D/64, D/32, D/16, D/8)
  2. Integration with LoRA: Apply VeLoRA to a LoRA-adapted model and measure combined memory savings and accuracy
  3. Layer sensitivity: Apply VeLoRA to different combinations of layers (value only, value+down-projection, all projections) and measure impact on memory and accuracy

## Open Questions the Paper Calls Out

- Question: How does VeLoRA perform on non-Transformer architectures like CNNs, RNNs, or SSMs?
  - Basis in paper: [inferred] from the statement "We performed all experiments on Transformer models" and "It remains unclear whether our methods can be extended to non-Transformer-based models"
  - Why unresolved: The paper explicitly states that all experiments were conducted on Transformer models and acknowledges uncertainty about extending the method to other architectures
  - What evidence would resolve it: Experiments applying VeLoRA to non-Transformer architectures and comparing performance against standard training methods

- Question: What is the optimal initialization strategy for the projection vector v in VeLoRA?
  - Basis in paper: [explicit] from the ablation study showing different initialization methods (random, SVD, fixed average, running average) with varying performance
  - Why unresolved: While the paper shows that fixed average initialization performs best among tested methods, it doesn't explore the full space of possible initialization strategies or provide theoretical justification for why this method works best
  - What evidence would resolve it: Theoretical analysis of optimal initialization strategies and systematic comparison of additional initialization methods

- Question: How does the trade-off between sub-token size M and model performance vary across different tasks and model architectures?
  - Basis in paper: [explicit] from the ablation study showing performance variation with different M values (D/64, D/32, D/16, D/8)
  - Why unresolved: The paper provides evidence of a sweet spot for M but doesn't fully characterize how this optimal value depends on task complexity, model size, or other architectural factors
  - What evidence would resolve it: Comprehensive study mapping optimal M values across a wide range of tasks and model architectures, potentially with theoretical bounds on performance degradation

## Limitations

- Theoretical guarantees remain unproven: The mechanism by which rank-1 compression maintains gradient flow in highly non-linear deep networks is not rigorously established
- Sub-token size sensitivity: The threshold where rank-1 compression becomes detrimental to training convergence remains unclear
- Integration with other memory-saving techniques: The interaction with quantization, gradient checkpointing, or optimizer state compression is unexplored

## Confidence

**High confidence** in memory efficiency claims: The memory savings are straightforward to measure and validate with substantial and consistent results across experiments.

**Medium confidence** in performance preservation: While the paper shows competitive accuracy across multiple benchmarks, the mechanisms explaining why rank-1 projections don't degrade performance are theoretical and rely on approximations.

**Medium confidence** in combined PEFT effectiveness: The paper demonstrates successful integration with LoRA and other PEFT methods, but systematic evaluation of different combinations and their tradeoffs is limited.

## Next Checks

1. **Gradient similarity preservation analysis**: Implement the exact gradient similarity measurement protocol used in the paper and verify the claim that "the probability that the similarity between the two gradients diverges by at least k is bounded by a function of the angle distribution between sub-tokens and the projection vector."

2. **Sub-token size sensitivity sweep**: Conduct a systematic ablation study varying M from D/8 down to D/128, measuring both memory savings and performance degradation to establish practical limits of rank-1 compression.

3. **Combined optimization evaluation**: Test VeLoRA integration with quantization (QLoRA-style) and gradient checkpointing simultaneously to measure whether memory savings compound additively or if there are diminishing returns or conflicts between these techniques.