---
ver: rpa2
title: '3-in-1: 2D Rotary Adaptation for Efficient Finetuning, Efficient Batching
  and Composability'
arxiv_id: '2409.00119'
source_url: https://arxiv.org/abs/2409.00119
tags:
- road
- tasks
- lora
- finetuning
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces RoAd, a 2D rotary adaptation method that efficiently
  fine-tunes large language models (LLMs) using a minimal number of trainable parameters.
  The key innovation is to rotate specific subspaces within the model's representations,
  primarily altering angular components rather than magnitudes.
---

# 3-in-1: 2D Rotary Adaptation for Efficient Finetuning, Efficient Batching and Composability

## Quick Facts
- **arXiv ID**: 2409.00119
- **Source URL**: https://arxiv.org/abs/2409.00119
- **Reference count**: 40
- **Primary result**: RoAd fine-tunes LLMs with <0.1% trainable parameters, outperforming other PEFT methods on GLUE and reasoning tasks

## Executive Summary
This paper introduces RoAd (Rotary Adaptation), a parameter-efficient fine-tuning method for large language models that leverages 2D rotations to adapt model representations. The key innovation is rotating specific subspaces within the model's representations, primarily altering angular components rather than magnitudes. RoAd addresses three challenges: parameter efficiency (using less than 0.1% trainable parameters), efficient batching (achieving twice the throughput of LoRA for heterogeneous requests), and interpretability (by modifying angular components). Experimental results show RoAd outperforms other PEFT methods on GLUE, commonsense reasoning, and arithmetic reasoning tasks while maintaining low computational overhead.

## Method Summary
RoAd employs a 2D rotary adaptation mechanism that rotates specific subspaces within the model's representations during fine-tuning. The method focuses on modifying angular components of the representation space rather than magnitudes, which enables parameter efficiency while maintaining model performance. The rotation-based adaptation allows for efficient batching of heterogeneous requests and demonstrates composability within distributed frameworks. The approach requires training only a minimal number of parameters (<0.1% of the total model parameters) while achieving competitive or superior performance compared to other parameter-efficient fine-tuning methods.

## Key Results
- RoAd outperforms other parameter-efficient fine-tuning methods on GLUE, commonsense reasoning, and arithmetic reasoning tasks
- Achieves twice the throughput of LoRA for batching heterogeneous requests
- Demonstrates composability by integrating within distributed interchange intervention frameworks for multitasking learning
- Uses less than 0.1% trainable parameters while maintaining competitive performance

## Why This Works (Mechanism)
RoAd works by rotating specific subspaces within the model's representation space, focusing on angular modifications rather than magnitude changes. This approach preserves the overall structure of the learned representations while enabling targeted adaptation. By limiting modifications to angular components, the method achieves parameter efficiency since fewer parameters need to be updated during fine-tuning. The rotational mechanism also enables better composability with other adaptation methods and frameworks, as rotations can be composed and distributed more easily than additive parameter modifications.

## Foundational Learning

### Rotary Position Embeddings (RPE)
**Why needed**: RPE encodes relative positional information in sequences by rotating the query and key vectors in the attention mechanism.
**Quick check**: Verify that RoAd builds upon the mathematical framework of RPE rather than replacing it entirely.

### Parameter-Efficient Fine-Tuning (PEFT)
**Why needed**: Traditional fine-tuning of LLMs is computationally expensive and risks catastrophic forgetting.
**Quick check**: Confirm that RoAd's parameter efficiency claims are benchmarked against established PEFT methods like LoRA, prefix tuning, and adapters.

### Composability in Multi-Task Learning
**Why needed**: Modern LLM deployments often require handling multiple tasks simultaneously or sequentially.
**Quick check**: Validate that RoAd's composability claims extend beyond the specific distributed framework tested in the paper.

## Architecture Onboarding

### Component Map
Input -> Rotary Adaptation Layer -> Modified Attention Mechanism -> Output

### Critical Path
The critical path involves the interaction between the rotary adaptation layer and the attention mechanism. The adaptation layer modifies the angular components of the query and key vectors before they enter the attention computation, which then influences the output representations.

### Design Tradeoffs
The primary tradeoff is between parameter efficiency and adaptation capacity. By focusing on angular modifications rather than magnitude changes, RoAd achieves high parameter efficiency but may have limited capacity to make large-magnitude adjustments to representations. This tradeoff appears acceptable for the tasks tested but may require evaluation for domains requiring more substantial representation changes.

### Failure Signatures
Potential failure modes include inadequate adaptation for tasks requiring large-magnitude changes in representations, reduced performance when rotating subspaces that are critical for task-specific features, and composability issues when integrating with incompatible adaptation frameworks.

### First Experiments
1. Benchmark RoAd against LoRA on a held-out GLUE task to verify the 2x throughput improvement claim
2. Test RoAd's parameter efficiency by varying the percentage of trainable parameters and measuring performance degradation
3. Evaluate composability by combining RoAd with another PEFT method on a multi-task learning benchmark

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implicit questions remain regarding the generalizability of the approach to different model architectures, the theoretical limits of angular-only adaptations, and the scalability of the composability claims to more complex multi-task scenarios.

## Limitations
- Evaluation primarily focuses on specific benchmark tasks, limiting generalizability to all LLM application domains
- Claims of "less than 0.1% trainable parameters" require careful scrutiny of baseline model size and architecture dependency
- Composability claims are demonstrated in limited scenarios and may face challenges when scaling to more complex multi-task settings
- Throughput comparison with LoRA assumes specific hardware configurations that may not reflect real-world deployment scenarios

## Confidence

| Claim Category | Confidence Level |
|---|---|
| Parameter Efficiency | Medium |
| Throughput Improvement | Medium |
| Composability | Low |

## Next Checks

1. Evaluate RoAd's performance across a broader range of model sizes (beyond the current 1B and 7B parameter models) to validate parameter efficiency claims at different scales.

2. Test composability claims in more complex multi-task learning scenarios with heterogeneous task distributions to verify practical applicability.

3. Benchmark throughput improvements across different hardware configurations and batch sizes to establish robustness of performance claims under varying deployment conditions.