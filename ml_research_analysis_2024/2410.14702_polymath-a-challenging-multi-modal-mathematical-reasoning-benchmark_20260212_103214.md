---
ver: rpa2
title: 'Polymath: A Challenging Multi-modal Mathematical Reasoning Benchmark'
arxiv_id: '2410.14702'
source_url: https://arxiv.org/abs/2410.14702
tags:
- arxiv
- reasoning
- preprint
- question
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: POLYMATH introduces a challenging multi-modal mathematical reasoning
  benchmark to evaluate the visual comprehension and abstract reasoning skills of
  multi-modal large language models (MLLMs). The benchmark includes 5,000 high-quality
  images across 10 distinct categories, such as pattern recognition, spatial reasoning,
  and relative reasoning, requiring both textual and visual problem-solving capabilities.
---

# Polymath: A Challenging Multi-modal Mathematical Reasoning Benchmark

## Quick Facts
- arXiv ID: 2410.14702
- Source URL: https://arxiv.org/abs/2410.14702
- Authors: Himanshu Gupta; Shreyas Verma; Ujjwala Anantheswaran; Kevin Scaria; Mihir Parmar; Swaroop Mishra; Chitta Baral
- Reference count: 40
- One-line primary result: Multi-modal LLMs achieve significantly lower performance than humans on a visual mathematical reasoning benchmark, with top models scoring ~41% compared to human baseline of 66.3%

## Executive Summary
POLYMATH is a multi-modal mathematical reasoning benchmark designed to evaluate the visual comprehension and abstract reasoning capabilities of MLLMs. The benchmark contains 5,000 high-quality images across 10 distinct categories, including pattern recognition, spatial reasoning, and relative reasoning. Evaluations on 15 MLLMs using diverse prompting strategies reveal that the best models achieve scores of ~41%, ~36%, and ~27%, respectively, significantly below human performance (66.3%). Error analysis indicates that models struggle with spatial relations and logical reasoning, with common failures including logical flaws (~60%) and spatial misunderstandings (~25%).

## Method Summary
The study evaluates 15 MLLMs on POLYMATH using four prompting strategies: zero-shot, few-shot, Chain-of-Thought, and Step-Back. The benchmark includes 5,000 images from Indian competitive exams, categorized into 10 reasoning types. Models are tested on both multi-modal (image + text) and text-only versions of questions. Performance is measured against human baselines (66.3%) and random chance. Error analysis categorizes failures into logical flaws, spatial misunderstandings, and other reasoning errors. An ablation study compares performance when diagrams are replaced with textual descriptions.

## Key Results
- Top models (Claude-3.5 Sonnet, GPT-4o, Gemini-1.5 Pro) achieve scores of ~41%, ~36%, and ~27% respectively
- Models perform 4-7% better on text-only versions compared to multi-modal versions
- Chain-of-Thought and Step-Back prompting strategies improve performance by 2-4% over zero-shot
- Common error types include logical flaws (~60%) and spatial misunderstandings (~25%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models fail to truly comprehend visual diagrams and spatial information, leading to logical errors
- Mechanism: When diagrams are replaced with textual descriptions, model performance improves by ~4-7%, indicating that models struggle with spatial reasoning and benefit from explicit verbal descriptions
- Core assumption: Visual comprehension is a distinct capability from textual reasoning, and models rely more on textual processing than visual understanding
- Evidence anchors:
  - [abstract] "A further fine-grained error analysis reveals that these models struggle to understand spatial relations and perform drawn-out, high-level reasoning. This is further strengthened by our ablation study estimating MLLM performance when given textual descriptions in place of diagrams. As evidenced by ~4% improvement over textual descriptions as opposed to actual images, we discover that models do not truly comprehend visual diagrams and the spatial information therein, and are thus prone to logical errors."
  - [section 4.3] "When we replaced the diagrams in test-img with text descriptions, the performance of all models improved by approximately ~3âˆ’4%, indicating that the models struggle with visualizing diagrams and benefit from textual representations."

### Mechanism 2
- Claim: Chain-of-Thought and Step-Back prompting strategies improve performance by 2-4% over zero-shot prompting
- Mechanism: These prompting strategies help models break down complex multi-modal reasoning tasks into smaller, more manageable steps, improving accuracy on categories requiring structured reasoning
- Core assumption: The improvement comes from better reasoning organization rather than from additional knowledge
- Evidence anchors:
  - [abstract] "We conducted a comprehensive, and quantitative evaluation of 15 MLLMs using four diverse prompting strategies, including Chain-of-Thought and Step-Back. The best scores achieved on POLYMATH are ~41%, ~36%, and ~27%, obtained by Claude-3.5 Sonnet, GPT-4o and Gemini-1.5 Pro respectively"
  - [section 4.2] "In terms of prompting strategies, Chain-of-Thought and Step Back Prompting enhanced the performance of top models like Claude-3.5 Sonnet and GPT-4o, allowing them to excel in tasks requiring structured reasoning and re-evaluation."

### Mechanism 3
- Claim: Models share similar reasoning patterns and make common logical errors, particularly in pattern recognition and spatial reasoning categories
- Mechanism: Error analysis shows that models frequently make the same types of mistakes, such as misunderstanding spatial relations (~25% of errors) and logical flaws (~60% of errors), indicating systematic reasoning deficiencies
- Core assumption: These shared errors reflect fundamental limitations in how models process multi-modal information rather than random failures
- Evidence anchors:
  - [abstract] "A further fine-grained error analysis reveals that these models struggle to understand spatial relations and perform drawn-out, high-level reasoning"
  - [section 4.3] "The most common error on this dataset was Logical Flaw (LF), occurring in nearly ~60% of incorrect samples. Spatial Misunderstanding (SM), which involves a lack of understanding of diagram structure and content, was a close second (~25%)."
  - [section 4.3] "Additionally, in questions involving extrapolation over multiple weakly connected data points, models came to conclusions that contradicted earlier data, pointing to a lack of information retention."

## Foundational Learning

- Concept: Spatial reasoning and visualization
  - Why needed here: The benchmark specifically tests spatial reasoning abilities, and models struggle significantly with this aspect, showing ~25% of errors are spatial misunderstandings
  - Quick check question: Can you describe how a 3D object would look when rotated 90 degrees around the Y-axis?

- Concept: Pattern recognition and logical extrapolation
  - Why needed here: Pattern recognition is one of the 10 categories tested, and models show systematic errors in identifying and extending patterns correctly
  - Quick check question: Given the sequence 2, 4, 8, 16, what would be the next three numbers and why?

- Concept: Multi-modal information integration
  - Why needed here: The benchmark requires combining textual and visual information, and models perform significantly better with text-only versions of problems
  - Quick check question: How would you solve a math problem if you had both the diagram and a detailed textual description of that diagram?

## Architecture Onboarding

- Component map: Data collection pipeline -> Question categorization -> Model evaluation -> Error analysis -> Performance comparison
- Critical path: Curate high-quality questions -> Categorize by reasoning type -> Evaluate multiple models with different prompting strategies -> Analyze error patterns -> Compare to human baseline
- Design tradeoffs: Manual curation ensures quality but limits scalability; ablation study provides insights but requires additional resources
- Failure signatures: Low performance on spatial reasoning (~4% drop when using diagrams vs text); consistent logical errors across models; poor extrapolation in pattern recognition
- First 3 experiments:
  1. Replicate the ablation study by converting diagrams to text descriptions and measuring performance changes
  2. Test different prompting strategies on a subset of questions to validate the 2-4% improvement claim
  3. Conduct error analysis on a small sample to verify the distribution of logical vs spatial errors

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of multi-modal models vary when handling abstract diagrams versus real-world visual contexts in mathematical reasoning tasks?
- Basis in paper: [inferred] The paper focuses on abstract visual reasoning tasks but does not explicitly compare performance across different types of visual contexts.
- Why unresolved: The paper does not provide a detailed breakdown of model performance based on the nature of the visual context (abstract vs. real-world), leaving a gap in understanding how model capabilities differ across these scenarios.
- What evidence would resolve it: A systematic evaluation of model performance on abstract diagrams versus real-world visual contexts within the same mathematical reasoning tasks, with comparative analysis of accuracy and error types.

### Open Question 2
- Question: To what extent do advanced prompting strategies like Chain-of-Thought and Step-Back improve model performance on spatial reasoning tasks compared to zero-shot approaches?
- Basis in paper: [explicit] The paper mentions that Chain-of-Thought and Step-Back prompting enhance performance, but it does not quantify the specific impact on spatial reasoning tasks.
- Why unresolved: While the paper highlights improvements in overall performance, it does not isolate the effects of these strategies on spatial reasoning tasks, which are particularly challenging for models.
- What evidence would resolve it: A detailed comparison of model performance on spatial reasoning tasks using zero-shot, Chain-of-Thought, and Step-Back prompting, with statistical analysis of the improvements.

### Open Question 3
- Question: What are the specific limitations of current models in understanding spatial relations, and how can these be addressed through architectural or training improvements?
- Basis in paper: [explicit] The paper identifies spatial misunderstanding as a common error type, indicating a gap in models' ability to comprehend spatial information.
- Why unresolved: The paper highlights the issue but does not propose specific solutions or architectural changes to address spatial reasoning limitations.
- What evidence would resolve it: Experimental results demonstrating the impact of architectural modifications or training techniques (e.g., spatial reasoning-specific fine-tuning) on model performance in spatial reasoning tasks.

### Open Question 4
- Question: How does the performance of text-only LLMs compare to MLLMs when given detailed textual descriptions of diagrams, and what does this reveal about the necessity of visual inputs for reasoning tasks?
- Basis in paper: [explicit] The paper shows that text-only LLMs perform significantly worse than MLLMs on diagram-based tasks, even with detailed descriptions.
- Why unresolved: While the paper highlights the advantage of MLLMs, it does not explore the specific factors that contribute to this difference or whether text-only models can be improved to match MLLM performance.
- What evidence would resolve it: Comparative studies evaluating the impact of different levels of textual detail on model performance, and experiments testing the effectiveness of training text-only models on visual reasoning tasks.

## Limitations

- The study relies on a manually curated dataset from Indian competitive exams, which may introduce cultural and educational biases
- The error analysis methodology depends on human evaluators who may have subjective interpretations of reasoning failures
- The performance improvements from prompting strategies (2-4%) are relatively modest and may not translate to real-world applications

## Confidence

- High confidence: The finding that models achieve significantly lower performance than humans (41% vs 66.3%) and the core observation that spatial reasoning remains a major weakness
- Medium confidence: The specific error distribution percentages (~60% logical flaws, ~25% spatial misunderstandings) and the exact magnitude of performance improvement from text-only ablation studies
- Medium confidence: The effectiveness of Chain-of-Thought and Step-Back prompting strategies, as the improvement is consistent but relatively small

## Next Checks

1. Replicate the ablation study by converting 100 sample diagrams to textual descriptions and measuring the exact performance delta across multiple models to verify the 4-7% improvement claim
2. Conduct inter-rater reliability testing on error analysis by having multiple evaluators independently categorize reasoning errors in 50 sample responses to assess consistency
3. Test model performance on a culturally diverse subset of mathematical reasoning problems to evaluate whether the observed limitations are universal or specific to the Indian exam context