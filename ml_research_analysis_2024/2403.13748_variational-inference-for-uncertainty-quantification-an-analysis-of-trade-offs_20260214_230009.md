---
ver: rpa2
title: 'Variational Inference for Uncertainty Quantification: an Analysis of Trade-offs'
arxiv_id: '2403.13748'
source_url: https://arxiv.org/abs/2403.13748
tags:
- divergence
- divergences
- fg-vi
- diagonal
- when
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes the behavior of variational inference (VI)
  when approximating non-factorized distributions with factorized approximations,
  particularly focusing on Gaussian targets with full covariance matrices. The key
  insight is an impossibility theorem: when the target distribution has non-diagonal
  covariance, a factorized approximation can correctly match at most one of three
  uncertainty measures: marginal variances, marginal precisions, or entropy.'
---

# Variational Inference for Uncertainty Quantification: an Analysis of Trade-offs

## Quick Facts
- arXiv ID: 2403.13748
- Source URL: https://arxiv.org/abs/2403.13748
- Reference count: 10
- Key outcome: When approximating non-factorized Gaussian distributions with factorized approximations, at most one of three uncertainty measures (marginal variances, marginal precisions, or entropy) can be correctly estimated

## Executive Summary
This paper analyzes the fundamental trade-offs in variational inference when using factorized approximations to estimate uncertainty in correlated target distributions. The authors prove an impossibility theorem showing that when the target has non-diagonal covariance, a factorized approximation can correctly match at most one of three uncertainty measures: marginal variances, marginal precisions, or entropy. The paper characterizes how different divergences (KL, Rényi, and score-based) used in VI affect these uncertainty estimates, deriving that reverse KL matches precisions, forward KL matches variances, and a unique Rényi divergence parameter matches entropy. Empirical results show this ordering holds for non-Gaussian targets in terms of entropy estimates, though the variance ordering may fail in high dimensions.

## Method Summary
The authors analyze variational inference by comparing different divergences used to approximate non-factorized Gaussian targets with factorized Gaussian approximations. They derive fixed-point equations for the variational parameters under different divergences, proving that KL(q||p) matches marginal precisions, KL(p||q) matches marginal variances, and there exists a unique Rényi divergence parameter that matches entropy. The empirical evaluation uses five benchmark models from inference gym (Rosenbrock, Eight Schools, German Credit, Radon Effect, Stochastic Volatility), comparing entropy estimates across different divergences using the Batch-and-Match algorithm for score-based divergences, ELBO for KL(q||p), MCMC for KL(p||q), and Monte Carlo for Rényi divergences.

## Key Results
- Impossibility theorem: Factorized approximations can correctly match at most one of three uncertainty measures (variances, precisions, or entropy) when the target has non-diagonal covariance
- Reverse KL divergence matches marginal precisions while forward KL matches marginal variances
- A unique Rényi divergence parameter exists that matches the entropy of the target distribution
- An ordering of divergences exists based on their variance estimates, which extends to precision and entropy orderings
- Empirical results validate the entropy ordering for non-Gaussian targets, though variance ordering may fail in high dimensions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The choice of divergence in variational inference determines which uncertainty measure is correctly estimated.
- Mechanism: Different divergences lead to different fixed-point equations for the variational parameters. KL(q||p) matches precisions, KL(p||q) matches variances, and there exists a unique α for which Rα(p||q) matches entropy.
- Core assumption: The target distribution p is Gaussian with non-diagonal covariance matrix, and the variational family Q consists of factorized Gaussians.
- Evidence anchors:
  - [abstract]: "The key insight is an impossibility theorem: when the target distribution has non-diagonal covariance, a factorized approximation can correctly match at most one of three uncertainty measures: marginal variances, marginal precisions, or entropy."
  - [section 3.1]: "we review certain key results in the notation of this paper... it has been widely observed that VI based on the reverse KL divergence tends to underestimate uncertainty... we can now refine and qualify this statement in the setting of FG-VI"
  - [corpus]: Weak evidence - the corpus papers discuss related uncertainty quantification topics but don't specifically address the impossibility theorem or the ordering of divergences.

### Mechanism 2
- Claim: Score-based divergences can lead to variational collapse when estimating factorized Gaussian approximations.
- Mechanism: The score-based divergences measure the discrepancy between ∇log p and ∇log q using a weighted norm. When minimizing these divergences, the variational parameters may converge to boundary values (zero or infinite variances), resulting in improper distributions.
- Core assumption: The target distribution p is Gaussian with non-diagonal covariance matrix, and the variational family Q consists of factorized Gaussians.
- Evidence anchors:
  - [section 3.3]: "We conclude this section by noting a peculiar property of the score-based divergences in Table 1. It is possible for the solutions of the NQPs in Propositions 3.4 and 3.5 to lie on the boundary of the nonnegative orthant."
  - [section 3.3]: "Such solutions never arise with FG-VI based on the KL or Rényi divergences for α ∈ {0, 1}. This phenomenon gives rise to the following definition... FG-VI undergoes variational collapse when arginf q∈Q D(q, p) ̸∈ Q."
  - [corpus]: Weak evidence - the corpus papers discuss related uncertainty quantification topics but don't specifically address the issue of variational collapse with score-based divergences.

### Mechanism 3
- Claim: The ordering of divergences based on the estimated marginal variances also induces an ordering based on the estimated marginal precisions and entropies.
- Mechanism: The divergences can be ordered such that S(q||p) ≺ KL(q||p) ≺ Rα1(p||q) ≺ Rα2(p||q) ≺ KL(p||q) ≺ S(p||q) for any 0 < α1 < α2 < 1. This ordering holds for Gaussian targets and approximately for non-Gaussian targets in terms of entropy estimates.
- Core assumption: The target distribution p is Gaussian with non-diagonal covariance matrix, and the variational family Q consists of factorized Gaussians.
- Evidence anchors:
  - [section 4]: "We have already proven one of these orderings... KL(q||p) ≺ KL(p||q). Next we prove the two outermost orderings in Theorem 4.3; specifically, we show that S(q||p) ≺ KL(q||p) and KL(p||q) ≺ S(p||q)."
  - [section 5]: "Does the ordering of divergences hold when FG-VI is applied to non-Gaussian targets? We study this question empirically on a range of models... The ordering of divergences that we proved for Gaussian targets is not guaranteed to hold for non-Gaussian targets, and empirically we observe that it is more likely to fail in high-dimensional problems."
  - [corpus]: Weak evidence - the corpus papers discuss related uncertainty quantification topics but don't specifically address the ordering of divergences or its extension to non-Gaussian targets.

## Foundational Learning

- Concept: Variational Inference (VI)
  - Why needed here: VI is the core technique being analyzed in the paper, and understanding its mechanics is essential for grasping the impossibility theorem and the ordering of divergences.
  - Quick check question: What is the goal of VI, and how does it differ from other approximation methods like MCMC?

- Concept: Kullback-Leibler (KL) Divergence
  - Why needed here: KL divergence is the most commonly used divergence in VI, and the paper extensively analyzes its properties and compares it with other divergences.
  - Quick check question: What are the differences between the forward and reverse KL divergences, and how do they affect the resulting variational approximation?

- Concept: Rényi Divergence
  - Why needed here: Rényi divergence is a generalization of KL divergence that interpolates between the forward and reverse KL divergences, and the paper analyzes its properties in the context of VI.
  - Quick check question: How does the order α of the Rényi divergence affect the resulting variational approximation, and what is the significance of the unique α that matches the entropy?

## Architecture Onboarding

- Component map:
  - Target distribution p (Gaussian with non-diagonal covariance) -> Variational family Q (factorized Gaussians) -> Divergences D (KL, Rényi, score-based) -> Variational parameters (mean and covariance) -> Optimization algorithm (fixed-point equations, NQP, or gradient-based methods)

- Critical path:
  1. Define the target distribution p and the variational family Q
  2. Choose a divergence D to minimize
  3. Derive the fixed-point equations or optimization problem for the variational parameters
  4. Solve the optimization problem to obtain the optimal variational approximation
  5. Evaluate the quality of the approximation in terms of the desired uncertainty measure

- Design tradeoffs:
  - Choosing a richer variational family Q may improve the quality of the approximation but increase computational complexity
  - Different divergences D may lead to different approximations, each with its own strengths and weaknesses
  - The choice of optimization algorithm may affect the convergence speed and numerical stability of the solution

- Failure signatures:
  - Variational collapse (estimating zero or infinite variances)
  - Poor approximation of the desired uncertainty measure (variance, precision, or entropy)
  - Slow or unstable convergence of the optimization algorithm
  - Numerical issues (e.g., singular covariance matrices, overflow/underflow)

- First 3 experiments:
  1. Implement FG-VI with KL(q||p) divergence on a 2D Gaussian target with varying degrees of correlation. Plot the estimated marginal variances and compare with the true values.
  2. Implement FG-VI with Rα(p||q) divergence for different values of α on the same 2D Gaussian target. Find the unique α that matches the entropy and compare with the KL(q||p) and KL(p||q) approximations.
  3. Implement FG-VI with score-based divergences on a 3D Gaussian target with varying degrees of correlation. Identify cases of variational collapse and analyze the conditions under which they occur.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the ordering of divergences change when the target distribution p is not Gaussian but belongs to other parametric families (e.g., exponential family distributions)?
- Basis in paper: [explicit] The paper acknowledges that the ordering of divergences was proven for Gaussian targets but asks whether it holds for non-Gaussian distributions, noting that "the ordering in Theorem 4.3 does correspond—across all of the non-Gaussian targets in our experiments—to the ordering of entropies estimated by FG-VI."
- Why unresolved: The experiments only tested a limited set of non-Gaussian distributions (Rosenbrock, Eight Schools, German Credit, Radon Effect, Stochastic Volatility), and the paper notes that "the ordering of divergences that we proved for Gaussian targets is not guaranteed to hold for non-Gaussian targets."
- What evidence would resolve it: Systematic experiments comparing all divergences across various parametric families (exponential families, heavy-tailed distributions, multimodal distributions) would clarify whether the ordering generalizes beyond Gaussians.

### Open Question 2
- Question: Can the impossibility theorem be extended to richer variational families beyond factorized distributions, such as Gaussian distributions with full covariance matrices or other structured approximations?
- Basis in paper: [explicit] The paper states "The intuitions developed from this analysis suggest useful directions, conjectures, and insights for studying broader classes of distributions" and discusses how richer families "may be addressed, for instance, by Gaussian variational approximations with full covariance matrices."
- Why unresolved: The impossibility theorem specifically addresses factorized approximations, and while the paper mentions potential extensions, it does not provide any formal results or proofs for richer variational families.
- What evidence would resolve it: Proving impossibility theorems for richer variational families (full-covariance Gaussians, structured approximations) would show which uncertainty measures can be simultaneously matched as the approximation class becomes more expressive.

### Open Question 3
- Question: What is the optimal α value for entropy-matching in FG-VI for general non-Gaussian distributions, and how does it depend on the target's properties?
- Basis in paper: [explicit] The paper proves that "there exists a unique value α ∈ (0, 1) such that H(qα) = H(p)" for Gaussian targets, but notes that "there does not exist a single value of α for which H(qα) = H(p), even when p is Gaussian" and shows numerically that the entropy-matching α depends on dimension and correlation structure.
- Why unresolved: The paper only characterizes the entropy-matching α for Gaussian targets with specific covariance structures, and the experiments suggest this optimal value varies significantly with target properties.
- What evidence would resolve it: Developing a general theory for determining the entropy-matching α for non-Gaussian targets, potentially based on higher-order moments or other distributional properties, would provide practical guidance for choosing α in real applications.

## Limitations
- The impossibility theorem and divergence ordering are proven only for Gaussian targets with factorized variational approximations
- Empirical validation is limited to relatively low-dimensional problems (up to 27 dimensions)
- The study focuses exclusively on diagonal Gaussian variational approximations, excluding richer families
- The paper does not explore how the results extend to non-Gaussian target distributions beyond specific benchmark examples

## Confidence
- High confidence in the impossibility theorem and its proof for Gaussian targets - the mathematical derivation appears rigorous and the result is consistent with established properties of KL divergences
- Medium confidence in the ordering of divergences and their properties for non-Gaussian targets - while the theoretical framework extends naturally, empirical validation is limited to specific benchmark problems and may not generalize across all model families
- Low confidence in the practical implications of score-based divergences and variational collapse - these phenomena are theoretically interesting but may rarely manifest in practical applications where divergence minimization is regularized or constrained

## Next Checks
1. **High-dimensional scalability test**: Replicate the divergence ordering experiments on synthetic Gaussian targets with dimensions ranging from 50 to 1000, systematically varying the degree of posterior correlation to identify when the theoretical ordering breaks down

2. **Non-Gaussian robustness analysis**: Apply the same VI algorithms to heavy-tailed and multimodal target distributions (e.g., t-distributions, mixture models) to evaluate whether the impossibility theorem's insights extend beyond the Gaussian setting

3. **Alternative variational families**: Implement the same experiments using full-rank Gaussian variational approximations to determine whether the impossibility theorem's constraints can be circumvented by richer variational families, and at what computational cost