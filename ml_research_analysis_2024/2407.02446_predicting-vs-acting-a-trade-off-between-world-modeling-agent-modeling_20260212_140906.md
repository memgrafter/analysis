---
ver: rpa2
title: 'Predicting vs. Acting: A Trade-off Between World Modeling & Agent Modeling'
arxiv_id: '2407.02446'
source_url: https://arxiv.org/abs/2407.02446
tags:
- rlhf
- base
- generations
- agent
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a trade-off between language models' ability
  to act as world models (accurately predicting next tokens) versus agent models (generating
  coherent long-form text). The authors show that RLHF-aligned models, while excelling
  at interactive tasks, underperform base models at next-token prediction and concentrate
  probability on predictable spans across generations.
---

# Predicting vs. Acting: A Trade-off Between World Modeling & Agent Modeling

## Quick Facts
- **arXiv ID**: 2407.02446
- **Source URL**: https://arxiv.org/abs/2407.02446
- **Reference count**: 23
- **Primary result**: RLHF-aligned models struggle with next-token prediction while excelling at interactive tasks, revealing a fundamental trade-off between world modeling and agent modeling capabilities.

## Executive Summary
This paper identifies a fundamental trade-off between language models' ability to act as world models (accurately predicting next tokens) versus agent models (generating coherent long-form text). The authors demonstrate that RLHF-aligned models, while excelling at interactive tasks, underperform base models at next-token prediction and concentrate probability on predictable spans across generations. They introduce the concept of "anchor spans" - text sequences that appear consistently across multiple samples for the same prompt - which serve as implicit blueprints for generation. This concentration allows for better long-form coherency but limits the model's ability to generate diverse, unpredictable text. The authors argue this trade-off may be fundamental: successful agent models must ensure their future actions are predictable to their current state, while world models must maintain uncertainty about natural text.

## Method Summary
The authors evaluate perplexity degradation in RLHF models compared to their base counterparts across multiple datasets (C4, Arxiv, Wikipedia, BBC news, new Arxiv papers, Humpback, OASST1, Anthropic Harmless/Helpful corpora). They generate multiple continuations for the same prompts and apply sequence alignment (MAFFT) to identify anchor spans - substrings of 30+ characters occurring in ≥20% of generations. Linear probes are used on hidden states to measure self-prediction accuracy. The analysis compares cumulative probability mass on top tokens, vocabulary coverage, and ngram diversity between base and RLHF models.

## Key Results
- RLHF models consistently underperform base models on next-token prediction tasks across multiple datasets
- RLHF generations show significantly higher concentration of probability mass on predictable anchor spans
- Linear probes on RLHF hidden states can predict future tokens with higher accuracy than base models
- Anchor spans identified through sequence alignment remain constant across many RLHF outputs, serving as textual scaffolding

## Why This Works (Mechanism)

### Mechanism 1
RLHF models concentrate probability mass onto anchor spans that serve as implicit blueprints for generation. During RLHF training, the reward signal encourages generation of coherent, high-quality text, leading the model to identify and reuse certain text spans that consistently produce high rewards across different generations. These spans act as scaffolding, allowing the model to predict its own future output more reliably. The core assumption is that anchor spans are causally responsible for improved long-form coherence in RLHF models. Evidence includes qualitative examples showing anchor span consistency across generations, though systematic corpus-level analysis is lacking.

### Mechanism 2
RLHF models sacrifice world modeling capabilities to become better agent models. The RLHF training process optimizes for reward-based outcomes rather than accurate probability estimation, shifting the model's distribution toward predictable, high-reward regions of the text space. This improves its ability to act coherently but reduces its fidelity as a world model that can accurately predict arbitrary text. The assumption is that performance gaps stem from RLHF adaptation rather than architectural differences. Evidence shows perplexity degradation across multiple datasets, but causality isn't definitively established.

### Mechanism 3
RLHF models exhibit lower entropy in their hidden states when predicting their own generations compared to Base models. The RLHF training process creates models that are better at self-prediction because they generate text within a more constrained, predictable distribution. This is measurable through linear probes that can more accurately predict future tokens from RLHF hidden states than from Base model hidden states. The assumption is that improved self-prediction accuracy translates to better long-form generation capabilities. Evidence shows probe accuracy differences, but practical significance for generation quality remains unclear.

## Foundational Learning

- **Concept: Probability distribution concentration**
  - Why needed here: Understanding how RLHF models shift from smooth to peaked distributions is central to explaining the trade-off between world and agent modeling
  - Quick check question: If a model assigns 90% probability to a single token, what does this tell you about its uncertainty compared to a model that assigns 30% to each of three tokens?

- **Concept: Reinforcement Learning from Human Feedback (RLHF)**
  - Why needed here: The RLHF training process is the mechanism by which models transition from world models to agent models, making its mechanics essential to understanding the trade-off
  - Quick check question: In RLHF, what is the relationship between the reward signal and the KL penalty term, and how might this balance affect model behavior?

- **Concept: Sequence alignment algorithms**
  - Why needed here: The paper uses MAFFT to identify anchor spans by aligning multiple generations, making understanding this technique crucial for interpreting the results
  - Quick check question: How does sequence alignment handle insertions and deletions differently than simple string matching, and why is this important for identifying anchor spans?

## Architecture Onboarding

- **Component map**: Pretrained model → RLHF fine-tuning → Generation → Anchor span identification → Performance evaluation
- **Critical path**: Base model undergoes RLHF adaptation → generates multiple continuations → sequence alignment identifies anchor spans → empirical evaluation of world vs agent modeling trade-off
- **Design tradeoffs**: 
  - Reward strength vs. KL penalty: Higher reward strength may increase agent capability but exacerbate world modeling degradation
  - Sampling temperature: Lower temperatures may hide distribution collapse but also reduce diversity artificially
  - Sequence length in alignment: Longer sequences reveal more anchor span patterns but increase computational cost and alignment complexity
- **Failure signatures**:
  - Perplexity much higher than Base model on standard datasets
  - N-gram diversity metrics showing less variation in RLHF generations
  - Sequence alignment showing high overlap in RLHF outputs
  - Linear probes showing better self-prediction in RLHF hidden states
- **First 3 experiments**:
  1. Measure perplexity degradation: Compare Base vs RLHF model perplexity on C4, Arxiv, and Wikipedia test sets
  2. Quantify distribution collapse: Calculate cumulative probability mass on top-k tokens and vocabulary coverage for Base vs RLHF models
  3. Identify anchor spans: Generate 100 continuations for 10 diverse prompts, align using MAFFT, and identify spans appearing in >20% of outputs

## Open Questions the Paper Calls Out

### Open Question 1
Is the trade-off between world modeling and agent modeling fundamental, or can future alignment methods overcome this limitation? The authors argue this trade-off may be fundamental: "agent modeling may require minimizing long-term uncertainty while world modeling requires maintaining the true uncertainty of natural text documents." This needs further investigation across more models, methods, and scales. Testing multiple alignment methods (RLHF, DPO, constitutional AI, etc.) across various model sizes and domains would resolve whether the trade-off is fundamental.

### Open Question 2
What is the exact mechanism by which RLHF models create and use anchor spans for long-form generation? While the paper identifies anchor spans as implicit blueprints, it doesn't fully characterize the mechanism - whether these spans emerge from the reward function, gradient updates, or some other mechanism during RLHF training. Detailed analysis of RLHF training dynamics showing when and how anchor spans emerge would clarify the underlying process.

### Open Question 3
Can world models and agent models be effectively combined in a single system that leverages both capabilities? The authors suggest this possibility: "General systems covering both sets of abilities might combine agent and world models rather than relying on agent models to both act and predict." The paper doesn't explore hybrid architectures or switching mechanisms between world and agent models. Developing and evaluating hybrid systems where a world model component is used for planning and uncertainty estimation while an agent model handles goal-directed generation would demonstrate whether this approach works.

### Open Question 4
How does the degree of probability concentration in RLHF models relate to their performance on different types of tasks? While the paper shows RLHF models concentrate probability more than base models, it doesn't systematically study how this concentration level correlates with task performance across different domains or prompt types. Systematic experiments varying the degree of probability concentration and measuring task performance across diverse benchmarks would reveal whether concentration level should be task-dependent.

## Limitations
- The paper demonstrates correlation between anchor spans and improved generation coherence but lacks causal evidence through ablation studies
- The practical significance of distribution collapse for downstream task performance is not quantified
- The claim that anchor spans are causally responsible for improved long-form coherence requires more evidence than provided
- The broader assertion that there is a fundamental trade-off between world modeling and agent modeling capabilities needs more systematic validation

## Confidence

- **High Confidence**: The empirical observations that RLHF models underperform base models on next-token prediction tasks (perplexity degradation) and that RLHF generations show more ngram repetition are well-supported by the presented data
- **Medium Confidence**: The identification and characterization of anchor spans as a phenomenon is supported by the evidence, though more systematic analysis of their frequency and impact across different prompt types and domains would strengthen the claims
- **Low Confidence**: The claim that anchor spans are causally responsible for improved long-form coherence in RLHF models, and the broader assertion that there is a fundamental trade-off between world modeling and agent modeling capabilities, require more evidence than provided

## Next Checks

1. **Anchor Span Ablation Study**: Design an experiment where anchor spans are identified and then explicitly prevented from appearing in RLHF generations (through constrained sampling or editing). Compare the performance of these modified generations on long-form coherence tasks versus standard RLHF outputs.

2. **Distribution Recovery Experiment**: Take an RLHF model and apply additional training specifically to recover next-token prediction accuracy on standard datasets. Measure whether this recovery comes at the cost of generation coherence or whether the two capabilities can be decoupled.

3. **Cross-Domain Anchor Span Analysis**: Generate anchor span identification results across multiple diverse domains (technical, creative, conversational) and analyze whether certain types of content are more prone to anchor span formation, or whether anchor spans represent domain-independent scaffolding patterns.