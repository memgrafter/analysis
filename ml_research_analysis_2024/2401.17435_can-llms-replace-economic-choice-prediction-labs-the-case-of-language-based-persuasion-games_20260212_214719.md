---
ver: rpa2
title: Can LLMs Replace Economic Choice Prediction Labs? The Case of Language-based
  Persuasion Games
arxiv_id: '2401.17435'
source_url: https://arxiv.org/abs/2401.17435
tags:
- data
- human
- players
- prediction
- trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores whether large language models (LLMs) can effectively
  generate training data for predicting human decision-making in economic persuasion
  games involving natural language interactions. The authors conduct experiments where
  LLM-based agents simulate human decision-makers in a hotel-review-based persuasion
  game, and then train prediction models on this synthetic data to forecast human
  choices.
---

# Can LLMs Replace Economic Choice Prediction Labs? The Case of Language-based Persuasion Games

## Quick Facts
- arXiv ID: 2401.17435
- Source URL: https://arxiv.org/abs/2401.17435
- Reference count: 29
- Models trained on LLM-generated data can effectively predict human behavior in language-based persuasion games, often outperforming models trained on actual human data.

## Executive Summary
This paper investigates whether large language models can generate synthetic training data to predict human decision-making in economic persuasion games involving natural language interactions. The authors demonstrate that models trained on LLM-generated data can accurately predict human behavior, sometimes outperforming models trained on actual human data. The approach shows strong potential for replacing expensive human data collection with scalable LLM-based data generation while maintaining or improving prediction accuracy.

## Method Summary
The study uses human-bot interaction datasets where 210 human decision-makers interacted with six expert strategies in hotel-review-based persuasion games. LLM datasets were created by replicating this process with multiple LLMs (Qwen-2-72B, Gemini-1.5, Llama-3-70B, Llama-3-8B, Chat-Bison) simulating DMs with diverse personas. Prediction models (LSTM, Mamba, transformer, XGBoost) were trained on synthetic data, human data, or combinations thereof. The experiments explored fine-tuning LLMs on human data and evaluated performance using accuracy and Expected Calibration Error (ECE).

## Key Results
- Models trained on LLM-generated data can effectively predict human behavior in persuasion games
- Interaction history proves more predictive than linguistic sentiment alone for repeated interactions
- Fine-tuning LLMs on limited human data improves synthetic data quality and calibration
- Combining LLM-generated data with human data yields better calibration while maintaining accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated synthetic data can effectively replace human data for training human choice prediction models in language-based persuasion games.
- Mechanism: LLMs simulate strategic decision-making by conditioning choices on both linguistic signals and interaction history, capturing behavioral patterns similar to humans.
- Core assumption: LLMs can model the same history-dependent decision-making processes as humans when provided with sufficient context.
- Evidence anchors: Abstract shows models trained on LLM-generated data can predict human behavior; section describes LLM players conditioning choices on linguistic signals and interaction outcomes.

### Mechanism 2
- Claim: Fine-tuning LLMs on limited human data improves synthetic data quality for prediction tasks.
- Mechanism: Fine-tuning adapts the LLM's internal representations to human behavioral patterns, producing synthetic data that better reflects human decision-making dynamics.
- Core assumption: Fine-tuning preserves the LLM's ability to generate diverse strategic behaviors while aligning them with human-like patterns.
- Evidence anchors: Abstract mentions human data can be incorporated into training sets; section discusses fine-tuning transforming LLMs into effective data generators.

### Mechanism 3
- Claim: Interaction history is more predictive of human behavior than linguistic sentiment alone in repeated persuasion games.
- Mechanism: Humans adapt their decisions based on past interactions, making historical patterns more informative than isolated message content.
- Core assumption: Human decision-making in persuasion contexts is fundamentally strategic and history-dependent rather than purely sentiment-driven.
- Evidence anchors: Abstract highlights interaction history's key role; section shows models accounting for history outperformed those relying on short interaction horizons or purely linguistic signals.

## Foundational Learning

- Concept: Strategic decision-making in repeated interactions
  - Why needed here: Understanding how players adapt strategies based on history is crucial for modeling human behavior in persuasion games.
  - Quick check question: How would a player's decision change if an expert consistently sent positive reviews for low-quality hotels?

- Concept: Calibration in prediction models
  - Why needed here: Calibration ensures confidence estimates match actual outcome frequencies, critical for reliable decision-making systems.
  - Quick check question: What's the difference between a model with high accuracy but poor calibration versus one with both high accuracy and calibration?

- Concept: Fine-tuning for domain adaptation
  - Why needed here: Fine-tuning enables LLMs to generate domain-specific synthetic data that better matches human behavioral patterns.
  - Quick check question: What happens to an LLM's diversity of responses when fine-tuned on a small dataset?

## Architecture Onboarding

- Component map: Data generation -> Model training -> Prediction -> Evaluation pipeline with LLM fine-tuning as optional enhancement
- Critical path: LLM data generation -> Feature engineering -> Model training -> Accuracy and calibration evaluation
- Design tradeoffs: Large synthetic datasets vs. small human datasets for balancing accuracy and calibration
- Failure signatures: Poor calibration despite high accuracy, degradation when history context is limited
- First 3 experiments:
  1. Generate synthetic data with varying history context lengths and evaluate prediction accuracy
  2. Fine-tune LLM on human data and compare synthetic vs. real data prediction performance
  3. Test calibration differences between models trained on pure synthetic vs. mixed synthetic-human data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed approach of using LLM-generated data for human choice prediction generalize to economic settings beyond language-based persuasion games?
- Basis in paper: The authors mention that future research could explore the predictive power of LLM-generated data in other economic contexts.
- Why unresolved: The paper only demonstrates the effectiveness of the approach in language-based persuasion games, and the authors acknowledge the need for further exploration in other settings.
- What evidence would resolve it: Conducting experiments using LLM-generated data for human choice prediction in different economic settings, such as auctions, bargaining, or voting, and comparing the results to those obtained using human data.

### Open Question 2
- Question: How can we characterize the specific cases or expert strategies in which models trained on LLM-generated data fall short in predicting human behavior?
- Basis in paper: The authors state that a key question that remains unanswered is when and why the LLM-based approach is less effective.
- Why unresolved: While the paper provides a per-expert analysis, it does not fully characterize the limitations of the approach in specific scenarios.
- What evidence would resolve it: Conducting a more detailed analysis of the LLM-generated data and the resulting prediction models to identify patterns or features that are not well-captured by the LLMs, and comparing these to human decision-making processes.

### Open Question 3
- Question: What are the ethical implications of using LLM-generated data for human choice prediction, and how can we ensure responsible use of this technology?
- Basis in paper: The authors discuss the potential ethical implications of their work, including the possibility of malicious use of the technology.
- Why unresolved: The paper raises the issue of ethical considerations but does not provide specific guidelines or solutions for addressing these concerns.
- What evidence would resolve it: Developing and implementing ethical guidelines and regulations for the use of LLM-generated data in human choice prediction, as well as conducting studies on the potential impact of this technology on individuals and society.

## Limitations

- Data generation fidelity remains incompletely understood, with unclear mechanisms for how LLMs capture human strategic patterns
- Generalization beyond hotel-review-based persuasion games has not been empirically validated
- Fine-tuning tradeoffs between data quality and behavioral diversity preservation are not fully explored

## Confidence

- High confidence: Core finding that LLM-generated synthetic data can effectively train prediction models for human decision-making in persuasion games
- Medium confidence: Superiority of history-based features over linguistic sentiment for prediction
- Medium confidence: Calibration improvements from mixed synthetic-human datasets

## Next Checks

1. Cross-domain validation: Test the LLM data generation approach on fundamentally different strategic decision-making tasks (e.g., negotiation or resource allocation games) to verify generalizability beyond persuasion games.

2. Human-in-the-loop evaluation: Conduct user studies where humans interact with LLM-simulated agents and rate the authenticity of interactions, directly measuring how closely LLM behavior matches human strategic patterns.

3. Fine-tuning sensitivity analysis: Systematically vary the amount and diversity of human data used for fine-tuning to identify optimal tradeoffs between synthetic data quality and preservation of behavioral diversity.