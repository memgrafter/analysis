---
ver: rpa2
title: 'Thinking Before Speaking: A Role-playing Model with Mindset'
arxiv_id: '2409.13752'
source_url: https://arxiv.org/abs/2409.13752
tags:
- character
- llms
- dialogue
- knowledge
- role
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of making large language models
  (LLMs) convincingly portray specific roles in dialogues. The authors propose a "Thinking
  Before Speaking" (TBS) model that enhances role-playing by extending the training
  data with character-specific scenarios, historical dialogues, and pre-dialogue mindset
  generation.
---

# Thinking Before Speaking: A Role-playing Model with Mindset

## Quick Facts
- arXiv ID: 2409.13752
- Source URL: https://arxiv.org/abs/2409.13752
- Authors: Baohua Zhang; Yongyi Huang; Wenyao Cui; Huaping Zhang
- Reference count: 40
- Key outcome: TBS Llama3 achieves 6.81/7.0 overall score, outperforming baseline models across 10 metrics including contextual immersion, emotional resonance, and logical thinking

## Executive Summary
This paper addresses the challenge of making large language models convincingly portray specific roles in dialogues. The authors propose a "Thinking Before Speaking" (TBS) model that enhances role-playing by extending training data with character-specific scenarios, historical dialogues, and pre-dialogue mindset generation. The method involves generating character backgrounds, creating realistic scenarios, producing dialogues that mimic the character's tone, and incorporating character-specific thinking logic before each response. Additionally, the model is trained with "hallucination knowledge" - content beyond the character's knowledge - to improve its ability to refuse answering questions outside the character's scope.

The TBS model is evaluated using both traditional metrics (memorization, personality, hallucination, stability) and new metrics (contextual immersion, emotional resonance, language style, logical thinking, adaptability). Experimental results show that the TBS model outperforms baseline models like CharacterLLM and RoleLLM across all metrics, with TBS Llama3 achieving the highest overall score of 6.81 out of 7.0. Ablation studies confirm that the introduction of role thinking, foresight knowledge, and special prompts are crucial for the model's effectiveness in role-playing tasks.

## Method Summary
The TBS model extends training data through character-specific scenario generation, dialogue mimicry, and pre-dialogue mindset generation. The approach involves segmenting character life experiences, generating plausible scenarios, creating dialogues that mimic the character's tone, and pairing them with mindset generation to enrich the training data. The model is fine-tuned using LoRA on this constructed dataset with special prompts that guide character adoption, mindset thinking, and out-of-scope knowledge handling. The training procedure uses batch size 64, learning rate 5e-5, and 10 epochs.

## Key Results
- TBS Llama3 achieves highest overall score of 6.81/7.0 across all metrics
- TBS model outperforms baseline models (CharacterLLM, RoleLLM) in all 10 evaluation metrics
- Ablation studies show role thinking, foresight knowledge, and special prompts are critical for performance
- Model successfully learns to refuse out-of-scope questions through hallucination knowledge training
- Context immersion score of 1.88/2.0 and emotional resonance score of 1.86/2.0 achieved

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Thinking-before-speaking mindset generation improves role consistency by embedding character-specific thought logic before each dialogue response
- Mechanism: The model is prompted to first generate a "thinking" segment that reflects the character's internal reasoning, relationship context, and situational awareness before producing the actual spoken response
- Core assumption: Character responses are better aligned with their persona when preceded by explicit internal thought modeling that incorporates personality, relationships, and current scenario context
- Evidence anchors:
  - [abstract] "supplementing each pair of dialogue with the character's mindset" and "generates the mindset of the characters' pre-dialogue thinking for each pair of dialogues based on the character backgrounds and the character relationships"
  - [section] "we generated the mindset of the characters' pre-dialogue thinking for each pair of dialogues based on the character backgrounds and the character relationships"
- Break condition: If the thinking segment does not actually reflect the character's internal logic or is disconnected from the current dialogue context, the improvement in role consistency disappears

### Mechanism 2
- Claim: Including "hallucination knowledge" (content outside the character's knowledge) during fine-tuning teaches the model to refuse answering questions beyond the character's scope
- Mechanism: The model is trained on negative samples that include scenarios and questions the character would not know, with the expectation that it learns to identify and decline such queries
- Core assumption: Fine-tuning on out-of-scope examples will generalize to real cases where the character lacks relevant knowledge, improving refusal behavior
- Evidence anchors:
  - [abstract] "add few data points that include elements beyond the role's knowledge, and fine-tune the LLMs" and "avoid responses that fall outside the role's knowledge base"
  - [section] "we add few data points that include elements beyond the role's knowledge, and fine-tune the LLMs" and "we also set a few new technologies that the character is unfamiliar with as negative samples to make the learners refuse to answer"
- Break condition: If the model fails to generalize from the training examples to unseen out-of-scope questions, it may still hallucinate or incorrectly attempt to answer

### Mechanism 3
- Claim: Data expansion through scenario generation and dialogue mimicry allows the model to learn richer character representation beyond limited historical dialogue
- Mechanism: The approach segments character life experiences, generates plausible scenarios, creates dialogues that mimic the character's tone, and pairs them with mindset generation to enrich the training data
- Core assumption: Expanding the dialogue dataset with generated but contextually consistent scenarios improves the model's ability to emulate the character's knowledge, tone, and logic
- Evidence anchors:
  - [abstract] "extend the data based on the character's real-life scenarios and the historical dialogue" and "expanded the data by generating dialogues in the scenarios by mimicking the real dialogues of the characters"
  - [section] "we propose mimicking each character while expanding our dialogue dataset" and "Generate dialogues that the character might have with others in the current scenario based on their profile summaries, life experiences, and prescribed scenarios"
- Break condition: If generated scenarios or dialogues diverge significantly from the character's authentic behavior or historical context, the model may learn incorrect character representations

## Foundational Learning

- Concept: Fine-tuning with LoRA (Low-Rank Adaptation)
  - Why needed here: Allows efficient adaptation of large pre-trained LLMs to specific roles without full retraining, enabling task-specific knowledge injection while preserving base capabilities
  - Quick check question: What are the key hyperparameters for LoRA fine-tuning in this context (rank, alpha, learning rate, batch size)?

- Concept: Prompt engineering for role-play tasks
  - Why needed here: Special prompts guide the model to adopt character personas, incorporate mindset thinking, and handle out-of-scope knowledge appropriately
  - Quick check question: How does the prompt structure differ between standard instruction following and role-playing with mindset generation?

- Concept: Dataset construction for character representation
  - Why needed here: High-quality, contextually rich datasets that include character profiles, historical dialogues, scenarios, and mindset generation are essential for effective role-playing model training
  - Quick check question: What are the key components that must be included in each training sample to capture character authenticity?

## Architecture Onboarding

- Component map: Base LLM (GLM-4-9B-chat, Llama-2-7B, Llama-3-8B) → LoRA Adapter → Special Prompt Layer → Mindset Generation → Dialogue Output
- Critical path: Character profile → Scenario generation → Dialogue mimicry → Mindset generation → Training data construction → LoRA fine-tuning → Inference with special prompts
- Design tradeoffs: Richer character representation vs. computational cost; more extensive data generation vs. potential hallucination; mindset generation vs. response latency
- Failure signatures: Inconsistent character voice across responses; failure to refuse out-of-scope questions; poor adaptation to unexpected scenarios; loss of character knowledge over conversation turns
- First 3 experiments:
  1. Test mindset generation independently: Feed a character profile and scenario to the model, check if the thinking output is contextually appropriate and character-consistent
  2. Test hallucination knowledge integration: Evaluate model responses to out-of-scope questions with and without hallucination knowledge training
  3. Test data expansion impact: Compare model performance on role-playing tasks using original vs. expanded datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Thinking Before Speaking (TBS) model handle the trade-off between maintaining character authenticity and adapting to unexpected user inputs that fall outside the character's knowledge?
- Basis in paper: Explicit
- Why unresolved: The paper mentions that the TBS model incorporates "hallucination knowledge" to avoid answering questions beyond the character's knowledge, but it does not provide a detailed analysis of how the model balances authenticity with adaptability in real-time interactions
- What evidence would resolve it: A comprehensive evaluation of the model's responses to a diverse set of unexpected questions, comparing the authenticity and adaptability of TBS-generated responses against baseline models

### Open Question 2
- Question: What are the computational costs associated with the TBS model's approach of generating character-specific scenarios and mindsets, and how do these costs scale with the complexity of the character?
- Basis in paper: Inferred
- Why unresolved: While the paper describes the data construction process and the inclusion of character-specific scenarios and mindsets, it does not discuss the computational resources required for these tasks or how they scale with more complex characters
- What evidence would resolve it: A detailed analysis of the computational resources (e.g., time, memory) required for generating scenarios and mindsets for different characters, along with scalability assessments

### Open Question 3
- Question: How does the inclusion of character-specific thinking logic before each response impact the overall coherence and user engagement in multi-turn dialogues?
- Basis in paper: Explicit
- Why unresolved: The paper introduces the concept of character-specific thinking logic to enhance role-playing, but it does not provide empirical evidence on how this feature affects the coherence and engagement of dialogues over multiple turns
- What evidence would resolve it: User studies or engagement metrics comparing multi-turn dialogues generated by TBS models with and without the thinking logic component, assessing factors such as coherence, immersion, and user satisfaction

## Limitations
- Data construction relies on automated processes without human validation, raising concerns about quality control and potential bias amplification
- Novel evaluation metrics lack transparency in scoring methodology and inter-rater reliability measures
- Generalization claims are limited to tested characters without systematic testing on new, unseen characters or dynamic role changes

## Confidence

**High Confidence**: The core technical implementation of LoRA fine-tuning and special prompt engineering is well-established and the reported training procedures (batch size 64, learning rate 5e-5, 10 epochs) are standard practice. The baseline comparisons with CharacterLLM and RoleLLM are methodologically sound.

**Medium Confidence**: The improvements in traditional metrics (memorization, personality, hallucination, stability) are measurable and comparable to established benchmarks. The ablation study results showing the importance of role thinking, foresight knowledge, and special prompts are internally consistent.

**Low Confidence**: The novel evaluation metrics (contextual immersion, emotional resonance, language style, logical thinking, adaptability) lack methodological rigor. The claimed superiority in these dimensions (e.g., TBS Llama3 scoring 6.81/7.0 overall) cannot be independently verified due to insufficient detail about the scoring process and human evaluator training.

## Next Checks

1. **Human Evaluation Protocol Validation**: Conduct a double-blind study where human evaluators rate the same role-play conversations using the paper's metrics without knowing which model generated them. Compare inter-rater reliability and test whether the metrics can distinguish between human and AI role-players.

2. **Cross-Character Generalization Test**: Train the TBS model on a subset of characters (3-4), then evaluate its performance on completely new characters (3-4) not seen during training. Measure degradation in all six metrics to assess true generalization capability.

3. **Long-form Conversation Analysis**: Test the model in extended dialogues (10+ turns) with dynamic scenario changes to evaluate whether the mindset generation mechanism maintains character consistency over time and can adapt to unexpected conversational developments.