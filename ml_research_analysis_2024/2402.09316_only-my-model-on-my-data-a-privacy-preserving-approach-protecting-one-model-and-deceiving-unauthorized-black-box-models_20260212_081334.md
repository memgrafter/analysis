---
ver: rpa2
title: 'Only My Model On My Data: A Privacy Preserving Approach Protecting one Model
  and Deceiving Unauthorized Black-Box Models'
arxiv_id: '2402.09316'
source_url: https://arxiv.org/abs/2402.09316
tags:
- image
- images
- authorized
- accuracy
- unauthorized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a privacy-preserving method that protects
  an authorized model's inference accuracy while degrading unauthorized black-box
  models' performance, without making images unrecognizable to humans. The approach
  leverages feature map distortion by optimizing a multi-objective loss that combines
  the mean squared error of feature maps and the cross-entropy loss for the authorized
  model.
---

# Only My Model On My Data: A Privacy Preserving Approach Protecting one Model and Deceiving Unauthorized Black-Box Models

## Quick Facts
- arXiv ID: 2402.09316
- Source URL: https://arxiv.org/abs/2402.09316
- Authors: Weiheng Chai, Brian Testa, Huantao Ren, Asif Salekin, Senem Velipasalar
- Reference count: 18
- Primary result: Maintains authorized model accuracy (100%, 99.92%, 99.67%) while reducing unauthorized model accuracy to 11.97%, 6.63%, 55.51% on average

## Executive Summary
This paper introduces a privacy-preserving method that protects an authorized model's inference accuracy while degrading unauthorized black-box models' performance, without making images unrecognizable to humans. The approach leverages feature map distortion by optimizing a multi-objective loss that combines the mean squared error of feature maps and the cross-entropy loss for the authorized model. Evaluated across three datasets (ImageNet, Celeba-HQ, AffectNet) and six different models, the method successfully maintains authorized model accuracy while reducing unauthorized model accuracy to near-chance levels. The study also demonstrates cross-task transferability, reducing object detection mAP to 4.13% and ethnicity classification accuracy to 63.50%-74.39%.

## Method Summary
The method uses feature map distortion (FMD) to protect an authorized model while degrading unauthorized models. It optimizes a multi-objective loss combining MSE between internal feature maps of the authorized model and cross-entropy loss for maintaining authorized model accuracy. The approach iteratively perturbs input images within a bounded budget constraint using gradient sign method, ensuring generated images remain visually similar to originals while disrupting feature extraction pathways used by unauthorized models.

## Key Results
- Maintains authorized model accuracy: 100% (ImageNet), 99.92% (Celeba-HQ), 99.67% (AffectNet)
- Reduces unauthorized model accuracy: 11.97% (ImageNet), 6.63% (Celeba-HQ), 55.51% (AffectNet) on average
- Cross-task transferability: Object detection mAP drops to 4.13%, ethnicity classification accuracy to 63.50%-74.39%
- Effectiveness increases with task complexity (more classes)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feature map distortion in early layers transfers adversarial effects to other models' later layers
- Mechanism: Optimizing MSE loss between internal feature maps of the authorized model disrupts shared low-level feature extraction pathways
- Core assumption: Early convolutional layers in different models learn similar low-level features (edges, textures, etc.)
- Evidence anchors:
  - [abstract] "early layers within a neural network perform extraction of features, which are then used for inference in later layers"
  - [section] "disruptions to the feature representation would transfer to the feature extraction layers of other networks"
- Break condition: If target models use fundamentally different feature extraction architectures or have been trained on completely different data distributions

### Mechanism 2
- Claim: Cross-entropy loss maintains authorized model accuracy while CE loss prevents complete feature destruction
- Mechanism: CE loss preserves at least one valid input-to-output path for the authorized model while MSE disrupts others
- Core assumption: Deep learning models have multiple input-to-output paths, and preserving one path is sufficient for maintaining accuracy
- Evidence anchors:
  - [section] "the use of CE loss ensures the retention of at least one pathway from input to output for the authorized model"
  - [section] "J = MSE(ff eat(x), ff eat(x∗)) − λCE(flogits(x∗), ygt)"
- Break condition: If the authorized model's decision boundary is too sensitive to feature perturbations

### Mechanism 3
- Claim: Task complexity correlates with attack effectiveness
- Mechanism: Higher class counts create more complex decision boundaries that are easier to disrupt
- Core assumption: More classes require more nuanced feature representations that are more vulnerable to distortion
- Evidence anchors:
  - [section] "Our hypothesis is further verified in Sec. 8.4"
  - [section] "the effectiveness of our approach increases as the complexity of the task increases"
- Break condition: If target models use decision boundaries that are inherently more robust to feature-level perturbations

## Foundational Learning

- Concept: Feature maps and convolutional layers
  - Why needed here: Understanding how feature maps represent image content is crucial for grasping why MSE loss between them disrupts model performance
  - Quick check question: What information do early convolutional layers typically extract from images?

- Concept: Transferability of adversarial examples
- Why needed here: The method relies on perturbations that generalize across different model architectures
- Quick check question: What conditions typically enable adversarial examples to transfer between models?

- Concept: Multi-objective optimization
- Why needed here: The method balances protecting one model while degrading others through a combined loss function
- Quick check question: How does adjusting the λ parameter affect the balance between protection and attack?

## Architecture Onboarding

- Component map: Authorized model -> Image preprocessing with feature map distortion -> Multi-objective loss function (MSE + CE) -> Perturbed image generation

- Critical path:
  1. Load authorized model and freeze weights
  2. Initialize perturbation within ϵ budget
  3. Compute MSE between original and perturbed feature maps
  4. Compute CE loss for authorized model
  5. Combine losses with λ weight
  6. Update image using gradient sign method
  7. Clip image to stay within bounds
  8. Repeat for N iterations

- Design tradeoffs:
  - Larger λ improves authorized model accuracy but reduces attack effectiveness
  - More iterations increase effectiveness but computational cost
  - Larger ϵ budget allows more aggressive attacks but may degrade image quality
  - Earlier feature map layers provide better transferability but may require more iterations

- Failure signatures:
  - Authorized model accuracy drops below acceptable threshold
  - Unauthorized model accuracy remains high
  - Generated images become visually unrecognizable
  - Performance degrades significantly with cross-task attacks

- First 3 experiments:
  1. Verify authorized model maintains 100% accuracy on ImageNet with λ=1, N=100, ϵ=16
  2. Test attack effectiveness on ResNet18 with VGG16 as authorized model
  3. Evaluate cross-task transferability on Faster-RCNN with ImageNet classification model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method's effectiveness vary across different task complexities (number of classes) and what is the underlying reason for this variation?
- Basis in paper: [explicit] The paper explicitly states that effectiveness increases with task complexity and provides evidence from experiments on datasets with varying numbers of classes (ImageNet: 1000 classes, Celeba-HQ: 307 classes, AffectNet: 8 classes).
- Why unresolved: While the paper demonstrates a correlation between task complexity and effectiveness, it does not provide a detailed explanation of the underlying reasons for this relationship or explore how it might scale to tasks with even more or fewer classes.
- What evidence would resolve it: Conducting experiments on a wider range of tasks with varying numbers of classes, including tasks with hundreds or thousands of classes, and analyzing the feature maps and model behavior to understand the underlying reasons for the observed correlation.

### Open Question 2
- Question: Can the proposed method be adapted to protect models across different modalities (e.g., text, audio) and what challenges might arise in such adaptations?
- Basis in paper: [inferred] The paper focuses on image-based tasks and does not explore the applicability of the method to other modalities. However, the general concept of feature map distortion could potentially be extended to other types of data.
- Why unresolved: The paper does not address the challenges and considerations involved in adapting the method to other modalities, such as the differences in feature extraction and representation learning between modalities.
- What evidence would resolve it: Implementing and evaluating the method on tasks involving different modalities, such as text classification or audio processing, and analyzing the effectiveness and limitations of the adapted approach.

### Open Question 3
- Question: How does the proposed method perform against more sophisticated attacks, such as adaptive attacks that specifically target the feature map distortion approach?
- Basis in paper: [inferred] The paper evaluates the method against black-box attacks but does not consider adaptive attacks that might exploit the specific characteristics of the proposed approach.
- Why unresolved: The paper does not explore the potential vulnerabilities of the method to adaptive attacks and how they might be mitigated.
- What evidence would resolve it: Conducting experiments with adaptive attacks that are designed to specifically target the feature map distortion approach and analyzing the robustness of the method against such attacks.

## Limitations
- Effectiveness significantly degrades in cross-task scenarios (object detection mAP drops to 4.13%)
- Limited evaluation against ensemble models or models with different architectures
- Assumes white-box access to only one model while treating others as black-box

## Confidence

**High confidence**: Authorized model accuracy preservation (100%, 99.92%, 99.67% on respective datasets)

**Medium confidence**: Attack effectiveness on single-task classification (11.97%, 6.63%, 55.51% degradation)

**Low confidence**: Cross-task transferability performance (4.13% mAP for object detection, 63.50%-74.39% for ethnicity classification)

## Next Checks
1. Test the approach against ensemble models combining multiple architectures to assess robustness
2. Evaluate performance when unauthorized models have been trained with adversarial defense mechanisms
3. Measure computational overhead for real-time applications and image quality degradation using multiple perceptual metrics beyond SSIM