---
ver: rpa2
title: 'Diagonalisation SGD: Fast & Convergent SGD for Non-Differentiable Models via
  Reparameterisation and Smoothing'
arxiv_id: '2402.11752'
source_url: https://arxiv.org/abs/2402.11752
tags:
- rrbracket
- llbracketf
- dsgd
- gradient
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses stochastic optimization for non-differentiable
  models, which commonly arise in variational inference and probabilistic programming
  due to if-statements. It introduces a syntactic framework to represent piecewise-defined
  functions and systematically smooth them using sigmoid functions.
---

# Diagonalisation SGD: Fast & Convergent SGD for Non-Differentiable Models via Reparameterisation and Smoothing

## Quick Facts
- arXiv ID: 2402.11752
- Source URL: https://arxiv.org/abs/2402.11752
- Reference count: 40
- Key outcome: DSGD achieves 4-20,000x variance reduction compared to existing methods for non-differentiable models with if-statements

## Executive Summary
This paper addresses the challenge of stochastic optimization for non-differentiable models arising in variational inference and probabilistic programming, particularly those containing if-statements. The authors introduce a systematic approach to smooth piecewise-defined functions using sigmoid approximations and propose Diagonalisation SGD (DSGD), a novel SGD variant that progressively enhances smoothing accuracy during optimization. DSGD converges to stationary points of the original unsmoothed objective while achieving dramatically lower variance (4-20,000x reduction) compared to existing unbiased estimators. The method requires no a priori choice of accuracy coefficients and performs comparably to existing methods with significantly better stability.

## Method Summary
The method introduces a syntactic framework to represent piecewise-defined functions and systematically smooth them using sigmoid functions (σ_η(x) = 1/(1+exp(-x/η))). DSGD progressively enhances the accuracy of these smoothed approximations during optimization by using a sequence of accuracy coefficients η_k that improve over iterations. The algorithm first uses coarse approximations with low variance to make large progress, then refines the approximation for precise adjustments near the optimum. The reparameterisation gradient estimator is unbiased for the smoothed objectives, and variance bounds are proven based on the nesting depth of if-statements. Convergence to stationary points of the original unsmoothed objective is guaranteed under appropriate step size and accuracy schedules.

## Key Results
- DSGD achieves 4-20,000x variance reduction compared to existing methods (Reparam, LYY18, Fixed, Score)
- Converges to stationary points of the original unsmoothed objective with no a priori choice of accuracy coefficients needed
- Performs comparably to existing unbiased estimators while requiring significantly less computational resources
- Maintains better stability than baseline methods across six benchmark models (temperature, random-walk, xornet, cheating, textmsg, influenza)

## Why This Works (Mechanism)

### Mechanism 1
Diagonalisation SGD progressively enhances the accuracy of the smoothed approximation during optimization, allowing the optimizer to get close to the optimum with low variance and then make fine adjustments as the approximation improves. By using a sequence of accuracy coefficients η_k that improve over iterations, the algorithm first uses coarse approximations (high η, low variance) to make large progress, then progressively refines the approximation (low η, higher variance) to make small, precise adjustments near the optimum. The variance of the gradient estimator increases as the accuracy improves, and this growth can be controlled by the rate at which η_k decreases.

### Mechanism 2
The reparameterisation gradient estimator is unbiased for the smoothed objective functions, allowing SGD to converge to stationary points of the original unsmoothed objective. The smoothing replaces discontinuous step functions with smooth sigmoid functions, making the objective differentiable everywhere. The reparameterisation trick then yields an unbiased gradient estimator for this smoothed objective, leveraging the differentiability of the smoothing function σ_η(x) = 1/(1+exp(-x/η)) and the reparameterisation transformation.

### Mechanism 3
The variance of the gradient estimator for smoothed expressions can be bounded based on the syntactic structure (nesting depth) of the expression, allowing for provable convergence guarantees. By classifying expressions by their maximum nesting depth ℓ of if-statements, the authors show that the variance is bounded by η^(-ℓ), which allows them to choose an appropriate η_k schedule for convergence. This classification enables theoretical analysis of the method's behavior across different model complexities.

## Foundational Learning

- **Reparameterisation trick and its bias for non-differentiable models**: Understanding why standard reparameterisation fails for models with if-statements is crucial to appreciating the need for smoothing. *Quick check*: What happens to the reparameterisation gradient estimator when the objective function has a discontinuity? (It becomes biased)

- **Variance reduction techniques in stochastic optimization**: The paper's main contribution is achieving orders of magnitude reduction in work-normalised variance compared to existing methods. *Quick check*: Why is work-normalised variance (variance × computational cost) a better metric than raw variance? (Because you can always reduce variance by taking more samples at increased computational cost)

- **Stochastic gradient descent convergence theory**: The convergence proof relies on standard SGD theory with modifications for the diagonalisation approach. *Quick check*: What are the key conditions for SGD to converge to stationary points in non-convex settings? (Unbiased gradients, bounded variance, appropriate step size schedule)

## Architecture Onboarding

- **Component map**: Expression parser -> Smoothing engine -> Gradient estimator -> DSGD optimizer -> Variance monitor

- **Critical path**: 1. Parse model into Expr representation, 2. Generate smoothed version with accuracy coefficient η_k, 3. Compute reparameterisation gradient using JAX automatic differentiation, 4. Update parameters using DSGD rule, 5. Incrementally improve η_k according to schedule, 6. Monitor convergence and variance

- **Design tradeoffs**: Accuracy vs. variance (higher accuracy gives better approximation but higher variance), Fixed vs. progressive accuracy (fixed requires manual tuning, progressive automates but adds complexity), Syntactic vs. semantic smoothing (depends on expression structure rather than semantic meaning)

- **Failure signatures**: High variance explosion (η_k decreasing too quickly), Slow convergence (η_k decreasing too slowly or step size too small), Bias persistence (inadequate smoothing or inappropriate model representation)

- **First 3 experiments**: 1. Implement temperature control model and compare DSGD vs. standard reparameterisation, 2. Test DSGD on xornet with varying initial η_0 values to demonstrate robustness, 3. Benchmark variance reduction on random-walk model across different expression nesting depths

## Open Questions the Paper Calls Out

- **Open Question 1**: How does DSGD's convergence rate compare to fixed smoothing when both methods use optimal step sizes and accuracy schedules? The paper mentions that DSGD's average variance is lower than fixed smoothing with η < η_N+1/2, but doesn't provide direct convergence rate comparisons. This remains unresolved as the paper focuses on asymptotic convergence guarantees rather than explicit convergence rate comparisons between DSGD and fixed smoothing.

- **Open Question 2**: Can the accuracy coefficient schedule in DSGD be adaptively tuned during optimization rather than being predetermined? The authors mention in the concluding remarks that they plan to explore adaptive methods for tuning the accuracy coefficient. This remains unresolved as the current DSGD implementation uses a predetermined schedule (η_k = η_0/√k), and the paper explicitly identifies adaptive tuning as future work.

- **Open Question 3**: What is the theoretical relationship between the nesting depth ℓ of if-statements and the practical performance of DSGD across different models? While the paper establishes that ℓ affects the accuracy schedule, it doesn't investigate how different ℓ values impact practical optimization performance across diverse models. This remains unresolved as the paper proves variance bounds that depend on ℓ but doesn't explore this relationship systematically.

## Limitations
- Theoretical convergence proof depends on specific assumptions about step size and accuracy schedules
- Empirical evaluation limited to synthetic benchmarks; real-world performance on complex models remains untested
- Variance bounds are worst-case theoretical bounds that may not reflect practical performance

## Confidence
- **High confidence**: The core mechanism of progressive smoothing is sound and the basic convergence proof structure is valid
- **Medium confidence**: The variance reduction claims are well-supported by experiments but depend on synthetic benchmarks
- **Low confidence**: The practical applicability to very deep nesting depths (>3) and real-world models hasn't been demonstrated

## Next Checks
1. Test DSGD on a real-world probabilistic programming model with nested if-statements to assess practical scalability
2. Compare DSGD with adaptive smoothing methods that adjust η based on observed gradient variance rather than fixed schedules
3. Implement a version of DSGD that automatically detects expression nesting depth to eliminate the need for manual specification of ℓ