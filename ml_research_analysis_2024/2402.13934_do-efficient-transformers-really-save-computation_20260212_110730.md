---
ver: rpa2
title: Do Efficient Transformers Really Save Computation?
arxiv_id: '2402.13934'
source_url: https://arxiv.org/abs/2402.13934
tags:
- transformer
- efficient
- output
- attention
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the reasoning capabilities of efficient
  Transformer architectures, specifically the Sparse Transformer and the Linear Transformer.
  The authors analyze these models' ability to solve dynamic programming (DP) problems
  by modeling reasoning as a DP process similar to Chain-of-Thought prompts.
---

# Do Efficient Transformers Really Save Computation?

## Quick Facts
- arXiv ID: 2402.13934
- Source URL: https://arxiv.org/abs/2402.13934
- Reference count: 40
- Primary result: Efficient Transformers require hidden dimensions scaling with √L for general DP tasks, negating their computational advantages

## Executive Summary
This paper investigates whether efficient Transformer architectures like Sparse Transformers and Linear Transformers can truly reduce computational complexity for reasoning tasks. The authors analyze these models' ability to solve dynamic programming problems by modeling reasoning as a DP process similar to Chain-of-Thought prompts. They prove that while both architectures are expressive enough to solve general DP tasks, they require hidden dimensions that scale with problem size (√L), matching the computational complexity of standard Transformers. However, the authors identify a class of DP problems satisfying the "locality assumption" where efficient Transformers can be more efficient. The theoretical findings are validated through experiments on arithmetic evaluation, longest increasing subsequence, and edit distance tasks.

## Method Summary
The authors analyze Sparse Transformers and Linear Transformers through theoretical complexity analysis and empirical validation. For theoretical analysis, they prove information bottleneck limitations requiring hidden dimensions to scale with problem size for general DP tasks. The experimental setup involves three tasks: arithmetic evaluation with expressions containing numbers and operators, longest increasing subsequence on integer sequences, and edit distance on sequence pairs. All models use AdamW optimizer with specific hyperparameters, train for 100 epochs with batch size 512, and minimize negative log-likelihood loss. During inference, models generate Chain-of-Thought processes token by token using greedy search.

## Key Results
- Both Sparse and Linear Transformers require hidden dimensions scaling as Ω(√L) for general DP problems
- Efficient Transformers can be more efficient when problems satisfy the "locality assumption" (dependencies only on recent m steps where m << L)
- Experiments show efficient Transformers need larger embedding dimensions than standard Transformers, with requirements increasing as problem size grows

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Efficient Transformers cannot reduce computational complexity for general DP tasks compared to standard Transformers.
- Mechanism: The hidden dimension of efficient Transformers must scale with problem size (√L) to maintain expressiveness, negating their theoretical efficiency advantage.
- Core assumption: Regular DP problems require unique hidden state representations for different inputs, creating an information bottleneck.
- Evidence anchors:
  - [abstract]: "both architectures require a model size that scales with the problem size"
  - [section]: "we prove that neither architecture can generate the DP solution unless the hidden dimension of the network layers scales as Ω(√L)"
  - [corpus]: Weak - corpus neighbors discuss efficiency but not the specific scaling relationship proven here
- Break condition: If DP problems have locality properties where each step only depends on recent m steps where m << L

### Mechanism 2
- Claim: Linear Transformer efficiency gains disappear when solving general DP problems due to information bottlenecks.
- Mechanism: The prefix sum computation in Linear Transformers creates a bottleneck where the number of possible output sequences is limited by the hidden dimension size.
- Core assumption: The regularity condition ensures different inputs produce different hidden states, making the pigeonhole principle applicable.
- Evidence anchors:
  - [abstract]: "they require a growing model size with respect to the problem scale L"
  - [section]: "the bottleneck is a set of neurons whose values completely determine all ensuing outputs from a specific position"
  - [corpus]: Weak - corpus neighbors discuss Linear Transformers but not this specific information bottleneck analysis
- Break condition: When locality assumption holds, allowing smaller hidden dimensions

### Mechanism 3
- Claim: Sparse Transformer efficiency depends on block size optimization relative to problem locality.
- Mechanism: With optimal block size Θ(√L), Sparse Transformer can achieve O(L√L) complexity, but requires hidden dimension scaling for general problems.
- Core assumption: The block-wise attention pattern can capture dependencies when locality is present.
- Evidence anchors:
  - [abstract]: "we prove that while these models are expressive enough to solve general DP tasks, contrary to expectations, they require a model size that scales with the problem size"
  - [section]: "When B = Θ(√L), the Sparse Transformer achieves a minimal complexity of Θ(M(L√LD + LD2))"
  - [corpus]: Weak - corpus neighbors discuss Sparse Transformers but not the specific complexity analysis
- Break condition: When problem exhibits m-locality with m << √L

## Foundational Learning

- Concept: Dynamic Programming decomposition
  - Why needed here: The paper models reasoning tasks as DP problems where each step depends on previous subproblems
  - Quick check question: Can you explain how Chain-of-Thought reasoning maps to DP state transitions?

- Concept: Information bottleneck analysis
  - Why needed here: The proof relies on showing that efficient Transformers have inherent bottlenecks limiting their expressiveness
  - Quick check question: Why does the pigeonhole principle imply that hidden dimensions must scale with problem size?

- Concept: Locality assumptions in algorithm design
  - Why needed here: The paper identifies when efficient Transformers become truly efficient based on dependency locality
  - Quick check question: How does the m-locality condition reduce the required hidden dimension for efficient Transformers?

## Architecture Onboarding

- Component map: Input tokens -> Positional embeddings -> Attention (standard/sparse/linear) -> Feed-forward network -> Residual connections -> Layer normalization -> Output projection

- Critical path:
  1. Input token embedding with positional information
  2. Attention computation (standard, sparse, or linear variant)
  3. Feed-forward network processing
  4. Residual connections and layer normalization
  5. Output projection for sequence generation

- Design tradeoffs:
  - Standard vs Efficient: Quadratic vs linear/rectangular complexity in sequence length
  - Hidden dimension vs expressiveness: Larger dimensions needed for efficient variants on general problems
  - Block size selection: Impacts Sparse Transformer efficiency based on problem locality

- Failure signatures:
  - Poor performance despite large hidden dimensions: Likely missing locality properties
  - Accuracy plateauing: May indicate hitting information bottleneck limits
  - Memory overflow: Excessive hidden dimensions needed for general DP problems

- First 3 experiments:
  1. Replicate the Arithmetic task experiment comparing standard vs efficient Transformers at different embedding dimensions
  2. Test the EDLocal variant to verify locality benefits empirically
  3. Scale problem size systematically to observe hidden dimension requirements for each architecture

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the theoretical complexity lower bound for Linear Transformers be tightened to match the upper bound, similar to what was achieved for Sparse Transformers on arithmetic evaluation tasks?
- Basis in paper: [explicit] The paper mentions "it remains a challenging open question of whether such a complexity lower bound can be matched" for Linear Transformers
- Why unresolved: The authors have not provided explicit constructions of Linear Transformer parameters that can solve the arithmetic task with constant hidden dimension, only a lower bound
- What evidence would resolve it: A complete construction of Linear Transformer parameters that solves the arithmetic evaluation task with constant hidden dimension, or a proof that such construction is impossible

### Open Question 2
- Question: Does the locality assumption fundamentally limit the types of reasoning tasks where efficient Transformers can achieve computational advantages over standard Transformers?
- Basis in paper: [inferred] The paper identifies a class of DP problems satisfying the "locality assumption" where efficient Transformers can be more efficient, suggesting this may be a special case
- Why unresolved: The paper only explores one direction (when efficient Transformers are efficient) but doesn't investigate whether this locality condition is necessary or sufficient for efficiency
- What evidence would resolve it: Either a proof that efficient Transformers require locality for efficiency, or examples of non-local DP problems where efficient Transformers remain efficient

### Open Question 3
- Question: How do the theoretical findings about efficient Transformers' limitations generalize to other efficient Transformer variants beyond Sparse and Linear Transformers?
- Basis in paper: [explicit] The authors note "Although we show our results for representative efficient Transformers, it does not mean that our findings directly transfer to all models with similar designs"
- Why unresolved: The analysis focuses specifically on Sparse and Linear Transformers, leaving open whether the scaling requirements apply to other architectures
- What evidence would resolve it: Similar theoretical analysis applied to other efficient Transformer variants (Reformer, Longformer, etc.) showing whether they require scaling hidden dimensions with problem size

## Limitations

- Theoretical analysis relies on regularity condition and m-locality assumptions that may not hold for real-world reasoning tasks
- Experimental validation focuses on synthetic DP-style tasks which may not represent the complexity of natural language reasoning
- Analysis assumes fixed model architectures without considering architectural modifications that could mitigate identified bottlenecks

## Confidence

**High Confidence**: The theoretical proof that efficient Transformers require Ω(√L) hidden dimensions for general DP problems is mathematically rigorous and well-supported.

**Medium Confidence**: The practical implications for real-world applications remain uncertain. While the analysis is sound for the specific DP problem class studied, the extent to which natural language tasks exhibit similar locality properties is unclear.

**Low Confidence**: The claim that efficient Transformers cannot provide computational savings for general reasoning tasks in practice is not fully established. The paper doesn't extensively explore whether alternative architectural modifications or training strategies could overcome the identified limitations.

## Next Checks

1. Test the theoretical predictions on natural language reasoning benchmarks (like GSM8K or MATH problems) to determine if the √L scaling relationship holds for practical reasoning tasks, not just synthetic DP problems.

2. Investigate whether hybrid architectures that combine standard and efficient Transformer components can achieve both computational efficiency and expressive power, potentially breaking the identified information bottlenecks.

3. Explore whether other reasoning paradigms beyond Chain-of-Thought (such as direct answer generation or different prompting strategies) would change the computational complexity requirements for efficient Transformers.