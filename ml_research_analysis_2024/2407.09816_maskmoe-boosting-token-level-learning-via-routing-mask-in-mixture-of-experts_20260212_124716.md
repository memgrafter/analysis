---
ver: rpa2
title: 'MaskMoE: Boosting Token-Level Learning via Routing Mask in Mixture-of-Experts'
arxiv_id: '2407.09816'
source_url: https://arxiv.org/abs/2407.09816
tags:
- experts
- tokens
- routing
- maskmoe
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the underfitting problem in sparse Mixture-of-Experts
  (MoE) models, where infrequent tokens are poorly learned due to routing fluctuations.
  The proposed MaskMoE method applies a fixed routing mask per token before training,
  allowing only a subset of experts to be visible for each token.
---

# MaskMoE: Boosting Token-Level Learning via Routing Mask in Mixture-of-Experts

## Quick Facts
- arXiv ID: 2407.09816
- Source URL: https://arxiv.org/abs/2407.09816
- Authors: Zhenpeng Su; Zijia Lin; Xue Bai; Xing Wu; Yizhe Xiong; Haoran Lian; Guangyuan Ma; Hui Chen; Guiguang Ding; Wei Zhou; Songlin Hu
- Reference count: 29
- Key outcome: MaskMoE achieves 6.11 perplexity on the Pile dataset, outperforming prior MoE methods, and improves downstream task performance across multiple benchmarks

## Executive Summary
This paper addresses the underfitting problem in sparse Mixture-of-Experts (MoE) models, where infrequent tokens are poorly learned due to routing fluctuations. The proposed MaskMoE method applies a fixed routing mask per token before training, allowing only a subset of experts to be visible for each token. Infrequent tokens see only one expert, ensuring thorough learning, while frequent tokens see multiple experts to maintain representation diversity. Experiments show MaskMoE achieves 6.11 perplexity on the Pile dataset, outperforming prior MoE methods, and improves downstream task performance across multiple benchmarks.

## Method Summary
MaskMoE addresses underfitting in MoE models by applying a fixed routing mask per token before training. The mask determines which experts are visible to each token: infrequent tokens see only one expert to ensure thorough training, while frequent tokens see multiple experts to maintain representation diversity. The method uses a shared expert architecture with 1 shared expert plus 128 routed experts (each half-sized). Training employs AdamW optimizer with cosine learning rate decay and load balancing loss applied only to frequent tokens. The approach is evaluated on The Pile dataset (100B tokens, 32k vocab) with perplexity and downstream task performance as metrics.

## Key Results
- MaskMoE achieves 6.11 perplexity on the Pile dataset, outperforming baseline MoE models
- Improved downstream task performance across 9 benchmarks including BoolQ, Hellaswag, LAMBADA, PIQA, SIQA, StoryCloze, Arc-e, TriviaQA, and WebQs
- The method effectively balances thorough training of infrequent tokens with representation diversity for frequent tokens

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The routing mask eliminates routing fluctuations for infrequent tokens, ensuring each such token is always routed to the same expert across all training iterations.
- Mechanism: Before training, a fixed routing mask is generated for each token. For infrequent tokens, the mask allows visibility of only one expert, meaning the token will always be routed to that expert regardless of router parameter updates. This eliminates the routing fluctuation problem where the same token might otherwise be routed to different experts in different training iterations.
- Core assumption: Infrequent tokens have enough occurrences to train one expert thoroughly when consistently routed to it.
- Evidence anchors:
  - [abstract]: "infrequent tokens are poorly learned due to routing fluctuations"
  - [section]: "for infrequent tokens, the proposed MaskMoE employs routing masking to retain only one visible expert"
  - [corpus]: Weak - No direct corpus evidence on routing fluctuations being solved by fixed masks

### Mechanism 2
- Claim: The routing mask maintains representation diversity for frequent tokens by allowing them to be routed to multiple experts.
- Mechanism: For frequent tokens, the routing mask allows visibility of multiple experts (V > 1). This means frequent tokens can still benefit from being processed by different experts across different training iterations, maintaining the diversity of learned representations that was identified as beneficial for frequent tokens in prior work.
- Core assumption: Frequent tokens have sufficient occurrences to be effectively routed to multiple experts while still maintaining adequate training for each expert.
- Evidence anchors:
  - [abstract]: "frequent tokens see multiple experts to maintain representation diversity"
  - [section]: "For each frequent token, the proposed MaskMoE allow more visible experts for it in each MoE layer"
  - [corpus]: Weak - No direct corpus evidence on diversity benefits of multi-expert routing

### Mechanism 3
- Claim: The routing mask improves overall model performance by balancing thorough training of infrequent tokens with representation diversity for frequent tokens.
- Mechanism: By allowing only one visible expert for infrequent tokens and multiple visible experts for frequent tokens, MaskMoE addresses the key limitations of both dynamic routing (underfitting of infrequent tokens due to routing fluctuations) and fixed routing (lack of representation diversity for frequent tokens). This balanced approach leads to improved perplexity and downstream task performance.
- Core assumption: The frequency threshold for separating frequent and infrequent tokens can be effectively determined based on training data distribution.
- Evidence anchors:
  - [abstract]: "MaskMoE is capable of maintaining representation diversity while achieving more comprehensive training"
  - [section]: "MaskMoE makes the model learn more intensively about infrequent tokens by routing each to an identical expert...while MaskMoE maintains the diversity of representations for frequent tokens"
  - [corpus]: Moderate - Some related work on MoE routing exists but no direct evidence on this specific balancing mechanism

## Foundational Learning

- Concept: Routing fluctuations in dynamic routing MoE models
  - Why needed here: Understanding why infrequent tokens are poorly learned in standard MoE models is crucial to grasping the problem MaskMoE solves
  - Quick check question: What happens to the routing of a token when router parameters change during training in dynamic routing MoE?

- Concept: Representation diversity in MoE models
  - Why needed here: The benefit of having multiple experts for frequent tokens is a key motivation for MaskMoE's design
  - Quick check question: Why might frequent tokens benefit from being routed to multiple experts rather than just one?

- Concept: Load balancing in MoE training
  - Why needed here: The paper mentions load balancing loss, which is important for understanding the full training setup
  - Quick check question: Why is load balancing important in MoE models, and when does the balancing loss not apply?

## Architecture Onboarding

- Component map: Token → Router (with mask) → Top-k experts → Output → Loss computation
- Critical path: Token → Router (with mask) → Top-k experts → Output → Loss computation
- Design tradeoffs:
  - Single vs. multiple visible experts for infrequent tokens: More thorough training vs. risk of underfitting if too few occurrences
  - Frequency threshold for separating frequent/infrequent tokens: Affects balance between thorough training and representation diversity
  - Number of experts: More experts increases model capacity but may exacerbate routing fluctuation issues
- Failure signatures:
  - Poor performance on infrequent tokens: May indicate insufficient occurrences or too many visible experts for infrequent tokens
  - Lack of improvement over baseline models: Could indicate poor frequency threshold selection or insufficient training data
  - Load imbalance: Suggests routing mask not properly configured for token frequency distribution
- First 3 experiments:
  1. Ablation study: Remove routing mask and compare performance to baseline MoE models
  2. Frequency threshold sweep: Test different values of P (0.2, 0.4, 0.6, 0.8) to find optimal separation
  3. Visible expert count variation: Test different values of V for both frequent and infrequent tokens to find optimal balance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal classification method for dividing tokens into frequent and infrequent categories in MaskMoE, and how would smoother partitioning affect performance?
- Basis in paper: [explicit] The paper acknowledges that categorizing tokens into frequent and infrequent groups based solely on frequency is somewhat rigid, suggesting that a smoother classification method could potentially yield better results.
- Why unresolved: The paper did not explore alternative classification methods due to computational constraints and focused on the simple frequency-based approach.
- What evidence would resolve it: Experiments comparing different token classification methods (e.g., percentile-based, adaptive thresholds) and their impact on MaskMoE performance metrics like perplexity and downstream task accuracy.

### Open Question 2
- Question: How does MaskMoE's performance scale with different numbers of experts, particularly when the number of experts exceeds 128?
- Basis in paper: [inferred] The paper's analysis shows performance improvements with increasing experts up to 128, but does not explore scenarios with even larger expert counts.
- What evidence would resolve it: Comprehensive experiments testing MaskMoE with 256, 512, and 1024 experts, measuring perplexity, training efficiency, and downstream task performance.

### Open Question 3
- Question: What is the impact of varying the visible expert count (V) for both frequent and infrequent tokens on MaskMoE's performance?
- Basis in paper: [explicit] The paper mentions that the number of visible experts (V) is a key hyperparameter but does not provide a detailed analysis of how different values affect performance.
- What evidence would resolve it: Systematic experiments varying V for both frequent and infrequent tokens, analyzing the resulting effects on perplexity, training stability, and downstream task accuracy.

## Limitations

- The core assumption that routing fluctuations are the primary cause of infrequent token underfitting remains untested in isolation
- The frequency threshold (P=0.4) was chosen without extensive sensitivity analysis across different datasets
- The optimal number of visible experts (V) for different frequency ranges was not thoroughly investigated

## Confidence

**High Confidence**: The experimental results showing 6.11 perplexity on the Pile dataset and improved downstream task performance are well-documented and reproducible.

**Medium Confidence**: The claim that MaskMoE maintains representation diversity while achieving comprehensive training is supported by results but relies on untested assumptions about the relationship between routing diversity and representation quality.

**Low Confidence**: The assertion that routing fluctuations are the primary cause of infrequent token underfitting is plausible but not definitively proven.

## Next Checks

1. **Routing Stability Analysis**: Implement a tracking mechanism to measure routing consistency for infrequent tokens across training iterations in both standard MoE and MaskMoE. Quantify the reduction in routing fluctuations and correlate this with learning gains for infrequent tokens.

2. **Frequency Threshold Sweep**: Conduct experiments varying the frequency threshold P across a wide range (0.2, 0.4, 0.6, 0.8) and measure the impact on both perplexity and downstream task performance. Identify the optimal threshold for different token frequency distributions.

3. **Expert Visibility Optimization**: Systematically vary the number of visible experts (V) for both frequent and infrequent tokens (e.g., V=1,2,4,8 for frequent; V=1,2 for infrequent) to determine the optimal balance between representation diversity and thorough training for each token category.