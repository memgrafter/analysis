---
ver: rpa2
title: Towards a RAG-based Summarization Agent for the Electron-Ion Collider
arxiv_id: '2403.15729'
source_url: https://arxiv.org/abs/2403.15729
tags:
- claims
- agent
- information
- data
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'A RAG-based AI agent, RAGS4EIC, was developed to summarize and
  reference EIC-related information, addressing challenges faced by new collaborators.
  The system uses a two-step approach: querying a vector database of EIC papers and
  generating summaries with citations using a Large Language Model.'
---

# Towards a RAG-based Summarization Agent for the Electron-Ion Collider

## Quick Facts
- arXiv ID: 2403.15729
- Source URL: https://arxiv.org/abs/2403.15729
- Reference count: 15
- One-line primary result: RAG-based AI agent achieves 96.4% claim recognition, 85.3% citation frequency, and 87.4% faithfulness on EIC summarization tasks.

## Executive Summary
The paper presents RAGS4EIC, a RAG-based AI agent designed to summarize and reference information related to the Electron Ion Collider (EIC) for new collaborators. The system addresses the challenge of navigating the rapidly growing EAG-specific literature by combining vector database retrieval with LLM-generated summaries. RAGS4EIC achieved high performance on standard evaluation metrics while significantly reducing hallucinations through grounding responses in verified knowledge.

## Method Summary
The system uses a two-step approach: first querying a vector database of EIC papers to retrieve relevant passages using cosine similarity, then generating summaries with citations using an LLM (GPT-3.5) via LangChain. The knowledge base consists of EIC arXiv papers from 2021 onwards, converted to text chunks and embedded with OpenAI's text-embedding-ada-002. The system was evaluated using RAGAs framework with synthetic QA datasets generated by GPT-4.0 and human-reviewed.

## Key Results
- Achieved 96.4% claim recognition rate on RAGAs evaluation
- Maintained 85.3% source citation frequency across queries
- Demonstrated 87.4% faithfulness score with only 2% hallucination frequency
- Successfully reduced hallucinations by grounding responses in verified knowledge base

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented generation (RAG) effectively grounds LLM responses in domain-specific truth, reducing hallucinations.
- Mechanism: The system queries a vector database of EIC papers, retrieves semantically relevant passages using cosine similarity, and injects them into a prompt for the LLM, constraining its output to verified information.
- Core assumption: Semantic embeddings capture the intended meaning well enough that retrieval + injection produces trustworthy responses.
- Evidence anchors:
  - [abstract]: "The RAG Agent significantly reduces hallucinations by grounding responses to the knowledge base"
  - [section]: "Utilizing a Large Language Model (LLM) to generate concise summaries enriched with citations based on user queries and retrieved data"
- Break condition: If retrieved vectors are semantically unrelated to the query or if the LLM ignores the context, hallucinations will increase.

### Mechanism 2
- Claim: Instruction-tuned prompts improve summarization accuracy and citation reliability.
- Mechanism: Templates containing example responses guide the LLM to structure output in GitHub Markdown, include proper citations, and follow claim-answering logic.
- Core assumption: LLMs can reliably follow structured instructions and replicate formatting patterns when provided with clear templates.
- Evidence anchors:
  - [abstract]: "utilizing a Large Language Model (LLM) to generate concise summaries enriched with citations"
  - [section]: "Instruction tuned prompts: To enhance the alignment of LLM for particular tasks, contextual learning techniques are used. Prompts that contain sample responses have been shown to improve summary performance."
- Break condition: If the prompt template is ambiguous or if the LLM's instruction-following degrades, citation accuracy and format will drop.

### Mechanism 3
- Claim: Synthetic QA datasets generated with LLM assistance provide high-quality evaluation data without requiring extensive human annotation.
- Mechanism: GPT-4.0 processes EIC papers, creates questions with claims, generates ideal answers, and annotators review and accept/reject these pairs, forming a benchmark.
- Core assumption: GPT-4.0 can generate semantically accurate and contextually relevant QA pairs when guided by templates, and human review ensures quality.
- Evidence anchors:
  - [section]: "To alleviate the requirement for domain-specific knowledge, an AI-driven approach was adopted to produce a high quality dataset"
  - [section]: "Each question in the dataset is associated with a clearly defined set of 'claims'... This approach also guarantees that even someone new to the field can generate a dataset with the help of LLMs."
- Break condition: If GPT-4.0 hallucinates facts or if human review is inconsistent, the evaluation dataset will be unreliable.

## Foundational Learning

- Vector embeddings
  - Why needed here: To transform raw text into numerical vectors that enable fast, semantic similarity searches in the knowledge base.
  - Quick check question: How does cosine similarity differ from Euclidean distance in measuring semantic similarity?
- Chunking strategies
  - Why needed here: To break large documents into manageable, semantically coherent pieces for embedding without losing context.
  - Quick check question: What trade-off exists between chunk size and semantic relevance?
- RAG evaluation metrics
  - Why needed here: To quantitatively measure system performance in terms of claim recognition, citation accuracy, and hallucination rate.
  - Quick check question: Why might the RAGAs "Answer Correctness" metric differ from "Claim Accuracy Rate"?

## Architecture Onboarding

- Component map: Vector database (PineCone/ChromaDB) → Query engine → LLM (GPT-3.5/GPT-4) ← Prompt templates → Web UI (Streamlit) → LangSmith (tracing)
- Critical path: User query → Vector retrieval (cosine similarity, 20 max) → Context injection → LLM response → Markdown rendering → Display
- Design tradeoffs: Chunk size vs. semantic loss; retrieval count vs. redundancy; local vs. cloud vector DB for latency vs. availability; embedding model choice vs. cost/performance.
- Failure signatures:
  - High hallucination frequency → Retrieval relevance or LLM instruction-following broken.
  - Low claim recognition rate → Context not matching claims or LLM not parsing claims.
  - Slow inference → Large vector DB or embedding model inefficiency.
- First 3 experiments:
  1. Vary chunk size (60, 120, 180 chars) and measure retrieval recall and hallucination rate.
  2. Swap cosine similarity for MMR and compare context relevancy scores.
  3. Compare GPT-3.5 vs GPT-4 outputs on same queries to quantify faithfulness differences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal chunking parameters (size and overlap) for EIC-specific PDF documents to maximize retrieval accuracy while minimizing information loss?
- Basis in paper: [explicit] The paper mentions ongoing investigations into chunking parameters, specifically noting that a chunk size of 120 characters with a 10-character overlap was used, but variations (60 and 180 characters) are being explored.
- Why unresolved: The paper states that optimization of parameters is crucial for enhancing vector retrieval efficiency but defers detailed optimization to future work.
- What evidence would resolve it: Systematic evaluation of retrieval accuracy and information loss across different chunking configurations using EIC-specific documents would determine the optimal parameters.

### Open Question 2
- Question: How does the performance of RAGS4EIC vary when using different embedding models (e.g., text-embedding-ada-002, BERT, seq2seq, word2vec) for the EIC knowledge base?
- Basis in paper: [explicit] The paper mentions that various deep learning-based embedding methods are used to compress information into high-dimensional vectors but only explored OpenAI's text-embedding-ada-002.
- Why unresolved: The paper only used one embedding model (text-embedding-ada-002) and states that other embedding models were not explored in this study.
- What evidence would resolve it: Comparative evaluation of retrieval performance and vector quality using different embedding models on the EIC knowledge base would reveal the most effective option.

### Open Question 3
- Question: What is the impact of using Maximal Marginal Relevance (MMR) versus cosine similarity for retrieving relevant vectors in the EIC context?
- Basis in paper: [explicit] The paper mentions that users can choose between cosine similarity and MMR when utilizing data from VectorDB, with MMR being beneficial for reducing redundant entries in Question and Answer tasks.
- Why unresolved: The paper does not provide empirical results comparing the effectiveness of MMR versus cosine similarity for EIC-specific queries.
- What evidence would resolve it: A controlled experiment comparing retrieval performance, redundancy reduction, and user satisfaction between MMR and cosine similarity metrics would quantify the impact.

### Open Question 4
- Question: How can the RAGS4EIC system be improved to better handle questions involving physics equations and special LaTeX characters?
- Basis in paper: [explicit] The paper notes that the RAG Agent's ability to provide accurate responses decreases significantly when dealing with questions that involve physics equations (including special LaTeX characters).
- Why unresolved: The paper suggests that enhancements can be made by implementing more effective chunking strategies and refining the LLM but does not provide specific solutions or results.
- What evidence would resolve it: Development and testing of improved chunking strategies and LLM refinements specifically for physics equations, followed by evaluation of response accuracy, would address this limitation.

## Limitations
- Prompt template specifics: The exact instruction-tuned prompt templates used to guide the LLM are not fully detailed, limiting reproducibility assessment.
- Dataset creation process: The specific review criteria and rejection rates for synthetic QA pairs are not provided, limiting confidence in evaluation dataset quality.
- Embedding model impact: Only one embedding model was explored, leaving open questions about whether alternative models could improve retrieval quality.

## Confidence
- High: The system's overall performance metrics (96.4% claim recognition, 85.3% citation frequency, 87.4% faithfulness) are reported with clear methodology and evaluation framework (RAGAs).
- Medium: The claim that RAG effectively reduces hallucinations is supported by metrics but relies on assumptions about retrieval relevance and LLM instruction-following that are not fully validated.
- Medium: The effectiveness of instruction-tuned prompts is demonstrated but the specific prompt engineering techniques are not detailed, limiting reproducibility.

## Next Checks
1. **Prompt ablation study**: Systematically vary the instruction-tuned prompt templates and measure the impact on citation accuracy and hallucination rates to isolate the contribution of prompt engineering.
2. **Embedding model comparison**: Replace the text-embedding-ada-002 with alternative models (e.g., sentence-transformers) and compare retrieval quality and downstream RAG performance to assess embedding model impact.
3. **Human evaluation of QA dataset**: Have independent human annotators review a sample of the synthetic QA pairs to validate the quality and accuracy of the dataset used for evaluation.