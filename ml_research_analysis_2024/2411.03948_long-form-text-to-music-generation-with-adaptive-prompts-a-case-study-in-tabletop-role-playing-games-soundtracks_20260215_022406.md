---
ver: rpa2
title: 'Long-Form Text-to-Music Generation with Adaptive Prompts: A Case Study in
  Tabletop Role-Playing Games Soundtracks'
arxiv_id: '2411.03948'
source_url: https://arxiv.org/abs/2411.03948
tags:
- music
- bardo
- babel
- audio
- text-to-music
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the use of text-to-audio models for generating
  long-form music with changing prompts in tabletop role-playing games (TRPGs). The
  Babel Bardo system uses large language models to convert speech transcriptions into
  music descriptions, which are then used by a text-to-music model to generate background
  music.
---

# Long-Form Text-to-Music Generation with Adaptive Prompts: A Case Study in Tabletop Role-Playing Games Soundtracks

## Quick Facts
- **arXiv ID**: 2411.03948
- **Source URL**: https://arxiv.org/abs/2411.03948
- **Reference count**: 0
- **Primary result**: Detailed music descriptions improve audio quality while maintaining consistency across consecutive descriptions enhances story alignment and transition smoothness in long-form text-to-music generation for TRPGs.

## Executive Summary
This paper investigates long-form music generation for tabletop role-playing games (TRPGs) where background music must adapt to changing narrative prompts. The Babel Bardo system uses large language models to convert speech transcriptions into music descriptions, which are then fed to a text-to-music model to generate adaptive background music. Four versions were evaluated across two TRPG campaigns: a baseline using direct transcriptions, and three LLM-based versions with different approaches to music description generation. Results show that the emotion-based approach achieved the best alignment with the story, demonstrating that adaptive prompts can maintain musical coherence while following narrative changes.

## Method Summary
The Babel Bardo system processes TRPG speech transcriptions through a multi-stage pipeline. First, speech is transcribed into text, then fed to a large language model (Ollama 3.1) to generate music descriptions using four different approaches: direct transcription (baseline), emotion classification, full description generation, and description continuation. These descriptions are then used by MusicGen (1.3B parameters) to generate 30-second music segments, with each segment conditioned on the previous audio context to maintain continuity. The system was evaluated on two TRPG campaigns (Call of the Wild in English and O Segredo da Ilha in Brazilian Portuguese) using Fréchet Audio Distance for quality assessment, Kullback-Leibler Divergence for story alignment with original music, and transition smoothness between consecutive segments.

## Key Results
- The emotion-based LLM approach achieved the best story alignment with the narrative
- Detailed music descriptions significantly improved audio quality compared to direct transcriptions
- Maintaining consistency across consecutive music descriptions enhanced transition smoothness between segments
- The sliding window approach enabled coherent long-form generation despite the 30-second context limitation of text-to-music models

## Why This Works (Mechanism)

### Mechanism 1
Text-to-music models generate longer sequences by sliding a context window through time, allowing them to maintain coherence across changing prompts. The model predicts the next token from a context window while considering both previous audio context and new prompts at each time step.

### Mechanism 2
Detailed music descriptions provide more specific information to the text-to-music model, resulting in generated music that better matches desired characteristics and quality attributes.

### Mechanism 3
Consistent music descriptions across consecutive segments help the text-to-music model generate music that aligns with the story and has smoother transitions, as the model can maintain thematic and emotional continuity.

## Foundational Learning

- **Text-to-music generation models and limitations**: Understanding how autoregressive models work and their context window constraints is essential to grasp why sliding window approaches are needed for long-form generation.
- **Large Language Models for music description**: Familiarity with LLM capabilities in transforming narrative content into musical descriptions helps understand the different approaches used in Babel Bardo.
- **Audio quality and alignment metrics**: Knowledge of FAD, KLD, and other evaluation metrics is necessary to understand how the system's performance is measured and compared.

## Architecture Onboarding

- **Component map**: Speech Recognition → LLM → Text-to-Music model → Generated music
- **Critical path**: Speech Recognition → LLM → Text-to-Music model → Generated music
- **Design tradeoffs**: Detailed vs. simple descriptions (quality vs. flexibility), consistency vs. adaptability, computational resources vs. real-time capability
- **Failure signatures**: Disjointed music segments, poor audio quality, misalignment with narrative
- **First 3 experiments**: 1) Compare baseline vs. LLM versions on audio quality, story alignment, and transition smoothness. 2) Test impact of description detail level on audio quality. 3) Assess consistency maintenance effects on story alignment and transition smoothness.

## Open Questions the Paper Calls Out

### Open Question 1
How does the system perform when transcribing and generating music descriptions in languages beyond English and Brazilian Portuguese? The evaluation was limited to two languages, and performance across a broader range would require systematic testing across multiple languages with quantitative comparisons.

### Open Question 2
What is the impact of increasing the context window size on long-form music generation quality and consistency? The study used fixed 30-second segments but didn't experiment with larger context windows or their effects on musical coherence.

### Open Question 3
How does the emotional classification approach compare to other semantic content-based methods for aligning generated music with narrative content? While emotion classification performed best among tested methods, comparison against other semantic approaches like topic modeling or sentiment analysis remains unexplored.

## Limitations

- Evaluation focused on two specific TRPG campaigns, limiting generalizability to other narrative domains
- Reliance on proxy metrics (FAD, KLD) that may not perfectly capture subjective musical quality and narrative coherence
- Computational overhead of repeatedly calling LLM and text-to-music models may limit real-time deployment

## Confidence

**High Confidence**: Text-to-music models can generate coherent long-form music through sliding window approaches; detailed music descriptions generally improve audio quality; maintaining consistency in music descriptions enhances story alignment.

**Medium Confidence**: The emotion-based approach achieves optimal story alignment; transition smoothness is significantly improved by description continuation; the system is suitable for real-world TRPG applications.

**Low Confidence**: Performance with languages other than Portuguese and English; generalizability to non-TRPG narrative contexts; real-time performance capabilities.

## Next Checks

1. **Cross-Domain Generalization Test**: Evaluate Babel Bardo on diverse narrative content types (podcasts, audiobooks, video game cutscenes) to assess generalizability beyond TRPG contexts.

2. **A/B Testing of Prompt Engineering**: Conduct systematic experiments varying LLM prompts, consistency thresholds, and description detail levels across multiple LLM models to identify optimal configurations.

3. **Real-time Performance Benchmarking**: Implement the system in a resource-constrained environment and measure latency, memory usage, and quality degradation when processing continuous speech input.