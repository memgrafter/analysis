---
ver: rpa2
title: Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed GFlowNets
arxiv_id: '2412.07775'
source_url: https://arxiv.org/abs/2412.07775
tags:
- reward
- residual
- diffusion
- diversity
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Nabla-GFlowNet (nabla-GFlowNet), a novel
  reinforcement learning method for aligning and finetuning pretrained diffusion models
  using reward functions. The key innovation is leveraging reward gradients through
  a probabilistic framework called gradient-informed Detailed Balance (nabla-DB),
  which enables faster convergence while preserving diversity and prior knowledge
  from the original model.
---

# Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed GFlowNets

## Quick Facts
- arXiv ID: 2412.07775
- Source URL: https://arxiv.org/abs/2412.07775
- Reference count: 40
- This paper introduces nabla-GFlowNet, a novel reinforcement learning method for aligning and finetuning pretrained diffusion models using reward functions while preserving diversity and prior knowledge.

## Executive Summary
This paper proposes nabla-GFlowNet (nabla-GFlowNet), a novel method for aligning and finetuning pretrained diffusion models using reward functions. The key innovation is leveraging reward gradients through a probabilistic framework called gradient-informed Detailed Balance (nabla-DB), which enables faster convergence while preserving diversity and prior knowledge from the original model. The method extends GFlowNet principles by incorporating first-order reward information, achieving better trade-offs between reward optimization, sample diversity, and prior preservation compared to existing finetuning approaches. Experiments on Stable Diffusion demonstrate that nabla-GFlowNet achieves faster reward convergence and better diversity preservation than baselines like DDPO, ReFL, and DRaFT, while avoiding mode collapse and catastrophic forgetting.

## Method Summary
The method introduces a probabilistic framework that leverages reward gradients for diffusion model finetuning. It constructs a Markov Decision Process from the diffusion model's sequential denoising process and derives a residual ∇-DB objective that preserves prior knowledge while optimizing for rewards. The approach uses a forward-looking (FL) technique to speed up convergence by providing better initialization for the residual flow score function. The method is trained using LoRA with rank 8 on Stable Diffusion v1.5, employing a 50-step DDPM sampler and collecting 64 generation trajectories per epoch with 4 gradient accumulation steps.

## Key Results
- Achieves faster reward convergence compared to DDPO, ReFL, and DRaFT baselines
- Better diversity preservation measured by DreamSim diversity scores
- Avoids mode collapse and catastrophic forgetting while maintaining prior knowledge
- Demonstrates effective trade-offs between reward optimization, diversity, and prior preservation

## Why This Works (Mechanism)

### Mechanism 1
The method leverages reward gradients to guide the finetuning process, achieving faster convergence while preserving diversity and prior knowledge. By taking derivatives on the logarithms of both sides of the Detailed Balance (DB) condition, the method obtains the conditional score function of the finetuned diffusion model. This allows the model to navigate the optimization landscape more efficiently, as the reward gradients provide additional information beyond the zeroth-order reward.

### Mechanism 2
The residual ∇-DB objective preserves the prior knowledge from the pretrained model while optimizing for the reward function. The residual ∇-DB objective is derived by subtracting the forward ∇-DB equation for the pretrained model from that of the finetuned model. This creates a "residual" term that accounts for the difference between the finetuned and pretrained models, ensuring that the finetuned model does not forget the prior knowledge.

### Mechanism 3
The forward-looking (FL) technique speeds up the finetuning process by providing a better initialization for the residual flow score function. The FL technique parameterizes the residual flow score function with a "baseline" of the "one-step predicted reward gradient". This initialization is better than a naïve zero or random initialization, as it sets the residual flow score function closer to its optimal value from the start.

## Foundational Learning

- Concept: Generative Flow Networks (GFlowNets)
  - Why needed here: GFlowNets provide the probabilistic framework for incorporating reward gradients into the finetuning process.
  - Quick check question: What is the key idea behind GFlowNets, and how do they differ from traditional generative models?

- Concept: Detailed Balance (DB) condition
  - Why needed here: The DB condition is the foundation for deriving the ∇-DB objective, which incorporates reward gradients into the finetuning process.
  - Quick check question: What is the DB condition, and how does it relate to the flow function in GFlowNets?

- Concept: Markov Decision Process (MDP)
  - Why needed here: The MDP formulation is used to model the sequential denoising process of diffusion models, allowing the application of reinforcement learning techniques.
  - Quick check question: How is an MDP constructed from a diffusion model's sampling process, and what are the states and transitions?

## Architecture Onboarding

- Component map: Pretrained diffusion model (Stable Diffusion v1.5) -> Reward function (Aesthetic Score, HPSv2, ImageReward) -> Residual flow score function -> Forward policy (finetuned model) -> Backward policy (noising process)

- Critical path:
  1. Sample trajectories from the pretrained diffusion model
  2. Compute the residual ∇-DB loss for each transition in the trajectories
  3. Update the forward policy and the residual flow score function using the computed loss

- Design tradeoffs:
  - Reward temperature (β): Higher values lead to faster convergence but may sacrifice diversity and prior preservation
  - Sub-sampling rate: Higher rates provide better gradient estimates but are more computationally expensive
  - Attenuation of predicted rewards: Helps stabilize training but may slow down convergence

- Failure signatures:
  - Mode collapse: Generated samples lose diversity, indicating over-optimization for reward function
  - Catastrophic forgetting: Model loses prior knowledge from pretrained model, resulting in unnatural images

- First 3 experiments:
  1. Train with different reward temperatures (β) and evaluate trade-off between convergence speed, diversity, and prior preservation
  2. Compare performance with and without forward-looking (FL) technique to assess impact on convergence speed
  3. Investigate effect of sub-sampling rate on gradient estimate quality and overall performance

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of nabla-GFlowNet scale with reward function complexity and dataset size for finetuning diffusion models? The paper mentions testing on different reward functions but doesn't explore the relationship between reward complexity, dataset size, and performance. This would be resolved by comprehensive experiments testing across a wide range of reward function complexities and dataset sizes with quantitative metrics.

### Open Question 2
Can nabla-GFlowNet be effectively applied to finetune diffusion models on tasks beyond image generation, such as video or 3D object generation? The paper focuses on image generation but mentions diffusion models can be applied to videos, 3D objects, and other domains. This would be resolved by experiments applying nabla-GFlowNet to finetune diffusion models for video generation, 3D object generation, and other domains with comparisons to existing methods.

### Open Question 3
How does the choice of the forward-looking (FL) scale parameter γt affect the trade-off between convergence speed and diversity preservation in nabla-GFlowNet? The paper mentions γt is important and proposes setting γt = αT - t, but doesn't provide a systematic study of how different values affect performance. This would be resolved by a comprehensive ablation study testing different values of γt across a range of settings with quantitative metrics.

## Limitations

- Implementation details for the residual flow score function and its initialization relative to the U-Net are underspecified
- Limited baseline comparisons and lack of comprehensive ablation studies for experimental results
- Only tested on image generation tasks without exploring other domains like video or 3D object generation

## Confidence

- High confidence in the theoretical framework and mathematical derivations (∇-DB condition, residual objective)
- Medium confidence in experimental results due to limited baseline comparisons and lack of ablation studies
- Medium confidence in the claimed advantages (faster convergence, better diversity preservation) based on the presented metrics

## Next Checks

1. Implement an ablation study comparing nabla-GFlowNet with and without the forward-looking technique to quantify its impact on convergence speed
2. Test the method on additional reward functions beyond the three mentioned to verify generalizability across different alignment objectives
3. Evaluate mode collapse detection by systematically monitoring DreamSim scores throughout training and comparing against established diversity metrics in the literature