---
ver: rpa2
title: Exploring Quantization for Efficient Pre-Training of Transformer Language Models
arxiv_id: '2407.11722'
source_url: https://arxiv.org/abs/2407.11722
tags:
- quantization
- training
- memory
- figure
- baseline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically investigates the impact of quantization
  on efficient pre-training of Transformer language models. It examines linear quantization
  (4-bit and 8-bit) across model weights, activations, gradients, and optimizer states,
  focusing on effects on training stability, efficiency, and performance.
---

# Exploring Quantization for Efficient Pre-Training of Transformer Language Models

## Quick Facts
- arXiv ID: 2407.11722
- Source URL: https://arxiv.org/abs/2407.11722
- Authors: Kamran Chitsaz; Quentin Fournier; GonÃ§alo Mordido; Sarath Chandar
- Reference count: 40
- One-line primary result: 8-bit weight and activation quantization achieves performance comparable to the floating-point baseline while providing significant memory savings.

## Executive Summary
This study systematically investigates the impact of quantization on efficient pre-training of Transformer language models. It examines linear quantization (4-bit and 8-bit) across model weights, activations, gradients, and optimizer states, focusing on effects on training stability, efficiency, and performance. Key findings show that 8-bit weight and activation quantization achieves performance comparable to the floating-point baseline while providing significant memory savings. However, 4-bit quantization causes training instability due to sharper loss landscapes and activation outliers. The work provides a practical recipe for quantized pre-training, balancing efficiency gains with model performance.

## Method Summary
The study pre-trains GPT-2 small (124M) on the OpenWebText corpus (157B tokens) using linear quantization with 4 and 8 bits for weights, activations, gradients, and optimizer states. The training configuration includes 300k gradient steps, global batch size of 512, context length of 1024 tokens, and AdamW optimizer with mixed precision bfloat16. Quantization granularity varies (per-tensor, per-channel, per-token) with fake quantization and straight-through estimator for backpropagation. Performance is evaluated through validation loss during training and few-shot accuracy on downstream tasks (GLUE, ARC, Hellaswag, LAMBADA).

## Key Results
- 8-bit weight and activation quantization achieves performance comparable to the floating-point baseline while providing significant memory savings
- 4-bit quantization causes training instability due to sharper loss landscapes and activation outliers
- Gradient quantization is particularly problematic, with 4-bit failing to converge and even 8-bit showing reduced performance
- First-order moments of Adam can be quantized to 4 bits without major loss, but second-order moments require more sophisticated approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: 8-bit weight and activation quantization can achieve performance comparable to the floating-point baseline while providing significant memory savings.
- Mechanism: The quantization process introduces minimal error when using 8-bit precision because the dynamic range of the parameters and activations is well-captured within this reduced precision. Per-channel quantization for weights allows for adaptive scaling factors across different channels, reducing the impact of outliers and maintaining performance.
- Core assumption: The model's loss landscape remains relatively smooth when using 8-bit quantization, allowing gradient updates to still navigate towards minima effectively.
- Evidence anchors:
  - [abstract]: "Key findings show that 8-bit weight and activation quantization achieves performance comparable to the floating-point baseline while providing significant memory savings."
  - [section 4.1]: "We observe that per-channel weight quantization with 8 bits outperforms the floating-point baseline since the beginning of training in terms of validation loss, while per-tensor weight quantization with 8 bits shows competitive performance."
  - [corpus]: Weak evidence; the related papers focus on post-training quantization rather than pre-training, making direct comparison difficult.
- Break condition: If the quantization introduces significant noise that disrupts the gradient updates, causing the model to diverge or converge to a suboptimal solution.

### Mechanism 2
- Claim: 4-bit quantization causes training instability due to sharper loss landscapes and activation outliers.
- Mechanism: Reducing the bit-width to 4 bits significantly compresses the dynamic range of the parameters and activations, leading to a coarser representation. This can cause the loss landscape to become sharper, making it more difficult for the optimizer to navigate. Additionally, activation outliers that were previously well-represented may now fall outside the representable range, causing significant errors.
- Core assumption: The presence of outliers in the activation distributions and the sharper loss landscape due to 4-bit quantization are the primary causes of instability.
- Evidence anchors:
  - [abstract]: "However, 4-bit quantization causes training instability due to sharper loss landscapes and activation outliers."
  - [section 4.2]: "We see that outliers predominantly reside within specific channels and persistently affect the same channels throughout training."
  - [section 4.1]: "Nagel et al. (2022) suggested that the oscillations in weights originated from performing quantization with STE during training may result in weight movements around decision thresholds."
- Break condition: If the quantization error is negligible or if the model can adapt to the sharper loss landscape, training might remain stable despite the reduced precision.

### Mechanism 3
- Claim: Gradient quantization is particularly problematic due to spikes in gradient norms during early training phases and the unstructured and sparse nature of gradients throughout training.
- Mechanism: Gradients in the early stages of training can have large norms, making them susceptible to significant quantization errors when using lower bit-widths. Additionally, the sparse and unstructured nature of gradients throughout training means that the quantization error is not uniformly distributed, leading to instability in the optimization process.
- Core assumption: The combination of large gradient norms in the early stages and the sparse nature of gradients throughout training contributes to the instability caused by gradient quantization.
- Evidence anchors:
  - [abstract]: "Gradient quantization is particularly problematic due to spikes in gradient norms during early training phases and the unstructured and sparse nature of gradients throughout training."
  - [section 4.3]: "We observe that gradients are mostly sparse during training and are prone to induce high quantization errors, rendering instabilities."
  - [corpus]: Weak evidence; the related papers do not directly address gradient quantization during pre-training.
- Break condition: If the gradients are dense and have small norms, gradient quantization might not cause significant instability.

## Foundational Learning

- Concept: Quantization Schemes
  - Why needed here: Understanding different quantization schemes (linear, symmetric, asymmetric) is crucial for selecting the appropriate method for each model component and bit-width.
  - Quick check question: What is the difference between symmetric and asymmetric quantization, and when would you use each?

- Concept: Sharpness of Loss Landscapes
  - Why needed here: The sharpness of the loss landscape affects the stability of training, especially when using lower bit-width quantization. Sharper landscapes are more difficult to navigate.
  - Quick check question: How does the sharpness of a loss landscape affect the optimization process, and why is this important for quantized training?

- Concept: Gradient Sparsity
  - Why needed here: Understanding the sparsity of gradients is essential for assessing the impact of gradient quantization. Sparse gradients are more susceptible to quantization errors.
  - Quick check question: Why are sparse gradients more challenging to quantize effectively, and how does this impact training stability?

## Architecture Onboarding

- Component map: Weights -> Activations -> Gradients -> Optimizer States (first and second moments of Adam)
- Critical path: Forward pass involves quantizing weights and activations; backward pass involves quantizing gradients; optimizer states are quantized after each weight update
- Design tradeoffs: Lower bit-widths offer greater memory savings and potential speedup but can introduce instability. Higher bit-widths are more stable but offer less efficiency gain.
- Failure signatures: Training instability (divergence or oscillation), degradation in downstream task performance, and increased validation loss are indicators of quantization failure.
- First 3 experiments:
  1. Quantize weights to 8 bits using per-channel granularity and assess the impact on validation loss and downstream task performance.
  2. Quantize activations to 8 bits using per-token granularity and evaluate the model's stability and performance.
  3. Quantize both weights and activations to 8 bits simultaneously and observe the combined effect on training efficiency and model performance.

## Open Questions the Paper Calls Out

- Question: How do different quantization schemes (beyond linear quantization) affect the training stability and performance of Transformer language models during pre-training?
  - Basis in paper: [explicit] The paper states that it focuses on linear quantization to maintain practical relevance, but acknowledges that more sophisticated quantization methods may enhance performance.
  - Why unresolved: The study deliberately limited itself to linear quantization, leaving the effects of more complex quantization schemes unexplored.
  - What evidence would resolve it: Comparative experiments using quantile quantization, learnable quantization parameters, or other advanced methods during pre-training of Transformer models.

- Question: What is the impact of quantization on larger Transformer models (e.g., GPT-2 medium, large, or even larger) during pre-training?
  - Basis in paper: [explicit] The authors acknowledge that their findings may not generalize to larger models due to computational constraints limiting their study to GPT-2 small.
  - Why unresolved: The study was limited to GPT-2 small due to computational constraints, leaving the effects on larger models unexplored.
  - What evidence would resolve it: Pre-training experiments on GPT-2 medium, large, and other larger Transformer models with the same quantization schemes.

- Question: How does gradient quantization impact memory savings in practice, given that gradients do not contribute to peak memory usage under certain conditions?
  - Basis in paper: [explicit] The memory analysis reveals that when a model can fit within GPU memory, activations dominate peak memory usage, and gradients do not contribute to peak memory usage.
  - Why unresolved: The paper profiles memory usage but doesn't directly measure the actual memory savings from gradient quantization during training.
  - What evidence would resolve it: Direct measurements of GPU memory usage during training with and without gradient quantization under various batch sizes and sequence lengths.

## Limitations

- The study's findings are limited to GPT-2 small (124M parameters), potentially limiting generalizability to larger models
- Evaluation relies on few-shot prompting rather than full fine-tuning, which may not capture all performance nuances
- The analysis focuses on linear quantization without exploring alternative methods like product quantization or adaptive approaches

## Confidence

**High Confidence**: The observation that 8-bit weight and activation quantization maintains baseline performance is well-supported by systematic ablation studies across different granularities and consistent downstream task results.

**Medium Confidence**: The analysis of gradient quantization failures is convincing but based on a smaller set of experiments. While the evidence for 4-bit gradient quantization causing instability is strong, the specific mechanisms proposed could benefit from more extensive validation.

**Low Confidence**: The claims about 4-bit quantization instability being primarily due to sharper loss landscapes and activation outliers lack quantitative evidence directly linking these phenomena to training failure.

## Next Checks

1. **Scale-up Validation**: Replicate the 8-bit quantization experiments with larger transformer models (e.g., LLaMA-7B) to verify whether the observed stability and performance gains extend beyond the 124M parameter scale tested in the current study.

2. **Mechanism Isolation**: Design controlled experiments to isolate the impact of loss landscape sharpness versus activation outliers on 4-bit quantization stability. This could involve artificially smoothing the loss landscape or clipping activation distributions to determine which factor is more critical for training instability.

3. **Alternative Quantization Methods**: Evaluate product quantization or adaptive quantization schemes for the optimizer states, particularly the second-order moments, to determine if more sophisticated approaches can achieve the efficiency benefits of 4-bit quantization without the convergence issues observed with linear quantization.