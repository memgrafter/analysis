---
ver: rpa2
title: The Implicit Bias of Gradient Descent on Separable Multiclass Data
arxiv_id: '2411.01350'
source_url: https://arxiv.org/abs/2411.01350
tags:
- lemma
- multiclass
- have
- loss
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the gap in implicit bias analysis for multiclass
  classification, extending the well-known binary classification results to a broader
  class of losses. The authors introduce a multiclass extension of the exponential
  tail property using the PERM (Permutation Equivariant and Relative Margin-based)
  loss framework, which generalizes beyond cross-entropy to include losses like PairLogLoss
  and multiclass exponential loss.
---

# The Implicit Bias of Gradient Descent on Separable Multiclass Data

## Quick Facts
- arXiv ID: 2411.01350
- Source URL: https://arxiv.org/abs/2411.01350
- Reference count: 40
- One-line primary result: Gradient descent on PERM losses with exponential tail property converges directionally to the hard-margin multiclass SVM solution

## Executive Summary
This paper addresses the gap in implicit bias analysis for multiclass classification by extending the well-known binary classification results to a broader class of losses. The authors introduce a multiclass extension of the exponential tail property using the PERM (Permutation Equivariant and Relative Margin-based) loss framework, which generalizes beyond cross-entropy to include losses like PairLogLoss and multiclass exponential loss. They prove that for linearly separable multiclass datasets, gradient descent with sufficiently small learning rate exhibits directional convergence to the hard-margin multiclass SVM solution.

## Method Summary
The paper analyzes the implicit bias of gradient descent for multiclass classification on linearly separable data using PERM losses with the exponential tail property. The method involves proving that gradient descent iterates converge directionally to the hard-margin multiclass SVM solution. The proof technique closely mirrors binary case proofs by leveraging the relative margin form of PERM losses, allowing the analysis to work with (K-1)-dimensional vectors of relative margins instead of individual class scores.

## Key Results
- PERM losses with exponential tail property generalize the implicit bias result to multiclass classification
- Gradient descent on these losses converges directionally to the hard-margin multiclass SVM solution
- The convergence occurs at a slow logarithmic rate
- PairLogLoss experiments validate the theoretical findings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient descent on PERM losses with exponential tail (ET) property converges directionally to the hard-margin multiclass SVM solution.
- Mechanism: The ET property ensures the negative gradient of the PERM template decays exponentially in the direction of large margins, creating a bias that pushes classifier weights toward maximizing the minimum margin across all classes.
- Core assumption: The dataset is linearly separable and the PERM loss is convex, β-smooth, strictly decreasing, and non-negative.
- Evidence anchors: [abstract] "we extend the implicit bias result of Soudry et al. [2018] to multiclass classification"
- Break condition: If data is not linearly separable or loss does not satisfy ET property, directional convergence may not occur.

### Mechanism 2
- Claim: The PERM framework allows simple generalization of binary proof techniques to multiclass setting.
- Mechanism: The relative margin form of PERM losses decouples labels from predicted scores, enabling proof to mirror binary case by working with (K-1)-dimensional vector of relative margins.
- Core assumption: PERM loss has differentiable template and dataset is linearly separable.
- Evidence anchors: [abstract] "our proof techniques closely mirror those of the binary case"
- Break condition: If loss is not PERM or lacks differentiable template, proof techniques may not generalize.

### Mechanism 3
- Claim: Exponential tail bounds on negative gradient ensure gradient norm converges to zero, necessary for directional convergence.
- Mechanism: ET property provides upper and lower bounds on negative gradient, used to bound gradient norm and show it converges to zero, combined with convexity implying directional convergence.
- Core assumption: PERM loss satisfies ET property and dataset is linearly separable.
- Evidence anchors: [abstract] "we introduce a multiclass extension of the exponential tail property"
- Break condition: If ET bounds are not tight enough or loss is not convex, gradient norm may not converge to zero.

## Foundational Learning

- Concept: Linear separability
  - Why needed here: The proof relies on existence of hyperplane that can separate all classes with margin of at least 1, a fundamental assumption for implicit bias result.
  - Quick check question: What does it mean for a multiclass dataset to be linearly separable?

- Concept: PERM losses and the relative margin form
  - Why needed here: PERM framework allows loss to be expressed in terms of relative margins between classes, crucial for generalizing binary proof techniques to multiclass setting.
  - Quick check question: How does the relative margin form of a PERM loss differ from standard multiclass loss formulation?

- Concept: Exponential tail property
  - Why needed here: ET property provides bounds on negative gradient of loss, essential for proving convergence of gradient norm to zero and directional convergence to hard-margin SVM.
  - Quick check question: What are the upper and lower bounds on negative gradient required by ET property?

## Architecture Onboarding

- Component map: Data → PERM loss with ET property → Gradient descent → Directional convergence to hard-margin SVM

- Critical path: Data → PERM loss with ET → Gradient descent → Directional convergence to hard-margin SVM

- Design tradeoffs:
  - PERM losses vs. non-PERM losses: PERM losses enable simpler proof by working with relative margins
  - ET property vs. non-ET losses: ET losses ensure convergence to hard-margin SVM, while non-ET losses may converge to different solution
  - Fixed learning rate vs. adaptive learning rate: Fixed learning rate simplifies proof, but adaptive learning rates may be more practical

- Failure signatures:
  - If data is not linearly separable, directional convergence may not occur
  - If loss does not satisfy ET property, gradient norm may not converge to zero
  - If loss is not convex, proof techniques may not apply

- First 3 experiments:
  1. Verify that PERM loss with ET property satisfies assumptions (convexity, β-smoothness, etc.)
  2. Check that gradient norm converges to zero for linearly separable dataset
  3. Confirm directional convergence to hard-margin SVM solution on synthetic dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the existence of the solution to Equation (8) (Assumption 4.1) be proven for almost all linearly separable datasets?
- Basis in paper: [explicit] The paper states that proving the existence of the solution to Equation (8) for almost all linearly separable datasets is posed as a conjecture in Appendix H.
- Why unresolved: The paper only assumes the existence of the solution to Equation (8) without providing a proof. The authors pose this as a conjecture and suggest that further research is needed to prove its validity.
- What evidence would resolve it: A rigorous mathematical proof demonstrating the existence of the solution to Equation (8) for almost all linearly separable datasets would resolve this open question.

### Open Question 2
- Question: Can the implicit bias result for multiclass classification be extended to non-ET losses?
- Basis in paper: [inferred] The paper focuses on losses with the exponential tail (ET) property and mentions that another possible line of future work is to analyze the gradient descent dynamics for non-ET losses.
- Why unresolved: The paper only analyzes implicit bias for losses with the ET property. The authors acknowledge that analyzing non-ET losses is a potential direction for future research but do not provide any results or insights in this area.
- What evidence would resolve it: Proving that gradient descent exhibits implicit bias towards the hard-margin multiclass SVM solution for a broader class of non-ET losses would resolve this open question.

### Open Question 3
- Question: Can the implicit bias result for multiclass classification be extended to other gradient-based methods beyond vanilla gradient descent?
- Basis in paper: [inferred] The paper mentions that another line of work involves exploring implicit bias effects of other gradient-based methods, such as those characterized in Gunasekar et al. [2018].
- Why unresolved: The paper only analyzes vanilla gradient descent and does not provide any results or insights for other gradient-based methods. The authors acknowledge that exploring implicit bias effects of other gradient-based methods is a potential direction for future research.
- What evidence would resolve it: Proving that gradient descent variants or other gradient-based methods exhibit implicit bias towards the hard-margin multiclass SVM solution would resolve this open question.

## Limitations

- The paper relies heavily on theoretical proofs without extensive empirical validation across diverse datasets
- Extension of binary proof techniques to multiclass settings may face practical limitations with real-world noisy or near-separable data
- Experimental results using PairLogLoss are mentioned but not fully detailed in the abstract

## Confidence

- High confidence: The PERM framework definition and its mathematical properties are well-established
- Medium confidence: The proof that PERM losses with ET property converge to hard-margin SVM, as it extends existing binary results
- Low confidence: The practical significance and empirical performance of PairLogLoss compared to standard cross-entropy, as experimental details are limited

## Next Checks

1. Implement PairLogLoss on benchmark multiclass datasets (MNIST, CIFAR-10) and compare convergence behavior and final performance against cross-entropy
2. Test the sensitivity of the implicit bias result to non-separability by adding noise to synthetic datasets and measuring deviation from the hard-margin solution
3. Verify the gradient norm convergence empirically across different PERM losses and learning rates on both synthetic and real datasets