---
ver: rpa2
title: 'WERank: Towards Rank Degradation Prevention for Self-Supervised Learning Using
  Weight Regularization'
arxiv_id: '2402.09586'
source_url: https://arxiv.org/abs/2402.09586
tags:
- werank
- bgrl
- learning
- collapse
- singular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses rank degradation (dimensional collapse) in
  self-supervised learning (SSL), a phenomenon where learned representations are mapped
  to a low-dimensional subspace, limiting their quality. The proposed method, WERank,
  introduces a novel regularization term applied directly to the weight parameters
  of the network, preventing rank degeneration across all layers rather than just
  the final output.
---

# WERank: Towards Rank Degradation Prevention for Self-Supervised Learning Using Weight Regularization

## Quick Facts
- **arXiv ID**: 2402.09586
- **Source URL**: https://arxiv.org/abs/2402.09586
- **Reference count**: 40
- **Primary result**: WERank prevents rank degradation in graph SSL by regularizing weight matrices across all layers

## Executive Summary
WERank addresses rank degradation (dimensional collapse) in self-supervised learning by introducing a novel weight regularization technique. The method applies orthogonality constraints directly to the weight parameters of neural networks, preventing rank degeneration across all layers rather than just the final output. This approach is particularly effective in graph SSL tasks where rank collapse is pronounced due to limited data augmentation. Experiments show that WERank increases the rank of learned representations and improves downstream classification accuracy across multiple graph datasets.

## Method Summary
WERank introduces a regularization term that enforces orthogonality in weight matrices across all layers of the network, specifically constraining $W^T W \approx I$ to push singular values toward 1. The method is tested on graph SSL tasks using the BGRL framework, where it effectively increases representation rank and improves downstream accuracy. The regularization is most impactful under weak augmentation settings and requires dataset-specific coefficient tuning. WERank operates by counteracting the implicit regularization that drives over-parameterized networks toward low-rank solutions.

## Key Results
- WERank effectively increases the rank of learned representations in graph SSL settings
- Improves downstream classification accuracy across multiple graph datasets
- Most effective under weak augmentation settings, with diminishing returns as augmentation strength increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: WERank prevents rank degradation by enforcing orthogonality in weight matrices across all layers.
- Mechanism: The regularization term directly constrains the weight matrices such that $W^T W \approx I$, pushing singular values toward 1. This ensures that transformations do not collapse dimensionality early in the network.
- Core assumption: Dimensional collapse propagates from earlier layers; thus, regularizing all layers is more effective than just the final embedding.
- Evidence anchors:
  - [abstract] "WERank, a new regularizer on the weight parameters of the network to prevent rank degeneration at different layers of the network."
  - [section] "WERank is directly computed on the weights of the neural network and is computationally more efficient."
- Break condition: If the weight matrices become rank-deficient before regularization is applied, the constraint cannot restore full rank.

### Mechanism 2
- Claim: Over-parameterization leads to implicit regularization that drives weights toward low-rank solutions.
- Mechanism: Deep networks with more parameters than necessary tend to find flatter minima, which correspond to lower-rank weight matrices. WERank counteracts this by explicitly discouraging low-rank weight configurations.
- Core assumption: Over-parameterized networks inherently prefer low-rank solutions due to implicit regularization.
- Evidence anchors:
  - [section] "Due to the implicit regularization caused by over-parameterization, the smallest group of singular values grow significantly slower throughout training."
  - [corpus] "Preventing Dimensional Collapse in Self-Supervised Learning via Orthogonality Regularization" - similar focus on orthogonality to prevent collapse.
- Break condition: If the network architecture or initialization already enforces full rank, additional regularization may be redundant.

### Mechanism 3
- Claim: Strong augmentations reduce common information between views, exacerbating rank degradation.
- Mechanism: When augmentation is extreme, the covariance matrix of embeddings becomes low-rank. WERank helps maintain higher rank representations by constraining weight matrices even when input views are highly distorted.
- Core assumption: The amount of shared information between augmented views determines the rank of the learned representations.
- Evidence anchors:
  - [section] "strong augmentation along feature dimensions is a secondary cause for dimensional collapse" and "WERank is distinctly impactful when the augmentation is weak."
  - [corpus] "LDReg: Local Dimensionality Regularized Self-Supervised Learning" - regularization to maintain dimensionality under varying conditions.
- Break condition: If augmentation is so strong that no meaningful shared structure remains, rank cannot be preserved regardless of regularization.

## Foundational Learning

- Concept: Singular Value Decomposition (SVD) and its role in measuring matrix rank.
  - Why needed here: WERank's effectiveness depends on understanding how singular values relate to rank and how regularization affects them.
  - Quick check question: If a weight matrix has singular values [5, 1, 0.1], what is its approximate rank? (Answer: 3, but the last dimension contributes little.)

- Concept: Implicit regularization in deep learning.
  - Why needed here: The paper argues that over-parameterized networks naturally find low-rank solutions; understanding this helps explain why explicit regularization is necessary.
  - Quick check question: Why might a network with more parameters than data points still generalize well? (Answer: Implicit regularization biases it toward simpler, lower-norm solutions.)

- Concept: Graph convolutions and their mathematical formulation.
  - Why needed here: The experiments are conducted in the graph SSL domain; understanding how GCN layers transform node features is essential.
  - Quick check question: What is the output of a single GCN layer with adjacency matrix A, degree matrix D, input features X, and weights W? (Answer: $\sigma(D^{-1/2} A D^{-1/2} X W)$)

## Architecture Onboarding

- Component map:
  - Encoder: GCN or MeanPooling layers with residual connections
  - Predictor: Linear layers mapping representations to embeddings
  - WERank: Regularization term applied to weight matrices of encoder layers
  - Loss: SSL loss (e.g., BGRL) + WERank term

- Critical path:
  1. Forward pass through augmented graph views
  2. Compute embeddings via predictor
  3. Calculate SSL loss (similarity between embeddings)
  4. Compute WERank regularization on encoder weights
  5. Backpropagate combined loss

- Design tradeoffs:
  - Applying WERank to all layers vs. only final layers: More comprehensive but computationally heavier
  - Coefficient selection: Too high may hinder learning; too low may be ineffective
  - Graph vs. other modalities: WERank's effectiveness may vary with data augmentation availability

- Failure signatures:
  - Rank still collapses despite WERank: Likely due to extreme augmentation or insufficient regularization strength
  - Training instability: WERank coefficient may be too high, causing conflicting gradients
  - No performance improvement: Dataset may not benefit from rank preservation (e.g., small feature dimensions)

- First 3 experiments:
  1. Apply WERank to a simple GCN on Cora with weak augmentation; check rank preservation and downstream accuracy
  2. Vary WERank coefficient on a larger dataset (e.g., ogbn-arXiv); observe impact on rank and accuracy
  3. Test WERank under strong augmentation; confirm reduced effectiveness as predicted

## Open Questions the Paper Calls Out
- The authors note they leave the experimental evaluation on other SSL approaches to future work.
- The authors mention that they leave the exploration of different WERank coefficients for different layers as future work.

## Limitations
- WERank's effectiveness is primarily demonstrated in graph SSL settings, with untested generalization to other SSL domains
- The relationship between augmentation strength and rank preservation is correlational rather than mechanistic
- Optimal WERank coefficient appears dataset-dependent without a principled tuning method

## Confidence
- **High Confidence**: WERank effectively prevents rank degradation in graph SSL settings under weak augmentation
- **Medium Confidence**: Rank collapse propagates from earlier layers in the network
- **Low Confidence**: WERank's effectiveness in non-graph SSL domains (vision, language) where augmentation is stronger

## Next Checks
1. Test WERank on vision SSL benchmarks (e.g., CIFAR-10 with SimCLR) to verify cross-domain effectiveness, particularly under strong augmentation
2. Conduct ablation studies removing WERank from different layers (early vs. late) to quantify the contribution of each layer to rank preservation
3. Develop and test an adaptive WERank coefficient that adjusts based on real-time rank measurements during training, addressing the dataset-dependent tuning issue