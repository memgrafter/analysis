---
ver: rpa2
title: 'SeRTS: Self-Rewarding Tree Search for Biomedical Retrieval-Augmented Generation'
arxiv_id: '2406.11258'
source_url: https://arxiv.org/abs/2406.11258
tags:
- query
- serts
- search
- retrieval
- retrieved
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SeRTS, a self-rewarding tree search method
  that combines large language models with Monte Carlo Tree Search for biomedical
  retrieval-augmented generation. The method formulates document retrieval as a tree
  search problem, using a query proposer to generate search queries and a result evaluator
  to provide feedback based on a 5-point rubric.
---

# SeRTS: Self-Rewarding Tree Search for Biomedical Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2406.11258
- Source URL: https://arxiv.org/abs/2406.11258
- Reference count: 40
- Primary result: 74.31% precision and 59.09% recall on BioASQ-QA vs 72.48% and 47.16% for BM25 alone

## Executive Summary
SeRTS introduces a self-rewarding tree search method that enhances biomedical retrieval-augmented generation by combining large language models with Monte Carlo Tree Search. The method formulates document retrieval as a tree search problem, using a query proposer to generate search queries and a result evaluator to provide feedback based on a 5-point rubric. Experiments on the BioASQ-QA dataset demonstrate significant improvements over BM25 baseline, achieving 74.31% precision and 59.09% recall compared to 72.48% and 47.16% for BM25 alone. The method also surpasses self-reflection baselines in both efficiency and scalability, and further enhances performance through PPO fine-tuning.

## Method Summary
SeRTS formulates document retrieval as a tree search problem where each node represents a state containing the question, query proposal, retrieved documents, result score, feedback, and previous process observations. The method uses MCTS to iteratively perform selection, expansion, evaluation, and backpropagation to find optimal queries that retrieve the most relevant documents. A query proposer generates new query proposals based on previous observations and feedback, while a result evaluator provides feedback on retrieved documents using a 5-point rubric. The system collects trajectories of the search process and uses them to fine-tune the LLM agents with PPO objectives, enabling them to learn from the feedback and improve their query proposal and evaluation capabilities.

## Key Results
- Achieved 74.31% precision and 59.09% recall on BioASQ-QA vs 72.48% and 47.16% for BM25 alone
- Generated 30.16% fewer tokens than Self-Reflection with 12 simulation rounds
- Outperformed Self-Reflection baseline in both efficiency and scalability metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SeRTS improves retrieval performance by combining LLM reasoning with MCTS to iteratively refine queries
- Mechanism: The method formulates document retrieval as a tree search problem, where each node represents a state containing the question, query proposal, retrieved documents, result score, feedback, and previous process observations. MCTS iteratively performs selection, expansion, evaluation, and backpropagation to find optimal queries that retrieve the most relevant documents.
- Core assumption: Higher-quality feedback from the Result Evaluator leads to more accurate reasoning and better query proposals
- Evidence anchors:
  - [abstract]: "By combining the reasoning capabilities of LLMs with the effectiveness of tree search, SeRTS boosts the zero-shot performance of retrieving high-quality and informative results for RAG."
  - [section]: "Our method acts as a plug-and-play retriever that can be integrated into any RAG pipeline."
  - [corpus]: Found 25 related papers with average FMR=0.481, suggesting moderate relevance to the field but limited direct evidence of this specific mechanism
- Break condition: If the feedback quality degrades or the LLM fails to generate meaningful query proposals, the tree search will not improve retrieval performance

### Mechanism 2
- Claim: PPO fine-tuning enhances the reasoning abilities of the Query Proposer and Result Evaluator
- Mechanism: SeRTS collects trajectories of the search process and uses them to fine-tune the LLM agents with PPO objectives, enabling them to learn from the feedback and improve their query proposal and evaluation capabilities
- Core assumption: The trajectories collected by SeRTS contain valuable information that can be used to improve the LLMs' performance in document retrieval tasks
- Evidence anchors:
  - [abstract]: "We further enhance retrieval performance by fine-tuning LLMs with Proximal Policy Optimization (PPO) objectives using the trajectories collected by SeRTS as feedback."
  - [section]: "By updating the language model using PPO with the feedback TSeRT S, we aim to let LMs know how to make good query proposals given current observations."
  - [corpus]: Limited direct evidence in corpus; related works mention PPO for LLM fine-tuning but not specifically for retrieval-augmented generation
- Break condition: If the PPO fine-tuning leads to catastrophic forgetting or the trajectories contain noisy feedback, the self-improvement capability may be compromised

### Mechanism 3
- Claim: SeRTS achieves better scalability than Self-Reflection by reducing computational overhead
- Mechanism: By incorporating a self-rewarding paradigm and expanding strategy, SeRTS improves search efficiency and reduces the number of generated tokens compared to Self-Reflection, as demonstrated by the 30.16% token reduction with 12 simulation rounds
- Core assumption: The efficient exploration strategy in SeRTS allows it to achieve comparable or better performance with fewer computational resources
- Evidence anchors:
  - [abstract]: "Moreover, SeRTS generates higher-quality feedback for PPO training than self-reflection."
  - [section]: "With a simulation round of 12, SERTS generates 30.16% fewer tokens than SELF -REFLECTION ."
  - [corpus]: Moderate evidence from related works on efficiency in tree search and MCTS, but limited direct comparison to Self-Reflection
- Break condition: If the efficient exploration strategy leads to premature convergence or missing optimal solutions, the scalability advantage may be negated by reduced performance

## Foundational Learning

- Concept: Monte Carlo Tree Search (MCTS)
  - Why needed here: MCTS provides an effective framework for exploring the vast query space and finding optimal retrieval strategies through iterative search and evaluation
  - Quick check question: How does MCTS balance exploration and exploitation during node selection in the context of document retrieval?

- Concept: Reinforcement Learning with Proximal Policy Optimization (PPO)
  - Why needed here: PPO enables the LLM agents to learn from the feedback collected during the SeRTS search process, improving their ability to generate relevant queries and evaluate retrieved documents
  - Quick check question: What is the role of the clipping function in PPO, and how does it prevent drastic policy updates during fine-tuning?

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: RAG is the foundational framework that SeRTS aims to improve by enhancing the retrieval component, ensuring that the LLM has access to the most relevant documents for generating accurate answers
  - Quick check question: How does the quality of retrieved documents impact the performance of the RAG pipeline in answering biomedical questions?

## Architecture Onboarding

- Component map:
  - Query Proposer (P query) -> Result Evaluator (P eval) -> MCTS Algorithm -> PPO Fine-tuning -> BM25 Retriever

- Critical path:
  1. Initialize tree with the input question
  2. Perform MCTS search to find optimal queries and retrieve relevant documents
  3. Evaluate retrieved documents and provide feedback using the Result Evaluator
  4. Collect trajectories of the search process for PPO fine-tuning
  5. Update LLM agents using PPO to improve their query proposal and evaluation capabilities

- Design tradeoffs:
  - Exploration vs. Exploitation: Balancing the need to explore new query possibilities with the exploitation of known good queries
  - Feedback Quality vs. Computational Cost: Providing detailed feedback requires more computational resources but leads to better query proposals
  - Fine-tuning Frequency vs. Catastrophic Forgetting: Frequent fine-tuning can improve performance but may lead to forgetting previously learned knowledge

- Failure signatures:
  - Degraded Retrieval Performance: If the BM25 retriever fails to retrieve relevant documents, the entire pipeline will be compromised
  - Poor Feedback Quality: If the Result Evaluator provides inaccurate or misleading feedback, the MCTS search will be guided in the wrong direction
  - Overfitting to Training Data: If the PPO fine-tuning leads to overfitting, the LLM agents may not generalize well to new questions

- First 3 experiments:
  1. Ablation study: Compare SeRTS performance with and without PPO fine-tuning to assess the impact of self-improvement
  2. Scalability test: Measure the computational overhead of SeRTS with varying numbers of simulation rounds and compare it to Self-Reflection
  3. Robustness evaluation: Test SeRTS on different biomedical QA datasets to assess its generalization capabilities and identify potential failure modes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the limitations of the current SeRTS approach in terms of computational overhead and how can they be mitigated?
- Basis in paper: [explicit] The paper mentions that both SeRTS and Self-Reflection require numerous simulations, limiting widespread application, and suggests future research should focus on developing more efficient reasoning strategies.
- Why unresolved: While the paper acknowledges the computational overhead issue, it does not provide concrete solutions or specific metrics on how to improve efficiency beyond general suggestions.
- What evidence would resolve it: Experimental results comparing SeRTS with various optimizations or alternative algorithms that reduce computational cost while maintaining or improving performance would provide concrete evidence for mitigating this limitation.

### Open Question 2
- Question: How does the quality of feedback from the Result Evaluator impact the effectiveness of SeRTS, and can this feedback quality be further improved?
- Basis in paper: [explicit] The paper states that the quality of feedback plays a crucial role in producing accurate reasoning and that low-quality feedback can hinder the self-improving ability of LLMs during self-reflection.
- Why unresolved: While the paper highlights the importance of feedback quality, it does not explore methods to systematically improve the evaluator's performance or measure the direct impact of feedback quality on retrieval results.
- What evidence would resolve it: Comparative studies showing the performance difference between SeRTS using different evaluator models or with enhanced feedback mechanisms would demonstrate the impact of feedback quality and potential improvements.

### Open Question 3
- Question: What are the long-term effects of PPO fine-tuning on LLM performance, particularly regarding catastrophic forgetting and the ability to follow instructions?
- Basis in paper: [explicit] The paper discusses catastrophic forgetting as a limitation, noting that PPO fine-tuning can degrade LLMs' instruction-following capability and restricts experiments to a single iteration of sampling and PPO fine-tuning.
- Why unresolved: The paper identifies catastrophic forgetting as a concern but does not investigate its extent or explore strategies to mitigate it, such as techniques to preserve instruction-following abilities during fine-tuning.
- What evidence would resolve it: Longitudinal studies tracking LLM performance across multiple PPO fine-tuning iterations, including metrics for both retrieval accuracy and instruction-following capability, would quantify the extent of catastrophic forgetting and inform strategies to address it.

## Limitations
- The paper acknowledges that both SeRTS and Self-Reflection require numerous simulations, limiting widespread application
- The effectiveness of the five-point rubric for result evaluation is assumed but not validated through ablation studies
- PPO fine-tuning's impact on long-term performance stability remains unclear

## Confidence
- High Confidence: Core MCTS mechanism for query refinement and the improvement over BM25 baseline
- Medium Confidence: PPO fine-tuning effectiveness and comparison with Self-Reflection
- Low Confidence: Generalization across different biomedical domains and long-term stability of self-improving components

## Next Checks
1. Conduct ablation studies removing the five-point rubric to quantify its impact on retrieval quality
2. Test SeRTS on multiple biomedical QA datasets to assess domain generalization
3. Perform long-term stability analysis by evaluating retrieval performance after multiple PPO fine-tuning iterations