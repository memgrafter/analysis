---
ver: rpa2
title: Conformalized Selective Regression
arxiv_id: '2402.16300'
source_url: https://arxiv.org/abs/2402.16300
tags:
- regression
- prediction
- selective
- coverage
- uncertainty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework for selective regression
  that leverages conformal prediction to improve the reliability and coverage of predictions.
  The proposed approach, Conformalized Selective Regression (CSR), uses conformal
  prediction to provide model-specific confidence measures, accounting for inherent
  biases in predictive models.
---

# Conformalized Selective Regression

## Quick Facts
- **arXiv ID:** 2402.16300
- **Source URL:** https://arxiv.org/abs/2402.16300
- **Reference count:** 39
- **Primary result:** Introduces Conformalized Selective Regression (CSR) that outperforms state-of-the-art baselines by balancing error rates and coverage using conformal prediction with a normalized distance evaluation metric.

## Executive Summary
This paper presents Conformalized Selective Regression (CSR), a novel framework for selective regression that leverages conformal prediction to improve prediction reliability and coverage. CSR uses conformal prediction to provide model-specific confidence measures that account for inherent biases in predictive models. The authors propose a standardized evaluation framework based on normalized distance from the ideal point of zero error at full coverage, rather than the commonly used AUC metric. Extensive experiments demonstrate that CSR outperforms traditional regression models with reject options across multiple datasets and algorithms.

## Method Summary
CSR combines split conformal prediction with quantile regression to generate prediction intervals with guaranteed coverage. The method trains quantile regressors on a training set, computes conformity scores on a calibration set, and uses these scores to adjust prediction intervals for new data. A rejection mechanism is implemented where predictions are only made if the interval width falls below a threshold λ, otherwise the prediction is rejected. The framework is evaluated using a normalized Mean Squared Error metric and Euclidean distance from the ideal point of zero error at full coverage.

## Key Results
- CSR consistently outperforms baseline models across multiple datasets including Insurance Data, Communities and Crime Data, COMPAS Recidivism Risk Assessment, and LSAC Law School Admissions Data
- CSR integrated with advanced algorithms like XGBoost and Neural Networks shows superior performance compared to traditional regression models with reject options
- The normalized distance evaluation metric provides a more principled way to compare selective regression methods than traditional AUC-based approaches

## Why This Works (Mechanism)

### Mechanism 1
Conformal prediction provides calibrated uncertainty intervals that inherently account for model-specific biases. By constructing prediction intervals that cover the true value with a user-specified probability, conformal prediction avoids reliance on distributional assumptions and captures heteroscedasticity, which conditional variance-based methods miss. Core assumption: Conformity scores computed on a calibration set accurately reflect model bias and variance structure. Evidence anchors: [abstract] and [section 4] discuss conformal prediction's ability to provide grounded confidence measures based on model-specific biases.

### Mechanism 2
The proposed normalized distance evaluation metric better reflects operational trade-offs than AUC. Euclidean distance from the ideal point (zero error, full coverage) directly quantifies how close a model is to the optimal selective regression performance, whereas AUC aggregates over irrelevant coverage levels. Core assumption: The maximum MSE across models is a stable normalizing factor and that the ideal point is reachable within the experimental domain. Evidence anchors: [abstract] and [section 5] introduce the normalized distance framework as an alternative to AUC.

### Mechanism 3
Integrating CSR with advanced algorithms (XGBoost, Neural Networks) yields superior performance compared to traditional reject-option regression. Advanced learners provide more accurate quantile estimates, and CSR refines these with coverage guarantees, leading to lower error rates at higher coverage levels. Core assumption: The quantile regression components are well-calibrated and the conformal adjustment step does not overly inflate interval widths. Evidence anchors: [section 6.4] and [section 6.5] show CSR models, especially with advanced techniques, outperform baseline models.

## Foundational Learning

- **Concept: Conformal prediction and split conformal method**
  - Why needed here: Provides the theoretical foundation for generating prediction intervals with guaranteed coverage without distributional assumptions
  - Quick check question: What is the role of the calibration set in split conformal prediction, and how does it affect coverage guarantees?

- **Concept: Quantile regression for interval estimation**
  - Why needed here: Supplies the initial bounds that are later conformalized to produce final prediction intervals
  - Quick check question: How do conditional quantiles relate to prediction interval coverage, and why are they preferred over mean predictions in selective regression?

- **Concept: Normalized distance evaluation metric**
  - Why needed here: Offers a principled way to compare selective regression methods based on operational performance rather than aggregated AUC
  - Quick check question: Why is Euclidean distance from the ideal point preferable to AUC when evaluating coverage-error trade-offs?

## Architecture Onboarding

- **Component map:** Data split → Training set, Calibration set, Test set → Quantile regressors (lower/upper) → Conformity scores → Conformal adjustment → Selective threshold → Evaluation
- **Critical path:**
  1. Train quantile regressors on training set
  2. Compute conformity scores on calibration set
  3. Determine quantile of conformity scores (ˆqα)
  4. For new data, compute interval width W(X)
  5. If W(X) ≤ λ, output adjusted interval; else reject
- **Design tradeoffs:**
  - Larger calibration set → more stable conformity scores but less data for training
  - Tighter significance level α → narrower intervals but lower coverage guarantee
  - Complex quantile regressors → better bounds but higher computational cost
- **Failure signatures:**
  - Coverage consistently below 1-α → miscalibration or small calibration set
  - Excessive rejections at low thresholds → interval widths too large or quantile regressors overconfident
  - High nMSE despite high coverage → poor base quantile predictions
- **First 3 experiments:**
  1. Verify coverage on a synthetic dataset with known heteroscedasticity
  2. Compare nMSE vs. coverage curves for CSR vs. variance-based reject option
  3. Test sensitivity of rejection rate to the threshold λ on a held-out calibration set

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the proposed standardized evaluation framework using normalized distance compare to traditional AUC-based metrics in practical scenarios? The paper presents theoretical advantages but lacks extensive empirical comparisons between the two evaluation methods. What evidence would resolve it: Conducting experiments that directly compare the outcomes of the normalized distance framework and AUC on the same datasets to demonstrate practical benefits.

- **Open Question 2:** What are the potential impacts of model-specific biases on the performance of conformalized selective regression in real-world applications? The paper identifies the issue but does not explore detailed impacts or mitigation strategies in diverse real-world settings. What evidence would resolve it: Implementing CSR in various real-world scenarios to measure how model biases affect performance and identifying effective bias mitigation techniques.

- **Open Question 3:** Can the CSR framework be extended to handle multi-output regression tasks effectively? The paper does not explore the application of CSR to multi-output scenarios, which are common in practical applications. What evidence would resolve it: Developing and testing CSR extensions for multi-output regression tasks to evaluate their effectiveness and reliability.

## Limitations

- Limited scope of evaluation: The paper focuses primarily on the trade-off between error rates and coverage, not thoroughly exploring computational efficiency or scalability to larger datasets
- Potential overfitting to specific datasets: Performance gains might be influenced by the characteristics of the datasets used, without comprehensive analysis of CSR's robustness to different data distributions or noise levels
- Sensitivity to hyperparameter choices: The paper does not extensively discuss how CSR's performance is affected by the choice of hyperparameters such as significance level α or rejection threshold λ

## Confidence

- **Claim cluster: CSR improves coverage and reduces error rates compared to baselines**: High confidence, supported by extensive experimental results across multiple datasets and algorithms
- **Claim cluster: Conformal prediction provides calibrated uncertainty intervals that account for model-specific biases**: Medium confidence, supported by theoretical foundations but lacking explicit empirical validation of bias correction
- **Claim cluster: The normalized distance evaluation metric better reflects operational trade-offs than AUC**: Low confidence, as the paper does not provide a thorough comparison with other evaluation metrics or discuss the limitations of the proposed approach

## Next Checks

1. Evaluate CSR's performance on larger, more diverse datasets to assess robustness and scalability by testing it on datasets with different characteristics, such as varying noise levels, data distributions, or problem domains

2. Investigate the sensitivity of CSR to hyperparameter choices by conducting a systematic study to understand how performance is affected by the choice of hyperparameters such as α and λ, and provide guidelines for their selection

3. Compare CSR with other selective regression methods using the normalized distance metric to evaluate CSR's performance relative to other approaches such as variance-based reject options, ensuring a fair comparison using the proposed evaluation framework