---
ver: rpa2
title: 'QuantAgent: Seeking Holy Grail in Trading by Self-Improving Large Language
  Model'
arxiv_id: '2402.03755'
source_url: https://arxiv.org/abs/2402.03755
tags:
- loop
- signal
- agent
- knowledge
- trading
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces QuantAgent, an autonomous agent that uses
  large language models to discover trading signals in financial markets. The key
  challenge is building a domain-specific knowledge base for quantitative investment
  without extensive human effort.
---

# QuantAgent: Seeking Holy Grail in Trading by Self-Improving Large Language Model

## Quick Facts
- arXiv ID: 2402.03755
- Source URL: https://arxiv.org/abs/2402.03755
- Reference count: 40
- Primary result: An autonomous agent using LLMs discovers trading signals through a two-layer framework combining knowledge retrieval and real-world testing

## Executive Summary
QuantAgent is a self-improving autonomous agent that leverages large language models to discover viable trading signals in financial markets. The system addresses the challenge of building domain-specific knowledge bases for quantitative investment without extensive human effort through a novel two-layer framework. By iteratively refining responses using a knowledge base (inner loop) and testing them in real-world scenarios (outer loop), the agent demonstrates the ability to improve its forecasting accuracy and signal quality over time.

## Method Summary
The method employs a two-layer framework where an LLM-based writer generates financial signals based on trading ideas and retrieves relevant knowledge from a growing knowledge base. A judge component evaluates these signals using additional knowledge retrieval and provides improvement feedback, creating an inner loop of iterative refinement. The outer loop tests these signals in real-world scenarios (backtesting) and incorporates the resulting feedback into the knowledge base, enabling autonomous expansion of the system's financial knowledge. The approach combines knowledge retrieval, in-context inference, and iterative refinement to progressively approach optimal trading behavior.

## Key Results
- The agent demonstrates self-improvement capabilities with increasing predictive accuracy as the knowledge base grows through iterations
- Empirical results show the system can uncover viable financial signals and improve forecasting accuracy over time
- The approach successfully combines knowledge retrieval, in-context inference, and iterative refinement for progressive optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The inner loop enables progressive refinement of financial signals by leveraging knowledge retrieval and iterative feedback from a simulated judge
- Mechanism: The writer generates an initial signal based on a trading idea, retrieves relevant knowledge from the knowledge base, and produces an answer. The judge evaluates the answer using additional knowledge retrieval, assigns a score, and provides improvement advice. This feedback is incorporated into the context buffer, and the process repeats until a satisfactory signal is generated or a threshold is reached
- Core assumption: The judge's evaluations are reliable enough to guide the writer toward better signals over multiple iterations
- Evidence anchors:
  - [abstract] "In the inner loop, the agent refines its responses by drawing from its knowledge base, while in the outer loop, these responses are tested in real-world scenarios to automatically enhance the knowledge base with new insights"
  - [section] "The inner loop functions as a simulated reasoning environment where an LLM or rule-based system interacts with a knowledge repository... The agent poses queries to the knowledge base, which in turn provides pertinent information"
  - [corpus] Weak - corpus neighbors discuss general AI self-improvement but do not specifically address the inner loop mechanism for financial signal refinement
- Break condition: The loop breaks when the judge's score meets or exceeds a predefined threshold, or the maximum number of iterations is reached

### Mechanism 2
- Claim: The outer loop enables autonomous knowledge base expansion by incorporating real-world feedback into the agent's learning process
- Mechanism: The agent generates a financial signal based on a trading idea, submits it to the real-world environment (e.g., backtesting), and receives feedback in the form of performance scores and qualitative reviews. This feedback is then incorporated into the knowledge base, enriching it with new insights for future iterations
- Core assumption: The real-world feedback is accurate and representative of the signal's true performance, enabling the knowledge base to improve over time
- Evidence anchors:
  - [abstract] "In the outer loop, these responses are tested in real-world scenarios to automatically enhance the knowledge base with new insights"
  - [section] "The outer loop encapsulates the agent's iterative interactions with the real-world environment, where its generated outputs are evaluated and refined... The environment provides feedback in the form of performance scores and qualitative reviews, which may be generated by a sophisticated LLM"
  - [corpus] Weak - corpus neighbors discuss general AI self-improvement but do not specifically address the outer loop mechanism for knowledge base expansion in the context of financial signal mining
- Break condition: The outer loop continues for a predefined number of iterations (K), allowing the knowledge base to accumulate insights over time

### Mechanism 3
- Claim: The combination of inner and outer loops enables the agent to progressively approximate optimal behavior with provable efficiency
- Mechanism: The inner loop converges to an optimal solution given the current knowledge base by iteratively refining the writer's responses based on the judge's feedback. The outer loop bridges the gap between the simulated environment (characterized by the knowledge base) and the real environment by incorporating real-world feedback, allowing the optimal policy in the simulated environment to converge to the optimal policy in the real environment
- Core assumption: The knowledge base accurately represents the real-world environment, and the judge's evaluations are reliable enough to guide the inner loop convergence
- Evidence anchors:
  - [abstract] "We demonstrate that our approach enables the agent to progressively approximate optimal behavior with provable efficiency"
  - [section] "We bolster the framework with a theoretical analysis, demonstrating that both the inner and outer loops can efficiently converge towards an optimal solution... This convergence is substantiated by applying analytical techniques from reinforcement learning"
  - [corpus] Weak - corpus neighbors discuss general AI self-improvement but do not specifically address the combination of inner and outer loops for provable efficiency in the context of financial signal mining
- Break condition: The overall system converges when the performance gap between the agent's policy and the optimal policy in the real environment becomes negligible

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The inner loop's reasoning process is formulated as an MDP, where the agent's goal is to maximize the value function by iteratively refining its responses based on the knowledge base and judge's feedback
  - Quick check question: What are the components of an MDP, and how do they relate to the inner loop's reasoning process?

- Concept: Bayesian Inference
  - Why needed here: The LLM's in-context inference is assumed to implicitly perform Bayesian inference of the simulated environment's parameter, enabling the agent to estimate the environment more accurately as the information state converges
  - Quick check question: How does the LLM's in-context inference relate to Bayesian inference, and why is this assumption important for the inner loop's convergence?

- Concept: Pessimism in Offline Reinforcement Learning
  - Why needed here: The outer loop assumes that the optimal policy in the simulated environment can be obtained via pessimism, allowing the performance gap between the simulated and real environments to converge as the knowledge base accumulates more information
  - Quick check question: What is pessimism in offline reinforcement learning, and how does it help bridge the gap between the simulated and real environments in the outer loop?

## Architecture Onboarding

- Component map:
  Writer -> Knowledge Base -> Judge -> Context Buffer -> Real-World Environment -> Knowledge Base

- Critical path:
  1. Writer retrieves relevant knowledge from the knowledge base
  2. Writer generates an initial financial signal based on the trading idea and retrieved knowledge
  3. Judge retrieves additional knowledge and evaluates the writer's signal
  4. Judge provides a score and improvement advice
  5. Feedback is incorporated into the context buffer
  6. Steps 1-5 repeat until a satisfactory signal is generated or a threshold is reached (inner loop)
  7. The final signal is submitted to the real-world environment for evaluation
  8. Real-world feedback is incorporated into the knowledge base (outer loop)

- Design tradeoffs:
  - Inner loop iterations vs. computational cost: More iterations lead to better signals but increase computational cost
  - Knowledge base size vs. retrieval efficiency: A larger knowledge base provides more diverse insights but may slow down retrieval
  - Judge's strictness vs. convergence speed: A stricter judge may lead to better signals but may also slow down convergence

- Failure signatures:
  - Inner loop: The writer's signals do not improve over iterations, or the judge's evaluations are inconsistent or unhelpful
  - Outer loop: The real-world feedback does not lead to knowledge base improvement, or the performance gap between the simulated and real environments does not converge

- First 3 experiments:
  1. Implement the inner loop with a simple judge and a small knowledge base, and test its ability to refine a basic financial signal over multiple iterations
  2. Implement the outer loop with a simulated real-world environment and test its ability to incorporate feedback into the knowledge base
  3. Combine the inner and outer loops and test the overall system's ability to progressively improve financial signal quality and knowledge base richness over multiple outer loop iterations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the agent's performance scale with increasing complexity of trading ideas and market conditions?
- Basis in paper: [inferred] The paper discusses the agent's ability to generate financial signals based on trading ideas, but does not explore the limits of this capability
- Why unresolved: The experiments focus on a specific domain (Chinese A-share market) and a limited set of trading ideas, leaving the scalability of the approach unexplored
- What evidence would resolve it: Testing the agent on more complex trading ideas and diverse market conditions, and comparing its performance to that of human experts or other automated systems

### Open Question 2
- Question: What is the impact of the knowledge base size on the agent's performance, and is there a point of diminishing returns?
- Basis in paper: [explicit] The paper mentions that the knowledge base grows through iterations, but does not analyze the relationship between its size and the agent's performance
- Why unresolved: The experiments do not explore the effects of varying the knowledge base size on the agent's ability to generate high-quality financial signals
- What evidence would resolve it: Systematically increasing the knowledge base size and measuring the corresponding changes in the agent's performance, identifying any points of diminishing returns

### Open Question 3
- Question: How does the agent's performance compare to other state-of-the-art methods for generating financial signals?
- Basis in paper: [inferred] The paper presents the agent's ability to generate financial signals, but does not compare its performance to other methods
- Why unresolved: The experiments focus on the agent's self-improvement capabilities, without benchmarking against other approaches
- What evidence would resolve it: Conducting a comparative study of the agent's performance against other state-of-the-art methods for generating financial signals, using standardized datasets and evaluation metrics

## Limitations
- Weak empirical validation of theoretical convergence guarantees
- Knowledge base update mechanisms are vaguely specified
- Chinese A-share market data limits generalizability to other financial markets

## Confidence
- **High confidence**: The two-layer framework architecture and the basic mechanism of using LLM for financial signal generation
- **Medium confidence**: The self-improvement capabilities and knowledge base expansion through outer loop feedback
- **Low confidence**: The theoretical convergence guarantees and the extent of provable efficiency claims

## Next Checks
1. Conduct ablation studies to quantify the contribution of each component (writer, judge, knowledge base) to overall performance improvements
2. Test the system's performance across multiple financial markets and time periods to assess generalizability
3. Implement and test alternative judge evaluation mechanisms to verify the robustness of the self-improvement loop