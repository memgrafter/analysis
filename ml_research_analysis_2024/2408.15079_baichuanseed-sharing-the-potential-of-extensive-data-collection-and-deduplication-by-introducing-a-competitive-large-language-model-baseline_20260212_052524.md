---
ver: rpa2
title: 'BaichuanSEED: Sharing the Potential of ExtensivE Data Collection and Deduplication
  by Introducing a Competitive Large Language Model Baseline'
arxiv_id: '2408.15079'
source_url: https://arxiv.org/abs/2408.15079
tags:
- data
- zhang
- wang
- training
- chen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents BaichuanSEED, a 7B parameter LLM trained on
  3 trillion tokens without deliberate downstream optimization. The authors focus
  on data collection and deduplication, employing a pipeline of broad collection and
  reweighting.
---

# BaichuanSEED: Sharing the Potential of ExtensivE Data Collection and Deduplication by Introducing a Competitive Large Language Model Baseline

## Quick Facts
- arXiv ID: 2408.15079
- Source URL: https://arxiv.org/abs/2408.15079
- Reference count: 34
- 7B parameter LLM trained on 3T tokens without deliberate downstream optimization achieves comparable performance to Llama3 and Qwen1.5 on comprehensive benchmarks

## Executive Summary
BaichuanSEED is a 7B parameter large language model trained on 3 trillion tokens using a novel data collection and deduplication pipeline. The authors focus on broad data collection followed by global multi-granularity deduplication rather than fine-grained data curation. They demonstrate that this approach achieves competitive performance on comprehensive benchmarks while highlighting untapped potential in mathematics and coding tasks. The model exhibits consistency and predictability throughout training, suggesting a robust training process that enables reliable performance forecasting.

## Method Summary
The method employs a data processing pipeline consisting of broad collection to scale up and reweighting to improve quality. Data is collected from CommonCrawl, knowledge-intensive sources, code repositories, and other web sources, totaling 3 trillion tokens. A global multi-granularity deduplication strategy removes duplicate documents and sentences at both levels using MinHash similarity clustering, filtering PII and low-quality data. The model is trained from scratch using transformer decoder architecture with 32 layers, 32 attention heads, and 4,096 hidden dimension size, followed by supervised fine-tuning with 450K samples.

## Key Results
- Achieves comparable performance to commercial models like Llama3 and Qwen1.5 on comprehensive benchmarks
- Demonstrates consistency and predictability throughout training, indicating robust training process
- Highlights untapped potential in tasks like mathematics and coding compared to existing models

## Why This Works (Mechanism)

### Mechanism 1
Global multi-granularity deduplication reduces data redundancy and improves model performance without sacrificing diversity by removing duplicate documents and sentences at both document and sentence levels using MinHash similarity clustering. Filters PII and low-quality data using heuristic rules and FastText classification. Core assumption: Lower frequency data points correlate with higher quality and less redundancy.

### Mechanism 2
Training on a larger, less curated dataset can achieve comparable performance to smaller, highly curated datasets by using broad collection and reweighting instead of fine-grained data selection. Leverages 3T tokens vs Llama3's 15T. Core assumption: The relationship between training tokens and model performance is stable and predictable across different dataset qualities.

### Mechanism 3
Consistency and predictability in training metrics indicate model robustness and enable reliable performance forecasting by tracking loss and benchmark performance across training checkpoints. Uses logarithmic curve fitting to predict future performance. Core assumption: Early training performance trends reliably indicate later performance.

## Foundational Learning

- Concept: Deduplication and data quality
  - Why needed here: Understanding how data deduplication improves model quality while maintaining diversity is crucial for reproducing results
  - Quick check question: What are the two main levels of deduplication used in BaichuanSEED, and how do they differ?

- Concept: Model scaling and performance predictability
  - Why needed here: The paper demonstrates that performance scales predictably with training tokens, which is important for planning resources
  - Quick check question: How does BaichuanSEED demonstrate predictability in its training process?

- Concept: Knowledge-intensive data (KID) impact
  - Why needed here: The paper explores how incorporating KID affects model performance and knowledge density
  - Quick check question: What proportion of KID was found to be optimal for balancing language modeling capabilities and knowledge density?

## Architecture Onboarding

- Component map:
  Data collection pipeline (CommonCrawl, KID, code, etc.) -> Global multi-granularity deduplication system -> Training framework (3T tokens, 16K sequence length, 224 global batch size) -> Supervised fine-tuning (SFT) pipeline

- Critical path:
  1. Data collection and preprocessing
  2. Global multi-granularity deduplication
  3. Model pre-training
  4. Supervised fine-tuning

- Design tradeoffs:
  - Broad collection vs. fine-grained curation
  - Larger dataset with deduplication vs. smaller curated dataset
  - Consistency in training vs. potential overfitting to specific tasks

- Failure signatures:
  - Inconsistent performance trends across checkpoints
  - Erratic training loss behavior
  - Significant performance drop on downstream tasks

- First 3 experiments:
  1. Reproduce deduplication pipeline on a small subset of data and verify reduction in duplicate content
  2. Train a small model (2B) with different KID proportions to identify optimal balance
  3. Compare performance of a model trained with and without deduplication on a held-out validation set

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal proportion of knowledge-intensive data (KID) for different model scales and training durations? The paper discusses empirical experiments on a 2B model exploring KID proportions from 0% to 50%, finding 25% optimal for language modeling capabilities, but this was limited to a 2B model and 100B training tokens.

### Open Question 2
How does the absence of synthetic data during pre-training affect long-term model capabilities compared to models trained with synthetic data? The paper deliberately excludes synthetic data to maintain model purity, noting that synthetic data might improve benchmark performance but could lead to loss of instruction-following ability.

### Open Question 3
What is the relationship between data deduplication granularity and model performance across different domains? The paper introduces a global multi-granularity deduplication strategy that significantly reduces data volume while improving model quality, but doesn't systematically study different granularities.

## Limitations

- Data quality assessment limitations: The claim that higher frequency data correlates with negative impact lacks rigorous empirical validation and quantitative demonstration of deduplication effectiveness
- Performance attribution gap: While achieving comparable performance, the paper lacks ablation studies to isolate the contribution of each architectural or data processing component
- Scaling predictability constraints: Consistency claims rely on observed trends without out-of-distribution validation, and the relationship between training tokens and performance is asserted but not rigorously tested

## Confidence

- Data Processing Claims: High - well-described methodology with clear implementation details
- Benchmark Performance Claims: Medium - comprehensive benchmarks presented but lack ablation studies and controlled experiments
- Training Consistency Claims: Low - predictability claims based on internal observations without external validation

## Next Checks

1. Implement metrics to measure actual redundancy reduction (Jaccard similarity, n-gram overlap) and diversity preservation (vocabulary coverage, topic distribution) to validate deduplication effectiveness

2. Train parallel models with systematic variations (full deduplication, document-only, sentence-only, no deduplication) to isolate component contributions to performance

3. Conduct controlled experiments training models on different token counts (1T, 2T, 3T, 4T) with identical data quality to verify whether observed performance trends hold across scales