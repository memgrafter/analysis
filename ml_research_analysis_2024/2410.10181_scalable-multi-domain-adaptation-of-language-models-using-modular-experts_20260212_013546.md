---
ver: rpa2
title: Scalable Multi-Domain Adaptation of Language Models using Modular Experts
arxiv_id: '2410.10181'
source_url: https://arxiv.org/abs/2410.10181
tags:
- mode
- training
- expert
- experts
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Modular Domain Experts (MoDE) addresses the challenge of efficiently
  adapting large language models to multiple specialized domains while preserving
  general capabilities and minimizing computational costs. MoDE introduces a mixture-of-experts
  architecture where domain-specific experts are trained independently and composed
  together through a lightweight gating mechanism.
---

# Scalable Multi-Domain Adaptation of Language Models using Modular Experts

## Quick Facts
- arXiv ID: 2410.10181
- Source URL: https://arxiv.org/abs/2410.10181
- Reference count: 27
- Primary result: MoDE achieves 77.5% accuracy on coding and 77.3% on math tasks while improving English retention by 1.65% compared to full fine-tuning

## Executive Summary
MoDE (Modular Domain Experts) introduces a mixture-of-experts architecture for efficiently adapting large language models to multiple specialized domains while preserving general capabilities. The method trains domain-specific experts independently with a frozen backbone, then composes them through a lightweight gating mechanism. MoDE demonstrates superior performance compared to LoRA on math and coding tasks while maintaining better retention of general English capabilities. The architecture also enables flexible sharding configurations that accelerate training by up to 38% through parallel execution.

## Method Summary
MoDE uses a two-stage training procedure where domain-specific experts are first trained independently on their respective datasets with a frozen backbone, then composed together through a lightweight gating mechanism fine-tuned on mixed domain data. The architecture consists of a frozen pre-trained language model backbone with domain-specialized expert blocks containing full transformer layers. The gating function uses a simple linear layer with softmax to route inputs to appropriate experts. MoDE achieves parameter efficiency by only updating the expert layers and gating weights during training, while maintaining expressiveness through full transformer layers as experts.

## Key Results
- Achieves 77.5% accuracy on coding tasks and 77.3% on math tasks
- Improves English retention by 1.65% compared to full fine-tuning
- Scales more effectively with additional training data and parameters than LoRA, showing 1.68% and 2.12% improvements on math and coding respectively
- Enables training speed improvements of up to 38% through MPMD parallel execution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Freezing backbone layers during expert training prevents catastrophic forgetting.
- Mechanism: By keeping the backbone frozen, the model retains its general language capabilities while only updating the specialized expert layers for domain-specific tasks.
- Core assumption: The backbone contains sufficient general knowledge that doesn't degrade when frozen during domain-specific adaptation.
- Evidence anchors:
  - [abstract] "freezing the backbone not only reduces the computational cost of backpropagation but also helps mitigate the issue of catastrophic forgetting"
  - [section] "Freezing the backbone not only reduces the computational cost of backpropagation but also helps mitigate the issue of catastrophic forgetting."
  - [corpus] Weak - corpus lacks specific evidence on backbone freezing effects.
- Break condition: If the backbone becomes outdated or if the general capabilities degrade over time due to distribution shifts in the data.

### Mechanism 2
- Claim: MoDE scales more effectively with additional training data and parameters compared to LoRA.
- Mechanism: MoDE uses full transformer layers as experts, providing more expressive capacity than the low-rank matrices in LoRA. This allows MoDE to better leverage increased training data and parameters for improved accuracy.
- Core assumption: Transformer layers provide more expressive capacity than low-rank adaptations for complex domain-specific tasks.
- Evidence anchors:
  - [abstract] "In contrast to standard low-rank adaptation methods, each MoDE expert consists of several transformer layers which scale better with more training examples and larger parameter counts."
  - [section] "Unlike MoDE, E models are primarily used to enhance pre-training performance" and "MoDE applies MoE at the transformer-level, offering a scalable and expressive solution for multi-domain adaptation."
  - [corpus] Weak - corpus neighbors don't directly compare scaling properties with LoRA.
- Break condition: When the domain-specific tasks don't require high expressiveness, or when computational efficiency becomes more critical than accuracy gains.

### Mechanism 3
- Claim: Parallel execution of backbone and expert blocks through MPMD enables faster training.
- Mechanism: By assigning backbone and experts to separate meshes, MoDE reduces communication overhead between devices. This allows simultaneous computation of different model components, improving overall training efficiency.
- Core assumption: The communication overhead reduction from MPMD outweighs the costs of resharding between meshes.
- Evidence anchors:
  - [abstract] "Moreover, MoDE's architecture enables flexible sharding configurations and improves training speeds by up to 38% over state-of-the-art distributed training configurations."
  - [section] "We exploit parallelism in MoDE's structure to enable flexible sharding configurations supported by the MPMD model of computation."
  - [corpus] Weak - corpus lacks specific evidence on MPMD performance gains.
- Break condition: When the size of expert blocks becomes too small or too large relative to backbone blocks, causing underutilization of accelerators.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) Architecture
  - Why needed here: MoDE builds on MoE principles by adding domain-specialized experts while maintaining a shared backbone, enabling both specialization and generalization.
  - Quick check question: What is the key difference between standard MoE and MoDE in terms of expert deployment?

- Concept: Catastrophic Forgetting
  - Why needed here: Understanding how MoDE prevents catastrophic forgetting is crucial for maintaining general capabilities while adapting to specialized domains.
  - Quick check question: How does freezing the backbone help prevent catastrophic forgetting in MoDE?

- Concept: Parameter-Efficient Fine-Tuning
  - Why needed here: MoDE is compared against parameter-efficient methods like LoRA, so understanding these approaches is essential for evaluating MoDE's advantages.
  - Quick check question: What are the main limitations of LoRA that MoDE aims to address?

## Architecture Onboarding

- Component map:
  Backbone (frozen) -> Expert Blocks (domain-specific) -> Gating Function (linear layer + softmax) -> Sharding Configuration (MPMD)

- Critical path:
  1. Initialize backbone from pre-trained PLM and freeze weights
  2. Train individual experts on their respective domains
  3. Compose experts using lightweight fine-tuning on multi-domain data
  4. Configure sharding for parallel execution

- Design tradeoffs:
  - Expressiveness vs. efficiency: More expert layers improve accuracy but increase computational cost
  - Specialization vs. generalization: More domain experts improve domain performance but may impact retention
  - Sharding complexity vs. training speed: MPMD configurations can speed up training but add complexity

- Failure signatures:
  - Poor domain performance: May indicate insufficient expert layers or inadequate training data
  - Loss of general capabilities: Could suggest backbone is not properly frozen or gating function is malfunctioning
  - Slow training: Might indicate suboptimal sharding configuration or communication bottlenecks

- First 3 experiments:
  1. Compare MoDE with different numbers of expert layers on a single domain to find the sweet spot between expressiveness and efficiency
  2. Test various sharding configurations (MPMD vs SPMD) on a small cluster to measure training speed improvements
  3. Evaluate retention performance by training MoDE on domain data and testing on general language tasks

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but based on the analysis, key unresolved questions include:

### Open Question 1
- Question: How does MoDE's performance scale when composed of more than two domain-specific experts, and what is the optimal number of experts for different domain combinations?
- Basis in paper: [explicit] The paper mentions "we evaluate MoDE configurations with one or two randomly initialized experts" and discusses composing experts for multi-domain adaptation, but doesn't explore configurations with more than two experts
- Why unresolved: The paper only tests MoDE with up to two experts, leaving open questions about scalability to many domains and the point of diminishing returns
- What evidence would resolve it: Systematic experiments varying the number of experts from 2 to 10+ across different domain combinations, measuring accuracy, training efficiency, and parameter efficiency

### Open Question 2
- Question: What is the impact of different gating function designs on MoDE's performance, particularly sequence-level routing versus the current token-level routing?
- Basis in paper: [explicit] The paper states "we use a simple token-level gating function" and mentions "future work could explore more advanced designs, such as sparse gating and sequence routing"
- Why unresolved: The paper only evaluates one simple gating design, despite acknowledging that more sophisticated routing could improve performance
- What evidence would resolve it: Comparative experiments testing various gating mechanisms including sparse routing, sequence-level routing, and attention-based gating, measuring both accuracy and computational efficiency

### Open Question 3
- Question: How does MoDE's parameter efficiency compare to LoRA when adapting to domains with very small or very large training datasets (beyond the range tested in the paper)?
- Basis in paper: [inferred] The paper shows MoDE scales better than LoRA for medium dataset sizes but only tests up to ~1e9 examples and doesn't explore extremely small datasets
- Why unresolved: The scalability experiments only cover a moderate range of dataset sizes, leaving uncertainty about MoDE's behavior at dataset extremes
- What evidence would resolve it: Experiments testing adaptation on datasets ranging from hundreds to billions of examples, measuring accuracy, parameter efficiency, and training stability across the full spectrum

### Open Question 4
- Question: What are the theoretical limitations of MoDE's scalability with respect to the number of parameters and experts, and at what point does the gating function become a bottleneck?
- Basis in paper: [explicit] The paper shows MoDE scales better than LoRA with parameters and training examples, but doesn't analyze theoretical limitations or the scaling properties of the gating function
- Why unresolved: While empirical scalability is demonstrated, there's no analysis of the theoretical bounds or potential bottlenecks in the architecture
- What evidence would resolve it: Theoretical analysis of MoDE's scaling properties, computational complexity analysis of the gating function, and empirical studies pushing MoDE to extreme scales (billions of parameters, hundreds of experts)

## Limitations
- Experimental scope limited to only two specialized domains (math and coding) and one general domain
- Limited architectural details about the backbone PLM beyond parameter count
- Uncertainty about computational overhead and conditions required for the claimed 38% training speedup
- Retention measurement only compared against full-parameter fine-tuning, not other parameter-efficient methods

## Confidence
- High confidence: Claims about MoDE's architecture and training procedure are well-specified and reproducible
- Medium confidence: Comparative performance claims against LoRA may favor MoDE's full transformer layer experts for specific task types
- Low confidence: Generalizability of training speedup to different hardware configurations and the conditions required to achieve it are not fully established

## Next Checks
- Check 1: Cross-domain generalization - Evaluate MoDE on a broader set of domains to test whether performance advantages extend beyond math and coding tasks
- Check 2: Retention comparison baseline - Compare MoDE's retention performance against other parameter-efficient methods on general language tasks
- Check 3: Scaling sensitivity analysis - Systematically vary model size, expert layer count, and dataset size to determine conditions under which MoDE's scaling advantages hold