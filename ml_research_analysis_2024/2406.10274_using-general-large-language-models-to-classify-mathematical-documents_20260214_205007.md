---
ver: rpa2
title: Using General Large Language Models to Classify Mathematical Documents
arxiv_id: '2406.10274'
source_url: https://arxiv.org/abs/2406.10274
tags:
- arxiv
- classification
- classifications
- primary
- mathematical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored using general large language models (LLMs)
  to classify mathematical documents according to the MSC 2020 system. The experiment
  analyzed 56 recent arXiv preprints using ChatGPT 3.5, providing only titles and
  abstracts as input.
---

# Using General Large Language Models to Classify Mathematical Documents

## Quick Facts
- arXiv ID: 2406.10274
- Source URL: https://arxiv.org/abs/2406.10274
- Reference count: 15
- LLMs matched arXiv classifications in 60% of cases; showed promise for mathematical document classification

## Executive Summary
This study explored using general large language models to classify mathematical documents according to the MSC 2020 system. The experiment analyzed 56 recent arXiv preprints using ChatGPT 3.5, providing only titles and abstracts as input. Results showed that in 60% of cases, the LLM produced primary classifications matching those on arXiv. However, in 40% of cases where classifications differed, detailed examination revealed the LLM suggestions were actually better than the arXiv classifications. The study concludes that LLMs show promise for mathematical document classification despite occasional hallucinations, warranting further investigation.

## Method Summary
The researchers tested ChatGPT 3.5's ability to classify mathematical documents using only titles and abstracts from 56 recent arXiv preprints. The LLM was prompted to provide primary MSC 2020 classifications for each document. Results were compared against arXiv's existing classifications, with cases of disagreement examined in detail to determine which classification was more appropriate.

## Key Results
- LLM classifications matched arXiv's in 60% of cases when using only titles and abstracts
- In 40% of differing cases, LLM suggestions were judged to be better than arXiv's classifications
- Study demonstrates LLMs can effectively classify mathematical documents despite occasional hallucinations

## Why This Works (Mechanism)
General LLMs trained on diverse web text can capture semantic patterns and mathematical concepts through their broad pretraining, enabling them to understand the content of mathematical abstracts and map them to appropriate MSC categories. The models' ability to process natural language descriptions and recognize domain-specific terminology allows them to perform classification tasks even without specialized mathematical training.

## Foundational Learning
- **MSC 2020 Classification System**: The Mathematics Subject Classification system organizes mathematical research into hierarchical categories; needed to evaluate classification accuracy; quick check: verify classification matches expected MSC hierarchy
- **Mathematical Abstract Structure**: Understanding how mathematical research is summarized in abstracts; needed to assess whether titles/abstracts contain sufficient information; quick check: identify key mathematical concepts in sample abstracts
- **LLM Text Processing**: How LLMs extract meaning from text input; needed to understand classification methodology; quick check: test LLM on simple classification tasks with clear inputs

## Architecture Onboarding
**Component Map**: arXiv preprints -> Title/Abstract extraction -> ChatGPT 3.5 -> MSC classification output -> Comparison with arXiv classifications

**Critical Path**: Document selection → Text extraction → LLM classification → Result comparison → Expert evaluation

**Design Tradeoffs**: Using only titles and abstracts reduces input complexity but may miss important context; ChatGPT 3.5 is accessible but may not be optimal for mathematical tasks; comparing only to arXiv classifications provides limited ground truth

**Failure Signatures**: Hallucinations in classifications, misclassification due to insufficient abstract information, systematic bias in LLM's understanding of mathematical domains

**First Experiments**:
1. Test LLM classification accuracy with full document text versus titles/abstracts
2. Evaluate multiple LLMs (including specialized models) on the same classification task
3. Assess inter-rater reliability among experts evaluating classification quality

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size of 56 documents may not be representative of broader mathematical literature
- Limited to titles and abstracts without full document content for classification
- Comparison only to arXiv classifications without access to original author submissions

## Confidence
- **Medium** confidence in 60% match rate with arXiv classifications (limited sample size)
- **Medium** confidence in LLM suggestions being "better" in disagreements (subjective expert judgment)
- **Medium** confidence in overall conclusion about LLMs showing promise (preliminary results)

## Next Checks
1. Test the same methodology on a larger, more diverse corpus of mathematical documents including full-text content to assess scalability and robustness
2. Implement a blinded evaluation where multiple independent experts assess classification quality without knowing which system (arXiv or LLM) produced each label
3. Compare LLM performance against both arXiv classifications and the original author-submitted classifications (when available) to establish a more reliable ground truth for evaluation