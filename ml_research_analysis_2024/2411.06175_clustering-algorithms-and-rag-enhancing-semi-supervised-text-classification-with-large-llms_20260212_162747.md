---
ver: rpa2
title: Clustering Algorithms and RAG Enhancing Semi-Supervised Text Classification
  with Large LLMs
arxiv_id: '2411.06175'
source_url: https://arxiv.org/abs/2411.06175
tags:
- data
- dataset
- augmentation
- llms
- clustering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Clustering, Labeling, then Augmenting framework
  that significantly enhances performance in Semi-Supervised Text Classification (SSTC)
  tasks, effectively addressing the challenge of vast datasets with limited labeled
  examples. Unlike traditional SSTC approaches that rely on a predefined small set
  of labeled data to generate pseudo-labels for the unlabeled data, this framework
  innovatively employs clustering to select representative "landmarks" for labeling.
---

# Clustering Algorithms and RAG Enhancing Semi-Supervised Text Classification with Large LLMs

## Quick Facts
- arXiv ID: 2411.06175
- Source URL: https://arxiv.org/abs/2411.06175
- Authors: Shan Zhong; Jiahao Zeng; Yongxin Yu; Bohong Lin
- Reference count: 40
- Primary result: Achieves 95.41% accuracy on Reuters and 82.43% on WOS datasets using clustering-based landmark selection and augmentation framework

## Executive Summary
This paper introduces a novel framework for semi-supervised text classification that combines clustering algorithms, manual labeling of representative landmarks, and multiple augmentation techniques including Retrieval-Augmented Generation (RAG). Unlike traditional approaches that rely on pseudo-labeling, the method selects diverse representative samples through clustering, then uses these landmarks as anchors for various augmentation techniques. The approach significantly reduces labeling costs while maintaining high classification accuracy, achieving state-of-the-art results on complex multi-class text classification tasks.

## Method Summary
The framework operates through a three-stage process: first, clustering algorithms (Gaussian Mixture Models or Hierarchical Clustering) partition the dataset and select representative "landmarks" that capture dataset diversity; second, human experts manually label these landmarks; third, the labeled landmarks undergo augmentation through three complementary techniques - WordNet synonym replacement, LLM rewriting, and RAG generation - to create synthetic labeled data. A smaller LLM (Qwen2.5-0.5B) is then fine-tuned on this augmented dataset. The approach avoids traditional pseudo-labeling while achieving high accuracy with minimal labeled examples.

## Key Results
- Achieves 95.41% accuracy on Reuters dataset with 300-600 labeled landmarks
- Achieves 82.43% accuracy on Web of Science dataset with 134 categories
- Outperforms random selection baselines by significant margins after augmentation
- Maintains high performance across datasets with over 100 categories

## Why This Works (Mechanism)

### Mechanism 1
- Clustering identifies representative landmarks that preserve dataset diversity and reduce redundancy.
- By applying clustering algorithms like Gaussian Mixture Models or Hierarchical Clustering, the method selects cluster centers or representative samples that encapsulate the core characteristics of each cluster, ensuring that the labeled subset captures the full diversity of the dataset without redundancy.
- Core assumption: Clustering algorithms effectively partition the dataset into meaningful groups where cluster centers or representative samples accurately reflect the dataset's diversity.
- Evidence anchors: Abstract states the framework "employs clustering to select representative 'landmarks' for labeling" and section 3 describes using cluster centers as landmarks.
- Break condition: If clustering algorithms fail to partition the dataset meaningfully, the selected landmarks may not represent the dataset's diversity, leading to poor model performance.

### Mechanism 2
- RAG augmentation with representative landmarks reduces hallucinations and improves semantic coherence.
- RAG uses the labeled landmarks as retrieval data to generate synthetic labeled data, grounding the LLM's generation in contextually relevant and accurately labeled information, thereby reducing hallucinations and improving the semantic coherence of the augmented data.
- Core assumption: The labeled landmarks serve as effective retrieval data for RAG, providing sufficient context for the LLM to generate semantically coherent and accurate synthetic data.
- Evidence anchors: Abstract mentions RAG generates synthetic labeled data, and section 2 describes RAG's ability to improve answer quality and reduce hallucinations.
- Break condition: If the landmarks do not provide sufficient context or are not accurately labeled, RAG may still generate hallucinations or semantically incoherent data.

### Mechanism 3
- Combining augmentation techniques with varying vocabulary overlap enhances model performance.
- The framework combines data augmented with WordNet (high vocabulary overlap), LLM rewriting (moderate overlap), and RAG (low overlap) to create a diverse training set that covers a broad range of case types while reinforcing predominant scenarios.
- Core assumption: Integrating data with varying levels of vocabulary overlap ensures that the model learns to handle both common and diverse scenarios effectively.
- Evidence anchors: Section 4 describes how vocabulary overlap decreases from RAG to LLM rewrite to WordNet, and section 5 states combining documents with varying similarity levels boosts prediction accuracy.
- Break condition: If the integration of data with varying vocabulary overlap is not balanced, the model may overfit to certain scenarios or fail to generalize effectively.

## Foundational Learning

- Concept: Clustering algorithms (e.g., K-means, Hierarchical Clustering, Gaussian Mixture Models)
  - Why needed here: To partition the dataset into meaningful groups and select representative landmarks that capture the dataset's diversity without redundancy.
  - Quick check question: How do clustering algorithms like K-means and Hierarchical Clustering differ in their approach to grouping data?

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: To generate synthetic labeled data by retrieving relevant information from external knowledge bases and incorporating it into the LLM's input, reducing hallucinations and improving semantic coherence.
  - Quick check question: What is the role of retrieval in RAG, and how does it improve the quality of generated data?

- Concept: Text augmentation techniques (e.g., synonym replacement, LLM rewriting)
  - Why needed here: To expand the labeled dataset by creating diverse variations of the text while preserving semantic meaning, enhancing the model's ability to generalize.
  - Quick check question: How do different text augmentation techniques (e.g., synonym replacement vs. LLM rewriting) affect the vocabulary overlap and semantic coherence of the augmented data?

## Architecture Onboarding

- Component map: Data preprocessing -> Clustering -> Landmark selection -> Manual labeling -> Augmentation (WordNet, LLM rewrite, RAG) -> Model training (Qwen2.5-0.5B)
- Critical path: Clustering → Landmark selection → Labeling → Augmentation → Model training
- Design tradeoffs:
  - Clustering algorithm choice: GMM vs. Hierarchical Clustering (computational efficiency vs. interpretability)
  - Augmentation technique balance: WordNet (high overlap) vs. LLM rewriting (moderate overlap) vs. RAG (low overlap)
  - Label extraction: Regular expressions vs. more sophisticated parsing methods
- Failure signatures:
  - Poor clustering results: Homogeneous clusters, low silhouette scores
  - Inaccurate labels: Mislabeled landmarks, hallucinatory RAG outputs
  - Ineffective augmentation: Low diversity in augmented data, poor model performance
- First 3 experiments:
  1. Compare clustering algorithms (GMM vs. Hierarchical Clustering) on a small subset of the dataset to assess cluster quality and computational efficiency.
  2. Evaluate the effectiveness of different landmark selection methods (cluster centers vs. LLM-selected representatives) by comparing model performance on a validation set.
  3. Test the impact of combining augmentation techniques with varying vocabulary overlap by training models on different augmented datasets and comparing their performance on a test set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of clustering-based landmark selection compare to more sophisticated active learning approaches for selecting representative samples?
- Basis in paper: The paper compares landmark selection via clustering to random selection and shows clustering outperforms random selection after augmentation, but does not compare to active learning methods.
- Why unresolved: The paper only benchmarks against random selection, leaving open whether clustering is optimal or if active learning could yield even better representative samples.
- What evidence would resolve it: Direct comparison experiments between clustering-based landmark selection and active learning methods (e.g., uncertainty sampling, query-by-committee) on the same datasets and metrics.

### Open Question 2
- Question: What is the optimal balance between different augmentation techniques (WordNet, LLM rewrite, RAG) for maximizing classification accuracy across different text domains?
- Basis in paper: The paper shows combining all three augmentation techniques yields best results, but does not systematically explore the optimal weighting or combination ratios for different domain types.
- Why unresolved: While the combined approach works well, the paper doesn't determine if certain domains benefit more from specific augmentation types or what the optimal proportions should be.
- What evidence would resolve it: Systematic ablation studies varying the proportions of each augmentation type across multiple diverse text domains, measuring classification accuracy for each combination.

### Open Question 3
- Question: How does the quality of synthetic data generated via RAG vary with the size and diversity of the reference document set used for retrieval?
- Basis in paper: The paper uses 5 labeled landmarks from nearest clusters plus 3 unlabeled samples from same cluster for RAG augmentation, but doesn't explore how changing these numbers affects output quality.
- Why unresolved: The paper fixes the reference set size but doesn't investigate whether larger or more diverse reference sets improve or degrade the quality and diversity of generated synthetic data.
- What evidence would resolve it: Experiments varying the number of reference documents (both labeled and unlabeled) used in RAG prompts, measuring both output quality metrics (e.g., label accuracy, vocabulary overlap) and downstream classification performance.

## Limitations

- Lack of comparative baselines against traditional semi-supervised learning methods (self-training, co-training) to establish genuine advantages
- Implementation opacity with key RAG prompts, LLM rewriting configurations, and fine-tuning procedures not fully specified
- Limited ablation studies that don't systematically measure the marginal contribution of each component to overall performance

## Confidence

- High confidence: Clustering effectively selects diverse landmarks and reduces labeling burden (supported by multiple clustering quality metrics and strong baseline performance)
- Medium confidence: RAG augmentation reduces hallucinations and improves semantic coherence (mechanism is sound but lacks direct empirical comparison with baseline augmentation)
- Medium confidence: Combined augmentation with varying vocabulary overlap improves model performance (theoretical justification is strong but empirical validation is incomplete)

## Next Checks

1. Implement direct comparison with self-training baseline: Run the same experiments using traditional self-training (pseudo-labeling) on the same datasets to quantify the specific advantage of the clustering-augmentation approach versus standard semi-supervised methods

2. Systematic ablation of augmentation techniques: Train separate models using only WordNet augmentation, only LLM rewriting, and only RAG augmentation to measure the marginal contribution of each technique and validate the claimed synergy

3. Cluster quality validation study: Compare classification performance when using landmarks selected via clustering versus random sampling of the same number of labeled examples to directly assess whether clustering provides meaningful advantages over simple random selection