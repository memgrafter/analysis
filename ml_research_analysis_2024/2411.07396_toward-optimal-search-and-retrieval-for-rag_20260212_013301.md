---
ver: rpa2
title: Toward Optimal Search and Retrieval for RAG
arxiv_id: '2411.07396'
source_url: https://arxiv.org/abs/2411.07396
tags:
- documents
- recall
- gold
- citation
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how the retriever component of a RAG system
  affects downstream QA performance. The authors conduct experiments with two retrievers
  (BGE-base and ColBERTv2) and two LLMs (Mistral and LLaMA) on QA and attributed QA
  tasks.
---

# Toward Optimal Search and Retrieval for RAG
## Quick Facts
- arXiv ID: 2411.07396
- Source URL: https://arxiv.org/abs/2411.07396
- Reference count: 40
- Key outcome: Including one gold document significantly improves QA accuracy in RAG systems

## Executive Summary
This paper investigates how the retriever component in RAG systems impacts downstream question-answering performance. The authors conduct experiments comparing two retrievers (BGE-base and ColBERTv2) and two LLMs (Mistral and LLaMA) on QA and attributed QA tasks. Their findings reveal that even retrieving a single relevant document substantially improves QA accuracy, and that using approximate nearest neighbor search with lower recall has only minimal impact on performance while offering computational savings.

## Method Summary
The authors evaluate RAG performance by systematically varying the number and relevance of retrieved documents across different retriever-LLM combinations. They use the QMSum dataset from FreshQA and measure performance using Exact Match (EM) for QA tasks and ROUGE scores for attributed QA. The study includes controlled experiments where gold documents are selectively removed or replaced with noisy documents of varying relevance levels. They also examine the trade-offs of using approximate nearest neighbor search (ANNS) with different recall settings to understand the impact on retrieval quality versus computational efficiency.

## Key Results
- Including just one gold document significantly improves QA accuracy compared to retrieval without relevant documents
- Performance correlates positively with the number of gold documents retrieved
- Using ANNS with lower recall settings has only minor impact on QA performance while saving retrieval time and memory
- Injecting noisy documents of varying relevance degrades correctness compared to gold-only retrieval, contrary to prior work

## Why This Works (Mechanism)
The retriever's ability to find relevant documents is crucial because RAG systems rely on retrieved context to generate accurate answers. When gold documents are present, the LLM can ground its responses in verified information, leading to better performance. The minimal impact of ANNS with lower recall suggests that even imperfect retrieval can capture sufficient relevant information for the LLM to perform well. The degradation from noisy documents indicates that RAG systems are sensitive to document quality, with irrelevant or misleading context potentially confusing the LLM's reasoning process.

## Foundational Learning
- **Retriever architectures**: Different models (BGE-base, ColBERTv2) use distinct approaches to match queries with relevant documents; understanding their strengths and weaknesses is essential for system design
- **Retrieval evaluation metrics**: EM, ROUGE, and recall metrics measure different aspects of retrieval quality and downstream performance
- **Approximate nearest neighbor search**: ANNS algorithms trade perfect recall for computational efficiency; understanding this trade-off is critical for production systems
- **Document relevance hierarchies**: Gold vs. noisy documents create different quality levels that affect LLM performance differently
- **RAG system components**: Retriever, reranker, and generator form a pipeline where each stage affects overall quality

## Architecture Onboarding
**Component map:** Document Store -> Retriever -> Reranker (optional) -> LLM Generator -> QA Output
**Critical path:** Query -> Retriever -> Document Selection -> Context Generation -> LLM Response
**Design tradeoffs:** Recall vs. precision in retrieval, computational cost vs. quality, latency vs. accuracy
**Failure signatures:** Low recall leads to missing relevant documents; low precision introduces noise; ANNS may miss some relevant documents but maintains overall quality
**First experiments:**
1. Compare EM scores when varying the number of retrieved documents from 0 to 10
2. Measure performance degradation when replacing gold documents with noisy documents of different relevance levels
3. Evaluate the trade-off between ANNS recall settings and retrieval time/memory usage

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on a specific subset of FreshQA (QMSum), limiting generalizability to other question types
- Only two retriever models and two LLMs are tested, leaving uncertainty about results with other architectures
- Attribution task evaluation is inconclusive due to limited results
- ANNS trade-off analysis is based on a single configuration, not exploring the full parameter space
- Does not consider the impact of different query types (multi-hop, conversational) on performance degradation

## Confidence
- High confidence: Including one gold document significantly improves QA accuracy
- Medium confidence: ANNS with lower recall has minimal impact on QA performance
- Low confidence: Conclusions about attribution task performance and generalizability to other retriever/LLM combinations

## Next Checks
1. Replicate ANNS trade-off experiments across multiple configurations and dataset splits to verify robustness
2. Extend evaluation to additional retriever models and larger LLMs to test generalizability
3. Conduct ablation studies on document relevance levels to better understand the relationship between document quality and downstream performance degradation