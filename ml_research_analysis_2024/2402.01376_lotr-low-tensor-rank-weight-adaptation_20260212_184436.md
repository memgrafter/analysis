---
ver: rpa2
title: 'LoTR: Low Tensor Rank Weight Adaptation'
arxiv_id: '2402.01376'
source_url: https://arxiv.org/abs/2402.01376
tags:
- lotr
- rank
- lora
- tensor
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LoTR (Low Tensor Rank), a novel parameter-efficient
  fine-tuning method for large language models (LLMs) based on Transformers. LoTR
  generalizes and extends the idea of LoRA by representing the gradient update to
  parameters in a form of tensor decomposition.
---

# LoTR: Low Tensor Rank Weight Adaptation

## Quick Facts
- arXiv ID: 2402.01376
- Source URL: https://arxiv.org/abs/2402.01376
- Reference count: 39
- One-line result: LoTR achieves competitive GLUE performance with fewer parameters than LoRA through tensor decomposition

## Executive Summary
LoTR (Low Tensor Rank) is a novel parameter-efficient fine-tuning method for large language models that extends LoRA by representing gradient updates as low-rank tensor decompositions rather than individual low-rank matrices. The method constructs adapters as products of three matrices, sharing left and right multipliers across transformer layers while maintaining small layer-specific core tensors. This tensor structure enables better parameter efficiency than LoRA, especially for deep models, by exploiting redundancy across layers and decoupling compression from model size.

## Method Summary
LoTR applies low-rank tensor decomposition to weight updates in transformer attention modules, specifically targeting query and value matrices. The method uses Tucker2 decomposition to jointly compress weight updates across layers, sharing factor matrices A and B while maintaining layer-specific core tensors S_s. This results in O(L r² + d r) trainable parameters compared to LoRA's O(L d r), where L is the number of layers, r is the rank, and d is the weight dimension. The approach is implemented on pre-trained RoBERTa models and evaluated on the GLUE benchmark using AdamW optimizer with learning rate search over 20 epochs.

## Key Results
- LoTR with rank 32 and 74k parameters achieves Matthews correlation of 60.5 on CoLA
- LoRA with rank 8 and 295k parameters achieves Matthews correlation of 61.1 on CoLA
- LoTR exhibits better parameter efficiency than LoRA as transformer depth increases
- Core tensor size depends only on rank and number of layers, not original weight dimensions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LoTR reduces parameters by jointly representing weight matrices as low-rank tensors rather than individual low-rank matrices
- Mechanism: Collects all weight matrices of a specific type into a 3D tensor and decomposes using Tucker2 decomposition, sharing left and right factor matrices across layers while keeping small core tensors per layer
- Core assumption: Weight updates across transformer layers share common low-dimensional structure that can be captured by sharing factor matrices
- Evidence anchors: Abstract mentions tensor structure from sharing multipliers; section 2.3 discusses tensor representation of corrections
- Break condition: If weight updates across layers are highly dissimilar or orthogonal, shared factors provide little compression benefit

### Mechanism 2
- Claim: Core tensor in LoTR is independent of original weight dimension, allowing arbitrary compression
- Mechanism: In Tucker2 decomposition, core tensor has shape (r x L x r) where L is number of layers, making its size depend only on rank r and layers L, not input/output dimension d
- Core assumption: Essential low-rank structure of update can be captured without dependence on full weight dimension
- Evidence anchors: Abstract states core tensor doesn't depend on original weight dimension; section 2.3 repeats this claim
- Break condition: If rank r must be large to capture necessary information, core tensor size grows and independence advantage diminishes

### Mechanism 3
- Claim: LoTR achieves better parameter efficiency than LoRA for deep models due to O(L r² + d r) scaling
- Mechanism: LoRA requires O(L d r) parameters for individual rank-r matrices per layer, while LoTR shares factor matrices requiring only O(L r²) for core tensors plus O(d r) for shared factors
- Core assumption: Sharing factor matrices across layers doesn't significantly harm expressivity while reducing parameter count
- Evidence anchors: Abstract mentions better parameter efficacy with increasing transformer blocks; section 2.3 provides the O(L r² + d r) formula
- Break condition: If r must be large to maintain performance, r² term dominates and efficiency gain over LoRA is lost

## Foundational Learning

- Concept: Low-rank matrix factorization
  - Why needed here: LoRA and LoTR both rely on approximating weight updates with low-rank matrices or tensors to reduce parameters
  - Quick check question: What is the rank of a matrix, and how does a low-rank approximation reduce the number of parameters?

- Concept: Tensor decompositions (Tucker, Tensor Train)
  - Why needed here: LoTR extends low-rank ideas to tensors, using Tucker2 and potentially Tensor Train decompositions to jointly compress multiple weight matrices
  - Quick check question: How does Tucker2 decomposition represent a 3D tensor with fewer parameters than storing it directly?

- Concept: Parameter-efficient fine-tuning methods
  - Why needed here: LoTR is a parameter-efficient method; understanding LoRA, adapters, and other PEFT approaches provides context for its design and evaluation
  - Quick check question: How does LoRA differ from full fine-tuning in terms of trainable parameters and computational cost?

## Architecture Onboarding

- Component map: Input batch X -> Original frozen weights W -> LoTR adapter (shared factors A, B and layer-specific cores S_s) -> Output Y = Aff(X) + α X A S_s B^T

- Critical path:
  1. Forward pass: Compute X A, multiply by S_s, multiply by B^T, add to original layer output
  2. Backward pass: Gradients flow through A, B, and S_s; A and B are shared so gradients accumulate across layers
  3. Parameter update: Update A, B, and all S_s using optimizer

- Design tradeoffs:
  - Shared factors A and B reduce parameters but may limit layer-specific adaptation
  - Small rank r saves parameters but may underfit if r is too small
  - Larger rank increases expressivity but reduces parameter efficiency gain

- Failure signatures:
  - Training instability or divergence: May indicate rank too small or learning rate too high
  - Poor downstream performance: Could mean rank too small or shared factors too restrictive
  - Memory usage higher than expected: Check if rank or number of layers is too large

- First 3 experiments:
  1. Apply LoTR to a single transformer layer on a small dataset (e.g., MRPC) with varying ranks to see parameter efficiency vs performance
  2. Compare LoTR and LoRA on RoBERTabase for CoLA task with matched parameter counts to validate efficiency claims
  3. Ablation: Apply LoTR to only query matrices vs both query and value matrices to assess impact of adapter placement

## Open Questions the Paper Calls Out

The paper raises questions about the scaling law of LoTR's performance with respect to its rank parameter and how this compares to other parameter-efficient fine-tuning methods. It mentions that target metrics heavily depend on the total number of adjustable parameters and that there might be a scaling law, but doesn't provide comprehensive analysis of this relationship across different tasks and model sizes.

## Limitations

- Empirical validation is limited to GLUE benchmark on RoBERTa models without extensive ablation studies
- Claims about arbitrary compression of core tensor need verification through systematic rank sensitivity analysis
- Computational overhead from tensor operations and comparison to emerging PEFT methods like DoTA or LoFT is not addressed
- Paper doesn't analyze how performance degrades with smaller ranks or fewer layers

## Confidence

- Mechanism 1 (Tensor sharing across layers): Medium confidence - mathematical formulation is sound but empirical evidence for redundancy is limited
- Mechanism 2 (Core tensor independence from weight dimension): High confidence - follows directly from Tucker2 decomposition properties
- Mechanism 3 (Better parameter efficiency for deep models): Medium confidence - O(L r² + d r) scaling is correct but practical efficiency gain depends on rank selection

## Next Checks

1. **Rank sensitivity analysis**: Systematically evaluate LoTR performance on GLUE as rank decreases from 32 to 2, measuring both parameter count and task performance to identify minimum viable rank

2. **Layer-wise contribution**: Apply LoTR to individual transformer layers separately (rather than jointly) and measure per-layer parameter efficiency to verify that sharing factors across layers actually provides compression benefits

3. **Scaling to larger models**: Test LoTR on BERT-large and GPT-2 models to verify that parameter efficiency advantages increase with model depth as claimed, comparing against LoRA across different model sizes