---
ver: rpa2
title: Improving the Downstream Performance of Mixture-of-Experts Transformers via
  Weak Vanilla Transformers
arxiv_id: '2403.01994'
source_url: https://arxiv.org/abs/2403.01994
tags:
- bert
- transfer
- capability
- pre-training
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of Mixture-of-Experts (MoE) Transformers
  underperforming in downstream tasks compared to vanilla Transformers, despite their
  advantages in model capacity and computational efficiency. The authors propose that
  the difference in performance stems from inferior transfer capability in MoE models.
---

# Improving the Downstream Performance of Mixture-of-Experts Transformers via Weak Vanilla Transformers

## Quick Facts
- arXiv ID: 2403.01994
- Source URL: https://arxiv.org/abs/2403.01994
- Authors: Xin Lu; Yanyan Zhao; Bing Qin; Ting Liu
- Reference count: 13
- Primary result: Transfer capability distillation significantly improves MoE BERT performance on GLUE benchmark tasks, often exceeding both original MoE and vanilla teacher models

## Executive Summary
This paper addresses the underperformance of Mixture-of-Experts (MoE) Transformers in downstream tasks compared to vanilla Transformers. The authors propose that this performance gap stems from inferior transfer capability in MoE models, despite their advantages in model capacity and computational efficiency. To address this, they introduce transfer capability distillation, where weaker vanilla Transformers (with strong transfer capability but weaker pre-training performance) are used as teachers to guide MoE Transformers. The proposed method aligns relationships between representations at three key locations - model trunk, residual inner, and multi-head attention - using cosine similarity instead of direct value alignment. Experiments on BERT architectures demonstrate significant improvements in downstream GLUE benchmark performance for MoE models with transfer capability distillation, often exceeding both their original MoE performance and even their vanilla teacher models.

## Method Summary
The method involves pre-training a vanilla BERT teacher model, then pre-training an MoE BERT student model with transfer capability distillation. The distillation aligns relationships between representations at three locations: model trunk, residual inner, and multi-head attention. Instead of aligning raw representation values, the method aligns cosine similarities between token representations using MSE loss. The student model is trained with a combined loss including masked language modeling (MLM), load balancing, and the three distillation losses. The teacher model is pre-trained to a "weak" level with strong transfer capability but lower pre-training performance. During fine-tuning on downstream GLUE tasks, both full-parameter and adapter-based approaches are used to evaluate the improved performance of the MoE models.

## Key Results
- MoE BERT with transfer capability distillation shows 0.3-2.2 point improvements on GLUE benchmark tasks compared to original MoE BERT
- The distilled MoE models often outperform both their original MoE versions and even the vanilla teacher models
- Multi-head attention distillation helps larger models but harms smaller models
- The method works across different MoE scales and fine-tuning approaches (full and adapter)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer capability distillation improves downstream performance by aligning intermediate representations between teacher and student models.
- Mechanism: The student MoE model learns to replicate the cosine similarity relationships between representations at three key locations: model trunk, residual inner, and multi-head attention.
- Core assumption: Cosine similarity alignment preserves important semantic relationships while providing flexibility in representation values.
- Evidence anchors:
  - [abstract]: "The proposed distillation method aligns relationships between representations in three key locations"
  - [section 2.3]: "we choose to align the relationships between representations, that is, to make the cosine similarity of a pair of sampled representations converge"
- Break condition: If the alignment becomes too rigid and forces student representations to match teacher values exactly, pre-training performance may degrade.

### Mechanism 2
- Claim: MoE models have inferior transfer capability because they learn lower-quality features during pre-training compared to vanilla models.
- Mechanism: The MoE routing mechanism creates a trade-off where some tokens are processed by suboptimal experts, leading to degraded feature learning.
- Core assumption: Feature quality can be measured by out-of-distribution performance, and MoE models inherently learn lower-quality features due to expert selection dynamics.
- Evidence anchors:
  - [abstract]: "We propose that the pre-training performance and transfer capability of a model are joint determinants of its downstream task performance"
  - [section 5]: "the original MoE BERT...exhibits significant differences from vanilla BERT"
- Break condition: If the teacher model itself has poor feature quality, the distillation may transfer inferior features rather than superior ones.

### Mechanism 3
- Claim: The counterintuitive success occurs because transfer capability is orthogonal to pre-training performance.
- Mechanism: A teacher with strong transfer capability but weak pre-training performance can guide a student to combine strong pre-training performance with strong transfer capability.
- Core assumption: Transfer capability and pre-training performance are independent attributes that can be combined multiplicatively in the student model.
- Evidence anchors:
  - [abstract]: "The underlying logic is that although the pre-training and downstream performance of vanilla models are relatively weak, the transfer capability of them is stronger"
  - [section 2.4]: "The student model acquires strong transfer capability on top of strong pre-training performance"
- Break condition: If transfer capability and pre-training performance are actually negatively correlated in some model families, combining them may create interference rather than synergy.

## Foundational Learning

- Concept: Mixture of Experts (MoE) architecture
  - Why needed here: Understanding how MoE differs from vanilla transformers is essential to grasp why transfer capability is inferior and how distillation helps
  - Quick check question: In MoE transformers, how many experts process each token during inference in the top-1 activation setting?

- Concept: Knowledge distillation
  - Why needed here: The paper uses a novel form of distillation, so understanding standard knowledge distillation helps identify what makes this approach unique
  - Quick check question: What is the key difference between traditional knowledge distillation and transfer capability distillation in terms of teacher model selection?

- Concept: Cosine similarity as alignment metric
  - Why needed here: The distillation method uses cosine similarity rather than direct value alignment, which is crucial for understanding why pre-training performance isn't degraded
  - Quick check question: Why might aligning cosine similarities between representations be preferable to aligning raw representation values in knowledge distillation?

## Architecture Onboarding

- Component map:
  - Teacher model: Vanilla BERT (smaller, weaker pre-training, stronger transfer capability)
  - Student model: MoE BERT (larger, stronger pre-training, weaker transfer capability)
  - Distillation losses: Three types aligned at model trunk, residual inner, and multi-head attention
  - Routing mechanism: Top-1 expert selection with probability scaling
  - Load balancing: KL divergence between actual and uniform expert usage

- Critical path:
  1. Pre-train vanilla BERT teacher
  2. Pre-train MoE BERT student with distillation losses
  3. Fine-tune both models on downstream tasks
  4. Compare performance metrics

- Design tradeoffs:
  - Alignment granularity: Token-level vs. sequence-level vs. batch-level
  - Expert selection: Top-1 vs. top-k for routing
  - Loss weighting: Balance between pre-training loss, load balancing, and distillation losses
  - Teacher strength: Weaker teacher provides better transfer capability but may transfer poorer features

- Failure signatures:
  - Pre-training performance degradation in student model
  - Load imbalance becoming worse after distillation
  - No improvement in downstream tasks despite successful pre-training
  - Student model overfitting to teacher's weaker representations

- First 3 experiments:
  1. Run pre-training with only model trunk alignment to isolate its effect
  2. Test with different teacher pre-training epochs to find optimal transfer capability
  3. Compare top-1 vs. top-2 expert selection with distillation to see routing impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal level of pre-training for the teacher model to maximize transfer capability distillation effectiveness?
- Basis in paper: [explicit] The paper states "We pre-trained the teacher model to a level we consider appropriate" and acknowledges that "the level of pre-training of the teacher model may affect the effect of transfer capability distillation."
- Why unresolved: The authors did not systematically explore how different levels of teacher pre-training affect distillation outcomes, only noting this as a limitation and future work direction.
- What evidence would resolve it: Systematic experiments varying teacher pre-training epochs or validation set performance, measuring corresponding downstream task improvements in student models.

### Open Question 2
- Question: What are the fundamental architectural differences between MoE and vanilla models that lead to MoE's inferior transfer capability?
- Basis in paper: [explicit] The paper mentions that "the quality of features learned during pre-training" may differ between models, and that MoE models show significantly lower out-of-distribution masked language modeling capability.
- Why unresolved: While the paper hypothesizes that feature quality differences explain transfer capability gaps, it does not provide definitive evidence of which architectural elements (routing mechanisms, expert specialization, etc.) cause this.
- What evidence would resolve it: Detailed feature analysis comparing intermediate representations, attention patterns, and routing decisions between MoE and vanilla models across multiple datasets.

### Open Question 3
- Question: Why does multi-head attention distillation help larger models but harm smaller models?
- Basis in paper: [explicit] The ablation analysis shows "for smaller-scale models, the constraint at the multi-head attention location had a negative impact" while "for larger-scale models, the constraint at the multi-head attention location showed a clear positive gain."
- Why unresolved: The paper acknowledges this difference but states "the general principles governing the effectiveness of multi-head attention location constraints are not yet fully clear."
- What evidence would resolve it: Systematic analysis of how attention head dynamics, expert interactions, and representational capacity affect the impact of attention-based distillation constraints across model scales.

## Limitations

- Architecture specificity: Results are limited to BERT models, leaving uncertainty about generalization to other transformer architectures
- Teacher selection heuristic: The choice of "weak" teacher is based on anecdotal observation rather than rigorous measurement of transfer capability across a spectrum of models
- Mechanism verification gap: The paper doesn't provide ablation studies isolating which alignment locations contribute most to improvements

## Confidence

**High Confidence** (4 claims):
- MoE transformers can be improved through knowledge distillation from vanilla transformers
- The three-location alignment approach (model trunk, residual inner, multi-head attention) is technically sound
- The pre-training pipeline with combined MLM, load balancing, and distillation losses is implementable
- The GLUE benchmark results show statistically significant improvements

**Medium Confidence** (3 claims):
- The transfer capability gap is the primary reason for MoE underperformance
- Weak vanilla transformers serve as better teachers than strong ones
- Cosine similarity alignment preserves pre-training performance while transferring transfer capability

**Low Confidence** (2 claims):
- The mechanism works through improved feature quality rather than parameter count efficiency
- The results generalize beyond BERT to other transformer architectures

## Next Checks

1. **Ablation study validation**: Systematically disable each of the three alignment locations (model trunk, residual inner, multi-head attention) to quantify their individual contributions to downstream performance gains.

2. **Teacher strength spectrum analysis**: Train vanilla transformers with varying pre-training performance levels and measure their effectiveness as teachers to test the core hypothesis about teacher weakness.

3. **Architecture generalization test**: Apply the transfer capability distillation method to RoBERTa or DeBERTa architectures to verify whether the mechanism generalizes beyond BERT.