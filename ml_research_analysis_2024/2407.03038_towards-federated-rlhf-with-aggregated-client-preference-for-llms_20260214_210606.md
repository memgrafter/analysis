---
ver: rpa2
title: Towards Federated RLHF with Aggregated Client Preference for LLMs
arxiv_id: '2407.03038'
source_url: https://arxiv.org/abs/2407.03038
tags:
- selector
- clients
- preference
- arxiv
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FedBis and FedBiscuit, two federated learning
  approaches for RLHF that enable privacy-preserving fine-tuning of LLMs using client
  preference data. FedBis trains a binary selector to choose between completions,
  reducing computational overhead compared to scalar reward models.
---

# Towards Federated RLHF with Aggregated Client Preference for LLMs

## Quick Facts
- arXiv ID: 2407.03038
- Source URL: https://arxiv.org/abs/2407.03038
- Reference count: 22
- Introduces two federated RLHF methods (FedBis and FedBiscuit) that outperform centralized training on summarization and QA tasks

## Executive Summary
This paper introduces FedBis and FedBiscuit, two federated learning approaches for RLHF that enable privacy-preserving fine-tuning of LLMs using client preference data. FedBis trains a binary selector to choose between completions, reducing computational overhead compared to scalar reward models. FedBiscuit improves on this by clustering clients with similar preferences and using multiple selectors to mitigate preference heterogeneity and reward hacking. The methods are evaluated on two tasks - summarization and QA - using the first benchmark for federated RLHF. Results show FedBiscuit achieves better performance than FedBis and even surpasses centralized training in generating human-preferred content, with Auto-J ratings of 5.305 (summarization) and 4.094 (QA) compared to centralized's 5.302 and 4.140 respectively.

## Method Summary
FedBis trains a binary selector on each client to distinguish between two LLM-generated completions, using the selector's scores as rewards for reinforcement learning. The binary selector architecture is more computationally efficient than traditional scalar reward models. FedBiscuit extends this by clustering clients based on their preference data and training multiple specialized binary selectors for each cluster. The clustering helps address the heterogeneity of preferences across different clients and reduces the risk of reward hacking. Both methods use federated learning to aggregate model updates without sharing raw preference data, preserving client privacy.

## Key Results
- FedBiscuit achieves Auto-J ratings of 5.305 for summarization and 4.094 for QA, surpassing centralized training (5.302 and 4.140 respectively)
- FedBiscuit outperforms FedBis across all evaluation metrics
- The federated approaches successfully preserve client privacy while achieving comparable or better performance than centralized methods

## Why This Works (Mechanism)
The binary selector approach in FedBis reduces computational overhead by converting the continuous reward prediction problem into a simpler binary classification task. This makes the reinforcement learning process more efficient while still capturing relative preferences. FedBiscuit's clustering mechanism addresses preference heterogeneity by grouping clients with similar taste profiles, allowing specialized selectors to better capture nuanced preferences within each cluster. The federated aggregation ensures that individual client data remains private while still benefiting from collective learning across the network.

## Foundational Learning
- Federated Learning: Decentralized model training across multiple clients while preserving data privacy. Needed because client preference data contains sensitive information that shouldn't be shared directly.
- Reinforcement Learning from Human Feedback (RLHF): Training models using human preference signals as rewards. Critical for aligning LLM outputs with human values and preferences.
- Binary Preference Modeling: Converting continuous reward prediction into binary classification. More efficient computationally while preserving the relative ordering information in preferences.
- Client Preference Heterogeneity: Different users have varying preferences and tastes. Understanding this helps design methods that can handle diverse user populations.
- Reward Hacking: When models exploit reward function weaknesses rather than genuinely optimizing for intended objectives. A key challenge in RLHF that FedBiscuit addresses through clustering.

## Architecture Onboarding

Component Map:
Client devices -> Local binary selector training -> Federated averaging -> Global model update -> Preference clustering -> Multiple selector deployment

Critical Path:
1. Clients generate pairwise preference data from LLM completions
2. Binary selectors are trained locally on client devices
3. Model updates are aggregated via federated averaging
4. Global model is distributed back to clients
5. Clients are clustered based on preference patterns
6. Multiple specialized selectors are trained and deployed

Design Tradeoffs:
- Computational efficiency vs. preference modeling accuracy (binary vs. scalar rewards)
- Model specialization vs. generalization (single vs. multiple selectors)
- Privacy preservation vs. model performance (local training vs. centralized)
- Communication overhead vs. aggregation quality (federated averaging parameters)

Failure Signatures:
- Degraded performance when client preferences are highly diverse without clustering
- Increased reward hacking when using single global selector
- Communication bottlenecks with large model updates
- Privacy leaks if preference data patterns are discernible from model updates

First 3 Experiments:
1. Compare binary selector performance vs. scalar reward models on preference prediction accuracy
2. Evaluate clustering quality using preference similarity metrics across clients
3. Test federated aggregation stability with varying numbers of participating clients

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to only two tasks (summarization and QA) without testing generalizability to other domains
- No analysis of potential security vulnerabilities like model inversion attacks in federated settings
- Computational overhead comparison lacks detailed runtime analysis across different scales of client participation

## Confidence
High: FedBiscuit's superior performance over FedBis and centralized training for summarization and QA tasks based on reported Auto-J ratings
Medium: Claim about mitigating reward hacking through clustering, as effectiveness is demonstrated but edge cases and adversarial scenarios aren't extensively explored
Low: Scalability assertions, since experiments were conducted with relatively small client populations and limited computational resources

## Next Checks
1. Evaluate FedBiscuit across a broader range of NLP tasks including code generation, dialogue systems, and creative writing to assess generalizability
2. Conduct privacy analysis including membership inference and model inversion attacks to quantify the actual privacy guarantees in federated RLHF settings
3. Perform large-scale experiments with hundreds of clients and varying data distributions to validate scalability claims and measure communication efficiency compared to centralized baselines