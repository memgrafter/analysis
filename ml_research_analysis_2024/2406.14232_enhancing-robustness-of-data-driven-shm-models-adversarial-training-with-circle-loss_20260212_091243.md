---
ver: rpa2
title: 'Enhancing robustness of data-driven SHM models: adversarial training with
  circle loss'
arxiv_id: '2406.14232'
source_url: https://arxiv.org/abs/2406.14232
tags:
- uni00000013
- adversarial
- training
- uni00000018
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of machine learning models
  used in structural health monitoring (SHM) to adversarial examples. The authors
  propose an adversarial training method that uses circle loss to optimize the distance
  between features during training, keeping examples away from the decision boundary.
---

# Enhancing robustness of data-driven SHM models: adversarial training with circle loss

## Quick Facts
- arXiv ID: 2406.14232
- Source URL: https://arxiv.org/abs/2406.14232
- Authors: Xiangli Yang; Xijie Deng; Hanwei Zhang; Yang Zou; Jianxi Yang
- Reference count: 40
- Primary result: Circle loss adversarial training improves SHM model robustness against white-box and black-box attacks while maintaining accuracy on clean data

## Executive Summary
This paper addresses the critical vulnerability of machine learning models in structural health monitoring (SHM) to adversarial attacks, where small input perturbations can cause catastrophic misclassifications. The authors propose an adversarial training method that integrates circle loss to optimize feature distances during training, effectively pushing samples away from decision boundaries. Experimental results on two SHM datasets demonstrate significant improvements in robustness against various attack types while maintaining competitive performance on clean data.

## Method Summary
The method combines standard adversarial training with circle loss regularization to enhance feature separation in the embedding space. During training, adversarial examples are generated using FGSM, PGD, and C&W attacks, while circle loss optimizes the distance between features to increase within-class compactness and between-class separation. The approach uses a combined loss function that balances classification accuracy with the circle loss regularization term. The method is evaluated on two SHM datasets using neural network architectures including CNN/BiLSTM for bridge structures and ANN for building structures.

## Key Results
- Achieves approximately 7% improvement in robustness against FGSM and BIM attacks
- Demonstrates substantial 30% improvement against C&W attacks compared to standard training
- Shows approximately 7% higher average accuracy than second-best model against transferred adversarial examples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Circle loss pushes samples away from the decision boundary by optimizing feature distances in the embedding space.
- Mechanism: Circle loss increases intra-class similarity and decreases inter-class similarity through weighted similarity scores that adaptively adjust during training, creating better feature compactness and class separation.
- Core assumption: Adversarial examples exist near decision boundaries where small perturbations can cause misclassification.
- Evidence anchors:
  - [abstract]: "uses circle loss to optimize the distance between features in training to keep examples away from the decision boundary"
  - [section]: "The first term in Eq. (6) guarantees the prediction accuracy of the model for adversarial examples by minimizing the difference between Óà≤ (ùë•adv) and ùë¶, while the second regularization term enhances adversarial robustness, that is, it further pushes the decision boundary of the classifier away from the sample instances via maximizing the within-class distance and minimizing the between-class distance"

### Mechanism 2
- Claim: Circle loss acts as a regularization term that can be combined with standard adversarial training to improve robustness without sacrificing accuracy.
- Mechanism: By integrating circle loss into the training loss function alongside cross-entropy, the model learns to maximize feature separation while maintaining classification accuracy, creating a more robust decision boundary.
- Core assumption: The feature space contains sufficient information to distinguish between classes through distance metrics.
- Evidence anchors:
  - [abstract]: "we introduce an adversarial training methodology for defense, optimizing the feature distances during training to ensure examples remain distant from the decision boundary"
  - [section]: "The experimental results show that this approach aims to enhance the overall robustness of the training models"

### Mechanism 3
- Claim: Circle loss provides transferability resistance by creating more robust feature representations that are less susceptible to adversarial perturbations across different models.
- Mechanism: The distance optimization creates features that are more stable and less sensitive to small perturbations, making adversarial examples generated for one model less effective when transferred to another model.
- Core assumption: Adversarial transferability relies on shared vulnerabilities in feature representations across models.
- Evidence anchors:
  - [section]: "Our method achieves approximately a 7% improvement in robustness against FGSM and BIM attacks, while demonstrating a substantial improvement of approximately 30% against C&W attacks"
  - [section]: "our model showcases remarkable resilience against adversarial samples transferred from other models. Particularly on the LANL dataset, our model boasts an average accuracy approximately 7% higher than the second-best model"

## Foundational Learning

- Concept: Adversarial examples and their vulnerability in machine learning models
  - Why needed here: The paper addresses a fundamental vulnerability in SHM models where small input perturbations can cause misclassification
  - Quick check question: What makes adversarial examples particularly dangerous in safety-critical applications like structural health monitoring?

- Concept: Metric learning and feature space optimization
  - Why needed here: Circle loss is derived from metric learning principles, and understanding how feature distances can be optimized for better classification is crucial
  - Quick check question: How does optimizing within-class compactness and between-class separation improve model robustness?

- Concept: Adversarial training methodologies and their limitations
  - Why needed here: The paper builds upon existing adversarial training approaches and extends them with circle loss
  - Quick check question: What is the main limitation of standard adversarial training that circle loss aims to address?

## Architecture Onboarding

- Component map: Clean data ‚Üí Feature extraction ‚Üí Circle loss regularization ‚Üí Classification ‚Üí Adversarial training loop ‚Üí Robust model
- Critical path: Input data flows through feature extraction layers, then through the penultimate layer where circle loss is applied, followed by classification layers. The training loop iteratively generates adversarial examples and updates weights using the combined loss function.
- Design tradeoffs: Circle loss provides better robustness but requires careful hyperparameter tuning (Œ≤ value, attack proportions). The method adds computational overhead during training but maintains inference efficiency. Using multiple attack types during training improves black-box robustness but increases training time.
- Failure signatures: Poor robustness against attacks despite training, significant drop in clean accuracy, overfitting to specific adversarial patterns, high computational cost during training, sensitivity to hyperparameter choices.
- First 3 experiments:
  1. Baseline comparison: Train standard model on TCRF dataset and evaluate against FGSM/BIM attacks to establish vulnerability baseline
  2. Circle loss integration: Add circle loss after penultimate layer with varying Œ≤ values and evaluate impact on both clean accuracy and attack robustness
  3. Multi-attack training: Train with different proportions of FGSM/PGD/C&W attacks and evaluate transferability resistance across models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of distance metric $\mathcal{D}$ in adversarial attacks on SHM models affect the transferability and effectiveness of adversarial examples?
- Basis in paper: [explicit] The paper discusses the importance of choosing an appropriate distance metric for SHM models, noting that it should focus on preserving damage-sensitive information.
- Why unresolved: The paper does not provide experimental results comparing different distance metrics for SHM models.
- What evidence would resolve it: Experiments comparing the effectiveness of various distance metrics (e.g., L2, L‚àû) on SHM datasets under different attack scenarios.

### Open Question 2
- Question: How does the integration of circle loss with other adversarial training methods (e.g., Fast-AT, PGD-AT) affect the overall robustness of SHM models?
- Basis in paper: [explicit] The paper suggests that circle loss can be incorporated into most existing defense methods for better robustness.
- Why unresolved: The paper only presents results for circle loss combined with standard training, not other adversarial training methods.
- What evidence would resolve it: Experiments comparing the robustness of SHM models trained with circle loss integrated into various adversarial training methods.

### Open Question 3
- Question: What is the impact of using different neural network architectures (e.g., deeper networks, different activation functions) on the effectiveness of circle loss in improving adversarial robustness?
- Basis in paper: [inferred] The paper uses specific neural network architectures and demonstrates the effectiveness of circle loss.
- Why unresolved: The paper does not explore the impact of different architectures on the performance of circle loss.
- What evidence would resolve it: Experiments comparing the robustness of SHM models with various architectures trained with and without circle loss.

## Limitations
- Limited generalizability due to evaluation on only two SHM datasets with specific structural types and damage scenarios
- Computational overhead from multi-attack adversarial training may limit practical deployment in resource-constrained settings
- Sensitivity to hyperparameter tuning, particularly the Œ≤ value for circle loss, requires extensive experimentation for optimal performance

## Confidence

**High Confidence**: The mechanism that circle loss optimizes feature distances to increase separation between classes and push samples away from decision boundaries is theoretically sound and aligns with metric learning principles.

**Medium Confidence**: The claim that circle loss provides transferability resistance by creating more stable feature representations is supported by experimental results but lacks direct theoretical justification.

**Low Confidence**: The assertion that this approach is particularly suitable for real-world SHM applications due to resilience against Gaussian noise is based on limited experimental evidence.

## Next Checks

1. **Ablation study**: Conduct experiments isolating the contribution of circle loss from standard adversarial training by comparing models trained with: (a) standard adversarial training, (b) circle loss without adversarial examples, and (c) combined approach.

2. **Generalization testing**: Evaluate the method on additional SHM datasets with different sensor types (strain gauges, displacement sensors), structural types (steel frames, concrete structures), and damage scenarios to assess broader applicability.

3. **Computational efficiency analysis**: Measure and compare training times, memory usage, and inference latency between the proposed method and baseline approaches across different hardware configurations to quantify practical deployment costs.