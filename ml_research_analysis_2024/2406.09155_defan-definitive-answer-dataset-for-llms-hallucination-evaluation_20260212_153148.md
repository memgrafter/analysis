---
ver: rpa2
title: 'DefAn: Definitive Answer Dataset for LLMs Hallucination Evaluation'
arxiv_id: '2406.09155'
source_url: https://arxiv.org/abs/2406.09155
tags:
- dataset
- llms
- responses
- prompts
- public
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DefAn, a large-scale benchmark dataset designed
  to evaluate hallucinations in Large Language Models (LLMs). The dataset contains
  over 75,000 prompts across eight domains, focusing on eliciting definitive, concise
  answers.
---

# DefAn: Definitive Answer Dataset for LLMs Hallucination Evaluation

## Quick Facts
- arXiv ID: 2406.09155
- Source URL: https://arxiv.org/abs/2406.09155
- Reference count: 24
- Major finding: DefAn benchmark reveals LLMs exhibit 59-82% factual hallucination rates

## Executive Summary
DefAn is a large-scale benchmark dataset containing over 75,000 prompts across eight domains designed to evaluate hallucinations in Large Language Models. The dataset focuses on eliciting definitive, concise answers and is divided into public and hidden segments for testing and benchmarking purposes. Experiments on six popular LLMs reveal significant hallucination rates, with factual hallucinations ranging from 59% to 82% on the public dataset and 57% to 76% on the hidden benchmark. The dataset demonstrates its efficacy as a comprehensive tool for evaluating LLM performance across various domains.

## Method Summary
The DefAn dataset was generated by curating questions from official sources, paraphrasing prompts, and creating public (68,093 samples) and hidden (7,485 samples) segments. Six LLMs (GPT-3.5, Llama 2, Llama 3, Gemini, Mixtral, and Zephyr) were evaluated using APIs to generate responses. Responses were then evaluated based on three metrics: Factual Hallucination Rate (FCH), Prompt Misalignment Hallucination Rate (PMH), and Response Consistency (RC). Claim extraction and evaluation used NLP techniques, regular expressions, and string matching to compare LLM responses against reference answers.

## Key Results
- Factual hallucination rates range from 59% to 82% on the public dataset and 57% to 76% on the hidden benchmark
- Prompt misalignment hallucinations range from 6% to 95% (public) and 17% to 94% (hidden)
- Average consistency ranges from 21% to 61% (public) and 22% to 63% (hidden)
- LLMs struggle most with specific numeric information while performing moderately with person, location, and date queries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dataset design ensures that each prompt elicits a single, definitive claim, reducing ambiguity in evaluation.
- Mechanism: By structuring prompts to target specific, unambiguous information (e.g., exact numbers, dates, names), the evaluation process can directly compare the LLM's output to a known reference without requiring complex reasoning or interpretation.
- Core assumption: LLMs are more likely to produce factual hallucinations when asked for specific, verifiable information rather than open-ended or subjective responses.
- Evidence anchors:
  - [abstract]: "These prompts are designed to elicit definitive, concise, and informative answers."
  - [section]: "We strategically design prompts so that the generated response becomes the sole claim, ensuring clarity and precision in the evaluation process."
  - [corpus]: Weak evidence - the corpus doesn't directly address the claim about reducing ambiguity through definitive answers.
- Break condition: If prompts are poorly constructed or the target information is inherently ambiguous, the evaluation becomes unreliable.

### Mechanism 2
- Claim: The dataset's large size and domain diversity provide a robust benchmark for evaluating LLM performance across various knowledge areas.
- Mechanism: By including over 75,000 prompts across eight domains, the dataset captures a wide range of LLM capabilities and weaknesses, allowing for a comprehensive assessment of hallucination tendencies in different contexts.
- Core assumption: Hallucination patterns are not uniform across domains; some domains may elicit more hallucinations than others.
- Evidence anchors:
  - [abstract]: "The dataset contains over 75,000 prompts across eight domains, focusing on eliciting definitive, concise answers."
  - [section]: "The proposed dataset contains around 75,000 samples from various domains of knowledge."
  - [corpus]: Weak evidence - the corpus doesn't provide specific insights into how domain diversity affects hallucination rates.
- Break condition: If the included domains are not representative of real-world usage or if certain domains are overrepresented, the benchmark may not accurately reflect overall LLM performance.

### Mechanism 3
- Claim: The dataset's public and hidden segments prevent overfitting during LLM training and evaluation.
- Mechanism: By providing a public dataset for testing and a hidden dataset for benchmarking, the dataset ensures that models cannot be optimized specifically for the evaluation data, leading to more reliable performance estimates.
- Core assumption: If models have access to the evaluation data during training, they may artificially inflate their performance by memorizing or overfitting to the dataset.
- Evidence anchors:
  - [abstract]: "The dataset is divided into two segments: one publicly available for testing and assessing LLM performance and a hidden segment for benchmarking various LLMs."
  - [section]: "The dataset has been partitioned into two categories: the public and hidden datasets."
  - [corpus]: Weak evidence - the corpus doesn't discuss the specific benefits of using separate public and hidden datasets for benchmarking.
- Break condition: If the hidden dataset is leaked or if the two datasets are not sufficiently distinct, the separation becomes ineffective.

## Foundational Learning

- Concept: Factual Hallucination
  - Why needed here: Understanding the nature of factual hallucinations is crucial for interpreting the results and evaluating the effectiveness of the benchmark dataset.
  - Quick check question: What is the difference between a factually correct and a factually hallucinated response?

- Concept: Prompt Misalignment
  - Why needed here: Assessing how well LLMs adhere to the given prompts is essential for evaluating their faithfulness and reliability.
  - Quick check question: How can a response be factually correct but still misaligned with the prompt?

- Concept: Consistency in LLM Responses
  - Why needed here: Measuring the consistency of LLM responses across paraphrased prompts helps identify models that produce coherent and reliable outputs.
  - Quick check question: Why is it important for an LLM to generate consistent responses to paraphrased versions of the same prompt?

## Architecture Onboarding

- Component map:
  - Dataset generation -> LLM evaluation -> Hallucination detection -> Consistency assessment
- Critical path:
  1. Generate dataset with definitive answers
  2. Evaluate LLMs using the dataset
  3. Extract factual claims from LLM responses
  4. Compare claims to reference answers and assess prompt adherence
  5. Calculate hallucination rates and consistency scores
- Design tradeoffs:
  - Dataset size vs. annotation quality: Larger datasets provide more robust evaluations but may require more resources for annotation
  - Specificity vs. generality: Highly specific prompts reduce ambiguity but may not reflect real-world usage
  - Public vs. hidden data: Separate segments prevent overfitting but require careful curation to ensure representativeness
- Failure signatures:
  - High hallucination rates: Indicate significant issues with LLM factuality and reliability
  - Low consistency scores: Suggest that LLMs struggle to maintain coherent responses across paraphrased prompts
  - Domain-specific weaknesses: Highlight areas where LLMs need improvement or where the dataset may be lacking
- First 3 experiments:
  1. Evaluate a single LLM on the public dataset and analyze hallucination rates across different domains
  2. Compare the performance of multiple LLMs on the hidden dataset to identify the most reliable model
  3. Investigate the impact of prompt paraphrasing on LLM consistency by measuring response consistency scores

## Open Questions the Paper Calls Out

- Question: How does DefAn's performance compare to human evaluators in detecting LLM hallucinations?
  - Basis in paper: [inferred] The paper mentions human evaluation as a limitation due to being time-consuming and resource-intensive, but doesn't compare DefAn's automated evaluation method to human performance.
  - Why unresolved: The paper focuses on developing and testing DefAn's automated evaluation method but doesn't include a direct comparison to human evaluators.
  - What evidence would resolve it: A study comparing DefAn's automated evaluation results with human evaluators on the same dataset would provide insights into the accuracy and reliability of DefAn's method.

- Question: How does DefAn's performance generalize to other languages beyond English?
  - Basis in paper: [explicit] The paper states that DefAn is designed for English language evaluation and mentions related works in other languages, but doesn't evaluate DefAn's performance on non-English prompts.
  - Why unresolved: The paper focuses on developing and testing DefAn's performance on English language prompts, but doesn't explore its effectiveness on other languages.
  - What evidence would resolve it: Testing DefAn on a multilingual dataset with prompts and answers in multiple languages would reveal its generalizability and potential biases towards English.

- Question: What are the long-term effects of using DefAn for LLM training and fine-tuning?
  - Basis in paper: [inferred] The paper presents DefAn as a benchmark for evaluating LLM performance, but doesn't explore the potential impact of using DefAn for training or fine-tuning LLMs.
  - Why unresolved: The paper focuses on DefAn's role as an evaluation tool, but doesn't investigate how using DefAn for training or fine-tuning might influence LLM behavior or performance over time.
  - What evidence would resolve it: A longitudinal study tracking LLM performance and behavior before and after being trained or fine-tuned using DefAn would provide insights into its potential long-term effects.

## Limitations

- Lack of detailed methodology for claim extraction and evaluation, particularly regarding specific NLP techniques used
- Missing LLM configuration details (temperature, max tokens, etc.) that could significantly impact hallucination rates
- Reliance on Wikipedia-derived questions may not fully represent real-world usage patterns

## Confidence

- **High Confidence:** The general finding that LLMs exhibit substantial hallucination rates (59-82% factual hallucinations)
- **Medium Confidence:** Domain-specific performance differences, particularly the difficulty with numeric information
- **Low Confidence:** Precise hallucination rate values due to unclear methodology and configuration details

## Next Checks

1. Replicate the study using identical LLM configurations and claim extraction methodology to verify the reported hallucination rates
2. Test the DefAn benchmark with additional open-source and commercial LLMs to assess generalizability
3. Evaluate model performance on a subset of DefAn prompts using human annotators to validate the automated hallucination detection methodology