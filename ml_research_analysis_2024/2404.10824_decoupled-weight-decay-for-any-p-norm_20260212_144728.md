---
ver: rpa2
title: Decoupled Weight Decay for Any $p$ Norm
arxiv_id: '2404.10824'
source_url: https://arxiv.org/abs/2404.10824
tags:
- weight
- learning
- decay
- sparsity
- regularization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes p-norm Weight Decay (pWD), a novel regularization
  method that generalizes L2 weight decay to any p norm. The method uses a proximal
  operator approach to avoid gradient instabilities associated with 0 < p < 1 norms
  while maintaining compatibility with adaptive optimizers.
---

# Decoupled Weight Decay for Any $p$ Norm

## Quick Facts
- arXiv ID: 2404.10824
- Source URL: https://arxiv.org/abs/2404.10824
- Reference count: 40
- Achieves 94.4% sparsity on ResNet18 with <2% accuracy drop

## Executive Summary
This paper introduces p-norm Weight Decay (pWD), a novel regularization method that generalizes L2 weight decay to any p norm. The method uses a proximal operator approach to avoid gradient instabilities associated with 0 < p < 1 norms while maintaining compatibility with adaptive optimizers. Experiments on ResNet18 (CIFAR-10) and nanoGPT (Tiny Shakespeare) show pWD achieves high sparsity (up to 99.5% for ResNet18) while maintaining accuracy comparable to or better than L2 regularization and other sparsification methods.

## Method Summary
pWD reformulates Lp regularization as a biconvex optimization problem with auxiliary parameters, enabling stable gradient-based optimization through a proximal operator. The method introduces auxiliary parameters s and uses the proximal gradient step to update weights, avoiding the exploding gradients that occur with direct Lp gradient computation for 0 < p < 1. This approach generalizes standard weight decay while maintaining compatibility with adaptive optimizers like Adam.

## Key Results
- ResNet18 on CIFAR-10: 94.4% sparsity achieved with <2% accuracy drop
- ResNet18 on CIFAR-10: 99.5% sparsity achieved at 80% of baseline accuracy
- nanoGPT on Tiny Shakespeare: 89.9% sparsity achieved with <1% accuracy drop

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** pWD generalizes L2 weight decay to any p norm while maintaining optimizer compatibility.
- **Mechanism:** The method reformulates Lp regularization as a biconvex optimization problem with auxiliary parameters, enabling stable gradient-based optimization through a proximal operator.
- **Core assumption:** The equivalence between the original Lp regularization and the extended formulation with auxiliary parameters holds for all p > 0.
- **Evidence anchors:**
  - [abstract]: "generalizes the standard L2 weight decay to any p norm"
  - [section]: "Rp(w, s) is non-convex... Nevertheless, we refer the reader again to App. A for a formal proof that both local and global minima of the original and extended optimization problems coincide"
  - [corpus]: Weak evidence - no direct citations about proximal operators in this context
- **Break condition:** If the equivalence proof fails for certain p values or the auxiliary parameter formulation becomes numerically unstable.

### Mechanism 2
- **Claim:** The proximal operator approach prevents gradient divergence for 0 < p < 1 norms.
- **Mechanism:** By introducing auxiliary parameters s and using the proximal gradient step, the method avoids exploding gradients near zero weights that occur with direct Lp gradient computation.
- **Core assumption:** The proximal gradient step correctly implements the weight decay without introducing instability.
- **Evidence anchors:**
  - [abstract]: "avoids the gradient divergence associated with 0 < p < 1 norms"
  - [section]: "Taking instead a decoupled weight decay approach... we are tempted to write the w update step... This update rule, however, still does not ensure stability... To overcome this instability, we propose to use the proximal operator of Rp"
  - [corpus]: No direct citations found about proximal operators preventing gradient divergence
- **Break condition:** If the proximal operator implementation introduces new numerical instabilities or computational overhead becomes prohibitive.

### Mechanism 3
- **Claim:** The method achieves high sparsity while maintaining generalization comparable to L2 regularization.
- **Mechanism:** The pWD step drives weights to zero through regularization while the proximal operator ensures stable optimization, resulting in sparse networks with maintained accuracy.
- **Core assumption:** The sparsity-inducing property of Lp norms for p < 1 can be effectively harnessed without sacrificing generalization.
- **Evidence anchors:**
  - [abstract]: "empirically demonstrate that it leads to highly sparse networks, while maintaining generalization performance comparable to standard L2 regularization"
  - [section]: "We empirically assess the performance of p-norm Weight Decay (pWD)... Our results show that pWD achieves high levels of sparsity while maintaining excellent network generalization"
  - [corpus]: No direct citations found about empirical sparsity results
- **Break condition:** If the sparsity comes at the cost of significant accuracy degradation or if the method fails to generalize across different architectures.

## Foundational Learning

- **Concept:** Proximal operators and their application in optimization
  - Why needed here: The entire pWD method relies on the proximal operator framework to implement stable weight decay for any p norm
  - Quick check question: What is the proximal operator for L2 regularization and how does it differ from the proximal operator used in pWD?

- **Concept:** Biconvex optimization and alternate convex search
  - Why needed here: The extended Lp regularization formulation is biconvex, and understanding how to optimize such functions is crucial for implementing pWD
  - Quick check question: What is the difference between convex and biconvex optimization, and why does it matter for the pWD approach?

- **Concept:** Regularization theory and its impact on generalization
  - Why needed here: Understanding how different regularization schemes affect model generalization is essential for interpreting the results and choosing appropriate p values
  - Quick check question: How does L1 regularization differ from L2 in terms of sparsity and generalization, and how does this relate to the pWD approach?

## Architecture Onboarding

- **Component map:**
  - Base optimizer (e.g., Adam) -> Auxiliary parameters s -> pWD step -> Weight updates

- **Critical path:**
  1. Compute gradients using the base optimizer
  2. Update auxiliary parameters s based on current weights
  3. Apply the pWD step to update weights using the proximal operator
  4. Repeat until convergence

- **Design tradeoffs:**
  - pWD vs. standard weight decay: pWD achieves higher sparsity but may require more hyperparameter tuning
  - Computational overhead: pWD adds minimal overhead but requires storing auxiliary parameters
  - Stability vs. sparsity: The choice of p affects the balance between stable optimization and achieving high sparsity

- **Failure signatures:**
  - Training instability or exploding gradients: May indicate issues with the pWD implementation or inappropriate p values
  - Lack of sparsity: Could result from using p values that are too close to 2 or from not properly updating the auxiliary parameters
  - Significant accuracy degradation: Might suggest that the chosen p value is too low, sacrificing generalization for sparsity

- **First 3 experiments:**
  1. Implement pWD with p=2 (equivalent to standard weight decay) to verify compatibility with existing optimizers
  2. Test pWD with p=1 (L1 regularization) to compare with standard sparse regularization methods
  3. Experiment with p=0.5 to evaluate the sparsity-generalization tradeoff and identify optimal p values for specific tasks

## Open Questions the Paper Calls Out
- **Open Question 1:** How does p-norm Weight Decay (pWD) perform on larger, more complex models like GPT-3 or BERT compared to existing sparsification methods?
- **Open Question 2:** What is the optimal scheduling strategy for the parameter p during training to balance sparsity and generalization?
- **Open Question 3:** How does the performance of pWD compare to other regularization methods when applied to non-convex optimization problems beyond neural networks?

## Limitations
- Limited architectural diversity in experiments (only ResNet18 and nanoGPT tested)
- Proof of equivalence between original and extended Lp formulations provided in appendix but not directly verifiable from main text
- No systematic ablation studies on impact of different p values on convergence speed and training stability

## Confidence
- **High confidence** in the theoretical framework and proximal operator implementation
- **Medium confidence** in the empirical results due to limited architectural diversity in experiments
- **Low confidence** in the claimed superiority over existing sparsification methods without more comprehensive comparisons

## Next Checks
1. Reproduce the pWD implementation on a third architecture (e.g., BERT or Vision Transformer) to validate generalization across different model types
2. Conduct a systematic ablation study on the impact of initialization strategies for the auxiliary parameter s
3. Compare pWD's computational efficiency with standard weight decay methods across different batch sizes and hardware configurations