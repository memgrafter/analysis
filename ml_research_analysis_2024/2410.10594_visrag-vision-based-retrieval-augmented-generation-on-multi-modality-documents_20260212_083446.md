---
ver: rpa2
title: 'VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents'
arxiv_id: '2410.10594'
source_url: https://arxiv.org/abs/2410.10594
tags:
- document
- generation
- visrag
- retrieval
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VisRAG, a vision-based retrieval-augmented
  generation framework that directly processes document images using vision-language
  models, bypassing the text parsing step required in traditional RAG systems. By
  replacing text-based retrievers and generators with VLMs, VisRAG preserves all visual
  information in multi-modal documents, eliminating information loss during parsing.
---

# VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents

## Quick Facts
- arXiv ID: 2410.10594
- Source URL: https://arxiv.org/abs/2410.10594
- Authors: Shi Yu; Chaoyue Tang; Bokai Xu; Junbo Cui; Junhao Ran; Yukun Yan; Zhenghao Liu; Shuo Wang; Xu Han; Zhiyuan Liu; Maosong Sun
- Reference count: 35
- Primary result: 20-40% end-to-end performance gains over traditional text-based RAG systems

## Executive Summary
VisRAG introduces a vision-based retrieval-augmented generation framework that directly processes document images using vision-language models, bypassing traditional text parsing. By leveraging VLMs for both retrieval and generation, VisRAG preserves all visual information in multi-modal documents and eliminates information loss during parsing. The framework achieves significant performance improvements across six real-world document datasets, demonstrating strong training data efficiency and generalization capability. The code and data are publicly available for reproducibility.

## Method Summary
VisRAG replaces text-based retrievers and generators with vision-language models that directly process document images. The framework consists of VisRAG-Ret, a dual-encoder VLM for retrieval using weighted mean pooling, and VisRAG-Gen, a VLM-based generator that handles multi-document generation through page concatenation, weighted selection, or multi-image input. Both components are trained on synthetic data and VQA datasets. The system achieves 20-40% end-to-end performance gains by preserving visual layout information and enabling cross-page reasoning through VLMs capable of processing multiple images.

## Key Results
- VisRAG achieves 20-40% end-to-end performance gains over traditional text-based RAG systems
- Retrieval performance shows MRR@10 improvement from 74.78 to 77.91
- Generation accuracy improves by 40% relative with 5% numeric error margin tolerance
- VisRAG-Ret outperforms text models with only 20K training examples versus 75K for text-only models

## Why This Works (Mechanism)

### Mechanism 1
Vision-based embedding via weighted mean pooling preserves visual information better than text parsing. VisRAG-Ret directly embeds document images using VLM hidden states, avoiding OCR errors and layout loss. Weighted mean pooling aggregates spatial and semantic visual cues into a single vector. Core assumption: Vision-language models encode sufficient visual and textual cues in their hidden states to represent document content without explicit text extraction. Evidence: [abstract] "VisRAG maximizes the retention and utilization of the data information in the original documents, eliminating the information loss introduced during the parsing process." Break condition: If VLMs fail to encode OCR-level text or if weighted pooling underweights critical visual tokens, performance collapses.

### Mechanism 2
Multi-image VLMs can reason over retrieved pages, improving generation with more context. VLMs like MiniCPM-V 2.6 trained on multi-image data can attend across document pages, enabling cross-page reasoning for multi-hop questions. Core assumption: Pre-training on multi-image datasets enables VLMs to aggregate information across pages in a single generation step. Evidence: [section 3.2.2] "Some recent VLMs like MiniCPM-V 2.6 and Qwen-VL 2 are designed and trained to accept multiple images as input to perform cross-image reasoning." Break condition: If the VLM lacks cross-image attention or the retrieved pages are too noisy, multi-image reasoning fails.

### Mechanism 3
Training data efficiency allows VisRAG-Ret to match or exceed large text-only retrievers with fewer examples. Vision-language joint training on synthetic query-document pairs captures richer document semantics than text-only models, reducing required training data. Core assumption: Joint vision-language embedding learns more generalizable document representations than text-only embeddings. Evidence: [section 5.2] "VisRAG requires training on only 20K examples, whereas MiniCPM (OCR) needs about 75K examples." Break condition: If synthetic data is too domain-specific or vision cues dominate at the expense of text semantics, out-of-domain performance drops.

## Foundational Learning

- Concept: Dense retrieval with dual-encoder architecture
  - Why needed here: VisRAG-Ret uses a dual-encoder VLM to encode queries and documents into a shared embedding space for ANN search
  - Quick check question: What is the role of the dual-encoder in dense retrieval, and how does it differ from lexical methods like BM25?

- Concept: Vision-language model pre-training objectives
  - Why needed here: VLMs like MiniCPM-V 2.0 combine vision encoders (e.g., SigLIP) with language models (e.g., MiniCPM) trained on contrastive and generative objectives
  - Quick check question: How do contrastive and generative pre-training objectives contribute to VLM performance on document understanding?

- Concept: Weighted mean pooling over hidden states
  - Why needed here: VisRAG-Ret uses position-weighted mean pooling to aggregate VLM hidden states into a fixed-size embedding
  - Quick check question: Why is weighted mean pooling preferred over simple averaging for VLM embeddings in retrieval tasks?

## Architecture Onboarding

- Component map: Document images -> VisRAG-Ret (VLM dual-encoder with weighted mean pooling) -> Retrieved pages -> VisRAG-Gen (VLM generator with page concatenation/weighted selection) -> Generated answers

- Critical path: 1. Render document pages as images. 2. Encode queries and documents with VisRAG-Ret. 3. Retrieve top-k pages using ANN search. 4. Generate answers with VisRAG-Gen using multi-document handling methods.

- Design tradeoffs: Vision-based processing preserves layout but requires VLM inference for all documents; text-based parsing is faster but loses visual information; synthetic data enables training but may not cover all real-world scenarios.

- Failure signatures: Poor retrieval when VLMs miss fine-grained text details; generation failures with complex multi-page layouts; degraded performance on documents with handwritten content or non-standard fonts.

- First experiments: 1. Test retrieval performance on ArXivQA dataset with MRR@10 metric. 2. Evaluate generation accuracy on ChartQA with relaxed exact match. 3. Compare training efficiency with MiniCPM (OCR) on synthetic data.

## Open Questions the Paper Calls Out

### Open Question 1
How does VisRAG perform on documents with significant handwritten content or non-standard fonts? The paper discusses the importance of visual information but does not specifically address handwritten content or non-standard fonts. Experiments primarily use documents with standard printed text and common font types. What evidence would resolve it: Experiments comparing VisRAG's performance on documents with handwritten content or non-standard fonts versus standard printed text, along with qualitative analysis of failure cases.

### Open Question 2
What is the computational overhead of VisRAG compared to TextRAG when scaling to large document corpora? While memory efficiency is addressed (4.5KB vs 256KB per page), the paper does not provide comprehensive comparison of computational costs for scaling to large document collections. What evidence would resolve it: A detailed comparison of computational resources (processing time, memory usage, indexing speed) required by VisRAG versus TextRAG when processing large document collections of varying sizes.

### Open Question 3
How does VisRAG handle documents with complex multi-page layouts where information is distributed across non-consecutive pages? The paper mentions cross-image reasoning capabilities but does not explore scenarios where relevant information is spread across non-consecutive pages. What evidence would resolve it: Experiments testing VisRAG's performance on documents where relevant information is distributed across non-consecutive pages, along with analysis of how the model handles such cases compared to TextRAG.

## Limitations

- Performance gains primarily measured on VQA-style tasks, which may not represent all document retrieval scenarios
- Multi-image reasoning capability depends on specific VLM architectures (MiniCPM-V 2.6, GPT-4o) that may not generalize to other VLMs
- Heavy reliance on synthetic training data raises questions about real-world generalization beyond tested document types

## Confidence

- High confidence: The fundamental approach of using VLMs for document image embedding is sound and well-supported by experimental results
- Medium confidence: The training efficiency claims and synthetic data generation pipeline are reasonable but not fully validated across diverse domains
- Medium confidence: The multi-image reasoning mechanism is demonstrated but depends on specific VLM architectures that may not be universally available

## Next Checks

1. Test VisRAG on non-VQA document tasks (e.g., contract analysis, financial reports) to verify generalization beyond question-answering scenarios
2. Compare VisRAG performance with high-precision OCR systems on documents requiring exact text extraction (e.g., numerical tables, legal documents)
3. Evaluate VisRAG with VLMs lacking cross-image attention to determine if the multi-image reasoning advantage is architecture-dependent