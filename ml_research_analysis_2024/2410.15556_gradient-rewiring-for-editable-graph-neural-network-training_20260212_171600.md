---
ver: rpa2
title: Gradient Rewiring for Editable Graph Neural Network Training
arxiv_id: '2410.15556'
source_url: https://arxiv.org/abs/2410.15556
tags:
- editing
- gradient
- training
- graph
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of correcting prediction errors
  in graph neural networks (GNNs) through model editing. The authors observe that
  direct fine-tuning using the target node's loss leads to inconsistent gradients
  with training nodes, resulting in degraded performance.
---

# Gradient Rewiring for Editable Graph Neural Network Training

## Quick Facts
- arXiv ID: 2410.15556
- Source URL: https://arxiv.org/abs/2410.15556
- Reference count: 33
- Authors: Zhimeng Jiang, Zirui Liu, Xiaotian Han, Qizhang Feng, Hongye Jin, Qiaoyu Tan, Kaixiong Zhou, Na Zou, Xia Hu
- Key outcome: This paper addresses the problem of correcting prediction errors in graph neural networks (GNNs) through model editing. The authors observe that direct fine-tuning using the target node's loss leads to inconsistent gradients with training nodes, resulting in degraded performance. To tackle this, they propose Gradient Rewiring for Editable graph neural network training (GRE), which stores anchor gradients from training nodes and rewires the target loss gradient to preserve training performance. An advanced version, GRE+, applies constraints on multiple disjoint training subsets. Experiments on various datasets show GRE and GRE+ effectively correct predictions while maintaining accuracy, outperforming existing methods. The approach is also compatible with the EGNN baseline, further improving its performance.

## Executive Summary
This paper addresses the challenge of correcting prediction errors in graph neural networks through model editing. The authors identify that directly fine-tuning on a target node's loss leads to inconsistent gradients with those from training nodes, causing performance degradation. To solve this, they propose Gradient Rewiring for Editable graph neural network training (GRE), which stores anchor gradients from training nodes and rewires the target loss gradient to preserve training performance. An advanced version, GRE+, applies constraints on multiple disjoint training subsets. Experiments on various datasets show GRE and GRE+ effectively correct predictions while maintaining accuracy, outperforming existing methods.

## Method Summary
The authors propose Gradient Rewiring for Editable graph neural network training (GRE) to address the problem of correcting prediction errors in GNNs. The method stores anchor gradients from training nodes before editing, then rewires the target node's gradient by projecting it onto the subspace orthogonal to the training gradient. This ensures that changes made to correct the target node don't adversely affect performance on training nodes. An advanced version, GRE+, applies gradient rewiring constraints on multiple disjoint training subsets for stronger protection. The approach is compatible with the EGNN baseline, further improving its performance.

## Key Results
- GRE and GRE+ effectively correct misclassified predictions while maintaining training performance
- GRE+ outperforms GRE by applying constraints on multiple disjoint training subsets
- The approach is compatible with the EGNN baseline, further improving its performance
- Experimental results show consistent improvements across various datasets (Cora, Flickr, ogbn-arxiv, Amazon Photo) and GNN architectures (GCN, GraphSAGE, MLP)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Directly fine-tuning a GNN on a target node's loss leads to inconsistent gradients with those from training nodes, causing performance degradation on the training set.
- Mechanism: GNNs aggregate information across neighbors, so changing parameters for one node's prediction can inadvertently affect others. The gradients of the target loss and training loss diverge significantly, meaning gradient descent on the target loss will worsen training loss.
- Core assumption: The gradient discrepancy between target and training nodes is the root cause of the degradation.
- Evidence anchors:
  - [abstract] "we first observe the gradient of cross-entropy loss for the target node and training nodes with significant inconsistency, which indicates that directly fine-tuning the base model using the loss on the target node deteriorates the performance on training nodes."
  - [section 3.1] "the gradient for training and target loss is given by gtrain = ∂Ltrain/∂θ and gtg = ∂Ltg/∂θ, respectively... To investigate why the model editing leads to training performance degradation, we use gradient RMSE... to measure the model editing discrepancy for training datasets and target sample."
  - [corpus] Weak: corpus contains rewiring and oversquashing literature but not directly about gradient inconsistency in editing.
- Break condition: If gradient discrepancy is small (e.g., in MLP models), the approach would have less benefit.

### Mechanism 2
- Claim: By storing the anchor gradient from the training set and rewiring the target loss gradient to remove its projection onto the training gradient, we can correct the target while preserving training performance.
- Mechanism: Store gtrain before editing. For each edit step, project gtg onto the subspace orthogonal to gtrain, then scale by (1+λ)⁻¹. This satisfies constraints that (1) training loss does not increase and (2) prediction changes on training nodes are bounded.
- Core assumption: The Taylor expansion approximations used to derive the constraints are valid for the small parameter updates typical in editing.
- Evidence anchors:
  - [section 3.2] "we first calculate and store the anchor gradient of the loss on the training nodes... Then, during the editing process, we adjust the gradient of the loss on the target node based on the stored anchor gradient... This adjustment, or 'rewiring', ensures that the changes made to the target node do not adversely affect the performance on the training nodes."
  - [section 3.2] "Once we tackle dual QP problem (11) for v*, we can recover the rewired gradient as g = (1 + λ)⁻¹(Gv + gtg)."
  - [corpus] Weak: corpus has rewiring methods for GNNs but not for gradient editing.
- Break condition: If the stored anchor gradient becomes stale after many edits, the rewiring may fail to preserve training performance.

### Mechanism 3
- Claim: GRE+ improves upon GRE by applying gradient rewiring constraints on multiple disjoint training subsets, further protecting against localized training degradation.
- Mechanism: Split training set into K subsets, compute gk_train for each, then solve a QP to ensure g is orthogonal to the span of all gk_train. This enforces the non-increase constraint on each subset.
- Core assumption: Disjoint subsets capture different local patterns, so constraining each subset prevents localized degradation that could slip through a global constraint.
- Evidence anchors:
  - [section 3.2] "we split training dataset Vtrain into K sub-training sets... Similarly, we define gk_train = ∂Lk_train/∂θ... Following the derivative clue in GRE, we can replace the training loss constraint on the whole training dataset with multiple training loss constraints on training subsets."
  - [section 3.2] "Eq.(10) is a quadratic program (QP) in L-variables... we can effectively solve this problem in the dual space via transforming as a smaller QP problem with only K variables v ∈ RK."
  - [corpus] Weak: corpus does not discuss subset-based gradient constraints.
- Break condition: If K is too large, QP solving becomes expensive; if K is too small, protection is insufficient.

## Foundational Learning

- Concept: Gradient descent and its behavior in multi-objective settings
  - Why needed here: Understanding how modifying gradients for one objective (target node) affects another (training set) is central to the mechanism.
  - Quick check question: If we perform gradient descent on a loss L1 while another loss L2 is important, what could go wrong if the gradients are not aligned?

- Concept: Taylor expansion for local approximations
  - Why needed here: The constraints in GRE are derived using first-order Taylor approximations to bound changes in loss and predictions.
  - Quick check question: Given a function f(x) and a small step δ, what is the first-order Taylor approximation of f(x+δ)?

- Concept: Quadratic programming and dual formulation
  - Why needed here: GRE and GRE+ reduce to QP problems; knowing how to transform to dual space for efficiency is key.
  - Quick check question: For a QP min ½xᵀPx + qᵀx s.t. Ax ≥ b, what is the form of the dual problem?

## Architecture Onboarding

- Component map:
  Data loader -> Model (GNN/MLP) -> Anchor gradient store -> Rewiring module -> Optimizer -> Monitor

- Critical path:
  1. Pre-train model on training set.
  2. Compute and store anchor gradient(s).
  3. For each edit iteration:
     a. Compute gtg for target node.
     b. Apply rewiring to get g*.
     c. Update θ via optimizer.
     d. Check stopping condition (target prediction correct).
  4. Evaluate on test set.

- Design tradeoffs:
  - GRE vs GRE+: GRE+ offers stronger protection but requires more memory and QP solve time (scales with K).
  - Anchor gradient freshness: Storing before editing avoids runtime overhead but may become stale in long edit sequences.
  - λ tuning: Balances correction speed vs. preservation of training performance.

- Failure signatures:
  - Training loss rises sharply → anchor gradient stale or λ too small.
  - Target not corrected → λ too large or rewiring over-constrains.
  - Slow convergence → insufficient gradient projection removal.

- First 3 experiments:
  1. Verify gradient inconsistency: Pre-train GNN, compute gtrain and gtg for a misclassified node, measure RMSE.
  2. Test basic GRE: Implement rewiring with λ=0.1, confirm target correction and training loss stability.
  3. Compare GRE vs GRE+: Run both on same edit task, measure accuracy drop and success rate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of gradient rewiring compare to curriculum learning strategies in terms of improving model performance and generalization?
- Basis in paper: [inferred] The paper discusses gradient rewiring and its effectiveness in model editing but does not compare it to curriculum learning approaches.
- Why unresolved: The paper focuses on gradient rewiring and its benefits but does not explore alternative strategies like curriculum learning for improving model performance.
- What evidence would resolve it: Experimental results comparing gradient rewiring with curriculum learning methods in terms of model accuracy, generalization, and efficiency would provide insights into their relative effectiveness.

### Open Question 2
- Question: Can the proposed gradient rewiring approach be adapted to handle sequential editing tasks more effectively, reducing the observed increase in accuracy drop compared to single-node editing?
- Basis in paper: [explicit] The paper mentions the challenge of maintaining locality in sequential editing and suggests that more comprehensive training subset selection may be promising.
- Why unresolved: While the paper identifies the issue with sequential editing, it does not provide a specific solution or adaptation of the gradient rewiring approach to address this challenge.
- What evidence would resolve it: Demonstrating improved performance on sequential editing tasks using an adapted gradient rewiring approach, with reduced accuracy drop compared to the original method, would indicate its effectiveness.

### Open Question 3
- Question: How sensitive is the gradient rewiring method to variations in the hyperparameter λ, and what is the optimal range for different graph datasets and model architectures?
- Basis in paper: [explicit] The paper discusses the sensitivity of the method to the hyperparameter λ and shows that test accuracy drop remains relatively stable despite variations in λ.
- Why unresolved: While the paper indicates that the method is not highly sensitive to λ, it does not explore the optimal range for different scenarios or provide guidelines for selecting λ.
- What evidence would resolve it: A comprehensive study exploring the impact of λ on different graph datasets and model architectures, with recommendations for optimal λ values, would provide valuable insights for practitioners.

## Limitations
- The approach assumes the training anchor gradient remains representative throughout editing, which may not hold for long edit sequences
- The quadratic programming step in GRE+ could become computationally expensive with large K
- The evaluation focuses on relatively standard citation and social network datasets without stress-testing on noisy or adversarial graphs

## Confidence
- Mechanism 1 (Gradient inconsistency causes degradation): High
- Mechanism 2 (Rewiring via projection removes inconsistency): High
- Mechanism 3 (GRE+ subset constraints improve protection): Medium
- Experimental evaluation across datasets: Medium

## Next Checks
1. Test gradient inconsistency on deeper GNN architectures (e.g., GAT, GIN) to verify the phenomenon generalizes beyond GCN/GraphSAGE.
2. Evaluate GRE+ on a dataset with known heterophily to check if subset constraints provide meaningful protection when local patterns diverge.
3. Measure the computational overhead of GRE+ with increasing K to determine practical limits for large-scale graphs.