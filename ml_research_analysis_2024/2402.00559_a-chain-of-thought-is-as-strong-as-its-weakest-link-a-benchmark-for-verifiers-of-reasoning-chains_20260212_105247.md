---
ver: rpa2
title: 'A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers
  of Reasoning Chains'
arxiv_id: '2402.00559'
source_url: https://arxiv.org/abs/2402.00559
tags:
- step
- evidence
- reasoning
- steps
- attribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: REVEAL introduces a fine-grained benchmark for verifying reasoning
  chains in language models. The dataset labels each step of a Chain-of-Thought answer
  for relevance, attribution to evidence, and logical correctness.
---

# A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains

## Quick Facts
- arXiv ID: 2402.00559
- Source URL: https://arxiv.org/abs/2402.00559
- Reference count: 40
- Primary result: Only 43.8% of attribution steps are fully supported by evidence in REVEAL benchmark

## Executive Summary
This paper introduces REVEAL, a fine-grained benchmark for evaluating verifiers of Chain-of-Thought (CoT) reasoning chains in language models. The dataset provides step-level annotations for 1,002 CoT answers across four reasoning datasets, labeling each step for relevance, attribution type, and logical correctness. The work reveals that while language models struggle more with attribution errors than logical errors, verifiers show the opposite pattern - they perform worse at detecting logical errors. The benchmark exposes the limitations of current verification approaches and provides a foundation for developing more robust reasoning chain validators.

## Method Summary
The REVEAL dataset is constructed by generating CoT answers using three language models (Flan-PaLM-540B, GPT-3, Flan-UL2-20B) on questions from StrategyQA, MuSiQue, Sports Understanding, and Fermi datasets. Each answer is decomposed into reasoning steps that are annotated for relevance to the question, type (observation, factual, logical, or hybrid), attribution to evidence, and logical correctness. For attribution steps, evidence is retrieved from Wikipedia and labeled as fully supporting, contradicting, or not supporting the claim. The evaluation compares few-shot prompting baselines, NLI classifiers, and specialized pipelines on both step-level and full-chain verification tasks.

## Key Results
- Attribution errors are most common (only 43.8% fully supported) while logical errors are less frequent
- Verifiers struggle most with logical correctness detection despite it being less common in CoT answers
- Step-level verification helps identify error locations but pipeline approaches only slightly improve full-chain verification
- All verification methods perform at or near majority baseline levels for most tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Step-level verification exposes weak links in reasoning chains better than full-chain verification
- Mechanism: Breaking CoT into atomic steps allows pinpointing exact error locations and distinguishing error types (attribution vs logic)
- Core assumption: Individual step correctness is a necessary condition for overall chain correctness
- Evidence anchors:
  - [abstract] "Evaluation on REVEAL shows that verifiers struggle at verifying reasoning chains - in particular, verifying logical correctness and detecting contradictions"
  - [section] "Step-level detection can additionally help with detecting cases of 'snowballing of hallucinations' from earlier steps (Zhang et al., 2023)"
  - [corpus] Weak corpus support - no direct paper on step-level verification effectiveness found

### Mechanism 2
- Claim: Attribution errors are more common than logical errors but harder for verifiers to detect
- Mechanism: LMs generate more factual errors than logical errors, but NLI-based verifiers are better suited for logical verification than attribution verification
- Core assumption: Factuality verification requires different capabilities than logical inference verification
- Evidence anchors:
  - [abstract] "models struggle most with attribution stepsâ€”only 43.8% are fully supported by evidence"
  - [section] "Between attribution and logic types of correctness, CoT-generating LMs struggle more with attribution, but in contrast, CoT verifiers struggle more with verifying logical correctness"
  - [corpus] Moderate corpus support - papers on attribution verification exist but focus on different domains

### Mechanism 3
- Claim: Fine-grained annotation schema enables reliable step-level verification evaluation
- Mechanism: Clear separation of relevance, type, attribution, and logic labels with free-text justifications creates unambiguous evaluation criteria
- Core assumption: Complex verification tasks can be decomposed into simpler, independently verifiable subtasks
- Evidence anchors:
  - [section] "The annotation of a step ends when either a fully-supporting or contradicting evidence is found, or the maximum number of paragraphs is labeled"
  - [section] "In addition to the verification labels, in both tasks we ask annotators to provide a free-text justification for their choices"
  - [corpus] Weak corpus support - no direct paper on annotation schema effectiveness found

## Foundational Learning

- Concept: Chain-of-Thought reasoning
  - Why needed here: Understanding how CoT works is essential to evaluate verification methods
  - Quick check question: Can you explain why CoT improves reasoning performance compared to direct answering?

- Concept: Natural Language Inference (NLI)
  - Why needed here: Attribution verification is framed as an NLI task (entailment, contradiction, or no support)
  - Quick check question: How would you formalize the task of checking if "A Boeing 737-800 costs around $90 million" is supported by evidence saying it was valued at $48.3 million?

- Concept: Inter-annotator agreement metrics
  - Why needed here: Understanding agreement measures (Krippendorff's alpha) is crucial for evaluating annotation quality
  - Quick check question: What does a Krippendorff's alpha of 0.49 indicate about the reliability of attribution labels?

## Architecture Onboarding

- Component map: Question selection -> CoT generation -> Evidence retrieval -> Annotation -> Evaluation -> Analysis
- Critical path: 1) Generate CoT answers using multiple models 2) Retrieve evidence for attribution steps 3) Annotate steps for relevance, type, logic, and attribution 4) Evaluate verifiers on annotated data 5) Analyze error patterns and verifier limitations
- Design tradeoffs:
  - Step-level vs full-chain verification: Granularity vs. efficiency
  - Wikipedia vs. broader knowledge sources: Reliability vs. coverage
  - Manual vs. automatic annotation: Quality vs. scalability
  - Prompt demonstrations: Sample efficiency vs. bias
- Failure signatures:
  - Low inter-annotator agreement (<0.4 Krippendorff's alpha)
  - Verifiers performing at or below majority baseline
  - Evidence retrieval failing to support factually correct claims
  - Step decontextualization introducing ambiguity
- First 3 experiments:
  1. Test baseline verifiers on a small subset of REVEAL-Eval to establish performance floor
  2. Compare step-level vs full-chain verification performance on identical data
  3. Analyze unsupported attribution steps to determine if retrieval or claim validity is the issue

## Open Questions the Paper Calls Out
The paper identifies several open questions but does not explicitly call out specific open questions in a dedicated section. The key limitations and future directions are discussed throughout the paper, particularly around improving verifier performance, handling hybrid steps, and addressing domain-specific knowledge requirements.

## Limitations
- The REVEAL dataset's annotation quality depends heavily on inter-annotator agreement, which shows mixed results (0.67-0.89 for CoT-level tasks but only 0.49-0.63 for step-level attribution)
- The evaluation focuses primarily on CoT answers generated by three specific language models (Flan-PaLM-540B, GPT-3, Flan-UL2-20B), which may not generalize to other model architectures
- The evidence retrieval system's effectiveness isn't independently validated - unsupported attribution steps could result from either model hallucination or retrieval failure

## Confidence
- **High confidence**: The finding that attribution errors are more common than logical errors in CoT answers (43.8% vs lower logical error rates)
- **Medium confidence**: The claim that attribution errors are harder for verifiers to detect than logical errors
- **Low confidence**: The effectiveness of step-level verification in improving overall chain verification

## Next Checks
1. **Inter-annotator reliability analysis**: Re-examine the 50% sample of attribution annotations with independent raters to determine if the 0.49-0.63 Krippendorff's alpha reflects task difficulty or guideline ambiguity
2. **Cross-model generalization test**: Apply REVEAL evaluation to CoT answers from additional model families (e.g., GPT-4, Claude, open-source alternatives) to assess whether the error patterns hold across architectures
3. **Evidence retrieval audit**: For a sample of unsupported attribution steps, manually verify whether the claims are factually correct but unsupported due to retrieval limitations, helping distinguish model errors from system limitations