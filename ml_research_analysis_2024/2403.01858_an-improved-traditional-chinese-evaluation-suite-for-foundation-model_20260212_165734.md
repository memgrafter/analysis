---
ver: rpa2
title: An Improved Traditional Chinese Evaluation Suite for Foundation Model
arxiv_id: '2403.01858'
source_url: https://arxiv.org/abs/2403.01858
tags:
- chinese
- question
- figure
- answer
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TMMLU+ is a new benchmark for evaluating Traditional Chinese language
  understanding. It contains 22,690 multi-choice questions across 66 subjects, six
  times larger than its predecessor TMMLU, with balanced subject distribution and
  a dedicated development set.
---

# An Improved Traditional Chinese Evaluation Suite for Foundation Model

## Quick Facts
- **arXiv ID**: 2403.01858
- **Source URL**: https://arxiv.org/abs/2403.01858
- **Reference count**: 40
- **Primary result**: TMMLU+ benchmark contains 22,690 Traditional Chinese questions across 66 subjects, showing Traditional Chinese models lag Simplified Chinese models by 19%

## Executive Summary
TMMLU+ is a comprehensive evaluation benchmark for Traditional Chinese language understanding, containing 22,690 multi-choice questions across 66 subjects ranging from primary to professional levels. The benchmark is six times larger than its predecessor and features balanced subject distribution with a dedicated development set. Evaluations on 23 Chinese language models reveal that Traditional Chinese models still underperform their Simplified Chinese counterparts by at least 19%, with no model exceeding human performance (68.2% average). The study also identifies tokenizer fertility score as a critical factor, showing a strong negative correlation (-0.742) with model performance.

## Method Summary
TMMLU+ provides a standardized evaluation framework for Traditional Chinese language models using multi-choice question answering. The dataset contains 22,690 questions across 66 subjects, organized into STEM, Humanities, Social Science, and Other categories. Models are evaluated using zero-shot and few-shot prompting strategies (5 examples from a dedicated dev set), with chain-of-thought prompting also tested. Performance is measured as average accuracy across subjects, then averaged by category and overall. The study also analyzes tokenizer efficiency through fertility scores, measuring the correlation between tokenization quality and downstream performance.

## Key Results
- TMMLU+ contains 22,690 questions across 66 subjects, six times larger than predecessor
- Traditional Chinese models trail Simplified Chinese models by at least 19% on average
- No model exceeds human performance of 68.2% average accuracy
- Fertility score shows strong negative correlation (-0.742) with model performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TMMLU+ is six times larger than its predecessor, providing broader coverage and more balanced subject distribution.
- Mechanism: Increased dataset size allows for more comprehensive evaluation across 66 subjects ranging from primary to professional levels, reducing the risk of overfitting to specific topics.
- Core assumption: Larger dataset with balanced distribution leads to more reliable model evaluation and better identification of model weaknesses.
- Evidence anchors:
  - [abstract] "TMMLU+ is six times larger and boasts a more balanced subject distribution than its predecessor"
  - [section] "TMMLU+ contains 22,690 questions across 66 subjects, ranging from primary, secondary, undergraduate, and professional levels of education"
  - [corpus] Weak - No direct citation of larger dataset leading to better evaluation in related papers
- Break condition: If the dataset becomes too large without maintaining quality control, evaluation accuracy may decrease due to potential inclusion of noisy or redundant questions.

### Mechanism 2
- Claim: Fertility score of tokenizers shows strong negative correlation with model performance, highlighting tokenizer design as a key factor in model effectiveness.
- Mechanism: Lower fertility scores indicate more efficient tokenization, allowing transformer modules to process information more effectively with shorter token sequences.
- Core assumption: Tokenizer design significantly impacts model performance, and lower fertility scores lead to better downstream performance.
- Evidence anchors:
  - [abstract] "The fertility score of tokenizers shows strong negative correlation (-0.742) with model performance"
  - [section] "Our result, as shown in Figure 4, reveals a significant Pearson correlation coefficient of -0.742 between fertility score and average accuracy"
  - [corpus] Weak - No direct citation of tokenizer fertility impact in related papers
- Break condition: If other tokenization metrics become more important than fertility score, or if model architectures evolve to handle higher fertility better.

### Mechanism 3
- Claim: Traditional Chinese models still trail behind Simplified Chinese counterparts by at least 19%, indicating a need for more focused advancements in LLMs catering to Traditional Chinese.
- Mechanism: Linguistic differences between Traditional and Simplified Chinese (shared only about 30% of vocabulary) create challenges for models trained on one variant when processing the other.
- Core assumption: The lexical divergence and different character meanings between Traditional and Simplified Chinese significantly impact model performance.
- Evidence anchors:
  - [abstract] "Traditional Chinese models still trail behind their Simplified Chinese counterparts by at least 19%"
  - [section] "Traditional and Simplified Chinese characters share only about 30% of their vocabulary"
  - [corpus] Weak - No direct citation of Simplified vs Traditional Chinese performance gap in related papers
- Break condition: If future models become more adept at handling both variants simultaneously, or if the gap narrows through improved training techniques.

## Foundational Learning

- Concept: Tokenizer design and fertility scores
  - Why needed here: Understanding how tokenizer efficiency impacts model performance is crucial for interpreting the strong negative correlation between fertility scores and TMMLU+ benchmark results
  - Quick check question: How does a lower fertility score in tokenizers contribute to better model performance in the TMMLU+ benchmark?

- Concept: Cultural and linguistic differences between Traditional and Simplified Chinese
  - Why needed here: Recognizing the significant lexical divergence and different character meanings is essential for understanding why Traditional Chinese models lag behind Simplified Chinese models by at least 19%
  - Quick check question: What percentage of vocabulary is shared between Traditional and Simplified Chinese, and how does this impact model performance?

- Concept: Multi-choice question answering format and subject categorization
  - Why needed here: Understanding the evaluation format and subject categories (STEM, Humanities, Social Science, Other) is crucial for interpreting model performance across different domains
  - Quick check question: How many subjects are included in TMMLU+, and what are the four main categories used for evaluation?

## Architecture Onboarding

- Component map: Data collection -> Dataset creation -> Model evaluation -> Performance analysis -> Tokenizer analysis
- Critical path: 1) Data collection and preprocessing, 2) Model evaluation using zero-shot and few-shot prompting, 3) Performance analysis across subjects and categories, 4) Tokenizer fertility score correlation analysis
- Design tradeoffs: Larger dataset size vs. quality control, balanced subject distribution vs. depth in specific areas, traditional Chinese focus vs. broader applicability to other languages
- Failure signatures: Inconsistent performance across subjects, weak correlation between tokenizer metrics and performance, models performing well on individual subjects but poorly on overall average
- First 3 experiments:
  1. Evaluate a small set of models on a subset of TMMLU+ subjects to verify the benchmark's functionality and identify any data quality issues
  2. Test the impact of different prompting strategies (zero-shot vs. few-shot) on model performance to validate the evaluation methodology
  3. Analyze the correlation between tokenizer fertility scores and model performance using a small set of 7B parameter models to confirm the reported -0.742 correlation coefficient

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training differences cause Traditional Chinese language models to underperform Simplified Chinese models by at least 19%, despite comparable parameter sizes?
- Basis in paper: [explicit] The paper shows Traditional Chinese models trail Simplified Chinese counterparts by at least 19% and mentions discrepancies among models with comparable parameter sizes attributed to different developmental approaches
- Why unresolved: The study identifies the performance gap but doesn't isolate which architectural choices, tokenization strategies, or training data characteristics create this difference
- What evidence would resolve it: Controlled experiments comparing Traditional vs Simplified Chinese models with identical architectures but different tokenizers, or ablation studies isolating the impact of vocabulary size, pretraining corpus composition, and tokenizer design

### Open Question 2
- Question: Why does chain-of-thought prompting consistently degrade performance on TMMLU+ compared to direct answer prompting, contrary to expectations from other domains?
- Basis in paper: [explicit] The paper finds chain-of-thought prompting degrades performance in five-shot settings, contrasting with improvements seen in GPT-4 and Gemini-Pro for STEM subjects in MMLU
- Why unresolved: The paper attributes this to "insufficient relevance among bridging objects" but doesn't systematically analyze what makes Traditional Chinese reasoning chains less effective or whether this is language-specific
- What evidence would resolve it: Detailed error analysis categorizing why CoT fails (misunderstanding, irrelevant reasoning steps, cultural context issues) and comparison with English CoT performance on analogous benchmarks

### Open Question 3
- Question: What is the optimal tokenizer vocabulary size and design for Traditional Chinese language models to maximize downstream performance on TMMLU+?
- Basis in paper: [explicit] The paper finds strong negative correlation (-0.742) between tokenizer fertility scores and benchmark performance, with 59.5% of Traditional Chinese characters encoded with 256 bytes in Taiwan-LLaMA
- Why unresolved: While fertility score correlates with performance, the paper doesn't determine whether this is causal or if there's an optimal vocabulary size that balances compression efficiency with linguistic coverage
- What evidence would resolve it: Systematic scaling studies varying tokenizer vocabulary sizes for Traditional Chinese models while holding other factors constant, measuring the impact on TMMLU+ and other downstream tasks

## Limitations

- **Data Quality and Representativeness**: The paper does not provide detailed analysis of question quality control or whether the 66 subjects adequately represent Traditional Chinese knowledge domains
- **Cross-linguistic Generalizability**: The study does not explore whether performance gaps would persist if models were trained on both Traditional and Simplified Chinese simultaneously
- **Evaluation Methodology Constraints**: The use of zero-shot and few-shot prompting may not fully capture the capabilities of larger models that could perform better with more sophisticated prompting strategies

## Confidence

**High Confidence Claims**:
- TMMLU+ contains 22,690 questions across 66 subjects (supported by dataset statistics)
- Traditional Chinese models perform 19% worse than Simplified Chinese counterparts (supported by direct model evaluations)
- No model exceeds human performance of 68.2% average accuracy (supported by reported results)

**Medium Confidence Claims**:
- Fertility score shows strong negative correlation (-0.742) with model performance (statistically significant but based on specific model set)
- Balanced subject distribution improves evaluation reliability (logical but not empirically validated in the paper)
- Traditional and Simplified Chinese share only 30% of vocabulary (cited from external source)

**Low Confidence Claims**:
- Larger dataset size necessarily leads to better evaluation (assumed but not validated)
- Tokenizer fertility is the primary factor affecting model performance (one of several possible factors)
- The 19% performance gap is solely due to lexical divergence (oversimplified explanation)

## Next Checks

1. **Replication Study with Additional Models**: Evaluate 5-10 additional Chinese language models not included in the original study (particularly newer models released after the paper's publication) to verify the consistency of the 19% performance gap between Traditional and Simplified Chinese models and to test whether the fertility score correlation holds across a broader model set.

2. **Tokenizer Ablation Analysis**: Conduct controlled experiments where the same model architecture uses different tokenizers with varying fertility scores while keeping all other factors constant, to isolate the causal relationship between tokenizer efficiency and downstream performance on TMMLU+.

3. **Cross-Variant Fine-tuning Experiment**: Take high-performing Simplified Chinese models and fine-tune them on Traditional Chinese data using the TMMLU+ dataset, then compare performance gains against native Traditional Chinese models to determine whether the performance gap is primarily due to training data differences or inherent architectural limitations.