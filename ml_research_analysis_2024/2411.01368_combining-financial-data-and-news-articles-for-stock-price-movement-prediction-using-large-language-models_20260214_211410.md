---
ver: rpa2
title: Combining Financial Data and News Articles for Stock Price Movement Prediction
  Using Large Language Models
arxiv_id: '2411.01368'
source_url: https://arxiv.org/abs/2411.01368
tags:
- price
- news
- stock
- company
- articles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents an LLM-based approach to predict stock price
  movements using a combination of financial data, historical prices, and news articles.
  The method employs retrieval-augmented generation to extract relevant news information
  and prompts pre-trained LLMs (GPT-3/4, LLaMA-2/3) in zero to four-shot settings.
---

# Combining Financial Data and News Articles for Stock Price Movement Prediction Using Large Language Models

## Quick Facts
- arXiv ID: 2411.01368
- Source URL: https://arxiv.org/abs/2411.01368
- Authors: Ali Elahi; Fatemeh Taghvaei
- Reference count: 27
- Key outcome: LLM-based approach achieves 58.5% weighted F1-score for 3-month and 59.1% for 6-month stock price movement predictions using financial data plus news articles

## Executive Summary
This study presents a novel approach to stock price movement prediction by combining financial data, historical prices, and news articles using large language models (LLMs). The method employs retrieval-augmented generation to extract relevant news information and prompts pre-trained LLMs (GPT-3/4, LLaMA-2/3) in zero to four-shot settings. Using a dataset of 20 high-volume stocks, the model achieves competitive performance with a weighted F1-score of 58.5% for 3-month and 59.1% for 6-month predictions, with GPT-4 and LLaMA3-8B emerging as top performers.

The research demonstrates that multimodal LLMs can effectively process both structured financial metrics and unstructured news content to predict stock movements. Notably, the study finds that few-shot learning did not significantly improve outcomes, likely due to prompt length constraints exceeding model token limits. This work highlights the potential of combining financial and qualitative data sources for improved financial forecasting while identifying important limitations in current prompting approaches.

## Method Summary
The method combines financial reports (10-K), historical price data, and news articles for 20 high-volume stocks. A retrieval module extracts relevant news chunks using embedding-based techniques, which are then combined with financial metrics and company information. The multimodal data is formatted into prompts for pre-trained LLMs (GPT-3/4, LLaMA-2/3) in zero, two, and four-shot settings. The model performs binary classification of stock price movement direction, evaluated using weighted F1-score and Matthews Correlation Coefficient.

## Key Results
- Achieved 58.5% weighted F1-score for 3-month and 59.1% for 6-month stock price movement predictions
- GPT-4 and LLaMA3-8B emerged as top-performing models
- Few-shot learning did not significantly improve performance due to prompt length constraints
- Matthews Correlation Coefficient of 0.175 indicates moderate predictive capability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented prompts with financial and news data improve stock movement prediction accuracy.
- Mechanism: The model retrieves relevant news chunks and combines them with structured financial metrics before prompting the LLM, allowing the model to contextualize numerical data with qualitative information.
- Core assumption: Financial data alone is insufficient for predicting price movements; qualitative news context improves predictions.
- Evidence anchors:
  - [abstract] "We utilize retrieval augmentation techniques to retrieve and attach relevant chunks of news articles to financial metrics related to a company and prompt the LLMs"
  - [section] "We utilize retrieval augmentation techniques to retrieve and attach relevant chunks of news articles to financial metrics related to a company and prompt the LLMs in zero, two, and four-shot settings"
  - [corpus] Weak evidence - no direct corpus support for retrieval augmentation effectiveness in this specific setup
- Break condition: If retrieved news chunks are irrelevant or too generic, the model cannot improve predictions beyond financial data alone.

### Mechanism 2
- Claim: Few-shot prompting does not significantly improve performance due to prompt length constraints.
- Mechanism: Adding examples increases prompt token count beyond model limits, causing confusion rather than improved task understanding.
- Core assumption: The model's attention span is limited by token capacity, and longer prompts degrade performance.
- Evidence anchors:
  - [section] "Noteworthy is the fact that some prompts in four-shot learning were so large that they exceeded the maximum input token limit of LLaMA2 models"
  - [section] "we observed that adding more examples in two-shot and four-shot learning did not significantly improve the performance of these top models"
  - [corpus] No corpus evidence directly supporting this specific claim about few-shot performance degradation
- Break condition: If prompt length can be managed within token limits, few-shot learning might improve performance.

### Mechanism 3
- Claim: Financial and news data combination in multimodal format enables effective classification by pre-trained LLMs.
- Mechanism: LLMs can process both structured (tabular financial data) and unstructured (news articles) data when properly formatted in prompts, leveraging their pre-training on diverse data types.
- Core assumption: Pre-trained LLMs have sufficient capability to interpret tabular data when presented in natural language format.
- Evidence anchors:
  - [abstract] "Recent research in LLMs has demonstrated that they are able to perform both tabular and text classification tasks"
  - [section] "We introduce an LLM-based classifier capable of performing classification tasks using combination of tabular (structured) and textual (unstructured) data"
  - [corpus] Weak evidence - corpus contains related papers but no direct validation of this specific multimodal approach
- Break condition: If LLMs cannot effectively parse tabular data format, the multimodal approach fails to outperform single-modality methods.

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: To extract relevant news information from large datasets and attach it to financial metrics before prompting
  - Quick check question: How does the retrieval step determine which news chunks are most relevant to a given company and date?

- Concept: Few-shot prompting vs fine-tuning
  - Why needed here: Understanding when to use in-context learning versus parameter updates for task adaptation
  - Quick check question: What are the computational tradeoffs between few-shot prompting and fine-tuning for financial prediction tasks?

- Concept: Classification metrics (F1-score, MCC, weighted F1)
  - Why needed here: To properly evaluate model performance on imbalanced financial datasets
  - Quick check question: Why is weighted F1-score preferred over simple accuracy for this imbalanced binary classification problem?

## Architecture Onboarding

- Component map:
  Data ingestion layer (financial reports, historical prices, news articles) -> Retrieval module (embedding-based chunk selection) -> Prompt generation pipeline (formatting multimodal data) -> LLM inference layer (multiple model variants) -> Evaluation module (metric calculation and statistical significance)

- Critical path:
  Data collection → Retrieval augmentation → Prompt formatting → LLM inference → Binary classification → Metric evaluation

- Design tradeoffs:
  - Retrieval granularity vs. information loss (chunk size selection)
  - Prompt length vs. model token limits
  - Few-shot examples vs. prompt complexity
  - Model parameter count vs. computational cost

- Failure signatures:
  - Low recall indicates retrieval module missing relevant news
  - High standard deviation in WF1 suggests model instability
  - Performance degradation with few-shot learning indicates prompt length issues

- First 3 experiments:
  1. Compare retrieval performance using different chunk sizes (1-sentence vs 3-sentence vs full article)
  2. Test zero-shot performance with and without financial data to isolate news contribution
  3. Evaluate model performance across different market conditions (bull vs bear markets)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLM-based stock prediction models change when using fine-tuned smaller language models compared to few-shot prompted large language models?
- Basis in paper: [explicit] The paper states that the authors plan to shift focus to fine-tuning smaller language models with a combination of textual and tabular data in future studies.
- Why unresolved: The current study only used few-shot prompting with pre-trained LLMs and did not explore fine-tuning approaches.
- What evidence would resolve it: A comparative study measuring the performance (e.g., weighted F1-score, MCC) of fine-tuned smaller models against few-shot prompted large models on the same dataset and task.

### Open Question 2
- Question: Does predicting the percentage change in stock price as a regression task yield better results than binary classification of price movement direction?
- Basis in paper: [explicit] The paper mentions that predicting the percentage of change in forward return in a regression setting will be the next area of focus.
- Why unresolved: The current study only performed binary classification of price movement direction.
- What evidence would resolve it: A study comparing the performance of regression models (e.g., mean absolute error, R-squared) to binary classification models on the same stock price prediction task.

### Open Question 3
- Question: How does the choice of summarization method (extractive vs. abstractive) affect the performance of LLM-based stock price movement prediction?
- Basis in paper: [explicit] The paper experimented with both extractive and abstractive summarization techniques for news article retrieval.
- Why unresolved: While the paper mentions using extractive summarization with OpenAI embeddings, it does not provide a direct comparison of the two methods' impact on prediction performance.
- What evidence would resolve it: A study comparing the performance of LLM-based models using extractive vs. abstractive summarization on the same dataset and task, with results measured by metrics like weighted F1-score and MCC.

## Limitations

- Limited dataset scope with only 20 high-volume stocks reduces generalizability
- Absence of direct corpus validation for key mechanisms (retrieval augmentation effectiveness, multimodal LLM capabilities)
- Performance findings may be specific to the particular stock set and time period studied

## Confidence

- High confidence: Basic feasibility of multimodal LLM classification using structured and unstructured data
- Medium confidence: Specific retrieval-augmented approach and its contribution to prediction accuracy
- Low confidence: Generalizability of results beyond the specific stock set and time period studied

## Next Checks

1. Test the retrieval module's precision and recall by evaluating whether relevant news articles are correctly identified and irrelevant articles are filtered out
2. Conduct ablation studies to quantify the exact contribution of news articles versus financial data alone to prediction performance
3. Validate model performance across different market regimes (bull vs bear markets) to assess robustness to changing market conditions