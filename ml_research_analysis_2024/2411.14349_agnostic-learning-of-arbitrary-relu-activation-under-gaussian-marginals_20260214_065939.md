---
ver: rpa2
title: Agnostic Learning of Arbitrary ReLU Activation under Gaussian Marginals
arxiv_id: '2411.14349'
source_url: https://arxiv.org/abs/2411.14349
tags:
- relu
- learning
- have
- vupdate
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the fundamental question of whether a single
  ReLU neuron can be learned in the non-realizable (agnostic) setting. Despite being
  a basic building block of neural networks, the learnability of arbitrary ReLU activations
  under Gaussian marginals with squared loss has remained open.
---

# Agnostic Learning of Arbitrary ReLU Activation under Gaussian Marginals

## Quick Facts
- **arXiv ID**: 2411.14349
- **Source URL**: https://arxiv.org/abs/2411.14349
- **Reference count**: 40
- **Primary result**: Polynomial-time SQ algorithm achieving O(OPT) + ε loss for agnostic learning of single ReLU neuron with arbitrary bias under Gaussian marginals

## Executive Summary
This paper addresses the fundamental question of whether a single ReLU neuron can be learned in the agnostic setting under Gaussian marginals. The authors present the first polynomial-time algorithm achieving O(OPT) + ε loss guarantee, where OPT is the optimal loss. The key insight is that standard gradient-based methods fail when the ReLU bias is highly negative, requiring a novel approach that combines thresholded PCA for initialization with a reweighted projected gradient descent. The paper also establishes a strong lower bound showing that no correlational statistical query (CSQ) algorithm can achieve constant-factor approximation, creating a separation between SQ and CSQ algorithms for this basic problem.

## Method Summary
The algorithm uses a two-stage approach: first, thresholded PCA with threshold τ = 1/|b| provides a coarse initialization w0 that is sufficiently close to the true direction v; second, a reweighted projected gradient descent with parameters ρ = 0.5 and λ = 0.9 performs fine-tuning while overcoming the correlation failure mode of standard gradient descent in the high-bias regime. The algorithm requires a grid search over bias and norm estimates with accuracy 0.1/√ε, and succeeds when the bias is sufficiently negative (b < -√(α/log α)).

## Key Results
- First polynomial-time algorithm achieving O(OPT) + ε loss for agnostic learning of single ReLU neuron
- Novel reweighted projected gradient descent overcomes gradient descent limitations in high-bias regime
- Strong CSQ lower bound showing exponential query complexity or superpolynomial tolerance required
- Establishes first separation between SQ and CSQ algorithms for the simplest neural network setting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reweighted PGD with shifted Gaussian measure overcomes gradient descent failure when ReLU bias is very negative
- Mechanism: Shifts Gaussian mean along current weight direction by ρ|b| and conditions updates on xw ≥ (ρ + λ - λ²ρ)|b|, suppressing noise contribution while preserving ReLU signal
- Core assumption: When ⟨wt, v⟩ ≥ λ, reweighted measure reduces problem to moderate-bias regime where ReLU dominates noise
- Evidence anchors: Abstract shows departure from gradient-based methods; section 3.1 details reweighting technique; corpus provides weak direct evidence
- Break condition: If ⟨w0, v⟩ < 0.9, reweighting cannot dominate noise and algorithm fails

### Mechanism 2
- Claim: Thresholded PCA with τ = 1/|b| provides coarse initialization w0 sufficiently close to v
- Mechanism: Thresholding on |y| ≥ 1/|b| removes most noise while preserving optimal ReLU contribution; top eigenvector approximates v in direction
- Core assumption: When b is sufficiently negative, adversary can only remove small fraction of ReLU contribution below threshold
- Evidence anchors: Section 3.2 describes algorithm using τ = 1/|b|; Lemma 3.11 shows eigenvector correlation for negative b; corpus provides weak direct evidence
- Break condition: If b > -√(α/log α), thresholded PCA cannot provide required initialization accuracy

### Mechanism 3
- Claim: CSQ-CSQ separation establishes fundamental limitations of gradient-based methods
- Mechanism: Lower bound shows no CSQ algorithm can achieve O(OPT) error, requiring exponential queries or superpolynomial tolerance
- Core assumption: Learning single ReLU neuron with arbitrary bias is computationally hard for CSQ algorithms due to correlation structure in high-bias regime
- Evidence anchors: Abstract states no polynomial-time CSQ algorithm achieves constant factor approximation; section 4 analyzes Hermite expansion and pairwise correlation; corpus provides moderate direct evidence
- Break condition: If noise structure changes, separation might not hold

## Foundational Learning

- **Concept**: Hermite polynomial expansion of ReLU functions
  - Why needed here: CSQ lower bound construction analyzes Hermite expansion of σ(x - b) to show removing low-order components preserves L² norm, creating functions with small pairwise correlation
  - Quick check question: What is the value of ⟨σ(x - b), H₁⟩N for b < 0, and how does it scale as b → -∞?

- **Concept**: Mills ratio and Gaussian tail bounds
  - Why needed here: Extensively used to bound integrals involving Gaussian tails in both algorithmic analysis (reweighted PGD) and lower bound construction
  - Quick check question: What is the asymptotic expansion of m(t) = (1 - Φ(t))/ϕ(t) as t → ∞, and what is the error bound for truncating after k terms?

- **Concept**: Correlation statistical query (CSQ) dimension
  - Why needed here: Lower bound construction uses CSQ dimension to show learning ReLU function is hard for CSQ algorithms, establishing separation from SQ algorithms
  - Quick check question: Given function class G with CSDD(G, γ, β) = D, what is the relationship between D, γ, and β that determines hardness of learning?

## Architecture Onboarding

- **Component map**: Grid search over (∥v∥, b) estimates → Thresholded PCA (τ = 1/|b|) → Coarse initialization w0 → Reweighted PGD (ρ = 0.5, λ = 0.9) → Final ReLU neuron

- **Critical path**: 1) Grid search over (∥v∥, b) space 2) For each guess, run thresholded PCA to get w0 3) Run reweighted PGD starting from w0 4) Return best result across all grid search points

- **Design tradeoffs**: Grid search accuracy vs runtime (0.1/√ε accuracy requires O(1/ε) grid points); threshold value τ = 1/|b| balances noise removal vs signal preservation; reweighting parameter ρ balances Gaussian shift amount vs noise suppression

- **Failure signatures**: If reweighted PGD doesn't converge → likely w0 too far from v (check ⟨w0, v⟩ < 0.9); if thresholded PCA fails → likely b not sufficiently negative (check if b > -√(α/log α)); if overall algorithm fails → likely grid search missed correct (∥v∥, b) region

- **First 3 experiments**: 1) Test thresholded PCA with known b < 0 on synthetic data where v is known; verify w0 has ⟨w0, v⟩ ≥ 0.9 2) Test reweighted PGD with initialization w0 close to v; verify convergence to direction v 3) Test end-to-end algorithm on synthetic data with varying b (moderate vs very negative); verify O(OPT) + ε guarantee holds only for sufficiently negative b

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the precise characterization of the optimal bias parameter b for which gradient descent fails in ReLU regression?
- **Basis in paper**: [explicit] Paper demonstrates gradient descent fails as b approaches negative infinity but doesn't characterize exact threshold where failure occurs
- **Why unresolved**: Analysis shows gradient descent fails in b → -∞ regime but doesn't provide precise boundary where failure begins
- **What evidence would resolve it**: Quantitative analysis showing exact relationship between bias magnitude and correlation dominance, potentially identifying critical value of b where correlation between ReLU and noise becomes problematic

### Open Question 2
- **Question**: Can the reweighted projected gradient descent algorithm be extended to learn neural networks with multiple ReLU neurons?
- **Basis in paper**: [inferred] Algorithm successfully learns single ReLU neuron by overcoming gradient descent limitations, suggesting potential applicability to more complex architectures
- **Why unresolved**: Paper focuses on single neuron case and doesn't explore whether reweighting technique generalizes to deeper networks
- **What evidence would resolve it**: Experimental results or theoretical analysis showing algorithm's performance on networks with multiple hidden layers or multiple neurons per layer

### Open Question 3
- **Question**: What is the relationship between correlation statistical query (CSQ) dimension and statistical query (SQ) dimension for learning ReLU activations?
- **Basis in paper**: [explicit] Paper establishes separation between CSQ and SQ algorithms, showing CSQ algorithms require exponentially many queries or super-polynomial tolerance
- **Why unresolved**: While paper shows separation exists, doesn't characterize relationship between these two dimensions for this specific problem
- **What evidence would resolve it**: Tight bound on both CSQ and SQ dimensions for ReLU learning, showing exactly how much separation exists between these two models

## Limitations

- The algorithm only guarantees success for sufficiently negative bias values (b < -√(α/log α)), leaving moderate and positive bias regimes unresolved
- The CSQ lower bound relies on specific properties of Gaussian noise distribution and may not extend to other noise models
- The algorithm requires polynomial sample complexity and runtime, but the paper doesn't establish whether this is optimal or could be improved

## Confidence

- **High Confidence**: The SQ algorithm achieves O(OPT) + ε loss for sufficiently negative b, with well-supported algorithmic framework and key technical lemmas
- **Medium Confidence**: The CSQ lower bound establishing separation between SQ and CSQ algorithms, though practical implications for real-world neural network training remain unclear
- **Low Confidence**: Complete characterization of when algorithm succeeds across all possible bias values, as paper only guarantees success for sufficiently negative b

## Next Checks

1. **Algorithmic Robustness**: Implement full algorithm and test on synthetic data with varying bias values (negative, zero, positive). Measure whether O(OPT) + ε guarantee holds only for sufficiently negative b as claimed, and characterize transition behavior around critical threshold.

2. **Lower Bound Generality**: Examine whether CSQ lower bound construction extends to other noise distributions beyond Gaussian marginals. Test algorithm's performance under alternative noise models (e.g., uniform, sub-Gaussian) to understand scope of SQ-CSQ separation.

3. **Practical Parameter Sensitivity**: Investigate sensitivity of algorithm to key parameters (threshold τ = 1/|b|, reweighting parameter ρ, conditioning threshold). Determine whether theoretical choices provide robustness to estimation errors and finite-sample effects in practical implementations.