---
ver: rpa2
title: Can an unsupervised clustering algorithm reproduce a categorization system?
arxiv_id: '2408.10340'
source_url: https://arxiv.org/abs/2408.10340
tags:
- clustering
- distance
- metric
- metrics
- ground
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether unsupervised clustering algorithms
  can accurately reproduce expert-provided categorization systems, using Morningstar
  mutual fund categories as a real-world test case. The authors show that standard
  K-means clustering with Euclidean distance typically fails to reproduce ground truth
  classes, even when optimized with internal or external evaluation metrics.
---

# Can an unsupervised clustering algorithm reproduce a categorization system?

## Quick Facts
- arXiv ID: 2408.10340
- Source URL: https://arxiv.org/abs/2408.10340
- Authors: Nathalia Castellanos; Dhruv Desai; Sebastian Frank; Stefano Pasquali; Dhagash Mehta
- Reference count: 40
- Primary result: Standard K-means clustering fails to reproduce ground truth classes, but succeeds when combined with supervised distance metric learning methods like RF-PHATE

## Executive Summary
This study investigates whether unsupervised clustering algorithms can accurately reproduce expert-provided categorization systems using Morningstar mutual fund categories as a real-world test case. The authors demonstrate that standard K-means clustering with Euclidean distance typically fails to reproduce ground truth classes, even when optimized with internal or external evaluation metrics. However, when combined with supervised distance metric learning methods like Mahalanobis or RF-PHATE, K-means can accurately reproduce the categorization system. The results show that RF-PHATE, in particular, significantly outperforms traditional approaches, achieving near-perfect clustering accuracy (up to 94.8%) and matching the number of ground truth classes in 75% of cases.

## Method Summary
The study evaluates K-means clustering with three different distance metrics: Euclidean distance (baseline), Mahalanobis distance (supervised metric learning), and RF-PHATE (Random Forest-based Potential of Heat-diffusion for Affinity-based Transition Embedding). The methodology involves training Random Forest classifiers on labeled datasets to generate proximity measures, applying distance metric learning, and then performing K-means clustering with the learned metrics. The approach is tested on toy datasets from UCI ML Repository and Morningstar mutual fund data (10.5K funds across 124 categories with 14 numerical and 2 categorical variables). Clustering performance is evaluated using both internal metrics (Inertia, Silhouette Score, Calinski-Harabasz Index, Davies-Bouldin Index, Gap Statistics) and external metrics (Homogeneity, Completeness, V-measure, Rand Index, Adjusted Rand Index, Normalized Mutual Information, Fowlkes-Mallows Score).

## Key Results
- Standard K-means with Euclidean distance fails to reproduce ground truth classes even with optimal cluster count selection
- RF-PHATE transformation significantly improves clustering accuracy, achieving up to 94.8% accuracy
- RF-PHATE matches ground truth class counts in 75% of cases compared to only 50% for traditional metrics
- The number of ground truth classes must be known a priori for Euclidean distance K-means to work

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised distance metric learning enables K-means to reproduce ground truth classes by transforming feature space to align with class boundaries
- Mechanism: Random Forest-based proximity learning captures non-linear feature relationships and class-specific distance patterns that Euclidean distance misses
- Core assumption: The input features contain sufficient information to predict ground truth labels
- Evidence anchors:
  - [abstract] "if appropriate features are available in the dataset, and a proper distance metric is known (e.g., using a supervised Random Forest-based distance metric learning method), then an unsupervised clustering can indeed reproduce the ground truth classes"
  - [section 2.3] "RF-based distance metric learning method based on Random Forest (RF) proximities"
  - [corpus] Found related work on "Fast unsupervised ground metric learning" suggesting supervised metric learning improves clustering performance
- Break condition: Features lack predictive power for target variable, causing RF to fail at learning meaningful distance metrics

### Mechanism 2
- Claim: Internal clustering evaluation metrics fail to identify optimal cluster counts because they optimize for compactness rather than class separation
- Mechanism: Metrics like Silhouette Score optimize within-cluster cohesion but don't account for ground truth class structure, leading to suboptimal K values
- Core assumption: Internal metrics are not designed to recover ground truth class structure when class labels are unavailable
- Evidence anchors:
  - [section 3.1] "evaluating unsupervised clustering algorithms is challenging as by definition there is no ground truth label"
  - [section 6.2] "internal evaluation metrics often fail when attempting to reproduce the ground truth classes as clusters"
  - [corpus] Weak corpus evidence for specific clustering metric limitations, but general understanding of metric design
- Break condition: When ground truth classes happen to align with natural cluster structure that internal metrics optimize for

### Mechanism 3
- Claim: RF-PHATE improves clustering by learning both local and global data structures through diffusion processes
- Mechanism: The algorithm learns GAP proximities from RF, applies diffusion to capture global relationships, then uses MDS to find optimal dimensionality for K-means
- Core assumption: The data contains meaningful local and global structures that can be captured through diffusion
- Evidence anchors:
  - [section 2.3] "RF-based PHATE, or RF-PHATE, algorithm follows the following steps: (1) train an RF on the given dataset to predict labels and generate GAP proximities; (2) learn the local structure... (3) learn the global structure... (4) extract the local and global structure via the potential distances"
  - [section 6.2] "RF-PHATE transformations significantly improved the performance of internal evaluation metrics"
  - [corpus] Found related work on "Unsupervised Optimisation of GNNs for Node Clustering" suggesting graph-based methods capture complex structures
- Break condition: When data lacks meaningful local/global structure or when diffusion process oversmooths the data

## Foundational Learning

- Concept: Random Forest proximity calculation and its relationship to distance metrics
  - Why needed here: Understanding how RF proximities are computed is essential for grasping how RF-PHATE learns distance metrics
  - Quick check question: How does the GAP proximity formula in Equation 1 differ from simple leaf co-occurrence counts?

- Concept: Evaluation metric types (internal vs external) and their appropriate use cases
  - Why needed here: The paper demonstrates that external metrics perform better than internal metrics for this task
  - Quick check question: Why would an external metric like Adjusted Rand Index be more appropriate than Silhouette Score when ground truth labels exist?

- Concept: Distance metric learning theory and the role of supervised vs unsupervised learning
  - Why needed here: The core contribution is that supervised distance metric learning enables unsupervised clustering to reproduce ground truth
  - Quick check question: What is the key difference between Mahalanobis metric learning and Euclidean distance in the context of clustering?

## Architecture Onboarding

- Component map: Data preprocessing -> Random Forest training -> Distance metric learning (Mahalanobis/RF-PHATE) -> K-means clustering -> Evaluation framework -> Visualization

- Critical path:
  1. Train RF classifier on labeled data
  2. Generate GAP proximities from trained RF
  3. Apply RF-PHATE transformation to learn distance metric
  4. Run K-means clustering with learned distance metric
  5. Evaluate clustering results using external metrics

- Design tradeoffs:
  - RF-PHATE vs Mahalanobis: RF-PHATE handles mixed data types and captures non-linear relationships but is computationally heavier
  - Number of RF trees: More trees improve stability but increase computation time
  - MDS dimensionality: Higher dimensions preserve more structure but may overfit

- Failure signatures:
  - Low RF classification accuracy → Poor distance metric learning
  - Internal metrics suggest optimal K far from ground truth → Distance metric not capturing class structure
  - MDS stress doesn't decrease with dimensionality → Data lacks meaningful low-dimensional structure

- First 3 experiments:
  1. Run K-means with Euclidean distance on toy dataset and record evaluation metrics
  2. Train RF classifier and evaluate accuracy to verify feature predictive power
  3. Apply RF-PHATE transformation and compare clustering results with baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions do internal evaluation metrics reliably predict the optimal number of clusters relative to ground truth classes?
- Basis in paper: [explicit] The authors demonstrate that internal metrics like Silhouette Score and Calinski-Harabasz Index only achieve exact matches with ground truth class counts around 50% of the time, while external metrics perform significantly better.
- Why unresolved: The paper shows variability in metric performance across datasets but doesn't establish clear criteria for when internal metrics can be trusted.
- What evidence would resolve it: Empirical studies across diverse datasets identifying specific data characteristics (dimensionality, cluster separation, feature relevance) that correlate with reliable internal metric performance.

### Open Question 2
- Question: How does the performance of RF-PHATE compare to other advanced distance metric learning methods for clustering mixed-type data?
- Basis in paper: [explicit] The authors claim RF-PHATE has advantages over Mahalanobis metric learning for mixed variable types but don't compare against other state-of-the-art methods like deep metric learning approaches.
- Why unresolved: The paper only benchmarks against Mahalanobis and Euclidean metrics, leaving open questions about relative performance.
- What evidence would resolve it: Head-to-head comparisons of RF-PHATE against modern metric learning methods (deep metric learning, contrastive learning) on benchmark datasets with mixed feature types.

### Open Question 3
- Question: What is the minimum feature relevance threshold required for supervised distance metric learning to successfully reproduce ground truth classes?
- Basis in paper: [explicit] The authors find that RF-PHATE can reproduce ground truth classes when features are predictive enough, but don't quantify the exact threshold of predictive power needed.
- Why unresolved: While the paper demonstrates the approach works, it doesn't establish specific criteria for when it will fail due to insufficient feature relevance.
- What evidence would resolve it: Systematic experiments varying feature relevance through controlled feature degradation or feature selection, measuring the minimum predictive power needed for successful clustering reproduction.

## Limitations

- The approach requires labeled data for supervised distance metric learning, creating a potential circularity in evaluation
- Results may not generalize to datasets with weaker feature-label relationships or unknown class counts
- Computational cost of RF-PHATE may limit practical application to very large datasets

## Confidence

- High confidence: Core finding that Euclidean distance K-means typically fails to reproduce ground truth classes
- Medium confidence: Superiority of RF-PHATE over Mahalanobis, as results depend on specific dataset characteristics
- Medium confidence: Evaluation framework, though reliance on supervised learning for unsupervised clustering raises methodological questions

## Next Checks

1. Test the approach on a dataset with known but complex cluster structure (e.g., synthetic datasets with non-linear boundaries) to validate the mechanism of supervised distance metric learning
2. Perform ablation studies removing the RF classification step to assess whether distance metrics can be learned without explicit ground truth labels
3. Evaluate computational scalability by testing on larger datasets (100K+ samples) to determine practical limitations of the RF-PHATE approach