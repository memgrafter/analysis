---
ver: rpa2
title: Scaling Speech-Text Pre-training with Synthetic Interleaved Data
arxiv_id: '2411.17607'
source_url: https://arxiv.org/abs/2411.17607
tags:
- speech
- data
- tokens
- text
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of scaling speech-language models
  by addressing the scarcity of speech data compared to text data. The core method
  involves synthesizing interleaved speech-text data from text corpora using a text-to-token
  model, eliminating the need for parallel speech-text datasets.
---

# Scaling Speech-Text Pre-training with Synthetic Interleaved Data

## Quick Facts
- **arXiv ID**: 2411.17607
- **Source URL**: https://arxiv.org/abs/2411.17607
- **Reference count**: 40
- **Primary result**: State-of-the-art speech language modeling and spoken question answering, improving accuracy from 13% to 31%

## Executive Summary
This paper addresses the challenge of scaling speech-language models by addressing the scarcity of speech data compared to text data. The core method involves synthesizing interleaved speech-text data from text corpora using a text-to-token model, eliminating the need for parallel speech-text datasets. A supervised speech tokenizer with strong semantic preservation is employed, and the model is pre-trained on 1 trillion tokens, including 600 billion synthetic interleaved speech-text data. The primary results show state-of-the-art performance in speech language modeling and spoken question answering, improving accuracy from 13% to 31% on spoken questions tasks. Additionally, the fine-tuned model achieves competitive performance as an end-to-end spoken chatbot, demonstrating strong conversational abilities and speech quality.

## Method Summary
The approach constructs synthetic interleaved speech-text data by sampling text spans from existing text corpora and converting them to speech tokens using a text-to-token model, bypassing the need for parallel speech-text datasets. A supervised speech tokenizer derived from an ASR model with vector-quantized bottleneck preserves semantic information even at low frame rates (12.5Hz). The model is pre-trained on 1 trillion tokens total, including 30% text, 1 epoch each of unsupervised and supervised speech data, and the remainder as interleaved data. Fine-tuning on speech dialogue data enables development of an end-to-end spoken chatbot.

## Key Results
- State-of-the-art performance in speech language modeling and spoken question answering
- Accuracy improvement from 13% to 31% on spoken questions tasks
- Competitive performance as end-to-end spoken chatbot with strong conversational abilities and speech quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic interleaved speech-text data enables speech-language models to leverage large-scale text corpora for speech pre-training, addressing the data scarcity issue in speech modeling.
- Mechanism: By sampling text spans from existing text corpora and converting them into corresponding speech tokens using a text-to-token model, the approach generates large-scale interleaved speech-text data without requiring parallel speech-text datasets. This synthetic data bridges the modality gap between speech and text, allowing the model to learn cross-modal alignment and transfer text-based knowledge to speech representations.
- Core assumption: The text-to-token model can accurately convert text spans into speech tokens that preserve semantic meaning, and the interleaved structure facilitates effective cross-modal learning.
- Evidence anchors:
  - [abstract] "Our method efficiently constructs speech-text interleaved data by sampling text spans from existing text corpora and synthesizing corresponding speech spans using a text-to-token model, bypassing the need to generate actual speech."
  - [section] "Interleaved speech-text data consists of tokens where speech and text sequences are interleaved at the word level. For example, a sequence might look like: 'Today is <Speech 24> <Speech 5> ... <Speech 128> day'."
  - [corpus] Weak evidence: No direct empirical evaluation of semantic preservation of synthetic speech tokens in corpus.
- Break condition: If the text-to-token model fails to preserve semantic meaning in the synthesized speech tokens, or if the model cannot effectively learn cross-modal alignment from the interleaved data, the approach would not work.

### Mechanism 2
- Claim: The supervised speech tokenizer trained from an ASR model with a vector-quantized bottleneck preserves semantic information even at low frame rates (e.g., 12.5Hz), enabling efficient speech modeling.
- Mechanism: By incorporating a vector quantization layer into the ASR encoder, the tokenizer learns to map continuous speech representations to discrete tokens while maintaining semantic fidelity. The supervised training on ASR datasets ensures that the tokens capture linguistic content, and the lower frame rates reduce computational cost without significant information loss.
- Core assumption: The vector quantization process can effectively discretize speech representations while preserving semantic content, and lower frame rates provide a good trade-off between efficiency and quality.
- Evidence anchors:
  - [abstract] "We also employ a supervised speech tokenizer derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. This supervised training approach results in discrete speech tokens with strong semantic preservation even at lower frame rates (e.g. 12.5Hz), while still maintaining speech reconstruction quality."
  - [section] "We evaluate the content preservation and quality of generated speech by our speech decoder on LibriSpeech (Panayotov et al., 2015). The results are shown in Table 1. We measure the content preservation by the Word Error Rate (WER) between the transcription with an ASR model and the true transcription."
  - [corpus] Weak evidence: Limited comparison with other tokenization methods in terms of semantic preservation and efficiency in the corpus.
- Break condition: If the vector quantization process leads to significant semantic loss, or if lower frame rates result in inadequate speech representation, the approach would fail.

### Mechanism 3
- Claim: Fine-tuning the pre-trained speech-language model on speech dialogue data enables the development of an end-to-end spoken chatbot with competitive performance in conversational abilities and speech quality.
- Mechanism: After pre-training on synthetic interleaved data to learn speech-text alignment, the model is fine-tuned on a curated speech dialogue dataset. This fine-tuning adapts the model to handle speech interactions, generating both text and speech responses in a single forward pass, enabling seamless spoken dialogue.
- Core assumption: The pre-trained model has learned sufficient speech-text alignment and conversational knowledge to be effectively adapted for spoken dialogue tasks through fine-tuning.
- Evidence anchors:
  - [abstract] "We further demonstrate that by fine-tuning the pre-trained model with speech dialogue data, we can develop an end-to-end spoken chatbot that achieves competitive performance comparable to existing baselines in both conversational abilities and speech quality, even operating exclusively in the speech domain."
  - [section] "Following speech-text pre-training, we fine-tune our model for speech dialogue tasks using a dataset derived from Magpie (Xu et al., 2024). We use GPT-4 to adapt the original text-based dialogues for speech scenarios by filtering examples, shortening responses, and avoiding outputting text that cannot be read aloud."
  - [corpus] Weak evidence: Limited details on the fine-tuning process and evaluation metrics in the corpus.
- Break condition: If the fine-tuning process fails to adapt the model for spoken dialogue, or if the generated speech responses are of poor quality or lack conversational coherence, the approach would not work.

## Foundational Learning

- Concept: Speech tokenization and discrete speech representations
  - Why needed here: The model needs to process speech as discrete tokens to leverage the capabilities of language models, which operate on discrete symbol sequences.
  - Quick check question: What is the role of the vector quantization layer in the speech tokenizer, and how does it contribute to semantic preservation?

- Concept: Cross-modal learning and alignment between speech and text
  - Why needed here: The synthetic interleaved data bridges the modality gap between speech and text, enabling the model to learn cross-modal alignment and transfer knowledge between modalities.
  - Quick check question: How does the span corruption ratio in the interleaved data construction process affect the model's ability to learn cross-modal alignment?

- Concept: Speech synthesis and text-to-speech (TTS) models
  - Why needed here: The text-to-token model, which converts text spans into speech tokens, is based on TTS technology. Understanding TTS is crucial for grasping how the synthetic interleaved data is generated.
  - Quick check question: What is the difference between the text-to-token model used in this approach and traditional TTS models, and how does this difference contribute to efficiency?

## Architecture Onboarding

- Component map: Speech Tokenizer -> Text-to-Token Model -> Speech Decoder -> Speech-Language Model -> TTS Datasets
- Critical path:
  1. Train the supervised speech tokenizer on ASR datasets.
  2. Train the text-to-token model on TTS datasets to synthesize speech tokens from text.
  3. Construct synthetic interleaved speech-text data by sampling text spans and converting them to speech tokens.
  4. Pre-train the speech-language model on the synthetic interleaved data to learn cross-modal alignment.
  5. Fine-tune the pre-trained model on speech dialogue data for end-to-end spoken dialogue.
- Design tradeoffs:
  - Frame rate vs. semantic preservation: Lower frame rates reduce computational cost but may lead to information loss. The optimal frame rate (12.5Hz) balances efficiency and quality.
  - Span corruption ratio vs. cross-modal alignment: The ratio determines the proportion of speech and text tokens in the interleaved data. A higher ratio may lead to better speech modeling but weaker text alignment.
  - Pre-training data composition: The balance between text, speech, interleaved, and parallel data affects the model's performance on different tasks. More interleaved data improves speech modeling but may reduce text comprehension.
- Failure signatures:
  - High Word Error Rate (WER) in speech tokenization or synthesis, indicating semantic loss.
  - Poor performance on speech-language modeling or spoken question answering tasks, suggesting ineffective cross-modal alignment.
  - Low-quality speech responses or lack of conversational coherence in the spoken chatbot, indicating inadequate fine-tuning or model capacity.
- First 3 experiments:
  1. Train the speech tokenizer at different frame rates (e.g., 50Hz, 25Hz, 12.5Hz, 6.25Hz) and evaluate semantic preservation using WER on a held-out ASR dataset.
  2. Construct synthetic interleaved data with different span corruption ratios (e.g., 0.2, 0.3, 0.4, 0.5) and pre-train the speech-language model. Evaluate performance on speech-language modeling and spoken question answering tasks.
  3. Fine-tune the pre-trained model on speech dialogue data and evaluate the spoken chatbot's performance on general question answering, knowledge-based tasks, speech quality (UTMOS), and alignment between generated speech and text (ASR-WER).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal frame rate for balancing semantic preservation and model efficiency in speech tokenization?
- Basis in paper: [explicit] The paper compares different frame rates (50Hz, 25Hz, 12.5Hz, 6.25Hz) and their impact on Word Error Rate (WER) and speech quality metrics (VisQOL, MOSNet)
- Why unresolved: While the paper identifies 12.5Hz as optimal based on current experiments, it's unclear if this holds across different languages, datasets, or model scales
- What evidence would resolve it: Systematic experiments across multiple languages, datasets, and model sizes to determine if 12.5Hz remains optimal or if different frame rates work better in specific scenarios

### Open Question 2
- Question: How does the span corruption ratio in interleaved data construction affect cross-modal knowledge transfer between speech and text?
- Basis in paper: [explicit] The paper explores different corruption ratios (0.2 to 0.4) and their impact on model performance
- Why unresolved: The study only examines a limited range of ratios and doesn't investigate the underlying mechanisms of how different ratios affect alignment learning between modalities
- What evidence would resolve it: Detailed analysis of how different corruption ratios impact the model's ability to align speech and text representations, potentially through probing tasks or attention visualization

### Open Question 3
- Question: What is the relationship between synthetic interleaved data scale and downstream task performance?
- Basis in paper: [explicit] The paper shows performance improvements as interleaved data increases from 0 to 600B tokens, but the curve appears to be still increasing
- Why unresolved: The paper doesn't explore whether there's a point of diminishing returns or if even larger scales would continue to improve performance
- What evidence would resolve it: Experiments with even larger synthetic data scales (e.g., 1T+ tokens) to determine if performance plateaus or continues improving, and analysis of what aspects of performance benefit most from scale

## Limitations
- Limited evaluation of semantic preservation and quality of synthetic speech tokens
- Limited comparison with other tokenization methods or speech synthesis approaches
- Evaluation of spoken chatbot based on limited tasks and baseline comparisons

## Confidence
- **High Confidence**: The approach of using synthetic interleaved speech-text data for speech-language model pre-training is sound and addresses the data scarcity issue in speech modeling
- **Medium Confidence**: The supervised speech tokenizer with strong semantic preservation enables efficient speech modeling at lower frame rates
- **Low Confidence**: The fine-tuned speech-language model achieves competitive performance as an end-to-end spoken chatbot with strong conversational abilities and speech quality

## Next Checks
1. Evaluate semantic preservation and quality of synthetic speech tokens using a larger and more diverse set of speech data
2. Assess the robustness of the speech-language model to varying speech conditions including different speech rates, accents, and background noise
3. Explore the generalization of the spoken chatbot to diverse conversational scenarios beyond the limited evaluation tasks