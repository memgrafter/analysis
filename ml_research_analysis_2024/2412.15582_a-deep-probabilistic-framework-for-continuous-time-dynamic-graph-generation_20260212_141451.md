---
ver: rpa2
title: A Deep Probabilistic Framework for Continuous Time Dynamic Graph Generation
arxiv_id: '2412.15582'
source_url: https://arxiv.org/abs/2412.15582
tags:
- graph
- graphs
- temporal
- dg-gen
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DG-Gen is a novel generative framework for continuous-time dynamic
  graphs that directly models temporal interactions, bypassing the limitations of
  static graph augmentation methods. The approach uses an encoder-decoder architecture
  where the encoder generates temporal embeddings and the decoder learns a joint probability
  over source, destination, time, and edge features using conditional probability
  distributions.
---

# A Deep Probabilistic Framework for Continuous Time Dynamic Graph Generation

## Quick Facts
- arXiv ID: 2412.15582
- Source URL: https://arxiv.org/abs/2412.15582
- Reference count: 15
- DG-Gen achieves high-fidelity synthetic graph generation with statistical similarity to real data, outperforming baselines in both graph generation and link prediction tasks.

## Executive Summary
DG-Gen introduces a novel generative framework for continuous-time dynamic graphs that directly models temporal interactions through an encoder-decoder architecture. The method uses a TGN encoder to generate temporal node embeddings and a deep probabilistic decoder that learns joint probabilities over source, destination, time, and edge features via conditional distributions. This approach enables inductive modeling of unseen nodes, scalable learning without explicit adjacency matrices, and support for arbitrary edge features. Experiments on five datasets demonstrate DG-Gen's ability to produce synthetic graphs with high statistical fidelity and strong link prediction performance.

## Method Summary
DG-Gen employs an encoder-decoder architecture where a TGN encoder transforms temporal interactions into node embeddings, and a deep probabilistic decoder models the joint probability of interactions as conditional distributions. The decoder uses a Time+MSG module to output distribution parameters for both categorical and continuous edge features through separate output heads. The model processes one (or small batch) temporal interaction at a time, learning parameters for invariant conditional probability distributions that enable autoregressive generation. Training minimizes negative log-likelihood, and inference involves sampling from the learned distributions to generate new synthetic dynamic graphs.

## Key Results
- Outperforms existing baselines in both graph generation quality and link prediction accuracy on five real-world datasets
- Successfully generates synthetic graphs with statistical similarity to real data, achieving low Jensen-Shannon distances for feature distributions
- Demonstrates inductive capabilities by generating interactions involving unseen nodes with high fidelity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct modeling of interaction probability enables inductive generation without node leakage.
- Mechanism: By treating each temporal interaction as a 4-tuple (src, dst, t, e) and modeling its joint probability as a product of conditional distributions (p(src) p(dst|src) p(t,e|dst,src)), the model learns to generate interactions from learned embeddings rather than raw node IDs, allowing generation of unseen nodes.
- Core assumption: Node embeddings capture sufficient temporal dynamics to represent both seen and unseen nodes.
- Evidence anchors:
  - [abstract] "This allows us to autoregressively generate new synthetic dynamic graphs in a largely assumption free, scalable, and inductive manner."
  - [section] "By learning to predict edge probabilities from a learned temporal embedding, rather than a node ID, our model is fully inductive by default"
- Break condition: If node embeddings fail to capture temporal patterns, the conditional distributions will not generalize to unseen nodes, breaking inductivity.

### Mechanism 2
- Claim: Factorization into conditional probabilities enables scalable learning without requiring explicit adjacency matrix computation.
- Mechanism: The model processes one (or small batch) temporal interaction at a time, learning parameters for invariant conditional probability distributions. This avoids the memory explosion from explicitly representing large adjacency matrices required by static graph approaches.
- Core assumption: Learning parameters to conditional distributions is computationally cheaper than maintaining and inverting large adjacency matrices.
- Evidence anchors:
  - [abstract] "Our method models the probability of a single (or small mini-batched) temporal interaction at a time and consequently avoids encountering the data scalability (and memory) challenges associated with previous methods."
  - [section] "By learning parameters to invariant conditional probability distributions, we model topological evolution in a largely assumption-free manner."
- Break condition: If conditional distributions become too complex or require excessive context, computational savings diminish and scalability benefits disappear.

### Mechanism 3
- Claim: Deep probabilistic decoder with multiple output distributions captures arbitrary edge feature structures.
- Mechanism: The Time+MSG module outputs n+1 values per interaction, where each value parametrizes either a Categorical (for discrete features) or GMM (for continuous features) distribution, allowing the model to learn complex, potentially multimodal feature distributions.
- Core assumption: Appropriate choice of distribution family (Categorical vs GMM) matches the underlying feature distribution.
- Evidence anchors:
  - [section] "p(ei|t, dst, src) is a Categorical or Gaussian Mixture Model (GMM) distribution for the i-th edge feature, depending on whether ei is a categorical or numerical variable, respectively."
  - [section] "For i > 0, the output is converted to a one-dimensional score if the edge feature ei is a categorical variable described by a Categorical (Multinomial) distribution. Otherwise ei is a numerical variable, so it is converted to 3 Â· m numbers corresponding to the mean, standard deviation and weight of the m components of a Gaussian Mixture Model."
- Break condition: If feature distributions are misspecified (e.g., using Categorical for multimodal continuous features), the model will fail to capture the true data distribution.

## Foundational Learning

- Concept: Temporal Graph Networks (TGN) encoder architecture
  - Why needed here: The encoder transforms raw temporal interaction data into node embeddings that capture both current state and historical interactions, providing the foundation for the probabilistic decoder.
  - Quick check question: What are the two main modules of TGN and what does each do?

- Concept: Conditional probability factorization
  - Why needed here: Decomposing the joint probability of interactions into tractable conditional components allows learning without requiring the intractable full joint distribution.
  - Quick check question: How does p(src, dst, t, e) = p(src)p(dst|src)p(t,e|dst,src) enable autoregressive generation?

- Concept: Distribution parameter estimation via neural networks
  - Why needed here: Neural networks provide flexible, differentiable ways to estimate distribution parameters from embeddings, enabling end-to-end training.
  - Quick check question: Why use different output heads for categorical vs continuous edge features in the decoder?

## Architecture Onboarding

- Component map: TGN encoder -> Temporal embeddings -> Reshape module -> Product module -> Merge module -> Time+MSG module -> Conditional distributions -> Generated interactions
- Critical path: Raw interaction data -> TGN memory updates -> Node embeddings -> Decoder probability estimation -> Sampled interactions
- Design tradeoffs: Inductivity vs potential loss of transductive accuracy; scalability vs potential loss of some global structural information; arbitrary feature support vs increased model complexity
- Failure signatures: Poor link prediction performance suggests encoder issues; unrealistic feature distributions suggest decoder parameterization problems; edge overlap with training data suggests inductive modeling failure
- First 3 experiments:
  1. Verify TGN encoder produces reasonable embeddings by checking reconstruction loss on held-out interactions
  2. Test conditional distributions by sampling from trained model and comparing feature histograms to training data
  3. Validate inductivity by holding out nodes during training and checking generation quality on unseen nodes

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several areas remain unexplored:
- Scalability to dynamic graphs with billions of interactions or nodes
- Generation of dynamic graphs with evolving node features (not just edge features)
- Impact of different encoder choices on generative performance

## Limitations

- Limited evaluation of scalability to very large-scale graphs (billions of interactions)
- No ablation study on the impact of the Time+MSG module's noise injection parameters
- Only one encoder architecture (TGN) is evaluated, limiting understanding of encoder impact

## Confidence

The core claims about DG-Gen's inductive capabilities and scalability are **High** confidence, directly supported by the mechanism descriptions and experimental results showing generation quality on held-out nodes. The claims about handling arbitrary edge features through conditional distributions are **Medium** confidence, as the paper demonstrates effectiveness but doesn't exhaustively test edge cases with highly complex feature distributions. The scalability claims relative to static graph approaches are **High** confidence based on the computational complexity arguments provided.

## Next Checks

1. **Inductive capacity validation**: Train DG-Gen on subsets of nodes and evaluate generation quality on completely unseen nodes to verify the claimed inductivity, measuring edge overlap and feature distribution similarity.

2. **Time+MSG module sensitivity**: Systematically vary the noise parameters and observe effects on training convergence and generation quality to understand the robustness of this critical component.

3. **Baseline comparison under identical conditions**: Implement and compare against static graph augmentation approaches (e.g., VGAE + edge perturbation) using the same datasets, training procedures, and evaluation metrics to quantify the claimed advantages.