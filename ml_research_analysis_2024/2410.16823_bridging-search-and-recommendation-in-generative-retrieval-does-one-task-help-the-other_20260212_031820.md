---
ver: rpa2
title: 'Bridging Search and Recommendation in Generative Retrieval: Does One Task
  Help the Other?'
arxiv_id: '2410.16823'
source_url: https://arxiv.org/abs/2410.16823
tags:
- search
- recommendation
- generative
- retrieval
- items
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores whether joint training of generative retrieval
  models for search and recommendation tasks improves performance compared to task-specific
  models. The authors propose two hypotheses: (1) joint training regularizes item
  popularity estimation, and (2) it regularizes item latent representations by leveraging
  complementary information from both tasks.'
---

# Bridging Search and Recommendation in Generative Retrieval: Does One Task Help the Other?

## Quick Facts
- arXiv ID: 2410.16823
- Source URL: https://arxiv.org/abs/2410.16823
- Authors: Gustavo Penha; Ali Vardasbi; Enrico Palumbo; Marco de Nadai; Hugues Bouchard
- Reference count: 40
- Primary result: Joint training of generative retrieval models for search and recommendation improves performance with an average 16% increase in R@30

## Executive Summary
This paper investigates whether joint training of generative retrieval models for search and recommendation tasks improves performance compared to task-specific models. The authors propose two hypotheses: (1) joint training regularizes item popularity estimation, and (2) it regularizes item latent representations by leveraging complementary information from both tasks. Through extensive experiments on simulated and real-world datasets (ML, MPD, and Podcasts), the joint model consistently outperforms task-specific models. The analysis reveals that the regularization effect on item representations is the primary driver of improvement, as evidenced by changes in model predictions and item pair relationships. The results support using multi-task learning to enhance generative retrieval systems in domains where search and recommendation co-exist.

## Method Summary
The authors use T5-base models fine-tuned for both search and recommendation tasks, either separately or jointly. The joint model is trained on concatenated search and recommendation datasets using a multi-task learning approach. Inference is performed using diversified beam search with a diversity penalty of 0.25. The model predicts item IDs rather than using semantic embeddings. Training uses 5 epochs, learning rate 0.002, batch size 128, and AdamW optimizer with weight decay 0.01. Datasets include ML (MovieLens 25M), MPD (Million Playlist Dataset), and Podcasts (Spotify logs), with items appearing in both search and recommendation splits.

## Key Results
- Joint model outperforms task-specific models with an average 16% increase in R@30 across all datasets
- Statistical significance confirmed through paired t-tests across multiple runs
- Analysis shows regularization of item representations (H2) is the primary driver of improvement
- Changes in item pair co-occurrences between task-specific and joint model predictions support the regularization hypothesis
- Qualitative evidence from t-SNE visualizations demonstrates improved clustering of related items

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint training regularizes item popularity estimation by averaging popularity signals across tasks.
- Mechanism: When search and recommendation tasks have different popularity distributions for items, joint training smooths the popularity estimation by combining these distributions. This creates a more balanced representation that better matches the test distribution.
- Core assumption: Popularity distributions differ between search and recommendation tasks, and this difference is beneficial when regularized.
- Evidence anchors:
  - [abstract]: "joint training regularizes the estimation of each item's popularity"
  - [section]: "the joint training regularizes the estimation of each item's popularity" with motivational example showing how combining different popularity distributions improves effectiveness
  - [corpus]: Weak evidence - only general mentions of popularity in related papers, no specific discussion of regularization
- Break condition: If popularity distributions are identical across tasks, or if one task's distribution completely dominates the other.

### Mechanism 2
- Claim: Joint training regularizes item latent representations by leveraging complementary information from search (content-based) and recommendation (collaborative-filtering).
- Mechanism: Search captures content-based relationships between items through query-item relevance patterns, while recommendation captures collaborative-filtering relationships through user interaction patterns. Joint training combines these complementary views to create more robust item embeddings.
- Core assumption: Search and recommendation provide different but complementary information about item relationships that can be combined effectively.
- Evidence anchors:
  - [abstract]: "joint training regularizes the item's latent representations, where search captures content-based aspects of an item and recommendation captures collaborative-filtering aspects"
  - [section]: Detailed examples showing how search can fill data scarcity in recommendation (S → R) and vice versa (R → S), with t-SNE visualizations demonstrating improved clustering
  - [corpus]: Weak evidence - mentions of "content-based" and "collaborative-filtering" but no specific discussion of regularization effects
- Break condition: If search and recommendation data provide redundant rather than complementary information about item relationships.

### Mechanism 3
- Claim: Multi-task learning creates regularization effects that improve generalization by preventing overfitting to task-specific patterns.
- Mechanism: Training on multiple objectives forces the model to learn representations that satisfy both tasks simultaneously, preventing it from overfitting to idiosyncrasies of a single task's data distribution.
- Core assumption: Task-specific models overfit to their respective data distributions in ways that joint training can mitigate.
- Evidence anchors:
  - [abstract]: "our extensive experiments with both simulated and real-world data support both [H1] and [H2] as key contributors to the effectiveness improvements"
  - [section]: Discussion of how joint model predictions differ from task-specific models, with specific metrics showing increased item pair co-occurrences in predictions
  - [corpus]: No direct evidence - multi-task learning is mentioned but not in the context of regularization effects
- Break condition: If the regularization effect from multi-task learning is outweighed by interference between incompatible task objectives.

## Foundational Learning

- Concept: Popularity bias in generative retrieval models
  - Why needed here: Understanding how popularity affects item predictions is crucial for Mechanism 1
  - Quick check question: Why do generative retrieval models tend to predict more popular items more frequently?

- Concept: Latent representations and embedding spaces
  - Why needed here: Essential for understanding how search and recommendation information combines in Mechanism 2
  - Quick check question: How do item embeddings capture relationships between items in the latent space?

- Concept: Multi-task learning and regularization
  - Why needed here: Core to understanding why joint training improves performance in Mechanism 3
  - Quick check question: What is the difference between inductive bias and regularization in machine learning?

## Architecture Onboarding

- Component map: Generative retrieval model (T5-based) -> Task-specific datasets (search/recommendation) -> Joint training objective -> Item ID mapping function -> Diversified beam search for inference
- Critical path: Data preprocessing -> Model training (joint objectives) -> Inference with diversified beam search -> Evaluation metrics
- Design tradeoffs: Atomic IDs vs. semantic IDs for item representation; number of training instances per item; diversity penalty in beam search
- Failure signatures: Decreased performance when popularity distributions are too similar; reduced effectiveness when item co-occurrences don't align across tasks; overfitting when training data is insufficient
- First 3 experiments:
  1. Compare joint model vs. task-specific models on simulated data with varying KL divergence between popularity distributions
  2. Analyze item embedding visualizations (t-SNE) to verify clustering improvements from joint training
  3. Measure changes in item pair co-occurrences between task-specific and joint model predictions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the effect of integrating additional IR tasks, such as generating explanations, within a unified multi-task learned LLM for information retrieval?
- Basis in paper: [explicit] The authors state "For future research, we plan to explore the effect of integrating additional tasks, such as generating explanations, within a unified multi-task learned LLM for IR."
- Why unresolved: This question was not addressed in the current study, which focused solely on search and recommendation tasks.
- What evidence would resolve it: Experimental results comparing the performance of a multi-task model with and without the explanation generation task, and analyzing how the additional task affects the effectiveness of the other tasks.

### Open Question 2
- Question: How do different ID strategies (e.g., semantic IDs vs. atomic IDs) affect the performance of multi-task generative retrieval models?
- Basis in paper: [explicit] The authors mention "Future work may explore replacing these atomic IDs with semantic IDs [15, 44, 52], based on content or collaborative embeddings, to scale to a larger set of items."
- Why unresolved: The current study used atomic IDs, and the impact of different ID strategies on multi-task learning effectiveness remains unexplored.
- What evidence would resolve it: Experimental results comparing the performance of models using different ID strategies (e.g., atomic IDs, semantic IDs based on content, and semantic IDs based on collaborative embeddings) in a multi-task learning setting.

### Open Question 3
- Question: How does the effectiveness of multi-task generative retrieval models scale with the size of the item collection?
- Basis in paper: [inferred] The authors discuss the challenges of generative retrieval in terms of scalability and the ingestion of new documents, suggesting that this is an important consideration for practical applications.
- Why unresolved: The current study used relatively small datasets, and the impact of item collection size on multi-task learning effectiveness remains unclear.
- What evidence would resolve it: Experimental results demonstrating the performance of multi-task generative retrieval models on datasets with varying sizes of item collections, and analyzing how the effectiveness changes as the collection size increases.

## Limitations
- Study focuses on three specific datasets (ML, MPD, Podcasts) with particular popularity distributions that may not generalize to all domains
- Analysis relies heavily on qualitative observations and visualizations rather than quantitative metrics that directly measure the proposed regularization effects
- Assumption of complementary information between search and recommendation is not rigorously tested across different dataset characteristics

## Confidence

- **High Confidence**: The empirical finding that joint training improves R@30 by ~16% on average is well-supported by extensive experiments with proper statistical validation (paired t-tests across multiple runs)
- **Medium Confidence**: The conclusion that regularization of item representations (H2) is the primary driver of improvement, based on analysis of prediction patterns and item relationships
- **Medium Confidence**: The proposed mechanisms explaining why joint training works (popularity regularization and complementary information capture), though these are supported by qualitative evidence rather than direct quantitative validation

## Next Checks

1. **Quantify Complementarity**: Measure the actual information overlap between search and recommendation data using mutual information or similar metrics to validate the assumption of complementary signals
2. **Mechanism Isolation**: Design ablation studies that isolate the popularity regularization effect from the representation regularization effect to determine their relative contributions
3. **Generalization Testing**: Test the joint model on datasets with varying degrees of popularity distribution similarity between search and recommendation to validate the claimed break conditions for the proposed mechanisms