---
ver: rpa2
title: 'Clustering Head: A Visual Case Study of the Training Dynamics in Transformers'
arxiv_id: '2410.24050'
source_url: https://arxiv.org/abs/2410.24050
tags:
- training
- loss
- gradient
- learning
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a small-scale visual sandbox for studying
  transformer training dynamics, focusing on a sparse modular addition task. The key
  innovation is identifying "clustering heads" - circuits that learn task invariants
  through a two-stage process: first clustering sequence embeddings, then fitting
  a classifier.'
---

# Clustering Head: A Visual Case Study of the Training Dynamics in Transformers

## Quick Facts
- arXiv ID: 2410.24050
- Source URL: https://arxiv.org/abs/2410.24050
- Reference count: 40
- Small-scale visual sandbox for studying transformer training dynamics

## Executive Summary
This paper introduces a small-scale visual sandbox for studying transformer training dynamics, focusing on a sparse modular addition task. The key innovation is identifying "clustering heads" - circuits that learn task invariants through a two-stage process: first clustering sequence embeddings, then fitting a classifier. The study reveals loss spikes due to high curvature in normalization layers and MLPs, with gradient norms correlating to these spikes. Using low-dimensional (d=2) embeddings enables comprehensive visualization of internal mechanisms. The authors also observe that successful models have denser activation patterns and demonstrate transferability of learned circuits through curriculum learning. Theoretical analysis provides gradient upper bounds and proves clustering heads can be learned via gradient descent, offering insights into practical training challenges and potential optimization improvements.

## Method Summary
The paper presents a visual sandbox for studying transformer training dynamics using a sparse modular addition task with low-dimensional (d=2) embeddings. The approach enables comprehensive visualization of internal mechanisms, allowing researchers to observe how models learn through a "clustering head" circuit. This circuit operates in two stages: first clustering sequence embeddings, then fitting a classifier. The study tracks gradient norms to identify loss spikes caused by high curvature in normalization layers and MLPs. The authors also demonstrate curriculum learning to transfer learned circuits to larger models, and provide theoretical analysis with gradient upper bounds to prove clustering heads can be learned via gradient descent.

## Key Results
- Identification of "clustering heads" as a learned circuit for modular addition tasks
- Loss spikes caused by high curvature in normalization layers and MLPs, with gradient norms correlating to these spikes
- Successful models exhibit denser activation patterns and can transfer learned circuits through curriculum learning

## Why This Works (Mechanism)
The clustering head mechanism works by first identifying invariant patterns in the input data through clustering of sequence embeddings, then using these clustered representations to build a classifier. This two-stage approach allows the model to capture task-specific invariances efficiently. The low-dimensional embedding space (d=2) enables clear visualization of this process, revealing how different components of the transformer interact during training. The mechanism is particularly effective for modular addition because the task has inherent symmetries that can be exploited through clustering.

## Foundational Learning
1. **Transformer Architecture Components** (Why needed: Understanding the building blocks; Quick check: Can identify self-attention, MLP, and normalization layers)
2. **Gradient Descent Optimization** (Why needed: Core training mechanism; Quick check: Can explain how gradients update weights)
3. **Modular Arithmetic** (Why needed: Task domain; Quick check: Can perform modular addition)
4. **Circuit Analysis in Neural Networks** (Why needed: Interpretability framework; Quick check: Can trace information flow through learned circuits)

## Architecture Onboarding

**Component Map:**
Input Embeddings -> Self-Attention -> Normalization -> MLP -> Output Head

**Critical Path:**
The critical path for learning the modular addition task flows through the clustering head circuit, where embeddings are first processed by self-attention, normalized, passed through the MLP, and finally classified. The clustering occurs implicitly in the embedding space before classification.

**Design Tradeoffs:**
- Low-dimensional embeddings (d=2) for visualization vs. practical applicability
- Simplified modular addition task vs. real-world complexity
- Focus on specific circuit patterns vs. general training dynamics

**Failure Signatures:**
- Loss spikes correlating with high gradient norms in normalization layers
- Sparse activation patterns indicating poor learning
- Failure to form coherent clusters in embedding space

**First Experiments:**
1. Train a small transformer on modular addition with d=2 embeddings and visualize the clustering head formation
2. Compare gradient norms during training to identify loss spike patterns
3. Test curriculum learning by transferring circuits from small to larger models

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on highly simplified modular addition task with d=2 embeddings limits generalizability
- The universality of clustering heads across different tasks and architectures needs further validation
- Low-dimensional visualization may oversimplify higher-dimensional transformer dynamics

## Confidence
- **Identification of Clustering Heads**: High
- **Two-Stage Learning Process**: Medium
- **Loss Spikes and High Curvature**: Medium
- **Transferability of Circuits**: Low

## Next Checks
1. Replicate the study using more complex tasks and higher-dimensional embeddings to assess the robustness of clustering heads and the observed training dynamics
2. Investigate whether clustering heads and the associated two-stage learning process are present in transformers with different architectures
3. Apply the insights gained from the modular addition task to a real-world problem to evaluate the practical utility of clustering heads and the proposed optimization strategies