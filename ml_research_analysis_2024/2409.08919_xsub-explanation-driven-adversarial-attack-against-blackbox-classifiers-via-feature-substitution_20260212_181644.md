---
ver: rpa2
title: 'XSub: Explanation-Driven Adversarial Attack against Blackbox Classifiers via
  Feature Substitution'
arxiv_id: '2409.08919'
source_url: https://arxiv.org/abs/2409.08919
tags:
- attack
- sample
- data
- features
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces XSub, a novel explanation-driven adversarial
  attack targeting black-box classifiers through feature substitution. The attack
  identifies important features in a sample via model explanations and substitutes
  them with corresponding important features from a "golden sample" of a different
  label, increasing misclassification likelihood.
---

# XSub: Explanation-Driven Adversarial Attack against Blackbox Classifiers via Feature Substitution

## Quick Facts
- arXiv ID: 2409.08919
- Source URL: https://arxiv.org/abs/2409.08919
- Reference count: 40
- Key outcome: Novel explanation-driven adversarial attack achieving 74-79% success on CIFAR-10 and 80-90% on Imagenette with constant O(1) query complexity

## Executive Summary
XSub introduces a novel explanation-driven adversarial attack that strategically replaces important features in correctly classified samples with corresponding important features from golden samples of different labels. By leveraging model explanations (SHAP values) to identify the most influential features, XSub achieves high attack success rates while maintaining constant query complexity and adjustable stealthiness. The attack demonstrates superior performance compared to existing methods while requiring significantly fewer queries, making it highly practical for black-box scenarios. Additionally, XSub can be extended to backdoor attacks when training data access is available.

## Method Summary
XSub operates by first pre-computing golden samples for each class based on highest SHAP explanation values, then strategically substituting the top-K important features from a correctly classified sample with corresponding important features from a golden sample of a different class. The substitution is controlled by amplification parameters α and β to balance attack effectiveness and imperceptibility. The attack maintains constant query complexity by avoiding iterative optimization, requiring only a fixed number of queries to obtain predictions and explanations. When training data access is available, the same mechanism can be applied to backdoor attacks by incorporating adversarial features during training.

## Key Results
- Achieves 74-79% attack success rate on CIFAR-10 and 80-90% on Imagenette
- Maintains low detectability with 8-13% detection rate against Beatrix defense
- Demonstrates constant O(1) query complexity compared to iterative black-box attacks
- Outperforms existing methods while requiring significantly fewer queries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: XSub uses model explanations to identify the most important features that drive the classifier's decision on a correctly classified sample.
- Mechanism: The attacker queries the black-box model to obtain both its prediction and the corresponding SHAP-based explanation. The explanation assigns importance scores to each feature; the attacker selects the top-K features with highest importance values. These important features are then substituted with corresponding important features from a "golden sample" of a different class.
- Core assumption: The explanation model (SHAP DeepExplainer) provides reliable importance scores that reflect the black-box model's actual decision-making process, and the top-K features are the most influential for the classifier's output.

### Mechanism 2
- Claim: The "golden sample" selection process ensures that substituted features are maximally effective at changing the model's prediction.
- Mechanism: For each class, the attacker pre-computes a golden sample by finding the sample with the highest explanation value for that class across a dataset. During attack, when targeting class Y, the attacker substitutes features with those from the golden sample of class Y' (where Y' ≠ Y). This ensures the substituted features have the highest possible impact on the model's decision-making for class Y'.
- Core assumption: The golden sample for a class represents the most "class-typical" sample according to the explanation model, and features from this sample will be most effective at pushing the model toward that class.

### Mechanism 3
- Claim: XSub achieves constant query complexity (O(1)) by pre-computing golden samples offline.
- Mechanism: The attacker computes golden samples for all classes in advance using a representative dataset. During the actual attack, only a constant number of queries are needed: one to get the model prediction, one to get the explanation, and a few to identify the golden sample from the pre-computed set. This avoids iterative query-based optimization methods used by other black-box attacks.
- Core assumption: The pre-computed golden samples remain representative of the target classes even for unseen test samples, and the feature importance rankings are stable enough to work across different samples.

## Foundational Learning

- Concept: SHAP (SHapley Additive exPlanations) values and their use in explaining model predictions
  - Why needed here: XSub relies on SHAP-based explanations to identify important features for substitution
  - Quick check question: What does a high SHAP value indicate about a feature's relationship to the model's prediction?

- Concept: Adversarial attacks and their categorization (white-box vs black-box)
  - Why needed here: Understanding the threat model and why constant query complexity is advantageous
  - Quick check question: How does XSub's O(1) query complexity compare to traditional black-box attack methods that require many queries?

- Concept: Feature importance and its role in model decision-making
  - Why needed here: The core mechanism of XSub depends on understanding which features are most influential
  - Quick check question: Why might replacing the top-K important features be more effective than random feature substitution?

## Architecture Onboarding

- Component map:
  - Black-box classifier (prediction model f)
  - SHAP DeepExplainer (explanation model g)
  - Pre-computed golden samples for each class
  - Substitution engine with parameters α, β, K
  - Query interface to the target model

- Critical path:
  1. Pre-compute golden samples for all classes offline
  2. For each target sample: query f(x) and g(x)
  3. Select golden sample from different class
  4. Substitute top-K important features using Equation 3
  5. Query f(x') to verify misclassification

- Design tradeoffs:
  - K vs stealthiness: Higher K increases attack success but makes perturbations more noticeable
  - α, β values: Control amplification of substitution effects, balancing effectiveness and imperceptibility
  - Pre-computation vs adaptability: Golden samples computed offline are faster but may not adapt to model updates

- Failure signatures:
  - Low attack success rate despite high K values: Golden samples may not be representative
  - High detection rates against defenses: Substitution patterns may be too obvious
  - Inconsistent results across similar samples: Feature importance rankings may be unstable

- First 3 experiments:
  1. Verify golden sample selection by comparing explanation values across samples in the same class
  2. Test single-pixel substitution (K=1) with varying α and β values to find optimal parameters
  3. Compare attack success rate against a simple baseline that randomly substitutes features instead of using explanations

## Open Questions the Paper Calls Out

- Question: How does the effectiveness of XSub vary when applied to different types of neural network architectures beyond CNNs and XResNet50?
  - Basis in paper: The paper evaluates XSub on CIFAR-10 and Imagenette datasets using a CNN and XResNet50-based model, but does not explore other architectures.
  - Why unresolved: The study focuses on two specific architectures, leaving open the question of how XSub performs across a broader range of models, such as transformers or recurrent networks.
  - What evidence would resolve it: Conducting experiments with diverse neural network architectures and comparing their susceptibility to XSub attacks would provide clarity on its generalizability.

- Question: What is the impact of XSub on the interpretability of model explanations themselves?
  - Basis in paper: The paper discusses the trade-off between transparency and security in XAI but does not address how XSub might affect the interpretability of explanations.
  - Why unresolved: While XSub exploits model explanations for adversarial purposes, its influence on the clarity and utility of explanations for legitimate users is not explored.
  - What evidence would resolve it: Analyzing changes in explanation quality and interpretability before and after XSub attacks would reveal its impact on XAI utility.

- Question: Can XSub be adapted to work effectively in real-time applications where models are continuously updated?
  - Basis in paper: The paper mentions the potential for XSub to be used in ML-as-a-Service scenarios but does not address its performance in dynamic, real-time environments.
  - Why unresolved: The static nature of the experiments does not reflect the challenges of applying XSub in settings where models are frequently retrained or updated.
  - What evidence would resolve it: Testing XSub in a simulated real-time environment with model updates would demonstrate its adaptability and effectiveness in such scenarios.

- Question: How does the choice of explanation model affect the success rate of XSub attacks?
  - Basis in paper: The paper uses SHAP for explanations but does not explore the impact of using different explanation models on XSub's effectiveness.
  - Why unresolved: Different explanation models may provide varying levels of detail and importance ranking, potentially influencing XSub's ability to identify and substitute features effectively.
  - What evidence would resolve it: Comparing the success rates of XSub attacks using multiple explanation models would highlight the dependency on the choice of explainer.

## Limitations
- The effectiveness relies on the assumption that SHAP-based explanations accurately reflect the black-box model's decision-making process, which may not hold for complex neural networks.
- The pre-computed golden sample approach assumes feature importance stability across different samples, which may not generalize to datasets with high intra-class variation.
- The constant O(1) query complexity depends on offline computation of golden samples, which may not adapt well to model updates or distribution shifts.

## Confidence
- High confidence: The basic framework of using explanations to identify important features for adversarial attacks is well-established in the literature.
- Medium confidence: The specific feature substitution mechanism and its effectiveness on the tested datasets, given that results are reported but implementation details are unclear.
- Low confidence: The O(1) query complexity claim and its generalizability to other datasets and models, due to lack of comparative analysis with other black-box attack methods.

## Next Checks
1. Test the stability of SHAP importance rankings by computing the correlation of feature importance scores across multiple samples from the same class to verify the golden sample assumption.
2. Compare the attack success rate and query efficiency of XSub against traditional iterative black-box attacks like Boundary or HopSkipJump on the same datasets to validate the O(1) complexity claim.
3. Evaluate the attack's effectiveness against different explanation methods (not just SHAP) to determine whether the success relies specifically on SHAP or generalizes to other attribution methods.