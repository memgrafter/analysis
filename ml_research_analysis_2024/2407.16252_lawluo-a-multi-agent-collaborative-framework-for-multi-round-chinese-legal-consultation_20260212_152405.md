---
ver: rpa2
title: 'LawLuo: A Multi-Agent Collaborative Framework for Multi-Round Chinese Legal
  Consultation'
arxiv_id: '2407.16252'
source_url: https://arxiv.org/abs/2407.16252
tags:
- legal
- agent
- lawluo
- consultation
- lawyer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LawLuo is a multi-agent collaborative framework for multi-round
  Chinese legal consultation that addresses the limitations of single-agent systems.
  The framework consists of four specialized agents (receptionist, lawyer, secretary,
  and boss) that simulate real-world law firm operations to provide more personalized
  and professional responses.
---

# LawLuo: A Multi-Agent Collaborative Framework for Multi-Round Chinese Legal Consultation

## Quick Facts
- arXiv ID: 2407.16252
- Source URL: https://arxiv.org/abs/2407.16252
- Reference count: 20
- Primary result: Multi-agent framework outperforming single-agent systems in Chinese legal consultation with 72% win rate over baselines

## Executive Summary
LawLuo is a multi-agent collaborative framework designed to address the limitations of single-agent systems in Chinese legal consultation. The framework consists of four specialized agents (receptionist, lawyer, secretary, and boss) that simulate real-world law firm operations to provide more personalized and professional responses. By employing distinct fine-tuning datasets for each agent and introducing case graph-based RAG to handle ambiguous user queries, LawLuo demonstrates superior performance in generating personalized responses, handling ambiguous queries, and maintaining instruction-following ability through multiple dialogue rounds.

## Method Summary
LawLuo implements a four-agent framework with LoRA fine-tuning on Chinese base models (BaiChuan, ChatGLM-3-6b) using specialized datasets for each agent role. The receptionist agent performs intent classification and lawyer routing across 16 legal domains, while the lawyer agent conducts multi-turn dialogue enhanced with case graph-based RAG for ambiguous queries. The secretary agent organizes conversation records and generates consultation reports, and the boss agent monitors performance and optimizes inter-agent communication through PPO reinforcement learning. The system is trained on the MURLED dataset (16,734 dialogues) and LCRG dataset (420 reports), with case graph construction using LightRAG with criminal and civil case collections.

## Key Results
- Outperforms baselines in generating personalized and professional responses
- Demonstrates 72% win rate over competitive models in GPT-4 evaluation
- Excels at handling ambiguous queries through case graph-based RAG
- Maintains superior instruction-following ability across multiple dialogue rounds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-agent collaboration improves personalization by decomposing legal consultations into specialized subtasks handled by different agents
- Mechanism: The receptionist agent identifies user intent and routes to appropriate domain specialists, allowing each agent to focus on their expertise area
- Core assumption: Different legal domains require distinct expertise that can be better captured by specialized agents
- Evidence anchors:
  - Abstract describes four specialized agents including receptionist for intent assessment and lawyer selection
  - Section 3.1 defines 16 descriptions for lawyers specializing in different legal areas
  - Corpus analysis shows related work on multi-agent medical consultation systems

### Mechanism 2
- Claim: Case graph-based RAG addresses ambiguous queries by actively guiding users to clarify case details
- Mechanism: Lawyer agent uses case graph retrieval to identify relevant legal precedents and asks targeted follow-up questions
- Core assumption: Users without legal background often provide incomplete information that structured retrieval can identify
- Evidence anchors:
  - Abstract mentions case graph-based RAG to help address vague user inputs
  - Section 3.2 describes case graph-based Retrieval-Augmented Generation during response generation
  - Weak corpus evidence with related medical consultation systems

### Mechanism 3
- Claim: Multi-turn dialogue capability is maintained through instruction fine-tuning on multi-round legal conversation datasets
- Mechanism: MURLED dataset trains lawyer agent to progressively gather information across multiple turns
- Core assumption: Real legal consultations require iterative information gathering that models need specific training to handle
- Evidence anchors:
  - Abstract highlights performance in following legal instructions in multi-turn conversations
  - Section 3.2 describes MURLED dataset construction for enhancing legal dialogue capabilities
  - Related work on multi-turn clinical dialogue but not specifically legal

## Foundational Learning

- Concept: Legal domain specialization and knowledge representation
  - Why needed here: LawLuo requires understanding different legal domains to route users appropriately and provide accurate advice
  - Quick check question: Can you explain the difference between civil and criminal law procedures in the Chinese legal system?

- Concept: Retrieval-Augmented Generation (RAG) and knowledge graphs
  - Why needed here: The case graph-based RAG system needs to retrieve relevant legal precedents and cases to help clarify user queries
  - Quick check question: How would you design a graph structure to represent relationships between legal cases and their key elements?

- Concept: Multi-agent coordination and communication protocols
  - Why needed here: The four agents must work together seamlessly, requiring clear interfaces and communication patterns
  - Quick check question: What communication patterns would you use to ensure the receptionist, lawyer, secretary, and boss agents coordinate effectively?

## Architecture Onboarding

- Component map: Receptionist Agent (intent classification → lawyer routing) → Lawyer Agent (multi-turn dialogue with case graph RAG) → Secretary Agent (conversation organization → report generation) → Boss Agent (performance monitoring → optimization)

- Critical path: User query → Receptionist assessment → Lawyer selection → Multi-turn dialogue with RAG → Secretary report generation → Boss evaluation

- Design tradeoffs:
  - Specialization vs. generalization: More specialized agents provide better expertise but require more complex coordination
  - RAG complexity vs. response time: Case graph retrieval provides better guidance but adds latency
  - Multi-turn capability vs. resource usage: Iterative dialogue requires more computational resources

- Failure signatures:
  - Poor routing decisions by receptionist → Wrong lawyer specialization
  - Ineffective RAG queries → Generic or irrelevant responses
  - Boss agent mis-evaluation → Suboptimal performance optimization
  - Secretary report generation errors → Incomplete or inaccurate summaries

- First 3 experiments:
  1. Test receptionist agent accuracy by providing sample queries and measuring correct lawyer routing
  2. Evaluate lawyer agent RAG effectiveness by measuring query clarification quality and case retrieval relevance
  3. Assess multi-turn dialogue capability by simulating conversations and measuring instruction-following accuracy across rounds

## Open Questions the Paper Calls Out

- Open Question 1: How does LawLuo handle cases where multiple agents provide conflicting legal advice, and what mechanisms are in place to resolve such conflicts?
  - Basis in paper: Inferred from boss agent's performance monitoring role
  - Why unresolved: Paper describes boss agent monitoring but not conflict resolution protocols
  - What evidence would resolve it: Documentation of conflict resolution protocols or examples of conflicting scenarios and their resolutions

- Open Question 2: What are the computational and latency implications of using a multi-agent framework compared to single-agent systems?
  - Basis in paper: Inferred from multi-agent complexity
  - Why unresolved: Paper demonstrates effectiveness but doesn't discuss computational overhead or resource requirements
  - What evidence would resolve it: Comparative performance benchmarks showing response times, GPU memory usage, and cost analysis

- Open Question 3: How does the system ensure consistency in legal advice when the same case is handled by different lawyer agents with varying specializations?
  - Basis in paper: Inferred from receptionist routing mechanism
  - Why unresolved: Paper doesn't explain consistency maintenance when multiple specialized lawyers handle overlapping case aspects
  - What evidence would resolve it: Case studies showing how system handles complex cases requiring multiple specializations

## Limitations
- Specific routing accuracy rates of the receptionist agent across different legal domains are not provided
- No quantitative comparison of response quality with and without case graph-based RAG component
- Limited information on how the boss agent's evaluation criteria were validated against real-world legal consultation quality standards

## Confidence
- Multi-agent collaboration mechanism: Medium confidence (well-defined agent roles but lacks individual agent performance metrics)
- Case graph-based RAG approach: Low confidence (abstract mentions effectiveness but lacks quantitative measures)
- Multi-turn dialogue capability: Medium confidence (MURLED dataset support but uncertain complexity capture)

## Next Checks
1. Implement controlled experiments comparing user query handling with and without case graph-based RAG to measure actual improvement in query clarification and response relevance
2. Conduct ablation studies removing the receptionist agent to quantify the impact of specialized routing versus direct lawyer access
3. Test the framework's performance on previously unseen legal domains not represented in the training data to assess generalization capabilities