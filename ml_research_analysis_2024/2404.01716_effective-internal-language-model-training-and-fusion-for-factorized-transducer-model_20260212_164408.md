---
ver: rpa2
title: Effective internal language model training and fusion for factorized transducer
  model
arxiv_id: '2404.01716'
source_url: https://arxiv.org/abs/2404.01716
tags:
- training
- rnn-t
- proposed
- transducer
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of effectively training and
  fusing internal language models (ILMs) in factorized transducer models for improved
  speech recognition performance. The proposed method introduces a novel ILM training
  strategy and a new decoding approach that combines blank, acoustic, and ILM scores.
---

# Effective internal language model training and fusion for factorized transducer model

## Quick Facts
- arXiv ID: 2404.01716
- Source URL: https://arxiv.org/abs/2404.01716
- Authors: Jinxi Guo; Niko Moritz; Yingyi Ma; Frank Seide; Chunyang Wu; Jay Mahadeokar; Ozlem Kalinli; Christian Fuegen; Mike Seltzer
- Reference count: 0
- Primary result: Achieves 17% relative improvement over standard decoding and outperforms strong RNN-T baselines with external LM fusion, particularly on rare word recognition.

## Executive Summary
This paper addresses the challenge of effectively training and fusing internal language models (ILMs) in factorized transducer models for improved speech recognition performance. The proposed method introduces a novel ILM training strategy and a new decoding approach that combines blank, acoustic, and ILM scores. The key innovation is the use of two ILM weights (α and β) to control the contribution of ILM scores inside and outside the non-blank token prediction. This approach significantly improves recognition accuracy, achieving a 17% relative improvement over standard decoding methods and outperforming strong RNN-T baselines with external language model fusion. Additionally, the paper presents an ILM-fusion-aware minimum word error rate (MWER) training method to further optimize the model.

## Method Summary
The paper proposes a comprehensive approach to improve internal language model training and fusion for factorized transducer models. The method consists of three main components: 1) Pre-training the non-blank predictor on large-scale text-only data using ILM loss, 2) Implementing a novel ILM fusion strategy with two weights (α and β) to control the contribution of ILM scores inside and outside the non-blank prediction, and 3) Introducing an ILM-fusion-aware MWER training method that incorporates the decoding strategy into the loss function. The model uses a factorized transducer architecture with separate predictors for blank and non-blank tokens, and employs an Emformer-based encoder for acoustic feature extraction.

## Key Results
- Achieves 17% relative improvement over standard decoding methods
- Outperforms strong RNN-T baselines with external LM fusion by 5.5% relative on general-sets
- Reduces WER by 8.9% for rare words compared to external LM fusion

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-weight ILM fusion strategy (α and β) improves recognition by selectively adjusting the contribution of internal language model scores inside and outside the non-blank prediction.
- Mechanism: Weight α scales the ILM contribution inside the softmax (log Pam + α*log Pilm), effectively allowing ILM subtraction or suppression, while weight β adds the ILM score externally after normalization, providing complementary LM information. This dual control allows better balance between acoustic and language model influences.
- Core assumption: The factorized transducer's explicit ILM probability can be meaningfully adjusted through scaling without destabilizing the blank prediction or acoustic modeling.
- Evidence anchors:
  - [abstract]: "The proposed method introduces a novel ILM training strategy and a new decoding approach that combines blank, acoustic and ILM scores. The key innovation is the use of two ILM weights (α and β) to control the contribution of ILM scores inside and outside the non-blank token prediction."
  - [section]: "A weight α is applied on top of the non-blank predictor and the LogSoftmax function output. α ranges from 0-1 at the decoding stage, and can be used to down-scale and subtract the ILM, when combining with other scores."
- Break condition: If α is set too low, the model loses valuable ILM information; if too high, it may overpower acoustic evidence and degrade recognition accuracy.

### Mechanism 2
- Claim: ILM-fusion-aware MWER training optimizes the integration of ILM scores by incorporating the decoding strategy into the loss function.
- Mechanism: During MWER training, hypotheses are scored using the same ILM fusion weights (α and β) as in decoding, and the loss is computed based on these fused scores. This aligns the training objective with the inference procedure, improving the model's ability to predict sequences that minimize word error rate under the actual decoding setup.
- Core assumption: The alignment-restricted MWER training can approximate full-sum alignment probabilities while being memory efficient, and that the chosen α and β values during training reflect optimal decoding settings.
- Evidence anchors:
  - [abstract]: "Furthermore, when compared to a strong RNN-T baseline enhanced with external LM fusion, the proposed model yields a 5.5% relative improvement on general-sets and an 8.9% WER reduction for rare words."
  - [section]: "We calculate the minimum word error rate (MWER) loss with the full-sum alignment probabilities from RNN-T loss, and the proposed ILM probabilities is also added as shown in Eq. 6."
- Break condition: If the alignment restriction is too aggressive, it may exclude important alignment paths, leading to suboptimal gradient estimates and degraded performance.

### Mechanism 3
- Claim: Pre-training the non-blank predictor with large-scale text-only data improves ILM quality and, when combined with the proposed fusion strategy, yields substantial WER reductions.
- Mechanism: By first training the non-blank predictor using only ILM loss on a large text corpus, the model learns a strong standalone language model. This ILM is then fixed and used in the factorized transducer, providing high-quality language predictions that are fine-tuned with acoustic data via RNN-T loss. The pre-training helps the model generalize better, especially on rare words.
- Core assumption: The text-only data distribution is sufficiently representative and larger than the ASR training transcripts, enabling the ILM to capture broader language patterns.
- Evidence anchors:
  - [abstract]: "The proposed model can achieve superior performance without relying on external language models, rendering it highly efficient for production use-cases."
  - [section]: "Therefore, in this paper, we investigate the approach of first pre-training the non-blank predictor with large-scale text-only data using ILM loss, and then fix the non-blank predictor for RNN-T loss training."
- Break condition: If the text corpus is too small or not representative of the target domain, the pre-trained ILM may not generalize well, limiting the benefits of the fusion strategy.

## Foundational Learning

- Concept: Factorized Transducer (FT) model architecture
  - Why needed here: Understanding how the blank and non-blank predictions are separated and how the ILM is explicitly modeled is crucial for grasping the novelty of the ILM fusion strategy.
  - Quick check question: In a factorized transducer, what are the two separate predictors used for, and how are their outputs combined?

- Concept: Internal Language Model (ILM) and its role in RNN-T/HAT models
  - Why needed here: The ILM is central to the proposed improvements; knowing how it's estimated and used in standard models helps explain why the new training and fusion methods are beneficial.
  - Quick check question: How is the ILM score typically estimated in standard RNN-T models, and why is it subtracted during shallow fusion?

- Concept: Minimum Word Error Rate (MWER) training and alignment-restricted computation
  - Why needed here: The proposed MWER training method is a key component for further optimizing ILM integration; understanding its mechanics and memory-saving tricks is essential.
  - Quick check question: What is the purpose of using alignment-restricted paths in MWER training, and how does it reduce memory usage?

## Architecture Onboarding

- Component map: Acoustic features -> Emformer Encoder -> henc; Previous label -> Non-blank Predictor -> hpre nb; henc + hpre b -> Blank Joiner -> Pb; henc -> Projection -> log Pam; log Pam + α*log Pilm -> Softmax -> Pnb; Pb (blank) or (1-Pb)*Pnb (non-blank) + β*log Pilm

- Critical path:
  1. Acoustic features → Encoder → henc
  2. Previous label → Predictors → hpre b, hpre nb
  3. henc + hpre b → Blank joiner → Pb
  4. henc → Projection → log Pam; hpre nb → LogSoftmax → log Pilm
  5. log Pam + α*log Pilm → Softmax → Pnb
  6. Final posterior: Pb (blank) or (1-Pb)*Pnb (non-blank) + β*log Pilm

- Design tradeoffs:
  - Larger non-blank predictor improves ILM quality but risks overfitting on small datasets
  - Pre-training ILM on text-only data helps generalization but requires additional data and training steps
  - Alignment-restricted MWER saves memory but may miss some alignment paths
  - Dual ILM weights offer flexibility but require careful tuning

- Failure signatures:
  - Over-subtraction (α too low): Poor recognition, especially on rare words
  - Over-reliance on ILM (β too high): Acoustic errors not corrected, possibly worse WER
  - Under-trained ILM: Minimal improvement over standard decoding
  - Aggressive alignment restriction: Degraded MWER performance, possible memory errors

- First 3 experiments:
  1. Train a baseline RNN-T and FT model with standard decoding; compare WERs to establish baseline.
  2. Pre-train the non-blank predictor on text-only data, fix it, and retrain the FT model; evaluate WER improvement.
  3. Apply the proposed two-weight ILM fusion (α and β) during decoding; sweep α and β to find optimal values and measure WER reduction.

## Open Questions the Paper Calls Out

The paper does not explicitly call out any open questions.

## Limitations
- The paper only evaluates the proposed method on the LibriSpeech dataset, limiting the generalizability to other languages and domains.
- The optimal values for the ILM fusion weights (α and β) are not thoroughly analyzed, and their sensitivity to different datasets and acoustic conditions is unclear.
- The memory vs. accuracy tradeoff of the alignment-restricted MWER training is not quantified, making it difficult to assess its practical impact on production deployment.

## Confidence

**High Confidence Claims:**
- The two-weight ILM fusion strategy (α and β) improves WER over standard decoding on LibriSpeech
- Pre-training the non-blank predictor on text-only data provides measurable benefits
- The proposed method outperforms strong RNN-T baselines with external LM fusion

**Medium Confidence Claims:**
- The improvements generalize to rare word recognition (RW-WER) across test sets
- The ILM-fusion-aware MWER training provides additional gains over standard MWER
- The method can achieve production efficiency without external LMs

**Low Confidence Claims:**
- The specific α and β values are universally optimal (given lack of sensitivity analysis)
- The alignment restriction doesn't significantly impact MWER training quality
- The improvements scale proportionally to larger datasets or more challenging acoustic conditions

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically sweep α and β values across a wider range (e.g., 0.0-1.0 in 0.1 increments) on both test-clean and test-other sets to identify robustness boundaries and optimal operating points. This will clarify whether the reported gains are robust to hyperparameter choices or fragile to specific values.

2. **Cross-Domain Generalization Test**: Apply the proposed method to a conversational speech dataset like Switchboard or a non-English dataset to verify whether the improvements transfer beyond read speech. This addresses the critical question of real-world applicability.

3. **MWER Training Accuracy vs. Memory Tradeoff**: Implement both full-sum alignment MWER training and the proposed alignment-restricted version, then measure the WER difference and memory usage on the same hardware. This will quantify the practical cost of the memory efficiency improvement and help determine when the tradeoff is worthwhile.