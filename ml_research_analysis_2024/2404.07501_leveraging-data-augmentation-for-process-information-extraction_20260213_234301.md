---
ver: rpa2
title: Leveraging Data Augmentation for Process Information Extraction
arxiv_id: '2404.07501'
source_url: https://arxiv.org/abs/2404.07501
tags:
- data
- process
- augmentation
- techniques
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the application of data augmentation techniques
  to the task of extracting process-relevant information from natural language business
  process descriptions. The authors adapt 19 NLP data augmentation methods from the
  NL-Augmenter framework for use with the PET dataset, which contains annotated business
  process descriptions.
---

# Leveraging Data Augmentation for Process Information Extraction

## Quick Facts
- arXiv ID: 2404.07501
- Source URL: https://arxiv.org/abs/2404.07501
- Reference count: 28
- Primary result: Data augmentation improves F1 scores by up to 4.5 percentage points for relation extraction and 2.9 percentage points for mention detection in business process information extraction

## Executive Summary
This study investigates data augmentation techniques for extracting process-relevant information from natural language business process descriptions. The authors adapt 19 NLP augmentation methods from the NL-Augmenter framework to the PET dataset, which contains annotated business process descriptions. Through systematic evaluation of both mention detection and relation extraction tasks, the research demonstrates that data augmentation can significantly improve extraction performance, with relation extraction benefiting more than mention detection. The findings show that simple augmentation techniques like paraphrasing using WordNet can achieve results comparable to more resource-intensive large language model approaches.

## Method Summary
The study adapts 19 NLP data augmentation methods from the NL-Augmenter framework for use with the PET dataset containing 45 annotated business process descriptions. The researchers employ a systematic evaluation approach using 5-fold cross-validation and hyper-parameter optimization via Optuna with Tree-Structured Parzen Estimator. They evaluate both mention detection (MD) and relation extraction (RE) tasks, comparing simple techniques like synonym substitution against more complex approaches like back-translation. The method includes parameter optimization for each augmentation technique to identify the most effective configurations for the specific business process information extraction task.

## Key Results
- Data augmentation improved F1 scores by up to 4.5 percentage points for relation extraction and 2.9 percentage points for mention detection
- Simple augmentation techniques like WordNet-based paraphrasing achieved comparable performance to resource-intensive large language model methods
- Relation extraction benefited more from data augmentation than mention detection, with more pronounced improvements across techniques
- The PET dataset's limited size makes it particularly suitable for data augmentation approaches to address data scarcity issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data augmentation preserves semantics while increasing linguistic variability
- Mechanism: By translating text spans individually and back-translating them, surface form changes while annotations remain intact
- Core assumption: Translating small text units separately prevents loss of entity annotations during transformation
- Evidence anchors:
  - [abstract] "We find that many of these methods are applicable to the task of business process information extraction, improving the accuracy of extraction."
  - [section] "By back-translating these seven spans separately, we obtain variation in their wording (surface form), but are still able to preserve annotations."
  - [corpus] Weak - related papers discuss augmentation but not specific span-level translation preservation
- Break condition: If translations introduce new named entities or alter entity boundaries, annotations become invalid

### Mechanism 2
- Claim: Simple augmentation techniques provide comparable benefits to resource-intensive LLM methods
- Mechanism: Basic transformations like synonym substitution and random deletion introduce controlled noise that improves model robustness
- Core assumption: The complexity of augmentation does not directly correlate with improvement in extraction performance
- Evidence anchors:
  - [abstract] "Simple data augmentation techniques improved the F1 score of mention extraction by 2.9 percentage points, and the F1 of relation extraction by 4.5"
  - [section] "Transformations based on large language models, especially back translation techniques...are very time-intensive. Yet, improvements in relation extraction performance is not significant, when comparing them to more lightweight approaches"
  - [corpus] Weak - corpus papers mention augmentation but don't compare simple vs. complex techniques directly
- Break condition: If dataset becomes too large or complex, simple methods may no longer capture necessary semantic variations

### Mechanism 3
- Claim: Data augmentation addresses the small dataset problem in business process information extraction
- Mechanism: Synthetic samples increase training data diversity without requiring expensive expert annotation
- Core assumption: Existing annotated data contains sufficient patterns to generate meaningful synthetic variations
- Evidence anchors:
  - [abstract] "To overcome this data scarcity issue, in this paper we investigate the application of data augmentation for natural language text data."
  - [section] "Datasets for extraction of named entities and their relations have similar extents, e.g., the DocRed dataset, which contains more than 1,500,000 relation examples. This fact makes PET a prime candidate for data augmentation techniques"
  - [corpus] Moderate - corpus shows augmentation used in legal and biomedical domains facing similar data scarcity
- Break condition: If synthetic data introduces artifacts that don't represent real-world variations, model performance may degrade

## Foundational Learning

- Concept: Named Entity Recognition (NER) and Relation Extraction (RE)
  - Why needed here: The paper evaluates augmentation effects on both MD (mention detection = NER) and RE tasks
  - Quick check question: What is the difference between sequence tagging for entities versus classifying relationships between them?

- Concept: Data augmentation in NLP vs. Computer Vision
  - Why needed here: Understanding why NLP augmentation is more complex than image augmentation helps explain technique selection
  - Quick check question: Why can't we simply add random words to sentences like we add noise to images?

- Concept: Hyperparameter optimization for augmentation
  - Why needed here: The paper uses Optuna to find optimal augmentation parameters, showing this is a critical component
  - Quick check question: What metric is being optimized when selecting augmentation parameters, and why?

## Architecture Onboarding

- Component map: Data augmentation framework → Parameter optimization (Optuna) → 5-fold cross-validation → Extraction model training → Performance evaluation
- Critical path: Augmentation parameter selection → Training data augmentation → Model training → Evaluation on original test set
- Design tradeoffs: Simple techniques (low resource cost) vs. complex techniques (higher potential but resource-intensive) - paper shows simple methods often suffice
- Failure signatures: Augmentation that breaks annotations, techniques that don't improve F1 scores, excessive computation time without performance gain
- First 3 experiments:
  1. Run all 19 augmentation techniques on PET dataset with default parameters to establish baseline improvements
  2. Use Optuna to optimize parameters for top 5 performing techniques from experiment 1
  3. Compare simple techniques (synonym substitution) vs. complex techniques (back-translation) on same parameter optimization framework

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Which specific types of relations would benefit most from targeted data augmentation to address data imbalance?
- Basis in paper: [explicit] The authors mention wanting to explore targeted data augmentation for certain types of mentions or relations in future work, noting the problem of data imbalance.
- Why unresolved: The paper only mentions the general concept of addressing data imbalance through targeted augmentation but does not identify which specific relation types would benefit most.
- What evidence would resolve it: Empirical results showing which relation types show the greatest performance improvements when augmented versus unaugmented, along with statistical significance testing.

### Open Question 2
- Question: How can the effectiveness of data augmentation techniques be measured beyond F1 score improvements?
- Basis in paper: [inferred] While the paper focuses on F1 score improvements, it does not discuss alternative metrics for evaluating the effectiveness of data augmentation techniques.
- Why unresolved: The paper primarily uses F1 score as the evaluation metric, but other metrics might provide additional insights into the effectiveness of data augmentation.
- What evidence would resolve it: Comparative analysis using multiple evaluation metrics (e.g., precision, recall, robustness to noise) to assess the impact of data augmentation techniques on model performance.

### Open Question 3
- Question: How can data augmentation techniques be adapted for different domains within business process management?
- Basis in paper: [inferred] The paper focuses on a specific dataset (PET) and domain (insurance), but does not explore how data augmentation techniques might need to be adapted for other BPM domains.
- Why unresolved: The effectiveness of data augmentation techniques may vary depending on the specific domain and characteristics of the textual data within that domain.
- What evidence would resolve it: Experimental results demonstrating the performance of data augmentation techniques across multiple BPM domains with varying textual data characteristics.

## Limitations
- Findings based on a single dataset (PET) with limited size (45 process descriptions), potentially limiting generalizability to other business process domains
- Hyper-parameter optimization may have converged to local optima rather than globally optimal augmentation parameters
- Comparison between simple and complex augmentation techniques did not account for computational resource constraints in real-world deployment scenarios

## Confidence
- **High Confidence**: The core finding that data augmentation improves extraction performance (F1 scores increase of 2.9-4.5 percentage points) is well-supported by 5-fold cross-validation results across multiple augmentation techniques
- **Medium Confidence**: The claim that simple techniques perform comparably to complex LLM-based methods is supported but requires further validation on larger, more diverse datasets
- **Medium Confidence**: The assertion that augmentation addresses data scarcity is logically sound but needs validation on datasets with different scarcity levels

## Next Checks
1. Replicate the study using a larger, more diverse business process dataset to test generalizability of findings across different process domains
2. Conduct ablation studies to isolate the impact of individual augmentation parameters and identify which transformations contribute most to performance gains
3. Evaluate the cost-benefit tradeoff by measuring computational resources required for each augmentation technique versus the marginal performance improvements achieved