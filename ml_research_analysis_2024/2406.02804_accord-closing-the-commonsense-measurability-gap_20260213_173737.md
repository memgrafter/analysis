---
ver: rpa2
title: 'ACCORD: Closing the Commonsense Measurability Gap'
arxiv_id: '2406.02804'
source_url: https://arxiv.org/abs/2406.02804
tags:
- reasoning
- answer
- suppose
- pairing
- csqa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ACCORD introduces a framework to evaluate and disentangle the reasoning
  and grounding abilities of LLMs in commonsense tasks. It uses anti-factual counterfactuals
  and formalizes reasoning with explicit skills and templates, enabling automated
  generation of benchmarks of arbitrary complexity.
---

# ACCORD: Closing the Commonsense Measurability Gap

## Quick Facts
- arXiv ID: 2406.02804
- Source URL: https://arxiv.org/abs/2406.02804
- Reference count: 40
- Key outcome: ACCORD introduces a framework to evaluate and disentangle the reasoning and grounding abilities of LLMs in commonsense tasks using anti-factual counterfactuals.

## Executive Summary
ACCORD addresses the measurability gap in commonsense reasoning by introducing formal elements to quantify reasoning complexity. The framework uses anti-factual counterfactuals to prevent LLMs from relying on parametric parroting and can automatically generate benchmarks of arbitrary complexity. Experiments on ACCORDCSQA show that while LLMs perform well on factual reasoning, their anti-factual performance drops below random chance as reasoning hops increase, highlighting the reliance on memorized knowledge rather than genuine reasoning.

## Method Summary
ACCORD creates reasoning benchmarks by composing commonsense reasoning skills through a reduction matrix, generating reasoning trees of arbitrary complexity. The framework grounds variables with anti-factual concepts from ConceptNet to prevent factual shortcuts, then creates factual and anti-factual question variants. It measures reasoning complexity through the number of reasoning hops and distractors, allowing precise quantification of LLM performance degradation patterns.

## Key Results
- LLMs perform significantly worse on anti-factual tasks compared to factual ones
- Performance degradation accelerates as reasoning hops increase, dropping below random chance
- LLMs handle distractors better than reasoning hops, but overall reasoning ability degrades rapidly with complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ACCORD disentangles commonsense grounding and reasoning by using anti-factual contexts that prevent LLMs from relying on parametric parroting.
- Mechanism: Creates contexts where correct answers require reasoning through provided statements rather than recalling pre-trained knowledge.
- Core assumption: LLMs have an inductive bias towards factual answers due to training data distribution.
- Evidence anchors: Abstract showing anti-factual performance drops below random chance; section discussing lexical matching risks.
- Break condition: If ConceptNet has high recall for anti-factual triples, factual shortcuts may be possible.

### Mechanism 2
- Claim: ACCORD can automatically generate benchmarks of arbitrary complexity by composing reasoning skills through a reduction matrix.
- Mechanism: Uses predefined reasoning skills and a reduction matrix to systematically generate reasoning trees while ensuring logical soundness.
- Core assumption: Reduction matrix accurately captures all valid commonsense reasoning skill combinations.
- Evidence anchors: Abstract mentioning control of reasoning complexity; section discussing composition of reasoning skills.
- Break condition: If reduction matrix is incomplete or contains errors, generated trees may have invalid reasoning paths.

### Mechanism 3
- Claim: ACCORD quantifies reasoning complexity through measurable reasoning hops and distractors.
- Mechanism: Measures complexity as reasoning hops (n) along a path plus distractors (d) outside the path, where T = n + d.
- Core assumption: Reasoning hops and distractors are primary factors affecting reasoning complexity.
- Evidence anchors: Abstract discussing quantification beyond 1-2 hops; section explaining difficulty scaling with tree size.
- Break condition: If other factors significantly affect reasoning performance beyond hops and distractors, quantification would be incomplete.

## Foundational Learning

- Concept: Formal vs. commonsense reasoning distinction
  - Why needed here: ACCORD bridges these two reasoning types by introducing formal elements to quantify commonsense reasoning complexity
  - Quick check question: What is the key difference between formal reasoning (which requires explicit rules) and commonsense reasoning (which relies on implicit knowledge)?

- Concept: Counterfactual reasoning types (hypothetical vs. anti-factual)
  - Why needed here: ACCORD specifically uses anti-factual counterfactuals to create stronger inductive bias mitigators
  - Quick check question: Why are anti-factual counterfactuals more effective than hypothetical ones at preventing LLMs from using parametric shortcuts?

- Concept: Tree data structures and reduction operations
  - Why needed here: ACCORD generates reasoning trees by composing reasoning skills through reduction operations
  - Quick check question: How does the reduction operation ensure that composed reasoning skills form valid commonsense reasoning rather than arbitrary combinations?

## Architecture Onboarding

- Component map: CSQA dataset → pairing templates (manual) → Reasoning skills + reduction matrix → reasoning trees (automated) → Path computation → pairing templates → reasoning paths (automated) → ConceptNet → anti-factual variable grounding (automated) → Negation logic → factual/anti-factual variants (automated)

- Critical path: Tree generation → Path computation → Grounding → Answer selection
  - Each stage depends on the previous one being correct
  - Errors in earlier stages propagate downstream

- Design tradeoffs:
  - Manual pairing templates vs. full automation: Provides quality control but requires human effort
  - Anti-factual grounding vs. factual grounding: Stronger bias mitigation but relies on ConceptNet recall
  - Tree duplication vs. unique answer templates: Prevents lexical matching bias but increases dataset size

- Failure signatures:
  - Low performance on factual tasks: Likely issues with tree generation or path computation
  - High performance on anti-factual tasks: Grounding may not be sufficiently anti-factual
  - Inconsistent performance across reasoning hops: Reduction matrix may have errors

- First 3 experiments:
  1. Generate ACCORD0 (no context) and verify it matches base CSQA performance
  2. Generate ACCORD1 with single reasoning hop and verify performance degradation compared to ACCORD0
  3. Generate ACCORD2 with two reasoning hops and verify the gap between factual and anti-factual performance increases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the naturalness of ACCORD without sacrificing its anti-factual grounding strength?
- Basis in paper: The paper discusses the trade-off between naturalness and the ability to force LLMs to reason rather than rely on parametric knowledge.
- Why unresolved: The paper acknowledges the challenge of smoothing out stilted templates without losing the anti-factual grounding benefit.
- What evidence would resolve it: Demonstrating a method to enhance template naturalness while maintaining the anti-factual grounding strength through controlled experiments comparing LLM performance.

### Open Question 2
- Question: What is the impact of reasoning skill variability on LLM performance in ACCORD?
- Basis in paper: The paper mentions that ACCORD uses a broad set of reasoning skills but does not explore the impact of variability within these skills.
- Why unresolved: The paper focuses on the structured approach of ACCORD but does not investigate how variability within reasoning skills affects LLM reasoning performance.
- What evidence would resolve it: Conducting experiments with varying levels of skill variability within ACCORD to measure LLM performance differences.

### Open Question 3
- Question: How does ACCORD compare to other benchmarks in terms of measuring LLM reasoning complexity?
- Basis in paper: The paper claims ACCORD is unique in its ability to measure reasoning complexity beyond typical one or two hops.
- Why unresolved: The paper does not provide a direct comparison with other benchmarks in terms of reasoning complexity measurement.
- What evidence would resolve it: Conducting a comparative study of ACCORD with other benchmarks to evaluate its effectiveness in measuring reasoning complexity.

## Limitations
- Anti-factual grounding reliability depends heavily on ConceptNet's ability to provide accurate anti-factual triples
- Reduction matrix completeness is critical for valid reasoning tree generation
- Evaluation scope limited to zero-shot multiple-choice questions, may not generalize to open-ended tasks

## Confidence

**High confidence**: Experimental methodology for generating ACCORDCSQA and the core observation that LLMs perform significantly worse on anti-factual tasks as reasoning hops increase. Framework's design principles for disentangling grounding and reasoning are well-founded.

**Medium confidence**: Claim that ACCORD closes the measurability gap between commonsense and formal reasoning. While framework introduces formal quantification, practical impact on improving LLM reasoning capabilities remains to be demonstrated.

**Low confidence**: Generalizability of reduction matrix approach to other domains beyond specific commonsense skills tested. Framework may require significant adaptation for domains with different reasoning patterns.

## Next Checks

1. **Anti-factual grounding recall analysis**: Systematically measure ConceptNet's recall rate for anti-factual triples across different semantic domains to quantify the risk of factual shortcut leakage in the grounding mechanism.

2. **Reduction matrix coverage validation**: Conduct a human evaluation study where experts rate the validity of randomly sampled reasoning trees generated by the framework to identify gaps or errors in the reduction matrix.

3. **Cross-domain benchmarking**: Apply the ACCORD framework to a different reasoning domain (e.g., scientific reasoning or legal reasoning) to test whether the formal quantification approach generalizes beyond commonsense reasoning.