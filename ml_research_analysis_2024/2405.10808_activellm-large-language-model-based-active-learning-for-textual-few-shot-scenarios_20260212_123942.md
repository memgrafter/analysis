---
ver: rpa2
title: 'ActiveLLM: Large Language Model-based Active Learning for Textual Few-Shot
  Scenarios'
arxiv_id: '2405.10808'
source_url: https://arxiv.org/abs/2405.10808
tags:
- learning
- instances
- activellm
- prompt
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ActiveLLM is a novel active learning method that uses Large Language
  Models like GPT-4 to select training instances for BERT-like classifiers, eliminating
  the need for iterative model training during labeling. By leveraging LLMs' zero-shot
  capabilities, ActiveLLM overcomes the cold-start problem that plagues traditional
  active learning strategies, making it highly effective in few-shot scenarios.
---

# ActiveLLM: Large Language Model-based Active Learning for Textual Few-Shot Scenarios

## Quick Facts
- arXiv ID: 2405.10808
- Source URL: https://arxiv.org/abs/2405.10808
- Authors: Markus Bayer; Justin Lutz; Christian Reuter
- Reference count: 40
- One-line primary result: ActiveLLM achieves up to 24.29% F1 score improvement over traditional active learning methods in few-shot scenarios

## Executive Summary
ActiveLLM is a novel active learning method that leverages Large Language Models (LLMs) like GPT-4 to select training instances for BERT-like classifiers, eliminating the need for iterative model training during labeling. By using LLMs' zero-shot capabilities, ActiveLLM overcomes the cold-start problem that plagues traditional active learning strategies, making it highly effective in few-shot scenarios where no initial labeled data exists. The method achieves significant performance improvements across multiple datasets and tasks while being scalable and efficient, with querying times of just seconds compared to minutes or hours for traditional methods.

## Method Summary
ActiveLLM uses an LLM as a query model to select instances from unlabeled data for training a BERT-like classifier. The method generates a prompt containing task description, unlabeled instances, and optional guidelines, then sends it to the LLM which returns selected instance indices. These instances are labeled by an oracle (simulated in experiments) and used to train the BERT classifier. The approach eliminates the dependency between query and successor models, allows for greater flexibility in model selection, and overcomes cold-start problems by leveraging LLM zero-shot capabilities. Chain of Thought prompting is used to improve selection quality through structured reasoning.

## Key Results
- ActiveLLM outperformed traditional active learning methods (LC, BALD, EKM, PE) and state-of-the-art few-shot learning techniques (ADAPET, PERFECT, SetFit)
- Achieved up to 24.29% F1 score improvement over traditional active learning methods
- Querying time of just seconds compared to minutes or hours for traditional methods
- Selection of 32 instances was found optimal for few-shot scenarios across datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large Language Models can overcome the cold-start problem in active learning by leveraging their zero-shot capabilities to select informative instances without requiring initial labeled data.
- Mechanism: The LLM acts as a query model that selects instances based on its understanding of the task and the unlabeled data provided in the prompt, bypassing the need for iterative model training.
- Core assumption: The LLM has sufficient understanding of the task domain and can accurately identify informative instances for the downstream BERT classifier.
- Evidence anchors: Abstract states "By leveraging LLMs' zero-shot capabilities, ActiveLLM overcomes the cold-start problem that plagues traditional active learning strategies."

### Mechanism 2
- Claim: The decoupling of the query model (LLM) from the successor model (BERT) eliminates the model-mismatch problem and allows for greater flexibility in model selection.
- Mechanism: By using an LLM as the query model, ActiveLLM is not constrained by the limitations of the BERT classifier during the instance selection process, enabling the selection of instances that are most beneficial for the BERT model's learning.
- Core assumption: The instances selected by the LLM are indeed informative for the BERT classifier and lead to improved performance.
- Evidence anchors: Abstract mentions "ActiveLLM can be extended to non-few-shot scenarios, allowing for iterative selections."

### Mechanism 3
- Claim: The use of Chain of Thought prompting with the LLM improves the quality of instance selection by enabling structured reasoning and reducing the impact of context length limitations.
- Mechanism: The "think step by step" CoT prompt encourages the LLM to reason through the instance selection process in a structured manner, leading to more informed and diverse selections.
- Core assumption: The LLM can effectively utilize the CoT prompt to improve its reasoning and selection process, and the benefits outweigh the increased context length.
- Evidence anchors: "We test the three CoT variants... The common 'think step by step' instruction yielded the best results."

## Foundational Learning

- Concept: Active Learning (AL) - the process of selectively choosing instances to be labeled to reduce labeling effort and improve model performance.
  - Why needed here: ActiveLLM is a novel AL method that leverages LLMs to select instances, so understanding the principles of AL is crucial for understanding how ActiveLLM works.
  - Quick check question: What is the main goal of active learning, and how does it differ from traditional supervised learning?

- Concept: Cold-start problem - the challenge of selecting informative instances when there is no initial labeled data available.
  - Why needed here: ActiveLLM specifically addresses the cold-start problem by using LLMs' zero-shot capabilities to select instances without requiring initial labeled data.
  - Quick check question: Why is the cold-start problem a significant challenge in active learning, and how does ActiveLLM overcome it?

- Concept: Chain of Thought (CoT) prompting - a technique that encourages LLMs to reason through a problem in a structured manner by providing step-by-step instructions.
  - Why needed here: ActiveLLM uses CoT prompting to improve the quality of instance selection by enabling the LLM to reason through the process in a structured way.
  - Quick check question: How does CoT prompting differ from standard prompting, and what are the potential benefits of using it with LLMs?

## Architecture Onboarding

- Component map: Task description + Unlabeled instances + Guidelines -> LLM (query model) -> Selected instance indices -> Oracle (true labels) -> BERT-like classifier (successor model) -> Trained model

- Critical path:
  1. Generate prompt with task description, unlabeled instances, and optional guidelines
  2. Send prompt to LLM and receive selected instance indices
  3. Retrieve true labels for the selected instances from the oracle
  4. Train BERT-like classifier on the selected instances with their true labels
  5. Evaluate the performance of the trained classifier on the test set

- Design tradeoffs:
  - Prompt length vs. instance selection quality - longer prompts may provide more context but can degrade LLM performance
  - Batch size of unlabeled instances vs. diversity of selection - larger batches may offer more diverse instances but increase context length
  - Selection size vs. training set size - larger selections provide more training data but may include less informative instances

- Failure signatures:
  - LLM consistently selects the same or very similar instances across different runs
  - BERT classifier fails to learn effectively from the selected instances, showing no improvement or even degradation in performance
  - Prompt length exceeds the LLM's context window, leading to incomplete or inaccurate responses

- First 3 experiments:
  1. Test different prompt configurations (e.g., with/without guidelines, different CoT variants) on a small dataset to identify the most effective design
  2. Evaluate the impact of varying the batch size of unlabeled instances on instance selection quality and context length limitations
  3. Compare the performance of ActiveLLM with different LLMs (e.g., GPT-4, GPT-3.5, Llama 3) to determine the most suitable model for the task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the token length of instances affect the performance of ActiveLLM across different LLMs and tasks?
- Basis in paper: The authors note that tasks with generally shorter text length performed better with Mistral Large when using 200 instances compared to 100 instances, suggesting token length may be a critical factor.
- Why unresolved: The paper uses a fixed batch size rather than a token-based limit, and does not systematically investigate how varying token lengths impact performance across models.
- What evidence would resolve it: Controlled experiments varying instance token length while keeping batch size constant, across multiple LLMs and tasks, to measure performance differences.

### Open Question 2
- Question: Would optimizing the prompt design for each specific LLM (rather than using a general prompt) significantly improve ActiveLLM's performance?
- Basis in paper: The authors mention that the prompt design was optimized for GPT-4, and other models may perform better with prompts specifically tailored to them.
- Why unresolved: The paper uses a single general prompt across all LLMs to demonstrate general applicability, but does not explore LLM-specific prompt optimization.
- What evidence would resolve it: Comparative experiments using both the general prompt and LLM-specific optimized prompts for each model, measuring performance improvements.

### Open Question 3
- Question: How does the choice of selection size (number of instances to select) impact ActiveLLM's performance in non-few-shot scenarios?
- Basis in paper: The authors test different selection sizes (32, 60, 90, 120, 180) specifically in few-shot scenarios, finding 32 optimal, but do not explore this parameter in iterative/non-few-shot settings.
- Why unresolved: The paper only explores selection size in the few-shot context, leaving open how this parameter should be chosen when accumulating data over multiple iterations.
- What evidence would resolve it: Experiments varying selection size in the iterated querying mode, measuring how different sizes affect performance as the dataset grows from 32 to 300 instances.

## Limitations

- The method's dependence on expensive API calls to LLMs like GPT-4 raises questions about scalability and cost-effectiveness for large-scale deployments
- Performance improvements are primarily demonstrated in few-shot scenarios (32 instances), leaving uncertainty about effectiveness in larger datasets
- Evaluation relies on simulated oracles providing true labels, which doesn't fully capture real-world annotation costs and human labeling variability

## Confidence

- **High Confidence**: The core claim that ActiveLLM can select informative instances without initial labeled data is well-supported by experimental results across multiple datasets and tasks
- **Medium Confidence**: The assertion that ActiveLLM outperforms state-of-the-art few-shot learning techniques is supported by F1 score improvements, but would benefit from additional baseline methods and larger-scale experiments
- **Low Confidence**: The claim about ActiveLLM's effectiveness in non-few-shot scenarios and its ability to help other active learning strategies overcome cold-start problems remains largely theoretical

## Next Checks

1. **Cost-Benefit Analysis**: Conduct experiments comparing ActiveLLM's performance against traditional active learning methods while tracking the actual costs of LLM API calls versus iterative model training, including analysis of diminishing returns at larger dataset scales

2. **Real-World Annotation Study**: Replace simulated oracles with human annotators to evaluate how well ActiveLLM-selected instances translate to practical labeling efficiency and whether the method maintains its advantages when accounting for human annotation time and costs

3. **Cross-Domain Generalization**: Test ActiveLLM across diverse domains (e.g., medical, legal, technical documentation) and languages to assess its robustness beyond the evaluated datasets, particularly examining how well LLM zero-shot capabilities transfer to specialized or low-resource domains