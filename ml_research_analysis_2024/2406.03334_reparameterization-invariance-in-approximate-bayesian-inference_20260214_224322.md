---
ver: rpa2
title: Reparameterization invariance in approximate Bayesian inference
arxiv_id: '2406.03334'
source_url: https://arxiv.org/abs/2406.03334
tags:
- laplace
- neural
- diffusion
- sampled
- approximation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper identifies that Bayesian neural networks (BNNs) suffer
  from a reparameterization invariance issue: approximate posteriors assign different
  probabilities to different parametrizations of the same function. This undermines
  the correspondence between parameter uncertainty and functional uncertainty.'
---

# Reparameterization invariance in approximate Bayesian inference

## Quick Facts
- arXiv ID: 2406.03334
- Source URL: https://arxiv.org/abs/2406.03334
- Reference count: 40
- The paper shows that Bayesian neural networks suffer from reparameterization invariance issues where approximate posteriors assign different probabilities to different parametrizations of the same function, and proposes Laplace diffusion using the generalized Gauss-Newton matrix to address this.

## Executive Summary
This paper addresses a fundamental issue in Bayesian neural networks (BNNs): approximate posterizations are not invariant to reparameterizations of the network, leading to inconsistencies between parameter uncertainty and functional uncertainty. The authors show that the generalized Gauss-Newton (GGN) matrix induces a pseudo-Riemannian metric on the parameter space that respects reparameterizations. They propose a Riemannian diffusion process (Laplace diffusion) that samples from this geometric structure, consistently outperforming both linearized and sampled Laplace approximations across MNIST, FMNIST, and CIFAR-10 datasets in terms of in-distribution fit, calibration, and out-of-distribution detection.

## Method Summary
The method involves computing the generalized Gauss-Newton (GGN) matrix for a trained neural network, using the Lanczos algorithm to obtain a low-rank approximation, and then running a Riemannian diffusion process using the Euler-Maruyama scheme. The diffusion samples from a distribution that respects the pseudo-Riemannian geometry induced by the GGN, avoiding the underfitting issues of standard sampled Laplace approximations. The approach is evaluated against both linearized and sampled Laplace methods using the same prior precision.

## Key Results
- Laplace diffusion consistently outperforms both linearized and sampled Laplace approximations across MNIST, FMNIST, and CIFAR-10
- The method achieves better negative log-likelihood, accuracy, and expected calibration error
- Laplace diffusion maintains competitive performance against strong baselines like SWAG and last-layer Laplace
- The approach shows improved out-of-distribution detection capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The generalized Gauss-Newton (GGN) matrix induces a pseudo-Riemannian metric on the parameter space that respects reparameterizations.
- Mechanism: The GGN matrix captures functional differences on the training data. For infinitesimal parameter displacements, the squared distance between function values is proportional to the GGN inner product, forming a pullback pseudo-metric. This metric is invariant under reparameterizations because it depends only on function values, not parameter values.
- Core assumption: The Hessian of the log-likelihood is positive definite (true for Gaussian and Bernoulli likelihoods).
- Evidence anchors:
  - [abstract]: "We investigate this issue in the context of the increasingly popular linearized Laplace approximation... We develop a new geometric view of reparametrizations from which we explain the success of linearization."
  - [section 4.2]: "Consider weights w and an infinitesimal displacement ϵ, we then define, dist2(w, w + ϵ) = NX n=1 ∥f(w, xn) − f(w + ϵ, xn)∥2 = ϵ⊤GGN wϵ + O(ϵ3), where the last step follows from a first-order Taylor expansion of f around w."
  - [corpus]: No direct evidence found for the GGN pullback metric construction.
- Break condition: If the Hessian is not positive definite (e.g., cross-entropy), the GGN becomes a pseudo-metric and may not provide a well-defined Riemannian structure.

### Mechanism 2
- Claim: The kernel of the GGN matrix corresponds to reparameterizations that do not change function values on the training data.
- Mechanism: The GGN matrix is J⊤wHwJ where J is the Jacobian of the network. Any parameter displacement in ker(GGN) produces zero change in function values on the training set. This means Gaussian posterior samples with components in the kernel add "incorrect" degrees of freedom that don't affect training data fit.
- Core assumption: The Jacobian is full rank (supported by NTK literature for overparametrized networks).
- Evidence anchors:
  - [section 3]: "Consider the linear setting... A(g(w) − w) = 0. This implies that for any reparameterization of a linear function, we have g(w) − w ∈ ker(A)"
  - [section 3]: "For a linearized neural network... the kernel of the stacked Jacobian Jw′ characterizes the reparameterizations of the linearized network."
  - [section 3]: "By construction, these have the same non-zero eigenvalues, and thereby also have identical ranks. We, thus, see that the kernel of the Jacobian coincides with that of the GGN, i.e. ker(Jw) = ker(GGN w)."
  - [corpus]: No direct evidence found for the kernel-GGN correspondence in neural networks.
- Break condition: If the Jacobian is rank-deficient, the kernel interpretation breaks down and the correspondence between functional invariance and kernel membership fails.

### Mechanism 3
- Claim: The Laplace diffusion process samples from a distribution that respects the pseudo-Riemannian geometry, avoiding underfitting.
- Mechanism: The diffusion follows dw = √2τG(w)−1/2 dW where G(w) is the GGN. This process concentrates samples in directions of functional change while avoiding reparameterization directions. The Euler-Maruyama discretization wt+1 = wt + √2htG(wt)−1/2 ϵ samples from the induced geometry.
- Core assumption: The Euler-Maruyama scheme with sufficient steps approximates the Riemannian diffusion.
- Evidence anchors:
  - [section 5]: "Practically speaking this simple process can be simulated using an Euler–Maruyama (Maruyama, 1955) scheme."
  - [section 5]: "The diffusion can be simulated with a multi-step Euler-Maruyama scheme from which the linearized Laplace approximation (LLA) is a single-step."
  - [section 7]: "Experimentally, our diffusion consistently improves posterior fit, suggesting that reparameterizations should be given more attention in Bayesian deep learning."
  - [corpus]: No direct evidence found for the effectiveness of Laplace diffusion in practice.
- Break condition: If the step size is too large, the Euler-Maruyama discretization fails to capture the geometry properly, leading to poor sampling.

## Foundational Learning

- Concept: Quotient groups and equivalence classes
  - Why needed here: The paper uses quotient groups to formalize that different parameter values can represent the same function (reparameterizations). Understanding how equivalence classes partition the parameter space is crucial for grasping why traditional posteriors fail.
  - Quick check question: If two parameter vectors w1 and w2 produce identical function outputs on the training data, what mathematical relationship describes them?

- Concept: Pseudo-Riemannian geometry
  - Why needed here: The GGN induces a pseudo-Riemannian (not Riemannian) metric because it's not positive definite in overparametrized networks. This distinction is critical for understanding why the metric can have zero distances between different parameter values.
  - Quick check question: What property of the GGN matrix makes it a pseudo-metric rather than a true Riemannian metric?

- Concept: Neural Tangent Kernel (NTK) and its connection to GGN
  - Why needed here: The paper relies on NTK theory to justify that the GGN has a non-trivial kernel structure in overparametrized networks. Understanding this connection helps explain why reparameterization issues become more severe as models grow.
  - Quick check question: How does the rank of the NTK matrix relate to the memorization capacity and generalization of overparametrized networks?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Neural network training (MAP estimation) -> GGN computation -> Lanczos decomposition -> Diffusion sampling -> Evaluation
- Critical path:
  1. Train network to obtain MAP parameters
  2. Compute GGN matrix implicitly via Jacobian-vector products
  3. Apply Lanczos algorithm to get top-k eigenvalues/eigenvectors
  4. Run diffusion sampling with Euler-Maruyama steps
  5. Use neural network forward pass for predictions from samples
- Design tradeoffs:
  - GGN approximation vs exact computation: Exact GGN is intractable for large networks; Lanczos provides a scalable approximation
  - Diffusion step size: Larger steps reduce runtime but may break geometric properties
  - Number of Lanczos iterations: More iterations give better GGN approximation but increase cost
  - Neural network predictive vs linearized: Neural network is more accurate but slower; linearized is faster but loses reparameterization invariance
- Failure signatures:
  - Underfitting with sampled Laplace but not linearized Laplace indicates reparameterization issues
  - Poor out-of-distribution detection suggests insufficient uncertainty quantification
  - Numerical instability in Lanczos decomposition indicates ill-conditioned GGN
- First 3 experiments:
  1. Small MLP on toy regression: Compare sampled Laplace vs linearized Laplace vs Laplace diffusion to verify underfitting behavior
  2. MNIST classification: Benchmark in-distribution accuracy and calibration across methods
  3. Rotated MNIST: Test robustness to distribution shift and out-of-distribution detection performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Riemannian diffusion process be extended to handle discrete weight reparameterizations or discrete latent variables?
- Basis in paper: [inferred] The paper focuses on continuous reparameterizations and their geometric structure, but discrete reparameterizations are common in neural networks (e.g., dropout, batch normalization).
- Why unresolved: The geometric framework relies on smooth manifolds and the generalized Gauss-Newton matrix, which may not directly apply to discrete cases.
- What evidence would resolve it: Developing a Riemannian diffusion process for discrete reparameterizations and comparing its performance against continuous cases.

### Open Question 2
- Question: How does the proposed Riemannian diffusion process scale to extremely large models with billions of parameters?
- Basis in paper: [explicit] The paper mentions that sampling from Laplace's approximation requires computing the inverse square root of a matrix of size D × D, which is intractable for most models.
- Why unresolved: The computational complexity of the Lanczos algorithm used for low-rank approximations may become prohibitive for extremely large models.
- What evidence would resolve it: Benchmarking the Riemannian diffusion process on extremely large models and developing more efficient algorithms for handling the generalized Gauss-Newton matrix.

### Open Question 3
- Question: Can the geometric insights from this paper be applied to improve the calibration of non-Bayesian neural networks?
- Basis in paper: [inferred] The paper shows that the linearized Laplace approximation, which is not Bayesian, benefits from linearization due to reparameterization invariance.
- Why unresolved: The relationship between reparameterization invariance and calibration is not fully explored in the paper.
- What evidence would resolve it: Applying the geometric insights to improve the calibration of non-Bayesian neural networks and comparing their performance against standard methods.

## Limitations

- The computational complexity of the Lanczos algorithm may become prohibitive for extremely large models with billions of parameters
- The theoretical framework relies on the positive definiteness of the Hessian, which may not hold for all likelihood functions
- The paper does not provide extensive ablation studies for hyperparameters like diffusion step size and Lanczos iteration count

## Confidence

- **High Confidence**: The empirical demonstration that Laplace diffusion outperforms both sampled and linearized Laplace approximations across multiple datasets and metrics (NLL, accuracy, calibration)
- **Medium Confidence**: The theoretical framework connecting GGN to pseudo-Riemannian geometry and reparameterization invariance, as some technical details (like the exact kernel-GGN correspondence) are stated without complete proof
- **Low Confidence**: The claim that this is a "novel" geometric view, as similar ideas about function-space metrics in neural networks have appeared in NTK literature

## Next Checks

1. **Ablation study on Lanczos iterations**: Run experiments varying the number of Lanczos iterations (e.g., 5, 10, 20) to verify that the geometric properties are preserved across different approximation qualities.

2. **Perturbation sensitivity analysis**: Systematically test how sensitive the results are to the diffusion step size and number of samples, to establish the robustness of the improvements.

3. **Alternative architectures**: Test the method on architectures beyond LeNet and ResNet (e.g., Vision Transformers) to verify the generality of the geometric approach across different model families.