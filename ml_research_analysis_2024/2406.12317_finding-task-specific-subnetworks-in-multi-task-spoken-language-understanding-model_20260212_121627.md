---
ver: rpa2
title: Finding Task-specific Subnetworks in Multi-task Spoken Language Understanding
  Model
arxiv_id: '2406.12317'
source_url: https://arxiv.org/abs/2406.12317
tags:
- pruning
- tasks
- task
- multi-task
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study addresses two key challenges in multi-task spoken language
  understanding (SLU): large model sizes and catastrophic forgetting when adapting
  to new data for a specific task. The authors propose finding task-specific subnetworks
  within a multi-task SLU model via neural network pruning.'
---

# Finding Task-specific Subnetworks in Multi-task Spoken Language Understanding Model

## Quick Facts
- arXiv ID: 2406.12317
- Source URL: https://arxiv.org/abs/2406.12317
- Reference count: 0
- Proposed task-specific subnetwork identification in multi-task SLU models to reduce model size and mitigate catastrophic forgetting

## Executive Summary
This study addresses two major challenges in multi-task spoken language understanding (SLU): large model sizes and catastrophic forgetting when adapting to new tasks. The authors propose identifying task-specific subnetworks within a multi-task SLU model through neural network pruning. By updating only these subnetworks during training, the approach aims to reduce computational overhead while preserving performance across tasks. The method was evaluated using the state-of-the-art UniverSLU model trained for emotion recognition, intent classification, and automatic speech recognition.

## Method Summary
The authors employ neural network pruning to identify task-specific subnetworks within a pre-trained multi-task SLU model. After initial multi-task training, task-specific subnetworks are discovered through iterative pruning based on task relevance. During adaptation to new data, only the relevant subnetwork is updated while other parameters remain frozen, preventing interference with previously learned tasks. This selective parameter update strategy enables efficient adaptation without catastrophic forgetting.

## Key Results
- Pruned models achieved better performance on emotion recognition and intent classification with significantly fewer parameters
- Successful adaptation to additional ASR and IC data with minimal degradation on previously trained tasks
- Extended results to 7 tasks demonstrating effectiveness in complex multi-task settings

## Why This Works (Mechanism)
The approach works by identifying and isolating task-specific parameters within the multi-task model, allowing each task to have its own dedicated subnetwork. During training or adaptation, only the relevant subnetwork is updated, which prevents interference between tasks and reduces the risk of catastrophic forgetting. This selective parameter sharing and updating mechanism maintains task-specific knowledge while minimizing redundant computations.

## Foundational Learning
1. **Multi-task Learning**: Training a single model on multiple related tasks simultaneously to leverage shared representations and improve generalization. Needed to understand how tasks interact in shared parameter spaces. Quick check: Verify understanding of task interference and parameter sharing benefits.
2. **Neural Network Pruning**: Removing redundant or less important parameters from a trained network to reduce model size and computational cost. Needed to comprehend how task-specific subnetworks are identified. Quick check: Understand different pruning criteria (magnitude-based, gradient-based, etc.).
3. **Catastrophic Forgetting**: The phenomenon where neural networks forget previously learned tasks when trained on new tasks. Needed to grasp the motivation for selective parameter updating. Quick check: Know examples of forgetting in sequential learning scenarios.
4. **Subnetwork Identification**: The process of discovering task-specific parameter subsets within a larger network. Needed to understand how the pruning methodology isolates relevant parameters. Quick check: Recognize the difference between unstructured and structured pruning approaches.

## Architecture Onboarding

**Component Map**: Input Speech -> Encoder -> Multi-task Head (ER, IC, ASR) -> Output Predictions

**Critical Path**: The pruning process that identifies task-specific subnetworks is the critical path, as it determines which parameters will be updated during adaptation.

**Design Tradeoffs**: 
- Parameter efficiency vs. performance: Smaller subnetworks reduce computational cost but may limit representational capacity
- Task isolation vs. knowledge sharing: Complete task isolation prevents interference but loses potential cross-task benefits
- Pruning aggressiveness vs. recovery ability: Aggressive pruning yields smaller models but may make adaptation more difficult

**Failure Signatures**:
- Performance degradation on previously learned tasks during adaptation
- Inability to converge when adapting subnetworks to new data
- Subnetworks becoming too small to effectively represent task-specific features

**First 3 Experiments**:
1. Apply pruning to identify task-specific subnetworks and measure parameter reduction
2. Adapt a single task subnetwork to new data while monitoring performance on other tasks
3. Compare full model vs. subnetwork adaptation performance on held-out test data

## Open Questions the Paper Calls Out
None

## Limitations
- Results may not generalize across different SLU architectures beyond the specific UniverSLU model tested
- Computational cost of finding optimal subnetworks is not fully characterized
- Experiments focus on a limited set of tasks, constraining external validity

## Confidence
- Parameter reduction claims: High for specific experimental setup, Medium for broader applicability
- Performance maintenance claims: High for tested tasks, Medium for generalization
- Catastrophic forgetting mitigation: Medium confidence, evaluated only for specific adaptation scenarios

## Next Checks
1. Evaluate the subnetwork identification approach across diverse SLU architectures beyond UniverSLU
2. Conduct ablation studies on pruning methodology to isolate the contribution of different pruning criteria
3. Test performance on previously unseen tasks to assess the generalization capability of the task-specific subnetworks