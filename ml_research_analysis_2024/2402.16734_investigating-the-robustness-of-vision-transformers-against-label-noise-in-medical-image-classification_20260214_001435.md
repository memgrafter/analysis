---
ver: rpa2
title: Investigating the Robustness of Vision Transformers against Label Noise in
  Medical Image Classification
arxiv_id: '2402.16734'
source_url: https://arxiv.org/abs/2402.16734
tags:
- noise
- label
- training
- image
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the robustness of Vision Transformers (ViTs)
  against label noise in medical image classification compared to CNN-based architectures
  like ResNet18. We find that ViTs are more susceptible to overfitting, particularly
  with larger model sizes.
---

# Investigating the Robustness of Vision Transformers against Label Noise in Medical Image Classification

## Quick Facts
- arXiv ID: 2402.16734
- Source URL: https://arxiv.org/abs/2402.16734
- Reference count: 30
- Key outcome: Vision Transformers are more susceptible to overfitting on noisy labels than CNNs, but self-supervised pretraining significantly improves their robustness in medical image classification tasks.

## Executive Summary
This study investigates how Vision Transformers (ViTs) handle label noise compared to CNN-based architectures like ResNet18 in medical image classification. The researchers find that ViTs, particularly larger models, are more prone to overfitting when trained from scratch on noisy labels. However, when ViTs are pretrained using self-supervised methods like MAE and SimMIM before applying Learning with Noisy Labels (LNL) techniques, their robustness against label noise significantly improves. The study demonstrates these findings on two medical image datasets: COVID-DU-Ex and NCT-CRC-HE-100K.

## Method Summary
The researchers conducted experiments on two medical image datasets by injecting synthetic label noise at various rates (0.1 to 0.9) and training both ViT and ResNet18 architectures. They compared performance with standard cross-entropy loss and Co-teaching methods, both with and without self-supervised pretraining using MAE and SimMIM. Performance was evaluated using balanced accuracy metrics, specifically measuring BEST (peak performance) and LAST (average of last five epochs) scores.

## Key Results
- ViTs are more susceptible to overfitting on noisy labels, especially larger models like ViT Base
- Without pretraining, ViTs are less effective than CNNs for Learning with Noisy Labels methods
- Self-supervised pretraining significantly improves ViT robustness against label noise when combined with LNL techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vision Transformers are more susceptible to overfitting when trained from scratch on noisy labels, especially with larger model sizes.
- Mechanism: ViTs have higher parameter count and greater architectural flexibility, allowing them to memorize noisy labels more easily than CNNs like ResNet18.
- Core assumption: Model capacity correlates with overfitting tendency under noisy supervision.
- Evidence anchors:
  - [section] "ViT Base LAST's performance for NCT-CRC-HE-100K is worse than the corresponding BEST performance, potentially due to ViT Base's large parameter size and greater flexibility, making it more prone to overfitting noisy labels."
  - [abstract] "We find that ViTs are more susceptible to overfitting, particularly with larger model sizes."
- Break condition: If the dataset size is large enough to dilute label noise impact or if regularization is heavily applied.

### Mechanism 2
- Claim: Self-supervised pretraining improves ViT robustness against label noise by learning more generalizable feature representations.
- Mechanism: Pretraining via methods like MAE or SimMIM reconstructs masked patches without using labels, forcing the model to learn structural and semantic patterns that are less dependent on spurious correlations introduced by noisy labels.
- Core assumption: Representations learned without label supervision are less biased by label noise.
- Evidence anchors:
  - [section] "Since self-supervised learning does not rely on given labels for training, it is not impacted by label noise, thereby learning more robust feature representations."
  - [abstract] "However, when ViTs are pretrained using self-supervised methods before applying LNL techniques, their robustness against label noise significantly improves."
- Break condition: If pretraining data distribution is too different from downstream noisy data.

### Mechanism 3
- Claim: Co-teaching with ViTs is ineffective without pretraining but effective with pretrained backbones.
- Mechanism: Co-teaching selects clean samples based on loss, but ViTs without pretraining lack the discriminative power to reliably identify clean samples, leading to co-selection of noisy ones. Pretraining improves feature quality, enabling effective clean sample selection.
- Core assumption: Feature quality directly impacts loss-based sample selection reliability.
- Evidence anchors:
  - [section] "Co-teaching does not perform well with ViTs when trained from scratch... However, when transformers are pretrained, both ViT Base and ViT Small show significant performance improvements."
  - [abstract] "Without pretraining, ViTs are less effective than CNNs for Learning with Noisy Label (LNL) methods."
- Break condition: If noise rate exceeds the critical threshold where clean sample identification is statistically unreliable.

## Foundational Learning

- Concept: Label noise and its impact on supervised training
  - Why needed here: Understanding how synthetic label flipping at various rates degrades model generalization is central to the study's design and evaluation.
  - Quick check question: What happens to a model's test accuracy as label noise rate increases from 0 to 0.9?

- Concept: Self-supervised learning methods (MAE, SimMIM)
  - Why needed here: These pretraining methods are key to improving ViT robustness, and understanding their reconstruction objectives explains why they help.
  - Quick check question: How do MAE and SimMIM differ in their masked patch reconstruction approaches?

- Concept: Co-teaching algorithm mechanics
  - Why needed here: The method's clean sample selection based on loss is critical to evaluating its interaction with ViTs versus CNNs.
  - Quick check question: Why does Co-teaching fail to improve performance when ViTs are trained from scratch but succeed after pretraining?

## Architecture Onboarding

- Component map:
  Input -> Patch tokenization (ViT) / Convolution (ResNet18) -> Positional embeddings (ViT only) -> Transformer encoder (ViT) / Residual blocks (ResNet18) -> MLP head for classification -> Loss computation (cross-entropy or Co-teaching)

- Critical path:
  1. Load and preprocess dataset (224x224)
  2. Inject synthetic label noise at specified rate
  3. Train from scratch with cross-entropy
  4. Optionally pretrain ViT with MAE/SimMIM
  5. Train with Co-teaching if applicable
  6. Evaluate BEST and LAST metrics

- Design tradeoffs:
  - ViT vs. ResNet18: higher parameter count and global attention in ViT vs. local feature extraction in CNN; ViT needs more data/compute but can capture richer patterns
  - Pretraining: improves robustness but adds training time and complexity
  - Co-teaching hyperparameters: warm-up epochs and sample selection thresholds must be tuned per backbone

- Failure signatures:
  - High LAST score drop compared to BEST -> overfitting to noisy labels
  - Pretrained ViT performance worse than ResNet18 -> pretraining did not transfer well
  - Co-teaching with ViT from scratch yields poor results -> backbone lacks discriminative features

- First 3 experiments:
  1. Train ViT Small and ResNet18 from scratch on clean labels (p=0) to establish baseline performance
  2. Train both models on noisy labels (p=0.3, 0.6) without pretraining to observe overfitting
  3. Pretrain ViT Small with MAE on clean data, then train on noisy labels to test robustness improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does pretraining with self-supervised learning consistently improve ViT performance across different medical image classification tasks and noise types beyond those studied here?
- Basis in paper: [explicit] The paper shows pretraining helps ViTs handle label noise in COVID-DU-Ex and NCT-CRC-HE-100K datasets, but notes this improvement is more pronounced at high noise levels.
- Why unresolved: The study only tested two medical datasets and focused on random label noise; performance may vary with other noise types or medical tasks.
- What evidence would resolve it: Systematic experiments testing various medical imaging tasks, noise distributions (class-dependent, asymmetric), and self-supervised methods would clarify generalizability.

### Open Question 2
- Question: Why does ViT Base overfit more to label noise compared to ViT Small and ResNet18, even with pretraining?
- Basis in paper: [explicit] ViT Base shows worse LAST performance and greater overfitting to noisy labels, especially without pretraining, likely due to its larger parameter size.
- Why unresolved: The study identifies the issue but does not explore architectural or regularization modifications to mitigate this overfitting.
- What evidence would resolve it: Comparative experiments testing regularization techniques, architecture scaling, or alternative ViT variants would reveal the underlying cause and potential solutions.

### Open Question 3
- Question: How do attention map visualizations correlate with model robustness against label noise, and can they be used to predict overfitting?
- Basis in paper: [inferred] Attention maps from ViTs without pretraining become noisier with increased label noise, while pretrained ViTs maintain cleaner attention, suggesting a visual indicator of robustness.
- Why unresolved: The study provides qualitative observations but does not quantify the relationship between attention map quality and overfitting or predictive performance.
- What evidence would resolve it: Quantitative metrics linking attention map entropy or dispersion to model generalization and overfitting would validate this hypothesis.

## Limitations
- Findings limited to two specific medical image datasets and may not generalize to other domains
- Label noise injection uses synthetic noise which may not capture real-world label corruption patterns
- Focus on ViT variants without comparison to other vision transformer architectures or exploration of alternative pretraining strategies

## Confidence
- High Confidence: The core finding that ViTs are more susceptible to overfitting with noisy labels, particularly for larger models (ViT Base vs. ViT Small)
- Medium Confidence: The effectiveness of self-supervised pretraining in improving ViT robustness against label noise, based on results from two pretraining methods
- Medium Confidence: The conclusion that Co-teaching is ineffective with untrained ViTs but effective with pretrained ones, though the analysis is based on a single LNL method

## Next Checks
1. Test the same experimental setup on additional medical imaging datasets (e.g., dermatology, mammography) to assess generalizability across different medical modalities
2. Implement and compare alternative self-supervised pretraining methods (e.g., DINO, SimCLR) to determine if the observed robustness improvements are specific to MAE and SimMIM
3. Conduct ablation studies on Co-teaching hyperparameters (warm-up epochs, loss thresholds) to identify optimal settings for pretrained vs. non-pretrained ViTs