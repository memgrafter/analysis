---
ver: rpa2
title: 'Acting for the Right Reasons: Creating Reason-Sensitive Artificial Moral Agents'
arxiv_id: '2409.15014'
source_url: https://arxiv.org/abs/2409.15014
tags:
- agent
- moral
- theory
- reason
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces a method to integrate normative reasons into\
  \ reinforcement learning agents, enabling them to act based on moral justifications\
  \ rather than just instrumental goals. The approach adds a shield generator that\
  \ filters actions using a reason theory formalized via Horty\u2019s default logic,\
  \ ensuring only morally permissible actions are executed."
---

# Acting for the Right Reasons: Creating Reason-Sensitive Artificial Moral Agents

## Quick Facts
- arXiv ID: 2409.15014
- Source URL: https://arxiv.org/abs/2409.15014
- Reference count: 25
- Primary result: Introduces method to integrate normative reasons into RL agents via shield generator and iterative feedback from moral judge

## Executive Summary
This work presents a novel approach to creating artificial moral agents by integrating normative reasons into reinforcement learning through a reason-based shield generator. The method uses Horty's default logic to formalize moral reasoning and ensures agents only execute actions that are morally justified. A moral judge provides case-based feedback to iteratively improve the agent's reason theory, enabling it to learn correct moral priorities. Applied to a bridge scenario with moral dilemmas, the approach demonstrates how agents can learn to prioritize saving a drowning person over other actions based on moral justification rather than just instrumental goals.

## Method Summary
The method extends standard reinforcement learning with a reason-based shield generator that filters actions using a reason theory formalized via Horty's default logic. The shield computes moral obligations from background facts and blocks actions not conforming to these obligations. A moral judge provides case-based feedback when the agent acts impermissibly, enabling iterative improvement of the reason theory. The agent learns to prioritize obligations (e.g., rescuing drowning persons) through this feedback loop, ensuring its actions become morally justified when it learns correct reasons.

## Key Results
- Introduces reason-based shield generator that filters actions using Horty's default logic to ensure moral justification
- Presents algorithm for iterative improvement of reason theory through case-based feedback from moral judge
- Demonstrates approach in bridge scenario where agent learns to prioritize saving drowning person over other actions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The reason-based shield generator filters actions using Horty's default logic to ensure moral justification.
- Mechanism: The shield extends a reason theory with background facts, computes proper scenarios, and blocks actions not conforming to moral obligations in the selected scenario.
- Core assumption: The default logic formalization correctly captures normative reasoning and produces coherent moral obligations.
- Evidence anchors:
  - [abstract] "Central to this approach is a reason-based shield generator yielding a moral shield that binds the agent to actions that conform with recognized normative reasons"
  - [section 4.1] "We introduce an algorithm to derive a shield from an agent's reason theory ⟨<, D⟩ in a state s"
  - [corpus] Weak - no direct comparisons or validation of Horty's formalism in RL contexts found.
- Break condition: If the default logic produces inconsistent or conflicting moral obligations, the shield may randomly select among them, leading to arbitrary moral decisions.

### Mechanism 2
- Claim: The moral judge provides case-based feedback to iteratively improve the agent's reason theory.
- Mechanism: When the agent acts impermissibly, the judge identifies the missing obligation and reason, then the agent updates its rule set and priority ordering.
- Core assumption: Human-provided feedback is consistent and philosophically sound, enabling convergence to a correct reason theory.
- Evidence anchors:
  - [abstract] "describe an algorithm that allows to iteratively improve the reason-based shield generator through case-based feedback from a moral judge"
  - [section 4.2] "The agent enhances its reasoning through case-based feedback... a moral judge detects if the agent has performed a morally impermissible action"
  - [corpus] Weak - no empirical evidence on feedback consistency or convergence rates found.
- Break condition: Inconsistent feedback from multiple judges could lead to cyclic updates or divergence in the reason theory.

### Mechanism 3
- Claim: The agent's actions become morally justified when it learns correct reasons through iterative feedback.
- Mechanism: By learning to strictly prioritize obligations (e.g., rescuing drowning persons over waiting), the agent aligns its behavior with morally correct actions in the bridge scenario.
- Core assumption: The learned reason theory converges to one that produces morally correct actions in all relevant situations.
- Evidence anchors:
  - [abstract] "Assuming that the agent receives feedback such that it learns valid reasons, the agent learns to restrict itself by design to actions that are morally justified"
  - [section 4.3] "Based on this information, it checks whether its reason theory already includes the default rule... The agent then updates the order among its default rules"
  - [corpus] Weak - no validation that learned theories produce correct moral judgments in practice.
- Break condition: If the learning process gets stuck in local optima or receives insufficient diverse feedback, the agent may never learn correct moral priorities.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The RL agent operates in an MDP environment where states, actions, and rewards are defined.
  - Quick check question: What are the five components of an MDP tuple (S,A,P,R,γ)?
- Concept: Default Logic and Horty's Framework
  - Why needed here: The shield generator uses default logic to derive moral obligations from normative reasons.
  - Quick check question: In Horty's framework, what distinguishes a proper scenario from other scenarios?
- Concept: Safe Reinforcement Learning and Shielding
  - Why needed here: The moral shield is a type of safety mechanism that restricts the action space based on specifications.
  - Quick check question: How does a shield differ from simply adjusting reward functions in RL?

## Architecture Onboarding

- Component map: Environment (MDP with labels) -> Shield Generator (reason theory + default logic) -> Agent (RL policy) -> Moral Judge (feedback) -> Reason Theory (updated rules)
- Critical path: State observation → Shield generation → Action selection → Environment transition → Judge evaluation → Reason theory update
- Design tradeoffs:
  - Expressiveness vs. tractability: More complex reason theories may be harder to compute proper scenarios for
  - Shield strictness vs. flexibility: Stricter shields may prevent learning, while lenient shields may allow immoral actions
  - Human feedback vs. automation: Human judges provide philosophically sound feedback but may be inconsistent or unavailable
- Failure signatures:
  - Agent gets stuck with no available actions (shield too strict)
  - Agent learns incorrect moral priorities (feedback inconsistency or insufficient diversity)
  - Shield computation becomes intractable (reason theory too complex)
- First 3 experiments:
  1. Implement shield generator with a simple reason theory and test in the bridge scenario with no moral dilemmas
  2. Add moral judge providing feedback and verify reason theory updates correctly in simple cases
  3. Test agent learning in bridge scenario with moral dilemmas and evaluate whether it learns to prioritize saving drowning persons

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we ensure the moral judge provides consistent feedback across different situations?
- Basis in paper: [explicit] The paper mentions that one potential drawback is the possibility of inconsistency in human feedback and suggests exploring automation to ensure consistent feedback.
- Why unresolved: Human feedback can be subjective and context-dependent, making it challenging to maintain consistency. The paper acknowledges this issue but does not provide a concrete solution.
- What evidence would resolve it: A study demonstrating a method for automating feedback that consistently aligns with a predefined ethical framework across various scenarios.

### Open Question 2
- Question: How can the agent learn to abstract action types from primitive actions effectively?
- Basis in paper: [inferred] The paper assumes the agent is initially given the abstraction of action types from primitive actions but identifies learning this abstraction as an interesting direction for future work.
- Why unresolved: The abstraction of complex action types from primitive actions requires sophisticated learning mechanisms that are not addressed in the current framework.
- What evidence would resolve it: An implementation showing the agent successfully learning and applying action type abstractions in a complex environment without predefined abstractions.

### Open Question 3
- Question: What is the impact of the shield generator on the agent's learning efficiency and performance?
- Basis in paper: [explicit] The paper introduces the shield generator to filter morally impermissible actions but does not evaluate its impact on the agent's learning efficiency or overall performance.
- Why unresolved: While the shield ensures moral permissibility, it may also limit the agent's exploration and learning speed, potentially affecting performance.
- What evidence would resolve it: Empirical results comparing the learning efficiency and performance of agents with and without the shield generator across various tasks.

## Limitations
- The method relies heavily on the assumption that default logic can adequately capture normative reasoning, which lacks empirical validation in RL contexts
- The iterative feedback process depends on human-provided feedback that may be inconsistent or incomplete, potentially leading to incorrect reason theories
- The approach does not evaluate the impact of the shield generator on the agent's learning efficiency or overall performance

## Confidence

| Claim | Confidence |
|-------|------------|
| Shield generator using Horty's default logic can filter morally permissible actions | Low |
| Iterative feedback from moral judge leads to correct reason theories | Medium |
| Agent learns to prioritize correct moral obligations through feedback | Medium |

## Next Checks

1. Implement the shield generator with a simple reason theory and test in a bridge scenario with no moral dilemmas to verify basic functionality
2. Add moral judge providing feedback and verify reason theory updates correctly in simple cases with known correct answers
3. Test agent learning in bridge scenario with moral dilemmas and evaluate whether it learns to prioritize saving drowning persons, comparing against a baseline without moral reasoning