---
ver: rpa2
title: How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context
  Heads are Two Towers for Metric Learning
arxiv_id: '2402.02872'
source_url: https://arxiv.org/abs/2402.02872
tags:
- label
- positions
- heads
- in-context
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the mechanism of in-context learning (ICL)
  in large language models (LLMs) by analyzing attention heads on sentence classification
  tasks with semantically-unrelated labels. The authors identify a small set of "in-context
  heads" (approximately 1% of all heads) that significantly impact ICL accuracy.
---

# How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning

## Quick Facts
- arXiv ID: 2402.02872
- Source URL: https://arxiv.org/abs/2402.02872
- Reference count: 19
- Key outcome: Identifies ~1% of attention heads as "in-context heads" that drive ICL accuracy through two-tower metric learning between query-key and value-output matrices

## Executive Summary
This paper investigates how large language models perform in-context learning (ICL) by analyzing attention heads on sentence classification tasks with semantically-unrelated labels ("foo"/"bar"). The authors identify a small set of "in-context heads" that significantly impact ICL accuracy and propose a hypothesis that ICL works through two distinct mechanisms: value-output matrices extract label features while query-key matrices compute similarity metrics between input and label demonstrations. This framework explains common ICL biases including majority label bias and recency bias, and the authors demonstrate methods to reduce these biases by 22% and 17% respectively.

## Method Summary
The authors analyze attention heads in GPT2-large, Llama-7B, and GPT-J models on sentence classification tasks using demonstration-label pairs. They identify "in-context heads" through causal tracing and logit probability increase methods, then study value-output vectors and attention scores in these heads. The analysis focuses on understanding how these heads extract label information and compute similarity between input text and demonstrations. Based on these findings, they propose a two-tower metric learning hypothesis and demonstrate methods to reduce majority label bias and recency bias by modifying attention weights and positional embeddings.

## Key Results
- Identified approximately 1% of attention heads as "in-context heads" that significantly impact ICL accuracy
- Proposed hypothesis that value-output matrices extract label features while query-key matrices compute similarity metrics
- Demonstrated 22% reduction in majority label bias and 17% reduction in recency bias through targeted interventions
- Showed that intervening in in-context heads causes significant accuracy drops, confirming their importance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context heads extract label features in value-output matrices and compute similarity metrics between input and label demonstrations in query-key matrices
- Mechanism: Value-output matrices transform label position features into semantic vectors containing label information ("foo"/"bar"). Query-key matrices compute attention scores based on similarity between the last position (input text) and each label position (demonstration features). Higher similarity scores increase corresponding label probabilities.
- Core assumption: Each in-context head functions as a "two-tower" metric learning system where value-output matrices extract features and query-key matrices compute similarity scores
- Evidence anchors:
  - [abstract]: "value-output matrices extract label features, while the query-key matrices compute the similarity between the features at the last position and those at each label position"
  - [section 3.3]: "we analyze the value-output vectors and attention scores in Section 3.3 and find that value-output matrices extract the label information and attention scores computed by query-key matrices control the label information flow"
  - [corpus]: Weak - no direct matching studies found in corpus
- Break condition: If value-output matrices don't contain label-specific information or query-key matrices don't correlate attention scores with prediction shifts

### Mechanism 2
- Claim: Majority label bias occurs because sum of similarity scores increases with label frequency
- Mechanism: When a label appears more frequently in demonstrations, the sum of attention weights for that label position increases. Since final prediction probability depends on weighted sum of value-output vectors, labels with more demonstrations receive more information flow and higher probabilities.
- Core assumption: Attention scores are computed independently for each position but summed when determining final label probability
- Evidence anchors:
  - [section 4.1]: "the majority label bias can be attributed to the lack of attention weights on imbalanced label positions" and "when a label has high frequency, the sum of similarity scores will be larger"
  - [abstract]: "Using this hypothesis, we explain the majority label bias and recency bias in ICL"
  - [corpus]: Weak - no direct matching studies found in corpus
- Break condition: If attention score computation doesn't depend on label frequency or if final probability calculation uses different mechanism

### Mechanism 3
- Claim: Recency bias is caused by positional embedding influence on attention score computation
- Mechanism: Positional embeddings consistently influence attention scores in both shallow and deep layers. The "position term" in attention computation causes models to extract more features from recent positions and assign higher attention weights to later positions. This creates recency bias in predictions.
- Core assumption: Positional embeddings affect both feature extraction in shallow layers and attention score computation in deep layers
- Evidence anchors:
  - [section 4.2]: "the recency bias is caused by the influence of positional embedding during attention score computation in both shallow and deep layers"
  - [section 4.2]: "we hypothesize that the recency bias is caused by the influence of positional embedding during attention score computation"
  - [corpus]: Weak - no direct matching studies found in corpus
- Break condition: If removing positional embeddings from in-context heads doesn't reduce recency bias

## Foundational Learning

- Concept: Attention mechanism and softmax normalization
  - Why needed here: Understanding how query-key matrices compute attention scores and how these scores control information flow is fundamental to the proposed mechanism
  - Quick check question: What mathematical operation converts raw attention scores into normalized weights that sum to 1?

- Concept: Metric learning and similarity computation
  - Why needed here: The hypothesis treats query-key matrices as two towers learning similarity metrics, so understanding how similarity scores relate to feature matching is crucial
  - Quick check question: How do two-tower architectures typically compute similarity between query and key features?

- Concept: Positional embeddings and their effects
  - Why needed here: The recency bias mechanism depends on understanding how positional embeddings influence attention scores and feature extraction
  - Quick check question: What happens to attention scores when positional embeddings are removed from query or key vectors?

## Architecture Onboarding

- Component map: Input text → shallow layers (feature extraction) → in-context heads (value projection + similarity computation) → final prediction. In-context heads consist of query matrix (Q), key matrix (K), value matrix (V), and output matrix (O). Q and K compute attention scores, V transforms value vectors, O produces final output.
- Critical path: Input text → shallow layers (feature extraction) → in-context heads (value projection + similarity computation) → final prediction. The in-context heads are the critical decision point where label probabilities are determined.
- Design tradeoffs: Using 12 heads (6 fooheads, 6 barheads) provides redundancy and ensemble voting but increases computational cost. Fewer heads would be more efficient but less robust.
- Failure signatures: If intervening in in-context heads doesn't significantly affect accuracy, the hypothesis is wrong. If value-output vectors don't contain label-specific information, the mechanism fails. If attention scores don't correlate with prediction shifts, the model doesn't work as hypothesized.
- First 3 experiments:
  1. Intervene in individual in-context heads by zeroing parameters and measure accuracy change
  2. Analyze value-output vectors at label positions to check for label-specific information content
  3. Remove positional embeddings from in-context heads and measure recency bias reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do shallow layers extract features from demonstrations and input text, and what mechanisms control the quality of these features?
- Basis in paper: [explicit] The authors hypothesize that shallow layers extract features from demonstrations and input text, but they acknowledge this needs further investigation.
- Why unresolved: The paper focuses on analyzing deep layers' in-context heads but does not thoroughly investigate the feature extraction process in shallow layers.
- What evidence would resolve it: Detailed analysis of attention patterns and feature representations in shallow layers across different ICL tasks and datasets.

### Open Question 2
- Question: Do different ICL tasks (e.g., classification, reasoning, generation) share the same in-context heads and query-key matrices, or do they develop task-specific mechanisms?
- Basis in paper: [inferred] The authors note their hypothesis explains classification tasks but mention more studies are needed for other ICL tasks like chain-of-thought reasoning.
- Why unresolved: The paper primarily examines classification tasks with semantically-unrelated labels, leaving open whether the proposed mechanism generalizes to other task types.
- What evidence would resolve it: Comparative analysis of in-context head importance and query-key matrix behavior across multiple ICL task types.

### Open Question 3
- Question: What is the optimal number and parameter configuration of in-context heads needed for effective ICL across different model sizes and tasks?
- Basis in paper: [explicit] The authors discuss that the number and parameters of in-context heads affect learning ability, treating them as "voting or ensemble models," but don't provide systematic analysis.
- Why unresolved: The paper identifies important heads but doesn't explore how head number or parameterization affects performance across different model scales.
- What evidence would resolve it: Systematic ablation studies varying the number and configuration of in-context heads across different model sizes and ICL tasks.

## Limitations
- Core hypothesis relies on specific attribution methods that may not generalize across different model architectures or tasks
- Analysis focuses on sentence classification with binary labels, limiting generalizability to more complex classification or generation tasks
- Proposed bias reduction methods show modest improvements (22% and 17% reduction) but don't eliminate biases entirely

## Confidence

- **High Confidence**: The identification of in-context heads as critical for ICL accuracy is well-supported by intervention experiments showing significant accuracy drops when these heads are modified.
- **Medium Confidence**: The two-tower metric learning hypothesis explaining value-output and query-key matrix functions is plausible but relies on correlation rather than direct causal evidence of the proposed mechanism.
- **Low Confidence**: The explanations for majority label bias and recency bias, while theoretically consistent with the hypothesis, lack strong empirical validation across diverse model architectures and task types.

## Next Checks

1. **Cross-model validation**: Apply the in-context head identification and bias reduction methods to models with different architectures (e.g., transformers with different attention patterns) to test generalizability of the two-tower hypothesis.

2. **Multi-class extension**: Test the hypothesis on multi-class classification tasks with more than two labels to determine if the same mechanisms explain more complex label relationships and potential new biases.

3. **Direct mechanism validation**: Conduct ablation studies where value-output vectors and query-key attention computations are independently modified to isolate their individual contributions to ICL performance, rather than relying on correlational analysis.