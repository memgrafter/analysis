---
ver: rpa2
title: Zeroth-Order Fine-Tuning of LLMs in Random Subspaces
arxiv_id: '2410.08989'
source_url: https://arxiv.org/abs/2410.08989
tags:
- subzero
- gradient
- mezo
- matrix
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the high memory cost of fine-tuning large language
  models (LLMs) due to backpropagation gradients. It introduces SubZero, a zeroth-order
  optimizer that uses layer-wise low-rank perturbations within random subspaces to
  reduce gradient variance and memory usage.
---

# Zeroth-Order Fine-Tuning of LLMs in Random Subspaces

## Quick Facts
- arXiv ID: 2410.08989
- Source URL: https://arxiv.org/abs/2410.08989
- Reference count: 40
- Key outcome: SubZero reduces memory cost of LLM fine-tuning by using zeroth-order optimization with layer-wise low-rank perturbations in random subspaces, achieving better accuracy than MeZO while maintaining memory efficiency comparable to inference.

## Executive Summary
This paper addresses the high memory cost of fine-tuning large language models (LLMs) by introducing SubZero, a zeroth-order optimizer that eliminates backpropagation gradients. SubZero uses layer-wise low-rank perturbations within random subspaces to estimate gradients using only forward passes, significantly reducing memory consumption while improving performance. The method combines theoretical analysis showing convergence guarantees with empirical validation demonstrating superior accuracy and efficiency compared to existing zeroth-order methods like MeZO.

## Method Summary
SubZero introduces a layer-wise low-rank perturbation strategy that confines gradient estimation to random subspaces, reducing variance compared to standard zeroth-order methods. The approach generates column-orthogonal matrices via QR decomposition for each layer, applies small Gaussian perturbations, and uses SPSA-like gradient estimation through forward pass differences. A lazy subspace update strategy regenerates projection matrices periodically to balance computational efficiency with estimation accuracy. The method is integrated with various fine-tuning schemes including LoRA, prefix tuning, and prompt tuning.

## Key Results
- SubZero outperforms MeZO across multiple fine-tuning schemes (LoRA, prefix tuning, prompt tuning) with better convergence and higher final accuracy
- Memory usage comparable to inference, achieving up to 10x memory savings compared to backpropagation fine-tuning
- Maintains accuracy on full-parameter fine-tuning while using significantly less memory than first-order methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** SubZero reduces gradient estimation variance by applying layer-wise low-rank perturbations within random subspaces.
- **Mechanism:** Each layer's parameter matrix is perturbed using a low-rank approximation $Z_i = U_i Z_i V_i^T$, where $U_i$ and $V_i$ are column-orthogonal matrices and $Z_i$ is a small Gaussian matrix. This confines the perturbation to a low-dimensional subspace, reducing the effective dimensionality from model parameter size $d$ to subspace size $q = lr^2$.
- **Core assumption:** The gradient subspace of LLM fine-tuning converges to a low-dimensional structure, making layer-wise perturbations sufficient for accurate estimation.
- **Evidence anchors:** [abstract] "We introduce a low-rank perturbation tailored for LLMs that significantly reduces memory consumption while improving performance." [section] "Our intuition is that exploring update directions in a low-dimensional subspace may result in a reduced variance of the estimated gradient compared to the estimation in the vanilla space as used in MeZO."
- **Break condition:** If the gradient subspace does not have low-rank structure, the variance reduction benefit disappears.

### Mechanism 2
- **Claim:** SubZero approximates backpropagation gradients more closely than standard zeroth-order methods by projecting gradients onto a structured subspace.
- **Mechanism:** Theorem 1 proves that SubZero's gradient estimate $\hat{g}_\varepsilon(x,P,z)$ closely approximates $P P^T \nabla f(x)$, the backpropagation gradient projected onto the subspace, with error bounded independently of model dimension $d$.
- **Core assumption:** The structured, block-diagonal projection matrix $P$ in SubZero captures the essential gradient directions better than unstructured projections.
- **Evidence anchors:** [abstract] "Additionally, we prove that our gradient estimation closely approximates the backpropagation gradient, exhibits lower variance than traditional ZO methods, and ensures convergence when combined with SGD." [section] "Theorem 1 (b) guarantees that the distance Φ(x) between the expected gradient estimate and the BP gradient in the subspace spanned by P is small."
- **Break condition:** If the structured projection fails to align with the true gradient subspace, approximation quality degrades.

### Mechanism 3
- **Claim:** Lazy subspace updates in SubZero reduce computational overhead without sacrificing convergence speed.
- **Mechanism:** Instead of updating projection matrices $U_i$ and $V_i$ every iteration, they are regenerated only every $F$ steps, reducing QR decomposition overhead while maintaining effective subspace tracking.
- **Core assumption:** The gradient subspace changes slowly enough that periodic updates suffice for accurate gradient estimation.
- **Evidence anchors:** [section] "We propose a lazy subspace update strategy that periodically regenerates the projection matrices Ui and Vi." [section] "This lazy subspace update strategy is both efficient and effective in all our experiments."
- **Break condition:** If the gradient subspace evolves rapidly, infrequent updates cause gradient estimation to become stale.

## Foundational Learning

- **Concept:** Zeroth-order optimization (gradient-free methods)
  - **Why needed here:** SubZero eliminates backpropagation by estimating gradients using only forward passes, crucial for memory-constrained LLM fine-tuning.
  - **Quick check question:** How does the SPSA estimator in Eqn. (2) approximate gradients without backpropagation?

- **Concept:** Low-rank matrix approximation and QR decomposition
  - **Why needed here:** SubZero uses QR decomposition to create column-orthogonal matrices $U_i$ and $V_i$ for constructing low-rank perturbations that reduce variance.
  - **Quick check question:** Why are column-orthogonal matrices preferred over random Gaussian matrices for the projection matrices?

- **Concept:** Random subspace optimization theory
  - **Why needed here:** The theoretical foundation showing that gradients can be accurately estimated in low-dimensional subspaces, as proven in Theorems 1 and 2.
  - **Quick check question:** What is the key difference between SubZero's block-diagonal projection and standard random subspace methods?

## Architecture Onboarding

- **Component map:** Projection matrix generator (QR decomposition) -> Low-rank perturbation generator -> Forward pass executor (two passes) -> Loss difference calculator -> Gradient estimator (SPSA-like) -> Layer-wise parameter updater
- **Critical path:** Generate projection matrices → Create low-rank perturbations → Compute loss difference via two forward passes → Estimate gradients per layer → Update parameters layer-wise
- **Design tradeoffs:** Memory vs. accuracy tradeoff in choosing rank $r$ (smaller $r$ saves memory but may reduce estimation quality), update frequency $F$ (higher $F$ reduces computation but may cause stale subspaces)
- **Failure signatures:** High gradient variance (poor convergence), memory usage approaching backpropagation levels (rank too large), slow convergence (update frequency too low), or NaN/inf values in training (improper perturbation scaling)
- **First 3 experiments:**
  1. Verify gradient estimation variance reduction: Compare cosine similarity and variance between SubZero and MeZO on a small model.
  2. Test memory efficiency: Measure GPU memory usage of SubZero vs. MeZO on full-parameter tuning with OPT-1.3B.
  3. Validate lazy updates: Compare convergence speed with different update frequencies $F$ on SST-2 dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SubZero's performance scale with extremely large models beyond 13B parameters?
- Basis in paper: [inferred] The paper mentions theoretical analysis shows SubZero's error doesn't scale with parameter dimensions, but doesn't provide empirical validation for much larger models.
- Why unresolved: The experiments only test up to OPT-13B parameters. Theoretical analysis doesn't guarantee practical performance on 100B+ parameter models.
- What evidence would resolve it: Empirical results on models with 50B+ parameters, demonstrating maintained memory efficiency and convergence speed.

### Open Question 2
- Question: What is the theoretical analysis of the reshaping strategy's effectiveness for non-square matrices?
- Basis in paper: [explicit] The paper mentions the reshaping strategy is critical for LoRA but leaves theoretical analysis for future work.
- Why unresolved: The paper provides empirical evidence but no theoretical guarantees about when and why reshaping works.
- What evidence would resolve it: Mathematical proof showing conditions under which reshaping preserves gradient information and maintains convergence properties.

### Open Question 3
- Question: How does SubZero compare to second-order ZO methods like HiZOO in practice?
- Basis in paper: [explicit] The paper mentions SubZero could be combined with second-order information methods but only provides basic comparisons.
- Why unresolved: The paper focuses on first-order combinations and only briefly mentions potential for second-order integration.
- What evidence would resolve it: Direct empirical comparison between SubZero and HiZOO on same tasks, measuring convergence speed and final performance.

### Open Question 4
- Question: What is the optimal subspace update frequency F for different model architectures and tasks?
- Basis in paper: [explicit] The paper shows SubZero is robust to rank variations but performance drops sharply with low update frequencies.
- Why unresolved: The paper only provides limited ablation study on update frequency without establishing systematic guidelines.
- What evidence would resolve it: Comprehensive study mapping update frequency to model size, task complexity, and training stage, with recommendations for different scenarios.

## Limitations

- Theoretical bounds vs practical performance: The theoretical convergence guarantees may not directly translate to observed accuracy improvements in practice.
- Task generalization: All experiments focus on English language understanding tasks, with no evidence for performance on code generation, multilingual tasks, or specialized domains.
- Memory measurement granularity: Memory usage comparisons lack detailed breakdowns across different model sizes and fine-tuning schemes.

## Confidence

**High confidence** in the core zeroth-order optimization framework and general superiority of SubZero over MeZO based on controlled experiments with standardized benchmarks.

**Medium confidence** in the layer-wise low-rank perturbation mechanism and lazy update strategy, as these design choices show consistent performance improvements but lack ablation studies proving their individual contributions.

**Low confidence** in the theoretical variance reduction claims, as the proof relies on assumptions about gradient subspace structure that are not empirically validated across diverse models and tasks.

## Next Checks

1. **Gradient subspace structure validation:** Measure the singular value spectrum of gradient matrices during fine-tuning to empirically verify that gradients indeed lie in low-dimensional subspaces, as assumed by the theoretical analysis.

2. **Cross-domain robustness test:** Evaluate SubZero on non-SuperGLUE tasks including code generation (HumanEval), multilingual understanding, and specialized domains to test generalization beyond the reported English benchmarks.

3. **Memory usage benchmarking:** Conduct detailed memory profiling comparing SubZero with backpropagation fine-tuning using gradient checkpointing across different model sizes (1B, 7B, 13B parameters) and fine-tuning schemes.