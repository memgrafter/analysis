---
ver: rpa2
title: 'GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local
  Refinements'
arxiv_id: '2402.10963'
source_url: https://arxiv.org/abs/2402.10963
tags:
- refinement
- sorm
- local
- data
- when
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving large language model
  (LLM) reasoning on math problems by developing a system that decides when, where,
  and how to refine a solution. The core method uses an Outcome-based Reward Model
  (ORM) to decide when to refine (if the draft answer is incorrect), a Stepwise ORM
  (SORM) to identify where to refine (the first incorrect reasoning step), and both
  global and local refinement models to actually correct the solution.
---

# GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements

## Quick Facts
- arXiv ID: 2402.10963
- Source URL: https://arxiv.org/abs/2402.10963
- Reference count: 25
- Primary result: Combining global and local refinements with ORM reranking improves LLaMA-2 13B GSM8K accuracy from 53% to 65% with greedy sampling

## Executive Summary
This paper addresses the challenge of improving LLM reasoning on math problems by decomposing refinement into three components: when to refine (using an Outcome-based Reward Model), where to refine (using a Stepwise ORM trained to approximate the optimal value function), and how to refine (via global and local refinement models). The authors find that combining global refinements (restarting from scratch) with local refinements (fixing specific errors) yields significant accuracy gains over either approach alone, with the ORM serving as an effective reranker to select the best solution.

## Method Summary
The method involves fine-tuning a base model using expert iteration to create a student model, then training ORMs and SORMs on student model outputs. ORMs predict final answer correctness while SORMs predict intermediate step correctness via rejection sampling. Global and local refinement models are trained to correct incorrect solutions, with global refinements starting from scratch and local refinements fixing specific errors. At test time, the system samples draft solutions, applies both refinement types, and uses the ORM to rerank candidates.

## Key Results
- Combining global and local refinements with ORM reranking improves LLaMA-2 13B GSM8K accuracy from 53% to 65% (greedy)
- SORMs outperform ORMs at detecting intermediate reasoning errors but are slightly worse at final answer prediction
- Global and local refinements solve partially disjoint, complementary sets of problems
- The SORM is trained to approximate V* rather than Vπ, improving intermediate step detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SORMs improve intermediate step correctness detection by approximating the optimal value function V* rather than the student policy value function Vπ.
- Mechanism: The SORM is trained to predict the correctness of the final answer when sampling the current policy many times from an intermediate step. This is equivalent to approximating V* via rejection sampling, which provides a lower bound of the optimal value function. This approximation is better at detecting incorrect reasoning steps because it reflects whether the step is logically valid, independent of the student model's limitations.
- Core assumption: Rejection sampling from the student policy can provide a reasonable approximation of the optimal policy π*, especially when the student model is already relatively strong.
- Evidence anchors:
  - [abstract]: "SORMs are trained, only on synthetic data, to approximate the expected future reward of the optimal policy or V*."
  - [section 4]: "In an effort to improve our ability to give intermediate step feedback, we introduce the Stepwise ORMs (SORMs) which explicitly predict labels at each step indicating the presence of an error."
  - [corpus]: "The SORM data generation process will suffer from both false positives and false negatives." (weak evidence, needs more investigation)
- Break condition: If the student policy is too weak, rejection sampling may not provide a good approximation of π*, leading to inaccurate SORM predictions.

### Mechanism 2
- Claim: Combining global and local refinements solves partially disjoint, complementary sets of problems, leading to improved overall accuracy.
- Mechanism: Global refinements can solve problems by starting from scratch, avoiding low-value solution paths that the student model struggles with. Local refinements can fix specific errors in the solution trace, taking advantage of valid reasoning in the prefix. By reranking both types of refinements using the ORM, the system can select the most promising solution, expanding the set of problems that can be solved.
- Core assumption: Global and local refinements have different strengths and weaknesses, and their combined use can solve a larger set of problems than either alone.
- Evidence anchors:
  - [abstract]: "We find combining global and local refinements, using the ORM as a reranker, significantly outperforms either one individually."
  - [section 5.2]: "Global and local refinements solve partially disjoint, complementary sets of problems: To better understand how global and local refinements compare we examine the overlap between the problems they correctly solve."
  - [corpus]: "The SORM data generation process will suffer from both false positives and false negatives." (weak evidence, needs more investigation)
- Break condition: If the ORM reranker is not accurate enough, it may not effectively select the best refinement, limiting the benefits of combining global and local refinements.

### Mechanism 3
- Claim: The ORM is better at predicting final answer correctness than the SORM, making it a more effective reranker for selecting the best solution among multiple candidates.
- Mechanism: The ORM is trained to predict the correctness of the final answer, and it can exploit statistical biases in the data to improve its accuracy. The SORM, on the other hand, is trained to predict the correctness of intermediate steps, which may lead to less accurate final answer predictions. By using the ORM as a reranker, the system can leverage its superior final answer prediction ability to select the best solution.
- Core assumption: The ORM's training objective (predicting final answer correctness) is more aligned with the goal of selecting the best solution than the SORM's training objective (predicting intermediate step correctness).
- Evidence anchors:
  - [abstract]: "However, when used to indicate where to refine, we find that ORMs tend to be overly-pessimistic when used to assess intermediate reasoning steps, resulting in excessive refinement of valid solutions."
  - [section 5.1]: "ORMs are better than SORMs at evaluating final answers: Despite the SORM being generally better at predicting intermediate steps, it is slightly worse at predicting final answer correctness compared to the ORM."
  - [corpus]: "The SORM data generation process will suffer from both false positives and false negatives." (weak evidence, needs more investigation)
- Break condition: If the SORM's intermediate step predictions are significantly more accurate than the ORM's, the SORM may become a better reranker despite its weaker final answer prediction ability.

## Foundational Learning

- Concept: Reinforcement Learning (RL) and Value Functions
  - Why needed here: The paper uses RL concepts, such as value functions and expert iteration, to train the student model and generate training data for the ORM and SORM. Understanding these concepts is crucial for grasping the paper's methodology and results.
  - Quick check question: What is the difference between the value function of a policy Vπ and the optimal value function V*?

- Concept: Process-based vs. Outcome-based Reward Models
  - Why needed here: The paper compares and contrasts process-based reward models (PRMs) and outcome-based reward models (ORMs), and introduces a new type of reward model called SORMs. Understanding the differences between these models is essential for understanding the paper's contributions and results.
  - Quick check question: How do PRMs, ORMs, and SORMs differ in terms of what they predict and how they are trained?

- Concept: Chain of Thought (CoT) Reasoning
  - Why needed here: The paper focuses on improving LLM reasoning on math problems, which often involves generating a chain of thought (CoT) to justify the final answer. Understanding CoT reasoning is important for understanding the paper's context and the challenges it addresses.
  - Quick check question: What is chain of thought (CoT) reasoning, and why is it useful for solving math problems?

## Architecture Onboarding

- Component map:
  Base model -> Student model (RL fine-tuned) -> ORM (final answer correctness) -> SORM (intermediate step correctness) -> Global refinement model -> Local refinement model -> ORM reranker

- Critical path:
  1. Fine-tune base model using expert iteration to produce student model
  2. Generate ORM training data by sampling student model on reasoning tasks
  3. Generate SORM training data by rejection sampling from student model outputs
  4. Train ORM and SORM on their respective training data
  5. Generate refinement training data by pairing incorrect and correct solutions
  6. Train global and local refinement models on refinement training data
  7. At test time, sample draft solutions from student model
  8. Apply global and local refinements to draft solutions
  9. Use ORM to rerank draft and refined solutions
  10. Select the best solution as the final answer

- Design tradeoffs:
  - Global vs. local refinements: Global refinements can solve problems from scratch but may struggle with specific errors, while local refinements can fix specific errors but may struggle with low-value solution paths.
  - ORM vs. SORM: ORM is better at predicting final answer correctness, while SORM is better at detecting incorrect reasoning steps.
  - Rejection sampling vs. human annotations: Rejection sampling is cheaper but may introduce noise in the training data, while human annotations are more expensive but may be more accurate.

- Failure signatures:
  - Low accuracy on intermediate steps: May indicate that the SORM is not accurately approximating V* or that the rejection sampling process is introducing noise.
  - Low accuracy on final answers: May indicate that the ORM is not effectively exploiting statistical biases or that the student model is not strong enough.
  - Poor refinement performance: May indicate that the refinement models are not effectively learning when and where to refine or that the training data is not representative of the test data.

- First 3 experiments:
  1. Train a student model on GSM8K using expert iteration and evaluate its accuracy on the test set.
  2. Train an ORM and SORM on the student model outputs and evaluate their accuracy on intermediate steps and final answers.
  3. Train global and local refinement models on the refinement training data and evaluate their performance on incorrect drafts from the test set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How much does the quality of the data generating student model (π) impact the performance of both ORMs and SORMs in identifying intermediate reasoning errors?
- Basis in paper: [explicit] The paper discusses how ORM performance on intermediate steps depends on the accuracy of the underlying student model π, with larger and more capable students providing better approximations to the optimal policy π* and thus better intermediate step evaluation. It notes the 13B model performs better than 7B on SVAMP but slightly worse on GSM8K despite being larger.
- Why unresolved: The paper provides some comparative results between 7B and 13B models on different benchmarks, but doesn't systematically explore how varying student model quality affects ORM/SORM performance across a range of reasoning tasks with different difficulty levels.
- What evidence would resolve it: A systematic study varying student model sizes and training methods (SFT vs EI) across multiple reasoning benchmarks with different difficulty levels, measuring both intermediate step accuracy and final answer accuracy for both ORM and SORM.

### Open Question 2
- Question: What is the optimal balance between using global refinement (restarting from scratch) versus local refinement (correcting specific errors) for different types of reasoning errors?
- Basis in paper: [explicit] The paper finds that global and local refinements solve partially disjoint, complementary sets of problems, with each type able to solve problems the other cannot. It shows that combining both via ORM reranking yields the best results.
- Why unresolved: While the paper demonstrates the complementarity of global and local refinements, it doesn't provide a principled way to determine when to use each approach or what characteristics of reasoning errors make one approach more suitable than the other.
- What evidence would resolve it: An analysis categorizing different types of reasoning errors (e.g., conceptual misunderstandings vs. arithmetic mistakes vs. incorrect strategy choices) and determining which refinement approach is most effective for each category, potentially leading to a learned decision mechanism.

### Open Question 3
- Question: How can the SORM data generation process be made more reliable to reduce false positives and false negatives in step-level verification?
- Basis in paper: [explicit] The paper acknowledges that the SORM data generation process suffers from both false positives (when student solves problem incorrectly but gets right answer) and false negatives (when rejection sampling fails to find correct solution despite valid prefix). It mentions post-processing steps like consistency constraints and balancing positive/negative labels.
- Why unresolved: The paper implements some post-processing steps but doesn't explore more sophisticated methods for improving data quality, such as using verification chains, incorporating tool usage, or developing better rejection sampling strategies.
- What evidence would resolve it: Experimental comparisons of different data quality improvement methods (e.g., verification chains, tool-assisted verification, alternative rejection sampling techniques) measuring their impact on SORM accuracy and downstream refinement performance.

## Limitations
- The effectiveness of SORMs depends on the strength of the student model, as weak models may not provide good approximations of V*.
- The SORM data generation process may introduce noise through false positives and false negatives in step-level verification.
- The benefits of combining global and local refinements rely on the ORM reranker being accurate enough to select the best solution.

## Confidence
- **High confidence**: The overall framework of decomposing refinement into when, where, and how components is well-established and the results show consistent improvements over baselines.
- **Medium confidence**: The SORM methodology and its approximation of V* through rejection sampling is theoretically sound but may have practical limitations depending on student model strength.
- **Medium confidence**: The claim that global and local refinements solve complementary problem sets is supported by empirical evidence but may be dataset-dependent.

## Next Checks
1. **Student model strength validation**: Systematically evaluate SORM performance across student models of varying quality to quantify how the approximation of V* degrades as model strength decreases.

2. **Ablation on rejection sampling parameters**: Test different values of K (samples per step) in the SORM training data generation to determine the sensitivity of SORM accuracy to the number of rejection samples.

3. **Cross-dataset generalization**: Evaluate the combined global+local refinement approach on math datasets with different characteristics (e.g., AQuA, MATH) to verify that the complementary nature of global and local refinements generalizes beyond GSM8K/SVAMP.