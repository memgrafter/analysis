---
ver: rpa2
title: 'Learning High-Degree Parities: The Crucial Role of the Initialization'
arxiv_id: '2412.04910'
source_url: https://arxiv.org/abs/2412.04910
tags:
- initialization
- where
- relu
- learning
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether gradient descent on standard neural
  networks can learn high-degree parities on uniform inputs, focusing on almost-full
  parities where the degree is close to the input dimension. Previous work showed
  that constant-degree parities are learnable, but dense parities with unbounded degree
  were challenging.
---

# Learning High-Degree Parities: The Crucial Role of the Initialization

## Quick Facts
- **arXiv ID**: 2412.04910
- **Source URL**: https://arxiv.org/abs/2412.04910
- **Reference count**: 40
- **Primary result**: Gradient descent can learn almost-full parity functions with Rademacher initialization but fails with Gaussian or large-σ perturbed initialization due to poor initial gradient alignment.

## Executive Summary
This paper investigates whether gradient descent on neural networks can learn high-degree parity functions on uniform binary inputs. The authors establish a sharp dichotomy: two-layer ReLU networks with Rademacher initialization can efficiently learn almost-full parities (degree d-1 where d is input dimension), while the same networks with Gaussian or large-σ perturbed initialization fail to learn beyond random guessing. The key insight is that learnability depends critically on "initial gradient alignment" - a novel measure that characterizes how well gradients at initialization point in the correct learning direction. This result provides a separation between statistical query algorithms (where parity is trivially learnable) and gradient descent methods, highlighting the special role of Rademacher initialization in enabling learning of highly structured functions.

## Method Summary
The authors study learning high-degree parity functions using two-layer fully connected ReLU networks trained with noisy stochastic gradient descent. They consider three initialization schemes: Rademacher (entries ±1 with equal probability), Gaussian (entries from normal distribution), and σ-perturbed Rademacher (Rademacher entries perturbed by Gaussian noise with variance σ²). The networks are trained on almost-full parity functions (where output is the product of all but one input) and full parity functions using correlation loss or hinge loss. The key analytical tool is "initial gradient alignment" (GAL), which measures the alignment between the initial gradient and the direction of decreasing loss. The authors prove that for Rademacher initialization, GAL is not super-polynomially small, enabling efficient learning, while for Gaussian or large-σ perturbed initialization, GAL becomes super-polynomially small, preventing learning.

## Key Results
- Two-layer ReLU networks with Rademacher initialization can learn almost-full parity functions with perfect accuracy in polynomial time
- Gaussian initialization or large-σ perturbed initialization causes initial gradient alignment to become super-polynomially small, preventing learning beyond random guessing
- The initial gradient alignment measure (GAL) successfully predicts learnability based on initialization scheme
- A sharp dichotomy exists between statistical query algorithms (where parity is learnable) and gradient descent methods (where learnability depends on initialization)

## Why This Works (Mechanism)
The success of Rademacher initialization stems from its ability to maintain sufficient gradient alignment with the target function from the start of training. For almost-full parities, the gradient at initialization needs to point in a direction that reduces the loss effectively. Rademacher initialization creates a balanced initialization where each neuron has a chance to contribute meaningfully to the output, maintaining GAL above a polynomial threshold. In contrast, Gaussian initialization or large-σ perturbations cause the network to start in regions where gradients point almost orthogonally to the learning direction, making progress impossible regardless of the number of training steps. This mechanism explains why statistical query algorithms can learn parity (they can query the target function directly) while gradient descent requires carefully chosen initialization to succeed.

## Foundational Learning
- **Gradient descent optimization**: Essential for understanding how the network updates weights during training and why initialization matters for the trajectory. Quick check: Verify that gradient descent converges for convex problems.
- **Parity functions and computational complexity**: Provides context for why parity functions are challenging - they're not linearly separable and require exponentially many samples for agnostic learning. Quick check: Confirm XOR function requires non-linear decision boundary.
- **Rademacher complexity and generalization**: Helps understand the theoretical bounds on learnability and why discrete initialization differs from continuous. Quick check: Compare Rademacher complexity bounds for discrete vs continuous distributions.
- **Statistical query learning vs gradient-based learning**: Crucial for understanding the separation result - SQ algorithms can learn parity trivially while gradient descent's success depends on initialization. Quick check: Verify SQ algorithms can learn parity in polynomial time.

## Architecture Onboarding

**Component Map**
Two-layer ReLU network -> Noisy SGD optimizer -> Correlation/hinge loss -> Parity target function

**Critical Path**
Initialization (Rademacher/Gaussian/perturbed) -> Initial gradient computation -> Gradient alignment measure (GAL) -> Learning success/failure

**Design Tradeoffs**
The choice between initialization schemes represents a fundamental tradeoff between computational tractability and expressivity. Rademacher initialization sacrifices the nice statistical properties of Gaussian initialization (like concentration bounds) but gains the crucial property of maintaining gradient alignment for structured functions like parity.

**Failure Signatures**
- Super-polynomially small initial gradient alignment (GAL)
- Gradient descent trajectory remains in regions where loss decreases sub-optimally
- Network parameters oscillate without converging to low-loss regions

**First Experiments**
1. Implement a two-layer ReLU network with Rademacher initialization and train on full parity for d=20; verify perfect accuracy is achieved
2. Replace with Gaussian initialization and verify the network fails to learn beyond random guessing
3. Implement σ-perturbed Rademacher initialization with σ=0.1 and test whether learning succeeds or fails

## Open Questions the Paper Calls Out

**Open Question 1**: Is there a sharp threshold value of σ where learning transitions from possible to impossible for almost-full parities? The paper establishes that σ = O(d^-1) preserves learnability while σ = Ω(1) prevents it, but the exact critical threshold σ* remains unknown.

**Open Question 2**: Does the initial gradient alignment measure extend to other architectures beyond two-layer ReLU networks? The authors prove results specifically for two-layer ReLU networks and acknowledge limitations when extending to deeper networks or different activation functions.

**Open Question 3**: Can the positive result for Rademacher initialization be extended to other discrete or continuous perturbations? The experiments show that sparse Rademacher, uniform mixtures, and other discrete initializations fail to enable learning, suggesting Rademacher is a special case.

## Limitations
- The perturbation threshold phenomenon is incompletely characterized - exact location of σ* threshold is unknown
- Experimental validation is minimal, relying primarily on theoretical arguments
- Results are specific to correlation/hinge loss settings and two-layer ReLU networks
- Polynomial bounds provided may be loose, potentially hiding more efficient learning regimes

## Confidence

**High confidence**: The Rademacher initialization learnability proof and the Gaussian initialization hardness results are rigorously established with mathematical proofs.

**Medium confidence**: The existence and nature of the σ* threshold phenomenon is theoretically plausible but lacks experimental verification across the full range of σ values.

**Medium confidence**: The general applicability of initial gradient alignment as a predictive measure for learnability is demonstrated for two-layer ReLU networks but remains to be validated for other architectures.

## Next Checks

1. Empirically verify the learning behavior across Rademacher, Gaussian, and σ-perturbed initializations for varying σ values to identify the threshold phenomenon
2. Test whether initial gradient alignment accurately predicts learnability across different network widths and depths
3. Evaluate the robustness of these results to different loss functions beyond correlation and hinge loss