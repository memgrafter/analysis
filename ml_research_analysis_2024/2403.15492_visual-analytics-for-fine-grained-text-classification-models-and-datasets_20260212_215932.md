---
ver: rpa2
title: Visual Analytics for Fine-grained Text Classification Models and Datasets
arxiv_id: '2403.15492'
source_url: https://arxiv.org/abs/2403.15492
tags:
- system
- which
- labels
- text
- fine-grained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SemLa, a novel visual analytics system designed
  to address the challenges of fine-grained text classification in natural language
  processing. SemLa enables hierarchical understanding of model reasoning, fine-grained
  explanation of individual predictions, and revealing model weaknesses through an
  interactive spatialization approach.
---

# Visual Analytics for Fine-grained Text Classification Models and Datasets

## Quick Facts
- arXiv ID: 2403.15492
- Source URL: https://arxiv.org/abs/2403.15492
- Reference count: 28
- Primary result: SemLa is a novel visual analytics system for understanding fine-grained text classification models through hierarchical semantic visualization, example-based explanations, and model debugging capabilities.

## Executive Summary
SemLa is a visual analytics system designed to address the challenges of understanding and debugging fine-grained text classification models in natural language processing. The system enables hierarchical understanding of model reasoning, fine-grained explanation of individual predictions, and revealing model weaknesses through an interactive spatialization approach. By integrating multiple feature importance metrics and employing example-based contrastive explanations, SemLa makes label semantics explicit and supports various model development workflow tasks including model validation, debugging, and data annotation.

## Method Summary
SemLa implements a BERT-base model fine-tuned on fine-grained classification datasets using metric-based representation learning. The system computes sample embeddings, applies dimension reduction for 2D spatialization, and implements the Localized Word Clouds (LWC) algorithm for pattern extraction. It integrates multiple feature importance metrics through Visually Integrated Feature Importance (VIFI) and builds relation graphs for contrastive explanations. The system provides interactive exploration through Map, List, Sample-level, and Label-level views to support comprehensive model analysis.

## Key Results
- Expert feedback and case studies demonstrate SemLa's effectiveness in uncovering model biases and identifying root causes of classification errors
- The system successfully reveals complex semantic structures in fine-grained datasets through spatialization in model embedding space
- Users found SemLa valuable for streamlining model validation, debugging, and data annotation tasks in NLP workflows

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The spatialization of fine-grained text samples into a 2D embedding space enables intuitive identification of hierarchical semantic structures.
- Mechanism: By projecting high-dimensional model embeddings into 2D via dimension reduction, semantically similar samples cluster together, making inter-label relationships visually discernible without explicit clustering assumptions.
- Core assumption: The model's embedding space preserves semantic similarity relevant to the fine-grained classification task.
- Evidence anchors: [abstract] "dissecting complex semantic structures in a dataset when it is spatialized in model embedding space"; [section] "Our final system supports the streamlining of various tasks in model development workflow... showing discrepancies between ground-truth data distribution and what the model has learned"

### Mechanism 2
- Claim: Localized Word Clouds (LWC) reveal fine-grained lexical patterns without relying on arbitrary clustering assumptions.
- Mechanism: LWC identifies words that are spatially localized in the embedding space by computing word locality and filtering by frequency and locality size, avoiding the need for clustering hyperparameters.
- Core assumption: Words that appear in the same local region of embedding space share semantic relevance to that region's fine-grained label distinctions.
- Evidence anchors: [section] "we propose an alternative approach based on our simple and fast Localized Word Clouds (LWC) algorithm... which extracts patterns directly from the model embedding space in a bottom-up manner without relying on clustering"; [corpus] Weak: No direct corpus evidence showing LWC outperforms clustering-based methods, but this is addressed as a limitation in the design.

### Mechanism 3
- Claim: Example-based contrastive explanations with token-to-similarity graphs make fine-grained label semantics explicit by showing how tokens contribute to similarity between similar labels.
- Mechanism: By comparing a query sample to its closest same-label sample and a closest different-label sample, and visualizing token contributions to computed similarities, the system reveals distinguishing features that coarse-grained feature importance misses.
- Core assumption: The similarity metrics used accurately reflect the model's reasoning about label distinctions at the fine-grained level.
- Evidence anchors: [abstract] "visualizing fine-grained nuances in the meaning of text samples to faithfully explain model reasoning"; [section] "To achieve fine-grained explanations that make the label semantics explicit... we adopt a novel approach that combines example-based explanations with contrastive explanations"

## Foundational Learning

- Concept: High-dimensional embeddings and dimension reduction techniques
  - Why needed here: Understanding how model embeddings capture semantic relationships and how projecting to 2D preserves this structure is fundamental to interpreting the Map view.
  - Quick check question: If two samples are close in the model's embedding space, what does that imply about their semantic relationship in the context of fine-grained classification?

- Concept: Feature importance metrics and their limitations
  - Why needed here: Recognizing why multiple feature importance metrics are integrated (VIFI) and why they alone are insufficient for fine-grained explanations requires understanding their coarse-grained nature.
  - Quick check question: What is the key limitation of standard feature importance methods when applied to distinguishing between very similar labels?

- Concept: Contrastive explanations and their role in fine-grained classification
  - Why needed here: Understanding why contrastive explanations (rather than just "why this label?") are essential for distinguishing between similar labels in fine-grained tasks.
  - Quick check question: Why is answering "Why label A rather than label B?" more informative than just explaining why label A was chosen for a sample?

## Architecture Onboarding

- Component map:
  - Map View -> List View -> Sample-level View -> Label-level View
  - Backend: Pre-trained BERT model fine-tuned on classification dataset; supports multiple feature importance metrics and similarity calculations

- Critical path:
  1. Load dataset and pre-trained/fine-tuned model
  2. Compute sample embeddings using the model
  3. Apply dimension reduction to project embeddings to 2D for Map view
  4. Implement LWC algorithm for local word extraction
  5. Integrate multiple feature importance metrics for VIFI
  6. Build relation graph visualizations for contrastive explanations
  7. Develop comparison mode for multi-group analysis

- Design tradeoffs:
  - Spatialization vs. clustering: Spatialization provides continuous view of relationships but can suffer from projection distortion; clustering would be more discrete but requires arbitrary parameter choices
  - Multiple metrics vs. single metric: Multiple metrics provide richer perspective but increase cognitive load; single metric is simpler but may miss important aspects
  - Example-based vs. abstract explanation: Example-based explanations are more concrete and intuitive but may not generalize; abstract explanations are more general but can be harder to interpret

- Failure signatures:
  - Map view shows random scattering with no discernible clusters: Embedding space may not capture semantic similarity or dimension reduction failed
  - LWC shows common words everywhere with no localization: Word locality in embedding space doesn't correlate with semantic relevance
  - Relation graphs show similar contributions for all tokens: Similarity metrics may not be capturing the fine-grained distinctions the model uses
  - Confusion table shows uniform confusion across all label pairs: Model may be performing at chance level or dataset may be too ambiguous

- First 3 experiments:
  1. Load a small fine-grained dataset (like BANKING77 with subset of labels) and verify the Map view correctly shows sample spatialization and basic interactions work
  2. Test LWC with a known localized word (e.g., a word that only appears in samples of one specific label) to confirm it appears in the correct region
  3. Select a sample with a clear error, verify the relation graphs show the distinguishing tokens between the correct and predicted labels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal way to evaluate the faithfulness of explanations in fine-grained text classification models, given the current lack of consensus on evaluation methods?
- Basis in paper: [explicit] The paper discusses the challenge of evaluating explanations, mentioning multiple metrics like perturbing/erasing important words and guessing back model predictions, but notes there is no consensus on which method is best.
- Why unresolved: The field has not reached a consensus on which evaluation method provides the most reliable measure of explanation faithfulness, making it difficult to determine which explanation methods can be fully trusted.
- What evidence would resolve it: A comprehensive comparative study demonstrating the reliability and effectiveness of different faithfulness evaluation metrics across multiple fine-grained text classification tasks and models.

### Open Question 2
- Question: How can the system be further optimized for production environments while maintaining its analytical capabilities and user-friendliness?
- Basis in paper: [explicit] The experts suggested that future enhancements should focus on operationalizing the system for production, including guiding users through common tasks with documentation and tutorials.
- Why unresolved: The paper identifies the need for production-ready features but does not provide specific implementation details or evaluate how such features would impact the system's usability and effectiveness.
- What evidence would resolve it: Implementation and user testing of production-ready features in real-world deployment scenarios, measuring both user satisfaction and system performance.

### Open Question 3
- Question: What are the most effective methods for visualizing and explaining multi-modal input processing in deep learning models, extending beyond text classification?
- Basis in paper: [explicit] The authors mention the potential to extend their approach to other domains like image processing and multi-modal input processing.
- Why unresolved: While the paper suggests this extension is possible, it does not provide specific methodologies or evaluate how the current techniques would translate to multi-modal scenarios.
- What evidence would resolve it: Development and evaluation of the system's visualization techniques on multi-modal datasets, demonstrating their effectiveness in explaining model reasoning across different input types.

## Limitations

- The evaluation relies primarily on expert feedback and case studies rather than controlled user studies or quantitative performance metrics
- The effectiveness of Localized Word Clouds depends on the assumption that word locality in embedding space correlates with semantic relevance, which may not always hold
- Spatialization can suffer from projection distortion that may obscure or misrepresent true semantic relationships

## Confidence

- **High Confidence**: The system architecture and technical implementation details are well-specified, including the Map view spatialization, LWC algorithm, and example-based contrastive explanations
- **Medium Confidence**: The utility claims based on expert feedback are supported by qualitative descriptions of how SemLa helped identify biases, debug errors, and explore semantic structures
- **Low Confidence**: The claim that SemLa provides "fine-grained explanations that make label semantics explicit" through contrastive explanations is based on a novel approach that requires further empirical validation

## Next Checks

1. **Quantitative User Study**: Conduct a controlled experiment comparing SemLa against baseline tools (e.g., standard feature importance visualizations) where users must identify specific model weaknesses or debug classification errors, measuring accuracy and time-to-completion

2. **LWC Algorithm Validation**: Create synthetic datasets where certain words are artificially localized to specific label regions, then verify that LWC correctly identifies these words in their corresponding spatial regions while filtering out non-localized common words

3. **Embedding Space Fidelity Test**: Systematically compare the 2D spatialization against the original high-dimensional embedding space by measuring preservation of pairwise distances and cluster structures, identifying cases where projection distortion may mislead interpretation