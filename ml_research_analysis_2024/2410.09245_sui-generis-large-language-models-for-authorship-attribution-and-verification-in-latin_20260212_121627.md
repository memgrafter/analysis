---
ver: rpa2
title: 'Sui Generis: Large Language Models for Authorship Attribution and Verification
  in Latin'
arxiv_id: '2410.09245'
source_url: https://arxiv.org/abs/2410.09245
tags:
- authorship
- verification
- attribution
- texts
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study explores the performance of large language models (LLMs)\
  \ in authorship verification and attribution tasks for Latin texts from the Patristic\
  \ Era. The authors test four major LLMs\u2014GPT-4o, Claude, Mistral, and Gemini\u2014\
  on authorship-related tasks using direct prompting in zero-shot settings."
---

# Sui Generis: Large Language Models for Authorship Attribution and Verification in Latin

## Quick Facts
- **arXiv ID**: 2410.09245
- **Source URL**: https://arxiv.org/abs/2410.09245
- **Reference count**: 11
- **Primary result**: GPT-4o and Claude-3.5 outperform traditional baselines in authorship verification for Latin texts

## Executive Summary
This study evaluates four major large language models (GPT-4o, Claude-3.5, Mistral, and Gemini) on authorship attribution and verification tasks using Latin texts from the Patristic Era. The authors test the models in zero-shot settings with direct prompting across three different guidance levels. The experiments reveal that GPT-4o and Claude-3.5 demonstrate robust performance in authorship verification, outperforming traditional baselines including classical machine learning classifiers and the pre-trained Latin transformer LaBerta. Interestingly, explicit philological and historical instructions often deteriorate performance, suggesting that LLMs perform better when given fewer constraints. The findings indicate that while LLMs can handle complex linguistic tasks in low-resource languages like Latin, their interpretability and adaptability to domain-specific nuances require further improvement.

## Method Summary
The study evaluates four LLMs (GPT-4o, Claude-3.5, Mistral, and Gemini) on authorship attribution and verification tasks using zero-shot prompting with three guidance levels (BASE, LIMITED, HIP). The evaluation uses Latin texts from the Patristic Era, split into 250-500 word chunks, with authors limited to 10 for manageable dataset size. Traditional baselines including TF-IDF with Random Forest and a LaBerta-based model provide comparison points. Performance is measured using accuracy, precision, recall, and F1 scores across different prompt settings.

## Key Results
- GPT-4o and Claude-3.5 significantly outperform traditional baselines in authorship verification accuracy
- Explicit philological and historical instructions consistently deteriorate model performance
- Models struggle to distinguish writing style from semantic content, often being "misled" by semantics
- Zero-shot prompting achieves competitive results without task-specific training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: GPT-4o and Claude-3.5 outperform traditional baselines in authorship verification because they can capture complex linguistic patterns without requiring manual feature engineering.
- **Mechanism**: Large language models leverage their pre-training on multilingual data to recognize subtle stylistic cues in Latin texts, achieving robust zero-shot performance.
- **Core assumption**: The linguistic "knowledge" acquired during pre-training includes relevant stylistic features for Latin authorship analysis.
- **Evidence anchors**:
  - [abstract]: "GPT-4o and Claude-3.5 demonstrate robust performance in authorship verification, outperforming baselines in terms of accuracy and precision"
  - [section 2]: "The linguistic 'knowledge' of LLMs, acquired through training on extensive multilingual textual datasets, along with their advanced inference capabilities"
  - [corpus]: Weak evidence - the corpus neighbors are not directly related to this mechanism
- **Break condition**: If the stylistic patterns in Patristic Latin differ significantly from the pre-training data distribution, or if the texts are too short to reveal meaningful patterns.

### Mechanism 2
- **Claim**: Explicit philological and historical instructions often deteriorate performance because they constrain the models' ability to leverage their intrinsic knowledge.
- **Mechanism**: When given strict prompts, models tend to follow them closely but may overinterpret features, limiting their effectiveness in tasks that are resistant to formalization.
- **Core assumption**: LLMs perform better when allowed to apply their intrinsic knowledge more freely rather than being constrained by formal instructions.
- **Evidence anchors**:
  - [abstract]: "explicit philological and historical instructions often deteriorate performance, suggesting that LLMs perform better when given fewer constraints"
  - [section 6]: "explicit instructions regarding philological and historical features had a negative impact on performance"
  - [corpus]: Weak evidence - the corpus neighbors do not provide direct support for this mechanism
- **Break condition**: If the task requires specific domain knowledge that the models lack in their pre-training, or if the instructions are formulated in a way that complements rather than constrains the models' capabilities.

### Mechanism 3
- **Claim**: Semantic similarity influences models' decisions in authorship verification, making it difficult to disentangle style from content.
- **Mechanism**: LLMs struggle to distinguish writing style independently of subject matter, often relying on thematic similarities when making predictions.
- **Core assumption**: Writing style and semantics are inherently connected, but in authorship analysis, the challenge lies in discerning style independently of content.
- **Evidence anchors**:
  - [abstract]: "The models can also be easily 'mislead' by semantics"
  - [section 6.1]: "the Pearson correlation coefficient between the cosine similarity of the prompted texts and the correctness of the model's answers"
  - [corpus]: Weak evidence - the corpus neighbors do not provide direct support for this mechanism
- **Break condition**: If the texts are sufficiently diverse in content but consistent in style, or if explicit instructions to ignore content are effectively followed by the models.

## Foundational Learning

- **Concept**: Zero-shot learning
  - **Why needed here**: The study evaluates LLMs' ability to perform authorship tasks without task-specific training
  - **Quick check question**: What distinguishes zero-shot from few-shot or fine-tuning approaches in LLM applications?

- **Concept**: Stylometry
  - **Why needed here**: Understanding the traditional methods and features used in authorship analysis provides context for evaluating LLM performance
  - **Quick check question**: What are the key stylistic features traditionally used in authorship attribution, and how might they differ from features LLMs might identify?

- **Concept**: Latin patristic literature
  - **Why needed here**: The study focuses on Latin texts from the Patristic Era, which have specific linguistic and thematic characteristics
  - **Quick check question**: What are the defining features of patristic Latin that might influence authorship analysis, such as biblical quotations or rhetorical figures?

## Architecture Onboarding

- **Component map**: Dataset of Latin texts (250-500 word chunks) → Four LLMs (GPT-4o, Claude-3.5, Mistral, Gemini) via LangChain API → Three prompt settings (BASE, LIMITED, HIP) → Performance metrics (accuracy, precision, recall, F1)
- **Critical path**: For authorship verification - prompt text pairs to models → receive predictions → compare against ground truth → calculate metrics. For authorship attribution - prompt query text and candidate texts → receive author predictions → compare against ground truth → calculate metrics.
- **Design tradeoffs**: Using zero-shot prompting avoids task-specific training but may limit performance compared to fine-tuning. The choice of prompt structure (BASE, LIMITED, HIP) affects model performance in non-obvious ways. Evaluating on historical Latin presents challenges due to limited data availability.
- **Failure signatures**: Models over-relying on semantic similarity rather than style, deterioration of performance with explicit instructions, significant performance differences between models on the same task.
- **First 3 experiments**:
  1. Run GPT-4o on authorship verification with BASE prompt setting to establish baseline performance
  2. Compare Claude-3.5 and GPT-4o on the same verification task to identify relative strengths
  3. Test different prompt settings (BASE, LIMITED, HIP) on a single model to observe the impact of guidance level

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the performance of large language models in authorship attribution and verification tasks for Latin texts compare to their performance on other low-resource historical languages?
- **Basis in paper**: [explicit] The study focuses on Latin texts and discusses the limited availability of Latin data compared to modern languages.
- **Why unresolved**: The paper only evaluates LLMs on Latin texts and does not compare their performance on other historical languages.
- **What evidence would resolve it**: Comparative studies of LLM performance on authorship tasks across multiple historical languages with varying resource levels.

### Open Question 2
- **Question**: What specific aspects of Latin text structure and vocabulary do large language models rely on when making authorship predictions?
- **Basis in paper**: [inferred] The paper mentions that LLMs struggle to distinguish between writing style and content, and that explicit instructions often deteriorate performance.
- **Why unresolved**: The paper does not provide a detailed analysis of which linguistic features LLMs prioritize in their decision-making process.
- **What evidence would resolve it**: In-depth feature importance analysis and attention visualization techniques applied to LLM predictions on Latin texts.

### Open Question 3
- **Question**: How does the performance of large language models in authorship attribution and verification tasks change when trained on domain-specific Latin corpora?
- **Basis in paper**: [explicit] The study uses zero-shot learning with publicly available LLMs not specifically trained on Latin authorship tasks.
- **Why unresolved**: The paper does not explore the potential benefits of fine-tuning LLMs on curated Latin authorship datasets.
- **What evidence would resolve it**: Experiments comparing zero-shot performance with fine-tuned LLM performance on authorship tasks using Latin-specific training data.

## Limitations
- Small dataset size (only 10 authors) may not represent the full diversity of Latin patristic writing styles
- Proprietary nature of source corpus (Corpus Christianorum Series Latina) prevents independent verification
- Zero-shot prompting approach may not fully leverage the potential of these models compared to fine-tuning
- Fundamental challenge of disentangling stylistic features from semantic content remains unresolved

## Confidence
- **High confidence**: The observation that GPT-4o and Claude-3.5 outperform traditional baselines in authorship verification is well-supported by the presented metrics and comparisons.
- **Medium confidence**: The finding that explicit instructions often deteriorate performance is supported by the data but may be context-dependent and require further exploration across different prompt formulations.
- **Low confidence**: The claim that LLMs can handle complex linguistic tasks in low-resource languages like Latin is promising but limited by the small sample size and specific nature of patristic Latin, which may not generalize to other low-resource languages.

## Next Checks
1. **Semantic independence test**: Create a controlled experiment with text pairs that have high semantic similarity but different authorship, and low semantic similarity but same authorship, to quantify how much semantic content influences the models' decisions versus stylistic features.

2. **Cross-linguistic generalization**: Apply the same methodology to another low-resource language (such as Ancient Greek or Old English) to determine whether the observed LLM advantages generalize beyond Latin and patristic texts.

3. **Fine-tuning comparison**: Implement a simple fine-tuning approach on a subset of the data and compare performance against zero-shot results to establish whether the convenience of zero-shot prompting comes at a significant accuracy cost.