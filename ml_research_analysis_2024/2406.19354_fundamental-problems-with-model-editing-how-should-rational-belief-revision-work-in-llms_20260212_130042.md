---
ver: rpa2
title: 'Fundamental Problems With Model Editing: How Should Rational Belief Revision
  Work in LLMs?'
arxiv_id: '2406.19354'
source_url: https://arxiv.org/abs/2406.19354
tags:
- editing
- beliefs
- llms
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper critically examines the conceptual foundations of model
  editing, highlighting 12 open challenges related to defining the problem, developing
  benchmarks, and assuming language models have editable beliefs. These challenges
  include issues like underspecification of desired behavior, difficulties in labeling
  probabilistic entailments, and ambiguity about whether LLMs possess coherent beliefs.
---

# Fundamental Problems With Model Editing: How Should Rational Belief Revision Work in LLMs?

## Quick Facts
- arXiv ID: 2406.19354
- Source URL: https://arxiv.org/abs/2406.19354
- Reference count: 28
- One-line primary result: Model edits fail to achieve rational belief revision, diverging significantly from exact Bayesian posteriors in probabilistic and logical coherence.

## Executive Summary
This paper critically examines the conceptual foundations of model editing, identifying 12 open challenges related to defining the problem, developing benchmarks, and understanding whether language models possess coherent beliefs that can be rationally revised. The authors introduce a semi-synthetic dataset based on Wikidata to create a controlled environment where exact Bayesian posterior probabilities can serve as gold standards for rational belief revision. Experiments with a 83M parameter Transformer model show that model edits fail to properly generalize, with edited models diverging significantly from Bayesian beliefs in both probabilistic and logical coherence, highlighting the need for more formal evaluation frameworks.

## Method Summary
The authors generate a semi-synthetic corpus from Wikidata relations with 100k true atomic facts and logical connectives, then train an 83M parameter Transformer language model on this corpus. They fit a Bayesian Categorical-Dirichlet model to the same data to obtain exact posterior probabilities. LoRA fine-tuning is applied to edit specific facts, and the resulting model is evaluated against the Bayesian gold standard using both probabilistic coherence (MAE between LM probabilities and Bayesian posteriors) and logical coherence (checking probability axioms for complex sentences). The controlled synthetic setting enables precise evaluation of whether model edits achieve rational generalization to downstream consequences.

## Key Results
- Model edits show poor generalization to downstream probabilistic consequences, with large gaps between LM probabilities and Bayesian posteriors.
- Edited models frequently violate basic logical coherence axioms, indicating internal inconsistency.
- Generative accuracy on atomic facts is high, but the model fails to capture rational belief revision principles.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The model editing problem inherits deep conceptual challenges from belief revision theory, making it inherently under-specified without clear rational standards for how beliefs should change.
- **Mechanism**: When new information contradicts existing beliefs, there is no universally accepted rule for which beliefs to discard while maintaining rational consistency. This manifests in model editing as ambiguity about what probabilities or logical consequences should follow from an edit.
- **Core assumption**: Language models can be treated as agents with beliefs that are subject to norms of rational belief revision.
- **Evidence anchors**:
  - [abstract] "model editing is essentially belief revision, a storied problem in philosophy that has eluded succinct solutions for decades"
  - [section] "There have been many attempts to specify standards for deciding which beliefs to give up, but no single accepted theory has arisen"
  - [corpus] Weak - corpus contains only factual statements without explicit belief revision scenarios
- **Break condition**: If LLMs are better modeled as databases or simulators rather than rational agents, the assumption that they have "editable beliefs" breaks down.

### Mechanism 2
- **Claim**: Evaluation of model edits requires exact Bayesian posteriors as gold standards, which can only be achieved in semi-synthetic settings with controlled dependencies.
- **Mechanism**: By constructing a formal language with known causal dependencies between facts, we can compute exact Bayesian posteriors that serve as rational targets for evaluating whether model edits generalize properly to downstream consequences.
- **Core assumption**: A Bayesian model fit to the same data as the language model can serve as a gold standard for rational belief revision.
- **Evidence anchors**:
  - [abstract] "We introduce a semi-synthetic dataset for model editing based on Wikidata, where we can evaluate edits against labels given by an idealized Bayesian agent"
  - [section] "Our Bayesian model responds to new edit requests, yielding posterior beliefs that we compare our language model against after model editing"
  - [corpus] Strong - corpus explicitly encodes upstream/downstream dependencies between relations
- **Break condition**: If real-world knowledge dependencies are too complex or unknown, this controlled synthetic approach may not generalize to practical model editing scenarios.

### Mechanism 3
- **Claim**: Language models express uncertainty through two channels (output probabilities and semantic text), and editing the wrong channel leads to misleading results about credence changes.
- **Mechanism**: Model editing typically targets output probabilities, but RLHF-trained models may express uncertainty primarily through generated text semantics. This creates a mismatch where probability edits don't align with semantic confidence expressions.
- **Core assumption**: LLMs have learned distinct mechanisms for expressing uncertainty through probabilities versus generated text.
- **Evidence anchors**:
  - [abstract] "Not Clear How To Edit Credences... LLMs can express uncertainty in language or in next-token probabilities. Which mechanism should be exploited during editing?"
  - [section] "RLHF encourages LLMs to express uncertainty through the semantics of generated text... with the consequence of reducing calibration of raw label probabilities"
  - [corpus] Weak - corpus contains only factual statements without uncertainty expressions
- **Break condition**: If uncertainty expression is primarily through a single channel or if the distinction between channels is not meaningful for the model, this mechanism breaks down.

## Foundational Learning

- **Concept**: Bayesian belief revision as the gold standard for rational belief updating
  - **Why needed here**: Provides the theoretical framework for what "rational" belief updating should look like, enabling evaluation of whether model edits achieve rational generalization
  - **Quick check question**: What is the key difference between AGM belief revision and Bayesian belief revision?

- **Concept**: Causal dependencies between facts and how they affect belief updating
  - **Why needed here**: Understanding how changing one fact should probabilistically affect related facts is central to evaluating whether model edits properly capture these dependencies
  - **Quick check question**: If we learn that someone graduated from MIT, how should this update our beliefs about their occupation?

- **Concept**: The distinction between agents, agent simulators, and databases in the context of LLMs
  - **Why needed here**: Determines whether it makes sense to talk about "editing beliefs" at all, and what kind of coherence we should expect from edited models
  - **Quick check question**: What is the key philosophical difference between treating an LLM as an agent versus an agent simulator?

## Architecture Onboarding

- **Component map**: Corpus generator → Pretraining dataset (204M tokens) → Language model (83M parameter Transformer) → LoRA fine-tuning module → Bayesian model (Categorical-Dirichlet) → Evaluation framework

- **Critical path**: 
  1. Generate semi-synthetic corpus with known dependencies
  2. Train language model on corpus
  3. Fit Bayesian model to same corpus
  4. Create test cases with edit requests and downstream consequences
  5. Apply LoRA editing to language model
  6. Compare edited model probabilities to Bayesian posteriors

- **Design tradeoffs**:
  - Synthetic vs. real data: Synthetic data enables exact Bayesian evaluation but may not capture real-world complexity
  - Model size: Smaller models are easier to train and edit but may lack capabilities of larger models
  - Dependency encoding: Explicitly encoding dependencies enables evaluation but requires manual specification

- **Failure signatures**:
  - Poor generative accuracy indicates failure to memorize basic facts
  - Large gaps between LM probabilities and Bayesian posteriors indicate poor generalization
  - Violations of logical coherence axioms indicate internal inconsistency

- **First 3 experiments**:
  1. Train language model on corpus and measure generative accuracy on memorized facts
  2. Apply LoRA editing to a simple edit request and measure change in probability for the target fact
  3. Apply LoRA editing and measure downstream probabilistic consequences compared to Bayesian model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How should the model editing problem be precisely defined for LLMs to ensure clear evaluation criteria?
- Basis in paper: [explicit] The paper discusses challenges in defining the model editing problem, including underspecification and unclear goals, emphasizing the need for a more precise problem definition.
- Why unresolved: The paper highlights that the current formulation of model editing lacks specificity, making it difficult to determine what constitutes successful model editing and how to measure it effectively.
- What evidence would resolve it: A formalized framework or set of criteria that clearly outlines the expected behavior of an LLM after a model edit, including specific benchmarks for evaluation.

### Open Question 2
- Question: What are the most effective methods for developing benchmarks that accurately measure the success of model editing in correcting errors?
- Basis in paper: [explicit] The paper identifies challenges in constructing datasets for model editing, such as labeling probabilistic entailments and targeting specific model errors, indicating a need for better benchmarking strategies.
- Why unresolved: Current benchmarks often focus on converting true outputs to false, which does not adequately address the correction of genuine errors in model beliefs, and the paper suggests that future benchmarks should focus on error correction.
- What evidence would resolve it: Development and validation of benchmark datasets that specifically target known errors in LLMs, along with metrics that assess the model's ability to correct these errors effectively.

### Open Question 3
- Question: How can we determine whether LLMs should be treated as agents with editable beliefs or as databases with static knowledge?
- Basis in paper: [explicit] The paper explores whether LLMs should be viewed as agents capable of maintaining coherent beliefs or as databases that store information, highlighting the ambiguity in how LLMs process and update knowledge.
- Why unresolved: The paper points out that the extent to which LLMs are optimized for truthfulness versus simply storing pretraining data is unclear, affecting how we approach model editing and belief revision.
- What evidence would resolve it: Empirical studies that assess the coherence and adaptability of LLM outputs in response to new information, comparing them to the behavior expected of agents versus databases.

## Limitations
- The semi-synthetic corpus based on Wikidata may not capture the complexity and ambiguity of real-world knowledge.
- The assumption that a Bayesian model trained on synthetic data serves as a valid gold standard may not hold for larger, more complex models trained on diverse real-world data.
- The study focuses exclusively on factual edits rather than more complex belief revision scenarios involving probabilistic or uncertain information.

## Confidence
- **High confidence**: The conceptual framework linking model editing to belief revision theory is well-established and the experimental methodology using exact Bayesian posteriors as gold standards is sound.
- **Medium confidence**: The specific results showing poor generalization of model edits to downstream consequences are robust within the synthetic domain but may not directly transfer to real-world scenarios.
- **Low confidence**: The claim that uncertainty is expressed through two distinct channels (probabilities vs. semantic text) in RLHF-trained models requires further empirical validation across different model architectures and training regimes.

## Next Checks
1. Apply the same evaluation framework to a model trained on real-world data with naturally occurring knowledge dependencies to assess whether the poor generalization patterns persist.
2. Conduct controlled experiments to empirically verify whether RLHF-trained models indeed express uncertainty through distinct channels and how this affects model editing outcomes.
3. Compare Bayesian posteriors against other potential rational standards for belief revision (such as AGM theory or Jeffrey conditioning) to determine if the observed poor generalization is specific to the Bayesian approach.