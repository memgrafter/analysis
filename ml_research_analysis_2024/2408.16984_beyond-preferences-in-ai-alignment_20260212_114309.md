---
ver: rpa2
title: Beyond Preferences in AI Alignment
arxiv_id: '2408.16984'
source_url: https://arxiv.org/abs/2408.16984
tags:
- preferences
- alignment
- human
- systems
- utility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the dominant preferentist approach to AI
  alignment, which assumes that human values can be adequately represented by preferences
  and that AI systems should be aligned with the preferences of humans. The authors
  argue that this approach is limited because it fails to capture the rich semantics
  of human values, assumes completeness of preferences when they may be incommensurable,
  and does not provide a normative standard for acceptable preferences.
---

# Beyond Preferences in AI Alignment

## Quick Facts
- arXiv ID: 2408.16984
- Source URL: https://arxiv.org/abs/2408.16984
- Reference count: 40
- Key outcome: Challenges dominant preferentist approach to AI alignment, proposing contractualist alignment with normative standards appropriate to AI's social roles and functions

## Executive Summary
This paper fundamentally challenges the dominant preferentist approach to AI alignment, which assumes human values can be adequately represented by preferences. The authors argue this approach is limited because it fails to capture the rich semantics of human values, assumes preference completeness when values may be incommensurable, and lacks normative standards for acceptable preferences. Instead, they propose a pluralistic and contractualist approach where AI systems are aligned with normative standards negotiated by relevant stakeholders according to the system's social role.

The paper critiques expected utility theory as both unnecessary and insufficient for rational agency, arguing that coherent EU maximization is intractable and that locally coherent, tool-like AI systems may be more appropriate. This reframing shifts focus from preference matching to alignment with negotiated normative standards that promote mutual benefit and limit harm despite plural and divergent human values.

## Method Summary
The paper employs a conceptual and theoretical approach, critiquing existing preference-based alignment frameworks and proposing contractualist alternatives. Rather than proposing a specific model or algorithm, it identifies limitations of rational choice theory, expected utility theory, and preference aggregation methods while exploring alternative frameworks including resource-rational decision-making models, non-scalar reward representations, and normative reasoning systems. The work synthesizes philosophical arguments with practical AI alignment considerations to propose a more pluralistic approach to ensuring AI systems serve diverse ends while respecting contextual boundaries and stakeholder agreements.

## Key Results
- Preferences are constructed from values and reasons rather than being ontologically fundamental
- Expected utility theory is both unnecessary and insufficient for rational agency
- Contractualist frameworks offer a viable alternative to preference aggregation for multi-principal alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reframes AI alignment from preference matching to normative standard alignment using contractualist frameworks
- Mechanism: By treating preferences as constructed from values and reasons rather than fundamental, enables shift from optimizing preference functions to aligning with negotiated normative standards appropriate to AI's social role
- Core assumption: Preferences are derived from values, reasons, and social norms rather than being ontologically basic
- Evidence anchors: Abstract statement about alignment with normative standards appropriate to social roles; section 4.3 discussion of role-appropriate normative ideals
- Break condition: If preferences prove fundamental rather than constructed, or contractualist negotiation proves infeasible

### Mechanism 2
- Claim: Provides theoretical grounding for moving beyond expected utility theory as a normative standard
- Mechanism: Demonstrates coherence arguments for EUT are not rationally required and globally coherent agents are neither necessary nor sufficient for rational agency
- Core assumption: Rational agency does not require global coherence or expected utility maximization
- Evidence anchors: Section 3 arguments that EUT is unnecessary and insufficient for rational agency; coherence is not rationally required and EU maximization is intractable
- Break condition: If future AI architectures require global coherence for safety, or EUT remains only tractable framework

### Mechanism 3
- Claim: Addresses multi-principal alignment challenges by replacing preference aggregation with contractualist negotiation
- Mechanism: Identifies limitations of utilitarian preference aggregation (computational intractability, political infeasibility, comparability difficulties) and proposes alignment with mutually agreed-upon normative standards
- Core assumption: Preference aggregation insufficient for multi-principal alignment due to theoretical, computational, and political limitations
- Evidence anchors: Section 5.2 discussion of political infeasibility and computational intractability of preference aggregation; section 5 advocacy for mutual benefit through stakeholder agreement
- Break condition: If preference aggregation proves computationally tractable at scale, or political conditions enable centralized benevolent planning

## Foundational Learning

- Concept: Revealed preference theory and its limitations
  - Why needed here: Understanding traditional preference derivation from choices is crucial for grasping paper's challenges to this approach
  - Quick check question: What are the key assumptions of revealed preference theory, and what are the main critiques of these assumptions?

- Concept: Expected utility theory and coherence arguments
  - Why needed here: Paper's critique of EUT as normative standard requires understanding what EUT claims and coherence arguments supporting it
  - Quick check question: What are the VNM axioms, and what are the strongest arguments for and against their status as rationality requirements?

- Concept: Contractualism and social choice theory
  - Why needed here: Paper's proposed solution to multi-principal alignment relies on understanding alternatives to preference aggregation
  - Quick check question: How does contractualism differ from utilitarianism in addressing social choice problems, and what are key advantages and disadvantages?

## Architecture Onboarding

- Component map: Preference construction module -> Normative reasoning engine -> Context-sensitive alignment module -> Contractualist negotiation framework
- Critical path: Preference construction → Normative reasoning → Context alignment → Stakeholder negotiation → System behavior
- Design tradeoffs:
  - Expressivity vs. tractability in preference representations
  - Global coherence vs. local tool-like behavior
  - Preference aggregation vs. contractualist negotiation
  - Individual autonomy vs. collective benefit
- Failure signatures:
  - Over-optimization of preferences leading to manipulation
  - Inability to handle preference incompleteness
  - Failure to respect contextual boundaries
  - Breakdown in stakeholder negotiation processes
- First 3 experiments:
  1. Implement ECD framework (Evaluate-Commensurate-Decide) with toy preference scenarios to test handling of incommensurable values
  2. Build prototype normative reasoning engine using argumentation frameworks and test on simple moral dilemmas
  3. Create context-sensitive preference model and test on multi-context assistance scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective computational frameworks for integrating normative reasoning (e.g., argumentation logics, deontic logics) with probabilistic models of human decision-making and value construction?
- Basis in paper: [explicit] Paper discusses need to integrate normative reasoning systems with machine learning and decision-making frameworks, suggesting this as research direction
- Why unresolved: While formal systems for normative reasoning exist, limited work on combining these with probabilistic programming or other ML approaches for AI systems reasoning about human values
- What evidence would resolve it: Demonstrations of AI systems reliably reasoning about normative concepts (e.g., fairness, harm) in novel situations using hybrid approach combining normative logics with probabilistic inference, showing improved performance over single-approach systems

### Open Question 2
- Question: How can AI systems be designed to remain locally coherent (i.e., with incomplete preferences across contexts) while operating with global scope, and what are practical trade-offs of such designs?
- Basis in paper: [explicit] Paper proposes designing AI systems with locally complete preferences that are incomplete across contexts to avoid manipulation incentives, but notes implementation challenge
- Why unresolved: While theoretical framework for locally coherent agents is proposed, limited understanding of practical implementation, performance trade-offs, and balancing local coherence with consistent behavior across contexts
- What evidence would resolve it: Empirical studies comparing performance and safety properties of AI systems designed with locally coherent preferences versus globally coherent ones, particularly in complex, multi-context environments

### Open Question 3
- Question: What are the most effective methods for aligning AI systems with normative standards mutually agreed upon by diverse stakeholders, given computational and political challenges of preference aggregation?
- Basis in paper: [explicit] Paper critiques preference aggregation approaches and advocates contractualist approach to multi-principal alignment, but acknowledges need for further research on implementation
- Why unresolved: While paper outlines theoretical advantages of contractualist alignment, limited practical guidance on implementation, particularly computational methods for negotiating and representing shared normative standards, and handling difficult agreement situations
- What evidence would resolve it: Case studies or simulations demonstrating effectiveness of contractualist alignment approaches in achieving mutually beneficial outcomes for diverse stakeholders, compared to traditional preference aggregation methods

## Limitations
- The paper's central claim that preferences are constructed rather than fundamental remains largely theoretical without empirical validation
- The proposed contractualist approach lacks concrete implementation details and faces significant practical challenges in stakeholder negotiation and norm establishment
- The critique of EUT as insufficient for rational agency is philosophically grounded but may not account for practical constraints in AI system design

## Confidence
- **High Confidence:** Critique of preference completeness assumptions and identification of EUT limitations as both design strategy and analytical lens
- **Medium Confidence:** Viability of contractualist frameworks as alternatives to preference aggregation for multi-principal alignment
- **Low Confidence:** Practical feasibility of implementing context-sensitive, pluralistic alignment systems in real-world AI deployments

## Next Checks
1. **Empirical Validation:** Design experiments testing preference construction mechanisms using real human decision data to verify preferences exhibit incompleteness and context-sensitivity claimed
2. **Implementation Prototyping:** Develop small-scale contractualist negotiation framework for multi-stakeholder AI alignment scenarios to assess tractability and identify failure modes
3. **Comparative Analysis:** Systematically compare proposed approach against existing preference-based alignment methods across key dimensions (computational tractability, stakeholder satisfaction, norm compliance) using benchmark scenarios