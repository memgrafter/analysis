---
ver: rpa2
title: Hierarchical Multi-modal Transformer for Cross-modal Long Document Classification
arxiv_id: '2407.10105'
source_url: https://arxiv.org/abs/2407.10105
tags:
- long
- multi-modal
- document
- features
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Hierarchical Multi-modal Transformer (HMT) addresses the problem
  of cross-modal long document classification by integrating text and image features
  at different hierarchical levels. HMT employs two multi-modal transformers to model
  complex relationships between section and sentence features, along with a dynamic
  multi-scale multi-modal transformer to capture multi-scale correlations between
  sentences and images.
---

# Hierarchical Multi-modal Transformer for Cross-modal Long Document Classification

## Quick Facts
- arXiv ID: 2407.10105
- Source URL: https://arxiv.org/abs/2407.10105
- Reference count: 40
- Key outcome: Hierarchical Multi-modal Transformer (HMT) achieves state-of-the-art results on cross-modal long document classification with accuracy of 90.8% on Materials and 83.8% on MAAPD datasets

## Executive Summary
Hierarchical Multi-modal Transformer (HMT) addresses the challenge of cross-modal long document classification by integrating text and image features at different hierarchical levels. The approach uses two specialized multi-modal transformers to capture complex relationships between section and sentence features, along with a dynamic multi-scale transformer to model multi-scale correlations between sentences and images. A dynamic mask transfer module enables effective information flow between hierarchical levels. Experiments on newly created and public datasets demonstrate significant performance improvements over state-of-the-art single-modality and multi-modality methods.

## Method Summary
HMT employs a hierarchical architecture that processes long documents by first extracting text features using pre-trained BERT models at both section and sentence levels, along with image features using CLIP. The system uses two main transformers: a Multi-modal Transformer for section-image interactions and a Dynamic Multi-scale Multi-modal Transformer (DMMT) for sentence-image interactions with multi-scale attention windows. A Dynamic Mask Transfer module propagates learned associations from section to sentence level. The model is trained using AdamW optimizer with learning rate 2e-5, weight decay 0.1, batch size 4, for 30 epochs, and evaluated using Accuracy, Precision, Recall, and Macro-F1 score metrics.

## Key Results
- HMT achieves accuracy of 90.8% and 83.8% on Materials and MAAPD datasets respectively
- Precision scores reach 90.6% and 82.0% on the same datasets
- Recall scores are 91.5% and 79.4% for Materials and MAAPD
- Macro-F1 scores achieve 90.9% and 80.3% on Materials and MAAPD datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Hierarchical multi-modal transformers capture multi-scale text-image relationships better than flat fusion approaches.
- **Mechanism**: The HMT uses two separate transformers—one for section-image interactions and another for sentence-image interactions—with a dynamic mask transfer module enabling hierarchical propagation of learned associations.
- **Core assumption**: Section-level features provide coarse-grained semantic context that improves sentence-level image association modeling.
- **Evidence anchors**:
  - [abstract] "Our approach uses a multi-modal transformer and a dynamic multi-scale multi-modal transformer to model the complex relationships between image features, and the section and sentence features."
  - [section] "To capture these complex and diverse relationships, we propose two multi-modal transformers, namely the Multi-modal Transformer and the Dynamic Multi-scale Multi-modal Transformer."
  - [corpus] Weak evidence - related papers focus on scale-aware vision-language transformers but don't directly address hierarchical structure propagation between levels.
- **Break condition**: If section features are semantically disconnected from sentence features, the mask transfer provides negative transfer rather than beneficial context propagation.

### Mechanism 2
- **Claim**: Dynamic multi-scale window masks improve sentence-image correspondence modeling by limiting attention scope.
- **Mechanism**: The DMMT applies multiple window mask matrices of different sizes to the multi-head attention, constraining how sentences attend to images at varying granularities, then dynamically fuses these representations.
- **Core assumption**: Multi-scale attention patterns capture complementary aspects of sentence-image relationships that single-scale attention misses.
- **Evidence anchors**:
  - [abstract] "we introduce an enhanced transformer, namely Dynamic Multi-scale Multi-modal Transformer (DMMT). The DMMT is designed by assigning window masks of different sizes to the multi-head attention matrix, which allows it to capture varying levels of detail in the interactions."
  - [section] "To model the multi-scale correspondence relationships between sentences and images, we choose to limit the scope of attention of sentence elements by placing multi-scale window masks on the multi-head adjacency matrix."
  - [corpus] Weak evidence - while multi-scale vision-language transformers exist (HSVLT), they focus on image scale rather than sentence-image correspondence scales.
- **Break condition**: If window sizes are poorly chosen relative to document structure, attention becomes too restrictive and misses important cross-modal connections.

### Mechanism 3
- **Claim**: Critical image selection with similarity thresholds reduces noise from uncorrelated images.
- **Mechanism**: For each section, the top-K images with highest similarity scores (meeting threshold η=0.65) are selected as critical images, and sentence-image associations are derived through section-image critical mappings.
- **Core assumption**: Long documents contain many irrelevant images, and filtering based on semantic similarity improves multi-modal representation quality.
- **Evidence anchors**:
  - [abstract] "Lastly, there may be weak correlations between the images and the texts, and some images may even be unrelated, which adds difficulties and poses challenges to the multi-modal representation of long documents."
  - [section] "For each section, we select the top-K images with the highest similarity scores as the critical image index set I +...we can obtain the binary mask matrix."
  - [corpus] No direct evidence - related papers don't discuss critical image selection with similarity thresholds.
- **Break condition**: If similarity metrics are unreliable (e.g., due to domain shift), the critical image selection may exclude relevant images and retain irrelevant ones.

## Foundational Learning

- **Concept**: Hierarchical document representation
  - Why needed here: Long documents have explicit section-sentence structure that provides semantic context for better image association
  - Quick check question: How does hierarchical modeling differ from simply concatenating all text tokens and applying a single transformer?

- **Concept**: Multi-modal transformer architectures
  - Why needed here: Standard transformers work on single modalities; cross-modal tasks require mechanisms to align and fuse heterogeneous representations
  - Quick check question: What are the key differences between token-based image input (ViT patches) versus region-based features (Faster-RCNN) for multi-modal transformers?

- **Concept**: Attention masking and window-based mechanisms
  - Why needed here: Full self-attention has quadratic complexity; window-based approaches enable scalable long-document processing while preserving local structure
  - Quick check question: How does fixed window attention differ from learnable sparse attention patterns in terms of flexibility and computational efficiency?

## Architecture Onboarding

- **Component map**: Input features (BERT sections, STG sentences, CLIP images) → MMT (section-image) → DMMT (sentence-image with multi-scale masks) → DMT (mask transfer from MMT to DMMT) → Fusion (max-pooling) → Classification

- **Critical path**: Feature extraction → MMT processing → DMT mask generation → DMMT processing with transferred masks → Final fusion and classification

- **Design tradeoffs**: 
  - Two separate transformers vs. single unified transformer: better hierarchical modeling but increased complexity
  - Multi-scale windows vs. single attention span: richer representation but more parameters and computation
  - Critical image selection vs. using all images: noise reduction vs. potential information loss

- **Failure signatures**: 
  - Poor performance despite good single-modality baselines: indicates multi-modal fusion isn't working
  - Degraded performance when adding images: suggests negative transfer or noise issues
  - Sensitivity to section length parameter: indicates improper hierarchical alignment

- **First 3 experiments**:
  1. Remove DMT module and measure performance drop to assess hierarchical benefit
  2. Replace multi-scale masks with single fixed window size to test scale sensitivity
  3. Compare critical image selection (top-K) vs. using all images with learned attention weights

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Dynamic Mask Transfer module impact classification performance on documents with varying degrees of image-text correlation?
- Basis in paper: [explicit] The authors introduce the Dynamic Mask Transfer module to facilitate information exchange between section-level and sentence-level multi-modal transformers.
- Why unresolved: The paper doesn't provide a detailed analysis of how the Dynamic Mask Transfer module performs under different levels of image-text correlation within documents.
- What evidence would resolve it: Comparative experiments showing classification accuracy with and without the Dynamic Mask Transfer module on datasets with varying degrees of image-text correlation would provide insights into its effectiveness.

### Open Question 2
- Question: What is the optimal number of window scales for the Dynamic Multi-scale Multi-modal Transformer (DMMT) across different types of long documents?
- Basis in paper: [explicit] The authors experiment with different numbers of window scales for DMMT but don't provide a definitive answer on the optimal number across various document types.
- Why unresolved: The paper only tests a limited range of window scales and doesn't explore the impact of different numbers of scales on classification performance for different document categories.
- What evidence would resolve it: Systematic experiments testing various numbers of window scales on a diverse set of long document datasets would reveal the optimal configuration for different document types.

### Open Question 3
- Question: How does the proposed HMT model compare to other state-of-the-art models in terms of computational efficiency and scalability for processing extremely long documents?
- Basis in paper: [inferred] The authors mention computational complexity but don't provide a detailed comparison with other models in terms of processing time and memory usage for extremely long documents.
- Why unresolved: The paper focuses on accuracy and performance metrics but lacks a comprehensive analysis of computational efficiency and scalability, especially for documents with thousands of tokens and images.
- What evidence would resolve it: Comparative experiments measuring processing time and memory usage of HMT against other models on datasets with extremely long documents would provide insights into its scalability and efficiency.

## Limitations

- The experimental validation is based on relatively small datasets (highest being MAAPD with 6,500 documents), raising questions about scalability and generalization to truly large-scale long document collections
- The fixed window mask sizes ([3,5,7]) were selected through limited experiments without systematic hyperparameter tuning, suggesting potential sensitivity to these choices
- The critical image selection mechanism relies on a fixed similarity threshold (η=0.65) that may not generalize across domains with different image-text correlation patterns

## Confidence

- **High confidence**: The core architectural innovations (hierarchical transformers with dynamic mask transfer) are well-defined and theoretically sound
- **Medium confidence**: The reported performance improvements over baselines are substantial but based on limited dataset diversity
- **Low confidence**: The effectiveness of specific hyperparameter choices (window sizes, similarity thresholds) across different document types and domains

## Next Checks

1. Test model robustness by varying the similarity threshold η across a wider range (0.5-0.9) and measuring impact on classification accuracy for different document domains
2. Conduct ablation studies with different window mask configurations ([1,3,5], [3,7,11], [2,4,6]) to identify sensitivity to multi-scale attention patterns
3. Evaluate performance on a larger, more diverse long document dataset (e.g., scientific papers or legal documents) to assess real-world applicability beyond the current datasets