---
ver: rpa2
title: Improved Visually Prompted Keyword Localisation in Real Low-Resource Settings
arxiv_id: '2409.06013'
source_url: https://arxiv.org/abs/2409.06013
tags:
- keyword
- speech
- pairs
- localisation
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses visually prompted keyword localisation (VPKL)
  - detecting and localising an image query depicting a word within spoken utterances
  - in low-resource languages. Previous approaches relied on transcriptions to construct
  positive and negative pairs for contrastive training, making them inapplicable to
  unwritten languages.
---

# Improved Visually Prompted Keyword Localisation in Real Low-Resource Settings

## Quick Facts
- arXiv ID: 2409.06013
- Source URL: https://arxiv.org/abs/2409.06013
- Reference count: 24
- Primary result: Few-shot mining approach for VPKL without transcriptions, with only ~11% drop in F1 compared to ground truth pairs on English

## Executive Summary
This paper addresses visually prompted keyword localisation (VPKL) - detecting and localising an image query depicting a word within spoken utterances - in low-resource languages where transcriptions are unavailable. The authors propose a few-shot mining approach that automatically constructs training pairs without transcriptions by using QbERT to predict keyword occurrences in unlabelled speech data. This enables training of a LOCATTNET model using contrastive learning. Experiments show the approach consistently outperforms visual tagger baselines on English, with only ~11% drop in F1 scores compared to using ground truth pairs. On Yoruba - the first VPKL work on a real low-resource language - results are reasonable but with larger performance gaps, highlighting the importance of language-tailored speech representations. Without CPC pretraining on Yoruba data, the approach fails entirely.

## Method Summary
The approach uses a small support set of isolated keyword examples to query unlabelled speech data via QbERT, which leverages HuBERT representations and a string-matching algorithm to predict which utterances contain the keyword. These predictions are used to create contrastive pairs for training LOCATTNET without relying on transcriptions. The LOCATTNET model uses AlexNet for vision and CPC-pretrained networks for audio, connected via matchmap attention. For low-resource languages like Yoruba, CPC pretraining on unlabelled language data is essential for capturing language-specific acoustic patterns.

## Key Results
- Few-shot mining approach consistently outperforms visual tagger baselines on English
- Only ~11% drop in F1 scores compared to using ground truth pairs on English
- On Yoruba, reasonable results with larger performance gaps, highlighting importance of language-tailored speech representations
- Without CPC pretraining on Yoruba data, the approach fails entirely

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot mining using QbERT and HuBERT enables VPKL training without transcriptions by automatically generating positive and negative pairs from unlabelled speech.
- Mechanism: The approach uses a small support set of isolated keyword examples to query unlabelled speech data via QbERT, which leverages HuBERT representations and a string-matching algorithm to predict which utterances contain the keyword. These predictions are used to create contrastive pairs for training LOCATTNET without relying on transcriptions.
- Core assumption: HuBERT's multilingual representations can adequately capture acoustic patterns in low-resource languages like Yoruba, even without direct exposure to the language.
- Evidence anchors:
  - [abstract] "Using a small support set of spoken keyword examples, they apply QbERT to predict which keywords occur in speech-image training data, enabling automatic construction of contrastive pairs."
  - [section] "We start by collecting a small number ( K) of isolated speech examples for each of the keywords... These spoken support set examples are used as queries in a query-by-example search approach called QbERT."
  - [corpus] Weak evidence - no direct validation of HuBERT performance on Yoruba-specific data; relies on multilingual pretraining assumption.
- Break condition: If the support set is too small or the HuBERT model fails to generate discriminative units for the target language, mining accuracy will drop significantly, as observed in Yoruba experiments.

### Mechanism 2
- Claim: Pretraining the audio branch with CPC on target language data is essential for model performance in low-resource settings.
- Mechanism: CPC pretraining on unlabelled Yoruba data provides a language-specific acoustic representation that enables the model to better capture phonetic and tonal patterns unique to the language, improving detection and localisation accuracy.
- Core assumption: Self-supervised pretraining on unlabelled speech captures useful linguistic structure that transfers to downstream VPKL tasks.
- Evidence anchors:
  - [abstract] "We also show that it is essential to pretrain the audio branch of the visually grounded speech model on unlabelled Yor`ub´a data – without this, the approach fails, even with perfect pairs."
  - [section] "To initialise the audio branch of the Yor`ub´a model, we also use the Yor`ub´a Bible data to train the CPC model."
  - [corpus] Limited direct evidence - assumes effectiveness of CPC on Yoruba Bible data, but no ablation without pretraining on Yoruba.
- Break condition: If sufficient unlabelled data is unavailable or CPC pretraining does not capture the necessary linguistic features, the model will fail to learn effective keyword representations.

### Mechanism 3
- Claim: Matchmap attention between vision and audio embeddings allows the model to focus on keyword-level features rather than whole utterance/image similarities.
- Mechanism: The dot-product similarity matrix M between audio and vision embeddings is used to compute frame-level scores for localisation, while the max over M provides an overall detection score. This mechanism encourages the model to align specific acoustic segments with visual features corresponding to the keyword.
- Core assumption: Frame-level attention via matchmap can effectively localise keywords within variable-length utterances.
- Evidence anchors:
  - [section] "The vision and audio branches are connected by a matchmap attention mechanism [7] that computes the dot product between each audio embedding in ya and each vision embedding in yv, yielding a similarity matrix M."
  - [section] "To predict at which frames an image query occurs, we take the maximum over the image axis of M and obtain a similarity score for each frame."
  - [corpus] No direct evidence comparing matchmap to other attention mechanisms; assumes it is sufficient for keyword localisation.
- Break condition: If the attention mechanism cannot resolve ambiguity in overlapping or similar-sounding keywords, localisation accuracy will suffer.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: The model needs to learn to distinguish keyword instances from non-instances without explicit transcriptions, which is achieved through a contrastive loss that pulls positive pairs together and pushes negative pairs apart.
  - Quick check question: What is the purpose of including both positive and negative pairs in the contrastive loss formulation?

- Concept: Self-supervised speech representation learning
  - Why needed here: Since transcriptions are unavailable, the model must learn useful speech representations from unlabelled data using methods like HuBERT and CPC, which capture phonetic and linguistic structure.
  - Quick check question: How does CPC pretraining on unlabelled speech improve downstream keyword localisation performance?

- Concept: Query-by-example search
  - Why needed here: To automatically mine training pairs without transcriptions, the approach uses spoken keyword examples to search through unlabelled utterances, predicting which contain the keyword.
  - Quick check question: What role does the noisy string matching algorithm play in the QbERT mining process?

## Architecture Onboarding

- Component map:
  - Vision branch: AlexNet (pretrained on ImageNet) → image embeddings
  - Audio branch: CPC-pretrained network + BiLSTM layers → frame-level speech embeddings
  - Attention mechanism: Matchmap attention → similarity matrix M
  - Loss function: Contrastive loss with positive/negative pairs mined via QbERT
  - Output: Detection score (max over M) and localisation scores (max over image axis of M)

- Critical path:
  1. Input: Image query and spoken utterance
  2. Encode image → vision embeddings
  3. Encode speech → audio embeddings
  4. Compute matchmap similarity matrix
  5. Extract detection and localisation scores
  6. Apply contrastive loss during training using mined pairs

- Design tradeoffs:
  - Using matchmap attention is simpler than context vector approaches but may be less precise for complex localisation
  - Multilingual HuBERT improves cross-lingual transfer but may miss language-specific features
  - CPC pretraining is essential for low-resource performance but requires unlabelled data in the target language

- Failure signatures:
  - Low detection precision but high recall → model is overgenerating detections, possibly due to weak negative mining
  - Poor localisation scores → attention mechanism is not effectively aligning keyword segments
  - Complete failure on low-resource languages → CPC pretraining is missing or HuBERT representations are inadequate

- First 3 experiments:
  1. Train LOCATTNET with ground truth pairs on English to establish topline performance and validate architecture
  2. Train with few-shot mined pairs on English to measure performance drop and validate mining approach
  3. Train with mined pairs on Yoruba without CPC pretraining to confirm pretraining is essential

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would training a Yor`ubá-specific HuBERT model on unlabeled Yor`ubá speech affect the accuracy of few-shot pair mining and overall VPKL performance?
- Basis in paper: [explicit] The authors note that current approach relies on multilingual HuBERT not optimized for Yor`ubá, missing language-specific acoustic properties
- Why unresolved: Authors hypothesize this would improve performance but haven't tested it
- What evidence would resolve it: Training results showing improved pair mining accuracy and VPKL scores using Yor`ubá HuBERT

### Open Question 2
- Question: Can the few-shot support set requirement be eliminated to enable search for arbitrary words without prior keyword examples?
- Basis in paper: [explicit] Authors identify this as a limitation and propose future work on adapting QbERT for fully unsupervised mining
- Why unresolved: Authors haven't implemented or tested this approach yet
- What evidence would resolve it: Results demonstrating VPKL performance without support set using only QbERT-based utterance comparison

### Open Question 3
- Question: How would improving the accuracy of keyword localization in cropped query images (with multiple objects) affect overall VPKL performance?
- Basis in paper: [inferred] Authors mention that current cropping approach is imperfect and multi-object scenarios are not handled
- Why unresolved: Authors haven't developed or tested methods for better object selection or multi-object handling
- What evidence would resolve it: Comparison of VPKL performance using improved object selection techniques vs current cropping method

## Limitations

- The approach relies heavily on multilingual HuBERT representations, but there is limited evidence that these adequately capture the acoustic and tonal patterns of low-resource languages like Yoruba without direct language-specific training.
- The effectiveness of the few-shot mining approach is highly dependent on the quality of the support set and the QbERT predictions, with accuracy dropping significantly on low-resource languages (37% for Yoruba vs 70% for English).
- The matchmap attention mechanism, while simple, may not be optimal for complex keyword localisation tasks, particularly when dealing with overlapping or similar-sounding keywords.

## Confidence

- High confidence in the overall framework: The combination of contrastive learning, self-supervised pretraining, and query-by-example search is well-established and logically sound for the VPKL task.
- Medium confidence in cross-lingual transfer: While multilingual HuBERT and CPC pretraining show promise, the significant performance gap between English and Yoruba suggests limitations in direct transfer without language-specific adaptation.
- Low confidence in the mining approach for very low-resource languages: The 37% mining accuracy on Yoruba indicates that the approach may struggle when support sets are small and language-specific representations are inadequate.

## Next Checks

1. **Validate HuBERT representations for Yoruba**: Conduct a direct evaluation of multilingual HuBERT's ability to capture Yoruba phonetic and tonal patterns by comparing keyword mining accuracy with and without Yoruba-specific fine-tuning.

2. **Ablate matchmap attention**: Replace matchmap attention with a context vector approach and compare localisation accuracy to determine if the simpler mechanism is sufficient or if a more sophisticated attention mechanism is needed.

3. **Scale support set size**: Systematically vary the number of keyword examples in the support set (e.g., 1, 5, 10, 20) and measure the impact on mining accuracy and downstream VPKL performance to determine the minimum viable support set size.