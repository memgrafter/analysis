---
ver: rpa2
title: What Makes Language Models Good-enough?
arxiv_id: '2406.03666'
source_url: https://arxiv.org/abs/2406.03666
tags:
- language
- processing
- memory
- good-enough
- humans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines what architectural features make language models
  learn human-like good-enough language processing. The authors create a good-enough
  language processing (GELP) evaluation dataset with 7,680 examples, designed to test
  the effects of two plausibility types, eight construction types, and three degrees
  of memory cost on language processing.
---

# What Makes Language Models Good-enough?

## Quick Facts
- arXiv ID: 2406.03666
- Source URL: https://arxiv.org/abs/2406.03666
- Authors: Daiki Asami; Saku Sugawara
- Reference count: 30
- Models with shallower depth and fewer attention heads can learn human-like good-enough language processing

## Executive Summary
This paper investigates what architectural features enable language models to exhibit human-like good-enough language processing. The authors create a Good-Enough Language Processing (GELP) evaluation dataset with 7,680 examples that systematically vary plausibility types, construction types, and memory costs. Through crowdsourcing experiments following psycholinguistic protocols, they establish human baselines for good-enough processing. The study then evaluates 24 BERT miniature models with varying layers and attention heads, revealing that shallower architectures (fewer layers) exhibit better human-model matching scores, suggesting that detailed syntactic analysis is unnecessary for good-enough processing.

## Method Summary
The authors construct the GELP dataset with 7,680 examples designed to test the effects of plausibility types, construction types, and memory costs on language processing. They conduct crowdsourcing experiments to obtain human responses following established psycholinguistic methodologies. For model evaluation, they create 24 BERT miniatures varying in layers (2-12) and self-attention heads (2-12), fine-tuning them on SNLI, MNLI, and HANS datasets before testing on GELP using NLI tasks. The human-model matching score measures the percentage of predicted labels that match human responses, while accuracy is analyzed across different memory load conditions.

## Key Results
- Shallow BERT models (L2-A2) achieve better human-model matching scores than deeper models (L12-A12) on GELP tasks
- Models exhibit decreasing accuracy as memory cost increases, mirroring human behavior
- Increasing layers from 8 to 12 does not improve human-like performance considerably
- The role of attention heads in good-enough processing remains unclear from the results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shallow architecture leads to good-enough performance
- Mechanism: Fewer layers reduce the depth of syntactic analysis, preventing detailed representation construction
- Core assumption: Good-enough processing requires underspecified syntactic analysis
- Evidence anchors:
  - [abstract] "models with shallower depth and fewer heads can learn good-enough language processing"
  - [section 5.3] "increasing the number of layers from 8 to 12 does not improve the models' human-like performance considerably"
  - [corpus] "Average neighbor FMR=0.513" (weak connection to architectural depth)
- Break condition: If detailed syntactic analysis becomes necessary for task performance, shallow models will fail

### Mechanism 2
- Claim: Memory load affects processing strategy
- Mechanism: High memory demand forces shallow processing to conserve cognitive resources
- Core assumption: Humans adopt good-enough strategies under memory pressure
- Evidence anchors:
  - [section 4.2] "accuracy decreases as the memory load increases" for both humans and models
  - [section 5.3] "models exhibit decreasing accuracy as the memory cost increases"
  - [corpus] "Linking In-context Learning in Transformers to Human Episodic Memory" (relevant but weak direct evidence)
- Break condition: If models can process complex syntax without memory trade-offs, this mechanism breaks down

### Mechanism 3
- Claim: Attention heads capture encoding phase of working memory
- Mechanism: Fewer heads limit detailed encoding, leading to good-enough representations
- Core assumption: Self-attention mechanism parallels human working memory encoding
- Evidence anchors:
  - [abstract] "models with fewer layers and/or self-attention heads exhibit a good-enough performance"
  - [section 5.3] "the role of the number of attention heads is unclear" - suggests complexity beyond simple encoding
  - [corpus] "Some Attention is All You Need for Retrieval" (relevant but focuses on retrieval, not encoding)
- Break condition: If attention heads primarily serve other functions beyond encoding, this mechanism fails

## Foundational Learning

- Concept: Good-enough theory in psycholinguistics
  - Why needed here: Understanding the theoretical foundation for why humans use heuristic processing
  - Quick check question: What distinguishes good-enough processing from algorithmic processing?

- Concept: Transformer architecture and self-attention
  - Why needed here: Core mechanism being evaluated for good-enough processing capabilities
  - Quick check question: How does the self-attention mechanism relate to human working memory?

- Concept: Natural Language Inference task design
  - Why needed here: Evaluation framework used to test model performance
  - Quick check question: What distinguishes entailment from non-entailment in NLI tasks?

## Architecture Onboarding

- Component map: Input → Layer stacking (depth) → Attention computation (width) → Output prediction
- Critical path: Input → Layer stacking (depth) → Attention computation (width) → Output prediction
- Design tradeoffs: Depth vs. width for good-enough vs. precise processing
- Failure signatures: Models failing at implausible sentences, memory-demanding conditions
- First 3 experiments:
  1. Compare L2-A2 vs L12-A12 on GELP accuracy by memory load
  2. Test transfer of good-enough behavior to out-of-distribution syntactic constructions
  3. Ablate attention heads systematically to identify critical thresholds for good-enough performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the shallow architecture of language models directly cause good-enough language processing, or is it a correlation with other factors?
- Basis in paper: [explicit] The paper suggests that shallower models (fewer layers) exhibit good-enough performance, consistent with the hypothesis that detailed syntactic analysis is unnecessary.
- Why unresolved: The paper only explores BERT models; it doesn't rule out whether other factors, such as training data or optimization, contribute to this performance.
- What evidence would resolve it: Testing a wider variety of models (e.g., GPT-2, RoBERTa) with controlled variations in architecture and training procedures would help isolate the effect of depth on good-enough performance.

### Open Question 2
- Question: How does the self-attention mechanism in Transformers relate to the encoding phase of the working memory system, and why does this connection impact good-enough language processing?
- Basis in paper: [explicit] The paper hypothesizes that fewer attention heads might reduce reliance on a strong working memory system, but the results don't support this.
- Why unresolved: The paper doesn't provide a clear explanation for the disconnect between the number of attention heads and the memory load effect observed in human processing.
- What evidence would resolve it: Further research into the role of self-attention in encoding linguistic input, potentially through controlled experiments or analysis of attention patterns, could clarify this relationship.

### Open Question 3
- Question: Can language models learn to balance robust, algorithmic language processing with non-robust, heuristic language processing, as humans do?
- Basis in paper: [explicit] The paper notes that humans have a good balance between these two types of processing, but it doesn't test whether models can achieve this balance.
- Why unresolved: The evaluation focuses solely on good-enough performance, without assessing the models' ability to engage in detailed syntactic analysis when necessary.
- What evidence would resolve it: Developing a task that requires both types of processing and evaluating model performance across different contexts would provide insights into their ability to balance these strategies.

## Limitations
- The causal mechanism linking architectural depth to processing strategy is not fully established
- The relationship between model parameters and working memory capacity is not explicitly modeled
- The GELP dataset may not fully capture the complexity of real-world language processing scenarios

## Confidence
- High confidence: The core empirical finding that shallow BERT models achieve better human-model matching scores than deeper models
- Medium confidence: The claim that "models with shallower depth and fewer heads can learn good-enough language processing" as a general principle
- Low confidence: The mechanistic explanation that "fewer layers reduce the depth of syntactic analysis" as the primary driver of good-enough performance

## Next Checks
1. Cross-linguistic validation: Test whether good-enough processing behavior generalizes across languages with different syntactic properties
2. Longitudinal training analysis: Monitor how shallow vs. deep models' processing strategies evolve during training
3. Cognitive plausibility testing: Compare model behavior with fine-grained psycholinguistic measures to establish stronger connections between architectural features and human cognitive processing signatures