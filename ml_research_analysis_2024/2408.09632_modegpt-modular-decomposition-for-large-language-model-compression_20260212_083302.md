---
ver: rpa2
title: 'MoDeGPT: Modular Decomposition for Large Language Model Compression'
arxiv_id: '2408.09632'
source_url: https://arxiv.org/abs/2408.09632
tags:
- compression
- modegpt
- matrix
- sparsity
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MoDeGPT introduces a training-free compression method for large\
  \ language models that addresses the trade-off between accuracy and computational\
  \ efficiency in traditional low-rank matrix decomposition techniques. The method\
  \ jointly decomposes pairs of consecutive matrices within transformer modules using\
  \ classical matrix approximation techniques (Nystr\xF6m, CR decomposition, and SVD)\
  \ applied at the module level rather than individually to each matrix."
---

# MoDeGPT: Modular Decomposition for Large Language Model Compression

## Quick Facts
- arXiv ID: 2408.09632
- Source URL: https://arxiv.org/abs/2408.09632
- Authors: Chi-Heng Lin; Shangqian Gao; James Seale Smith; Abhishek Patel; Shikhar Tuli; Yilin Shen; Hongxia Jin; Yen-Chang Hsu
- Reference count: 40
- One-line primary result: Training-free compression method that maintains 90-95% zero-shot performance with 25-30% compression rates and saves 98% compute costs

## Executive Summary
MoDeGPT introduces a training-free compression method for large language models that addresses the trade-off between accuracy and computational efficiency in traditional low-rank matrix decomposition techniques. The method jointly decomposes pairs of consecutive matrices within transformer modules using classical matrix approximation techniques (Nyström, CR decomposition, and SVD) applied at the module level rather than individually to each matrix. This approach eliminates the need for recovery fine-tuning while achieving state-of-the-art structured compression efficiency. On Llama-2/3 and OPT models, MoDeGPT maintains 90-95% zero-shot performance with 25-30% compression rates, saves 98% of compute costs when compressing a 13B model, and increases inference throughput by up to 46%. The compression process completes on a single GPU within hours, making it practical for real-world deployment.

## Method Summary
MoDeGPT compresses LLMs by partitioning transformer blocks into modules comprised of matrix pairs and reducing hidden dimensions via reconstructing module-level outputs. The method uses different matrix decomposition techniques for different module types based on their nonlinearity characteristics: Nyström approximation for Type I modules (MLP with one nonlinearity), CR decomposition for Type II modules (Key-Query pairs with two nonlinearities), and SVD for Type III modules (Value-Output pairs with no nonlinearities). Global sparsity allocation using Block Influence scores and entropic regularization optimizes the trade-off between accuracy and compression rate across layers. The approach achieves compression without recovery fine-tuning by leveraging the structural properties of transformer modules and error bounds from matrix approximation theory.

## Key Results
- Maintains 90-95% zero-shot performance with 25-30% compression rates on Llama-2/3 and OPT models
- Saves 98% of compute costs when compressing a 13B model
- Increases inference throughput by up to 46% while reducing memory usage
- Completes compression process on a single GPU within hours without recovery fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint decomposition of consecutive matrix pairs within transformer modules reduces approximation error compared to individual matrix decomposition.
- Mechanism: By decomposing matrix pairs together rather than separately, MoDeGPT leverages shared structure between related matrices (e.g., Q and K matrices in attention heads) to achieve more accurate low-rank approximations with fewer parameters.
- Core assumption: The reconstruction error of joint decomposition is bounded by the error of decomposing the product of related matrices, as formalized in Theorem 2.
- Evidence anchors: [abstract] "MoDeGPT jointly decomposes pairs of consecutive matrices within transformer modules using classical matrix approximation techniques"; [section 3.2] "The core technical contribution of this work is the establishment of a one-to-one mapping between a specific type of modular compression problem and a corresponding matrix decomposition problem"
- Break condition: If the relationships between consecutive matrices are weak or if the nonlinearity in the module makes joint decomposition intractable, the error bounds may not hold.

### Mechanism 2
- Claim: Applying different matrix decomposition methods (Nyström, CR, SVD) to different module types based on their nonlinearity characteristics optimizes compression efficiency.
- Mechanism: Type I modules with one nonlinearity use Nyström approximation, Type II modules with two nonlinearities use CR decomposition, and Type III modules with no nonlinearities use SVD. This tailoring maximizes compression while maintaining accuracy.
- Core assumption: The number of nonlinearities in a module determines which decomposition method will provide the tightest error bounds.
- Evidence anchors: [abstract] "MoDeGPT partitions the Transformer block into modules comprised of matrix pairs and reduces the hidden dimensions via reconstructing the module-level outputs"; [section 2.2] "These three types are distinguished by varying levels of nonlinearity. We will employ different matrix decomposition methods for compression based on the optimization tractability of each type."
- Break condition: If the module classification based on nonlinearity levels doesn't capture the true structure of the computation, the wrong decomposition method might be applied.

### Mechanism 3
- Claim: Global sparsity allocation using Block Influence scores and entropic regularization optimizes the trade-off between accuracy and compression rate across layers.
- Mechanism: The method allocates higher sparsity to less important layers while smoothing the distribution with entropic regularization, maximizing the sum of importance scores weighted by retained parameters.
- Core assumption: Block Influence scores effectively measure layer importance for the specific task, and entropic regularization prevents extreme sparsity allocations that could harm performance.
- Evidence anchors: [abstract] "MoDeGPT presents a thorough evaluation of MoDeGPT, comparing it against existing methods across key metrics, including perplexity, downstream accuracy, and real-world speed improvements"; [section 3.3] "While MoDeGPT modules are optimized locally, we propose a global optimization strategy that translates layer importance scores into sparsity allocations across layers"
- Break condition: If Block Influence scores don't correlate well with actual importance for the target task, or if the entropic regularization parameter is poorly chosen, the allocation may be suboptimal.

## Foundational Learning

- Concept: Low-rank matrix approximation and its error bounds
  - Why needed here: Understanding how different matrix decomposition methods (SVD, Nyström, CR) approximate matrices and their associated error guarantees is fundamental to grasping why MoDeGPT's modular approach works.
  - Quick check question: What is the key difference between Nyström approximation and standard SVD in terms of what matrices they can approximate?

- Concept: Transformer architecture and module structure
  - Why needed here: Knowing how transformer blocks are organized into MLP and MHA components, and how these contain matrix pairs, is essential to understanding what MoDeGPT decomposes and why.
  - Quick check question: In a standard transformer MHA block, which matrices form a natural pair that could benefit from joint decomposition?

- Concept: Error propagation in neural networks
  - Why needed here: Understanding how errors introduced by compression in one module propagate through subsequent modules helps explain why joint decomposition within modules is beneficial.
  - Quick check question: How might errors from compressing the Q matrix in attention propagate differently than errors from compressing Q and K jointly?

## Architecture Onboarding

- Component map: MLP Module (Type I) -> Nyström approximation; MHA Module - Query/Key (Type II) -> CR decomposition; MHA Module - Value/Output (Type III) -> SVD; Global Sparsity Allocator -> Block Influence scores

- Critical path: 1. Forward pass through model to collect correlation matrices; 2. Apply module-specific decomposition algorithms to generate compressed weights; 3. Apply global sparsity allocation to determine which layers to compress more heavily; 4. Store compressed model with reduced-rank matrices

- Design tradeoffs:
  - Memory vs. speed: Higher compression rates save memory but may reduce throughput if not properly optimized
  - Accuracy vs. compression: More aggressive compression yields greater savings but at the cost of model performance
  - Complexity of implementation: Different decomposition methods per module type increase code complexity but improve results

- Failure signatures:
  - High perplexity increase indicates excessive compression or poor decomposition choices
  - Memory usage higher than expected suggests issues with correlation matrix storage or rank selection
  - Slow inference despite compression may indicate inefficient handling of compressed matrices in residual paths

- First 3 experiments:
  1. Compress a single layer's MLP module with Nyström approximation and measure perplexity change
  2. Apply global sparsity allocation to a full model and verify the distribution matches Block Influence scores
  3. Compare throughput of compressed model with different batch sizes to identify the optimal operating point

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical limit of compression rates that MoDeGPT can achieve before performance degradation becomes unacceptable?
- Basis in paper: [explicit] The paper notes a significant breakdown point at 50% compression where perplexity increased sharply from 41% to 123%, indicating the compression limit of the method.
- Why unresolved: The paper identifies a clear performance cliff at 50% compression but doesn't explore what happens at even higher compression rates (60-80%) in detail, nor does it establish a theoretical framework for predicting this limit.
- What evidence would resolve it: Systematic testing of compression rates from 50% to 90% with detailed analysis of performance metrics and identification of the precise mathematical relationship between compression rate and reconstruction error.

### Open Question 2
- Question: How does MoDeGPT's performance compare to gradient-based methods when both use recovery fine-tuning?
- Basis in paper: [inferred] The paper states MoDeGPT "matches or surpasses previous structured compression methods that rely on gradient information" but only compares against gradient-based methods without fine-tuning, noting that MoDeGPT without RFT achieves "very similar performance" to fine-tuned methods.
- Why unresolved: While the paper demonstrates MoDeGPT's effectiveness without fine-tuning, it doesn't explore whether adding fine-tuning to MoDeGPT would provide additional performance gains over gradient-based methods that already use fine-tuning.
- What evidence would resolve it: Head-to-head comparison of MoDeGPT with and without recovery fine-tuning against gradient-based methods with recovery fine-tuning, measuring both computational cost and performance metrics.

### Open Question 3
- Question: What is the optimal strategy for combining MoDeGPT with other compression techniques like quantization or layer pruning?
- Basis in paper: [explicit] The paper mentions MoDeGPT is "orthogonal to SliceGPT" and explores their combination, finding that pure MoDeGPT performs better, but suggests "better tuning of the slicing and compression ratios could enhance performance."
- Why unresolved: The paper only briefly explores combining MoDeGPT with SliceGPT and doesn't investigate combinations with other techniques like quantization, unstructured pruning, or different layer pruning strategies.
- What evidence would resolve it: Systematic exploration of hybrid approaches combining MoDeGPT with various other compression techniques, including automated methods for determining optimal combination strategies based on target application requirements.

## Limitations
- Theoretical error bounds are established but lack comprehensive empirical validation across different model architectures and compression rates
- Global sparsity allocation mechanism using Block Influence scores is not fully explained and lacks sensitivity analysis for the entropic regularization parameter
- Limited exploration of combining MoDeGPT with other compression techniques beyond the brief SliceGPT comparison

## Confidence
- Performance Retention (90-95% zero-shot performance): High confidence - Well-supported by empirical results across multiple benchmarks and model architectures
- Compression Efficiency (25-30% compression with 98% compute savings): High confidence - Strong quantitative evidence with clear before/after comparisons
- Theoretical Error Bounds: Medium confidence - Theorem statements are provided but limited empirical validation of the bounds
- Module-Type Decomposition Strategy: Medium confidence - Reasonable theoretical justification but lacks systematic comparison with alternative strategies
- Global Sparsity Allocation: Low-medium confidence - Methodology described but insufficient validation of Block Influence scores and parameter sensitivity

## Next Checks
1. **Error Bound Validation Study**: Conduct systematic experiments measuring the actual reconstruction error across different matrix decomposition methods (Nyström, CR, SVD) and compare these against the theoretical error bounds. This should include varying rank values, different layer types, and multiple model architectures to establish the reliability of the theoretical guarantees.

2. **Block Influence Score Sensitivity Analysis**: Implement a controlled study varying the entropic regularization temperature parameter ε and measuring its impact on compression performance and accuracy trade-offs. Additionally, compare the Block Influence scores against alternative layer importance metrics to validate their effectiveness in guiding sparsity allocation.

3. **Cross-Model Generalization Test**: Apply MoDeGPT to model architectures beyond the current scope (OPT, Llama-2/3) including models with different attention mechanisms or architecture variants. This would validate the robustness of the module-type classification system and the decomposition method assignments across diverse transformer implementations.