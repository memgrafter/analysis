---
ver: rpa2
title: 'Chain-of-Action: Faithful and Multimodal Question Answering through Large
  Language Models'
arxiv_id: '2403.17359'
source_url: https://arxiv.org/abs/2403.17359
tags:
- information
- answer
- arxiv
- reasoning
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a Chain-of-Action (CoA) framework to improve
  the faithfulness and reasoning ability of large language models in complex question
  answering tasks. The key idea is to decompose complex questions into reasoning chains
  via systematic prompting and pre-designed actions that retrieve real-time information
  from heterogeneous sources like web text, domain knowledge, and tabular data.
---

# Chain-of-Action: Faithful and Multimodal Question Answering through Large Language Models

## Quick Facts
- arXiv ID: 2403.17359
- Source URL: https://arxiv.org/abs/2403.17359
- Reference count: 38
- One-line primary result: CoA framework achieves 6.14% improvement in tasks with information retrieval while reducing system latency through knowledge boundary detection

## Executive Summary
This paper introduces Chain-of-Action (CoA), a framework that enhances large language model (LLM) question answering by decomposing complex queries into systematic reasoning chains with three types of retrieval actions (web-querying, knowledge-encoding, data-analyzing). The framework uses a multi-reference faith score (MRFS) to verify and correct LLM-generated answers against retrieved information, ensuring faithfulness. By detecting LLM knowledge boundaries, CoA reduces unnecessary retrieval operations, improving efficiency. Experiments on public benchmarks and a Web3 case study demonstrate significant improvements in accuracy, efficiency, and resistance to misinformation compared to state-of-the-art baselines.

## Method Summary
The CoA framework employs in-context learning to generate action chains where each node contains sub-questions, missing flags, and LLM-generated guess answers. Three retrieval modules address multimodal information needs: web-querying for text data, knowledge-encoding for vector-based retrieval, and data-analyzing for structured tabular data. The multi-reference faith score compares LLM answers with retrieved information using precision, recall, and word length metrics to verify faithfulness. Knowledge boundary detection identifies when LLM internal knowledge suffices, avoiding unnecessary retrieval. The framework synthesizes verified answers into coherent final responses.

## Key Results
- 6.14% improvement in accuracy for tasks requiring information retrieval
- Reduced system latency through effective knowledge boundary detection
- Enhanced resistance to misinformation through MRFS verification mechanism
- Successful real-world deployment in Web3 QA application

## Why This Works (Mechanism)

## Mechanism 1
- **Claim:** Decomposing complex questions into reasoning chains via systematic prompting improves accuracy.
- **Mechanism:** The CoA framework uses in-context learning to prompt LLM to generate an action chain where each node contains a sub-question, missing flag, and initial guess answer. This decomposition allows the model to handle complex questions step-by-step rather than attempting to answer in one pass.
- **Core assumption:** LLMs can effectively break down complex questions when given appropriate prompting templates and can maintain coherence across the reasoning chain.
- **Evidence anchors:**
  - [abstract]: "Our key contribution is a novel reasoning-retrieval mechanism that decomposes a complex question into a reasoning chain via systematic prompting and pre-designed actions."
  - [section]: "We use in-context learning to generate an action chain by LLM... Each action node represents four elements, including Actioni, the sub-questions Subi, the missing flag MFi, the guess answer from LLMs Ai"
  - [corpus]: Weak evidence - no direct corpus evidence for this specific mechanism, but related work on chain-of-thought prompting provides supporting context
- **Break condition:** If the prompting template fails to generate coherent sub-questions or if the LLM cannot maintain logical connections between nodes, the chain will break down.

## Mechanism 2
- **Claim:** Multi-reference faith score (MRFS) effectively resolves conflicts between LLM-generated answers and retrieved information.
- **Mechanism:** MRFS compares the LLM's guess answer against retrieved information using precision, recall, and average word length metrics. If the score falls below a threshold, the answer is corrected with retrieved content.
- **Core assumption:** The combination of precision, recall, and word length provides a reliable measure of faithfulness between generated and retrieved content.
- **Evidence anchors:**
  - [abstract]: "We also propose a multi-reference faith score (MRFS) to verify and resolve conflicts in the answers."
  - [section]: "After getting the MRFS through: M RF S = arg k max S(rk, Ai), we setup a threshold T to decide whether the answer Ai is faithful"
  - [corpus]: Weak evidence - no direct corpus evidence for MRFS, but related work on ROUGE and faithfulness metrics provides context
- **Break condition:** If the threshold is set too high, few corrections will occur; if too low, excessive corrections may degrade coherence.

## Mechanism 3
- **Claim:** Detecting LLM knowledge boundaries reduces system latency and token usage.
- **Mechanism:** By recognizing when the LLM has sufficient internal knowledge to answer sub-questions without retrieval, the system avoids unnecessary external queries and summarization steps.
- **Core assumption:** LLMs with extensive parameters have acquired comprehensive knowledge during training that can be leveraged to answer certain sub-questions directly.
- **Evidence anchors:**
  - [abstract]: "In addition, our system demonstrates that detecting the knowledge boundaries of LLMs can significantly reduce both system latency and LLM usage in QA tasks."
  - [section]: "Since LLMs with extensive parameters have undergone costly training, leveraging this comprehensive knowledge can minimize unnecessary efforts during the filtering and summarization phases."
  - [corpus]: Weak evidence - no direct corpus evidence for knowledge boundary detection, but related work on knowledge probing provides context
- **Break condition:** If knowledge boundary detection is inaccurate, the system may either over-rely on internal knowledge (missing important updates) or over-query external sources (increasing latency).

## Foundational Learning

- **Concept:** Chain-of-thought prompting
  - **Why needed here:** CoA builds on CoT principles but extends them with retrieval and action modules. Understanding CoT is essential for grasping how CoA decomposes questions.
  - **Quick check question:** How does chain-of-thought prompting differ from standard prompting in terms of question complexity handling?

- **Concept:** Information retrieval and RAG systems
  - **Why needed here:** CoA integrates three types of retrieval actions (web-querying, knowledge-encoding, data-analyzing). Understanding RAG fundamentals is crucial for implementing the action modules.
  - **Quick check question:** What are the key differences between vector-based retrieval (knowledge-encoding) and keyword-based retrieval (web-querying)?

- **Concept:** Faithfulness metrics and evaluation
  - **Why needed here:** MRFS is central to CoA's conflict resolution mechanism. Understanding faithfulness metrics helps in implementing and tuning the verification module.
  - **Quick check question:** How does MRFS extend traditional ROUGE metrics to better handle answer verification?

## Architecture Onboarding

- **Component map:** User Question → Prompt Generator → Action Chain Generator → Action Executors (Web-querying, Knowledge-encoding, Data-analyzing) → MRFS Verifier → Answer Synthesizer → Final Answer
- **Critical path:** User Question → Prompt Generator → Action Chain Generator → MRFS Verifier → Answer Synthesizer → Final Answer
  - The most time-sensitive path involves generating the action chain and verifying answers against retrieved information
- **Design tradeoffs:**
  - Depth vs. breadth in action chain generation (more sub-questions increase accuracy but also complexity)
  - Threshold tuning for MRFS (higher thresholds increase faithfulness but may reduce efficiency)
  - Knowledge boundary detection sensitivity (too sensitive may miss important updates, not sensitive enough increases latency)
- **Failure signatures:**
  - Excessive action chains with many sub-questions may indicate poor prompt template design
  - Low MRFS scores across all answers suggest threshold may be too high or retrieval quality is poor
  - High latency despite knowledge boundary detection may indicate incorrect implementation of boundary detection logic
- **First 3 experiments:**
  1. **Basic functionality test:** Implement a simple CoA system with only web-querying action and test on a straightforward question to verify the core chain-of-action mechanism works
  2. **MRFS tuning experiment:** Test different MRFS thresholds on a dataset with known conflicts to find the optimal balance between correction and efficiency
  3. **Knowledge boundary detection validation:** Create questions where some sub-questions can be answered from LLM knowledge and others require retrieval, then verify the system correctly identifies which to answer internally vs. externally

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed CoA framework scale when integrating additional modalities such as images or audio, and what are the potential bottlenecks in such extensions?
- Basis in paper: [inferred] The paper mentions the extensibility to diverse modalities, e.g., images in the future, but does not provide empirical results or detailed analysis on the integration of additional modalities.
- Why unresolved: The current framework focuses on text and tabular data, and while the authors mention future work involving Vision Language Models, there is no concrete evidence or experiments demonstrating the scalability and performance of the framework with additional modalities.
- What evidence would resolve it: Conducting experiments that integrate image or audio data into the CoA framework and comparing its performance and efficiency with existing multimodal models would provide insights into its scalability and potential bottlenecks.

### Open Question 2
- Question: What are the specific challenges and limitations of the MRFS (Multi-Reference Faith Score) when applied to domains with highly subjective or ambiguous information, such as news sentiment analysis?
- Basis in paper: [explicit] The paper introduces the MRFS to verify and resolve conflicts in answers but does not discuss its performance in domains with subjective or ambiguous information.
- Why unresolved: The MRFS is designed to handle conflicts between generated answers and retrieved information, but its effectiveness in domains where the information is subjective or lacks clear ground truth is not addressed.
- What evidence would resolve it: Testing the MRFS in scenarios involving subjective information, such as news sentiment analysis, and comparing its accuracy and reliability with other evaluation metrics would highlight its strengths and limitations in such contexts.

### Open Question 3
- Question: How does the CoA framework handle dynamic and rapidly changing information sources, such as real-time cryptocurrency market data, and what are the implications for its accuracy and reliability?
- Basis in paper: [explicit] The paper mentions the application of CoA in a Web3 QA system and its ability to retrieve real-time information, but does not provide a detailed analysis of its performance in dynamic environments.
- Why unresolved: While the framework is designed to handle real-time information retrieval, the paper does not explore how it adapts to rapidly changing data sources or the impact on its accuracy and reliability.
- What evidence would resolve it: Evaluating the CoA framework's performance in real-time scenarios, such as tracking cryptocurrency prices or news updates, and comparing it with other real-time data processing systems would provide insights into its adaptability and reliability.

## Limitations
- Prompt templates for action chain generation are not specified, making faithful reproduction uncertain
- MRFS threshold determination method is not detailed, affecting faithfulness vs. efficiency tradeoff
- Knowledge boundary detection mechanism lacks specification and validation

## Confidence
- **High Confidence:** Core concept of decomposing complex questions into reasoning chains
- **Medium Confidence:** Multi-reference faith score mechanism effectiveness
- **Low Confidence:** Knowledge boundary detection claims and implementation details

## Next Checks
1. **Prompt Template Validation:** Implement and test multiple variations of the action chain generation prompts to determine which template designs most effectively decompose complex questions while maintaining coherence across the reasoning chain.

2. **MRFS Sensitivity Analysis:** Systematically vary the MRFS threshold T across a range of values and measure the tradeoff between faithfulness (reduction in misinformation) and efficiency (token usage and latency) to identify optimal operating points.

3. **Knowledge Boundary Detection Accuracy:** Create a controlled test set with questions where ground truth exists for which sub-questions require retrieval versus which can be answered from LLM knowledge, then measure the accuracy of the knowledge boundary detection mechanism.