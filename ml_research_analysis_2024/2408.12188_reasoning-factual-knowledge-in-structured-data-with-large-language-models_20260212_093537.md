---
ver: rpa2
title: Reasoning Factual Knowledge in Structured Data with Large Language Models
arxiv_id: '2408.12188'
source_url: https://arxiv.org/abs/2408.12188
tags:
- data
- llms
- structured
- factual
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'StructFact is a benchmark for evaluating large language models''
  ability to reason with factual knowledge in structured data. The benchmark contains
  8,340 questions across five tasks: Arithmetic Calculation, Spatiotemporal Cognition,
  Multi-hop Reasoning, Composition Understanding, and Combining Structured and Unstructured
  data.'
---

# Reasoning Factual Knowledge in Structured Data with Large Language Models

## Quick Facts
- arXiv ID: 2408.12188
- Source URL: https://arxiv.org/abs/2408.12188
- Reference count: 40
- LLMs struggle with heterogeneous data in structured facts, particularly in complex arithmetic and spatiotemporal tasks.

## Executive Summary
StructFact is a benchmark designed to evaluate large language models' ability to reason with factual knowledge in structured data. It contains 8,340 questions across five tasks, testing models on challenges unique to structured data such as heterogeneity, order invariance, and sparsity. Experiments with 10 LLMs reveal significant performance gaps, especially in tasks involving heterogeneous data types and complex reasoning, highlighting the need for improved techniques to enhance LLM comprehension and utilization of structured data.

## Method Summary
The study evaluates LLMs using the StructFact benchmark, which includes 8,340 questions across five tasks. The experiments involve 10 LLMs tested with various prompting strategies, including zero-shot and few-shot approaches with and without chain-of-thought. The structured data and questions are preprocessed and formatted for LLM input, and performance is measured using accuracy and weighted F1 scores.

## Key Results
- LLMs struggle with heterogeneous data, particularly in complex arithmetic operations and spatiotemporal cognition.
- Performance drops significantly when structured evidence is absent, indicating reliance on explicit context.
- The benchmark highlights the need for improved techniques to help LLMs better comprehend and utilize structured data for reliable reasoning.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs struggle with heterogeneous data in structured facts because their transformer-based architecture is optimized for sequential text, not multi-modal value types.
- Mechanism: The pretraining objective of next-word prediction assumes homogeneous token streams, so numerical, temporal, and categorical values are tokenized without explicit type awareness. When these types are interleaved in a table, the model cannot easily align reasoning across type boundaries.
- Core assumption: The heterogeneity challenge arises primarily from the lack of explicit type representation in the pretraining objective and architecture.
- Evidence anchors:
  - [abstract] "This difference can introduce imperceptible inference parameter deviations, posing challenges for LLMs in effectively utilizing and reasoning with structured data to accurately infer factual knowledge."
  - [section] "Most LLMs are based on the Transformer architecture (2017) and are trained with a next-word prediction loss objective, primarily designed to process continuous text data."
  - [corpus] Weak—related work focuses on KG-LLM integration, not heterogeneity in raw structured data.
- Break condition: If the model is explicitly trained or fine-tuned with type-aware objectives or structured input representations (e.g., explicit schema tokens or type embeddings).

### Mechanism 2
- Claim: LLMs underperform on order-invariant structured reasoning tasks because they over-rely on token position for semantic meaning.
- Mechanism: Transformer positional encodings encode sequence order as a key semantic cue. In structured data, the order of rows/columns is arbitrary, but the model may still condition predictions on positional embeddings, leading to spurious dependencies.
- Core assumption: Positional encodings in transformers create a bias toward treating order as semantically meaningful, even when it is not.
- Evidence anchors:
  - [section] "However, in structured data, the permutation of entities (e.g., rows in tables) does not alter the underlying factual knowledge."
  - [section] "We exploit the order invariance property of structured data to introduce semantically irrelevant interventions by shuffling the rows and columns in tables."
  - [corpus] Weak—no direct mention of positional encoding effects on structured data.
- Break condition: If the model uses position-invariant encodings for structured inputs (e.g., row/column ID tokens instead of relative positional embeddings).

### Mechanism 3
- Claim: LLMs' reasoning resilience drops when structured evidence is absent because they cannot reliably reconstruct tabular facts from parametric memory alone.
- Mechanism: Tabular data contains dense relational facts that are sparsely represented in the model's weights. Without explicit structural context, the model must rely on diffuse parametric memory, leading to lower precision and higher NEI responses.
- Core assumption: The parametric representation of structured facts is sparser and less accessible than unstructured text facts.
- Evidence anchors:
  - [section] "The dramatic drop in Arithmetic Calculation is attributed to the loss of numerical information in structured data."
  - [section] "The decline in Composition Understanding highlights that the LLM struggles to grasp complex structures and effectively utilize general knowledge."
  - [corpus] Weak—related work discusses retrieval augmentation but not parametric memory sparsity for structured data.
- Break condition: If retrieval-augmented generation (RAG) or explicit knowledge retrieval is used to supplement parametric memory.

## Foundational Learning

- Concept: Data heterogeneity in structured facts (text, numeric, date, categorical).
  - Why needed here: Different value types require different reasoning strategies; misunderstanding one type propagates errors across the fact.
  - Quick check question: Given a table with a date, a number, and a category, can you identify which reasoning step (comparison, lookup, or inference) applies to each type?

- Concept: Order invariance in structured data.
  - Why needed here: The model must learn that row/column permutations do not change the underlying fact; failure here leads to spurious dependencies.
  - Quick check question: If you shuffle the rows of a table, does the answer to "Which country has the highest GDP?" change?

- Concept: Parametric vs. explicit memory for structured facts.
  - Why needed here: Structured facts are often not as densely encoded in model weights as unstructured text; explicit context is critical for accuracy.
  - Quick check question: If you remove the table from a question, does the model's confidence and accuracy drop significantly?

## Architecture Onboarding

- Component map: Input preprocessor (tokenization + type tagging) -> LLM core (transformer) -> Output decoder (label prediction). Optional retrieval module for structured context.
- Critical path: Structured data -> type-aware tokenization -> LLM inference -> answer selection. Bottleneck is tokenization + type alignment.
- Design tradeoffs: Type tagging improves reasoning accuracy but increases input length and token cost; shuffling invariance testing trades off evaluation fidelity for robustness.
- Failure signatures: High NEI rate when evidence absent; degraded performance on numerical tasks; sensitivity to row/column order in tables.
- First 3 experiments:
  1. Add explicit type tokens (e.g., `<NUM>`, `<DATE>`) to structured input and measure impact on heterogeneous tasks.
  2. Replace positional encodings with row/column ID embeddings and test order invariance.
  3. Implement a lightweight retrieval module to supply structured context and compare resilience to evidence-absent scenarios.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be improved to better handle heterogeneous data in structured formats, especially for complex arithmetic operations?
- Basis in paper: Explicit. The paper identifies that LLMs struggle with heterogeneous data, particularly in complex arithmetic operations, and suggests that current models heavily depend on the order of information.
- Why unresolved: The paper does not provide specific techniques or methodologies to enhance LLMs' ability to process heterogeneous data beyond suggesting the need for improved techniques.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of new architectures or training methods that significantly improve LLMs' performance on arithmetic calculations involving heterogeneous data.

### Open Question 2
- Question: What are the most effective strategies for LLMs to accurately utilize their knowledge base when reasoning over sparse structured data?
- Basis in paper: Explicit. The paper notes that LLMs face challenges in accurately utilizing their knowledge base to comprehend and reason over sparse structured data, as evidenced by the decline in performance when evidence is absent.
- Why unresolved: The paper highlights the problem but does not propose or test specific strategies to mitigate the issue.
- What evidence would resolve it: Comparative studies showing improved performance on sparse data tasks using novel prompting strategies, retrieval-augmented generation, or other techniques designed to enhance knowledge utilization.

### Open Question 3
- Question: How can LLMs be trained or fine-tuned to better understand and reason about spatiotemporal knowledge in structured data?
- Basis in paper: Explicit. The paper finds that LLMs exhibit inadequate performance in the Spatiotemporal Cognition task, indicating difficulties in interpreting and aligning diverse formats of temporal and spatial data.
- Why unresolved: The paper does not explore or suggest specific methods for improving spatiotemporal cognition in LLMs.
- What evidence would resolve it: Development and evaluation of models or training approaches that significantly improve performance on tasks requiring the interpretation and reasoning of spatiotemporal information in structured data.

## Limitations

- The benchmark relies on English-only datasets and Wikipedia-derived facts, limiting generalizability to multilingual or domain-specific data.
- The study does not account for variations in structured data formats (e.g., JSON, XML) beyond tables.
- Performance gaps may be influenced by training data biases or prompting strategies, not just architectural limitations.

## Confidence

- **High confidence:** The benchmark design and task categorization are well-founded, as evidenced by the clear distinction between structured and unstructured data challenges and the systematic evaluation across 10 LLMs.
- **Medium confidence:** The mechanisms explaining LLM struggles (heterogeneity, order invariance, parametric memory sparsity) are plausible but lack direct experimental validation within the paper.
- **Low confidence:** The generalizability of findings to non-English or domain-specific structured data remains untested.

## Next Checks

1. Test model performance on StructFact tasks using multilingual structured data to assess cross-lingual reasoning capabilities.
2. Evaluate the impact of explicit type embeddings (e.g., `<NUM>`, `<DATE>`) on heterogeneous data tasks to validate Mechanism 1.
3. Compare performance when using row/column ID tokens instead of positional encodings to test order invariance (Mechanism 2).