---
ver: rpa2
title: 'Claim Check-Worthiness Detection: How Well do LLMs Grasp Annotation Guidelines?'
arxiv_id: '2404.12174'
source_url: https://arxiv.org/abs/2404.12174
tags:
- claim
- factual
- information
- level
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the performance of large language models (LLMs)
  for claim detection (CD) and check-worthiness detection (CW) tasks using zero- and
  few-shot prompting. The study investigates how best to distill factuality and worthiness
  criteria from annotation guidelines into prompts and what amount of context to provide
  for each claim.
---

# Claim Check-Worthiness Detection: How Well do LLMs Grasp Annotation Guidelines?

## Quick Facts
- arXiv ID: 2404.12174
- Source URL: https://arxiv.org/abs/2404.12174
- Reference count: 13
- Primary result: Optimal prompt verbosity is domain-dependent for claim check-worthiness detection

## Executive Summary
This paper investigates how well large language models (LLMs) can perform claim check-worthiness detection (CWD) and claim detection (CD) tasks using zero- and few-shot prompting. The study systematically explores the impact of prompt verbosity and contextual information on model performance across five diverse datasets. Experiments reveal that optimal prompt verbosity varies by domain, adding contextual information does not improve performance, and LLM confidence scores can be directly used for reliable check-worthiness rankings.

## Method Summary
The study evaluates LLMs on five claim check-worthiness datasets using zero- and few-shot prompting. The authors vary prompt verbosity levels (V0-V3) and context levels (C0-C3) with gpt-3.5-turbo and gpt-4-turbo. Performance is measured using F1 scores for classification accuracy, expected calibration error (ECE) for calibration accuracy, and average precision/P@10/P@R for ranking performance. The datasets span diverse domains including political debates, environmental claims, news articles, and parliamentary speeches.

## Key Results
- Optimal prompt verbosity is domain-dependent, with different verbosity levels performing best on different datasets
- Adding co-text and speaker information does not improve LLM performance for claim detection
- LLM confidence scores can be directly used to produce reliable check-worthiness rankings with high average precision

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt verbosity level impacts LLM accuracy differently across domains
- Mechanism: LLM performance depends on the balance between providing enough specificity from annotation guidelines and avoiding over-complication that confuses the model
- Core assumption: Domain-specific worthiness criteria require different amounts of prompt detail to be properly conveyed
- Evidence anchors:
  - [abstract] "optimal prompt verbosity is domain-dependent"
  - [section] "The optimal verbosity level is not consistent across datasets: the performance increases with verbosity levels for CB, but the trend is reversed for ENV"

### Mechanism 2
- Claim: Adding situational context (co-text, speaker info) does not improve LLM performance for claim detection
- Mechanism: LLMs can infer necessary context from the claim itself without explicit contextual information in the prompt
- Core assumption: The model's internal knowledge is sufficient to understand claims without additional context
- Evidence anchors:
  - [abstract] "adding context does not improve performance"
  - [section] "expanding the prompt with co-text and speaker information did not improve the model's accuracy on either dataset"

### Mechanism 3
- Claim: LLM confidence scores can be directly used for reliable check-worthiness rankings
- Mechanism: Models with high predictive accuracy produce well-calibrated confidence scores that correlate with true check-worthiness
- Core assumption: High predictive accuracy implies good calibration of confidence scores
- Evidence anchors:
  - [abstract] "confidence scores can be directly used to produce reliable check-worthiness rankings"
  - [section] "The rank-based performance scores mirror the classification accuracy scores: they are high for datasets with high predictive accuracy"

## Foundational Learning

- Concept: Zero-shot vs. few-shot prompting
  - Why needed here: Understanding the difference between using internal model knowledge vs. providing examples
  - Quick check question: What is the main difference between zero-shot and few-shot prompting?

- Concept: Calibration and expected calibration error (ECE)
  - Why needed here: To understand how well confidence scores reflect true probabilities for ranking tasks
  - Quick check question: What does ECE measure in the context of LLM predictions?

- Concept: Domain-specific annotation criteria
  - Why needed here: To understand how different factuality and worthiness criteria affect model performance
  - Quick check question: Why might the same prompt verbosity level work differently across domains?

## Architecture Onboarding

- Component map: LLM inference system -> Prompt generation -> Context expansion -> Confidence extraction -> Ranking evaluation
- Critical path: Prompt generation -> LLM inference -> Confidence extraction -> Ranking evaluation
- Design tradeoffs: Verbosity level vs. model confusion; Context inclusion vs. prompt clarity
- Failure signatures: Inconsistent performance across domains; Poor calibration; Low confidence in positive cases
- First 3 experiments:
  1. Test different verbosity levels (V0-V3) on a single dataset to find optimal prompt detail
  2. Compare performance with and without context expansion on datasets with available context
  3. Evaluate ranking performance using confidence scores vs. ground truth labels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompt verbosity levels affect model performance across diverse factuality and worthiness criteria domains?
- Basis in paper: [explicit] The paper investigates how varying prompt verbosity (V0-V3) affects LLM performance across five datasets with different criteria
- Why unresolved: While the paper shows optimal verbosity is domain-dependent, it doesn't explain why certain verbosity levels work better for specific domains or criteria
- What evidence would resolve it: Comparative analysis of model performance across domains with different annotation guidelines, explaining why certain verbosity levels succeed/fail

### Open Question 2
- Question: Does adding contextual information (co-text and speaker details) consistently improve claim detection and check-worthiness detection performance?
- Basis in paper: [explicit] The paper finds that adding context does not improve performance on CB and POLI datasets, contrary to expectations
- Why unresolved: The study only tested two datasets with limited context types, and the reasons for lack of improvement are unclear
- What evidence would resolve it: Systematic testing of various context types (co-text, speaker info, metadata) across multiple datasets with different claim types

### Open Question 3
- Question: Can LLM confidence scores reliably serve as a proxy for check-worthiness prioritization across different domains and criteria?
- Basis in paper: [explicit] The paper suggests using LLM confidence scores for ranking but only tests on four datasets
- Why unresolved: The study doesn't explore whether this approach generalizes to other domains or criteria, or how calibration varies
- What evidence would resolve it: Extensive testing across diverse datasets with different worthiness criteria, comparing LLM rankings to human expert prioritization

## Limitations
- Evaluation limited to English datasets only
- Only two LLM variants (gpt-3.5-turbo and gpt-4-turbo) were tested
- Datasets represent a limited range of domains and claim types
- Focus on zero- and few-shot prompting without exploring fine-tuning approaches

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Optimal prompt verbosity is domain-dependent | High |
| Adding context doesn't improve performance | Medium |
| Confidence scores can be directly used for ranking | Medium |

## Next Checks

1. Test the prompt verbosity findings across additional diverse domains and languages to verify the domain-dependency pattern
2. Evaluate whether context effects vary when using different types of contextual information (e.g., temporal vs. speaker context)
3. Assess the calibration quality of confidence scores across multiple LLM models to confirm the generalizability of using confidence for ranking