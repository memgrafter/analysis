---
ver: rpa2
title: 'Explaining Decisions in ML Models: a Parameterized Complexity Analysis'
arxiv_id: '2407.15780'
source_url: https://arxiv.org/abs/2407.15780
tags:
- every
- size
- example
- classi
- only
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive theoretical investigation into
  the parameterized complexity of explanation problems in various machine learning
  (ML) models. Contrary to the prevalent black-box perception, our study focuses on
  models with transparent internal mechanisms.
---

# Explaining Decisions in ML Models: a Parameterized Complexity Analysis

## Quick Facts
- arXiv ID: 2407.15780
- Source URL: https://arxiv.org/abs/2407.15780
- Reference count: 32
- Primary result: Theoretical analysis of explanation complexity across multiple ML models

## Executive Summary
This paper presents a comprehensive theoretical investigation into the parameterized complexity of explanation problems in various machine learning models. The study focuses on models with transparent internal mechanisms, analyzing abductive and contrastive explanations in both local and global variants. The research encompasses Decision Trees, Decision Sets, Decision Lists, Ordered Binary Decision Diagrams, Random Forests, and Boolean Circuits, providing foundational insights into the complexities of generating explanations for these models.

## Method Summary
The paper employs parameterized complexity theory to analyze explanation problems across different ML models. The approach involves reducing explanation problems to well-known computational problems, developing specialized algorithms, and applying results from Boolean circuits domain. The analysis systematically examines local and global variants of abductive and contrastive explanations for each model type, establishing complexity results through formal proofs and reductions.

## Key Results
- Establishes parameterized complexity results for explanation problems across multiple ML models
- Demonstrates that local abductive explanations are typically easier than global or contrastive variants
- Identifies models and explanation types where explanation generation becomes computationally intractable
- Provides complexity boundaries that guide practical implementation of explainable AI systems

## Why This Works (Mechanism)
The theoretical framework works by decomposing explanation problems into fundamental computational components that can be analyzed using parameterized complexity tools. By mapping explanation generation to established complexity classes and problems, the analysis reveals inherent computational boundaries that govern when and how explanations can be efficiently computed.

## Foundational Learning
1. Parameterized Complexity Theory - why needed: Provides framework for analyzing computational problems with multiple parameters; quick check: understand difference between FPT and W[1]-hard problems
2. Abductive vs Contrastive Explanations - why needed: Different explanation types have different computational requirements; quick check: can distinguish between explaining why an example is classified vs why it's classified as one class over another
3. Local vs Global Explanations - why needed: Scope of explanation affects computational complexity; quick check: understand difference between explaining single instance vs entire model behavior
4. Reduction Techniques - why needed: Used to establish complexity results by mapping problems to known hard problems; quick check: can follow reduction proofs in the paper
5. ML Model Transparency - why needed: Models must have inspectable internal mechanisms for explanation generation; quick check: can identify decision boundaries in Decision Trees
6. Ensemble Methods Complexity - why needed: Combining multiple models increases explanation complexity; quick check: understand how Random Forests aggregate explanations

## Architecture Onboarding

**Component Map:**
ML Model -> Explanation Type -> Complexity Class -> Algorithmic Approach

**Critical Path:**
1. Select ML model and explanation type
2. Apply parameterized complexity analysis
3. Determine computational complexity class
4. Design appropriate algorithm or approximation method

**Design Tradeoffs:**
- Model complexity vs explanation tractability
- Explanation specificity vs computational cost
- Exact vs approximate explanation methods
- Local vs global explanation scope

**Failure Signatures:**
- Incorrect complexity classification
- Overlooking model-specific constraints
- Misapplication of reduction techniques
- Ignoring practical implementation limitations

**3 First Experiments:**
1. Analyze Decision Tree local abductive explanation complexity
2. Compare complexity of Decision Set vs Decision List explanations
3. Test parameterized algorithm performance against theoretical bounds

## Open Questions the Paper Calls Out
None specified in the provided material.

## Limitations
- Theoretical analysis may not fully capture practical implementation challenges
- Some complexity results for complex models like Random Forests remain open questions
- Analysis assumes idealized conditions without implementation-specific factors
- Global explanation problems for most complex models lack definitive theoretical results

## Confidence

**Major Limitations and Uncertainties:**
- Theoretical approach may not fully capture practical challenges
- Some complexity results for complex models remain open questions
- Analysis assumes idealized conditions

**Confidence Assessment:**
- High confidence: Simpler models (Decision Trees, Decision Sets) and basic explanation types
- Medium confidence: More complex models (Decision Lists, OBDD) and explanation types
- Low confidence: Most complex models (Random Forests, Boolean Circuits) and their explanation problems

## Next Checks
1. Implement and benchmark explanation algorithms for Decision Trees and Decision Sets to validate theoretical complexity against practical performance
2. Conduct literature review to identify recent advancements in parameterized complexity of explanation problems, particularly for Random Forests
3. Develop and analyze approximation algorithms for global explanation problems where exact solutions are computationally infeasible