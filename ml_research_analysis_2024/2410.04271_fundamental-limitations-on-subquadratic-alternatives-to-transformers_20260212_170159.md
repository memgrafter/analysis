---
ver: rpa2
title: Fundamental Limitations on Subquadratic Alternatives to Transformers
arxiv_id: '2410.04271'
source_url: https://arxiv.org/abs/2410.04271
tags:
- time
- such
- attention
- exists
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proves fundamental limitations on subquadratic alternatives
  to transformers by establishing that document similarity tasks require quadratic
  time under standard complexity assumptions. Specifically, the authors show that
  finding the most/least similar pair of documents (MSD/LSD) and their variants cannot
  be solved in truly subquadratic time assuming the Strong Exponential Time Hypothesis
  (SETH).
---

# Fundamental Limitations on Subquadratic Alternatives to Transformers

## Quick Facts
- **arXiv ID:** 2410.04271
- **Source URL:** https://arxiv.org/abs/2410.04271
- **Reference count:** 40
- **Primary result:** Document similarity tasks require quadratic time under SETH, proving subquadratic alternatives to transformers cannot solve these tasks without sacrificing accuracy

## Executive Summary
This paper establishes fundamental computational limits on subquadratic alternatives to transformers by proving that document similarity tasks inherently require quadratic time under the Strong Exponential Time Hypothesis (SETH). The authors demonstrate that finding the most/least similar document pairs cannot be solved in truly subquadratic time, creating a computational barrier that any attention approximation or replacement technique cannot overcome. Complementing this hardness result, they show that standard transformers with a single attention layer and simple MLPs can solve these problems, establishing a clear separation between transformers and their subquadratic alternatives.

## Method Summary
The paper uses fine-grained complexity theory to prove quadratic lower bounds for document similarity tasks by reducing them from the Orthogonal Vectors problem, which is known to be SETH-hard. The authors construct specific transformer architectures with carefully designed query/key matrices that can solve these hard problems, while proving that any subquadratic algorithm would violate SETH. The analysis covers standard document similarity tasks (MSD/LSD) as well as their bichromatic variants, establishing that even approximate solutions require quadratic time.

## Key Results
- Document similarity tasks (MSD/LSD) require quadratic time under SETH, creating fundamental limits on subquadratic alternatives
- Standard transformers with a single attention layer and simple MLPs can solve these problems, establishing a separation from subquadratic approaches
- Bichromatic variants of document similarity problems are even harder, requiring larger embedding dimensions (log n vs log log n)
- Heuristics for attention, faster attention replacements like Mamba, or completely different architectures cannot avoid the quadratic bottleneck for these tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Transformers with a single attention layer and simple MLPs can solve document similarity tasks (MSD/LSD) that require quadratic time under SETH.
- **Mechanism:** The attention mechanism with carefully chosen query/key matrices (e.g., QK⊤ = -3 log n · Iℓ) can amplify signals when cosine similarity is zero (orthogonal vectors) and suppress them otherwise, enabling discrimination between similar and dissimilar document pairs.
- **Core assumption:** The document embeddings are binary vectors and the cosine similarity threshold can be encoded through matrix design.
- **Evidence anchors:**
  - [abstract]: "standard transformers with a single attention layer and simple MLPs can solve these problems, establishing a separation between transformers and subquadratic alternatives"
  - [section]: Theorem 4.1 proves that a single attention unit with input and output MLPs can solve OVn,ℓ, MSDn,ℓ,t, and LSDn,ℓ,t
  - [corpus]: Weak evidence - corpus mentions "Subquadratic Algorithms and Hardness for Attention" but doesn't directly address transformer representational strength
- **Break condition:** If document embeddings are not binary vectors or the similarity measure cannot be approximated by the attention mechanism's output.

### Mechanism 2
- **Claim:** Any subquadratic algorithm (heuristics, attention replacements, or different architectures) cannot solve document similarity tasks without sacrificing accuracy.
- **Mechanism:** The Strong Exponential Time Hypothesis (SETH) implies that finding the most/least similar pair of documents (MSD/LSD) requires quadratic time, creating a computational barrier that subquadratic approaches cannot overcome.
- **Core assumption:** SETH holds and document similarity problems are at least as hard as the Orthogonal Vectors problem.
- **Evidence anchors:**
  - [abstract]: "any such approach cannot perform important tasks that Transformer is able to perform (assuming a popular conjecture from fine-grained complexity theory)"
  - [section]: Theorem 3.1 and Theorem 3.3 prove quadratic time lower bounds for LSD, MSD and their variants assuming SETH
  - [corpus]: Corpus evidence is weak - no direct mention of SETH or its implications for document similarity
- **Break condition:** If SETH is false or document similarity problems have subquadratic algorithms that don't reduce to Orthogonal Vectors.

### Mechanism 3
- **Claim:** Bichromatic variants of document similarity problems (finding similar pairs across two document sets) are even harder, requiring larger embedding dimensions.
- **Mechanism:** The hardness reduction from SETH to bichromatic problems requires larger dimensions (log n vs log log n), making these variants computationally harder.
- **Core assumption:** The reduction from Orthogonal Vectors to bichromatic problems preserves the quadratic time lower bound.
- **Evidence anchors:**
  - [abstract]: "we prove that Transformer is able to perform this task, and we prove that this task cannot be performed in truly subquadratic time by any algorithm"
  - [section]: Theorem 1.3 shows bichromatic MSD requires O(n^2-ε) time when ℓ = c log n
  - [corpus]: No direct corpus evidence - this is a novel theoretical contribution
- **Break condition:** If the reduction from Orthogonal Vectors to bichromatic problems can be made more efficient or if bichromatic problems have special structure allowing subquadratic solutions.

## Foundational Learning

- **Concept:** Fine-grained complexity theory and the Strong Exponential Time Hypothesis (SETH)
  - **Why needed here:** The paper's main results rely on proving quadratic time lower bounds under SETH, which is fundamental to understanding why subquadratic alternatives cannot solve certain tasks
  - **Quick check question:** What does SETH imply about the complexity of k-SAT problems, and how does this relate to the Orthogonal Vectors Conjecture?

- **Concept:** Cosine similarity and document embedding representations
  - **Why needed here:** The paper focuses on document similarity tasks using bag-of-words embeddings and cosine similarity as the similarity measure, which is crucial for understanding the problems being analyzed
  - **Quick check question:** How does cosine similarity differ from inner product, and why is normalization important for document similarity?

- **Concept:** Transformer attention mechanism and its computational complexity
  - **Why needed here:** Understanding how attention works (computing correlations between all token pairs) and why it requires quadratic time is essential for grasping the paper's contributions
  - **Quick check question:** What is the computational complexity of standard attention, and how does the softmax operation contribute to this complexity?

## Architecture Onboarding

- **Component map:** Input binary document embeddings → MLP1 → Attention Unit (Q, K, V matrices) → MLP2 → Output similarity decision
- **Critical path:**
  1. Embed documents as binary vectors
  2. Apply first MLP to transform to attention-compatible format
  3. Compute attention using carefully designed Q, K, V matrices
  4. Apply second MLP to extract similarity information
  5. Output decision (similar/not similar or specific similarity value)
- **Design tradeoffs:**
  - Higher embedding dimension ℓ allows solving harder problems but increases computational cost
  - More complex MLPs could improve accuracy but may obscure the fundamental computational barriers
  - Different attention matrix designs (QK⊤ values) trade off between sensitivity to similarity and computational tractability
- **Failure signatures:**
  - If MLP1 cannot properly encode the document similarity information into the attention input space
  - If attention matrices cannot amplify the signal for similar pairs while suppressing dissimilar ones
  - If the output MLP cannot distinguish between the different attention output patterns
- **First 3 experiments:**
  1. Implement the Orthogonal Vectors problem solution using the provided QK⊤ = -3 log n · Iℓ construction and verify it correctly identifies orthogonal pairs
  2. Test the Max-IP/Min-IP solution with varying t thresholds to ensure correct classification of document pairs
  3. Benchmark the transformer solution against a brute-force quadratic algorithm on synthetic document similarity datasets to measure accuracy and runtime

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the quadratic lower bound for document similarity tasks be extended to more complex attention mechanisms beyond standard transformers?
- **Basis in paper:** [explicit] The paper establishes that standard transformers with a single attention layer can solve MSD/LSD problems, while subquadratic alternatives cannot due to computational complexity constraints.
- **Why unresolved:** The current analysis focuses on comparing standard transformers with subquadratic alternatives. It remains open whether more complex transformer architectures (e.g., deeper networks, more attention heads) would face similar limitations or could potentially overcome the quadratic barrier.
- **What evidence would resolve it:** Extending the reduction techniques to show hardness results for more complex transformer architectures, or demonstrating that additional architectural features could circumvent the quadratic complexity requirement.

### Open Question 2
- **Question:** How do these complexity limitations apply to practical large language models with different architectures (e.g., Mamba, Longformer)?
- **Basis in paper:** [explicit] The paper proves that any approach taking subquadratic time - including heuristics for attention, faster attention replacements like Mamba, or completely different architectures - cannot solve document similarity tasks.
- **Why unresolved:** While the theoretical results are established, the practical implications for specific models like Mamba or Longformer are not fully explored. The paper shows theoretical impossibility but doesn't quantify the accuracy loss in practice for different architectures.
- **What evidence would resolve it:** Empirical studies comparing different transformer alternatives on document similarity benchmarks, measuring both accuracy and runtime to validate the theoretical predictions.

### Open Question 3
- **Question:** Could alternative document embedding methods change the computational complexity landscape?
- **Basis in paper:** [explicit] The paper focuses on bag-of-words embeddings and cosine similarity, noting that results extend to "almost any reasonable alternatives."
- **Why unresolved:** The paper doesn't explore whether different embedding techniques (like contextual embeddings from BERT or other methods) could lead to different complexity results. The choice of embedding method might fundamentally change the computational requirements.
- **What evidence would resolve it:** Proving hardness results for document similarity with alternative embedding methods, or demonstrating cases where different embeddings lead to tractable solutions for document similarity tasks.

### Open Question 4
- **Question:** What are the implications for distributed or approximate algorithms that trade exactness for speed?
- **Basis in paper:** [explicit] The paper considers variants like γ-MSD/γ-LSD which allow approximate solutions, but establishes that even approximate solutions require quadratic time.
- **Why unresolved:** The analysis assumes exact computation of document similarity. It remains open whether distributed computing approaches or other approximation schemes could provide practical speedups while maintaining acceptable accuracy.
- **What evidence would resolve it:** Demonstrating distributed algorithms that achieve subquadratic time complexity for approximate document similarity with provable accuracy guarantees, or proving that no such algorithm exists.

## Limitations

- The results rely on SETH, which remains unproven and represents a fundamental assumption that could be false
- The transformer constructions use idealized binary vectors and precise matrix designs that may not be learnable through standard training procedures
- The analysis focuses on worst-case theoretical bounds rather than empirical performance on real-world datasets

## Confidence

- **High confidence:** The quadratic lower bounds for document similarity tasks under SETH (Theorems 3.1 and 3.3) are well-established within fine-grained complexity theory and follow standard reduction techniques
- **Medium confidence:** The transformer construction's ability to solve Orthogonal Vectors and Max-IP/Min-IP problems with single attention layers is theoretically sound but relies on idealized conditions (exact binary vectors, precise matrix constructions) that may not hold in practical settings
- **Low confidence:** The practical implications of these results for real-world transformer architectures and training procedures remain unclear, as the paper focuses on worst-case theoretical bounds rather than empirical performance

## Next Checks

1. **Empirical verification of transformer constructions:** Implement the theoretical transformer constructions (Theorem 4.1) on synthetic binary document datasets to verify they achieve the claimed performance in practice, not just in theory

2. **Relaxation of complexity assumptions:** Investigate whether the quadratic lower bounds still hold under weaker complexity assumptions than SETH, or whether specific problem structures allow for subquadratic algorithms in practice

3. **Practical attention approximation analysis:** Test whether common subquadratic attention approximations (like linear attention or kernel methods) can achieve reasonable accuracy on the theoretical problems defined in the paper, bridging the gap between theory and practice