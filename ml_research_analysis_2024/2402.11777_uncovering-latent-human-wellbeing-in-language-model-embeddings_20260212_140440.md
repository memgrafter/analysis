---
ver: rpa2
title: Uncovering Latent Human Wellbeing in Language Model Embeddings
arxiv_id: '2402.11777'
source_url: https://arxiv.org/abs/2402.11777
tags:
- language
- ethics
- accuracy
- arxiv
- paired
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) implicitly
  learn a concept of human wellbeing during pretraining. The authors examine the ETHICS
  Utilitarianism task, where models must judge which of two scenarios is more pleasant.
---

# Uncovering Latent Human Wellbeing in Language Model Embeddings

## Quick Facts
- arXiv ID: 2402.11777
- Source URL: https://arxiv.org/abs/2402.11777
- Authors: Pedro Freire; ChengCheng Tan; Adam Gleave; Dan Hendrycks; Scott Emmons
- Reference count: 20
- Primary result: Text-embedding-ada-002's leading PCA component achieves 73.9% accuracy on ETHICS Utilitarianism task without fine-tuning

## Executive Summary
This paper investigates whether large language models implicitly learn concepts of human wellbeing during pretraining. The authors examine the ETHICS Utilitarianism task, where models must judge which of two scenarios is more pleasant. Without any prompt engineering or fine-tuning, the leading principal component from OpenAI's text-embedding-ada-002 achieves 73.9% accuracy, closely matching BERT-large fine-tuned on the entire ETHICS dataset (74.6%). This suggests pretraining conveys some understanding about human wellbeing. The authors also find that performance is nondecreasing with increased model size when using sufficient principal components.

## Method Summary
The authors extract embeddings from various language models for scenarios in the ETHICS Utilitarianism dataset. They normalize embeddings, apply PCA for dimensionality reduction, and train logistic regression models to predict which scenario in each pair is more pleasant based on PCA projections. The approach is evaluated across multiple embedding sources including text-embedding-ada-002, DeBERTa, SentenceTransformers, GPT-3, and Cohere models. Performance is measured using ETHICS Utilitarianism test accuracy.

## Key Results
- Leading PCA component from text-embedding-ada-002 achieves 73.9% accuracy on ETHICS Utilitarianism task without fine-tuning
- Performance matches BERT-large fine-tuned on entire ETHICS dataset (74.6% accuracy)
- Accuracy is nondecreasing with model size when using sufficient principal components
- Results generalize across multiple embedding sources (text-embedding-ada-002, DeBERTa, SentenceTransformers, GPT-3, Cohere)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PCA on LLM embeddings isolates directions corresponding to ethical judgements
- Mechanism: The ETHICS Utilitarianism dataset labels pairwise comparisons of pleasantness, so the variance in embeddings along the ethical dimension is amplified. PCA's leading component captures this variance, isolating the "utility" direction
- Core assumption: Ethical judgements are encoded linearly in the embedding space
- Evidence anchors: [abstract] "the leading principal component from OpenAI's text-embedding-ada-002 achieves 73.9% accuracy"; [section] "We normalize the embeddings to zero mean and unit variance before applying PCA"
- Break condition: If ethical dimensions are nonlinearly encoded or entangled with other semantic dimensions, PCA may not isolate them

### Mechanism 2
- Claim: Pretraining on human-generated data transfers ethical understanding to downstream tasks
- Mechanism: LLMs trained on large text corpora learn representations of human concepts including wellbeing, without explicit fine-tuning. This implicit knowledge surfaces when embeddings are analyzed
- Core assumption: Human text data contains sufficient moral concepts for models to learn implicitly
- Evidence anchors: [abstract] "pretraining conveys some understanding about human wellbeing"; [section] "Our research explores the encoding of human wellbeing within Large Language Model (LLM) embeddings"
- Break condition: If pretraining data lacks diverse moral scenarios, models may not capture broad ethical concepts

### Mechanism 3
- Claim: Larger models better encode abstract human concepts
- Mechanism: Increased model capacity allows learning more nuanced relationships between concepts like pleasure and pain, improving the signal-to-noise ratio in the leading PCA component
- Core assumption: Model size correlates with capacity to encode abstract concepts
- Evidence anchors: [abstract] "performance is nondecreasing with increased model size when using sufficient numbers of principal components"; [section] "With 300 principal components, test accuracy within a model family is a nondecreasing function of model size"
- Break condition: If larger models overfit to pretraining data or if ethical concepts plateau in complexity

## Foundational Learning

- Concept: Principal Component Analysis (PCA)
  - Why needed here: PCA reduces high-dimensional embeddings to the most informative lower-dimensional subspace
  - Quick check question: How does PCA decide which dimensions to keep when reducing dimensionality?

- Concept: Logistic Regression
  - Why needed here: Logistic regression classifies scenario pairs based on their PCA projections to predict which is more pleasant
  - Quick check question: What role does the logistic regression threshold play in classification accuracy?

- Concept: Embedding Spaces
  - Why needed here: Embeddings represent scenarios in a continuous space where semantic relationships are preserved
  - Quick check question: How do embeddings capture semantic similarity between different scenarios?

## Architecture Onboarding

- Component map: Input scenarios -> Language model embeddings -> Normalization -> PCA -> Logistic regression -> Binary classification

- Critical path: Embed → Normalize → PCA → Logistic regression → Evaluate accuracy

- Design tradeoffs:
  - Number of PCA components vs. overfitting
  - Model size vs. computational cost
  - Prompt engineering vs. simplicity

- Failure signatures:
  - Random performance (~50%) indicates PCA components don't capture ethical variance
  - Degraded performance with more components suggests overfitting
  - Inconsistent results across prompts imply instability in embedding extraction

- First 3 experiments:
  1. Run PCA with 1 component on a small subset and verify variance explained
  2. Train logistic regression with varying numbers of components and plot accuracy
  3. Compare single vs paired mode classification accuracy on the same dataset split

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise scaling trend of performance with model size and number of principal components?
- Basis in paper: [explicit] "Defining this scaling trend precisely is an open question for future work."
- Why unresolved: The paper observes that performance is generally nondecreasing with model size when using sufficient principal components, but this trend depends on the prompt format and language model family, making it unclear how to precisely characterize the scaling relationship
- What evidence would resolve it: Systematic experiments varying model size and number of principal components across a wide range of language models and prompt formats, followed by rigorous statistical analysis to determine the scaling trend

### Open Question 2
- Question: How does the performance of supervised embedding extraction methods compare to unsupervised methods like PCA?
- Basis in paper: [inferred] The paper notes that they used only unsupervised embedding extraction via PCA, contrasting with BERT models that were finetuned using training labels. It suggests that experiments with both supervised and unsupervised methods could elucidate performance disparities
- Why unresolved: The paper does not directly compare supervised vs unsupervised embedding extraction methods, leaving the relative performance of these approaches unclear
- What evidence would resolve it: Experiments comparing the performance of supervised embedding extraction methods (e.g., supervised finetuning) against unsupervised methods (e.g., PCA) on the ETHICS Utilitarianism task across various language models

### Open Question 3
- Question: How well do LLM representations capture broader aspects of human wellbeing and ethical reasoning beyond the Utilitarianism subset of ETHICS?
- Basis in paper: [explicit] The paper notes that the ETHICS dataset presents a narrowed view of ethics and may not fully capture the complex nuances of human wellbeing or ethical reasoning. It suggests exploring the wider ETHICS dataset to assess broader aspects of human ethics
- Why unresolved: The paper's experiments focused on the Utilitarianism subset, which only addresses pleasure and pain judgments. The extent to which LLM representations capture other ethical frameworks and wellbeing considerations remains unclear
- What evidence would resolve it: Experiments evaluating LLM representations on the full ETHICS dataset, including subsets on justice, duties, virtues, and commonsense morality, to assess their performance on a wider range of ethical scenarios and wellbeing considerations

## Limitations

- Linear assumption: PCA assumes ethical dimensions are linearly separable in embedding space, which may not capture complex relationships
- Limited scope: Experiments focus only on the Utilitarianism subset of ETHICS, not broader ethical frameworks
- Incomplete encoding: 73.9% accuracy, while impressive for zero-shot performance, indicates significant room for improvement

## Confidence

- Pretraining mechanism claim: Medium
- Linear encoding assumption: Low
- Scaling relationship with model size: Medium

## Next Checks

1. Test whether nonlinear dimensionality reduction techniques (UMAP, t-SNE, or autoencoders) outperform PCA in isolating ethical dimensions, which would challenge the linear encoding assumption

2. Conduct ablation studies on pretraining data by fine-tuning models on corpora with systematically varied moral content to establish causal relationships between pretraining data and ethical understanding

3. Extend evaluation beyond pairwise comparisons to more complex ethical scenarios requiring multi-step reasoning, which would better test whether models truly understand wellbeing concepts rather than memorizing simple patterns