---
ver: rpa2
title: 'AdaptEval: Evaluating Large Language Models on Domain Adaptation for Text
  Summarization'
arxiv_id: '2407.11591'
source_url: https://arxiv.org/abs/2407.11591
tags:
- domain
- arxiv
- llama2
- adaptation
- summarization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AdaptEval, the first domain adaptation evaluation
  suite for large language models (LLMs) on text summarization. The authors evaluate
  11 models across scientific, medical, and governmental domains using both fine-tuning
  and in-context learning settings.
---

# AdaptEval: Evaluating Large Language Models on Domain Adaptation for Text Summarization

## Quick Facts
- arXiv ID: 2407.11591
- Source URL: https://arxiv.org/abs/2407.11591
- Reference count: 26
- Primary result: Small 7B-parameter models achieve comparable performance to larger models in in-context learning with just two examples

## Executive Summary
This paper introduces AdaptEval, the first comprehensive evaluation suite for domain adaptation in large language models on text summarization tasks. The authors evaluate 11 models across scientific, medical, and governmental domains using fine-tuning and in-context learning approaches. Results show that small 7B-parameter models can match larger models' performance with minimal in-context examples, while fine-tuned models excel in automatic metrics but struggle with domain vocabulary adaptation. The medical domain proves most challenging for adaptation across all models and settings.

## Method Summary
The authors create domain-specific benchmark datasets (arXiv for scientific, PubMed for medical, and GovReport for governmental domains) and evaluate 11 models including BART, PEGASUS-X, Llama2, Vicuna, Falcon, Mistral, ChatGPT, and GPT-4o mini. Models are tested in zero-shot, two-shot in-context learning, and fine-tuning settings. Evaluation uses ROUGE and BERTScore for summary quality, domain vocabulary overlap (DVO) for vocabulary adaptation, token distribution shift analysis for style changes, and GPT-4-based domain adaptation scoring for human-like evaluation.

## Key Results
- Small 7B-parameter models achieve comparable performance to larger models in in-context learning with just two examples
- Fine-tuned models excel in automatic metrics but show weaker domain vocabulary adaptation compared to in-context learning
- The medical domain proves most challenging for adaptation across all models and settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Small 7B-parameter models achieve comparable performance to larger models in in-context learning with only two examples.
- Mechanism: In-context learning enables smaller models to leverage example-driven adaptation without parameter updates, allowing them to match larger models' domain adaptation capabilities when provided with minimal guidance.
- Core assumption: Two examples provide sufficient signal for the model to understand domain-specific patterns and vocabulary requirements.
- Evidence anchors:
  - [abstract] "Results show that small 7b-parameter models achieve comparable performance to larger models in in-context learning with just two examples"
  - [section 3.2] "However, this performance gap is considerable reduced in the ICL setting with only two learning examples. In fact, the scores of the small 7b models are comparable to the large Llama 70b or the even larger ChatGPT."
  - [corpus] Weak - no direct corpus evidence supporting this mechanism.

### Mechanism 2
- Claim: Fine-tuned models achieve better automatic metrics but show weaker domain vocabulary adaptation compared to in-context learning.
- Mechanism: Fine-tuning optimizes parameters for ROUGE-based metrics, leading to extractive summaries that favor common tokens over domain-specific vocabulary.
- Core assumption: The fine-tuning objective (ROUGE optimization) prioritizes n-gram overlap over domain vocabulary usage.
- Evidence anchors:
  - [section 3.2] "In contrast, the fine-tuning results in Table 3 are mixed. Overall, the models outperform their counterparts in the two-shot setting in terms of ROUGE scores; however, there is a decrease in DVO."
  - [section 3.2] "We argue that this is attributed to the model's fine-tuning process, since the parameters are adjusted to optimize on ROUGE."
  - [corpus] Weak - no direct corpus evidence supporting this mechanism.

### Mechanism 3
- Claim: Medical domain is most challenging for domain adaptation across all models and settings.
- Mechanism: Medical domain contains highly specialized terminology and concepts that require deeper domain expertise than scientific or governmental domains.
- Core assumption: The complexity and specificity of medical terminology exceeds what can be learned from limited examples or general fine-tuning.
- Evidence anchors:
  - [abstract] "Results show that small 7b-parameter models achieve comparable performance to larger models in in-context learning with just two examples, while fine-tuned models excel in automatic metrics but show weaker domain vocabulary adaptation. The medical domain proves most challenging for adaptation."
  - [section 3.2] "However, PubMed obtains remarkably low scores, which highlights the difficulty of the models to adapt to the medical domain."
  - [corpus] Weak - no direct corpus evidence supporting this mechanism.

## Foundational Learning

- Concept: Domain vocabulary overlap (DVO) metric
  - Why needed here: Traditional ROUGE metrics may not capture domain-specific language usage, making DVO essential for assessing true domain adaptation
  - Quick check question: How does DVO differ from ROUGE in measuring summary quality for domain adaptation tasks?

- Concept: In-context learning (ICL) mechanics
  - Why needed here: ICL is the primary comparison point against fine-tuning, and understanding its mechanics is crucial for interpreting the results
  - Quick check question: What is the key difference between how ICL and fine-tuning modify model behavior during inference?

- Concept: Token distribution shift analysis
  - Why needed here: Understanding how model predictions change between base and adapted models is crucial for evaluating domain adaptation effectiveness
  - Quick check question: Can you explain how KL-divergence and token shift rate measure distribution changes between base and adapted models?

## Architecture Onboarding

- Component map: Domain benchmark datasets (arXiv, PubMed, GovReport) -> Model implementations (BART, PEGASUS-X, various LLMs from 7B to 70B parameters) -> Evaluation metrics (ROUGE, BERTScore, DVO, token distribution shift, GPT-4 evaluation) -> Analysis of distribution shifts and domain adaptation scores
- Critical path: Data preparation → Model inference (zero-shot, ICL, fine-tuning) → Metric calculation → Analysis of distribution shifts and domain adaptation scores
- Design tradeoffs: Using GPT-4 for domain adaptation scoring provides human-like evaluation but is expensive and limited to small sample sizes; ROUGE/BERTScore are cheaper but may not capture domain-specific language usage
- Failure signatures: High ROUGE scores with low DVO indicate extractive summaries lacking domain vocabulary; high token shift rates with low KL-divergence suggest stylistic changes rather than meaningful domain adaptation
- First 3 experiments:
  1. Compare zero-shot vs. two-shot ICL performance on arXiv dataset to verify small model capability
  2. Test token distribution shift between base and fine-tuned Llama2 7B on PubMed to identify vocabulary adaptation gaps
  3. Run GPT-4 domain adaptation scoring on GovReport samples to validate automatic metric findings

## Open Questions the Paper Calls Out

- How does the performance of domain adaptation vary across different types of scientific domains (e.g., physics vs. biology vs. computer science)?
- What is the optimal number of in-context examples for domain adaptation, and how does this vary by domain and model size?
- How do domain adaptation capabilities transfer across related domains (e.g., from general science to medical science)?
- What specific linguistic features (beyond vocabulary) contribute most to successful domain adaptation?
- How do domain adaptation capabilities generalize to other NLP tasks beyond summarization?

## Limitations

- Limited exploration of in-context learning example numbers (only tested two examples)
- Domain vocabulary overlap metric may not fully capture the complexity of domain adaptation
- Medical domain results lack deeper analysis of underlying causes for poor adaptation
- No systematic investigation of cross-domain transfer learning capabilities

## Confidence

- ICL performance comparison (Small models vs Large models): Medium confidence - systematic evaluation but limited example size
- Fine-tuned vs ICL vocabulary adaptation gap: Medium confidence - clear metric difference but causal mechanisms need more rigorous establishment
- Medical domain difficulty: Medium confidence - consistent poor performance but lacks deeper analysis of root causes

## Next Checks

1. Systematically test ICL performance scaling by varying the number of examples from 1 to 10 to determine if two examples represent an optimal or arbitrary choice.
2. Conduct ablation studies on fine-tuning objectives to determine if explicitly incorporating domain vocabulary usage into the loss function would resolve the DVO gap.
3. Perform detailed error analysis on medical domain failures to distinguish between vocabulary coverage issues versus deeper domain reasoning requirements.