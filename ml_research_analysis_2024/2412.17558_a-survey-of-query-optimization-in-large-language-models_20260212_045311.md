---
ver: rpa2
title: A Survey of Query Optimization in Large Language Models
arxiv_id: '2412.17558'
source_url: https://arxiv.org/abs/2412.17558
tags:
- query
- language
- queries
- llms
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey comprehensively analyzes query optimization (QO) techniques
  in retrieval-augmented large language models (RAG), addressing the challenge of
  improving LLM efficiency and accuracy in answering complex queries. The paper categorizes
  QO methods into four main approaches: Expansion (Internal/External), Decomposition,
  Disambiguation, and Abstraction.'
---

# A Survey of Query Optimization in Large Language Models

## Quick Facts
- arXiv ID: 2412.17558
- Source URL: https://arxiv.org/abs/2412.17558
- Authors: Mingyang Song; Mao Zheng
- Reference count: 27
- Key outcome: Comprehensive analysis of query optimization techniques in RAG systems, categorizing methods into four approaches: Expansion, Decomposition, Disambiguation, and Abstraction

## Executive Summary
This survey comprehensively analyzes query optimization (QO) techniques in retrieval-augmented large language models (RAG), addressing the challenge of improving LLM efficiency and accuracy in answering complex queries. The paper categorizes QO methods into four main approaches: Expansion (Internal/External), Decomposition, Disambiguation, and Abstraction. Key techniques include leveraging LLMs to generate contextual documents, decomposing complex queries into simpler sub-queries, clarifying ambiguous queries, and applying high-level reasoning principles. The survey identifies current challenges such as lack of benchmarks, inefficient optimization paths, and disconnect between optimized queries and retrieval quality.

## Method Summary
The survey synthesizes existing query optimization techniques in RAG systems, categorizing them into four main approaches: Expansion (using LLM-generated contextual documents), Decomposition (breaking complex queries into simpler sub-queries), Disambiguation (clarifying ambiguous queries), and Abstraction (applying high-level reasoning principles). The methodology involves reviewing literature on QO techniques, analyzing their mechanisms and effectiveness, and identifying challenges and future directions. The paper proposes a framework for classifying query types based on evidence requirements and maps them to appropriate optimization approaches.

## Key Results
- Categorization of QO methods into four main approaches: Expansion, Decomposition, Disambiguation, and Abstraction
- Identification of challenges including lack of benchmarks, inefficient optimization paths, and disconnect between optimized queries and retrieval quality
- Proposal of future directions including query-centric process reward models and comprehensive evaluation frameworks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Query optimization techniques enhance RAG performance by addressing different query types through four distinct operations: expansion, decomposition, disambiguation, and abstraction.
- Mechanism: The paper proposes a structured categorization where different query complexities (single/multiple pieces of explicit/implicit evidence) map to specific optimization operations. This allows LLMs to process queries more effectively by applying the appropriate transformation.
- Core assumption: Query types can be reliably classified into four categories, and each category has an optimal optimization approach.
- Evidence anchors:
  - [abstract] "The paper categorizes QO methods into four main approaches: Expansion (Internal/External), Decomposition, Disambiguation, and Abstraction."
  - [section] "We classify the difficulty of most queries into four types: those that can be solved with a single piece of explicit evidence, those requiring multiple pieces of explicit evidence, those solvable with a single piece of implicit evidence, and those needing multiple pieces of implicit evidence."
  - [corpus] Weak evidence - the corpus contains related papers on query optimization but doesn't directly validate the four-category framework.
- Break condition: If queries cannot be reliably classified into these four types, or if a query type requires multiple optimization approaches simultaneously, the framework breaks down.

### Mechanism 2
- Claim: Query expansion works by leveraging LLM capabilities to generate contextual documents that bridge the gap between query understanding and answer generation.
- Mechanism: The paper describes techniques like GENREAD and QUERY2DOC that use LLMs to generate pseudo-documents based on initial queries, which are then used to enhance retrieval performance by providing additional context.
- Core assumption: LLMs trained on web-scale text corpora can generate relevant contextual documents that improve retrieval outcomes.
- Evidence anchors:
  - [section] "GENREAD (Yu et al., 2023a), which employs a well-designed instruction to prompt LLMs to generate contextual documents based on the initial query."
  - [section] "QUERY2DOC (Wang et al., 2023b) introduces a simple yet effective approach to improve both sparse and dense retrieval systems."
  - [corpus] Weak evidence - the corpus shows related work on query expansion but doesn't specifically validate the contextual document generation approach.
- Break condition: If generated pseudo-documents contain too many hallucinations or irrelevant information, the expansion technique fails and potentially degrades retrieval performance.

### Mechanism 3
- Claim: Query decomposition improves handling of complex multi-hop queries by breaking them into simpler sub-queries that can be answered individually and integrated.
- Mechanism: Techniques like DSP, LEAST-TO-MOST, and PLAN-AND-SOLVE systematically break down complex queries into manageable sub-components, allowing models to process each component effectively before integrating responses.
- Core assumption: Complex queries can be decomposed into simpler sub-queries that, when answered individually and integrated, provide accurate answers to the original query.
- Evidence anchors:
  - [section] "One such method is the Demonstrate Search Predict (DSP) framework (Khattab et al., 2022), which relies on passing natural language texts through sophisticated pipelines between an LLM and a retrieval model (RM)."
  - [section] "Similarly, techniques like LEAST-TO-MOST (Zhou et al., 2023) prompting utilize few-shot prompts to first decompose a complex problem into a series of simpler subproblems."
  - [corpus] Weak evidence - the corpus contains related work on query decomposition but doesn't validate the integration approach.
- Break condition: If sub-queries are interdependent or require context from other sub-queries, simple sequential decomposition fails and the integrated answer becomes inaccurate.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG) fundamentals
  - Why needed here: Understanding how RAG works is essential to grasp why query optimization is critical - RAG's effectiveness depends on retrieving relevant information that matches the query intent.
  - Quick check question: What are the two main stages of RAG, and how does query optimization specifically impact the first stage?

- Concept: Query intent classification
  - Why needed here: The framework relies on classifying queries into four types based on evidence requirements, so understanding how to identify explicit vs implicit evidence and single vs multiple evidence needs is crucial.
  - Quick check question: How would you distinguish between a query requiring a single piece of explicit evidence versus one requiring multiple pieces of implicit evidence?

- Concept: Large Language Model prompting techniques
  - Why needed here: Many optimization techniques use LLM prompting to generate expansions, decompositions, or clarifications, so understanding zero-shot, few-shot, and chain-of-thought prompting is essential.
  - Quick check question: What's the difference between using few-shot prompting versus zero-shot prompting when generating query expansions with an LLM?

## Architecture Onboarding

- Component map: Query Classifier → Query Optimizer (Expansion/Decomposition/Disambiguation/Abstraction modules) → Retriever → Retrieved Documents → Answer Generator → Final Answer

- Critical path: Query → Classifier → Optimizer (selects technique) → Optimized Query → Retriever → Retrieved Documents → Answer Generator → Final Answer. The optimizer is the critical component that determines overall system effectiveness.

- Design tradeoffs: Complex queries benefit from decomposition but incur higher computational costs and potential integration errors. Expansion provides context but risks hallucination. Disambiguation improves accuracy but may require multiple retrieval attempts. Abstraction enables high-level reasoning but loses specific details.

- Failure signatures: Poor retrieval quality indicates optimizer selection errors. Inconsistent answers suggest decomposition/integration issues. Vague responses indicate insufficient expansion or disambiguation. Overly complex reasoning paths suggest abstraction failures.

- First 3 experiments:
  1. Implement a simple classifier that categorizes queries into the four types and verify accuracy on a labeled dataset of 100 diverse queries.
  2. Build an expansion module using LLM prompting and test its effectiveness by comparing retrieval performance with and without expansion on 50 ambiguous queries.
  3. Create a decomposition system for multi-hop queries and evaluate whether breaking queries into sub-queries improves answer accuracy compared to direct retrieval.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal approach to developing query-centric process reward models that can effectively guide LLMs through complex reasoning tasks?
- Basis in paper: [explicit] The paper suggests that query-centric process reward models could be simpler and more effective than traditional outcome reward models, as they provide rewards at each sub-query step of a multi-step reasoning process.
- Why unresolved: While the paper identifies this as a promising direction, it does not provide specific methodologies or empirical evidence for implementing such reward models. The optimal design and training strategies for these models remain unexplored.
- What evidence would resolve it: Empirical studies comparing different process reward model architectures and training approaches on benchmark datasets would demonstrate which methods most effectively guide LLMs through complex reasoning tasks.

### Open Question 2
- Question: How can we develop comprehensive benchmarks specifically designed to evaluate query optimization techniques across different RAG scenarios?
- Basis in paper: [explicit] The paper explicitly identifies the lack of benchmarks for query optimization as a significant gap, particularly for complex contexts like multi-turn retrieval-augmented dialogues and intricate problem decomposition.
- Why unresolved: Current evaluation frameworks are insufficient for systematically assessing query optimization techniques across the diverse scenarios in which they are applied. Without standardized benchmarks, it's difficult to compare different approaches or track progress in the field.
- What evidence would resolve it: Development and validation of benchmark datasets that cover various query types, complexity levels, and RAG scenarios, along with standardized evaluation metrics, would provide the foundation for assessing and comparing query optimization techniques.

### Open Question 3
- Question: What algorithms can efficiently identify optimal query optimization pathways without resorting to exhaustive search, thereby improving both efficiency and quality?
- Basis in paper: [inferred] The paper discusses how many existing methods rely on exhaustive enumeration strategies that increase computational time and search costs while potentially introducing irrelevant information. It suggests that future research should focus on designing efficient algorithms for identifying optimal optimization pathways.
- Why unresolved: Current query optimization methods often lack the sophistication to identify the most promising optimization paths without exploring numerous non-optimal alternatives. This results in inefficient use of computational resources and potentially lower quality outcomes.
- What evidence would resolve it: Empirical comparisons demonstrating that new algorithmic approaches can consistently identify optimal or near-optimal query optimization pathways with significantly less computational overhead than existing exhaustive methods would validate this direction.

## Limitations

- Lack of comprehensive benchmarks and standardized evaluation metrics for query optimization techniques
- Disconnect between optimized queries and actual retrieval quality, leading to situations where queries appear well-optimized but produce unsatisfactory results
- Computational inefficiency in many optimization approaches, particularly those relying on exhaustive enumeration of possible query variations

## Confidence

- High confidence: Categorization of QO techniques into four main approaches (Expansion, Decomposition, Disambiguation, Abstraction) - well-supported by existing literature
- Medium confidence: Effectiveness of individual QO techniques - limited empirical evidence directly validating performance improvements
- Low confidence: Practical applicability of proposed query-centric process reward model - represents a future direction with significant implementation challenges

## Next Checks

1. **Benchmark Development**: Create a standardized evaluation framework with labeled query datasets representing the four identified query types, along with ground truth retrieval results, to enable systematic comparison of different QO techniques.

2. **End-to-End Performance Validation**: Conduct experiments measuring the complete RAG pipeline performance (retrieval quality, answer accuracy, computational efficiency) when applying different QO techniques, rather than evaluating optimization in isolation.

3. **Query Type Classification Accuracy**: Validate the proposed four-category query classification system by testing its accuracy on a diverse set of real-world queries and examining cases where queries exhibit characteristics of multiple categories.