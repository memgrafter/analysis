---
ver: rpa2
title: 'Testing and Evaluation of Large Language Models: Correctness, Non-Toxicity,
  and Fairness'
arxiv_id: '2409.00551'
source_url: https://arxiv.org/abs/2409.00551
tags:
- llms
- language
- bias
- page
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis introduces six novel testing and evaluation frameworks
  for Large Language Models (LLMs) focusing on correctness, non-toxicity, and fairness.
  It addresses the problem of unreliable LLM outputs by designing automatic testing
  methods that generate comprehensive test cases without human effort.
---

# Testing and Evaluation of Large Language Models: Correctness, Non-Toxicity, and Fairness

## Quick Facts
- arXiv ID: 2409.00551
- Source URL: https://arxiv.org/abs/2409.00551
- Authors: Wenxuan Wang
- Reference count: 0
- Primary result: Novel frameworks for LLM testing with up to 91.2% error finding rates

## Executive Summary
This thesis introduces six automatic testing and evaluation frameworks addressing correctness, non-toxicity, and fairness in Large Language Models. The frameworks systematically uncover failures in commercial LLMs through knowledge graph-based test case generation, metamorphic testing for content moderation, and multilingual safety assessment. Key contributions include FactChecker for factual correctness, LogicAsker for logical reasoning, MTTM for content moderation robustness, and XSafety for multilingual safety evaluation. The work demonstrates significant error detection capabilities across diverse scenarios while highlighting limitations in current LLM evaluation methodologies.

## Method Summary
The thesis presents automatic test case generation frameworks that eliminate manual annotation requirements. FactChecker constructs knowledge graphs from Wikidata triplets and generates diverse question types through rule-based transformations. MTTM applies semantic-preserving perturbations to test content moderation robustness, while XSafety creates multilingual safety benchmarks by translating existing English safety tests. The frameworks employ similarity matching, metamorphic relations, and comprehensive evaluation pipelines to systematically assess LLM performance across correctness, non-toxicity, and fairness dimensions.

## Key Results
- FactChecker and LogicAsker achieve high error finding rates (up to 91.2%) for factual and logical correctness testing
- MTTM demonstrates 83.9%, 51%, and 82.5% error finding rates for content moderation software across different perturbation types
- XSafety reveals significant performance drops in LLMs' multilingual safety capabilities compared to English
- Generated test cases show measurable improvements in model performance across all evaluated dimensions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Automatic test case generation via knowledge graph construction eliminates manual annotation bottleneck.
- Mechanism: FactChecker retrieves fact triplets from Wikidata, builds a directed knowledge graph, and converts them into structured question-answer pairs that serve as test cases.
- Core assumption: Knowledge triplets are not present in LLM pretraining corpora, reducing data contamination risk.
- Evidence anchors:
  - [abstract] "FactChecker constructs knowledge graphs by retrieving fact triplets from large-scale knowledge databases..."
  - [section] "Our testing frameworks can automatically and comprehensively generate test cases..."
  - [corpus] Weak - no direct citation of Wikidata usage in LLM pretraining logs; stated as assumption.

### Mechanism 2
- Claim: Diverse question types (Yes-No, MC, WH) probe multiple reasoning dimensions, exposing deeper model failures.
- Mechanism: FactChecker uses rule-based transformations to generate different syntactic forms from same fact triplet, forcing model to generalize beyond pattern matching.
- Core assumption: Different question types require distinct internal reasoning paths; model cannot rely on superficial heuristics.
- Evidence anchors:
  - [abstract] "...generates various types of questions as well as the expected answers from the knowledge graphs..."
  - [section] "FactChecker supports to generate all three types of questions, covering all main question types in English..."
  - [corpus] Moderate - rule-based question generation literature exists but not cited for LLM testing specifically.

### Mechanism 3
- Claim: Metamorphic testing framework MTTM finds content moderation failures by applying semantic-preserving perturbations.
- Mechanism: MTTM defines metamorphic relations (e.g., visual-based substitution, homophone substitution) that preserve toxicity while altering surface form, then checks if moderation software maintains detection.
- Core assumption: Real users apply similar perturbation strategies to evade moderation; perturbations remain human-recognizable.
- Evidence anchors:
  - [abstract] "MTTM...with the metamorphic relation that a toxic sentence should still be identified as toxic after semantic-preserved perturbations."
  - [section] "MTTM achieves up to 83.9%, 51%, and 82.5% error finding rates..."
  - [corpus] Strong - pilot study with 2,000 real messages shows 12.3% visual-based substitution usage.

## Foundational Learning

- Concept: Knowledge graph construction from structured data
  - Why needed here: Provides clean, contamination-resistant test case source for LLMs
  - Quick check question: Can you explain how Wikidata triplets differ from natural text in terms of LLM training exposure?

- Concept: Metamorphic relations in software testing
  - Why needed here: Enables systematic generation of adversarial inputs that preserve semantic meaning
  - Quick check question: What distinguishes a metamorphic relation from a simple input transformation?

- Concept: Cross-lingual generalization in safety alignment
  - Why needed here: Explains why English-aligned models fail on non-English safety queries
  - Quick check question: How does prompting "think in English" attempt to bridge cross-lingual safety gaps?

## Architecture Onboarding

- Component map: Knowledge graph layer → Question generation module → Answer assessment engine → Result visualization
- Critical path: Triplet retrieval → Graph construction → Question generation → LLM query → Similarity matching → Error classification
- Design tradeoffs: Comprehensive coverage vs. computational cost; contamination resistance vs. realistic question variety
- Failure signatures: High false positive rates in similarity matching; generation of ungrammatical questions; missing critical reasoning skills
- First 3 experiments:
  1. Generate 100 questions from Wikidata on "politician" topic, measure fact accuracy across 3 LLMs
  2. Apply MTTM perturbations to 50 toxic sentences, measure moderation software error rates
  3. Translate 200 safety benchmark questions into 5 languages, measure LLM safety performance drop

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are current LLM evaluation frameworks at detecting cultural bias across multiple languages?
- Basis in paper: [explicit] The paper identifies cultural dominance in LLMs and proposes XCulturalBench for evaluating cultural bias, but notes limitations in the scope of concrete cultural objects examined.
- Why unresolved: The paper only considers eight concrete objects and eleven languages, leaving room for broader evaluation.
- What evidence would resolve it: Studies evaluating a wider range of cultural objects and languages using existing and new evaluation frameworks.

### Open Question 2
- Question: Can prompting methods effectively improve the cultural sensitivity of LLMs for abstract cultural concepts?
- Basis in paper: [explicit] The paper proposes prompting methods that improve performance on concrete cultural objects but are less effective on abstract ones requiring complex cultural knowledge.
- Why unresolved: The effectiveness of prompting for abstract cultural concepts is not fully established, and the reasons for its limitations are not fully explored.
- What evidence would resolve it: Experiments comparing the effectiveness of different prompting strategies on abstract cultural concepts and analyzing the factors contributing to their success or failure.

### Open Question 3
- Question: What are the most effective strategies for mitigating cultural dominance in LLMs during training and deployment?
- Basis in paper: [explicit] The paper suggests pretraining on diverse data and advanced prompting as potential solutions but acknowledges the limitations of each approach.
- Why unresolved: The paper does not provide a comprehensive comparison of different strategies or explore the trade-offs between them.
- What evidence would resolve it: Comparative studies evaluating the effectiveness of various mitigation strategies on different LLM architectures and cultural contexts.

## Limitations

- Contamination risk uncertainty: No systematic audit of LLM training corpora to verify knowledge graph contamination resistance
- Cross-lingual prompting effectiveness: Limited empirical validation of "think in English" strategy across diverse language families
- Causal attribution gap: Correlation between generated test cases and model improvements not established through controlled experiments

## Confidence

- **High confidence**: Error finding rates for MTTM content moderation testing (83.9%, 51%, 82.5%) with pilot study backing
- **Medium confidence**: Factual correctness evaluation frameworks (FactChecker/LogicAsker) given their systematic methodology but limited contamination analysis
- **Medium confidence**: Multilingual safety assessment given the comprehensive benchmark construction but simplified prompting strategy
- **Low confidence**: Direct attribution of model improvements to generated test cases without controlled experiments

## Next Checks

1. Conduct corpus analysis comparing Wikidata triplets against known LLM pretraining datasets to quantify contamination risk for FactChecker
2. Perform controlled experiments testing whether generated test cases directly improve LLM performance on held-out reasoning tasks
3. Evaluate cross-lingual safety prompting strategies across diverse language families beyond the initial English-centric approach