---
ver: rpa2
title: 'ProTrix: Building Models for Planning and Reasoning over Tables with Sentence
  Context'
arxiv_id: '2403.02177'
source_url: https://arxiv.org/abs/2403.02177
tags:
- table
- answer
- reasoning
- context
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ProTrix, a Plan-then-Reason framework designed
  to answer diverse user queries over tables with sentence context. The framework
  first plans reasoning paths by analyzing the query and context, then assigns each
  step to program-based (SQL) or textual reasoning to reach the final answer.
---

# ProTrix: Building Models for Planning and Reasoning over Tables with Sentence Context

## Quick Facts
- arXiv ID: 2403.02177
- Source URL: https://arxiv.org/abs/2403.02177
- Reference count: 40
- ProTrix framework enables models to plan and reason over tables with sentence context, generalizing to diverse unseen tabular tasks with only 6k training instances

## Executive Summary
This paper introduces ProTrix, a Plan-then-Reason framework designed to answer diverse user queries over tables with sentence context. The framework first plans reasoning paths by analyzing the query and context, then assigns each step to program-based (SQL) or textual reasoning to reach the final answer. ProTrix enhances table reasoning for both in-context learning and fine-tuning methods. GPT-3.5-Turbo following this framework surpasses prompting baselines without self-consistency while using fewer API calls and demonstrations. The authors construct TrixInstruct, an instruction-tuning dataset, and fine-tune Llama-2-7B and CodeLlama-7B to create the ProTrix model family. Experiments show ProTrix generalizes to unseen tabular tasks with only 6k training instances and generates accurate, faithful explanations for complex free-form questions.

## Method Summary
The ProTrix framework operates in two phases: planning and reasoning. During planning, the model analyzes the query and context to identify gaps, decomposes the query into sub-questions, and plans a reasoning path. In the reasoning phase, each step is assigned to either program-based reasoning (using SQL for precise table operations) or textual reasoning (using natural language processing for handling program-unsolvable queries and commonsense knowledge). The framework blends these methods to leverage their respective strengths while avoiding individual limitations. The authors construct TrixInstruct, an instruction-tuning dataset based on benchmarks with queries that are program-unsolvable or require combining information from tables and sentence context, then fine-tune Llama-2-7B and CodeLlama-7B models on this dataset.

## Key Results
- ProTrix framework achieves strong performance on diverse tabular reasoning tasks with only 6k training instances
- GPT-3.5-Turbo following the Plan-then-Reason framework surpasses prompting baselines without self-consistency while using fewer API calls and demonstrations
- Fine-tuned ProTrix models (Llama-2-7B and CodeLlama-7B) generalize to unseen tabular tasks and generate accurate, faithful explanations for complex free-form questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Planning the entire reasoning chain before acting reduces error accumulation compared to step-by-step reactive methods like ReAct.
- Mechanism: The framework performs global analysis of the query and context, decomposing it into sub-questions and assigning each to either program-based (SQL) or textual reasoning. This prevents the model from getting trapped in local reasoning steps and allows it to use the optimal method for each sub-task.
- Core assumption: The model can accurately decompose queries and correctly assign reasoning methods when given the full context upfront rather than relying on feedback from previous actions.
- Evidence anchors:
  - [abstract] "Previous works incorporating LLMs' dynamic planning abilities for table reasoning primarily focus on selecting local actions during the reasoning process, rather than globally planning the entire reasoning strategy beforehand."
  - [section] "Instead of fixed reasoning patterns, our approach offers greater flexibility by adaptively assigning reasoning sub-tasks to either SQL-based or text-based methods, depending on the nature of the decomposed sub-questions."
  - [corpus] Weak - no direct corpus evidence for this specific mechanism.

### Mechanism 2
- Claim: Blending program-based and textual reasoning leverages the strengths of both methods while avoiding their individual limitations.
- Mechanism: The framework uses SQL for precise table operations (counting, sorting, filtering) and textual reasoning for handling program-unsolvable queries and commonsense knowledge gaps. This combination allows handling both structured and unstructured information effectively.
- Core assumption: Program-based reasoning can achieve high precision for table operations while textual reasoning can fill information gaps and handle complex reasoning that cannot be expressed in SQL.
- Evidence anchors:
  - [abstract] "The textual reasoning method such as Chain-of-Thought can be used to enhance the tabular reasoning ability but often lacks precision in tabular operations such as sorting, counting and filtering... The program-based reasoning method can reason with high precision with SQL or Python code."
  - [section] "The framework first plans the reasoning paths over the context, then assigns each step to program-based or textual reasoning to reach the final answer."
  - [corpus] Weak - no direct corpus evidence for this specific blending mechanism.

### Mechanism 3
- Claim: Instruction tuning on program-unsolvable queries and table-text integration tasks builds models with planning and reasoning abilities that generalize to unseen tasks.
- Mechanism: The TrixInstruct dataset contains queries that require either program-unsolvable reasoning or combining information from tables and sentences. Fine-tuning on this dataset teaches models to plan reasoning pathways and blend different reasoning methods, which generalizes to diverse tabular tasks.
- Core assumption: Training on queries that specifically require planning and reasoning abilities will transfer to other tabular tasks even when they have different input/output configurations.
- Evidence anchors:
  - [abstract] "We construct an instruction tuning set TrixInstruct based on benchmarks with queries that are program-unsolvable or need combining information from table and sentence context."
  - [section] "Our experiments show that the PROTRIX family can generalize to diverse unseen tabular tasks with only 6k training instances."
  - [corpus] Weak - no direct corpus evidence for this specific fine-tuning approach.

## Foundational Learning

- Concept: Decomposition of complex queries into sub-questions
  - Why needed here: The framework requires breaking down multi-hop queries into manageable sub-tasks that can be solved with different reasoning methods
  - Quick check question: Can you identify the sub-questions needed to answer "Who was the silver medalist at the 2001 Goodwill Games and did they ever win an Olympic gold medal?"

- Concept: Understanding the limitations of SQL for certain types of reasoning
  - Why needed here: The framework must recognize when SQL cannot solve a query (e.g., comparing general concepts like "color" or "success") and switch to textual reasoning
  - Quick check question: Why can't SQL directly answer "Which steak cooking method produces the lightest color"?

- Concept: Integration of structured and unstructured information
  - Why needed here: Many queries require combining data from tables with contextual information from sentences, requiring the model to plan how to use both sources
  - Quick check question: How would you verify the claim "The silver medalist at the 2001 Goodwill Games has won an Olympic gold medal" using both table and sentence context?

## Architecture Onboarding

- Component map: Query Analyzer -> Planner -> SQL Executor/Context Retriever -> Reasoner -> Answer Generator
- Critical path: Query → Analysis → Planning → SQL Execution/Textual Reasoning → Integration → Answer
- Design tradeoffs:
  - Global planning vs. reactive step-by-step: Global planning reduces error accumulation but requires more complex initial analysis
  - Program vs. text assignment: Program offers precision but has limitations; text offers flexibility but less precision
  - Training data size vs. generalization: Small dataset (6k examples) achieved good generalization, but larger datasets might improve performance
- Failure signatures:
  - SQL errors: Invalid column references, unexecutable queries, complex cell content handling issues
  - Planning errors: Missing sub-questions, wrong decomposition, incorrect method assignment
  - Integration errors: Failing to properly combine program and textual reasoning outputs
- First 3 experiments:
  1. Test on WikiTQ with and without planning to measure impact of global planning
  2. Test on FEVEROUS with different SQL complexity levels to identify SQL generation limitations
  3. Test on FetaQA (free-form) to verify generalization of planning and reasoning abilities to unseen task types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the maximum table size that the Plan-then-Reason framework can effectively handle?
- Basis in paper: [inferred] The paper mentions that the framework plans reasoning paths over both tabular and sentence context, but does not specify any limits on table size.
- Why unresolved: The paper does not provide experiments or analysis on how the framework performs with increasingly large tables.
- What evidence would resolve it: Experiments showing performance degradation or processing limitations as table size increases would provide concrete evidence of the maximum effective table size.

### Open Question 2
- Question: How does the framework handle tables with complex hierarchical structures or multiple nested tables?
- Basis in paper: [inferred] The paper focuses on relational tables without hierarchical headers, suggesting this is a limitation of the current approach.
- Why unresolved: The paper explicitly states that TrixInstruct does not include complex tables with hierarchical headers or multiple tables, leaving this capability unexplored.
- What evidence would resolve it: Successful application of the framework to tables with hierarchical structures or multiple related tables, along with performance metrics, would demonstrate this capability.

### Open Question 3
- Question: What is the impact of the framework's planning stage on overall reasoning accuracy compared to reactive approaches?
- Basis in paper: [explicit] The paper contrasts its planning approach with existing ReAct-style methods that focus on next-step actions rather than global planning.
- Why unresolved: While the paper claims planning provides advantages, it does not directly compare the accuracy impact of planning versus purely reactive approaches in controlled experiments.
- What evidence would resolve it: Controlled experiments comparing the same model with and without the planning stage on identical tasks would quantify the accuracy impact of planning.

### Open Question 4
- Question: How does the framework's performance scale with the complexity of queries requiring multi-hop reasoning?
- Basis in paper: [inferred] The paper mentions handling multi-hop scenarios but does not analyze performance degradation with increasing reasoning complexity.
- Why unresolved: The paper demonstrates capability for multi-hop reasoning but does not provide systematic analysis of how performance changes as the number of reasoning steps increases.
- What evidence would resolve it: Experiments measuring accuracy, API calls, or processing time as the number of reasoning hops increases would reveal performance scaling characteristics.

## Limitations
- Lack of direct corpus evidence for core mechanisms, particularly the benefits of global planning over reactive methods
- Incomplete specification of evaluation methodology, raising concerns about potential false positives or negatives
- No thorough analysis of failure modes, particularly SQL generation errors for complex operations or issues with handling multi-step reasoning

## Confidence
- High Confidence: The ProTrix framework's ability to generalize to unseen tabular tasks with limited training data (6k instances)
- Medium Confidence: The superiority of GPT-3.5-Turbo following the Plan-then-Reason framework compared to prompting baselines
- Low Confidence: The specific mechanisms by which global planning reduces error accumulation compared to step-by-step reactive methods

## Next Checks
1. Conduct ablation studies comparing global planning versus reactive step-by-step methods across multiple tabular reasoning benchmarks to isolate the impact of planning strategy
2. Perform detailed error analysis categorizing failures by type (planning errors, SQL errors, execution errors, reasoning errors) to identify systematic weaknesses and improvement opportunities
3. Test model performance on increasingly complex table operations and program structures to establish the boundaries of SQL generation capabilities and identify failure patterns