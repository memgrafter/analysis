---
ver: rpa2
title: An Automated Explainable Educational Assessment System Built on LLMs
arxiv_id: '2412.13381'
source_url: https://arxiv.org/abs/2412.13381
tags:
- assessment
- llms
- student
- automated
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AERA Chat is an interactive platform that automates educational
  assessment using large language models (LLMs) to generate both marks and explainable
  rationales for student answers. It addresses the challenge of limited transparency
  in traditional automated scoring systems by providing visual highlighting of key
  answer elements and rationale aspects, along with tools for annotation and performance
  evaluation.
---

# An Automated Explainable Educational Assessment System Built on LLMs

## Quick Facts
- arXiv ID: 2412.13381
- Source URL: https://arxiv.org/abs/2412.13381
- Reference count: 8
- AERA Chat is an interactive platform that automates educational assessment using large language models (LLMs) to generate both marks and explainable rationales for student answers.

## Executive Summary
AERA Chat is an interactive platform that automates educational assessment using large language models (LLMs) to generate both marks and rationales for student answers. It addresses the challenge of limited transparency in traditional automated scoring systems by providing visual highlighting of key answer elements and rationale aspects, along with tools for annotation and performance evaluation. The system supports multiple LLM backends and offers bulk marking, chat-based exploration, and human-in-the-loop refinement through preference annotation and rationale correction.

## Method Summary
The system uses a microservices architecture with Docker, React frontend, Flask backend, PostgreSQL database, and flexible LLM integration via public APIs or private models. It employs in-context learning to generate assessments and rationales, with GPT-4o performing word-level tagging for visual highlighting. The platform includes a human-in-the-loop annotation toolkit for ground-truth correction, preference selection, and rationale submission, enabling educators to refine the system's performance over time.

## Key Results
- Provides automated marking and rationale explanations for student answers
- Supports multiple LLM backends including GPT-3.5-turbo, GPT-4o, and custom AERA model
- Features visual highlighting of key answer elements and rationale aspects
- Includes human-in-the-loop annotation tools for system refinement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AERA Chat improves trust in automated scoring by generating natural language rationales that explain model decisions.
- Mechanism: LLMs use in-context learning to produce explanations alongside marks, making the assessment process transparent to educators and students.
- Core assumption: Generated rationales are sufficiently accurate and faithful to the scoring logic.
- Evidence anchors:
  - [abstract] "This system leverages large language models (LLMs) to generate automated marking and rationale explanations, addressing the challenge of limited explainability in automated educational assessment..."
  - [section] "The recent development of large language models (LLMs) has introduced a new approach that leverages in-context learning and reasoning capabilities... to generate natural language rationales that justify model decisions (Camburu et al. 2018; Marasovic et al. 2022; Gurrapu et al. 2023)."
- Break condition: If generated rationales are inconsistent with the actual scoring logic or contain factual errors, trust in the system would erode.

### Mechanism 2
- Claim: Visual highlighting of key answer elements and rationale aspects enhances user understanding and verification efficiency.
- Mechanism: GPT-4o performs word-level tagging to identify and highlight key elements in student answers and positive/negative aspects in rationales.
- Core assumption: The highlighting accurately identifies the relevant portions of text that correspond to scoring decisions.
- Evidence anchors:
  - [section] "Our platform aims to enhance user experience by providing clear, high-contrast visual cues within student answers and assessment rationales... This functionality is powered by GPT-4o, which performs word-level tagging and generates context highlights in a JSON format, ensuring effective and efficient context visualization (Li et al. 2023c)."
- Break condition: If the highlighting frequently misidentifies or misses relevant text elements, users will lose confidence in the visual aids.

### Mechanism 3
- Claim: Human-in-the-loop annotation and preference selection improves the quality of generated rationales over time.
- Mechanism: Users can correct ground-truth labels, select preferred rationales, and submit their own annotations, creating a feedback loop for model improvement.
- Core assumption: Human annotations and preferences are sufficiently accurate and representative to guide model training.
- Evidence anchors:
  - [section] "(2) Human Preference Annotation: Evaluating the quality of rationales often models the qualitative evaluation task as binary preferences (Li et al. 2024b,a), where the factually correct and more detailed rationale is preferred. Consequently, our platform includes options for users to select 'preferred' or 'not preferred' rationales."
- Break condition: If the human feedback is inconsistent, noisy, or biased, the quality of rationale generation may not improve and could potentially degrade.

## Foundational Learning

- Concept: Automated Student Answer Scoring (ASAS) systems
  - Why needed here: Understanding traditional ASAS approaches helps contextualize how AERA Chat differs by adding explainability.
  - Quick check question: What are the key limitations of traditional text classifier-based ASAS systems that AERA Chat aims to address?

- Concept: In-context learning and few-shot reasoning capabilities of LLMs
  - Why needed here: These capabilities enable LLMs to generate rationales without extensive fine-tuning, making the system more flexible.
  - Quick check question: How does in-context learning allow AERA Chat to work with different question types without model retraining?

- Concept: Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization
  - Why needed here: These techniques are referenced as methods to improve rationale generation using the preference annotations collected through the platform.
  - Quick check question: How can binary preference annotations collected through AERA Chat be used to optimize LLM rationale generation?

## Architecture Onboarding

- Component map:
  Frontend (Remix framework) -> Backend (Flask API) -> LLM Services (OpenAI/HuggingFace) -> Database (PostgreSQL) -> Containerization (Docker)

- Critical path:
  1. User uploads question and student answers via bulk marking interface
  2. System compiles question information into template prompt
  3. LLM services process prompts and generate marks and rationales
  4. Results are stored in database and displayed as card views
  5. Users can interact with highlights and annotations for verification

- Design tradeoffs:
  - Microservices architecture enables flexibility but adds operational complexity
  - Supporting multiple LLM backends increases versatility but requires unified interface design
  - Visual highlighting improves usability but adds computational overhead
  - Human-in-the-loop approach improves quality but requires user engagement

- Failure signatures:
  - Slow response times: LLM API latency or database query bottlenecks
  - Incorrect highlighting: GPT-4o tagging errors or JSON parsing issues
  - Missing annotations: Frontend-backend communication failures or database write errors
  - Inconsistent scoring: LLM model variations or prompt template issues

- First 3 experiments:
  1. Test basic functionality: Upload a simple question with one student answer and verify mark and rationale generation
  2. Test highlighting feature: Verify that key elements and rationale aspects are correctly identified and visually highlighted
  3. Test annotation workflow: Use all three annotation types (ground-truth correction, preference selection, rationale submission) and verify they're properly stored in the database

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does AERA Chat's visualization of key answer elements and rationale aspects impact educator assessment accuracy and efficiency compared to traditional automated scoring systems?
- Basis in paper: [explicit] The paper states that AERA Chat provides "clear, high-contrast visual cues within student answers and assessment rationales" and highlights "key answer elements mentioned in the student answers or visualize the positive aspects... and negative aspects... within rationales."
- Why unresolved: While the system implements these visualization features, the paper does not present any empirical evaluation of their impact on assessment accuracy or efficiency compared to systems without such visualizations.
- What evidence would resolve it: Controlled user studies comparing assessment performance and time when using AERA Chat's visualization features versus traditional automated scoring systems, with metrics like inter-rater reliability and time per assessment.

### Open Question 2
- Question: What is the optimal balance between automated assessment and human-in-the-loop refinement in AERA Chat to maximize both assessment quality and efficiency?
- Basis in paper: [explicit] The paper describes three annotation functionalities: ground-truth label correction, human preference annotation, and rationale annotation, suggesting human involvement is important but doesn't specify optimal usage patterns.
- Why unresolved: The paper describes these features but doesn't provide guidance on when or how frequently educators should use them, or how this affects overall system performance.
- What evidence would resolve it: Empirical studies measuring assessment quality and efficiency across different levels of human intervention, identifying thresholds where additional human input provides diminishing returns.

### Open Question 3
- Question: How do different LLM backends (GPT-3.5-turbo, GPT-4o, and AERA model) compare in terms of assessment accuracy, rationale quality, and computational efficiency for educational assessment tasks?
- Basis in paper: [explicit] The paper states "our platform can automatically assess student answers and generate rationales with the user-selected LLMs" and mentions using GPT-3.5-turbo, GPT-4o, and a custom AERA model, but doesn't provide comparative analysis.
- Why unresolved: While multiple models are integrated, the paper doesn't present any comparative evaluation of their performance across the key metrics of interest.
- What evidence would resolve it: Systematic benchmarking of all supported LLM backends across diverse educational assessment tasks, measuring accuracy, rationale faithfulness, and computational costs.

## Limitations
- Quality and consistency of generated rationales depend heavily on underlying LLM's reasoning capabilities
- Effectiveness of human-in-the-loop refinement depends on user engagement and representativeness of feedback
- Performance across diverse educational domains and question types remains uncertain without extensive cross-validation

## Confidence
**High confidence**: The platform's architectural implementation (microservices, Docker containerization, React frontend, Flask backend) and basic functionality (automated scoring, rationale generation, visual highlighting) are well-specified and technically sound.

**Medium confidence**: The claimed improvements in trust and understanding through explainability are plausible given the literature on explainable AI, but would require empirical validation with actual users.

**Low confidence**: The long-term quality improvement claims from human-in-the-loop refinement lack quantitative evidence.

## Next Checks
1. Conduct a blind study where educators evaluate the accuracy and usefulness of LLM-generated rationales compared to human-written explanations. Measure inter-rater reliability and correlation with actual scoring accuracy.

2. Create a benchmark dataset of student answers with expert-identified key elements and rationale aspects. Measure the precision and recall of GPT-4o's highlighting against these ground-truth annotations across multiple question types.

3. Implement the annotation tools and collect preference data from at least 20 educators over one month. Analyze whether consistently selected rationales share identifiable features and whether these patterns correlate with scoring accuracy.