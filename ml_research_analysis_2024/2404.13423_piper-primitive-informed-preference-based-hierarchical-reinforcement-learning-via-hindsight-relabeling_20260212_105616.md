---
ver: rpa2
title: 'PIPER: Primitive-Informed Preference-based Hierarchical Reinforcement Learning
  via Hindsight Relabeling'
arxiv_id: '2404.13423'
source_url: https://arxiv.org/abs/2404.13423
tags:
- learning
- piper
- reward
- policy
- primitive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces PIPER, a hierarchical reinforcement learning
  approach that addresses two key challenges in HRL: non-stationarity due to changing
  lower-level policies and infeasible subgoal generation by higher-level policies.
  PIPER uses preference-based learning with a novel "primitive-in-the-loop" feedback
  mechanism to learn a high-level reward model, which is then used to relabel higher-level
  replay buffers and mitigate non-stationarity.'
---

# PIPER: Primitive-Informed Preference-based Hierarchical Reinforcement Learning via Hindsight Relabeling

## Quick Facts
- arXiv ID: 2404.13423
- Source URL: https://arxiv.org/abs/2404.13423
- Reference count: 15
- Key outcome: PIPER achieves over 50% success rates in challenging sparse-reward robotic tasks where most baselines fail to make significant progress

## Executive Summary
PIPER introduces a novel hierarchical reinforcement learning approach that addresses two critical challenges in HRL: non-stationarity from changing lower-level policies and infeasible subgoal generation by higher-level policies. The method combines preference-based learning with a "primitive-in-the-loop" feedback mechanism to learn a high-level reward model, which is then used to relabel higher-level replay buffers and mitigate non-stationarity. Additionally, PIPER incorporates primitive-informed regularization to encourage the higher-level policy to generate feasible subgoals for the lower-level primitive. Extensive experiments on sparse-reward robotic manipulation tasks demonstrate significant performance improvements over state-of-the-art baselines.

## Method Summary
PIPER is a hierarchical RL approach that learns to decompose complex tasks into subtasks using a higher-level policy that predicts subgoals for a lower-level policy (primitive). The method employs preference-based learning to train a reward model using primitive-in-the-loop feedback, which captures the relative quality of higher-level trajectories. This reward model is used to relabel higher-level replay buffers, addressing non-stationarity issues. PIPER also uses hindsight relabeling to generate additional preference data and improve sample efficiency. Primitive-informed regularization encourages the higher-level policy to predict subgoals that are achievable by the current lower-level policy.

## Key Results
- Achieves over 50% success rates in challenging sparse-reward robotic manipulation tasks
- Outperforms multiple baselines including hierarchical and non-hierarchical methods
- Demonstrates significant training speedup through hindsight relabeling in certain environments

## Why This Works (Mechanism)

### Mechanism 1: Non-stationarity mitigation
- **Claim:** PIPER mitigates non-stationarity in hierarchical reinforcement learning by relabeling higher-level replay buffer transitions using a preference-based reward model that is decoupled from lower-level policy changes.
- **Mechanism:** The higher-level replay buffer contains transitions with rewards generated by a lower-level policy. As the lower-level policy improves, these rewards become outdated. PIPER learns a preference-based reward model (br_ϕ) using primitive-in-the-loop feedback, which does not depend on the lower-level policy. This reward model is then used to relabel the higher-level replay buffer transitions, effectively removing the non-stationarity issue.
- **Confidence:** Low - primarily theoretical with weak empirical validation

### Mechanism 2: Primitive-informed regularization
- **Claim:** PIPER addresses the problem of infeasible subgoal generation by higher-level policies through primitive-informed regularization that encourages the higher-level policy to predict subgoals achievable by the current lower-level policy.
- **Mechanism:** The higher-level policy may predict subgoals that are too difficult for the current lower-level policy to achieve, leading to poor learning at the lower level. PIPER introduces a value function regularization term that encourages the higher-level policy to select subgoals with high value according to the current lower-level policy's value function. This ensures that the higher-level policy predicts subgoals that are feasible for the lower-level policy to achieve.
- **Confidence:** Medium - conceptually sound but lacks thorough empirical validation

### Mechanism 3: Hindsight relabeling for sample efficiency
- **Claim:** PIPER improves sample efficiency in preference-based learning by incorporating hindsight relabeling to reduce sparsity and increase the informativeness of observed trajectories.
- **Mechanism:** Standard goal-conditioned rewards in sparse-reward scenarios are too sparse for efficient learning. PIPER uses hindsight relabeling to relabel trajectories with achieved goals, generating additional preference data. This denser reward signal allows the preference model to learn more effectively, leading to improved sample efficiency.
- **Confidence:** Medium - shows mixed results across different environments

## Foundational Learning

- **Concept:** Hierarchical Reinforcement Learning (HRL)
  - **Why needed here:** PIPER is a hierarchical reinforcement learning approach that decomposes complex tasks into subtasks using a higher-level policy that predicts subgoals for a lower-level policy.
  - **Quick check question:** What are the key challenges in hierarchical reinforcement learning that PIPER aims to address?

- **Concept:** Preference-based Learning
  - **Why needed here:** PIPER uses preference-based learning to learn a reward model from preferences over trajectories, rather than relying on a predefined reward function.
  - **Quick check question:** How does preference-based learning differ from standard reinforcement learning, and what are its advantages in the context of HRL?

- **Concept:** Hindsight Experience Replay (HER)
  - **Why needed here:** PIPER incorporates hindsight relabeling, which is based on the idea of HER, to improve sample efficiency by relabeling trajectories with achieved goals.
  - **Quick check question:** What is the main idea behind hindsight experience replay, and how does it help in overcoming sparsity in goal-conditioned reinforcement learning?

## Architecture Onboarding

- **Component map:** Higher-level policy -> Lower-level policy (primitive) -> Environment -> Preference reward model (br_ϕ) -> Higher-level replay buffer -> Higher-level policy
- **Critical path:** Higher-level policy predicts subgoal → Lower-level policy executes actions to achieve subgoal → Environment provides sparse reward → Preference data generated using primitive-in-the-loop feedback → Reward model learned using preference-based learning → Higher-level replay buffer relabeled using reward model → Higher-level policy trained using relabeled data
- **Design tradeoffs:** Using preference-based learning instead of predefined rewards allows for more flexible reward shaping but requires additional computation for learning the reward model. Incorporating hindsight relabeling improves sample efficiency but may introduce noise if relabeled goals are not meaningful. Primitive-informed regularization ensures feasible subgoal generation but requires an accurate estimate of the lower-level policy's value function.
- **Failure signatures:** Poor performance on complex tasks may indicate issues with the preference reward model or infeasible subgoal generation. Slow learning could be due to insufficient preference data or ineffective hindsight relabeling. Unstable training may be caused by non-stationary rewards or improper tuning of the primitive-informed regularization weight.
- **First 3 experiments:**
  1. Validate that the preference reward model can accurately rank higher-level trajectories using primitive-in-the-loop feedback.
  2. Test the effectiveness of hindsight relabeling in improving sample efficiency by comparing learning curves with and without relabeling.
  3. Evaluate the impact of primitive-informed regularization on subgoal feasibility and overall performance by comparing with and without regularization.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the performance of PIPER scale with the dimensionality of the goal space, particularly in high-dimensional state spaces like images?
  - **Basis in paper:** [inferred] The paper mentions that L2 distance between states is used as the metric, which might be hard to compute in high-dimensional scenarios.
  - **Why unresolved:** The paper only explores tasks with relatively low-dimensional goal spaces and does not investigate the scalability of PIPER to high-dimensional spaces.
  - **What evidence would resolve it:** Experiments applying PIPER to tasks with high-dimensional goal spaces, such as image-based navigation or manipulation tasks, would demonstrate its scalability.

- **Open Question 2:** Can the primitive-in-the-loop (PiL) approach capture additional information present in human preferences, such as preferences for safe trajectories?
  - **Basis in paper:** [explicit] The paper acknowledges that human preferences may contain information about safety that PiL might miss.
  - **Why unresolved:** The paper proposes PiL as a replacement for human feedback but does not explore ways to incorporate additional information like safety preferences.
  - **What evidence would resolve it:** Experiments incorporating safety rewards or constraints into the PiL framework would demonstrate whether it can capture and utilize such information.

- **Open Question 3:** How sensitive is PIPER's performance to the choice of the primitive regularization weight hyperparameter α?
  - **Basis in paper:** [explicit] The paper shows that setting an appropriate value of α is crucial for improved performance, with both too small and too large values leading to poor results.
  - **Why unresolved:** While the paper provides some guidance on choosing α, it does not provide a systematic analysis of its sensitivity or strategies for tuning it.
  - **What evidence would resolve it:** A more comprehensive study of PIPER's performance across a wider range of α values and different tasks would provide insights into its sensitivity and tuning strategies.

## Limitations

- The evidence for non-stationarity mitigation relies heavily on theoretical claims rather than thorough empirical validation
- Mixed results for hindsight relabeling effectiveness across different environments suggest inconsistent sample efficiency improvements
- Limited exploration of PIPER's scalability to high-dimensional goal spaces like images

## Confidence

- Mechanism 1 (Non-stationarity mitigation): Low - primarily theoretical with weak empirical validation
- Mechanism 2 (Primitive-informed regularization): Medium - conceptually sound but lacks thorough empirical validation
- Mechanism 3 (Hindsight relabeling): Medium - shows mixed results across different environments

## Next Checks

1. Validate non-stationarity mitigation empirically by comparing reward distributions in higher-level replay buffers before and after relabeling across multiple training stages to measure the actual reduction in non-stationarity.

2. Test subgoal feasibility predictions by analyzing the correlation between the primitive-informed regularization terms and actual lower-level success rates for generated subgoals across different training epochs.

3. Quantify hindsight relabeling effectiveness by conducting ablation studies that measure learning curves with varying levels of relabeling frequency and different goal relabeling strategies to identify optimal configurations.