---
ver: rpa2
title: 'GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection'
arxiv_id: '2403.03507'
source_url: https://arxiv.org/abs/2403.03507
tags:
- galore
- low-rank
- gradient
- training
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GaLore addresses the high memory cost of training large language
  models by exploiting the low-rank structure of gradient matrices during training.
  Instead of approximating weights as low-rank like LoRA, GaLore projects gradients
  into low-rank form, allowing full-parameter learning while reducing optimizer memory
  usage by up to 65.5%.
---

# GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection

## Quick Facts
- arXiv ID: 2403.03507
- Source URL: https://arxiv.org/abs/2403.03507
- Authors: Jiawei Zhao; Zhenyu Zhang; Beidi Chen; Zhangyang Wang; Anima Anandkumar; Yuandong Tian
- Reference count: 40
- One-line primary result: Reduces optimizer memory usage by up to 65.5% while maintaining full-parameter learning capability

## Executive Summary
GaLore introduces a novel approach to memory-efficient training of large language models by exploiting the low-rank structure of gradient matrices during training. Unlike existing methods that approximate weights as low-rank (like LoRA), GaLore projects gradients into low-rank form, enabling full-parameter learning while dramatically reducing memory consumption. The method achieves comparable performance to full-rank training on LLaMA models up to 7B parameters and enables pre-training on a single 24GB GPU without memory offloading or model parallelism.

## Method Summary
GaLore implements gradient low-rank projection by computing SVD-based projection matrices P and Q that capture the most significant eigenvectors of gradient-related matrices. During training, gradients are projected into a low-rank subspace (P^T G Q) before being used by the optimizer, reducing memory requirements from O(mn) to O(r(m+n)). The method periodically recomputes projection matrices (every T iterations) to allow the training to traverse multiple low-rank subspaces, enabling full-parameter learning. GaLore is compatible with existing optimizers like AdamW, Adafactor, and 8-bit optimization techniques.

## Key Results
- Achieves up to 65.5% reduction in optimizer memory usage compared to full-rank training
- Maintains comparable pre-training performance to full-rank methods on LLaMA models (60M to 7B parameters)
- Outperforms existing low-rank methods on fine-tuning tasks including GLUE and SQuAD
- Enables pre-training of a 7B parameter model on a single 24GB GPU without memory offloading

## Why This Works (Mechanism)

### Mechanism 1
The gradient matrix G naturally becomes low-rank during training of reversible networks and transformers. The backpropagation structure creates gradients that are sums of low-rank updates, causing their stable rank to decrease over time. This occurs when the gradient has a parametric form G_t = 1/N Î£_i(A_i - B_i W_t C_i) with constant A_i, PSD matrices B_i and C_i after some training step t0.

### Mechanism 2
By projecting the gradient G into a low-rank subspace using projection matrices P and Q, the memory required for optimizer states can be substantially reduced. Instead of storing the full gradient G and its statistics, GaLore projects G into P^T G Q and only stores statistics of this projected gradient R_t = P^T G_t Q, reducing memory from O(mn) to O(r(m+n)).

### Mechanism 3
GaLore allows full-parameter learning by switching between multiple low-rank subspaces during training. By periodically recomputing P and Q based on the current gradient, the training can traverse through multiple low-rank subspaces, enabling learning of full-rank weights while still benefiting from memory efficiency.

## Foundational Learning

- **Low-rank matrix approximation**: Understanding that matrices can be approximated by lower-rank versions where the singular values decay rapidly. Why needed: GaLore relies on gradient matrices being approximable by low-rank matrices to reduce memory. Quick check: What is the relationship between matrix rank and singular values? How does low-rank approximation affect storage?

- **Projected gradient descent**: Optimization where gradients are projected into a constrained subspace before updates. Why needed: GaLore is essentially projected gradient descent in a low-rank subspace. Quick check: Under what conditions does projected gradient descent converge? How does projection choice affect convergence rate?

- **Memory-efficient optimization techniques**: Methods like LoRA, 8-bit optimizers, and ZeRO that reduce memory usage in large-scale ML. Why needed: GaLore is positioned among these techniques, and understanding their tradeoffs is crucial. Quick check: What are the main memory bottlenecks in LLM training? How do different memory-efficient techniques reduce memory usage?

## Architecture Onboarding

- **Component map**: Model parameters (W) -> Gradient computation (G) -> Projection matrices (P, Q) -> Optimizer (uses R_t = P^T G_t Q) -> Subspace switching (periodic recomputation of P, Q)

- **Critical path**: 
  1. Forward pass to compute loss
  2. Backward pass to compute gradient G
  3. Project G into low-rank subspace using P and Q to get R_t
  4. Update optimizer states using R_t
  5. Compute low-rank update N_t from optimizer
  6. Project N_t back to original space and update W
  7. Periodically recompute P and Q based on current gradient

- **Design tradeoffs**:
  - Rank r: Higher rank provides better gradient approximation but increases memory usage; lower rank reduces memory but may hurt performance
  - Subspace switching frequency T: Higher frequency allows adaptation to changing gradient subspaces but increases computational overhead; lower frequency reduces overhead but may cause suboptimal convergence
  - Optimizer choice: Different optimizers have varying state requirements, affecting memory savings

- **Failure signatures**:
  - Performance degradation if rank r is too low or subspace switching frequency is too low
  - Memory usage not reduced if gradient is not actually low-rank
  - Training instability if subspace switching frequency is too high

- **First 3 experiments**:
  1. Verify low-rankness of gradients by computing singular values of gradient matrix G during training for a small model and dataset
  2. Test memory savings by implementing GaLore for a small model and measuring optimizer memory usage compared to full-rank training
  3. Compare performance by training a small model using GaLore with different ranks r and comparing validation loss to full-rank training and LoRA

## Open Questions the Paper Calls Out

### Open Question 1
How does GaLore's performance scale to models larger than 7B parameters, such as those used in state-of-the-art LLMs? The paper demonstrates effectiveness on models up to 7B parameters but does not explore larger scales or provide theoretical analysis for scaling behavior.

### Open Question 2
How does the choice of projection matrices P and Q affect GaLore's convergence and performance? While the paper mentions that P and Q should project into subspaces corresponding to largest eigenvectors of certain matrices, it does not explore the impact of different choices or provide empirical studies.

### Open Question 3
Can GaLore be effectively combined with other memory-efficient techniques, such as quantization or sparsity, to further reduce memory usage? The paper mentions compatibility with 8-bit optimizers but does not explore combinations with other techniques beyond what is explicitly mentioned.

## Limitations

- The theoretical analysis assumes specific conditions on gradient matrix structure (PSD properties, full-rank B_i and C_i matrices) that may not hold universally across all architectures
- Empirical validation focuses on transformer-based models up to 7B parameters, leaving uncertainty about scalability to trillion-parameter models or non-transformer architectures
- Memory savings claims depend heavily on rank selection and subspace switching frequency, but sensitivity analysis is limited

## Confidence

- **High confidence** in the core mechanism: The gradient low-rank projection approach is mathematically sound and the memory reduction formula is well-established
- **Medium confidence** in the theoretical claims: While proofs show gradients become low-rank under certain conditions, practical applicability across diverse architectures needs more validation
- **Low confidence** in scalability claims: The paper demonstrates effectiveness on models up to 7B parameters, but claims about enabling pre-training on a single 24GB GPU may not generalize

## Next Checks

1. **Cross-architecture validation**: Test GaLore on non-transformer architectures (CNNs, RNNs, or vision transformers) to verify if the low-rank gradient assumption holds universally or is specific to transformer structures

2. **Long-term stability analysis**: Run extended training sessions (multiple epochs) to check if periodic subspace switching introduces long-term training instability or if optimizer states accumulate errors over time

3. **Ablation study on rank selection**: Systematically vary the rank r across multiple orders of magnitude (e.g., 1, 10, 100, 500, 1000) on a fixed task to precisely quantify the performance-memory tradeoff and identify optimal rank for different model sizes