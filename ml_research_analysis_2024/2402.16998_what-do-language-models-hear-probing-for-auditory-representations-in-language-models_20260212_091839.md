---
ver: rpa2
title: What Do Language Models Hear? Probing for Auditory Representations in Language
  Models
arxiv_id: '2402.16998'
source_url: https://arxiv.org/abs/2402.16998
tags:
- language
- sound
- representations
- audio
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates whether language models encode grounded
  representations of object sounds. The authors use a contrastive probe to align text
  representations of objects with their corresponding audio representations from pretrained
  audio models.
---

# What Do Language Models Hear? Probing for Auditory Representations in Language Models

## Quick Facts
- arXiv ID: 2402.16998
- Source URL: https://arxiv.org/abs/2402.16998
- Reference count: 12
- Key outcome: Language models encode nontrivial knowledge of object sounds despite being trained only on text, as evidenced by contrastive probe generalization above chance across multiple model combinations.

## Executive Summary
This work investigates whether language models encode grounded representations of object sounds by using a contrastive probe to align text representations with audio representations from pretrained audio models. The probe is trained to map language and sound representations close together for training objects and tested on its ability to generalize to unseen objects. Across six language models and three audio models, the probe generalizes above chance in many cases, with better performance for sound representations from supervised models. The results suggest that despite being trained only on raw text, language models encode nontrivial knowledge of sounds for some objects.

## Method Summary
The study extracts text representations from various language models using the template "the sound of [object]" and audio representations from pretrained audio models for corresponding sound clips from the FSD50K dataset. A contrastive probe learns linear transformations to align these representations by minimizing the distance between matching object pairs while maximizing distance between non-matching pairs. The probe is trained on 70% of object classes and tested on held-out classes, with performance measured by accuracy@K in retrieving correct objects given audio snippets. Results are averaged across 5 random class splits to assess generalization.

## Key Results
- The probe generalizes above chance across different language and audio model combinations, indicating language models encode grounded knowledge of sounds
- Larger language models consistently outperform smaller models within the same family
- Supervised audio models (PaSST, AudioMAE-FT) show better alignment with language models than self-supervised models (AudioMAE, PANN)
- Certain object categories (human speech, instruments) show more consistent generalization across language models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models encode grounded representations of object sounds through statistical co-occurrence patterns in text, even without explicit grounding.
- Mechanism: When language models process large amounts of text mentioning objects and their associated sounds, they learn statistical associations between words and the perceptual properties of those objects. The contrastive probe aligns these learned text representations with audio representations from pretrained audio models by minimizing the distance between representations of the same object across modalities.
- Core assumption: The geometry of text-based representations for objects can capture enough perceptual information about their associated sounds to enable meaningful alignment with audio representations.
- Evidence anchors:
  - [abstract] "Across different language models and audio models, we find that the probe generalization is above chance in many cases, indicating that despite being trained only on raw text, language models encode grounded knowledge of sounds for some objects."
  - [section 2.2] "If we learn W1, W2 via minimizing L(Ctrain) and these transformations generalize to Ctest, i.e., for c ∈ Ctest c = arg maxc′∈C sim(text(c′), sound(c)), then this would suggest that the language representation space Clanguage is structurally similar to the sound representation space Csound."

### Mechanism 2
- Claim: Supervised audio models that have been trained on sound event classification learn representations that better align with language models than self-supervised audio models.
- Mechanism: Supervised audio models learn to map acoustic signals to human-defined sound categories, which implicitly incorporates human perceptual priors about what constitutes a distinct sound event. This makes their representations more "auditory" (human perception-like) rather than purely "acoustic" (physical signal-like), creating better alignment with language models that encode textually-derived knowledge about sounds.
- Core assumption: Human perception of sounds and the way language describes those sounds share enough common structure that representations trained to predict human-labeled sound categories will align better with language representations than purely acoustic representations.
- Evidence anchors:
  - [section 3.1] "The representations from supervised models implicitly encode more human perception-like priors given that the classification task itself incorporates information about what snippet of sound constitutes a salient-enough signal to humans to warrant its being classified as a distinct event."
  - [section 4] "Across the different audio models, we find that alignment is overall better for sound representations from PaSST, which arguably is most aligned to human perception insofar as it is pretrained as an image classifier, then finetuned as a sound event classifier."

### Mechanism 3
- Claim: Larger language models generally perform better at encoding grounded representations of sounds than smaller models.
- Mechanism: Larger language models have more parameters and are trained on more data, allowing them to capture more nuanced statistical patterns and associations between words and their perceptual properties. This increased capacity enables better encoding of grounded knowledge about sounds.
- Core assumption: The increased capacity and training data of larger language models allows them to learn more detailed statistical associations between words and their perceptual properties, including sounds.
- Evidence anchors:
  - [section 4] "Within a model family, larger models almost always outperform their smaller siblings (e.g., BERT-Large vs. BERT, GPT-2-XL vs. GPT-2, T5-Large vs. T5)."
  - [section 3.2] "To extract the language representations from the above models for a given object c, we obtain a sentence using the template 'the sound of c' and average the contextualized representations for the tokens corresponding to c within the resulting sentence."

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: The probe is trained using a contrastive loss that pushes language and sound representations of the same object close together while pushing representations of different objects apart. Understanding how contrastive learning works is essential for understanding the probe's training mechanism.
  - Quick check question: How does the contrastive loss function push representations of the same object close together while pushing representations of different objects apart?

- Concept: Procrustes analysis
  - Why needed here: The preliminary study uses Procrustes analysis to qualitatively assess the alignment between language and sound representation spaces before conducting the main experiments. Understanding this technique helps explain the motivation for the contrastive probe approach.
  - Quick check question: What is the mathematical objective that Procrustes analysis minimizes when aligning two matrices?

- Concept: Masked autoencoders
  - Why needed here: One of the audio models (AudioMAE) is based on the masked autoencoder architecture, which is relevant for understanding its strengths and limitations as a source of audio representations for probing.
  - Quick check question: How does the masked autoencoder architecture work, and what kind of representations does it tend to learn?

## Architecture Onboarding

- Component map: Language models -> Text representations -> Contrastive probe <- Audio representations <- Audio models
- Critical path: Data preparation → Language model inference → Audio model inference → Contrastive probe training → Evaluation on held-out classes
- Design tradeoffs:
  - Using a simple linear probe vs. more complex architectures
  - Training on 70% of classes vs. using all available data
  - Using cosine similarity vs. other distance metrics
  - Using a single audio snippet per object vs. averaging multiple snippets
- Failure signatures:
  - Probe performs at chance level on held-out classes
  - Probe overfits to training classes and fails to generalize
  - Large variance in performance across different random splits
  - Certain object categories consistently underperform
- First 3 experiments:
  1. Run Procrustes analysis on a small subset of objects to visually inspect alignment between language and sound representations
  2. Train the contrastive probe on a single random split of classes and evaluate on held-out classes
  3. Compare performance of different audio models (AudioMAE vs. PaSST) on the same language model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do language models encode representations of lower-level acoustic features (e.g., raw spectrograms) that are not explicitly tied to labeled sound events?
- Basis in paper: [explicit] The authors note that their audio representations are not completely independent of language as their training sets included a significant amount of human speech. They suggest it would be interesting to see if audio models trained without any human speech learn representations that can be aligned to language models.
- Why unresolved: The current study only probes for representations of labeled sound events. Lower-level acoustic features that may be present in language models but not tied to specific events remain unexplored.
- What evidence would resolve it: Training audio models on datasets without human speech and testing their alignment with language models would provide evidence for or against the encoding of lower-level acoustic features.

### Open Question 2
- Question: Are there specific classes of objects for which language models consistently encode more grounded representations of their sounds across different model architectures and sizes?
- Basis in paper: [explicit] The authors analyze GPT-2-XL and find that classes corresponding to human speech and instruments seem to generalize well. They also calculate rank correlations of accuracies across different language models and find that there is a common set of classes for which sound representation is meaningfully encoded.
- Why unresolved: While the authors identify some classes that generalize well, they do not perform a comprehensive analysis across all model types and sizes to determine if there are consistent patterns.
- What evidence would resolve it: Conducting a thorough analysis of accuracy across all classes, model architectures, and sizes would reveal if there are specific object categories that consistently yield better alignment between language and sound representations.

### Open Question 3
- Question: How does the presence of human-like representations in audio models impact their alignment with language models compared to purely self-supervised models?
- Basis in paper: [explicit] The authors find that alignment is overall better for sound representations from PaSST, which is pretrained as an image classifier and then finetuned as a sound event classifier. They also note that self-supervised AudioMAE underperforms the supervised AudioMAE-FT, suggesting the importance of human-like representations.
- Why unresolved: While the authors observe differences in alignment between supervised and self-supervised models, they do not systematically investigate the impact of human-like representations on alignment.
- What evidence would resolve it: Conducting experiments with audio models trained with varying degrees of human-like supervision (e.g., different types of labels, different amounts of human speech) and measuring their alignment with language models would provide insights into the role of human-like representations in alignment.

## Limitations

- The study uses a relatively coarse linear alignment approach that may miss more complex relationships between text and audio representations
- Evaluation is limited to 100 object categories from FSD50K, potentially missing important edge cases or underrepresented sound types
- The study doesn't examine whether the encoded sound knowledge is useful for downstream tasks beyond the probe itself

## Confidence

**High confidence**: Language models encode some grounded knowledge of sounds, evidenced by above-chance probe generalization across multiple model combinations and consistent performance improvements with larger models.

**Medium confidence**: Supervised audio models align better with language models than self-supervised models, as the evidence shows PaSST (supervised) performs well but doesn't conclusively prove supervised models are universally superior across all language model types.

**Medium confidence**: Larger language models encode more grounded sound knowledge, as the pattern holds within model families but doesn't account for potential confounding factors like training data differences or architectural variations.

## Next Checks

1. **Probe architecture sensitivity**: Replicate the main experiments using nonlinear probe architectures (e.g., small MLPs) to determine if the linear probe is a limiting factor in capturing the relationship between text and audio representations.

2. **Cross-modal transfer**: Evaluate whether the sound representations learned by language models through this alignment process can improve performance on a downstream audio classification task when combined with the language model representations.

3. **Fine-grained semantic analysis**: Conduct qualitative analysis of specific object categories where the probe succeeds versus fails to identify whether successful cases reflect genuine perceptual grounding versus lexical patterns, examining both highly accurate and poorly performing categories.