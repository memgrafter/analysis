---
ver: rpa2
title: A Data-Centric Perspective on Evaluating Machine Learning Models for Tabular
  Data
arxiv_id: '2407.02112'
source_url: https://arxiv.org/abs/2407.02112
tags:
- data
- feature
- engineering
- datasets
- tabular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that model-centric evaluations in tabular
  data research are biased due to overly standardized preprocessing that fails to
  capture real-world modeling pipelines. The authors propose a data-centric evaluation
  framework that includes dataset-specific expert-level preprocessing pipelines and
  uses Kaggle competition leaderboards as an external performance reference.
---

# A Data-Centric Perspective on Evaluating Machine Learning Models for Tabular Data

## Quick Facts
- arXiv ID: 2407.02112
- Source URL: https://arxiv.org/abs/2407.02112
- Reference count: 40
- Primary result: Model-centric evaluations in tabular data research are biased due to overly standardized preprocessing, and dataset-specific feature engineering significantly changes model rankings and performance.

## Executive Summary
This paper demonstrates that traditional model-centric evaluations of machine learning models for tabular data are fundamentally flawed due to overly standardized preprocessing pipelines that fail to capture real-world modeling practices. The authors propose a data-centric evaluation framework that incorporates dataset-specific expert-level preprocessing and uses Kaggle competition leaderboards as external performance references. Their experiments show that when proper feature engineering is applied, model rankings change considerably, performance differences decrease, and the importance of model selection reduces. Recent models, including neural networks, still significantly benefit from manual feature engineering, suggesting that research efforts should shift toward a data-centric perspective that acknowledges the importance of feature engineering and temporal characteristics in tabular data.

## Method Summary
The authors developed a comprehensive evaluation framework that includes three preprocessing pipelines: standardized preprocessing, expert feature engineering, and test-time adaptation using unlabeled test data. They evaluated seven model implementations (XGBoost, LightGBM, CatBoost, ResNet, FTTransformer, MLP-PLR, and GRANDE) with three hyperparameter optimization regimes across 10 datasets from Kaggle competitions. The framework integrates with Kaggle's API to submit predictions and retrieve leaderboard percentiles as an external performance reference. Each model was trained using 5-fold cross-validation with an additional holdout fold for ensembling, and all experiments were conducted using the same random seed to ensure reproducibility.

## Key Results
- Model rankings change considerably after applying dataset-specific expert feature engineering, with some models improving by up to 21 percentage points in Kaggle leaderboard percentile
- Performance differences between models decrease significantly after expert preprocessing, reducing the importance of model selection
- Test-time adaptation using unlabeled test data provides substantial performance improvements, revealing that tabular datasets often have temporal characteristics or distribution shifts
- Neural networks, despite their recent advances, still benefit significantly from manual feature engineering on tabular data tasks

## Why This Works (Mechanism)

### Mechanism 1
Model-centric evaluations produce biased rankings because standardized preprocessing fails to capture dataset-specific feature engineering pipelines used in real-world applications. When preprocessing is overly standardized across datasets, models that perform well with generic preprocessing appear superior, but their advantage diminishes when dataset-specific feature engineering is applied. This shifts model rankings and reduces the importance of model selection. Core assumption: Real-world practitioners routinely apply dataset-specific preprocessing and feature engineering that are not captured in standardized evaluation setups.

### Mechanism 2
External performance references like Kaggle leaderboards reveal the true upper bounds of achievable performance on tabular data tasks. Using competition leaderboards as benchmarks shows that standardized academic evaluations only measure a fraction of possible performance, motivating the need for data-centric approaches. Core assumption: Kaggle competition leaderboards represent expert-level performance achievable on real-world tabular data tasks.

### Mechanism 3
Test-time adaptation (TTA) is crucial for tabular data even when datasets are treated as static, because distribution shifts exist despite i.i.d. assumptions. When models are allowed to adapt at test time using unlabeled test data, performance improves significantly, revealing that many tabular datasets have temporal characteristics or distribution shifts not captured in static evaluation setups. Core assumption: Tabular datasets often contain temporal characteristics or distribution shifts that violate the i.i.d. assumption, even when timestamps are not explicitly provided.

## Foundational Learning

- Concept: Feature engineering techniques for tabular data
  - Why needed here: The paper demonstrates that manual feature engineering remains crucial for achieving top performance on tabular data tasks, even with recent model advances.
  - Quick check question: What are three common feature engineering techniques mentioned in the paper that significantly improve tabular data model performance?

- Concept: Test-time adaptation for domain shifts
  - Why needed here: The paper shows that using unlabeled test data for feature engineering (test-time adaptation) can substantially improve performance on tabular data competitions.
  - Quick check question: How does test-time adaptation differ from traditional cross-validation approaches in machine learning?

- Concept: Kaggle competition evaluation framework
  - Why needed here: The paper uses Kaggle competitions as an external benchmark to evaluate model performance against expert solutions and understand real-world performance bounds.
  - Quick check question: What are the key advantages of using Kaggle competition leaderboards as external performance references?

## Architecture Onboarding

- Component map: Dataset loading -> Preprocessing pipeline selection -> Model training with cross-validation -> Hyperparameter optimization -> Test set prediction -> Kaggle API submission -> Leaderboard position retrieval
- Critical path: The framework loads Kaggle competition datasets, applies one of three preprocessing pipelines (standardized, expert feature engineering, or test-time adaptation), trains models with 5-fold cross-validation and ensembling, performs hyperparameter optimization, generates predictions, and submits to Kaggle to retrieve leaderboard percentiles.
- Design tradeoffs: Standardized preprocessing allows fair model comparison but may underestimate achievable performance; expert preprocessing captures real-world practices but introduces dataset-specific bias; test-time adaptation reveals distribution shift importance but may not be applicable to all real-world scenarios.
- Failure signatures: Poor performance on Kaggle leaderboards despite good standardized evaluation results suggests missing feature engineering; consistent improvement from test-time adaptation indicates distribution shifts; model rankings changing between preprocessing pipelines indicates standardized preprocessing bias.
- First 3 experiments:
  1. Run the standardized preprocessing pipeline with default hyperparameters on a sample dataset to establish baseline model rankings
  2. Implement the expert feature engineering pipeline for the same dataset and compare performance changes
  3. Apply test-time adaptation to the expert feature engineering pipeline and measure additional performance gains

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of tabular deep learning models change when using a carefully designed feature engineering pipeline specifically tailored for neural networks, beyond the general preprocessing used in this study? The paper shows that neural networks benefit from feature engineering on categorical features but does not explore whether model-specific feature engineering pipelines could further improve performance.

### Open Question 2
What is the minimum test set size required for test-time adaptation to be effective in tabular data competitions, and how does this threshold vary across different types of distribution shifts? The paper identifies test-time adaptation as important but does not quantify the minimum sample size needed or analyze different types of distribution shifts.

### Open Question 3
How do the findings about model-centric vs data-centric evaluation translate to other data modalities like text or images, where feature engineering practices differ significantly from tabular data? The paper focuses exclusively on tabular data and does not address whether similar biases exist in model-centric evaluations for other data modalities.

## Limitations
- The generalizability of findings to non-competition datasets and industrial tabular data remains unclear, as Kaggle competitions may have unique characteristics
- The selection of datasets and preprocessing pipelines may influence the observed ranking changes, potentially limiting broader applicability
- The impact of different evaluation metrics on model performance and ranking stability was not extensively explored

## Confidence
- Model ranking changes with expert preprocessing: **High** - Supported by multiple experiments and clear performance differences
- Kaggle leaderboards as performance references: **Medium** - Strong for competition context but limited evidence for broader applicability
- Test-time adaptation importance: **Medium** - Demonstrated improvements but may depend on specific dataset characteristics

## Next Checks
1. Replicate findings on industrial tabular datasets with temporal characteristics not from Kaggle competitions
2. Compare model rankings using different evaluation metrics (beyond the original competition metrics)
3. Investigate the relationship between dataset size, temporal properties, and the effectiveness of test-time adaptation