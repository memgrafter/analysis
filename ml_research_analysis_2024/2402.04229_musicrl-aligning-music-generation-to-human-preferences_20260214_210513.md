---
ver: rpa2
title: 'MusicRL: Aligning Music Generation to Human Preferences'
arxiv_id: '2402.04229'
source_url: https://arxiv.org/abs/2402.04229
tags:
- music
- reward
- human
- generation
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MusicRL, the first music generation system
  fine-tuned from human feedback. It addresses the challenge of aligning text-to-music
  models with subjective musical preferences by leveraging reinforcement learning.
---

# MusicRL: Aligning Music Generation to Human Preferences

## Quick Facts
- arXiv ID: 2402.04229
- Source URL: https://arxiv.org/abs/2402.04229
- Authors: Geoffrey Cideron; Sertan Girgin; Mauro Verzetti; Damien Vincent; Matej Kastelic; Zalán Borsos; Brian McWilliams; Victor Ungureanu; Olivier Bachem; Olivier Pietquin; Matthieu Geist; Léonard Hussenot; Neil Zeghidour; Andrea Agostinelli
- Reference count: 30
- Primary result: First text-to-music model fine-tuned from human feedback, achieving 87% win rate over baseline

## Executive Summary
MusicRL introduces a novel approach to aligning music generation with human preferences by applying reinforcement learning from human feedback (RLHF) to a pretrained MusicLM model. The system collects 300,000 pairwise user preferences and trains a reward model that captures subjective musical qualities beyond text adherence and audio quality. Through sequential fine-tuning on automatic metrics followed by human preference rewards, MusicRL significantly outperforms the baseline model in human evaluations, demonstrating that user preferences are driven by complex musical attributes rather than simple quality measures.

## Method Summary
MusicRL fine-tunes a pretrained MusicLM model using KL-regularized REINFORCE with three reward signals: MuLan score (text adherence), quality score (audio fidelity), and a user preference reward model trained on 300,000 pairwise comparisons. The training follows a sequential approach - first optimizing on automatic metrics to establish a strong base (MusicRL-R), then finetuning on user preference rewards (MusicRL-U), and finally combining both approaches (MusicRL-RU). The model generates music as sequences of discrete audio tokens, enabling compatibility with standard RL methods while maintaining audio quality through KL regularization.

## Key Results
- MusicRL-RU achieves 87% win rate against baseline MusicLM in human evaluations
- Sequential fine-tuning (RL-R then RL-RU) outperforms joint optimization of all three rewards
- Text adherence shows weak correlation with human preferences (51.6% MuLan preference match)
- User preference reward model maintains 60% accuracy even when trained without text tokens

## Why This Works (Mechanism)

### Mechanism 1
RLHF improves subjective musical appeal by optimizing for human pairwise preferences rather than proxy metrics. The reward model learns from 300k user comparisons, capturing complex musicality attributes beyond text adherence and audio quality, allowing the policy to optimize directly for what humans subjectively prefer.

### Mechanism 2
Sequential finetuning (RL-R then RL-RU) outperforms joint optimization by avoiding over-optimization of any single reward signal. First finetuning on quality and MuLan rewards establishes a strong base model, then the second stage refines this base without collapsing other attributes.

### Mechanism 3
Text adherence has limited impact on human preferences for music generation. When training the user preference reward model without text tokens, accuracy remains stable (~60%), indicating that human listeners prioritize other musical qualities over literal text matching.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: Musical quality is subjective and hard to encode automatically, requiring learning from human pairwise preferences
  - Quick check question: What are the three main components of the RLHF pipeline described in the paper?

- Concept: Autoregressive audio generation with discrete tokens
  - Why needed here: MusicLM generates music as sequences of discrete audio tokens, making it compatible with standard RL methods
  - Quick check question: How does MusicLM represent audio for generation, and why is this representation suitable for RL?

- Concept: KL regularization in RL finetuning
  - Why needed here: Prevents the policy from drifting too far from the pretrained model, maintaining audio quality and coherence
  - Quick check question: What is the role of the KL regularization term in the RL objective, and what could happen if it's set too low or too high?

## Architecture Onboarding

- Component map: Base MusicLM model -> Reward functions (MuLan, quality, user preference) -> KL-regularized REINFORCE -> Policy updates -> Music generation

- Critical path: 1) Generate music from captions using base MusicLM 2) Compute reward signals 3) Run RL training loop with policy gradient updates 4) Evaluate generations using both automatic metrics and human ratings

- Design tradeoffs: Automatic vs. human rewards (scalable vs. subjective), token-level vs. sequence-level optimization (stable vs. global structure), KL regularization strength (quality preservation vs. exploration)

- Failure signatures: Reward over-optimization (gaming rewards), mode collapse (repetitive generations), text adherence failure (irrelevant outputs)

- First 3 experiments: 1) Verify RL finetuning improves MuLan and quality scores while maintaining KL divergence 2) Test user preference reward model accuracy on held-out comparisons 3) Compare MusicRL-R and MusicRL-U head-to-head

## Open Questions the Paper Calls Out

- How can we effectively align feedback from general users with the evaluation process conducted by selected raters to minimize the population gap?

- What is the optimal approach for collecting on-policy data to iteratively improve the model's performance and user experience?

- How can we identify and retain examples where users express a confident and clear preference to reduce noise and improve the overall dataset quality?

- What techniques can be employed to train robust reward models on smaller, but highly relevant datasets to facilitate model personalization for specific users?

## Limitations

- Limited systematic analysis of prompt adherence versus musical quality trade-offs
- Evaluation methodology may have unmeasured biases from specific interface design
- Sequential training approach effectiveness not thoroughly compared against alternatives

## Confidence

High confidence in: Fundamental RLHF methodology and claim that human feedback improves subjective musical appeal

Medium confidence in: Specific superiority of sequential RL-RU approach over other training strategies

Low confidence in: Generalizability of findings to other domains or different user populations

## Next Checks

1. Conduct controlled experiment varying relative weight of text adherence versus musical quality in reward function to map preference landscape

2. Perform cross-population validation by collecting preference data from users with different musical backgrounds

3. Implement ablation study comparing MusicRL-RU against direct user preference finetuning and other reward combination strategies