---
ver: rpa2
title: 'Selection-p: Self-Supervised Task-Agnostic Prompt Compression for Faithfulness
  and Transferability'
arxiv_id: '2410.11786'
source_url: https://arxiv.org/abs/2410.11786
tags:
- compression
- tokens
- selection-p
- tasks
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Selection-p, a self-supervised prompt compression
  method for large language models that discretizes uninformative tokens during in-context
  learning. The approach uses a small number of additional parameters to produce token-level
  preservation probabilities, enabling up to 10x compression while maintaining performance
  with only 0.8% decrease across nine classification tasks.
---

# Selection-p: Self-Supervised Task-Agnostic Prompt Compression for Faithfulness and Transferability

## Quick Facts
- arXiv ID: 2410.11786
- Source URL: https://arxiv.org/abs/2410.11786
- Authors: Tsz Ting Chung; Leyang Cui; Lemao Liu; Xinting Huang; Shuming Shi; Dit-Yan Yeung
- Reference count: 11
- Key outcome: Achieves up to 10x compression with only 0.8% performance decrease across nine classification tasks while demonstrating superior transferability to different models including black-box models like ChatGPT and Gemini.

## Executive Summary
Selection-p introduces a self-supervised prompt compression method for large language models that discretizes uninformative tokens during in-context learning. The approach uses a small number of additional parameters to produce token-level preservation probabilities, enabling up to 10x compression while maintaining performance. By leveraging self-supervised pre-training without external data, Selection-p achieves 5.3x inference speedup and demonstrates superior transferability to different models compared to existing methods.

## Method Summary
Selection-p is a self-supervised prompt compression method that learns to identify and remove uninformative tokens from in-context demonstrations. The method introduces a small number of parameters (via LoRA) that create token preservation probabilities using the base model's last layer hidden states. During training, the model learns to compress context while maintaining language modeling ability through a self-supervised objective. For inference, a single forward pass generates a binary mask that discretely removes tokens, achieving efficient compression without iterative decoding. The approach is trained on 100M tokens from RedPajama and evaluated on nine classification tasks with 10x compression targets.

## Key Results
- Achieves up to 10x compression with only 0.8% performance decrease on nine classification tasks
- Demonstrates 5.3x inference speedup through single-pass compression
- Shows superior transferability to LLaMA-2-13B and black-box models (ChatGPT, Gemini)
- Maintains performance on long-context tasks (BANKING77) at 2K, 4K, and 7K token levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selection-p achieves better transferability by using a self-supervised pre-training technique without external data.
- Mechanism: The model learns to predict the next token based on compressed context, enabling it to generalize across different models.
- Core assumption: LLMs can autonomously learn to identify less informative tokens within a given context.
- Evidence anchors:
  - [abstract] "By introducing a small number of parameters during the continual pre-training, the proposed Selection-p produces a probability for each input token, indicating whether to preserve or discard it."
  - [section 3.2] "Our training criterion for the selection model aims to preserve the language modeling ability of the LLM while also learning to discard tokens effectively."
  - [corpus] Weak evidence - no direct comparison to other self-supervised approaches found.
- Break condition: If the self-supervised signal is not sufficient to distinguish informative from uninformative tokens across diverse contexts.

### Mechanism 2
- Claim: Selection-p outperforms existing compression methods by discretizing uninformative tokens during in-context learning.
- Mechanism: The model uses a forward pass to create token preservation probabilities, enabling efficient compression without iterative decoding.
- Core assumption: Redundant texts often exist in context and their removal does not hinder model understanding.
- Evidence anchors:
  - [abstract] "Experiments show Selection-p achieves state-of-the-art performance across numerous classification tasks, achieving compression rates of up to 10 times while experiencing only a marginal 0.8% decrease in performance."
  - [section 3.2] "To ensure the efficiency in performing compression, a single forward pass on our model creates the token preservation probability for all input tokens for simplicity."
  - [corpus] Weak evidence - no direct comparison to discrete compression methods found.
- Break condition: If the discretization process loses critical contextual information needed for downstream tasks.

### Mechanism 3
- Claim: Selection-p maintains performance on long-context in-context learning by chunk-wise compression.
- Mechanism: The model applies compression to segments of the context and concatenates the results, preserving information flow.
- Core assumption: Long-context models can handle concatenated compressed segments without performance degradation.
- Evidence anchors:
  - [section 4.3] "We further analyze if compression models work well in normal few-shot settings in classification tasks... The comparison to the result with the token size of 750 in Section 4.3 is presented in Table 6."
  - [section 5.2] "Our model can achieve 5.3x speed up on 10x compressed in-context demonstration."
  - [corpus] Weak evidence - no direct comparison to long-context compression methods found.
- Break condition: If the concatenation of compressed segments introduces boundary artifacts that disrupt context understanding.

## Foundational Learning

- Concept: In-context learning
  - Why needed here: Understanding how LLMs use demonstrations to perform tasks without fine-tuning is essential for grasping the purpose of prompt compression.
  - Quick check question: How does in-context learning differ from traditional fine-tuning in terms of model adaptation?

- Concept: Language modeling
  - Why needed here: The selection model builds upon language modeling principles to predict next tokens and identify informative content.
  - Quick check question: What is the relationship between language modeling and the self-supervised pre-training used in Selection-p?

- Concept: Transfer learning
  - Why needed here: Evaluating the transferability of compressed prompts to different models is a key contribution of this work.
  - Quick check question: How does the concept of transfer learning apply to the evaluation of prompt compression methods?

## Architecture Onboarding

- Component map: Input context → Selection model → Discretization → Compression → Inference
- Critical path: Input → Selection model → Discretization → Compression → Inference
- Design tradeoffs:
  - Single-pass compression vs. iterative decoding (LLMLingua)
  - Self-supervised pre-training vs. external data (LLMLingua-2)
  - Discretized compression vs. continuous vectors (AutoCompressor)
- Failure signatures:
  - Significant performance drop (>0.8%) with 10x compression
  - Poor transferability to different model architectures
  - Inability to maintain performance on long-context tasks
- First 3 experiments:
  1. Evaluate Selection-p on traditional classification tasks with 10x compression and compare to full-shot performance.
  2. Test transferability by compressing prompts with Selection-p and evaluating on LLaMA-2-13B and black-box models.
  3. Analyze performance on long-context classification tasks (BANKING77) at 2K, 4K, and 7K token levels with 10x compression.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several key uncertainties remain regarding the generalizability and limitations of the approach.

## Limitations

- The evaluation primarily focuses on classification tasks, leaving uncertainty about performance on other task types like code generation or mathematical reasoning.
- The transferability claims are limited to testing on LLaMA variants and black-box models without exploring fundamentally different architectures.
- The self-supervised pre-training mechanism's superiority over other approaches is not rigorously established through ablation studies.

## Confidence

**High Confidence (8/10):** The core compression mechanism and basic performance metrics are well-supported with clear experimental evidence.
**Medium Confidence (6/10):** Claims about transferability to black-box models are moderately supported but limited in scope and comparative analysis.
**Low Confidence (4/10):** The assertion that Selection-p's self-supervised approach is fundamentally superior lacks strong comparative evidence and ablation studies.

## Next Checks

1. **Cross-architecture transferability test**: Evaluate Selection-p-compressed prompts on diverse LLM architectures including BERT-based models, OPT variants, and specialized models to verify universal applicability.
2. **Ablation study on pre-training methodology**: Compare Selection-p's self-supervised pre-training against supervised and hybrid approaches to quantify the specific contribution to transferability.
3. **Long-context boundary analysis**: Conduct detailed error analysis on long-context tasks to identify boundary artifacts where critical information spans chunk boundaries.