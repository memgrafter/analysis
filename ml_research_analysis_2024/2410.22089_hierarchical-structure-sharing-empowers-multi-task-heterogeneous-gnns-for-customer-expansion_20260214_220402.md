---
ver: rpa2
title: Hierarchical Structure Sharing Empowers Multi-task Heterogeneous GNNs for Customer
  Expansion
arxiv_id: '2410.22089'
source_url: https://arxiv.org/abs/2410.22089
tags:
- tasks
- task
- information
- graph
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the negative transfer issue in multi-task
  learning on heterogeneous graphs, where MTL models often perform worse than single-task
  models due to conflicting information sharing across tasks. The authors propose
  InLINE, which enables fine-grained information exchange within each graph layer
  by disentangling structural patterns and using relation-aware gates to selectively
  share beneficial information.
---

# Hierarchical Structure Sharing Empowers Multi-task Heterogeneous GNNs for Customer Expansion

## Quick Facts
- arXiv ID: 2410.22089
- Source URL: https://arxiv.org/abs/2410.22089
- Reference count: 36
- Primary result: 51.41% average precision gain on private industry dataset

## Executive Summary
This paper addresses negative transfer in multi-task learning on heterogeneous graphs, where conflicting information sharing across tasks degrades performance. The authors propose InLINE, which enables fine-grained information exchange within each graph layer through structure disentangled experts and relation-aware gates. Evaluations on two public datasets (DBLP and Aminer) and one private industry dataset (Logistic) demonstrate significant improvements, with InLINE achieving 51.41% average precision gain on the private dataset and 10.52% macro F1 gain on DBLP.

## Method Summary
The proposed Inner-Layer Information Exchange (InLINE) model addresses negative transfer in multi-task learning on heterogeneous graphs by disentangling structural patterns within each graph layer using relation-aware gates. The architecture consists of T+1 blocks per layer (T task-specific + 1 shared), with Structure Disentangled Experts performing message operations without aggregation to maintain separate embeddings per inter-relation type, and Structure Aware Gates generating relation-aware weights for selective fusion across tasks. The model operates on heterogeneous graphs with multiple node and edge types, using 3 graph layers for DBLP/Aminer and 4 for Logistic, trained with Adam optimizer and evaluated using micro/macro F1 and AUC/AP metrics.

## Key Results
- 51.41% average precision gain on private industry Logistic dataset
- 10.52% macro F1 gain on DBLP dataset
- 41.67% improvement in contract-signing rate and 453K new orders generated in real-world deployment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Negative transfer in MTL on heterogeneous graphs occurs because inter-relation importance between nodes varies across tasks and gets entangled in output embeddings
- Mechanism: The model disentangles structural patterns within each graph layer using relation-aware gates, allowing selective sharing of beneficial information between tasks while avoiding detrimental interference
- Core assumption: Inter-relation importance differences across tasks are the primary source of negative transfer in heterogeneous graph MTL
- Evidence anchors:
  - [abstract]: "negative transfer in heterogeneous graphs arises not simply from the varying importance of an individual node feature across tasks, but also from the varying importance of the inter-relation between two nodes across tasks"
  - [section]: "negative transfer in heterogeneous graphs arises not simply from the varying importance of an individual node feature across tasks, but also from the varying importance of the inter-relation between two nodes across tasks"
  - [corpus]: Weak evidence - corpus contains related multi-task learning papers but none specifically address inter-relation importance in heterogeneous graphs

### Mechanism 2
- Claim: Inner-layer information exchange is more effective than outer-layer exchange because it operates before information entanglement
- Mechanism: The model performs information exchange within each graph layer rather than through output embeddings, preventing the mixing of beneficial and detrimental information
- Core assumption: Information exchange at the output embedding level is too coarse-grained to effectively discriminate beneficial from detrimental information
- Evidence anchors:
  - [abstract]: "we propose the Inner-Layer Information Exchange (InLINE) model that facilitates fine-grained information exchange within each graph layer rather than through the output embedding from these layers"
  - [section]: "these methods only exchange information at the output embedding level, which is not fine-grained enough to address negative transfer in heterogeneous graph data"
  - [corpus]: Weak evidence - corpus contains multi-task learning methods but none specifically discuss inner-layer vs outer-layer exchange in heterogeneous graphs

### Mechanism 3
- Claim: Structure Disentangled Experts and Structure Aware Gates work together to enable selective information sharing based on inter-relation types
- Mechanism: Experts disentangle neighboring nodes with different inter-relations without aggregation, then gates generate relation-aware weights for selective fusion across tasks
- Core assumption: Separating the disentanglement and gating functions allows more precise control over information sharing than monolithic approaches
- Evidence anchors:
  - [abstract]: "Structure Disentangled Experts for layer-wise structure disentanglement and (2) Structure Aware Gates for assigning disentangled information to different tasks"
  - [section]: "our expert executes the Message operation on neighboring nodes without aggregation, allowing the subsequent gate networks to discriminate and exchange information across tasks more easily"
  - [corpus]: Weak evidence - corpus contains expert-based multi-task learning but none specifically address structure disentanglement in heterogeneous graphs

## Foundational Learning

- Concept: Heterogeneous graph neural networks
  - Why needed here: The entire method operates on heterogeneous graphs with multiple node and edge types, requiring understanding of specialized GNN architectures
  - Quick check question: What distinguishes heterogeneous graph neural networks from homogeneous ones in terms of node/edge types and aggregation strategies?

- Concept: Multi-task learning negative transfer
  - Why needed here: The paper specifically addresses negative transfer in MTL, where tasks interfere with each other during training
  - Quick check question: How does negative transfer manifest differently in graph-based MTL compared to non-graph MTL?

- Concept: Graph attention mechanisms
  - Why needed here: The method uses attention weights to determine inter-relation importance and guide information sharing between tasks
  - Quick check question: How do attention mechanisms in heterogeneous graphs account for different edge types when computing node importance?

## Architecture Onboarding

- Component map: Input layer -> InLINE layers (T+1 blocks) -> Structure Disentangled Experts -> Structure Aware Gates -> Task-specific prediction heads -> Output

- Critical path:
  1. Message operation extracts information from neighboring nodes
  2. Structure Disentangled Experts maintain separate embeddings per inter-relation type
  3. Structure Aware Gates generate weights for each edge type
  4. Combination step fuses information based on task-specific needs
  5. Attention and aggregation produce final node representations
  6. Task-specific prediction heads generate outputs

- Design tradeoffs:
  - Inner-layer vs outer-layer exchange: More precise but computationally heavier
  - Separate experts per task vs shared experts: More specialized but requires more parameters
  - Relation-aware gating vs task-aware gating: More fine-grained but requires more computation

- Failure signatures:
  - Performance degradation on all tasks simultaneously: Likely indicates issues with the base GNN architecture
  - Improvement on some tasks but degradation on others: Suggests improper gating or expert specialization
  - No improvement over baseline MTL: Indicates the disentanglement process may be losing important information

- First 3 experiments:
  1. Implement a single InLINE layer and compare performance against baseline MTL on a simple heterogeneous graph
  2. Gradually add more InLINE layers and measure improvement in negative transfer mitigation
  3. Compare inner-layer exchange vs outer-layer exchange with identical expert/gate architectures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical limit on how many experts can effectively address negative transfer in heterogeneous graphs, given that experts struggle to exactly learn what we expect?
- Basis in paper: [explicit] The paper mentions that while ideally more experts could specialize in learning specific inter-relation types to avoid information blending, experts struggle to exactly learn what we expect (Tang et al. 2020), and simply increasing the number of experts still fails to solve the negative transfer issue.
- Why unresolved: The paper demonstrates that increasing experts doesn't improve baseline MTL models, but doesn't establish a theoretical limit on expert effectiveness or identify the optimal number of experts needed to address negative transfer.
- What evidence would resolve it: Experiments systematically varying the number of experts across different graph structures and task complexities to identify the point of diminishing returns, combined with analysis of expert specialization quality.

### Open Question 2
- Question: How does the effectiveness of inner-layer information exchange scale with graph size and average node degree in heterogeneous graphs?
- Basis in paper: [inferred] The paper analyzes time complexity and shows that InLINE and PLE perform similarly despite DBLP's high average node degree (8.6), but notes that a large number of tasks can cause computational challenges as the number of experts grows with the number of tasks.
- Why unresolved: While the paper provides time complexity analysis and shows comparable performance between InLINE and PLE, it doesn't systematically explore how inner-layer information exchange effectiveness changes with graph size or node degree variations.
- What evidence would resolve it: Comprehensive experiments testing InLINE on graphs with varying sizes and node degrees, measuring both performance and computational efficiency to establish scaling relationships.

### Open Question 3
- Question: What are the fundamental limitations of disentangling inter-relations within graph layers that prevent complete elimination of negative transfer in MTL?
- Basis in paper: [explicit] The paper shows that InLINE successfully captures inter-relation importance divergence and mitigates performance drops, but acknowledges that "negative transfer does not affect all tasks equally" and InLINE shows improvement only on tasks more affected by negative transfer.
- Why unresolved: The paper demonstrates InLINE's effectiveness but doesn't fully explain why some tasks remain less affected or what fundamental barriers prevent complete elimination of negative transfer through disentanglement alone.
- What evidence would resolve it: Detailed analysis of remaining performance gaps across different task pairs and graph structures, combined with investigation of whether certain inter-relation patterns are inherently difficult to disentangle or if other factors contribute to residual negative transfer.

## Limitations
- Limited empirical validation scope across only three datasets from similar domains
- Computational complexity concerns with inner-layer information exchange not thoroughly benchmarked
- Evaluation methodology gaps including lack of ablation studies and comparison against recent SOTA methods

## Confidence
- High confidence: The negative transfer problem in heterogeneous graph MTL is well-documented and the proposed disentanglement approach addresses a real technical challenge
- Medium confidence: The claimed improvements (51.41% AP gain on private dataset, 10.52% F1 gain on DBLP) are substantial but lack statistical significance testing across multiple runs
- Medium confidence: The industrial deployment results (41.67% contract-signing rate improvement, 453K new orders) are impressive but may reflect deployment-specific factors beyond model performance

## Next Checks
1. **Ablation study**: Remove Structure Disentangled Experts or Structure Aware Gates individually to quantify their separate contributions to negative transfer mitigation
2. **Cross-domain evaluation**: Test InLINE on heterogeneous graphs from different domains (e.g., social networks, molecular graphs) to assess generalizability
3. **Computational overhead analysis**: Measure training/inference time and memory usage compared to baseline MTL methods to evaluate practical deployment viability