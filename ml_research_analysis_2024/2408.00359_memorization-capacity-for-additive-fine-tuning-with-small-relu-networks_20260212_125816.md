---
ver: rpa2
title: Memorization Capacity for Additive Fine-Tuning with Small ReLU Networks
arxiv_id: '2408.00359'
source_url: https://arxiv.org/abs/2408.00359
tags:
- network
- layer
- fine-tuning
- neurons
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the memorization capacity of neural networks
  under the additive fine-tuning setting, where a frozen pre-trained network is fine-tuned
  by adding a small neural network. The authors introduce a new metric called Fine-Tuning
  Capacity (FTC) to measure the maximum number of samples that can be fine-tuned.
---

# Memorization Capacity for Additive Fine-Tuning with Small ReLU Networks

## Quick Facts
- arXiv ID: 2408.00359
- Source URL: https://arxiv.org/abs/2408.00359
- Authors: Jy-yong Sohn; Dohyun Kwon; Seoyeon An; Kangwook Lee
- Reference count: 38
- Primary result: N samples can be fine-tuned with Θ(√N) neurons using 3-layer ReLU networks

## Executive Summary
This paper introduces the concept of Fine-Tuning Capacity (FTC) to measure the maximum number of samples that can be fine-tuned when adding a small neural network to a frozen pre-trained network. The authors establish tight bounds showing that for 2-layer ReLU networks, Θ(N) neurons are needed to fine-tune N samples, while for 3-layer ReLU networks, only Θ(√N) neurons are required. These results demonstrate that deeper networks can achieve significant efficiency gains in additive fine-tuning scenarios, recovering known memorization capacity results when all samples need to be fine-tuned.

## Method Summary
The paper studies additive fine-tuning where a frozen pre-trained network f is fine-tuned by adding a small neural network g. The FTC metric measures the minimum number of neurons needed to arbitrarily change N labels among K samples. For theoretical analysis, the authors use synthetic data with normally distributed features and uniformly distributed labels, training a 3-layer fully-connected neural network with ReLU activation for fine-tuning. The fine-tuning performance is evaluated using the FTC metric and mean-squared-error loss.

## Key Results
- For 2-layer ReLU networks, Θ(N) neurons are required to fine-tune N samples
- For 3-layer ReLU networks, Θ(√N) neurons are sufficient to fine-tune N samples, regardless of total samples K
- The FTC framework recovers standard memorization capacity bounds when N ≈ K
- Deeper networks achieve sublinear scaling in neuron requirements versus sample count

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The number of neurons required to fine-tune N samples grows sublinearly with N for deeper ReLU networks due to piecewise linear function complexity.
- Mechanism: For a 3-layer ReLU network, the fine-tuned network output is a piecewise linear function. Each additional neuron adds complexity by increasing the number of linear pieces. The proof shows that the minimum number of pieces needed to fit N samples scales as Θ(√N), hence the number of neurons needed is Θ(√N).
- Core assumption: The pre-trained network already fits all samples except for a subset T of size N, so only these N samples need correction.
- Evidence anchors:
  - [abstract] "For 3-layer ReLU networks, they show that N samples can be fine-tuned with Θ(√N) neurons, no matter how large K is."
  - [section 5] "As stated in the proof of Theorem 3.3 of [Yun et al., 2019], ¯gθ(t) has 2d1d2 + d2 + 1 pieces, where d1 and d2 are the number of neurons in layer 1 and 2, respectively."
- Break condition: If the pre-trained network doesn't already fit the majority of samples (N ≈ K), the advantage of additive fine-tuning disappears and the bounds revert to full memorization capacity.

### Mechanism 2
- Claim: The additive fine-tuning setting allows decoupling of the memorization task by leveraging the pre-trained network's existing fit.
- Mechanism: Since the pre-trained network f already satisfies f(xi) = yi for all i ∈ [K] \ T, the fine-tuning network g only needs to correct the N samples in T. This reduces the memorization burden from K samples to N samples, leading to the improved Θ(√N) bound for 3-layer networks versus Θ(N) for 2-layer networks.
- Core assumption: The pre-trained network f is sufficiently expressive to fit all but N samples from the dataset.
- Evidence anchors:
  - [abstract] "Our aim is to add a neural network gθ to the pre-trained network f in a way that the fine-tuned network f + gθ satisfies (f + gθ)(xi) = yi for all i ∈ [K]."
  - [section 3] "Our new measure, the Fine-Tuning Capacity (FTC), is defined as the maximum number of samples a neural network can fine-tune, or equivalently, as the minimum number of neurons (m) needed to arbitrarily change N labels among K samples."
- Break condition: If the pre-trained network requires significant correction on most samples (N ≈ K), the fine-tuning network must essentially memorize the entire dataset, losing the efficiency gain.

### Mechanism 3
- Claim: The consecutive partition technique enables efficient neuron allocation by grouping samples with similar activation patterns.
- Mechanism: The proof constructs a consecutive partition P([K] \ T) of samples not needing fine-tuning. By strategically removing interior points of each partition block and keeping only endpoints, the network can efficiently represent the required corrections with fewer neurons. The bound depends on the size of these partition blocks.
- Core assumption: The feature space allows some ordering (aT x1 < aT x2 < ... < aT xK) for some projection vector a.
- Evidence anchors:
  - [section 4.2] "The main strategy is to remove data points so that equation 13 holds. Specifically, except for two endpoints of each block P ∈ P([K] \ T), we remove i ∈ [K] \ T from [K]."
  - [section 4.4] "By Lemma 4.7, we conclude that m ≤ K − 1 − max{K − 3N − 2, 0} = min{3N + 1, K − 1}."
- Break condition: If the feature space cannot be ordered meaningfully (no suitable projection vector a exists), the consecutive partition technique cannot be applied, potentially increasing the required neuron count.

## Foundational Learning

- Concept: Memorization capacity of neural networks
  - Why needed here: FTC is defined as an extension of memorization capacity to the fine-tuning scenario. Understanding standard memorization bounds provides context for why FTC achieves better scaling.
  - Quick check question: What is the memorization capacity bound for 2-layer ReLU networks with N samples?

- Concept: Piecewise linear function complexity
  - Why needed here: The proofs rely on counting the number of linear pieces needed to fit the target function. Each ReLU neuron contributes to the piecewise linear structure.
  - Quick check question: How many linear pieces does a 3-layer ReLU network with d1 and d2 neurons in layers 1 and 2 have?

- Concept: Consecutive partition and block decomposition
  - Why needed here: The key technique for proving upper bounds involves partitioning the sample indices and strategically removing points to reduce neuron requirements.
  - Quick check question: Given a set I = {1, 2, 3, 5, 6, 8, 10, 11, 12, 13}, what is its consecutive partition P(I)?

## Architecture Onboarding

- Component map: Pre-trained network f (frozen) -> Fine-tuning network g (trainable, small) -> Additive combination f + g -> Target dataset D with N samples needing correction

- Critical path:
  1. Identify samples T where f(xi) ≠ yi
  2. Construct fine-tuning network g with minimal neurons
  3. Train g to correct only samples in T
  4. Combine f + g to achieve perfect fit on all K samples

- Design tradeoffs:
  - Depth vs width: Deeper networks require fewer neurons (Θ(√N) vs Θ(N))
  - Pre-training quality: Better pre-trained fit (smaller N) reduces fine-tuning burden
  - Activation functions: ReLU chosen for piecewise linear properties enabling tight bounds

- Failure signatures:
  - If N grows large (N ≈ K), fine-tuning efficiency degrades
  - If pre-trained network fits poorly, additive fine-tuning cannot compensate
  - If feature space lacks ordering, consecutive partition technique fails

- First 3 experiments:
  1. Synthetic data test: Generate K=1000 samples, randomly select N=100 samples to modify labels, verify that 3-layer network with ~30 neurons achieves small fine-tuning loss
  2. Depth comparison: Compare 2-layer vs 3-layer network performance on same fine-tuning task, verify Θ(N) vs Θ(√N) scaling
  3. Pre-training quality: Vary the number of samples the pre-trained network fits correctly (N from 1 to K), measure how required neurons scale with N

## Open Questions the Paper Calls Out
The paper mentions that their theoretical results are limited to additive fine-tuning and extending them to other popular methods like LoRA is a future work. The current theoretical framework is specifically designed for additive fine-tuning scenarios.

## Limitations
- Theoretical bounds rely on idealized assumptions about pre-trained network quality
- Consecutive partition technique requires specific geometric properties of feature space
- Analysis focuses on ReLU networks specifically, results may not generalize to other activation functions

## Confidence
- High Confidence: The Θ(√N) bound for 3-layer networks given perfect pre-training and ordered feature space
- Medium Confidence: The general FTC framework applicability to real-world fine-tuning scenarios  
- Low Confidence: The tightness of bounds when N approaches K or when pre-training is imperfect

## Next Checks
1. **Robustness to Pre-training Quality**: Test FTC bounds across varying levels of pre-trained network accuracy (from 50% to 99% correct predictions) to identify the threshold where additive fine-tuning loses its efficiency advantage.

2. **Feature Space Structure Dependence**: Systematically evaluate how FTC bounds change with different feature space structures - ordered vs. clustered vs. random distributions - to quantify the impact of the consecutive partition assumption.

3. **Scaling Verification**: Conduct empirical validation of the theoretical scaling laws (Θ(N) for 2-layer vs Θ(√N) for 3-layer) across multiple orders of magnitude in N (10 to 10,000 samples) to confirm the predicted sublinear scaling holds in practice.