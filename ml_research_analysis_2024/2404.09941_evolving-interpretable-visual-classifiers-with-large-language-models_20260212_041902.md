---
ver: rpa2
title: Evolving Interpretable Visual Classifiers with Large Language Models
arxiv_id: '2404.09941'
source_url: https://arxiv.org/abs/2404.09941
tags:
- class
- attributes
- visual
- learning
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLM-Mutate, a novel method that uses large
  language models to learn interpretable visual classifiers by discovering discriminative
  attributes from images without relying on class names or external knowledge. The
  core idea is to integrate evolutionary search with LLMs, where the LLM generates
  mutations to iteratively improve a concept bottleneck of attributes for classification.
---

# Evolving Interpretable Visual Classifiers with Large Language Models

## Quick Facts
- arXiv ID: 2404.09941
- Source URL: https://arxiv.org/abs/2404.09941
- Authors: Mia Chiquier; Utkarsh Mall; Carl Vondrick
- Reference count: 40
- Outperforms state-of-the-art by 18.4% and 22.2% on iNaturalist and KikiBouba datasets respectively

## Executive Summary
This paper introduces LLM-Mutate, a novel method that uses large language models to learn interpretable visual classifiers by discovering discriminative attributes from images without relying on class names or external knowledge. The core idea is to integrate evolutionary search with LLMs, where the LLM generates mutations to iteratively improve a concept bottleneck of attributes for classification. The method is evaluated on fine-grained iNaturalist datasets (5 families of plants and animals) and novel KikiBouba datasets, outperforming state-of-the-art baselines by 18.4% and 22.2% respectively, despite the baselines having access to privileged information about class names. The approach produces interpretable attributes that are highly discriminative, as demonstrated by qualitative and quantitative results, including improved confidence scores and the ability to audit dataset bias.

## Method Summary
LLM-Mutate uses an evolutionary search framework where the mutation step is replaced by a large language model that generates attribute mutations based on in-context learning from past attribute sets and their performance scores. The method first discovers common attributes through binary classifiers trained to separate one class from all others, then uses these attributes to initialize joint multi-class training. CLIP scores serve as the fitness function to evaluate how well attribute descriptions match image content. The LLM generates new attribute sets that are scored by CLIP, with the best performers selected and added to an attribute bank. This process iterates until convergence, producing interpretable attributes for visual classification.

## Key Results
- Achieves 18.4% higher accuracy than state-of-the-art baselines on iNaturalist fine-grained classification
- Improves accuracy by 22.2% on novel KikiBouba datasets compared to baseline methods
- Produces highly discriminative and interpretable attributes that enable dataset bias auditing
- Successfully discovers meaningful attributes without access to class names or external knowledge

## Why This Works (Mechanism)

### Mechanism 1
LLM-guided mutation replaces inefficient random search in evolutionary algorithms. Instead of randomly generating attribute mutations, the LLM uses in-context learning on past attribute sets and their performance scores to predict high-utility mutations. The core assumption is that LLMs can generalize from few in-context examples to predict attributes that will improve the objective function.

### Mechanism 2
Pre-training with binary classifiers discovers common attributes before fine-grained classification. Initial binary classifiers are trained to separate one class from all others (including unrelated classes), encouraging the discovery of attributes common across the dataset. These attributes initialize the joint multi-class training. The core assumption is that discovering common attributes first helps the joint training focus on class-specific discriminative features rather than spurious noise.

### Mechanism 3
CLIP scores as the fitness function enable optimization over discrete attributes. The vision-language model (CLIP) scores how well each attribute describes an image, providing differentiable-like feedback for the evolutionary search over discrete attribute sets. The core assumption is that CLIP's text-image similarity scores are meaningful proxies for attribute presence and can guide the search toward discriminative attributes.

## Foundational Learning

- Concept: Evolutionary algorithms and their components (mutation, crossover, selection)
  - Why needed here: The optimization problem involves discrete attributes and a non-differentiable loss, making gradient-based methods unsuitable.
  - Quick check question: What are the three main operations in an evolutionary algorithm, and what does each accomplish?

- Concept: Large language models and in-context learning
  - Why needed here: The LLM replaces the inefficient random mutation step by predicting high-utility attribute mutations based on past performance.
  - Quick check question: How does in-context learning differ from traditional fine-tuning, and why is it suitable for this mutation task?

- Concept: Vision-language models (specifically CLIP)
  - Why needed here: CLIP provides the scoring function that evaluates how well attribute descriptions match image content, guiding the evolutionary search.
  - Quick check question: What is the core mechanism by which CLIP computes similarity between text and images?

## Architecture Onboarding

- Component map:
  Evolutionary Search Engine -> LLM Module -> CLIP Model -> Pre-training Module -> Joint Training Module

- Critical path:
  1. Pre-train binary classifiers to discover common attributes
  2. Initialize attribute bank with pre-trained attributes
  3. Iteratively mutate attribute sets using LLM
  4. Score mutated sets using CLIP
  5. Select best performers and add to bank
  6. Repeat until convergence

- Design tradeoffs:
  - LLM size vs. mutation quality: Larger models may generate better mutations but are more expensive
  - In-context examples length vs. diversity: More examples may improve quality but reduce diversity
  - CLIP model size vs. scoring accuracy: Larger CLIP models may provide better guidance but increase computational cost

- Failure signatures:
  - LLM consistently generates attributes unrelated to the images
  - CLIP scores do not correlate with actual discriminative power
  - Attribute bank converges to similar attributes across classes (lack of diversity)
  - Pre-training fails to discover useful common attributes

- First 3 experiments:
  1. Test LLM-guided mutation vs. random mutation on a simple binary classification task
  2. Evaluate CLIP scoring quality by manually checking top-scoring attributes against images
  3. Test pre-training effectiveness by comparing joint training with and without pre-training initialization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLM-Mutate be extended to discover attributes for multi-modal tasks involving both images and text?
- Basis in paper: The paper focuses on visual classification, but the method's reliance on LLMs suggests potential for extension to other modalities.
- Why unresolved: The paper does not explore multi-modal tasks beyond visual classification, leaving the question of how well LLM-Mutate generalizes to other modalities unanswered.
- What evidence would resolve it: Experiments applying LLM-Mutate to multi-modal tasks like visual question answering or image captioning, demonstrating its effectiveness in discovering attributes for both visual and textual inputs.

### Open Question 2
- Question: Can LLM-Mutate be used to discover attributes for abstract or conceptual categories that are not directly observable in images?
- Basis in paper: The paper shows LLM-Mutate can discover attributes for fine-grained visual categories, but it is unclear if it can handle abstract concepts.
- Why unresolved: The paper does not address the challenge of discovering attributes for abstract concepts, leaving the question of LLM-Mutate's limitations in this area open.
- What evidence would resolve it: Experiments applying LLM-Mutate to abstract categories like emotions or ideas, evaluating its ability to discover relevant and discriminative attributes.

### Open Question 3
- Question: How does the performance of LLM-Mutate compare to other methods for discovering attributes in the presence of noisy or incomplete data?
- Basis in paper: The paper mentions the potential for LLM-Mutate to be used in specialized domains with infrequent data, but does not explicitly compare its performance to other methods in such scenarios.
- Why unresolved: The paper does not provide a comprehensive comparison of LLM-Mutate's performance against other attribute discovery methods in the presence of noise or missing data.
- What evidence would resolve it: Experiments comparing LLM-Mutate's performance to other attribute discovery methods on datasets with varying levels of noise or missing data, demonstrating its robustness and effectiveness in such scenarios.

## Limitations

- Computational overhead from iterative LLM queries and CLIP scoring limits practical deployment on larger datasets
- Evaluation focuses on specific fine-grained datasets, leaving generalizability to other visual domains untested
- Reliance on CLIP scores assumes CLIP's attribute detection capabilities are robust across all domains

## Confidence

**High Confidence Claims:**
- The LLM-guided mutation mechanism replaces random search with more targeted attribute generation
- CLIP-based scoring provides meaningful feedback for evolutionary optimization
- The pre-training strategy helps discover common attributes that benefit joint training
- The method produces interpretable attributes that humans can understand

**Medium Confidence Claims:**
- The 18.4% and 22.2% performance improvements over baselines
- The attributes are genuinely more discriminative than alternative approaches
- The method's robustness to dataset bias

**Low Confidence Claims:**
- The method's scalability to much larger datasets
- Performance on visual domains outside the evaluation scope
- Computational efficiency for real-world deployment

## Next Checks

1. **Cross-domain robustness test**: Evaluate the method on completely different visual domains (e.g., medical imaging, satellite imagery, industrial defect detection) to assess generalizability beyond biological categories.

2. **Ablation on computational costs**: Systematically measure the contribution of each LLM generation and CLIP scoring iteration to overall performance, establishing a cost-benefit tradeoff curve.

3. **Human interpretability validation**: Conduct human studies where domain experts evaluate the semantic quality and discriminative power of generated attributes compared to ground truth or expert-defined attributes.