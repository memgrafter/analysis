---
ver: rpa2
title: "The lazy (NTK) and rich ($\u03BC$P) regimes: a gentle tutorial"
arxiv_id: '2404.19719'
source_url: https://arxiv.org/abs/2404.19719
tags:
- training
- learning
- gradient
- richness
- scale
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This tutorial explores the fundamental relationship between neural\
  \ network width and training behavior, particularly the transition between lazy\
  \ (NTK) and rich (\xB5P) regimes. The key finding is that there exists only one\
  \ degree of freedom in choosing hyperparameters (learning rate, weight initialization\
  \ scale) for wide networks, which controls the richness of training behavior."
---

# The lazy (NTK) and rich ($μ$P) regimes: a gentle tutorial

## Quick Facts
- arXiv ID: 2404.19719
- Source URL: https://arxiv.org/abs/2404.19719
- Authors: Dhruva Karkada
- Reference count: 24
- Primary result: A single degree of freedom controls the richness of training behavior in wide neural networks

## Executive Summary
This tutorial explores the fundamental relationship between neural network width and training behavior, particularly the transition between lazy (NTK) and rich ($μ$P) regimes. The key finding is that there exists only one degree of freedom in choosing hyperparameters (learning rate, weight initialization scale) for wide networks, which controls the richness of training behavior. The author derives this by analyzing a 3-layer linear model and showing that satisfying three training criteria constrains the hyperparameters to a single parameter that scales the hidden representation updates.

## Method Summary
The method analyzes a 3-layer linear model using scaling arguments to understand how hyperparameters affect training behavior as network width increases. The analysis derives three training criteria (nontriviality, useful updates, and maximality) that must be satisfied for effective training. These criteria constrain nine initial degrees of freedom down to one free parameter controlling the richness of training. The theory is validated empirically on both simple models and practical architectures like CNNs.

## Key Results
- There exists exactly one degree of freedom in choosing hyperparameters for wide networks that controls training richness
- The richness scale spans from lazy training (NTK) at r=0 to rich feature learning at r=1/2
- Standard parameterization lies outside the richness scale and causes instability at large widths
- Small initial outputs are necessary for representation learning in the infinite-width limit

## Why This Works (Mechanism)

### Mechanism 1
- Claim: There exists only one degree of freedom in choosing hyperparameters for wide networks, which controls the richness of training behavior.
- Mechanism: Three training criteria (nontriviality, useful updates, maximality) constrain the nine initial degrees of freedom down to one free parameter that scales hidden representation updates ∥∆h∥.
- Core assumption: Wide networks can be analyzed through scaling arguments focusing on relative sizes of feedforward signals and backpropagation gradients.
- Evidence anchors:
  - [abstract] "there is only one degree of freedom in choosing hyperparameters such as the learning rate and the size of the initial weights"
  - [section 2.2] "Satisfying the six constraints leaves three remaining degrees of freedom... The remaining degree of freedom will control the richness of training"
  - [corpus] Weak evidence - no directly relevant corpus entries discussing the single degree of freedom constraint
- Break condition: The constraint breaks if the network is not sufficiently wide, or if nonlinear activation functions introduce additional scaling relationships.

### Mechanism 2
- Claim: Lazy training corresponds to a linearized kernel regime; representation learning necessarily does not.
- Mechanism: The model is linearized if the curvature term ∆θ⊤H∆θ ≪ ∆θ⊤∇θf(x;θ0), which occurs when ∥∆h∥²/n ≪ 1. This condition is satisfied everywhere except at µP where ∥∆h∥ ∼ √n.
- Core assumption: The size of representation updates relative to network width determines whether second-order terms in the Taylor expansion become negligible.
- Evidence anchors:
  - [section 2.3.4] "if ∆θ is a gradient descent update, the first-order gradient term scales like ∆θ⊤(∇θf(x;θ0))∼1, while the second-order curvature term scales like ∆θ⊤(∇²θf(x;θ0))∆θ∼∥∆h∥²/n"
  - [section A] "The only point on the richness scale which avoids this condition is µP, ∥∆h∥∼√n"
  - [corpus] Weak evidence - no directly relevant corpus entries discussing the linearization condition
- Break condition: The linearization breaks if the network width is finite, or if the learning rate is too large causing representation updates to scale faster than √n.

### Mechanism 3
- Claim: Small initial outputs are necessary for representation learning in the infinite-width limit.
- Mechanism: Since ∥h₃∥ ∼ 1/∥∆h∥ and feature learning requires ∥∆h∥ ∼ √n in the n→∞ limit, initial outputs must scale as 1/√n to enable rich training.
- Core assumption: The output layer must satisfy the nontriviality criterion while hidden layers are free to have smaller activations.
- Evidence anchors:
  - [section 2.3.5] "Since ∥h₃∥∼1/∥∆h∥, in order to achieve feature learning in the n→∞ limit, we must necessarily have small outputs at initialization"
  - [section 2.2] "We do know that the output activations shouldn't blow up with width, or else the size of the error signal ∥y−h₃∥ will also blow up"
  - [corpus] Weak evidence - no directly relevant corpus entries discussing initial output scaling
- Break condition: The requirement breaks if the network width is finite, or if the learning rate is adjusted to compensate for larger initial outputs.

## Foundational Learning

- Concept: Scaling analysis of neural networks
  - Why needed here: The entire derivation relies on understanding how various quantities scale with network width, using asymptotic relations like a ∼ b to denote "scales with"
  - Quick check question: If a quantity scales as n^0.3 with width n, would it be considered width-independent in this framework?

- Concept: Central-limit-style arguments for random matrices
  - Why needed here: Used repeatedly to simplify expressions involving sums over random matrix elements, assuming independence and zero mean
  - Quick check question: When evaluating g²ℓσ²ℓnℓ₋₁ for random Gaussian weights, what distribution does this quantity converge to as nℓ₋₁ → ∞?

- Concept: Gauge freedom in parameterization
  - Why needed here: Understanding that layerwise learning rates and gradient multipliers are equivalent ways to control update sizes, introducing redundancy in the parameter space
  - Quick check question: If you double all learning rates and halve all gradient multipliers, how does this affect the training dynamics?

## Architecture Onboarding

- Component map: Weight matrices Wℓ -> Gradient multipliers gℓ -> Hidden representations hℓ -> Output layer

- Critical path: Width scaling → Constraint satisfaction → Single degree of freedom identification → Richness scale definition → Training regime characterization

- Design tradeoffs:
  - Standard parameterization (gℓ=1, σℓ∼1/√nℓ₋₁) lies outside the richness scale and causes instability at large widths
  - Model rescaling requires careful choice of γ = n^r to stay within the richness scale
  - Layerwise learning rates introduce gauge freedom but don't change fundamental training dynamics

- Failure signatures:
  - Training divergence at large widths indicates parameters outside the richness scale
  - Very slow convergence suggests r < 0 regime
  - Output explosion indicates insufficient control of initial output scaling

- First 3 experiments:
  1. Train 3-layer linear model on Gaussian data across widths r ∈ [0, 0.5] to verify NTC, UUC, MAX constraints
  2. Measure change in gradient across optimization step to confirm linearization condition ∆θ⊤H∆θ ∼ ∥∆h∥²/n
  3. Apply model rescaling to standard parameterization and verify that γ = n^r restores training stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mechanism by which the richness scale parameter r controls the performance gap between NTK learners and practical neural networks?
- Basis in paper: [explicit] The paper states that "understanding the rich regime as well as we understand the kernel regime is arguably the major open problem in developing a scientific theory of practical deep learning" and suggests the richness scale may enable systematic study of this performance gap.
- Why unresolved: While the paper derives the richness scale and shows it controls the transition between lazy and rich training regimes, it does not provide a complete theory explaining how different values of r quantitatively affect generalization performance or the performance gap with NTK models.
- What evidence would resolve it: Empirical studies measuring generalization performance across different values of r on practical datasets, combined with theoretical analysis of how r affects the learned representations' alignment with the data distribution.

### Open Question 2
- Question: How does the richness scale behave in the presence of nonlinearities, particularly ReLU activations, and how does this affect the derived scaling relationships?
- Basis in paper: [inferred] The paper derives the richness scale for a 3-layer linear model and provides empirical validation on a CNN with ReLU activations, but the theoretical analysis is limited to linear models.
- Why unresolved: The analysis in Appendix A shows that the linearization condition depends on the curvature term scaling as Δh²/n, but this relationship may change significantly when nonlinear activations are introduced, potentially altering the critical values of r for the lazy/rich transition.
- What evidence would resolve it: Rigorous theoretical analysis of the curvature scaling in nonlinear networks, combined with empirical measurements of the linearization condition across different activation functions and network depths.

### Open Question 3
- Question: What is the relationship between the richness scale and the empirical phenomena associated with large learning rates in practical deep learning?
- Basis in paper: [explicit] The paper suggests that "richer architectures have larger gradient multipliers, so rich training is likely related to the empirical phenomena associated with large learning rates" but notes this requires further investigation.
- Why unresolved: While the paper establishes a theoretical connection between the richness scale and gradient multiplier size, it does not explore how this relates to the practical challenges and benefits of training with large learning rates, such as stability issues and improved generalization.
- What evidence would resolve it: Systematic experiments varying both the richness scale parameter and learning rate magnitude to identify their independent and combined effects on training stability and generalization, potentially revealing whether they are distinct mechanisms or different manifestations of the same underlying phenomenon.

## Limitations
- The theory relies heavily on central-limit-style approximations that become exact only in the infinite-width limit
- The analysis focuses on linear networks, with limited theoretical extension to nonlinear networks
- The three training criteria are derived from intuitive arguments rather than formal proofs

## Confidence
- Core mathematical derivation of richness scale: High
- Extension to finite-width networks and practical architectures: Medium
- Training criteria necessity: Medium
- Single degree of freedom claim: Medium

## Next Checks
1. Test the richness scale predictions across different activation functions (ReLU, GELU, Swish) to verify the theory extends beyond linear networks.
2. Empirically measure the linearization condition ∆θ⊤H∆θ ∼ ∥∆h∥²/n across different training regimes to validate the NTK correspondence.
3. Apply the derived scaling rules to modern architectures (ResNets, Transformers) and measure training stability and convergence speed as functions of the richness parameter.