---
ver: rpa2
title: 'LLM-Inference-Bench: Inference Benchmarking of Large Language Models on AI
  Accelerators'
arxiv_id: '2411.00136'
source_url: https://arxiv.org/abs/2411.00136
tags:
- a100
- size
- h100
- batch
- vllm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents LLM-Inference-Bench, a comprehensive benchmarking
  suite for evaluating the inference performance of large language models (LLMs) across
  diverse AI accelerators and frameworks. The study benchmarks models like LLaMA-2-7B,
  LLaMA-3-8B, Mistral-7B, Mixtral-8x7B, Qwen-2-7B, and LLaMA-3-70B across hardware
  platforms including NVIDIA A100, H100, GH200, AMD MI250, MI300X, Intel Habana Gaudi2,
  and SambaNova SN40L.
---

# LLM-Inference-Bench: Inference Benchmarking of Large Language Models on AI Accelerators

## Quick Facts
- **arXiv ID**: 2411.00136
- **Source URL**: https://arxiv.org/abs/2411.00136
- **Reference count**: 40
- **Key outcome**: Comprehensive benchmarking suite evaluating LLM inference across diverse AI accelerators, frameworks, and models

## Executive Summary
This paper introduces LLM-Inference-Bench, a comprehensive benchmarking suite for evaluating large language model (LLM) inference performance across diverse AI accelerators and frameworks. The study benchmarks models including LLaMA-2-7B, LLaMA-3-8B, Mistral-7B, Mixtral-8x7B, Qwen-2-7B, and LLaMA-3-70B across hardware platforms such as NVIDIA A100, H100, GH200, AMD MI250, MI300X, Intel Habana Gaudi2, and SambaNova SN40L. The evaluation reveals significant performance variations across frameworks, with TensorRT-LLM delivering the highest throughput on NVIDIA GPUs, vLLM providing broader hardware support, and specialized accelerators offering competitive performance in specific scenarios. The results emphasize the critical importance of matching model architecture, framework optimizations, and hardware capabilities to achieve optimal LLM inference performance.

## Method Summary
The paper presents a comprehensive benchmarking methodology that evaluates LLM inference across multiple dimensions: model architectures (including LLaMA, Mistral, and Qwen families with 7B and 70B parameters), AI accelerators (NVIDIA GPUs, AMD GPUs, Intel Habana Gaudi2, and SambaNova SN40L), and inference frameworks (TensorRT-LLM, vLLM, DeepSpeed-MII, llama.cpp). The evaluation measures key performance metrics including throughput, time-to-first-token, inter-token latency, and perplexity. The benchmarking approach includes systematic testing across different batch sizes and precision configurations (FP16, FP8, Int8) to provide a holistic view of inference performance characteristics across the diverse hardware and software stack.

## Key Results
- TensorRT-LLM delivers highest throughput on NVIDIA GPUs, while vLLM offers broader hardware support
- Model architecture significantly impacts performance, with GQA models outperforming MHSA models on optimized frameworks
- Specialized accelerators like SN40L and Gaudi2 provide competitive performance in specific scenarios, with SN40L excelling at smaller batch sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model architecture (MHSA vs GQA) strongly influences inference throughput due to different compute and memory demands.
- Mechanism: MHSA uses unique query/key/value heads for each attention head, increasing compute and memory usage, while GQA shares key/value heads among groups, reducing parameters and improving throughput on frameworks that optimize for it.
- Core assumption: Frameworks and hardware are properly optimized for the attention type used in the model.
- Evidence anchors:
  - [abstract] "We thoroughly analyze diverse hardware platforms, including GPUs from Nvidia and AMD and specialized AI accelerators, Intel Habana and SambaNova. Our evaluation includes several LLM inference frameworks and models from LLaMA, Mistral, and Qwen families with 7B and 70B parameters."
  - [section] "We see increasing throughput with increasing batch sizes due to GPU processing more input sequences simultaneously and fixed costs such as kernel launch times occurring only once for the entire batch."
  - [corpus] Weak - no corpus paper directly addresses MHSA vs GQA performance comparison.
- Break condition: If the framework or hardware does not support the specific attention optimization (e.g., llama.cpp not fully utilizing GQA).

### Mechanism 2
- Claim: Quantization (FP8, Int8) improves throughput on supported hardware by reducing precision and memory footprint.
- Mechanism: Lower precision reduces the amount of data transferred and processed, allowing faster computation and more efficient memory usage, especially when the hardware has specialized support for low-precision operations.
- Core assumption: The hardware platform supports the quantization precision used and the framework can leverage it effectively.
- Evidence anchors:
  - [abstract] "Our evaluation includes several LLM inference frameworks and models from LLaMA, Mistral, and Qwen families with 7B and 70B parameters across a variety of AI accelerators."
  - [section] "Figure 3 compares FP16, FP8 and Int8 precision using vLLM and TRT-LLM on A100 and H100. We observe that FP8 on H100 and Int8 on A100 can provide performance benefit compared to FP16..."
  - [corpus] Weak - corpus papers discuss quantization but not specific to LLM inference benchmarking.
- Break condition: If the hardware lacks support for the quantization precision or the framework cannot effectively use it.

### Mechanism 3
- Claim: Hardware accelerators with specialized AI architectures (SN40L, Gaudi2) can outperform traditional GPUs in specific scenarios due to optimized memory hierarchies and compute units.
- Mechanism: Specialized accelerators use unique memory systems (e.g., 3-tier in SN40L) and heterogeneous compute units (e.g., MME and TPC in Gaudi2) to reduce data movement and overlap computation, leading to higher throughput for certain model sizes and batch configurations.
- Core assumption: The model and framework are compatible with the accelerator's architecture and optimization capabilities.
- Evidence anchors:
  - [abstract] "Our evaluation includes several LLM inference frameworks and models from LLaMA, Mistral, and Qwen families with 7B and 70B parameters across a variety of AI accelerators, including Nvidia A100, H100, GH200, AMD MI250, MI300X, Intel Habana Gaudi2, and SambaNova SN40L."
  - [section] "The accelerator has a 3-tier memory system unlike the traditional 2-tier memory system in GPUs."
  - [corpus] Moderate - corpus paper "Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective" discusses hardware optimization for LLMs.
- Break condition: If the model is not supported by the accelerator or the framework cannot utilize the specialized features.

## Foundational Learning

- Concept: Large Language Models (LLMs) architecture and components (transformer, attention mechanisms, dense vs MoE).
  - Why needed here: Understanding the model architecture is crucial for interpreting performance differences across hardware and frameworks.
  - Quick check question: What is the difference between MHSA and GQA, and how does it affect compute and memory usage?

- Concept: AI accelerators (GPUs, specialized accelerators) and their key features (memory bandwidth, compute units, precision support).
  - Why needed here: Knowing the hardware capabilities is essential for understanding performance bottlenecks and optimization opportunities.
  - Quick check question: What are the key differences between the memory systems of traditional GPUs and specialized accelerators like SN40L?

- Concept: LLM inference frameworks (TensorRT-LLM, vLLM, DeepSpeed-MII, llama.cpp) and their optimization techniques.
  - Why needed here: Different frameworks offer varying levels of optimization for specific hardware and model architectures, impacting performance.
  - Quick check question: What are the main optimization techniques used by TensorRT-LLM and vLLM, and how do they differ?

## Architecture Onboarding

- Component map: Models (LLaMA-2-7B, LLaMA-2-70B, LLaMA-3-8B, LLaMA-3-70B, Mistral-7B, Mixtral-8x7B, Qwen-2-7B, Qwen-2-72B) -> Frameworks (TensorRT-LLM, vLLM, DeepSpeed-MII, llama.cpp) -> Hardware (NVIDIA A100, H100, GH200, AMD MI250, MI300X, Intel Habana Gaudi2, SambaNova SN40L) -> Metrics (Throughput, TTFT, ITL, perplexity, power consumption)

- Critical path: Model -> Framework -> Hardware -> Metrics
  - The model architecture determines the computational requirements, the framework optimizes for specific hardware and model features, the hardware provides the compute and memory resources, and the metrics quantify the performance.

- Design tradeoffs:
  - Framework choice: TensorRT-LLM offers highest performance on NVIDIA GPUs but limited hardware support, while vLLM supports broader hardware but may have lower performance.
  - Hardware selection: NVIDIA GPUs offer mature software ecosystem and optimizations, while specialized accelerators may provide better performance for specific scenarios but have limited framework support.
  - Model selection: Smaller models offer better throughput, while larger models or MoE models may provide better performance per parameter but require more memory and compute.

- Failure signatures:
  - Low throughput: Could be due to model size exceeding hardware memory, framework not optimized for the model architecture, or hardware limitations.
  - High TTFT: Could indicate inefficient attention computation, memory access patterns, or framework overhead.
  - Memory issues: Could be caused by large model size, inefficient memory management in the framework, or hardware memory limitations.

- First 3 experiments:
  1. Run a small model (e.g., LLaMA-2-7B) on a single GPU using TensorRT-LLM to establish a baseline performance.
  2. Compare the performance of the same model using vLLM on the same GPU to understand the impact of the framework.
  3. Scale up to a larger model (e.g., LLaMA-3-70B) on multiple GPUs using TensorRT-LLM to test scalability and identify potential bottlenecks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GQA models compare to MHSA models across different frameworks and hardware?
- Basis in paper: [explicit] The paper states that GQA models like LLaMA-3-8B and Mistral-7B outperform MHSA models like LLaMA-2-7B with TensorRT-LLM and vLLM, but the opposite is observed with llama.cpp and Deepspeed-MII.
- Why unresolved: The performance discrepancy between GQA and MHSA models across different frameworks and hardware is not fully explained. The paper mentions that llama.cpp and Deepspeed-MII do not support model-wise optimizations well, but the exact reasons for this are not detailed.
- What evidence would resolve it: A comprehensive analysis of the optimization techniques used by each framework and their impact on GQA and MHSA models would help explain the performance differences. Additionally, testing GQA and MHSA models with other frameworks that support advanced optimizations could provide further insights.

### Open Question 2
- Question: How does the performance of AI accelerators like SambaNova SN40L and Habana Gaudi2 scale with an increasing number of computing chips?
- Basis in paper: [explicit] The paper mentions that SN40L has the best performance up to batch size 32, and Gaudi2 faces out-of-memory issues for large batch sizes. However, the scalability of these accelerators with an increasing number of computing chips is not fully explored.
- Why unresolved: The paper only tests a limited number of configurations for SN40L and Gaudi2. Testing these accelerators with a larger number of computing chips and varying batch sizes could provide insights into their scalability.
- What evidence would resolve it: Benchmarking SN40L and Gaudi2 with a wider range of computing chip configurations and batch sizes would help determine their scalability. Additionally, comparing their performance to GPUs like H100 and A100 at scale would provide a more comprehensive understanding of their capabilities.

### Open Question 3
- Question: How do different quantization methods (e.g., FP8, Int8) impact the performance and power consumption of LLMs across various hardware platforms?
- Basis in paper: [explicit] The paper compares the performance of LLaMA-3-8B using different quantization methods (FP16, FP8, Int8) on A100 and H100 GPUs. However, the impact of quantization on power consumption is not explored.
- Why unresolved: The paper focuses on the performance benefits of quantization but does not investigate its impact on power consumption. Understanding how different quantization methods affect power consumption is crucial for optimizing LLM inference.
- What evidence would resolve it: Benchmarking LLMs with different quantization methods on various hardware platforms while measuring power consumption would help determine the optimal quantization strategy for each platform. Additionally, exploring the trade-offs between performance and power consumption for different quantization methods would provide valuable insights for LLM deployment.

## Limitations

- The evaluation focuses on specific model families (LLaMA, Mistral, Qwen) and may not represent the full diversity of LLM architectures
- Framework optimizations are hardware-specific - TensorRT-LLM's superior performance on NVIDIA GPUs may not translate to other hardware platforms
- The study uses synthetic benchmarking workloads which may not reflect real-world deployment scenarios with variable input patterns

## Confidence

**High Confidence**: Framework performance comparisons within the same hardware family (e.g., TensorRT-LLM vs vLLM on NVIDIA GPUs). The methodology uses consistent benchmarking approaches and the results show clear, repeatable patterns.

**Medium Confidence**: Cross-hardware platform comparisons. While the benchmarking methodology is sound, differences in software maturity, driver versions, and framework support across heterogeneous hardware platforms introduce variability that's difficult to control.

**Medium Confidence**: Model architecture performance impacts (MHSA vs GQA). The observed performance differences align with theoretical expectations about memory access patterns and computational requirements, but the study doesn't isolate these effects from framework optimizations.

**Low Confidence**: Quantization benefits across all hardware platforms. The paper shows mixed results with quantization, and the benefits appear highly dependent on specific hardware-software combinations rather than being universally applicable.

## Next Checks

1. **Real-world workload validation**: Deploy selected models and frameworks on production-like workloads with variable input lengths and patterns to verify that synthetic benchmark results translate to actual deployment scenarios.

2. **Framework portability assessment**: Test whether TensorRT-LLM optimizations for NVIDIA GPUs can be replicated on AMD and Intel platforms using equivalent techniques, or whether the performance gap is inherent to the framework's design limitations.

3. **Memory efficiency analysis**: Conduct detailed memory bandwidth utilization measurements across all hardware platforms to determine whether performance differences stem from computational throughput or memory system bottlenecks.