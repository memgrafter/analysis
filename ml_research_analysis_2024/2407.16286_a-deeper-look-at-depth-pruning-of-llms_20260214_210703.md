---
ver: rpa2
title: A deeper look at depth pruning of LLMs
arxiv_id: '2407.16286'
source_url: https://arxiv.org/abs/2407.16286
tags:
- block
- loss
- relative
- cosine
- shapley
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work examines depth pruning of large language models (LLMs)\
  \ to reduce inference costs while preserving downstream performance. It compares\
  \ multiple block influence metrics\u2014including static (cosine distance, relative\
  \ norms) and adaptive (Shapley value)\u2014to identify less important transformer\
  \ blocks for removal."
---

# A deeper look at depth pruning of LLMs

## Quick Facts
- arXiv ID: 2407.16286
- Source URL: https://arxiv.org/abs/2407.16286
- Reference count: 40
- This work examines depth pruning of large language models (LLMs) to reduce inference costs while preserving downstream performance.

## Executive Summary
This paper investigates depth pruning strategies for large language models, focusing on identifying and removing less important transformer blocks to reduce inference costs while maintaining performance. The authors evaluate multiple block influence metrics including static measures (cosine distance, relative norms) and adaptive methods (Shapley value) to rank blocks for pruning. They also examine the trade-offs between pruning complete blocks versus individual self-attention and feed-forward layers, finding that self-attention layers are more amenable to pruning. The study evaluates two lightweight recovery techniques: an empirical mean block update and low-rank linear adapters, finding surprisingly that the simple emulated update performs comparably or better than learned adapters.

## Method Summary
The study computes block influence using multiple metrics: cosine distance, relative L1/L2 norms, and Shapley value based on loss or logits. These influence scores rank transformer blocks for pruning, with experiments conducted on LLaMa-2 7b and Mistral 7b models using 150k sequences from OpenWebText for calibration. The primary evaluation metric is 5-shot MMLU accuracy, with additional tasks including GSM-8k, ARC, and BoolQ. Two recovery techniques are evaluated: an empirical mean block update ("emulated update") that uses the average update from later blocks, and low-rank linear adapters trained with different strategies. The methods are tested by incrementally pruning blocks and measuring performance degradation across multiple tasks.

## Key Results
- Adaptive influence metrics (Shapley value) can improve task-specific performance (e.g., MMLU) but may hurt other tasks not considered during pruning.
- Self-attention layers are more amenable to pruning than feed-forward layers, with up to 33% removable without degrading MMLU accuracy while providing KV-cache efficiency gains.
- The simple emulated update recovery technique performs on par with or better than learned low-rank adapters, potentially due to adapter overfitting and catastrophic forgetting.

## Why This Works (Mechanism)
The paper demonstrates that transformer block importance varies across tasks, with some blocks contributing more to certain downstream tasks than others. By computing influence metrics that capture how much each block contributes to task performance, the method can identify blocks that can be removed with minimal impact. The surprising effectiveness of the emulated update suggests that differences between later blocks can be captured by simple statistical averaging, avoiding the overfitting issues that plague learned adapters. The finding that self-attention layers are more prunable than feed-forward layers indicates architectural differences in their contributions to model performance.

## Foundational Learning
**Transformer block structure**: Understanding that each transformer block contains self-attention and feed-forward sublayers, and how they interact to process information. Why needed: The study dissects blocks into these components to analyze their individual prunability. Quick check: Can you identify the self-attention and feed-forward components in a transformer block diagram?

**Shapley value computation**: The method for fairly distributing credit among components based on marginal contributions across all possible coalitions. Why needed: Used as an adaptive influence metric that considers task-specific performance impact. Quick check: Can you explain how Shapley value differs from simple averaging when ranking block importance?

**KV-cache optimization**: The mechanism by which removing self-attention layers reduces memory requirements during autoregressive generation. Why needed: Critical for understanding the practical benefits of layer-level pruning beyond just parameter count reduction. Quick check: Can you describe how KV-cache size relates to the number of self-attention layers in a model?

## Architecture Onboarding

**Component map**: Input sequences → Token embedding → Transformer blocks (self-attention + feed-forward layers) → Output projection → Task-specific evaluation (MMLU, GSM-8k, etc.)

**Critical path**: Token embedding → Transformer blocks → Output projection → Task evaluation. The study focuses on modifying the transformer block sequence through pruning.

**Design tradeoffs**: Block pruning vs. layer-level pruning (simplicity vs. fine-grained control), adaptive vs. static influence metrics (task-specific optimization vs. generalization), learned vs. statistical recovery techniques (flexibility vs. robustness to overfitting).

**Failure signatures**: Performance degradation on specific tasks despite minimal overall accuracy loss; adapter overfitting indicated by training loss continuing to decrease while validation performance plateaus; catastrophic forgetting when adapters shift too far from original weights.

**First experiments**: 1) Compute block influence using cosine distance on validation set; 2) Prune 10% of blocks and measure MMLU accuracy change; 3) Apply emulated update recovery and compare to baseline.

## Open Questions the Paper Calls Out

**Open Question 1**: How does task-specific Shapley-value block influence (computed on the test set) affect downstream performance on other held-out tasks compared to task-agnostic methods? The paper shows that computing loss-based Shapley-value directly on the MMLU test set improves MMLU performance compared to cosine baseline, but notes this comes at the expense of performance on other tasks not directly considered. Systematic experiments measuring performance on multiple held-out tasks when using task-specific Shapley-value metrics optimized for different target tasks would resolve this.

**Open Question 2**: What is the optimal combination strategy for pruning both complete blocks and individual self-attention/feed-forward layers to maximize efficiency while maintaining performance? The authors observe that self-attention layers are more amenable to pruning than feed-forward layers and that pruning self-attention provides KV-cache efficiency gains, but they don't explore hybrid pruning strategies that combine block-level and layer-level pruning. Empirical comparison of different hybrid pruning strategies that combine block-level and layer-level pruning decisions on the same model would resolve this.

**Open Question 3**: What causes the emulated update to outperform learned low-rank adapters, and can this insight be used to design better recovery techniques? The authors find that the simple emulated update (empirical mean of block update) is either competitive or superior to low-rank linear adapters trained with different strategies, potentially due to overfitting and catastrophic forgetting of the adapters. Detailed analysis comparing the distribution of updates across blocks, investigating adapter overfitting patterns, and testing hybrid approaches that combine statistical methods with limited learning would resolve this.

## Limitations
- Limited ablation studies on low-rank adapter hyperparameters (learning rate, training steps, rank) reduce confidence in recovery performance comparisons.
- Focus primarily on LLaMa-2 7b and Mistral 7b models limits generalizability to other model architectures.
- No clear decision framework for choosing influence metrics across different task types, despite demonstrating trade-offs in performance.

## Confidence
High: Comparative analysis of influence metrics and pruning strategies with well-specified experimental setup.
Medium: Recovery technique comparisons due to limited hyperparameter ablation studies.
Medium: Generalizability to other model architectures beyond LLaMa-2 and Mistral.

## Next Checks
1. Conduct ablation studies on low-rank adapter hyperparameters (learning rate, training steps, rank) to verify the robustness of recovery performance comparisons.
2. Test the proposed pruning strategies on additional model families (e.g., GPT-style, OPT) to assess architecture-specific effectiveness.
3. Implement and evaluate alternative influence metrics (e.g., gradient-based, attention-based) to establish whether Shapley value truly provides optimal pruning decisions across diverse task distributions.