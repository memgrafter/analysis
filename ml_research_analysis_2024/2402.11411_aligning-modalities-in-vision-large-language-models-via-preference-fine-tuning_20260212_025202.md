---
ver: rpa2
title: Aligning Modalities in Vision Large Language Models via Preference Fine-tuning
arxiv_id: '2402.11411'
source_url: https://arxiv.org/abs/2402.11411
tags:
- image
- arxiv
- preference
- povid
- vllms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes POVID, a preference fine-tuning framework for
  vision-language models (VLLMs) to address hallucination issues. POVID generates
  dispreferred responses using AI models without requiring human feedback.
---

# Aligning Modalities in Vision Large Language Models via Preference Fine-tuning

## Quick Facts
- arXiv ID: 2402.11411
- Source URL: https://arxiv.org/abs/2402.11411
- Authors: Yiyang Zhou; Chenhang Cui; Rafael Rafailov; Chelsea Finn; Huaxiu Yao
- Reference count: 16
- One-line primary result: POVID achieves 31.78% improvements on hallucination benchmarks through AI-generated dispreferred responses without requiring human feedback

## Executive Summary
POVID introduces a preference fine-tuning framework for Vision-Language Models (VLLMs) that addresses hallucination issues through AI-generated dispreferred responses. The approach uses ground-truth responses as preferred answers and generates dispreferred data through two strategies: GPT-4V injecting plausible hallucinations into correct answers, and diffusion noise triggering the VLLM's inherent hallucination patterns. These strategies are integrated into a Direct Preference Optimization (DPO) pipeline, eliminating the need for human feedback collection. Experiments across multiple benchmarks demonstrate significant reductions in hallucinations while improving overall VLLM performance.

## Method Summary
POVID fine-tunes LLaVA-1.5 using AI-generated dispreferred responses via a two-stage Direct Preference Optimization approach. First, GPT-4V creates plausible hallucinations by modifying ground-truth answers across object co-occurrence, logical relationships, and attribute changes. Second, diffusion noise is applied to images to trigger the VLLM's inherent hallucination patterns during comprehension. The framework combines these dispreferred responses with ground-truth preferred responses in a DPO loss function, achieving hallucination reduction without requiring human feedback data collection.

## Key Results
- 31.78% average improvement across hallucination benchmarks (CHAIRS, CHAIRi, POPE, MMHal)
- Outperforms other preference tuning methods while maintaining or improving performance on general VLLM benchmarks
- Achieves hallucination reduction without requiring human feedback collection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: POVID reduces hallucinations by contrasting hallucinatory responses with ground-truth correct responses, creating a clear alignment signal.
- Mechanism: The framework generates dispreferred responses via two strategies: GPT-4V-injected hallucinations and noise-triggered VLLM hallucinations, then optimizes via Direct Preference Optimization (DPO) to align language generation with image input.
- Core assumption: Ground-truth responses are always correct and serve as stable reference points for alignment.
- Evidence anchors:
  - [abstract] "We use ground-truth instructions as the preferred response and a two-stage approach to generate dispreferred data."
  - [section 3] "In POVID we employ a high-quality ground truth multi-modal instruction as the preferred answer..."
  - [corpus] Weak: No corpus evidence directly supports this alignment claim; relies on assumed correctness of ground truth.
- Break condition: If ground-truth responses contain errors or biases, the alignment signal may reinforce incorrect patterns.

### Mechanism 2
- Claim: Noise injection triggers VLLM's inherent hallucination patterns, making them visible and correctable during training.
- Mechanism: Diffusion noise disrupts image comprehension, causing VLLM to rely more on textual context or training data associations, exposing its hallucination tendencies in real-time.
- Core assumption: VLLMs have consistent hallucination patterns that can be triggered and corrected through controlled noise.
- Evidence anchors:
  - [section 3.2] "We introduce diffusion noise into the original image... The noise disrupts the VLLM's understanding of the image, leading it to produce uncertain responses..."
  - [figure 4] Illustrates how noise changes token predictions from "plate" to "fork" to "pixel" as noise increases.
  - [corpus] Weak: No corpus evidence directly confirms that noise-triggered hallucinations improve alignment; assumed from ablation results.
- Break condition: If noise levels are too high, the image becomes unrecognizable, breaking the alignment task entirely.

### Mechanism 3
- Claim: GPT-4V-generated hallucinations provide diverse, plausible dispreferred responses that expose specific hallucination types.
- Mechanism: GPT-4V modifies ground-truth answers to introduce object co-occurrence errors, logical relationship mistakes, and attribute changes, creating targeted dispreferred data.
- Core assumption: GPT-4V can reliably generate plausible hallucinations that expose VLLM weaknesses without introducing new errors.
- Evidence anchors:
  - [section 3.1] "We utilize GPT-4V to introduce plausible hallucinations into the answer... First, we hallucinate the image captioning tasks by considering three fundamental causes of hallucination..."
  - [figure 3] Shows examples of object co-occurrence, logical relationship, and attribute hallucinations.
  - [corpus] Weak: No corpus evidence validates GPT-4V's hallucination quality or its effectiveness in exposing VLLM issues.
- Break condition: If GPT-4V generates hallucinations that are too unrealistic or too similar to correct answers, the alignment signal becomes ineffective.

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO provides a lightweight alternative to RLHF that directly optimizes preference data without training a reward model.
  - Quick check question: What is the key difference between DPO and traditional RLHF in terms of computational complexity?

- Concept: Vision-Language Model Alignment
  - Why needed here: The core problem POVID addresses is misalignment between image and text modalities causing hallucinations.
  - Quick check question: Why might separately trained vision and language components fail to align properly without additional training?

- Concept: Diffusion-based Image Noise
  - Why needed here: Controlled noise injection is the mechanism for triggering VLLM's hallucination patterns during training.
  - Quick check question: How does diffusion noise differ from simple Gaussian noise in terms of image degradation patterns?

## Architecture Onboarding

- Component map:
  Input: Image + Text prompt -> Ground-truth data extraction -> GPT-4V hallucination injector + Noise-based VLLM hallucination trigger -> DPO optimizer -> Fine-tuned VLLM with reduced hallucinations

- Critical path:
  1. Load image and ground-truth caption/reasoning answer
  2. Generate GPT-4V hallucinated dispreferred response
  3. Apply diffusion noise to image and generate VLLM-triggered dispreferred response
  4. Combine preferred and dispreferred pairs
  5. Run DPO optimization step
  6. Repeat until convergence

- Design tradeoffs:
  - Ground-truth dependency vs. automation: Requires clean ground-truth data but avoids human feedback collection
  - Noise level selection: Too little noise won't trigger hallucinations; too much breaks image recognition
  - GPT-4V hallucination quality: Poor hallucinations provide weak training signals; overly complex ones may introduce new errors

- Failure signatures:
  - Model performance degrades on clean images after training with noisy images
  - Generated dispreferred responses are too similar to preferred responses, providing weak gradients
  - GPT-4V hallucinations introduce factual errors that reinforce incorrect patterns

- First 3 experiments:
  1. Ablation study: Compare performance with only GPT-4V hallucinations vs. only noise-triggered hallucinations vs. both
  2. Noise sensitivity analysis: Test different noise levels (Î¾ values) to find optimal hallucination triggering without breaking image recognition
  3. GPT-4V hallucination diversity: Analyze different hallucination types (object co-occurrence, logical errors, attribute changes) and their impact on alignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal noise step k in the image distortion strategy that balances triggering hallucination patterns without making the image unrecognizable?
- Basis in paper: [explicit] The paper discusses using diffusion noise to distort images and trigger inherent hallucination patterns in VLLMs, but notes that the noise step should remain within a reasonable range to ensure the image remains recognizable by humans.
- Why unresolved: The paper mentions that establishing an appropriate noise step is a reasonable approach but does not provide empirical evidence or specific guidelines for determining this optimal value.
- What evidence would resolve it: Systematic experiments varying the noise step k and measuring its impact on both the degree of hallucination and the model's performance on standard benchmarks would provide concrete guidelines for optimal noise step selection.

### Open Question 2
- Question: How does the POVID approach perform when applied to VLLMs with different vision encoders and language models, beyond the LLaVA-1.5 (7B) model used in the experiments?
- Basis in paper: [inferred] The paper demonstrates POVID's effectiveness on LLaVA-1.5 but does not explore its performance across different VLLM architectures, which could vary in their susceptibility to hallucinations and their response to alignment techniques.
- Why unresolved: The experiments are limited to a single VLLM architecture, leaving open questions about the generalizability and adaptability of POVID to other VLLM configurations.
- What evidence would resolve it: Applying POVID to a diverse set of VLLM architectures and comparing their performance improvements would indicate the approach's versatility and effectiveness across different model types.

### Open Question 3
- Question: Can the AI-generated dispreference data produced by POVID be further refined or augmented with human feedback to achieve even greater reductions in hallucination?
- Basis in paper: [explicit] The paper emphasizes the scalability and efficiency of using AI-generated dispreferences without human data generation, but does not explore the potential benefits of combining this approach with human feedback.
- Why unresolved: While the automated approach is a significant strength, the paper does not investigate whether human oversight could enhance the quality of dispreference data or further improve model alignment.
- What evidence would resolve it: Conducting experiments that compare the performance of POVID alone versus POVID augmented with human feedback would reveal the potential advantages of integrating human input into the automated process.

## Limitations

- Ground-truth dependency: The framework's effectiveness is fundamentally limited by the quality and availability of clean ground-truth data, as errors or biases in preferred responses may reinforce incorrect patterns.
- GPT-4V hallucination validation: The paper assumes GPT-4V can reliably generate plausible hallucinations but provides no empirical validation of hallucination quality or its effectiveness in exposing VLLM weaknesses.
- Limited cross-architecture testing: The approach is only tested on LLaVA-1.5, leaving open questions about generalizability to different VLLM architectures with varying hallucination patterns.

## Confidence

- High confidence: The experimental results showing 31.78% improvements on hallucination benchmarks are well-supported by quantitative data. The ablation studies comparing different hallucination generation strategies provide strong evidence for the effectiveness of the combined approach.
- Medium confidence: The mechanism claims about how GPT-4V hallucinations and noise-triggered responses improve alignment are theoretically sound but lack direct corpus evidence. The effectiveness relies on assumptions about hallucination patterns that are reasonable but not empirically validated.
- Low confidence: The paper provides limited analysis of failure modes and edge cases. The claims about robustness to different noise levels and hallucination types are not thoroughly tested across diverse scenarios.

## Next Checks

1. **Ground-truth error analysis**: Systematically evaluate the impact of ground-truth errors on POVID's alignment effectiveness by introducing controlled errors into the preferred responses and measuring performance degradation.

2. **Cross-VLLM generalization**: Test POVID on multiple VLLM architectures beyond LLaVA-1.5 to assess whether the hallucination correction mechanisms generalize across different model architectures and training approaches.

3. **Long-term stability assessment**: Evaluate whether the hallucination reduction achieved by POVID persists over extended use and whether the model develops new hallucination patterns over time that the current training approach cannot address.