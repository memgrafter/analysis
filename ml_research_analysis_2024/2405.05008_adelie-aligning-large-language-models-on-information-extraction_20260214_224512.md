---
ver: rpa2
title: 'ADELIE: Aligning Large Language Models on Information Extraction'
arxiv_id: '2405.05008'
source_url: https://arxiv.org/abs/2405.05008
tags:
- data
- tasks
- general
- training
- extraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ADELIE, a series of large language models
  (LLMs) specifically aligned for information extraction (IE) tasks. The authors address
  the problem of LLMs struggling with IE tasks due to a lack of alignment with human
  expectations, as mainstream alignment datasets typically do not include IE data.
---

# ADELIE: Aligning Large Language Models on Information Extraction

## Quick Facts
- arXiv ID: 2405.05008
- Source URL: https://arxiv.org/abs/2405.05008
- Authors: Yunjia Qi; Hao Peng; Xiaozhi Wang; Bin Xu; Lei Hou; Juanzi Li
- Reference count: 40
- One-line primary result: ADELIE achieves state-of-the-art performance among open-source models on various held-out IE datasets, particularly in few-shot evaluation scenarios, while maintaining general capabilities.

## Executive Summary
This paper introduces ADELIE, a series of large language models (LLMs) specifically aligned for information extraction (IE) tasks. The authors address the problem of LLMs struggling with IE tasks due to a lack of alignment with human expectations, as mainstream alignment datasets typically do not include IE data. To solve this, they construct a high-quality instruction tuning dataset for IE tasks, IEInstruct, which includes 83,585 instances covering various IE tasks such as named entity recognition, relation classification, and event extraction. They then train ADELIESFT using supervised fine-tuning on a mixture of IEInstruct and generic alignment data, and further train ADELIESFT with direct preference optimization (DPO) on a preference dataset, resulting in ADELIEDPO.

## Method Summary
The authors construct IEInstruct, a high-quality instruction tuning dataset for IE tasks containing 83,585 instances across various IE tasks. They train ADELIESFT using supervised fine-tuning on a mixture of IEInstruct and generic alignment data (20% IE, 80% general) for 6,306 gradient steps. They then construct IEFeedback, a preference dataset with 9,985 pairs, and train ADELIESFT with DPO for 937 additional gradient steps to obtain ADELIEDPO. The models are evaluated on held-out IE datasets across closed IE, open IE, and on-demand IE tasks in both zero-shot and few-shot settings.

## Key Results
- ADELIE achieves state-of-the-art performance among open-source models on various held-out IE datasets
- Significant improvements in few-shot evaluation scenarios with minimal performance degradation on general capabilities
- Mixing 20% IE data with 80% general data optimally balances task-specific performance with general capability preservation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: IE-specific alignment data addresses the "alignment gap" for information extraction tasks
- Mechanism: By constructing IEInstruct, the model receives instruction-tuning examples that include complex IE task specifications (schemas, output formats, demonstration examples) which are absent from mainstream alignment datasets
- Core assumption: The lack of IE-specific data in mainstream alignment corpora is a primary reason LLMs struggle with IE tasks
- Evidence anchors:
  - [abstract]: "This primarily arises from LLMs not being aligned with humans, as mainstream alignment datasets typically do not include IE data"
  - [section 1]: "LLMs usually struggle to understand and follow the complex instructions of IE tasks... which indicates existing LLMs are not aligned with human needs on IE tasks"

### Mechanism 2
- Claim: Mixing IE and general alignment data preserves general capabilities while improving IE performance
- Mechanism: The 20% IE / 80% general data mixture strategy balances task-specific alignment with maintaining broader language understanding abilities
- Core assumption: Models have sufficient capacity to learn both IE-specific and general capabilities without catastrophic forgetting
- Evidence anchors:
  - [section 6.1]: "We can observe that: (1) There is a substantial improvement in IE tasks, even with only 10% of the training data being IE data. (2) Adding IE data in training leads to a decrease in the model's general capabilities, but this decline is limited when the proportion is below 50%"

### Mechanism 3
- Claim: DPO with automatically constructed preference pairs enables self-improvement on extractive tasks
- Mechanism: Using BLEU score to automatically construct preference pairs from model outputs and ground truths creates a scalable training signal for DPO
- Core assumption: BLEU score differences reliably indicate quality differences between model outputs for extractive tasks with ground truth answers
- Evidence anchors:
  - [section 4]: "We employ the BLEU (Papineni et al., 2002) score as the metric to automatically construct preference pairs. We sample the output of ADELIESFT 5 times for an instance with the sampling temperature as 1.0. If the difference between the highest and lowest BLEU scores exceeds 10%, we treat the corresponding outputs as a preference pair"

## Foundational Learning

- Concept: Supervised Fine-Tuning (SFT)
  - Why needed here: SFT provides the foundational alignment of the model to IE tasks before applying preference optimization
  - Quick check question: What is the difference between SFT and DPO in terms of training objective and data requirements?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO refines the SFT model by optimizing for human preferences between different model outputs
  - Quick check question: How does DPO differ from traditional reinforcement learning from human feedback (RLHF)?

- Concept: In-context learning (ICL)
  - Why needed here: ICL capabilities are crucial for few-shot performance on IE tasks, and the alignment process specifically targets improving these capabilities
  - Quick check question: What is the relationship between in-context learning and the demonstration examples included in the training data?

## Architecture Onboarding

- Component map: LLAMA 2 7B -> IEInstruct construction -> SFT training (2 epochs) -> IEFeedback construction -> DPO training (3 epochs) -> Evaluation

- Critical path: IEInstruct construction → SFT training (2 epochs) → IEFeedback construction → DPO training (3 epochs) → Evaluation

- Design tradeoffs:
  - Data scale vs. quality: 83,585 instances in IEInstruct balances comprehensive coverage with high-quality annotations
  - General vs. task-specific capabilities: 20% IE / 80% general data mixture trades some IE performance for capability preservation
  - Automatic vs. human annotation: BLEU-based preference pair construction trades annotation quality for scalability

- Failure signatures:
  - Performance degradation on general tasks indicates "Alignment Tax" is too high
  - Inconsistent few-shot performance suggests in-context learning capabilities weren't properly aligned
  - Overfitting to specific output formats indicates insufficient data diversity

- First 3 experiments:
  1. Test zero-shot performance on a held-out IE dataset to establish baseline capability
  2. Evaluate few-shot performance with varying numbers of demonstrations to assess in-context learning
  3. Compare general task performance (MMLU, BBH) between base model, SFT-only, and full DPO model to measure capability preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would increasing the model size (e.g., using a 13B or 70B parameter model) affect the performance trade-off between IE tasks and general capabilities?
- Basis in paper: [inferred] The paper notes that using a 7B model may limit performance due to insufficient capacity, and suggests that training a larger model could yield better results.
- Why unresolved: The authors only tested a 7B parameter model due to computational constraints, leaving the impact of larger models unexplored.
- What evidence would resolve it: Training and evaluating ADELIE on larger models (e.g., 13B or 70B) while measuring both IE task performance and general capabilities (e.g., MMLU, BBH) would provide insights into the scaling effects.

### Open Question 2
- Question: How would incorporating human-annotated preference pairs in DPO training affect the performance gains compared to the automatically constructed pairs used in this study?
- Basis in paper: [explicit] The paper states that the preference pairs for DPO were automatically constructed without human annotation, and suggests that using human-annotated pairs could be a future direction.
- Why unresolved: The authors used a mix of online and offline data for DPO training, but did not include human-annotated preference pairs, which may limit the potential performance improvements.
- What evidence would resolve it: Conducting DPO training with human-annotated preference pairs and comparing the results to the current model's performance would clarify the impact of human annotation on alignment quality.

### Open Question 3
- Question: What is the impact of varying the proportion of IE data in the training mixture on the model's ability to generalize to unseen IE tasks or domains?
- Basis in paper: [explicit] The paper explores the effect of different proportions of IE data (from 0% to 100%) on model performance, finding that even 10% IE data significantly improves IE task performance but reduces general capabilities.
- Why unresolved: While the paper identifies an optimal proportion (20% IE data) for balancing IE and general capabilities, it does not explore how this affects generalization to new tasks or domains.
- What evidence would resolve it: Evaluating models trained with different IE data proportions on a diverse set of unseen IE tasks or domains would reveal how the mixture strategy influences cross-task and cross-domain generalization.

## Limitations

- The evaluation corpus is limited to specific benchmark sets with only 4 few-shot samples per task in the ONION dataset
- General capability preservation claims rely on narrow comparisons with TULU 2 and minimal baseline diversity
- The automatic preference pair construction using BLEU scores lacks validation that these pairs meaningfully capture human preferences for IE outputs

## Confidence

- **High Confidence**: The fundamental premise that IE tasks require specialized alignment data is well-supported by the performance gap between baseline models and ADELIE across multiple IE benchmarks
- **Medium Confidence**: The claim that mixing 20% IE data with 80% general data optimally balances task-specific performance with general capability preservation is supported by ablation studies, but the analysis is limited to a narrow set of general benchmarks
- **Low Confidence**: The assertion that DPO with automatically constructed preference pairs significantly enhances IE performance lacks strong empirical support and comparison to human-annotated preferences

## Next Checks

1. **Cross-Dataset Generalization**: Evaluate ADELIE on additional IE benchmarks not included in the training corpora (e.g., ERE, ACE 2005) to verify that performance gains generalize beyond the specific datasets used in the paper.

2. **Human Preference Validation**: Conduct a human evaluation study comparing model outputs from ADELIESFT and ADELIEDPO across representative IE tasks to validate whether the BLEU-based automatic preference pairs used in DPO training actually align with human judgments of output quality.

3. **Long-Form Capability Analysis**: Test ADELIE's performance on longer, more complex IE scenarios that require multi-document processing or extended reasoning chains to assess whether the alignment process has preserved or enhanced the model's ability to handle realistic, complex IE tasks.