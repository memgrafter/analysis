---
ver: rpa2
title: 'The Iterative Optimal Brain Surgeon: Faster Sparse Recovery by Leveraging
  Second-Order Information'
arxiv_id: '2408.17163'
source_url: https://arxiv.org/abs/2408.17163
tags:
- i-obs
- pruning
- sparse
- sparsity
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Iterative Optimal Brain Surgeon (I-OBS),
  a family of sparse recovery algorithms that leverage second-order information to
  improve upon classic iterative hard thresholding methods. The key idea is to use
  curvature information from the Hessian matrix during the projection step of sparse
  recovery, leading to faster convergence rates under standard smoothness and strong
  convexity assumptions.
---

# The Iterative Optimal Brain Surgeon: Faster Sparse Recovery by Leveraging Second-Order Information

## Quick Facts
- arXiv ID: 2408.17163
- Source URL: https://arxiv.org/abs/2408.17163
- Reference count: 40
- This paper introduces I-OBS, a family of sparse recovery algorithms leveraging second-order information to improve upon iterative hard thresholding methods.

## Executive Summary
This paper introduces the Iterative Optimal Brain Surgeon (I-OBS), a family of sparse recovery algorithms that leverage second-order information to improve upon classic iterative hard thresholding methods. The key idea is to use curvature information from the Hessian matrix during the projection step of sparse recovery, leading to faster convergence rates under standard smoothness and strong convexity assumptions. The authors prove that I-OBS achieves local quadratic convergence, matching the rate of Newton's method up to constants. They also show that popular neural network pruning algorithms like WoodFisher and OBC are special cases of their framework.

## Method Summary
The Iterative Optimal Brain Surgeon (I-OBS) framework improves sparse recovery by incorporating Hessian-preconditioned projections during the iterative optimization process. At each iteration, the algorithm computes a Newton step using the current Hessian and gradient, then selects a k-sparse support either through optimal (NP-hard) selection or a top-k heuristic. The update is projected onto this support, with an optional re-optimization of remaining parameters. The method unifies and generalizes existing pruning algorithms like WoodFisher and OBC, while providing theoretical guarantees of local quadratic convergence under standard smoothness and strong convexity assumptions.

## Key Results
- I-OBS achieves local quadratic convergence, matching Newton's method rate up to constants
- On synthetic sparse linear regression tasks, I-OBS demonstrates faster convergence than IHT
- When applied to pruning vision transformers and large language models, I-OBS provides improved accuracy, achieving up to 1 perplexity point improvement on C4 dataset for Phi-1.5B model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using second-order curvature information from the Hessian matrix in the projection step accelerates convergence by adapting step sizes per coordinate and capturing parameter correlations.
- Mechanism: The algorithm replaces the Euclidean norm in standard IHT with a Hessian-preconditioned norm, so the projection respects the local loss geometry. This reduces the number of iterations needed to reach a sparse solution because the update direction aligns better with the true descent direction.
- Core assumption: The loss function is strongly convex and (k, d-k, L)-restricted first-order smooth, so the Hessian provides meaningful curvature information without being too ill-conditioned.
- Evidence anchors:
  - [abstract] "leverage curvature information from the Hessian matrix during the projection step... leading to faster convergence rates"
  - [section 3.4] "we can go beyond the standard Euclidean norm... by incorporating local curvature information via the Hessian matrix"
- Break condition: If the Hessian is not positive definite or the loss violates strong convexity, the preconditioning can mislead the search and slow convergence.

### Mechanism 2
- Claim: By selecting the optimal support set using the Hessian-projected residual, the algorithm ensures that pruned parameters have minimal impact on the loss.
- Mechanism: After computing a dense Newton step, the method finds the set of indices to keep by solving a combinatorial subproblem that minimizes the quadratic form of the Hessian-projected update over all possible supports. This is equivalent to choosing parameters that are least harmful to remove.
- Core assumption: The restricted second-order smoothness assumption holds, allowing the Hessian to be a reliable local model of the loss.
- Evidence anchors:
  - [section 3.4] "we optimize the remaining parameters to minimize loss... the theoretical Option 2 adjusts the unpruned part of θ⁺ to precisely solve the sub-problem"
  - [section 3.5] Proof sketch references the structure of HS_t to bound the error introduced by sparsity.
- Break condition: When the sparsity constraint is very tight (k << k*), the combinatorial selection becomes infeasible and the greedy heuristic may select suboptimal supports.

### Mechanism 3
- Claim: The Topk-I-OBS variant achieves local quadratic convergence by approximating the optimal support selection with a simple top-k magnitude selection on the Newton step.
- Mechanism: Instead of solving the NP-hard support selection, the algorithm keeps the k largest-magnitude entries of the Newton update. This provides a practical trade-off between computational cost and convergence speed, still leveraging curvature information.
- Core assumption: The error from approximating the optimal support with the top-k is bounded by the ratio of k* to k, as shown in Lemma 8.
- Evidence anchors:
  - [section 3.6] "one natural implementation of the I-OBS method in practice is to replace the multiplication by the matrix I − H_t^{-1} H_{S_t} with directly applying the Top-k operator"
  - [section 3.5] "The matrix H^{-1}_t H_{S_t} is a projection matrix... allowing bounding the error by ||θ_t − H_t^{-1}∇f(θ_t)||"
- Break condition: If the Newton step has many small-magnitude but important parameters, the top-k heuristic may discard them, leading to slower convergence or poorer solutions.

## Foundational Learning

- Concept: Strong convexity and smoothness of the loss function
  - Why needed here: These properties ensure that the Hessian is positive definite and provides a good local quadratic approximation, which is critical for the second-order preconditioning to work.
  - Quick check question: If a function is µ-strongly convex, what is the minimum eigenvalue of its Hessian at any point?
    - Answer: µ

- Concept: Restricted smoothness and sparsity constraints
  - Why needed here: The restricted smoothness assumption bounds the Hessian's effect on sparse vectors, enabling analysis of convergence under the k-sparsity constraint.
  - Quick check question: What does (k, d-k, L)-restricted L-smoothness mean in terms of Hessian behavior?
    - Answer: For any vector v with sparsity ≤ d-k, v^T ∇²f(θ) v ≤ L ||v||² for all θ with sparsity ≤ k.

- Concept: Quadratic convergence of Newton's method
  - Why needed here: The theoretical result shows that I-OBS inherits Newton's quadratic convergence rate up to constants, justifying the use of curvature information.
  - Quick check question: What is the typical iteration complexity for achieving ε-error with quadratic convergence?
    - Answer: O(log log(1/ε))

## Architecture Onboarding

- Component map: Gradient computation -> Hessian computation -> Dense Newton step -> Support selection -> Projection -> Update
- Critical path:
  1. Compute gradient and Hessian at current iterate
  2. Apply Newton step to get dense update
  3. Select support (top-k or solve subproblem)
  4. Project update to k-sparse vector
  5. Optionally re-optimize remaining coordinates

- Design tradeoffs:
  - Exact vs approximate Hessian: exact gives better convergence but is expensive; Fisher/approximation scales better.
  - Optimal vs top-k support: optimal is intractable for large d; top-k is fast but may lose accuracy.
  - One-shot vs iterative pruning: iterative can refine sparsity mask but costs more passes.

- Failure signatures:
  - Hessian not positive definite → algorithm may diverge or fail to improve
  - Very small k relative to k* → poor solutions due to aggressive pruning
  - Ill-conditioned Hessian → numerical instability in inversion
  - Stochastic gradients too noisy → convergence slows or oscillates

- First 3 experiments:
  1. Sparse linear regression with Gaussian prior: compare I-OBS vs IHT on recovery error vs iterations.
  2. Vision transformer pruning: apply I-OBS to DeiT-Tiny with 50% sparsity, measure accuracy vs iterations.
  3. Large language model pruning: use OPT-125M, compare I-OBS vs SparseGPT perplexity after 3 iterations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions on the loss function f and its smoothness parameters can the theoretical assumptions (µ-strong convexity, restricted smoothness) be relaxed while still guaranteeing local quadratic convergence of I-OBS?
- Basis in paper: [inferred] The paper explicitly states in Section 5 that their theoretical assumptions are mild and do not hold in general for practical problems like model pruning, suggesting this as an open direction for future work.
- Why unresolved: The current convergence guarantees rely on standard assumptions (µ-strong convexity, restricted smoothness) which are restrictive and not always satisfied in practical pruning scenarios. Proving convergence under weaker conditions would significantly broaden the applicability of I-OBS.
- What evidence would resolve it: A rigorous proof showing local (or even global) quadratic convergence of I-OBS under relaxed assumptions, such as only requiring convexity and first-order smoothness, or under assumptions commonly satisfied in deep learning (e.g., Polyak-Łojasiewicz inequality).

### Open Question 2
- Question: Can the Topk-I-OBS variant be modified to achieve global convergence guarantees, similar to how cubic regularization or adaptive quadratic regularization are used for standard Newton's method?
- Basis in paper: [inferred] The paper mentions in Section 5 that while local convergence is typical for Newton-type methods, modifications like cubic or adaptive quadratic regularization can provide global convergence rates, but incorporating sparsity constraints into such modifications is an open challenge.
- Why unresolved: The current Topk-I-OBS only has local convergence guarantees. Extending it to global convergence would make it more robust and reliable in practice, especially when the initial iterate is far from the optimum.
- What evidence would resolve it: A theoretical analysis and proof of global convergence for a modified version of Topk-I-OBS that incorporates a suitable regularization term (e.g., cubic or adaptive quadratic) while maintaining the sparsity constraint during each iteration.

### Open Question 3
- Question: How does the performance of I-OBS scale to extremely large language models (e.g., >10B parameters) and what are the practical computational bottlenecks?
- Basis in paper: [explicit] The paper states in Section 5 that applying I-OBS to larger models is an interesting direction for future work, and their current experiments are limited to models up to 1B parameters.
- Why unresolved: While the paper demonstrates I-OBS on models up to 1B parameters, the computational complexity of estimating and inverting the Hessian matrix scales poorly with model size, making it unclear if I-OBS can be effectively applied to truly massive models.
- What evidence would resolve it: Empirical results showing the performance and scalability of I-OBS on language models with 10B+ parameters, along with an analysis of the computational bottlenecks (e.g., Hessian estimation, inversion) and potential solutions (e.g., low-rank approximations, stochastic approximations).

## Limitations
- The theoretical analysis assumes exact second-order information and strong convexity, which may not hold for all neural network pruning tasks
- The computational cost of Hessian estimation and inversion could limit scalability to very large models
- The convergence guarantees are local and may not extend to the full optimization landscape

## Confidence

| Claim | Level |
|-------|-------|
| Local quadratic convergence rate under standard assumptions | High |
| Empirical improvements on vision transformers and small language models | Medium |
| Applicability of theoretical framework to general neural network architectures | Low |

## Next Checks

1. Test I-OBS convergence on a wider range of sparsity levels (5%-95%) for different model families to assess robustness
2. Compare I-OBS against other second-order methods like K-FAC and AdaHessian in terms of wall-clock time to solution
3. Evaluate the effect of Hessian approximation quality (rank, frequency of update) on final solution accuracy