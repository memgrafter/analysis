---
ver: rpa2
title: Preserving logical and functional dependencies in synthetic tabular data
arxiv_id: '2409.17684'
source_url: https://arxiv.org/abs/2409.17684
tags:
- data
- dependencies
- synthetic
- logical
- functional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates whether synthetic tabular data generation models
  preserve logical and functional dependencies among attributes. The authors introduce
  a Q-function to quantify logical dependencies and compare seven state-of-the-art
  generative models (CTGAN, CTABGAN, CTABGAN Plus, TVAE, NextConvGeN, TabDDPM, TabuLa)
  across five datasets.
---

# Preserving logical and functional dependencies in synthetic tabular data

## Quick Facts
- arXiv ID: 2409.17684
- Source URL: https://arxiv.org/abs/2409.17684
- Reference count: 40
- Primary result: Transformer-based models preserve logical dependencies up to 100%, while GAN/VAE models fail on both logical and functional dependencies

## Executive Summary
This paper evaluates how well synthetic tabular data generation models preserve logical and functional dependencies among attributes. The authors introduce a Q-function to quantify logical dependencies and compare seven state-of-the-art generative models across five datasets. Results show that convex-space, diffusion-based, and transformer-based models (NextConvGeN, TabDDPM, TabuLa) preserve logical dependencies effectively, while GAN and VAE models show poor performance. None of the models adequately preserve functional dependencies. TabuLa, leveraging transformer self-attention, demonstrates superior performance in maintaining logical dependencies across all datasets.

## Method Summary
The study evaluates seven generative models (CTGAN, CTABGAN, CTABGAN Plus, TVAE, NextConvGeN, TabDDPM, TabuLa) on five datasets with complete data. The Q-function measures logical dependency strength between attribute pairs (0 = functional dependency, 1 = no dependency). Functional dependencies are extracted using FDTool for categorical features. Models are trained on complete datasets using default parameters, then generate synthetic data of matching size. Dependencies are extracted from both real and synthetic data and compared using Venn diagrams and bar plots to assess preservation rates.

## Key Results
- Convex-space, diffusion-based, and transformer-based models preserve logical dependencies up to 100% for some datasets
- GAN and VAE models show poor logical dependency preservation, with CTGAN/CTABGAN/CTABGAN Plus clustering at Q=1
- None of the models adequately preserve functional dependencies across all datasets
- TabuLa demonstrates superior performance in maintaining logical dependencies across all datasets

## Why This Works (Mechanism)
Assumption: The transformer architecture in TabuLa leverages self-attention mechanisms to capture long-range dependencies and contextual relationships between attributes more effectively than GAN/VAE models, which rely on adversarial training or variational inference that may not preserve complex dependency structures as faithfully.

## Foundational Learning
Assumption: The superior performance of transformer-based models suggests that self-attention mechanisms can better capture the multi-dimensional relationships inherent in tabular data dependencies compared to convolutional or adversarial approaches used in other generative models.

## Architecture Onboarding
Assumption: Understanding the transformer architecture's attention mechanism is crucial for interpreting TabuLa's performance, as it allows the model to weigh the importance of different attribute relationships when generating synthetic data, potentially explaining its superior dependency preservation.

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly call out specific open questions, but the inability of any model to preserve functional dependencies adequately suggests an open research direction for improving dependency preservation in synthetic data generation.

## Limitations
- Limited generalizability due to use of only five datasets with complete data and no missing values
- Reliance on default parameters may not reflect optimal performance for each algorithm
- Inability of any model to adequately preserve functional dependencies represents a significant limitation
- The Q-function, while useful for quantifying logical dependencies, may not capture all nuances of attribute relationships

## Confidence
- High confidence: Transformer-based models preserve logical dependencies more effectively than GAN/VAE approaches
- Medium confidence: None of the models adequately preserve functional dependencies due to sensitivity to synthetic data perturbations
- Medium confidence: TabuLa's superior performance is attributable to transformer self-attention mechanisms, though alternative explanations (e.g., training stability) cannot be ruled out

## Next Checks
1. **Sensitivity analysis of Q-function**: Test whether small perturbations in synthetic data generation significantly affect logical dependency preservation scores
2. **Missing data handling evaluation**: Re-run experiments with artificially introduced missing values to assess robustness
3. **Cross-dataset dependency transfer**: Generate synthetic data from one dataset and evaluate dependency transfer to structurally similar datasets
4. **Functional dependency preservation enhancement**: Investigate hybrid approaches combining transformer architectures with explicit functional dependency constraints