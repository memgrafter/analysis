---
ver: rpa2
title: Faster Vision Mamba is Rebuilt in Minutes via Merged Token Re-training
arxiv_id: '2412.12496'
source_url: https://arxiv.org/abs/2412.12496
tags:
- token
- reduction
- tokens
- mamba
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accelerating Vision Mamba
  models through token reduction. The core method, R-MeeTo, combines token merging
  with re-training to preserve key knowledge lost during reduction.
---

# Faster Vision Mamba is Rebuilt in Minutes via Merged Token Re-training

## Quick Facts
- **arXiv ID**: 2412.12496
- **Source URL**: https://arxiv.org/abs/2412.12496
- **Reference count**: 40
- **Primary result**: Vim-S models can be retrained within minutes with only 1.3% accuracy drop while achieving 1.2x speedup

## Executive Summary
This paper addresses the challenge of accelerating Vision Mamba models through token reduction. The core method, R-MeeTo, combines token merging with re-training to preserve key knowledge lost during reduction. Empirical results show that pruned Vims drop up to 0.9% accuracy on ImageNet-1K, which is recovered by R-MeeTo. Notably, a 35.9% accuracy spike is achieved over 3 epochs of training on Vim-Ti, with re-training completed in minutes. Vim-Ti/S/B models are retrained within 5/7/17 minutes, with Vim-S dropping only 1.3% accuracy and achieving 1.2x (up to 1.5x) speedup in inference.

## Method Summary
R-MeeTo is a framework that accelerates Vision Mamba models by combining token merging with re-training. The method first reduces token count through merging similar tokens based on cosine similarity, then recovers lost performance through fine-tuning. Unlike pruning which directly deletes tokens, merging preserves more information by fusing similar tokens. The approach is specifically designed for Mamba's sequential state space model, which is more sensitive to token reduction than Transformers due to information enrichment effects in the hidden state.

## Key Results
- Vim-Ti models achieve 35.9% accuracy improvement over 3 epochs of re-training after token reduction
- Vim-S models drop only 1.3% accuracy while achieving 1.2x speedup with 31% token reduction
- Full re-training completed within minutes: Vim-Ti/S/B models in 5/7/17 minutes respectively
- Performance recovery validated across multiple model scales and video Mamba models on Kinetics-400

## Why This Works (Mechanism)

### Mechanism 1
Mamba models are more sensitive to token pruning than Transformers because pruning removes tokens containing enriched general knowledge. In Mamba's selective state space model, the hidden state accumulates information over time through the SSM operation, creating an enrichment effect where tokens later in the sequence contain more general knowledge about all previous inputs. When pruning occurs, tokens with higher general knowledge are more likely to be removed, causing larger performance drops.

### Mechanism 2
Token merging preserves more key knowledge than pruning by fusing similar tokens rather than deleting them. Merging operates by identifying similar tokens and combining them, which retains the general knowledge contained in those tokens. Pruning directly removes tokens, causing irreversible loss of both specific and general knowledge. Merging's similarity-based approach prevents immediate loss of all key knowledge in reduced tokens.

### Mechanism 3
Re-training after token merging effectively rebuilds lost specific knowledge and restores performance. After merging reduces token count but preserves general knowledge, re-training can recover the specific knowledge that was lost during reduction. The framework retains most key knowledge through merging, making performance recovery achievable through limited re-training epochs.

## Foundational Learning

- **State Space Models (SSM) and sequential processing**: Understanding Mamba's fundamental difference from Transformers is crucial for grasping why token reduction affects it differently. *Quick check*: How does the SSM's hidden state update equation (h_t = A h_{t-1} + B x_t) create sequential dependencies that don't exist in Transformers?

- **Information Bottleneck Theory and mutual information**: The paper uses information theory to analyze knowledge distribution and loss during token reduction. *Quick check*: In the Markov chain Y → X → Y formulation, what does minimizing I(X;Y) - βI(Ŷ;Y) accomplish for model training?

- **Token reduction strategies (pruning vs merging)**: Understanding the fundamental differences between these approaches explains why merging + re-training works better than pruning alone. *Quick check*: What is the key difference between how pruning and merging handle token information during reduction?

## Architecture Onboarding

- **Component map**: Input image → tokenization → Forward pass through Mamba layers → Token reduction operation → Reordering to preserve original sequence → Re-training for 1-5 epochs on full ImageNet-1K → Inference with reduced token count

- **Critical path**: 1. Input image → tokenization, 2. Forward pass through Mamba layers, 3. Token reduction operation (every 2 blocks by default), 4. Reordering to preserve original sequence, 5. Re-training for 1-5 epochs on full ImageNet-1K, 6. Inference with reduced token count

- **Design tradeoffs**: Token reduction ratio (higher ratios give more speedup but risk more performance loss), Merge frequency (more frequent merging reduces tokens faster but may lose more information), Re-training duration (more epochs recover more performance but increase computational cost), Distance metric (different metrics may affect merging quality but show minimal impact)

- **Failure signatures**: Performance drop when reduction ratio exceeds 0.14 for Vim-Ti or 0.31 for larger models, I/O and computational overhead dominating benefits at very low reduction ratios, Overfitting on small subsets during re-training (1% data subset), Shuffling tokens causing severe performance degradation in Mamba vs minimal impact in Transformers

- **First 3 experiments**: 1. Baseline comparison: Measure Vim-S performance with and without token reduction (pruning vs merging) to establish sensitivity, 2. Re-training validation: Apply R-MeeTo to a reduced Vim-Ti model and measure performance recovery over 3 epochs, 3. Shuffle analysis: Apply odd-even shuffle to Vim-Ti tokens before re-training and measure performance impact vs keeping original order

## Open Questions the Paper Calls Out

### Open Question 1
How does the token order preservation mechanism specifically interact with the sequential dependency structure of Mamba's state space model, and could alternative ordering strategies yield better performance? The paper demonstrates that maintaining token order is critical for Mamba performance, showing that shuffling tokens causes significant accuracy drops, while DeiT models are unaffected by reordering.

### Open Question 2
What is the optimal balance between token reduction ratio and re-training duration for different Mamba model scales, and how does this relationship change with dataset size? While the paper demonstrates effectiveness across different scales and provides some ablation studies on re-training duration, it doesn't establish a clear relationship between reduction ratio, model scale, dataset size, and optimal re-training duration.

### Open Question 3
How does the cosine similarity-based token merging in R-MeeTo compare to other distance metrics in terms of preserving task-relevant information versus general information, and what theoretical justification exists for using cosine similarity specifically? The paper includes an ablation study showing that different distance functions (cosine, ℓ1, ℓ2) yield comparable results, suggesting robustness to the choice of distance function.

## Limitations
- Theoretical analysis of Mamba's enrichment effect lacks rigorous proof that it's stronger than attention mechanisms
- Token merging algorithm details are incomplete (Algorithm 4 not fully specified)
- Re-training efficacy claims may not generalize beyond ImageNet-1K and specific Vim models
- Speedup measurements are based on FLOPs reduction rather than actual wall-clock measurements

## Confidence

**High Confidence**: The empirical results showing that R-MeeTo achieves better accuracy retention than pruning alone are well-supported by the experimental data. The observation that Mamba models show greater performance degradation from token reduction than Transformers is clearly demonstrated through controlled experiments.

**Medium Confidence**: The theoretical framework explaining why Mamba is more sensitive to pruning has reasonable grounding in information theory, but the specific claims about enrichment effects require more rigorous proof. The mechanism connecting SSM sequential processing to knowledge concentration is plausible but not definitively established.

**Low Confidence**: Claims about the generalizability of the method to other Mamba variants, video models, or different vision tasks are based on limited experiments. The assertion that merging preserves "general knowledge" more effectively than pruning is conceptually sound but lacks quantitative validation of what constitutes general vs. specific knowledge.

## Next Checks

1. **Enrichment Effect Isolation**: Design an experiment that directly measures information concentration in Mamba vs Transformer tokens through information-theoretic analysis. Track how mutual information between tokens and labels changes across the sequence in both architectures to validate whether Mamba shows stronger enrichment effects.

2. **Merge Algorithm Verification**: Implement the complete token merging algorithm with all details specified (Algorithm 4). Test multiple token combination strategies (averaging, weighted averaging based on similarity, learned merging) to determine which approach best preserves performance and whether the choice significantly impacts results.

3. **Generalization Benchmark**: Apply R-MeeTo to a different vision task (e.g., object detection on COCO or semantic segmentation on ADE20K) and measure whether the 3-epoch re-training recovery pattern holds. Compare performance and training dynamics to the ImageNet-1K results to assess method robustness across tasks.