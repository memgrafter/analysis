---
ver: rpa2
title: 'Meerkat: Audio-Visual Large Language Model for Grounding in Space and Time'
arxiv_id: '2407.01851'
source_url: https://arxiv.org/abs/2407.01851
tags:
- audio
- image
- audio-visual
- arxiv
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Meerkat is an audio-visual large language model that unifies five\
  \ fine-grained grounding tasks\u2014audio referred image grounding, image guided\
  \ audio temporal localization, and audio-visual fact-checking\u2014along with two\
  \ coarse-grained tasks, audio-visual question answering and audio-visual captioning.\
  \ It introduces a novel modality alignment module based on optimal transport and\
  \ a cross-attention module that enforces audio-visual consistency."
---

# Meerkat: Audio-Visual Large Language Model for Grounding in Space and Time

## Quick Facts
- arXiv ID: 2407.01851
- Source URL: https://arxiv.org/abs/2407.01851
- Reference count: 40
- Primary result: State-of-the-art performance across 5 fine-grained grounding tasks with up to 37.12% relative improvement

## Executive Summary
Meerkat is an audio-visual large language model that unifies five fine-grained grounding tasks (audio referred image grounding, image guided audio temporal localization, audio-visual fact-checking) with two coarse-grained tasks (audio-visual question answering, audio-visual captioning). The model introduces a novel optimal transport-based modality alignment module and a cross-attention consistency module, trained on a curated 3M-sample dataset. It achieves state-of-the-art performance across all tasks, demonstrating the effectiveness of combining fine-grained and coarse-grained supervision in a unified framework.

## Method Summary
Meerkat extends Llama 2-Chat (7B) with LoRA fine-tuning and integrates two key modules: AVOpT for patch-level optimal transport alignment between image and audio features, and AVACE for cross-attention consistency enforcement using ground-truth bounding boxes. The model is trained on AVFIT, a unified dataset combining five fine-grained grounding tasks with two coarse-grained tasks. The architecture uses CLIP-ViT-B/16 for image encoding, CLAP for audio encoding, and employs a multi-task training approach with combined losses including cross-entropy, optimal transport distance, and attention consistency losses.

## Key Results
- Achieves state-of-the-art performance on all five fine-grained grounding tasks
- Demonstrates 37.12% relative improvement over existing methods
- Shows multi-task training benefits both fine-grained and coarse-grained tasks
- Validates effectiveness of optimal transport alignment and cross-attention consistency modules

## Why This Works (Mechanism)

### Mechanism 1
Optimal Transport (OT) patch-level alignment in AVOpT module provides weak supervision that improves cross-modal semantic consistency. AVOpT uses Earth Mover's Distance to compute Wasserstein distance between image and audio patch embeddings, minimizing patch-level cross-modal cosine distances. This weakly aligns representations before strong supervision is applied. Core assumption: Patch-level alignment captures richer semantic relationships than global contrastive loss, enabling better fine-grained localization downstream.

### Mechanism 2
AVACE cross-attention module enforces region-level consistency by confining attention maps to object boundaries. AVACE uses a mask derived from ground-truth bounding boxes to maximize attention within object regions and minimize attention on background during cross-modal attention computation. Loss LAC is minimized to enforce this constraint. Core assumption: Cross-modal attention maps should focus on the same spatial regions as the ground-truth object to achieve precise grounding.

### Mechanism 3
Multi-task training with both fine-grained and coarse-grained tasks improves generalization across all tasks. Meerkat is trained on a unified dataset (AVFIT) containing diverse tasks with varying granularity. Fine-grained tasks provide detailed grounding supervision while coarse-grained tasks add semantic richness. Core assumption: Learning fine-grained localization helps the model extract richer visual and audio features that benefit even coarse-grained tasks like captioning and QA.

## Foundational Learning

- Concept: Optimal Transport (Wasserstein distance) for cross-modal alignment
  - Why needed here: Provides weak supervision by aligning image and audio patches without requiring explicit correspondence labels, which are expensive to obtain
  - Quick check question: How does the transport plan Ω represent the alignment between image and audio patches?

- Concept: Cross-modal attention with spatial masking
  - Why needed here: Ensures that audio and visual features attend to the same spatial regions, enforcing consistency between modalities at the region level
  - Quick check question: What happens to the attention map outside the ground-truth bounding box during training?

- Concept: Instruction tuning with multi-modal prompts
  - Why needed here: Allows Meerkat to understand and follow complex natural language instructions involving both audio and visual content
  - Quick check question: How are bounding box coordinates represented in the instruction text for grounding tasks?

## Architecture Onboarding

- Component map: Image/Audio → Encoders → AVOpT (weak alignment) → AVACE (strong alignment) → LLM → Output
- Critical path: Image/Audio → Encoders → AVOpT (weak alignment) → AVACE (strong alignment) → LLM → Output
- Design tradeoffs:
  - Single-stage vs two-stage training: Single-stage is more efficient but may require careful balancing of loss terms
  - LoRA vs full fine-tuning: LoRA is parameter-efficient but may limit adaptation capacity
  - Patch-level vs global alignment: Patch-level provides finer supervision but increases computational cost
- Failure signatures:
  - Poor grounding accuracy: Check AVOpT alignment quality and AVACE mask accuracy
  - Inconsistent attention maps: Verify mask computation and loss weighting
  - Degraded language understanding: Examine LLM tokenization and instruction template quality
- First 3 experiments:
  1. Verify AVOpT computes reasonable Wasserstein distances between image and audio patches
  2. Test AVACE attention confinement with synthetic masks and ground-truth boxes
  3. Validate instruction templates produce expected LLM outputs for simple grounding tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal transport (OT) alignment module (AVOpT) compare to other alignment methods like contrastive learning in terms of robustness to noisy or ambiguous audio-visual pairs?
- Basis in paper: [explicit] The authors state that OT-based alignment operates on the level of patches in a weakly-supervised manner and is more suitable for fine-grained downstream tasks compared to contrastive loss-based global supervision.
- Why unresolved: The paper only compares OT to contrastive loss, but doesn't explore other alignment methods or test robustness to noisy data.
- What evidence would resolve it: Experiments comparing AVOpT to other alignment methods (e.g., contrastive learning with different temperature scaling, attention-based alignment) on datasets with varying levels of noise and ambiguity.

### Open Question 2
- Question: What is the impact of the cross-attention consistency module (AVACE) on the model's ability to handle complex audio-visual scenes with multiple overlapping sound sources?
- Basis in paper: [explicit] The authors mention that AVACE confines the cross-modality attention map within the boundaries of the object of interest, but don't provide specific results on handling overlapping sounds.
- Why unresolved: The paper focuses on single sound source localization and doesn't explore scenarios with multiple overlapping sounds.
- What evidence would resolve it: Experiments evaluating AVACE on datasets with multiple overlapping sound sources, such as MUSIC-AVQA or PanoAVQA, and comparing performance to models without AVACE.

### Open Question 3
- Question: How does the model's performance scale with the size and diversity of the training dataset (AVFIT)?
- Basis in paper: [inferred] The authors mention that AVFIT contains 3M instruction tuning samples, but don't explore the impact of dataset size on model performance.
- Why unresolved: The paper doesn't provide experiments with varying dataset sizes or analyze the impact of dataset diversity on model generalization.
- What evidence would resolve it: Experiments training Meerkat on subsets of AVFIT with different sizes and analyzing the relationship between dataset size/diversity and model performance on downstream tasks.

## Limitations
- Model generalization across unseen audio-visual scenarios remains uncertain
- Computational overhead of patch-level optimal transport alignment
- Dataset representation may introduce task-specific bias

## Confidence
- High Confidence: AVOpT and AVACE architectural design is well-specified with clear mathematical formulations
- Medium Confidence: Multi-task training benefits demonstrated through ablation studies, but causal relationships remain partially speculative
- Low Confidence: "Up to 37.12% relative improvement" claim lacks context about baseline selection and evaluation conditions

## Next Checks
1. Ablation on Transport Plan Sparsity: Systematically vary the patch-level granularity in AVOpT and measure the impact on grounding accuracy
2. Cross-Attention Robustness Test: Evaluate AVACE performance with progressively degraded bounding box masks to determine sensitivity to annotation quality
3. Task Distribution Sensitivity: Train Meerkat on systematically varied ratios of fine-grained to coarse-grained samples to quantify exact contribution of each task type