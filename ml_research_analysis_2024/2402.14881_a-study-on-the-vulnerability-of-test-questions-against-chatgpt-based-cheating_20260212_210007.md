---
ver: rpa2
title: A Study on the Vulnerability of Test Questions against ChatGPT-based Cheating
arxiv_id: '2402.14881'
source_url: https://arxiv.org/abs/2402.14881
tags:
- chatgpt
- questions
- accuracy
- answer
- above
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how well ChatGPT can answer test questions
  and develops methods to identify and avoid such vulnerable questions. The authors
  analyze over 10,000 medical school entrance exam questions using ChatGPT API, finding
  it answers correctly about 60% of the time.
---

# A Study on the Vulnerability of Test Questions against ChatGPT-based Cheating

## Quick Facts
- arXiv ID: 2402.14881
- Source URL: https://arxiv.org/abs/2402.14881
- Authors: Shanker Ram; Chen Qian
- Reference count: 16
- Key outcome: ChatGPT answers 60% of medical school entrance exam questions correctly, with performance varying significantly by topic and answer type

## Executive Summary
This study investigates ChatGPT's ability to answer test questions and develops methods to identify vulnerable questions that could enable cheating. Using the MedMCQA dataset of 200,000 medical school entrance exam questions, the authors find that ChatGPT achieves approximately 60% accuracy overall, with significant variation across topics - excelling at biochemistry and psychiatry questions (over 70% correct) while struggling with dentistry questions (under 50% correct). The authors also discover that ChatGPT systematically overpredicts "All of the above" and "None of the above" options even when they are incorrect. To help test-makers identify vulnerable questions, they develop an NLP model using BERT that predicts ChatGPT's correctness with 60% overall accuracy, rising to over 70% when the model is highly confident.

## Method Summary
The authors collected ChatGPT responses for over 10,000 medical school entrance exam questions using the ChatGPT API, analyzing accuracy patterns across question features and topics. They developed a BERT-based NLP model to predict whether ChatGPT would answer questions correctly, training on 75% of the data with 20% for validation and 5% for testing. The model uses token IDs from BERT tokenization, applies a pooling layer, dropout, and linear transformation to generate predictions. They evaluated the model's performance and identified confidence thresholds where predictions become more reliable.

## Key Results
- ChatGPT achieves approximately 60% accuracy on medical school entrance exam questions
- Performance varies dramatically by topic: biochemistry/psychiatry >70% correct vs dentistry <50% correct
- ChatGPT overpredicts "All of the above" and "None of the above" options (81-98% selection when incorrect)
- BERT-based NLP model predicts ChatGPT correctness with 60% overall accuracy, rising to >70% for high-confidence predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT overpredicts "All of the above" and "None of the above" options when they are not the correct answer
- Mechanism: ChatGPT treats questions as free-response problems, selecting "(D) None/All of the above" 81-98% of the time even when incorrect
- Core assumption: ChatGPT's training data contains strong patterns linking these phrases to correct answers
- Evidence anchors:
  - [abstract] "ChatGPT overpredicts 'All of the above' and 'None of the above' options"
  - [section] "ChatGPT's accuracy was a shockingly low 48%... for questions where 'None of the above' was incorrect"
  - [corpus] Weak - no corpus evidence found
- Break condition: If training data distribution changes or model updates alter answer selection patterns

### Mechanism 2
- Claim: Question topic significantly impacts ChatGPT's accuracy, with biochemistry/psychiatry >70% correct vs dentistry <50%
- Mechanism: ChatGPT performs better on topics with more internet training data
- Core assumption: Model training data reflects internet content distribution
- Evidence anchors:
  - [abstract] "biochemistry and psychiatry questions answered correctly over 70% of the time, while dentistry questions are answered correctly under 50% of the time"
  - [section] "biochemistry and psychiatry... information is more readily available [11] on the Internet in comparison to dentistry"
  - [corpus] Weak - no corpus evidence found
- Break condition: If training corpus becomes more balanced across topics

### Mechanism 3
- Claim: Word complexity and question structure don't affect ChatGPT's performance
- Mechanism: ChatGPT processes questions at semantic level rather than surface features
- Core assumption: Model has sufficient semantic understanding to bypass superficial complexity
- Evidence anchors:
  - [abstract] "ChatGPT was advanced enough to answer correctly irrespective of these metrics"
  - [section] "all of the lexical richness metrics were the same independent of whether ChatGPT got the problem right or wrong"
  - [corpus] Weak - no corpus evidence found
- Break condition: If model architecture changes to incorporate surface-level processing

## Foundational Learning

- Concept: Natural Language Processing (NLP) preprocessing
  - Why needed here: Required to convert questions into numerical format for neural network training
  - Quick check question: What preprocessing steps are applied before feeding text to BERT?

- Concept: BERT tokenization and token IDs
  - Why needed here: BERT model requires tokenized input with corresponding token IDs for processing
  - Quick check question: How does BERT handle words that aren't in its vocabulary?

- Concept: Neural network architecture and training
  - Why needed here: The NLP model uses BERT layers with dropout and linear transformation for prediction
  - Quick check question: What is the purpose of the dropout layer in the model?

## Architecture Onboarding

- Component map:
  Data collection (ChatGPT API) → Preprocessing (BERT tokenization) → Neural network (BERT + pooling + dropout + linear) → Prediction output
  Training pipeline: Training data (75%) → Validation data (20%) → Testing data (5%)

- Critical path:
  1. Generate ChatGPT responses for questions
  2. Preprocess text using BERT tokenization
  3. Train neural network with adam optimizer
  4. Evaluate model accuracy and confidence thresholds

- Design tradeoffs:
  - Accuracy vs. confidence: 60% overall accuracy vs 70%+ when confidence >85%
  - Model complexity: Using pre-trained BERT vs custom tokenization
  - Dataset size: 11,657 questions vs potential for domain-specific data

- Failure signatures:
  - Model accuracy plateaus below 60%
  - Confidence scores don't correlate with accuracy
  - Topic-specific performance varies wildly

- First 3 experiments:
  1. Test model on questions with "except" phrasing to verify 6% accuracy drop
  2. Evaluate model on questions with "All of the above" when it's incorrect
  3. Compare model performance across different medical specialties

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different language models (e.g., Bard, Claude, Bing AI) perform on the same types of questions that ChatGPT struggles with or excels at?
- Basis in paper: [explicit] The authors mention testing ChatGPT's performance and suggest future work to validate or revise the trends noticed for ChatGPT for other chatbots such as Bard AI, Claude, or Bing AI.
- Why unresolved: The paper focuses solely on ChatGPT and does not provide comparative data on other language models' performance on similar question types.
- What evidence would resolve it: Conducting the same experiments with other language models and comparing their performance on questions with specific characteristics (e.g., "except" questions, multi-select problems) would provide evidence to resolve this question.

### Open Question 2
- Question: How does the accuracy of the NLP model in predicting ChatGPT's performance vary when trained on different types of questions (e.g., math, computer science, literature)?
- Basis in paper: [inferred] The authors mention future work to test ChatGPT in other fields such as math, computer science, and literature, and to increase the accuracy of the current NLP model.
- Why unresolved: The paper only tests the NLP model on medical school entrance exam questions and does not explore its performance on other types of questions.
- What evidence would resolve it: Training the NLP model on different types of questions and evaluating its accuracy in predicting ChatGPT's performance on those questions would provide evidence to resolve this question.

### Open Question 3
- Question: How does the performance of ChatGPT on test questions change over time as the model is updated and fine-tuned?
- Basis in paper: [explicit] The authors test ChatGPT's performance on different versions of the model (e.g., gpt-3.5-turbo, gpt-4) and observe significant differences in accuracy.
- Why unresolved: The paper only provides a snapshot of ChatGPT's performance at a specific point in time and does not explore how its performance changes as the model is updated.
- What evidence would resolve it: Continuously monitoring and testing ChatGPT's performance on the same set of questions as the model is updated and fine-tuned would provide evidence to resolve this question.

## Limitations
- Study only examined medical school entrance exam questions, limiting generalizability to other domains
- ChatGPT's performance could change with future model updates, making identified patterns temporary
- NLP model's 60% accuracy indicates significant room for improvement in reliably predicting vulnerable questions

## Confidence

**High confidence**: ChatGPT's accuracy varies significantly by question topic (biochemistry/psychiatry >70% vs dentistry <50%). This finding is supported by clear numerical evidence and reasonable explanations about topic-specific internet content availability.

**Medium confidence**: The NLP model can predict ChatGPT's correctness with 60% overall accuracy and >70% for high-confidence predictions. While the methodology is sound, the relatively modest accuracy suggests the model captures only partial patterns.

**Medium confidence**: ChatGPT overpredicts "All of the above" and "None of the above" options when incorrect. The dramatic 81-98% selection rate when wrong provides strong evidence, but this could be an artifact of the specific dataset or model version.

## Next Checks
1. Test the NLP model on a different domain (e.g., physics or humanities questions) to assess generalizability of vulnerability predictions.
2. Replicate the study using a newer version of ChatGPT (e.g., GPT-4) to determine if identified patterns persist across model updates.
3. Conduct a controlled experiment with actual students using ChatGPT during practice exams to measure real-world impact on cheating behavior and performance.