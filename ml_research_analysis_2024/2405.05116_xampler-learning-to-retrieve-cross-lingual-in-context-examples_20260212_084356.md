---
ver: rpa2
title: 'XAMPLER: Learning to Retrieve Cross-Lingual In-Context Examples'
arxiv_id: '2405.05116'
source_url: https://arxiv.org/abs/2405.05116
tags:
- learning
- examples
- language
- in-context
- xampler
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "XAMPLER improves cross-lingual in-context learning by training\
  \ a multilingual retriever using only English data. It constructs positive and negative\
  \ examples from a multilingual LLM\u2019s predictions, fine-tunes a small multilingual\
  \ model (Glot500) with contrastive loss, and uses the resulting retriever to fetch\
  \ English few-shot examples for queries in any of 500+ languages."
---

# XAMPLER: Learning to Retrieve Cross-Lingual In-Context Examples

## Quick Facts
- arXiv ID: 2405.05116
- Source URL: https://arxiv.org/abs/2405.05116
- Reference count: 21
- Primary result: Achieves up to 75.91% accuracy on cross-lingual classification across 176 languages

## Executive Summary
XAMPLER addresses the challenge of cross-lingual in-context learning by training a multilingual retriever using only English data. The method leverages a large multilingual LLM (MaLA500) to construct positive and negative examples from English predictions, then fine-tunes a smaller multilingual model (Glot500) as a retriever. This approach enables retrieval of English few-shot examples for queries in any of 500+ languages without requiring labeled data in target languages. Evaluated on SIB200 (176 languages) and MasakhaNEWS (16 African languages), XAMPLER significantly outperforms baselines including multilingual models, off-the-shelf retrievers, and translation-based methods.

## Method Summary
XAMPLER constructs positive and negative examples by evaluating MaLA500's predictions on English queries when provided with candidate examples as one-shot demonstrations. These signals are used to fine-tune Glot500 with contrastive loss, creating a retriever that can identify which English examples are helpful for predicting labels of multilingual queries. The fine-tuned retriever then selects English examples for in-context learning with MaLA500 across target languages. The entire training process requires only English data, making it scalable to the 500+ languages covered by the underlying models.

## Key Results
- Achieves 75.91% accuracy on SIB200 across 176 languages, outperforming MaLA500, SBERT, LaBSE, and Multilingual E5
- Outperforms translation-based methods by 1.41% and traditional cross-lingual transfer by 6-7%
- Shows consistent improvement across MasakhaNEWS (16 African languages) with 61.46% accuracy
- Performance degrades when retrieving more than 10 candidates, indicating optimal k=10

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The retriever learns to identify which English examples are helpful for predicting labels of multilingual queries without needing labeled data in target languages
- Mechanism: Positive and negative examples are constructed by evaluating whether MaLA500 correctly predicts the label of an English query when provided with each candidate example as a one-shot example
- Core assumption: MaLA500's predictions on English queries can serve as a reliable signal for determining example usefulness across all 500+ languages
- Evidence anchors: [abstract] "XAMPLER first trains a retriever based on Glot500... using positive and negative English examples constructed from the predictions of... MaLA500"
- Break condition: If MaLA500's multilingual generalization is weak or biased toward certain language families, the positive/negative signals may not transfer well

### Mechanism 2
- Claim: Cross-lingual semantic alignment in Glot500 enables direct retrieval of English examples for queries in any language without translation
- Mechanism: Glot500 is trained with contrastive loss to minimize distance between queries and relevant examples in the embedding space
- Core assumption: The multilingual pretraining of Glot500 creates shared semantic space across languages, so similarity in embedding space reflects semantic similarity across languages
- Evidence anchors: [abstract] "Leveraging the cross-lingual capacity of the retriever, it can directly retrieve English examples as few-shot examples"
- Break condition: If cross-lingual alignment is poor for distant language pairs or if the retriever overfits to English-specific semantic patterns

### Mechanism 3
- Claim: In-context learning with retrieved examples is more effective than traditional cross-lingual transfer via fine-tuning
- Mechanism: The paper compares XAMPLER to cross-lingual transfer methods and finds that ICL with retrieved examples outperforms both fine-tuning of Glot500 and MaLA500 on target languages
- Core assumption: Providing task-relevant examples in context leverages the few-shot capabilities of LLMs more effectively than parameter updates
- Evidence anchors: [section 3.3] "XAMPLER outperforms XLT (Glot500) and XLT (MaLA500) by 6.40% and 6.01%, respectively"
- Break condition: If the LLM's few-shot learning capabilities are limited for certain task types

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: Used to train the retriever to distinguish between helpful and unhelpful examples by minimizing distance to positive examples and maximizing distance to negative examples
  - Quick check question: What loss function is typically used in contrastive learning to push apart negative pairs while pulling together positive pairs?

- Concept: Cross-lingual embeddings
  - Why needed here: Glot500's pretrained cross-lingual embeddings allow it to map queries from any language into a shared semantic space with English examples
  - Quick check question: What property must a multilingual embedding space have to enable retrieval of English examples for non-English queries?

- Concept: In-context learning (few-shot prompting)
  - Why needed here: The final step uses retrieved examples as demonstrations to guide MaLA500's predictions without updating model parameters
  - Quick check question: How does the selection of in-context examples affect the performance of few-shot learning in LLMs?

## Architecture Onboarding

- Component map: MaLA500 (10B multilingual LLM) -> Glot500 (395M multilingual model) -> SBERT/LaBSE (off-the-shelf retrievers) -> Data pipeline (constructs Dpos/Dneg) -> Evaluation (SIB200, MasakhaNEWS)

- Critical path: Construct Dpos/Dneg from MaLA500 → Fine-tune Glot500 with contrastive loss → Retrieve English examples for target language queries → Perform in-context learning with MaLA500

- Design tradeoffs:
  - Using a smaller retriever (Glot500) vs. fine-tuning MaLA500 directly (computational cost vs. performance)
  - Constructing signals only from English data vs. needing labeled data in all target languages (scalability vs. potential signal quality)
  - Selecting top-k candidates vs. evaluating all pairs (efficiency vs. completeness)

- Failure signatures:
  - Poor retrieval performance on low-resource languages may indicate weak cross-lingual alignment in Glot500
  - Marginal improvement over baselines may suggest MaLA500's signals are not reliable across languages
  - High variance across languages may indicate the method works better for certain language families

- First 3 experiments:
  1. Run XAMPLER on a subset of SIB200 (e.g., 10 languages) to verify the full pipeline works and measure baseline performance
  2. Compare retrieval quality using different k values (1, 5, 10, 20) to find optimal candidate selection
  3. Evaluate the impact of using different layers of Glot500 for embeddings to find the best layer for retrieval quality

## Open Questions the Paper Calls Out

- Open Question 1: How does XAMPLER's performance compare when using a non-English source language for training the retriever?
  - Basis: The paper acknowledges that English may not be the optimal source language for cross-lingual transfer across all target languages
  - Why unresolved: The paper does not explore the selection of different source languages due to the predominant availability of training data in English
  - What evidence would resolve it: Experiments comparing XAMPLER's performance when trained on annotated data from different source languages (e.g., French, Spanish) versus English

- Open Question 2: What is the impact of varying the number of positive and negative examples (k) on XAMPLER's performance?
  - Basis: The paper discusses the selection of top k similar examples as candidates using existing off-the-shelf retrievers and mentions that XAMPLER performs optimally when k = 10
  - Why unresolved: The paper does not provide a detailed analysis of how varying k affects the quality of the retrieved examples
  - What evidence would resolve it: A comprehensive study analyzing the performance of XAMPLER with different values of k and their impact on the quality of retrieved examples

- Open Question 3: How does XAMPLER's performance compare to other cross-lingual retrieval methods when evaluated on languages not covered by the MaLA500/Glot500 models?
  - Basis: The paper mentions that XAMPLER relies on the cross-lingual capacity of the Glot500 model, which covers 534 languages
  - Why unresolved: The paper does not discuss the performance on languages outside this coverage
  - What evidence would resolve it: Experiments evaluating XAMPLER's performance on languages not covered by the MaLA500/Glot500 models

## Limitations

- Heavy dependence on MaLA500's multilingual capabilities as the source of supervision signals, which may break down for low-resource languages
- Lack of analysis on how well the cross-lingual alignment in Glot500 generalizes to all language pairs after fine-tuning
- Evaluation scope limited to two datasets without extensive ablation studies or analysis of performance across language families

## Confidence

**High Confidence** - The core technical approach of using English-only data to train a multilingual retriever for cross-lingual in-context learning is clearly described and methodologically sound

**Medium Confidence** - The claim that XAMPLER outperforms translation-based methods and traditional cross-lingual transfer by substantial margins (up to 6-7% accuracy) is supported by the reported results, but evaluation only covers two datasets

**Low Confidence** - The assumption that MaLA500's predictions on English queries can reliably signal example quality for 500+ target languages is not empirically validated

## Next Checks

1. **Language Family Analysis**: Evaluate XAMPLER's performance separately on different language families (e.g., Indo-European, Afro-Asiatic, Niger-Congo) to identify which families benefit most from the approach

2. **Low-Resource Language Test**: Test the method on truly low-resource languages not well-represented in pretraining data to assess whether the cross-lingual alignment assumption holds for these cases

3. **Ablation on MaLA500 Quality**: Compare XAMPLER's performance when using MaLA500 vs. using ground-truth labels for constructing positive/negative examples to quantify how much the method's success depends on the LLM's quality