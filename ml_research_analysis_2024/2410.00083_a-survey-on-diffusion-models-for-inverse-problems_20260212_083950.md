---
ver: rpa2
title: A Survey on Diffusion Models for Inverse Problems
arxiv_id: '2410.00083'
source_url: https://arxiv.org/abs/2410.00083
tags:
- diffusion
- inverse
- problems
- methods
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides a comprehensive overview of methods that utilize
  pre-trained diffusion models to solve inverse problems without requiring further
  training. The paper introduces taxonomies to categorize these methods based on the
  problems they address and the techniques they employ.
---

# A Survey on Diffusion Models for Inverse Problems

## Quick Facts
- arXiv ID: 2410.00083
- Source URL: https://arxiv.org/abs/2410.00083
- Reference count: 40
- This survey provides a comprehensive overview of methods that utilize pre-trained diffusion models to solve inverse problems without requiring further training.

## Executive Summary
This survey systematically categorizes diffusion-based inverse problem solvers into four families based on their approach to handling the intractable posterior distribution: explicit approximations, variational inference, CSGM-type methods, and asymptotically exact methods. It provides practitioners with a clear framework for understanding how different algorithms leverage pre-trained diffusion models as priors for solving inverse problems, focusing on unsupervised methods that sample from the posterior distribution. The survey also addresses specific challenges introduced by latent diffusion models and offers insights into practical implementation considerations.

## Method Summary
The survey categorizes diffusion-based inverse problem solvers by how they handle the intractable term ∇_x log p(y|x_t) in the reverse diffusion process. Explicit approximation methods use closed-form expressions like Tweedie's formula, variational methods employ simpler distributions to approximate the posterior, CSGM-type methods optimize in latent space, and asymptotically exact methods rely on Monte Carlo sampling. The survey provides detailed descriptions of representative methods from each family, along with discussions of latent diffusion model challenges and potential solutions.

## Key Results
- Introduces comprehensive taxonomies categorizing diffusion-based inverse problem methods into four families based on their mathematical approaches
- Analyzes connections between different methods, providing practitioners with clear guidance on selecting appropriate algorithms
- Addresses specific challenges of using latent diffusion models for inverse problems, including non-linearity and information loss in the encoding-decoding process

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The survey's taxonomies help practitioners navigate the vast landscape of diffusion-based inverse problem solvers by grouping methods according to shared techniques and problem types.
- **Mechanism:** By categorizing methods into four families—explicit approximations, variational inference, CSGM-type, and asymptotically exact methods—the survey provides a clear mental model for how different algorithms address the fundamental challenge of intractable posterior distributions. Each family targets the intractable term ∇_x log p(y|x_t) differently: explicit approximations replace it with closed-form expressions, variational methods use simpler distributions to approximate the posterior, CSGM-type methods optimize in latent space, and asymptotically exact methods rely on Monte Carlo sampling.
- **Core assumption:** That practitioners can recognize which category their problem fits into and therefore select an appropriate method without needing to understand every algorithmic detail.
- **Evidence anchors:**
  - [abstract]: "We introduce taxonomies to categorize these methods based on both the problems they address and the techniques they employ."
  - [section 3]: Detailed descriptions of the four families with concrete examples like Score ALD, RED-Diff, DMPlug, and PnP-DM.
  - [corpus]: The related paper "Briding Diffusion Posterior Sampling and Monte Carlo methods: a survey" reinforces the survey's focus on connecting diffusion models to Monte Carlo methods, supporting the validity of the asymptotically exact family.
- **Break condition:** If the problem involves a forward model that changes dynamically (e.g., blind inverse problems with unknown parameters), the static taxonomies may not adequately capture the algorithmic adaptations required.

### Mechanism 2
- **Claim:** The explicit approximation methods work because they leverage Tweedie's formula to connect denoising score matching with the conditional score needed for inverse problems.
- **Mechanism:** Tweedie's formula states that the gradient of the log-density of a noisy image equals the denoising residual divided by the noise variance. This allows training diffusion models to predict clean images from noisy ones, then converting those predictions into score estimates. For inverse problems, this score is combined with a measurement consistency term to guide the reverse diffusion process toward solutions that satisfy both the data fidelity and the learned prior.
- **Core assumption:** The corruption process in the inverse problem is either linear or can be approximated as such, allowing the measurement consistency term to be expressed in a tractable form.
- **Evidence anchors:**
  - [section 2.2]: Formal statement of Tweedie's formula and its connection to denoising score matching.
  - [section 3.1]: Multiple methods (Score ALD, Score-SDE, ILVR, DPS) explicitly use this connection, with equations showing how the measurement score is approximated.
  - [corpus]: The related paper "Error Analysis of Bayesian Inverse Problems with Generative Priors" suggests ongoing work on understanding the theoretical foundations of these approximations.
- **Break condition:** When the forward model is highly non-linear or the noise is non-Gaussian, the closed-form expressions for the measurement score may become inaccurate or intractable.

### Mechanism 3
- **Claim:** Latent diffusion models can solve inverse problems efficiently by operating in a compressed latent space, but this introduces non-linearity that must be carefully managed.
- **Mechanism:** The encoder-decoder architecture of latent diffusion models compresses images into a lower-dimensional latent space where diffusion occurs. However, since measurements are in pixel space, the forward model (even if linear in pixel space) becomes non-linear in latent space. Methods like Latent DPS, PSLD, and P2L address this by projecting measurements into the latent space or incorporating additional consistency terms that account for the non-linear encoding-decoding map.
- **Core assumption:** The latent space learned by the autoencoder preserves enough information about the image to enable meaningful reconstruction when combined with diffusion-based priors.
- **Evidence anchors:**
  - [section 3.5]: Discussion of the challenges specific to latent diffusion models, including loss of linearity and the non-one-to-one nature of the encoding-decoding map.
  - [section 3.5.2-3.5.6]: Multiple methods proposed to address these challenges, with PSLD adding a term for fixed-point consistency and P2L optimizing text embeddings during sampling.
  - [corpus]: The related paper "SILO: Solving Inverse Problems with Latent Operators" suggests active research on latent space approaches, supporting the survey's treatment of this topic.
- **Break condition:** If the autoencoder's latent space is too lossy or the measurement model involves operations that are poorly represented in latent space (e.g., certain types of non-local operations), the reconstruction quality may degrade significantly.

## Foundational Learning

- **Concept: Tweedie's formula and its connection to denoising score matching**
  - Why needed here: This is the mathematical foundation that enables diffusion models to learn score functions from noisy data, which is then used for inverse problem solving.
  - Quick check question: Given a noisy observation y = x + σz where z ~ N(0,I), what is the gradient of the log-density of y with respect to y?

- **Concept: Forward and reverse stochastic differential equations in diffusion models**
  - Why needed here: Understanding the SDE formulation is crucial for grasping how diffusion models generate samples and how this process can be modified to incorporate measurement constraints.
  - Quick check question: In the reverse SDE, what term represents the score function that needs to be estimated during training?

- **Concept: Bayesian inference and posterior sampling in inverse problems**
  - Why needed here: Inverse problems are fundamentally about recovering unknown quantities from noisy measurements, which is a Bayesian inference problem where the posterior distribution needs to be characterized or sampled from.
  - Quick check question: What is the relationship between the posterior distribution p(x|y) and the prior p(x) and likelihood p(y|x) according to Bayes' rule?

## Architecture Onboarding

- **Component map:** Pre-trained diffusion model -> Measurement model A(x) + noise -> Inversion algorithm -> Encoder/decoder (for latent models) -> Text encoder (for text-conditioned models)

- **Critical path:**
  1. Load pre-trained diffusion model
  2. Define measurement model and parameters
  3. Select inversion algorithm family
  4. Implement measurement consistency term
  5. Run reverse diffusion with guidance

- **Design tradeoffs:**
  - Explicit approximations offer computational efficiency but may sacrifice accuracy for complex measurement models
  - Variational methods provide theoretical guarantees but require careful choice of proposal distribution
  - CSGM-type methods leverage existing optimization techniques but need multiple diffusion steps
  - Asymptotically exact methods guarantee convergence but are computationally expensive

- **Failure signatures:**
  - Poor reconstruction quality despite correct implementation often indicates mismatch between the assumed measurement model and the actual data
  - Instability during sampling may result from inappropriate step sizes or guidance strengths
  - Mode collapse (overly smooth reconstructions) suggests insufficient diversity in the learned prior

- **First 3 experiments:**
  1. Implement Score ALD for denoising with a simple U-Net-based diffusion model and synthetic Gaussian noise
  2. Apply DPS to linear inverse problems like inpainting or compressive sensing with known forward operators
  3. Test Latent DPS on a latent diffusion model for image super-resolution, comparing with pixel-space methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the computational performance trade-offs between different approximation methods for the measurement matching term in diffusion-based inverse problem solvers?
- Basis in paper: [explicit] The paper mentions that methods propose increasingly complex "lifting" matrices and that some approximations require increased computation (e.g., Moment Matching method).
- Why unresolved: While the paper categorizes methods and describes their approaches, it does not provide a standardized benchmark or quantitative comparison of computational efficiency across different methods.
- What evidence would resolve it: A systematic benchmark comparing the computational requirements (e.g., memory usage, runtime) of different approximation methods on a standardized set of inverse problems.

### Open Question 2
- Question: How do approximation errors propagate through different diffusion-based inverse problem solvers under various distributional assumptions?
- Basis in paper: [inferred] The paper discusses that different methods make approximations to the intractable integral in the measurement matching term, but notes that the field would benefit from understanding the propagation of these approximation errors.
- Why unresolved: The paper acknowledges that approximation errors exist but does not provide analytical characterization of how these errors propagate through the solution process under different assumptions.
- What evidence would resolve it: Analytical studies deriving error bounds for different approximation methods under specific distributional assumptions, or empirical studies showing how approximation errors affect solution quality across different problem types.

### Open Question 3
- Question: Can asymptotically exact methods for sampling from the true posterior in diffusion-based inverse problems scale to practical settings beyond infinite computation?
- Basis in paper: [explicit] The paper states that asymptotically exact methods "only hold under the setting of infinite computation and it remains to be seen if they can scale to more practical settings."
- Why unresolved: While theoretically appealing, these methods rely on Monte Carlo approaches that require increasing computation for convergence, and the paper acknowledges uncertainty about their practical scalability.
- What evidence would resolve it: Empirical studies demonstrating the performance of asymptotically exact methods on practical inverse problems with finite computational resources, comparing their accuracy and efficiency against approximate methods.

## Limitations

- The survey's taxonomies may not fully capture emerging hybrid approaches that combine techniques from multiple families
- Focus on unsupervised methods excludes supervised techniques that might offer better performance for specific inverse problems
- The survey does not extensively address the computational costs associated with different approaches, which can be significant for high-resolution images or real-time applications

## Confidence

- **High Confidence:** The categorization of methods into four families based on their approach to handling the intractable posterior term is well-supported by the literature and mathematical foundations.
- **Medium Confidence:** The analysis of latent diffusion models' challenges is thorough, but the survey may not fully capture all the nuances of the non-linearities introduced by the encoding-decoding process.
- **Low Confidence:** The survey's assessment of the practical performance of different methods is limited, as it does not provide extensive empirical comparisons or benchmarks.

## Next Checks

1. **Empirical Benchmarking:** Conduct a systematic comparison of the four method families on a standardized set of inverse problems (e.g., denoising, inpainting, super-resolution) using both quantitative metrics (PSNR, SSIM) and qualitative assessments.

2. **Computational Cost Analysis:** Measure the inference time and memory requirements for each method family across different image resolutions and measurement models to provide practitioners with practical guidance on resource allocation.

3. **Hybrid Approach Exploration:** Investigate combinations of techniques from different families (e.g., explicit approximations with variational inference) to determine if hybrid methods can offer improved performance or efficiency for specific inverse problem classes.