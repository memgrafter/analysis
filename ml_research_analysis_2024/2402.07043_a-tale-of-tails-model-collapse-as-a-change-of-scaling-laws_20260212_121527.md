---
ver: rpa2
title: 'A Tale of Tails: Model Collapse as a Change of Scaling Laws'
arxiv_id: '2402.07043'
source_url: https://arxiv.org/abs/2402.07043
tags:
- data
- scaling
- figure
- collapse
- laws
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how synthetic data generated by AI models
  affects the scaling laws of downstream models. It introduces a theoretical framework
  that explains the loss of scaling when models are trained on AI-generated data.
---

# A Tale of Tails: Model Collapse as a Change of Scaling Laws

## Quick Facts
- arXiv ID: 2402.07043
- Source URL: https://arxiv.org/abs/2402.07043
- Reference count: 40
- Models trained on AI-generated data experience scaling law changes due to "tail cutting" and "tail narrowing" mechanisms

## Executive Summary
This paper investigates how synthetic data generated by AI models affects the scaling laws of downstream models. It introduces a theoretical framework that explains the loss of scaling when models are trained on AI-generated data. The framework considers both "tail cutting" (removing low-probability outcomes) and "tail narrowing" (concentrating the probability distribution) mechanisms. The paper demonstrates new scaling laws for these scenarios, showing how model performance plateaus or degrades with increased synthetic data. It also explores strategies to mitigate model collapse, such as mixing AI-generated data with original data. The theory is validated through experiments on various models, including large language models and arithmetic transformers.

## Method Summary
The paper develops a theoretical framework to analyze how synthetic data generation affects model performance scaling. It models two key phenomena: tail cutting (where low-probability data points are systematically removed from the training distribution) and tail narrowing (where the distribution becomes more concentrated around high-probability regions). The framework derives new scaling laws that predict how performance changes as the proportion of synthetic data increases. The theoretical predictions are validated through controlled experiments on synthetic data generation tasks, arithmetic transformers, and language models up to 1B parameters. The experiments systematically vary the proportion of synthetic data and measure the resulting performance changes, comparing them against the predicted scaling law modifications.

## Key Results
- The paper derives new scaling laws that predict performance degradation when models are trained on AI-generated data
- Tail cutting and tail narrowing mechanisms are identified as the primary drivers of model collapse, with different mathematical characterizations
- Mixing original and synthetic data can mitigate collapse, with optimal mixing ratios depending on the generation quality and task complexity

## Why This Works (Mechanism)
The mechanism operates through fundamental changes in the data distribution that models learn from. When models generate synthetic data, they tend to reproduce patterns they've already learned well while struggling with rare or complex patterns. This creates a systematic bias in the training distribution - either by removing rare events entirely (tail cutting) or by making common events even more dominant (tail narrowing). The original scaling laws, which assume a fixed and representative data distribution, no longer apply. The theoretical framework captures how these distribution changes affect the model's ability to learn and generalize, showing that performance improvements that would normally scale with model size and data quantity instead plateau or degrade.

## Foundational Learning
- **Scaling Laws**: Why needed - Provide baseline performance expectations as models grow; Quick check - Compare predicted vs actual performance on controlled tasks
- **KL Divergence**: Why needed - Quantifies distributional differences between original and synthetic data; Quick check - Measure divergence between data distributions at different generation stages
- **Tail Statistics**: Why needed - Characterize rare event handling in distributions; Quick check - Track tail behavior as synthetic data proportion increases
- **Data Mixing Strategies**: Why needed - Potential mitigation for model collapse; Quick check - Test different mixing ratios on model performance
- **Synthetic Data Generation**: Why needed - Core process affecting downstream model training; Quick check - Control generation parameters to isolate effects
- **Distributional Shift**: Why needed - Explains fundamental mechanism of performance degradation; Quick check - Track feature distributions across training iterations

## Architecture Onboarding

Component Map:
Original Data -> Model A (generates synthetic data) -> Mixed Data (original + synthetic) -> Model B (trains on mixed data) -> Performance Evaluation

Critical Path:
The critical path flows from original data through generation to mixed training data, as this sequence determines the distribution shifts that cause model collapse. The generation quality and proportion of synthetic data directly impact downstream performance.

Design Tradeoffs:
The framework trades theoretical precision for practical applicability. While the exact mathematical characterization of tail cutting vs tail narrowing is complex, the practical insight - that synthetic data degrades performance in predictable ways - is actionable. The mixing strategy introduces a hyperparameter (mixing ratio) that must be tuned per task.

Failure Signatures:
Model collapse manifests as performance plateaus despite increased model size or training data, or actual performance degradation with more synthetic data. Early warning signs include increasing KL divergence between original and synthetic data distributions, and decreasing model sensitivity to rare events.

3 First Experiments:
1. Train Model B with varying proportions of synthetic data (0% to 100%) and measure performance scaling
2. Compare tail cutting vs tail narrowing effects by controlling generation parameters differently
3. Test mixing strategies with different original:synthetic ratios to find optimal mitigation approach

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Assumes static underlying data distribution, which may not hold in iterative training scenarios
- Theoretical distinction between tail cutting and tail narrowing may not cleanly map to real-world scenarios
- Experimental validation primarily on synthetic data may not capture full complexity of real-world distribution shifts

## Confidence

High:
- Mathematical derivation of scaling laws for both tail cutting and tail narrowing scenarios
- Experimental validation on controlled synthetic data generation tasks
- Basic framework for understanding model collapse

Medium:
- Practical implications of scaling law modifications
- Effectiveness of data mixing strategies
- Generalizability to different model architectures and tasks

Low:
- Long-term behavior of iteratively trained models
- Framework's applicability to multimodal data
- Real-world impact on deployed AI systems

## Next Checks
1. Conduct experiments with iterative training cycles where models generate synthetic data, which is then used to train new models, to observe the long-term evolution of scaling laws and potential feedback loops.
2. Test the framework's predictions on larger language models (10B+ parameters) and across different modalities (images, audio) to assess generalizability and scalability limits.
3. Implement and evaluate more sophisticated diversity preservation techniques, including adaptive mixing ratios and dynamic thresholding, to determine optimal strategies for preventing model collapse in production settings.