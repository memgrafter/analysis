---
ver: rpa2
title: Information Anxiety in Large Language Models
arxiv_id: '2411.10813'
source_url: https://arxiv.org/abs/2411.10813
tags:
- layer
- popularity
- each
- questions
- popular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how entity popularity affects the internal
  processing of large language models (LLMs) in open-domain question answering. Using
  the PopQA dataset, the authors systematically analyze the effect of query popularity
  on three key dimensions: convergence of predicted probability distributions, attention
  to relevant tokens, and similarity of retrieved facts across lexical variants.'
---

# Information Anxiety in Large Language Models

## Quick Facts
- arXiv ID: 2411.10813
- Source URL: https://arxiv.org/abs/2411.10813
- Reference count: 40
- Primary result: Popular queries cause LLMs to converge quickly but retrieve inconsistent facts across lexical variants, termed "information anxiety"

## Executive Summary
This paper investigates how entity popularity affects LLM internal processing in open-domain QA. Using the PopQA dataset, the authors analyze how query popularity impacts predicted probability distribution convergence, attention to constraint tokens, and fact retrieval consistency across lexical variants. They find that highly popular queries accelerate convergence to correct answers but cause LLMs to attend poorly to constraint tokens and retrieve increasingly dissimilar facts across lexical variations. This phenomenon, termed "information anxiety," reveals robustness concerns in LLMs when processing frequently occurring entities.

## Method Summary
The study uses the PopQA dataset with 14k questions and lexical variations for each query (10 per relation), with Wikipedia page views as popularity proxy. The authors systematically analyze three dimensions: convergence of predicted probability distributions using KL-divergence, attention to constraint tokens, and similarity of retrieved facts across lexical variants using cosine similarity. They employ few-shot prompting with Llama-2 family models (7B, 13B, 70B variants) and intercept hidden states after each decoder block to examine internal processing patterns.

## Key Results
- Popular queries accelerate convergence of predicted probability distributions toward correct answers
- As query popularity increases, retrieved facts across lexical variations become increasingly dissimilar and less accurate
- LLMs struggle to disentangle facts from distinct relations when dealing with highly popular subjects
- Constraint tokens receive significantly lower attention scores for popular queries

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: High entity popularity accelerates convergence of predicted probability distributions toward correct answers but also triggers inconsistent fact retrieval across lexical variants.
- **Mechanism**: When queries involve highly popular entities, LLMs leverage strong memorization of these entities, causing rapid convergence of the predicted probability distribution. However, the same popularity causes the model to rely more on general patterns and less on specific query tokens, leading to retrieval of dissimilar facts across different lexical forms.
- **Core assumption**: Popularity correlates with frequency in pretraining data, which enhances memorization and speeds convergence but reduces attention to query-specific tokens.
- **Evidence anchors**:
  - [abstract]: "popular questions facilitate early convergence of internal states toward the correct answer" and "as the popularity of a query increases, retrieved attributes across lexical variations become increasingly dissimilar and less accurate."
  - [section]: "For queries surrounding highly popular entities, LLMs attend to the relevant tokens poorly" and "LLMs retrieve less similar facts sensitive to lexical perturbations in query."
- **Break condition**: If popularity does not correlate with pretraining frequency, or if models use mechanisms that compensate for popularity-induced token neglect (e.g., stronger constraint attention), this mechanism would break.

### Mechanism 2
- **Claim**: LLMs exhibit "information anxiety" by showing lower attention scores to constraint tokens and higher sensitivity to lexical variations when processing popular queries.
- **Mechanism**: High popularity reduces the model's attention to constraint tokens (subject and relation words) because it assumes the answer is already well memorized. This leads to reliance on internal memorized knowledge, which becomes more sensitive to lexical perturbations, causing inconsistent retrieval.
- **Core assumption**: Attention mechanisms are responsible for focusing on query-specific tokens; reduced attention to these tokens forces reliance on memorized knowledge, which is more variable across lexical forms.
- **Evidence anchors**:
  - [abstract]: "LLMs tend to overlook essential parts of the query" and "LLMs retrieve less similar facts sensitive to lexical perturbations."
  - [section]: "LLMs poorly attend to relevant sections of queries that pertain to highly popular entities" and "the similarity of knowledge retrieved from the model parameters decreases in the early layers."
- **Break condition**: If the model uses alternative reasoning pathways that do not depend on attention to constraint tokens, or if lexical sensitivity is mitigated by stronger alignment mechanisms, this mechanism would break.

### Mechanism 3
- **Claim**: Popular queries cause LLMs to retrieve increasingly similar facts across distinct relations, indicating difficulty in distinguishing facts grounded in different relations.
- **Mechanism**: High popularity leads to strong memorization of entities, causing the model to retrieve facts that are relationally ambiguous. This results in higher similarity scores between facts from different relations, especially in final layers, because the model cannot effectively separate distinct relational knowledge.
- **Core assumption**: Memorization of popular entities is relationally entangled, so retrieval conflates facts from different relations when the entity is highly popular.
- **Evidence anchors**:
  - [abstract]: "LLMs struggle to disentangle facts, grounded in distinct relations, from their parametric memory when dealing with highly popular subjects."
  - [section]: "LLMs encounter challenges in effectively distinguishing knowledge across relational boundaries" and "higher similarity score among facts retrieved for highly popular queries spanning different relations."
- **Break condition**: If the model has strong relational disentanglement mechanisms or if popularity does not lead to relational entanglement in pretraining, this mechanism would break.

## Foundational Learning

- **Concept**: Attention mechanisms in transformers
  - Why needed here: The study analyzes how attention scores vary with query popularity, especially for constraint tokens.
  - Quick check question: What role does multi-head self-attention play in focusing on query-specific tokens during reasoning?

- **Concept**: Feed-forward networks as knowledge stores
  - Why needed here: The paper examines how facts are retrieved from FFN parameters and how this retrieval is affected by popularity.
  - Quick check question: How do feed-forward networks in transformers store and retrieve relational facts?

- **Concept**: Lexical variation and semantic equivalence
  - Why needed here: The study creates lexical variants of queries to test sensitivity; understanding semantic equivalence is crucial.
  - Quick check question: Why might lexical variants of the same query lead to different internal representations in LLMs?

## Architecture Onboarding

- **Component map**: Input layer -> Embedding lookup -> Multi-head self-attention -> Feed-forward network -> Output layer
- **Critical path**: Query -> Attention over constraint tokens -> Retrieval from FFN -> Output token prediction
- **Design tradeoffs**: Speed vs. robustness: High popularity speeds convergence but reduces robustness to lexical variations. Memorization vs. reasoning: Strong memorization aids quick answers but may impair relational distinction.
- **Failure signatures**: Rapid convergence with high confidence but inconsistent answers across lexical variants. Low attention scores to constraint tokens in popular queries. High similarity between facts from different relations for popular entities.
- **First 3 experiments**:
  1. Measure attention scores to constraint tokens for popular vs. unpopular queries across lexical variants.
  2. Compare fact retrieval similarity (cosine similarity of FFN projections) across lexical variants for different popularity levels.
  3. Test relational disentanglement by measuring similarity between facts from different relations for popular vs. unpopular entities.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific neural mechanisms that cause LLMs to pay less attention to constraint tokens when processing highly popular queries?
- Basis in paper: [explicit] The paper identifies that constraint tokens receive significantly lower attention scores for popular queries, but does not explain the underlying neural mechanisms.
- Why unresolved: The paper observes the phenomenon but does not provide a mechanistic explanation for why this occurs.
- What evidence would resolve it: Detailed analysis of attention head behavior, gradient flow, or circuit analysis showing how popularity affects attention mechanisms.

### Open Question 2
- Question: How does the pretraining corpus composition (e.g., diversity of sources, redundancy of information) influence the severity of information anxiety in LLMs?
- Basis in paper: [inferred] The paper suggests that multiple sources of information about entities might contribute to information anxiety, but does not investigate the relationship between corpus composition and the phenomenon.
- Why unresolved: The paper raises this as a potential factor but does not empirically test how different corpus characteristics affect information anxiety.
- What evidence would resolve it: Controlled experiments varying corpus diversity, redundancy, and source distribution to measure impact on information anxiety.

### Open Question 3
- Question: Can architectural modifications to LLMs (e.g., modified attention mechanisms, additional regularization) reduce information anxiety without compromising performance on popular queries?
- Basis in paper: [explicit] The paper identifies information anxiety as a robustness concern but does not explore potential architectural solutions.
- Why unresolved: The paper focuses on characterizing the problem rather than proposing solutions.
- What evidence would resolve it: Empirical evaluation of modified architectures on the PopQA dataset, comparing information anxiety metrics before and after modifications.

## Limitations
- Analysis relies on Wikipedia page views as a proxy for entity popularity, which may not perfectly correlate with pretraining frequency
- Study focuses specifically on Llama-2 family models, limiting generalizability to other LLM architectures
- Lexical variation generation process may not capture all meaningful semantic variations that could affect model behavior

## Confidence
- High confidence: Core finding that highly popular queries accelerate convergence but reduce attention to constraint tokens and retrieval consistency across lexical variants
- Medium confidence: Mechanism explanation linking popularity to information anxiety through reduced constraint attention and relational entanglement
- Medium confidence: Generalizability of findings to other LLM families and task types beyond Llama-2 models and factual QA

## Next Checks
1. **Cross-architecture validation**: Test whether information anxiety manifests similarly in other LLM families (e.g., GPT, BERT, Claude) using the same PopQA dataset and analysis pipeline to assess generalizability.

2. **Mechanism isolation**: Design controlled experiments varying pretraining frequency independently of popularity to distinguish between popularity-induced effects and general memorization effects on constraint attention and relational disentanglement.

3. **Robustness intervention evaluation**: Test whether targeted interventions (e.g., constraint token boosting in attention mechanisms, relation-specific fine-tuning) can mitigate information anxiety while preserving the benefits of rapid convergence for popular queries.