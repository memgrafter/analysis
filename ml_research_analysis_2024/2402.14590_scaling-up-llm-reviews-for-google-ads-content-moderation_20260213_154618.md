---
ver: rpa2
title: Scaling Up LLM Reviews for Google Ads Content Moderation
arxiv_id: '2402.14590'
source_url: https://arxiv.org/abs/2402.14590
tags:
- google
- images
- content
- prompt
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of efficiently scaling up large
  language model (LLM) reviews for Google Ads content moderation, given the prohibitive
  costs and latency of applying LLMs to large datasets. The proposed solution combines
  heuristic-based candidate selection, clustering with representative sampling, LLM
  labeling on representatives, and label propagation to cluster members.
---

# Scaling Up LLM Reviews for Google Ads Content Moderation

## Quick Facts
- arXiv ID: 2402.14590
- Source URL: https://arxiv.org/abs/2402.14590
- Reference count: 8
- Primary result: Reduced LLM reviews by >3 orders of magnitude while achieving 2x recall vs baseline non-LLM model

## Executive Summary
This paper presents a pipeline to scale up LLM-based reviews for Google Ads content moderation by reducing the number of LLM inferences through clustering, representative sampling, and label propagation. The approach successfully labels twice as many policy-violating images as a non-LLM baseline while removing over 15% of "Non-Family Safe" policy violations. The key innovation is combining heuristic-based candidate selection with cross-modal similarity representations to enable efficient and accurate moderation at scale.

## Method Summary
The proposed pipeline consists of four main stages: (1) funneling to select candidate ads through heuristics, deduplication, filtering, and diversified sampling; (2) LLM labeling using prompt engineering and soft-prompt tuning for policy-specific classification; (3) label propagation where LLM decisions on representative ads are extended to similar cluster members; and (4) a feedback loop that re-feeds labeled images back into the funnel. The system leverages cross-modal embeddings for similarity calculations, which proved more effective than uni-modal approaches for clustering and propagation accuracy.

## Key Results
- Reduced LLM reviews by more than 3 orders of magnitude while achieving 2x recall compared to a baseline non-LLM model
- Among 400 million ad images, the pipeline labeled roughly twice as many images as a multi-modal non-LLM model
- Helped remove more than 15% of policy-violating impressions for the "Non-Family Safe" ad policy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM labeling improves recall through prompt engineering and soft-prompt tuning to adapt large models for policy-specific classification
- Mechanism: Combines human-crafted prompts with small trainable prompt embeddings to align LLM output with domain-specific policy labels without full fine-tuning
- Core assumption: LLMs can generalize from carefully designed prompts and small tuning datasets to detect nuanced policy violations
- Evidence anchors: Abstract mentions prompt engineering/tuning for high-quality LLM content moderation; section 3.2 describes in-context learning and parameter-efficient tuning
- Break condition: If prompts fail to capture policy nuance or tuning dataset is too small, recall will degrade

### Mechanism 2
- Claim: Clustering and representative sampling reduce compute by limiting LLM reviews to a small, diverse subset
- Mechanism: Ads are grouped by content similarity, with only one representative per cluster reviewed by the LLM, then labels are propagated to all cluster members
- Core assumption: Ads within a cluster are similar enough that LLM decision on one is valid for all
- Evidence anchors: Abstract describes selecting one representative ad per cluster; section 3.3 explains propagating labels to similar images from stored traffic
- Break condition: Poor clustering or similarity thresholds will cause incorrect label propagation

### Mechanism 3
- Claim: Cross-modal similarity embeddings outperform uni-modal ones for clustering and label propagation
- Mechanism: Embeddings combining text and image features provide better semantic matching, improving grouping and label propagation accuracy
- Core assumption: Text descriptions and visual features together capture more policy-relevant nuance than either modality alone
- Evidence anchors: Abstract states cross-modal representations yield better results than uni-modal; results discussion implies this finding
- Break condition: If cross-modal models are unavailable or degraded, switching to uni-modal will reduce accuracy

## Foundational Learning

- Concept: Prompt engineering and in-context learning
  - Why needed here: To adapt general-purpose LLM to detect policy violations without costly fine-tuning
  - Quick check question: What is the difference between zero-shot, few-shot, and in-context learning in LLMs?

- Concept: Clustering and similarity-based sampling
  - Why needed here: To reduce number of LLM inferences while maintaining coverage of policy-violating content
  - Quick check question: How does k-means or graph-based clustering handle near-duplicate content?

- Concept: Label propagation in graph-based systems
  - Why needed here: To extend LLM decisions from representative images to all cluster members efficiently
  - Quick check question: What similarity threshold ensures accurate label propagation without false positives?

## Architecture Onboarding

- Component map: Funneling (heuristics + deduping + sampling) → LLM Labeling (prompt-tuned model) → Label Propagation (similarity graph) → Feedback Loop (re-feed labeled images to funnel)
- Critical path: Funnel → LLM → Propagate → Feedback
- Design tradeoffs: Clustering quality vs. compute cost; similarity threshold vs. false positives; prompt specificity vs. generalization
- Failure signatures: Low recall if LLM is under-prompted; high false positives if clustering is too coarse; stalled feedback loop if similarity embeddings degrade
- First 3 experiments:
  1. Test LLM recall on small labeled dataset with different prompts and soft-prompt tuning configs
  2. Vary clustering similarity thresholds and measure label propagation accuracy
  3. Compare cross-modal vs. uni-modal embeddings on sample of clustered ads

## Open Questions the Paper Calls Out

- Question: How does clustering quality and label propagation accuracy change when scaling to other ad policies beyond "Non-Family Safe"?
  - Basis in paper: Authors state expanding to more ad policies but provide no results or evaluation metrics for these extensions
  - Why unresolved: Paper only reports results for "Non-Family Safe" policy, leaving effectiveness on other policies uncertain
  - What evidence would resolve it: Empirical results showing clustering performance, label propagation accuracy, and overall recall/precision metrics across multiple ad policies

- Question: What is the impact of using different types of embeddings (text-based, multimodal, or domain-specific) on clustering quality and label propagation accuracy?
  - Basis in paper: Authors note cross-modal representations outperform uni-modal ones but don't explore other embedding types or combinations
  - Why unresolved: Paper only compares cross-modal vs. uni-modal embeddings, leaving potential benefits of other approaches unexplored
  - What evidence would resolve it: Comparative analysis of clustering quality and label propagation accuracy using different embedding types or combinations

- Question: How does the proposed pipeline handle adversarial cases where ads are deliberately designed to evade content moderation systems?
  - Basis in paper: Paper doesn't address adversarial attacks or evasion techniques, which are common challenges in content moderation
  - Why unresolved: Paper focuses on scaling LLM reviews but doesn't discuss robustness against adversarial examples or evasion strategies
  - What evidence would resolve it: Experiments testing pipeline's performance on adversarially crafted ads or simulations of evasion techniques

## Limitations

- The exact methodology for recall calculation and baseline comparison is not specified, introducing uncertainty about the robustness of the 2x improvement claim
- The paper doesn't specify exact similarity thresholds or clustering algorithms used, making it unclear how robust the method is to noisy or heterogeneous ad content
- No comparative results or ablation studies are provided for cross-modal embeddings, making it difficult to assess whether improvement is due to cross-modal approach or other factors

## Confidence

- **High confidence**: Overall pipeline design (funneling → LLM → propagation → feedback) is clearly specified and logically sound for reducing compute while maintaining recall
- **Medium confidence**: Claim of >3 orders of magnitude reduction in LLM reviews is plausible given clustering and sampling strategy, but exact implementation details are not provided
- **Low confidence**: Specific recall improvement (2x vs baseline) and exact contribution of cross-modal embeddings are not fully supported by details in the abstract

## Next Checks

1. Validate recall improvement: Replicate recall calculation on small labeled dataset with different prompts and soft-prompt tuning configurations to confirm 2x improvement claim
2. Test clustering robustness: Vary clustering similarity thresholds and measure label propagation accuracy to determine optimal threshold and assess sensitivity to noisy content
3. Compare embedding modalities: Conduct ablation study comparing cross-modal vs. uni-modal embeddings on sample of clustered ads to quantify contribution of cross-modal features