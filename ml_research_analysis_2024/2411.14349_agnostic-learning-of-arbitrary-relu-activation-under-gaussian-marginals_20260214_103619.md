---
ver: rpa2
title: Agnostic Learning of Arbitrary ReLU Activation under Gaussian Marginals
arxiv_id: '2411.14349'
source_url: https://arxiv.org/abs/2411.14349
tags:
- relu
- learning
- have
- vupdate
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the fundamental question of whether a single
  ReLU neuron can be learned in the non-realizable (agnostic) setting. Despite being
  a basic building block of neural networks, the learnability of arbitrary ReLU activations
  under Gaussian marginals with squared loss has remained open.
---

# Agnostic Learning of Arbitrary ReLU Activation under Gaussian Marginals

## Quick Facts
- **arXiv ID**: 2411.14349
- **Source URL**: https://arxiv.org/abs/2411.14349
- **Reference count**: 40
- **Primary result**: Polynomial-time SQ algorithm achieving O(OPT) + ε loss for learning single ReLU neuron with arbitrary bias

## Executive Summary
This paper resolves the fundamental question of whether a single ReLU neuron can be learned in the agnostic setting under Gaussian marginals. Prior work was limited to restricted bias settings or could not achieve constant-factor approximations. The authors present the first polynomial-time algorithm that achieves O(OPT) + ε loss for arbitrary ReLU activation with arbitrary bias, marking a significant advance in our understanding of neural network learnability. Their approach combines thresholded PCA for initialization with a novel reweighted projected gradient descent, departing from standard gradient-based methods that fail in the high-bias regime.

## Method Summary
The algorithm uses a two-stage approach: thresholded PCA with threshold τ = 1/|b| provides a coarse initialization w₀, followed by reweighted projected gradient descent that shifts the Gaussian measure along the current weight direction by ρ|b| and conditions updates on specific thresholds. The algorithm combines these components with grid search over bias and norm estimates to achieve the O(OPT) + ε guarantee for sufficiently negative bias values.

## Key Results
- First polynomial-time algorithm achieving O(OPT) + ε loss for single ReLU neuron with arbitrary bias
- Establishes separation between SQ and CSQ algorithms - no polynomial-time CSQ algorithm can achieve constant-factor approximation
- Identifies the simplest setting (single neuron) where gradient descent fundamentally fails

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reweighted PGD with shifted Gaussian measure overcomes gradient descent failure when ReLU bias is very negative
- Mechanism: Shifts Gaussian mean along current weight direction by ρ|b| and conditions updates on xw ≥ (ρ + λ - λ²ρ)|b|, suppressing noise contribution while preserving ReLU signal
- Core assumption: When current weight wₜ aligns with true vector v (⟨wₜ, v⟩ ≥ λ), reweighted measure reduces problem to moderate-bias regime
- Evidence anchors: Abstract shows departure from gradient-based methods; section 3.1 details reweighting technique
- Break condition: If initialization w₀ is too far from v (⟨w₀, v⟩ < 0.9), reweighting cannot dominate noise

### Mechanism 2
- Claim: Thresholded PCA with τ = 1/|b| provides coarse initialization sufficiently close to v
- Mechanism: Thresholding on |y| ≥ 1/|b| removes most noise while preserving optimal ReLU contribution; top eigenvector approximates v
- Core assumption: When b is sufficiently negative, adversary can only remove small fraction of ReLU contribution below threshold
- Evidence anchors: Section 3.2 describes thresholded PCA algorithm; Lemma 3.11 proves top eigenvector has substantial correlation with v
- Break condition: If b is not sufficiently negative (b > -√(α/log α)), thresholded PCA cannot provide required accuracy

### Mechanism 3
- Claim: CSQ-CSQ separation establishes fundamental limitations of gradient-based methods
- Mechanism: Lower bound shows no CSQ algorithm can achieve O(OPT) error, requiring exponential queries or superpolynomial tolerance
- Core assumption: Learning single ReLU neuron with arbitrary bias is computationally hard for CSQ algorithms due to correlation structure
- Evidence anchors: Abstract states CSQ lower bound; section 4 constructs functions with small pairwise correlation
- Break condition: If noise structure changes, separation might not hold

## Foundational Learning

- **Concept**: Hermite polynomial expansion of ReLU functions
  - Why needed here: CSQ lower bound construction analyzes Hermite expansion to show removing low-order components preserves L² norm
  - Quick check question: What is the value of ⟨σ(x - b), H₁⟩ₙ for b < 0, and how does it scale as b → -∞?

- **Concept**: Mills ratio and Gaussian tail bounds
  - Why needed here: Used to bound integrals involving Gaussian tails in both algorithmic analysis and lower bound construction
  - Quick check question: What is asymptotic expansion of m(t) = (1 - Φ(t))/ϕ(t) as t → ∞, and what is error bound for truncating after k terms?

- **Concept**: Correlation statistical query (CSQ) dimension
  - Why needed here: Lower bound construction uses CSQ dimension to show ReLU learning is hard for CSQ algorithms
  - Quick check question: Given function class G with CSDD(G, γ, β) = D, what relationship between D, γ, and β determines learning hardness?

## Architecture Onboarding

- **Component map**: Grid search → Thresholded PCA → Reweighted PGD → Best result
- **Critical path**: 1) Grid search over (∥v∥, b) space, 2) Run thresholded PCA for each point to get w₀, 3) Run reweighted PGD from w₀, 4) Return best result
- **Design tradeoffs**: Grid search accuracy vs. runtime (0.1/√ε requires O(1/ε) points); threshold τ = 1/|b| balances noise removal vs. signal preservation; ρ balances Gaussian shift amount vs. noise suppression
- **Failure signatures**: Reweighted PGD doesn't converge → w₀ too far from v; thresholded PCA fails → b not sufficiently negative; overall failure → grid search missed correct region
- **First 3 experiments**: 1) Test thresholded PCA with known b < 0 on synthetic data where v is known; 2) Test reweighted PGD with initialization w₀ close to v; 3) Test end-to-end algorithm on synthetic data with varying b

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the precise characterization of optimal bias parameter b where gradient descent fails in ReLU regression?
  - Basis: Paper demonstrates gradient descent fails as b → -∞ but doesn't characterize exact threshold
  - Why unresolved: Analysis shows failure in b → -∞ regime without precise boundary
  - Evidence needed: Quantitative analysis showing relationship between bias magnitude and correlation dominance

- **Open Question 2**: Can reweighted PGD algorithm be extended to learn neural networks with multiple ReLU neurons?
  - Basis: Algorithm successfully learns single ReLU neuron by overcoming gradient descent limitations
  - Why unresolved: Paper focuses on single neuron case without exploring deeper networks
  - Evidence needed: Experimental results or theoretical analysis on multi-layer networks

- **Open Question 3**: What is the relationship between CSQ dimension and SQ dimension for learning ReLU activations?
  - Basis: Paper establishes separation between CSQ and SQ algorithms
  - Why unresolved: Shows separation exists but doesn't characterize relationship between dimensions
  - Evidence needed: Tight bounds on both CSQ and SQ dimensions for ReLU learning

- **Open Question 4**: How does thresholded PCA initialization compare to other initialization methods?
  - Basis: Paper uses thresholded PCA but doesn't compare to alternatives
  - Why unresolved: Presents thresholded PCA as effective without exploring alternatives
  - Evidence needed: Comparative analysis of different initialization methods on same problem

- **Open Question 5**: What is the minimum sample complexity required for agnostic learning of single ReLU neuron?
  - Basis: Paper provides polynomial sample complexity without proving optimality
  - Why unresolved: Achieves polynomial complexity but doesn't prove this is tight
  - Evidence needed: Lower bound proofs showing minimum samples required or algorithm with better complexity

## Limitations

- Algorithm performance critically depends on having sufficiently negative bias values (b < -√(α/log α))
- Grid search over (∥v∥, b) parameters introduces significant computational overhead
- Theoretical constants (α, λ = 0.9, ρ ∈ (0.3, 0.6)) may require empirical tuning for practical performance

## Confidence

**High confidence**: SQ algorithm achieving O(OPT) + ε loss guarantee for sufficiently negative b; CSQ lower bound establishing computational hardness; fundamental separation between SQ and CSQ algorithms.

**Medium confidence**: Specific parameter values (λ = 0.9, ρ = 0.5) for reweighted PGD; threshold value τ = 1/|b| for thresholded PCA; grid search accuracy requirements (0.1/√ε).

**Low confidence**: Algorithm behavior when b is not sufficiently negative; practical sample complexity for theoretical guarantees; sensitivity to choice of ρ within specified range.

## Next Checks

1. **Initialization quality verification**: Generate synthetic data with known v and varying b (including highly negative values), then test whether thresholded PCA consistently produces w₀ with ⟨w₀, v⟩ ≥ 0.9.

2. **Reweighted PGD convergence analysis**: Starting from known good initialization w₀, test reweighted PGD on synthetic data with varying noise levels. Verify convergence to correct direction v and that conditioning/reweighting suppress noise correlation.

3. **Grid search sensitivity**: Evaluate algorithm performance across different grid search granularities and compare resulting loss guarantees to validate theoretical grid search accuracy requirements.