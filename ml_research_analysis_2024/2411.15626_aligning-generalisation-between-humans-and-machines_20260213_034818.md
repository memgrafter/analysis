---
ver: rpa2
title: Aligning Generalisation Between Humans and Machines
arxiv_id: '2411.15626'
source_url: https://arxiv.org/abs/2411.15626
tags:
- generalisation
- learning
- methods
- data
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies the misalignment between human and machine\
  \ generalization as a critical challenge for effective human-AI teaming. The authors\
  \ analyze three dimensions\u2014notions, methods, and evaluation of generalization\u2014\
  revealing that humans excel at compositionality, abstraction from few examples,\
  \ and robustness, while statistical AI methods are strong in inference efficiency\
  \ and handling large-scale data but struggle with out-of-distribution generalization\
  \ and explainability."
---

# Aligning Generalisation Between Humans and Machines

## Quick Facts
- arXiv ID: 2411.15626
- Source URL: https://arxiv.org/abs/2411.15626
- Reference count: 40
- Key outcome: Identifies misalignment between human and machine generalization as critical challenge for effective human-AI teaming

## Executive Summary
This paper analyzes the fundamental misalignment between human and machine generalization capabilities, revealing that humans excel at compositionality, abstraction from few examples, and robustness while statistical AI methods are strong in inference efficiency and handling large-scale data but struggle with out-of-distribution generalization and explainability. The authors propose a three-dimensional framework examining notions, methods, and evaluation of generalization to systematically identify gaps and complementarity between human and machine approaches. This analysis drives recommendations for interdisciplinary research directions including neurosymbolic methods, foundation model theories, and improved evaluation frameworks to enable effective human-AI teaming.

## Method Summary
This theoretical perspective paper combines insights from AI and cognitive science to analyze misalignment between human and machine generalization through a three-dimensional framework: notions of generalization (processes, products, operators), methods for generalization (statistical, knowledge-informed, instance-based), and evaluation of generalization (measuring distributional shifts, determining under/overgeneralization, distinguishing memorisation from generalization). The paper synthesizes existing research to characterize strengths and weaknesses of different approaches and identifies key research challenges for achieving effective human-AI teaming through aligned generalization capabilities.

## Key Results
- Humans excel at compositionality, abstraction from few examples, and robustness while machines struggle with out-of-distribution generalization and explainability
- Statistical ML methods achieve high performance through universal approximation but produce black-box models, while analytical methods offer compositionality and explainability but are restricted to simpler scenarios
- Effective human-AI teaming requires not just aligned outputs but also aligned processes, necessitating research into neurosymbolic methods and explicit context modeling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The paper provides a structured three-dimensional framework that systematically identifies misalignment between human and machine generalization, enabling targeted research interventions.
- Mechanism: By categorizing generalization into distinct dimensions, the framework reveals that humans and machines use fundamentally different processes (abstraction vs. data-driven learning), products (categories/rules vs. probability distributions), and evaluation methods. This systematic mapping exposes gaps and complementarity that guide interdisciplinary research.
- Core assumption: The three-dimensional framework captures the essential aspects of generalization across both domains, and misalignment identified at this abstract level translates to actionable research directions.
- Evidence anchors:
  - [abstract] "We analyse three dimensions from the perspective of AI alignment: notions of, methods for, and evaluation of generalisation."
  - [section] "We analyse three dimensions from the perspective of AI alignment: notions of, methods for, and evaluation of generalisation."
  - [corpus] Weak - no corpus papers directly discuss this three-dimensional framework structure.
- Break condition: If the framework fails to capture critical aspects of generalization or if the identified misalignments prove to be superficial rather than fundamental.

### Mechanism 2
- Claim: The paper demonstrates that statistical ML methods excel at inference correctness and efficiency for large-scale data but fail at out-of-distribution generalization, while analytical methods support compositionality and explainable predictions but lack universality.
- Mechanism: By comparing the properties of different AI method families (statistical, knowledge-informed, instance-based), the paper reveals a fundamental tradeoff between generalizability and interpretability/compositionality. Statistical methods achieve high performance through universal approximation but produce black-box models, while analytical methods offer compositionality and explainability but are restricted to simpler scenarios.
- Core assumption: The identified strengths and weaknesses of each method family are accurate and persist across different applications and domains.
- Evidence anchors:
  - [abstract] "humans excel at compositionality, abstraction from few examples, and robustness, while statistical AI methods are strong in inference efficiency and handling large-scale data but struggle with out-of-distribution generalization and explainability."
  - [section] "Statistical methods enable universality of approximation and inference correctness, instance-based methods enable robustness and memorisation, while analytical techniques are designed for compositionality and explainable predictions."
  - [corpus] Weak - corpus papers don't provide direct evidence for this specific characterization of method families.
- Break condition: If new method families emerge that combine the strengths of existing approaches without the associated weaknesses, or if empirical evidence contradicts the claimed tradeoffs.

### Mechanism 3
- Claim: The paper establishes that effective human-AI teaming requires not just aligned outputs but also aligned processes, necessitating research into neurosymbolic methods and explicit context modeling.
- Mechanism: By distinguishing between output-level alignment (aligned results) and process-level alignment (shared reasoning), the paper identifies that misalignment in generalization processes (abstraction vs. data-driven learning) leads to failures in collaboration. This insight drives the proposed research directions toward neurosymbolic AI and context-aware methods.
- Core assumption: Process-level misalignment is a primary cause of human-AI teaming failures, and addressing this through neurosymbolic methods and context modeling will improve collaboration outcomes.
- Evidence anchors:
  - [abstract] "Recent results show that, to date, the performance of human-AI teams lags behind that of the best AI model or the best human alone in many domains."
  - [section] "The achievement of effective human-AI teaming... requires transparent collaboration workflows, with explanations bridging the gaps between human and AI reasoning."
  - [corpus] Weak - corpus papers don't provide evidence for the importance of process-level alignment in human-AI teaming.
- Break condition: If human-AI teaming failures are found to be primarily caused by factors other than process-level misalignment, or if neurosymbolic methods fail to deliver the promised improvements in alignment.

## Foundational Learning

- Concept: Independent and Identically Distributed (IID) assumption
  - Why needed here: The paper discusses how statistical ML methods assume IID data, which fails for out-of-distribution generalization. Understanding this assumption is crucial for grasping the fundamental limitations of statistical approaches.
  - Quick check question: What does it mean when data are described as "Independent and Identically Distributed" (IID), and why is this assumption problematic for real-world generalization?

- Concept: Compositionality in generalization
  - Why needed here: The paper emphasizes that humans excel at compositionality (combining known components to create novel ones) while most statistical ML methods struggle with this capability. This concept is central to understanding the difference between human and machine generalization.
  - Quick check question: How does compositionality differ from simple pattern recognition, and why is it particularly important for achieving human-like generalization in AI systems?

- Concept: Few-shot learning
  - Why needed here: The paper contrasts human ability to generalize from few examples with machine requirements for large datasets. Understanding few-shot learning is essential for grasping the gap in sample efficiency between humans and machines.
  - Quick check question: What makes few-shot learning challenging for traditional machine learning approaches, and how do humans achieve this capability through prior knowledge and common sense?

## Architecture Onboarding

- Component map: The paper's framework consists of three main components: (1) Notions of generalization - covering processes (abstraction, extension, analogy), products (categories, rules, models), and operators (application to new data); (2) Methods for generalization - statistical (data-driven), knowledge-informed (theory-based), and instance-based (local inference) approaches; (3) Evaluation of generalization - measuring distributional shifts, determining under/overgeneralization, and distinguishing memorisation from generalization.

- Critical path: To apply this framework to a new human-AI teaming problem, first identify the relevant generalization notions in both human and machine domains, then map the current methods being used to the appropriate categories, and finally determine the most appropriate evaluation metrics that capture the alignment requirements for that specific application context.

- Design tradeoffs: The framework reveals a fundamental tradeoff between the universality of statistical methods and the interpretability/compositionality of analytical methods. For human-AI teaming applications, this translates to choosing between high-performance but opaque models versus transparent but limited models, depending on the criticality of explainability and the need for compositional reasoning.

- Failure signatures: The framework predicts failures when there is a mismatch between human and machine generalization processes (e.g., humans using causal reasoning while machines use correlation-based learning), products (e.g., humans expecting symbolic rules while machines provide probability distributions), or evaluation methods (e.g., humans using few examples while machines require large datasets).

- First 3 experiments:
  1. Compare human and machine performance on a compositionality task (e.g., visual reasoning with novel combinations) using both traditional statistical ML and neurosymbolic approaches to quantify the gap and test whether neurosymbolic methods reduce it.
  2. Conduct a user study where humans interact with both black-box statistical models and explainable analytical models on the same task to measure trust, understanding, and teaming effectiveness differences.
  3. Create a benchmark that systematically varies the degree of distributional shift between training and test data to evaluate how different method families (statistical, analytical, instance-based) handle out-of-distribution generalization compared to human performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific theoretical framework is needed to explain how foundation models achieve generalization across tasks that were not present in their training data?
- Basis in paper: [explicit] The paper states that the assumption that LLMs have implicitly generalized to new tasks remains unsubstantiated and calls for new generalization processes and products to provide guarantees for zero-shot applications.
- Why unresolved: Current learning theory doesn't adequately explain the generalization capabilities of large language models, especially their ability to perform zero-shot and few-shot learning on unseen tasks.
- What evidence would resolve it: A formal mathematical framework that defines the conditions under which foundation models can successfully generalize to novel tasks, potentially through encoding invariances or cognitively inspired representations.

### Open Question 2
- Question: How can neurosymbolic systems be designed to compose generalizations themselves, rather than just composing symbolic representations?
- Basis in paper: [explicit] The paper notes that while a theory about the compositionality of neurosymbolic systems is emerging, a theory on how to compose the generalizations themselves is lacking.
- Why unresolved: Current neurosymbolic approaches focus on combining symbolic and neural components but don't address how the generalizations produced by these systems can be composed to handle increasingly complex tasks.
- What evidence would resolve it: A formal framework that defines how generalizations learned by neurosymbolic systems can be combined and composed to create more powerful and flexible reasoning capabilities.

### Open Question 3
- Question: What evaluation methods can effectively measure the alignment between human and machine generalization capabilities in real-world human-AI teaming scenarios?
- Basis in paper: [explicit] The paper identifies that current evaluation practices focus on task-specific benchmarks but lack frameworks for evaluating generalization in collaborative contexts where humans and AI must work together.
- Why unresolved: Existing benchmarks measure individual model performance rather than how well AI systems can align their generalization processes with human reasoning in collaborative settings.
- What evidence would resolve it: Development and validation of evaluation frameworks that assess both objective task outcomes and subjective process-related experiences in human-AI teams, including metrics for measuring successful alignment of generalization strategies.

## Limitations

- The three-dimensional framework provides useful conceptual structure but lacks quantitative validation of the claimed misalignments between human and machine generalization processes
- The characterization of AI method families and their limitations is based on theoretical analysis rather than systematic empirical testing across diverse domains
- The proposed research directions toward neurosymbolic methods and context-aware generalization face significant technical challenges that are not fully addressed

## Confidence

Medium - The claims align with established cognitive science findings about human learning capabilities, but the specific characterization of AI method families and their limitations requires more systematic empirical testing across diverse domains.

## Next Checks

1. Empirical comparison of human and machine performance on systematic compositionality tasks using both statistical and neurosymbolic approaches to quantify the actual gap
2. Controlled human-AI teaming experiments that isolate process-level alignment effects on collaboration outcomes, comparing black-box vs. explainable models on identical tasks
3. Development of a benchmark suite that varies distributional shift characteristics to empirically test the claimed tradeoffs between statistical, analytical, and instance-based generalization methods