---
ver: rpa2
title: Efficient Compression of Multitask Multilingual Speech Models
arxiv_id: '2405.00966'
source_url: https://arxiv.org/abs/2405.00966
tags:
- languages
- speech
- whisper
- distilwhisper
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work examines bias in the Whisper speech model, demonstrating
  that model-related biases (resourcefulness and model size) are amplified by quantization,
  particularly impacting low-resource languages and smaller models. To address this,
  we propose DistilWhisper, a parameter-efficient distillation approach that enhances
  whisper-small performance by leveraging language-specific experts and knowledge
  distillation from whisper-large-v2.
---

# Efficient Compression of Multitask Multilingual Speech Models

## Quick Facts
- arXiv ID: 2405.00966
- Source URL: https://arxiv.org/abs/2405.00966
- Reference count: 3
- This work examines bias in the Whisper speech model, demonstrating that model-related biases (resourcefulness and model size) are amplified by quantization, particularly impacting low-resource languages and smaller models. To address this, we propose DistilWhisper, a parameter-efficient distillation approach that enhances whisper-small performance by leveraging language-specific experts and knowledge distillation from whisper-large-v2. DistilWhisper bridges the ASR performance gap for under-represented languages while retaining multilingual and multitask capabilities. Results show consistent improvements across languages and test sets with minimal parameter overhead during inference.

## Executive Summary
This paper addresses bias amplification in multilingual speech models, particularly how quantization disproportionately harms low-resource languages and smaller models. The authors introduce DistilWhisper, a parameter-efficient approach combining Conditional Language-Specific Routing (CLSR) modules with knowledge distillation from a larger teacher model. The method achieves significant WER improvements for under-represented languages while maintaining multilingual and multitask capabilities, with minimal parameter overhead during inference.

## Method Summary
DistilWhisper enhances whisper-small performance through two key strategies: lightweight modular ASR fine-tuning using language-specific experts, and knowledge distillation from whisper-large-v2. The approach uses CLSR modules that replace feed-forward layers with token-level routing to language-specific experts when beneficial. A gate budget loss balances shared and language-specific knowledge utilization, while Jensen-Shannon divergence-based knowledge distillation transfers robustness from the larger teacher model.

## Key Results
- DistilWhisper outperforms standard fine-tuning and LoRA adapters across multiple languages
- Achieves notable WER improvements for low-resource languages while maintaining multilingual capabilities
- Introduces only 25 million additional parameters (10% of original model size) during inference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model-related bias (resourcefulness and model size) is amplified by quantization, impacting low-resource languages and smaller models more severely.
- Mechanism: Quantization reduces numerical precision, causing smaller models and lower-resource languages to lose more critical representational capacity due to their inherently lower capacity and less diverse training data.
- Core assumption: Lower-resource languages have less training data and thus less robust internal representations; smaller models have less representational capacity overall.
- Evidence anchors:
  - [abstract] "model-related biases (resourcefulness and model size) are amplified by quantization, particularly impacting low-resource languages and smaller models."
  - [section 3.3] "low-resource languages are more significantly affected by quantization... the bias related to architecture size is significantly amplified by quantization."
  - [corpus] Weak - corpus lacks direct quantization impact studies.
- Break condition: If quantization introduces no bias amplification, or if larger models or high-resource languages suffer more than predicted.

### Mechanism 2
- Claim: DistilWhisper bridges the ASR performance gap for under-represented languages while retaining multilingual and multitask capabilities.
- Mechanism: Conditional Language-Specific Routing (CLSR) layers route tokens to language-specific experts only when beneficial, preserving shared multilingual knowledge and reducing interference; knowledge distillation from whisper-large-v2 transfers robustness without needing full training data.
- Core assumption: Language-specific routing can improve performance without sacrificing multilingual capability; KD can effectively transfer robustness to smaller models.
- Evidence anchors:
  - [abstract] "DistilWhisper... enhances whisper-small performance by leveraging language-specific experts and knowledge distillation from whisper-large-v2."
  - [section 4.2] "Our approach involves two key strategies: lightweight modular ASR fine-tuning of whisper-small using language-specific experts, and knowledge distillation from whisper-large-v2."
  - [corpus] Weak - corpus lacks direct KD+CLSR fusion studies.
- Break condition: If language-specific routing introduces significant interference, or if KD fails to transfer robustness effectively.

### Mechanism 3
- Claim: DistilWhisper outperforms standard fine-tuning and LoRA adapters, boosting performance in targeted languages for both in- and out-of-domain test sets, while introducing only negligible parameter overhead at inference.
- Mechanism: CLSR modules replace feed-forward layers entirely, providing token-level routing with minimal parameter increase; KD loss prevents overfitting to in-domain data and maintains robustness.
- Core assumption: Replacing FFN with CLSR is more parameter-efficient than adapters; KD with JS divergence prevents mode-averaging and preserves teacher robustness.
- Evidence anchors:
  - [abstract] "Results show consistent improvements across languages and test sets with minimal parameter overhead during inference."
  - [section 5.2] "Our approach, DistilWhisper, yields notable enhancements in performance... with only a mere 25 million parameter overhead during inference (10 % of the original model size)."
  - [corpus] Weak - corpus lacks direct comparison of CLSR vs LoRA+KD.
- Break condition: If CLSR parameter overhead exceeds stated bounds, or if KD fails to improve out-of-domain generalization.

## Foundational Learning

- Concept: Bias analysis in machine learning models
  - Why needed here: Understanding how quantization amplifies model-related bias is central to motivating DistilWhisper.
  - Quick check question: What is the difference between speaker-related bias and model-related bias in Whisper?

- Concept: Knowledge distillation and its variants
  - Why needed here: DistilWhisper relies on KD with JS divergence to transfer robustness from whisper-large-v2.
  - Quick check question: How does Jensen-Shannon divergence differ from Kullback-Leibler divergence in sequence-level distillation?

- Concept: Parameter-efficient adaptation methods
  - Why needed here: Comparing DistilWhisper to LoRA adapters and standard fine-tuning establishes its relative effectiveness.
  - Quick check question: What is the key architectural difference between CLSR and LoRA adapters?

## Architecture Onboarding

- Component map:
  - Whisper-small backbone (pre-trained multilingual multitask model)
  - Conditional Language-Specific Routing (CLSR) layers replacing FFN layers
  - Language-specific experts per language
  - Gates per token deciding routing to shared vs. language-specific path
  - Knowledge distillation loss from whisper-large-v2
  - Gate budget loss for balancing shared vs. LS usage
  - Skip-gate probability for regularization

- Critical path:
  1. Token passes through encoder layers
  2. At each FFN position, CLSR module decides routing
  3. Gates activate either shared path or language-specific path
  4. Forward pass continues to decoder for multitask prediction
  5. Losses computed: cross-entropy, gate budget, KD
  6. Only language-specific experts and gates for current language are updated

- Design tradeoffs:
  - CLSR vs. LoRA: CLSR allows token-level routing, potentially more expressive but more complex; LoRA is simpler but fixed per adapter.
  - Temperature in KD: Lower temperature focuses on top predictions, higher smooths distribution; 1 chosen here for whisper-large-v2 quality.
  - Gate budget: Balances shared vs. LS usage; too low may underfit, too high may overfit to specific languages.

- Failure signatures:
  - If LS routing rarely activates, CLSR is ineffective.
  - If gate budget loss dominates, model may ignore shared knowledge.
  - If KD loss is too high, model may overfit to teacher distribution.

- First 3 experiments:
  1. Ablation: Train CLSR-FT without KD to isolate routing impact.
  2. Scalability: Vary training data size (3k, 10k, 28k) to test robustness.
  3. Temperature sweep: Compare JS loss with τ=1 vs τ=3 for KD effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DistilWhisper vary across different language families, and are there specific language families that benefit more from this approach?
- Basis in paper: [explicit] The paper discusses language sub-families (Slavic, Romance, Finno-Ugrian, Austroasiatic, Dravidian, Tai, Indo-Iranian) and their representation in the Whisper training data.
- Why unresolved: The paper provides results for individual languages but does not specifically analyze performance variations across language families.
- What evidence would resolve it: A detailed analysis of DistilWhisper's performance across different language families, comparing the average improvement in WER for each family.

### Open Question 2
- Question: What is the impact of using a different knowledge distillation method, such as KL divergence, on the performance of DistilWhisper?
- Basis in paper: [explicit] The paper mentions that the choice of distillation optimization (JS vs KL divergence) could affect results, and that further research is needed to determine the optimal approach.
- Why unresolved: The paper only briefly mentions the comparison of JS and KL divergence, leaving the question of their relative performance unanswered.
- What evidence would resolve it: A comprehensive comparison of DistilWhisper's performance using different knowledge distillation methods, including JS and KL divergence, across various settings.

### Open Question 3
- Question: How does the performance of DistilWhisper change when using a larger or smaller gate budget (b) in the CLSR module?
- Basis in paper: [explicit] The paper mentions the gate budget (b) as a hyperparameter in the CLSR module, but does not explore its impact on performance.
- Why unresolved: The paper does not provide any analysis of how different gate budget values affect the model's performance or the utilization of language-specific modules.
- What evidence would resolve it: An experiment varying the gate budget (b) in the CLSR module and measuring its effect on WER performance and the ratio of language-specific module activation.

## Limitations

- The quantization bias amplification mechanism remains largely theoretical with limited empirical validation
- CLSR mechanism's scalability to more languages is uncertain, evaluated only on 9 languages
- Generalizability of findings to other multilingual speech models beyond Whisper is unproven

## Confidence

**High Confidence**: The core empirical results showing WER improvements for low-resource languages with DistilWhisper, and the basic observation that quantization amplifies bias against smaller models and low-resource languages.

**Medium Confidence**: The proposed mechanism that quantization specifically amplifies model-related bias through numerical precision loss, and that CLSR+KD combination provides robust improvements across in- and out-of-domain data.

**Low Confidence**: The generalizability of findings to other multilingual speech models beyond Whisper, and the long-term stability of CLSR routing patterns during extended deployment.

## Next Checks

1. **Quantization Isolation Test**: Train identical models with and without quantization on the same low-resource language data, controlling for all other variables, to definitively establish quantization's specific contribution to performance degradation.

2. **Language Scalability Experiment**: Extend DistilWhisper to 20+ languages and measure routing efficiency, parameter overhead growth, and performance trade-offs to validate architectural scalability claims.

3. **Cross-Model Generalization**: Apply the same bias quantification methodology and DistilWhisper approach to a non-Whisper multilingual model (e.g., XLS-R or mSLAM) to test whether the findings and solutions transfer beyond the original model family.