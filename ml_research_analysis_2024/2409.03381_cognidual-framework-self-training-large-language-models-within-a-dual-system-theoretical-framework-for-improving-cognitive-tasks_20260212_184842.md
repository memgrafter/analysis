---
ver: rpa2
title: 'CogniDual Framework: Self-Training Large Language Models within a Dual-System
  Theoretical Framework for Improving Cognitive Tasks'
arxiv_id: '2409.03381'
source_url: https://arxiv.org/abs/2409.03381
tags:
- llms
- reasoning
- system
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The CogniDual Framework demonstrates that large language models
  can internalize complex reasoning processes into intuitive responses through self-training,
  mimicking human dual-system cognitive processes. The framework employs a three-phase
  approach where models first respond without Chain of Thought (CoT), then with CoT
  guidance, and finally self-train using the correct responses to improve their ability
  to answer accurately without CoT.
---

# CogniDual Framework: Self-Training Large Language Models within a Dual-System Theoretical Framework for Improving Cognitive Tasks

## Quick Facts
- arXiv ID: 2409.03381
- Source URL: https://arxiv.org/abs/2409.03381
- Reference count: 40
- Self-training framework achieves up to 76.2% accuracy improvement on LogiQA2.0 without CoT guidance

## Executive Summary
The CogniDual Framework introduces a novel approach to improving large language model performance on reasoning tasks by simulating human dual-system cognitive processes. The framework enables models to internalize complex reasoning processes into intuitive responses through a three-phase self-training method. By first generating responses without Chain of Thought (CoT), then with CoT guidance, and finally self-training using correct responses, the framework achieves significant accuracy improvements while reducing computational demands. Experiments demonstrate that larger models show better generalization with fewer training examples, and the approach eliminates the need for additional training data or CoT prompts in many scenarios.

## Method Summary
The CogniDual Framework employs a three-phase self-training process to improve LLM reasoning capabilities without requiring CoT guidance. Models first generate responses without CoT (System 1), then with CoT (System 2), and finally undergo self-training using correct/incorrect answer pairs. The framework uses semantic matching to identify accurate responses and answer rewriting to distill complex reasoning into concise forms. Knowledge distillation is applied to smaller models before self-training to enable essential skills like synonymy judgment. LoRA fine-tuning is used for efficient model adaptation. The approach is tested across multiple datasets (GSM8K, ReClor, LogiQA2.0) and model sizes (Llama2-7B/13B, Vicuna-7B/13B/30B).

## Key Results
- Self-trained models achieve accuracy improvements of up to 76.2% on LogiQA2.0 without CoT guidance
- Accuracy improvements of 49% on ReClor dataset achieved through self-training
- Larger models demonstrate better generalization with fewer training examples
- Computational demands reduced by enabling faster responses without CoT prompts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can internalize System 2's complex reasoning into System 1's intuitive responses through self-training
- Mechanism: Three-phase iterative process (direct answers → CoT-guided → self-training) mimics human skill acquisition where deliberate practice leads to automaticity
- Core assumption: LLMs can generalize from structured CoT patterns to develop implicit reasoning capabilities
- Evidence anchors: [abstract] "Our findings reveal the cognitive mechanisms behind LLMs' response generation"; [section] "We hypothesize that LLMs, by mimicking human rapid skill acquisition, can generate fast, intuitive answers"
- Break condition: If semantic matching fails to capture correct reasoning essence during distillation phase

### Mechanism 2
- Claim: Self-training improves reasoning accuracy without CoT guidance
- Mechanism: Exposing models to correct CoT answers and incorrect non-CoT answers, then training them to reproduce correct responses without reasoning steps
- Core assumption: Models can extract logical structure from detailed CoT responses and apply it implicitly
- Evidence anchors: [abstract] "Experiments on Llama2 and Vicuna models across GSM8K, ReClor, and LogiQA2.0 datasets show that self-trained models achieve accuracy improvements"; [section] "Following a period of self-training, LLMs exhibited a substantial increase in response precision"
- Break condition: If models overfit to specific patterns without generalizing to new reasoning tasks

### Mechanism 3
- Claim: Larger models show better generalization with fewer training examples
- Mechanism: Larger models leverage richer pre-training representations to extract patterns more efficiently from limited data
- Core assumption: Model size correlates with sample efficiency due to better pattern extraction capabilities
- Evidence anchors: [abstract] "Larger models show better generalization with fewer training examples"; [section] "Larger models require fewer examples to approach their System 1 capacity ceiling"
- Break condition: If relationship between model size and sample efficiency breaks down due to architectural limitations

## Foundational Learning

- Concept: Dual-system cognitive theory (Kahneman's System 1 and System 2)
  - Why needed here: Framework explicitly models LLM reasoning as analogous to human dual-system cognition
  - Quick check question: Can you explain the difference between System 1 (fast, intuitive) and System 2 (slow, deliberative) cognition in Kahneman's framework?

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: Framework relies on comparing LLM performance with and without CoT to measure self-training effectiveness
  - Quick check question: How does CoT prompting typically improve LLM performance on reasoning tasks, and why might this create computational overhead?

- Concept: Knowledge distillation
  - Why needed here: Framework uses distillation to equip smaller models with semantic judgment and answer rewriting capabilities before self-training
  - Quick check question: What is the purpose of knowledge distillation in model training, and how does it differ from standard fine-tuning?

## Architecture Onboarding

- Component map: Initial assessment without CoT → CoT-guided reasoning → Self-training with correct/incorrect pairs → Performance evaluation. Supporting components include semantic matching for accuracy assessment and answer rewriting for distillation.

- Critical path: collect question-answer pairs → generate CoT responses → identify correct/incorrect pairs → rewrite answers into concise form → train on rewritten pairs → evaluate performance. Any failure in rewriting phase can compromise the entire pipeline.

- Design tradeoffs: Trades computational efficiency (avoiding additional training data) against potential limitations of self-training not capturing all reasoning patterns. Requires careful prompt engineering for semantic matching and answer rewriting.

- Failure signatures: Poor semantic matching incorrectly identifying correct/incorrect answers, ineffective answer rewriting losing critical reasoning information, or overfitting to specific question formats without generalizing to new reasoning tasks.

- First 3 experiments:
  1. Run baseline experiment comparing accuracy with and without CoT on small dataset to establish performance gap
  2. Implement self-training phase with 100 examples and evaluate accuracy improvement without CoT
  3. Test framework across different model sizes (7B vs 13B) to observe relationship between model capacity and sample efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the CogniDual Framework be generalized to other types of reasoning tasks beyond mathematics, reading comprehension, and logical reasoning, such as spatial reasoning or creative problem-solving?
- Basis in paper: [explicit] Framework evaluated on GSM8K (mathematics), ReClor (reading comprehension), and LogiQA2.0 (logical reasoning), but not tested on other reasoning types
- Why unresolved: Paper only tests framework on three types of reasoning tasks
- What evidence would resolve it: Testing framework on variety of other reasoning tasks and comparing results to current study

### Open Question 2
- Question: How does the CogniDual Framework perform when applied to different model architectures, such as encoder-decoder models or models with different attention mechanisms?
- Basis in paper: [inferred] Authors only test framework on transformer-based models (Llama2 and Vicuna)
- Why unresolved: Paper only tests framework on one type of model architecture
- What evidence would resolve it: Testing framework on variety of model architectures and comparing results to current study

### Open Question 3
- Question: Can the CogniDual Framework be used to improve the performance of smaller models on tasks that they typically struggle with, such as complex reasoning or natural language inference?
- Basis in paper: [explicit] Smaller models like Llama2-7B and Vicuna-7B show significant improvements in performance on LogiQA2.0 after applying framework
- Why unresolved: Paper only tests framework on few specific tasks and model sizes
- What evidence would resolve it: Testing framework on variety of tasks and model sizes, including smaller models, and comparing results to current study

## Limitations

- Framework effectiveness depends critically on quality of semantic matching and answer rewriting phases, which are not fully specified
- Potential task contamination issue where models may already generate reasoning-like responses even when asked for direct answers
- Evaluation focuses primarily on accuracy metrics without examining actual reasoning processes employed by models post-training

## Confidence

**High Confidence**: Core finding that self-training can improve LLM performance on reasoning tasks without CoT guidance is supported by reported accuracy improvements across multiple datasets and model sizes.

**Medium Confidence**: Claim that this process mimics human dual-system cognitive development is theoretically plausible but requires more direct evidence showing LLMs develop intuitive reasoning capabilities similar to humans.

**Low Confidence**: Mechanism by which LLMs internalize complex reasoning processes into implicit capabilities remains speculative without examining internal representations or reasoning traces post-training.

## Next Checks

1. **Reasoning Trace Analysis**: Conduct detailed analysis of model outputs post-self-training to determine whether responses still contain implicit reasoning steps or have truly become intuitive. Compare structure of successful responses with and without CoT.

2. **Cross-dataset Generalization Test**: Evaluate whether models trained on one reasoning dataset (e.g., GSM8K) can successfully apply improved non-CoT capabilities to entirely different reasoning domains. Test for genuine reasoning skill transfer versus domain-specific pattern learning.

3. **Ablation Study on Semantic Matching**: Systematically vary strictness of semantic matching criteria used to identify correct/incorrect answer pairs during self-training. Measure how different matching thresholds affect final performance to determine if current approach is optimal.