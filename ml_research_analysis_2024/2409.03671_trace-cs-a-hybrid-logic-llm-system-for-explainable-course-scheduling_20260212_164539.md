---
ver: rpa2
title: 'TRACE-CS: A Hybrid Logic-LLM System for Explainable Course Scheduling'
arxiv_id: '2409.03671'
source_url: https://arxiv.org/abs/2409.03671
tags:
- scheduling
- course
- language
- logical
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TRACE-CS is a hybrid system that combines symbolic reasoning with
  large language models (LLMs) to generate explainable course schedules. The system
  encodes scheduling constraints as Boolean satisfiability problems and uses logic-based
  techniques to generate provably correct explanations, while LLMs handle natural
  language query processing and convert logical explanations into user-friendly responses.
---

# TRACE-CS: A Hybrid Logic-LLM System for Explainable Course Scheduling

## Quick Facts
- arXiv ID: 2409.03671
- Source URL: https://arxiv.org/abs/2409.03671
- Reference count: 9
- Primary result: Hybrid symbolic-LLM approach achieves 100% explanation correctness vs 54.1% for pure LLM

## Executive Summary
TRACE-CS is a hybrid system that combines symbolic reasoning with large language models (LLMs) to generate explainable course schedules. The system encodes scheduling constraints as Boolean satisfiability problems and uses logic-based techniques to generate provably correct explanations, while LLMs handle natural language query processing and convert logical explanations into user-friendly responses. The approach was evaluated on 550 contrastive queries across various complexity levels, demonstrating that combining symbolic methods with LLMs creates explainable AI agents that balance logical correctness with natural language accessibility.

## Method Summary
TRACE-CS implements a neuro-symbolic architecture that partitions course scheduling tasks between SAT-based symbolic reasoning and LLM-based natural language processing. The system encodes scheduling constraints (prerequisites, degree requirements, semester limits) into Boolean satisfiability problems, uses SAT solvers to find minimal unsatisfiable sets (MUSes) for generating minimal explanations, and employs GPT-4.1 for parsing natural language queries and refining logical explanations into accessible responses. The hybrid approach ensures logical correctness while maintaining natural language accessibility, achieving 100% correctness compared to 54.1% for pure LLM approaches.

## Key Results
- 100% explanation correctness achieved by SAT-based MUS extraction vs 54.1% for pure LLM approach
- Significantly more concise explanations (81.8 words vs 159.4 words) by identifying minimal unsatisfiable subsets
- 14.7 second average runtime for the hybrid system vs 4.3 seconds for pure LLM approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TRACE-CS achieves 100% correctness by using SAT solvers to verify explanations rather than relying on LLM reasoning alone.
- Mechanism: The symbolic encoder converts scheduling constraints into Boolean satisfiability problems. When a user query φ is received, the system constructs KB ∧ ¬φ and uses SAT solvers to find minimal unsatisfiable sets (MUSes), which directly prove why φ cannot be satisfied.
- Core assumption: SAT solvers can efficiently find MUSes for the scheduling constraint set, and these MUSes correspond to minimal explanations.
- Evidence anchors:
  - [abstract] "TRACE-CS leverages logic-based techniques to encode scheduling constraints and generate provably correct explanations"
  - [section] "The Explainer generates a minimal explanation for the user contrastive query φ...using the logic-based explanation generation algorithm from (Vasileiou, Previti, and Yeoh 2021)"
  - [corpus] Weak evidence - no direct corpus papers address MUS-based explanation generation specifically
- Break condition: If the SAT encoding becomes too large or complex for MUS extraction to remain tractable, or if the MUS algorithm fails to find minimal explanations.

### Mechanism 2
- Claim: TRACE-CS achieves concise explanations by identifying minimal unsatisfiable subsets rather than returning all applicable constraints.
- Mechanism: The MUS extraction algorithm identifies the smallest set of clauses that make KB ∧ ¬φ unsatisfiable. This minimal set directly explains why the query cannot be satisfied, avoiding the verbose enumeration of all relevant constraints.
- Core assumption: The minimal unsatisfiable set corresponds to the most relevant and concise explanation for user queries.
- Evidence anchors:
  - [section] "it outputs a set of logical clauses ϵ ⊆ KB such that ϵ |= φ, and ∄ϵ′ ⊂ ϵ such that ϵ′ |= φ. In other words, it outputs a ⊆-minimal explanation"
  - [section] "GPT-4.1 exhibited a tendency to generate non-minimal explanations that included most or all applicable constraints"
  - [corpus] Weak evidence - no corpus papers directly compare MUS-based vs LLM-based explanation minimality
- Break condition: If the minimal explanation is still too complex for users to understand, or if users prefer comprehensive explanations over minimal ones.

### Mechanism 3
- Claim: TRACE-CS maintains natural language accessibility by using LLMs only for query parsing and explanation refinement, not core reasoning.
- Mechanism: The Query Parser uses LLMs with in-context learning to extract structured information from natural language queries and convert them to logical representations. The Explanation Refiner takes minimal logical explanations and converts them to natural language using LLMs with course descriptions and contextual information.
- Core assumption: LLMs are more reliable at natural language processing tasks than logical reasoning tasks, especially when the reasoning component is handled by symbolic methods.
- Evidence anchors:
  - [section] "utilizing an LLM to process natural language queries and refine logical explanations into user-friendly responses"
  - [section] "GPT-4.1 was faster, averaging 4.3 seconds compared to TRACE-CS's 14.7 seconds" - showing LLM-only is faster but less accurate
  - [corpus] Moderate evidence - several related papers discuss neuro-symbolic approaches for education and recommendation systems
- Break condition: If LLM parsing errors become too frequent, or if the explanation refinement fails to capture the logical meaning accurately.

## Foundational Learning

- Concept: Boolean Satisfiability (SAT) and Minimal Unsatisfiable Sets (MUSes)
  - Why needed here: The core correctness mechanism relies on encoding scheduling constraints as SAT problems and finding MUSes to generate minimal explanations.
  - Quick check question: If a scheduling constraint KB requires "course A must precede course B" and a query asks "why can't B be scheduled before A", what would the MUS extraction algorithm return?

- Concept: Contrastive Query Processing
  - Why needed here: TRACE-CS handles queries that compare alternative scheduling decisions (e.g., "Why course X instead of course Y?"), requiring extraction of both positive and negative conditions.
  - Quick check question: Given the query "Why not X