---
ver: rpa2
title: Multi-step Inference over Unstructured Data
arxiv_id: '2406.17987'
source_url: https://arxiv.org/abs/2406.17987
tags:
- answer
- causal
- cora
- does
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Cora, a neuro-symbolic AI platform for high-stakes
  decision-making tasks requiring precision and logical consistency. Cora integrates
  fine-tuned LLMs for knowledge extraction with a symbolic reasoning engine for formal
  reasoning, addressing limitations of pure LLM or RAG approaches.
---

# Multi-step Inference over Unstructured Data

## Quick Facts
- arXiv ID: 2406.17987
- Source URL: https://arxiv.org/abs/2406.17987
- Authors: Aditya Kalyanpur; Kailash Karthik Saravanakumar; Victor Barres; CJ McFate; Lori Moon; Nati Seifu; Maksim Eremeev; Jose Barrera; Abraham Bautista-Castillo; Eric Brown; David Ferrucci
- Reference count: 40
- Key outcome: Cora platform excels in complex research tasks with higher claim density, citation rates, and relevance scores compared to LLM and RAG baselines.

## Executive Summary
This paper introduces Cora, a neuro-symbolic AI platform designed for high-stakes decision-making tasks requiring precision and logical consistency. Cora combines fine-tuned large language models (LLMs) for knowledge extraction with a symbolic reasoning engine to perform multi-step inference over unstructured data. The platform addresses limitations of pure LLM or RAG approaches by integrating statistical learning with formal reasoning, enabling accurate and verifiable answers in complex research scenarios such as drug discovery and macro-economic analysis. Cora's approach involves building a unified knowledge source from structured and unstructured data, then using symbolic reasoning to analyze causal influences and generate comprehensive, evidence-backed answers.

## Method Summary
Cora employs a neuro-symbolic approach that combines fine-tuned LLMs for knowledge extraction and alignment with a symbolic reasoning engine for logical inference, planning, and interactive constraint solving. The system first uses a Query Interpretation Model to extract key concepts from user queries, then builds an evidence-backed causal graph using multi-step graph search. Finally, a symbolic reasoning engine (Cogent) derives inferences from the causal map. The platform also includes an NLU Ingestion Pipeline for offline knowledge extraction, transforming unstructured text into structured causal relationships. This approach ensures claims are grounded in verifiable evidence and allows for precise counterfactual reasoning and What-if analysis.

## Key Results
- Cora achieved a 100% citation rate and 93.65% justification rate for representative queries.
- For multi-hop queries, Cora maintained a 98.51% citation rate and 90.30% justification rate.
- The platform demonstrated superior performance compared to LLM and RAG baselines in claim density, citation density, and relevance scores.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cora achieves high claim density and justification rates by using a structured causal map generated from fine-tuned LLMs and a symbolic reasoning engine.
- Mechanism: The system first uses a Query Interpretation Model (fine-tuned LLM) to extract key concepts from user queries, then builds an evidence-backed causal graph using multi-step graph search, and finally applies a symbolic reasoning engine (Cogent) to derive inferences. This approach ensures claims are grounded in verifiable evidence.
- Core assumption: Fine-tuned LLMs can accurately extract and structure domain-specific knowledge, and the symbolic reasoning engine can effectively process this knowledge to produce logically consistent results.
- Evidence anchors:
  - [abstract] "The platform integrates fine-tuned LLMs for knowledge extraction and alignment with a robust symbolic reasoning engine for logical inference, planning and interactive constraint solving."
  - [section 3.3.2] "The Evidenced Graph Builder's goal is to find multi-hop relational and causal chains connecting input concepts in the scenario to the query or target concepts."
- Break condition: If the fine-tuned LLM fails to extract accurate domain knowledge, or if the symbolic reasoning engine cannot process the knowledge effectively, the system's performance will degrade.

### Mechanism 2
- Claim: Cora's approach of using a general research template for connecting concepts allows for comprehensive and detailed answers.
- Mechanism: The system automatically induces a research template from the data using knowledge extraction algorithms and then instantiates this template with specific bindings for concepts based on inter-connected linkages. This allows for a detailed causal understanding of the scenario.
- Core assumption: A general research template can be effectively induced from the data and instantiated for specific use-cases.
- Evidence anchors:
  - [section 2.1] "Cora's approach is radically different in that it uses a general research template for connecting the dots between the two concepts of interest (RA and IRAK4 inhibitor), and then instantiates this template with specific bindings (answers) for concepts based on the inter-connected linkages."
- Break condition: If the research template cannot be effectively induced or instantiated, the system's ability to provide comprehensive answers will be compromised.

### Mechanism 3
- Claim: Cora's use of a symbolic reasoning engine allows for precise counterfactual reasoning and What-if analysis.
- Mechanism: The extracted causal map is a fully executable logical model where users can refine any part of the graph structure and then re-run inference to compute the effects of the changes. This allows for interactive exploration of different scenarios.
- Core assumption: The symbolic reasoning engine can effectively process the causal map and provide accurate results for different scenarios.
- Evidence anchors:
  - [section 2.1] "The extracted causal map is a fully executable logical model, where the user can refine any part of the graph structure - e.g. drop edges, merge nodes, force the values of specific nodes (based on known facts or hypothetical scenarios), alter edge weights (based on domain knowledge) etc. - and then re-run inference to compute the effects of the changes."
- Break condition: If the symbolic reasoning engine cannot accurately process the causal map or handle different scenarios, the system's ability to provide precise counterfactual reasoning will be limited.

## Foundational Learning

- Concept: Knowledge extraction and alignment
  - Why needed here: To transform unstructured data into a structured format that can be processed by the symbolic reasoning engine.
  - Quick check question: How does the system ensure that the extracted knowledge is accurate and relevant to the user's query?

- Concept: Symbolic reasoning and inference
  - Why needed here: To derive logical conclusions from the structured knowledge and provide evidence-backed answers.
  - Quick check question: How does the symbolic reasoning engine handle contradictions or ambiguities in the knowledge base?

- Concept: Causal mapping and graph search
  - Why needed here: To identify relevant causal relationships between concepts and build a comprehensive understanding of the scenario.
  - Quick check question: How does the system ensure that all relevant causal paths are considered in the graph search?

## Architecture Onboarding

- Component map: Query Interpretation Model (fine-tuned LLM) -> Evidenced Graph Builder -> Symbolic Reasoning Engine (Cogent) -> Result Generation
- Critical path: User query → Query Interpretation Model → Evidenced Graph Builder → Symbolic Reasoning Engine → Result Generation → User answer
- Design tradeoffs: The system trades off speed for accuracy by using a multi-step process involving both statistical (LLM) and symbolic (reasoning engine) components.
- Failure signatures: If the system fails to provide accurate answers, it could be due to issues with the Query Interpretation Model, the Evidenced Graph Builder, or the symbolic reasoning engine.
- First 3 experiments:
  1. Test the Query Interpretation Model with a simple query to ensure it can accurately extract key concepts.
  2. Verify that the Evidenced Graph Builder can find relevant causal relationships for a given query.
  3. Confirm that the symbolic reasoning engine can derive logical conclusions from the structured knowledge.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the integration of real economic time-series data with the causal maps extracted from unstructured text improve the accuracy of statistical predictions in macro-economic analysis?
- Basis in paper: [inferred]
- Why unresolved: The paper mentions that as a next step, they are exploring connecting the causal map extracted from theory with real economic time-series data to make more informed statistical predictions, but does not provide results or evidence of the effectiveness of this approach.
- What evidence would resolve it: Empirical results comparing the accuracy of predictions made using only the causal map versus those made by integrating real economic time-series data with the causal map.

### Open Question 2
- Question: How does the performance of Cora's neuro-symbolic approach compare to pure LLM or RAG-based approaches in domains other than Life Sciences and Macro-Economics, such as legal or financial domains?
- Basis in paper: [inferred]
- Why unresolved: The paper focuses on demonstrating Cora's performance in Life Sciences and Macro-Economics, but does not provide evidence of its effectiveness in other high-stakes domains like legal or finance.
- What evidence would resolve it: Comparative evaluation results of Cora's performance against LLM and RAG baselines in legal and financial domains.

### Open Question 3
- Question: What is the impact of the granularity of the knowledge representation schema on the accuracy and completeness of knowledge extraction from unstructured text?
- Basis in paper: [explicit]
- Why unresolved: The paper describes the development of a general-purpose Meaning Representation Schema for capturing knowledge, but does not provide evidence of how the granularity of this schema affects the quality of knowledge extraction.
- What evidence would resolve it: Comparative evaluation results of knowledge extraction accuracy and completeness using different levels of granularity in the knowledge representation schema.

### Open Question 4
- Question: How does the performance of Cora's answer generation algorithm compare to other state-of-the-art question answering systems in terms of precision, comprehensiveness, and logical consistency?
- Basis in paper: [inferred]
- Why unresolved: The paper provides initial evaluation results comparing Cora to LLM and RAG baselines, but does not compare its performance to other advanced question answering systems.
- What evidence would resolve it: Comparative evaluation results of Cora's performance against other state-of-the-art question answering systems on the same set of queries.

## Limitations

- The specific architectures and training details of the fine-tuned LLMs are not disclosed, which limits reproducibility.
- The exact implementation of the symbolic reasoning engine, including the Answer Set Programming solver and Cogent meta-model, is unspecified.
- The evaluation metrics, while detailed, may not fully capture the system's performance in real-world scenarios with incomplete or contradictory information.

## Confidence

- **High Confidence:** The overall approach of combining fine-tuned LLMs with symbolic reasoning for multi-step inference is well-founded and addresses key limitations of pure LLM or RAG approaches.
- **Medium Confidence:** The evaluation results demonstrating Cora's superior performance compared to baselines are promising, but the specific metrics and datasets used may not fully represent the complexity of real-world research tasks.
- **Low Confidence:** The system's ability to handle incomplete or contradictory information, as well as its scalability to very large knowledge bases, remains uncertain without further testing.

## Next Checks

1. **Fine-tuned LLM Verification:** Validate the performance of the fine-tuned LLM models used for knowledge extraction and alignment by testing them on a diverse set of domain-specific queries and comparing the results with human annotations.
2. **Symbolic Reasoning Engine Testing:** Evaluate the symbolic reasoning engine's ability to handle contradictions and ambiguities in the knowledge base by creating synthetic scenarios with known inconsistencies and measuring the system's response.
3. **Scalability Assessment:** Assess the system's performance and resource requirements when scaling to larger knowledge bases and more complex queries, including measuring the impact on latency and accuracy.