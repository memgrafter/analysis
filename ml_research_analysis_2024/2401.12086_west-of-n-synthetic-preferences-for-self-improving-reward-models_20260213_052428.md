---
ver: rpa2
title: 'West-of-N: Synthetic Preferences for Self-Improving Reward Models'
arxiv_id: '2401.12086'
source_url: https://arxiv.org/abs/2401.12086
tags:
- preference
- west-of-n
- reward
- data
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: West-of-N generates synthetic preference data for self-improving
  reward models by selecting the best and worst generations from N candidates. This
  approach outperforms other synthetic preference methods and achieves comparable
  gains to adding similar amounts of human feedback data.
---

# West-of-N: Synthetic Preferences for Self-Improving Reward Models

## Quick Facts
- arXiv ID: 2401.12086
- Source URL: https://arxiv.org/abs/2401.12086
- Reference count: 16
- Improves reward model accuracy by up to 2.3% and Best-of-N win rates by up to 11.0% over human feedback baselines

## Executive Summary
West-of-N generates synthetic preference data for reward models by sampling N responses and selecting the best and worst according to a base preference model. This self-training approach improves reward model quality by creating on-policy preference pairs that match the model's inference distribution. The method works across different base preference data types and model architectures, achieving gains comparable to adding similar amounts of human feedback data. Iterative application of West-of-N further improves performance, with gains increasing as N grows due to higher-quality pseudolabels.

## Method Summary
West-of-N is a self-training technique that generates synthetic preference pairs for reward model improvement. The method samples N responses from a language model policy, ranks them using a base preference model, and selects the best and worst to create pairwise preferences. These synthetic pairs are filtered based on confidence and likelihood criteria before being added to the training data. The self-trained reward model is then trained on a mixture of base and synthetic preferences, with iterative applications showing further improvements. The approach is specifically designed to create on-policy training data that matches the distribution of responses generated during inference.

## Key Results
- Improves reward model accuracy by up to 2.3% over human feedback baselines
- Achieves Best-of-N win rates up to 11.0% higher than human feedback baselines
- Works across different base preference data types (human vs synthetic) and model architectures (T5-XXL, Gemma 2B)

## Why This Works (Mechanism)

### Mechanism 1
West-of-N self-training improves reward model accuracy by generating high-quality on-policy preference pairs. The method samples N responses from the language model policy, selects best/worst according to base preference model, and adds these pseudo-preference pairs to training. Core assumption: base preference model can accurately rank responses. Break condition: base model performance degrades significantly.

### Mechanism 2
West-of-N provides gains comparable to or greater than human feedback data because synthetic pairs are on-policy while human feedback is often off-policy. This distributional match leads to better generalization during RLHF. Core assumption: on-policy training data is more effective than off-policy data. Break condition: when base preference model becomes too noisy.

### Mechanism 3
West-of-N improves performance regardless of base preference data type (human or synthetic). The self-training approach refines any initial preference model through iterative generation of better-labeled synthetic data. Core assumption: any initial preference model can generate useful pseudo-labels. Break condition: base data is too noisy for iterative refinement.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: West-of-N is designed to improve reward models used in RLHF pipelines
  - Quick check question: What are the three main components of a typical RLHF system?

- Concept: Bradley-Terry model for pairwise preferences
  - Why needed here: The reward model uses pairwise comparisons to learn preference ordering
  - Quick check question: How does the Bradley-Terry model convert pairwise preferences into a scalar reward function?

- Concept: Self-training and semi-supervised learning
  - Why needed here: West-of-N is fundamentally a self-training technique that generates pseudolabels
  - Quick check question: What is the key difference between self-training and traditional supervised learning?

## Architecture Onboarding

- Component map: Language model (policy) -> Response sampling -> Base reward model -> Best/Worst selection -> Synthetic preference pairs -> Student reward model -> RL fine-tuning
- Critical path: Sampling -> Ranking -> Selection -> Training (must be fast for iteration)
- Design tradeoffs: Larger N improves label quality but increases computation cost; pairwise vs pointwise base models affect accuracy vs efficiency
- Failure signatures: Poor base model accuracy -> noisy pseudolabels; too large N -> out-of-distribution responses; insufficient filtering -> low-quality synthetic data
- First 3 experiments:
  1. Implement basic West-of-N with N=2 on small dataset to verify self-training works
  2. Scale to N=64 and compare accuracy gains vs human feedback baseline
  3. Add filtering criteria and measure impact on downstream Best-of-N performance

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal N value for West-of-N sampling to balance performance gains with computational cost? The paper tests N=2, 8, 64 but doesn't explore full range or conduct comprehensive cost-benefit analysis. Systematic experiments testing wider range (4, 16, 32, 128, 256) while measuring performance gains and computational costs would identify optimal N.

### Open Question 2
How does West-of-N performance scale with different base model architectures and sizes? The paper tests T5-XXL and Gemma 2B but doesn't systematically explore relationship between model size and West-of-N effectiveness. Experiments across spectrum of model sizes (125M, 760M, 3B, 7B, 13B, 70B parameters) would reveal scaling patterns.

### Open Question 3
Does West-of-N generate preference pairs that improve reward models for domains beyond text generation tasks? All experiments focus on language model alignment tasks, but method is presented as generally applicable. Applying West-of-N to reinforcement learning tasks in other domains (Atari games, robotic control, image generation) would answer this question.

## Limitations
- Limited empirical validation scope (three datasets, two model architectures)
- Heavy dependency on base model quality with unclear degradation thresholds
- Claims of generalizability beyond tested configurations
- Weak corpus evidence with only 25 related papers found

## Confidence

- **High Confidence**: Core mechanism of selecting best/worst candidates from N generations is well-supported with clear implementation details
- **Medium Confidence**: Comparative advantage over human feedback (2.3% accuracy, 11.0% win rate gains) is supported but may not generalize to all scenarios
- **Low Confidence**: Claims of working "across different base preference data types and model architectures" extend beyond empirical evidence provided

## Next Checks

1. Systematically test West-of-N with progressively noisier base preference models to identify exact accuracy threshold where self-training begins to harm performance
2. Conduct extensive cross-architecture validation using diverse model families and varied preference data sources to verify generalizability claims
3. Implement ablation studies isolating contribution of on-policy vs off-policy data by creating controlled experiments with matched quality and distribution characteristics