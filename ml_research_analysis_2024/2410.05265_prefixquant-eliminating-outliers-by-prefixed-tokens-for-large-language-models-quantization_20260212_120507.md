---
ver: rpa2
title: 'PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language Models
  Quantization'
arxiv_id: '2410.05265'
source_url: https://arxiv.org/abs/2410.05265
tags:
- median
- tokens
- quantization
- min-1
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PrefixQuant addresses token-wise outliers in LLM quantization by
  prefixing high-frequency outlier tokens in the KV cache to isolate them offline,
  preventing outlier-induced quantization errors during inference. The method introduces
  trainable parameters for block-wise fine-tuning to compensate for quantization errors.
---

# PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language Models Quantization

## Quick Facts
- arXiv ID: 2410.05265
- Source URL: https://arxiv.org/abs/2410.05265
- Reference count: 40
- Primary result: Eliminates token-wise outliers in LLM quantization via prefixed tokens, achieving up to 3.08 points accuracy improvement over SpinQuant on zero-shot reasoning tasks.

## Executive Summary
PrefixQuant addresses the critical challenge of token-wise outliers in LLM quantization by isolating them through a simple yet effective technique: prefixing high-frequency outlier tokens in the KV cache. This training-free approach eliminates outliers during inference without retraining, enabling both dynamic and static quantization schemes. The method introduces block-wise fine-tuning with trainable parameters to compensate for quantization errors, achieving state-of-the-art performance across multiple quantization precisions while enabling significant inference speedups.

## Method Summary
PrefixQuant eliminates token-wise outliers by prefixing high-frequency outlier tokens at the beginning of input sequences, constraining outliers to prefixed positions where they can be isolated offline. This approach works by detecting outlier tokens from calibration data, inserting them as prefix tokens, and precomputing full-precision KV cache for these prefixed tokens. During inference, the KV cache is reused, ensuring quantized activations never encounter token-wise outliers. The method also introduces block-wise fine-tuning with trainable quantization parameters to compensate for quantization errors, supporting both dynamic (O1) and static (O2) quantization schemes.

## Key Results
- Achieves up to 3.08 points accuracy improvement over SpinQuant on zero-shot reasoning tasks for Llama-3-8B
- Enables 2.74× prefilling speedup and 2.16× decoding speedup for W4A4 KV4 precision models
- Demonstrates state-of-the-art performance across W4A4KV4 and W4A8KV4 precisions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prefixing high-frequency outlier tokens in the KV cache isolates token-wise outliers offline, eliminating them during inference without retraining.
- Mechanism: By adding specific tokens (e.g., `.`, `\n`, `[BOS]`) at the start of each input, token-wise outliers are confined to prefixed positions. Since KV cache for prefixed tokens is computed once and reused, outliers never appear in quantized activations during inference.
- Core assumption: Outlier tokens occur consistently at predictable positions and can be identified efficiently from calibration data.
- Evidence anchors: [abstract] "PrefixQuant eliminates token-wise outliers by prefixing outlier tokens in the KV cache" [section] "PrefixQuant is based on a key observation: Prefixing high-frequency outlier tokens at the beginning of the input sequence constrains token-wise outliers to only occur in the prefixed tokens."
- Break condition: If outlier tokens are not consistently positioned or their identities vary widely across inputs, the prefixed-token strategy will fail.

### Mechanism 2
- Claim: Block-wise fine-tuning with trainable activation clipping parameters compensates for quantization errors introduced by outlier removal.
- Mechanism: During fine-tuning, quantization parameters (e.g., clipping factors for dynamic quantization or scaling factors for static quantization) are made trainable and jointly optimized with model weights to minimize quantization-induced error at each transformer block output.
- Core assumption: Inter-layer interactions matter; optimizing quantization parameters per block can recover accuracy lost due to low-bit quantization.
- Evidence anchors: [section] "We introduce a block-wise fine-tuning to compensate for quantization error by jointly training weights and quantization parameters" [section] "For dynamic activation quantization in PrefixQuant-O1, we set the tensor-wise clipping factors as trainable."
- Break condition: If training dataset is too small or unrepresentative, fine-tuning may not adequately compensate for quantization error.

### Mechanism 3
- Claim: Using static activation quantization instead of dynamic yields faster inference while maintaining accuracy when token-wise outliers are removed.
- Mechanism: With outliers isolated to prefixed tokens, activation values become more uniformly distributed, enabling accurate per-tensor static quantization without per-token parameter overhead.
- Core assumption: Once outliers are removed, activation distribution is sufficiently uniform for static quantization to be accurate and efficient.
- Evidence anchors: [section] "Since PrefixQuant is compatible with various quantization schemes, we introduce two settings for PrefixQuant (see Table 1): O1 for dynamic quantization and O2 for static quantization" [abstract] "Additionally, we demonstrate up to 2.74× prefilling speedup and 2.16× decoding speedup for LLMs using W4A4 PrefixQuant."
- Break condition: If activation distribution is not uniform enough after outlier removal, static quantization will introduce unacceptable accuracy degradation.

## Foundational Learning

- Concept: Quantization in LLMs
  - Why needed here: Understanding how weight, activation, and KV cache quantization reduce memory and compute is essential to grasp why outlier removal improves accuracy.
  - Quick check question: What are the three main components of LLM quantization, and how does each reduce resource usage?

- Concept: Token-wise vs. channel-wise outliers
  - Why needed here: Differentiating these outlier types explains why existing methods (focused on channel-wise) are insufficient and why the prefixed-token approach is novel.
  - Quick check question: How do token-wise outliers differ from channel-wise outliers in their effect on quantization error?

- Concept: Hadamard rotation for outlier redistribution
  - Why needed here: Knowing how prior methods (e.g., QuaRot, SpinQuant) use Hadamard rotation to spread outliers helps explain why PrefixQuant's isolation approach is more effective.
  - Quick check question: What is the primary mechanism by which Hadamard rotation reduces quantization error, and what limitation does it have?

## Architecture Onboarding

- Component map:
  Input sequence -> Outlier detection -> Prefixed token insertion -> KV cache generation (full precision) -> Quantized inference with isolated outliers -> Block-wise fine-tuning (optional)

- Critical path:
  1. Detect high-frequency outlier tokens from calibration data
  2. Insert these tokens as prefix at start of each input sequence
  3. Precompute and store full-precision KV cache for prefixed tokens
  4. During inference, reuse KV cache; quantized activations never see token-wise outliers
  5. Optionally fine-tune with trainable quantization parameters to recover accuracy

- Design tradeoffs:
  - Prefixing tokens increases input length slightly but removes need for per-token dynamic quantization parameters
  - Static quantization is faster but requires more uniform activation distribution; outlier removal enables this
  - Fine-tuning improves accuracy but adds training time; can be omitted for faster deployment with minor accuracy loss

- Failure signatures:
  - If prefixed tokens do not capture all outlier tokens, quantization error spikes at those positions
  - If outlier tokens are not consistently positioned, prefixed tokens will not isolate them effectively
  - If activation distribution is not uniform after outlier removal, static quantization will fail

- First 3 experiments:
  1. Run outlier detection on small calibration set; verify outlier tokens are consistently positioned and can be isolated by prefixing
  2. Measure quantization error with and without prefixed tokens on representative input; confirm error reduction
  3. Implement static quantization with prefixed tokens; compare accuracy and inference speed to dynamic quantization baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PrefixQuant's performance scale with different numbers of prefixed tokens beyond the empirically determined value?
- Basis in paper: [explicit] The paper mentions determining the number of prefixed tokens through average outlier token count plus an additional [BOS] token, but includes ablation results showing that 2 tokens without [BOS] also works well for some models.
- Why unresolved: The paper doesn't systematically explore performance across different prefix token counts or investigate whether optimal prefix size varies by model size, dataset, or context length.
- What evidence would resolve it: Comprehensive ablation studies testing multiple prefix token counts (e.g., 1-10 tokens) across various model families, context lengths, and quantization precisions to identify optimal configurations.

### Open Question 2
- Question: What is the impact of PrefixQuant on long-context generation beyond 8192 tokens, particularly for models with context windows of 32K+ tokens?
- Basis in paper: [explicit] The paper includes an ablation study showing PrefixQuant works well at 8192 context length but doesn't explore longer contexts, despite recent trends toward very long context windows.
- Why unresolved: The paper's long-context analysis is limited to 8192 tokens, while newer models support much longer contexts where outlier patterns might differ significantly.
- What evidence would resolve it: Systematic evaluation of PrefixQuant at context lengths of 16K, 32K, and 64K tokens across different model architectures to assess scalability and identify any context-length-dependent performance degradation.

### Open Question 3
- Question: Can PrefixQuant's outlier isolation technique be extended to other neural network architectures beyond transformers, such as CNNs or RNNs?
- Basis in paper: [inferred] The paper focuses exclusively on transformer-based LLMs and discusses outlier detection in attention mechanisms and linear layers, but the general concept of isolating token-wise outliers through prefixing could potentially apply to other architectures.
- Why unresolved: The paper doesn't explore whether the outlier isolation concept generalizes to non-transformer architectures that might have different activation patterns and outlier characteristics.
- What evidence would resolve it: Applying PrefixQuant's prefixing methodology to CNNs (e.g., for image classification) and RNNs (e.g., for sequence modeling) to test whether similar outlier isolation benefits can be achieved in these domains.

## Limitations

- The effectiveness of PrefixQuant critically depends on the consistent positioning of token-wise outliers in input sequences.
- Block-wise fine-tuning to compensate for quantization errors may introduce instability, particularly with small or unrepresentative datasets.
- Scalability of outlier detection and fine-tuning to models larger than Llama-3-70B is asserted but not empirically validated.
- Static quantization assumes sufficiently uniform activation distribution post-outlier removal, which may not hold for all datasets or architectures.

## Confidence

- **High Confidence**: Core mechanism of isolating token-wise outliers via prefixed tokens is well-supported by ablation studies and comparison with baseline methods like SpinQuant and QuaRot.
- **Medium Confidence**: Claim that static quantization enables 2.74× prefilling and 2.16× decoding speedups is plausible but relies on assumptions about activation distribution uniformity.
- **Low Confidence**: Scalability of outlier detection and fine-tuning to models larger than Llama-3-70B is asserted but not empirically validated.

## Next Checks

1. **Outlier Detection Robustness**: Validate that outlier tokens are consistently positioned and identifiable across diverse datasets (e.g., different languages or domains) beyond WikiText2 and zero-shot reasoning tasks.

2. **Static Quantization Uniformity**: Measure activation distribution before and after outlier removal to confirm it is sufficiently uniform for static quantization to be accurate.

3. **Fine-Tuning Stability**: Evaluate impact of dataset size and distribution shift on stability and effectiveness of block-wise fine-tuning.