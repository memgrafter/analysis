---
ver: rpa2
title: Intriguing Properties of Modern GANs
arxiv_id: '2402.14098'
source_url: https://arxiv.org/abs/2402.14098
tags:
- images
- gans
- distribution
- training
- generative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper challenges the common belief that modern GANs capture
  the training data manifold. Through extensive experiments with state-of-the-art
  GANs, the authors show that: (1) training examples are not part of the learned GAN
  manifold, with reconstruction errors being an order of magnitude worse than for
  generated images; (2) GAN manifolds pass equally close to in-distribution and out-of-distribution
  images, leading to poor classification and outlier detection performance; (3) GANs
  assign higher likelihood to out-of-distribution images than to training images,
  with likelihood being anti-correlated with local image variance; and (4) training
  images are not part of the typical set described by the GAN distribution.'
---

# Intriguing Properties of Modern GANs

## Quick Facts
- arXiv ID: 2402.14098
- Source URL: https://arxiv.org/abs/2402.14098
- Authors: Roy Friedman; Yair Weiss
- Reference count: 22
- Key outcome: Modern GANs do not capture training data manifold; training examples are not part of the learned GAN manifold with reconstruction errors an order of magnitude worse than for generated images

## Executive Summary
This paper challenges the common belief that modern GANs capture the training data manifold. Through extensive experiments with state-of-the-art GANs (StyleGAN-XL, ReACGAN, BigGAN, StyleGAN2-ADA) across multiple datasets (ImageNet, CIFAR10, AFHQ), the authors demonstrate that training examples are not part of the learned GAN manifold. They show that GAN manifolds pass equally close to in-distribution and out-of-distribution images, leading to poor classification and outlier detection performance. Additionally, GANs assign higher likelihood to out-of-distribution images than to training images, with likelihood being anti-correlated with local image variance.

## Method Summary
The authors evaluate GAN properties using four main approaches: GAN inversion through latent space optimization to test manifold proximity, annealed importance sampling (AIS) for log-likelihood estimation, typical set analysis to determine if training images belong to the model's distribution, and classification/outlier detection performance using manifold distances. The GAN inversion uses ADAM optimizer with cosine learning rate schedule to minimize reconstruction error, while AIS employs 500 intermediate distributions with HMC transitions to estimate log-likelihoods. The experiments systematically compare reconstruction errors, likelihoods, and classification performance across training images, generated images, and out-of-distribution samples.

## Key Results
- Training examples are not part of the learned GAN manifold, with reconstruction errors being an order of magnitude worse than for generated images
- GAN manifolds pass equally close to in-distribution and out-of-distribution images, resulting in poor classification and outlier detection performance
- GANs assign higher likelihood to out-of-distribution images than to training images, with likelihood being anti-correlated with local image variance
- Training images are not part of the typical set described by the GAN distribution

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GAN manifolds do not pass through training examples because optimization in latent space finds realistic reconstructions rather than exact matches.
- **Mechanism:** When reconstructing training images via GAN inversion, gradient descent finds latent codes that generate perceptually similar images rather than exact originals. This occurs because the optimization objective minimizes perceptual distance (ℓ2 or LPIPS) rather than exact pixel-wise match.
- **Core assumption:** The GAN generator creates a continuous manifold where nearby latent codes produce similar images, making exact reconstruction of arbitrary training points impossible if they lie off-manifold.
- **Evidence anchors:**
  - [section] "Figure 2 (right) shows that for samples generated by the GAN, the optimization algorithm succeeds in finding a latent z so that the generated images is almost identical to the input image x. But for images from the training set, the best z found by the optimization algorithm generates an image that looks very different, although realistic."
  - [corpus] Weak correlation with related work on GAN inversion limitations
- **Break condition:** If the GAN were trained with explicit reconstruction loss or the latent space were discrete, exact matches might be possible.

### Mechanism 2
- **Claim:** GANs assign higher likelihood to out-of-distribution images due to bias toward low-variance regions.
- **Mechanism:** The GAN density model, when evaluated with observation noise, assigns higher probability to images with larger flat areas and low local variance. This occurs because the observation model's variance is calibrated from reconstruction errors, which are lower for simpler images.
- **Core assumption:** The observation noise variance used in AIS likelihood estimation is smaller than signal strength for low-variance images, inflating their likelihood scores.
- **Evidence anchors:**
  - [section] "Figure 7 shows how the average coefficient of variance in small 8x8 patches of the image is anti-correlated with the LL... GANs tend to have a bias towards images with larger flat region."
  - [corpus] No direct evidence found in related works
- **Break condition:** If the observation noise variance were adapted per image or if the GAN were trained with explicit diversity constraints.

### Mechanism 3
- **Claim:** Training images are not part of the typical set because the GAN's learned distribution differs fundamentally from the true data distribution.
- **Mechanism:** The GAN's implicit density assigns low probability to training images while assigning higher probability to generated images and certain out-of-distribution samples. This creates a mismatch where the typical set (defined by entropy) excludes training data.
- **Core assumption:** The typical set definition based on log-likelihood deviation from entropy correctly captures distributional properties even when the model is misspecified.
- **Evidence anchors:**
  - [section] "Using the typicality test... we find that for the GANs we studied the set of training samples is not part of the typical set of the distribution defined by the GANs... Sometimes images from SVHN are even closer to being part of the typical set of the GAN than training images."
  - [corpus] Related work on typicality testing exists but no direct evidence found
- **Break condition:** If the typical set definition were modified or if the GAN's implicit density were corrected through additional training objectives.

## Foundational Learning

- **Concept:** Annealed Importance Sampling (AIS) for likelihood estimation
  - **Why needed here:** GANs don't provide tractable likelihoods, so AIS is required to estimate log-likelihoods through sequential intermediate distributions.
  - **Quick check question:** Why does AIS provide a stochastic lower bound rather than exact likelihood?

- **Concept:** GAN inversion through latent space optimization
  - **Why needed here:** Testing whether training images lie on the GAN manifold requires finding latent codes that reconstruct them, which is essential for evaluating manifold properties.
  - **Quick check question:** What optimization objective would guarantee exact reconstruction of training images?

- **Concept:** Typical set definition and its role in distributional validation
  - **Why needed here:** Determining whether training images are outliers requires comparing their likelihood to the typical set of the model's distribution.
  - **Quick check question:** How does the choice of ε in the typical set definition affect whether training images are considered typical?

## Architecture Onboarding

- **Component map:** GAN generators (StyleGAN-XL, ReACGAN, BigGAN, StyleGAN2-ADA) -> GAN inversion optimizer (ADAM with cosine schedule) -> AIS implementation (500 steps, 4 chains) -> Typical set calculator (bootstrap confidence intervals)

- **Critical path:**
  1. Load pre-trained GAN and dataset
  2. For each image type (train, generated, OOD), perform GAN inversion to measure manifold distance
  3. Run AIS to estimate log-likelihoods for each image group
  4. Calculate typical set boundaries and determine if training images fall within them
  5. Evaluate classification/outlier detection performance using manifold distances

- **Design tradeoffs:**
  - AIS step count (500 vs 10,000): computational cost vs. likelihood accuracy
  - GAN inversion restarts (500 samples): reconstruction quality vs. runtime
  - Observation noise variance: likelihood calibration vs. PSNR impact

- **Failure signatures:**
  - Reconstruction errors showing no separation between train/generated images indicates manifold fitting
  - High classification accuracy suggests the manifold captures class boundaries
  - Training images falling within typical set indicates density alignment

- **First 3 experiments:**
  1. Run GAN inversion on 100 training and 100 generated images from StyleGAN-XL on CIFAR10, plot reconstruction error distributions
  2. Estimate log-likelihoods for the same images using AIS with 500 steps, compare means
  3. Calculate typical set boundaries and check if training image means fall outside the 95% confidence interval

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do GANs with better FID scores necessarily learn a better approximation of the true data distribution when evaluated using log-likelihood?
- Basis in paper: [explicit] The paper shows that GANs with low FID scores (e.g., StyleGAN-XL on ImageNet with FID 1.94) consistently underperform compared to other generative models like Glow and DDPM++ when evaluated using log-likelihood (LL) on unseen data.
- Why unresolved: While the paper demonstrates that GANs with excellent FID scores still assign lower LL to test samples compared to other generative models, it does not investigate whether improvements in GAN architectures or training procedures could lead to better LL performance.
- What evidence would resolve it: Training newer GAN architectures with enhanced FID scores and evaluating their LL on unseen data would provide insights into whether better FID scores correlate with improved LL performance.

### Open Question 2
- Question: Can GANs be modified to learn a density model that better approximates the true data distribution while maintaining their ability to generate high-quality samples?
- Basis in paper: [inferred] The paper shows that GANs tend to assign higher density to out-of-distribution images than to training images, and that training images are not part of the typical set described by the GAN distribution. This suggests that GANs do not learn a density model that accurately represents the true data distribution.
- Why unresolved: The paper does not explore potential modifications to GAN architectures or training procedures that could enable GANs to learn a better density model while maintaining their generative capabilities.
- What evidence would resolve it: Developing and evaluating GAN architectures or training procedures that incorporate explicit density estimation objectives, such as those used in flow-based models or score-based models, would provide insights into whether GANs can be modified to learn a better density model.

### Open Question 3
- Question: How do the findings of this paper generalize to other datasets and GAN architectures not investigated in the study?
- Basis in paper: [explicit] The paper evaluates a range of GAN architectures (StyleGAN-XL, ReACGAN, BigGAN, StyleGAN2-ADA) and datasets (ImageNet, CIFAR10, AFHQ) to demonstrate that the observed phenomena are not specific to a particular GAN or dataset.
- Why unresolved: While the paper provides evidence that the observed phenomena generalize across the evaluated GAN architectures and datasets, it does not investigate whether these findings hold for other datasets or GAN architectures not included in the study.
- What evidence would resolve it: Evaluating a wider range of GAN architectures and datasets, including those with different data modalities (e.g., audio, video) and generative tasks (e.g., image-to-image translation), would provide insights into the generalizability of the observed phenomena.

## Limitations

- The study relies heavily on AIS for likelihood estimation, which provides only a stochastic lower bound rather than exact likelihoods
- The GAN inversion approach assumes the latent space is continuous and well-behaved, which may not hold for all architectures
- While experiments cover multiple datasets, conclusions may not generalize to other domains or smaller-scale GANs not tested

## Confidence

**High Confidence:** The finding that training images are not part of the GAN manifold is supported by clear reconstruction error differences (order of magnitude) between training and generated images across all tested architectures.

**Medium Confidence:** The observation that GANs assign higher likelihood to out-of-distribution images is well-supported but depends on the specific AIS implementation and observation noise model used.

**Medium Confidence:** The typical set analysis showing training images are not part of the typical set is methodologically sound but the interpretation depends on the specific ε threshold chosen for the typical set definition.

## Next Checks

1. **AIS Sensitivity Analysis:** Repeat the likelihood estimation using different AIS step counts (100, 500, 1000, 10000) to verify that the relative ordering of training vs. generated vs. out-of-distribution likelihoods remains consistent.

2. **Alternative Inversion Methods:** Compare the reconstruction error results using different GAN inversion approaches (e.g., optimization-based vs. encoder-based methods) to verify that the observed separation between training and generated images is robust.

3. **Cross-Architecture Validation:** Test additional GAN architectures not included in the original study (e.g., VQ-VAE, diffusion models) to determine whether the observed properties are specific to the studied GAN variants or represent a broader phenomenon in generative modeling.