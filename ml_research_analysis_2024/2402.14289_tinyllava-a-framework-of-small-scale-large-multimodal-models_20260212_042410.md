---
ver: rpa2
title: 'TinyLLaVA: A Framework of Small-scale Large Multimodal Models'
arxiv_id: '2402.14289'
source_url: https://arxiv.org/abs/2402.14289
tags:
- uni00000011
- uni00000019
- uni00000018
- uni0000001b
- uni0000001a
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the TinyLLaVA framework for small-scale large
  multimodal models (LMMs), focusing on empirical analysis of model architectures,
  training recipes, and data quality. The study investigates the effects of different
  vision encoders, connection modules, language models, training data, and training
  recipes on small-scale LMM performance.
---

# TinyLLaVA: A Framework of Small-scale Large Multimodal Models

## Quick Facts
- arXiv ID: 2402.14289
- Source URL: https://arxiv.org/abs/2402.14289
- Reference count: 40
- Key outcome: Smaller LMMs can achieve on-par performance with larger models when using better quality data and improved training recipes.

## Executive Summary
This paper introduces TinyLLaVA, a framework for small-scale large multimodal models (LMMs) that focuses on empirical analysis of model architectures, training recipes, and data quality. The study investigates how different vision encoders, connection modules, language models, training data, and training recipes affect small-scale LMM performance. Results demonstrate that smaller models (1.1B-2.7B parameters) can match or exceed the performance of larger models (7B parameters) when using high-quality data and optimized training approaches. The best model, TinyLLaVA-3.1B, outperforms existing 7B models on various benchmarks including VQAv2, GQA, and MM-Vet.

## Method Summary
The TinyLLaVA framework combines small-scale language models (TinyLlama-1.1B, StableLM-2-1.6B, Phi-2-2.7B) with vision encoders (CLIP or SigLIP) and a two-layer MLP connector. The training follows a two-stage approach: pre-training for feature alignment and supervised fine-tuning. Two training recipes are explored - a base recipe that freezes vision encoder and LLM while training only the connector, and a share recipe that partially fine-tunes the vision encoder. The study evaluates performance across different model architectures, data mixtures (LLaVA-1.5 vs ShareGPT4V), and training recipes on benchmarks including VQAv2, GQA, MM-Vet, POPE, LLaVA-W, MME, and MMBench.

## Key Results
- TinyLLaVA-3.1B achieves 79.9% accuracy on VQAv2, outperforming Qwen-VL-7B (78.8%) and LLaVA-1.5-7B (77.0%)
- Models using SigLIP vision encoder with higher resolution show substantial performance improvements over CLIP variants
- Share training recipe with partial vision encoder fine-tuning significantly improves performance for small-scale LLMs
- Better quality training data (ShareGPT4V) combined with improved recipes enables smaller models to match larger ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Better quality training data combined with improved training recipes allows smaller LMMs to match the performance of larger ones.
- Mechanism: High-quality, diverse data improves model learning, while optimized training recipes enable smaller models to better fit the data distribution and extract more useful features.
- Core assumption: Quality and diversity of training data, combined with optimized training recipes, are primary drivers of performance gains.
- Evidence anchors:
  - [abstract] "Our extensive experiments showed that better quality of data combined with better training recipes, smaller LMMs can consistently achieve on-par performances compared to bigger LMMs."
  - [section 4.2.2] "We observe that when models pre-trained on the larger and more diverse ShareGPT4V dataset [7], the share recipe can significantly improve performance for all variants."
  - [corpus] Weak - no direct corpus evidence supporting this specific mechanism.
- Break condition: If data quality is low or training recipe is suboptimal, performance advantage of smaller models will diminish or disappear.

### Mechanism 2
- Claim: Using SigLIP as the vision encoder, with higher input resolution and more visual tokens, significantly improves performance compared to CLIP.
- Mechanism: SigLIP's higher resolution and token count provide richer visual information, enabling better fine-grained image understanding and feature extraction for downstream tasks.
- Core assumption: Increased visual information from SigLIP's higher resolution and token count directly translates to improved model performance on multimodal tasks.
- Evidence anchors:
  - [section 4.2.1] "It is noteworthy that model variants with SigLIP [58] exhibit substantial enhancements in model performances compared to those with CLIP [44], which is particularly evident in TextVQA [45] and LLaV A-W [38] benchmarks."
  - [corpus] Weak - no direct corpus evidence supporting this specific mechanism.
- Break condition: If downstream tasks don't benefit significantly from fine-grained visual information, performance gain from SigLIP may be minimal.

### Mechanism 3
- Claim: For smaller LLMs, fine-tuning more model parameters during pre-training improves performance, while for larger LLMs, it may lead to more hallucinations.
- Mechanism: Smaller LLMs have less capacity to fit the data distribution, so fine-tuning more parameters allows them to better learn from training data. Larger LLMs may overfit or generate less reliable outputs when more parameters are fine-tuned.
- Core assumption: Relationship between model size and optimal number of fine-tuned parameters is non-linear, with smaller models benefiting more from fine-tuning more parameters.
- Evidence anchors:
  - [section 4.2.2] "We conjecture that whether fine-tuning the vision encoders can improve performance depends on the size of the accompanied LLMs and the size of the training data."
  - [corpus] Weak - no direct corpus evidence supporting this specific mechanism.
- Break condition: If training data is insufficient or of low quality, fine-tuning more parameters may lead to overfitting and degraded performance for both smaller and larger models.

## Foundational Learning

- Concept: Multimodal learning and integration of visual and textual information
  - Why needed here: Understanding how LMMs process and combine information from different modalities is crucial for designing effective architectures and training strategies
  - Quick check question: How do LMMs typically align visual features with textual embeddings?

- Concept: Training data quality and its impact on model performance
  - Why needed here: Paper emphasizes importance of high-quality, diverse data in achieving good performance, especially for smaller models
  - Quick check question: What are strategies for curating high-quality training data for LMMs?

- Concept: Training recipes and hyperparameter optimization
  - Why needed here: Paper explores different training recipes and their effects on model performance, highlighting importance of careful optimization
  - Quick check question: How do different training recipes (e.g., fine-tuning more parameters vs. freezing more layers) impact performance of LMMs?

## Architecture Onboarding

- Component map: Vision encoder (CLIP/SigLIP) → Connector (MLP) → Small-scale LLM
- Critical path: Vision encoder → Connector → Small-scale LLM
- Design tradeoffs: Model size vs. performance (smaller models may require more fine-tuning or higher-quality data), vision encoder choice (CLIP vs. SigLIP), and connector architecture (MLP vs. resampler)
- Failure signatures: Poor performance on multimodal tasks, excessive hallucinations, or failure to generalize to new domains may indicate issues with data quality, training recipes, or architectural choices
- First 3 experiments:
  1. Ablation study on impact of different vision encoders (CLIP vs. SigLIP) on model performance
  2. Comparison of training recipes (base vs. share) and their effects on smaller vs. larger LMM variants
  3. Evaluation of impact of data quality (LLaVA-1.5 vs. ShareGPT4V) on model performance under different training recipes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between training the vision encoder and freezing it when using small-scale LLMs?
- Basis in paper: [explicit] The paper shows that partially fine-tuning the vision encoder improves performance for small-scale LLMs, contrary to findings with standard LLMs. It states: "This observation suggests that fine-tuning the vision encoder can improve the performance when using small-scale LLMs, which is contrary to the result in [27] that fine-tuning the vision encoder dramatically degrades performance when using standard LLMs."
- Why unresolved: Paper only explores one specific approach (partially fine-tuning the first 12 layers) and does not systematically investigate the full trade-off space between freezing and fine-tuning different portions of the vision encoder
- What evidence would resolve it: Systematic experiments varying the proportion of the vision encoder that is fine-tuned (0%, 25%, 50%, 75%, 100%) while measuring performance and computational cost would identify optimal fine-tuning strategies

### Open Question 2
- Question: How do different connector architectures beyond MLP and resampler affect the performance of small-scale LMMs?
- Basis in paper: [explicit] The paper states: "We anticipate further exploration of diverse connectors in future studies" after showing MLP outperforms resampler, and mentions: "the connector Pϕ is designed for effectively leveraging the capability of both the pre-trained LLM and vision encoder."
- Why unresolved: Paper only compares two connector types (MLP and resampler) and does not explore other potential architectures like attention-based connectors, Fourier feature mappings, or learned linear projections that might better align visual and textual representations
- What evidence would resolve it: Comprehensive experiments testing various connector architectures (attention mechanisms, Fourier mappings, cross-attention, etc.) with different small-scale LLMs and vision encoders would identify optimal connector designs

### Open Question 3
- Question: What is the relationship between model size, dataset scale, and hallucination propensity in small-scale LMMs?
- Basis in paper: [explicit] The paper observes: "when employing the share recipe, model variants with StableLM-2 and Phi-2 exhibit a significant decline in performance on POPE (indicating more hallucinations) while experiencing improvements on other benchmarks." It also notes that smaller models may require more trainable parameters to reduce hallucinations
- Why unresolved: Paper identifies a correlation between training recipes, model size, and hallucination rates but does not provide a theoretical framework or systematic study of how these factors interact to produce hallucinations
- What evidence would resolve it: Controlled experiments varying model size, dataset size, and fine-tuning strategies while measuring hallucination rates on standardized benchmarks would reveal the causal relationships and optimal configurations for minimizing hallucinations

## Limitations
- Limited exploration of alternative model architectures beyond three specific small-scale LLMs and two vision encoders
- Evaluation focused on specific benchmarks without comprehensive testing on real-world, out-of-distribution data
- Lack of detailed implementation specifics and hyperparameter configurations that could affect reproducibility

## Confidence
High Confidence:
- Better quality training data combined with improved training recipes enables smaller LMMs to match performance of larger ones

Medium Confidence:
- SigLIP vision encoder with higher resolution and more visual tokens significantly improves performance compared to CLIP

Low Confidence:
- For smaller LLMs, fine-tuning more model parameters during pre-training improves performance, while for larger LLMs, it may lead to more hallucinations

## Next Checks
1. **Ablation Study on Hyperparameters**: Conduct systematic evaluation of different hyperparameters (learning rates, batch sizes, training durations) on TinyLLaVA model performance to identify optimal configurations.

2. **Generalization to Out-of-Distribution Data**: Evaluate TinyLLaVA models on out-of-distribution data and real-world scenarios to assess generalization capabilities beyond specific benchmarks.

3. **Exploration of Alternative Model Architectures**: Investigate performance of TinyLLaVA using alternative small-scale LLMs and vision encoders not explored in the original study to determine generalizability of observed improvements.