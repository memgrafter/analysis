---
ver: rpa2
title: Last-Iterate Global Convergence of Policy Gradients for Constrained Reinforcement
  Learning
arxiv_id: '2407.10775'
source_url: https://arxiv.org/abs/2407.10775
tags:
- policy
- risk
- learning
- cost
- c-pgae
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes C-PG, a general policy-based primal-dual algorithm
  for constrained reinforcement learning, exhibiting global last-iterate convergence
  guarantees. The method optimizes a regularized Lagrangian function using an alternate
  ascent/descent scheme.
---

# Last-Iterate Global Convergence of Policy Gradients for Constrained Reinforcement Learning

## Quick Facts
- arXiv ID: 2407.10775
- Source URL: https://arxiv.org/abs/2407.10775
- Authors: Alessandro Montenegro; Marco Mussi; Matteo Papini; Alberto Maria Metelli
- Reference count: 40
- Key outcome: Introduces C-PG, a general policy-based primal-dual algorithm for constrained RL with global last-iterate convergence guarantees under weak gradient domination assumptions

## Executive Summary
This paper proposes C-PG, a general policy-based primal-dual algorithm for constrained reinforcement learning that achieves global last-iterate convergence. The method optimizes a regularized Lagrangian function using an alternate ascent/descent scheme with ridge regularization. The framework supports both action-based (C-PGAE) and parameter-based (C-PGPE) exploration paradigms, naturally extending to risk-constrained problems. Theoretical analysis provides sample complexity bounds, while empirical validation demonstrates effectiveness compared to state-of-the-art baselines across multiple environments.

## Method Summary
C-PG is a primal-dual algorithm that optimizes a regularized Lagrangian for constrained RL problems. The core innovation is using ridge regularization on Lagrange multipliers, making the dual maximization problem strongly concave with a closed-form solution. This enables simultaneous optimization of primal policy parameters and dual variables without projection. The framework supports both action-based learning (direct policy parameters) and parameter-based learning (hyperpolicy parameters), with the latter naturally handling risk measures like CVaR and Mean-Variance. The algorithm uses constant learning rates scaled by the regularization parameter ω and error tolerance ϵ, with sample complexity depending on these quantities and the gradient domination exponent ψ.

## Key Results
- C-PG achieves global last-iterate convergence for constrained RL under weak gradient domination assumptions
- C-PGPE outperforms baselines on risk-constrained problems, achieving lowest costs with CVaR and MV risk measures
- The algorithm converges to globally optimal feasible policies without dependence on state/action space cardinality
- Empirical validation shows effectiveness compared to state-of-the-art baselines on multiple benchmark environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The alternate ascent/descent scheme with ridge regularization enables dimension-free last-iterate convergence.
- Mechanism: By regularizing the Lagrangian with a ridge term (ω‖λ‖²/2), the dual maximization problem becomes strongly concave and admits a closed-form solution. This allows simultaneous optimization of primal and dual variables without requiring projection of Lagrange multipliers, enabling convergence rates independent of state/action space cardinality.
- Core assumption: The primal function satisfies a (weak) gradient domination condition (Assumption 3.2).
- Evidence anchors:
  - [abstract]: "C-PG, a general policy-based primal-dual algorithm optimizing the regularized Lagrangian function... exhibits global last-iterate convergence guarantees under (weak) gradient domination assumptions"
  - [section]: "The ridge regularization makes Lω(υ, λ) a strongly concave function of λ at the price of a bias... Thus, we address the ω-regularized min-max optimization problem"
- Break condition: If the gradient domination assumption fails or the ridge parameter ω is set too large, the regularization bias overwhelms the convergence benefit.

### Mechanism 2
- Claim: The parameter-based exploration paradigm extends the framework to risk-constrained problems.
- Mechanism: By learning parameters of a stochastic hyperpolicy (νρ) instead of directly learning policy parameters, the method can incorporate risk measures through additional optimization over auxiliary variables η. This allows handling constraints defined in terms of risk measures like CVaR and Mean-Variance.
- Core assumption: The unified risk measure formulation maintains sufficient smoothness for gradient-based optimization.
- Evidence anchors:
  - [abstract]: "C-PGAE and C-PGPE, the action-based and the parameter-based versions of C-PG... illustrate how they naturally extend to constraints defined in terms of risk measures"
  - [section]: "Parameter-based (PB) PG methods focus on learning the parameters ρ ∈ R ⊆ ℝᵈᴿ of a parametric stochastic hyperpolicy νρ ∈ Δ(Θ)"
- Break condition: If the risk measure formulation introduces non-differentiable points or violates gradient domination, convergence guarantees may fail.

### Mechanism 3
- Claim: Sample complexity scales with regularization parameter ω and error tolerance ϵ.
- Mechanism: The algorithm uses constant learning rates ζυ ∝ ω and ζλ ∝ ω⁻¹ for exact gradients, and ζυ ∝ ω³ϵ²/ᵠ, ζλ ∝ ωϵ²/ᵠ for estimated gradients. This scaling ensures the potential function Pₖ(χ) converges to the desired tolerance.
- Core assumption: Variance of gradient estimators is bounded (Assumption 3.4) and depends on ω.
- Evidence anchors:
  - [abstract]: "K = O(ω⁻³ϵ⁻⁴/ᵠ⁺¹) if ψ ∈ [1,2] and the gradients are estimated"
  - [section]: "for the estimated gradient case, we choose ζλ = O(ωϵ²/ᵠ) and ζυ = O(ω³ϵ²/ᵠ)"
- Break condition: If gradient variance scales faster than ω⁻² or if the learning rate scaling is violated, the theoretical rates no longer hold.

## Foundational Learning

- Concept: Gradient domination (weak and strong forms)
  - Why needed here: The convergence guarantees rely on the primal function satisfying a gradient domination condition, which generalizes convexity for non-convex problems.
  - Quick check question: What is the difference between weak gradient domination (ψ=1) and the Polyak-Łojasiewicz condition (ψ=2)?

- Concept: Primal-dual optimization and Lagrangian methods
  - Why needed here: The algorithm optimizes a constrained problem by alternating updates to primal variables (policy parameters) and dual variables (Lagrange multipliers).
  - Quick check question: How does ridge regularization of the Lagrangian enable closed-form solutions for the dual variables?

- Concept: Policy gradient methods (action-based vs parameter-based)
  - Why needed here: The framework supports both exploration paradigms, with action-based learning policy parameters directly and parameter-based learning hyperpolicy parameters.
  - Quick check question: What is the semantic difference between enforcing risk-based constraints in action-based vs parameter-based exploration?

## Architecture Onboarding

- Component map: C-PG -> C-PGAE/C-PGPE -> Estimators -> Policy updates -> Convergence
- Critical path:
  1. Initialize primal parameters (θ or ρ), dual multipliers (λ), and risk parameters (η)
  2. Collect N trajectories using current policy/hyperpolicy
  3. Update primal parameters using gradient estimates
  4. Update risk parameters using risk-specific gradients
  5. Update dual multipliers using cost gradient estimates
  6. Repeat until convergence criteria met
- Design tradeoffs:
  - Ridge regularization provides convergence but introduces bias proportional to ω‖λ*‖²
  - Parameter-based exploration enables risk constraints but requires sampling hyperpolicy parameters
  - Sample complexity depends on both ω and desired accuracy ϵ, requiring careful tuning
- Failure signatures:
  - Oscillating dual variables suggest learning rate ζλ too large
  - Slow primal convergence suggests learning rate ζυ too small or gradient domination assumption violated
  - High variance in cost estimates suggests insufficient trajectories per iteration
- First 3 experiments:
  1. Test convergence on a simple grid world with tabular softmax policy, comparing C-PGAE to baselines
  2. Verify risk measure handling by implementing CVaR constraints on a continuous control task
  3. Study regularization sensitivity by varying ω on a linear quadratic regulator problem

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the sample complexity of C-PG change for risk-constrained optimization problems compared to the standard constrained optimization problem?
- Basis in paper: [inferred] The paper states that the convergence guarantees of C-PG hold for the standard COP but are not directly applicable to the RCOP due to potential violations of assumptions like (weak) gradient domination for risk-based objectives.
- Why unresolved: The paper does not provide convergence rates for C-PG when applied to RCOPs, leaving the sample complexity unknown for this more general setting.
- What evidence would resolve it: Theoretical analysis of C-PG's convergence rates for RCOPs with different risk measures, or empirical studies comparing sample complexity between COP and RCOP settings.

### Open Question 2
- Question: What is the impact of the choice of risk measure (e.g., CVaRα, MV, Chance) on the performance and safety of the learned policies in practice?
- Basis in paper: [explicit] The paper mentions that C-PGPE and C-PGAE can handle constraints defined in terms of risk measures like CVaR and MV, and presents empirical results on Swimmer-v4 showing differences in cost distributions under different risk measures.
- Why unresolved: The paper provides limited empirical analysis of the impact of different risk measures on policy performance and safety, and does not offer theoretical insights into how the choice of risk measure affects these aspects.
- What evidence would resolve it: Extensive empirical studies comparing policy performance and safety metrics across different risk measures in various environments, or theoretical analysis of the relationship between risk measures and policy properties.

### Open Question 3
- Question: How does the choice of the regularization parameter ω affect the trade-off between constraint satisfaction and objective optimization in practice?
- Basis in paper: [explicit] The paper discusses the role of ω in the regularized Lagrangian approach and mentions that a choice of ω = Op(ϵ) is needed to achieve an overall ϵ error on both the objective function gap and constraint violation. It also presents a regularization sensitivity study in CostLQR.
- Why unresolved: The paper does not provide a systematic analysis of the impact of ω on the trade-off between constraint satisfaction and objective optimization, nor does it offer guidance on how to choose ω in practice.
- What evidence would resolve it: Empirical studies investigating the relationship between ω and the trade-off between constraint satisfaction and objective optimization across different environments and problem settings, or theoretical analysis of the role of ω in this trade-off.

## Limitations
- Theoretical guarantees rely on gradient domination assumptions that may be difficult to verify in practice
- Sample complexity bounds depend polynomially on regularization parameter ω, which may be prohibitive for high-dimensional problems
- Limited empirical validation with only three benchmark environments, restricting generalizability assessment

## Confidence
- Global last-iterate convergence guarantees: **Medium**
- Extension to risk-constrained problems: **Medium**
- Sample complexity bounds: **Medium**
- Empirical effectiveness vs. baselines: **Low**

## Next Checks
1. Test gradient domination conditions on benchmark CMDPs to assess practical applicability of convergence guarantees
2. Conduct ablation studies varying ω to quantify the bias-variance tradeoff and identify optimal scaling
3. Validate the unified risk measure formulation by comparing CVaR and MV constraint handling across multiple environments