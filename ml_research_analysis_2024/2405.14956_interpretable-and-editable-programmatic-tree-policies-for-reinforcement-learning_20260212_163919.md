---
ver: rpa2
title: Interpretable and Editable Programmatic Tree Policies for Reinforcement Learning
arxiv_id: '2405.14956'
source_url: https://arxiv.org/abs/2405.14956
tags:
- learning
- tree
- interpreter
- policy
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "INTERPRETER is a method for distilling neural oracle policies\
  \ from reinforcement learning into compact, interpretable, and editable tree programs.\
  \ It uses oblique decision trees to fit the oracle\u2019s state-action mappings\
  \ and converts the resulting tree into readable Python code."
---

# Interpretable and Editable Programmatic Tree Policies for Reinforcement Learning

## Quick Facts
- arXiv ID: 2405.14956
- Source URL: https://arxiv.org/abs/2405.14956
- Authors: Hector Kohler; Quentin Delfosse; Riad Akrour; Kristian Kersting; Philippe Preux
- Reference count: 40
- Key outcome: INTERPRETER distills neural oracle policies into compact, interpretable, and editable tree programs using oblique decision trees, matching or exceeding oracle performance on Atari, MuJoCo, and classic control tasks.

## Executive Summary
INTERPRETER addresses the challenge of making reinforcement learning policies interpretable by distilling neural oracle policies into compact tree programs. The method uses oblique decision trees to fit the oracle's state-action mappings and converts the resulting tree into executable Python code. On a range of benchmark tasks, INTERPRETER achieves performance matching or exceeding the oracle while using only 16-64 nodes, often within minutes of computation time.

## Method Summary
INTERPRETER extracts interpretable policies by first collecting state-action pairs from a trained neural oracle through environment rollouts. It then masks idle features to reduce dimensionality, generates oblique features as linear combinations of state features, and fits an oblique decision tree using a modified CART algorithm. The best tree is converted to Python code with if-else statements, creating an executable programmatic policy that can be easily understood and modified by humans.

## Key Results
- Matches or exceeds oracle performance using only 16-64 nodes in tree programs
- Extracts interpretable policies within minutes for most tested environments
- Oblique splits and feature masking are critical to performance
- User studies show INTERPRETER programs are more interpretable and editable than competing methods

## Why This Works (Mechanism)

### Mechanism 1
Neural oracles often learn policies with oblique decision boundaries that test linear combinations of features. INTERPRETER extends CART by creating oblique features as differences between pairs of state features (e.g., si1 - si0), allowing the tree to represent oblique splits using linear combinations of at most two features, matching the oracle's behavior.

### Mechanism 2
Many state features in RL environments are constant or irrelevant for decision-making. INTERPRETER identifies and removes these features before fitting the oblique tree, significantly reducing the feature space size and preventing out-of-memory errors during tree fitting.

### Mechanism 3
After learning the oblique tree through imitation learning, INTERPRETER translates the tree structure into readable Python code with if-else statements. This programmatic representation allows experts to understand the decision logic, verify correctness, and make targeted modifications to correct misalignments.

## Foundational Learning

- **Reinforcement Learning fundamentals (MDPs, policies, value functions, Q-learning)**: Essential for understanding how RL agents learn to map states to actions and how INTERPRETER distills these learned mappings into interpretable programs. *Quick check: What is the difference between a deterministic policy π(s) and a stochastic policy π(a|s) in the context of MDPs?*

- **Decision Trees and CART algorithm**: Crucial for understanding how INTERPRETER uses tree structures to represent policies and how the oblique extension modifies the standard CART approach. *Quick check: How does CART choose which feature to split on at each node during tree construction?*

- **Imitation Learning (Behavior Cloning, DAgger)**: Important for understanding how INTERPRETER trains the interpretable tree policy to mimic the neural oracle's behavior by collecting state-action pairs and fitting a policy to them. *Quick check: What is the main difference between behavior cloning and DAgger in terms of how they collect training data?*

## Architecture Onboarding

- **Component map**: MDP Environment -> Neural Oracle -> Feature Masking -> Oblique Feature Generation -> CART Tree Learner -> Tree-to-Program Converter -> MDP Rollout Engine

- **Critical path**: 1) Collect state-action pairs from neural oracle through rollouts, 2) Mask idle features to reduce dimensionality, 3) Generate oblique features (linear combinations), 4) Fit oblique decision tree using CART, 5) Convert best tree to Python program, 6) Evaluate program in environment

- **Design tradeoffs**: Tree size vs. performance (larger trees better approximate oracle but less interpretable), Oblique vs. axis-parallel splits (oblique captures complex boundaries but increases feature space dimensionality), Feature masking (reduces complexity but risks removing useful features)

- **Failure signatures**: Out-of-memory errors during tree fitting (likely caused by too many features), Poor performance matching oracle (could be insufficient tree size, incorrect feature masking, or oracle using non-linear decision boundaries), Extremely long Python programs (tree may be too deep)

- **First 3 experiments**: 1) Test INTERPRETER on CartPole with known oracle to verify basic functionality, 2) Compare axis-parallel vs. oblique trees on Pong to test oblique mechanism, 3) Test feature masking effectiveness on Seaquest with known idle features

## Open Questions the Paper Calls Out
- **Open Question 1**: How does runtime and resource usage scale with state space dimensionality in complex environments? The paper mentions runtime bottlenecks but lacks detailed scaling analysis for more complex environments.

- **Open Question 2**: What are the specific limitations of using oblique decision trees for policy extraction in environments requiring non-linear or multi-step reasoning? While the paper demonstrates success in many environments, it doesn't thoroughly investigate boundaries of what oblique decision trees can capture.

- **Open Question 3**: How does interpretability and usability of INTERPRETER programs compare across different user groups such as non-experts versus machine learning practitioners? The current user study focuses on machine learning practitioners but doesn't address accessibility to broader audiences.

## Limitations
- Oblique feature mechanism has weak empirical validation and limited evidence that linear combinations of two features can capture complex oracle decision boundaries
- Feature masking effectiveness is not thoroughly tested - removing seemingly idle features might subtly impact policy performance
- Interpretability claims rely on a user study with limited details on methodology, sample size, and participant expertise levels

## Confidence
- **High confidence**: Basic framework of using oblique decision trees for policy distillation
- **Medium confidence**: Performance claims on Atari, MuJoCo, and classic control tasks
- **Low confidence**: Oblique feature mechanism, feature masking effectiveness, interpretability benefits

## Next Checks
1. Test INTERPRETER on environments where oracle policies use known oblique decision boundaries to verify the linear combination mechanism actually captures the oracle's behavior.

2. Systematically test INTERPRETER with and without feature masking on environments with known idle features while measuring both performance and computational complexity to quantify the trade-off.

3. Replicate the user study with a larger sample of RL practitioners, measuring time to understand programs, confidence in edits, and ability to correct misalignments, with control comparisons to other methods.