---
ver: rpa2
title: Are Synthetic Time-series Data Really not as Good as Real Data?
arxiv_id: '2402.00607'
source_url: https://arxiv.org/abs/2402.00607
tags:
- data
- synthetic
- real
- time-series
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces InfoBoost, a cross-domain data synthesizing
  framework with time series representation learning capability. InfoBoost generates
  synthetic time-series data without relying on real data or deep learning models,
  and trains a universal feature extractor based on this synthetic data that is applicable
  to all time-series data.
---

# Are Synthetic Time-series Data Really not as Good as Real Data?

## Quick Facts
- arXiv ID: 2402.00607
- Source URL: https://arxiv.org/abs/2402.00607
- Authors: Fanzhe Fu; Junru Chen; Jing Zhang; Carl Yang; Lvbin Ma; Yang Yang
- Reference count: 31
- Key outcome: InfoBoost generates synthetic time-series data without real data that outperforms models trained on real data

## Executive Summary
This paper introduces InfoBoost, a cross-domain synthetic time-series data framework that generates high-quality synthetic data without requiring real data or deep learning models. The method synthesizes data by combining multi-source rhythmic components (sine waves), noise distributions, and trend information, then trains a universal feature extractor on this synthetic data. Experiments demonstrate that models trained solely on InfoBoost's synthetic data achieve superior reconstruction performance and feature extraction compared to those trained on real data across 35 public datasets.

## Method Summary
InfoBoost generates synthetic time-series data through a parametric approach combining multi-source rhythmic data (MRD) created from sine wave superposition, noise distributions across 15 types in 5 categories, and trend information. These components are weighted by random ratios to form the final synthetic dataset. A universal feature extractor is then trained on this synthetic data to learn to decompose time-series into explicit rhythmic, noise, and trend components. The method operates without any real data during training and claims to provide better generalization than models trained on real data.

## Key Results
- Models trained on InfoBoost's synthetic data outperform those trained on real data in 55 out of 60 testing scenarios
- The universal feature extractor can effectively extract representations from various real-world datasets across different domains
- InfoBoost overcomes challenges including multi-source interference, rhythmic signals, noise interference, and long-period features

## Why This Works (Mechanism)

### Mechanism 1
InfoBoost generates synthetic time-series data that generalizes better than real data without requiring any real data during training. It synthesizes data by superimposing sine waves with varying phases, frequencies, and amplitudes to create multi-source rhythmic data (MRD), then combines MRD with randomly sampled noise distributions and trend information, each weighted by a random ratio, to form the final synthetic dataset. The core assumption is that the space of possible sine wave combinations and noise distributions can sufficiently cover the diversity of real-world time-series patterns without needing to train on real data. If real-world time-series data contains patterns that cannot be represented by combinations of sine waves, noise distributions, and trends (e.g., chaotic systems or non-periodic dynamics), the synthetic data would fail to generalize.

### Mechanism 2
InfoBoost can extract universal time-series representations that explicitly separate rhythmic components, noise, and trend information from any time-series data. It trains a representation extractor using synthetic data where the underlying parameters (frequencies, amplitudes, noise types, trend types) are known and embedded as labels, allowing the model to learn to decompose any input time-series into these explicit components. The core assumption is that since the synthetic data generation process is fully parametric and known, a model can learn to reverse-engineer these parameters from the output data, making it generalizable to real data with similar characteristics. If real-world time-series data contains non-stationary patterns or long-range dependencies that cannot be captured by the synthetic data's parameterization, the representation extractor would fail to provide meaningful decomposition.

### Mechanism 3
InfoBoost's synthetic data enables models to achieve superior reconstruction performance compared to models trained on real data. By training reconstruction models solely on InfoBoost's synthetic data, the models learn robust representations that generalize across domains. The experiments show that these models outperform those trained on real data when tested on real data. The core assumption is that the synthetic data's diversity and the explicit parameterization of its components provide richer training signals than real data, leading to better generalization. If the synthetic data fails to capture domain-specific characteristics that are crucial for accurate reconstruction in certain domains, the reconstruction performance would degrade.

## Foundational Learning

- **Concept: Fourier Transform and its variants (CFT, DFT, DCT)**
  - Why needed here: The paper explicitly states it draws inspiration from Fourier transforms for time-series feature extraction, and the synthetic data generation is based on combining sine waves with various frequencies and phases.
  - Quick check question: What is the fundamental principle behind Fourier transforms that makes them useful for time-series analysis?

- **Concept: Nyquist-Shannon Sampling Theorem**
  - Why needed here: The paper mentions adhering to this theorem when determining frequency limits for synthetic data generation to prevent aliasing and ensure proper sampling.
  - Quick check question: What is the minimum sampling frequency required to accurately reconstruct a signal with maximum frequency f_max?

- **Concept: Probability distributions and noise modeling**
  - Why needed here: The paper designs synthetic noise generators using 15 different noise distributions across 5 categories to simulate realistic noise in time-series data.
  - Quick check question: What are the key differences between continuous and discrete probability distributions, and why would you choose one over the other for modeling time-series noise?

## Architecture Onboarding

- **Component map:** Data Synthesis Module -> Parameter Normalization Module -> Representation Extractor -> Reconstruction Module
- **Critical path:** Data Synthesis → Parameter Normalization → Representation Extractor Training → Real Data Feature Extraction → Downstream Task Performance
- **Design tradeoffs:**
  - Using non-deep learning synthesis vs. deep generative models: Simpler, more interpretable, but may miss complex patterns
  - Fixed vs. dynamic synthetic data generation: Static data is simpler but may lead to overfitting; dynamic generation provides more diversity but increases computational cost
  - Explicit vs. implicit feature extraction: Explicit decomposition provides interpretability but may be less flexible than learned representations
- **Failure signatures:**
  - Poor reconstruction performance on specific domains (e.g., Energy class in PatchTST experiments)
  - Degradation in representation extraction quality for data with non-stationary patterns
  - Overfitting to synthetic data patterns when validation loss increases with epochs
- **First 3 experiments:**
  1. Validate that synthetic data generation covers the full range of frequencies and patterns by visualizing the synthetic dataset's power spectrum
  2. Test representation extraction on synthetic data where ground truth parameters are known to verify the extractor's accuracy
  3. Compare reconstruction performance of models trained on synthetic data vs. real data on a small, diverse test set to confirm the generalization advantage

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the performance of InfoBoost's synthetic data compare to other data augmentation techniques when evaluated on real-world datasets?
  - Basis in paper: The paper mentions that InfoBoost's synthetic data outperforms models trained on real data, but it does not provide a direct comparison with other data augmentation techniques.
  - Why unresolved: The paper does not provide a comprehensive comparison with other data augmentation methods, leaving uncertainty about the relative effectiveness of InfoBoost.
  - What evidence would resolve it: Conducting experiments comparing InfoBoost's performance with other data augmentation techniques on a variety of real-world datasets would provide clarity on its relative effectiveness.

- **Open Question 2:** Can the universal feature extractor trained on InfoBoost's synthetic data effectively extract features from time-series data in domains not included in the original 35 publicly available datasets?
  - Basis in paper: The paper demonstrates the extractor's effectiveness on 35 datasets but does not test its generalizability to completely new domains.
  - Why unresolved: The study focuses on a specific set of datasets, which may not fully represent the diversity of all possible time-series domains.
  - What evidence would resolve it: Testing the feature extractor on datasets from entirely new domains, such as genomic data or financial market trends, would validate its universal applicability.

- **Open Question 3:** How does the performance of InfoBoost's synthetic data scale with increasing data complexity and noise levels in real-world applications?
  - Basis in paper: The paper mentions that InfoBoost handles noise interference and complex signal sources, but does not explore performance under varying levels of complexity and noise.
  - Why unresolved: The study does not provide insights into how InfoBoost's synthetic data performs under extreme conditions of data complexity and noise.
  - What evidence would resolve it: Conducting experiments with datasets of varying complexity and noise levels would help determine the robustness and scalability of InfoBoost's synthetic data.

## Limitations

- The synthetic data's ability to generalize to all time-series domains is uncertain, particularly for non-periodic dynamics and chaotic systems
- The paper lacks direct comparison with other data augmentation techniques to establish relative effectiveness
- The universal feature extractor's performance on completely new domains outside the 35 tested datasets remains unverified

## Confidence

- **High Confidence:** The technical approach of using parametric sine wave superposition, noise modeling, and trend generation is well-defined and reproducible
- **Medium Confidence:** The claim that synthetic data outperforms real data in reconstruction tasks is supported by experimental results, though specific domains where real data performs better need further investigation
- **Medium Confidence:** The universal feature extraction capability is promising but requires more extensive validation across diverse domains to establish its true universality

## Next Checks

1. **Domain Coverage Analysis:** Conduct a systematic evaluation of InfoBoost's performance across diverse time-series domains, including those with non-stationary patterns, chaotic dynamics, and long-range dependencies that may not be well-represented by sine wave combinations.

2. **Parameter Sensitivity Testing:** Perform ablation studies to determine how sensitive the reconstruction and feature extraction performance is to the specific parameters used in synthetic data generation (frequency ranges, noise distributions, trend types).

3. **Real-time Adaptation Evaluation:** Test whether InfoBoost's representation extractor maintains its performance when applied to streaming time-series data with evolving patterns that differ from the static synthetic training set.