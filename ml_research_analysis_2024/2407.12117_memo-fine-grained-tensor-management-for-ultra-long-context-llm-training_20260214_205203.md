---
ver: rpa2
title: 'MEMO: Fine-grained Tensor Management For Ultra-long Context LLM Training'
arxiv_id: '2407.12117'
source_url: https://arxiv.org/abs/2407.12117
tags:
- memory
- training
- sequence
- layer
- computation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training large language models
  (LLMs) with extremely long context lengths, which poses significant memory constraints
  due to quadratic scaling of computation and linear scaling of memory with sequence
  length. The proposed MEMO framework introduces fine-grained activation memory management,
  combining token-wise activation recomputation and swapping with a bi-level memory
  planning approach.
---

# MEMO: Fine-grained Tensor Management For Ultra-long Context LLM Training

## Quick Facts
- arXiv ID: 2407.12117
- Source URL: https://arxiv.org/abs/2407.12117
- Reference count: 40
- Primary result: MEMO achieves 2.42x and 2.26x MFU improvements over Megatron-LM and DeepSpeed for ultra-long context LLM training

## Executive Summary
MEMO introduces a fine-grained tensor management framework for training large language models with extremely long context lengths. The framework addresses the fundamental memory constraints that arise from quadratic computational scaling and linear memory scaling with sequence length. By implementing token-wise activation recomputation and swapping strategies, MEMO can efficiently train a 7B parameter LLM with 1 million sequence length using only 8 A800 GPUs while achieving an MFU of 52.30%.

## Method Summary
MEMO implements a bi-level memory planning approach that combines activation recomputation and swapping at the token level. The framework strategically offloads memory-intensive activations to CPU memory while reusing memory across transformer layers. This fine-grained management approach allows for dynamic allocation and reclamation of memory resources during training, enabling the efficient handling of ultra-long sequences that would typically exceed GPU memory capacity. The system optimizes both computation and memory usage through careful planning of when to recompute versus when to swap activations to external memory.

## Key Results
- Achieves 2.42x higher Model FLOPS Utilization compared to Megatron-LM
- Achieves 2.26x higher Model FLOPS Utilization compared to DeepSpeed
- Successfully trains a 7B LLM with 1M sequence length on only 8 A800 GPUs at 52.30% MFU

## Why This Works (Mechanism)
MEMO works by breaking down the traditional monolithic approach to activation memory management into fine-grained token-level operations. The framework uses a bi-level planning strategy where it first determines which activations can be recomputed on-the-fly versus which should be stored or swapped to external memory. At the token level, MEMO selectively recomputes activations that are memory-intensive but computationally cheaper to regenerate, while offloading only the most expensive-to-recompute tensors to CPU memory. This hybrid approach minimizes both memory usage and computational overhead by avoiding the extremes of either storing all activations or recomputing everything.

## Foundational Learning

**Token-wise activation recomputation**: Regenerating intermediate activations during the backward pass instead of storing them.
*Why needed*: Reduces memory footprint at the cost of additional computation.
*Quick check*: Verify that recomputation cost < storage cost for target activations.

**Memory swapping**: Moving tensor data between GPU and CPU memory during training.
*Why needed*: Leverages cheaper CPU memory to overcome GPU memory limitations.
*Quick check*: Ensure memory bandwidth between CPU and GPU is sufficient for swapping overhead.

**Bi-level memory planning**: Hierarchical decision-making for memory management across layers and tokens.
*Why needed*: Optimizes the trade-off between recomputation and swapping at multiple granularities.
*Quick check*: Validate that planning overhead doesn't negate memory savings.

**Model FLOPS Utilization (MFU)**: Metric measuring the fraction of theoretical computational capacity actually used during training.
*Why needed*: Indicates training efficiency and hardware utilization.
*Quick check*: Compare MFU across different memory management strategies under identical hardware.

## Architecture Onboarding

**Component map**: Memory Manager -> Token Scheduler -> Layer Planner -> Computation Engine -> GPU Memory <-> CPU Memory

**Critical path**: Token Scheduler receives layer dependencies → Layer Planner determines recomputation/swapping decisions → Memory Manager allocates/reclaims memory → Computation Engine executes with optimized memory access pattern

**Design tradeoffs**: The framework balances between recomputation overhead and swapping latency, with the optimal strategy varying based on sequence length, model size, and hardware characteristics. The fine-grained approach adds planning overhead but provides superior memory efficiency compared to coarse-grained alternatives.

**Failure signatures**: Memory thrashing occurs when swapping frequency is too high relative to computation speed; suboptimal MFU indicates poor planning decisions; out-of-memory errors suggest inadequate memory budget allocation.

**First 3 experiments**:
1. Baseline comparison: Run identical training workload with Megatron-LM and DeepSpeed to establish MFU baseline
2. Memory overhead analysis: Profile memory usage patterns with varying sequence lengths (256K → 1M tokens)
3. Scaling study: Test MEMO performance with different GPU counts (1 → 32) to identify optimal scaling configuration

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to specific hardware (A800 GPUs) and sequence lengths (1M tokens), limiting generalizability
- No extensive exploration of different batch sizes or model architectures beyond the tested 7B parameter model
- Overhead of fine-grained tensor operations and potential communication costs in distributed settings not fully characterized

## Confidence

MFU improvements over baselines: **High** - Well-supported by controlled experiments
Memory efficiency claims: **Medium** - Strong theoretical foundation but limited hardware diversity
Generalizability to other models/hardware: **Low** - Narrow experimental scope

## Next Checks

1. Evaluate MEMO's performance across different GPU architectures (H100, A100) and compare scaling behavior
2. Test the framework with various model sizes (1B-70B parameters) and different attention mechanisms (local, sliding window)
3. Conduct ablation studies to quantify the overhead of fine-grained tensor management and its impact on end-to-end training time