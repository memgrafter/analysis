---
ver: rpa2
title: 'Let''s Focus on Neuron: Neuron-Level Supervised Fine-tuning for Large Language
  Model'
arxiv_id: '2403.11621'
source_url: https://arxiv.org/abs/2403.11621
tags:
- neurons
- neft
- rank
- fine-tuning
- neuron
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Neuron-Level Fine-Tuning (NeFT), a novel
  approach that refines parameter training down to individual neurons for Large Language
  Models (LLMs). Traditional fine-tuning methods update all model parameters, while
  Parameter-Efficient Fine-Tuning (PEFT) methods operate at a macro scale like layers.
---

# Let's Focus on Neuron: Neuron-Level Supervised Fine-tuning for Large Language Model

## Quick Facts
- arXiv ID: 2403.11621
- Source URL: https://arxiv.org/abs/2403.11621
- Reference count: 22
- This paper introduces Neuron-Level Fine-Tuning (NeFT), a novel approach that refines parameter training down to individual neurons for Large Language Models (LLMs).

## Executive Summary
This paper introduces Neuron-Level Fine-Tuning (NeFT), a novel approach that refines parameter training down to individual neurons for Large Language Models (LLMs). Traditional fine-tuning methods update all model parameters, while Parameter-Efficient Fine-Tuning (PEFT) methods operate at a macro scale like layers. NeFT identifies sensitive neurons through similarity analysis between original and fine-tuned models, then selectively updates only these neurons.

Experiments on machine translation and summarization tasks using Llama-2-7b-chat demonstrate that NeFT consistently outperforms full-parameter fine-tuning and other PEFT methods. On average, NeFT achieves better BLEU scores for translation tasks and higher ROUGE scores for summarization tasks while training only 6-12% of total parameters. Compared to LoRA, NeFT shows superior or comparable performance across various parameter settings. The analysis reveals that NeFT identifies three types of neurons: strongly affected, suppressed, and indirectly affected, with strongly affected neurons showing the most significant rank differences in utilization patterns.

## Method Summary
NeFT operates by first performing a preliminary full-parameter fine-tuning on a small subset of data to establish a reference model. It then computes neuron-level similarity scores between the original and fine-tuned models using cosine similarity on neuron activation patterns. Based on these scores, NeFT identifies neurons that are most affected by the task and selectively updates only these neurons during the actual fine-tuning process. The method employs a threshold-based selection mechanism where neurons with similarity scores below a certain threshold are marked for updating. During the main training phase, only the parameters associated with these selected neurons are updated while all other parameters remain frozen. This approach achieves parameter efficiency by focusing computational resources on the most task-relevant components of the model while preserving the general knowledge encoded in unaffected neurons.

## Key Results
- NeFT achieves better BLEU scores for translation tasks and higher ROUGE scores for summarization tasks while training only 6-12% of total parameters
- Compared to LoRA, NeFT shows superior or comparable performance across various parameter settings
- The analysis reveals three types of neurons: strongly affected, suppressed, and indirectly affected, with strongly affected neurons showing the most significant rank differences in utilization patterns

## Why This Works (Mechanism)
NeFT works by identifying and updating only the neurons that are most sensitive to the target task, rather than updating all parameters or entire layers. The method leverages similarity analysis between original and fine-tuned models to detect which neurons have their activation patterns most altered by the fine-tuning process. By focusing updates on these task-sensitive neurons, NeFT preserves the general knowledge in unaffected neurons while efficiently adapting the model to specific downstream tasks. This selective updating approach reduces overfitting risk and computational overhead while maintaining or improving task performance compared to full fine-tuning or layer-level PEFT methods.

## Foundational Learning

1. **Cosine similarity in neural networks**: Measures the angle between activation vectors to quantify how much a neuron's behavior changes during fine-tuning. Why needed: To identify neurons whose activation patterns are most affected by task adaptation. Quick check: Verify similarity scores between -1 and 1, where lower scores indicate more significant changes.

2. **Neuron activation patterns**: The output values of neurons across different inputs, representing learned features. Why needed: To analyze which neurons are most relevant to specific tasks. Quick check: Ensure activation patterns are computed on representative validation data.

3. **Parameter-efficient fine-tuning (PEFT)**: Methods that update only a subset of model parameters to adapt LLMs to specific tasks. Why needed: To reduce computational cost and memory requirements compared to full fine-tuning. Quick check: Confirm that frozen parameters remain unchanged during training.

4. **Layer-wise parameter distribution**: Understanding how parameters are organized across different layers in transformer architectures. Why needed: To contextualize neuron-level updates within the broader model structure. Quick check: Verify that neuron selection respects layer boundaries and attention mechanisms.

5. **Rank difference analysis**: Comparing the relative importance of neurons before and after fine-tuning. Why needed: To identify which neurons become more or less important for the target task. Quick check: Ensure rank differences are computed consistently across all neurons.

6. **Transformer neuron structure**: Understanding how neurons are organized in multi-head attention and feed-forward layers. Why needed: To properly identify and update individual neurons rather than arbitrary parameter groups. Quick check: Verify neuron boundaries align with the actual computational graph.

## Architecture Onboarding

**Component Map**: Input Data -> Similarity Computation -> Neuron Selection -> Parameter Update (NeFT neurons only) -> Output

**Critical Path**: The similarity computation between original and fine-tuned models is the critical path, as it determines which neurons will be selected for updating. This step must be completed before the actual task-specific fine-tuning can begin.

**Design Tradeoffs**: NeFT trades off the overhead of initial similarity computation and neuron selection against the benefits of reduced parameter updates during main training. The method requires storing both the original and fine-tuned reference models temporarily, increasing memory requirements during the setup phase but reducing them during actual task training.

**Failure Signatures**: Poor performance may indicate inappropriate similarity thresholds (too conservative misses important neurons, too aggressive updates irrelevant ones), or that the reference fine-tuning was insufficient to capture task-relevant patterns. Catastrophic forgetting might occur if too many neurons are selected for updating.

**First Experiments**:
1. Compare NeFT performance against full fine-tuning and LoRA on a single translation task with varying parameter budgets (1%, 5%, 10% of total parameters)
2. Analyze the distribution of selected neuron types (strongly affected, suppressed, indirectly affected) across different layers to understand architectural patterns
3. Test sensitivity of results to similarity threshold variations by running experiments with thresholds at 0.1, 0.3, 0.5, and 0.7 intervals

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis of neuron types and their effects appears primarily empirical without theoretical grounding, making it difficult to predict performance across different model architectures or tasks
- The similarity-based neuron selection criterion, while effective in the reported experiments, lacks formal validation of its sensitivity to hyperparameter choices like similarity thresholds
- The claim of superior performance over full fine-tuning and LoRA methods is supported by average improvements but shows inconsistent results across specific parameter settings and tasks

## Confidence
- High: NeFT achieves better BLEU scores for translation tasks and higher ROUGE scores for summarization tasks while training only 6-12% of total parameters
- Medium: NeFT shows superior or comparable performance compared to LoRA across various parameter settings
- Low: The theoretical explanations of neuron behavior (strongly affected, suppressed, indirectly affected) remain somewhat speculative without clear mechanisms

## Next Checks
1. Test NeFT's performance across different model families (GPT, Mistral, etc.) and scales (13B, 70B parameters) to verify architectural generalizability
2. Conduct ablation studies varying the similarity threshold and neuron selection criteria to establish robustness boundaries
3. Implement computational efficiency analysis comparing the overhead of similarity computation against parameter savings during training and inference