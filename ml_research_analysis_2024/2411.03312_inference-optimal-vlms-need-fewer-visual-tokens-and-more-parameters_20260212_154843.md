---
ver: rpa2
title: Inference Optimal VLMs Need Fewer Visual Tokens and More Parameters
arxiv_id: '2411.03312'
source_url: https://arxiv.org/abs/2411.03312
tags:
- visual
- tokens
- token
- scaling
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the optimal trade-off between LLM size
  and visual token count for inference-efficient VLMs. The authors propose compute-optimal
  scaling laws showing that for visual reasoning tasks, inference is best with the
  largest LLM possible paired with minimal visual tokens (often just one).
---

# Inference Optimal VLMs Need Fewer Visual Tokens and More Parameters

## Quick Facts
- arXiv ID: 2411.03312
- Source URL: https://arxiv.org/abs/2411.03312
- Authors: Kevin Y. Li; Sachin Goyal; Joao D. Semedo; J. Zico Kolter
- Reference count: 34
- Primary result: For visual reasoning tasks, optimal VLMs use the largest LLM possible with minimal visual tokens (often just one)

## Executive Summary
This paper challenges the conventional wisdom in vision-language models (VLMs) by demonstrating that for visual reasoning tasks, the most inference-efficient approach is to use the largest LLM possible while minimizing visual token count—often to just one token. Through extensive experiments with varying LLM sizes (0.5B to 14B parameters) and token compression ratios, the authors establish compute-optimal scaling laws showing that performance degrades log-linearly with token reduction, but reducing tokens has less impact than reducing LLM size. The key insight is that larger LLMs can better handle reduced visual information, making extreme token compression compute-optimal for visual reasoning tasks.

## Method Summary
The authors investigate the trade-off between LLM size and visual token count using the LLaVA-Next framework with Qwen-1.5 family language models and CLIP ViT-L/14 visual encoder. They train VLMs with different configurations (varying LLM sizes from 0.5B to 14B and token counts from 1 to 576) on nine visual reasoning benchmarks. Token compression is implemented using convolutional cross-attention with optional query injection. Scaling laws are fitted using grid search to model performance as a function of LLM parameters and token count. Based on these findings, they introduce QueCC, a query-aware compression method that incorporates the user query into the compression algorithm to improve performance at extreme compression levels (1-4 tokens).

## Key Results
- For visual reasoning tasks, inference-optimal VLMs use the largest LLM possible with minimal visual tokens (often one token)
- Performance depends log-linearly on token count but 5× faster on LLM size, making it optimal to reduce visual tokens first
- QueCC, a query-aware compression method, outperforms baseline methods at extreme compression (1-4 tokens) on six of eight tested datasets
- The compute-optimal regime requires much higher token compression ratios than previously explored in the literature

## Why This Works (Mechanism)

### Mechanism 1
Visual reasoning tasks are more efficiently handled by larger LLMs with fewer visual tokens. Scaling laws show performance depends log-linearly on token count but 5× faster on LLM size, making it optimal to reduce visual tokens first. The marginal gain from reducing visual tokens outweighs the loss from fewer tokens. This may break down for tasks requiring high visual detail like OCR.

### Mechanism 2
High token compression (1-4 tokens) is compute-optimal for visual reasoning. Existing research focuses on moderate reductions (576→144), but scaling laws show extreme compression yields better inference efficiency. Performance degrades gracefully with extreme compression, so one token can suffice. This may not hold if the visual encoder cannot capture enough salient information in one token.

### Mechanism 3
Query-aware compression improves extreme token compression performance. Incorporating the user query into the compression algorithm helps identify key visual tokens when token count is very low. The user query contains information that can guide which visual tokens to retain. This may fail if the query is too generic or unrelated to the image content.

## Foundational Learning

- **Floating Point Operations (FLOPs) estimation**: The paper's core tradeoff analysis relies on FLOPs as the fixed inference budget metric. Quick check: If a 7B LLM processes 16 visual tokens, how does the FLOPs compare to a 4B LLM processing 36 visual tokens (assuming equal text tokens)?

- **Log-linear scaling relationships**: The paper models performance as a log-linear function of LLM parameters and token count. Quick check: If reducing tokens from 576 to 16 causes 20% performance loss, what fraction of that loss would occur when reducing from 16 to 4 tokens (assuming log-linear scaling)?

- **Vision projector design**: The paper builds on existing token compression techniques that use vision projectors to reduce visual tokens. Quick check: What is the difference between query-based projectors (like Q-Former) and linear projectors in how they handle visual tokens?

## Architecture Onboarding

- **Component map**: Vision encoder (CLIP ViT-L/14) → Vision projector → LLM backbone → Output
- **Critical path**: User query → LLM hidden states → Text embedding → Vision encoder processes image → Visual tokens → Text embedding + Visual tokens → Combined input to vision projector → Vision projector applies convolution + cross-attention → Compressed visual tokens → Compressed tokens + text tokens → LLM for generation
- **Design tradeoffs**: Extreme compression (1-4 tokens) vs. moderate compression (16-144 tokens); Query injection complexity vs. performance gains; Convolution kernel size vs. spatial coverage; Memory usage for caching text embeddings vs. real-time query adaptation
- **Failure signatures**: Performance collapse when token count drops below critical threshold for task type; Inconsistent results across different datasets; Overfitting to specific query patterns in training data
- **First 3 experiments**: 1) Validate scaling laws by training VLMs with varying LLM sizes and token counts on visual reasoning benchmarks; 2) Test query injection ablation by comparing performance with and without query embedding at 1 and 4 token compression levels; 3) Validate generalization by applying scaling laws to predict 14B VLM performance and comparing with actual results

## Open Questions the Paper Calls Out

**Open Question 1**: What is the precise relationship between token compression algorithms and task complexity for VLMs? The authors show visual reasoning tasks favor extreme token compression while OCR tasks require more tokens, but it's unclear whether this is a continuous spectrum or discrete classification.

**Open Question 2**: How do different token compression algorithms compare in extreme compression regimes (1-4 tokens) beyond the query-based approach presented? The paper only tests one additional algorithm (PruMerge) while noting most work focuses on modest reductions.

**Open Question 3**: What is the theoretical limit of how much visual information can be compressed while maintaining task performance? While the paper demonstrates practical effectiveness of extreme compression, it doesn't establish theoretical bounds or explain why such low token counts suffice.

**Open Question 4**: How does the optimal token-LLM trade-off change as VLMs scale to trillion-parameter models? The scaling laws were only validated up to 14B parameters, leaving uncertainty about whether trends continue at much larger scales.

## Limitations
- Scaling laws derived from only Qwen-1.5 LLMs and CLIP ViT-L/14 visual encoder, limiting generalizability
- Weak empirical validation of query-aware compression without absolute performance numbers
- Limited exploration of when and why the proposed approach breaks down for different task types
- Analysis assumes fixed FLOPs budget, not accounting for variable inference constraints like latency or memory

## Confidence

**High Confidence**:
- Visual reasoning tasks can be handled with fewer visual tokens when paired with larger LLMs
- Extreme token compression (1-4 tokens) is more efficient than moderate compression for visual reasoning
- Log-linear relationship between performance, LLM size, and token count exists within tested ranges

**Medium Confidence**:
- Specific scaling law parameters (α=0.077, β=0.015) for visual reasoning tasks
- Reducing visual tokens has less impact on performance than reducing LLM size (5× difference)
- QueCC's performance improvements over baseline compression methods

**Low Confidence**:
- One token is optimal for all visual reasoning tasks
- Generalizability of scaling laws to different LLM families or visual encoders
- Robustness of query-aware compression across diverse query types

## Next Checks

1. **Cross-architecture scaling validation**: Test whether scaling laws hold when using different LLM families (e.g., Llama, Mistral) and visual encoders (e.g., SigLIP, DINOv2) on the same visual reasoning benchmarks. Compare fitted α and β values to reported ones.

2. **Performance degradation analysis**: Systematically measure performance degradation as token count decreases from 16 to 1, verifying whether the log-linear model accurately predicts this degradation. Include confidence intervals and test consistency across different task types.

3. **Query embedding ablation study**: Perform controlled experiment removing query injection from QueCC to quantify exact contribution of query-awareness to performance gains. Test with both relevant and irrelevant queries to measure robustness.