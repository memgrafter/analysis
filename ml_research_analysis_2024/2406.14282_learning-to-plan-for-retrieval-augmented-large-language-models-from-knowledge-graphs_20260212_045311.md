---
ver: rpa2
title: Learning to Plan for Retrieval-Augmented Large Language Models from Knowledge
  Graphs
arxiv_id: '2406.14282'
source_url: https://arxiv.org/abs/2406.14282
tags:
- question
- answer
- query
- planning
- info
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces LPKG, a framework that enhances the planning
  ability of large language models (LLMs) for complex question answering by utilizing
  planning data derived from knowledge graphs (KGs). LPKG constructs training data
  by grounding predefined KG patterns, verbalizing them into natural language questions,
  and using these to fine-tune LLMs.
---

# Learning to Plan for Retrieval-Augmented Large Language Models from Knowledge Graphs

## Quick Facts
- arXiv ID: 2406.14282
- Source URL: https://arxiv.org/abs/2406.14282
- Reference count: 13
- Key outcome: KG-sourced planning data improves LLM planning for complex QA tasks

## Executive Summary
This paper introduces LPKG, a framework that enhances large language models' planning capabilities for complex question answering by leveraging knowledge graph patterns. The approach constructs training data by grounding predefined KG patterns, verbalizing them into natural language questions, and fine-tuning LLMs to generate step-by-step plans. The framework demonstrates superior performance on multiple datasets compared to baseline methods, particularly for complex logical questions involving multi-hop, intersection, and union reasoning.

## Method Summary
LPKG constructs planning data by defining KG patterns (e.g., 2p, 3p, 2i, 2u, 2ia, 3ia, 3u) representing different reasoning types, grounding these patterns in Wikidata15k to extract instances, and verbalizing them into natural language questions using GPT-4. This data is used to fine-tune LLMs to generate plans for complex questions. During inference, the fine-tuned models produce plans that are parsed and executed using external retrieval and QA tools to obtain final answers. The framework is evaluated on datasets including HotPotQA, 2WikiMultiHopQA, Bamboogle, and a newly proposed benchmark CLQA-Wiki.

## Key Results
- LPKG outperforms baseline methods on HotPotQA, 2WikiMultiHopQA, Bamboogle, and CLQA-Wiki
- Fine-tuned models show significant improvement over raw models and in-context learning baselines
- LPKG demonstrates superior performance on complex logical questions involving multi-hop, intersection, and union reasoning
- KG-sourced planning data proves more effective than distillation from teacher LLMs

## Why This Works (Mechanism)

### Mechanism 1
Knowledge graph patterns act as abstract plans for complex questions. KG patterns capture multi-step reasoning paths; grounding them produces concrete instances; verbalizing these instances yields sub-questions and complex questions that preserve the reasoning structure. Core assumption: KG patterns are sufficiently general to cover the types of reasoning needed in QA tasks.

### Mechanism 2
Fine-tuning LLMs on KG-sourced planning data improves their ability to generate plans for complex questions. The fine-tuned model learns to decompose questions into sub-questions in a structured way, following the patterns seen during training. Core assumption: The structure of the training data (sub-questions → complex question) is similar enough to real downstream questions.

### Mechanism 3
KG-sourced planning data is more effective than normal distillation from teacher LLMs. KG patterns encode richer and more accurate reasoning types than what can be distilled from a single teacher model's outputs. Core assumption: The reasoning diversity in KG patterns exceeds that available from teacher LLM distillation.

## Foundational Learning

- **Concept**: Knowledge Graph Patterns
  - Why needed here: Patterns define the types of reasoning steps the model will learn to generate.
  - Quick check question: Can you list the seven pattern types defined in the paper and describe what each represents?

- **Concept**: Fine-tuning vs In-Context Learning
  - Why needed here: The paper contrasts fine-tuning on KG data vs using raw models with ICL to show the benefit of training.
  - Quick check question: What is the key difference in performance between LPKG and ICLPKG on HotPotQA?

- **Concept**: Plan Parsing and Execution
  - Why needed here: The generated plans must be parsed and executed to obtain final answers; this bridges generation and retrieval.
  - Quick check question: In the plan parsing step, what functions are called when encountering a "Search" or "Get Answer" step?

## Architecture Onboarding

- **Component map**: KG Pattern Definition → Grounding → Verbalization → Template Filling → Fine-tuning → Plan Inference → Plan Parsing → Execution → Answer
- **Critical path**: Data Construction (patterns → instances → questions) → Fine-tuning (LLM + data) → Inference (LLM + downstream question) → Execution (parse + retrieve + answer)
- **Design tradeoffs**: Using fine-tuned smaller models vs. raw large models (trade-off between parameter count and specialized planning ability); Code-formatted plans vs. natural language (easier parsing but less human-readable)
- **Failure signatures**: Poor planning (incorrect sub-question decomposition or missing steps); Retrieval failures (low recall in retrieved documents); QA LLM failures (incorrect answer extraction from retrieved info)
- **First 3 experiments**:
  1. Compare LPKG(CodeQwen) vs. raw CodeQwen on a held-out multi-hop dataset.
  2. Ablation: Replace fine-tuned LLM with GPT-3.5 (ICLPKG) to measure benefit of fine-tuning.
  3. Error analysis: Categorize failures into planning, retrieval, and QA LLM errors on a small sample.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the distribution of question types in the training data affect the performance of the fine-tuned planning LLMs?
- Basis in paper: The paper mentions that various types of questions are mixed together uniformly for training, but does not explore the impact of different distributions of question types on the experimental results.
- Why unresolved: The paper does not conduct experiments to determine the optimal distribution of question types in the training data for maximizing the performance of the planning LLMs.
- What evidence would resolve it: Conducting experiments with different distributions of question types in the training data and comparing the performance of the fine-tuned planning LLMs on downstream tasks.

### Open Question 2
- Question: Can the framework handle implicit or unseen question types that are not explicitly defined in the current set of patterns?
- Basis in paper: The paper acknowledges that some question types in reality may be implicit or not included in the defined types, and suggests that future work could study planning methods for these types of unclear questions.
- Why unresolved: The current framework relies on predefined KG patterns, and its ability to handle implicit or unseen question types is not evaluated or demonstrated in the paper.
- What evidence would resolve it: Developing and evaluating the framework's performance on a dataset containing implicit or unseen question types, and comparing it to the performance on the explicitly defined question types.

### Open Question 3
- Question: What is the impact of the quality and diversity of the KG on the performance of the framework?
- Basis in paper: The framework relies on KGs for constructing planning data, but the paper does not discuss the impact of the quality and diversity of the KG on the performance of the framework.
- Why unresolved: The paper does not provide any analysis or experiments to determine how the quality and diversity of the KG affect the framework's ability to construct effective planning data and improve the planning capabilities of LLMs.
- What evidence would resolve it: Conducting experiments with KGs of varying quality and diversity, and comparing the performance of the framework in terms of the quality of the constructed planning data and the improvement in the planning capabilities of LLMs.

## Limitations
- Pattern Coverage Gap: The seven KG patterns may not capture all complex reasoning types encountered in real-world QA tasks
- No Corpus Validation: Advantages over teacher LLM distillation lack corpus-level validation or qualitative analysis
- Manual Annotation Bottleneck: Defining and grounding patterns requires significant human effort, particularly for new domains

## Confidence
- High Confidence: Experimental results demonstrating LPKG's superiority on tested datasets
- Medium Confidence: Claim that KG patterns act as abstract plans for complex questions
- Medium Confidence: Assertion that KG-sourced planning data is more effective than teacher LLM distillation

## Next Checks
1. **Pattern Coverage Analysis**: Analyze a diverse sample of complex questions from multiple domains to assess whether the seven KG patterns cover the full spectrum of reasoning types. Identify gaps and propose new patterns if needed.
2. **Cross-Dataset Generalization**: Evaluate LPKG on a broader range of QA datasets, particularly those involving temporal reasoning, spatial reasoning, or other complex logical constructs not explicitly covered by the current KG patterns.
3. **Qualitative Comparison of Reasoning Diversity**: Conduct a qualitative analysis comparing the reasoning diversity captured by KG patterns versus teacher LLM distillation. This could involve annotating a sample of generated plans and sub-questions for complexity and correctness.