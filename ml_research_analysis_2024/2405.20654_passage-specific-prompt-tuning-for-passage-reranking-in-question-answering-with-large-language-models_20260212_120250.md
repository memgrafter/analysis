---
ver: rpa2
title: Passage-specific Prompt Tuning for Passage Reranking in Question Answering
  with Large Language Models
arxiv_id: '2405.20654'
source_url: https://arxiv.org/abs/2405.20654
tags:
- prompt
- passage
- question
- passage-specific
- pspt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a parameter-efficient prompt tuning method,
  PSPT, for passage reranking in open-domain question answering using large language
  models. The core idea is to learn a passage-specific soft prompt that incorporates
  passage-specific knowledge, enhancing the reranking capabilities of LLMs.
---

# Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models

## Quick Facts
- arXiv ID: 2405.20654
- Source URL: https://arxiv.org/abs/2405.20654
- Reference count: 40
- Key outcome: PSPT achieves Recall@10 of 36.89% and Hit@10 of 62.24% on Natural Questions dataset

## Executive Summary
This paper introduces Passage-Specific Prompt Tuning (PSPT), a parameter-efficient method for passage reranking in open-domain question answering using large language models. The approach learns passage-specific soft prompts that incorporate passage knowledge to enhance LLM reranking capabilities. PSPT fine-tunes these soft prompts on limited question-passage relevance pairs, then ranks passages based on the log-likelihood of generating questions conditioned on each passage and learned prompt.

## Method Summary
PSPT employs a novel parameter-efficient approach where soft prompts are learned for individual passages rather than shared across all passages. The method involves fine-tuning these learnable soft prompts using a small set of question-passage relevance pairs. During reranking, passages are scored based on the log-likelihood of generating the question given each passage and its corresponding learned soft prompt. This approach leverages the generative capabilities of LLMs while maintaining parameter efficiency through soft prompt tuning rather than full model fine-tuning.

## Key Results
- PSPT achieves Recall@10 of 36.89% and Hit@10 of 62.24% on Natural Questions dataset
- Consistently outperforms both basic retrievers and baseline models across three datasets
- Demonstrates improvements over UPR model across both unsupervised and supervised retrievers

## Why This Works (Mechanism)
PSPT works by learning passage-specific soft prompts that encode passage-specific knowledge, allowing the LLM to better assess relevance between questions and passages. By conditioning the generation of questions on both passages and learned prompts, the model can capture nuanced relationships that generic prompts might miss. The parameter-efficient nature of soft prompt tuning enables effective adaptation without the computational cost of full fine-tuning.

## Foundational Learning
- **Passage-specific soft prompts**: Why needed - to encode passage-specific knowledge for better relevance assessment; Quick check - verify prompts can be learned efficiently
- **Log-likelihood scoring**: Why needed - provides principled ranking mechanism; Quick check - ensure scores correlate with human judgments
- **Parameter-efficient tuning**: Why needed - reduces computational overhead while maintaining performance; Quick check - compare parameter counts to full fine-tuning
- **Question-passage relevance pairs**: Why needed - supervision signal for prompt learning; Quick check - validate quality of training pairs
- **Generative LLM conditioning**: Why needed - leverages LLM's ability to model complex relationships; Quick check - test different conditioning strategies
- **Retrieval-augmented QA pipeline**: Why needed - situates reranking in practical QA context; Quick check - measure end-to-end QA performance

## Architecture Onboarding

**Component Map**
Retriever -> PSPT Rancer -> Final Answer Generator

**Critical Path**
Question + Retrieved Passages -> PSPT Scoring -> Ranked Passages -> Answer Generation

**Design Tradeoffs**
- Soft prompts vs. full fine-tuning: PSPT trades some potential performance for parameter efficiency
- Passage-specific vs. shared prompts: Individual prompts capture more nuance but require more parameters
- Log-likelihood vs. alternative scoring: Probabilistic approach is principled but may be computationally intensive

**Failure Signatures**
- Poor ranking when passages lack sufficient distinctive features
- Degradation when training data distribution differs significantly from test data
- Performance bottlenecks with very large document collections

**Three First Experiments**
1. Ablation study removing passage-specific components to measure their contribution
2. Comparison of different prompt sizes to find optimal balance between efficiency and performance
3. Testing on datasets with varying question complexity to assess robustness

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to three publicly available datasets, may not capture full diversity of real-world scenarios
- Computational overhead of learned soft prompts during inference not thoroughly analyzed
- Reliance on limited relevance pairs raises questions about scalability to new domains

## Confidence
- Technical feasibility of PSPT method: **High**
- Broader applicability across domains: **Medium**
- Efficiency for large-scale applications: **Medium**

## Next Checks
1. Evaluate PSPT on more diverse QA datasets, including those requiring complex reasoning
2. Conduct comprehensive analysis of computational overhead and memory requirements during inference
3. Test PSPT scalability by evaluating performance on datasets from different domains with varying annotation levels