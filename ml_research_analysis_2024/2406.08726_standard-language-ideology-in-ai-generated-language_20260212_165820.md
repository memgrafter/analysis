---
ver: rpa2
title: Standard Language Ideology in AI-Generated Language
arxiv_id: '2406.08726'
source_url: https://arxiv.org/abs/2406.08726
tags:
- language
- standard
- varieties
- linguistic
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a taxonomy of open problems illustrating how
  standard language ideology manifests in AI-generated language. Through a multidisciplinary
  literature review, the authors identify how large language models (LLMs) reproduce
  linguistic hierarchies by privileging Standard American English (SAE) and producing
  lower-quality outputs for minoritized language varieties.
---

# Standard Language Ideology in AI-Generated Language

## Quick Facts
- arXiv ID: 2406.08726
- Source URL: https://arxiv.org/abs/2406.08726
- Authors: Genevieve Smith; Eve Fleisig; Madeline Bossi; Ishita Rustagi; Xavier Yin
- Reference count: 40
- Primary result: Taxonomy of open problems illustrating how standard language ideology manifests in AI-generated language, with recommendations for structural change

## Executive Summary
This paper presents a taxonomy of open problems illustrating how standard language ideology manifests in AI-generated language. Through a multidisciplinary literature review, the authors identify how large language models (LLMs) reproduce linguistic hierarchies by privileging Standard American English (SAE) and producing lower-quality outputs for minoritized language varieties. The taxonomy links specific technical behaviors—such as defaulting to SAE, stereotyping, and appropriation—to broader societal harms including discrimination and erasure. Rather than proposing narrow technical fixes, the paper offers three structural recommendations: expanding evaluation metrics to include language variety harms, supporting community-led dataset and model development, and implementing participatory processes that center marginalized communities while avoiding extractive data practices.

## Method Summary
The paper employs a multidisciplinary literature review approach, synthesizing research from sociolinguistics, natural language processing, and human-computer interaction to develop a taxonomy of open problems. The authors analyze how standard language ideology manifests in LLM outputs through three technical behaviors: defaulting to SAE, stereotyping non-standard varieties, and appropriation of linguistic features. They connect these technical behaviors to broader societal harms and propose structural interventions rather than purely technical solutions.

## Key Results
- LLMs consistently produce lower-quality outputs for minoritized language varieties compared to Standard American English, reflecting training data composition biases
- Standard language ideology in AI outputs reinforces existing linguistic hierarchies through automation bias and repeated exposure
- Corporate control of major LLMs creates structural barriers to equitable language technology development

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language model performance disparities between English varieties stem from training data composition rather than linguistic complexity.
- Mechanism: LLMs trained primarily on SAE text learn associations between linguistic features and semantic meaning that don't transfer to non-standard varieties, causing degraded performance when processing non-SAE inputs.
- Core assumption: Performance differences reflect data distribution bias rather than inherent language difficulty.
- Evidence anchors:
  - [section 2.2]: "Internet data overwhelmingly represents English... Non-standard varieties of English are disproportionately removed from scraped Internet data when scraped pages are filtered using blocklists"
  - [section 4]: "Outputs have lower quality of service for minoritized varieties... may not comprehend the user's prompt as well, and result in an incorrect or unhelpful response"
  - [corpus]: Multiple papers show performance gaps for African American English and other varieties, with no evidence of linguistic complexity causing the differences
- Break condition: If controlled studies show performance gaps persist when trained on balanced data across varieties, the data composition hypothesis would be falsified.

### Mechanism 2
- Claim: Standard language ideology in AI outputs reinforces existing linguistic hierarchies through repeated exposure.
- Mechanism: Users who repeatedly encounter SAE as the default output begin to internalize the belief that SAE is more appropriate or correct, amplifying linguistic bias through automation bias and perceived authority of AI systems.
- Core assumption: Repeated exposure to SAE-default outputs shifts user perceptions about language appropriateness.
- Evidence anchors:
  - [section 4]: "default production of 'standard' language varieties reinforces 'correct' ways of communicating... could have a cascading effect that impacts people's own linguistic biases"
  - [section 4]: "automation bias... humans can have greater trust and overconfidence in AI... could exacerbate the idea that 'standard' languages are more 'correct' ways of speaking"
  - [corpus]: Empirical work on linguistic bias shows that exposure to biased outputs can reinforce discriminatory attitudes
- Break condition: If longitudinal studies show no change in users' language attitudes after prolonged exposure to biased AI outputs, the exposure-reinforcement mechanism would be challenged.

### Mechanism 3
- Claim: Corporate control of major LLMs creates structural barriers to equitable language technology development.
- Mechanism: Decision-making concentrated in US-based companies with predominantly white, male leadership results in systems optimized for dominant linguistic norms, limiting resources for minoritized language communities and reinforcing existing power structures.
- Core assumption: Demographics and incentives of decision-makers shape system design priorities and resource allocation.
- Evidence anchors:
  - [section 2.2]: "Leading industry developers of LLMs... are largely headquartered in the United States, with employees and corporate leadership that skew White and male"
  - [section 5]: "there is no clear 'correct' behavior... Those who hold decision-making power regarding the largest generative AI language technologies are their for-profit corporate owners with incentive structures tied to delivering value to shareholders"
  - [corpus]: Research documents demographic disparities in AI industry and their impact on system design
- Break condition: If diverse teams at major companies consistently produce equitable language systems despite profit incentives, the structural barrier hypothesis would need revision.

## Foundational Learning

- Concept: Standard Language Ideology
  - Why needed here: Understanding this concept is essential for recognizing how linguistic hierarchies are constructed and maintained in AI systems
  - Quick check question: Why is Standard American English considered "standard" rather than simply being one dialect among many?

- Concept: Linguistic Discrimination
  - Why needed here: This concept explains the real-world harms that result from biased AI outputs, connecting technical issues to social justice
  - Quick check question: How can language discrimination serve as a proxy for other forms of discrimination like racism or xenophobia?

- Concept: Automation Bias
  - Why needed here: Understanding this psychological tendency helps explain why users might over-rely on biased AI outputs and internalize their assumptions
  - Quick check question: What cognitive tendency might cause users to place undue trust in AI outputs despite known biases?

## Architecture Onboarding

- Component map: Training data pipeline → Model architecture → Fine-tuning procedures → Evaluation framework → Deployment interface
- Critical path: Data collection and curation → Model training → Bias evaluation → Community feedback → Iterative improvement
- Design tradeoffs: Balancing performance across varieties vs. optimizing for majority vs. building separate models for different communities
- Failure signatures: Performance gaps between varieties, stereotyped outputs for non-standard varieties, refusal to process certain linguistic features
- First 3 experiments:
  1. Test model performance on controlled prompts in SAE vs. AAE vs. other varieties to quantify disparities
  2. Evaluate outputs for stereotypical content when prompted to generate text in different varieties
  3. Test community responses to model outputs to assess perceived appropriateness and harm

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms or features in LLM training data contribute to the disproportionate stereotyping of minoritized language varieties?
- Basis in paper: [explicit] The paper notes that models produce stereotyped versions of minoritized varieties and carry demeaning content, but does not identify specific training data features responsible.
- Why unresolved: While the paper identifies the problem, it does not analyze the composition of training data to determine which patterns (e.g., overrepresentation of certain dialects, specific types of web content) drive these stereotypes.
- What evidence would resolve it: Detailed corpus analysis showing correlations between specific linguistic features, content types, or demographic markers in training data and increased stereotyping in outputs.

### Open Question 2
- Question: How do speakers of minoritized language varieties experience and cope with digital code-switching in practice, and what are the measurable psychological impacts?
- Basis in paper: [explicit] The paper mentions digital code-switching as a consequence of lower-quality outputs for minoritized varieties and suggests potential psychological tolls.
- Why unresolved: The paper hypothesizes about psychological impacts but does not present empirical studies measuring actual experiences or mental health outcomes of speakers who must code-switch digitally.
- What evidence would resolve it: Longitudinal studies or surveys measuring psychological well-being, stress levels, and identity impacts among speakers who regularly use code-switching with AI tools.

### Open Question 3
- Question: What participatory governance structures would most effectively center marginalized communities in LLM development while respecting their sovereignty and preferences for data use?
- Basis in paper: [explicit] The paper recommends participatory processes and community ownership but does not specify which governance models work best.
- Why unresolved: While the paper advocates for community involvement, it does not evaluate or compare different participatory frameworks or their outcomes in practice.
- What evidence would resolve it: Case studies comparing outcomes of different participatory models (co-creation, consultation, shared governance) across multiple language communities.

## Limitations
- The analysis relies heavily on existing literature rather than original empirical testing of LLMs
- The three structural recommendations lack specific implementation roadmaps or success metrics
- Direct validation of proposed mechanisms would require controlled experiments not conducted in this study

## Confidence
- **High confidence**: The documentation of performance disparities between SAE and non-standard English varieties is well-supported by existing research and represents an observable phenomenon across multiple studies.
- **Medium confidence**: The connection between training data composition and performance gaps is plausible but requires more controlled experiments to definitively establish causation rather than correlation.
- **Medium confidence**: The societal harm mechanisms linking AI outputs to reinforcement of linguistic hierarchies are theoretically grounded but would benefit from longitudinal studies measuring actual attitude changes in users.

## Next Checks
1. Conduct controlled experiments comparing LLM performance on identical semantic content expressed in SAE versus non-standard varieties, controlling for vocabulary difficulty and grammatical complexity to isolate variety effects from other factors.

2. Implement the proposed participatory design process with at least two minoritized language communities, documenting both the technical outcomes and the power dynamics throughout the collaboration to evaluate whether the process avoids extractive data practices.

3. Develop and test evaluation metrics specifically designed to capture language variety harms, then apply these metrics to current state-of-the-art models to establish baseline performance and identify priority areas for improvement.