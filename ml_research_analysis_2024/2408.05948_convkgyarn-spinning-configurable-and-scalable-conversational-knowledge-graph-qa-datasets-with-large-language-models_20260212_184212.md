---
ver: rpa2
title: 'ConvKGYarn: Spinning Configurable and Scalable Conversational Knowledge Graph
  QA datasets with Large Language Models'
arxiv_id: '2408.05948'
source_url: https://arxiv.org/abs/2408.05948
tags:
- conversation
- turn
- each
- conversational
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ConvKGYarn is a scalable framework for generating conversational
  knowledge graph QA datasets using large language models. It leverages structured
  knowledge graphs to produce configurable datasets that simulate diverse user interaction
  modes including text and voice.
---

# ConvKGYarn: Spinning Configurable and Scalable Conversational Knowledge Graph QA datasets with Large Language Models

## Quick Facts
- arXiv ID: 2408.05948
- Source URL: https://arxiv.org/abs/2408.05948
- Reference count: 33
- ConvKGYarn generates high-quality conversational QA datasets that match or exceed human-curated data in fluency, relevance, diversity, and grammar

## Executive Summary
ConvKGYarn is a scalable framework that uses large language models to generate configurable conversational knowledge graph QA datasets. The system leverages structured knowledge graphs to produce datasets that simulate diverse user interaction modes including text and voice, with configurable linguistic phenomena such as deixis, disfluencies, and typos. Human evaluation shows the generated conversations achieve parity or superior quality compared to human-curated datasets across multiple quality dimensions.

## Method Summary
ConvKGYarn extracts predicates from knowledge graphs and uses an LLM predicate selector to identify conversationally useful properties while filtering out generic or irrelevant ones. The system then generates synthetic question templates for different interaction types and linguistic phenomena, filling them with entity-specific facts. Related entities are identified using KG embeddings to enrich conversations with diverse but relevant information. The framework produces datasets that can be configured for different interaction modes and linguistic characteristics, enabling large-scale generation of high-quality conversational QA data.

## Key Results
- Human evaluation shows ConvKGYarn datasets achieve parity or superior quality compared to human-curated datasets across fluency, relevance, diversity, and grammar metrics
- LLM-based evaluation demonstrates improved performance on ConvKGYarn datasets compared to existing benchmarks, though fact recall remains challenging
- The framework enables creation of large-scale conversational QA datasets with configurable linguistic phenomena and interaction modes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM predicate selector improves dataset quality by filtering irrelevant or generic predicates
- Mechanism: Uses structured prompt to select only predicates that are "interesting" for conversational QA, excluding generic properties and identifiers
- Core assumption: LLMs can reliably distinguish between conversationally useful and useless predicates when given explicit criteria
- Evidence anchors: Abstract states "LLM predicate selector...selecting only the most interesting predicates"; section 2.3 describes prompt instructions for excluding generic predicates
- Break condition: If LLM lacks semantic understanding of predicate types or prompt fails to capture what makes predicates "interesting"

### Mechanism 2
- Claim: Templated question generation ensures linguistic diversity and configurability across interaction modes
- Mechanism: Generates synthetic question templates for different interaction types and linguistic phenomena, then fills with entity-specific facts
- Core assumption: Templating can capture essential linguistic features of different interaction modes while preserving naturalness
- Evidence anchors: Abstract mentions "templated approach that incorporates placeholder entity type"; section 2.6 describes generating three question variants per fact and phenomenon
- Break condition: If templates are too rigid or fail to capture nuances of natural conversation, questions may sound artificial

### Mechanism 3
- Claim: Related entity generator enriches conversations with diverse but relevant information beyond immediate KG neighbors
- Mechanism: Identifies entities linked through KG embedding similarity to provide relevant follow-up content for richer conversational contexts
- Core assumption: KG embeddings capture semantic relatedness effectively enough that related entities provide relevant follow-up content
- Evidence anchors: Section 2.4 describes identifying entities linked to primary entity through embeddings; section 2.6 mentions using most-similar related entity for popular Person entities
- Break condition: If KG embeddings poorly capture semantic relatedness or selected related entities introduce irrelevant information

## Foundational Learning

- Concept: Knowledge Graph structure and terminology (entities, predicates, facts, qualifiers)
  - Why needed here: ConvKGYarn operates directly on KG data, and understanding these concepts is essential for implementing extraction and generation components
  - Quick check question: What's the difference between a simple fact and a qualified fact in the ConvKGYarn framework?

- Concept: Prompt engineering for LLMs
  - Why needed here: System relies heavily on carefully crafted prompts for predicate selection and question generation
  - Quick check question: What specific instructions does ConvKGYarn include in LLM prompts to avoid generic or irrelevant predicate selection?

- Concept: Linguistic phenomena in conversational interfaces (deixis, disfluencies, typos)
  - Why needed here: System explicitly generates questions with these phenomena to simulate different interaction modes
  - Quick check question: How does ConvKGYarn's approach to typos differ between voice and text interaction settings?

## Architecture Onboarding

- Component map: KG Predicate Extraction → LLM Predicate Selector → Related Entity Generator → Fact Extraction → Synthetic Question Template Generation → Conversation Instance Creation
- Critical path: KG Predicate Extraction → LLM Predicate Selector → Synthetic Question Template Generation → Conversation Instance Creation
- Design tradeoffs:
  - Templating vs. fully generative approaches: Templating provides more control and consistency but may limit naturalness
  - Related entity inclusion: Adds diversity but increases complexity and potential noise
  - LLM-based selection: Leverages semantic understanding but adds cost and dependency on external models
- Failure signatures:
  - Poor predicate selection → Many irrelevant or generic questions in final dataset
  - Template issues → Questions that don't sound natural or are grammatically incorrect
  - Related entity problems → Conversations that drift off-topic or include confusing references
- First 3 experiments:
  1. Test LLM predicate selector with small KG subset and manually verify selected predicates are conversationally useful
  2. Generate questions for single entity type and interaction mode, conduct human evaluation for fluency and relevance
  3. Create conversation instance with and without related entities, compare diversity and coherence metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does ConvKGYarn's synthetic data generation method outperform human-curated datasets across all linguistic phenomena in conversational QA?
- Basis in paper: Explicit - paper shows parity or outperforms human-curated datasets in overall quality but didn't specifically compare across all linguistic phenomena
- Why unresolved: Evaluation focused on overall dataset quality without analyzing how different linguistic phenomena impact performance relative to human-curated data
- What evidence would resolve it: Controlled study comparing ConvKGYarn-generated data against human-curated data specifically for deixis, disfluency, and typo phenomena

### Open Question 2
- Question: What is the optimal balance between related entity inclusion and conversation coherence in ConvKGYarn-generated datasets?
- Basis in paper: Inferred - paper shows related entities improve relevance and diversity but doesn't specify optimal inclusion levels or methods for maintaining coherence
- Why unresolved: While related entities enhance diversity, paper doesn't address potential negative impacts on conversation coherence or provide guidelines for optimal inclusion
- What evidence would resolve it: Systematic evaluation across varying levels of related entity inclusion, measuring both diversity gains and coherence impacts

### Open Question 3
- Question: How do different LLM architectures perform on ConvKGYarn-generated datasets compared to traditional QA benchmarks?
- Basis in paper: Explicit - paper evaluates GPT-3.5 and GPT-4 but doesn't compare performance against traditional QA benchmarks or other LLM architectures
- Why unresolved: Limited evaluation scope - only two LLM variants tested without benchmark comparison
- What evidence would resolve it: Comprehensive benchmarking of multiple LLM architectures on ConvKGYarn data versus traditional QA datasets

## Limitations

- Evaluation relies heavily on GPT-4 as automated judge, which may not fully capture human preferences for conversational quality
- Templated question generation approach may produce questions that lack naturalness and diversity of truly conversational interactions
- Limited validation of KG embedding-based related entity selection effectiveness in consistently identifying relevant entities

## Confidence

- **High confidence**: The scalability and configurability of the ConvKGYarn framework - technical approach for generating conversational QA datasets using LLMs is well-specified and demonstrably produces large-scale datasets
- **Medium confidence**: Quality improvements over existing datasets - human evaluation shows positive results but reliance on GPT-4 as judge and limited comparison to human-curated datasets creates uncertainty
- **Low confidence**: Effectiveness of KG embedding-based related entity selection - paper provides limited validation that this approach consistently identifies relevant entities for conversational enrichment

## Next Checks

1. Conduct blind human evaluation comparing ConvKGYarn-generated conversations against both human-curated datasets and LLM-generated datasets without KG grounding, measuring not just quality metrics but also user preference and engagement

2. Test KG embedding-based related entity selection independently by sampling 100 related entities and having human judges rate their relevance and usefulness for follow-up questions in conversational contexts

3. Implement an ablation study removing LLM predicate selector and templated generation, instead using fully generative LLM approach, to quantify specific contribution of ConvKGYarn's design choices to final dataset quality