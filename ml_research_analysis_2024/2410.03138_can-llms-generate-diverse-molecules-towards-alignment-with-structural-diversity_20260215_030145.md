---
ver: rpa2
title: Can LLMs Generate Diverse Molecules? Towards Alignment with Structural Diversity
arxiv_id: '2410.03138'
source_url: https://arxiv.org/abs/2410.03138
tags:
- molecules
- diverse
- llms
- molecule
- molecular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of generating structurally
  diverse molecules using large language models (LLMs), which is critical for drug
  discovery to increase the chances of finding viable candidates. The authors propose
  a two-stage fine-tuning approach: first, supervised fine-tuning to repurpose LLMs
  to autoregressively generate sequences of molecules, and second, reinforcement learning
  to maximize structural diversity within generated molecules.'
---

# Can LLMs Generate Diverse Molecules? Towards Alignment with Structural Diversity

## Quick Facts
- arXiv ID: 2410.03138
- Source URL: https://arxiv.org/abs/2410.03138
- Reference count: 27
- Key outcome: Two-stage fine-tuning approach achieves 14.35 NCirclesh=0.65 vs 6.16 for best baseline (BioT5+) on ChEBI-20 dataset

## Executive Summary
This paper addresses the challenge of generating structurally diverse molecules using large language models (LLMs), which is critical for drug discovery to increase the chances of finding viable candidates. The authors propose a two-stage fine-tuning approach: first, supervised fine-tuning to repurpose LLMs to autoregressively generate sequences of molecules, and second, reinforcement learning to maximize structural diversity within generated molecules. The method outperforms existing decoding schemes (diverse beam search, contrastive beam search) and other LLMs on the ChEBI-20 dataset, achieving 14.35 NCirclesh=0.65 (a metric measuring both quality and diversity) compared to 6.16 for the best baseline (BioT5+). The approach consistently improves molecular diversity when applied to generalist LLMs like DrugAssist, and demonstrates better performance with respect to both the number of generations and time costs compared to decoding schemes.

## Method Summary
The proposed method uses a two-stage fine-tuning approach on pre-trained LLMs. First, supervised fine-tuning repurposes the LLM to autoregressively generate sequences of molecules using beam search to collect and filter molecules from the pre-trained model. Second, reinforcement learning with multi-stage generation maximizes structural diversity by conditioning each new molecule generation on previously generated molecules, using proximal policy optimization (PPO) to maximize diversity rewards computed via Tanimoto similarity on molecular fingerprints. The method generates multiple molecules per prompt and evaluates diversity using metrics like NCirclesh=0.65, which balances quality and diversity.

## Key Results
- Achieves 14.35 NCirclesh=0.65 compared to 6.16 for best baseline (BioT5+)
- Outperforms diverse beam search and contrastive beam search decoding schemes
- Consistently improves molecular diversity when applied to generalist LLMs like DrugAssist
- Better performance with respect to both the number of generations and time costs compared to decoding schemes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised fine-tuning alone is insufficient for structural diversity because the collected molecule set from pre-trained LLMs lacks sufficient structural variation.
- Mechanism: The supervised fine-tuning stage collects molecules using beam search from the pre-trained LLM, filters them, and trains the model to generate sequences autoregressively. However, this stage does not guarantee molecular structural diversity because the initial samples may already be too similar.
- Core assumption: The diversity of molecules collected via beam search from a pre-trained LLM is limited and does not span the chemical space effectively.
- Evidence anchors:
  - [abstract]: "However, this stage does not necessarily incorporate molecular diversity, as the training may not involve sufficiently distinct molecules"
  - [section 3.1]: "The policy πSFT does not necessarily incorporate a molecular structural diversity, as the set of molecules {mi}K i=1 collected from πpre may insufficiently involve diverse molecular structures"
  - [corpus]: Weak - the corpus contains related papers but none specifically address the insufficiency of supervised fine-tuning for molecular diversity.
- Break condition: If the pre-trained LLM already generates highly diverse molecules, supervised fine-tuning may be sufficient without RL.

### Mechanism 2
- Claim: Reinforcement learning with multi-stage molecule generation solves the credit assignment problem that single-stage RL faces when maximizing diversity.
- Mechanism: Instead of treating the entire sequence as one unit, the approach generates each molecule conditioned on previously generated ones, applying RL at each stage to maximize the diversity reward with respect to earlier molecules. This provides clear credit assignment for diversity improvements.
- Core assumption: The credit assignment problem in single-stage RL prevents effective learning of diversity when optimizing over long sequences.
- Evidence anchors:
  - [section 3.2]: "conventional sequence-wise reinforcement learning (Ouyang et al., 2022) suffers from the credit assignment problem (Zhou et al., 2024): the challenges in identifying and promoting the generation of molecules responsible for increasing diversity"
  - [section 4.4]: "One can see that the multi-stage setting significantly outperforms the single-stage setting. We hypothesize that this result stems from credit assignment issues in the single-stage setting."
  - [corpus]: Weak - no corpus evidence directly addresses credit assignment in molecular diversity RL.
- Break condition: If the diversity reward can be decomposed in a way that allows single-stage RL to work effectively, multi-stage may be unnecessary.

### Mechanism 3
- Claim: Conditioning each new molecule generation on previously generated molecules forces the model to explore structurally different regions of chemical space.
- Mechanism: By generating molecule k conditioned on M1:k-1, the model must consider what structural features are already represented and generate something different, creating an exploration-exploitation dynamic that promotes diversity.
- Core assumption: LLMs can learn to condition generation on previous outputs in a way that systematically increases diversity.
- Evidence anchors:
  - [abstract]: "By enabling the LLMs to generate a new molecule conditioned on previously generated molecules, we expect the LLMs to learn to enhance the structural diversity between the generated molecules."
  - [section 3.2]: "Each stage corresponds to generating a molecule mk conditioned on a sequence of previously generated molecules M1:k−1."
  - [corpus]: Weak - the corpus contains related molecular generation work but lacks specific evidence about conditioning-based diversity enhancement.
- Break condition: If the model learns to ignore previous molecules and generate similar structures anyway, conditioning will not promote diversity.

## Foundational Learning

- Concept: Molecular representation and similarity (SMILES, Tanimoto similarity)
  - Why needed here: The entire approach relies on representing molecules as strings and measuring structural diversity using Tanimoto similarity on molecular fingerprints
  - Quick check question: What does a Tanimoto similarity of 0.8 between two molecules indicate about their structural relationship?

- Concept: Reinforcement learning and policy optimization (PPO)
  - Why needed here: The RL stage uses proximal policy optimization to maximize diversity rewards, requiring understanding of policy gradients and trust region methods
  - Quick check question: How does PPO's clipped objective help prevent destructive policy updates during fine-tuning?

- Concept: Sequence generation and decoding strategies (beam search, nucleus sampling)
  - Why needed here: The paper compares its approach against various decoding schemes, requiring understanding of how these methods affect output diversity
  - Quick check question: What is the key difference between diverse beam search and contrastive beam search in terms of diversity promotion?

## Architecture Onboarding

- Component map: Pre-trained LLM -> Supervised Fine-tuning (collect molecules -> filter -> train on sequences) -> Reinforcement Learning (multi-stage generation with diversity rewards) -> Output diverse molecules
- Critical path: Pre-trained LLM -> Supervised Fine-tuning -> Reinforcement Learning -> Output diverse molecules. The RL stage is the most critical for achieving diversity.
- Design tradeoffs: Using multi-stage RL increases complexity but solves credit assignment; using Tanimoto similarity on fingerprints is efficient but may miss some structural nuances; conditioning on previous molecules limits parallelism but improves diversity.
- Failure signatures: Low NCircles despite many accepted molecules indicates diversity isn't being learned; similar performance to beam search baselines suggests RL isn't effective; collapse to similar structures during generation indicates conditioning isn't working.
- First 3 experiments:
  1. Implement supervised fine-tuning only on BioT5+ and evaluate NCircles to establish baseline
  2. Add multi-stage RL with diversity rewards and compare against single-stage RL
  3. Test generalization by applying the fine-tuned model to unseen property prompts (e.g., QED)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed fine-tuning method compare to using a larger, more diverse external dataset for pre-training?
- Basis in paper: [inferred] The paper mentions that the method leverages self-improvement techniques and does not require additional datasets containing diverse molecules, but it doesn't compare the performance to using an external dataset.
- Why unresolved: The paper focuses on demonstrating the effectiveness of the fine-tuning method without comparing it to a baseline that uses an external dataset for pre-training.
- What evidence would resolve it: An experiment comparing the performance of the fine-tuning method to a baseline that uses a large, diverse external dataset for pre-training would provide evidence to resolve this question.

### Open Question 2
- Question: How does the proposed method handle prompts with multiple, conflicting molecular properties?
- Basis in paper: [inferred] The paper demonstrates the method's ability to generate diverse molecules for prompts with single properties, but it doesn't explore scenarios with multiple, potentially conflicting properties.
- Why unresolved: The paper focuses on single-property prompts and doesn't address the complexity of handling multiple, conflicting properties in a single prompt.
- What evidence would resolve it: An experiment testing the method's performance on prompts with multiple, conflicting molecular properties would provide evidence to resolve this question.

### Open Question 3
- Question: How does the proposed method scale to larger, more complex molecular structures?
- Basis in paper: [inferred] The paper demonstrates the method's effectiveness on relatively simple molecular structures, but it doesn't explore its performance on larger, more complex structures.
- Why unresolved: The paper focuses on demonstrating the method's effectiveness on a specific dataset and doesn't address its scalability to more complex molecular structures.
- What evidence would resolve it: An experiment testing the method's performance on a dataset with larger, more complex molecular structures would provide evidence to resolve this question.

## Limitations

- The method's effectiveness relies heavily on Tanimoto similarity on Morgan fingerprints, which may miss important chemical properties that correlate with biological activity
- The approach assumes the ChEBI-20 dataset adequately represents the diversity of molecules needed for practical drug discovery applications
- The paper does not investigate catastrophic forgetting of the original language modeling capabilities or the robustness of the diversity-promoting behavior across extended generations

## Confidence

**High confidence** in the experimental results and quantitative comparisons on the ChEBI-20 dataset. The paper demonstrates clear improvements over baselines with statistically significant differences in NCircles scores and provides thorough ablation studies supporting the multi-stage RL approach.

**Medium confidence** in the generalizability of the approach. While the method shows promise when applied to different pre-trained models (BioT5+, MolT5, DrugAssist), the evaluation is limited to a single dataset and property-matching tasks. The real-world applicability for diverse drug discovery scenarios remains to be validated.

**Low confidence** in the long-term stability of the fine-tuned models. The paper does not investigate catastrophic forgetting of the original language modeling capabilities or the robustness of the diversity-promoting behavior across extended generations.

## Next Checks

1. **Diversity-Activity Correlation**: Test whether structurally diverse molecules generated by the fine-tuned model show better coverage of active chemical space by comparing against known bioactivity databases.

2. **Cross-Dataset Generalization**: Evaluate the approach on additional molecular datasets beyond ChEBI-20, particularly those representing different chemical classes and property distributions.

3. **Multi-Property Generation**: Extend the evaluation to scenarios requiring generation of molecules satisfying multiple diverse properties simultaneously, testing the model's ability to balance diversity with constraint satisfaction.