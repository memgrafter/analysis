---
ver: rpa2
title: How Different AI Chatbots Behave? Benchmarking Large Language Models in Behavioral
  Economics Games
arxiv_id: '2412.12362'
source_url: https://arxiv.org/abs/2412.12362
tags:
- human
- chatbots
- large
- llama
- claude
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper benchmarks five major LLM-based chatbots in behavioral\
  \ economics games, finding that they produce highly concentrated decision distributions\
  \ that capture specific human behavior modes but lack full distributional similarity.\
  \ The models exhibit distinct behavioral patterns\u2014emphasizing fairness and\
  \ altruism more than humans, with varying degrees of trust, risk aversion, and cooperation."
---

# How Different AI Chatbots Behave? Benchmarking Large Language Models in Behavioral Economics Games

## Quick Facts
- **arXiv ID**: 2412.12362
- **Source URL**: https://arxiv.org/abs/2412.12362
- **Reference count**: 40
- **Primary result**: LLM chatbots produce concentrated decision distributions that capture specific human behavior modes but lack full distributional similarity across behavioral economics games

## Executive Summary
This paper benchmarks five major LLM-based chatbots (GPT-4o, Llama 3.1 405B, Gemini 1.5 Pro, Claude 3.5 Sonnet, Mistral Large 2) across six behavioral economics games to understand their decision-making patterns. The study finds that while these models can pass Turing tests at high rates, their behavior distributions are less diverse than human distributions and exhibit distinct patterns emphasizing fairness and altruism. Different models show varying degrees of trust, risk aversion, and cooperation, with notable inconsistencies across games. The research demonstrates that behavioral benchmarking effectively profiles AI chatbots and reveals systematic differences in their strategic preferences.

## Method Summary
The study uses six classic behavioral economics games (Dictator, Ultimatum, Trust, Public Goods, Bomb Risk, Prisoner's Dilemma) with prompts from Mei et al. [7]. For each game and model, 50 independent valid responses are collected. Behavior distributions are compared using Wasserstein distance, Turing tests are conducted by sampling human and AI actions, and utility functions are fitted to estimate payoff preferences (weight ùëè). Behavior inconsistency is measured via mean absolute error of payoff curves across games. Human behavior distributions from Mei et al. [7] serve as the baseline for comparison.

## Key Results
- LLM chatbots exhibit highly concentrated decision distributions that capture specific human behavior modes but lack full distributional similarity
- Most models pass Turing tests at high rates, but their behavior distributions remain less diverse than human distributions
- AI chatbots place greater emphasis on maximizing fairness in their payoff preferences compared to humans
- Different models show distinct behavioral patterns with varying levels of trust, risk aversion, and cooperation across games

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Behavioral economics games effectively reveal decision-making patterns in LLMs
- Mechanism: Classic games like Dictator, Ultimatum, and Trust simulate real-world scenarios where choices reflect altruism, fairness, and trust. By observing LLM responses across these games, we capture systematic differences in strategic preferences
- Core assumption: LLMs produce consistent responses to game prompts, allowing reliable distribution comparisons
- Evidence anchors:
  - [abstract] "This paper presents a comprehensive analysis of five leading LLM-based chatbot families as they navigate a series of behavioral economics games"
  - [section 3.1] "Figure 1 illustrates the distributions of AI choices across the six games"
- Break condition: If LLMs produce highly inconsistent or non-deterministic responses, distribution comparisons lose validity

### Mechanism 2
- Claim: Turing test performance reveals LLM behavioral similarity to humans
- Mechanism: In each game, a human action and an LLM action are sampled and compared based on their probability within the human distribution. High success rates indicate behavioral similarity
- Core assumption: Human behavior distributions are representative and stable
- Evidence anchors:
  - [section 3.2] "Figure 2 presents the results of the Turing tests. Overall, all tested AI chatbots demonstrate a remarkable ability to pass the Turing test"
  - [abstract] "While most models pass Turing tests at high rates, their behavior distributions remain less diverse than human distributions"
- Break condition: If human distributions are non-stationary or if sampling is biased, Turing test results become unreliable

### Mechanism 3
- Claim: Utility function fitting quantifies intrinsic preferences in LLMs
- Mechanism: By assuming a utility function that balances own and partner payoffs, we estimate the weight parameter (b) that best explains LLM behavior. Lower optimization errors indicate alignment with that objective
- Core assumption: LLMs behave as if optimizing a well-defined utility function
- Evidence anchors:
  - [section 3.4] "The objective function of AI chatbots is quantitatively estimated by assessing the degree to which their behaviors align with the optimization goals"
  - [abstract] "AI chatbots place greater emphasis on maximizing fairness in their payoff preferences"
- Break condition: If LLM behavior is driven by factors not captured by the assumed utility function (e.g., prompt adherence, safety constraints), the fitting may be misleading

## Foundational Learning

- **Concept: Behavioral economics games**
  - Why needed here: These games create controlled decision-making environments that reveal strategic preferences (altruism, fairness, trust, risk aversion, cooperation)
  - Quick check question: What is the key difference between the Dictator Game and the Ultimatum Game?

- **Concept: Wasserstein distance**
  - Why needed here: Used to measure distributional similarity between human and AI behaviors across games
  - Quick check question: What does a smaller Wasserstein distance indicate in the context of this study?

- **Concept: Multinomial logit discrete choice model**
  - Why needed here: Fits a probabilistic model to LLM choices, estimating the preference weight (b) for own vs. partner payoff
  - Quick check question: In the multinomial logit model, what does a b value close to 0.5 imply about the player's preferences?

## Architecture Onboarding

- **Component map**: Game prompts ‚Üí LLM responses ‚Üí behavior distribution ‚Üí analysis (Wasserstein, Turing tests, utility fitting) ‚Üí behavioral profiles
- **Critical path**: 1) Run game prompts across all LLM variants. 2) Collect and clean response distributions. 3) Compute Wasserstein distances and run Turing tests. 4) Fit utility models and calculate optimization errors. 5) Analyze consistency and report findings
- **Design tradeoffs**: Number of prompts vs. response diversity; choice of utility function form (linear vs. CES); granularity of behavior bins in distribution comparison
- **Failure signatures**: Inconsistent responses from same model ‚Üí unreliable distributions; poor convergence in utility fitting ‚Üí model mismatch; high Wasserstein distances across all games ‚Üí systemic differences
- **First 3 experiments**: 1) Run Dictator Game prompts across all models and check for concentration of responses. 2) Compute Wasserstein distances between human and LLM distributions for a single game. 3) Fit a simple utility model (linear) to Ultimatum Proposer responses and estimate b

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific architectural differences between LLM families (e.g., transformer variants, training objectives, or data composition) contribute to their distinct behavioral patterns in economics games?
- Basis in paper: [inferred] The paper observes that different AI chatbot families exhibit distinct behavioral patterns across games (e.g., varying levels of trust, fairness emphasis, and risk aversion), but does not investigate the underlying architectural factors driving these differences
- Why unresolved: The study focuses on behavioral benchmarking rather than technical analysis of model architectures or training methodologies
- What evidence would resolve it: Comparative analysis of model architectures, training datasets, and objective functions correlated with behavioral outcomes across different LLM families

### Open Question 2
- Question: Can the observed inconsistencies in AI chatbot behavior across different economics games be reduced through targeted fine-tuning or prompt engineering strategies?
- Basis in paper: [explicit] The paper identifies behavioral inconsistencies across games, noting that AI chatbots may display different payoff preferences in different scenarios
- Why unresolved: The study presents findings on inconsistencies but does not explore potential mitigation strategies or their effectiveness
- What evidence would resolve it: Experiments testing various fine-tuning approaches or prompt engineering techniques to improve behavioral consistency across games

### Open Question 3
- Question: How do the behavioral patterns of AI chatbots evolve over extended interaction periods or repeated game play compared to single-instance responses?
- Basis in paper: [inferred] The study collects 50 independent responses per game but does not examine how chatbot behavior might change with repeated interactions or learning over time
- Why unresolved: The research design focuses on static benchmarking rather than dynamic behavioral analysis over time
- What evidence would resolve it: Longitudinal studies tracking behavioral changes across multiple game iterations or extended interaction sessions

## Limitations
- The study relies on human behavior data from a single source, which may not be fully representative of broader human decision-making patterns
- The analysis assumes LLMs produce stable, comparable responses across games, but significant inconsistencies are observed
- The utility function fitting assumes specific forms (linear or CES) that may not capture the true decision processes of LLMs

## Confidence

**High confidence**:
- LLM chatbots exhibit distinct, consistent behavioral patterns across behavioral economics games
- Most models can pass Turing tests at high rates when compared to human behavior distributions
- LLMs show greater emphasis on fairness and altruism compared to human baselines

**Medium confidence**:
- Behavior distributions are less diverse than human distributions
- Specific patterns of trust, risk aversion, and cooperation vary meaningfully across models
- Optimization efficiency metrics accurately capture payoff preference weights

**Low confidence**:
- The specific weight parameters (b values) precisely quantify true LLM preferences
- Inconsistencies across games fully explain behavioral differences
- Wasserstein distances alone sufficiently capture behavioral similarity

## Next Checks
1. **Cross-validation of behavior distributions**: Run additional response samples (e.g., 100 vs 50 per game) to verify the stability and concentration patterns observed in the current distributions
2. **Alternative utility function testing**: Fit alternative utility models (e.g., non-linear, multi-attribute) to test whether the linear/CES assumptions significantly affect the estimated preference parameters
3. **Temporal consistency check**: Run the same game prompts across different time periods to assess whether LLM behavior distributions remain stable or show drift, particularly for models known to be updated frequently