---
ver: rpa2
title: Context-Aware Siamese Networks for Efficient Emotion Recognition in Conversation
arxiv_id: '2404.11141'
source_url: https://arxiv.org/abs/2404.11141
tags:
- emotion
- learning
- recognition
- computational
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses Emotion Recognition in Conversation (ERC),
  a challenging task due to the plurality and subjectivity of human emotions. Existing
  approaches rely heavily on graph-based representations and face issues with label
  imbalance.
---

# Context-Aware Siamese Networks for Efficient Emotion Recognition in Conversation

## Quick Facts
- **arXiv ID**: 2404.11141
- **Source URL**: https://arxiv.org/abs/2404.11141
- **Reference count**: 0
- **Primary result**: State-of-the-art ERC model achieving macro F1 of 57.71 on DailyDialog dataset

## Executive Summary
This paper introduces a novel two-step metric learning approach for Emotion Recognition in Conversation (ERC) using Siamese Networks. The method addresses key challenges in ERC including label imbalance and the need for contextual understanding. By combining cross-entropy loss for direct classification with triplet loss for relative label assignments, the model achieves state-of-the-art performance on the DailyDialog dataset while maintaining computational efficiency through the use of sentence-level embeddings instead of word-piece representations.

## Method Summary
The proposed method employs a Siamese Network architecture with a two-step training approach. First, an emotion classifier is trained using cross-entropy loss on sentence transformer embeddings. Then, the model is fine-tuned using triplet loss to optimize the embedding space for relative emotion relationships. The architecture uses sentence-level representations (MPNet, MiniLM, or RoBERTa-based) concatenated into dialogue sequences, processed through a Transformer encoder for contextual attention, then split back into utterance-level contextual embeddings. Weighted random sampling and batch-balanced cross-entropy loss address label imbalance issues throughout training.

## Key Results
- Achieved state-of-the-art macro F1 score of 57.71 on DailyDialog dataset
- Obtained micro F1 of 57.75 and Matthews Correlation Coefficient of 0.49
- Demonstrated efficient performance using sentence embeddings rather than word-piece representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-step training (CE + triplet loss) enables both direct and relative emotion learning
- Mechanism: First, a standard emotion classifier is pre-trained with cross-entropy loss to learn absolute label mappings. Then, Siamese Networks optimize a shared embedding space using triplet loss to capture relative relationships between emotion categories
- Core assumption: Absolute label predictions and relative distance learning are complementary and can reinforce each other when trained sequentially
- Evidence anchors: [abstract] "This allows us to perform ERC in a flexible classification scenario and to end up with a lightweight yet efficient model." [section 3] "Meanwhile, contextual representations are optimized according to the metric learning objective, using triplet loss."
- Break condition: If the two losses interfere rather than reinforce, e.g., the triplet loss pulls embeddings away from the CE-classifier's decision boundaries

### Mechanism 2
- Claim: Using sentence-level embeddings instead of word-piece embeddings reduces memory footprint while preserving contextual information
- Mechanism: Sentence transformers produce fixed-length utterance representations, which are concatenated into a dialogue-level sequence. A Transformer encoder layer then applies attention to capture conversational context before splitting back into utterance-level contextual embeddings
- Core assumption: Sentence embeddings retain sufficient semantic richness for emotion classification while being more compact than word-piece sequences
- Evidence anchors: [abstract] "The presented model leverages sentence embeddings and Transformer encoder layers... to represent dialogue utterances and deploy attention on the conversational context." [section 3] "Sentence embeddings are preferred to word-piece embeddings (like BERT produces) as they provide lighter utterance representations."
- Break condition: If sentence embeddings lose nuance critical for fine-grained emotion distinctions, leading to degraded performance

### Mechanism 3
- Claim: Weighted random sampling and batch-balanced CE loss mitigate extreme label imbalance
- Mechanism: Inverse label frequency weighting is applied to the data sampler, ensuring rarer emotions are oversampled proportionally. The CE loss is also weighted per batch to further counter imbalance effects
- Core assumption: Balancing at both sampling and loss levels is necessary to prevent the model from collapsing to majority emotion predictions
- Evidence anchors: [section 4] "Thus, we addressed the imbalance problem all along the training pipeline, by implementing a random sampler weighted with inverse label frequencies... and we weight the cross entropy loss from the emotion classifier considering the remaining imbalance on each batch." [section 5] "By considering triplets we remove the imbalance factor while using hidden states that come from balanced representation."
- Break condition: If sampling introduces bias or the loss weighting is miscalibrated, resulting in unstable training or poor generalization

## Foundational Learning

- **Concept**: Metric learning and contrastive objectives
  - Why needed here: To learn emotion embeddings where similar emotions are close and dissimilar ones are far apart, enabling flexible classification beyond fixed label sets
  - Quick check question: What is the mathematical form of the triplet loss used to train the Siamese network?

- **Concept**: Siamese Networks and parameter sharing
  - Why needed here: Allows efficient learning of pairwise (or triplet) relationships between utterances by sharing weights across identical subnetworks
  - Quick check question: How does parameter sharing in a Siamese network reduce the number of parameters compared to training two separate networks?

- **Concept**: Attention mechanisms in Transformer encoders
  - Why needed here: To aggregate conversational context across utterances before emotion prediction, capturing speaker dynamics and topic flow
  - Quick check question: In the context encoder, what is the role of the [SEP] token when splitting the dialogue representation back into utterances?

## Architecture Onboarding

- **Component map**: Sentence transformer → dialogue concatenation → Transformer encoder → utterance split → emotion classifier (CE loss) + triplet loss
- **Critical path**:
  1. Encode utterances with sentence transformer
  2. Concatenate and encode dialogue context with Transformer
  3. Split back to utterance-level contextual embeddings
  4. Pass through emotion classifier (CE loss)
  5. Sample triplets → compute and backpropagate triplet loss
- **Design tradeoffs**:
  - Sentence vs. word-piece embeddings: memory vs. granularity
  - Two-step vs. end-to-end training: stability vs. efficiency
  - Uniform vs. hard triplet sampling: coverage vs. difficulty
- **Failure signatures**:
  - CE loss dominates and embeddings collapse to class prototypes
  - Triplet loss fails to converge due to extreme imbalance
  - Attention fails to capture cross-utterance context, leading to utterance-only predictions
- **First 3 experiments**:
  1. Train only the emotion classifier (CE loss) on isolated utterances; measure macro F1
  2. Replace sentence transformers with word-piece embeddings; measure memory usage and performance
  3. Remove triplet loss, train only with CE loss on contextual embeddings; compare to full model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed metric learning approach generalize to other emotion recognition datasets with different label sets or granularities?
- Basis in paper: [explicit] The authors state their model can be adapted to other emotion labels and different contexts requiring emotion recognition of different label granularities
- Why unresolved: The paper only evaluates on DailyDialog dataset and does not test the model's adaptability to other datasets or label sets
- What evidence would resolve it: Experiments applying the model to multiple other ERC datasets with varying label sets and comparing performance would provide evidence of generalization

### Open Question 2
- Question: How does the model's performance compare when using more complex meta-learning setups beyond Siamese Networks?
- Basis in paper: [explicit] The authors mention their model structure is easily adaptable to more complex meta-learning setups and that Siamese Networks are conceptually simple but potentially less performant than few-shot learning approaches
- Why unresolved: The paper only implements a Siamese Network architecture and does not explore more complex meta-learning alternatives
- What evidence would resolve it: Implementing and comparing the model's performance using alternative meta-learning architectures (e.g., Prototypical Networks, Relation Networks) would provide evidence of the benefits of more complex approaches

### Open Question 3
- Question: What is the impact of using different sentence embedding models (e.g., MPNet, MiniLM, RoBERTa) on the model's performance and efficiency?
- Basis in paper: [explicit] The authors conducted experiments with three different sentence transformer models but did not provide a detailed comparison of their impact on performance and efficiency
- Why unresolved: The paper mentions using different sentence transformers but does not analyze their individual contributions to the model's results
- What evidence would resolve it: A systematic comparison of the model's performance and training efficiency using each sentence transformer model individually would provide insights into their relative impact

## Limitations

- The paper lacks detailed implementation specifications for critical components including triplet sampling strategy and inverse frequency weighting mechanisms
- Performance evaluation is limited to a single dataset (DailyDialog) which has known limitations in label quality and dialogue complexity
- No ablation study comparing two-step training to end-to-end approaches to quantify the benefit of sequential training

## Confidence

- **Medium**: Core claims about achieving SOTA results are supported by experimental evidence, but implementation details are insufficient for exact reproduction and dataset limitations weaken generalizability

## Next Checks

1. **Triplet Loss Ablation**: Train a baseline model with only CE loss on contextual embeddings and compare macro F1, micro F1, and MCC to the full model to quantify the contribution of the triplet loss

2. **Sampling Strategy Sensitivity**: Implement and compare different triplet sampling strategies (batch-hard, semi-hard, uniform) and evaluate their impact on training stability and final performance

3. **Embedding Granularity Test**: Replace sentence transformer embeddings with word-piece embeddings (e.g., from BERT) and measure both memory usage and performance to validate the claimed efficiency gains