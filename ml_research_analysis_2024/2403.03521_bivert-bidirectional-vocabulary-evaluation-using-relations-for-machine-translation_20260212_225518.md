---
ver: rpa2
title: 'BiVert: Bidirectional Vocabulary Evaluation using Relations for Machine Translation'
arxiv_id: '2403.03521'
source_url: https://arxiv.org/abs/2403.03521
tags:
- translation
- evaluation
- word
- words
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BiVert, a bidirectional semantic-based evaluation
  method for machine translation that uses a multilingual encyclopedic dictionary
  (BabelNet) to assess the semantic similarity between source and back-translated
  sentences. The approach aligns words between the two sentences using cosine similarity
  of their embeddings, identifies word pair relations (e.g., same, extra, missing,
  stop words, inflection, derivation, sense), and scores each relation type based
  on its semantic distance.
---

# BiVert: Bidirectional Vocabulary Evaluation using Relations for Machine Translation

## Quick Facts
- **arXiv ID**: 2403.03521
- **Source URL**: https://arxiv.org/abs/2403.03521
- **Reference count**: 0
- **Primary result**: Bidirectional semantic-based MT evaluation using back-translation and BabelNet achieves 0.694 Pearson correlation with human assessments for English-German

## Executive Summary
BiVert introduces a novel bidirectional semantic-based evaluation method for machine translation that compares source sentences with back-translations of target translations. The approach leverages a multilingual encyclopedic dictionary (BabelNet) to assess semantic similarity between aligned word pairs, identifying seven relation types (Same, Extra, Missing, Stopwords, Inflection, Derivation, Sense) and scoring them based on their semantic distance. The final evaluation score is a weighted combination of all relation types, optimized through gradient boosting regression for each language pair. BiVert demonstrates strong correlation with human assessments, particularly for English-German (0.694), while showing promising results for English-Russian (0.657) and Chinese-English (0.376).

## Method Summary
BiVert implements a bidirectional evaluation framework where target translations are first back-translated into the source language, enabling monolingual semantic comparison. Words between source and back-translated sentences are aligned using cosine similarity of contextual embeddings through a linear sum assignment problem. Each aligned word pair is categorized into one of seven relation types based on their semantic and morphological properties. For sense relations, BabelNet constructs semantic graphs to calculate shortest path distances between synsets. Each relation type receives a specific score, and these scores are combined using weights learned through gradient boosting regression on human-annotated datasets. The method requires language-specific preprocessing (lowercase and expand contractions for English, keep only Chinese characters for Chinese) and language-specific weight optimization through training on WMT Metrics Task data.

## Key Results
- Achieves 0.694 Pearson correlation with human assessments for English-German translation quality
- Shows promising performance for English-Russian (0.657) and Chinese-English (0.376) language pairs
- Outperforms other reference-free evaluation metrics like BERTScore and COMET-Kiwi for some language pairs
- Demonstrates effectiveness of semantic-based evaluation without requiring reference translations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Back-translation into the same language as source enables monolingual semantic comparison
- Mechanism: By translating the target sentence back into the source language, both sentences share a common linguistic representation, allowing direct semantic comparison using monolingual resources like BabelNet
- Core assumption: The back-translation system is sufficiently accurate to preserve core semantic content
- Evidence anchors:
  - [abstract]: "By scoring the semantic similarity between the source and its back translation of the output, our method introduces a quantifiable approach that empowers sentence comparison on the same linguistic level."
  - [section]: "This is achieved through comparing the source sentences and the target sentence t by scoring the semantic similarity between the s and its back-translated sentences′, both of whom share a common language l1"
  - [corpus]: Weak - No corpus evidence available for this specific mechanism
- Break condition: If the back-translation system introduces significant semantic drift or errors, the monolingual comparison becomes unreliable

### Mechanism 2
- Claim: Semantic graph distance through BabelNet captures true sense relationships better than embedding similarity alone
- Mechanism: When word pairs are identified as having different senses, BabelNet is used to construct a semantic graph and calculate the shortest path distance between their synsets, capturing true semantic relationships including hypernymy and antonymy
- Core assumption: BabelNet provides comprehensive and accurate sense networks for the language pairs being evaluated
- Evidence anchors:
  - [section]: "We aim to grade the actual distance of their intentional sense in the given context, using the multilingual encyclopedia BabelNet. For this issue we assemble a semantic subgraph described in section 3.3.1."
  - [section]: "BabelNet is unique in providing extensive coverage of words and their meanings across multiple languages"
  - [corpus]: Weak - No corpus evidence available for this specific mechanism
- Break condition: If BabelNet coverage is insufficient for a language pair, or if the sense disambiguation fails to identify the correct context, the semantic distance calculation becomes inaccurate

### Mechanism 3
- Claim: Weighted combination of relation types trained on human assessments optimizes evaluation quality
- Mechanism: Seven relation types (Same, Extra, Missing, Stopwords, Inflection, Derivation, Sense) are scored and then combined using weights learned through gradient boosting regression on human-annotated datasets
- Core assumption: Human assessments provide a reliable gold standard for training the relation weight optimization
- Evidence anchors:
  - [section]: "The final score of BiVert is a trained combination of all relation types into a final score. We use gradient descent to train our method in order to achieve optimal predictions for each language pair."
  - [section]: "We learn the feature values by training a Gradient Boosting Regression model for each language pair"
  - [section]: "Table 1: Feature importance scores learned by a Gradient Boosting Regression model for BiVert language pairs"
- Break condition: If the training data is biased or unrepresentative, or if the relation types don't capture the full range of translation quality factors, the learned weights may not generalize well

## Foundational Learning

- Concept: Semantic graph theory and WordNet-style lexical databases
  - Why needed here: BiVert relies on BabelNet's semantic graph structure to calculate sense distances between words
  - Quick check question: What are the main types of semantic relations (hypernym, hyponym, holonym, meronym, antonym) and how do they affect graph distance calculations?

- Concept: Back-translation methodology in machine translation evaluation
  - Why needed here: The bidirectional approach requires understanding how back-translation can be used as a quality estimation tool
  - Quick check question: What are the potential failure modes of back-translation (e.g., error compounding, semantic drift)?

- Concept: Word embedding alignment and subword tokenization handling
  - Why needed here: BiVert needs to align words between source and back-translated sentences using their embeddings, which requires handling subword tokenization
  - Quick check question: How do different subword pooling strategies (max, average, first token) affect alignment quality?

## Architecture Onboarding

- Component map: Source sentence → Back-translation → Word alignment → Relation identification → Semantic graph calculation (for sense relations) → Relation scoring → Weighted combination → Final score

- Critical path: Source sentence → Back-translation → Word alignment → Relation identification → Semantic graph calculation (for sense relations) → Relation scoring → Weighted combination → Final score

- Design tradeoffs:
  - Using back-translation vs. requiring reference translations: Back-translation enables reference-free evaluation but introduces dependency on back-translation quality
  - Semantic graph depth limit: Deeper graphs capture more nuanced relationships but increase computational cost and risk of irrelevant paths
  - Relation type granularity: More relation types could capture finer distinctions but increase complexity and data requirements for training

- Failure signatures:
  - Low correlation with human scores despite high internal consistency: Indicates the relation types or weights don't align with human judgment criteria
  - Performance degradation on morphologically rich languages: Suggests the current relation types don't adequately capture inflectional and derivational phenomena
  - High computational cost relative to evaluation quality: May indicate the semantic graph calculations are adding overhead without sufficient benefit

- First 3 experiments:
  1. Evaluate BiVert's performance on a synthetic dataset where the "ground truth" semantic distance is known through controlled word substitutions
  2. Compare BiVert's performance with and without the semantic graph component to quantify its contribution to overall accuracy
  3. Test BiVert on a low-resource language pair where BabelNet coverage is limited to understand the impact of encyclopedia coverage on performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of BiVert vary when the back-translation is performed using the same MT system being evaluated, rather than a state-of-the-art system?
- Basis in paper: [explicit] The paper suggests an alternative use case where the evaluated system itself could be used for back-translation, which could enhance consistency across evaluations.
- Why unresolved: This scenario was proposed as a future direction but not tested in the current experiments.
- What evidence would resolve it: Comparative experiments running BiVert with both the evaluated system and a separate state-of-the-art system for back-translation across multiple language pairs.

### Open Question 2
- Question: How does BiVert's performance change when incorporating phrase-level or idiom-level alignment rather than just word-level alignment?
- Basis in paper: [explicit] The authors acknowledge that their current word alignment algorithm does not handle phrases or idioms, and suggest this as an avenue for future development.
- Why unresolved: The current implementation only operates at the word level, missing potential semantic units that span multiple words.
- What evidence would resolve it: Experimental results comparing BiVert with word-level alignment versus an extended version incorporating phrase-level alignment across various MT systems.

### Open Question 3
- Question: How does BiVert's performance differ when using alternative multilingual semantic resources beyond BabelNet, such as Wiktionary or other knowledge graphs?
- Basis in paper: [explicit] The authors propose expanding their graph knowledge of senses using resources other than BabelNet as future work.
- Why unresolved: The current implementation relies exclusively on BabelNet for sense information, potentially limiting coverage for certain languages or domains.
- What evidence would resolve it: Comparative experiments using BiVert with different semantic resources (Wiktionary, WordNet variants, etc.) and measuring correlation with human judgments across multiple language pairs.

## Limitations

- Performance significantly drops for Chinese-English (0.376) compared to English-German (0.694), suggesting the method may not generalize well across all language families
- The current relation taxonomy appears Western-centric, with inflection and derivation relations receiving zero weights for Chinese, indicating insufficient adaptation for non-Indo-European languages
- Dependency on BabelNet's coverage and quality introduces uncertainty, particularly for low-resource languages or those with different morphological structures

## Confidence

**High Confidence**: The core bidirectional evaluation framework and the use of gradient boosting regression for weight optimization are well-established techniques. The implementation details for word alignment using cosine similarity and the linear sum assignment problem are clearly specified and reproducible.

**Medium Confidence**: The correlation results for English-German and English-Russian are promising, but the significant drop for Chinese-English raises questions about the method's robustness. The claim that BiVert "significantly outperforms" other reference-free metrics needs more nuanced interpretation given the variable performance across language pairs.

**Low Confidence**: The paper's claims about semantic graph distance calculations using BabelNet are theoretically sound but lack empirical validation. The specific implementation details for semantic graph construction, including depth limits and relation filtering, are not fully specified, making it difficult to assess whether the method is being implemented as intended.

## Next Checks

1. **Cross-linguistic relation taxonomy validation**: Conduct a controlled experiment where BiVert is applied to synthetic datasets for different language pairs with known semantic distances. This would test whether the current seven relation types adequately capture translation quality across languages, or whether language-specific relation categories are needed.

2. **Ablation study of semantic graph component**: Compare BiVert's performance with and without the BabelNet semantic graph calculations to quantify the contribution of this component to overall evaluation quality. This would help determine whether the computational overhead of semantic graph construction is justified by performance gains.

3. **Back-translation quality impact analysis**: Evaluate how different levels of back-translation quality affect BiVert's performance by deliberately introducing controlled errors into the back-translation process. This would reveal the method's sensitivity to back-translation errors and help establish minimum quality thresholds for reliable evaluation.