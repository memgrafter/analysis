---
ver: rpa2
title: Under manipulations, are some AI models harder to audit?
arxiv_id: '2402.09043'
source_url: https://arxiv.org/abs/2402.09043
tags:
- audit
- hypothesis
- platform
- capacity
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the feasibility of robust auditing of AI
  models under the manipulation-proof framework, where platforms can adaptively modify
  their models post-audit to evade detection. The authors prove that if a platform
  uses models that can fit any data (high capacity), no audit strategy can outperform
  random sampling when estimating properties like demographic parity.
---

# Under manipulations, are some AI models harder to audit?

## Quick Facts
- arXiv ID: 2402.09043
- Source URL: https://arxiv.org/abs/2402.09043
- Authors: Augustin Godinot; Gilles Tredan; Erwan Le Merrer; Camilla Penzo; Francois Taïani
- Reference count: 40
- Primary result: High-capacity AI models are fundamentally harder to audit under manipulation-proof frameworks, with random sampling being optimal for such models.

## Executive Summary
This paper investigates the feasibility of robust auditing of AI models under manipulation-proof frameworks, where platforms can adaptively modify their models post-audit to evade detection. The authors prove that for models with sufficient capacity to fit any data labeling, no audit strategy can outperform random sampling when estimating properties like demographic parity. They introduce the concept of "manipulability under random audits" and show a strong empirical link between model capacity (measured via Rademacher complexity) and audit difficulty. Experiments with popular models on three tabular datasets confirm that high-capacity models are significantly harder to audit than low-capacity ones.

## Method Summary
The authors train multiple ML models (linear, perceptron, tree, GBDT) with varied hyperparameters on three tabular datasets. For each model-hypothesis class pair, they compute Rademacher complexity as a capacity measure and simulate random audits to estimate manipulability (µ-diameter). They compare the relationship between capacity and manipulability, and calculate the "cost of exhaustion" - the accuracy loss when switching from generalization-optimal to audit-evasion-optimal models. The theoretical analysis proves that random sampling is optimal for high-capacity models under the manipulation-proof framework.

## Key Results
- High-capacity models (e.g., deep trees, GBDTs) are significantly harder to audit than low-capacity models, with random sampling being optimal
- There is a strong empirical correlation between Rademacher complexity and audit manipulability across all tested models
- The cost of exhaustion (accuracy loss for audit evasion) is below 1% for all models, indicating platforms can easily evade audits
- Current black-box auditing approaches are fundamentally limited against high-capacity models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random sampling is optimal for auditing high-capacity models because these models can fit any labeling of the audit set, making active query selection useless.
- Mechanism: When a hypothesis class can fit any labeling of the input space, the audit set's composition in terms of sensitive attribute proportions becomes the only factor determining the audit's robustness. Active strategies cannot improve upon random sampling since all queries provide equal information.
- Core assumption: The model has sufficient capacity to interpolate any subset of the input space while maintaining generalization performance (benign overfitting).
- Evidence anchors:
  - [abstract]: "large-capacity models, which are commonly used in practice, are particularly hard to audit robustly."
  - [section]: "Theorem 1 (No need to aim). LetH ={0, 1}X. For any audit set S⊆X and hypothesis h∈H, diamµH(h, S ) = 2−(P(X∈ S|XA = 1)+P(X∈ S|XA = 0))"
  - [corpus]: Weak evidence - related papers focus on audit frameworks but don't directly address capacity-induced audit impossibility.

### Mechanism 2
- Claim: The manipulability under random audits increases monotonically with model capacity, as measured by Rademacher complexity.
- Mechanism: Higher capacity models can achieve a wider range of fairness/accuracy trade-offs post-audit, making them harder to audit reliably. The Rademacher complexity quantifies this capacity by measuring the model's ability to fit random labelings of subsets.
- Core assumption: Rademacher complexity is a valid proxy for the model's ability to evade audits by fitting arbitrary audit set labelings.
- Evidence anchors:
  - [abstract]: "strong empirical link between model capacity (measured via Rademacher complexity) and audit difficulty"
  - [section]: "We empirically confirm the strong connection between the classical Rademacher complexity and the manipulability of manipulation-proof auditing."
  - [corpus]: Moderate evidence - related work on audit manipulation exists but doesn't explicitly connect to Rademacher complexity.

### Mechanism 3
- Claim: Faithful platforms (those optimizing for generalization) naturally implement low-capacity models that are easier to audit, while malicious platforms can choose high-capacity models at minimal accuracy cost.
- Mechanism: The cost of exhaustion metric quantifies the accuracy loss when switching from a generalization-optimal model to one optimized for audit evasion. Low cost indicates platforms can easily evade audits.
- Core assumption: Platforms can freely choose their hypothesis class without constraints beyond accuracy requirements.
- Evidence anchors:
  - [abstract]: "large models currently used in production are not auditable more efficiently than by random sampling"
  - [section]: "For all models, on all considered datasets... the cost of exhaustion is below 1%."
  - [corpus]: Weak evidence - related papers discuss audit evasion but don't quantify the accuracy cost of such evasion.

## Foundational Learning

- Concept: Manipulation-proof auditing framework
  - Why needed here: This paper builds on this framework to show its limitations with high-capacity models. Understanding the framework is essential to grasp why random sampling becomes optimal.
  - Quick check question: In manipulation-proof auditing, what constraint must the platform respect when changing its model post-audit?

- Concept: Rademacher complexity as a measure of model capacity
  - Why needed here: The paper uses Rademacher complexity to quantify how hard different models are to audit. This concept is central to understanding the empirical results.
  - Quick check question: How does Rademacher complexity relate to a model's ability to fit random labelings of a dataset?

- Concept: Benign overfitting
  - Why needed here: The theoretical results rely on models that can fit any subset of the data while maintaining good generalization. This phenomenon is crucial for understanding why high-capacity models are hard to audit.
  - Quick check question: What is the key difference between benign overfitting and regular overfitting?

## Architecture Onboarding

- Component map:
  Data pipeline -> Model training -> Capacity measurement -> Manipulability measurement -> Cost of exhaustion calculation -> Visualization

- Critical path:
  1. Load dataset and preprocess
  2. Train all model-hyperparameter combinations
  3. Compute capacity (Rademacher complexity) for each
  4. Simulate random audits to measure manipulability
  5. Identify Hopt and Hµ for cost of exhaustion calculation
  6. Generate visualizations and analyze results

- Design tradeoffs:
  - Computational cost vs. model coverage: Training 500+ model configurations is expensive but provides comprehensive coverage
  - Simulation vs. real audits: Using simulated random audits is faster but may miss real-world platform behaviors
  - Capacity proxy vs. true capacity: Rademacher complexity is easier to compute than VC dimension but may be less precise

- Failure signatures:
  - Unexpectedly low manipulability for high-capacity models: May indicate bugs in the audit simulation or capacity measurement
  - Non-monotonic relationship between capacity and manipulability: Could suggest the need for additional factors in the analysis
  - Cost of exhaustion exceeding 5%: May indicate issues with the optimization procedure or dataset characteristics

- First 3 experiments:
  1. Verify that random sampling achieves the same µ-diameter as optimal auditing for a model class that can fit any labeling (e.g., high-capacity trees)
  2. Confirm the monotonic relationship between Rademacher complexity and manipulability by plotting these metrics for all trained models
  3. Calculate the cost of exhaustion for each model family and verify it's below 1% as reported in the paper

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise computational complexity of the manipulation-proof auditing algorithm (AFA) for large-capacity models, and how does it scale with dataset size and model complexity?
- Basis in paper: [inferred] The paper mentions that AFA is too computationally intensive for large models due to the need to train many copies efficiently, and that the query complexity and computational complexity depend on the value of Cost(H), which is hard to compute.
- Why unresolved: The paper only provides asymptotic bounds for AFA's query complexity and does not provide empirical results or detailed analysis of its computational complexity for large models.
- What evidence would resolve it: Empirical studies measuring the runtime and resource usage of AFA on large datasets and models, along with theoretical analysis of its scaling behavior.

### Open Question 2
- Question: How does the manipulability under random audits relate to other notions of model capacity, such as the VC dimension or the number of parameters, beyond the Rademacher complexity?
- Basis in paper: [explicit] The paper uses Rademacher complexity to measure model capacity and shows a strong empirical link between it and manipulability, but mentions other notions like VC dimension.
- Why unresolved: The paper focuses on Rademacher complexity and does not explore the relationship between manipulability and other capacity measures in detail.
- What evidence would resolve it: Experiments comparing manipulability with VC dimension and other capacity metrics across various models and datasets.

### Open Question 3
- Question: Can the theoretical impossibility results for robust auditing of high-capacity models be extended to other types of properties beyond demographic parity, such as group fairness or individual fairness?
- Basis in paper: [explicit] The paper focuses on demographic parity as the measure of interest but mentions that the results could be extended to other parity measures.
- Why unresolved: The paper does not provide a formal extension of the impossibility results to other fairness measures or explore the implications for different types of properties.
- What evidence would resolve it: Formal proofs or counterexamples showing the applicability or limitations of the impossibility results for various fairness metrics and properties.

## Limitations

- The theoretical results assume platforms can freely choose high-capacity models without constraints from regulation, computational resources, or business needs
- Empirical evidence is based on simulations rather than real-world audits, potentially missing platform behaviors
- The relationship between Rademacher complexity and manipulability, while empirically strong, lacks theoretical justification beyond observed correlations

## Confidence

- **High Confidence**: The theoretical impossibility result for high-capacity models under manipulation-proof auditing is mathematically rigorous and well-established.
- **Medium Confidence**: The empirical link between Rademacher complexity and audit difficulty is strongly supported but based on limited datasets and model families.
- **Low Confidence**: The practical implications for real-world auditing frameworks, particularly the recommendation for certification-based approaches, lack direct empirical validation.

## Next Checks

1. **Theoretical Extension**: Prove whether the relationship between Rademacher complexity and manipulability holds under different learning frameworks (e.g., online learning, transfer learning).
2. **Empirical Generalization**: Test the capacity-audit difficulty relationship on non-tabular datasets (images, text) and with deep learning models to assess robustness beyond the current experimental scope.
3. **Real-World Simulation**: Implement a realistic platform simulation that includes economic incentives and operational constraints to validate whether platforms would actually choose high-capacity models for audit evasion.