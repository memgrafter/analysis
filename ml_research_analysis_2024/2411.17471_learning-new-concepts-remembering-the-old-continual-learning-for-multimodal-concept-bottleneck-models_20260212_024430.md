---
ver: rpa2
title: 'Learning New Concepts, Remembering the Old: Continual Learning for Multimodal
  Concept Bottleneck Models'
arxiv_id: '2411.17471'
source_url: https://arxiv.org/abs/2411.17471
tags:
- learning
- concept
- concepts
- multimodal
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CONCIL, a framework for continual learning
  in Concept Bottleneck Models (CBMs) that addresses the problem of catastrophic forgetting
  during concept- and class-incremental learning. The core idea is to reformulate
  the updates for concept and decision layers as linear regression problems, eliminating
  the need for gradient-based optimization.
---

# Learning New Concepts, Remembering the Old: Continual Learning for Multimodal Concept Bottleneck Models

## Quick Facts
- arXiv ID: 2411.17471
- Source URL: https://arxiv.org/abs/2411.17471
- Reference count: 34
- This paper introduces CONCIL, a framework for continual learning in Concept Bottleneck Models (CBMs) that addresses catastrophic forgetting during concept- and class-incremental learning

## Executive Summary
This paper introduces CONCIL, a framework for continual learning in Concept Bottleneck Models (CBMs) that addresses the problem of catastrophic forgetting during concept- and class-incremental learning. The core idea is to reformulate the updates for concept and decision layers as linear regression problems, eliminating the need for gradient-based optimization. This approach is implemented through recursive matrix operations, which makes it computationally efficient and well-suited for real-time and large-scale multimodal data applications. The experimental results on the CUB and AwA datasets demonstrate that CONCIL achieves "absolute knowledge memory," significantly outperforming traditional CBM methods in both concept- and class-incremental settings, with improvements in average concept accuracy (19% on CUB, 19.2% on AwA), average class accuracy (31.5 percentage points on CUB, 50.7 percentage points on AwA), and a drastic reduction in forgetting rates (88.7% on CUB, 89% on AwA).

## Method Summary
CONCIL reformulates the concept and classifier updates as linear regression problems, which are then solved using recursive matrix operations. The framework works by first training a baseline CBM on initial data, then freezing the backbone parameters. For each new incremental phase, features are extracted and passed through a feature expansion layer. The concept weights are updated using a recursive formula based on the Sherman-Morrison-Woodbury theorem, which allows for efficient computation without retraining. Similarly, the classifier weights are updated recursively. This approach eliminates the need for gradient-based optimization and enables efficient learning of new concepts and classes while preserving knowledge of previously learned concepts and classes.

## Key Results
- CONCIL achieves "absolute knowledge memory" on CUB and AwA datasets, significantly outperforming traditional CBM methods in both concept- and class-incremental settings
- Improvements in average concept accuracy: 19% on CUB, 19.2% on AwA
- Improvements in average class accuracy: 31.5 percentage points on CUB, 50.7 percentage points on AwA
- Drastic reduction in forgetting rates: 88.7% on CUB, 89% on AwA

## Why This Works (Mechanism)
The core mechanism behind CONCIL's success is the reformulation of concept and classifier updates as linear regression problems. By solving these problems using recursive matrix operations based on the Sherman-Morrison-Woodbury theorem, CONCIL avoids the catastrophic forgetting typically associated with gradient-based optimization in continual learning. The feature expansion layer allows for seamless integration of new concepts by increasing the feature dimension, while the recursive updates ensure that previously learned concepts are not forgotten. This approach combines the interpretability of CBMs with the efficiency of recursive updates, making it well-suited for real-time and large-scale multimodal data applications.

## Foundational Learning
- Concept Bottleneck Models (CBMs): Why needed: Provide interpretable intermediate representations between input and output. Quick check: Verify that the model can accurately predict both concepts and final classes.
- Catastrophic forgetting: Why needed: Understanding the challenge of preserving knowledge in continual learning. Quick check: Monitor performance degradation on previous tasks when learning new ones.
- Linear regression formulation: Why needed: Enables efficient updates without gradient-based optimization. Quick check: Verify that the recursive updates converge to the same solution as direct linear regression.
- Sherman-Morrison-Woodbury theorem: Why needed: Allows for efficient recursive updates of matrix inverses. Quick check: Compare computational complexity with and without recursive updates.

## Architecture Onboarding
- Component map: Backbone -> Feature expansion -> Concept layer -> Classifier layer
- Critical path: Feature extraction -> Recursive concept update -> Recursive classifier update -> Evaluation
- Design tradeoffs: Recursive updates vs. gradient-based optimization (efficiency vs. flexibility), frozen backbone vs. fine-tuning (stability vs. adaptability)
- Failure signatures: Catastrophic forgetting (baseline), numerical instability in recursive updates, degradation in concept prediction accuracy
- First experiments: 1) Baseline CBM performance across incremental phases, 2) CONCIL performance comparison with baseline, 3) Ablation study on feature expansion impact

## Open Questions the Paper Calls Out
None

## Limitations
- The exact data split methodology across the 9 incremental phases is not fully specified, which may affect reproducibility
- The initialization scheme for expansion matrices Wfe and W'fe when concept dimension increases is not detailed
- The computational efficiency claims are based on theoretical complexity analysis but require empirical validation on large-scale datasets

## Confidence
- High: The core algorithmic contribution of reformulating CBM updates as linear regression problems is clearly defined and mathematically sound
- Medium: The reported performance improvements are plausible given the methodology, but exact reproducibility depends on implementation details not fully specified
- Medium: The claim of computational efficiency through recursive operations is theoretically justified but requires empirical validation

## Next Checks
1. Implement numerical stability monitoring during recursive matrix updates, tracking condition numbers and detecting potential overflow/underflow issues
2. Verify the incremental learning protocol by implementing the exact 50%/50% split across 9 phases and confirming the number of new classes/concepts per phase matches the reported results
3. Test the feature expansion mechanism with varying concept dimensions to ensure the initialization scheme for Wfe and W'fe maintains model performance across all phases