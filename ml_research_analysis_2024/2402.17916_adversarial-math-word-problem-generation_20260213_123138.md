---
ver: rpa2
title: Adversarial Math Word Problem Generation
arxiv_id: '2402.17916'
source_url: https://arxiv.org/abs/2402.17916
tags:
- llms
- adversarial
- problem
- problems
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a method to generate adversarial math word
  problems that cause large language models to produce incorrect answers. The approach
  uses abstract syntax trees to systematically modify numeric values in problems while
  preserving their original difficulty and coherence.
---

# Adversarial Math Word Problem Generation

## Quick Facts
- arXiv ID: 2402.17916
- Source URL: https://arxiv.org/abs/2402.17916
- Authors: Roy Xie; Chengxuan Huang; Junlin Wang; Bhuwan Dhingra
- Reference count: 22
- One-line primary result: Method generates adversarial math word problems that cause LLMs to fail, achieving up to 100% attack success rates

## Executive Summary
This work introduces a method to generate adversarial math word problems that cause large language models to produce incorrect answers. The approach uses abstract syntax trees to systematically modify numeric values in problems while preserving their original difficulty and coherence. Experiments on multiple open- and closed-source LLMs show significant performance degradation, with attack success rates reaching up to 100%. The method outperforms previous attacks by an average of 62 percentage points. Analysis reveals distinct vulnerabilities across models, and a cost-effective approach is proposed to target high-cost models while reducing API requests by up to 90%.

## Method Summary
The method converts math word problems (MWPs) to Python code using GPT-4, then builds abstract syntax trees (ASTs) to map each calculation step into a node. Numeric values are systematically modified under educational constraints to generate adversarial examples while preserving problem validity and difficulty. The approach tests three generation methods (M1: Free Generation, M2: Count of Digits, M3: Count of Scientific Numbers) on both open-source models (Llama-2, Llama-3, Llama-3.1, Qwen2.5) and closed-source models (GPT-3.5, GPT-4, GPT-4o, GPT-4o-mini) using GSM8K and MultiArith datasets.

## Key Results
- Attack success rates reach up to 100% on tested LLMs
- Method outperforms previous attacks by an average of 62 percentage points
- Cost-effective approach reduces API requests by up to 90% for high-cost models
- Distinct vulnerabilities identified across different LLM architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial math word problems work by systematically modifying numeric values in a problem's AST while preserving difficulty and coherence.
- Mechanism: The method uses abstract syntax trees to map each calculation step into a node, allowing controlled numeric value changes that maintain the original problem's logical structure and difficulty level.
- Core assumption: Preserving intermediate calculation consistency and maintaining order of magnitude constraints ensures the modified problem remains solvable by humans but not by LLMs.
- Evidence anchors:
  - [abstract] "leverage abstract syntax trees to structurally generate adversarial examples that cause LLMs to produce incorrect answers by simply editing the numeric values in the problems"
  - [section] "To tackle these challenges, we first convert MWPs to a code representation and leverage abstract syntax trees (ASTs) to map each calculation step into a node"
- Break condition: If the AST conversion fails to capture all calculation steps or if constraint enforcement allows logically inconsistent modifications, the method would fail to preserve problem validity while maintaining attack effectiveness.

### Mechanism 2
- Claim: LLMs fail on adversarial problems because they rely on pattern memorization rather than true mathematical reasoning.
- Mechanism: When numeric values are changed, the learned statistical associations that LLMs use for problem-solving are disrupted, causing incorrect predictions even when the underlying mathematical logic remains the same.
- Core assumption: LLMs primarily solve math problems through pattern matching and memorization rather than understanding mathematical concepts and reasoning steps.
- Evidence anchors:
  - [abstract] "analysis reveals distinct vulnerabilities across models"
  - [section] "Modifying the numbers in a problem may disrupt these learned patterns, causing the models to make errors"
- Break condition: If LLMs develop stronger reasoning capabilities that can generalize beyond memorized patterns, or if they learn to focus on structural relationships rather than specific numeric values, the attack would become less effective.

### Mechanism 3
- Claim: Different LLMs have distinct vulnerabilities to adversarial examples based on their training data and architectural characteristics.
- Mechanism: The regression analysis shows that various features like operation counts, answer value ranges, and node counts in the problem's AST correlate differently with model correctness, indicating model-specific weaknesses.
- Core assumption: Each LLM has learned different specializations and weaknesses based on its training data distribution and tokenization choices.
- Evidence anchors:
  - [abstract] "Analysis reveals distinct vulnerabilities across models"
  - [section] "We conduct a regression analysis and find that our adversarial examples exploit different weaknesses of each model"
- Break condition: If models are fine-tuned specifically to handle adversarial examples or if they develop more robust generalization capabilities across different numeric ranges and operation types.

## Foundational Learning

- Concept: Abstract Syntax Trees (ASTs)
  - Why needed here: ASTs provide a structured way to represent the computational steps in math word problems, enabling systematic modification of numeric values while preserving the problem's logical structure.
  - Quick check question: What are the two main types of nodes in the AST representation used for math word problems, and what does each represent?

- Concept: Adversarial Attack Methodology
  - Why needed here: Understanding how adversarial examples work in the context of language models is crucial for designing effective attacks that maintain problem validity while causing model failures.
  - Quick check question: How does the attack success rate (ASR) metric differ from standard accuracy metrics, and why is it more appropriate for evaluating adversarial attacks?

- Concept: Mathematical Reasoning vs Pattern Matching
  - Why needed here: The fundamental difference between how humans solve math problems versus how LLMs approach them explains why simple numeric modifications can cause significant performance degradation.
  - Quick check question: What key distinction between human mathematical reasoning and LLM problem-solving approaches makes numeric modifications effective as adversarial attacks?

## Architecture Onboarding

- Component map: MWP problems -> GPT-4 code generator -> Python code -> AST builder -> Constraint checker -> Adversarial generator -> Modified problems -> LLM evaluator
- Critical path: Converting original problems to Python code, building ASTs, generating modified numeric values under constraints, and evaluating modified problems against target LLMs
- Design tradeoffs: The system trades off between attack effectiveness and problem validity - more aggressive numeric modifications increase attack success rates but risk creating unrealistic or illogical problems
- Failure signatures: Common failures include invalid Python code generation, AST conversion errors for complex expressions, constraint violations that create unrealistic problems, and LLMs that actually perform better on modified versions due to unexpected patterns
- First 3 experiments:
  1. Validate the code generation pipeline by testing it on a small sample of problems and checking for correct Python output and AST conversion
  2. Test the constraint enforcement system by generating modified problems and manually verifying they maintain logical consistency and difficulty level
  3. Run a small-scale attack on one open-source model to verify the basic attack mechanism works before scaling up to larger experiments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the difficulty level of generated adversarial examples correlate with actual student performance in solving them?
- Basis in paper: [inferred] The paper mentions that human evaluation confirmed the generated problems maintain educational validity and difficulty, but does not provide empirical data on how students perform on these problems compared to the original ones.
- Why unresolved: The paper focuses on LLM performance and validation through human evaluators, but does not conduct actual student testing to establish a quantitative correlation between problem complexity and student difficulty.
- What evidence would resolve it: Experimental results showing student success rates on original vs. adversarial examples, with statistical analysis of difficulty correlation.

### Open Question 2
- Question: What is the long-term effectiveness of adversarial examples as LLM capabilities improve over time?
- Basis in paper: [explicit] The paper notes that as LLMs become stronger, detecting their outputs will become more difficult, but adversarial examples may still persist, and mentions this is a challenge faced by all adversarial attack methods.
- Why unresolved: The study is conducted on current LLM capabilities without longitudinal testing to see how quickly models adapt to these adversarial patterns.
- What evidence would resolve it: Longitudinal studies testing the same adversarial examples against newer model versions over time, with performance degradation metrics.

### Open Question 3
- Question: How does tokenization strategy impact LLM vulnerability to numerical reasoning attacks?
- Basis in paper: [explicit] The paper mentions that Llama-based models tokenize each digit individually while GPT-3.5 and GPT-4 encode every three digits into a single token, and observes consistent performance differences related to answer token count.
- Why unresolved: While the paper notes this difference, it doesn't conduct controlled experiments isolating tokenization effects from other model differences.
- What evidence would resolve it: Controlled experiments with models using different tokenization strategies but identical architectures, measuring attack success rates specifically for problems with varying numerical representations.

## Limitations
- The method relies heavily on the quality of Python code generation from MWPs using GPT-4, which is not fully specified in the methodology
- The constraint enforcement system's effectiveness depends on accurate AST mapping and number alignment, which may not generalize well to more complex problem types
- The evaluation focuses primarily on attack success rates without comprehensive analysis of how these adversarial examples transfer across different LLM architectures

## Confidence

- **High Confidence**: The basic methodology of using ASTs to systematically modify numeric values in MWPs is well-established and technically sound. The experimental results showing significant performance degradation (up to 100% ASR) are well-documented.
- **Medium Confidence**: The claim that distinct vulnerabilities exist across different LLM models is supported by the regression analysis, but the interpretation of what these vulnerabilities mean for practical deployment scenarios requires additional validation.
- **Low Confidence**: The assertion that the attack success is primarily due to pattern memorization rather than reasoning is largely theoretical and not directly tested in the experiments.

## Next Checks

1. **Code Generation Validation**: Test the GPT-4 code generation pipeline on a larger, diverse sample of MWPs to measure the error rate and identify failure patterns that could affect downstream attack generation.

2. **Cross-Model Transferability**: Generate adversarial examples for one model and test them across multiple other models to quantify how transferable these attacks are and whether certain attack patterns are universally effective.

3. **Human vs LLM Performance Gap**: Conduct controlled experiments measuring both human and LLM performance on the same set of original and adversarial MWPs to validate that the modifications maintain educational validity while breaking LLM performance.