---
ver: rpa2
title: Can Language Models Rival Mathematics Students? Evaluating Mathematical Reasoning
  through Textual Manipulation and Human Experiments
arxiv_id: '2412.11908'
source_url: https://arxiv.org/abs/2412.11908
tags:
- problem
- problems
- gpt-4
- mathematical
- participants
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the mathematical reasoning capabilities of
  large language models (LLMs) on combinatorial problems through the Combi-Puzzles
  dataset. The dataset contains 125 problem variants based on 25 combinatorial reasoning
  problems, presented in five distinct forms through systematic textual manipulation.
---

# Can Language Models Rival Mathematics Students? Evaluating Mathematical Reasoning through Textual Manipulation and Human Experiments

## Quick Facts
- arXiv ID: 2412.11908
- Source URL: https://arxiv.org/abs/2412.11908
- Authors: Andrii Nikolaiev; Yiannos Stathopoulos; Simone Teufel
- Reference count: 34
- LLMs, particularly GPT-4, outperform humans on mathematical variations of combinatorial problems

## Executive Summary
This study evaluates the mathematical reasoning capabilities of large language models (LLMs) on combinatorial problems through systematic textual manipulation. The researchers created the Combi-Puzzles dataset containing 125 problem variants based on 25 combinatorial reasoning problems, each presented in five distinct forms. Four models (LLaMA-2, LLaMA-3.1, GPT-4, and Mixtral) were compared against human participants with mathematical olympiad experience. GPT-4 demonstrated the highest performance across all problem variations, significantly outperforming other models and human participants on mathematical variations. The study reveals that while LLMs excel at mathematical reasoning, their performance is highly sensitive to textual variations in problem statements.

## Method Summary
The researchers systematically created the Combi-Puzzles dataset by manipulating 25 combinatorial problems into 125 variants across five forms: common, mathematical, adversarial, parameterisation, and linguistic obfuscation. They evaluated four LLMs (LLaMA-2, LLaMA-3.1, GPT-4, Mixtral) using two prompting strategies (with and without "The short and correct answer is" prompt) and compared results against human participants with mathematical olympiad experience. Model responses were manually evaluated against an established marking scheme, and performance was measured by percentage of correct answers across all problem variations.

## Key Results
- GPT-4 outperformed all other models and achieved significantly better results than human participants on mathematical variations
- LLM performance was highly sensitive to textual variations in problem statements, with significant performance drops across different variations
- Human performance remained unaffected by modifications to problem statements, while LLM performance varied considerably
- LLaMA-2, LLaMA-3.1, and Mixtral showed substantially lower performance compared to GPT-4 across all problem variations

## Why This Works (Mechanism)
The study demonstrates that GPT-4's superior mathematical reasoning capabilities stem from its ability to parse and understand mathematical notation and structure, particularly in mathematical variations of problems. However, the sensitivity of LLMs to textual variations suggests that their reasoning capabilities are heavily dependent on the specific formulation of problems rather than underlying mathematical understanding.

## Foundational Learning
- **Combinatorial problem structures**: Understanding different types of combinatorial problems is needed to recognize the mathematical core that must be preserved across variations. Quick check: Can identify the invariant mathematical structure in each problem type.
- **Textual manipulation techniques**: Knowledge of how to systematically vary problem statements while preserving mathematical content is crucial for creating controlled experiments. Quick check: Can generate multiple valid variations of a given problem while maintaining mathematical equivalence.
- **Prompt engineering strategies**: Understanding different prompting approaches helps in optimizing model performance and interpreting results. Quick check: Can design prompts that elicit clear, structured responses from language models.

## Architecture Onboarding
**Component Map**: Dataset Creation -> Model Evaluation -> Human Testing -> Performance Analysis -> Comparison
**Critical Path**: Creating controlled problem variations → Running models with consistent prompts → Manual evaluation → Statistical comparison with human performance
**Design Tradeoffs**: The study chose controlled textual manipulation over naturalistic problem variation, prioritizing experimental control over ecological validity. Manual evaluation ensures accuracy but limits scalability.
**Failure Signatures**: Models produce long, inconsistent responses; performance drops significantly on linguistic obfuscation; mathematical notation parsing fails on complex variations.
**First Experiments**:
1. Test each model separately on each variation type to identify specific weaknesses
2. Run sensitivity analysis on prompt variations to optimize performance
3. Compare chain-of-thought vs direct answer approaches for complex problems

## Open Questions the Paper Calls Out
### Open Question 1
How do different types of linguistic obfuscation (narrative vs descriptive styles) specifically affect LLM performance compared to humans? The paper notes that adversarial variation is more descriptive while linguistic obfuscation adopts a narration style, and this might influence the ability of GPT-4 to extract relevant information from unfamiliar mathematical text. This remains unresolved because the study only tested one type of linguistic obfuscation and did not compare different narrative styles directly.

### Open Question 2
Does the sensitivity of LLMs to textual variations indicate fundamental limitations in mathematical reasoning or just a need for fine-tuning? The paper states that GPT-4's performance drops significantly across different variations and suggests this "might not be able to generalise effectively without explicit fine-tuning." This remains unresolved because the study only tested performance on a single dataset without attempting to fine-tune the models on similar variations.

### Open Question 3
What specific cognitive differences in mathematical reasoning exist between LLMs and human participants? The authors state they intend to design experiments specifically for examining the reasoning steps taken by LLMs and human participants in order to better understand their cognitive differences in mathematical reasoning. This remains unresolved because the current study only evaluated final answers, not the reasoning processes.

## Limitations
- Ukrainian translations for human participants may introduce language-specific effects not generalizable to other contexts
- Small dataset size (125 variants across 25 problems) limits statistical power for analyzing performance across different problem types
- Only four specific LLM models tested, which may not represent the full spectrum of available language models

## Confidence
- High: GPT-4 significantly outperforms other models and humans on mathematical variations
- Medium: Sensitivity of LLM performance to textual variations is well-established but mechanisms unclear
- Low: Generalizability to non-olympiad participants and other language contexts

## Next Checks
1. Replicate the study using English translations for human participants to control for potential language effects and improve comparability with other research
2. Expand the dataset size by adding more combinatorial problems and increasing the number of variants per problem to improve statistical power
3. Test additional LLM models including newer architectures and explore more diverse prompting strategies including chain-of-thought and few-shot prompting approaches