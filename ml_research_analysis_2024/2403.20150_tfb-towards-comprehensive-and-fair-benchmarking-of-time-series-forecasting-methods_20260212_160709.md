---
ver: rpa2
title: 'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting
  Methods'
arxiv_id: '2403.20150'
source_url: https://arxiv.org/abs/2403.20150
tags:
- time
- methods
- series
- forecasting
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'TFB provides a comprehensive and fair benchmarking framework for
  time series forecasting (TSF) methods. It addresses three key limitations in existing
  benchmarks: insufficient coverage of data domains, stereotype bias against traditional
  methods, and inconsistent evaluation pipelines.'
---

# TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods

## Quick Facts
- arXiv ID: 2403.20150
- Source URL: https://arxiv.org/abs/2403.20150
- Reference count: 40
- Key outcome: TFB provides comprehensive benchmarking across 10 domains with 22 methods, revealing that statistical methods often outperform deep learning on specific datasets.

## Executive Summary
TFB addresses critical limitations in existing time series forecasting benchmarks by providing a comprehensive and fair evaluation framework. It tackles three key issues: insufficient domain coverage, stereotype bias against traditional methods, and inconsistent evaluation pipelines. The framework includes datasets from 10 diverse domains spanning 8,068 univariate and 25 multivariate time series, evaluating 22 methods from statistical learning, machine learning, and deep learning approaches. Key findings reveal that simpler statistical methods like VAR and LinearRegression can outperform recent deep learning methods on specific datasets, highlighting the importance of method selection based on time series characteristics.

## Method Summary
TFB implements a unified pipeline that standardizes data handling, splitting, normalization, and evaluation across all forecasting methods to ensure fair comparisons. The framework supports both univariate and multivariate forecasting tasks with rolling and fixed evaluation strategies, multiple error metrics (MAE, MSE, MAPE, etc.), and compatibility with various TSF libraries. It evaluates 22 methods spanning statistical learning (ARIMA, ETS, VAR, KF), machine learning (XGBoost, LinearRegression, Random Forest), and deep learning approaches (RNN, CNN, MLP, Transformer, Model-Agnostic). The pipeline eliminates biases from inconsistent experimental settings by standardizing experimental configurations including batch sizes, data splitting, and evaluation procedures.

## Key Results
- Statistical methods like VAR and LinearRegression often outperform recent deep learning methods on certain datasets (NASDAQ, ILI, Wind)
- Linear-based methods excel with increasing trends or significant shifts in time series data
- Transformer-based methods outperform linear methods on datasets with strong seasonality and nonlinear patterns
- Methods considering channel dependencies significantly improve multivariate forecasting performance on correlated datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TFB's unified pipeline eliminates biases from inconsistent experimental settings.
- Mechanism: By standardizing data handling, splitting, normalization, and evaluation across all methods, TFB ensures that performance differences reflect true method capabilities rather than implementation artifacts.
- Core assumption: Differences in experimental setup (e.g., batch size, data splitting) significantly impact reported performance.
- Evidence anchors:
  - [section] "we identify and address three issues in existing evaluation frameworks, thereby advancing our evaluation capabilities. Issue 3. Lack of consistent and flexible pipelines."
  - [section] "To accelerate the testing, it is common to split the data into batches. However, if we discard the last incomplete batch with fewer instances than the batch size, this causes unfair comparisons."

### Mechanism 2
- Claim: Comprehensive dataset coverage across 10 domains captures diverse time series characteristics, preventing overfitting to narrow data patterns.
- Mechanism: By including datasets from traffic, electricity, energy, environment, nature, economic, stock markets, banking, health, and web domains, TFB ensures methods are tested on varied trend, seasonality, shifting, and correlation patterns.
- Core assumption: Method performance is domain-dependent and narrow benchmarks miss critical failure modes.
- Evidence anchors:
  - [abstract] "TFB includes datasets from 10 diverse domains (traffic, electricity, energy, environment, nature, economic, stock markets, banking, health, and web)"
  - [section] "Time series from different domains may exhibit diverse characteristics. Figure 1a depicts a time series from the environment domain called AQShunyi [93] that records temperature information at hourly intervals, exhibiting a distinct seasonal pattern."

### Mechanism 3
- Claim: Including both traditional and modern methods prevents stereotype bias against statistical approaches.
- Mechanism: By benchmarking statistical methods like VAR and LinearRegression alongside deep learning methods, TFB reveals when simpler methods outperform complex ones on specific dataset characteristics.
- Core assumption: Recent deep learning methods are not universally superior and may underperform on datasets with certain patterns.
- Evidence anchors:
  - [section] "Surprisingly, VAR outperforms all recently proposed SOTA methods on NASQAD and is better than FEDformer and Crossformer on ILI. Furthermore, LR performs better than recently proposed SOTA methods on Wind."
  - [section] "This discovery highlights the need for evaluating method performance across diverse datasets."

## Foundational Learning

- Concept: Time series characteristics (trend, seasonality, stationarity, shifting, transition, correlation)
  - Why needed here: Understanding these characteristics is essential for interpreting why different methods perform better on different datasets and for designing appropriate forecasting approaches.
  - Quick check question: What characteristic would make linear-based methods particularly effective, and why?

- Concept: Evaluation strategies (fixed vs. rolling forecasting)
  - Why needed here: Different forecasting applications require different evaluation approaches, and understanding these helps in selecting appropriate methods for specific use cases.
  - Why needed here: The choice between fixed and rolling forecasting affects how methods are implemented and compared, impacting fairness and applicability.
  - Quick check question: When would rolling forecasting be more appropriate than fixed forecasting for a production system?

- Concept: Error metrics (MAE, MSE, MAPE, SMAPE, RMSE, WAPE, MSMAPE, MASE)
  - Why needed here: Different metrics emphasize different aspects of forecasting performance, and comprehensive evaluation requires understanding when each metric is most informative.
  - Quick check question: Which metric would be most appropriate for comparing methods on datasets with varying scales?

## Architecture Onboarding

- Component map: Dataset repository -> Method interface -> Evaluation strategy -> Metrics calculation -> Reporting
- Critical path: Load dataset → Configure method with hyperparameters → Run evaluation strategy → Calculate metrics → Generate report
- Design tradeoffs: Comprehensive coverage vs. computational cost, flexibility vs. standardization, depth of analysis vs. ease of use
- Failure signatures: Inconsistent results across runs (suggests non-deterministic processing), methods failing to load (interface compatibility issues), metrics not calculated (evaluation pipeline errors)
- First 3 experiments:
  1. Run TFB with default configuration on a single small dataset to verify basic functionality
  2. Compare two methods with known performance differences on a dataset where one should excel
  3. Test the pipeline with a custom method to verify the Universal Interface works correctly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different time series characteristics (trend, seasonality, stationarity, shifting, transition, correlation) interact to influence the performance of specific forecasting methods, and can these interactions be quantified to guide method selection?
- Basis in paper: [explicit] The paper demonstrates that method performance varies significantly with time series characteristics and provides examples of how linear-based and transformer-based methods perform differently on datasets with specific characteristics. However, the interactions between multiple characteristics are not systematically explored.
- Why unresolved: The paper analyzes individual characteristics' effects on method performance but doesn't investigate how combinations of characteristics affect method selection. For example, how does the presence of both strong seasonality and significant shifting impact transformer vs linear methods?
- What evidence would resolve it: Controlled experiments testing method performance across all combinations of characteristic strengths, with statistical analysis of interaction effects. This would require systematic generation or selection of datasets representing all characteristic combinations.

### Open Question 2
- Question: What is the optimal balance between considering channel dependencies and computational efficiency in multivariate time series forecasting, and how does this trade-off vary across different correlation strengths?
- Basis in paper: [explicit] The paper shows that Crossformer (which considers channel dependencies) outperforms PatchTST on highly correlated datasets, but the performance advantage diminishes when correlations are weak. However, the computational costs of each approach are not quantified.
- Why unresolved: While the paper demonstrates the performance benefits of channel dependency modeling on correlated datasets, it doesn't quantify the computational overhead or determine when the benefits outweigh the costs across different correlation levels.
- What evidence would resolve it: Comparative studies measuring both forecasting accuracy and computational efficiency (time, memory) across datasets with varying correlation strengths, identifying the break-even points where channel dependency modeling becomes worthwhile.

### Open Question 3
- Question: How do statistical learning methods like VAR and LinearRegression maintain competitive performance against deep learning methods on certain datasets, and what dataset characteristics make them particularly effective?
- Basis in paper: [explicit] The paper finds that VAR outperforms recent deep learning methods on NASDAQ and is better than FEDformer and Crossformer on ILI, while LinearRegression performs better than recent SOTA methods on Wind. However, the underlying reasons for this performance are not explored.
- Why unresolved: The paper demonstrates that traditional methods can outperform deep learning approaches but doesn't investigate the mechanisms behind this performance or identify the specific dataset properties that favor statistical methods.
- What evidence would resolve it: Detailed analysis of the mathematical properties of statistical methods that give them advantages on certain datasets, combined with characterization of dataset features that predict when these advantages will manifest. This could include sensitivity analysis of method parameters across different dataset types.

## Limitations
- High computational resource requirements due to comprehensive evaluation of 22 methods across 8,068 univariate and 25 multivariate datasets
- May not capture all possible time series characteristics that exist in real-world applications, particularly emerging domains
- Reliance on standardized evaluation strategies may not fully account for domain-specific nuances in forecasting requirements

## Confidence
- High confidence: The core claim that TFB provides a unified, standardized pipeline for fair comparison of TSF methods is well-supported by the detailed methodology and implementation specifics provided.
- Medium confidence: The findings about statistical methods outperforming deep learning on certain datasets are supported by empirical results, but may be sensitive to the specific datasets and evaluation settings used.
- Medium confidence: The assertion that transformer-based methods excel on datasets with strong seasonality and nonlinear patterns is based on observed performance differences, though the generalizability to all such datasets requires further validation.

## Next Checks
1. **Cross-validation on new datasets**: Apply TFB's methodology to a held-out set of datasets not used in the original evaluation to verify that the observed performance patterns hold across broader time series characteristics.

2. **Resource scaling analysis**: Systematically evaluate how TFB's resource requirements scale with dataset size and number of methods to identify practical limitations for different computational environments.

3. **Domain-specific adaptation**: Test whether TFB's standardized pipeline can be effectively adapted for domain-specific forecasting requirements (e.g., medical time series with irregular sampling) without compromising the fairness guarantees.