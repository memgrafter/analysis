---
ver: rpa2
title: Learning from Implicit User Feedback, Emotions and Demographic Information
  in Task-Oriented and Document-Grounded Dialogues
arxiv_id: '2401.09248'
source_url: https://arxiv.org/abs/2401.09248
tags:
- user
- feedback
- dialogues
- dialogue
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FEDI, the first English dialogue dataset for
  task-oriented and document-grounded conversations annotated with demographic information,
  user emotions, and implicit user feedback. It demonstrates that incorporating these
  annotations into language models like Flan-T5, GPT-2, and Llama 2 improves task
  completion and factual consistency of generated responses.
---

# Learning from Implicit User Feedback, Emotions and Demographic Information in Task-Oriented and Document-Grounded Dialogues

## Quick Facts
- **arXiv ID**: 2401.09248
- **Source URL**: https://arxiv.org/abs/2401.09248
- **Reference count**: 40
- **Primary result**: Incorporating demographic information, user emotions, and implicit feedback into language models improves task completion and factual consistency in task-oriented dialogues.

## Executive Summary
This paper introduces FEDI, the first English dialogue dataset for task-oriented and document-grounded conversations annotated with demographic information, user emotions, and implicit user feedback. The dataset was created using GPT-3.5-Turbo to generate synthetic dialogues, which were then curated by human annotators. The authors demonstrate that incorporating these additional annotations into language models like Flan-T5, GPT-2, and Llama 2 significantly improves task completion and factual consistency of generated responses. Human evaluation confirmed that responses from feedback-trained models are perceived as more informative, relevant, and factually consistent.

## Method Summary
The authors used GPT-3.5-Turbo to generate synthetic dialogues with annotations for demographic information, user emotions, and implicit feedback. Human annotators curated the data to ensure quality. Three language models (Flan-T5, GPT-2, and LLaMA-2) were fine-tuned first on feedback-free dialogues incorporating demographic information and emotions, then further trained on feedback dialogues that included generation errors and user reaction types. This continual learning approach aimed to improve the models' ability to handle errors and implicit feedback scenarios.

## Key Results
- Incorporating demographic information, user emotions, and implicit feedback into language models improves task completion and factual consistency in task-oriented dialogues.
- LLaMA-2 showed the most significant improvements when incorporating user emotions into the input sequences.
- Responses from feedback-trained models were perceived by human evaluators as more informative, relevant, and factually consistent compared to baseline models.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorporating demographic information, user emotions, and implicit feedback as input features improves task completion and factual consistency in dialogue systems.
- Mechanism: These additional contextual signals provide the model with a richer understanding of the user's needs, preferences, and emotional state, allowing it to generate more relevant and accurate responses.
- Core assumption: The model can effectively process and utilize the additional information provided in the input sequence to improve its generation capabilities.
- Evidence anchors:
  - [abstract] "Our experiments with FLAN-T5, GPT-2 and LLaMA-2 show a particularly positive impact on task completion and factual consistency."
  - [section 7] "In general, we find that including user emotions (+Emotions) has a positive impact. This is particularly significant in the case of LLaMA-2..."
  - [corpus] Weak evidence - related papers focus on emotion detection and task-oriented dialogue modeling, but do not directly address the combined impact of demographic information, emotions, and feedback.

### Mechanism 2
- Claim: Using GPT-3.5-Turbo to generate synthetic dialogue data with annotations for demographic information, emotions, and implicit feedback is an efficient and effective approach.
- Mechanism: GPT-3.5-Turbo can generate large amounts of high-quality dialogue data with the desired annotations, reducing the need for expensive and time-consuming human annotation.
- Core assumption: GPT-3.5-Turbo can generate coherent and diverse dialogues that accurately reflect the target use cases and annotation schemes.
- Evidence anchors:
  - [section 3] "To create FEDI in a cost and resource efficient way, we use GPT-3.5-Turbo to generate and annotate the training and validation data..."
  - [section 6] "To assess the quality of our generated dialogues, we used INCEpTION... and asked two participants from our test data collection to assess and curate the intent, slot and emotion annotations..."
  - [corpus] Weak evidence - related papers focus on dataset generation and annotation, but do not directly address the use of GPT-3.5-Turbo for this specific purpose.

### Mechanism 3
- Claim: The continual learning approach of resolving feedback scenarios in generated dialogues improves the model's ability to handle errors and implicit feedback.
- Mechanism: By providing the model with examples of both erroneous and corrected system utterances, it learns to recognize and respond appropriately to different types of errors and user reactions.
- Core assumption: The model can effectively learn from the contrasting examples of erroneous and corrected responses.
- Evidence anchors:
  - [section 4.2] "We first mask the system utterance with generation error and generate a replacement using the task description and the preceding dialogue context..."
  - [section 7] "In the feedback experiments, we observe great improvements in task completion and factual consistency of generated responses (Q2 metric), which are both crucial for task-oriented document-grounded dialogues."
  - [corpus] Weak evidence - related papers focus on learning from human feedback, but do not directly address the specific approach of resolving feedback scenarios in generated dialogues.

## Foundational Learning

- Concept: Task-oriented dialogue systems
  - Why needed here: FEDI focuses on task-oriented and document-grounded dialogues, which require the system to complete specific tasks and provide accurate information from external documents.
  - Quick check question: What are the key differences between task-oriented and open-domain dialogue systems?

- Concept: Dialogue state tracking
  - Why needed here: The paper mentions the use of belief states (Bt) in the dialogue generation process, which involve tracking the user's intent and slot values throughout the conversation.
  - Quick check question: How does dialogue state tracking contribute to the overall performance of task-oriented dialogue systems?

- Concept: Implicit user feedback
  - Why needed here: FEDI includes annotations for implicit user feedback, which refers to user reactions that indicate dissatisfaction or confusion with the system's previous utterance.
  - Quick check question: What are some common types of implicit user feedback, and how can dialogue systems learn to recognize and respond to them?

## Architecture Onboarding

- Component map: Data generation (GPT-3.5-Turbo) -> Data curation (human annotators) -> Model training (fine-tuning of FLAN-T5, GPT-2, LLaMA-2) -> Evaluation (metrics and human evaluation)
- Critical path: Generate synthetic data → Curate and assess data quality → Train models on curated data → Evaluate model performance
- Design tradeoffs:
  - Synthetic vs. human-generated data: Synthetic data is more cost-effective and scalable, but may lack the diversity and naturalness of human-generated data.
  - Model architecture: Different architectures (FLAN-T5, GPT-2, LLaMA-2) may have varying strengths and weaknesses in handling the additional input features and feedback scenarios.
- Failure signatures:
  - Poor data quality: Inaccurate or inconsistent annotations, lack of diversity in generated dialogues
  - Model overfitting: Inability to generalize beyond the training data, especially if the synthetic data is not representative
  - Evaluation bias: Metrics may not fully capture the nuances of task completion and factual consistency in dialogue systems
- First 3 experiments:
  1. Generate a small set of synthetic dialogues using GPT-3.5-Turbo and manually assess their quality and diversity.
  2. Train a simple model (e.g., a basic seq2seq model) on a subset of the curated data and evaluate its performance on a held-out test set.
  3. Compare the performance of different model architectures (FLAN-T5, GPT-2, LLaMA-2) on the same task and data split to identify the most promising approach.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the quality of annotations in FEDI, especially for slot annotations which currently have a high rate of missing values?
- Basis in paper: [inferred] The paper mentions that slot annotations are particularly challenging and have a high rate of missing values, especially in parcel shipping dialogues. They attribute this to the complex annotation scheme and the dependence of slots on the background story.
- Why unresolved: The paper does not provide specific solutions or future work directions for improving slot annotation quality.
- What evidence would resolve it: Developing a more sophisticated slot annotation scheme that takes into account the background story, or exploring alternative annotation methods like active learning or weak supervision.

### Open Question 2
- Question: How can we generalize the dialogue generation and annotation framework to other tasks and domains to increase the representativeness of FEDI?
- Basis in paper: [inferred] The paper mentions that FEDI is currently limited to four use cases in three domains. They plan to extend it to other tasks and domains in future work.
- Why unresolved: The paper does not provide details on how to adapt the framework to new tasks and domains.
- What evidence would resolve it: Developing a modular framework that can be easily adapted to new tasks and domains, or conducting experiments to evaluate the framework's generalizability.

### Open Question 3
- Question: How can we extend FEDI to be multimodal, including images that represent emotions and audio signals?
- Basis in paper: [explicit] The paper mentions that they plan to extend FEDI to be multimodal in future work, including images and audio signals.
- Why unresolved: The paper does not provide details on how to collect and annotate multimodal data, or how to integrate it into the existing framework.
- What evidence would resolve it: Developing a pipeline for collecting and annotating multimodal data, or conducting experiments to evaluate the impact of multimodal information on dialogue performance.

## Limitations
- The synthetic nature of the FEDI dataset introduces potential quality concerns, as hallucinations and unnatural dialogues may occur despite human curation efforts.
- The evaluation metrics, while comprehensive, may not fully capture the nuanced improvements in task completion and factual consistency when incorporating emotions and feedback.
- The study focuses on English dialogues only, limiting generalizability to other languages and cultural contexts.

## Confidence
- **High Confidence**: The core finding that incorporating emotions and implicit feedback improves task completion and factual consistency in task-oriented dialogues. This is supported by multiple model architectures showing consistent improvements.
- **Medium Confidence**: The effectiveness of GPT-3.5-Turbo for generating synthetic dialogue data with annotations. While the approach is shown to be feasible, the quality of generated data may vary.
- **Medium Confidence**: The continual learning approach for resolving feedback scenarios. The paper demonstrates improvements in handling errors and feedback, but the long-term effectiveness across diverse error types remains to be fully validated.

## Next Checks
1. Conduct a systematic analysis of the synthetic data quality by measuring hallucination rates and naturalness scores across different demographic combinations and emotional contexts.
2. Perform cross-lingual validation by adapting the FEDI dataset to other languages and evaluating whether the observed improvements in task completion and factual consistency transfer across linguistic and cultural boundaries.
3. Implement an ablation study to isolate the individual contributions of demographic information, emotions, and implicit feedback to model performance, determining which factors provide the most significant improvements.