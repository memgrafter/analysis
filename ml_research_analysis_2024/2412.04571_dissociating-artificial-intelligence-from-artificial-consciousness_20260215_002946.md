---
ver: rpa2
title: Dissociating Artificial Intelligence from Artificial Consciousness
arxiv_id: '2412.04571'
source_url: https://arxiv.org/abs/2412.04571
tags:
- state
- units
- computer
- effect
- cause
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies Integrated Information Theory (IIT) to examine
  whether a computer functionally equivalent to a human brain would also have human-like
  consciousness. The authors simulate a simple four-bit computer (117 units) programmed
  to replicate the behavior of a minimal 4-unit target system (PQRS).
---

# Dissociating Artificial Intelligence from Artificial Consciousness

## Quick Facts
- arXiv ID: 2412.04571
- Source URL: https://arxiv.org/abs/2412.04571
- Reference count: 40
- Primary result: A computer functionally equivalent to a human brain would not have human-like consciousness under IIT

## Executive Summary
This paper applies Integrated Information Theory (IIT) to examine whether a computer functionally equivalent to a human brain would also have human-like consciousness. The authors simulate a simple four-bit computer (117 units) programmed to replicate the behavior of a minimal 4-unit target system (PQRS). Despite functional equivalence, the computer fragments into 24 small complexes, each specifying trivial cause–effect structures, unlike the single rich structure of the target. This dissociation holds across states, grain sizes, and even for Turing-complete versions simulating complex systems. The findings challenge computational functionalism by demonstrating that functional equivalence does not imply phenomenal equivalence under IIT.

## Method Summary
The authors apply IIT's causal powers analysis to a 4-unit target system (PQRS) and a 117-unit 4-bit computer that simulates it. They identify complexes and unfold their cause-effect structures at both micro and macro grains. The computer's architecture consists of a clock, frequency dividers, program memory, an instruction register, data registers, and a multiplexer, all implemented using simple Boolean units. The analysis demonstrates that despite functional equivalence, the computer fragments into multiple small complexes with trivial cause-effect structures, while the target forms a single integrated complex.

## Key Results
- The 117-unit computer fragments into 24 small complexes, each with trivial cause-effect structures, unlike the single rich structure of the 4-unit target
- Treating the computer's units at macro grains does not change this conclusion
- Extending the computer to Turing-complete versions confirms the dissociation holds for arbitrarily complex simulations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Functional equivalence does not imply phenomenal equivalence under IIT.
- Mechanism: IIT evaluates intrinsic causal powers at multiple grains; a computer simulating a target system fails to match the target's cause-effect structure because it fragments into many small complexes, each with trivial cause-effect structures.
- Core assumption: The computer's architecture inherently creates bottlenecks and weak connectivity that prevent it from forming a single integrated complex.
- Evidence anchors:
  - [abstract] "Despite functional equivalence, the computer fragments into 24 small complexes, each specifying trivial cause–effect structures, unlike the single rich structure of the target."
  - [section] "The computer fragments into multiple complexes, none of which specifies a cause–effect structure identical to that of PQRS."
  - [corpus] Weak, but the related paper "Is artificial consciousness achievable? Lessons from the human brain" explores similar structural dissociation themes.
- Break condition: If the computer's architecture were fundamentally changed to eliminate bottlenecks and enable full integration, this mechanism would break.

### Mechanism 2
- Claim: Even at macro grains, the computer cannot replicate the target's cause-effect structure.
- Mechanism: Macroing the computer's units according to IIT's postulates still results in overlapping causal powers or omission of essential interactions, violating the integration and exclusion postulates.
- Core assumption: The computer's functional organization inherently prevents the formation of macro units that satisfy IIT's postulates.
- Evidence anchors:
  - [abstract] "Furthermore, we show that treating the computer's units and states at macro grains does not change this conclusion."
  - [section] "In fact, as shown in the 'Macro grain analysis of the strongly connected computer' supplement, there is no macroing of the computer reflecting its function as a simulans that can replicate the cause–effect structure of PQRS."
  - [corpus] Weak, but the related paper "Neuromorphic Correlates of Artificial Consciousness" suggests that physical architecture impacts causal integration.
- Break condition: If a new macroing scheme could be found that both respects IIT's postulates and replicates the target's cause-effect structure, this mechanism would break.

### Mechanism 3
- Claim: The dissociation holds for arbitrarily large computers simulating arbitrarily complex systems.
- Mechanism: As the computer size increases, the number of small complexes grows linearly while the potential cause-effect structures of simulated systems can grow double-exponentially, widening the gap.
- Core assumption: The computer's architecture, when scaled up, maintains its tendency to fragment into small complexes.
- Evidence anchors:
  - [abstract] "Finally, we extend the four-bit computer to be Turing-complete and demonstrate that the previous results do not depend on the complexity of the function being implemented."
  - [section] "The dissociation shown above...can be exacerbated if the targets are large and have highΦ...The magnitude of this dissociation can be a double-exponential function of the size of the simulated system."
  - [corpus] Weak, but the related paper "Agency in Artificial Intelligence Systems" touches on the limitations of current architectures for achieving general intelligence.
- Break condition: If a radically different computer architecture could be designed that scales up without fragmenting into small complexes, this mechanism would break.

## Foundational Learning

- Concept: Integrated Information Theory (IIT)
  - Why needed here: IIT provides the framework for evaluating whether a system is conscious and what the content of its experience is.
  - Quick check question: According to IIT, what are the five essential properties of consciousness (axioms) and their corresponding causal properties (postulates)?

- Concept: Functional vs. Phenomenal Equivalence
  - Why needed here: The paper's central claim is that these two types of equivalence are not the same, and understanding the distinction is crucial.
  - Quick check question: What is the key difference between a system that is functionally equivalent to a conscious system and one that is phenomenally equivalent?

- Concept: Cause-Effect Structures and Complexes
  - Why needed here: IIT's analysis revolves around identifying complexes (subsets of a system that are maximally integrated) and their cause-effect structures.
  - Quick check question: What are the key properties of a complex according to IIT, and how is the cause-effect structure of a complex unfolded?

## Architecture Onboarding

- Component map: Clock -> Frequency dividers -> Program memory -> Instruction register -> Data registers -> Multiplexer
- Critical path: The fetch-decode-execute loop, where the computer fetches an instruction from program memory, decodes it using the instruction register, and executes it using the data registers and multiplexer.
- Design tradeoffs: The computer prioritizes functional equivalence over integration, leading to a modular architecture with bottlenecks. This design choice is what allows it to simulate any four-unit system but prevents it from forming a single integrated complex.
- Failure signatures: If the computer were to support a complex with high integrated information, it would likely be due to a different architecture that prioritizes integration over functional equivalence. However, this would likely come at the cost of the computer's ability to simulate arbitrary systems.
- First 3 experiments:
  1. Modify the computer's architecture to eliminate the multiplexer bottleneck and see if it can form a single integrated complex.
  2. Try macroing the computer's units in different ways to see if any macroing scheme can replicate the target's cause-effect structure.
  3. Scale up the computer to simulate larger systems and observe how the dissociation between functional and phenomenal equivalence changes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can neuromorphic computers designed to mimic the physical organization of the human brain achieve both functional and phenomenal equivalence?
- Basis in paper: [explicit] The paper explicitly raises this as an "intriguing question" in the discussion, noting that neuromorphic computers mimicking brain organization "might be engineered to possess the required cause–effect structures and high integrated information."
- Why unresolved: Current neuromorphic architectures are still in early development stages and haven't been comprehensively analyzed using IIT's framework. The paper's analysis focuses on traditional von Neumann architectures with central processing units.
- What evidence would resolve it: Rigorous application of IIT's mathematical framework to detailed causal models of neuromorphic architectures, demonstrating whether they can support complexes with high Φ and cause–effect structures equivalent to biological brains.

### Open Question 2
- Question: Under what specific conditions (if any) could a digital computer simulate human brain activity neuron-by-neuron without replicating human subjective experience?
- Basis in paper: [explicit] The paper demonstrates that a simple stored-program computer can simulate a minimal 4-unit system without replicating its cause–effect structure, and extends this to Turing-complete versions that could simulate arbitrarily complex systems.
- Why unresolved: The analysis provides theoretical proof but doesn't address practical limitations or specific architectural features that might enable or prevent consciousness in simulation scenarios.
- What evidence would resolve it: Empirical testing of IIT on simulated neural systems of varying complexity, particularly examining how architectural bottlenecks (like CPUs and multiplexers) fragment cause–effect structures.

### Open Question 3
- Question: What are the quantitative upper bounds on integrated information (Φ) that can be achieved by computers with architectures similar to the one analyzed in the paper?
- Basis in paper: [inferred] The paper provides theoretical bounds showing Φ grows linearly or remains constant in the analyzed computer architecture, contrasting with double-exponential growth possible in rich systems.
- Why unresolved: The paper establishes theoretical limits but doesn't provide comprehensive empirical validation across different computer architectures and scales.
- What evidence would resolve it: Systematic measurement of Φ across various computer architectures using IIT's framework, particularly examining how different design choices affect integrated information.

## Limitations

- The analysis is based on a simplified four-bit computer, and results may not generalize to more complex systems
- The paper focuses on traditional von Neumann architectures and doesn't explore alternative designs that might achieve both functional and phenomenal equivalence
- While the dissociation holds for Turing-complete versions, empirical validation for large-scale systems is not presented

## Confidence

- High Confidence: The demonstration that the specific four-bit computer fragments into multiple small complexes with trivial cause-effect structures, unlike the single rich structure of the target system.
- Medium Confidence: The claim that this dissociation holds for arbitrarily large computers simulating complex systems.
- Medium Confidence: The interpretation that this dissociation challenges computational functionalism.

## Next Checks

1. **Architectural Exploration**: Implement and analyze alternative computer architectures (e.g., neuromorphic, analog, or quantum-inspired designs) to determine if any can achieve both functional and phenomenal equivalence for the four-bit simulation task.
2. **Scale-Up Validation**: Scale up the analysis to computers simulating systems with hundreds or thousands of units, measuring how the dissociation between functional and phenomenal equivalence changes with system size.
3. **Cross-Theory Comparison**: Apply alternative theories of consciousness (e.g., Global Workspace Theory, Recurrent Processing Theory) to the same computer-target pairs to determine if the dissociation is specific to IIT or represents a more general phenomenon.