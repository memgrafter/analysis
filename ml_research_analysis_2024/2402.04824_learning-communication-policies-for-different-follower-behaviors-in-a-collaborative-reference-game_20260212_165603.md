---
ver: rpa2
title: Learning Communication Policies for Different Follower Behaviors in a Collaborative
  Reference Game
arxiv_id: '2402.04824'
source_url: https://arxiv.org/abs/2402.04824
tags:
- follower
- piece
- guide
- learning
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of how neural artificial agents
  can adapt their communication strategies to different assumed partner behaviors
  in a collaborative reference game. The authors frame this as a reinforcement learning
  problem where a Guide agent must verbally lead a Follower agent to select a specific
  puzzle piece among distractors.
---

# Learning Communication Policies for Different Follower Behaviors in a Collaborative Reference Game

## Quick Facts
- arXiv ID: 2402.04824
- Source URL: https://arxiv.org/abs/2402.04824
- Authors: Philipp Sadler; Sherzod Hakimov; David Schlangen
- Reference count: 24
- Primary result: Guide policies achieve 92% success rate while learning to adapt communication strategies to Follower confidence and autonomy levels

## Executive Summary
This paper addresses how neural artificial agents can adapt their communication strategies to different partner behaviors in a collaborative reference game. The authors frame this as a reinforcement learning problem where a Guide agent must verbally lead a Follower agent to select a specific puzzle piece among distractors. By using a hand-crafted Follower policy that varies along confidence and autonomy dimensions, and training separate Guide policies with Proximal Policy Optimization, the learned policies successfully adapt their strategies - learning when to stay silent and how to reference pieces based on the Follower's behavior.

## Method Summary
The authors use Proximal Policy Optimization (PPO) to train Guide agents in a collaborative reference game where the Guide must verbally lead a Follower to select a specific puzzle piece. The Guide predicts intent-based actions (silence, confirm, decline, directive, reference) which are verbalized into natural language, reducing the action space from 37 words to 14 intents. Separate policies are trained for each combination of Follower confidence levels (6 levels) and autonomy modes (2 modes), with rewards that balance task success and assumed communicative effort.

## Key Results
- Guide policies achieve 92% success rate on test tasks
- Policies learn to stay silent in at least 23% of steps, with silence rates increasing for more confident Follower behaviors
- Guide policies adapt their reference preference orders based on Follower autonomy levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning communication policies in this reference game adapts the Guide's behavior to the Follower's confidence and autonomy levels.
- Mechanism: The Guide learns when to speak, what to say (intent type), and how to say it (preference order) by observing the Follower's actions and receiving a reward that includes both task success and assumed communicative effort.
- Core assumption: The hand-crafted Follower policy varies predictably along confidence and autonomy dimensions, allowing the Guide to learn effective adaptations.
- Evidence anchors:
  - [abstract] "our results indicate that this novel ingredient leads to communicative strategies that are less verbose (staying silent in some of the steps) and that with respect to that the Guide's strategies indeed adapt to the partner's level of confidence and autonomy."
  - [section 5.3] "the differences in the intent selection strategy of the learned policies (Guides) shown in Table 3 indicate that Guides learnt from interaction with more confident Follower's (ϕ > 0.9) produce less or no confirm actions."
  - [corpus] Weak evidence: Related papers discuss multi-agent communication but don't directly address learning adaptive policies for different partner behaviors.

### Mechanism 2
- Claim: Using intent-based actions instead of word-level predictions reduces the action space exploration burden.
- Mechanism: The Guide predicts intents (silence, confirm, decline, directive, reference) which are then verbalized into natural language using templates, reducing the vocabulary size from 37 words to 14 intents.
- Core assumption: Verbalizing intents into natural language doesn't lose important semantic information for the Follower to understand and act upon.
- Evidence anchors:
  - [section 3.2] "We let the Guide predict 'intent' actions and translate them into sentences instead of predicting words directly to reduce the agent's burden on action space exploration."
  - [section 3.4] The action-based efforts follow the assumed cognitive load for producing them, e.g., saying nothing is the cheapest and comparing pieces to produce a reference is the highest.
  - [corpus] Weak evidence: While related work discusses action space reduction, none directly address the intent-based approach used here.

### Mechanism 3
- Claim: The reward signal that includes assumed communicative effort encourages the Guide to learn to stay silent when possible.
- Mechanism: The reward includes both the game reward (based on success and episode length) and the Guide's effort reward (based on the assumed cognitive load of each action type), encouraging the Guide to minimize effort while still succeeding.
- Core assumption: The assumed cognitive load for each action type (0 for silence, 1.0 for confirm/decline, 1.1 for directive, 1.2 for reference) accurately reflects the actual effort required.
- Evidence anchors:
  - [section 3.4] "we introduce a sparse reward for the Guide's individual effort in an episode: RGuide = 1 − 0.9 ∗ (EGuide/Tmax)"
  - [section 5.3] "Figure 4 shows that the policies converge to a mode where the silence intent is chosen in at least 23% of the steps: The policies are in general able to learn to say nothing."
  - [corpus] Weak evidence: Related work discusses reward shaping but doesn't directly address the specific effort-based reward used here.

## Foundational Learning

- Concept: Reinforcement Learning (RL)
  - Why needed here: The Guide learns through trial and error by receiving rewards for successful task completion and efficient communication.
  - Quick check question: Can you explain the difference between the actor and critic in a policy gradient method like PPO?

- Concept: Natural Language Understanding (NLU)
  - Why needed here: The Follower needs to understand the Guide's verbalized intents and extract relevant information to update its plan.
  - Quick check question: How would you design a simple parser to extract directives (e.g., "Go left") and references (e.g., "Take the blue piece") from the Guide's utterances?

- Concept: Multi-Agent Systems
  - Why needed here: The Guide and Follower need to coordinate their actions to achieve the shared goal, with each having different capabilities and information.
  - Quick check question: What are the key challenges in designing a hand-crafted Follower policy that can work with a learned Guide policy?

## Architecture Onboarding

- Component map:
  - Guide: Vision encoder (CNN) -> LSTM memory -> Actor-Critic policy (PPO) -> Intent -> Verbalization
  - Follower: Plan-based policy with adjustable confidence and autonomy
  - Environment: Pentomino pieces, gripper, partial/full views
  - Reward: Sparse task success + assumed communicative effort

- Critical path: Guide observation → CNN → LSTM → Actor-Critic → Intent → Verbalization → Follower observation → Plan update → Action → Environment update → Reward

- Design tradeoffs:
  - Using intent-based actions reduces exploration complexity but may limit expressiveness
  - Hand-crafting Follower policy ensures predictability but may not capture all real-world behaviors
  - Sparse rewards encourage efficient communication but may slow learning

- Failure signatures:
  - Guide policy not adapting to Follower confidence/autonomy: Check if Follower policy is actually varying as expected
  - Guide not learning to stay silent: Check if effort-based reward is strong enough or if verbalization is too complex
  - Follower not understanding Guide: Check if verbalization templates cover all necessary information

- First 3 experiments:
  1. Test Guide with a simple Follower that always follows the plan (no confidence/autonomy variation) to ensure basic communication works
  2. Test Guide with a Follower that has high confidence and autonomy to see if Guide learns to stay silent more
  3. Test Guide with a Follower that has low confidence and autonomy to see if Guide learns to provide more guidance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the communication policy learned by the Guide agent generalize to different environments beyond the CoGRIP-GL task?
- Basis in paper: [inferred] The paper presents a specific environment (CoGRIP-GL) and evaluates the learned policies within that environment. It does not discuss the generalization of the learned policies to other environments.
- Why unresolved: The paper focuses on a single environment and does not explore the transferability of the learned communication policies to other settings.
- What evidence would resolve it: Experiments evaluating the learned policies on different environments or tasks, demonstrating their ability to adapt to new situations.

### Open Question 2
- Question: What is the impact of different reward formulations on the learned communication strategies?
- Basis in paper: [explicit] The paper mentions that they use a specific reward formulation that includes both the goal condition and an assumed communicative effort. However, it does not explore the impact of different reward formulations on the learned strategies.
- Why unresolved: The paper only uses one reward formulation and does not compare it to other potential formulations.
- What evidence would resolve it: Experiments using different reward formulations and analyzing the resulting communication strategies, highlighting the differences in behavior and effectiveness.

### Open Question 3
- Question: How does the learned communication policy change when the Follower agent is not a fixed heuristic policy but another learning agent?
- Basis in paper: [inferred] The paper uses a hand-crafted Follower policy that varies along dimensions of confidence and autonomy. It does not explore the case where the Follower is also a learning agent.
- Why unresolved: The paper assumes a fixed Follower policy and does not consider the dynamics of learning interactions between two adaptive agents.
- What evidence would resolve it: Experiments where both the Guide and Follower are learning agents, observing the evolution of their communication strategies and the resulting interactions.

## Limitations
- The use of a hand-crafted Follower policy, while providing predictability, may not capture the full range of real-world partner behaviors
- The evaluation scope is limited to a single environment, without testing cross-behavior generalization
- The assumption of linear effort costs for different intent types is a simplification that may not reflect actual cognitive load

## Confidence
- High confidence in the core mechanism: The paper provides strong empirical evidence that the Guide policies adapt to Follower behaviors and learn to balance success with effort.
- Medium confidence in the claimed benefits of intent-based actions: While the paper argues that this approach reduces exploration complexity, it doesn't provide direct comparisons to word-level prediction methods.
- Low confidence in the generalizability to real-world scenarios: The use of a hand-crafted Follower and simplified effort assumptions limits the applicability of the results to real human partners.

## Next Checks
1. **Cross-behavior generalization**: Train Guide policies on one Follower behavior (e.g., high confidence) and test on another (e.g., low confidence) to assess adaptability.
2. **Effort cost validation**: Conduct user studies to validate the assumed linear effort costs for different intent types against actual human cognitive load.
3. **Real-world partner testing**: Replace the hand-crafted Follower with a learned policy trained on human-human interaction data to assess performance with more realistic partner behaviors.