---
ver: rpa2
title: A Riemannian Approach to Ground Metric Learning for Optimal Transport
arxiv_id: '2409.10085'
source_url: https://arxiv.org/abs/2409.10085
tags:
- metric
- target
- source
- ground
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for learning the ground metric in
  optimal transport (OT) by jointly optimizing the transport plan and a symmetric
  positive definite (SPD) matrix that parameterizes the metric. The key insight is
  that the SPD matrix can be updated in closed form via a geometric mean computation
  on the Riemannian manifold, while the transport plan can be updated efficiently
  using the Sinkhorn algorithm.
---

# A Riemannian Approach to Ground Metric Learning for Optimal Transport

## Quick Facts
- arXiv ID: 2409.10085
- Source URL: https://arxiv.org/abs/2409.10085
- Reference count: 30
- Primary result: Jointly optimizing transport plan and SPD metric matrix via alternating updates outperforms fixed-metric OT baselines in domain adaptation on MNIST and Caltech-Office datasets.

## Executive Summary
This paper addresses the problem of learning a ground metric for optimal transport (OT) in domain adaptation scenarios. The authors propose an alternating optimization framework that jointly updates the transport plan γ and a symmetric positive definite (SPD) matrix A that parameterizes the metric. The key insight is that A can be updated in closed form via a geometric mean computation on the Riemannian manifold, while γ can be efficiently computed using the Sinkhorn algorithm. This approach avoids trivial solutions through an appropriate regularizer and demonstrates consistent performance improvements over OT baselines using fixed metrics (Euclidean, whitening) on MNIST and Caltech-Office datasets.

## Method Summary
The method jointly optimizes the transport plan γ and an SPD matrix A that parameterizes the ground metric. It alternates between two convex subproblems: (1) updating γ using the Sinkhorn algorithm for fixed A, and (2) updating A using a closed-form geometric mean solution involving Cγ and a regularization matrix D. The regularization term ⟨A⁻¹, D⟩ prevents A from collapsing to zero while encouraging alignment between transformed features' covariance and D. After convergence, source points are mapped to the target space via barycentric projection using γ, and a 1-NN classifier is trained on these mapped points for domain adaptation.

## Key Results
- The alternating optimization consistently converges and avoids trivial solutions where A → 0
- On MNIST with varying skew levels, the learned metric improves 1-NN target classification accuracy by 2-5% over OT with fixed metrics
- Across all 12 Caltech-Office task pairs, the method achieves higher average accuracy than baselines using Euclidean, whitening, or W metrics
- The approach is particularly effective when source and target distributions differ significantly

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The alternating optimization decouples the transport plan update and the metric matrix update into two convex subproblems, each solvable in closed form.
- Mechanism: When the transport plan γ is fixed, the metric matrix A is updated via the geometric mean of two SPD matrices, which has a closed-form solution using the Riccati equation. Conversely, when A is fixed, the Sinkhorn algorithm efficiently computes γ.
- Core assumption: The objective function is convex in A for fixed γ and vice versa, enabling alternating minimization to converge to a stationary point.
- Evidence anchors:
  - [abstract] The key insight is that the SPD matrix can be updated in closed form via a geometric mean computation on the Riemannian manifold, while the transport plan can be updated efficiently using the Sinkhorn algorithm.
  - [section] "From the viewpoint of Riemannian geometry, the objective function ⟨A, Cγ⟩ + ⟨A−1, D⟩ on the space of the Riemannian manifold of SPD matrices is viewed as computing the geometric mean between two SPD matrices: C−1γ and D."
- Break condition: If the alternating updates cycle or the geometric mean computation fails (e.g., due to numerical instability when Cγ is not SPD), convergence is not guaranteed.

### Mechanism 2
- Claim: The regularizer ⟨A−1, D⟩ avoids the trivial solution A → 0 and aligns the transformed features' covariance with D.
- Mechanism: Minimizing trace(A−1D) forces A−1 to shrink, while minimizing the transport cost with A forces A to shrink. The balance bounds A away from zero and aligns its covariance structure with D, especially when D=I (whitening effect).
- Core assumption: Without this regularizer, the metric learning problem is ill-posed and leads to degenerate solutions.
- Evidence anchors:
  - [section] "It should be noted that Problem (4) without Φ(A) or with commonly employed regularizers such as Φ(A) = ||A||2F or Φ(A) = trace(AD) where D is a given (fixed) SPD matrix is not a suitable problem as they lead to a trivial solution with A = 0."
  - [section] "The setting D = I implies that A promotes the transformed features to become more uncorrelated."
- Break condition: If D is not well-chosen or the trade-off between the two terms is too extreme, the metric learning may not improve performance or may overfit.

### Mechanism 3
- Claim: Barycentric projection with the learned metric enables effective domain adaptation by mapping source points into the target space with transport plan-guided weights.
- Mechanism: For each source point xi, its barycentric image is a weighted average of target points using γ, with the learned metric A governing the distance. This provides a soft alignment that respects the transport plan and the adapted geometry.
- Core assumption: The transport plan captures the optimal mass movement under the learned metric, and the barycentric mapping preserves label consistency across domains.
- Evidence anchors:
  - [section] "The barycentric mapping (9) maps the i-th source instance xi to ˆxi, which is a weighted average of the target set instances."
  - [section] "Thus, instead of directly using the source points, their barycentric mappings could be used to classify the target set instances for domain adaptation scenarios."
- Break condition: If the transport plan is poor (e.g., due to an ill-suited metric A), the barycentric projections will not align domains well, hurting classification accuracy.

## Foundational Learning

- Concept: Optimal Transport (OT) basics and Wasserstein distance.
  - Why needed here: OT defines the framework for comparing distributions; understanding the transport plan and ground cost is essential for grasping the problem setup.
  - Quick check question: What is the role of the ground cost matrix G in the OT problem formulation?

- Concept: Riemannian geometry of SPD matrices and geometric mean.
  - Why needed here: The metric A lies on the SPD manifold; knowing how geometric mean and Riccati equations work is key to understanding the closed-form update.
  - Quick check question: Why is the geometric mean between C−1γ and D used to update A?

- Concept: Alternating minimization and convex subproblems.
  - Why needed here: The algorithm alternates between two convex problems; understanding convergence and the Sinkhorn method is important for implementation.
  - Quick check question: How does the Sinkhorn algorithm efficiently solve the regularized OT problem for fixed A?

## Architecture Onboarding

- Component map: Data loader -> Metric learner (SPD manifold) -> OT solver (Sinkhorn) -> Barycentric mapper -> 1-NN classifier -> Evaluation
- Critical path:
  1. Initialize γ (uniform).
  2. For each iteration: update A using Cγ and D, then update γ using Sinkhorn.
  3. After convergence, compute barycentric mappings for source points.
  4. Train 1-NN classifier on mapped source points.
  5. Evaluate on target test set.
- Design tradeoffs:
  - Fixed vs. learned metric: hand-crafted metrics (Euclidean, whitening) are fast but may not adapt; learned metric adapts but requires more computation and tuning.
  - Regularizer choice: trace(A−1D) avoids trivial solutions; other regularizers may lead to A=0.
  - Iteration count: more iterations may improve convergence but increase runtime; need to monitor change in objective.
- Failure signatures:
  - A becomes ill-conditioned or singular (numerical instability in geometric mean).
  - γ collapses to a sparse or degenerate transport plan.
  - Classification accuracy does not improve despite learning A (possibly due to poor D or λ choice).
- First 3 experiments:
  1. Reproduce the MNIST experiment with z=10 (no skew) and verify that OT I baseline matches reported performance.
  2. Run the alternating optimization for a small number of iterations and check that A updates follow the Riccati equation and that the Sinkhorn loss decreases.
  3. Compare classification accuracy with OT I, OTW, OTW−1 baselines on a subset of Caltech tasks to confirm improvement with learned metric.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas remain unexplored based on the presented work.

## Limitations
- The computational complexity of alternating optimization is significant for large-scale datasets, though detailed runtime comparisons are not provided.
- Convergence is only guaranteed to local optimality rather than global optimality, and performance is sensitive to the choice of regularization parameter λ and matrix D.
- The method is only evaluated on two datasets (MNIST and Caltech-Office), limiting generalizability to other domain adaptation scenarios.

## Confidence
- High confidence: The mechanism of alternating between Sinkhorn updates for γ and geometric mean updates for A is clearly specified and theoretically grounded.
- Medium confidence: The claim that the learned metric consistently outperforms fixed metrics is supported by experimental results, but the performance gains vary significantly across tasks.
- Medium confidence: The effectiveness in cases with large domain shifts is demonstrated, but the paper doesn't systematically explore the limits of when the method fails.

## Next Checks
1. Implement ablation studies varying λ and D to quantify their impact on both convergence and final accuracy across all task pairs.
2. Evaluate runtime and memory requirements for different dataset sizes to establish scalability bounds and compare against fixed-metric OT baselines.
3. Test the method on additional domain adaptation datasets (e.g., Office-31 or VisDA) to assess generalizability beyond the two evaluated datasets.