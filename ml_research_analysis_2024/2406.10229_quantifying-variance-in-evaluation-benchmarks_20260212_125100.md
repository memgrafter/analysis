---
ver: rpa2
title: Quantifying Variance in Evaluation Benchmarks
arxiv_id: '2406.10229'
source_url: https://arxiv.org/abs/2406.10229
tags:
- item
- benchmarks
- variance
- metrics
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper quantifies variance in evaluation benchmarks for large
  language models. It measures seed variance, confidence intervals, and monotonicity
  across 13 benchmarks and 280 models, including 7B models trained from scratch.
---

# Quantifying Variance in Evaluation Benchmarks

## Quick Facts
- arXiv ID: 2406.10229
- Source URL: https://arxiv.org/abs/2406.10229
- Reference count: 40
- Primary result: Seed variance in LLM evaluation benchmarks is often lower than confidence intervals, and simple prompt reformulation can reduce variance for smaller models

## Executive Summary
This paper quantifies variance in evaluation benchmarks for large language models across 13 benchmarks and 280 models, including 7B models trained from scratch. The study measures seed variance, confidence intervals, and monotonicity, finding that variance from random seeds is often lower than confidence intervals derived from single runs. The paper evaluates both continuous and discrete performance metrics, discovering that continuous metrics have higher signal-to-noise ratios. Simple changes like reformulating choice tasks as completion tasks can reduce variance for smaller models, but advanced statistical methods from human testing (item analysis, IRT) do not meaningfully help.

## Method Summary
The authors trained 10 Llama-2-7B models with different seeds and evaluated 210 checkpoints across 13 benchmarks using both discrete and continuous metrics. They computed seed variance as the standard deviation of performance scores across seeds, compared this to confidence intervals from bootstrap sampling, and measured monotonicity of performance improvements during training. The study also tested item analysis and item response theory methods for variance reduction, and compared cloze-style MMLU formulation against standard multiple-choice format. Probability mass and negative log-likelihood were used as continuous metrics for choice-based and generation-based benchmarks respectively.

## Key Results
- Seed variance across model initializations is typically lower than confidence intervals from single-run evaluations
- Continuous metrics (probability mass, NLL) have significantly higher signal-to-noise ratios than discrete accuracy metrics
- Cloze-formatted MMLU reduces variance for 7B models compared to standard multiple-choice format
- Item analysis and IRT methods fail to meaningfully reduce variance in LLM evaluations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Seed variance in evaluation benchmarks is often lower than confidence intervals derived from single runs, making multiple training runs more informative for detecting true model differences.
- **Mechanism**: The paper defines seed variance as the standard deviation of performance metrics across models trained with identical setups but different random seeds. This variance is computed over intermediate checkpoints, providing a lower-bound estimate of inherent benchmark variability.
- **Core assumption**: Training runs with different seeds but identical data ordering and hyperparameters produce comparable model populations for variance estimation.
- **Evidence anchors**:
  - [abstract]: "we define and measure a range of metrics geared towards measuring variance in evaluation benchmarks, including seed variance across initialisations"
  - [section]: "Seed variance (E(S, M)) Given a benchmark, a preferred metric S, and a set of models M = {M1, M2, . . . Mn}, we define the benchmark seed variance E(S, M) as the standard deviation of the metric S scores"
  - [corpus]: Found related work on stochasticity in agentic evaluations and quantifying inconsistency, suggesting seed variance is a recognized concern.
- **Break condition**: If data ordering is not deterministic across seeds, variance estimates may be confounded by data ordering effects rather than pure initialization noise.

### Mechanism 2
- **Claim**: Continuous performance metrics yield higher signal-to-noise ratios (SNR) than discrete metrics, making them more reliable for comparing models during training.
- **Mechanism**: The paper uses probability mass for choice-based tasks and negative log-likelihood for generation tasks as continuous metrics, then compares their SNR to discrete accuracy metrics. Higher SNR indicates less variance relative to the mean, improving discrimination between models.
- **Core assumption**: The underlying probability distributions estimated by continuous metrics are more stable across seeds than the binary outcomes of discrete metrics.
- **Evidence anchors**:
  - [abstract]: "We also evaluate the utility and tradeoffs of continuous versus discrete performance measures"
  - [section]: "To maintain consistency, we used probability mass of the predicted answer for all choice-based benchmarks and NLL of the correct answer for generation-based benchmarks"
  - [corpus]: Related work on output format bias suggests continuous metrics may reduce sensitivity to formatting variations.
- **Break condition**: If the continuous metric is poorly calibrated or the model's probability estimates are unreliable, SNR improvements may not translate to better model comparison.

### Mechanism 3
- **Claim**: Simple prompt reformulation (e.g., cloze-style for MMLU) can significantly reduce variance for smaller models without sacrificing correlation with final performance on standard formats.
- **Mechanism**: The paper compares standard MMLU (multiple-choice with explicit options) to MMLU-cloze (fill-in-the-blank style), finding lower seed variance and higher monotonicity for the cloze version in 7B models.
- **Core assumption**: The cloze format reduces ambiguity in model output interpretation, leading to more consistent scoring across seeds.
- **Evidence anchors**:
  - [abstract]: "simple changes, such as framing choice tasks (like MMLU) as completion tasks, can often reduce variance for smaller scale (~7B) models"
  - [section]: "Motivated by prior work considering the inconsistency of multiple choice benchmarks... we examined two formulations of MMLU"
  - [corpus]: Work on LLM sensitivity to output formats supports the idea that prompt structure affects variance.
- **Break condition**: If larger models have already learned to handle standard formats robustly, variance reduction from cloze prompts may plateau or reverse.

## Foundational Learning

- **Concept: Variance quantification in ML evaluation**
  - Why needed here: Understanding seed variance and confidence intervals is essential for interpreting benchmark results and making sound model comparisons.
  - Quick check question: If two models differ in accuracy by less than the 95% confidence interval, can you claim one is better?

- **Concept: Item response theory (IRT) and item analysis**
  - Why needed here: The paper tests these human-testing methods for variance reduction, finding them ineffective for LLMs, which is a key negative result.
  - Quick check question: Why might IRT-based methods increase variance when applied to intermediate checkpoints?

- **Concept: Signal-to-noise ratio (SNR) in evaluation metrics**
  - Why needed here: SNR determines how well a metric can distinguish between models; higher SNR means more reliable comparisons.
  - Quick check question: If discrete accuracy has SNR=10 and continuous NLL has SNR=100, how much more sensitive is the latter to model differences?

## Architecture Onboarding

- **Component map**: Train Llama-2-7B models with different seeds -> Evaluate on 13 benchmarks using discrete/continuous metrics -> Compute variance metrics (seed variance, confidence intervals, monotonicity) -> Test variance reduction techniques
- **Critical path**: Ensuring deterministic data ordering across seeds; without this, seed variance conflates initialization and data ordering effects
- **Design tradeoffs**: Continuous metrics improve SNR but may require more careful calibration; cloze prompts reduce variance for small models but may not generalize to larger ones
- **Failure signatures**: If seed variance is comparable to or exceeds confidence intervals, the benchmark may be too noisy for reliable model comparison
- **First 3 experiments**:
  1. Train two 7B models with different seeds on the same data mix and compare their MMLU scores using both standard and cloze formats
  2. Compute 95% confidence intervals on a single model's performance across multiple bootstrap samples and compare to seed variance from step 1
  3. Evaluate a set of open-source models on a benchmark using both discrete accuracy and continuous probability mass, then calculate SNR for each

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do item analysis methods, which are effective for human testing, fail to reduce variance in LLM evaluations?
- Basis in paper: [explicit] The paper explicitly states that item analysis and item response theory (IRT) methods, which are effective for human standardized testing, fail to meaningfully reduce variance in LLM evaluations.
- Why unresolved: The paper provides empirical evidence of the failure of these methods but does not offer a clear theoretical explanation for why they are ineffective in the context of LLM evaluations.
- What evidence would resolve it: A detailed theoretical analysis comparing the assumptions and mechanisms of item analysis/IRT in human testing versus LLM evaluations, identifying key differences that lead to their ineffectiveness in the latter.

### Open Question 2
- Question: What are the specific features of LLM evaluation benchmarks that cause item discrimination scores to be less informative for stronger models?
- Basis in paper: [inferred] The paper observes that high item discrimination on train (weaker) models often does not correspond to high discrimination on test (stronger) models, suggesting that the features of the benchmarks may be less informative for stronger models.
- Why unresolved: The paper identifies this issue but does not provide a detailed analysis of the specific features of LLM benchmarks that contribute to this problem.
- What evidence would resolve it: A comprehensive analysis of LLM benchmarks, identifying specific features (e.g., prompt length, complexity, ambiguity) that correlate with item discrimination scores and how these features change as models become stronger.

### Open Question 3
- Question: How can we develop LLM-specific techniques for variance reduction that are more effective than methods borrowed from human testing?
- Basis in paper: [explicit] The paper suggests that LLM-specific techniques, such as using continuous metrics or cloze-formatted tasks, can improve the signal-to-noise ratio in evaluations, but these are limited in scope.
- Why unresolved: While the paper provides some examples of LLM-specific techniques, it does not offer a comprehensive framework for developing and evaluating such techniques.
- What evidence would resolve it: A systematic study exploring a wide range of LLM-specific techniques for variance reduction, including their effectiveness across different benchmark types and model scales, and a framework for evaluating and comparing these techniques.

## Limitations
- Variance measurements are primarily based on Llama-2-7B models trained from scratch; results may not generalize to other model families or scales
- The paper does not account for potential interactions between random seeds and data ordering effects, which could inflate variance estimates
- Item analysis and IRT methods are tested on intermediate checkpoints rather than final model versions, potentially missing effects that would be visible on converged models

## Confidence
- Seed variance < confidence intervals: High
- Continuous metrics have higher SNR than discrete: High
- Cloze prompts reduce variance for 7B models: Medium
- Item analysis and IRT don't help: Medium

## Next Checks
1. Replicate the cloze MMLU experiment on additional 7B model families (e.g., Mistral, Gemma) to verify the variance reduction effect is not architecture-specific
2. Conduct ablation studies varying data ordering while keeping seeds fixed to isolate initialization effects from data ordering effects in variance measurements
3. Apply item analysis and IRT methods to final converged checkpoints rather than intermediate versions to test if the negative results hold across the full training trajectory