---
ver: rpa2
title: Testing Large Language Models on Driving Theory Knowledge and Skills for Connected
  Autonomous Vehicles
arxiv_id: '2407.17211'
source_url: https://arxiv.org/abs/2407.17211
tags: []
core_contribution: The paper investigates the use of large language models (LLMs)
  for connected autonomous vehicles (CAVs) by evaluating their understanding of driving
  theory and skills. The authors design and conduct driving theory tests on several
  proprietary and open-source LLM models using more than 500 multiple-choice questions
  similar to the official UK driving theory test.
---

# Testing Large Language Models on Driving Theory Knowledge and Skills for Connected Autonomous Vehicles

## Quick Facts
- arXiv ID: 2407.17211
- Source URL: https://arxiv.org/abs/2407.17211
- Authors: Zuoyin Tang; Jianhua He; Dashuai Pei; Kezhong Liu; Tao Gao
- Reference count: 16
- Key outcome: GPT-4 passes UK driving theory test with 95% accuracy; multimodal GPT-4o achieves 96% on image-based questions; cost of GPT-4 is ~50x higher than GPT-3.5

## Executive Summary
This paper evaluates large language models (LLMs) on their understanding of driving theory and skills relevant to connected autonomous vehicles (CAVs) using a benchmark based on the official UK driving theory test. The authors test multiple proprietary and open-source models, including OpenAI's GPT-3.5, GPT-4, GPT-4o, and several others, on over 500 multiple-choice questions. Results show that while GPT-4 passes the test with high accuracy, other models fail to meet the passing threshold. The study also highlights significant differences in cost and latency between models, with GPT-4 being substantially more expensive but offering superior performance, especially on image-based questions.

## Method Summary
The study uses more than 500 multiple-choice driving theory test questions similar to those used in the official UK driving theory test, including both text and image-based questions. The models are evaluated on accuracy, cost, and processing latency. The evaluation is conducted using standardized prompts and API calls, with responses parsed for correctness. Cost and latency are measured for each model, and results are compared against the official UK driving theory test passing threshold of 86%.

## Key Results
- GPT-4 passes the driving theory test with 95% accuracy; other models (GPT-3.5, Ernie, Qwen, MiniCPM-2B) fail.
- For image-based questions, GPT-4o achieves 96% accuracy and MiniCPM-Llama3-V2.5 reaches 76%.
- GPT-4's cost is nearly 50 times higher than GPT-3.5, with average processing latencies of 0.9s (GPT-4) and 0.7s (GPT-3.5) per question.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using UK driving theory test questions provides a controlled, reproducible benchmark for LLM driving knowledge.
- Mechanism: Official test questions are standardized, publicly available (or close proxies exist), and cover the same knowledge domain that would be required for safe driving. By mapping LLM responses to pass/fail thresholds (86%), the evaluation directly mirrors real-world qualification criteria.
- Core assumption: The selected test questions accurately reflect the breadth and depth of driving knowledge needed for CAV applications.
- Evidence anchors:
  - [abstract] "These questions are close to those used in the official UK driving theory tests."
  - [section] "In the UK driving multiple choice test there are 50 multiple choice questions... To pass... at least 43 out of the 50 test questions must be answered correctly."
- Break condition: If the test question pool is too narrow, outdated, or misaligned with real-world driving scenarios, the benchmark would not generalize to actual CAV deployment needs.

### Mechanism 2
- Claim: Cost and latency measurements enable trade-off decisions between model capability and operational feasibility.
- Mechanism: By quantifying the expense (tokens, dollars) and response time for each model, stakeholders can weigh the safety benefit of higher accuracy (e.g., GPT-4) against the constraints of real-time, safety-critical driving tasks.
- Core assumption: The measured costs and latencies are representative of real-world deployment scenarios (e.g., remote/cloud vs. edge).
- Evidence anchors:
  - [abstract] "While GPT-4 holds stronger potential for CAV driving assistance applications, the cost of using model GPT4 is much higher, almost 50 times of that of using GPT3.5."
  - [section] "It takes about 0.7 second, 0.9 second and 3.4 second on average to test GPT-3.5, GPT-4 and GPT-4o for one question, respectively."
- Break condition: If deployment environments change (e.g., cheaper compute, lower latency networks), the trade-offs shift, invalidating the current cost/benefit analysis.

### Mechanism 3
- Claim: Multimodal LLMs (e.g., GPT-4o) significantly outperform text-only models on image-based driving questions.
- Mechanism: Vision-capable models can process traffic signs, road scenes, and visual hazards directly, improving accuracy on questions that depend on image interpretation.
- Core assumption: The visual reasoning capability of multimodal LLMs is robust enough to handle the diversity of real-world driving imagery.
- Evidence anchors:
  - [abstract] "For the test questions with images, the multimodal model GPT4-o has an excellent accuracy result of 96%..."
  - [section] "In the dataset with questions with images (denoted as DS-Image), there are 53 questions."
- Break condition: If image quality, lighting, or scene complexity exceed the model's training distribution, accuracy may drop sharply.

## Foundational Learning

- Concept: Driving theory test structure and pass criteria
  - Why needed here: To understand the benchmark's validity and to interpret model accuracy results correctly.
  - Quick check question: What is the UK driving theory test passing threshold for multiple-choice questions?

- Concept: LLM evaluation metrics (accuracy, cost, latency)
  - Why needed here: To assess not just correctness but also operational viability for safety-critical applications.
  - Quick check question: Why might a model with higher accuracy still be unsuitable for real-time driving assistance?

- Concept: Multimodal vs. unimodal model capabilities
  - Why needed here: To reason about when and why multimodal models may be necessary for CAV tasks.
  - Quick check question: What types of driving theory questions require image input?

## Architecture Onboarding

- Component map:
  UK driving theory test questions (text + images) -> Prompt generation -> API calls to LLM models -> Response parsing (first letter answer) -> Accuracy scoring -> Cost/latency measurement -> Model comparison table

- Critical path:
  Prompt formatting -> Model inference -> Response parsing (first letter answer) -> Accuracy calculation

- Design tradeoffs:
  - Use open-source models (cheaper, lower latency) vs. proprietary models (higher accuracy)
  - Text-only vs. multimodal models depending on question set
  - Full test battery vs. sampled evaluation for speed

- Failure signatures:
  - Consistently low accuracy across models -> Question dataset mismatch or prompt misformatting
  - High variance in latency -> Network issues or model load
  - GPT-4o not improving on image questions -> Vision input not being properly passed

- First 3 experiments:
  1. Run a small subset (e.g., 10 questions) with GPT-3.5 to verify prompt parsing and answer extraction.
  2. Test GPT-4 on the same subset to establish a baseline accuracy for high-cost models.
  3. Run GPT-4o on the image question subset to confirm multimodal capability and isolate vision-only performance.

## Open Questions the Paper Calls Out
None

## Limitations
- The UK driving theory test may not fully represent the complexity of real-world driving scenarios or CAV-specific tasks like vehicle-to-vehicle communication.
- Cost and latency measurements are based on remote API calls and may not reflect edge or embedded deployment environments.
- Proprietary models' internal reasoning processes are not transparent, limiting diagnosis of model failures.

## Confidence
- Accuracy ranking of models: High
- Cost and latency comparisons: Medium
- Claims about CAV readiness: Low

## Next Checks
1. Test model performance on a held-out set of UK driving theory questions not seen during evaluation to assess overfitting or memorization.
2. Evaluate the same models on a set of real-world driving scenarios (e.g., annotated dashcam footage) to compare performance on official test questions versus practical situations.
3. Conduct a pilot deployment in a simulated CAV environment to measure the impact of model latency and accuracy on safety-critical decision-making.