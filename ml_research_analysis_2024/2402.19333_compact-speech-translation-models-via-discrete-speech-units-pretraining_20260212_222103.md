---
ver: rpa2
title: Compact Speech Translation Models via Discrete Speech Units Pretraining
arxiv_id: '2402.19333'
source_url: https://arxiv.org/abs/2402.19333
tags:
- speech
- dsu-to-trl
- translation
- bleu
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using Discrete Speech Units (DSU) from self-supervised
  speech models as a proxy for model initialization to create more compact speech-to-text
  translation models. Instead of using dense representations, the method pretrains
  two smaller encoder-decoder models on Filterbank-to-DSU and DSU-to-Translation tasks,
  then uses their components to initialize a compact model that is finetuned on paired
  speech-translation data.
---

# Compact Speech Translation Models via Discrete Speech Units Pretraining

## Quick Facts
- arXiv ID: 2402.19333
- Source URL: https://arxiv.org/abs/2402.19333
- Reference count: 24
- This paper proposes using Discrete Speech Units (DSU) from self-supervised speech models as a proxy for model initialization to create more compact speech-to-text translation models.

## Executive Summary
This paper introduces DSU-Adapter, a method for creating compact speech-to-text translation models by leveraging Discrete Speech Units (DSU) extracted from self-supervised speech models. Instead of using dense speech representations, the approach pretrains two smaller encoder-decoder models on Filterbank-to-DSU and DSU-to-Translation tasks, then uses their components to initialize a compact model that is finetuned on paired speech-translation data. The method avoids using quantization modules in inference, requires no transcripts, and is robust to DSU tokenization. Evaluation on CoVoST-2 (21 language pairs) shows the method achieves 3 BLEU points improvement over a scratch-trained model and performs similarly to ASR pretraining while being more compact (48M vs 52M parameters).

## Method Summary
The method involves three stages: first, extracting Discrete Speech Units (DSU) from self-supervised speech models (HuBERT) using K-Means clustering on audio representations; second, pretraining two smaller encoder-decoder models - one mapping filterbank features to DSU sequences (Fbk-to-DSU) and another mapping DSU to translations (DSU-to-Trl); third, combining the encoder from Fbk-to-DSU and decoder from DSU-to-Trl to initialize a compact speech-to-text translation model, which is then finetuned on paired data using both cross-entropy and CTC loss. This approach distills knowledge from self-supervised speech models into a smaller, more efficient model that can be deployed without quantization modules.

## Key Results
- Achieves 3 BLEU points improvement over scratch-trained model on CoVoST-2
- Performs similarly to ASR pretraining with 48M parameters vs 52M for scratch
- Outperforms HuBERT-based transformer despite having half the parameters
- Demonstrates robustness to DSU tokenization with BPE, showing 0.6-1.0 BLEU improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Discrete Speech Units (DSU) serve as an effective proxy for dense SSS representations, enabling more compact model initialization.
- Mechanism: DSU are obtained by clustering SSS model representations, resulting in shorter, discrete sequences that retain phonetic information. These sequences are easier to model in text-based architectures compared to dense representations.
- Core assumption: DSU sequences capture sufficient linguistic information from SSS representations to enable effective pretraining for ST tasks.
- Evidence anchors:
  - [abstract]: "DSU are K-Means clusters of speech representations from selected layers of the SSS model. It represents sequence of discrete tokens, which are easier to model within a text processing architecture."
  - [section]: "DSU sequences are far smaller than the sequences of dense representations. Therefore, a straightforward method to distill the SSS models is to use DSU as speech inputs."
  - [corpus]: Weak - corpus neighbors focus on DSU applications but don't directly validate the proxy effectiveness claim.
- Break condition: If DSU clustering fails to preserve essential phonetic/linguistic information, the pretraining would not transfer effectively to ST tasks.

### Mechanism 2
- Claim: Pretraining two smaller encoder-decoder models on Fbk-to-DSU and DSU-to-Translation tasks creates effective initialization for a compact ST model.
- Mechanism: The Fbk-to-DSU model learns to map filterbank features to DSU sequences, while the DSU-to-Translation model learns to map DSU to text. Components from these models are then combined to initialize a compact ST model.
- Core assumption: The encoder from Fbk-to-DSU and decoder from DSU-to-Trl can be effectively combined to form a functional ST model.
- Evidence anchors:
  - [abstract]: "In the first step, our method pretrains two smaller encoder-decoder models on 1) Filterbank-to-DSU (Fbk-to-DSU) and 2) DSU-to-Translation (DSU-to-Trl) data respectively."
  - [section]: "Subsequently, the encoder from the Fbk-to-DSU model and the decoder from the DSU-to-Trl model are taken to initialise the compact model."
  - [corpus]: Missing - corpus neighbors don't discuss this specific two-stage pretraining approach.
- Break condition: If the combined components cannot effectively process the full ST task, or if modality mismatch occurs between the two pretraining stages.

### Mechanism 3
- Claim: Avoiding quantization modules in inference reduces memory footprint while maintaining performance.
- Mechanism: By pretraining with DSU rather than using them as inputs, the compact model operates directly on filterbank features without needing the SSS model and K-Means clustering at inference time.
- Core assumption: The knowledge from SSS model can be effectively distilled into the compact model through pretraining, making the SSS model unnecessary during inference.
- Evidence anchors:
  - [abstract]: "Our method avoids using the quantization modules in inference."
  - [section]: "In contrast to using the SSS model for initialization, our method is more suitable to memory constrained scenario such as on-device deployment."
  - [corpus]: Weak - corpus neighbors mention on-device applications but don't validate the inference efficiency claim.
- Break condition: If the distilled knowledge is insufficient to maintain ST performance without the original SSS model.

## Foundational Learning

- Concept: Self-Supervised Speech (SSS) models like wav2vec 2.0 and HuBERT
  - Why needed here: These models provide the speech representations that are clustered into DSU for pretraining
  - Quick check question: What is the primary difference between wav2vec 2.0 and HuBERT in terms of their training objectives?

- Concept: Connectionist Temporal Classification (CTC) loss
  - Why needed here: Used in Fbk-to-DSU pretraining to handle alignment between variable-length filterbank sequences and DSU sequences
  - Quick check question: Why is CTC loss particularly suitable for the Fbk-to-DSU task compared to standard cross-entropy?

- Concept: Byte Pair Encoding (BPE) tokenization
  - Why needed here: Applied to DSU sequences to reduce their length and improve modeling efficiency
  - Quick check question: How does BPE tokenization affect the sequence length of DSU representations compared to the original DSU sequences?

## Architecture Onboarding

- Component map: Filterbank → Fbk-to-DSU encoder → DSU-to-Trl decoder → Translation
- Critical path: Filterbank → Fbk-to-DSU encoder → DSU-to-Trl decoder → Translation
  This is the path that must work effectively for the system to produce quality translations.

- Design tradeoffs:
  - Model size vs. performance: The method achieves 3 BLEU improvement with 48M parameters vs 52M for scratch
  - Memory efficiency vs. complexity: Avoids quantization modules but requires two-stage pretraining
  - Tokenization robustness: BPE on DSU can improve performance but may introduce instability

- Failure signatures:
  - Poor translation quality despite good pretraining BLEU scores indicates modality mismatch
  - Performance degradation with BPE on DSU suggests tokenization sensitivity
  - Inconsistent improvements across language pairs may indicate insufficient cross-lingual generalization

- First 3 experiments:
  1. Train Fbk-to-DSU model and evaluate DSU prediction accuracy to verify the first pretraining stage works
  2. Train DSU-to-Trl model and evaluate translation quality to verify the second pretraining stage works
  3. Combine encoder and decoder components and finetune on paired data, comparing to scratch baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DSU-Adapter scale with larger K-Means clustering sizes beyond 1,000?
- Basis in paper: [explicit] The paper mentions K-Means clustering with K=1,000 but doesn't explore larger cluster sizes or their impact on performance.
- Why unresolved: The paper focuses on demonstrating the concept rather than exploring optimal clustering parameters, and different cluster sizes could significantly affect DSU quality and downstream performance.
- What evidence would resolve it: Experiments comparing DSU-Adapter performance across different K-Means clustering sizes (e.g., 2,000, 5,000, 10,000) while keeping other parameters constant would show the relationship between cluster size and translation quality.

### Open Question 2
- Question: How does DSU-Adapter performance compare when using SSS models trained on multilingual versus monolingual data?
- Basis in paper: [explicit] The paper notes that HuBERT-Base was trained solely on English data but achieved cross-lingual improvements, suggesting this could be further explored.
- Why unresolved: The paper uses only English-trained HuBERT-Base but mentions alternatives like XLS-R and Wavlm that are multilingual, yet doesn't compare their impact on DSU quality or translation performance.
- What evidence would resolve it: Direct comparison of DSU-Adapter using DSU from English-only HuBERT-Base versus multilingual SSS models like XLS-R or Wavlm across the same language pairs would reveal the impact of SSS model training data.

### Open Question 3
- Question: What is the relationship between DSU sequence length and translation quality, and is there an optimal length?
- Basis in paper: [explicit] The paper shows that applying BPE to DSU changes sequence length and mentions that the correlation between DSU length and performance is "not straightforward."
- Why unresolved: While the paper explores BPE application on DSU and notes length changes affect performance, it doesn't systematically analyze how sequence length specifically impacts translation quality or identify optimal lengths.
- What evidence would resolve it: Systematic experiments varying DSU sequence lengths through different BPE configurations or merging strategies, while measuring translation quality, would reveal the relationship and potential optimal lengths for different language pairs.

## Limitations

- The method's effectiveness appears highly dependent on DSU quality and pretraining stage compatibility
- Performance improvements over scratch training are similar to ASR pretraining, suggesting DSU approach may not fundamentally outperform existing methods
- BPE tokenization of DSU sequences introduces a hyperparameter that can significantly affect performance in both positive and negative directions

## Confidence

- High confidence: The method achieves parameter efficiency (48M vs 52M parameters) while maintaining competitive performance with ASR pretraining baselines.
- Medium confidence: DSU sequences effectively capture phonetic information for speech translation tasks.
- Medium confidence: Avoiding quantization modules at inference provides practical benefits for on-device deployment.

## Next Checks

1. **DSU Quality Analysis**: Measure the correlation between DSU extraction quality (e.g., sequence length, token diversity) and final translation performance across different language pairs to validate that DSU quality directly impacts ST performance.

2. **Cross-Lingual Transfer Evaluation**: Test the method on non-English target languages and language pairs beyond X-En to assess whether the DSU-based approach generalizes effectively across different translation directions.

3. **Inference Efficiency Benchmarking**: Measure actual inference latency and memory consumption of the compact model versus the full SSS-based approach on representative on-device hardware to quantify the claimed deployment benefits.