---
ver: rpa2
title: Fine-Tuning Pre-trained Large Time Series Models for Prediction of Wind Turbine
  SCADA Data
arxiv_id: '2412.00403'
source_url: https://arxiv.org/abs/2412.00403
tags:
- time
- data
- series
- large
- wind
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the application of Timer, a pre-trained
  large time series model, for forecasting wind turbine SCADA data. The model was
  fine-tuned on datasets from two wind farms with differing data volumes and turbine
  types, and its performance was compared against baseline models including LSTM and
  transformer architectures.
---

# Fine-Tuning Pre-trained Large Time Series Models for Prediction of Wind Turbine SCADA Data

## Quick Facts
- arXiv ID: 2412.00403
- Source URL: https://arxiv.org/abs/2412.00403
- Authors: Yuwei Fan; Tao Song; Chenlong Feng; Keyu Song; Chao Liu; Dongxiang Jiang
- Reference count: 23
- Primary result: Pre-trained Timer model shows superior few-shot learning capabilities in data-scarce wind turbine SCADA scenarios

## Executive Summary
This study investigates the application of Timer, a pre-trained large time series model, for forecasting wind turbine SCADA data. The model was fine-tuned on datasets from two wind farms with differing data volumes and turbine types, and its performance was compared against baseline models including LSTM and transformer architectures. Results show that while the pre-trained model does not consistently outperform baselines in data-rich scenarios, it demonstrates superior few-shot learning capabilities in data-scarce settings. In a one-turbine fine-tuning scenario for whole-plant prediction, the pre-trained model achieved lower mean squared error (MSE) across all prediction horizons, highlighting its advantages in rapid deployment and generalization. These findings underscore the potential of large time series models for practical applications in wind farm operations, particularly when data collection is limited.

## Method Summary
The Timer model, a decoder-only transformer with 67M parameters, was pre-trained on the Unified Time Series Dataset (UTSD) spanning 29 datasets across 10 domains. For wind turbine SCADA applications, the model was fine-tuned using autoregressive next-token prediction on cleaned SCADA data (wind speed, power, generator speed, ambient temperature) sampled at 10-minute intervals. Data preprocessing included outlier detection using wind speed-power-pitch angle relationships and DBSCAN/LOF algorithms, followed by sliding window segmentation (768 time points) and S3 format conversion. The model was fine-tuned for 100 epochs with Adam optimizer (learning rate 5×10⁻⁶) and early stopping, comparing against LSTM, transformer, and transformer-mini baselines trained under identical conditions.

## Key Results
- Timer achieved lower MSE than baselines in one-turbine fine-tuning scenarios for whole-plant prediction across all prediction horizons
- Pre-trained Timer demonstrated superior few-shot learning capabilities in data-scarce settings, with accuracy gaps increasing as data volume decreased
- In data-rich scenarios, Timer's performance was comparable to baselines, showing no consistent advantage over training from scratch

## Why This Works (Mechanism)

### Mechanism 1
Pre-trained large time series models demonstrate superior few-shot learning in data-scarce wind turbine SCADA scenarios. The Timer model was pre-trained on a diverse multi-domain time series dataset (UTSD) encompassing 29 datasets across 10 fields. This broad exposure enables the model to capture generalizable time series patterns that transfer effectively to new domains with limited data. Core assumption: Time series patterns from diverse domains contain sufficient shared structure to enable cross-domain transfer learning for wind turbine SCADA data. Break condition: When domain-specific patterns in SCADA data are too dissimilar from pre-training data, or when data scarcity is extreme (fewer than ~100 samples).

### Mechanism 2
Channel-independent (CI) strategy with S3 format enables effective handling of heterogeneous SCADA data. The Timer model splits multivariate time series into univariate sequences and normalizes them using instance normalization. This allows the model to process time series with different numbers of variables, sampling frequencies, and scales without requiring temporal alignment. Core assumption: The loss of inter-variable correlation information in CI strategy is compensated by the model's ability to learn temporal patterns independently and implicitly capture relationships during fine-tuning. Break condition: When inter-variable correlations are critical for prediction accuracy and cannot be recovered through independent modeling.

### Mechanism 3
Tokenization with non-overlapping patches reduces computational complexity while preserving local trend information for long-term forecasting. The Timer model segments time series into patches of 96 time points each, creating tokens that balance computational efficiency (quadratic attention complexity) with the preservation of local temporal patterns essential for forecasting. Core assumption: Local trend characteristics within 96-time-point patches are sufficient for capturing the information needed for accurate time series forecasting, particularly for longer prediction horizons. Break condition: When important temporal patterns span beyond the patch size or when fine-grained temporal resolution is critical for prediction accuracy.

## Foundational Learning

- **Time series preprocessing and outlier detection**: Why needed here: SCADA data contains sensor noise, transient errors, and equipment failures that must be cleaned before model training. Quick check question: What are the three main steps in the data cleaning pipeline described in Section 2.2.1?

- **Transformer architecture and attention mechanisms**: Why needed here: The Timer model uses a decoder-only transformer with patch-based tokenization, requiring understanding of self-attention and positional encoding. Quick check question: How does the decoder-only architecture differ from encoder-decoder transformers in terms of attention masking?

- **Transfer learning and fine-tuning strategies**: Why needed here: The model is pre-trained on UTSD and fine-tuned on SCADA data, requiring knowledge of how to adapt pre-trained models to new domains. Quick check question: What learning rate strategy is used during fine-tuning versus training from scratch?

## Architecture Onboarding

- **Component map**: SCADA data (4 variables) -> Outlier detection -> Sliding window segmentation (768 points) -> S3 format conversion -> Tokenization (8 patches of 96) -> Timer backbone (8 decoder blocks, 1024 dimension, 2048 FFN, 8-head attention) -> Autoregressive next-token prediction -> MSE evaluation

- **Critical path**: Data cleaning → Sliding window segmentation → S3 format conversion → Tokenization → Fine-tuning → Inference via iterative next-token prediction

- **Design tradeoffs**: CI strategy loses inter-variable correlations but gains adaptability to heterogeneous data; Patch-based tokenization reduces computational cost but may lose fine-grained temporal patterns; Pre-training on diverse domains enables few-shot learning but may introduce domain mismatch

- **Failure signatures**: Poor performance on short-term predictions despite good long-term results (suggests insufficient patch resolution); Degradation when adding more variables (indicates CI strategy limitations); Sensitivity to data volume changes (suggests overfitting or underfitting)

- **First 3 experiments**: 1) Compare Timer-finetuned vs Timer-pretrained vs Timer-scratch on Plant 1 with varying prediction lengths (1, 6, 12, 24, 48, 96); 2) Evaluate model performance across different data volume percentages (10%, 25%, 50%, 75%, 100%) on Plant 1; 3) Test one-turbine fine-tuning generalization by training on turbine A and evaluating on turbines B, C, D from Plant 1

## Open Questions the Paper Calls Out

### Open Question 1
How do the pre-training datasets' characteristics (domains, sampling rates, etc.) influence Timer's performance on wind turbine SCADA data? Basis in paper: [explicit] The paper mentions Timer was pre-trained on the Unified Time Series Dataset (UTSD) spanning various domains with diverse sampling rates. Why unresolved: The paper does not analyze how specific pre-training data characteristics affect SCADA performance, only that pre-training helps but not consistently. What evidence would resolve it: Systematic ablation studies varying pre-training data domains and sampling rates, then measuring downstream SCADA performance.

### Open Question 2
What architectural modifications to Timer would improve long-term forecasting accuracy for wind turbine SCADA data? Basis in paper: [inferred] The paper shows Timer's advantages diminish for longer prediction horizons, suggesting architectural limitations. Why unresolved: The paper only uses Timer's existing architecture without exploring modifications tailored to wind data characteristics. What evidence would resolve it: Comparative studies testing architectural variants (e.g., different tokenization strategies, attention mechanisms) specifically designed for wind SCADA patterns.

### Open Question 3
How does Timer's performance scale with the number of turbines in a wind farm during cross-turbine generalization? Basis in paper: [explicit] The paper tests one-turbine fine-tuning for whole-plant prediction but doesn't explore scaling with farm size. Why unresolved: The experiment only considers a single farm with 64 turbines, not examining performance across different farm sizes. What evidence would resolve it: Experiments testing Timer's generalization across farms with varying numbers of turbines (10s to 1000s) to establish scalability limits.

## Limitations

- Data domain mismatch between pre-training data (10 diverse fields) and wind turbine SCADA data may limit transfer learning effectiveness
- Channel-independent strategy explicitly discards inter-variable correlations, which may be critical for accurate wind turbine predictions
- Fixed 96-time-point patch size may not optimally capture temporal patterns across different prediction horizons

## Confidence

**High Confidence Claims**:
- The Timer model can be successfully fine-tuned on wind turbine SCADA data using the described methodology
- The model demonstrates few-shot learning capabilities when data is limited
- The S3 format and CI strategy effectively handle heterogeneous multivariate time series

**Medium Confidence Claims**:
- Timer outperforms LSTM and transformer baselines in data-scarce scenarios
- Pre-training provides meaningful advantage over training from scratch
- The 96-point patch size represents an appropriate balance for wind turbine forecasting

**Low Confidence Claims**:
- Timer's superiority is consistent across all data volumes and prediction horizons
- CI strategy performance matches or exceeds CD strategy for wind turbine applications
- Pre-training on UTSD is optimal for wind turbine SCADA data specifically

## Next Checks

1. **Domain Similarity Analysis**: Compute statistical similarity metrics (e.g., Kolmogorov-Smirnov test, Earth Mover's Distance) between wind turbine SCADA data distributions and each pre-training domain in UTSD. Correlate these similarity scores with fine-tuning performance to identify whether domain alignment predicts transfer learning success.

2. **CI vs CD Strategy Comparison**: Implement and train Timer with channel-dependent strategy on the same wind turbine datasets. Compare MSE performance across all prediction horizons and data volume scenarios to empirically validate whether CI's adaptability advantages outweigh correlation information loss for this specific application.

3. **Patch Size Sensitivity Study**: Systematically evaluate Timer performance across a range of patch sizes (e.g., 48, 64, 96, 128, 192 time points) for each prediction horizon. Identify optimal patch sizes for short-term vs long-term forecasting and determine whether a single fixed size is appropriate or adaptive sizing would improve results.