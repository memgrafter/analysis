---
ver: rpa2
title: Self-Adaptive Paraphrasing and Preference Learning for Improved Claim Verifiability
arxiv_id: '2412.11653'
source_url: https://arxiv.org/abs/2412.11653
tags:
- claim
- claims
- fact
- checking
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving fact-checking performance
  on colloquial social media claims by optimizing the claim representation itself
  rather than modifying the fact-checking model. The proposed self-adaptive framework
  uses direct preference optimization (DPO) to iteratively refine claim paraphrases
  based on feedback from a fact-checking model, aiming to generate more verifiable
  inputs.
---

# Self-Adaptive Paraphrasing and Preference Learning for Improved Claim Verifiability

## Quick Facts
- arXiv ID: 2412.11653
- Source URL: https://arxiv.org/abs/2412.11653
- Reference count: 16
- Self-adaptive paraphrasing improves fact-checking performance on social media claims through iterative refinement

## Executive Summary
This paper introduces a self-adaptive framework that optimizes claim representations for fact-checking by iteratively refining paraphrases based on feedback from a fact-checking model. The approach uses Direct Preference Optimization (DPO) to align language model outputs with fact-checking model preferences, generating more verifiable inputs from colloquial social media claims. Experiments on medical claims show that self-adapted paraphrases consistently outperform original tweets and achieve comparable performance to zero-shot extraction baselines, with the most significant improvements observed for refuted claims.

## Method Summary
The method uses an iterative optimization loop where a paraphrasing model (Llama-3-8B-Instruct) generates claim paraphrases, which are evaluated by a fact-checking model (mDeBERTa). Preference pairs are created based on prediction accuracy comparisons between current and previous iterations, and these pairs are used to fine-tune the paraphrasing model via DPO. The process repeats for multiple iterations, with the goal of generating claims that are more amenable to fact-checking verification while maintaining semantic fidelity to the original social media posts.

## Key Results
- Self-adapted claims achieve F1 scores of 0.43 compared to 0.40 for original tweets
- Self-adapted paraphrases become very concise (14-15 words) compared to social media variants (41 words)
- Performance improvements are most pronounced for refuted claims
- Similarity to original claims remains modest despite optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct Preference Optimization (DPO) aligns language model outputs with fact-checking model preferences through iterative feedback
- Mechanism: The system generates paraphrases, compares fact-checking model predictions for current vs. previous iterations, creates preference pairs based on prediction accuracy, and uses these pairs to fine-tune the paraphrasing model via DPO loss
- Core assumption: Fact-checking model prediction accuracy serves as a reliable proxy for claim verifiability
- Evidence anchors:
  - [abstract]: "we iteratively optimize the LM to generate a claim paraphrase that increases the performance of a fact checking model"
  - [section]: "For each claim, we compare the prediction of the fact checking model with the prediction for the claim-evidence pair from the previous iteration"
  - [corpus]: Weak - related papers focus on claim detection and fact-checking but don't specifically validate DPO for claim optimization
- Break condition: If fact-checking model predictions plateau or become unstable across iterations, the preference signal becomes unreliable and DPO learning degrades

### Mechanism 2
- Claim: Self-adaptive paraphrasing produces more concise claims that are easier for fact-checking models to process
- Mechanism: Iterative refinement systematically removes contextual elements and redundant phrasing, creating shorter, more focused claim representations
- Core assumption: Fact-checking models perform better on concise, well-structured claims than on lengthy, contextualized social media posts
- Evidence anchors:
  - [abstract]: "self-adapted claims become very concise (around 14-15 words) compared to their social media variants (41 words)"
  - [section]: "the performance increases slightly (from .40F1 to .43F1)" as claims become more concise
  - [corpus]: Weak - related work mentions claim extraction but doesn't analyze length optimization effects
- Break condition: If paraphrases become too terse and lose essential semantic content, fact-checking performance may decline despite brevity

### Mechanism 3
- Claim: Iterative refinement captures claim properties that align with fact-checking model expectations rather than human readability
- Mechanism: Continuous alignment through preference learning shifts paraphrases away from original social media phrasing toward model-optimized representations
- Core assumption: Fact-checking model's input preferences differ from human expectations of checkworthy claims
- Evidence anchors:
  - [abstract]: "similarity to original claims remains modest" and "self-adapted claims are shorter than the seed claims"
  - [section]: "self-adapted claims are very concise compared to their social media variants and even shorter than human-written claims"
  - [corpus]: Weak - no direct evidence in related work about model-optimized vs human-optimized claim representations
- Break condition: If alignment causes paraphrases to become semantically divergent from original claims, losing factual correspondence

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: Provides a way to fine-tune language models using preference pairs rather than labeled data, enabling alignment with black-box fact-checking models
  - Quick check question: What is the key difference between DPO and traditional supervised fine-tuning approaches?
- Concept: Natural Language Inference (NLI) framing for fact-checking
  - Why needed here: Transforms fact-checking into a three-way classification problem (ENTAILMENT, NEUTRAL, CONTRADICTION) that works with existing NLI models
  - Quick check question: How does framing fact-checking as NLI differ from traditional veracity classification approaches?
- Concept: Preference pair construction from model predictions
  - Why needed here: Creates training signals from fact-checking model outputs when no human-annotated preference data exists
  - Quick check question: What criteria determine which paraphrase becomes the "preferred" choice in the preference pair?

## Architecture Onboarding

- Component map: Social media post -> Paraphrasing engine (Llama-3-8B-Instruct) -> Fact-checking model (mDeBERTa) -> Preference generator -> DPO trainer -> Optimized claim paraphrases
- Critical path: Social media post → Paraphrase generation → Fact-checking prediction → Preference pair creation → DPO fine-tuning → New paraphrase
- Design tradeoffs:
  - Using NLI model vs. specialized medical fact-checking model: General model enables broader applicability but may miss domain-specific nuances
  - Reference model constraint in DPO: Prevents reward hacking but may limit optimization potential
  - Synthetic tweet generation: Provides controlled data but may not capture full diversity of real social media discourse
- Failure signatures:
  - Fact-checking performance plateaus early → Preference signal becomes weak or optimization saturates
  - Paraphrases become semantically unstable → Reference model constraint too loose or preference pairs noisy
  - Model degradation across iterations → Learning rate too high or preference signal inconsistent
- First 3 experiments:
  1. Baseline comparison: Run fact-checking on original tweets vs. zero-shot extracted claims to establish performance gap
  2. Single iteration DPO: Apply one DPO update and measure immediate performance change to validate preference learning signal
  3. Length analysis: Track claim length reduction across iterations to confirm conciseness optimization mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the self-adaptive framework perform if we removed the reference model constraint in the DPO optimization process?
- Basis in paper: [explicit] The authors mention this as a potential avenue for future research, noting that the constraint might be causing the paraphrases to stagnate after initial iterations
- Why unresolved: The authors hypothesize that the reference model constraint might be limiting exploration of potentially more effective claim formulations, but did not experimentally test this hypothesis
- What evidence would resolve it: Comparing the fact-checking performance and claim evolution trajectories with and without the reference model constraint across multiple iterations would demonstrate whether unconstrained optimization leads to more effective claim representations

### Open Question 2
- Question: How sensitive is the self-adaptive paraphrasing process to variations in the prompt used for claim extraction?
- Basis in paper: [explicit] The authors note that "LLMs are sensitive with respect to prompt variation" and suggest this as an area for future investigation
- Why unresolved: While the authors used two prompt variants (0-ex and 0-cw) as baselines, they did not systematically explore how different prompts for the iterative DPO process might affect outcomes
- What evidence would resolve it: Systematically testing multiple prompt formulations during the DPO iterations while tracking fact-checking performance and claim properties would reveal the impact of prompt sensitivity

### Open Question 3
- Question: Would incorporating more diverse synthetic data generation strategies improve the self-adaptive framework's performance across different claim types?
- Basis in paper: [inferred] The authors acknowledge that their synthetic tweets "frequently use similar paraphrases" and that "fact checking relies on claim-specific evidence" which is often lacking in social media data
- Why unresolved: The current approach uses a limited set of personas and may not capture the full diversity of how claims are expressed in real social media discourse
- What evidence would resolve it: Testing the framework with more varied synthetic data generation approaches (different personas, rhetorical styles, or even incorporating actual social media data) while measuring performance across claim types would demonstrate whether diversity improves outcomes

### Open Question 4
- Question: How does the performance of self-adaptive paraphrasing compare when using a fact-checking model specifically trained for biomedical claims versus a general-domain NLI model?
- Basis in paper: [explicit] The authors chose a general-domain NLI model "to gauge the capability of our method" and acknowledge this as a limitation, suggesting that using a specialized biomedical model could improve performance
- Why unresolved: The current experiments use a general NLI model, making it unclear whether performance improvements are due to the self-adaptive framework itself or limitations of the fact-checking component
- What evidence would resolve it: Running the same self-adaptive framework with both the general NLI model and a biomedical-specific fact-checking model on the same dataset would isolate the contribution of the framework from the quality of the fact-checking component

## Limitations
- Weak evidence for core DPO mechanism effectiveness - no direct validation that fact-checking model preferences accurately reflect claim verifiability
- Modest performance improvements (F1 score increases from 0.40 to 0.43) suggest potential ceiling effects or optimization plateaus
- Synthetic test data limits generalizability to real-world social media discourse
- Lack of human evaluation for semantic fidelity of paraphrased claims

## Confidence

**High Confidence**: The mechanism of claim length reduction through iterative refinement is well-supported by the evidence showing self-adapted claims consistently become shorter than original tweets and even human-written claims.

**Medium Confidence**: The claim that DPO alignment improves fact-checking performance has moderate support, though the magnitude of improvement is modest and could be influenced by other factors beyond the preference learning mechanism.

**Low Confidence**: The assertion that self-adapted claims better match fact-checking model expectations rather than human readability has weak support, as the analysis focuses on length and similarity metrics rather than semantic alignment with model preferences.

## Next Checks

1. **Human Evaluation Study**: Conduct a blind evaluation where human fact-checkers assess whether self-adapted paraphrases maintain semantic equivalence with original claims while being more amenable to verification, measuring both preservation of meaning and checkworthiness.

2. **Cross-Domain Robustness Test**: Apply the self-adaptive framework to non-medical social media claims (politics, finance, etc.) to evaluate whether the performance improvements generalize beyond the biomedical domain used in the study.

3. **Preference Signal Analysis**: Implement ablation studies to determine whether fact-checking model prediction accuracy is the optimal preference signal by comparing against alternative signals like confidence scores or uncertainty measures from the fact-checking model.