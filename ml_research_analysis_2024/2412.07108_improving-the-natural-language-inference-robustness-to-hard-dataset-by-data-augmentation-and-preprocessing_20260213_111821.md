---
ver: rpa2
title: Improving the Natural Language Inference robustness to hard dataset by data
  augmentation and preprocessing
arxiv_id: '2412.07108'
source_url: https://arxiv.org/abs/2412.07108
tags:
- dataset
- data
- anli
- premise
- snli
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles Natural Language Inference (NLI) robustness
  issues where models fail on out-of-distribution or hard datasets due to learning
  spurious correlations instead of genuine inference. The author proposes data augmentation
  and preprocessing methods to address three key weaknesses: word overlap, numerical
  reasoning, and length mismatch.'
---

# Improving the Natural Language Inference robustness to hard dataset by data augmentation and preprocessing

## Quick Facts
- arXiv ID: 2412.07108
- Source URL: https://arxiv.org/abs/2412.07108
- Authors: Zijiang Yang
- Reference count: 3
- Key outcome: ELECTRA-small with data augmentation improves HANS accuracy by 12-14% and ANLI by 6-9% with only ~1000 augmented samples

## Executive Summary
This paper addresses Natural Language Inference (NLI) robustness issues where models fail on out-of-distribution or hard datasets due to learning spurious correlations instead of genuine inference. The author proposes data augmentation and preprocessing methods to address three key weaknesses: word overlap, numerical reasoning, and length mismatch. Experiments using ELECTRA-small on datasets like SNLI, HANS, and ANLI show that these methods improve performance on hard datasets by 12-14% on HANS and 6-9% on ANLI, while only slightly reducing accuracy on simple datasets. The approach requires minimal additional data (around 1000 augmented samples) to achieve significant robustness gains.

## Method Summary
The method combines data augmentation and preprocessing to improve NLI robustness. Data augmentation addresses word overlap by generating synthetic examples with controlled overlap, numerical reasoning through year comparison examples, and length mismatch through a split algorithm that processes long paragraphs sentence by sentence. The approach uses ELECTRA-small fine-tuned on augmented SNLI data, with the split algorithm applied during evaluation. The augmented dataset contains approximately 1000 examples (500 for word overlap, 500 for numerical reasoning) combined with the original 570K SNLI examples for training.

## Key Results
- HANS accuracy improves by 12-14% on hard inference patterns
- ANLI performance increases by 6-9% on out-of-distribution examples
- Only ~1000 augmented samples (0.5% of original data) needed for substantial robustness gains
- SNLI accuracy decreases slightly, trading simple dataset performance for robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic examples with controlled word overlap disrupt spurious correlation learning between word overlap and entailment labels.
- Mechanism: By generating premise-hypothesis pairs with high word overlap that have known gold labels (entailment, contradiction, or neutral based on syntactic structure), the model is forced to learn semantic relationships rather than memorizing overlap patterns.
- Core assumption: The model initially relies heavily on word overlap as a heuristic for classification.
- Evidence anchors:
  - [abstract] "We propose the data augmentation and preprocessing methods to solve the word overlap, numerical reasoning and length mismatch problems."
  - [section] "We train the models via augmenting simple premise-hypothesis pairs that have large overlapping."
  - [corpus] Weak evidence - no direct citations about word overlap disruption in NLI models.
- Break condition: If the synthetic examples do not cover sufficient linguistic diversity, the model may find new spurious correlations.

### Mechanism 2
- Claim: Numerical reasoning augmentation improves chronological comparison capability.
- Mechanism: Generating premise-hypothesis pairs with explicit year comparisons forces the model to learn arithmetic reasoning rather than relying on lexical cues.
- Core assumption: The model fails on numerical reasoning tasks because it lacks exposure to such examples during training.
- Evidence anchors:
  - [abstract] "We propose the data augmentation and preprocessing methods to solve the word overlap, numerical reasoning and length mismatch problems."
  - [section] "We solve it by augmenting simple premise-hypothesis pairs that ask for comparing the years."
  - [corpus] Weak evidence - no direct citations about numerical reasoning in NLI models.
- Break condition: If the augmented examples do not cover edge cases (e.g., birth years vs death years), the model may still fail on unseen numerical patterns.

### Mechanism 3
- Claim: Split algorithm prevents information loss from long paragraph truncation.
- Mechanism: By processing long premises sentence-by-sentence and aggregating predictions, the model can focus on relevant information without losing context due to fixed input length constraints.
- Core assumption: Transformer models lose important information when truncating long paragraphs.
- Evidence anchors:
  - [abstract] "For length mismatch, a split algorithm processes long paragraphs sentence by sentence."
  - [section] "We advocate that for the paragraph input, it is advised to split the paragraph into sentences and then test the hypothesis for each sentence."
  - [corpus] Weak evidence - no direct citations about truncation effects in NLI models.
- Break condition: If the sentence splitting disrupts contextual dependencies, the model may lose coherence in reasoning.

## Foundational Learning

- Concept: Data augmentation for robustness
  - Why needed here: Standard NLI datasets contain biases that models exploit rather than learning genuine inference.
  - Quick check question: What would happen if you trained only on the original SNLI dataset without any augmentation?
- Concept: Heuristic learning in NLI models
  - Why needed here: Understanding why models fail on hard datasets helps design targeted interventions.
  - Quick check question: Can you identify examples where word overlap would lead to incorrect classification?
- Concept: Transformer input length limitations
  - Why needed here: Long premises contain important information that gets truncated in standard processing.
  - Quick check question: How would you modify the input processing pipeline to handle paragraphs of arbitrary length?

## Architecture Onboarding

- Component map: ELECTRA-small base model -> Data augmentation module -> Split preprocessing module -> HANS/ANLI evaluation
- Critical path: Generate augmented data -> Train model on combined dataset -> Apply Split algorithm during evaluation -> Measure robustness improvement
- Design tradeoffs: More augmented data improves robustness but increases training time; Split algorithm adds inference overhead but handles long contexts better
- Failure signatures: Accuracy drops on HANS indicate heuristic learning; poor ANLI performance suggests lack of generalization; length mismatch errors show truncation issues
- First 3 experiments:
  1. Train ELECTRA-small on SNLI only, measure HANS and ANLI performance
  2. Add word overlap augmentation, retrain, and compare HANS improvement
  3. Apply Split algorithm to long premises, measure impact on both HANS and ANLI

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number and distribution of augmented samples needed to maximize robustness improvements while minimizing performance degradation on simple datasets?
- Basis in paper: [explicit] The paper shows performance plateaus around 1000 augmented samples (3% of original training data) but doesn't explore the full parameter space of augmentation strategies
- Why unresolved: The experiments only tested discrete increments up to 4000 samples, without exploring different ratios, distributions, or sampling strategies for the augmented data
- What evidence would resolve it: Systematic experiments varying augmentation ratios (0.5%, 1%, 2%, 3%, 5%, 10% of training data), different sampling distributions, and cross-validation to find optimal trade-offs

### Open Question 2
- Question: How does the proposed approach generalize to other transformer architectures beyond ELECTRA-small (e.g., BERT, RoBERTa, larger models)?
- Basis in paper: [inferred] The paper exclusively uses ELECTRA-small and makes no claims about other architectures, despite the general nature of the proposed methods
- Why unresolved: The experiments were limited to a single model architecture, and larger models might respond differently to the same augmentation and preprocessing strategies
- What evidence would resolve it: Replication studies with multiple transformer architectures, comparing performance gains across different model sizes and pre-training approaches

### Open Question 3
- Question: Can the Split algorithm be extended to handle longer sequences while maintaining or improving performance, possibly through hierarchical processing or attention mechanisms?
- Basis in paper: [explicit] The paper notes that transformers have fixed-length limitations and proposes sentence-level splitting, but doesn't explore more sophisticated approaches to handling longer inputs
- Why unresolved: The proposed Split algorithm is relatively simple and may not capture long-range dependencies or context across sentence boundaries
- What evidence would resolve it: Comparative studies of different long-sequence handling strategies (hierarchical models, sliding window approaches, memory networks) applied to the same problem domain

### Open Question 4
- Question: What are the computational trade-offs between training with augmented data versus using larger models or more sophisticated architectures for achieving similar robustness gains?
- Basis in paper: [inferred] The paper emphasizes the efficiency of its approach but doesn't directly compare computational costs against alternative methods
- Why unresolved: The paper focuses on accuracy improvements without quantifying the computational resources required for data augmentation versus alternative approaches
- What evidence would resolve it: Comprehensive benchmarking of training time, inference latency, and resource utilization across different model sizes, architectures, and data augmentation strategies

## Limitations

- Implementation details for data augmentation algorithms are not fully specified, making exact reproduction difficult
- Sample size of 1000 augmented examples appears small relative to 570K SNLI training examples, raising questions about statistical robustness
- Methodology focuses on three specific weaknesses but may not address other common NLI model failure modes

## Confidence

- High Confidence: The general approach of using data augmentation and preprocessing to improve NLI robustness is sound and aligns with established ML practices
- Medium Confidence: The specific improvements on HANS (12-14%) and ANLI (6-9%) datasets are reported but lack detailed statistical significance analysis
- Low Confidence: The claim that minimal augmentation data (around 1000 examples) achieves substantial robustness gains requires further validation across different model architectures

## Next Checks

1. **Statistical Robustness Test**: Conduct multiple runs with different random seeds and report confidence intervals for the improvement metrics on HANS and ANLI datasets
2. **Ablation Study**: Systematically remove each augmentation type (word overlap, numerical reasoning, length mismatch) to quantify their individual contributions to the overall performance improvement
3. **Generalization Test**: Apply the same augmentation and preprocessing pipeline to different NLI models (e.g., BERT, RoBERTa) to verify whether the improvements are model-specific or generalize across architectures