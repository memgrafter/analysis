---
ver: rpa2
title: Improving Fast Adversarial Training via Self-Knowledge Guidance
arxiv_id: '2409.17589'
source_url: https://arxiv.org/abs/2409.17589
tags:
- accuracy
- training
- clean
- robust
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies and addresses the class imbalance problem
  in fast adversarial training (FAT). The authors analyze the misalignment between
  clean and robust accuracy across different classes, revealing significant disparities
  in model performance.
---

# Improving Fast Adversarial Training via Self-Knowledge Guidance

## Quick Facts
- arXiv ID: 2409.17589
- Source URL: https://arxiv.org/abs/2409.17589
- Reference count: 40
- Primary result: SKG-FAT significantly improves robust accuracy across various attacks while maintaining competitive clean accuracy, outperforming state-of-the-art FAT methods without introducing additional hyperparameters.

## Executive Summary
This paper addresses the class imbalance problem in fast adversarial training (FAT) by identifying significant disparities in model performance across different classes. The authors propose the Self-Knowledge Guided Fast Adversarial Training (SKG-FAT) framework, which incorporates two key components: self-knowledge guided regularization (CWR and AGR) and self-knowledge guided label relaxation (SKLR). CWR assigns differentiated regularization weights based on class-wise training state, while AGR dynamically adjusts regularization strength according to accuracy alignment groups. SKLR dynamically adjusts labels based on training performance to stabilize the training process. Extensive experiments demonstrate that SKG-FAT significantly improves robust accuracy across various attacks while maintaining competitive clean accuracy, outperforming state-of-the-art FAT methods without introducing additional hyperparameters.

## Method Summary
SKG-FAT is a framework that addresses class imbalance in fast adversarial training through three main components: self-knowledge guided regularization (CWR and AGR) and self-knowledge guided label relaxation (SKLR). The method first computes clean and adversarial outputs during training, then calculates class-wise clean accuracy to guide the regularization process. CWR uses clean accuracy as a proxy for feature learning quality to adjust regularization strength, while AGR groups examples based on accuracy alignment (GCGR, GCBR, BCGR, BCBR) and adjusts regularization based on group size. SKLR dynamically adjusts label relaxation factors based on class-wise training performance to stabilize training and prevent catastrophic overfitting. The method is implemented on top of FGSM-RS with PGI-BP initialization for baseline FAT.

## Key Results
- SKG-FAT improves robust accuracy against PGD-10/50, MI, CW, and AA attacks on CIFAR-10/100, Tiny ImageNet, and ImageNet-100 datasets
- The method maintains competitive clean accuracy while achieving significant robustness gains
- Performance improvements are achieved without introducing additional hyperparameters beyond the baseline FAT method
- SKG-FAT outperforms state-of-the-art FAT methods in comprehensive experimental evaluations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-Knowledge Guided Regularization (CWR) improves robust accuracy by differentially weighting regularization based on class-wise clean accuracy.
- Mechanism: CWR uses naturally generated clean accuracy of each class during training as guidance to adjust regularization strength, with higher clean accuracy classes receiving stronger regularization.
- Core assumption: Clean accuracy is a reliable proxy for the model's learned features and is easier to improve than robust accuracy.
- Evidence anchors:
  - [abstract]: "First, we introduce self-knowledge guided regularization, which assigns differentiated regularization weights to each class based on its training state, alleviating class disparity."
  - [section]: "First, given that clean accuracy is higher than robust accuracy, we aim to enhance robust accuracy by minimizing the outputs for clean examples and AEs."
- Break condition: If clean accuracy does not correlate with feature learning quality or if robust accuracy becomes easier to improve than clean accuracy.

### Mechanism 2
- Claim: Self-Knowledge Guided Label Relaxation (SKLR) stabilizes training and mitigates catastrophic overfitting by dynamically adjusting label confidence based on class-wise training performance.
- Mechanism: SKLR uses clean accuracy to dynamically adjust label relaxation factor for each class, initially using higher confidence to learn primary features then increasing label smoothness to prevent over-memory.
- Core assumption: The imbalance between inner and outer optimization in FAT causes catastrophic overfitting, which can be mitigated by adjusting label confidence based on training state.
- Evidence anchors:
  - [abstract]: "Additionally, we propose self-knowledge guided label relaxation, which adjusts label relaxation according to the training accuracy, alleviating the misalignment and improving robustness."
  - [section]: "Motivated by our findings of class-wise differences in example accuracy, we propose self-knowledge guided label relaxation (SKLR) as... This approach stabilizes the minimax optimization in FAT, effectively preventing catastrophic overfitting."
- Break condition: If dynamic adjustment of label confidence does not effectively stabilize training or if catastrophic overfitting persists despite label relaxation.

### Mechanism 3
- Claim: Accuracy Alignment Guided Regularization (AGR) improves robust accuracy by emphasizing regularization for classes with misaligned clean and robust accuracy.
- Mechanism: AGR groups examples based on alignment between clean and robust accuracy (GCGR, GCBR, BCGR, BCBR) and adjusts regularization strength based on the number of classes in each group.
- Core assumption: The number of classes in each accuracy alignment group is a more effective differentiator for regularization strength than clean accuracy alone, especially when the number of classes is large.
- Evidence anchors:
  - [abstract]: "First, we introduce self-knowledge guided regularization, which assigns differentiated regularization weights to each class based on its training state, alleviating class disparity."
  - [section]: "Considering the classes are divided into four groups and displaying distinct accuracy situations... we use the number of classes in each group of the perspective of accuracy alignment to implement the AGR guidance factor."
- Break condition: If the number of classes in each group does not correlate with the need for differentiated regularization strength or if performance gains are not significant.

## Foundational Learning

- Concept: Adversarial training and fast adversarial training (FAT)
  - Why needed here: Understanding the basics of adversarial training and FAT is crucial for comprehending the problem of class imbalance and the proposed solutions.
  - Quick check question: What is the key difference between multi-step adversarial training and fast adversarial training in terms of computational efficiency and robustness?

- Concept: Class-wise and accuracy alignment perspectives
  - Why needed here: These perspectives are essential for analyzing the class imbalance problem in FAT and understanding the proposed solutions.
  - Quick check question: How does the accuracy alignment perspective differ from the class-wise perspective in analyzing class imbalance in FAT?

- Concept: Catastrophic overfitting in FAT
  - Why needed here: Catastrophic overfitting is a critical issue in FAT that the proposed SKLR aims to mitigate.
  - Quick check question: What is catastrophic overfitting in FAT, and how does it affect the robustness of the model?

## Architecture Onboarding

- Component map: Training loop -> Compute clean and adversarial outputs -> Calculate class-wise clean accuracy -> Apply CWR, SKLR, and AGR -> Update model parameters
- Critical path: Training loop → Compute clean and adversarial outputs → Calculate class-wise clean accuracy → Apply CWR, SKLR, and AGR → Update model parameters
- Design tradeoffs:
  - Using clean accuracy as a proxy for feature learning quality may not always be accurate
  - Dynamic label relaxation may introduce instability if not implemented carefully
  - Emphasizing regularization for misaligned classes may lead to overfitting if not balanced with other regularization techniques
- Failure signatures:
  - If CWR does not improve robust accuracy, it may indicate that clean accuracy is not a reliable proxy for feature learning quality
  - If SKLR does not stabilize training or mitigate catastrophic overfitting, it may indicate that dynamic adjustment of label confidence is not effective
  - If AGR does not improve robust accuracy, it may indicate that number of classes in each group is not a good differentiator for regularization strength
- First 3 experiments:
  1. Implement CWR with a simple dataset (e.g., CIFAR-10) and compare the robust accuracy with the baseline FAT method
  2. Implement SKLR with the same dataset and compare the training stability and catastrophic overfitting with the baseline method
  3. Implement AGR with the same dataset and compare the robust accuracy for classes with misaligned clean and robust accuracy with the baseline method

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the class imbalance problem in fast adversarial training (FAT) manifest differently across various architectures (e.g., ResNet, DenseNet, Inception, Swin Transformer)?
- Basis in paper: [explicit] The paper states, "Empirical results from both class-wise and accuracy alignment perspectives reveal two key conclusions... We conduct experiments on CIFAR-10/100, Tiny ImageNet, and ImageNet-100 with ResNet-18 as default, but also evaluate various models on CIFAR-100, including DenseNet-121, Inception V3, and Swin Transformer."
- Why unresolved: While the paper extends experiments to different models, it does not provide a detailed comparative analysis of how the class imbalance problem manifests across these architectures.
- What evidence would resolve it: A comprehensive study comparing the class-wise and accuracy alignment perspectives across different architectures, highlighting specific differences in the manifestation of class imbalance.

### Open Question 2
- Question: What are the long-term effects of using self-knowledge guided label relaxation (SKLR) on the model's ability to generalize to unseen data?
- Basis in paper: [explicit] The paper introduces SKLR and states, "SKLR dynamically adjusts labels based on the training performance of each class, improving training stability and overall model robustness."
- Why unresolved: The paper focuses on the immediate effects of SKLR on robustness and training stability but does not explore its impact on long-term generalization to unseen data.
- What evidence would resolve it: Experiments evaluating the generalization performance of models trained with SKLR on datasets not seen during training, comparing it to models trained without SKLR.

### Open Question 3
- Question: How does the self-knowledge guided regularization (CWR and AGR) perform on datasets with significantly different class distributions compared to CIFAR-10/100, Tiny ImageNet, and ImageNet-100?
- Basis in paper: [explicit] The paper evaluates CWR and AGR on CIFAR-10/100, Tiny ImageNet, and ImageNet-100, demonstrating improved robust accuracy.
- Why unresolved: The paper does not explore the performance of CWR and AGR on datasets with class distributions that are markedly different from those used in the experiments.
- What evidence would resolve it: Testing CWR and AGR on datasets with varying class distributions, such as those with long-tail distributions or highly imbalanced classes, to assess their effectiveness and adaptability.

## Limitations
- The paper does not provide detailed hyperparameter values for λ (regularization strength) and κmin (minimum relaxation factor), which are crucial for reproducing the results
- Exact implementation details of class-wise accuracy calculation and group assignment in AGR are not specified
- The proposed methods rely on the assumption that clean accuracy is a reliable proxy for feature learning quality, which may not always hold true

## Confidence

**Confidence Levels:**
- **High**: The empirical results showing improved robust accuracy across multiple datasets and attacks are well-supported by the experimental data
- **Medium**: The theoretical motivation for using clean accuracy as a proxy for feature learning quality and the mechanism of SKLR to mitigate catastrophic overfitting are plausible but require further validation
- **Low**: The effectiveness of AGR in grouping classes based on accuracy alignment and the specific implementation details of the proposed methods are not fully validated due to lack of detailed hyperparameter values and implementation specifics

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Conduct experiments to determine the sensitivity of the proposed method to the regularization strength λ and minimum relaxation factor κmin. Vary these hyperparameters and observe the impact on robust accuracy and training stability.

2. **Ablation Study**: Perform an ablation study to isolate the effects of CWR, AGR, and SKLR. Train models with each component individually and in combination to understand their individual contributions to the overall performance improvement.

3. **Cross-Dataset Generalization**: Evaluate the proposed method on additional datasets beyond CIFAR-10/100, Tiny ImageNet, and ImageNet-100 to assess its generalization capabilities. Use datasets with different characteristics (e.g., different number of classes, image sizes) to test the robustness of the approach.