---
ver: rpa2
title: 'FairGridSearch: A Framework to Compare Fairness-Enhancing Models'
arxiv_id: '2401.02183'
source_url: https://arxiv.org/abs/2401.02183
tags:
- fairness
- metrics
- methods
- datasets
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of selecting the best fairness-enhancing
  machine learning model for binary classification tasks, given the variety of available
  base estimators, bias mitigation methods, and evaluation metrics. The FairGridSearch
  framework is proposed to systematically compare different model parameter combinations,
  including six base estimators, nine bias mitigation methods, and multiple classification
  thresholds, while evaluating them using six accuracy metrics and eight fairness
  metrics.
---

# FairGridSearch: A Framework to Compare Fairness-Enhancing Models

## Quick Facts
- arXiv ID: 2401.02183
- Source URL: https://arxiv.org/abs/2401.02183
- Reference count: 0
- Key outcome: FairGridSearch systematically compares fairness-enhancing models using multiple base estimators, bias mitigation methods, and metrics, revealing no universal best approach

## Executive Summary
This paper addresses the challenge of selecting the best fairness-enhancing machine learning model for binary classification tasks, given the variety of available base estimators, bias mitigation methods, and evaluation metrics. The FairGridSearch framework is proposed to systematically compare different model parameter combinations, including six base estimators, nine bias mitigation methods, and multiple classification thresholds, while evaluating them using six accuracy metrics and eight fairness metrics. Experiments on three popular datasets (Adult, COMPAS, and German Credit) reveal that no single base estimator or bias mitigation method consistently outperforms others across all datasets, and that metric selection significantly impacts model evaluation due to inconsistent correlations and varying responses to bias mitigation. The results emphasize the importance of considering multiple factors—base estimators, classification thresholds, and metrics—when building fair machine learning models, beyond bias mitigation methods alone.

## Method Summary
The FairGridSearch framework systematically evaluates model configurations by combining six base estimators (Logistic Regression, Random Forest, Gradient Boosting, SVM, Naive Bayes, and TabTransformer), nine bias mitigation methods (pre-processing, in-processing, and post-processing techniques), and multiple classification thresholds. The framework uses stratified k-fold cross-validation to assess each configuration using six accuracy metrics (ACC, BACC, F1, AUC, MCC, NORM_MCC) and eight fairness metrics (SPD, AOD, EOD, FORD, PPVD, CNS, GEI, TI). The best model is selected based on a cost-based criterion that balances accuracy and fairness. The implementation is available on GitHub and tested on three datasets: Adult, COMPAS, and German Credit.

## Key Results
- No single base estimator or bias mitigation method consistently outperforms others across all datasets
- Metric selection significantly impacts model evaluation due to inconsistent correlations and varying responses to bias mitigation
- Classification threshold values can introduce varying degrees of volatility in model fairness
- Accuracy metrics decrease significantly in an average of 40% of all scenarios after bias mitigation
- Individual fairness metrics show minimal improvement after bias mitigation implementations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** No single base estimator consistently outperforms others in improving model fairness across datasets.
- **Mechanism:** Different base estimators interact differently with bias mitigation methods depending on dataset characteristics, leading to inconsistent fairness improvements.
- **Core assumption:** The effectiveness of bias mitigation methods is dependent on the base estimator used and the specific properties of the dataset.
- **Evidence anchors:**
  - [abstract] "different base estimators and classification threshold values affect the effectiveness of bias mitigation methods and fairness stability respectively, but the effects are not consistent across all datasets."
  - [section] "Base estimators exhibit varying degrees of sensitivity to BM in different datasets... These findings highlight the absence of a universal solution for selecting the optimal base estimator for fair models."
  - [corpus] Weak evidence; corpus papers do not directly address base estimator consistency across datasets.

### Mechanism 2
- **Claim:** Metric selection significantly impacts model evaluation due to inconsistent correlations and varying responses to bias mitigation.
- **Mechanism:** Different accuracy and fairness metrics capture different aspects of model performance and fairness, leading to divergent conclusions about model quality when different metrics are used.
- **Core assumption:** Accuracy and fairness metrics are not perfectly correlated and respond differently to bias mitigation interventions.
- **Evidence anchors:**
  - [abstract] "The results highlight the significance of selecting appropriate accuracy and fairness metrics for model evaluation."
  - [section] "accuracy metrics decrease significantly in an average of 40% of all scenarios... Notably, individual fairness metrics show minimal improvement after BM implementations... Overall, metrics are not always correlated with their alternatives and exhibit distinct reactions to BM methods."
  - [corpus] Weak evidence; corpus papers do not provide detailed analysis of metric correlations and responses to bias mitigation.

### Mechanism 3
- **Claim:** The choice of classification threshold values can introduce varying degrees of volatility in model fairness.
- **Mechanism:** Different classification thresholds affect the trade-off between false positives and false negatives, which in turn impacts fairness metrics that depend on these error types.
- **Core assumption:** The relationship between classification threshold and fairness is not monotonic and varies across datasets.
- **Evidence anchors:**
  - [abstract] "different base estimators and classification threshold values affect the effectiveness of bias mitigation methods and fairness stability respectively, but the effects are not consistent across all datasets."
  - [section] "Fig. 6a and 6c show that accuracy-maximizing threshold values may also generate high volatility in fairness... Given the variability in optimal classification threshold across datasets and the significant impact it has, it is advisable to include a comprehensive set in the comparison to identify the most suitable one."
  - [corpus] Weak evidence; corpus papers do not provide detailed analysis of threshold effects on fairness volatility.

## Foundational Learning

- **Concept:** Binary classification and evaluation metrics
  - **Why needed here:** The framework focuses on binary classification problems and requires understanding of various accuracy and fairness metrics for model evaluation.
  - **Quick check question:** What is the difference between accuracy and balanced accuracy, and when would you prefer one over the other?

- **Concept:** Bias mitigation methods and their categorization
  - **Why needed here:** The framework includes multiple bias mitigation methods (pre-processing, in-processing, post-processing) that need to be understood for proper implementation and evaluation.
  - **Quick check question:** How do pre-processing, in-processing, and post-processing bias mitigation methods differ in their approach to improving fairness?

- **Concept:** Group fairness vs. individual fairness
  - **Why needed here:** The framework incorporates both group and individual fairness metrics, requiring understanding of their differences and implications.
  - **Quick check question:** What is the key difference between group fairness and individual fairness, and why is it challenging to satisfy both simultaneously?

## Architecture Onboarding

- **Component map:**
  Parameter tuning module -> Cross-validation engine -> Metric calculation module -> Best model selection module -> Result aggregation and reporting module

- **Critical path:**
  1. Load dataset and define parameter grid
  2. Iterate through all parameter combinations
  3. For each combination, perform stratified k-fold cross-validation
  4. Apply bias mitigation method (if specified)
  5. Calculate accuracy and fairness metrics
  6. Aggregate results across folds
  7. Select best model based on cost criterion
  8. Generate result table and report

- **Design tradeoffs:**
  - Comprehensive parameter tuning vs. computational efficiency
  - Flexibility in metric selection vs. complexity in result interpretation
  - Inclusion of multiple bias mitigation methods vs. potential interference effects

- **Failure signatures:**
  - Inconsistent or unexpected metric values across folds
  - Failure to improve fairness despite applying bias mitigation methods
  - Computational timeout or memory issues due to large parameter grid

- **First 3 experiments:**
  1. Run FairGridSearch with default parameters on the Adult dataset, using all base estimators and bias mitigation methods, and evaluate using NORM_MCC and SPD.
  2. Compare the results of using different accuracy metrics (ACC, BACC, F1, AUC, MCC, NORM_MCC) on the COMPAS dataset with the same base estimators and bias mitigation methods.
  3. Investigate the impact of different classification threshold values on the German Credit dataset by running FairGridSearch with varying threshold ranges and analyzing the fairness stability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Which specific bias mitigation methods are most effective for different types of datasets or fairness concerns?
- Basis in paper: [explicit] The paper mentions that the effectiveness of bias mitigation methods varies significantly across datasets and no single method consistently outperforms others.
- Why unresolved: The paper acknowledges the variability in effectiveness but does not provide a clear guideline for selecting bias mitigation methods based on dataset characteristics or fairness criteria.
- What evidence would resolve it: A comprehensive study that categorizes datasets based on their characteristics and systematically evaluates the effectiveness of bias mitigation methods across these categories.

### Open Question 2
- Question: How does the choice of classification threshold impact model fairness and accuracy across different datasets?
- Basis in paper: [explicit] The paper discusses the impact of classification thresholds on model fairness and accuracy but notes that optimal threshold values vary across datasets.
- Why unresolved: The paper does not provide a clear methodology for determining the optimal threshold values for different datasets or explain the underlying reasons for the variability.
- What evidence would resolve it: An in-depth analysis of how classification thresholds interact with dataset features and bias mitigation methods to influence model fairness and accuracy.

### Open Question 3
- Question: Are there alternative parameter optimization methods that could be more efficient than grid search for fairness-enhancing models?
- Basis in paper: [inferred] The paper mentions that grid search can be computationally intensive and suggests exploring alternative optimization methods.
- Why unresolved: The paper does not investigate or compare the efficiency of alternative optimization methods with grid search.
- What evidence would resolve it: A comparative study of different parameter optimization methods, including their computational efficiency and effectiveness in identifying optimal model configurations for fairness.

## Limitations
- Inconsistent performance of bias mitigation methods across datasets and base estimators suggests high context-dependency
- Computational complexity of evaluating 3,500+ model configurations poses practical implementation challenges
- Framework focuses solely on structured datasets, leaving uncertainty about applicability to unstructured data

## Confidence
- High Confidence: The core finding that no single base estimator or bias mitigation method consistently outperforms others across datasets is well-supported by experimental results
- Medium Confidence: Claims about metric selection impact and threshold volatility are supported by data but could benefit from additional statistical validation
- Low Confidence: Framework's scalability to larger datasets and more complex model architectures remains unproven

## Next Checks
1. Conduct statistical significance testing on metric correlation coefficients across all dataset-metric pairs to quantify the strength of relationships between different evaluation metrics.
2. Test the framework's computational efficiency by scaling up to larger datasets (10x the current size) and measuring runtime and memory requirements.
3. Evaluate the framework's performance on a multi-class classification task by adapting the methodology and assessing whether core findings about estimator and mitigation method consistency hold.