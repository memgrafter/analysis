---
ver: rpa2
title: 'SEQ+MD: Learning Multi-Task as a SEQuence with Multi-Distribution Data'
arxiv_id: '2408.13357'
source_url: https://arxiv.org/abs/2408.13357
tags:
- learning
- tasks
- data
- features
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the SEQ+MD framework, which addresses two
  key challenges in e-commerce search: multi-task learning (MTL) and multi-distribution
  input from different regions. The authors propose a sequential learning approach
  for MTL, leveraging the natural order of tasks like click, add to cart, and purchase
  to improve performance on complex tasks.'
---

# SEQ+MD: Learning Multi-Task as a SEQuence with Multi-Distribution Data

## Quick Facts
- arXiv ID: 2408.13357
- Source URL: https://arxiv.org/abs/2408.13357
- Reference count: 31
- Primary result: SEQ+MD framework improves purchase task performance by 1.8% while maintaining click performance in multi-task e-commerce search

## Executive Summary
This paper introduces the SEQ+MD framework to address two key challenges in e-commerce search: multi-task learning (MTL) and multi-distribution input from different regions. The authors propose a sequential learning approach that leverages the natural order of tasks like click, add to cart, and purchase to improve performance on complex tasks. Additionally, they introduce a "plug-and-play" multi-distribution (MD) learning module that handles regional variations in input features by separating region-invariant and region-dependent features. The framework was evaluated on in-house e-commerce data, showing improvements across different tasks and platforms compared to state-of-the-art baseline models.

## Method Summary
The SEQ+MD framework combines sequential task learning with multi-distribution handling. For sequential learning, input features are transformed into a task-specific sequence and processed through shared RNN layers, allowing later tasks to benefit from knowledge of earlier tasks. The MD module handles regional variations by separating features into region-invariant and region-dependent groups, processing region-dependent features with country-specific mask weights to transform them into invariant features. The framework was evaluated on an in-house e-commerce dataset with over 20 million interactions from 10 regions and 2 platforms, using Normalized Discounted Cumulative Gain (NDCG) as the primary metric.

## Key Results
- 1.8% increase in purchase task performance compared to state-of-the-art baseline models
- Overall improvements across different tasks and platforms while maintaining click performance
- Demonstrated effectiveness in handling both multi-task learning and multi-regional data simultaneously

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SEQ learns multi-task as a sequence by transforming a single input into a task-specific sequence and processing it through shared RNN layers, allowing later tasks to benefit from knowledge of earlier tasks.
- Mechanism: The input is passed through k-1 MLPs to create a length-k sequence, where each token corresponds to a task. GRU layers process this sequence, with later tasks receiving information from earlier ones through hidden states. A descending probability regularizer ensures output probabilities decrease across tasks.
- Core assumption: Tasks in e-commerce (click → add to cart → purchase) have a natural sequential order where later tasks depend on earlier ones, and this order reflects decreasing probabilities.
- Evidence anchors:
  - [abstract]: "This approach leverages the sequential order within tasks and accounts for regional heterogeneity, enhancing performance on multi-source data."
  - [section]: "We propose learning multi-task sequences within our SEQ architecture... Some tasks naturally form a sequence, e.g., click, add to cart, purchase, where each action occurs in a sequential order, conditional on the previous ones."
  - [corpus]: Weak evidence - related papers focus on different multi-task architectures but don't explore sequential task ordering.
- Break condition: If tasks don't have natural sequential relationships, the SEQ approach would provide no advantage over standard multi-task learning and could potentially harm performance by forcing artificial dependencies.

### Mechanism 2
- Claim: The MD module handles multi-distribution input by separating features into region-invariant and region-dependent groups, processing region-dependent features with country-specific mask weights to transform them into invariant features.
- Mechanism: Input features are split into three parts: country features (cause of distribution shift), dependent features (with multi-distributions), and invariant features (country-agnostic). Country features generate mask weights through an MLP, which are element-wise multiplied with dependent features. The product passes through another MLP to produce transformed dependent features, which are concatenated with invariant features to create a consistent input distribution.
- Core assumption: Regional variations in e-commerce features can be effectively modeled by learning country-specific transformations of dependent features while preserving invariant features.
- Evidence anchors:
  - [abstract]: "Our solution needs to incorporate shopping preference and cultural traditions in different buyer markets... This approach leverages the sequential order within tasks and accounts for regional heterogeneity, enhancing performance on multi-source data."
  - [section]: "We separate input features into region-invariant and region-dependent groups. The region-dependent features are processed with a country embedding in our multi-distribution (MD) learning module, meaning these features are transformed according to their region, and then concatenated with the region-invariant features."
  - [corpus]: Weak evidence - related papers focus on different aspects of multi-distribution learning but don't explore the specific feature separation and transformation approach described here.
- Break condition: If the feature separation heuristic fails (incorrectly classifying features as invariant or dependent), or if country-specific transformations don't effectively address distribution shifts, the MD module would not improve performance and could add unnecessary complexity.

### Mechanism 3
- Claim: The SEQ+MD framework achieves better performance on complex tasks (purchase) by decomposing them into simpler sequential subtasks while simultaneously handling regional heterogeneity through the MD module.
- Mechanism: SEQ decomposes complex purchase prediction into simpler sequential tasks (click → add to cart → purchase), with knowledge flowing from simpler to more complex tasks. MD handles regional heterogeneity by learning country-specific transformations. Together, they address both challenges simultaneously rather than separately.
- Core assumption: Complex e-commerce tasks benefit from sequential decomposition and that regional heterogeneity can be effectively modeled through country-specific feature transformations.
- Evidence anchors:
  - [abstract]: "Evaluations on in-house data showed a strong increase on the high-value engagement including add-to-cart and purchase while keeping click performance neutral compared to state-of-the-art baseline models."
  - [section]: "To this end, we propose the learning multi-task as a SEQuence + Multi-Distribution ( SEQ+MD) framework, which can tackle the two challenges simultaneously."
  - [corpus]: Weak evidence - related papers address either multi-task learning or multi-distribution learning separately but don't combine these approaches as described here.
- Break condition: If sequential decomposition doesn't provide benefits for the specific task relationships in e-commerce, or if regional heterogeneity cannot be effectively modeled through the proposed feature transformation approach, the combined framework would not outperform separate solutions.

## Foundational Learning

- Concept: Recurrent Neural Networks (RNNs) and their variants (GRU, LSTM)
  - Why needed here: SEQ uses RNNs to process the task sequence, allowing information to flow from earlier tasks to later tasks through hidden states
  - Quick check question: How does a GRU cell update its hidden state, and what are the three gates involved in this process?

- Concept: Multi-task learning (MTL) architectures and tradeoffs
  - Why needed here: Understanding different MTL approaches (hard parameter sharing vs soft parameter sharing) is crucial for appreciating why SEQ's sequential approach is novel
  - Quick check question: What are the key differences between shared-bottom (hard parameter sharing) and MOE (soft parameter sharing) approaches in multi-task learning?

- Concept: Domain adaptation and distribution shift
  - Why needed here: The MD module addresses multi-distribution input by learning region-specific transformations, which builds on concepts from domain adaptation
  - Quick check question: What is the difference between domain adaptation and domain generalization, and which one is more relevant to the MD module's approach?

## Architecture Onboarding

- Component map: Input features → Feature splitting (country/dependent/invariant) → MD transformation (country mask weights × dependent features) → SEQ processing (RNN layers) → Output regularization → Final task scores

- Critical path: Input → Feature splitting → MD transformation → SEQ processing → Output regularization → Final task scores

- Design tradeoffs:
  - Sequential vs parallel task processing: SEQ trades parallelism for knowledge transfer between tasks
  - Feature separation in MD: Heuristic-based separation of invariant vs dependent features may not be optimal for all datasets
  - Complexity vs performance: Adding MD and SEQ layers increases model complexity but improves performance on complex tasks

- Failure signatures:
  - Performance degradation on click task after adding MD: Indicates noise in click data or misalignment between region-dependent features and click prediction
  - No improvement over baseline MTL models: Suggests either tasks don't have sequential relationships or feature separation in MD is ineffective
  - Overfitting on small regional datasets: Indicates the model is too complex relative to available data per region

- First 3 experiments:
  1. Compare SEQ with standard shared-bottom MTL on a dataset with clear task sequences (e.g., click → purchase) to validate sequential learning benefits
  2. Test MD module alone by applying it to a single-task model with multi-regional data to isolate its effect on handling distribution shifts
  3. Evaluate transferability by training SEQ+MD on two tasks and testing on three tasks to verify the claimed benefits of sequential learning for task addition

## Open Questions the Paper Calls Out
- No open questions explicitly called out in the paper

## Limitations
- Proprietary evaluation dataset prevents independent verification and limits generalizability
- Feature separation heuristic may not work optimally for datasets with different regional patterns
- Sequential ordering assumption may not hold for all multi-task scenarios beyond e-commerce

## Confidence
- Sequential task learning effectiveness: Medium - Based on internal evaluation showing 1.8% improvement on purchase task, but lacks external validation
- Multi-distribution handling: Medium - Demonstrates improvements across platforms but feature separation approach lacks theoretical guarantees
- Combined framework performance: Medium - Outperforms baselines but component contributions are not isolated through ablation studies

## Next Checks
1. **Ablation study on component contributions**: Train and evaluate models with only SEQ, only MD, and the full SEQ+MD framework on the same dataset to quantify individual and combined effects on task performance.

2. **Cross-domain transfer evaluation**: Apply the SEQ+MD framework to a non-e-commerce multi-task dataset (e.g., medical diagnosis with sequential symptoms) to test generalizability of the sequential task learning approach.

3. **Feature separation sensitivity analysis**: Systematically vary the distribution distance threshold used to classify invariant versus dependent features and measure the impact on model performance across different regional distributions.