---
ver: rpa2
title: Latent mixed-effect models for high-dimensional longitudinal data
arxiv_id: '2409.11008'
source_url: https://arxiv.org/abs/2409.11008
tags:
- latent
- data
- lmm-vae
- longitudinal
- covariates
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of modeling high-dimensional
  longitudinal data with non-linear effects and time-varying covariates. It proposes
  LMM-VAE, a scalable model that leverages linear mixed models (LMMs) and amortized
  variational inference to provide conditional priors for VAEs.
---

# Latent mixed-effect models for high-dimensional longitudinal data

## Quick Facts
- arXiv ID: 2409.11008
- Source URL: https://arxiv.org/abs/2409.11008
- Reference count: 40
- Key outcome: LMM-VAE achieves competitive performance on imputation and prediction tasks for high-dimensional longitudinal data while maintaining interpretability through structured priors

## Executive Summary
This paper introduces LMM-VAE, a scalable model for high-dimensional longitudinal data that combines linear mixed models with variational autoencoders. The model leverages amortized variational inference to incorporate auxiliary covariates into the prior, enabling structured latent representations. LMM-VAE demonstrates strong performance on Health MNIST, Rotating MNIST, and Physionet Challenge 2012 datasets, particularly excelling at imputation and future prediction tasks. The approach bridges the gap between interpretable linear mixed models and flexible deep learning architectures.

## Method Summary
LMM-VAE is a conditional latent variable model that integrates linear mixed models (LMMs) with variational autoencoders (VAEs) to handle high-dimensional longitudinal data. The model uses LMMs to structure the latent space through shared and random effects of auxiliary covariates, while the VAE architecture enables non-linear transformations. A key innovation is the theoretical connection to Gaussian process (GP) prior VAEs, where LMM-VAE can be viewed as a reduced-rank approximation using basis functions. The model is trained using the Gaussian stochastic neural network (GSNN) objective and demonstrates both scalability and interpretability.

## Key Results
- LMM-VAE achieves competitive MSE performance compared to GP-based methods on Health MNIST, Rotating MNIST, and Physionet Challenge 2012 datasets
- The model shows particular strength in imputation and future prediction tasks, especially when relevant auxiliary covariates are included
- LMM-VAE's interpretability and identifiability properties are maintained while achieving performance comparable to more complex models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LMM-VAE leverages linear mixed models to incorporate auxiliary covariates into the prior, enabling scalable modeling of high-dimensional longitudinal data
- Mechanism: The model uses linear mixed models (LMMs) to structure the latent space by modeling both shared and random effects of auxiliary covariates. This allows the model to capture complex dependencies in longitudinal data while maintaining scalability.
- Core assumption: Auxiliary covariates are sufficiently informative to characterize the data generation process.
- Evidence anchors:
  - [abstract] "leverage linear mixed models (LMMs) and amortized variational inference to provide conditional priors for VAEs"
  - [section] "LMM-VAE, a scalable, interpretable and identifiable model"
  - [corpus] No direct evidence in corpus papers, but related to "Uncovering Population PK Covariates from VAE-Generated Latent Spaces"
- Break condition: If auxiliary covariates are not sufficiently informative, the model's performance will degrade.

### Mechanism 2
- Claim: LMM-VAE can be viewed as a reduced-rank approximation of Gaussian process (GP) prior VAEs, providing theoretical connections and practical advantages
- Mechanism: By using basis functions in LMMs, the model approximates the spectral domain representation of GPs, achieving similar expressiveness with reduced computational complexity
- Core assumption: The spectral domain representation of GPs can be effectively approximated using a finite number of basis functions
- Evidence anchors:
  - [section] "LMM-VAEs can be viewed as a reduced-rank approximation method for GP prior VAEs"
  - [section] "LMM-VAE simplifies the overall training procedure... via the spectral representation and global parameterization"
  - [corpus] No direct evidence in corpus papers, but related to "Neighbour-Driven Gaussian Process Variational Autoencoders for Scalable Structured Latent Modelling"
- Break condition: If the basis functions are not chosen appropriately, the approximation may lose important characteristics of the GP prior

### Mechanism 3
- Claim: LMM-VAE's design ensures identifiability, allowing for meaningful interpretation of model parameters and data generation mechanisms
- Mechanism: The model belongs to the family of conditional VAEs, which constrains the latent space via a conditional prior, guaranteeing identifiability up to rotations and translations
- Core assumption: The model satisfies the conditions for identifiability as defined in Khemakhem et al. (2020)
- Evidence anchors:
  - [section] "LMM-VAE's design as a conditional latent variable model guarantees it the property of identifiability"
  - [section] "our oLMM-VAE as well as our sLMM-VAE variation fulfill the constraints"
  - [corpus] No direct evidence in corpus papers, but related to "Uncovering Population PK Covariates from VAE-Generated Latent Spaces"
- Break condition: If the model does not satisfy the identifiability conditions, the parameter estimation may not be consistent

## Foundational Learning

- Concept: Linear Mixed Models (LMMs)
  - Why needed here: LMMs are used to model both shared and random effects of auxiliary covariates in the latent space
  - Quick check question: How do LMMs differ from standard linear models in handling longitudinal data?

- Concept: Variational Autoencoders (VAEs)
  - Why needed here: VAEs are extended to incorporate structured priors using LMMs, enabling modeling of longitudinal data
  - Quick check question: What is the role of the encoder and decoder in a VAE, and how do they relate to the LMM-VAE model?

- Concept: Gaussian Processes (GPs)
  - Why needed here: LMM-VAE is theoretically connected to GP prior VAEs, providing a reduced-rank approximation method
  - Quick check question: How does the spectral domain representation of GPs relate to the basis functions used in LMM-VAE?

## Architecture Onboarding

- Component map:
  Encoder -> Linear Mixed Model -> Decoder
  (observation to variational parameters) -> (structured latent space) -> (generated observations)

- Critical path:
  1. Encode observation to obtain variational parameters
  2. Compute latent space using LMM with auxiliary covariates
  3. Decode latent space to generate observation
  4. Calculate ELBO for optimization

- Design tradeoffs:
  - Complexity vs. interpretability: LMM-VAE balances model complexity with interpretability by using LMMs
  - Scalability vs. expressiveness: The reduced-rank approximation of GP priors allows for scalable modeling while maintaining expressiveness

- Failure signatures:
  - Poor imputation or prediction performance: May indicate insufficient informative covariates or inadequate basis functions
  - Identifiability issues: Could result from model violations of identifiability conditions

- First 3 experiments:
  1. Compare LMM-VAE with and without basis functions on a simple longitudinal dataset to assess the impact of expressiveness
  2. Evaluate LMM-VAE's performance on a dataset with known covariate effects to test interpretability
  3. Test LMM-VAE's scalability by increasing the number of auxiliary covariates and observations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the number of basis functions in LMM-VAE affect its performance on datasets with varying levels of non-linearity?
- Basis in paper: [explicit] The paper mentions that the performance of LMM-VAE improves with the addition of basis functions, particularly on the Rotating MNIST dataset. However, the impact of different numbers of basis functions on datasets with varying levels of non-linearity is not explicitly explored.
- Why unresolved: The paper focuses on a specific dataset (Rotating MNIST) and does not provide a comprehensive analysis of how the number of basis functions impacts performance across different types of datasets with varying levels of non-linearity.
- What evidence would resolve it: Conducting experiments on multiple datasets with varying degrees of non-linearity and systematically varying the number of basis functions to observe the impact on model performance.

### Open Question 2
- Question: How does the choice of prior distribution for the parameters A in LMM-VAE affect the model's identifiability and performance?
- Basis in paper: [explicit] The paper assumes Gaussian priors for the parameters A but does not explore the impact of different prior distributions on identifiability and performance.
- Why unresolved: The paper assumes Gaussian priors without investigating how alternative prior distributions might affect the model's ability to learn meaningful representations and its overall performance.
- What evidence would resolve it: Experimenting with different prior distributions (e.g., Laplace, Cauchy) for the parameters A and evaluating their impact on identifiability and model performance across various datasets.

### Open Question 3
- Question: Can LMM-VAE be effectively extended to handle high-dimensional longitudinal data with a large number of time points?
- Basis in paper: [inferred] The paper focuses on high-dimensional data but does not explicitly address the scalability of LMM-VAE to datasets with a large number of time points.
- Why unresolved: While the paper demonstrates the effectiveness of LMM-VAE on datasets with a moderate number of time points, its performance on datasets with a significantly larger number of time points remains unexplored.
- What evidence would resolve it: Applying LMM-VAE to datasets with a large number of time points and evaluating its performance in terms of computational efficiency and predictive accuracy.

## Limitations

- The model's performance heavily depends on the quality and informativeness of auxiliary covariates, with limited guidance on covariate selection
- The theoretical connection to GP prior VAEs is primarily mathematical, with limited empirical validation across diverse datasets
- The paper focuses on GP-based baselines without comparing against other state-of-the-art methods for high-dimensional longitudinal data

## Confidence

- Theoretical framework and identifiability claims: **High** - well-supported by mathematical proofs and established literature
- Scalability and computational efficiency: **Medium** - demonstrated on specific datasets but needs broader validation
- Practical performance on diverse datasets: **Medium** - competitive results on three datasets, but limited generalizability testing

## Next Checks

1. Conduct systematic ablation studies varying the number and type of auxiliary covariates to quantify their impact on model performance
2. Test the model's performance on datasets with varying temporal resolutions and missing data patterns beyond those presented
3. Compare LMM-VAE against non-GP-based methods for high-dimensional longitudinal data to establish relative performance boundaries