---
ver: rpa2
title: Methods of improving LLM training stability
arxiv_id: '2410.16682'
source_url: https://arxiv.org/abs/2410.16682
tags:
- layer
- norm
- training
- layers
- normalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work analyzes transformer training stability by examining
  the magnitude of linear layer outputs during model divergence. The authors found
  that QKV, Proj, and FC2 layers exhibit the largest growth in output magnitude when
  learning rates are too high.
---

# Methods of improving LLM training stability

## Quick Facts
- arXiv ID: 2410.16682
- Source URL: https://arxiv.org/abs/2410.16682
- Authors: Oleg Rybakov; Mike Chrzanowski; Peter Dykas; Jinze Xue; Ben Lanir
- Reference count: 8
- Primary result: Layer normalization after QKV, Proj, and FC2 layers improves transformer training stability by controlling output magnitude growth during divergence

## Executive Summary
This work analyzes transformer training stability by examining the magnitude of linear layer outputs during model divergence. The authors found that QKV, Proj, and FC2 layers exhibit the largest growth in output magnitude when learning rates are too high. Based on this analysis, they propose several methods to improve training stability: applying layer normalization after QKV, Proj, and FC2 layers; applying layer normalization only after QKV; and combining QK layer normalization with softmax capping. The results show that the last two methods enable a 1.5x increase in learning rate without model divergence compared to QK layer normalization alone. Additionally, all three methods demonstrated significant perplexity improvements over the baseline model.

## Method Summary
The authors analyze transformer training stability by measuring L2 norms of linear layer outputs during divergence. They propose three stabilization methods: (1) applying layer normalization after QKV layers, (2) combining QK layer normalization with softmax capping, and (3) applying layer normalization after QKV, Proj, and FC2 layers. These methods are tested on an 830M parameter language model trained on a multilingual dataset. The experiments measure maximum stable learning rates and perplexity improvements compared to a baseline with layer normalization after QK layers.

## Key Results
- QKV norm, QK norm cap, and QK FC norm methods all enabled 1.5x higher stable learning rates compared to baseline
- All three methods showed significant perplexity improvements: QKV norm achieved 10.85 vs baseline 11.19
- QK norm cap provided complementary stability benefits when combined with softmax capping
- Layer normalization after QKV layers alone was sufficient to control the primary source of instability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Applying layer normalization after QKV layers (QKV norm) prevents output magnitude explosion by normalizing activations before they propagate further in the transformer block.
- Mechanism: The QKV layer produces attention query, key, and value matrices whose L2 norms can grow significantly during divergence. Adding layer normalization after QKV rescales these activations to unit variance, preventing the softmax from collapsing into near-one-hot encodings that destabilize gradients.
- Core assumption: The primary source of instability is the magnitude growth in QKV layer outputs, and normalizing after this layer is sufficient to control it.
- Evidence anchors:
  - [abstract] "Specifically we observe that QKV, Proj and FC2 layers have the largest growth of the output magnitude"
  - [section 3] "In Table 1 we show that the QKV layer has much higher magnitude output in divergent model in comparison to the converging one"
  - [corpus] Weak/no direct corpus evidence for QKV-specific normalization benefits

### Mechanism 2
- Claim: Combining QK layer normalization with softmax capping (QK norm cap) provides complementary stability by addressing logits growth at two different stages.
- Mechanism: QK layer normalization controls the magnitude of queries and keys before dot-product attention, while softmax capping limits the softmax output's sensitivity to large logits. This dual approach prevents both the initial magnitude explosion and the subsequent softmax collapse.
- Core assumption: QK layer normalization and softmax capping address different aspects of the instability problem and their combination is more effective than either alone.
- Evidence anchors:
  - [abstract] "apply QK layer normalization together with softmax capping"
  - [section 4.7] "combination of QK layer normalization with softmax capping can compliment each other and further improve model stability"
  - [corpus] No corpus evidence found for combined QK norm + softmax capping approaches

### Mechanism 3
- Claim: Applying layer normalization after all linear layers (QKV, Proj, FC2) would control magnitude growth throughout the transformer block.
- Mechanism: By normalizing the output of each linear layer, we prevent cumulative magnitude growth as activations propagate through the network. This would address the observation that multiple layers show increased L2 norms during divergence.
- Core assumption: The magnitude growth is distributed across multiple linear layers and requires normalization after each to maintain stability.
- Evidence anchors:
  - [abstract] "apply layer normalization not only after QK layers... but after Proj and FC2 layers too"
  - [section 3] "Specifically we observe that QKV, Proj and FC2 layers have the largest growth of the output magnitude"
  - [corpus] No corpus evidence found for comprehensive layer normalization approaches

## Foundational Learning

- Concept: Layer normalization mechanics and its placement in transformer blocks
  - Why needed here: Understanding when and where to apply normalization is crucial for implementing the proposed stability methods correctly
  - Quick check question: What is the difference between pre-normalization (before linear layer) and post-normalization (after linear layer) in terms of gradient flow?

- Concept: Attention mechanism and softmax behavior
  - Why needed here: The instability originates from attention logits magnitude affecting softmax output, so understanding this relationship is essential
  - Quick check question: How does the magnitude of input logits affect the entropy of softmax output?

- Concept: Learning rate scheduling and its impact on training stability
  - Why needed here: The paper experiments with different learning rates to induce divergence, so understanding this relationship helps interpret results
  - Quick check question: Why does increasing learning rate typically increase the risk of model divergence?

## Architecture Onboarding

- Component map:
  - Transformer block with QKV, Proj, FC1, FC2 linear layers
  - Layer normalization placement options (before/after each layer)
  - Softmax function with temperature/capping options
  - Attention mechanism with dot-product computation

- Critical path: QKV layer output → layer normalization (if applied) → attention computation → softmax → gradient propagation

- Design tradeoffs:
  - QKV norm vs QK norm: Fewer normalization layers but potentially sufficient vs more comprehensive normalization
  - Combined approaches: Potentially better stability but added complexity
  - Placement of normalization: Pre-normalization vs post-normalization affects gradient flow differently

- Failure signatures:
  - Divergence indicated by increasing validation loss
  - L2 norm growth in linear layer outputs (especially QKV, Proj, FC2)
  - Softmax outputs becoming near-one-hot encodings
  - Gradient explosion in attention layers

- First 3 experiments:
  1. Implement QKV norm variant and verify it converges at 1.5x higher learning rate than baseline
  2. Test QK norm cap combination and measure learning rate improvement over QK norm alone
  3. Compare perplexity of QKV norm, QK norm cap, and baseline models at standard learning rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does applying layer normalization after all linear layers (QKV, Proj, FC1, and FC2) provide better training stability compared to applying it only after QK layers?
- Basis in paper: [inferred] The paper found that QKV, Proj, and FC2 layers exhibit the largest growth in output magnitude during model divergence. However, applying layer normalization after all these layers (QK FC norm) did not improve model stability compared to QK norm alone.
- Why unresolved: The paper did not explore the effect of applying layer normalization after FC1 layer, which also shows growth in output magnitude during divergence.
- What evidence would resolve it: Experiment with applying layer normalization after all linear layers (QKV, Proj, FC1, and FC2) and compare its performance with QK norm and QK FC norm methods.

### Open Question 2
- Question: How do the proposed methods (QK norm cap, QKV norm) perform on larger language models (e.g., billions of parameters) compared to smaller models (830M parameters)?
- Basis in paper: [explicit] The authors state that "The methods presented in this work improved model training stability on small language models (830M parameters)" and suggest testing these approaches on much larger models.
- Why unresolved: The experiments were conducted only on a small language model with 830M parameters. The effectiveness of the methods on larger models is unknown.
- What evidence would resolve it: Conduct experiments on larger language models (e.g., billions of parameters) using the proposed methods and compare their performance with the baseline methods.

### Open Question 3
- Question: How does the combination of QK layer normalization with other techniques (e.g., weight decay, gradient clipping) affect training stability and perplexity?
- Basis in paper: [inferred] The paper explored combining QK layer normalization with softmax capping (QK norm cap) and found it improved training stability. However, it did not investigate combining QK layer normalization with other techniques mentioned in the literature, such as weight decay or gradient clipping.
- Why unresolved: The paper focused on combining QK layer normalization with softmax capping and did not explore other potential combinations with existing techniques.
- What evidence would resolve it: Experiment with combining QK layer normalization with other techniques (e.g., weight decay, gradient clipping) and compare their performance with the proposed methods and baseline methods.

## Limitations
- Generalization scope: Methods tested only on 830M parameter model with specific architecture and dataset composition
- Implementation details: Limited explanation of softmax capping mechanism and integration with normalization
- Causality: Unclear whether magnitude growth is symptom or cause of divergence

## Confidence
- High confidence: Observation of L2 norm growth in specific linear layers during divergence; validation of QKV normalization effectiveness through controlled experiments
- Medium confidence: Reported perplexity improvements (11.19 to 10.85) and 1.5x learning rate increases; exact mechanisms beyond magnitude control not fully explained
- Low confidence: Claims about synergistic benefits of QK norm cap combination; generalization of learning rate improvements to different architectures

## Next Checks
1. **Cross-architecture validation**: Test the proposed stabilization methods on different transformer architectures (encoder-only, decoder-only, and encoder-decoder models) and scales (from 100M to 10B parameters) to assess generalizability. Measure both maximum stable learning rate and final model quality metrics across architectures.
2. **Ablation study with controlled magnitude growth**: Implement a synthetic experiment where linear layer output magnitudes are artificially controlled (clamped) during training. Compare training stability and final model quality between clamped and normalized approaches to determine whether magnitude control is the primary mechanism of stability improvement.
3. **Long-duration training analysis**: Train models with the proposed methods for extended periods (2-3x longer than reported) and monitor for delayed instability or degradation that might not appear in shorter training runs. Track attention entropy, gradient norms, and layer output statistics throughout training to identify early warning signs of potential issues.