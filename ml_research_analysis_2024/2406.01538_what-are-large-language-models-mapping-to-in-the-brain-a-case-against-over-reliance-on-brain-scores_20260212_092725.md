---
ver: rpa2
title: What Are Large Language Models Mapping to in the Brain? A Case Against Over-Reliance
  on Brain Scores
arxiv_id: '2406.01538'
source_url: https://arxiv.org/abs/2406.01538
tags:
- gpt2-xl
- neural
- each
- word
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Shuffled train-test splits in brain encoding studies can inflate
  model performance by allowing non-linguistic features like temporal autocorrelation
  to dominate predictions. When properly controlled for sentence length, position,
  and basic static word embeddings, large language models (LLMs) show little to no
  additional predictive power for fMRI data during passage reading.
---

# What Are Large Language Models Mapping to in the Brain? A Case Against Over-Reliance on Brain Scores

## Quick Facts
- arXiv ID: 2406.01538
- Source URL: https://arxiv.org/abs/2406.01538
- Authors: Ebrahim Feghhi; Nima Hadidi; Bryan Song; Idan A. Blank; Jonathan C. Kao
- Reference count: 40
- Key outcome: Shuffled train-test splits in brain encoding studies can inflate model performance by allowing non-linguistic features like temporal autocorrelation to dominate predictions.

## Executive Summary
This study challenges the interpretation of high brain scores for large language models (LLMs) as evidence of computational similarity to the brain. Using fMRI, ECoG, and fMRI datasets from Pereira, Fedorenko, and Blank studies, the authors demonstrate that simple non-linguistic features—particularly sentence length, position, and static word embeddings—account for most of the neural variance explained by LLMs. Temporal autocorrelation in neural signals, when not properly controlled for via contiguous train-test splits, can inflate brain scores dramatically. The findings suggest that high brain scores alone are insufficient evidence of deep linguistic alignment between LLMs and brain activity.

## Method Summary
The study uses banded ridge regression with nested cross-validation to predict brain voxel/electrode activity from various linguistic feature spaces extracted from GPT-2 XL and RoBERTa-Large, as well as simpler baselines like sentence position, length, static word embeddings, and OASM (orthogonal autocorrelated sequences). Contiguous train-test splits are used to control for temporal autocorrelation, and voxel-wise correction identifies optimal feature subsets. Performance is measured by out-of-sample R², with statistical tests for significance across feature combinations.

## Key Results
- Shuffled train-test splits allow temporal autocorrelation to dominate predictions, inflating brain scores beyond true linguistic alignment.
- Untrained LLMs achieve high brain scores primarily due to encoding of sentence length and absolute position, not linguistic content.
- Trained LLM brain scores are largely explained by sentence length, position, and static word embeddings, with only modest additional gains from sense embeddings and syntactic representations.
- LLMs fail to predict brain activity during story listening beyond chance levels.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shuffled train-test splits allow temporal autocorrelation in fMRI/ECoG signals to dominate model predictions, inflating brain scores beyond true linguistic alignment.
- Mechanism: Temporal autocorrelation within passages/stories causes nearby brain states to be highly similar. Shuffled splits break this structure, so models that simply encode "being in this passage" can achieve high R² even without understanding language. OASM exploits this by constructing passage-specific orthogonal autocorrelated sequences, achieving R² close to or exceeding that of LLMs.
- Core assumption: Brain activity during story/passage comprehension contains substantial non-linguistic, temporally structured noise that is preserved within but not across passages when using contiguous splits.
- Evidence anchors:
  - [abstract]: "a trivial feature that encodes temporal autocorrelation not only outperforms LLMs but also accounts for the majority of neural variance that LLMs explain"
  - [section]: "when using shuffled splits, a trivial feature that encodes temporal autocorrelation not only outperforms LLMs but also accounts for the majority of neural variance that LLMs explain"
- Break condition: If brain activity is nearly instantaneous with stimuli (e.g., ECoG at word-level) or temporal structure is artificially decorrelated, shuffled splits may not inflate scores.

### Mechanism 2
- Claim: Untrained LLMs achieve high brain scores because they encode sentence length and absolute position, not linguistic content.
- Mechanism: In GPT2-XL, the GELU activation in early layers produces a positive mean output from zero-mean inputs, which accumulates when sum-pooling tokens to form sentence embeddings, encoding sentence length. Untrained absolute positional embeddings cause sentences at the same position across passages to have similar representations. These two features alone explain nearly all of the untrained model's predictive power.
- Core assumption: The untrained transformer's architecture introduces systematic, decodable structure (length, position) into token embeddings even without training on language.
- Evidence anchors:
  - [abstract]: "untrained LLMs have higher-than-expected brain scores ... they do not account for additional neural variance beyond two simple features: sentence length and sentence position"
  - [section]: "Sentence length is captured by GPT2-XLU because the GELU nonlinearity in the first layer's MLP transforms normally distributed inputs with zero mean into outputs with a positive mean. Sentence position is encoded within GPT2-XLU due to absolute positional embeddings..."
- Break condition: If positional embeddings are randomized per batch or if alternative pooling methods destroy the length/position signal.

### Mechanism 3
- Claim: Trained LLM brain scores are largely explained by non-contextual features (length, position, static word embeddings) plus modest contributions from sense disambiguation and syntactic structure.
- Mechanism: By building a baseline model with sentence length, position, and static word embeddings (with pronoun resolution), the majority of variance explained by a trained LLM is already captured. Adding sense-specific embeddings and syntactic representations yields only a small additional gain, suggesting that most of what LLMs map to is low-level, non-linguistic structure rather than deep contextual processing.
- Core assumption: Contextual linguistic processing in LLMs contributes little to brain score beyond what is already captured by simpler, non-contextual features.
- Evidence anchors:
  - [abstract]: "brain scores of trained LLMs on this dataset can largely be explained by sentence length, position, and static word embeddings; a small, additional amount is explained by sense embeddings and contextual representations of sentence structure"
  - [section]: "SP+SL+WORD accounted for the majority of neural variance explained by GPT2-XL... SENSE and SYNT account for an additional, modest portion of neural variance explained by GPT2-XL"
- Break condition: If the brain encodes rich contextual information that is not captured by these simple features, or if the experimental paradigm strongly emphasizes context (e.g., ambiguous pronoun resolution).

## Foundational Learning

- Concept: Ridge regression and banded ridge regression
  - Why needed here: Used to predict brain voxel/electrode activity from linguistic features; banded ridge allows separate regularization for feature spaces of different sizes.
  - Quick check question: Why might a standard ridge regression be biased against small feature spaces in this context?

- Concept: Temporal autocorrelation in neural signals
  - Why needed here: Understanding why shuffled splits inflate scores and how OASM exploits this structure.
  - Quick check question: What aspect of fMRI/ECoG data makes autocorrelation particularly problematic for shuffled train-test splits?

- Concept: Static vs contextual word embeddings
  - Why needed here: To understand what features are being compared and how contextual processing in LLMs might differ from static representations.
  - Quick check question: How do static word embeddings differ from the contextual embeddings produced by a transformer?

## Architecture Onboarding

- Component map:
  Data loaders for Pereira (fMRI), Fedorenko (ECoG), and Blank (fMRI) datasets -> Language model feature extractors (GPT2-XL, RoBERTa-Large, static embeddings, OASM, etc.) -> Banded ridge regression module with nested cross-validation -> Evaluation pipeline computing R², Ω, Φ metrics -> Voxel-wise correction logic and FDR adjustment -> Glass brain plotting utilities

- Critical path:
  1. Load neural data and preprocess (z-score, select language network)
  2. For each feature space, extract representations for all stimuli
  3. Run banded ridge regression with nested CV to get voxel-level predictions
  4. Compute and compare R² for different feature combinations
  5. Perform voxel-wise correction to find best sub-model with/without LLM
  6. Visualize results (glass brains, 2D histograms, layer-wise R²)

- Design tradeoffs:
  - Sum-pooling vs last-token method for converting token embeddings to sentence/word embeddings
  - Contiguous vs shuffled train-test splits (accuracy vs computational simplicity)
  - Number of random search iterations for banded regression hyperparameters vs runtime
  - Inclusion of function words in syntactic representations (coherence vs structure purity)

- Failure signatures:
  - R² values negative or near zero across all voxels → likely issue with feature extraction or regression
  - OASM outperforming LLM on contiguous splits → temporal autocorrelation still leaking in
  - No voxel improves when adding LLM after voxel-wise correction → LLM features not capturing brain-relevant variance
  - High variance in R² across CV folds → insufficient data or overfitting

- First 3 experiments:
  1. Reproduce R² vs layer curves for GPT2-XL on Pereira using both sum-pooling and last-token methods; confirm intermediate layers perform best with contiguous splits.
  2. Implement OASM feature space and verify it outperforms LLM on shuffled splits for all three datasets; check that Φ values are large.
  3. Fit baseline model (SP+SL+WORD) on Pereira; confirm it explains most of LLM's variance and that Ω values are high.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific linguistic features beyond sentence length, position, and static word embeddings do LLMs capture that could explain their neural predictive power?
- Basis in paper: [explicit] The paper finds that sentence length, position, and static word embeddings account for most of the neural variance explained by LLMs, but some small additional variance is explained by sense embeddings and syntactic representations.
- Why unresolved: The paper does not delve into what other linguistic features might be captured by LLMs beyond these, and the exact nature of the additional variance explained by sense and syntactic embeddings is not explored in detail.
- What evidence would resolve it: Detailed analysis of other potential linguistic features (e.g., semantic roles, discourse structure) and their correlation with LLM performance on brain data.

### Open Question 2
- Question: How do LLMs' internal representations evolve during training to capture brain-like linguistic processing?
- Basis in paper: [explicit] The paper mentions that untrained LLMs have high brain scores, suggesting that the transformer architecture itself may bias computations to be more brain-like.
- Why unresolved: The paper does not investigate the specific changes in LLM representations during training that lead to improved brain score performance.
- What evidence would resolve it: Analysis of LLM representations at different stages of training to identify which aspects of the representations change to better align with brain activity.

### Open Question 3
- Question: Are there specific brain regions or networks where LLMs' predictions are more accurate, and what does this imply about the nature of the mapping?
- Basis in paper: [inferred] The paper focuses on the language network but mentions other networks (multiple demand, default mode, auditory, visual) in the context of Pereira data.
- Why unresolved: The paper does not provide a detailed analysis of how LLM predictions vary across different brain regions or networks.
- What evidence would resolve it: Comparative analysis of LLM brain score performance across different brain networks and regions to identify specific areas of better alignment.

## Limitations

- The findings assume that temporal autocorrelation within passages is a major source of inflated brain scores, but do not directly test fully decorrelated conditions.
- The deconstruction of LLM brain scores relies on the assumption that sentence length, position, and static embeddings are orthogonal and fully capture non-contextual variance.
- The failure of LLMs on story-listening tasks may reflect differences in cognitive processes rather than poor linguistic alignment.

## Confidence

- High: The OASM control provides strong evidence that temporal autocorrelation drives inflated brain scores with shuffled splits.
- Medium: The feature-space analysis convincingly shows that simple features explain most LLM brain scores, though potential interactions are not explored.
- Low: The negative inference about LLM-brain alignment for stories is weakened by alternative explanations involving task differences.

## Next Checks

1. Replicate the core findings using an alternative temporal control, such as phase-randomized surrogates of the neural signals, to verify that temporal autocorrelation drives inflated brain scores.
2. Test whether alternative non-contextual baselines (e.g., character n-grams, acoustic features) can explain additional variance in LLM brain scores beyond length, position, and static embeddings.
3. Evaluate LLM performance on story-listening data using models explicitly trained on narrative coherence (e.g., story-specific transformers) to determine if the null result generalizes or is task-specific.