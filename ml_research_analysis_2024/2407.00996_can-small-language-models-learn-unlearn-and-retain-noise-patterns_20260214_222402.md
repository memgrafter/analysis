---
ver: rpa2
title: Can Small Language Models Learn, Unlearn, and Retain Noise Patterns?
arxiv_id: '2407.00996'
source_url: https://arxiv.org/abs/2407.00996
tags:
- noise
- train
- training
- cflipped
- wflipped
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how small language models (SLMs) with 1-3
  billion parameters handle noise in training data. Four SLMs (Olmo 1B, Qwen1.5 1.8B,
  Gemma2B, Phi2 2.7B) were instruction-tuned on clean data and then tested with in-context
  learning to assess noise adaptation.
---

# Can Small Language Models Learn, Unlearn, and Retain Noise Patterns?

## Quick Facts
- **arXiv ID**: 2407.00996
- **Source URL**: https://arxiv.org/abs/2407.00996
- **Reference count**: 19
- **Key outcome**: Small language models (1-3B parameters) can effectively learn and unlearn noise patterns, with Phi2 excelling at word-level noise but failing at character-level noise due to tokenization disruption.

## Executive Summary
This study investigates how small language models (SLMs) with 1-3 billion parameters handle noise in training data. Four SLMs (Olmo 1B, Qwen1.5 1.8B, Gemma2B, Phi2 2.7B) were instruction-tuned on clean data and then tested with in-context learning to assess noise adaptation. Models were then trained with various noise patterns including word/character flips, irrelevant answers, and counterfactual information. Results showed Phi2 consistently excelled with word-level noise but failed completely with character-level noise due to its carefully curated pretraining data. Despite being smallest (1B parameters), Olmo performed consistently well across noise types. Subsequent training on clean data effectively mitigated noise effects. Models demonstrated ability to unlearn noise patterns when retrained on clean data, though character-level noise severely impacted performance. These findings provide practical strategies for developing robust SLMs for real-world applications where noise handling is critical.

## Method Summary
The study used four pretrained SLMs (Olmo 1B, Qwen1.5 1.8B, Gemma 2B, Phi2 2.7B) that were instruction-tuned on clean data using AdamW optimizer with bfloat16 precision, cosine learning rate schedule starting at 3e-6, weight decay of 0.1, and beta values of 0.9 and 0.95. Models were trained on noise-free, word-flipped, and character-flipped versions of the AlpaGasus and Dolly datasets, plus irrelevant and counterfactual response datasets. Performance was evaluated across multiple training phases with different noise combinations, followed by unlearning experiments where models were retrained on clean data to test noise pattern retention.

## Key Results
- Phi2 consistently excelled with word-level noise but completely failed with character-level noise due to tokenization disruption
- Despite being the smallest model (1B parameters), Olmo performed consistently well across all noise types
- Subsequent training on clean data effectively mitigated noise effects, demonstrating the models' ability to unlearn noise patterns
- Character-level noise severely impacted performance across all models due to fundamental changes in token boundaries

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Character-level noise severely degrades model performance due to tokenization disruption.
- **Mechanism**: Character-level flipping fundamentally alters token boundaries, forcing the tokenizer to create entirely different token sequences compared to word-level flipping. This disrupts the learned relationships between tokens and their positional context in self-attention.
- **Core assumption**: The model's ability to handle noise is heavily dependent on maintaining consistent token structures.
- **Evidence anchors**:
  - [section] "The tokenization process changes drastically with character-level flipped noise, affecting how tokens are combined to form words and sentences."
  - [corpus] Weak evidence - no direct citations on tokenization disruption from character noise in small models.
- **Break condition**: If a model uses a character-aware tokenizer or subword tokenization that is more robust to character permutations.

### Mechanism 2
- **Claim**: Models trained on carefully curated, high-quality data struggle more with learning noisy patterns.
- **Mechanism**: Phi2's pretraining on textbook-quality data created a model that is optimized for clean, structured information. This makes it less adaptable to noise patterns because it lacks exposure to noisy or less structured data during pretraining.
- **Core assumption**: Pretraining data quality and curation directly influence a model's adaptability to noise.
- **Evidence anchors**:
  - [section] "Phi's inability to learn character-level noise patterns can be attributed to the textbook quality data used in the model pertaining process."
  - [corpus] Weak evidence - no direct citations linking pretraining data quality to noise adaptability in small models.
- **Break condition**: If the model undergoes additional training on diverse, noisy data or uses data augmentation techniques during pretraining.

### Mechanism 3
- **Claim**: Unlearning is effective when noise-free data is provided in subsequent training phases.
- **Mechanism**: When models are retrained on clean data after exposure to noise, they can overwrite the noisy patterns learned earlier. This is possible because the model can adapt to the new patterns in the clean data, effectively "forgetting" the noise.
- **Core assumption**: The model's capacity for catastrophic forgetting works in favor of unlearning when clean data is provided.
- **Evidence anchors**:
  - [section] "The models demonstrated the ability to unlearn noise patterns when subsequently trained on noise-free datasets."
  - [corpus] Weak evidence - no direct citations on unlearning mechanisms in small language models.
- **Break condition**: If the model has limited capacity or if the noise patterns are deeply embedded in the model's weights.

## Foundational Learning

- **Concept**: Tokenization and its impact on model performance
  - Why needed here: Understanding how different types of noise affect tokenization is crucial for interpreting the model's performance on noisy data.
  - Quick check question: How does character-level flipping differ from word-level flipping in terms of tokenization?

- **Concept**: Catastrophic forgetting in neural networks
  - Why needed here: This concept explains why models can unlearn noise patterns when retrained on clean data.
  - Quick check question: What is catastrophic forgetting, and how can it be leveraged for unlearning?

- **Concept**: Self-attention mechanism and its role in processing sequential data
  - Why needed here: Self-attention is a core component of transformer models, and understanding its function is essential for grasping how models handle different noise patterns.
  - Quick check question: How does self-attention contribute to a model's ability to process and learn from sequential data?

## Architecture Onboarding

- **Component map**: Input text → Tokenizer → Embedding layer → Transformer layers → Output layer
- **Critical path**: Input text → Tokenizer → Embedding layer → Transformer layers → Output layer
- **Design tradeoffs**: Smaller models (like Olmo 1B) are more adaptable to noise but less accurate overall; larger models (like Phi2 2.7B) are more accurate but less adaptable to noise.
- **Failure signatures**: Complete inability to generate correct responses with character-level noise; grammatical correctness remains high even with incorrect answers.
- **First 3 experiments**:
  1. Test model performance on word-level noise vs. character-level noise to confirm tokenization disruption.
  2. Retrain a model on clean data after noise exposure to verify unlearning capability.
  3. Compare performance of models with different pretraining data qualities on noise adaptation.

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis of character-level noise degradation relies on theoretical assumptions about tokenization without empirical validation of the tokenization process itself.
- The claim about pretraining data quality influencing noise adaptability is based on a single model comparison without broader testing across models with varying pretraining data characteristics.
- The unlearning mechanism explanation depends on catastrophic forgetting concepts without measuring actual weight changes or using established forgetting metrics.

## Confidence
- High confidence: Core findings about noise learning and unlearning capabilities are supported by direct experimental results across multiple models and noise types.
- Medium confidence: Claims about character-level noise specifically degrading performance due to tokenization disruption.
- Low confidence: Mechanistic explanations of why specific noise types affect models differently, particularly the tokenization disruption hypothesis and pretraining data quality claims.

## Next Checks
1. Conduct ablation studies specifically testing tokenization effects by comparing model performance on character-level noise using different tokenization strategies (subword vs character-level tokenizers).
2. Systematically vary pretraining data quality across model versions to directly test whether cleaner pretraining data correlates with worse noise adaptation capabilities.
3. Implement forgetting curve analysis using established metrics like Learning Without Forgetting (LWF) to quantify exactly how much knowledge is retained versus forgotten during unlearning phases.