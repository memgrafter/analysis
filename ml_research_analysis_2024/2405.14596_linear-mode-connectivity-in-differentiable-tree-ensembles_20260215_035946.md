---
ver: rpa2
title: Linear Mode Connectivity in Differentiable Tree Ensembles
arxiv_id: '2405.14596'
source_url: https://arxiv.org/abs/2405.14596
tags:
- interpolation
- tree
- trees
- accuracy
- perm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether Linear Mode Connectivity (LMC)
  - a property where linearly interpolated models maintain consistent performance
  - can be achieved for soft tree ensembles, a class of differentiable tree-based
  models. The authors identify that achieving LMC for tree ensembles requires accounting
  for two additional invariances beyond tree permutation invariance: subtree flip
  invariance and splitting order invariance.'
---

# Linear Mode Connectivity in Differentiable Tree Ensembles

## Quick Facts
- arXiv ID: 2405.14596
- Source URL: https://arxiv.org/abs/2405.14596
- Reference count: 40
- One-line primary result: Linear Mode Connectivity (LMC) can be achieved for soft tree ensembles by incorporating subtree flip invariance and splitting order invariance beyond tree permutation invariance.

## Executive Summary
This paper investigates whether Linear Mode Connectivity (LMC) - a property where linearly interpolated models maintain consistent performance - can be achieved for soft tree ensembles, a class of differentiable tree-based models. The authors identify that achieving LMC for tree ensembles requires accounting for two additional invariances beyond tree permutation invariance: subtree flip invariance and splitting order invariance. They show that by incorporating these invariances, LMC can be achieved for soft oblivious trees. Additionally, the authors demonstrate that it is possible to exclude these additional invariances while preserving LMC by designing decision list-based tree architectures. The proposed method significantly reduces barriers in model merging operations, with barriers approaching zero when all relevant invariances are considered.

## Method Summary
The paper proposes achieving LMC for soft tree ensembles by incorporating three types of invariances: tree permutation, subtree flip, and splitting order. The method involves training two independently initialized soft tree ensemble models, then using a matching strategy (either activation matching or weight matching) that optimizes permutations considering the relevant invariances. For soft oblivious trees, the matching strategy accounts for all three invariances through linear assignment problem (LAP) optimization. For decision list architectures, the method eliminates subtree flip and splitting order invariances by design, requiring only permutation invariance consideration. The models are interpolated between λ=0 and λ=1, and the accuracy barrier is measured to evaluate LMC achievement.

## Key Results
- Soft tree ensembles achieve LMC by incorporating subtree flip invariance and splitting order invariance in addition to tree permutation invariance
- Decision list-based architectures can achieve LMC by considering only tree permutation invariance, eliminating the need for additional invariances
- Performance barriers approach zero when all relevant invariances are considered for soft oblivious trees
- Deep trees show increased barriers even with invariances, suggesting they are less important for model merging considerations

## Why This Works (Mechanism)

### Mechanism 1
Linear Mode Connectivity (LMC) can be achieved for soft tree ensembles by incorporating additional invariances beyond tree permutation. By accounting for subtree flip invariance and splitting order invariance, the authors reduce the barrier between two independently trained tree ensemble models. These additional invariances correspond to structural properties of binary trees that preserve functional equivalence when parameters are transformed. The core assumption is that these additional invariances are valid and necessary for achieving LMC in tree ensembles, and the linear interpolation of transformed parameters preserves model performance.

### Mechanism 2
It is possible to exclude the additional invariances while preserving LMC by modifying the tree architecture to a decision list. By designing a decision list-based tree architecture where branches extend in only one direction and designating one terminal leaf as an empty node, the authors eliminate both subtree flip invariance and splitting order invariance. This allows achieving LMC by considering only tree permutation invariance, which is computationally less expensive. The core assumption is that the modified decision list architecture does not introduce the additional invariances, and considering only tree permutation invariance is sufficient to achieve LMC for this architecture.

### Mechanism 3
The presence of additional invariances varies depending on the tree structure, and deep perfect binary trees are less important for model merging considerations. The authors observe that the barrier tends to increase as trees become deeper, even when considering invariances. This suggests that deep models are fundamentally less important for model merging considerations. Additionally, deep perfect binary trees are rarely used in practical scenarios due to the degeneracy of the Neural Tangent Kernel (NTK) with increasing depth.

## Foundational Learning

- Concept: Linear Mode Connectivity (LMC)
  - Why needed here: LMC is the core phenomenon that the paper investigates and aims to achieve for soft tree ensembles. Understanding LMC is essential for grasping the significance of the paper's contributions.
  - Quick check question: What is the definition of Linear Mode Connectivity, and why is it considered important in the context of modern machine learning models?

- Concept: Invariance in model parameters
  - Why needed here: The paper relies on the concept of invariance in model parameters to achieve LMC for soft tree ensembles. Understanding the different types of invariances (e.g., tree permutation, subtree flip, splitting order) is crucial for following the paper's arguments and mechanisms.
  - Quick check question: What are the three types of invariances identified in the paper for soft tree ensembles, and how do they differ from the permutation invariance commonly considered in neural networks?

- Concept: Soft tree ensembles
  - Why needed here: Soft tree ensembles are the specific type of model architecture that the paper focuses on. Understanding the key characteristics and properties of soft tree ensembles (e.g., differentiable formulation, parameter sharing in oblivious trees) is necessary for comprehending the paper's contributions and experimental results.
  - Quick check question: What are the main differences between soft tree ensembles and traditional hard decision trees, and how does the differentiable formulation of soft trees enable the application of gradient-based optimization techniques?

## Architecture Onboarding

- Component map: Soft tree ensemble model -> Parameters (w, b, π) -> Invariances (permutation, subtree flip, splitting order) -> Matching strategy (AM/WM) -> Aligned parameters -> Interpolation -> Performance barrier evaluation

- Critical path:
  1. Define the soft tree ensemble architecture and parameters
  2. Identify the relevant invariances for the specific tree structure
  3. Implement the matching strategy (AM or WM) considering the identified invariances
  4. Train two independently initialized models on the same dataset
  5. Apply the matching strategy to align the parameters of the two models
  6. Interpolate between the aligned parameters and evaluate the performance barrier

- Design tradeoffs:
  - Considering additional invariances (subtree flip, splitting order) vs. computational complexity: Including more invariances can lead to better LMC but increases the computational cost of the matching strategy.
  - Deep vs. shallow trees: Deep trees may have more expressive power but can suffer from increased barriers and NTK degeneracy, making them less suitable for model merging.
  - Oblivious vs. non-oblivious trees: Oblivious trees have parameter sharing, which reduces the number of parameters but may limit the flexibility of the model.
  - Decision lists vs. perfect binary trees: Decision lists can achieve LMC with less computational overhead but may have larger barriers compared to perfect binary trees with additional invariances.

- Failure signatures:
  - High performance barrier despite considering invariances: Indicates that the identified invariances are insufficient or incorrect for the specific tree architecture used.
  - Degradation in interpolation performance: Suggests that the matching strategy is not properly aligning the parameters or that the interpolation is not preserving the functional equivalence of the models.
  - Instability or divergence during training: May indicate issues with the soft tree ensemble architecture, optimization process, or data preprocessing.

- First 3 experiments:
  1. Implement a basic soft tree ensemble with M=256 trees, D=2 levels, and evaluate the performance barrier using only tree permutation invariance.
  2. Modify the tree architecture to include subtree flip invariance and re-evaluate the performance barrier using the updated matching strategy.
  3. Further modify the tree architecture to include splitting order invariance (for oblivious trees) and re-evaluate the performance barrier using the complete matching strategy considering all three invariances.

## Open Questions the Paper Calls Out

- What is the practical impact of subtree flip invariance and splitting order invariance on real-world model merging tasks beyond the observed performance improvements? The paper demonstrates improved interpolation performance but doesn't explore broader practical implications for real-world model merging applications like task arithmetic or continual learning.

- How does the depth of oblivious trees affect the computational feasibility of considering all possible subtree flip and splitting order invariances in practice? The paper acknowledges the computational challenge but doesn't provide concrete solutions or quantify the trade-off between depth, computational cost, and performance benefits.

- How do the proposed decision list-based architectures compare to other tree architectures in terms of accuracy, interpretability, and efficiency for different types of tabular datasets? The paper presents some empirical results but doesn't provide a comprehensive comparison across diverse tabular datasets with varying characteristics.

## Limitations
- The computational complexity of considering all three invariances increases significantly with tree depth, making the approach less practical for deep trees.
- Experimental results focus exclusively on soft oblivious trees and decision lists, leaving open questions about applicability to other tree architectures.
- The claim that deep trees are "less important" for model merging lacks empirical validation beyond the observation that barriers increase with depth.

## Confidence
- Theoretical identification of necessary invariances and their impact on LMC: **High**
- Experimental validation across diverse datasets and architectures: **Medium**
- Practical implications for deep trees and real-world model merging: **Low**

## Next Checks
1. Reproduce the accuracy barrier results on at least 3-5 datasets from the 16 mentioned, focusing on comparing barriers with and without considering subtree flip and splitting order invariances for soft oblivious trees.

2. Test the decision list architecture by implementing the modified decision list with empty node designation and verify that LMC can be achieved with only permutation invariance, comparing barriers to those of soft oblivious trees with full invariance consideration.

3. Experiment with deep trees (D > 4) to empirically validate whether barriers consistently increase with depth and whether the computational complexity of invariance matching becomes prohibitive in practice.