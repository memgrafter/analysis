---
ver: rpa2
title: You're (Not) My Type -- Can LLMs Generate Feedback of Specific Types for Introductory
  Programming Tasks?
arxiv_id: '2412.03516'
source_url: https://arxiv.org/abs/2412.03516
tags:
- circle
- feedback
- programming
- types
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated whether Large Language Models (LLMs) could
  generate specific types of formative feedback for introductory programming tasks.
  The researchers iteratively designed prompts to elicit particular feedback types
  (e.g., feedback on mistakes, task constraints, or concepts) from GPT-4 in response
  to authentic student code submissions.
---

# You're (Not) My Type -- Can LLMs Generate Feedback of Specific Types for Introductory Programming Tasks?

## Quick Facts
- arXiv ID: 2412.03516
- Source URL: https://arxiv.org/abs/2412.03516
- Reference count: 40
- Primary result: GPT-4 successfully generated desired feedback types in 63 out of 66 cases, demonstrating potential for varied programming feedback while highlighting accuracy concerns.

## Executive Summary
This study investigates whether Large Language Models (LLMs) can generate specific types of formative feedback for introductory programming tasks. The researchers iteratively designed prompts to elicit particular feedback types (e.g., feedback on mistakes, task constraints, or concepts) from GPT-4 in response to authentic student code submissions. They evaluated the generated feedback for type accuracy and characteristics like personalization, completeness, and potential misleading information. Results showed that GPT-4 successfully generated the desired feedback type in 63 out of 66 cases. While the feedback was often personalized and addressed real errors, some instances included misleading suggestions, especially for correct solutions. The study demonstrates the potential of LLMs for providing richer, more varied feedback in programming education, while also highlighting the need for careful prompt design and awareness of potential inaccuracies.

## Method Summary
The researchers used an iterative prompt design process to generate specific feedback types for introductory programming tasks using GPT-4. They started with authentic student code submissions and refined their prompts over five iterations based on test outputs. The final prompt included detailed context (task description, student code, model solution, template) and explicit instructions for each feedback type. They analyzed 66 cases across six programming exercises, evaluating feedback for type accuracy and characteristics like personalization, completeness, and potential misleading information.

## Key Results
- GPT-4 successfully generated the desired feedback type in 63 out of 66 cases (95.5% accuracy)
- Feedback was personalized and addressed real errors in student submissions
- Some instances included misleading suggestions, particularly for correct solutions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative prompt refinement improves LLM feedback accuracy.
- Mechanism: The researchers used a five-iteration prompt design process, testing outputs and adjusting instructions based on observed shortcomings. This iterative cycle allowed them to address issues like excessive length, irrelevant model solution comparisons, and inclusion of unintended feedback types.
- Core assumption: LLMs can respond to fine-grained prompt modifications and that prompt engineers can identify problematic outputs.
- Evidence anchors:
  - [section] "To refine the prompt, we followed the steps below..." describes the iterative evaluation and adjustment process.
  - [section] "In our improvements, we simplified the definition supplied with KR..." shows how specific issues led to prompt changes.
- Break condition: If the LLM fails to produce consistent responses to small prompt changes, or if the evaluators cannot reliably detect issues, the iterative approach becomes ineffective.

### Mechanism 2
- Claim: Task-specific feedback generation outperforms generic prompts.
- Mechanism: The prompt includes detailed context (task description, student code, model solution, template) and explicit instructions for each feedback type. This targeted input enables the LLM to generate feedback aligned with the specific exercise and student submission.
- Core assumption: Providing comprehensive context allows the LLM to better understand the task constraints and student errors.
- Evidence anchors:
  - [abstract] "We iteratively designed prompts to elicit particular feedback types (e.g., feedback on mistakes, task constraints, or concepts) from GPT-4 in response to authentic student code submissions."
  - [section] "We included the following elements: A system prompt with instructions about the target audience, the elements given below, and an explanation of the different feedback types."
- Break condition: If the LLM consistently ignores or misinterprets the provided context, or if the context becomes too complex to process effectively, the task-specific approach may not yield better results.

### Mechanism 3
- Claim: Using established feedback taxonomies improves feedback quality and relevance.
- Mechanism: The researchers applied Keuning et al.'s feedback taxonomy to categorize and evaluate the generated feedback. This structured approach ensures that the feedback addresses various aspects of the student's work (mistakes, concepts, task constraints, etc.).
- Core assumption: Feedback taxonomies developed for educational contexts are applicable and beneficial when used with LLM-generated feedback.
- Evidence anchors:
  - [abstract] "We revisit existing feedback taxonomies to capture the specifics of the generated feedback, such as randomness, uncertainty, and degrees of variation."
  - [section] "In this work, we explore to what extent the feedback generated by LLMs can be controlled and how its quality can be assessed."
- Break condition: If the LLM-generated feedback consistently fails to fit into the taxonomy categories, or if the taxonomy proves too rigid for the flexible nature of LLM output, its utility may be limited.

## Foundational Learning

- Concept: Feedback taxonomy (Keuning et al.)
  - Why needed here: The study uses this taxonomy to categorize and evaluate the types of feedback generated by the LLM, providing a structured framework for analysis.
  - Quick check question: What are the six main categories of feedback in Keuning et al.'s taxonomy?

- Concept: Prompt engineering
  - Why needed here: The researchers iteratively designed and refined prompts to elicit specific feedback types from the LLM, demonstrating the importance of careful prompt construction.
  - Quick check question: What are the key elements included in the final prompt to ensure relevant and accurate feedback generation?

- Concept: Formative feedback
  - Why needed here: The study focuses on generating formative feedback, which is aimed at improving the learning process during task execution, rather than summative feedback given after completion.
  - Quick check question: How does formative feedback differ from summative feedback in terms of its purpose and timing?

## Architecture Onboarding

- Component map:
  - Dataset selection and preparation -> Iterative prompt design and refinement -> GPT-4 feedback generation -> Feedback analysis and categorization -> Results evaluation

- Critical path:
  1. Select programming exercises and student submissions
  2. Iteratively design and refine prompts based on test outputs
  3. Generate feedback for the full dataset using the final prompt
  4. Analyze generated feedback for type accuracy and characteristics
  5. Evaluate results and draw conclusions

- Design tradeoffs:
  - Specificity vs. generalizability: Highly specific prompts may work well for particular exercises but may not generalize to other programming tasks.
  - Context inclusion vs. processing limits: Providing extensive context improves feedback relevance but may approach the LLM's input token limit.
  - Automation vs. human oversight: Fully automated feedback generation may be efficient but risks missing nuanced issues that human evaluation would catch.

- Failure signatures:
  - Consistent misclassification of feedback types
  - Frequent inclusion of misleading information
  - Inability to generate feedback for correct solutions
  - Excessive token usage leading to incomplete responses

- First 3 experiments:
  1. Test the final prompt on a diverse set of correct and incorrect solutions to assess type accuracy and characteristics.
  2. Compare the performance of the iterative prompt design approach with a baseline generic prompt on the same dataset.
  3. Evaluate the feedback generated by the LLM against expert human feedback for a subset of solutions to assess quality and relevance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do students perceive and utilize different types of LLM-generated feedback in actual programming tasks?
- Basis in paper: [inferred] The study focused on expert analysis of feedback characteristics but did not investigate student perspectives or usage patterns.
- Why unresolved: The paper only evaluated feedback from an expert perspective, not examining how students actually interact with, understand, or benefit from the different feedback types in real learning scenarios.
- What evidence would resolve it: Studies measuring student learning outcomes, satisfaction, and usage patterns when receiving different LLM feedback types in actual programming courses.

### Open Question 2
- Question: What is the optimal combination of LLM-generated feedback with traditional automated assessment methods?
- Basis in paper: [explicit] The paper suggests hybrid approaches combining test cases with LLM feedback could be more promising, but does not investigate specific combinations.
- Why unresolved: While the paper mentions potential benefits of hybrid approaches, it does not empirically test or determine optimal combinations of LLM feedback with traditional methods like test cases.
- What evidence would resolve it: Controlled experiments comparing various combinations of LLM feedback and traditional automated assessment methods on student learning outcomes.

### Open Question 3
- Question: How can LLM feedback be effectively personalized to individual student needs and skill levels?
- Basis in paper: [explicit] The paper notes that while feedback was personalized in some ways, there is potential for even more specific personalization to student preferences.
- Why unresolved: The study used a general prompt for all students without adapting to individual needs, skill levels, or learning preferences.
- What evidence would resolve it: Research demonstrating methods for adapting LLM prompts or feedback generation based on individual student characteristics, with evaluation of effectiveness.

## Limitations
- The iterative prompt design process relies heavily on subjective evaluation, potentially introducing bias
- The relatively small sample size (66 cases) limits generalizability across diverse programming tasks
- The study does not address how feedback quality might vary across different LLM models or student skill levels

## Confidence
- **High confidence**: GPT-4 can generate specific feedback types as defined in the taxonomy (63/66 cases successful)
- **Medium confidence**: The iterative prompt design process significantly improves feedback quality and accuracy
- **Medium confidence**: The feedback demonstrates personalization and addresses real errors, though some instances include misleading suggestions

## Next Checks
1. Conduct a larger-scale study across diverse programming languages and task types to assess generalizability of results
2. Implement a human evaluation study comparing LLM-generated feedback with expert human feedback to validate quality and helpfulness
3. Test the prompt design approach with multiple LLM models (e.g., GPT-3.5, Claude, Llama) to evaluate model-specific performance differences