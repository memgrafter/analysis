---
ver: rpa2
title: 'COEF-VQ: Cost-Efficient Video Quality Understanding through a Cascaded Multimodal
  LLM Framework'
arxiv_id: '2412.10435'
source_url: https://arxiv.org/abs/2412.10435
tags:
- video
- mllm
- audio
- multimodal
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'COEF-VQ introduces a cascaded multimodal LLM framework for efficient
  video quality understanding on short-video platforms. The method employs a two-stage
  approach: an entropy-based lightweight pre-filtering model followed by a comprehensive
  multimodal LLM evaluation.'
---

# COEF-VQ: Cost-Efficient Video Quality Understanding through a Cascaded Multimodal LLM Framework

## Quick Facts
- arXiv ID: 2412.10435
- Source URL: https://arxiv.org/abs/2412.10435
- Authors: Xin Dong; Sen Jia; Ming Rui Wang; Yan Li; Zhenheng Yang; Bingfeng Deng; Hongyu Xiong
- Reference count: 23
- Primary result: Cascade MLLM framework achieves 9.9% reduction in inappropriate content view rate while using 4.45% of full MLLM QPS

## Executive Summary
COEF-VQ introduces a cascaded multimodal LLM framework for efficient video quality understanding on short-video platforms. The method employs a two-stage approach: an entropy-based lightweight pre-filtering model followed by a comprehensive multimodal LLM evaluation. This architecture significantly reduces GPU resource requirements while maintaining strong classification performance. When deployed on TikTok's video management platform, the framework demonstrated substantial performance gains in two in-house video quality understanding tasks.

## Method Summary
COEF-VQ implements a cascaded multimodal framework where a lightweight baseline model performs initial filtering based on prediction uncertainty, followed by MLLM evaluation for high-uncertainty cases. The multimodal architecture integrates video frames, text (captions, subtitles, on-screen text), and audio through a unified MLLM with Whisper audio encoder. LoRA fine-tuning with rank 32 and alpha 64 enables efficient task adaptation. The cascade serving structure activates MLLM processing only when baseline model confidence falls below predetermined thresholds, achieving near-MLLM performance at 4.45% of the computational cost.

## Key Results
- Cascade structure achieves nearly MLLM-level performance while requiring only 4.45% of the query per second
- Online A/B testing shows 9.9% reduction in inappropriate content video view rate without affecting user engagement
- Late fusion outperforms early fusion for audio integration, with audio features proving integral rather than supplementary

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entropy-based pre-filtering reduces MLLM compute by 95.55% while maintaining performance
- Mechanism: Lightweight baseline model assigns initial scores; high-uncertainty samples trigger MLLM evaluation. This creates a two-stage cascade where MLLM resources are reserved for complex cases requiring nuanced multimodal reasoning.
- Core assumption: The lightweight model can reliably identify high-uncertainty samples that would benefit from MLLM's multimodal capabilities.
- Evidence anchors:
  - [abstract] "significantly reduces GPU usage while maintaining the strong classification performance"
  - [section 2.2] "This demand, when scaled for online deployment on platforms like TikTok, would lead to considerable GPU resource consumption"
  - [corpus] Weak - no direct citation of entropy-based filtering in related works
- Break condition: If lightweight model cannot reliably identify uncertainty, cascade fails and either wastes MLLM compute or misses important cases.

### Mechanism 2
- Claim: Multimodal integration (visual, textual, audio) improves classification accuracy over single-modality approaches
- Mechanism: Unified MLLM architecture fuses video frames, text, and audio through early and late fusion strategies. The model leverages complementary information across modalities to capture domain-specific signals that single-modality models miss.
- Core assumption: Different modalities contain complementary information that enhances overall understanding when properly fused.
- Evidence anchors:
  - [abstract] "This enhanced model architecture allows for more sophisticated feature fusion and deeper cross-modal interactions, boosting the video classification performance"
  - [section 2.1] "This audio encoder extracts audio features relevant to the classification task, allowing the model to incorporate sound cues"
  - [section 3.3] "The results indicate that Late fusion achieves a slightly higher F1 score" and "audio features are not merely supplementary; rather, they are integral to comprehensive video understanding"
- Break condition: If modality fusion is poorly implemented, performance degrades below single-modality baselines.

### Mechanism 3
- Claim: LoRA fine-tuning with specific rank/alpha parameters provides optimal balance between performance and efficiency
- Mechanism: Low-Rank Adaptation modifies pretrained MLLM weights with rank 32 and alpha 64, enabling task-specific fine-tuning without full model retraining. This approach maintains generalization while achieving high task-specific accuracy.
- Core assumption: LoRA parameters (rank 32, alpha 64) are optimal for this domain and task combination.
- Evidence anchors:
  - [section 2.1] "We use ZERO2[13] and LoRA[4] to train our model. The rank and alpha used in LoRA is 32 and 64"
  - [section 3.3] "The setting (32, 64) yielded the best F1 score compared to other configurations"
  - [corpus] Weak - no related work discussing LoRA parameter optimization for video quality tasks
- Break condition: If LoRA parameters are suboptimal, either performance suffers or GPU efficiency gains diminish.

## Foundational Learning

- Concept: Multimodal fusion strategies (early vs late fusion)
  - Why needed here: The paper compares early and late fusion approaches for audio integration, showing late fusion achieves better performance despite requiring stronger modality alignment in early fusion.
  - Quick check question: Why does late fusion outperform early fusion in this context, and what are the trade-offs between the two approaches?

- Concept: Cascade serving architecture
  - Why needed here: The two-stage cascade structure is central to the paper's efficiency gains, with the lightweight model filtering inputs before MLLM processing.
  - Quick check question: How does the cascade threshold selection affect the balance between computational efficiency and classification accuracy?

- Concept: Video content moderation metrics
  - Why needed here: The paper evaluates performance using F1 scores, recall at various precision thresholds (R@P60, R@P70, etc.), which are critical for understanding content moderation effectiveness.
  - Quick check question: Why are recall metrics at high precision thresholds particularly important for inappropriate content detection tasks?

## Architecture Onboarding

- Component map:
  Video frames → Vision encoder → LLM → Late fusion → Classification
  Video text → LLM → Late fusion → Classification
  Audio stream → Whisper encoder → Late fusion → Classification
  Baseline model → Initial filtering → Cascade decision → MLLM evaluation

- Critical path: Video → Base model prediction → Uncertainty assessment → MLLM evaluation → Classification output
- Design tradeoffs: Computational efficiency vs. classification accuracy, early vs. late fusion strategies, parameter optimization for LoRA
- Failure signatures: High false negative rates (missed inappropriate content), excessive GPU usage, degraded performance with limited training data
- First 3 experiments:
  1. Compare base model performance vs. MLLM on ICD task to establish baseline improvement
  2. Test different cascade thresholds (10%, 20%, 30% QPS) to optimize efficiency-accuracy tradeoff
  3. Evaluate different LoRA rank/alpha combinations to find optimal fine-tuning parameters

## Open Questions the Paper Calls Out

- Question: What are the precise computational savings achieved by the cascade structure compared to a full MLLM deployment when varying the pre-filtering threshold?
- Question: How does the cascade structure's performance change when deployed on content types beyond short videos, such as long-form videos or live streaming content?
- Question: What is the optimal balance between pre-filtering accuracy and computational efficiency across different types of video quality understanding tasks?

## Limitations
- Effectiveness depends heavily on lightweight model's ability to accurately identify high-uncertainty cases across content distribution shifts
- Evaluation limited to two specific in-house tasks without broader generalization testing across diverse video quality understanding scenarios
- 9.9% inappropriate content reduction measured in single platform context may not translate to other video-sharing environments

## Confidence

**High Confidence**: Computational efficiency claims (4.45% QPS vs full MLLM) are well-supported by cascade architecture description and align with established cascade serving principles.

**Medium Confidence**: LoRA fine-tuning parameters (rank 32, alpha 64) show optimal performance in tested configuration, but sensitivity analysis is limited to small parameter space.

**Low Confidence**: Online A/B testing results showing 9.9% reduction in inappropriate content view rate lack detailed methodology description and statistical significance measures.

## Next Checks

1. **Cascade Robustness Testing**: Evaluate cascade architecture's performance across content distribution shifts by testing on temporally separated data to assess whether lightweight model maintains reliable uncertainty detection over time.

2. **Cross-Platform Generalization**: Deploy COEF-VQ framework on at least one other short-video platform or dataset to verify that 9.9% inappropriate content reduction generalizes beyond TikTok's specific user base and content characteristics.

3. **Parameter Sensitivity Analysis**: Conduct comprehensive testing of LoRA rank and alpha parameters across wider range (e.g., ranks 16-128, alphas 32-256) to establish full performance-efficiency tradeoff space and identify whether selected parameters are truly optimal.