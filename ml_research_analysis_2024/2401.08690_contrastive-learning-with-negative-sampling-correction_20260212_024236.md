---
ver: rpa2
title: Contrastive Learning with Negative Sampling Correction
arxiv_id: '2401.08690'
source_url: https://arxiv.org/abs/2401.08690
tags:
- learning
- negative
- positive
- samples
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the issue of negative sampling bias in contrastive
  learning, where the generated negative samples are often polluted by positive samples,
  leading to a biased loss and performance degradation. The authors propose a novel
  contrastive learning method called Positive-Unlabeled Contrastive Learning (PUCL)
  that treats the generated negative samples as unlabeled samples and uses information
  from positive samples to correct the bias in the contrastive loss.
---

# Contrastive Learning with Negative Sampling Correction

## Quick Facts
- arXiv ID: 2401.08690
- Source URL: https://arxiv.org/abs/2401.08690
- Reference count: 6
- Primary result: Proposed PUCL method corrects negative sampling bias in contrastive learning and outperforms state-of-the-art methods on image and graph classification tasks.

## Executive Summary
This paper addresses the critical issue of negative sampling bias in contrastive learning, where generated negative samples often contain positive pairs, leading to performance degradation. The authors propose Positive-Unlabeled Contrastive Learning (PUCL), which treats generated negatives as unlabeled samples and uses positive sample information to correct the biased contrastive loss. The method is theoretically grounded in PU learning and compatible with any standard contrastive learning algorithm. Extensive experiments on image and graph datasets demonstrate significant improvements over baseline methods including SimCLR, MoCo, and InfoGraph.

## Method Summary
PUCL corrects negative sampling bias by treating generated "negative" pairs as unlabeled samples and estimating the true negative distribution from positive and unlabeled distributions. Under the Selected Completely At Random (SCAR) assumption, labeled data forms an i.i.d. sample from the positive distribution. The method computes a correction term using the positive prior α and label frequency c to adjust the unlabeled distribution into an unbiased estimate of the negative distribution. The corrected contrastive loss LDeCL replaces the sum of individual negative pair similarities with their expected value N*µ_x, where µ_x is computed from the corrected unlabeled distribution. This approximation incurs only negligible bias as the number of negative samples N increases, making PUCL both theoretically sound and practically effective.

## Key Results
- PUCL outperforms state-of-the-art methods including SimCLR, MoCo, and InfoGraph on CIFAR10, CIFAR100, and STL10 image classification tasks
- Significant improvements achieved on graph classification benchmarks (PTC, ENZYMES, DD, REDDIT-BINARY) compared to InfoGraph baseline
- Theoretical analysis proves the corrected loss only incurs negligible bias compared to the ideal unbiased contrastive loss as N increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PUCL corrects negative sampling bias by treating generated "negative" pairs as unlabeled samples and estimating the true negative distribution from positive and unlabeled distributions.
- Mechanism: The method uses the single-training-set scenario assumption where unlabeled data is a biased subset excluding positive pairs. It applies a correction term based on the positive prior α and label frequency c to adjust the unlabeled distribution into an unbiased estimate of the negative distribution.
- Core assumption: The labeling mechanism satisfies the Selected Completely At Random (SCAR) assumption, meaning labeled positive samples are selected randomly independent of their attributes.
- Evidence anchors:
  - [abstract]: "PUCL treats the generated negative samples as unlabeled samples and uses information from positive samples to correct bias in contrastive loss."
  - [section]: "Under this assumption, labeled data can be viewed as i.i.d. samples from the positive distribution with pl_x = p+_x."
  - [corpus]: Weak evidence - the corpus contains related work on contrastive learning and PU learning but no direct experimental validation of this specific mechanism.

### Mechanism 2
- Claim: The corrected contrastive loss LDeCL only incurs negligible bias compared to the ideal unbiased loss LIdealCL as the number of negative samples N increases.
- Mechanism: LDeCL replaces the sum of individual negative pair similarities with their expected value N*µ_x, where µ_x is computed from the corrected unlabeled distribution. This reduces variance while maintaining low bias.
- Core assumption: Large N is used in practice (as in standard contrastive learning methods), making the difference between individual and expected negative terms negligible.
- Evidence anchors:
  - [section]: "Theorem 1 shows that LDeCL only incurs a negligible bias compared to the ideal loss LIdealCL as large N is often used in standard contrastive learning methods."
  - [abstract]: "We prove that the corrected loss used in PUCL only incurs a negligible bias compared to the unbiased contrastive loss."
  - [corpus]: Weak evidence - corpus papers discuss contrastive learning theory but don't specifically validate this approximation theorem.

### Mechanism 3
- Claim: PUCL is compatible with any algorithm that optimizes standard contrastive loss and improves performance across different backbone architectures.
- Mechanism: PUCL modifies only the negative sampling component while keeping the core contrastive learning framework intact. It estimates the correction parameters α and c without requiring changes to the encoder or training pipeline.
- Core assumption: The underlying encoder architecture and training procedure remain effective for representation learning when applied to the corrected loss.
- Evidence anchors:
  - [section]: "PUCL can be applied to general contrastive learning problems and outperforms state-of-the-art methods on various image and graph classification tasks."
  - [abstract]: "PUCL can be applied to general contrastive learning problems and outperforms state-of-the-art methods on various image and graph classification tasks."
  - [corpus]: Moderate evidence - the corpus contains related methods (Ring, Hard, DEB) that also modify negative sampling, suggesting this is a viable research direction.

## Foundational Learning

- Concept: Positive-Unlabeled (PU) Learning
  - Why needed here: PU learning provides the theoretical framework for estimating the negative distribution when only positive and unlabeled samples are available.
  - Quick check question: In PU learning, if you have 100 positive samples and 900 unlabeled samples, and you estimate that 10% of unlabeled samples are actually positive, how many true negative samples do you have?

- Concept: Noise Contrastive Estimation (NCE)
  - Why needed here: NCE is the foundation of contrastive learning, and PUCL builds upon it by correcting the negative sampling bias in NCE-based losses.
  - Quick check question: In NCE, what is the relationship between the log-likelihood ratio of positive pairs versus negative pairs and the encoder's output?

- Concept: Single-Training-Set Scenario in PU Learning
  - Why needed here: This scenario models the realistic situation in contrastive learning where unlabeled samples form a biased subset excluding positive pairs.
  - Quick check question: How does the single-training-set scenario differ from the case-control scenario in terms of the relationship between unlabeled and true population distributions?

## Architecture Onboarding

- Component map:
  - Data sample -> Positive pair generator -> Unlabeled sampler -> Correction estimator -> Loss calculator -> Encoder

- Critical path:
  1. Sample observation x from data distribution
  2. Generate positive pair (x, x+) using transformations
  3. Sample M_u unlabeled samples {x_u_i} from training data
  4. Compute corrected negative distribution estimate using α and c
  5. Calculate PUCL loss with correction term
  6. Backpropagate through encoder

- Design tradeoffs:
  - Hyperparameter sensitivity: PUCL requires tuning α and c, which can be costly
  - Sample efficiency: Requires M_u unlabeled samples per positive pair, increasing memory usage
  - Computational overhead: Correction calculation adds minimal computation compared to encoder forward pass

- Failure signatures:
  - Performance worse than baseline: Likely indicates incorrect α or c values, or violation of SCAR assumption
  - Unstable training: May occur if α is set too high relative to true positive prior
  - No improvement on easy datasets: PUCL is designed for cases with negative sampling bias, which may not exist in simple datasets

- First 3 experiments:
  1. Baseline comparison: Run standard SimCLR on CIFAR10 with varying negative sample sizes to establish performance baseline
  2. Hyperparameter sensitivity: Test PUCL with different α values (0.05, 0.1, 0.15) on CIFAR10 to find optimal positive prior
  3. Architecture compatibility: Apply PUCL to MoCo baseline and compare performance gains versus SimCLR to verify cross-architecture effectiveness

## Open Questions the Paper Calls Out
- How can the optimal values for the hyperparameters α (positive prior) and c (label frequency) be automatically determined for large-scale datasets without relying on grid-search methods?
- Can the correction for positive sample distribution be introduced alongside the correction for negative sample distribution in PUCL?
- How does the performance of PUCL vary with different types of data augmentation techniques used in generating positive and unlabeled pairs?

## Limitations
- Reliance on the SCAR assumption, which may not hold in real-world labeling scenarios where labeling depends on sample attributes
- Need to tune hyperparameters α and c, which can be costly and dataset-dependent without automatic estimation methods
- Does not address potential biases in the positive sampling process, focusing only on negative sampling correction

## Confidence
- Mechanism 1: High - Clear theoretical foundation in PU learning with mathematical proofs
- Mechanism 2: High - Theoretical analysis provides rigorous proof of negligible bias approximation
- Mechanism 3: Medium - Experiments show cross-architecture effectiveness but broader validation needed

## Next Checks
1. Test PUCL on datasets with known labeling bias (e.g., CIFAR10 with systematic augmentation bias) to validate the correction mechanism under controlled conditions.
2. Apply PUCL to non-standard architectures like Vision Transformers to verify the claimed universal compatibility across different backbone types.
3. Conduct ablation studies varying the number of unlabeled samples M_u to determine the optimal trade-off between correction accuracy and computational cost.