---
ver: rpa2
title: Just How Flexible are Neural Networks in Practice?
arxiv_id: '2406.11463'
source_url: https://arxiv.org/abs/2406.11463
tags:
- data
- training
- neural
- labels
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper empirically examines how many training samples neural
  networks can fit in practice, challenging the common belief that networks can fit
  as many samples as they have parameters. The authors define and measure the Effective
  Model Complexity (EMC) as the largest training set size a network can perfectly
  fit.
---

# Just How Flexible are Neural Networks in Practice?

## Quick Facts
- arXiv ID: 2406.11463
- Source URL: https://arxiv.org/abs/2406.11463
- Reference count: 40
- Primary result: Neural networks often fit far fewer samples than parameters in practice, challenging common assumptions about overfitting and parameter efficiency

## Executive Summary
This paper empirically examines the Effective Model Complexity (EMC) of neural networks - the largest training set size a network can perfectly fit. Contrary to common belief, standard optimizers like SGD often find minima where networks fit far fewer samples than their parameter count, questioning fundamental assumptions about overfitting and parameter efficiency. The study reveals that many factors including data, architecture, optimizer, and activation functions significantly influence a network's practical capacity to fit data, demonstrating that neural networks are often parameter-wasteful in their standard form.

## Method Summary
The paper defines EMC as the largest training set size a network can perfectly fit, measured through an iterative approach where models are trained on increasingly larger random subsets of data until 100% accuracy is no longer achieved. Convergence is verified using three criteria: gradient norms below threshold, loss stabilization, and Hessian eigenvalues showing no negative values. The methodology is applied across various datasets (MNIST, CIFAR-10, CIFAR-100, ImageNet, synthetic ImageNet-20MS), architectures (CNNs, MLPs, ViTs), optimizers (SGD, Adam, full-batch GD), and activation functions (ReLU, sigmoid, linear) to systematically explore factors affecting network flexibility.

## Key Results
- Standard optimizers like SGD often find minima where networks fit far fewer samples than parameters, questioning overfitting assumptions
- CNNs are more parameter-efficient than MLPs and ViTs even on randomly labeled data
- SGD actually fits more data than full-batch GD, contrary to its assumed regularizing effect
- The gap between fitting correctly vs randomly labeled data predicts generalization
- ReLU activations enable fitting more data than sigmoid, despite being designed for gradient stability

## Why This Works (Mechanism)
None provided

## Foundational Learning

### Concept 1: Effective Model Complexity (EMC)
- Why needed: Provides a concrete metric for measuring the practical capacity of neural networks beyond theoretical parameter counts
- Quick check: Verify that EMC calculations converge using the three criteria (gradient norm, loss stabilization, Hessian eigenvalues)

### Concept 2: Minimum convergence verification
- Why needed: Ensures that EMC measurements represent true minima rather than incomplete training
- Quick check: Monitor gradient norms and loss curves during training to confirm convergence

### Concept 3: Random labeling experiments
- Why needed: Helps distinguish between fitting capacity and memorization ability
- Quick check: Compare EMC on correctly vs randomly labeled data to measure the gap

### Concept 4: Architecture-dataset interaction
- Why needed: Shows how different architectures perform across various data distributions
- Quick check: Test same architecture on multiple datasets with varying complexity

## Architecture Onboarding

### Component Map
Data -> EMC Measurement -> Architecture Comparison -> Optimizer/Activation Analysis -> Generalization Prediction

### Critical Path
1. Data preparation and random subset generation
2. Model training with convergence verification
3. EMC calculation across increasing sample sizes
4. Comparative analysis across architectures/optimizers
5. Generalization gap analysis

### Design Tradeoffs
- Computational cost vs measurement precision: More convergence verification steps increase confidence but require more resources
- Sample size vs statistical significance: Larger random subsets provide more stable EMC estimates but increase training time
- Architecture complexity vs parameter efficiency: More complex architectures may fit more data but could be less parameter-efficient

### Failure Signatures
- Models failing to reach 100% accuracy even on small sample sizes
- Inconsistent EMC values across random seeds
- Convergence criteria not being met within reasonable training time
- Memory constraints preventing training on large datasets

### First Experiments
1. Implement EMC calculation on a simple 2-layer CNN with CIFAR-10 to verify the methodology works
2. Compare EMC values for CNN vs MLP on the same dataset to observe architectural differences
3. Test EMC with correctly vs randomly labeled data to measure the generalization gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mathematical relationship between a neural network's ability to fit correctly labeled data versus randomly labeled data, and how does this relationship predict generalization performance across different architectures and datasets?
- Basis in paper: [explicit] The paper states "The ability of a neural network to fit many more correctly labeled samples than incorrectly labeled samples is predictive of generalization" and shows an inverse correlation between this metric and generalization gap.
- Why unresolved: While the paper demonstrates this correlation empirically, it does not provide a formal theoretical framework or mathematical proof explaining why this relationship exists or how it can be quantified.
- What evidence would resolve it: A rigorous mathematical proof establishing the relationship between EMC gap (correctly vs randomly labeled data) and generalization bounds, validated across diverse architectures and datasets.

### Open Question 2
- Question: How do different architectural design choices (e.g., activation functions, layer types, connectivity patterns) interact to influence the Effective Model Complexity, and can we develop a unified theoretical framework to predict EMC based on architectural properties?
- Basis in paper: [explicit] The paper finds that "ReLU activation functions enable fitting more training samples than sigmoidal activations" and that CNNs are "more parameter-efficient than MLPs and ViTs," but the underlying mechanisms are not fully explained.
- Why unresolved: The paper shows empirical results but does not provide a comprehensive theoretical understanding of how different architectural components contribute to EMC or how they interact with each other.
- What evidence would resolve it: A unified theoretical framework that predicts EMC based on architectural properties, validated through extensive empirical testing across diverse architectures.

### Open Question 3
- Question: Can we develop optimization techniques that consistently find minima with higher EMC without sacrificing generalization, and what are the fundamental trade-offs between optimization objectives and model capacity?
- Basis in paper: [explicit] The paper finds that "SGD actually fits more training data than full-batch gradient descent" and that "SAM improves generalization without reducing capacity," suggesting optimization choices significantly impact EMC.
- Why unresolved: While the paper demonstrates that different optimizers find minima with different EMC values, it does not provide a systematic approach to designing optimizers that maximize EMC while maintaining good generalization.
- What evidence would resolve it: New optimization algorithms that provably maximize EMC while maintaining generalization bounds, validated through extensive empirical testing.

## Limitations

- The study focuses exclusively on supervised learning with classification tasks, leaving unclear whether results generalize to regression, unsupervised learning, or reinforcement learning settings
- The convergence verification method relying on gradient norms, loss stabilization, and Hessian eigenvalues is computationally expensive, potentially limiting practical applicability
- Claims about optimizer effects and activation function differences may be sensitive to hyperparameter choices not fully specified

## Confidence

- High confidence: The empirical observations about EMC values being consistently lower than parameter counts across multiple architectures and datasets are robust and reproducible
- Medium confidence: Claims about optimizer effects (SGD vs full-batch GD) and activation function differences are supported by experiments but may be sensitive to hyperparameter choices not fully specified
- Low confidence: The generalization prediction capability (gap between correct vs random labels) needs further validation on truly out-of-distribution data

## Next Checks

1. Replicate EMC measurements across diverse architectures (CNNs, MLPs, ViTs) on standard datasets (CIFAR-10, ImageNet subsets) using the three-convergence-criteria methodology to verify consistency

2. Test the optimizer effect claim (SGD fitting more data than full-batch GD) with controlled hyperparameter sweeps across learning rates and batch sizes to isolate optimizer impact

3. Validate the generalization prediction hypothesis by measuring EMC gaps on correctly vs randomly labeled data and correlating with test set performance across multiple random seeds and data splits