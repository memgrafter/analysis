---
ver: rpa2
title: 'NEAR: A Training-Free Pre-Estimator of Machine Learning Model Performance'
arxiv_id: '2408.08776'
source_url: https://arxiv.org/abs/2408.08776
tags:
- near
- neural
- activation
- network
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NEAR is a zero-cost proxy for estimating neural network performance
  without training. It uses the effective rank of pre- and post-activation matrices
  to measure network expressivity.
---

# NEAR: A Training-Free Pre-Estimator of Machine Learning Model Performance

## Quick Facts
- arXiv ID: 2408.08776
- Source URL: https://arxiv.org/abs/2408.08776
- Reference count: 40
- NEAR achieves state-of-the-art correlation with final model accuracy among zero-cost proxies on NAS benchmarks

## Executive Summary
NEAR (Network Expressivity by Activation Rank) is a training-free method that estimates neural network performance by measuring the effective rank of pre- and post-activation matrices. It provides a zero-cost proxy for model accuracy prediction that outperforms existing methods on NAS benchmarks. The approach is broadly applicable, working across different activation functions and network architectures without requiring output labels during evaluation.

## Method Summary
NEAR calculates a score based on the effective rank of pre-activation and post-activation matrices from neural network layers. The method extracts the values of each layer before and after applying the activation function, computes the effective rank for each matrix, and sums these values across all layers to produce the final NEAR score. This score correlates with final model accuracy and can be used to guide architecture selection, activation function choice, weight initialization, and even estimate optimal layer sizes for MLPs through power-law fitting.

## Key Results
- NEAR achieves state-of-the-art correlation with final accuracy on NAS-Bench-101 and NATS-Bench benchmarks
- NEAR score correlates with final performance across different activation functions without requiring output labels
- NEAR can estimate optimal layer sizes in MLPs by fitting a power law to relative score decay

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: NEAR measures network expressivity by calculating the effective rank of pre- and post-activation matrices.
- **Mechanism**: The effective rank captures how much a layer stretches the input space along different dimensions. A high effective rank means the layer spreads inputs into diverse activation patterns, which correlates with the network's ability to represent complex functions.
- **Core assumption**: Diverse activation patterns across samples indicate higher expressivity and, consequently, better final performance.
- **Evidence anchors**:
  - [abstract] NEAR is based on the effective rank of the pre- and post-activation matrix, i.e., the values of a neural network layer before and after applying its activation function.
  - [section] We define the pre-activation matrix for layer l as Zl ∈ Rnl×nl... Intuitively, we would expect a well-performing network when the rows in the pre-/post-activation matrix are very different to each other, while a matrix with all similar rows would indicate a poorly performing network.
- **Break condition**: If the activation patterns become similar due to dataset redundancy or network saturation, the effective rank will drop and the correlation with final accuracy will weaken.

### Mechanism 2
- **Claim**: NEAR can select optimal activation functions and weight initialization schemes without training.
- **Mechanism**: By comparing the effective rank of different activation functions or initialization schemes on the same untrained network, NEAR identifies which configuration produces the most diverse activation patterns, predicting better training dynamics and final performance.
- **Core assumption**: The activation function and initialization that yield the highest effective rank before training will also lead to the best performance after training.
- **Evidence anchors**:
  - [abstract] Furthermore, we show that this score can be utilized to select hyperparameters such as the activation function and the neural network weight initialization scheme.
  - [section] We therefore evaluate the suitability of our zero-cost proxy NEAR for this purpose... A comparison of the loss and NEAR score of different activation functions applying the same weight initialization scheme also shows that the score can guide the selection of an appropriate activation function.
- **Break condition**: If the training process depends critically on factors not captured by activation pattern diversity (e.g., specific gradient dynamics), the correlation between NEAR score and final accuracy may break.

### Mechanism 3
- **Claim**: NEAR can estimate optimal layer sizes in multi-layer perceptrons without a search space.
- **Mechanism**: NEAR score per neuron decreases as layer size increases. Fitting a power law to the relative NEAR score reveals when additional neurons provide diminishing returns, indicating an optimal size.
- **Core assumption**: The relative NEAR score (score divided by number of neurons) follows a predictable power law decay with increasing layer size, and the slope threshold accurately indicates the optimal size.
- **Evidence anchors**:
  - [abstract] In addition, we present a simple approach to estimate the optimal layer sizes in multi-layer perceptrons.
  - [section] We observed that the relative score, i.e., the score divided by the number of neurons, appears to be well represented by a simple power function of the form f(s) = α + β · s^γ... This behavior is consistent for two different datasets.
- **Break condition**: If the relationship between NEAR score and layer size deviates from the assumed power law (e.g., due to dataset characteristics or architecture constraints), the estimated optimal size may be inaccurate.

## Foundational Learning

- **Concept**: Effective rank of a matrix
  - Why needed here: NEAR relies on effective rank to quantify how much a layer spreads input activations into diverse patterns.
  - Quick check question: If a matrix has orthogonal rows, what is its effective rank relative to its actual rank?

- **Concept**: Power law scaling in neural networks
  - Why needed here: NEAR uses a power law to model the decay of relative score with layer size to estimate optimal sizes.
  - Quick check question: In what scenarios might the power law assumption for NEAR score decay break down?

- **Concept**: Neural network expressivity and activation patterns
  - Why needed here: NEAR assumes that diverse activation patterns indicate higher expressivity and better performance.
  - Quick check question: How might dataset redundancy affect the diversity of activation patterns and NEAR's predictions?

## Architecture Onboarding

- **Component map**: Sample input data -> Forward pass through network -> Extract pre- and post-activation matrices -> Compute effective rank for each -> Sum to get NEAR score
- **Critical path**: Sample input data -> Forward pass through network -> Extract pre- and post-activation matrices -> Compute effective rank for each -> Sum to get NEAR score
- **Design tradeoffs**: NEAR requires no training but depends on representative input samples; it generalizes across activation functions but may be less precise than training-based methods for fine-grained comparisons.
- **Failure signatures**: Low correlation between NEAR score and final accuracy; NEAR score fails to distinguish between activation functions or initialization schemes; estimated optimal layer sizes perform poorly.
- **First 3 experiments**:
  1. Verify NEAR score correlates with final accuracy on a small NAS benchmark (e.g., NATS-Bench-SSS with CIFAR-10).
  2. Test NEAR's ability to rank different activation functions on a simple MLP task (e.g., EMNIST).
  3. Use NEAR to estimate optimal layer sizes for an MLP on a toy dataset and validate against trained models.

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Question: How does the effective rank-based measure of expressivity generalize to architectures beyond standard convolutional networks, such as transformers or graph neural networks?
  - Basis in paper: [explicit] The paper notes that NEAR works without requiring specific activation functions and could theoretically be applied to various architectures, but only evaluates CNNs and MLPs.
  - Why unresolved: The theoretical connection between effective rank and expressivity for non-grid-structured data is not established.
  - What evidence would resolve it: Empirical validation of NEAR on transformers, GNNs, and other non-standard architectures, showing consistent correlation with final accuracy.

- **Open Question 2**
  - Question: What is the theoretical relationship between the slope threshold (0.5% of initial slope) used for estimating optimal layer sizes and the actual generalization performance of the resulting networks?
  - Basis in paper: [inferred] The paper uses a heuristic threshold based on the power-law fit to relative NEAR scores without theoretical justification.
  - Why unresolved: The threshold appears arbitrary and its impact on generalization error is not explored.
  - What evidence would resolve it: Mathematical derivation connecting the slope threshold to generalization bounds, or empirical study of threshold sensitivity across diverse datasets and architectures.

- **Open Question 3**
  - Question: How does NEAR's performance compare to training-based architecture search methods when computational resources are not severely constrained?
  - Basis in paper: [explicit] The paper demonstrates that NEAR achieves high correlation with final accuracy on multiple benchmarks, but only compares to zero-cost proxies.
  - Why unresolved: The paper focuses on the efficiency benefits of NEAR without addressing its absolute performance relative to traditional NAS methods.
  - What evidence would resolve it: Direct comparison of architectures selected by NEAR versus those found by computationally expensive methods like DARTS or evolutionary algorithms, measuring final accuracy and efficiency trade-offs.

## Limitations
- NEAR's correlation with final accuracy remains moderate (Kendall's τ around 0.4-0.5 on benchmarks)
- The method requires representative input samples for the untrained network
- The power law assumption for optimal layer size estimation may not hold across all architectures and datasets

## Confidence
- NEAR's core mechanism (effective rank correlation with performance): High
- State-of-the-art status among zero-cost proxies: Medium
- Ability to select activation functions/initialization: Medium
- Optimal layer size estimation: Low

## Next Checks
1. Test NEAR on additional diverse datasets and architectures to verify robustness beyond the tested benchmarks
2. Evaluate whether NEAR scores remain predictive when using limited or unrepresentative training samples
3. Verify the power law assumption for optimal layer size estimation across different network depths and activation functions