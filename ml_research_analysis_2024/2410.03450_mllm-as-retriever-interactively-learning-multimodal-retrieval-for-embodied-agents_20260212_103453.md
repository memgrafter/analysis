---
ver: rpa2
title: 'MLLM as Retriever: Interactively Learning Multimodal Retrieval for Embodied
  Agents'
arxiv_id: '2410.03450'
source_url: https://arxiv.org/abs/2410.03450
tags:
- task
- trajectory
- action
- tasks
- mllm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MART enhances embodied agents by fine-tuning a multimodal large
  language model (MLLM) retriever using interactive learning and preference pairs
  derived from task success rates. It also introduces Trajectory Abstraction to condense
  trajectories into key milestones, reducing token requirements while preserving essential
  information.
---

# MLLM as Retriever: Interactively Learning Multimodal Retrieval for Embodied Agents

## Quick Facts
- arXiv ID: 2410.03450
- Source URL: https://arxiv.org/abs/2410.03450
- Reference count: 40
- Primary result: MART improves task success rates by over 10% using fine-tuned MLLM retrieval with interactive learning

## Executive Summary
MART introduces a novel approach for embodied agents by fine-tuning a multimodal large language model (MLLM) as a retriever using interactive learning and preference pairs derived from task success rates. The method addresses limitations in current retrieval approaches that rely on surface-level similarity by prioritizing trajectories that are actually effective for completing specific tasks. MART also introduces Trajectory Abstraction, which condenses full trajectories into key milestones using GPT-4o, significantly reducing token requirements while preserving essential information for task completion.

## Method Summary
MART fine-tunes an MLLM retriever using preference pairs based on interactive feedback from task completion success rates. The method implements Trajectory Abstraction to condense multimodal trajectories into key milestones, reducing computational overhead while maintaining task-relevant information. During training, the retriever learns to prioritize effective trajectories for unseen tasks through a Bradley-Terry reward modeling loss function. The approach is evaluated across AI2-THOR and LEGENT environments, demonstrating significant improvements in task success rates compared to baseline methods.

## Key Results
- MART achieves over 10% improvement in task success rates compared to baselines (PA, LP, SL, RAP)
- Significant improvements in both Success Rate (SR) and Average Steps (AS) metrics across multiple environments
- Trajectory Abstraction reduces token requirements while preserving essential information for task completion
- MART demonstrates strong performance in zero-shot generalization to unseen tasks

## Why This Works (Mechanism)
MART leverages fine-tuned MLLMs to assess trajectory effectiveness rather than relying on surface-level similarity matching. By using interactive learning and preference pairs derived from actual task success rates, the retriever learns to identify which trajectories will actually help complete tasks, not just which ones look similar. The Trajectory Abstraction component enables efficient processing by condensing full trajectories into key milestones while retaining the information most relevant to task completion.

## Foundational Learning

**Embodied AI Task Completion** - Why needed: Understanding how agents navigate and interact with environments to complete tasks provides context for retrieval-based approaches. Quick check: Verify understanding of task success metrics and evaluation environments.

**Multimodal Large Language Models** - Why needed: MLLMs process both visual and textual information, essential for handling multimodal trajectory data. Quick check: Confirm familiarity with MLLM architectures and their capabilities in visual-language tasks.

**Preference Learning and Reward Modeling** - Why needed: MART uses preference pairs and Bradley-Terry loss for training, requiring understanding of these techniques. Quick check: Verify knowledge of how preference pairs are constructed and used in reward modeling.

**Trajectory Abstraction** - Why needed: The method reduces computational complexity by condensing trajectories while preserving essential information. Quick check: Understand how GPT-4o determines which observations are "helpful" for tasks.

## Architecture Onboarding

**Component Map**: Task Instructions -> MLLM Retriever -> Trajectory Memory -> Trajectory Abstraction -> Agent Actions -> Environment Feedback -> Preference Pairs -> Retriever Fine-tuning

**Critical Path**: The core pipeline flows from task instructions through the fine-tuned MLLM retriever to select effective trajectories, which are then abstracted and used to guide agent actions. Environment feedback generates preference pairs that drive the interactive learning process.

**Design Tradeoffs**: MART trades computational efficiency (through trajectory abstraction) for potential information loss, while interactive learning requires additional training overhead but improves retrieval effectiveness. The use of MLLMs enables sophisticated multimodal understanding but increases model size and inference costs.

**Failure Signatures**: Retriever failure manifests as poor trajectory selection leading to low task success rates. Trajectory Abstraction failure results in loss of critical information, causing agents to miss essential steps. Interactive learning may converge slowly or get stuck in local optima if preference pairs are not well-constructed.

**First Experiments**:
1. Test the Trajectory Abstraction mechanism by having GPT-4o condense sample trajectories and verify preservation of task-relevant milestones
2. Implement the Bradley-Terry reward modeling loss with synthetic preference pairs to ensure correct ranking behavior
3. Evaluate the MLLM retriever on a small dataset to confirm it can distinguish between effective and ineffective trajectories

## Open Questions the Paper Calls Out
- How does MART's performance scale with the size of the trajectory memory? The paper uses relatively small memory pools (40 training, 32 testing trajectories) but doesn't explore larger scales.
- Can MART generalize to tasks requiring multi-step reasoning or planning beyond individual trajectory abstraction? The paper focuses on single trajectory retrieval rather than complex planning tasks.
- How does the choice of base MLLM affect MART's performance compared to other models? The paper uses LLaVA-7B but doesn't compare against alternatives like GPT-4V or Gemini Pro Vision.

## Limitations
- The paper doesn't provide specific implementation details for the Trajectory Abstraction mechanism or exact hyperparameter settings
- Memory pool sizes are relatively small, limiting understanding of scalability
- Performance comparison against alternative base MLLMs is missing

## Confidence
High: Core methodology and evaluation framework are clearly defined
Medium: Implementation details for key components like Trajectory Abstraction are unspecified
Low: Long-term generalization and scalability aspects are not thoroughly explored

## Next Checks
1. Implement a minimal prototype of the Trajectory Abstraction mechanism using GPT-4o to condense sample trajectories into milestones, then verify that essential task-relevant information is preserved
2. Conduct ablation studies to quantify the contribution of Trajectory Abstraction versus interactive learning alone, as the paper doesn't clearly separate these effects
3. Test the preference pair generation mechanism on a small dataset to ensure the Bradley-Terry reward modeling loss is correctly implemented and produces meaningful trajectory rankings