---
ver: rpa2
title: 'SKILL: Similarity-aware Knowledge distILLation for Speech Self-Supervised
  Learning'
arxiv_id: '2402.16830'
source_url: https://arxiv.org/abs/2402.16830
tags:
- layers
- layer
- distillation
- speech
- skill
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SKILL, a similarity-aware knowledge distillation
  method for compressing speech self-supervised learning models. SKILL addresses the
  limitations of existing layer-selection-based distillation approaches by clustering
  teacher layers based on their similarity and distilling information from these clusters
  instead of individual layers.
---

# SKILL: Similarity-aware Knowledge distILLation for Speech Self-Supervised Learning

## Quick Facts
- **arXiv ID**: 2402.16830
- **Source URL**: https://arxiv.org/abs/2402.16830
- **Reference count**: 0
- **Primary result**: SKILL outperforms DPHuBERT in compressing speech SSL models, achieving state-of-the-art results in the 30M parameters model class across SUPERB tasks.

## Executive Summary
This paper introduces SKILL, a similarity-aware knowledge distillation method for compressing speech self-supervised learning (SSL) models. SKILL addresses limitations of existing layer-selection-based distillation approaches by clustering teacher layers based on their similarity and distilling information from these clusters instead of individual layers. The method simplifies the distillation process and leads to better generalization capabilities. Extensive experiments on WavLM Base+ and HuBERT Base models demonstrate that SKILL outperforms DPHuBERT, achieving state-of-the-art results in the 30M parameters model class across several SUPERB tasks.

## Method Summary
SKILL extends DPHuBERT with a CKA-based clustering module that runs before distillation. The method first computes CKA similarity between teacher layers on a calibration dataset, then applies hierarchical clustering to group similar layers. During distillation, instead of mapping individual layers, SKILL averages activations within each cluster and distills these averaged representations to the student. The structured pruning stage remains unchanged from DPHuBERT, using Hard Concrete distributions for parameter sparsity. The two-stage training process involves joint distillation and structured pruning (75% sparsity target) followed by distillation with fixed architecture.

## Key Results
- SKWavLM (SKILL-compressed WavLM Base+) achieves PER 6.90, WER 10.03, and DER 5.53 on phoneme recognition, ASR, and speaker diarization, respectively.
- SKILL outperforms DPHuBERT on multiple SUPERB tasks while targeting ~23.5M parameters.
- The method demonstrates data-dependent clustering that adapts to specific teacher model characteristics.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SKILL's clustering-based layer selection prevents bias toward arbitrary layers.
- Mechanism: Instead of manually selecting fixed layers, SKILL computes layer-wise similarity using CKA on a calibration set and clusters layers into groups. Distillation then averages representations within each cluster, ensuring all layers contribute proportionally to their information content.
- Core assumption: Layers encoding similar information can be averaged without losing critical distinctions.
- Evidence anchors: Layer-wise distance analysis shows strong bias in DPHuBERT toward selected layers; CKA clustering identifies meaningful layer groupings.
- Break condition: If CKA similarity fails to capture functionally relevant differences between layers.

### Mechanism 2
- Claim: Averaging within clusters reduces redundancy and weights unique information more heavily.
- Mechanism: By clustering similar layers and distilling their averages, SKILL implicitly downweights redundant information while preserving unique signals. The layer-specific influence on the loss is inversely proportional to the number of layers in the cluster.
- Core assumption: Redundant information across layers is distributed such that averaging within similarity-based clusters preserves essential content.
- Evidence anchors: Theoretical argument that redundant information is automatically weighted down; no direct empirical validation provided.
- Break condition: If layer redundancy is not structured enough for CKA to capture meaningful groupings.

### Mechanism 3
- Claim: SKILL's data-dependent clustering adapts to the specific characteristics of the teacher model and task.
- Mechanism: The CKA similarity is computed on a calibration dataset, making the clustering and subsequent distillation sensitive to the actual data distribution and teacher model structure.
- Core assumption: The calibration dataset is representative enough of the target domain to produce meaningful similarity measures.
- Evidence anchors: Contrast with DPHuBERT's heuristic layer selection; CKA similarity computation described in methodology.
- Break condition: If the calibration dataset is too small or unrepresentative.

## Foundational Learning

- **Concept**: Hierarchical clustering and similarity metrics (CKA)
  - Why needed here: SKILL uses hierarchical clustering based on CKA similarity to group teacher layers before distillation.
  - Quick check question: What does CKA measure between two layers' activations, and why is it preferred over simple cosine similarity?

- **Concept**: Knowledge distillation mechanics and layer-wise alignment
  - Why needed here: SKILL extends standard distillation by replacing layer-to-layer mapping with cluster-to-cluster mapping.
  - Quick check question: In standard distillation, why do we need a trainable linear projection between teacher and student layer outputs?

- **Concept**: Structured pruning and the Hard Concrete distribution
  - Why needed here: SKILL inherits DPHuBERT's two-stage training with structured pruning using Hard Concrete distributions.
  - Quick check question: How does the Hard Concrete distribution enable gradient-based optimization of discrete pruning masks?

## Architecture Onboarding

- **Component map**: SKILL extends DPHuBERT with a CKA-based clustering module (calibration → CKA similarity → agglomerative clustering → cluster averaging) that runs before distillation. The structured pruning stage remains unchanged.
- **Critical path**: 1) Compute CKA similarity matrix on calibration data. 2) Apply agglomerative clustering to group layers. 3) During distillation, average teacher representations within each cluster. 4) Distill averaged representations to student with linear projections. 5) Apply structured pruning as in DPHuBERT.
- **Design tradeoffs**: Clustering adds preprocessing complexity but removes manual layer selection. Averaging may lose fine-grained distinctions but reduces redundancy. Method is model-agnostic but depends on calibration set quality.
- **Failure signatures**: Poor performance if CKA fails to capture meaningful similarity, calibration set is unrepresentative, or averaging smooths out critical task-specific information.
- **First 3 experiments**:
  1. Run CKA similarity matrix computation on WavLM Base+ and visualize layer-wise similarities to verify meaningful groupings.
  2. Test different numbers of clusters (e.g., 4, 6, 8) and measure downstream task performance to find optimal clustering granularity.
  3. Compare distillation performance when using averaged cluster representations vs. single representative layers from each cluster.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text. However, several important questions emerge from the methodology and results:

1. How does SKILL's performance vary with different numbers of clusters when compressing HuBERT Base models?
2. Can SKILL be effectively applied to other SSL models beyond WavLM and HuBERT, such as Wav2Vec 2.0 or XLSR?
3. How does SKILL perform when compressing SSL models to even smaller sizes, beyond the 30M parameter range?

## Limitations

- The method's effectiveness depends heavily on the quality and representativeness of the calibration dataset for CKA similarity computation.
- The paper lacks ablation studies directly validating whether layer similarity correlates with distillation effectiveness.
- Generalization to teacher models from different SSL frameworks remains untested.

## Confidence

- **High Confidence**: Empirical results showing SKILL outperforms DPHuBERT on SUPERB tasks.
- **Medium Confidence**: The claim that SKILL simplifies the distillation process by removing manual layer selection.
- **Medium Confidence**: The assertion that averaging within clusters reduces redundancy while preserving essential information.

## Next Checks

1. Conduct ablation studies varying the number of clusters and comparing distillation performance using single representative layers vs. averaged cluster representations.
2. Test SKILL's performance on teacher models from different SSL frameworks (e.g., Wav2Vec2, APC) to evaluate CKA similarity generalization.
3. Perform domain transfer experiments where the calibration dataset differs from the target domain to assess robustness to dataset shifts.