---
ver: rpa2
title: 'DsDm: Model-Aware Dataset Selection with Datamodels'
arxiv_id: '2401.12926'
source_url: https://arxiv.org/abs/2401.12926
tags:
- data
- training
- samples
- train
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DsDm, a model-aware dataset selection method
  that improves language model (LM) performance by modeling how the learning process
  uses training data to predict on target tasks. The core idea is to approximate the
  mapping between training subset and model performance using datamodels, then select
  the subset that maximizes this estimate.
---

# DsDm: Model-Aware Dataset Selection with Datamodels

## Quick Facts
- arXiv ID: 2401.12926
- Source URL: https://arxiv.org/abs/2401.12926
- Authors: Logan Engstrom; Axel Feldmann; Aleksander Madry
- Reference count: 40
- Primary result: Model-aware dataset selection using datamodels improves LM performance and delivers 2× compute multiplier over baseline methods

## Executive Summary
This paper introduces DsDm, a model-aware dataset selection method that improves language model performance by approximating how training data choices affect model performance on target tasks. The core innovation is using datamodels to predict loss on target tasks given training data inclusion, then selecting the subset that minimizes this estimated loss. DsDm consistently outperforms baseline selection methods and achieves a 2× compute multiplier, demonstrating that thoughtful data selection can significantly enhance model quality without increasing training compute.

## Method Summary
DsDm uses datamodels to approximate the mapping from training subsets to model performance on target tasks. The method trains linear datamodels using TRAK (Transformer-based Regression with Attentive Kernels) to predict loss given a characteristic vector indicating which training examples are included. The selection process identifies the subset of training data that minimizes the predicted loss on target tasks. The approach is validated by training GPT-2 style language models on selected subsets and measuring performance on both pre-specified target tasks and held-out benchmarks. The method uses 4 small proxy models to estimate datamodels, which are then applied to select data for larger models ranging from 125M to 1.8B parameters.

## Key Results
- DsDm consistently improves language model performance on diverse target tasks compared to random selection and similarity-based methods
- The method delivers a 2× compute multiplier, outperforming randomly selected data with 10× the compute budget
- DsDm generalizes to unseen tasks when proxy tasks cover a broad range of language modeling problem categories

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** DsDm approximates the mapping from training subset to model performance on target tasks using datamodels.
- **Mechanism:** Linear datamodels are trained to predict loss on target tasks given a characteristic vector of training data inclusion. The subset with smallest predicted loss is selected.
- **Core assumption:** The mapping from training subset to loss is approximately linear in the characteristic vector of inclusion.
- **Evidence anchors:** [abstract] "We use datamodels (Ilyas et al., 2022) to approximate how the learning algorithm uses data to predict on the tasks of interest." [section 2.3] "our size-k dataset selection with datamodels (DsDm) estimate of the optimal dataset selection is the subset that minimizes the approximated target loss."
- **Break condition:** If the true mapping is highly nonlinear, the linear approximation will fail to capture key interactions between training examples.

### Mechanism 2
- **Claim:** Selecting data that is predicted to reduce target task loss improves actual model performance on those tasks.
- **Mechanism:** The datamodel estimates are validated by training models on the selected subsets and measuring true loss, which decreases with DsDm.
- **Core assumption:** Datamodel predictions generalize to unseen model sizes and training configurations.
- **Evidence anchors:** [abstract] "Our resulting method greatly improves language model (LM) performance on both pre-specified tasks and previously unseen tasks." [section 3.2] "DsDm most improves target task performance on all tasks. Models trained with DsDm even outperform a larger model trained with 10× the compute on randomly selected data."
- **Break condition:** If datamodel estimates are inaccurate for the actual model size or training regime used, selected data may not yield the predicted performance gains.

### Mechanism 3
- **Claim:** DsDm can improve performance on unseen tasks by selecting data optimal for a mixture of proxy tasks.
- **Mechanism:** By targeting a mix of tasks that cover different problem categories, DsDm selects data that generalizes well to held-out benchmarks.
- **Core assumption:** Proxy tasks are representative of the distribution of unseen tasks the model will encounter.
- **Evidence anchors:** [abstract] "choosing target tasks similar to those we expect to see at deployment time, we can greatly improve model performance on yet unseen tasks." [section 4] "we consider three target tasks that cover a broad range of language modeling problem categories... and estimate the optimal training dataset selection for these tasks together."
- **Break condition:** If proxy tasks are not representative of unseen tasks, selected data may not generalize, and performance may not improve.

## Foundational Learning

- **Concept:** Datamodeling as a framework for approximating model behavior given training data.
  - **Why needed here:** DsDm relies on datamodels to predict how training data choice affects model performance on target tasks without exhaustive training.
  - **Quick check question:** What is the role of the characteristic vector 1S in datamodel estimation?

- **Concept:** Linear approximation of the mapping from training subset to model loss.
  - **Why needed here:** DsDm uses linear datamodels, so understanding linear regression and influence functions is key to grasping the method.
  - **Quick check question:** How do influence functions approximate the effect of removing a training point on model predictions?

- **Concept:** Tradeoff between computational cost and approximation accuracy in dataset selection.
  - **Why needed here:** DsDm trades optimality for feasibility by approximating the optimal subset rather than finding it exactly, enabling selection from massive datasets.
  - **Quick check question:** Why is it computationally infeasible to find the exact optimal training subset for large-scale models?

## Architecture Onboarding

- **Component map:** Proxy models -> TRAK estimator -> Selection optimizer -> Target LM training -> Evaluation
- **Critical path:** 1. Train 4 small proxy models on random C4 subsets to compute datamodels. 2. Compute TRAK estimates for all C4 examples using proxy models. 3. Select top k examples with smallest TRAK scores. 4. Train target LM on selected data and evaluate on benchmarks.
- **Design tradeoffs:** Linear vs. nonlinear datamodels: Linear are cheaper to compute but may miss interactions. Proxy model size: Smaller models are cheaper but may yield less accurate estimates. Target task choice: More tasks improve generalization but increase computation.
- **Failure signatures:** Low performance on target tasks: Datamodel estimates are inaccurate. No improvement over random: Proxy tasks are not representative of unseen tasks. High variance in results: Datamodel estimates are unstable across runs.
- **First 3 experiments:** 1. Train DsDm on a small subset of C4 (e.g., 1%) and evaluate on a single target task (e.g., SQuAD) to verify the basic pipeline works. 2. Compare DsDm with random selection on a held-out benchmark to confirm it improves performance. 3. Vary the number of proxy models and projection dimension to find a cost-effective configuration.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do datamodels generalize across model scales?
- **Basis in paper:** [explicit] The paper uses 125M parameter models to estimate datamodels and then applies these estimates to select data for larger models (up to 1.8B parameters), showing improved performance. However, the paper does not investigate how well these datamodels generalize across different model scales.
- **Why unresolved:** The paper does not provide a systematic study on the effect of model scale on the accuracy of datamodels. It would be beneficial to understand if and how the performance of datamodels changes when applied to models of different sizes.
- **What evidence would resolve it:** A study comparing the performance of datamodels estimated using models of different scales and applied to models of various sizes would provide insights into the generalizability of datamodels.

### Open Question 2
- **Question:** What is the optimal number of reference models for TRAK estimation?
- **Basis in paper:** [explicit] The paper uses 4 reference models for TRAK estimation, stating that the cost scales linearly with the number of models. However, it does not investigate the impact of using a different number of models on the accuracy of the datamodel estimates.
- **Why unresolved:** The paper does not provide a systematic study on the effect of the number of reference models on the accuracy of the TRAK estimates. Understanding the trade-off between computational cost and accuracy would be valuable for optimizing the TRAK estimation process.
- **What evidence would resolve it:** A study comparing the performance of TRAK estimates using different numbers of reference models would provide insights into the optimal number of models for balancing accuracy and computational cost.

### Open Question 3
- **Question:** How do datamodels perform in non-English language modeling tasks?
- **Basis in paper:** [explicit] The paper focuses on English language modeling tasks. It does not investigate the performance of datamodels in non-English language modeling tasks.
- **Why unresolved:** The paper does not provide any evidence on the effectiveness of datamodels in non-English language modeling tasks. Understanding the generalizability of datamodels across different languages would be valuable for their broader application.
- **What evidence would resolve it:** A study evaluating the performance of datamodels in non-English language modeling tasks would provide insights into their generalizability across different languages.

## Limitations

- The linearity assumption in datamodel estimation may not capture complex interactions between training examples for certain language tasks.
- The method's effectiveness on non-English datasets and specialized domains remains unproven, as validation was limited to English C4 data.
- Claims about generalizing to unseen tasks are based on limited validation with only 15 benchmarks tested.

## Confidence

- **High Confidence:** The core methodology of using datamodels for dataset selection is technically sound and the implementation details are well-specified.
- **Medium Confidence:** Claims about the 2× compute multiplier and performance improvements are supported by experimental results, though limited to specific benchmarks and model sizes.
- **Low Confidence:** The paper's assertions about generalizing to unseen tasks and domains are based on limited validation, with only 15 benchmarks tested.

## Next Checks

1. **Linearity Validation:** Test the accuracy of linear datamodels on datasets where the relationship between training examples and target loss is known to be nonlinear, to quantify the approximation error.

2. **Domain Generalization:** Evaluate DsDm on non-English datasets and specialized domains (e.g., biomedical text) to assess performance outside the C4 corpus.

3. **Scalability Analysis:** Measure the computational overhead of DsDm as model size increases beyond 1.8B parameters to determine if the efficiency gains persist at scale.