---
ver: rpa2
title: A Multi-Faceted Evaluation Framework for Assessing Synthetic Data Generated
  by Large Language Models
arxiv_id: '2404.14445'
source_url: https://arxiv.org/abs/2404.14445
tags:
- data
- synthetic
- real
- evaluation
- privacy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SynEval, a comprehensive framework to evaluate
  synthetic tabular data generated by large language models across three dimensions:
  fidelity, utility, and privacy. For fidelity, the framework assesses structure preservation,
  data integrity, and column shape, as well as sentiment distribution and text characteristics
  for review data.'
---

# A Multi-Faceted Evaluation Framework for Assessing Synthetic Data Generated by Large Language Models

## Quick Facts
- arXiv ID: 2404.14445
- Source URL: https://arxiv.org/abs/2404.14445
- Reference count: 40
- Primary result: SynEval framework evaluates synthetic tabular data across fidelity, utility, and privacy dimensions

## Executive Summary
This paper introduces SynEval, a comprehensive framework to evaluate synthetic tabular data generated by large language models across three dimensions: fidelity, utility, and privacy. For fidelity, the framework assesses structure preservation, data integrity, and column shape, as well as sentiment distribution and text characteristics for review data. Utility is evaluated through sentiment classification accuracy and mean absolute error when training models on synthetic data. Privacy is measured using membership inference attack success rates. Experiments on synthetic product review data from ChatGPT, Claude, and Llama show Claude best preserves text fidelity, while all models demonstrate comparable utility. However, high privacy attack success rates indicate potential information leakage. SynEval provides a critical tool for assessing synthetic data quality and guiding responsible deployment.

## Method Summary
The SynEval framework evaluates synthetic tabular data by training LLMs on real data samples and generating synthetic data, then applying a three-pronged evaluation approach. For fidelity, it uses Structure Preserving Score (SPS), Integrity Score (IS), and Column Shapes Score combining KS statistic and TVD to measure distributional similarity. Text analysis evaluates sentiment, keywords, and length distribution for review data. Utility is assessed through the TSTR framework by training sentiment classification models on synthetic data and testing on real data, measuring accuracy and MAE. Privacy is quantified using membership inference attacks with Random Forest classifiers to distinguish between member and non-member data. The framework was tested on Amazon product review data with three LLMs (Claude 3 Opus, ChatGPT 3.5, Llama 2 13B), each generating 300 synthetic entries from 50 real samples.

## Key Results
- Claude 3 Opus best preserves text fidelity among evaluated LLMs
- All models demonstrate comparable utility in sentiment classification tasks
- High membership inference attack success rates (>80%) indicate potential privacy leakage across all models
- Categorical features show significant duplication within synthetic datasets, raising privacy concerns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The fidelity evaluation captures structural, distributional, and semantic similarity between real and synthetic tabular data.
- Mechanism: The framework uses multiple complementary metrics: Structure Preserving Score (SPS) ensures column names and order match; Integrity Score (IS) validates categorical and continuous data boundaries; Column Shapes Score uses KS statistic and TVD to measure distributional similarity; text analysis evaluates sentiment, keywords, and length distribution.
- Core assumption: Different data types require different statistical measures to accurately capture similarity.
- Evidence anchors:
  - [abstract] "fidelity, utility, and privacy preservation of synthetically generated tabular data via a suite of diverse evaluation metrics"
  - [section] "For continuous numerical values, such as helpful vote, we utilize the Kolmogorov-Smirnov [38] statistic to measure the similarity in marginal distributions between the synthetic and real columns. For discrete data, such as verified purchases, we apply the Total Variation Distance (TVD) [37] to compare the synthetic and real columns."
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism combination.
- Break condition: If synthetic data preserves column structure but completely distorts data distributions within columns, the fidelity score would be misleading.

### Mechanism 2
- Claim: Utility is evaluated by measuring how well machine learning models trained on synthetic data perform on real data.
- Mechanism: The TSTR (Train-Synthetic-Test-Real) framework trains sentiment classification models on synthetic data and tests them on real data, measuring accuracy and MAE to assess utility preservation.
- Core assumption: Performance on real test data is a valid proxy for synthetic data utility.
- Evidence anchors:
  - [abstract] "utility is evaluated through sentiment classification accuracy and mean absolute error when training models on synthetic data"
  - [section] "We begin by extracting (review, rating) pairs from both the original and synthetic datasets to train sentiment classification models using logistic regression. The models are then validated on a separate set of untouched real data"
  - [corpus] Weak - no direct corpus evidence found for TSTR methodology specifically.
- Break condition: If synthetic data captures correct distributions but introduces subtle biases that don't affect accuracy but harm real-world deployment.

### Mechanism 3
- Claim: Privacy leakage is quantified through membership inference attack success rates.
- Mechanism: Random Forest classifiers are trained to distinguish between member (real) and non-member (synthetic) data, with high successful attack rates indicating privacy risks.
- Core assumption: Successful membership inference attacks on synthetic data indicate privacy leakage from the training data.
- Evidence anchors:
  - [abstract] "privacy is measured using membership inference attack success rates"
  - [section] "Using the prepared datasets, we construct a combined dataset of real members and a randomly selected half of all synthetic datasets. This combined dataset serves as our training set. The test set comprises real non-member data combined with the remaining half of the synthetic data."
  - [corpus] Weak - no direct corpus evidence found for this specific MIA implementation.
- Break condition: If synthetic data generation introduces sufficient noise or generalization that prevents membership inference even when real data patterns are preserved.

## Foundational Learning

- Concept: Kolmogorov-Smirnov statistic for continuous distributions
  - Why needed here: To measure similarity between continuous column distributions in synthetic and real data
  - Quick check question: What does a KS statistic value close to 0 indicate about two distributions?
  
- Concept: Total Variation Distance for discrete distributions
  - Why needed here: To measure similarity between categorical column distributions in synthetic and real data
  - Quick check question: What range does TVD fall within and what does 0 represent?

- Concept: Membership Inference Attacks (MIA)
  - Why needed here: To quantify privacy leakage by testing if synthetic data reveals membership in training data
  - Quick check question: What would a 50% successful attack rate indicate about privacy preservation?

## Architecture Onboarding

- Component map: Data preprocessing -> Metric computation (SPS, IS, Column Shapes, Text Analysis) -> Utility model training -> MIA model training -> Score aggregation and reporting
- Critical path: Data preprocessing → Metric computation (SPS, IS, Column Shapes, Text Analysis) → Utility model training → MIA model training → Score aggregation and reporting
- Design tradeoffs: Comprehensive evaluation vs. computational cost; using multiple metrics vs. potential redundancy; focusing on product reviews vs. generalizability to other tabular data types
- Failure signatures: High fidelity but low utility suggests synthetic data looks real but lacks predictive power; high utility but high privacy attack success suggests data leakage; inconsistent results across metrics indicate model-specific weaknesses
- First 3 experiments:
  1. Run the complete evaluation pipeline on a small synthetic dataset generated by any LLM to verify all components execute without errors
  2. Test the framework with synthetic data where you know the ground truth (e.g., synthetic data copied directly from real data) to verify maximum scores are achieved
  3. Evaluate the same synthetic dataset using only the fidelity metrics first, then add utility and privacy to understand incremental value of each component

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the fidelity, utility, and privacy trade-offs change when scaling from 300 to 10,000+ synthetic samples?
- Basis in paper: [explicit] The paper notes that "given that the training involved no more than 300 data samples, we anticipate that accuracy could further improve with the inclusion of more training data" and "it is also conceivable that with more comprehensive datasets for training, these models could enhance their data generation capabilities, reducing the risk of privacy breaches."
- Why unresolved: The experiments were limited to 300 synthetic samples per model, leaving uncertainty about how evaluation metrics would scale with larger datasets.
- What evidence would resolve it: Comparative experiments using the same evaluation framework with varying dataset sizes (e.g., 300, 1,000, 5,000, 10,000+ samples) to measure changes in fidelity, utility, and privacy metrics.

### Open Question 2
- Question: How does SynEval perform when applied to synthetic data generated from domains other than product reviews?
- Basis in paper: [inferred] The paper concludes that "while this study focuses on product review data, the evaluation framework can be extended to other domains and data types to assess."
- Why unresolved: The framework was validated only on Amazon product review data, and its generalizability to other tabular data domains remains untested.
- What evidence would resolve it: Application of SynEval to synthetic data from diverse domains (e.g., healthcare records, financial transactions, sensor data) with varying data types and distributions.

### Open Question 3
- Question: What specific fine-tuning techniques could improve LLMs' ability to generate unique categorical features while preserving privacy?
- Basis in paper: [explicit] The paper notes that "without additional fine-tuning, LLMs struggle to maintain the security and uniqueness of data in complex synthetic data generation tasks" and that "categorical features such as product IDs, parent product IDs, and user IDs are largely duplicated within each synthetic dataset."
- Why unresolved: The experiments used out-of-the-box LLMs without exploring fine-tuning approaches to address privacy concerns.
- What evidence would resolve it: Comparative experiments testing different fine-tuning strategies (e.g., differential privacy training, data augmentation, synthetic data filtering) and their impact on privacy metrics while maintaining utility.

## Limitations

- Limited to product review data domain, reducing generalizability to other tabular data types
- High membership inference attack success rates (>80%) suggest either significant privacy leakage or overly permissive attack parameters
- No direct corpus evidence for the proposed evaluation methodology, raising questions about novelty versus established practices

## Confidence

- Fidelity claims: Medium confidence - relies on specific statistical measures without extensive validation across diverse tabular data types
- Utility claims: Medium confidence - assumes sentiment classification accuracy directly translates to general utility
- Privacy claims: Low confidence - consistently high attack success rates suggest either significant leakage or methodological issues

## Next Checks

1. **Cross-dataset validation**: Apply SynEval to diverse tabular datasets (medical, financial, census data) to assess framework robustness beyond product reviews and verify if the three-metric approach maintains validity across data domains.

2. **Baseline comparison study**: Compare SynEval results against established synthetic data evaluation frameworks (e.g., SDMetrics, SDGym) on identical synthetic datasets to quantify relative performance and identify potential metric redundancies or gaps.

3. **Privacy parameter sensitivity analysis**: Systematically vary membership inference attack parameters (classifier type, feature selection, training ratios) to determine if high attack success rates persist across different attack configurations or represent specific methodological choices.