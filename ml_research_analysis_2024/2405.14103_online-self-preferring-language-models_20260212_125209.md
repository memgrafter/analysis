---
ver: rpa2
title: Online Self-Preferring Language Models
arxiv_id: '2405.14103'
source_url: https://arxiv.org/abs/2405.14103
tags:
- preference
- response
- arxiv
- loss
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OSP addresses the problem of improving LLM alignment by leveraging
  preference strength information in human preference datasets. The core method idea
  is to use online self-preferring language models that learn from self-generated
  response pairs and self-judged preference strengths.
---

# Online Self-Preferring Language Models

## Quick Facts
- arXiv ID: 2405.14103
- Source URL: https://arxiv.org/abs/2405.14103
- Authors: Yuanzhao Zhai; Zhuo Zhang; Kele Xu; Hanyang Peng; Yue Yu; Dawei Feng; Cheng Yang; Bo Ding; Huaimin Wang
- Reference count: 40
- Primary result: OSP achieves state-of-the-art alignment performance across various metrics by leveraging preference strength information and online self-preferring

## Executive Summary
OSP addresses the problem of improving LLM alignment by leveraging preference strength information in human preference datasets. The core method idea is to use online self-preferring language models that learn from self-generated response pairs and self-judged preference strengths. OSP proposes a soft-preference cross-entropy (SPCE) loss to effectively utilize preference strength information. The primary results show that OSP achieves state-of-the-art alignment performance across various metrics in two widely used human preference datasets. OSP outperforms existing online and offline alignment methods, is parameter-efficient, more robust when limited offline data are available, and can generalize to out-of-domain tasks.

## Method Summary
OSP is an online alignment method that trains language models using self-generated response pairs and self-judged preference strengths. The method samples K responses for each prompt, ranks them, constructs K//2 response pairs with associated preference strengths, and uses the LLM itself to judge these pairs. The soft-preference cross-entropy (SPCE) loss is then used to train the model based on these self-judged preferences. OSP can be applied to any LLM and improves alignment performance while being parameter-efficient and robust to limited data availability.

## Key Results
- OSP achieves state-of-the-art alignment performance across various metrics in two widely used human preference datasets
- OSP outperforms existing online and offline alignment methods while being parameter-efficient
- OSP language models can effectively generalize to out-of-domain tasks, mitigating reward hacking issues faced by RLHF

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OSP avoids overfitting by leveraging preference strength information instead of binary preferences.
- Mechanism: The soft-preference cross-entropy (SPCE) loss uses continuous preference strength values to create a dynamic margin that adapts to the relative quality of response pairs, preventing the model from pushing all preference predictions to extremes.
- Core assumption: Preference strength information is more informative than binary preferences and helps the model learn nuanced distinctions between responses.
- Evidence anchors:
  - [abstract]: "we propose the soft-preference cross-entropy loss to leverage such information. Empirically, we demonstrate that leveraging preference strength is crucial for avoiding overfitting and enhancing alignment performance."
  - [section 3.1]: "The SPCE loss is utilized to train the model whenever there is a discrepancy between them, enhancing the model's awareness of preference strength. In comparison, the gradient of DPO is as follows: ... DPO trains the model until p(yh ≻ yl|x, πθ) reaches the maximum value 1, which necessitates log( πθ(yh|x)/πθ(yl|x) ) → ∞ . Consequently, DPO is susceptible to overfitting."

### Mechanism 2
- Claim: OSP achieves state-of-the-art alignment performance by using online self-preferring instead of external reward models.
- Mechanism: The LLM judges its own generated response pairs, eliminating the need for a separate reward model and reducing computational overhead while maintaining alignment quality.
- Core assumption: The LLM can effectively judge its own responses when provided with appropriate instructions.
- Evidence anchors:
  - [abstract]: "OSP language models established by LLMs with proficiency in self-preferring can efficiently self-improve without external supervision."
  - [section 3.2]: "To overcome this limitation, we create a seed set of LLM-as-a-judge instructions using an offline human preference dataset. We then enhance the preference capabilities of LLMs using these instructions, finding that even a small amount of data can greatly enhance the LLM-as-a-judge ability."

### Mechanism 3
- Claim: OSP generalizes better to out-of-domain tasks by modeling general preferences instead of substituted rewards.
- Mechanism: By learning preference strength directly rather than approximating it with a reward function, OSP avoids reward hacking and maintains performance on tasks outside the training distribution.
- Core assumption: Reward models are prone to reward hacking when faced with out-of-distribution samples, while preference strength modeling is more robust.
- Evidence anchors:
  - [abstract]: "OSP language models can effectively generalize to OOD tasks, mitigating the reward hacking issue faced by RLHF."
  - [section 4.3]: "In Figure 4(a) and Figure 4(b), methods with 2% dataset achieve better generalization than ones with the full dataset. Notably, RLHF using the reward model trained with the full HH dataset encounters serious reward hacking problems [24], where optimizing an LLM to pursue maximal training rewards provided by the reward model results in high-reward yet low-quality LLMs that do not align with genuine human preferences."

## Foundational Learning

- Concept: Preference strength modeling
  - Why needed here: Binary preferences lose information about how strongly one response is preferred over another, limiting alignment quality.
  - Quick check question: What is the mathematical definition of preference strength used in OSP?

- Concept: Online vs offline learning
  - Why needed here: Online methods can generate new data on-the-fly, improving coverage and quality compared to fixed offline datasets.
  - Quick check question: How does OSP's online approach differ from traditional offline methods like DPO?

- Concept: Self-preferring vs external judgment
  - Why needed here: Using the LLM itself as a judge eliminates the need for separate reward models and reduces computational overhead.
  - Quick check question: What mechanism does OSP use to enhance an LLM's ability to judge its own responses?

## Architecture Onboarding

- Component map:
  Prompt dataset (X) -> Current model πθ (sampling responses) -> Self-preference model (judging response pairs) -> SPCE loss function (alignment optimization) -> SFT model πSFT (normalization/reference)

- Critical path: Prompt → Sample K responses → Rank and pair responses → Self-prefer (judge pairs) → Calculate SPCE loss → Update πθ

- Design tradeoffs:
  - Computational cost vs data coverage: Online sampling requires more computation but generates better coverage
  - Preference augmentation vs direct self-preferring: Small seed data can bootstrap LLM-as-a-judge ability
  - K value selection: Larger K improves coverage but increases computational cost

- Failure signatures:
  - Overfitting to training data (indicated by decreasing SPCE loss but poor evaluation performance)
  - Reward hacking on OOD tasks (high rewards but low quality responses)
  - Poor self-preferring accuracy (inability to distinguish response quality)

- First 3 experiments:
  1. Implement ranked pairing with K=7 and verify preference strength coverage across response pairs
  2. Test self-preferring accuracy on a small seed dataset to validate LLM-as-a-judge capability
  3. Compare SPCE loss convergence and evaluation performance against DPO baseline on 2% dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does OSP perform when scaling to models larger than 7B parameters, and what computational challenges arise?
- Basis in paper: [explicit] The paper explicitly states that OSP language models established by LLMs larger than 7B can be evaluated in future work, indicating this remains untested.
- Why unresolved: Larger models may introduce new challenges in terms of computational cost, memory requirements, and potential changes in the dynamics of self-preferring behavior that weren't observed in the 7B model experiments.
- What evidence would resolve it: Experiments scaling OSP to 13B, 33B, and 70B parameter models, comparing performance metrics (win rates, ROUGE scores) and computational resource usage against smaller models.

### Open Question 2
- Question: What specific mechanisms cause OSP to be more resistant to reward hacking in out-of-domain tasks compared to RLHF?
- Basis in paper: [explicit] The paper demonstrates that OSP language models do not suffer from overoptimization in OOD tasks, while RLHF encounters serious reward hacking problems, but doesn't explain the underlying mechanisms.
- Why unresolved: Understanding the precise reasons for this difference could help improve other alignment methods and inform theoretical understanding of preference modeling versus reward modeling.
- What evidence would resolve it: Detailed analysis of the learned preference distributions in OSP versus reward functions in RLHF, showing how they generalize differently to OOD prompts and why one leads to more robust behavior.

### Open Question 3
- Question: What is the relationship between the number of sampled responses (K) and the quality of preference strength estimation in OSP?
- Basis in paper: [inferred] The paper mentions that larger K values improve performance in the self-improvement setting and discusses ranked pairing, but doesn't systematically study how K affects preference strength estimation quality.
- Why unresolved: The optimal K likely depends on factors like prompt complexity, model size, and available computational resources, making it difficult to determine without systematic study.
- What evidence would resolve it: Experiments varying K from 3 to 25 responses per prompt, measuring how preference strength estimation accuracy (versus human judgments) and alignment performance change with K.

## Limitations
- OSP assumes that LLMs can effectively self-judge their responses, but the quality of self-preferring is not directly measured
- The reliance on a small seed dataset for preference augmentation raises questions about robustness across different LLM architectures
- The method's performance on models larger than 7B parameters remains untested

## Confidence

- **High confidence**: OSP achieves state-of-the-art alignment performance across various metrics in two widely used human preference datasets
- **Medium confidence**: OSP avoids overfitting by leveraging preference strength information instead of binary preferences
- **Medium confidence**: OSP generalizes better to out-of-domain tasks by modeling general preferences instead of substituted rewards

## Next Checks

1. **Self-preferring accuracy validation**: Measure the accuracy of the LLM-as-a-judge on a held-out test set of human-judged response pairs to quantify how well the model can distinguish response quality without external supervision.

2. **Preference strength coverage analysis**: Analyze the distribution of preference strength values generated during online self-preferring to verify that the method produces meaningful and diverse preference information across different response qualities.

3. **Cross-domain generalization test**: Evaluate OSP-trained models on completely different task domains (e.g., code generation, creative writing) that were not present in the original human preference datasets to assess true out-of-domain generalization beyond the tested summarization and helpfulness tasks.