---
ver: rpa2
title: 'AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents'
arxiv_id: '2410.24024'
source_url: https://arxiv.org/abs/2410.24024
tags:
- task
- android
- action
- operation
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ANDROID LAB is a framework for training and evaluating Android
  autonomous agents. It provides a standard operational environment with consistent
  action spaces across text-only and multimodal models, along with a reproducible
  benchmark of 138 tasks across 9 apps.
---

# AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents

## Quick Facts
- arXiv ID: 2410.24024
- Source URL: https://arxiv.org/abs/2410.24024
- Reference count: 40
- Primary result: Framework enabling fair comparison of Android agents with 138 tasks, achieving up to 31.16% success rate

## Executive Summary
AndroidLab introduces a comprehensive framework for training and evaluating Android autonomous agents that supports both text-only (LLM) and multimodal (LMM) models in a unified action space. The framework addresses key challenges in mobile agent development including fair cross-model comparison, reproducible evaluation, and systematic training through the Android Instruct dataset. By providing standardized XML and SoM modes with aligned action spaces, AndroidLab enables consistent benchmarking across different model types while supporting both human-like visual interaction and faster text-based navigation.

## Method Summary
The framework trains Android agents using the Android Instruct dataset containing 10.5k traces and 94.3k steps collected through self-exploration and human annotation. Models are fine-tuned using batch size 32, max sequence length 4096, 5 epochs, and learning rate 1e-5. Evaluation occurs on 138 tasks across 9 apps using fixed Android Virtual Device images with preloaded usage states to ensure reproducibility. The benchmark employs comprehensive metrics including Success Rate, Sub-Goal Success Rate, Reversed Redundancy Ratio, and Reasonable Operation Ratio to assess agent performance across different evaluation dimensions.

## Key Results
- Fine-tuning with Android Instruct dataset increased open-source model success rates from 4.59% to 21.50% for LLMs and from 1.93% to 13.28% for LMMs
- Closed-source models like GPT-4o achieved up to 31.16% success rate, with fine-tuned open-source models approaching this performance
- The framework enabled fair cross-model comparison by aligning action spaces across XML and SoM modes
- Models showed optimal performance on screens matching commonly used smartphones, with performance degradation on different screen sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AndroidLab enables fair cross-model comparison by aligning action spaces across XML and SoM modes.
- Mechanism: By assigning the same set of action primitives (Tap, Swipe, Type, Long Press, Home, Back, Finish) to both XML and SoM modes, and mapping identical UI elements across these representations, the framework ensures that model differences reflect capability rather than interface mismatch.
- Core assumption: UI element identifiers are consistently extractable from both XML trees and screenshot-based set-of-marks.
- Evidence anchors:
  - [abstract] "It supports both large language models (LLMs) and multimodal models (LMMs) in the same action space."
  - [section 3.1] "The selected elements in SoM mode align with those in the compressed XML list, allowing both modes to interact with the same action space and objects."
  - [corpus] Weak - no direct citations, but similar alignment strategies appear in AndroidWorld (2405.14573) and Android in the Zoo (2403.02713).

### Mechanism 2
- Claim: AndroidLab's reproducible benchmark design removes environmental variance that typically confounds mobile agent evaluation.
- Mechanism: By using fixed Android Virtual Device images, preloaded app usage states, and offline testing, the benchmark eliminates network, time, and device variability.
- Core assumption: The virtual device snapshot fully captures the runtime state needed to evaluate all tasks.
- Evidence anchors:
  - [section 3.2.2] "Using phone XML data, we identify screen information that uniquely defines task completion... By checking and matching specific UI tree elements, we assess each sub-goal completion status individually."
  - [section 3.2.2] "We use ADB commands at the start of each evaluation to set the machine's time and virtual geolocation to predetermined values."
  - [corpus] Moderate - AndroidWorld (2405.14573) uses reward signals but not fixed device snapshots; AutoGLM (2411.00820) focuses on GUI agents without fixed device states.

### Mechanism 3
- Claim: Instruction tuning with AndroidInstruct dataset significantly improves open-source model performance by providing aligned XML-SoM training pairs.
- Mechanism: The dataset contains 10.5k traces with 94.3k steps, each step recorded in both XML and SoM formats, enabling models to learn the mapping between text-based and visual representations of UI states.
- Core assumption: The alignment between XML and SoM representations is accurate and complete enough for the model to learn the mapping.
- Evidence anchors:
  - [abstract] "Android Instruct dataset, which contains 10.5k traces and 94.3k steps collected through self-exploration and human annotation."
  - [section 4.1] "Using self-exploration and manual annotation, we generate example operation traces."
  - [section 5.2] "Fine-tuning with our dataset raises average success rates from 4.59% to 21.50% for LLMs and from 1.93% to 13.28% for LMMs."
  - [corpus] Strong - Android in the Zoo (2403.02713) also uses human-annotated traces but lacks aligned multimodal data.

## Foundational Learning

- Concept: UI element identification from structured data (XML) vs unstructured data (screenshots).
  - Why needed here: AndroidLab uses both XML and SoM modes; understanding how to parse and map elements in each is essential for implementing the framework.
  - Quick check question: Given an XML node with bounds [100,200][300,400], how would you identify the corresponding element in a screenshot-based set-of-marks?

- Concept: Action space design and API specification.
  - Why needed here: The framework defines a specific set of actions (Tap, Swipe, Type, etc.) that all models must use; understanding their semantics and parameters is critical for correct implementation.
  - Quick check question: What parameters are required for a "Swipe" action in AndroidLab's action space?

- Concept: Task decomposition and sub-goal evaluation.
  - Why needed here: AndroidLab evaluates tasks by breaking them into sub-goals and checking UI state changes; understanding this process is key to interpreting benchmark results.
  - Quick check question: How does AndroidLab determine whether a sub-goal like "alarm set" has been completed?

## Architecture Onboarding

- Component map: Environment (Android Virtual Device) -> Benchmark (Task definitions) -> Data Collection (Annotation tool) -> Models (Fine-tuned LLMs/LMMs) -> Evaluation (Success metrics)
- Critical path: Task definition → Data collection (XML + SoM) → Model training/fine-tuning → Evaluation with fixed metrics
- Design tradeoffs:
  - Fixed vs dynamic device states: Fixed states ensure reproducibility but limit task variety
  - XML vs SoM: XML is faster but less intuitive; SoM is more human-like but requires image processing
  - Manual vs automated annotation: Manual ensures quality but is expensive; automated is cheaper but may have errors
- Failure signatures:
  - XML extraction fails: Check ADB permissions and device compatibility
  - SoM element detection fails: Verify screenshot quality and set-of-marks generation
  - Task evaluation gives false negatives: Review sub-goal definitions and UI state matching logic
- First 3 experiments:
  1. Implement basic XML mode with one app (e.g., Clock) and verify action execution
  2. Add SoM mode for the same app and confirm element alignment between XML and SoM
  3. Create a small benchmark with 5 tasks and evaluate a simple LLM to verify the full pipeline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would models perform if trained on a larger, more diverse Android instruction dataset beyond the current 10.5k traces?
- Basis in paper: [explicit] The paper shows fine-tuning with the Android Instruct dataset significantly improves model performance, suggesting potential for further gains with more data.
- Why unresolved: The current dataset size may be limiting model capabilities, and the paper only explores the impact of this specific dataset.
- What evidence would resolve it: Training models on a substantially larger dataset (e.g., 100k+ traces) and comparing performance metrics to the current results would show the impact of dataset scale.

### Open Question 2
- Question: Would using alternative representation methods (beyond XML and SoM) improve model performance on Android tasks?
- Basis in paper: [inferred] The paper focuses on XML and SoM modes, but doesn't explore other potential representations like semantic parsing or hybrid approaches.
- Why unresolved: The paper establishes XML and SoM as standard but doesn't investigate if other representations could be more effective.
- What evidence would resolve it: Comparing model performance using different representation methods (e.g., semantic parsing, hybrid XML+semantic approaches) on the same benchmark tasks.

### Open Question 3
- Question: How do Android agents generalize to real-world devices with different screen sizes, resolutions, and UI variations compared to the controlled virtual device environment?
- Basis in paper: [explicit] The paper mentions optimal performance on screens matching commonly used smartphones and notes performance drops on different screen sizes.
- Why unresolved: The benchmark uses standardized virtual devices, but real-world deployment involves significant device diversity not captured in the current evaluation.
- What evidence would resolve it: Testing models on a wide range of actual Android devices with varying specifications and comparing performance to the virtual device results would reveal generalization capabilities.

## Limitations
- The benchmark's fixed device states and preloaded app usage may not capture the full complexity of real-world Android interactions
- The XML-SoM alignment mechanism relies on perfect UI element mapping that may not hold across different Android versions or device manufacturers
- Success metrics may over-penalize models that take reasonable but non-optimal action sequences

## Confidence
- High confidence in framework design and benchmark reproducibility
- Medium confidence in performance improvements from instruction tuning
- Low confidence in long-term generalizability to real-world scenarios

## Next Checks
1. Cross-device validation: Test the framework across different Android versions and device manufacturers to verify that XML-SoM alignment holds consistently, measuring element mapping accuracy rates.
2. Dynamic content evaluation: Extend the benchmark to include tasks requiring live data interactions (e.g., weather apps, stock tickers) to assess reproducibility guarantees under dynamic conditions.
3. Real-world deployment: Deploy fine-tuned models in actual user environments with varying network conditions and usage patterns, measuring success rate degradation compared to benchmark performance.