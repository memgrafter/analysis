---
ver: rpa2
title: 'ODIN: A Single Model for 2D and 3D Segmentation'
arxiv_id: '2401.02416'
source_url: https://arxiv.org/abs/2401.02416
tags:
- point
- segmentation
- cloud
- instance
- odin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ODIN, a unified transformer model that can
  perform both 2D and 3D instance segmentation using the same architecture. The key
  innovation is alternating between 2D within-view and 3D cross-view fusion operations,
  with the model differentiating between 2D and 3D feature operations through positional
  encodings of tokens (2D pixel coordinates vs 3D coordinates).
---

# ODIN: A Single Model for 2D and 3D Segmentation

## Quick Facts
- arXiv ID: 2401.02416
- Source URL: https://arxiv.org/abs/2401.02416
- Reference count: 40
- Primary result: Unified transformer model achieving state-of-the-art performance on 2D and 3D instance segmentation benchmarks

## Executive Summary
ODIN introduces a novel unified transformer architecture that performs both 2D and 3D instance segmentation using the same model. The key innovation is an alternating fusion strategy that interleaves 2D within-view and 3D cross-view attention layers, allowing the model to leverage pre-trained 2D features while adapting them for 3D tasks. By using different positional encodings (pixel coordinates for 2D, XYZ coordinates for 3D), the model differentiates between modalities without requiring separate architectures. ODIN achieves state-of-the-art performance on multiple 3D benchmarks including ScanNet200, Matterport3D, and AI2THOR, while maintaining competitive performance on 2D datasets like COCO.

## Method Summary
ODIN uses a unified transformer architecture with alternating 2D within-view and 3D cross-view fusion layers. The model processes single RGB images or posed RGB-D image sequences, with 2D features from a pre-trained backbone (ResNet50 or Swin-B) being unprojected to 3D using depth maps and camera parameters. A shared Mask2Former-like decoder with learnable object queries produces instance segmentations. The model differentiates between 2D and 3D operations through positional encodings—relative pixel coordinates for 2D tokens and relative XYZ coordinates for 3D tokens. Joint training on both 2D and 3D datasets enables the model to learn shared representations while maintaining modality-specific feature operations.

## Key Results
- Achieves state-of-the-art performance on ScanNet200, Matterport3D, and AI2THOR 3D instance segmentation benchmarks
- Outperforms all previous works when using sensed 3D point clouds instead of post-processed mesh point clouds
- Sets new state-of-the-art on TEACh action-from-dialogue benchmark when used as 3D perception engine
- Demonstrates competitive performance on ScanNet, S3DIS, and COCO while being trained jointly

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Alternating 2D within-view and 3D cross-view fusion enables effective joint training and leverages 2D pre-trained features for 3D tasks.
- Mechanism: The model interleaves 2D backbone processing with 3D cross-view fusion layers, allowing the 2D backbone to adapt to 3D features while maintaining 2D pre-training benefits.
- Core assumption: 2D pre-trained features can be effectively adapted to 3D tasks through interleaving fusion operations.
- Evidence anchors:
  - [abstract] "ODIN achieves state-of-the-art performance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation benchmarks, and competitive performance on ScanNet, S3DIS and COCO."
  - [section] "ODIN alternates between a 2D within-view fusion and a 3D attention-based cross-view fusion... Our model differentiates between 2D and 3D features through the positional encodings of the tokens involved."
  - [corpus] Weak - corpus neighbors don't directly address alternating fusion mechanisms
- Break condition: If cross-view fusion is removed, mAP drops significantly (8.5% drop observed in ablation).

### Mechanism 2
- Claim: Using positional encodings that capture 2D pixel coordinates for 2D tokens and 3D coordinates for 3D tokens allows the same transformer architecture to handle both modalities.
- Mechanism: Positional encodings are computed differently for 2D (pixel coordinates) and 3D (XYZ coordinates) tokens, enabling the model to distinguish between them without separate architectures.
- Core assumption: Positional encodings are sufficient to differentiate 2D and 3D tokens within the same transformer architecture.
- Evidence anchors:
  - [abstract] "Our model differentiates 2D and 3D feature operations through the positional encodings of the tokens involved, which capture pixel coordinates for 2D patch tokens and 3D coordinates for 3D feature tokens."
  - [section] "The positional embeddings in this operation are relative to the query token's location... In this way, the attention operation is invariant to the absolute coordinates of the 3D tokens and only depends on their relative spatial arrangements."
  - [corpus] Weak - corpus neighbors don't directly address positional encoding strategies
- Break condition: If positional encodings are not used or are identical for 2D and 3D, the model cannot distinguish between modalities.

### Mechanism 3
- Claim: Joint training on 2D and 3D datasets improves 3D perception while maintaining reasonable 2D performance.
- Mechanism: The model is trained on both 2D RGB images and 3D RGB-D data using an open-vocabulary semantic class-decoding head, allowing it to learn shared representations.
- Core assumption: 2D and 3D perception tasks can benefit from shared representations when trained jointly.
- Evidence anchors:
  - [section] "Joint 2D-3D training helps 3D perception... Results in Tab. 4 show that joint training yields a 1.3% absolute improvement in 3D, and causes a similar drop in 2D."
  - [section] "We compare joint training of ODIN on sensor RGB-D point clouds from ScanNet and 2D RGB images from COCO to variants trained independently on 2D and 3D data."
  - [corpus] Weak - corpus neighbors don't directly address joint 2D-3D training benefits
- Break condition: If joint training is not used, 3D performance is lower and 2D performance is slightly better but without the 3D benefit.

## Foundational Learning

- Concept: Transformer architecture with self-attention and cross-attention mechanisms
  - Why needed here: ODIN uses transformer-based architecture for both 2D and 3D feature fusion
  - Quick check question: Can you explain how self-attention and cross-attention work in transformers?

- Concept: Positional encodings in transformers
  - Why needed here: ODIN uses different positional encodings for 2D (pixel coordinates) and 3D (XYZ coordinates) tokens
  - Quick check question: How do positional encodings help transformers understand spatial relationships in data?

- Concept: 3D point cloud processing and voxelization
  - Why needed here: ODIN unprojects 2D features to 3D and uses voxelization for cross-view fusion
  - Quick check question: What is voxelization and why is it useful for 3D point cloud processing?

## Architecture Onboarding

- Component map: RGB-D images -> 2D Backbone (ResNet50/Swin-B) -> Unprojection -> 3D Fusion -> Projection -> 2D Fusion -> Segmentation Decoder

- Critical path: RGB-D images → 2D Backbone → Unprojection → 3D Fusion → Projection → 2D Fusion → Segmentation Decoder

- Design tradeoffs:
  - Using pre-trained 2D features vs training from scratch on 3D data
  - Interleaving 2D and 3D fusion vs separate processing
  - Sensor RGB-D input vs mesh-sampled point clouds

- Failure signatures:
  - Poor 2D performance: Check 2D backbone initialization and training
  - Poor 3D performance: Check 3D fusion layers and unprojection/projection modules
  - Misalignment issues: Check depth maps and camera parameters

- First 3 experiments:
  1. Train with 2D backbone frozen, then unfrozen - observe performance difference
  2. Remove 3D cross-view fusion layers - measure mAP drop
  3. Train on 2D data only vs joint 2D-3D training - compare 3D performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the performance gap between models using sensor RGB-D point clouds and those using mesh-sampled point clouds be fully eliminated through improved depth estimation and camera pose refinement?
- Basis in paper: [explicit] The paper notes that misalignments between the 3D mesh and sensor point cloud cause performance drops, and mentions this as a limitation.
- Why unresolved: The paper suggests this is a limitation but doesn't explore whether improved depth and pose estimation could fully solve it.
- What evidence would resolve it: Experimental results showing whether models using sensor RGB-D point clouds can match or exceed the performance of models using mesh-sampled point clouds when provided with highly accurate depth and camera poses.

### Open Question 2
- Question: How does the performance of ODIN scale with increasing context views, and is there a point of diminishing returns?
- Basis in paper: [explicit] The paper shows performance increases with more context views in AI2THOR experiments, but doesn't explore the full scaling behavior or identify when additional views stop providing benefits.
- Why unresolved: The experiments only tested up to all images in a scene, without exploring whether performance plateaus or how it scales with even more context views.
- What evidence would resolve it: A comprehensive study varying the number of context views across multiple datasets to identify the scaling relationship and point of diminishing returns.

### Open Question 3
- Question: Can the competition between 2D and 3D segmentation performance during joint training be resolved through architectural modifications?
- Basis in paper: [explicit] The paper notes a performance drop in 2D segmentation when jointly training with 3D data, suggesting this as a future work direction.
- Why unresolved: The paper doesn't explore architectural modifications that could make 2D and 3D training more synergistic.
- What evidence would resolve it: Experimental results showing whether specific architectural changes (e.g., task-specific feature sharing, adaptive fusion strategies) can eliminate the performance trade-off between 2D and 3D segmentation during joint training.

## Limitations

- The performance gap between sensor RGB-D point clouds and mesh-sampled point clouds remains due to potential depth estimation and camera pose inaccuracies
- Joint 2D-3D training causes a performance drop in 2D segmentation while improving 3D perception
- The model requires posed RGB-D image sequences, limiting its applicability to scenarios where camera poses are unavailable

## Confidence

- **High Confidence**: The core architectural contribution (alternating 2D/3D fusion) and the state-of-the-art results on ScanNet200, Matterport3D, and AI2THOR benchmarks are well-supported by the empirical evidence presented.
- **Medium Confidence**: The mechanism by which positional encodings differentiate 2D and 3D tokens is plausible and supported by results, but could benefit from more detailed analysis of how the model learns to interpret these encodings.
- **Medium Confidence**: The claim that joint 2D-3D training improves 3D perception while maintaining reasonable 2D performance is supported by the ablation study, but the long-term effects and potential trade-offs warrant further investigation.

## Next Checks

1. **Ablation on Fusion Layer Configuration**: Systematically vary the number and arrangement of 2D within-view and 3D cross-view fusion layers to identify the optimal configuration and understand the contribution of each component to overall performance.

2. **Alternative Positional Encoding Schemes**: Replace the current positional encoding strategy with alternative approaches (e.g., absolute coordinates, learned positional embeddings) to verify that the observed performance gains are specifically due to the relative coordinate-based encoding scheme.

3. **Cross-Dataset Generalization**: Evaluate ODIN on additional 3D datasets beyond ScanNet200, Matterport3D, and AI2THOR to assess its generalization capabilities and identify potential dataset-specific biases in the reported performance gains.