---
ver: rpa2
title: A safety realignment framework via subspace-oriented model fusion for large
  language models
arxiv_id: '2405.09055'
source_url: https://arxiv.org/abs/2405.09055
tags:
- safety
- arxiv
- realignment
- task
- somf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a safety realignment framework via subspace-oriented
  model fusion (SOMF) to address the safety degradation of large language models (LLMs)
  after fine-tuning on downstream tasks. The core idea is to identify safety-related
  regions within task vectors through subspace masking and then fuse these masked
  task vectors with the initially aligned model to restore safety.
---

# A safety realignment framework via subspace-oriented model fusion for large language models

## Quick Facts
- arXiv ID: 2405.09055
- Source URL: https://arxiv.org/abs/2405.09055
- Authors: Xin Yi; Shunfan Zheng; Linlin Wang; Xiaoling Wang; Liang He
- Reference count: 40
- Key outcome: SOMF improves safety of fine-tuned LLMs while preserving task performance across Chinese, English, Hindi, Code, and Math tasks

## Executive Summary
This paper introduces a safety realignment framework that addresses the degradation of safety in large language models after fine-tuning on downstream tasks. The approach uses subspace-oriented model fusion (SOMF) to identify safety-related regions within task vectors through subspace masking, then fuses these masked vectors with initially aligned models to restore safety. The framework is tested across multiple languages and domains, showing significant improvements in harmlessness metrics while maintaining task-specific performance.

## Method Summary
The framework constructs task vectors from fine-tuned models and uses Gumbel-Concrete relaxation to learn continuous masks that identify safety-related parameters through preference data optimization. These masks are applied to task vectors to isolate safety-critical regions, which are then fused with safety-aligned base models using various fusion techniques including Task Arithmetic, TIES-Merging, and Weight Averaging. The approach enables selective preservation of safety knowledge while allowing task-specific updates.

## Key Results
- Achieved 86.24% harmlessness preference rate on CATQA dataset
- Outperformed initially aligned models and other safety realignment methods
- Preserved downstream task performance across multiple languages (Chinese, English, Hindi) and domains (Code, Math)
- Demonstrated effectiveness for both single-task and multi-task realignment scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Subspace masking isolates safety-related parameters within task vectors, enabling selective preservation of safety knowledge while allowing task-specific updates.
- Mechanism: The framework uses Gumbel-Concrete relaxation to learn a continuous mask over task vectors, identifying which parameters are associated with safety. This mask is optimized using preference data (safe vs unsafe responses) and then applied to create a "masked" task vector that excludes safety-critical parameters from downstream updates.
- Core assumption: Safety-relevant parameters in task vectors can be identified through differentiable optimization using preference data.
- Evidence anchors:
  - [abstract] "identify safety-related regions within task vectors by subspace masking techniques"
  - [section] "leveraging the publicly available safety pairwise preference data to train a mask capable of representing the safety subspace"
  - [corpus] Weak - the related papers focus on post-hoc alignment but don't detail subspace identification methods
- Break condition: If safety parameters are distributed non-locally or if preference data doesn't capture the full safety landscape, the mask may incorrectly preserve unsafe parameters or discard safe ones.

### Mechanism 2
- Claim: Model fusion can combine initially aligned safety parameters with task-specific knowledge without catastrophic forgetting.
- Mechanism: After masking task vectors to isolate safety regions, the framework fuses the masked task vector with the initially aligned model using techniques like Task Arithmetic or TIES-Merging. This allows the model to retain safety knowledge while incorporating task-specific capabilities.
- Core assumption: The initially aligned model's safety parameters remain valid and useful after task-specific fine-tuning.
- Evidence anchors:
  - [abstract] "combine the safeguard capabilities of initially aligned model and the current fine-tuned model into a realigned model"
  - [section] "explore the fusion of the initial safely aligned LLM with all task vectors based on the identified safety subspace"
  - [corpus] Moderate - some papers discuss task arithmetic fusion but don't address safety preservation specifically
- Break condition: If the initially aligned safety parameters become obsolete or if task-specific fine-tuning fundamentally changes the model's behavior, simple fusion may not restore adequate safety.

### Mechanism 3
- Claim: Subspace-oriented fusion works across multiple tasks by identifying shared safety parameters across task vectors.
- Mechanism: The framework can identify safety-related regions that are common across multiple task vectors (from different fine-tuned models) and use these shared parameters during fusion to maintain safety across all tasks simultaneously.
- Core assumption: There exist common safety-related parameter regions across different task-specific models that can be identified and preserved.
- Evidence anchors:
  - [abstract] "Our SOMF is inherently well-suited for identifying safety-related shared regions in task vectors corresponding to multiple task-specific models"
  - [section] "Our investigation delves into two pivotal scenarios: (1) the instance where a lone task-specific model undergoes safety realignment, and (2) the more intricate scenario where in multiple task-specific models are securely realigned and fused into a multi-task model"
  - [corpus] Weak - related papers focus on individual model safety but not multi-model fusion scenarios
- Break condition: If different tasks corrupt safety parameters in fundamentally different ways, there may be no meaningful shared safety subspace to preserve.

## Foundational Learning

- Concept: Gumbel-Concrete relaxation for differentiable sampling
  - Why needed here: Enables gradient-based optimization of discrete mask selection for safety parameter identification
  - Quick check question: How does the Concrete distribution approximate discrete Bernoulli sampling in a differentiable way?

- Concept: Preference optimization (DPO) for safety alignment
  - Why needed here: Provides the optimization framework for learning safety-relevant parameter masks
  - Quick check question: What is the key difference between DPO and traditional RLHF in terms of computational efficiency?

- Concept: Task arithmetic and model merging techniques
  - Why needed here: Forms the basis for combining masked task vectors with the aligned model
  - Quick check question: How does Task Arithmetic differ from Weight Averaging in model merging?

## Architecture Onboarding

- Component map: Safety subspace identification -> Task vector construction -> Model fusion engine -> Evaluation pipeline
- Critical path:
  1. Construct task vectors from fine-tuned models
  2. Train safety mask using preference data
  3. Apply mask to create masked task vectors
  4. Fuse masked vectors with aligned model
  5. Evaluate safety and task performance

- Design tradeoffs:
  - Binary mask vs continuous mask: Binary provides clear separation but less gradient information; continuous allows smoother optimization but may require thresholding
  - Choice of fusion method: Task Arithmetic is simple but may cause interference; TIES-Merging reduces interference but adds complexity
  - Preference data quality: More diverse preference data improves mask accuracy but increases computational cost

- Failure signatures:
  - Safety degradation: Mask not properly identifying safety parameters or fusion method not preserving them
  - Task performance loss: Over-aggressive masking removing too many task-specific parameters
  - Instability: Poor choice of hyperparameters (learning rate, mask temperature) causing optimization issues

- First 3 experiments:
  1. Single task safety realignment using PEFT strategy on a simple instruction-following dataset
  2. Multi-task fusion with 2-3 tasks using different fusion methods
  3. Ablation study comparing continuous vs binary masks on safety preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality and representativeness of the safety-related preference data pairs used for training impact the model's performance in understanding safety concerns?
- Basis in paper: [explicit] The authors acknowledge that the quality of the safety-related preference data pairs is crucial for training reliable and effective models.
- Why unresolved: The authors do not provide a detailed analysis of how different qualities of training data affect the model's safety performance.
- What evidence would resolve it: A study comparing the performance of models trained on safety preference datasets of varying quality and representativeness.

### Open Question 2
- Question: How does the SOMF method perform on larger language models beyond the 7B parameter models tested in this study?
- Basis in paper: [explicit] The authors mention that their experiments are constrained by computing resources and focus on 7B parameter models, leaving potential improvements for larger models unexplored.
- Why unresolved: The computational limitations prevented the authors from testing the method on larger models.
- What evidence would resolve it: Experiments applying the SOMF method to language models with parameter sizes exceeding 7B.

### Open Question 3
- Question: What is the impact of subspace-oriented model fusion for safety realignment on larger language models, and how does it compare to its effectiveness on smaller models?
- Basis in paper: [inferred] The authors mention that their experiments are constrained by computing resources and focus on 7B parameter models, leaving potential improvements for larger models unexplored.
- Why unresolved: The computational limitations prevented the authors from testing the method on larger models.
- What evidence would resolve it: Experiments applying the SOMF method to language models with parameter sizes exceeding 7B and comparing the results with those obtained on smaller models.

## Limitations

- Computational constraints limited testing to 7B parameter models, leaving scalability to larger models unexplored
- Reliance on preference data quality introduces uncertainty about mask effectiveness across different safety scenarios
- Assumes safety parameters can be effectively identified through differentiable optimization, which may not hold for complex safety landscapes

## Confidence

- **High Confidence**: Experimental results showing SOMF outperforms baseline safety realignment methods on harmlessness metrics (86.24% harmlessness preference rate on CATQA) and preserves downstream task performance across multiple languages and domains
- **Medium Confidence**: Claim that subspace masking can identify shared safety parameters across multiple task vectors, requiring strong assumptions about parameter locality
- **Low Confidence**: Generalizability of results to models beyond the tested architectures (WizardLM-7B-Uncensored, TinyLlama-1.1B-Chat-v1.0), as different model families may have different parameter sensitivities

## Next Checks

1. **Robustness to Preference Data Quality**: Conduct ablation studies varying the diversity and size of safety preference datasets to quantify how mask quality degrades with reduced preference data coverage.

2. **Cross-Architecture Transferability**: Test SOMF on additional model families (e.g., Llama, Mistral) to validate whether the subspace masking approach generalizes beyond the specific base models used in experiments.

3. **Long-Tail Safety Scenarios**: Evaluate realigned models on adversarial safety prompts and long-tail harmful scenarios not present in training preference data to assess whether masked fusion creates blind spots in safety coverage.