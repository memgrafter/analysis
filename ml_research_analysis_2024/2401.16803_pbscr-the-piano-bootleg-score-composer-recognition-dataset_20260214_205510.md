---
ver: rpa2
title: 'PBSCR: The Piano Bootleg Score Composer Recognition Dataset'
arxiv_id: '2401.16803'
source_url: https://arxiv.org/abs/2401.16803
tags:
- music
- bootleg
- composer
- score
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents the Piano Bootleg Score Composer Recognition
  (PBSCSR) dataset, a large-scale dataset for studying composer style recognition
  in classical piano music. The dataset contains 40,000 62x64 bootleg score images
  for a 9-class task, 100,000 images for a 100-class task, and 29,310 unlabeled variable-length
  bootleg score images for pretraining.
---

# PBSCR: The Piano Bootleg Score Composer Recognition Dataset

## Quick Facts
- arXiv ID: 2401.16803
- Source URL: https://arxiv.org/abs/2401.16803
- Authors: Arhan Jain; Alec Bunn; Austin Pham; TJ Tsai
- Reference count: 9
- The paper presents the Piano Bootleg Score Composer Recognition (PBSCSR) dataset, a large-scale dataset for studying composer style recognition in classical piano music.

## Executive Summary
The Piano Bootleg Score Composer Recognition (PBSCSR) dataset is a large-scale contribution to the field of composer style recognition in classical piano music. The dataset comprises 40,000 62x64 bootleg score images for a 9-class task, 100,000 images for a 100-class task, and 29,310 unlabeled variable-length bootleg score images for pretraining. Bootleg scores encode notehead positions relative to staff lines, providing a compact representation for rapid exploration. The authors demonstrate baseline results using CNN, GPT-2, and RoBERTa models, showing that pretraining on unlabeled data significantly improves performance, with the best GPT-2 model achieving a top-5 accuracy of 34.8% on the challenging 100-way task. The dataset is designed to facilitate research on composer style recognition with potential applications in few-shot and zero-shot learning, multimodal learning, and encoding scheme exploration.

## Method Summary
The PBSCSR dataset utilizes bootleg score images as its primary representation format, encoding the position of noteheads relative to staff lines in a compact 62x64 format. The dataset is divided into three main components: 40,000 labeled images for a 9-class composer recognition task, 100,000 labeled images for an extended 100-class task, and 29,310 unlabeled variable-length images for pretraining purposes. The authors provide baseline results using three different model architectures - CNN, GPT-2, and RoBERTa - demonstrating that pretraining on the unlabeled data significantly improves performance across all models. The dataset's design specifically targets composer style recognition while maintaining simplicity and scalability for rapid experimentation.

## Key Results
- The dataset contains 40,000 labeled images for 9-class composer recognition and 100,000 images for 100-class recognition
- Pretraining on 29,310 unlabeled images significantly improves model performance
- Best GPT-2 model achieves 34.8% top-5 accuracy on the challenging 100-way classification task

## Why This Works (Mechanism)
The bootleg score representation captures essential musical features through the spatial positioning of noteheads relative to staff lines, encoding composer-specific stylistic patterns that are detectable by machine learning models. The dataset's large scale, particularly the inclusion of unlabeled pretraining data, enables models to learn robust representations of musical style before fine-tuning on specific composer classes. The combination of multiple model architectures (CNN for visual patterns, GPT-2 and RoBERTa for sequence modeling) demonstrates that composer style can be recognized through different computational approaches, suggesting multiple viable pathways for solving this task.

## Foundational Learning
- **Bootleg Score Representation**: A compact encoding of musical notation showing notehead positions relative to staff lines - needed for efficient data storage and processing; quick check: verify 62x64 image dimensions preserve musical information
- **Composer Style Recognition**: Identifying composers based on musical patterns and characteristics - needed as the core task; quick check: ensure dataset covers diverse stylistic periods and composers
- **Pretraining in Music Information Retrieval**: Using large unlabeled datasets to improve model performance - needed for better generalization; quick check: compare performance with and without pretraining
- **Multi-class Classification**: Handling tasks with many possible output classes (100 composers) - needed for realistic composer recognition; quick check: verify class balance across composers
- **Symbolic Music Representation**: Encoding musical information in non-audio formats - needed for computational analysis; quick check: validate that bootleg scores capture essential musical features
- **Transfer Learning**: Applying knowledge from pretraining to downstream tasks - needed for efficient model development; quick check: measure performance gains from pretraining

## Architecture Onboarding

**Component Map**: Bootleg Score Images -> CNN/GPT-2/RoBERTa -> Composer Classification

**Critical Path**: The critical path involves loading bootleg score images, processing through the chosen model architecture (CNN for spatial patterns, GPT-2/RoBERTa for sequential patterns), and outputting composer predictions. Pretraining on unlabeled data precedes fine-tuning on labeled data.

**Design Tradeoffs**: The bootleg score format prioritizes compactness and simplicity over complete musical fidelity, potentially losing some expressive details but enabling rapid experimentation. The choice of three diverse model architectures allows for comparison but requires more computational resources than a single approach.

**Failure Signatures**: Poor performance may indicate inadequate pretraining, insufficient model capacity for the 100-class task, or loss of critical musical information in the bootleg score representation. Imbalanced class representation or insufficient stylistic diversity could also lead to poor generalization.

**First Experiments**: 1) Train CNN baseline without pretraining to establish performance floor; 2) Train GPT-2 with pretraining to measure improvement; 3) Compare top-1 vs top-5 accuracy to understand model confidence.

## Open Questions the Paper Calls Out
None

## Limitations
- The dataset focuses on a single representation format (bootleg scores) limiting exploration of alternative encodings
- The composer pool is relatively small (9 composers in main task, 100 in extended task)
- Baseline results may not represent optimal configurations for the task
- Top-5 accuracy of 34.8% on 100-way task indicates substantial room for improvement

## Confidence
- Dataset contribution: High - well-documented data collection and size
- Baseline results: Medium - limited architectural exploration and lack of state-of-the-art comparisons
- Application claims: Low - few-shot and multimodal learning potential not empirically validated

## Next Checks
1. Benchmark additional state-of-the-art architectures (e.g., vision transformers, modern CNN variants) on the dataset to establish stronger baselines.
2. Conduct cross-validation studies using different train/validation/test splits to verify result stability.
3. Perform ablation studies on the bootleg score representation to quantify its impact on performance compared to alternative symbolic music representations.