---
ver: rpa2
title: 'No More Adam: Learning Rate Scaling at Initialization is All You Need'
arxiv_id: '2412.11768'
source_url: https://arxiv.org/abs/2412.11768
tags:
- learning
- adam
- gradient
- rate
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the necessity of adaptive gradient methods
  like Adam for training deep neural networks, particularly Transformers. It proposes
  SGD-SaI, a memory-efficient optimization method that scales learning rates at initialization
  based on gradient signal-to-noise ratio (g-SNR) across parameter groups.
---

# No More Adam: Learning Rate Scaling at Initialization is All You Need

## Quick Facts
- arXiv ID: 2412.11768
- Source URL: https://arxiv.org/abs/2412.11768
- Reference count: 40
- Primary result: Achieves comparable performance to AdamW while reducing memory usage by 50% and speeding up optimization by 3x

## Executive Summary
This paper challenges the necessity of adaptive gradient methods like Adam for training deep neural networks, particularly Transformers. It proposes SGD-SaI, a memory-efficient optimization method that scales learning rates at initialization based on gradient signal-to-noise ratio (g-SNR) across parameter groups. Unlike Adam, SGD-SaI eliminates the need for storing and updating second-order momentum terms, reducing memory usage by 50% compared to AdamW and 75% compared to Prodigy. The method achieves comparable or better performance than AdamW on ImageNet classification with ViTs, GPT-2 pretraining, and LoRA fine-tuning tasks, while being significantly faster in optimization step time (3x faster than Adam-mini for GPT-2). The approach demonstrates robustness to hyperparameter variations and maintains simplicity by working with PyTorch's default parameter partitioning. Empirical analysis shows that g-SNR remains stable during training and varies predictably across different parameter types, enabling effective learning rate scaling without dynamic adaptation.

## Method Summary
SGD-SaI is an optimization method that scales learning rates at initialization based on gradient signal-to-noise ratio (g-SNR) across parameter groups. The method calculates g-SNR as the ratio of gradient norm to gradient variance for each parameter block at initialization, then uses these precomputed values to scale learning rates throughout training. Unlike AdamW, SGD-SaI eliminates second-order momentum terms, reducing memory usage by 50%. The method works with PyTorch's default parameter partitioning and requires only minimal modifications to the existing SGD optimizer. It combines this with decoupled weight decay and momentum-based updates, achieving comparable performance to AdamW while being 3x faster in optimization step time and significantly more memory-efficient.

## Key Results
- Memory usage reduced by 50% compared to AdamW and 75% compared to Prodigy
- Optimization step time 3x faster than Adam-mini for GPT-2 pretraining
- Comparable or better performance than AdamW on ImageNet classification with ViTs, GPT-2 pretraining, and LoRA fine-tuning tasks
- g-SNR remains stable during training and varies predictably across different parameter types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Precomputing gradient signal-to-noise ratio (g-SNR) at initialization captures stable architectural characteristics that predict optimal learning rate scaling.
- Mechanism: The g-SNR is calculated as the ratio of gradient norm to gradient variance for each parameter block. This ratio remains relatively constant during training because parameter updates are small and gradient distribution shapes are preserved.
- Core assumption: Gradient sparsity and noise patterns are primarily determined by architecture rather than data distribution or initialization specifics.
- Evidence anchors: [abstract]: "g-SNR remains stable during training and varies predictably across different parameter types"; [section 4.2]: "we observed that g-SNR remains relatively stable over time while exhibiting distinct patterns across different parameter classes"

### Mechanism 2
- Claim: Parameter block partitioning based on PyTorch defaults captures sufficient granularity for effective learning rate scaling without fine-grained partitioning.
- Mechanism: Different parameter types (attention weights, MLP weights, normalization layers) have distinct g-SNR patterns that correlate with their architectural roles. PyTorch's default partitioning groups parameters by function, naturally separating blocks with different g-SNR values.
- Core assumption: PyTorch's default parameter grouping aligns with the architectural heterogeneity that causes different optimal learning rates.
- Evidence anchors: [abstract]: "works with PyTorch's default parameter partitioning"; [section 4.2]: "our method works effectively with PyTorch's Default Partition and does not require any additional fine-grained partitioning strategies"

### Mechanism 3
- Claim: Eliminating second-order momentum reduces memory usage by 50% while maintaining performance through architecture-aware learning rate scaling.
- Mechanism: The second-order momentum in Adam serves to adapt learning rates based on gradient history. By using precomputed g-SNR values, this adaptation can be done once at initialization rather than continuously, eliminating the need to store and update second-order momentum tensors.
- Core assumption: The adaptive learning rate adjustment needed for training stability can be precomputed rather than computed dynamically.
- Evidence anchors: [abstract]: "cuts the optimizer's memory usage by half compared to AdamW"; [section 4.3]: "Our method requires only minimal modifications to the existing SGD optimizer"

## Foundational Learning

- Concept: Gradient signal-to-noise ratio (g-SNR) calculation
  - Why needed here: Forms the basis for learning rate scaling without adaptive methods
  - Quick check question: How is g-SNR computed from gradient norm and variance?

- Concept: Parameter partitioning and block-wise optimization
  - Why needed here: Enables efficient learning rate scaling across different parameter types
  - Quick check question: What are the PyTorch default parameter partitions for a Transformer?

- Concept: Momentum-based optimization (SGDM)
  - Why needed here: Provides the base optimization algorithm that SGD-SaI builds upon
  - Quick check question: How does momentum affect the update direction in SGDM?

## Architecture Onboarding

- Component map: SGD with momentum base -> g-SNR calculation at initialization -> Parameter block-wise scaling using precomputed g-SNR -> Decoupled weight decay regularization

- Critical path: 1. Model forward pass; 2. Gradient computation; 3. g-SNR calculation (only at initialization); 4. Parameter block updates with scaled learning rates

- Design tradeoffs:
  - Memory: 50% reduction vs AdamW, but requires storing g-SNR values
  - Speed: 3x faster than Adam-mini, comparable to SGDM
  - Flexibility: Works with PyTorch defaults, no complex partitioning needed

- Failure signatures:
  - Poor convergence: g-SNR calculation incorrect or parameter blocks too coarse
  - Memory issues: Unexpected large number of parameter blocks
  - Speed degradation: g-SNR calculation not properly cached

- First 3 experiments:
  1. CIFAR-10 with ResNet18: Verify basic functionality and memory savings
  2. ImageNet-1K with ViT-S/16: Test performance on vision transformers
  3. GPT-2-small pretraining: Validate on language models with memory profiling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SGD-SaI's performance scale when training models larger than 7B parameters, particularly in the context of LLM pretraining?
- Basis in paper: [inferred] The paper explicitly states it was constrained by computational resources and could not conduct large-scale pretraining on models larger than Llama-2-7B, but notes preliminary profiling suggests potential scalability.
- Why unresolved: The paper only provides profiling results for memory usage and optimizer step speed, not comprehensive training results for models larger than 7B parameters.
- What evidence would resolve it: Full training runs of SGD-SaI on models larger than 7B parameters (e.g., 10B, 30B, or 70B parameter models) demonstrating convergence, final performance metrics, and efficiency comparisons with AdamW and other optimizers.

### Open Question 2
- Question: What is the optimal partitioning strategy for SGD-SaI across different model architectures, and how sensitive is performance to these partitioning choices?
- Basis in paper: [explicit] The paper demonstrates that SGD-SaI works effectively with PyTorch's default partitioning and doesn't require fine-grained partitioning like Adam-mini, but the g-SNR analysis shows parameter blocks sharing the same structure have similar g-SNR values.
- Why unresolved: While the paper shows SGD-SaI works with default partitioning, it doesn't systematically explore whether alternative partitioning strategies (e.g., by function type, by layer depth, by parameter magnitude) could yield better performance.
- What evidence would resolve it: Systematic experiments comparing SGD-SaI performance across different partitioning strategies on the same model architectures, including analysis of g-SNR distributions under each strategy and corresponding training outcomes.

### Open Question 3
- Question: How does the g-SNR at initialization correlate with final model performance, and can this relationship be used to predict which models will train successfully with SGD-SaI versus requiring adaptive methods?
- Basis in paper: [explicit] The paper analyzes g-SNR distributions across parameter blocks and shows they remain relatively stable during training, but doesn't establish a direct correlation between initial g-SNR patterns and final model performance.
- Why unresolved: The paper demonstrates that g-SNR values can be calculated at initialization and used to scale learning rates, but doesn't explore whether the initial g-SNR distribution itself serves as a predictor of training success or model quality.
- What evidence would resolve it: Correlation analysis between initial g-SNR distributions (e.g., variance, skew, kurtosis across parameter blocks) and final model performance metrics across multiple model architectures and tasks, potentially enabling predictive selection of optimization methods.

## Limitations
- Limited architectural generalization beyond Transformers and vision models
- Hyperparameter sensitivity not fully explored across broader search space
- Lacks theoretical guarantees for convergence without second-order momentum

## Confidence
- High confidence: Memory usage reduction claims (50% vs AdamW, 75% vs Prodigy) - These are straightforward measurements with clear baselines.
- Medium confidence: Performance parity claims (comparable to AdamW on ImageNet, GPT-2) - Strong empirical support but limited architectural diversity in evaluation.
- Low confidence: g-SNR stability claims - While observed empirically, the mechanism is not theoretically justified, and the claim relies heavily on a single observation across limited training scenarios.

## Next Checks
1. **Architectural robustness test**: Evaluate SGD-SaI on a diverse set of architectures including CNNs with depthwise convolutions, RNNs/LSTMs, and dynamic architectures to verify that g-SNR-based scaling generalizes beyond the tested Transformers and vision models.

2. **Gradient distribution analysis**: Conduct a systematic study of how g-SNR values evolve under different initialization schemes (Kaiming, Xavier, orthogonal), learning rate schedules (cosine decay, step decay), and batch sizes to validate the claim of stability and identify edge cases where the method may fail.

3. **Memory-accuracy tradeoff analysis**: Perform a detailed ablation study comparing SGD-SaI against memory-efficient adaptive methods (Adafactor, Prodigy) across multiple tasks, measuring not just peak memory but also memory-accuracy Pareto frontiers to quantify the actual optimization benefits in resource-constrained scenarios.