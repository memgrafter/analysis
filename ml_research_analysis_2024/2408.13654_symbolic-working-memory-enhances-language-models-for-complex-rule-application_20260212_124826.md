---
ver: rpa2
title: Symbolic Working Memory Enhances Language Models for Complex Rule Application
arxiv_id: '2408.13654'
source_url: https://arxiv.org/abs/2408.13654
tags:
- rule
- facts
- schema
- symbolic
- fact
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of multi-step deductive reasoning
  in Large Language Models (LLMs), particularly when rules are presented non-sequentially.
  The authors propose a neurosymbolic framework that augments LLMs with external working
  memory, which stores facts and rules in both natural language and symbolic forms.
---

# Symbolic Working Memory Enhances Language Models for Complex Rule Application

## Quick Facts
- arXiv ID: 2408.13654
- Source URL: https://arxiv.org/abs/2408.13654
- Authors: Siyuan Wang; Zhongyu Wei; Yejin Choi; Xiang Ren
- Reference count: 24
- Primary result: Neurosymbolic framework significantly outperforms CoT-based and symbolic baselines on multi-step deductive reasoning tasks

## Executive Summary
This paper addresses the challenge of multi-step deductive reasoning in Large Language Models (LLMs) when rules are presented non-sequentially. The authors propose a neurosymbolic framework that augments LLMs with external working memory, which stores facts and rules in both natural language and symbolic forms. This dual representation enables precise tracking and supports iterative symbolic rule grounding combined with LLM-based rule implementation. Experiments on four datasets demonstrate significant improvements over existing approaches, particularly for complex reasoning tasks requiring multiple steps.

## Method Summary
The framework uses external working memory to store facts and rules in both natural language and symbolic (Prolog) forms. It iteratively performs symbolic rule grounding through predicate and variable matching to identify applicable rules, then uses LLM-based inference to implement these rules and generate new facts. The memory schema maintains canonical predicates and objects to avoid semantic duplication. The process continues until the query is resolved or no more applicable rules are found.

## Key Results
- Framework significantly outperforms CoT-based and symbolic baselines on CLUTRR, ProofWriter, AR-LSAT, and Boxes datasets
- High accuracy maintained even with open-source LLMs like Llama-3-8B-Instruct
- Robust performance across various rule application steps and settings
- Shows improvement in complex reasoning tasks requiring multiple deduction steps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neurosymbolic framework improves multi-step rule application by separating grounding from implementation
- Mechanism: Symbolic rule grounding uses predicate and variable matching to precisely identify applicable rules and supporting facts at each step, while LLM-based rule implementation infers new facts from grounded rules
- Core assumption: Symbolic grounding can accurately match predicates and variables without semantic ambiguity
- Evidence: [abstract] "Utilizing this memory, our framework iteratively performs symbolic rule grounding and LLM-based rule implementation. The former matches predicates and variables of symbolic rules and facts to ground applicable rules at each step."
- Break condition: If symbolic formulations contain semantic ambiguities that cannot be resolved through schema-based canonicalization

### Mechanism 2
- Claim: External working memory with dual representations enables precise long-term tracking
- Mechanism: Memory stores facts and rules in both natural language and symbolic forms, with symbolic forms enabling precise predicate and variable matching while natural language supports LLM processing
- Core assumption: Maintaining dual representations doesn't introduce synchronization errors
- Evidence: [abstract] "The memory stores facts and rules in both natural language and symbolic forms, enabling precise tracking."
- Break condition: If synchronization between natural language and symbolic forms becomes inconsistent during iterative updates

### Mechanism 3
- Claim: Memory schema construction through dynamic predicate/object integration handles diverse input expressions
- Mechanism: Schema starts empty and grows as facts/rules are processed, adding new predicates/objects when needed while reusing existing ones
- Core assumption: Dynamic schema growth won't create inconsistencies as more diverse inputs are processed
- Evidence: [section] "The dynamic construction process of the memory schema can be viewed in Appendix A."
- Break condition: If input expressions contain semantic conflicts that cannot be resolved through simple predicate/object distinction

## Foundational Learning

- Concept: Predicate and variable matching in symbolic logic
  - Why needed: Core mechanism for symbolic rule grounding requires understanding how to match predicates and variables between rules and facts
  - Quick check: Given rule "parent_of(X, Y) :- father_of(X, Y)" and fact "father_of(John, Mary)", what would the matching produce?

- Concept: Prolog notation and logic programming fundamentals
  - Why needed: Framework uses Prolog-style symbolic representations for facts and rules
  - Quick check: How would you represent "If A is married to B and C is sibling of B, then A is sibling-in-law of C" in Prolog notation?

- Concept: Working memory concepts and external memory augmentation
  - Why needed: Framework extends LLMs with external working memory, requiring understanding of how explicit memory storage differs from internalized reasoning
  - Quick check: What are the key differences between how humans use working memory versus how LLMs process information internally?

## Architecture Onboarding

- Component map: Working memory (fact base + rule base + memory schema) ↔ Symbolic grounding engine ↔ LLM inference engine ↔ Query resolution module
- Critical path: Initialize memory → Symbolic grounding → LLM implementation → Update memory → Repeat until query resolved
- Design tradeoffs: Dual representation increases storage/computation but improves precision; schema-based canonicalization reduces flexibility but prevents semantic drift
- Failure signatures: Symbolic grounding errors manifest as incorrect rule selection; LLM implementation errors appear as wrong fact inference; memory synchronization failures cause inconsistent state
- First 3 experiments:
  1. Single-step rule application with known correct grounding to verify LLM implementation works
  2. Multi-step reasoning with sequential rules to test memory tracking
  3. Rule grounding with semantic variations to test schema effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the framework's performance scale with increasingly complex logical rules involving nested predicates and multiple variable instantiations?
- Basis in paper: [explicit] The paper mentions that the framework uses symbolic predicate and variable matching, but doesn't extensively test its limits with highly complex rules
- Why unresolved: The experiments focus on datasets with relatively straightforward rules
- What evidence would resolve it: Experiments on datasets with increasingly complex logical rules, measuring performance degradation and identifying breaking points

### Open Question 2
- Question: What is the impact of the framework's rule grounding strategy on performance when dealing with ambiguous or conflicting facts in the input context?
- Basis in paper: [inferred] The paper mentions symbolic rule grounding using predicate and variable matching, but doesn't explore scenarios with ambiguous or conflicting facts
- Why unresolved: The datasets used likely have clean, unambiguous facts
- What evidence would resolve it: Experiments introducing ambiguous or conflicting facts into the datasets and measuring the framework's ability to handle them correctly

### Open Question 3
- Question: How does the framework's performance compare to specialized symbolic reasoning systems when dealing with large-scale knowledge bases?
- Basis in paper: [explicit] The paper mentions that the framework combines symbolic rule grounding with LLM-based rule implementation, but doesn't compare it to specialized symbolic reasoning systems on large-scale knowledge bases
- Why unresolved: The experiments focus on relatively small datasets
- What evidence would resolve it: Experiments comparing the framework's performance to specialized symbolic reasoning systems on large-scale knowledge bases, measuring both accuracy and efficiency

## Limitations
- Framework's effectiveness relies heavily on assumption that symbolic grounding can accurately resolve semantic ambiguities through schema-based canonicalization
- No explicit comparison showing impact of using dual versus single representations for facts and rules
- Exact implementation details for memory schema updates and rule grounding processes remain unspecified

## Confidence

- **High Confidence**: The core claim that neurosymbolic frameworks can outperform pure LLM or pure symbolic approaches is well-supported by experimental results across four datasets
- **Medium Confidence**: The mechanism separating symbolic grounding from LLM implementation is logically sound and supported by framework description
- **Medium Confidence**: The claim about dual natural language and symbolic representations enabling precise tracking is supported by framework design but lacks direct empirical validation

## Next Checks

1. Test schema handling of semantic conflicts: Create input instances with semantically equivalent but syntactically different expressions to verify the schema's ability to canonicalize without introducing inconsistencies

2. Compare dual vs single representation performance: Implement a variant using only natural language or only symbolic representation to measure the actual impact of the dual approach on reasoning accuracy

3. Stress-test symbolic grounding accuracy: Use controlled inputs with known correct groundings to measure the precision of predicate and variable matching, particularly for complex rules with nested variables