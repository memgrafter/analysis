---
ver: rpa2
title: ADMM Based Semi-Structured Pattern Pruning Framework For Transformer
arxiv_id: '2407.08334'
source_url: https://arxiv.org/abs/2407.08334
tags:
- pruning
- pattern
- admm
- framework
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an ADMM-based semi-structured pattern pruning
  framework for compressing Transformer models, addressing the challenge of deploying
  large-scale models on resource-constrained devices. The method formulates pattern
  pruning as a constrained optimization problem and uses ADMM to reshape the distribution
  of activation maps, enabling more effective regional sparsity.
---

# ADMM Based Semi-Structured Pattern Pruning Framework For Transformer

## Quick Facts
- arXiv ID: 2407.08334
- Source URL: https://arxiv.org/abs/2407.08334
- Authors: TianChen Wang
- Reference count: 40
- One-line primary result: Achieves 50% compression ratio on GLUE benchmark while maintaining 80.1 overall score

## Executive Summary
This paper introduces an ADMM-based semi-structured pattern pruning framework for compressing Transformer models, addressing the challenge of deploying large-scale models on resource-constrained devices. The method formulates pattern pruning as a constrained optimization problem and uses ADMM to reshape the distribution of activation maps, enabling more effective regional sparsity. The framework is extended with SR-STE to avoid gradient vanishing and improve generalization. Extensive experiments on GLUE benchmark tasks show that the proposed approach achieves a 50% compression ratio while maintaining an overall score of 80.1, outperforming previous methods.

## Method Summary
The framework uses Alternating Direction Method of Multipliers (ADMM) to solve the constrained optimization problem of pattern pruning. It divides weight matrices into blocks (4x4 patterns) and applies sparsity constraints to each block. The ADMM optimizer decomposes the problem into weight updates and Euclidean projection steps. SR-STE is integrated to maintain gradient flow during retraining after pruning. The framework is applied to both FFN and attention layers, with a 50% sparsity target. Training uses 5 epochs with batch size 24, AdamW optimizer, and learning rate of 7e-5.

## Key Results
- Achieves 50% compression ratio on GLUE benchmark tasks
- Maintains overall score of 80.1 while compressed
- Outperforms previous pruning methods on GLUE tasks
- Pattern sparsity is hardware-friendly for FPGA/ASIC deployment

## Why This Works (Mechanism)

### Mechanism 1
ADMM-based pattern pruning reshapes the distribution of activation maps to enable more effective regional sparsity. ADMM reformulates the pruning problem as a constrained optimization, decomposing it into subproblems solvable by SGD and closed-form projection. This reshapes the weight distribution so that each pattern block has a consistent sparsity level (~50%), avoiding polarized sparsity patterns. Core assumption: The weight distribution can be reshaped without significant accuracy loss through ADMM optimization. Evidence anchors: [abstract] "reshape the distribution of activation map... initial dense feature maps is transformed to rather regionally sparsified ones"; [section] "The Alternating Direction Method of Multipliers (ADMM)... can decompose the overall problem into smaller subproblems, solve them in parallel, and aggregate them to get the complete solution"; [corpus] No direct corpus evidence for ADMM reshaping in transformers, but general ADMM usage in neural networks is established.

### Mechanism 2
SR-STE integration avoids gradient vanishing during retraining after pruning. SR-STE modifies the backpropagation step to include the sparse-refined term, maintaining gradient flow through the pruned network and preventing accuracy degradation. Core assumption: The sparse-refined term in the loss function can effectively preserve gradient information through the pruning mask. Evidence anchors: [abstract] "extend the proposed ADMM based framework with SR-STE to demonstrate its generalization and to avoid the gradient vanishing problem"; [section] "SR-STE... focus on the back propagation of the weight matrix... greatly enhances the robustness and positively avoids the gradient vanishing problem"; [corpus] No corpus evidence for SR-STE specifically in transformer pruning.

### Mechanism 3
Pattern pruning applied to both FFN and attention layers enables higher compression ratios while maintaining accuracy. By extending pattern pruning beyond just FFN layers to include attention mechanisms, the framework achieves more comprehensive model compression. The attention map is partitioned into blocks, and sparsity is applied based on relative importance. Core assumption: Attention layers can be effectively pruned using the same pattern-based approach as FFN layers without severe accuracy loss. Evidence anchors: [abstract] "we first introduced... pattern pruning framework to reshape the distribution of activation map... achieve a higher compression ratio with better performance"; [section] "We not only choose the dense weight matrix of the model, but also the attention mechanism to apply the pattern pruning to extend our framework"; [corpus] Some corpus papers discuss attention pruning, but none specifically validate pattern pruning on attention with ADMM.

## Foundational Learning

- Concept: Alternating Direction Method of Multipliers (ADMM)
  - Why needed here: ADMM provides a way to solve constrained optimization problems by decomposing them into simpler subproblems, which is essential for the pattern pruning formulation.
  - Quick check question: How does ADMM differ from standard gradient descent when solving constrained optimization problems?

- Concept: Pattern Pruning
  - Why needed here: Pattern pruning allows for semi-structured sparsity that is hardware-friendly while maintaining better accuracy than unstructured pruning.
  - Quick check question: What is the difference between pattern pruning and N:M structured pruning in terms of how blocks are defined?

- Concept: Straight-Through Estimator (STE)
  - Why needed here: STE is used to approximate gradients through non-differentiable operations during pruning, and SR-STE extends this to handle sparse-refined gradients.
  - Quick check question: Why is a straight-through estimator necessary when pruning involves non-differentiable operations?

## Architecture Onboarding

- Component map: ADMM optimizer module -> Pattern pruning engine -> SR-STE layer -> Attention pruning module -> Hardware accelerator interface
- Critical path: 1. Initialize weight matrices and ADMM variables; 2. ADMM optimization loop (weight update + projection); 3. Apply SR-STE during backpropagation; 4. Pattern prune attention and FFN layers; 5. Fine-tune on downstream task
- Design tradeoffs:
  - Pattern size vs. hardware efficiency: Larger patterns (e.g., 8x8) may offer better compression but reduce hardware compatibility
  - Sparsity level vs. accuracy: Higher sparsity reduces model size but may degrade performance
  - ADMM iterations vs. training time: More iterations improve weight distribution but increase training time
  - Attention vs. FFN pruning ratio: Different ratios may be optimal for different tasks
- Failure signatures: Accuracy drops significantly after pruning despite ADMM optimization; Training becomes unstable with exploding/vanishing gradients; Hardware acceleration fails due to incompatible sparsity patterns; Pattern clustering produces non-uniform sparsity distribution
- First 3 experiments: 1. Run baseline BERT on GLUE without any pruning to establish performance baseline; 2. Apply pattern pruning with ADMM but without SR-STE to isolate ADMM effects; 3. Apply pattern pruning with both ADMM and SR-STE to verify the combined approach

## Open Questions the Paper Calls Out

### Open Question 1
Question: How does the effectiveness of ADMM-based pattern pruning scale when applied to extremely large models like GPT-3 or GPT-4, considering the computational overhead of ADMM optimization?
Basis in paper: [inferred] The paper mentions that applying the framework to larger models like GPT is estimated to be less efficient due to the restriction of pattern pruning itself and the long pruning process for large weight matrices.
Why unresolved: The paper only discusses theoretical estimates and does not provide experimental results for extremely large models.
What evidence would resolve it: Experimental results comparing ADMM-based pattern pruning performance on models of varying scales, particularly GPT-3 or GPT-4, would provide concrete evidence of scalability limitations.

### Open Question 2
Question: Can the ADMM-based pattern pruning framework be effectively combined with other model compression techniques like knowledge distillation or quantization to achieve even higher compression ratios without significant accuracy loss?
Basis in paper: [explicit] The paper states that the ADMM-based pattern pruning framework is orthogonal to many other optimization techniques and can be widely applied to almost all transformer models.
Why unresolved: The paper does not explore the combination of ADMM-based pattern pruning with other compression techniques in detail.
What evidence would resolve it: Experimental results demonstrating the effectiveness of combining ADMM-based pattern pruning with other compression techniques on various transformer models would provide evidence of potential synergies.

### Open Question 3
Question: What is the optimal sparsity distribution across different layers of a transformer model for achieving the best trade-off between compression ratio and accuracy?
Basis in paper: [inferred] The paper mentions that the framework allows for setting the sparsity level for each layer and even the distribution of each individual weight matrix, but does not provide specific guidance on optimal distributions.
Why unresolved: The paper does not explore the impact of different sparsity distributions on model performance in detail.
What evidence would resolve it: A comprehensive study analyzing the impact of various sparsity distributions across transformer layers on compression ratio and accuracy would provide insights into optimal configurations.

## Limitations
- The effectiveness of the method depends on precise implementation details of ADMM projection and SR-STE that are not fully specified
- Performance on tasks beyond GLUE benchmark remains unverified, limiting generalizability claims
- Computational overhead of ADMM optimization may limit scalability to extremely large models

## Confidence
- **High Confidence**: The effectiveness of ADMM for decomposing constrained optimization problems in neural network pruning is well-established in the broader ML literature, though not specifically for transformer models with pattern sparsity
- **Medium Confidence**: The claim that pattern pruning applied to both FFN and attention layers enables higher compression ratios while maintaining accuracy is supported by experimental results but lacks extensive ablation studies isolating the contribution of each component
- **Low Confidence**: The assertion that SR-STE integration completely avoids gradient vanishing during retraining requires further validation, as the paper provides limited empirical evidence of this specific benefit compared to standard retraining approaches

## Next Checks
1. **Ablation Study on Pattern Size**: Systematically vary pattern sizes (2x2, 4x4, 8x8) and measure the tradeoff between hardware efficiency and accuracy degradation to identify the optimal pattern size for different deployment scenarios
2. **Gradient Flow Analysis**: Compare gradient norms and stability during training with and without SR-STE integration across multiple pruning iterations to quantitatively verify the claim of gradient vanishing prevention
3. **Cross-Domain Generalization Test**: Apply the framework to non-GLUE tasks such as summarization (CNN/DailyMail) or question answering (SQuAD) to evaluate whether the 50% compression ratio with maintained accuracy generalizes beyond natural language inference tasks