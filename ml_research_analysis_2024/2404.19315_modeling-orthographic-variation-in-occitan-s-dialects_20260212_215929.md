---
ver: rpa2
title: Modeling Orthographic Variation in Occitan's Dialects
arxiv_id: '2404.19315'
source_url: https://arxiv.org/abs/2404.19315
tags:
- data
- dialects
- occitan
- lengadocian
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores how multilingual language models handle orthographic
  variation in Occitan, a low-resource Romance language with multiple dialects. The
  authors fine-tune mBERT using multi-dialect Occitan data and evaluate its ability
  to represent dialectal variation both intrinsically (analogy tasks, lexicon induction)
  and extrinsically (PoS tagging, UD parsing).
---

# Modeling Orthographic Variation in Occitan's Dialects

## Quick Facts
- arXiv ID: 2404.19315
- Source URL: https://arxiv.org/abs/2404.19315
- Reference count: 15
- This study explores how multilingual language models handle orthographic variation in Occitan, a low-resource Romance language with multiple dialects.

## Executive Summary
This study investigates how multilingual language models handle orthographic variation in Occitan, a low-resource Romance language with multiple dialects. The authors fine-tune mBERT using multi-dialect Occitan data and evaluate its ability to represent dialectal variation both intrinsically (analogy tasks, lexicon induction) and extrinsically (PoS tagging, UD parsing). While fine-tuning with non-standardized dialectal data did not significantly improve downstream task performance, it enhanced the model's ability to relate parallel words across dialects—particularly when surface similarity was high. The findings suggest that large multilingual models can learn from orthographically inconsistent data without requiring costly normalization, supporting their use in low-resource scenarios.

## Method Summary
The authors fine-tuned mBERT using multi-dialect Occitan data from Wikipedia discussion forums and WikiMatrix parallel sentences. They evaluated intrinsic representations through analogy computation and lexicon induction tasks, and extrinsic performance through PoS tagging and dependency parsing on the Tolosa Treebank. The study compared models trained on all dialects versus a single dialect (Lengadocian) to assess robustness to dialectical variation.

## Key Results
- Fine-tuning with non-standardized dialectal data did not significantly improve downstream task performance
- Lexicon induction accuracy improved when surface similarity between dialectal forms was high
- Performance on PoS tagging and dependency parsing was robust to dialectical variation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning mBERT with multi-dialect data improves the model's ability to relate parallel words across dialects when surface similarity is high.
- Mechanism: The fine-tuning process allows the model to learn representations that capture lexical and orthographic variation across dialects. When two dialectal forms are highly similar (low Levenshtein distance), the model represents them more similarly, enabling effective lexicon induction.
- Core assumption: Surface similarity between dialectal variants is a strong predictor of their representation similarity in the fine-tuned model.
- Evidence anchors:
  - [abstract] "fine-tuning with non-standardized dialectal data did not significantly improve downstream task performance, it enhanced the model’s ability to relate parallel words across dialects—particularly when surface similarity was high."
  - [section] "These results indicate that the further apart two word forms are, the less similarly our model represents them, even though they are semantically similar."
  - [corpus] Weak: No direct corpus evidence provided for surface similarity correlation.
- Break condition: If lexical variation between dialects increases beyond minimal orthographic changes, the model's ability to relate parallel words diminishes.

### Mechanism 2
- Claim: Large multilingual models like mBERT can learn from orthographically inconsistent data without requiring costly normalization, supporting their use in low-resource scenarios.
- Mechanism: The pre-training of mBERT on diverse multilingual data allows it to handle orthographic variation effectively. Fine-tuning on non-standardized dialectal data leverages this capability, enabling the model to learn from noisy data without explicit normalization.
- Core assumption: The pre-training process of mBERT has equipped it with robust handling of orthographic variation across languages.
- Evidence anchors:
  - [abstract] "Our findings suggest that large multilingual models minimize the need for spelling normalization during pre-processing."
  - [section] "the fine-tuning we carried out with multiple dialects of Occitan did not deprecate mBERT’s baseline performance on downstream tasks like part-of-speech tagging."
  - [corpus] Weak: No direct corpus evidence provided for mBERT's pre-training robustness.
- Break condition: If the orthographic variation exceeds the model's learned capacity from pre-training, performance may degrade.

### Mechanism 3
- Claim: Performance on downstream tasks like POS tagging and dependency parsing is robust to dialectical variation, even when trained on data from a single dialect.
- Mechanism: The fine-tuned mBERT model generalizes well across dialects for syntactic tasks, likely due to the shared syntactic structures across Occitan dialects and the model's ability to capture these commonalities during fine-tuning.
- Core assumption: Syntactic structures are shared across Occitan dialects, allowing a model trained on one dialect to perform well on others for syntactic tasks.
- Evidence anchors:
  - [abstract] "its performance was robust to dialectical variation, even when trained solely on part-of-speech data from a single dialect."
  - [section] "In PoS tagging, accuracy is relatively high in both training scenarios."
  - [corpus] Weak: No direct corpus evidence provided for syntactic structure sharing.
- Break condition: If dialects diverge significantly in syntactic structures, the model's generalization ability may fail.

## Foundational Learning

- Concept: Understanding of fine-tuning large language models
  - Why needed here: The study involves fine-tuning mBERT with multi-dialect data, requiring knowledge of how fine-tuning affects model performance and representation.
  - Quick check question: What is the primary purpose of fine-tuning a pre-trained model like mBERT?

- Concept: Familiarity with orthographic variation and its impact on NLP
  - Why needed here: The study focuses on modeling orthographic variation in Occitan dialects, necessitating an understanding of how such variation affects language models.
  - Quick check question: How does orthographic variation typically impact the performance of language models in NLP tasks?

- Concept: Knowledge of lexicon induction and its evaluation
  - Why needed here: The study includes lexicon induction experiments to evaluate the model's ability to relate parallel words across dialects, requiring understanding of this task and its evaluation metrics.
  - Quick check question: What is the primary goal of lexicon induction in the context of multilingual embeddings?

## Architecture Onboarding

- Component map:
  - mBERT (multilingual BERT) -> OcWikiDisc corpus -> WikiMatrix corpus -> Tolosa Treebank -> MaChAmp framework

- Critical path:
  1. Compile parallel lexicon of Occitan dialects
  2. Fine-tune mBERT with multi-dialect data
  3. Evaluate intrinsic representations (analogy computation, lexicon induction)
  4. Evaluate extrinsic performance (POS tagging, dependency parsing)
  5. Analyze results and draw conclusions

- Design tradeoffs:
  - Using a large multilingual model (mBERT) allows handling of orthographic variation but may not be as efficient as dialect-specific models
  - Fine-tuning on multi-dialect data improves lexicon induction but does not significantly enhance downstream task performance
  - Evaluating on a small parallel lexicon provides controlled comparisons but may not capture full dialectal variation

- Failure signatures:
  - Poor performance on analogy computation may indicate inadequate representation of semantic relations
  - Decreased accuracy in lexicon induction for words with low surface similarity suggests limitations in handling significant orthographic variation
  - Inconsistent performance across dialects in downstream tasks may reveal issues with generalization or data imbalance

- First 3 experiments:
  1. Evaluate mBERT's performance on analogy computation using the fine-tuned model and compare with the baseline
  2. Conduct lexicon induction from each dialect to Lengadocian and analyze the impact of surface similarity on accuracy
  3. Train POS taggers and dependency parsers using data from all dialects and from a single dialect, then evaluate on all dialects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does fine-tuning with non-standardized dialectal data improve multilingual language models' ability to relate parallel words across dialects in low-resource languages?
- Basis in paper: [explicit] The authors found that fine-tuning mBERT with multi-dialect Occitan data improved the model's ability to relate parallel words across dialects, particularly when surface similarity was high. This was observed in the lexicon induction task, where the fine-tuned model correctly selected the Lengadocian form of Provençau words in 40.9% of cases.
- Why unresolved: The study focused on Occitan, a single low-resource language. It is unclear whether the findings generalize to other low-resource languages with dialectal variation.
- What evidence would resolve it: Replicating the study with other low-resource languages exhibiting dialectal variation, such as Swiss German or Low Saxon, would provide evidence for the generalizability of the findings.

### Open Question 2
- Question: How does the surface similarity between dialects impact the effectiveness of cross-lingual transfer learning for low-resource languages?
- Basis in paper: [explicit] The authors found that the fine-tuned model represented parallel words from different dialects more similarly when their spelling was more similar, as measured by Levenshtein distance. This suggests that surface similarity plays a role in the model's ability to learn from non-standardized data.
- Why unresolved: The study did not explicitly manipulate the surface similarity between dialects to isolate its effect on transfer learning. Other factors, such as the amount of data available for each dialect, may also contribute to the observed results.
- What evidence would resolve it: Conducting controlled experiments that manipulate the surface similarity between dialects while keeping other factors constant would help determine the specific impact of surface similarity on transfer learning.

### Open Question 3
- Question: Can injecting character-level noise into pre-training data improve the robustness of multilingual language models to dialectal variation in low-resource languages?
- Basis in paper: [inferred] The authors suggest that increasing the surface similarity between pre-training and fine-tuning data may help the model learn from non-standardized data. Injecting character-level noise into pre-training data, as done in previous work, could be a way to achieve this.
- Why unresolved: The study did not experiment with injecting noise into pre-training data. It is unclear whether this approach would be effective for low-resource languages with dialectal variation.
- What evidence would resolve it: Experimenting with injecting character-level noise into pre-training data for low-resource languages with dialectal variation, and evaluating the impact on downstream tasks, would provide evidence for the effectiveness of this approach.

## Limitations

- The findings are based on a single language family (Romance) and may not generalize to languages with more divergent orthographies
- The evaluation relied on a relatively small parallel lexicon, which may not capture the full complexity of dialectal variation
- Surface similarity was used as a proxy for model performance, but this relationship may break down for languages with more substantial orthographic differences

## Confidence

- **High confidence**: The core experimental methodology and evaluation framework are sound
- **Medium confidence**: The generalization to other low-resource languages with orthographic variation
- **Medium confidence**: The claim that normalization can be skipped in pre-processing

## Next Checks

1. Test the approach on a language family with greater orthographic divergence (e.g., Slavic or Germanic languages) to assess robustness beyond Romance languages
2. Conduct ablation studies comparing models trained on normalized vs. non-normalized data across varying degrees of orthographic distance
3. Evaluate model performance on a larger, more diverse parallel lexicon to determine if results scale with corpus size and dialectal diversity