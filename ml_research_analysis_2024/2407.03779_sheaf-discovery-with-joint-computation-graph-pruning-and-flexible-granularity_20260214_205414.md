---
ver: rpa2
title: Sheaf Discovery with Joint Computation Graph Pruning and Flexible Granularity
arxiv_id: '2407.03779'
source_url: https://arxiv.org/abs/2407.03779
tags:
- circuit
- pruning
- discovery
- graph
- discogp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of extracting self-contained,
  task-specific modular units (sheaves) from neural language models, which are more
  functional and modular than previously identified circuits. The authors introduce
  DiscoGP, a novel framework that discovers sheaves by jointly pruning model weights
  and connections using a gradient-based algorithm.
---

# Sheaf Discovery with Joint Computation Graph Pruning and Flexible Granularity

## Quick Facts
- arXiv ID: 2407.03779
- Source URL: https://arxiv.org/abs/2407.03779
- Reference count: 24
- Primary result: DiscoGP discovers task-specific sheaves that retain 93%-100% of model performance while using only 1%-7% of original weights and connections

## Executive Summary
This paper introduces DiscoGP, a novel framework for discovering sheaves - self-contained, task-specific modular units - in neural language models through joint weight and connection pruning. DiscoGP outperforms previous methods in modularity, functional fidelity, and sparsity by using gradient-based pruning with flexible granularity control. The discovered sheaves achieve superior performance compared to previously identified LM circuits while revealing new insights into model internal organization.

## Method Summary
DiscoGP employs a joint optimization approach that simultaneously prunes both weights and connections in neural networks to discover task-specific sheaves. The method uses gradient-based pruning with a flexible granularity mechanism that allows discovery of modular units at varying levels of abstraction. By optimizing for sparsity while maintaining functional performance, DiscoGP identifies subnetworks that preserve 93%-100% of the original model's task performance while comprising only 1%-7% of the total weights and connections.

## Key Results
- DiscoGP achieves 93%-100% task performance retention while reducing weights to 1%-7% of original size
- Discovered sheaves demonstrate superior modularity and functional fidelity compared to previous LM circuit discovery methods
- Outperforms existing approaches on gender bias and sentiment analysis tasks
- Reveals new insights into the internal modular organization of large language models

## Why This Works (Mechanism)
DiscoGP works by jointly optimizing weight and connection pruning through gradient-based methods, allowing it to discover sparse, functionally complete subnetworks (sheaves) that maintain task performance. The flexible granularity control enables discovery of modular units at appropriate levels of abstraction, while the joint pruning approach ensures both structural and weight sparsity are optimized simultaneously. This coordinated approach allows identification of truly self-contained functional modules rather than just sparse weight matrices.

## Foundational Learning

**Neural network pruning**: Removing redundant weights/connections while preserving performance - needed for efficient model compression; quick check: verify sparsity levels maintain accuracy

**Modular neural networks**: Networks composed of functionally distinct subcomponents - needed to understand sheaf concept; quick check: identify modular patterns in discovered subnetworks

**Gradient-based optimization**: Using gradients to guide pruning decisions - needed for effective joint pruning; quick check: verify gradient signals correlate with functional importance

**Task-specific subnetwork extraction**: Isolating model components responsible for specific behaviors - needed for sheaf discovery; quick check: validate functional completeness of extracted subnetworks

## Architecture Onboarding

**Component map**: Input data → Model layers → Gradient computation → Pruning mask update → Sheaf extraction

**Critical path**: Model forward pass → Task-specific gradient computation → Joint pruning optimization → Sheaf validation and extraction

**Design tradeoffs**: Joint vs. sequential pruning (joint achieves better modularity but higher computational cost); fixed vs. flexible granularity (flexible finds better modular units but requires more hyperparameter tuning)

**Failure signatures**: Poor modularity despite high sparsity (indicates ineffective joint optimization); significant performance drop (suggests over-aggressive pruning); unstable sheaf discovery across runs (indicates sensitivity to initialization)

**First experiments**: 1) Run on simple MLP with known modular structure to verify discovery capability; 2) Test on single task with varying sparsity targets to identify optimal pruning ratios; 3) Compare joint vs. sequential pruning approaches on benchmark task

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Limited ablation studies to validate necessity of joint pruning and granularity control mechanisms
- Evaluation restricted to specific tasks (gender bias and sentiment analysis) without demonstrating generalization
- Evaluation metrics for modularity and functional fidelity lack detailed validation and cross-verification
- No analysis of stability across multiple discovery runs or hyperparameter sensitivity

## Confidence

**High confidence**: Reported sparsity results (1%-7% of original weights) and performance retention (93%-100%) based on methodology

**Medium confidence**: Claims about superior modularity and functional fidelity compared to previous methods, as these rely on insufficiently validated metrics

**Low confidence**: Generalizability across different model architectures, tasks, and domains due to limited evaluation scope

## Next Checks

1. Conduct comprehensive ablation studies to isolate contributions of joint pruning versus granularity control mechanisms

2. Test DiscoGP on diverse tasks across different domains (code generation, reasoning, etc.) to assess generalizability beyond gender bias and sentiment analysis

3. Perform multiple independent runs with varying random seeds and analyze stability and consistency of discovered sheaves to establish robustness