---
ver: rpa2
title: 'MDAgents: An Adaptive Collaboration of LLMs for Medical Decision-Making'
arxiv_id: '2404.15155'
source_url: https://arxiv.org/abs/2404.15155
tags:
- medical
- complexity
- answer
- question
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MDAgents introduces an adaptive multi-agent framework for medical
  decision-making that automatically assigns collaboration structures based on task
  complexity. The system employs a moderator agent to assess medical query complexity
  and recruit appropriate teams of specialists, ranging from solo general practitioners
  for simple cases to integrated care teams for complex scenarios.
---

# MDAgents: An Adaptive Collaboration of LLMs for Medical Decision-Making

## Quick Facts
- arXiv ID: 2404.15155
- Source URL: https://arxiv.org/abs/2404.15155
- Reference count: 40
- Primary result: Adaptive multi-agent framework for medical decision-making that dynamically assigns collaboration structures based on task complexity

## Executive Summary
MDAgents introduces an adaptive framework that automatically determines the appropriate team structure for medical queries based on their complexity. The system uses a moderator agent to classify each query and recruit teams ranging from solo general practitioners for simple cases to integrated care teams for complex scenarios. When evaluated across ten medical benchmarks, MDAgents achieved superior performance in seven benchmarks, with up to 4.2% accuracy improvement over previous methods while using fewer API calls than static group settings.

## Method Summary
The MDAgents framework operates in four stages: medical complexity check, expert recruitment, analysis and synthesis, and final decision-making. The moderator agent first classifies each query into low, moderate, or high complexity. Based on this classification, appropriate teams are recruited: solo PCP agents for simple cases, MDT for moderate complexity, and ICT for complex cases. The system employs iterative discussions among agents for moderate cases and integrates external medical knowledge through MedRAG. The framework was tested across ten medical benchmarks using GPT-4, GPT-3.5, and Gemini-Pro models with 3-shot prompting for low-complexity and zero-shot for moderate/high-complexity cases.

## Key Results
- MDAgents achieved superior performance in 7 out of 10 medical benchmarks tested
- Up to 4.2% accuracy improvement over previous methods in MedQA dataset
- Average 11.8% accuracy improvement when combining moderator review with external medical knowledge
- More efficient than static group settings, requiring fewer API calls while maintaining high accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework improves performance by dynamically matching team structure to task complexity, reducing unnecessary computation for simple cases while ensuring sufficient expertise for complex ones.
- Mechanism: A moderator agent classifies each medical query into low, moderate, or high complexity. Based on this classification, the system recruits either a solo PCP agent, an MDT, or an ICT. This ensures that each query is handled by an appropriately sized and specialized team.
- Core assumption: The moderator can accurately classify medical complexity, and this classification aligns with the optimal number of agents needed for effective problem-solving.
- Evidence anchors:
  - [abstract] "MDAgents work in three steps: 1) Medical complexity check; 2) Recruitment based on medical complexity; 3) Analysis and synthesis and 4) Final decision-making"
  - [section] "Our adaptive method significantly outperforms (p < 0.05) both Solo and Group setting methods, showing best performance in 7 out of 10 medical benchmarks tested"

### Mechanism 2
- Claim: Multi-turn collaborative discussion among agents improves accuracy by allowing refinement of initial opinions through structured debate and consensus-building.
- Mechanism: For moderate complexity cases, agents engage in iterative discussions with multiple rounds. Each agent can communicate with others, provide feedback, and revise their opinions. The moderator reviews conversation history and provides feedback when consensus isn't reached.
- Core assumption: Agent opinions converge toward better solutions through structured discussion, and the moderator can effectively guide this process.
- Evidence anchors:
  - [abstract] "Ablation studies reveal that MDAgents effectively determines medical complexity to optimize for efficiency and accuracy"
  - [section] "The parallel collaboration with discussion yielded the highest accuracy (59.0%), suggesting that enabling agents to work simultaneously and engage in dialogue is crucial for handling intricate medical queries"

### Mechanism 3
- Claim: Incorporating external medical knowledge through RAG and moderator review significantly enhances decision quality beyond what role-playing alone achieves.
- Mechanism: The system uses MedRAG to access biomedical literature and clinical databases during decision-making. The moderator also reviews agent discussions and provides structured feedback, creating a hybrid approach combining retrieval, collaboration, and expert oversight.
- Core assumption: Access to current medical literature and structured feedback mechanisms improves agent performance beyond their pre-trained knowledge.
- Evidence anchors:
  - [abstract] "the combination of moderator review and external medical knowledge in group collaboration resulted in an average accuracy improvement of 11.8%"
  - [section] "Integrating MedRAG increases accuracy to 75.2% (up 4.7%), while the moderator's review alone raises it to 77.6% (up 8.1%). The combined use of both methods achieves the highest accuracy at 80.3% (up 11.8%)"

## Foundational Learning

- Concept: Medical complexity classification and its relationship to clinical decision-making
  - Why needed here: The entire framework depends on accurately assessing whether a case requires solo, team, or integrated care approaches
  - Quick check question: What are the three complexity levels used by MDAgents and what clinical scenarios correspond to each level?

- Concept: Multi-agent collaboration patterns and consensus-building mechanisms
  - Why needed here: Understanding how agents debate, share information, and reach agreement is crucial for debugging and improving the system
  - Quick check question: How does the iterative discussion process work in moderate complexity cases, and what triggers a new round of discussion?

- Concept: Retrieval-Augmented Generation (RAG) and its application to medical knowledge
  - Why needed here: The system uses MedRAG to access external medical databases; understanding this technology is essential for evaluating its contribution
  - Quick check question: What types of medical corpora does MedRAG leverage, and how does this differ from relying solely on pre-trained model knowledge?

## Architecture Onboarding

- Component map:
  Moderator Agent -> Recruiter Agent -> [PCC | MDT | ICT] -> Decision-maker Agent

- Critical path:
  1. Query received â†’ Complexity classification by moderator
  2. Team recruitment based on complexity
  3. Initial assessment by recruited agents
  4. Collaborative discussion (if applicable)
  5. Moderator review and feedback (if needed)
  6. Final decision synthesis and output

- Design tradeoffs:
  - Single vs. multiple agents: Solo approaches are faster but less accurate for complex cases; multi-agent approaches are more accurate but computationally expensive
  - Static vs. adaptive structure: Fixed team sizes are simpler but less efficient than complexity-based adaptation
  - Discussion depth vs. speed: More discussion rounds improve accuracy but increase latency and cost

- Failure signatures:
  - Moderator misclassification: System uses wrong team size for task complexity
  - Agent disagreement without resolution: Discussion loops without reaching consensus
  - Over-reliance on RAG: System spends too much time retrieving irrelevant information
  - Moderator bias: Final decisions consistently favor certain agent opinions regardless of merit

- First 3 experiments:
  1. Test moderator accuracy by having it classify a set of known-complexity cases and measuring agreement with human experts
  2. Compare performance of different team sizes on a fixed set of moderate complexity cases to find optimal agent count
  3. Evaluate the impact of discussion rounds by running cases with 0, 1, 2, and 3 discussion rounds and measuring accuracy improvements

## Open Questions the Paper Calls Out

- Question: How does the performance of MDAgents change when applied to more diverse and complex medical datasets, such as those involving longitudinal patient data or rare diseases?
  - Basis in paper: [inferred]
  - Why unresolved: The current evaluation focuses on standardized datasets like MedQA, PubMedQA, and visual datasets. The paper acknowledges the need for testing on more challenging video medical datasets and real-world patient data, but does not provide results for such cases.
  - What evidence would resolve it: Experiments comparing MDAgents performance on datasets with longitudinal patient data, rare disease cases, and real-world clinical scenarios versus current benchmarks.

- Question: What is the optimal balance between moderator review and external medical knowledge (MedRAG) in different medical domains, and how does this balance affect accuracy across various complexity levels?
  - Basis in paper: [explicit]
  - Why unresolved: While the paper shows that combining moderator review and MedRAG improves performance by 11.8%, it does not explore how this optimal combination varies across different medical specialties or complexity levels.
  - What evidence would resolve it: Ablation studies showing performance variations across medical specialties when adjusting the weight of moderator review versus external knowledge retrieval.

- Question: How does the adaptive complexity assessment mechanism perform when deployed in real-time clinical settings with noisy or incomplete medical queries?
  - Basis in paper: [explicit]
  - Why unresolved: The paper demonstrates that the complexity assessment works well on clean benchmark datasets, but does not address how it handles real-world scenarios with ambiguous, incomplete, or noisy medical queries.
  - What evidence would resolve it: Clinical trials comparing the accuracy of complexity assessment and subsequent decision-making quality between MDAgents and human clinicians using real patient queries from electronic health records.

## Limitations
- The complexity classification mechanism may not generalize well to all medical domains or novel query types
- Evaluation focuses primarily on accuracy metrics without comprehensive analysis of false positive/negative patterns critical in clinical settings
- Framework's performance varies significantly across different large language model variants, suggesting potential dependence on specific model capabilities

## Confidence

- High confidence: The framework's ability to improve performance over static approaches (7/10 benchmark improvements observed)
- Medium confidence: The claimed efficiency benefits (fewer API calls than static group settings) - while stated, detailed cost-benefit analysis is limited
- Medium confidence: The moderator's complexity assessment accuracy - validated internally but not independently verified against clinical expert judgment

## Next Checks

1. **External Validity Testing**: Evaluate MDAgents on medical benchmark datasets not seen during development, particularly focusing on cases where the framework showed lower performance (MedBullets, Path-VQA) to identify systematic weaknesses.

2. **Human Expert Validation**: Have practicing physicians independently classify the same query set used in the study to measure agreement with the moderator's complexity assessments and validate that the framework's team assignments align with clinical judgment.

3. **Resource Efficiency Analysis**: Conduct comprehensive measurements of API calls, latency, and computational costs across all complexity levels to verify the claimed efficiency improvements over static group approaches, particularly for high-complexity cases where multiple agents are recruited.