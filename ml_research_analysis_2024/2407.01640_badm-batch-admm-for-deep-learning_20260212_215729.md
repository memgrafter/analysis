---
ver: rpa2
title: 'BADM: Batch ADMM for Deep Learning'
arxiv_id: '2407.01640'
source_url: https://arxiv.org/abs/2407.01640
tags:
- badm
- adam
- rmsprop
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new data-driven algorithm, Batch ADMM (BADM),
  for training deep neural networks. BADM splits training data into batches and sub-batches,
  updating primal and dual variables in parallel to generate global parameters through
  aggregation.
---

# BADM: Batch ADMM for Deep Learning

## Quick Facts
- arXiv ID: 2407.01640
- Source URL: https://arxiv.org/abs/2407.01640
- Authors: Ouya Wang; Shenglong Zhou; Geoffrey Ye Li
- Reference count: 40
- Key outcome: Proposes BADM algorithm achieving up to 4x faster pre-training for language modeling and 3.2x faster training for image generation models

## Executive Summary
This paper introduces Batch ADMM (BADM), a data-driven algorithm for training deep neural networks that splits training data into batches and sub-batches for parallel optimization. The method updates primal and dual variables in parallel within each batch and aggregates them to generate global parameters. BADM achieves a sublinear convergence rate under mild assumptions and demonstrates faster convergence and superior testing accuracy compared to state-of-the-art optimizers across various deep learning tasks including language modeling, image generation, and classification.

## Method Summary
BADM partitions training data into batches and sub-batches, enabling parallel computation of primal and dual variables within each batch. The algorithm solves sub-problems approximately using gradient information from sub-batches rather than exact solutions, balancing accuracy and computational efficiency. Unlike previous ADMM approaches that reformulate neural network architectures with equality constraints, BADM constructs an optimization problem directly from data partitioning, avoiding model-specific complexity while leveraging ADMM's decomposition benefits.

## Key Results
- Up to 4x faster pre-training for language modeling tasks
- 3.2x faster training for image generation models
- Higher testing accuracy for most classification tasks compared to Adam and other optimizers

## Why This Works (Mechanism)

### Mechanism 1
Parallel sub-batch optimization reduces wall-clock training time while preserving convergence properties. BADM partitions training data into batches and sub-batches, allowing parallel computation of primal and dual variables within each batch. This parallelism means multiple sub-batches can be processed simultaneously, reducing the sequential dependency that slows down traditional SGD.

### Mechanism 2
The data-driven ADMM framework avoids model-specific constraints while maintaining optimization efficiency. Unlike previous ADMM approaches that reformulate neural network architectures with equality constraints, BADM constructs an optimization problem directly from data partitioning. This avoids the complexity of handling model-specific constraints while still leveraging ADMM's decomposition benefits.

### Mechanism 3
The inexact sub-problem solution with gradient approximation maintains convergence while reducing computational cost. BADM solves sub-problems approximately using gradient information from sub-batches rather than exact solutions. This inexactness is controlled by parameters ρ and σ, allowing a balance between accuracy and computational efficiency.

## Foundational Learning

- Concept: Alternating Direction Method of Multipliers (ADMM)
  - Why needed here: ADMM provides the mathematical framework for decomposing the optimization problem and handling the data partitioning structure.
  - Quick check question: What are the three main steps in each ADMM iteration, and how do they apply to the BADM algorithm?

- Concept: Stochastic Gradient Descent (SGD) and its variants
  - Why needed here: Understanding SGD's limitations (slow convergence, sensitivity to learning rate) motivates the development of BADM as an alternative.
  - Quick check question: What are the key differences between standard SGD and BADM in terms of parameter updates and convergence guarantees?

- Concept: Lipschitz continuity and smoothness assumptions
  - Why needed here: These mathematical properties are essential for proving the convergence rate of BADM and understanding its theoretical guarantees.
  - Quick check question: How does the Lipschitz continuity assumption affect the convergence analysis of optimization algorithms like BADM?

## Architecture Onboarding

- Component map: Data partitioning -> Parallel processing -> Aggregation -> Convergence monitoring
- Critical path: Initialize parameters and partition data → Process batches in sequence, sub-batches in parallel → Update global parameters through aggregation → Monitor convergence and adjust hyperparameters
- Design tradeoffs: Sub-batch size vs. parallelism (smaller sub-batches enable more parallelism but increase approximation error); Exact vs. inexact sub-problem solutions (exact solutions guarantee better convergence but are computationally expensive); Batch size vs. memory usage (larger batches improve gradient estimates but require more memory)
- Failure signatures: Slow convergence (sub-batch size too small or approximation too coarse); Oscillating loss (improper parameter balancing between ρ and σ); Memory errors (batch/sub-batch sizes exceeding available memory)
- First 3 experiments: Run BADM on MNIST with B=2, S=2 to verify basic functionality and convergence; Compare BADM convergence speed against Adam on a medium-sized dataset with identical batch sizes; Test BADM's parallel efficiency by measuring speedup when increasing S on a multi-core machine

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of BADM scale with increasing batch and sub-batch sizes in terms of computational efficiency and convergence speed?
- Basis in paper: The paper discusses the data-splitting strategy of BADM into batches and sub-batches, and mentions parallel computing capabilities for sub-problems using sub-batch data.
- Why unresolved: While the paper demonstrates the effectiveness of BADM, it does not provide a detailed analysis of how varying the sizes of batches and sub-batches impacts computational efficiency and convergence speed across different tasks and model architectures.
-