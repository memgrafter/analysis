---
ver: rpa2
title: 'ODICE: Revealing the Mystery of Distribution Correction Estimation via Orthogonal-gradient
  Update'
arxiv_id: '2402.00348'
source_url: https://arxiv.org/abs/2402.00348
tags:
- learning
- gradient
- update
- offline
- o-dice
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the performance gap between DICE-based methods\
  \ and action-constrained offline RL methods, despite DICE\u2019s theoretical advantage\
  \ of imposing state-action-level constraints. The authors identify that the value\
  \ learning objective in DICE contains two conflicting gradient terms: a forward\
  \ gradient (imposing action-level constraint) and a backward gradient (on the next\
  \ state)."
---

# ODICE: Revealing the Mystery of Distribution Correction Estimation via Orthogonal-gradient Update

## Quick Facts
- arXiv ID: 2402.00348
- Source URL: https://arxiv.org/abs/2402.00348
- Reference count: 40
- Primary result: O-DICE achieves state-of-the-art performance on 13/15 D4RL tasks

## Executive Summary
This paper addresses the performance gap between DICE-based methods and action-constrained offline RL methods. The authors identify that the backward gradient term in DICE's value learning objective interferes with the forward gradient, causing gradient cancellation and poor performance. They propose orthogonal-gradient update, which projects the backward gradient onto the normal plane of the forward gradient, preserving state-level behavior regularization while avoiding interference with action-level constraints. O-DICE achieves state-of-the-art performance on D4RL benchmarks and demonstrates improved robustness to out-of-distribution states.

## Method Summary
O-DICE modifies the true-gradient update in DICE methods by projecting the backward gradient onto the normal plane of the forward gradient. This orthogonal-gradient update preserves the action-level constraint from the forward gradient while maintaining state-level behavior regularization through the projected backward gradient. The method uses a bidirectional target network and introduces hyperparameters η (strength of projected backward gradient) and λ (trade-off between linear and nonlinear terms). The algorithm is trained on offline datasets using the DICE framework and extracts policies from the learned value function.

## Key Results
- O-DICE achieves state-of-the-art performance on 13 out of 15 D4RL benchmark tasks
- The method demonstrates superior robustness to out-of-distribution states compared to baseline methods
- Orthogonal-gradient update shows better convergence properties and helps alleviate feature co-adaptation issues

## Why This Works (Mechanism)

### Mechanism 1
The backward gradient interferes with the forward gradient because consecutive states have similar representations, causing gradient cancellation. In true-gradient update, the backward gradient (on next state s') has a parallel component to the forward gradient (on current state s). This parallel component cancels out the action-level constraint imposed by the forward gradient, leading to poor performance.

### Mechanism 2
Orthogonal-gradient update projects the backward gradient onto the normal plane of the forward gradient, preserving state-level constraint while avoiding interference. The backward gradient is decomposed into parallel and perpendicular components. The parallel component is removed through projection, leaving only the perpendicular component which provides state-level regularization without interfering with action-level constraint.

### Mechanism 3
The projected backward gradient enforces state-level behavior regularization by improving feature representation across states. By preventing feature co-adaptation (where different states have similar representations), orthogonal-gradient update helps the value function learn better state representations. This improved representation makes the policy more robust to out-of-distribution states encountered during evaluation.

## Foundational Learning

- Concept: DICE-based methods and stationary distribution correction
  - Why needed here: Understanding DICE is essential to grasp why the backward gradient exists and why it causes problems
  - Quick check question: What is the key insight that allows DICE methods to work with only offline data without requiring explicit policy evaluation?

- Concept: True-gradient vs semi-gradient updates in reinforcement learning
  - Why needed here: The paper explicitly compares true-gradient update (which causes problems) with semi-gradient update (which works better but lacks state-level constraint)
  - Quick check question: What is the main difference between true-gradient and semi-gradient updates in terms of how they handle the value function backup?

- Concept: Feature co-adaptation and its impact on generalization
  - Why needed here: The paper argues that orthogonal-gradient helps alleviate feature co-adaptation, which is crucial for understanding its robustness benefits
  - Quick check question: How does feature co-adaptation manifest in value function learning, and why does it hurt generalization to out-of-distribution states?

## Architecture Onboarding

- Component map:
  - Value function Vθ with parameters θ
  - Policy πϕ with parameters ϕ (extracted from V)
  - Bidirectional target network (Vθ for s', Vθ for s)
  - Gradient projection module that computes orthogonal-gradient
  - Loss function with λ parameter trading off linear and nonlinear terms

- Critical path:
  1. Sample transition (s, a, r, s') from dataset
  2. Compute forward gradient g→ = ∇θf ∗(r + γVθ(s') - Vθ(s))
  3. Compute backward gradient g← = ∇θf ∗(r + γVθ(s') - Vθ(s))
  4. Project backward gradient onto normal plane: g⊥ = g← - (g←⊤g→/||g→||²)g→
  5. Update θ using combined gradient: θ ← θ - α[(1-λ)∇θVθ(s) + λ(g→ + ηg⊥)]
  6. Extract policy weights ω(s,a) and update πϕ

- Design tradeoffs:
  - η controls strength of state-level constraint vs action-level constraint
  - λ controls trade-off between direct V minimization and behavior regularization
  - Using bidirectional target network adds complexity but improves stability
  - Removing the +1 term in policy extraction weights improves performance

- Failure signatures:
  - If η is too large, the algorithm may behave like true-gradient update and suffer from poor performance
  - If η is too small, the state-level constraint may be ineffective
  - If λ is too small, the algorithm may not adequately constrain the policy
  - If λ is too large, the algorithm may underutilize the dataset

- First 3 experiments:
  1. Implement orthogonal-gradient projection layer and verify it produces perpendicular gradient to forward gradient
  2. Run on a simple grid-world toy problem comparing true-gradient, semi-gradient, and orthogonal-gradient variants
  3. Benchmark on a single D4RL task (e.g., hopper-medium) with different η values to find optimal setting

## Open Questions the Paper Calls Out

### Open Question 1
How sensitive is O-DICE's performance to the hyperparameter η (strength of projected backward gradient) across different environments? The paper states "we find in practice that the tuning of η is not sensitive" but only shows this within a limited range.

### Open Question 2
Does the orthogonal-gradient update generalize to online RL settings where DICE can be used as an off-policy algorithm? The paper mentions this as a future direction but focuses entirely on offline settings.

### Open Question 3
How does the orthogonal-gradient update perform when extended to minimize MSE loss of Bellman equation rather than DICE's f-divergence objectives? The paper suggests this as a future work but doesn't explore this extension.

## Limitations
- The theoretical analysis assumes bounded gradient norms and Lipschitz continuity, which may not hold in practice with neural network function approximation
- Optimal hyperparameter values require task-specific tuning through grid search, suggesting potential sensitivity
- The method is primarily evaluated on continuous control tasks and may not generalize to all RL domains

## Confidence
- High confidence in empirical superiority on D4RL benchmarks (13/15 tasks)
- Medium confidence in theoretical analysis of gradient interference and orthogonal projection
- Medium confidence in claims about alleviating feature co-adaptation

## Next Checks
1. **Gradient Interference Validation**: Conduct controlled experiments on a simple MDP where forward and backward gradients can be explicitly computed to verify gradient cancellation effect
2. **Hyperparameter Sensitivity Analysis**: Perform systematic ablation studies on η and λ values to understand their impact and identify robust default settings
3. **Generalization Across Domains**: Evaluate O-DICE on discrete action space tasks (e.g., Atari games) and other offline RL benchmarks beyond D4RL to assess broader applicability