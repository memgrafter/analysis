---
ver: rpa2
title: Robust Image Classification in the Presence of Out-of-Distribution and Adversarial
  Samples Using Attractors in Neural Networks
arxiv_id: '2406.10579'
source_url: https://arxiv.org/abs/2406.10579
tags:
- network
- samples
- adversarial
- training
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a robust method for image classification in
  the presence of out-of-distribution (OOD) and adversarial samples using attractor
  networks. The method utilizes a fully connected neural network trained with an eigenvalue-based
  loss function (Leigen) to learn training samples as attractors, enhancing robustness.
---

# Robust Image Classification in the Presence of Out-of-Distribution and Adversarial Samples Using Attractors in Neural Networks

## Quick Facts
- arXiv ID: 2406.10579
- Source URL: https://arxiv.org/abs/2406.10579
- Reference count: 0
- Primary result: Achieves 87.13% accuracy on highly perturbed MNIST test data and 98.84%/99.28% OOD detection accuracy for Fashion-MNIST/CIFAR-10-bw under severe adversarial attacks

## Executive Summary
This paper proposes a novel approach to robust image classification that simultaneously handles out-of-distribution (OOD) detection and adversarial attacks using attractor networks. The method trains a fully connected neural network with an eigenvalue-based loss function (Leigen) to learn training samples as stable attractors, creating a robust feature space where inputs converge to learned patterns even under perturbation. By comparing input images with their reconstructions using Pearson correlation, the network can distinguish between in-distribution samples and OOD samples with high accuracy while maintaining strong classification performance under adversarial attacks.

## Method Summary
The approach utilizes a fully connected autoencoder architecture trained on a subset of MNIST samples (1000 samples) using a combination of reconstruction loss, classification loss, and the novel Leigen loss that constrains eigenvalues of the Jacobian matrix to ensure attractor stability. During inference, the network iteratively processes inputs through the autoencoder, with perturbed inputs converging toward their nearest attractor through multiple passes. OOD detection is performed by comparing the final reconstruction with the original input using Pearson correlation - high correlation indicates in-distribution samples while low correlation indicates OOD samples. The method is evaluated on MNIST, Fashion-MNIST, and CIFAR-10-bw datasets under various perturbation levels and adversarial attacks.

## Key Results
- Achieves 87.13% classification accuracy on highly perturbed MNIST test data
- OOD detection accuracy of 98.84% for Fashion-MNIST and 99.28% for CIFAR-10-bw
- Maintains 98.48% and 98.88% OOD detection accuracy under severe adversarial attacks
- Significantly outperforms baseline training without Leigen loss under high perturbation levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training ID samples as attractors stabilizes their fixed points in the autoencoder dynamics.
- Mechanism: The eigenvalue-based loss (Leigen) constrains the Jacobian of the network to have eigenvalues with magnitude < 1, ensuring convergence to learned patterns even under perturbations.
- Core assumption: The network dynamics form a stable recurrent system where input trajectories converge to attractors.
- Evidence anchors:
  - [abstract] "The proposed approach utilizes a fully connected neural network that is trained to use training samples as its attractors, enhancing its robustness."
  - [section] "According to Theorem 1, this loss function causes training data to be attractors of the network rather than repellers."
  - [corpus] Weak - no direct neighbor paper discusses attractor-based training or Leigen loss.

### Mechanism 2
- Claim: High similarity between input and reconstruction (measured by Pearson correlation) reliably separates ID from OOD samples.
- Mechanism: ID samples, being attractors, reconstruct with minimal error, yielding high correlation. OOD samples lack representation in the attractor landscape, producing low correlation.
- Core assumption: The attractor training ensures ID samples lie in distinct basins from OOD samples in the feature space.
- Evidence anchors:
  - [abstract] "the network can distinguish these samples from MNIST samples with an accuracy of 98.84% and 99.28%, respectively."
  - [section] "If the similarity is greater than a certain threshold, the input is classified as an ID example. On the other hand, if the similarity is smaller than the threshold, the input is classified as an OOD example."
  - [corpus] Weak - neighbors focus on geometric or GAN-based defenses, not similarity-based reconstruction.

### Mechanism 3
- Claim: Iterative reconstruction through the attractor network removes adversarial perturbations.
- Mechanism: Each iteration projects the perturbed input closer to its attractor, effectively denoising the input before final classification or OOD detection.
- Core assumption: The attractor basin is large enough to encompass perturbed versions of the clean input.
- Evidence anchors:
  - [abstract] "achieving 87.13% accuracy when dealing with highly perturbed MNIST test data."
  - [section] "This system can be expressed as a first-order recurrent relationship... When k → ∞ and the system's output remain constant, xk+1 equals xk, indicating that xk is a fixed point."
  - [corpus] Missing - no neighbor discusses iterative attractor-based denoising.

## Foundational Learning

- Concept: Recurrent neural networks and fixed-point dynamics
  - Why needed here: The attractor network relies on iterative feedback to converge to stable patterns.
  - Quick check question: What condition on the Jacobian guarantees that a fixed point is stable?

- Concept: Eigenvalue analysis of Jacobian matrices
  - Why needed here: Leigen loss directly constrains eigenvalues to ensure attractor stability.
  - Quick check question: How does the magnitude of eigenvalues affect the convergence behavior of a dynamical system?

- Concept: Pearson correlation for image similarity
  - Why needed here: Used as the similarity metric between input and reconstruction for OOD detection.
  - Quick check question: Why is Pearson correlation preferred over mean squared error in this context?

## Architecture Onboarding

- Component map:
  Input layer (784 neurons) -> Encoder stack (500 -> 300 -> 300 -> 300) -> Bottleneck layer (300 neurons) -> Decoder stack (300 -> 500 -> 784) -> Classification head (10 neurons, Softmax) -> Feedback loop from output to input during inference

- Critical path:
  1. Forward pass through encoder/decoder
  2. Compute reconstruction error and classification loss
  3. Compute Leigen loss (Jacobian eigenvalues)
  4. Backpropagate combined loss

- Design tradeoffs:
  - Larger networks increase attractor capacity but risk overfitting
  - More iterations improve denoising but increase inference time
  - Threshold selection for OOD detection balances false positives/negatives

- Failure signatures:
  - Low reconstruction similarity for clean ID samples → training instability
  - High similarity for OOD samples → insufficient feature discrimination
  - Classification accuracy drops sharply under perturbation → attractor basin too small

- First 3 experiments:
  1. Train baseline network (no Leigen) and measure accuracy drop under FGSM attack
  2. Train attractor network with Leigen and compare OOD detection ROC curves
  3. Vary FGSM epsilon and plot classification accuracy vs. perturbation level for both models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the attractor network scale with the size of the training dataset? Specifically, would using more than 1000 training samples further improve robustness and classification accuracy?
- Basis in paper: [explicit] The paper mentions that 1000 samples were used due to computational cost constraints, and that the network achieved 88.73% accuracy on MNIST test data with this subset.
- Why unresolved: The paper does not explore the performance impact of using larger training sets, which would be important for real-world applications.
- What evidence would resolve it: Systematic experiments varying the training set size (e.g., 1000, 5000, 10000, 50000) while measuring classification accuracy and OOD detection performance under adversarial attacks.

### Open Question 2
- Question: How does the proposed method compare to state-of-the-art OOD detection techniques like ODIN, Mahalanobis, and OE in terms of robustness to adversarial attacks?
- Basis in paper: [explicit] The paper mentions these methods are vulnerable to adversarial perturbations but does not directly compare its approach to them.
- Why unresolved: Without direct comparison, it's unclear how much improvement the attractor-based approach provides over existing methods.
- What evidence would resolve it: Benchmark experiments comparing the attractor network's OOD detection and classification performance against these methods under identical adversarial attack conditions.

### Open Question 3
- Question: Can the attractor network's robustness be further enhanced by incorporating additional architectural modifications or training strategies?
- Basis in paper: [inferred] The paper demonstrates significant improvement over baseline training but does not explore other potential enhancements like different network architectures, loss functions, or training procedures.
- Why unresolved: The current approach may not represent the optimal configuration for combining attractor training with OOD detection.
- What evidence would resolve it: Systematic ablation studies testing various architectural modifications (e.g., different activation functions, layer configurations) and training strategies (e.g., adversarial training, curriculum learning) while measuring robustness to attacks.

## Limitations

- The paper lacks detailed implementation of the Leigen loss function and eigenvalue constraints, making exact reproduction difficult
- Limited ablation studies prevent understanding the individual contribution of each training component to overall performance
- Only tested on grayscale datasets (MNIST, Fashion-MNIST, CIFAR-10-bw), limiting generalizability to color images

## Confidence

- **High**: Experimental results on MNIST/Fashion-MNIST/CIFAR-10-bw datasets and baseline comparisons
- **Medium**: The general attractor network concept and reconstruction-based OOD detection
- **Low**: Specific implementation of Leigen loss and eigenvalue constraints

## Next Checks

1. Implement the Leigen loss function and verify that training samples become stable fixed points by measuring Jacobian eigenvalues
2. Compare reconstruction similarity distributions for ID vs OOD samples across multiple similarity metrics (MSE, cosine similarity, Pearson correlation)
3. Test attractor network robustness against adaptive adversarial attacks where attackers know the reconstruction mechanism