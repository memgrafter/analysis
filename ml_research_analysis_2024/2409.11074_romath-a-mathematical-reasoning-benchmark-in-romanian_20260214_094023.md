---
ver: rpa2
title: 'RoMath: A Mathematical Reasoning Benchmark in Romanian'
arxiv_id: '2409.11074'
source_url: https://arxiv.org/abs/2409.11074
tags:
- problems
- solution
- problem
- arxiv
- romanian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces RoMath, a Romanian mathematical reasoning
  benchmark suite consisting of three subsets: Baccalaureate, Competitions, and Synthetic.
  The benchmark addresses the lack of mathematical reasoning datasets in non-English
  languages by providing 76,910 problems across various mathematical domains and difficulty
  levels.'
---

# RoMath: A Mathematical Reasoning Benchmark in Romanian

## Quick Facts
- arXiv ID: 2409.11074
- Source URL: https://arxiv.org/abs/2409.11074
- Reference count: 40
- This paper introduces RoMath, a Romanian mathematical reasoning benchmark suite consisting of three subsets: Baccalaureate, Competitions, and Synthetic, addressing the lack of mathematical reasoning datasets in non-English languages.

## Executive Summary
This paper introduces RoMath, a Romanian mathematical reasoning benchmark suite consisting of three subsets: Baccalaureate, Competitions, and Synthetic. The benchmark addresses the lack of mathematical reasoning datasets in non-English languages by providing 76,910 problems across various mathematical domains and difficulty levels. The authors collected and curated problems from Romanian baccalaureate exams and competitions, and generated synthetic problems programmatically. They benchmarked several open-weight language models under zero-shot, fine-tuning, and verifiable reward scenarios, finding that Romanian-specialized models perform competitively with English-centric models despite limited training on Romanian math tokens. The work highlights the importance of dedicated resources for underrepresented languages, showing that direct translation of problems degrades performance significantly due to challenges in translating precise mathematical language.

## Method Summary
The authors developed a semi-automatic workflow to collect and curate mathematical problems from Romanian baccalaureate exams, competitions, and synthetic generation. They used foundational LLMs (specifically Claude 3 Sonnet) to parse and annotate problems, ensuring structured output in JSON format with metadata. The benchmark suite consists of 76,910 problems across three subsets: Baccalaureate (5,777 problems), Competitions (1,133 problems), and Synthetic (63,000 problems). The evaluation framework includes zero-shot inference, LoRA fine-tuning, and verifiable reward scenarios using LLM-as-a-judge approaches. Models are evaluated on accuracy and solution correctness, with special attention to the challenges of mathematical translation and cross-linguistic generalization.

## Key Results
- Romanian-specialized models achieve competitive performance with English-centric models despite limited Romanian math token training
- Direct translation of mathematical problems from Romanian to English significantly degrades performance due to loss of precise mathematical language
- Fine-tuning effectiveness varies significantly based on training data format matching test data format

## Why This Works (Mechanism)

### Mechanism 1
Romanian language models achieve competitive performance on Romanian mathematical reasoning despite limited math-specific training data by leveraging broader Romanian language understanding to compensate for sparse mathematical token exposure. Mathematical reasoning ability is transferable across languages, and strong general language understanding facilitates mathematical comprehension. This mechanism breaks down when mathematical problems require deep integration of specialized mathematical concepts that differ significantly from general language patterns.

### Mechanism 2
Translation from Romanian to English degrades mathematical reasoning performance due to loss of precise mathematical language because mathematical expressions and terminology cannot be adequately preserved during translation. Mathematical language has unique characteristics that are not fully captured by general-purpose translation models. This mechanism fails when translation models are specifically trained on mathematical corpora or when mathematical expressions are kept intact during translation.

### Mechanism 3
Fine-tuning improves mathematical reasoning performance only when training data matches the target problem format and style because models learn to adapt to specific problem presentation formats and solution styles present in the training data. Mathematical reasoning performance depends on familiarity with problem format rather than just mathematical knowledge. This mechanism breaks when training data format matches test data format, or when models are evaluated on problems requiring generalization beyond training format.

## Foundational Learning

- **Mathematical reasoning vs natural language processing**: Understanding that mathematical reasoning involves different cognitive processes than general NLP tasks, requiring specialized evaluation approaches. *Quick check*: Can you identify three key differences between mathematical reasoning and standard NLP tasks?
- **Chain-of-thought reasoning and intermediate steps**: Many mathematical problems require multiple reasoning steps, and the presence of intermediate steps significantly impacts model performance. *Quick check*: Why does the presence of intermediate reasoning steps in solutions enable better model performance on mathematical problems?
- **Mathematical notation and LaTeX processing**: Mathematical problems often contain complex notation that requires proper parsing and understanding for correct solutions. *Quick check*: What challenges arise when mathematical expressions are embedded in natural language text?

## Architecture Onboarding

- **Component map**: OCR pipeline → LLM parsing → JSON structuring → Judge evaluation → Performance analysis
- **Critical path**: Problem collection → Structured formatting → Model evaluation → Results analysis
- **Design tradeoffs**: Balance between automated processing (OCR, LLM parsing) and quality control (manual inspection, validation)
- **Failure signatures**: Poor OCR quality leading to incorrect LaTeX parsing, LLM parsing errors creating malformed JSON, judge evaluation bias affecting results
- **First 3 experiments**:
  1. Test OCR pipeline on sample PDFs with varying quality to establish accuracy baseline
  2. Evaluate LLM parsing performance on different problem types and complexity levels
  3. Compare judge model performance across different problem categories and solution types

## Open Questions the Paper Calls Out

1. How does performance on RoMath compare to English math benchmarks when both are translated to Romanian? The paper only tests English-to-Romanian translation degradation, not Romanian-to-English translation comparison.
2. What specific linguistic features of Romanian make mathematical reasoning uniquely challenging compared to other languages? The paper identifies Romanian's uniqueness but doesn't specify which linguistic features create mathematical reasoning challenges.
3. Why does fine-tuning not reliably improve performance across all RoMath subsets and model types? The paper proposes potential explanations but doesn't investigate the underlying reasons for inconsistent fine-tuning effects.

## Limitations

- Limited corpus evidence for mechanism validation, with minimal related work (25 papers, average 0 citations)
- Translation performance degradation claims based on single-reference scoring against Romanian ground truth without comprehensive validation
- Fine-tuning effectiveness variability lacks detailed ablation studies showing specific aspects of format mismatch impact

## Confidence

**High confidence**: The existence of RoMath as a Romanian mathematical reasoning benchmark is well-established with clear methodology for data collection and curation.

**Medium confidence**: The claim that direct translation degrades mathematical reasoning performance has experimental support but lacks comprehensive validation across different translation methods and mathematical domains.

**Low confidence**: The assertion that fine-tuning effectiveness depends critically on training data matching test data format is based on limited experimental variations.

## Next Checks

1. Conduct controlled experiments comparing mathematical reasoning performance across multiple translation approaches (human translation, specialized math translation models, different commercial translation APIs) to quantify the specific impact of translation quality on mathematical problem solving accuracy.

2. Evaluate whether models trained on RoMath can generalize to English mathematical reasoning tasks and vice versa, testing the hypothesis that mathematical reasoning ability transfers across languages despite linguistic differences.

3. Systematically vary problem presentation formats (solution style, notation systems, problem structure) in the training data to identify which specific format elements most strongly influence fine-tuning effectiveness and model performance.