---
ver: rpa2
title: 'AdaComp: Extractive Context Compression with Adaptive Predictor for Retrieval-Augmented
  Large Language Models'
arxiv_id: '2409.01579'
source_url: https://arxiv.org/abs/2409.01579
tags:
- documents
- compression
- context
- query
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AdaComp, an extractive context compression
  method for Retrieval-Augmented Generation (RAG) that adaptively determines the compression
  rate based on query complexity and retrieval quality. AdaComp annotates the minimum
  top-k documents needed to answer a query, constructs triplets of query, documents,
  and compression rate, and trains a predictor to select the optimal subset of documents
  during inference.
---

# AdaComp: Extractive Context Compression with Adaptive Predictor for Retrieval-Augmented Large Language Models

## Quick Facts
- **arXiv ID**: 2409.01579
- **Source URL**: https://arxiv.org/abs/2409.01579
- **Reference count**: 4
- **Primary result**: AdaComp achieves nearly identical performance to uncompressed RAG models while significantly reducing inference costs by 40-60% on three QA datasets and one conversational multi-doc QA dataset.

## Executive Summary
This paper introduces AdaComp, an adaptive context compression method for Retrieval-Augmented Generation (RAG) systems that dynamically determines the optimal compression rate based on query complexity and retrieval quality. The method uses a compression-rate predictor trained to select the minimal subset of retrieved documents needed to answer each query accurately. By annotating triplets of query, documents, and optimal compression rate from real RAG system feedback, AdaComp learns to predict the appropriate number of documents to retain during inference, achieving substantial efficiency gains without performance degradation.

## Method Summary
AdaComp employs a compression-rate predictor that determines the optimal number of documents to retain from retrieved context based on query complexity and retrieval quality. The method annotates training data by identifying the minimum top-k documents needed for correct answer generation from real RAG system feedback. During inference, the predictor selects the appropriate compression rate, which is then used to filter the retrieved documents before passing them to the generation model. The predictor is fine-tuned using LoRA on a Llama2-7B model with annotated triplets of query, retrieved documents, and compression rates.

## Key Results
- AdaComp reduces inference costs by 40-60% while maintaining performance nearly identical to uncompressed models
- Outperforms baseline compression methods (RECOMP and FILCO) across all three standard QA datasets and one conversational multi-doc QA dataset
- Demonstrates significant reduction in token count while preserving exact match and F1 scores

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The compression rate predictor improves efficiency by selecting only the minimal number of documents needed to answer the query, reducing computational load.
- **Mechanism**: The predictor is trained to estimate the optimal number of documents (compression rate) based on the query and retrieved documents. During inference, it selects the top-k documents based on this prediction, ensuring concise context without over-compression.
- **Core assumption**: The retriever ranks relevant documents at the top, and the predictor can accurately estimate the number of documents needed based on query complexity and retrieval quality.
- **Evidence anchors**: [abstract] "Experiments on three QA datasets and one conversational Multi-doc QA dataset show that AdaComp significantly reduces inference costs while maintaining performance nearly identical to uncompressed models." [section 3.2] "We employ a data annotation method based on real RAG system feedback... The optimal subset D′ is the smallest subset Dk for which the system generates a correct response."

### Mechanism 2
- **Claim**: The method adapts to both query complexity and retrieval quality, dynamically adjusting the number of documents retained.
- **Mechanism**: Complex queries like multi-hop questions require more documents for comprehensive judgment, while simpler queries need fewer. Low-quality retrievals may require more documents to synthesize accurate outputs.
- **Core assumption**: Query complexity and retrieval quality significantly impact the number of documents needed for accurate responses.
- **Evidence anchors**: [abstract] "complex queries like multi-hop questions may require retaining more documents than simpler queries, and a low-quality retrieval may need to rely on more documents to generate accurate outputs." [section 1] "We find that in most cases, the retriever can rank relevant documents at the top, but the exact number of documents needed to answer the query is uncertain due to the impact of query complexity and retrieval quality."

### Mechanism 3
- **Claim**: The method maintains performance while significantly reducing the number of tokens used in inference.
- **Mechanism**: By selecting the optimal subset of documents, the method ensures the generation model has access to sufficient information while minimizing noise and computational overhead.
- **Core assumption**: Reducing the number of tokens without losing critical information improves inference efficiency without degrading performance.
- **Evidence anchors**: [abstract] "Experiments on three QA datasets and one conversational Multi-doc QA dataset show that AdaComp significantly reduces inference costs while maintaining performance nearly identical to uncompressed models." [section 4.2] "Our AdaComp method performs better across all three datasets compared to the RECOMP and FILCO methods. It also significantly reduces the number of tokens, indicating that the adaptive compression strategy strikes a good balance between token count and performance."

## Foundational Learning

- **Concept**: Understanding of retrieval-augmented generation (RAG) and its challenges
  - **Why needed here**: The method is designed to improve RAG by addressing issues of noise and computational inefficiency in retrieved documents
  - **Quick check question**: What are the main challenges faced by RAG systems when dealing with noisy or irrelevant documents?

- **Concept**: Familiarity with context compression techniques and their limitations
  - **Why needed here**: The method builds on existing compression strategies but introduces adaptive mechanisms to overcome their limitations
  - **Quick check question**: How do traditional extractive and generative compression methods differ, and what are their common pitfalls?

- **Concept**: Knowledge of query complexity and its impact on document retrieval and selection
  - **Why needed here**: The method's effectiveness relies on accurately assessing query complexity to determine the optimal number of documents
  - **Quick check question**: How does the complexity of a query (e.g., multi-hop vs. simple) affect the number of documents needed to generate an accurate response?

## Architecture Onboarding

- **Component map**: Retriever module (R) -> Compression module (Cθ) -> Generation module (G)
- **Critical path**: 1) Query is processed by the retriever to obtain a set of documents. 2) The compression module predicts the optimal number of documents (compression rate) based on the query and retrieved documents. 3) The selected documents are used by the generation module to produce the response.
- **Design tradeoffs**: Balancing between over-compression (losing critical information) and under-compression (retaining too much noise). Ensuring the predictor accurately estimates the compression rate without extensive retraining for different RAG systems.
- **Failure signatures**: Predictor consistently underestimates the number of documents needed, leading to incomplete answers. Model fails to recognize query complexity, resulting in inappropriate document selection. Significant performance drop when reducing the number of tokens due to loss of critical information.
- **First 3 experiments**: 1) Test the predictor's accuracy on a small dataset to ensure it can correctly estimate the compression rate. 2) Evaluate the method's performance on a single dataset (e.g., NQ) with varying levels of query complexity to assess adaptability. 3) Compare the method's efficiency and performance against baseline compression methods on multiple datasets to validate improvements.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text provided.

## Limitations

- The approach assumes relevant documents are consistently ranked at the top by the retriever, which may not hold across different retrieval systems or domains.
- The compression-rate predictor's accuracy is crucial for performance, yet the paper provides limited evaluation of its reliability, particularly for edge cases where queries require information from lower-ranked documents.
- The method's generalization across different retriever architectures (beyond the Dense Passage Retriever) and larger models remains unclear.

## Confidence

**High confidence**: The method's core mechanism of adaptive compression rate prediction based on query complexity and retrieval quality is well-supported by the experimental results showing maintained performance with reduced token counts.

**Medium confidence**: The claim that the method significantly reduces inference costs is supported by token count reductions, but the actual computational efficiency gains depend on implementation details not fully specified in the paper.

**Low confidence**: The generalizability of the approach to different retriever architectures and the robustness of the predictor across diverse query distributions remain uncertain due to limited ablation studies and cross-domain evaluations.

## Next Checks

1. **Predictor Robustness Test**: Conduct a detailed analysis of the compression-rate predictor's performance across different query types, including edge cases where the required documents span across the top-5 retrieved documents. Measure the frequency of under-estimation and its impact on final answer quality.

2. **Cross-Retriever Evaluation**: Test AdaComp with alternative retriever architectures (e.g., BM25, sparse retrievers) to assess whether the method's effectiveness depends on the Dense Passage Retriever's specific ranking behavior. Compare compression rate predictions and performance across retrievers.

3. **Computational Efficiency Benchmark**: Measure the actual inference time and memory usage of the full AdaComp pipeline (including predictor inference) versus baseline RAG systems. Compare the overhead of the predictor against the savings from reduced context length to determine net efficiency gains.