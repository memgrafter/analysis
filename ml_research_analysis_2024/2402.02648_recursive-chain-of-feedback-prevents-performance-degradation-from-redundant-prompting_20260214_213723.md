---
ver: rpa2
title: Recursive Chain-of-Feedback Prevents Performance Degradation from Redundant
  Prompting
arxiv_id: '2402.02648'
source_url: https://arxiv.org/abs/2402.02648
tags:
- prompting
- response
- feedback
- llms
- incorrect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how repetitive, meaningless feedback ("make
  another attempt") degrades LLM performance on multi-step reasoning tasks. The authors
  define this behavior as Chain-of-Feedback (CoF), showing that with each additional
  attempt, the deviation from the correct answer increases.
---

# Recursive Chain-of-Feedback Prevents Performance Degradation from Redundant Prompting

## Quick Facts
- arXiv ID: 2402.02648
- Source URL: https://arxiv.org/abs/2402.02648
- Authors: Jinwoo Ahn; Kyuseung Shin
- Reference count: 2
- One-line primary result: R-CoF enables solving problems that initial LLM attempts failed without requiring sample data or extensive training

## Executive Summary
This paper investigates how repetitive, meaningless feedback ("make another attempt") degrades LLM performance on multi-step reasoning tasks. The authors define this behavior as Chain-of-Feedback (CoF), showing that with each additional attempt, the deviation from the correct answer increases. To address this, they propose Recursive Chain-of-Feedback (R-CoF), which breaks down incorrect reasoning steps into smaller subproblems and recursively revises them until correct. Using the MATH dataset and GPT-3.5-turbo, they demonstrate that R-CoF enables solving problems that initial LLM attempts failed, without requiring sample data or extensive training. While quantitative results are limited in this preliminary work, the approach shows promise for improving unsupervised reasoning in real-world settings where users lack domain expertise to verify full solutions.

## Method Summary
The Recursive Chain-of-Feedback (R-CoF) method addresses performance degradation from redundant prompting by recursively revising incorrect reasoning steps. When an LLM provides an incorrect multi-step solution, R-CoF identifies the incorrect step, isolates it as a subproblem, and solves it recursively using the same multi-step reasoning approach. This process preserves all previously correct work while fixing specific errors. The method uses manual verification to identify incorrect steps, then requests the LLM to adjust only the incorrect portion while freezing all correct steps. This continues recursively until the correct solution is reached or a failure threshold is met.

## Key Results
- R-CoF successfully solves problems that initial LLM attempts fail to answer correctly
- The method requires no sample data or extensive training
- Manual verification of intermediate steps is more tractable than verifying complete solutions
- Performance degradation from generic feedback can be prevented through recursive step-by-step correction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Repetitive meaningless feedback causes LLMs to deviate further from the correct answer
- Mechanism: Each "make another attempt" prompt forces the LLM to restart reasoning from scratch, potentially losing track of correct intermediate steps and compounding errors
- Core assumption: LLMs don't have persistent memory of their previous correct reasoning when given generic feedback
- Evidence anchors:
  - [abstract] "with each additional attempt, the deviation from the correct answer increases"
  - [section] "we provide a meaningless prompt that does not provide much feedback for its previous response"
  - [corpus] Weak - no direct corpus evidence found for this specific degradation mechanism
- Break condition: If the LLM encounters a step it can solve correctly despite the generic feedback, the deviation may not increase

### Mechanism 2
- Claim: Recursive breakdown of incorrect steps allows targeted correction without losing correct progress
- Mechanism: By isolating only the incorrect step and solving it independently with the same multi-step reasoning approach, R-CoF preserves all previously correct work while fixing specific errors
- Core assumption: Smaller sub-problems are easier for LLMs to solve correctly than the full problem
- Evidence anchors:
  - [abstract] "R-CoF recursively revises the initially incorrect response by breaking down each incorrect reasoning step into smaller individual problems"
  - [section] "request the LLMs to adjust its previously incorrect response by freezing all the correct steps and adjusting only the incorrect step"
  - [corpus] Weak - related work on recursive reasoning exists but not this specific mechanism
- Break condition: If the recursive breakdown creates subproblems that are still too complex or if the LLM fails to solve even simplified subproblems

### Mechanism 3
- Claim: Manual verification of intermediate steps is more tractable than verifying complete solutions
- Mechanism: Users can more easily identify which specific step in a multi-step solution is incorrect rather than evaluating the entire reasoning chain
- Core assumption: Individual reasoning steps are simpler to verify than complete multi-step solutions
- Evidence anchors:
  - [section] "the user verifies manually which step is incorrect--the difficulty of this action is easier than attempting to identify errors in the entire reasoning"
  - [abstract] "allows us to break down problems into smaller steps that do not require intensive knowledge to verify"
  - [corpus] Weak - no direct corpus evidence found for this verification difficulty claim
- Break condition: If the individual steps remain too complex for manual verification or if the user cannot identify the incorrect step

## Foundational Learning

- Concept: Multi-step reasoning in LLMs
  - Why needed here: Understanding how LLMs break down complex problems into sequential steps is fundamental to both CoF and R-CoF mechanisms
  - Quick check question: What are the typical failure modes when LLMs perform multi-step reasoning on complex problems?

- Concept: Prompt engineering and feedback loops
  - Why needed here: The paper's core contribution relies on understanding how different types of feedback affect LLM behavior and performance
  - Quick check question: How does the specificity of feedback influence the quality of LLM responses in iterative settings?

- Concept: Recursive problem decomposition
  - Why needed here: R-CoF's effectiveness depends on the LLM's ability to recursively break down problems into smaller subproblems
  - Quick check question: What characteristics make a problem suitable for recursive decomposition by an LLM?

## Architecture Onboarding

- Component map: Input parser -> LLM reasoning engine -> Error detector -> Recursive solver -> Solution assembler
- Critical path: Question → Initial LLM response → Error identification → Subproblem isolation → Recursive solving → Solution assembly
- Design tradeoffs:
  - Manual vs. automated error detection (current version uses manual)
  - Single vs. multiple LLM instances for recursive solving
  - Fixed vs. adaptive subproblem sizing
- Failure signatures:
  - Increasing deviation from correct answer in CoF experiments
  - Failure to solve subproblems in recursive calls
  - Inability to identify incorrect steps in manual verification
- First 3 experiments:
  1. Reproduce CoF baseline showing degradation with generic feedback
  2. Implement automated error detection and compare to manual verification
  3. Test R-CoF on problems with varying complexity levels and step counts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Recursive Chain-of-Feedback (R-CoF) approach consistently improve performance across different types of mathematical problems, such as geometry, algebra, and calculus?
- Basis in paper: [inferred] The authors mention expanding the domain of the dataset to include more closed-ended questions in other STEM fields, but the current results are limited to the MATH dataset.
- Why unresolved: The paper is still ongoing work, and the authors have not yet performed experiments with diverse mathematical problem types.
- What evidence would resolve it: Quantitative results showing the performance of R-CoF on a broader range of mathematical problems across different domains.

### Open Question 2
- Question: How does the Recursive Chain-of-Feedback (R-CoF) method compare to other established prompting techniques like Chain-of-Thought (CoT) in terms of accuracy and efficiency?
- Basis in paper: [explicit] The authors acknowledge the need to compare R-CoF to other prompting methods like Chain-of-Thought prompting, but this comparison has not been conducted yet.
- Why unresolved: The paper is ongoing, and the authors have not performed a comparative analysis with other prompting methods.
- What evidence would resolve it: Experimental results comparing R-CoF to CoT and other methods on standardized benchmarks.

### Open Question 3
- Question: Can the Recursive Chain-of-Feedback (R-CoF) approach be generalized to non-mathematical reasoning tasks, such as logical reasoning or natural language inference?
- Basis in paper: [inferred] The authors focus on mathematical problems in their preliminary experiments, but the method's applicability to other reasoning tasks is not explored.
- Why unresolved: The paper is still ongoing, and the authors have not yet tested R-CoF on diverse reasoning tasks beyond mathematics.
- What evidence would resolve it: Results from experiments applying R-CoF to logical reasoning, natural language inference, or other non-mathematical tasks.

## Limitations
- No quantitative results presented to validate core claims about performance degradation or improvement
- Manual error detection limits scalability and real-world applicability
- Limited discussion of when R-CoF might fail or be less effective than alternatives

## Confidence

**Low confidence** in the CoF degradation mechanism - While the authors claim that meaningless feedback causes increasing deviation from correct answers, no empirical data demonstrates this effect across multiple examples or quantifies the degradation rate.

**Medium confidence** in the R-CoF theoretical framework - The recursive decomposition approach is logically sound and builds on established concepts, but its practical effectiveness remains unproven without quantitative results.

**Low confidence** in the manual verification advantage claim - The assertion that intermediate step verification is easier than full solution verification lacks supporting evidence or user studies.

## Next Checks

1. **Empirical CoF baseline validation**: Run controlled experiments with 20-30 MATH problems to measure actual deviation increases across 5-7 generic feedback iterations, establishing the baseline degradation pattern claimed by the authors.

2. **Automated error detection comparison**: Implement and test an automated approach for identifying incorrect reasoning steps (e.g., using LLM-based verification) against the manual method to assess scalability and accuracy differences.

3. **R-CoF performance benchmarking**: Measure the success rate and iteration counts of R-CoF on problems of varying complexity (2-10 step problems) and compare against baseline approaches like simply restarting the problem with a new prompt.