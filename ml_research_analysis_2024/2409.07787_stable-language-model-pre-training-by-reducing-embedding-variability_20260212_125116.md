---
ver: rpa2
title: Stable Language Model Pre-training by Reducing Embedding Variability
arxiv_id: '2409.07787'
source_url: https://arxiv.org/abs/2409.07787
tags:
- pre-training
- gradient
- arxiv
- preprint
- mlra
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Token Embedding Variability (TEV) as an efficient
  proxy for assessing pre-training stability in language models with pre-layer normalization,
  addressing the high computational cost of traditional gradient variance monitoring.
  The authors propose Multi-head Low-Rank Attention (MLRA) to mitigate instability
  by reducing output embedding variance through factorized attention projection matrices.
---

# Stable Language Model Pre-training by Reducing Embedding Variability

## Quick Facts
- arXiv ID: 2409.07787
- Source URL: https://arxiv.org/abs/2409.07787
- Reference count: 24
- Primary result: MLRA achieves lower gradient variance and improved perplexity in deep GPT-2 models compared to baselines

## Executive Summary
This paper addresses pre-training instability in deep language models with pre-layer normalization by introducing Token Embedding Variability (TEV) as an efficient stability proxy and Multi-head Low-Rank Attention (MLRA) as an architectural solution. TEV provides a computationally efficient alternative to gradient variance monitoring by tracking the standard deviation of token embedding weights. MLRA reduces instability by factorizing attention projection matrices to limit the exponential growth of output embedding variance through layers. The authors demonstrate that MLRA achieves lower gradient variance (10^1 vs 10^2 at 1B tokens for 96-layer models) and improved perplexity (53.69 vs 62.31 on Lambada for 192 layers) compared to baseline GPT-2 models.

## Method Summary
The authors propose two complementary approaches to address pre-training instability: TEV as a proxy metric and MLRA as an architectural modification. TEV measures the standard deviation of token embedding weights to assess stability without expensive gradient variance calculations. MLRA factorizes query, key, and value projection matrices in the attention mechanism using low-rank matrices W_U ∈ ℝ^(d_model×r) and W_D ∈ ℝ^(r×d_model), where r = d_model/2. The method is evaluated by pre-training GPT-2 variants (vanilla, σReparam, and MLRA) with 48, 96, and 192 layers on WebText for 4 epochs, tracking TEV and gradient variance during training, and evaluating zero-shot perplexity on five benchmark datasets.

## Key Results
- MLRA achieves lower gradient variance (10^1 vs 10^2 at 1B tokens for 96-layer models) compared to vanilla GPT-2
- Improved perplexity on Lambada: 53.69 vs 62.31 for 192-layer models with MLRA
- TEV successfully identifies unstable training runs and correlates with gradient variance patterns
- MLRA maintains performance while reducing computational overhead through low-rank factorization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TEV serves as an efficient proxy for assessing pre-training stability by capturing gradient variance through token embedding standard deviation
- Mechanism: The standard deviation of token embedding weights reflects gradient noise levels during training, with lower values indicating more stable training
- Core assumption: Token embedding standard deviation accurately captures gradient variance and is the dominant term determining embedding norm
- Evidence anchors: Paper proposes TEV as stability proxy; weak corpus evidence on stability monitoring
- Break condition: If correlation between embedding standard deviation and gradient variance breaks down or layer normalization patterns change

### Mechanism 2
- Claim: MLRA reduces instability by limiting exponential growth of output embedding variance through low-rank factorization
- Mechanism: Factorizing attention matrices into low-rank components reduces variance propagation through layers, mitigating gradient explosion
- Core assumption: Variance grows exponentially through layers due to residual connections, and factorization effectively reduces this growth
- Evidence anchors: Paper claims MLRA mitigates variance growth; weak corpus evidence on low-rank attention
- Break condition: If factorization creates bottlenecks degrading performance or variance reduction proves insufficient

### Mechanism 3
- Claim: Pre-LN architecture causes gradient explosion in shallow layers, making token embedding layer particularly sensitive to instability
- Mechanism: Residual connections amplify gradient norms disproportionately in shallower layers under pre-LN, causing token embedding layer to have largest gradients
- Core assumption: Gradient explosion is specific to pre-LN and amplified in token embedding layer
- Evidence anchors: Paper notes gradient norms larger in shallower layers; weak corpus evidence on pre-LN dynamics
- Break condition: If architecture changes to post-LN or residual connections modified to prevent amplification

## Foundational Learning

- Concept: Token embedding variability (TEV)
  - Why needed here: Core proxy metric for assessing pre-training stability without expensive gradient variance calculations
  - Quick check question: How does TEV differ from simply measuring the norm of token embeddings?

- Concept: Low-rank matrix factorization
  - Why needed here: MLRA uses low-rank factorization of attention matrices to control variance growth through layers
  - Quick check question: Why does factorizing a matrix into two low-rank matrices reduce overall variance compared to original matrix?

- Concept: Gradient explosion in pre-layer normalization
  - Why needed here: Understanding why shallow layers have larger gradients in pre-LN is crucial for why TEV is effective proxy
  - Quick check question: How do residual connections contribute to gradient explosion in pre-LN architectures?

## Architecture Onboarding

- Component map: Token embedding layer → Multi-head attention (MLRA-modified) → Feed-forward network → Layer normalization (pre-LN) → Output projection

- Critical path: Token embedding layer → Attention projection matrices (MLRA applied) → Multi-head attention computation → Feed-forward network → Layer normalization → Output

- Design tradeoffs: MLRA reduces computational overhead without increasing inference cost but may introduce low-rank bottlenecks; TEV provides efficient monitoring but may not capture all instability aspects; pre-LN improves feature preservation but increases gradient explosion risk

- Failure signatures: High TEV values indicating unstable training; gradient variance spikes during training; poor downstream perplexity performance; low-rank bottlenecks causing model degradation

- First 3 experiments:
  1. Measure TEV distribution on pre-trained GPT-2 to establish baseline values and verify correlation with stable/unstable training runs
  2. Implement MLRA on small GPT-2 variant (12 layers) and compare gradient variance and TEV values against baseline during training
  3. Train GPT-2 with MLRA on WebText subset and evaluate zero-shot perplexity on Lambada to verify performance improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MLRA's performance scale to models with 7B+ parameters regarding TEV stability and perplexity?
- Basis in paper: Authors note study limited to 1.5B parameters and suggest investigating scalability to 7B models
- Why unresolved: Current experiments only tested up to 1.5B parameters, leaving uncertainty about performance at larger scales
- What evidence would resolve it: Training results from MLRA on 7B+ parameter models showing TEV distributions, gradient variance, and perplexity metrics

### Open Question 2
- Question: What is optimal rank reduction ratio (dr/dmodel) for MLRA across different model sizes and depths?
- Basis in paper: Authors use dr=192 for dmodel=384 but note low-rank reparameterization can degrade performance if applied too aggressively
- Why unresolved: Paper uses fixed ratio without exploring how different rank reductions affect performance across configurations
- What evidence would resolve it: Systematic experiments varying rank reduction ratio across model sizes and depths

### Open Question 3
- Question: Does TEV remain effective proxy when using alternative normalization strategies like RMSNorm or weight normalization?
- Basis in paper: Authors specifically study TEV in pre-LN models but don't test other normalization schemes
- Why unresolved: Theoretical connection between TEV and gradient variance derived under layer normalization assumption
- What evidence would resolve it: Comparative studies of TEV effectiveness across different normalization strategies

### Open Question 4
- Question: How does MLRA's factorized attention compare to other attention variants (linear attention, low-rank attention) in stability and efficiency trade-offs?
- Basis in paper: Authors compare MLRA to σReparam but don't compare to other attention variants reducing computational complexity
- Why unresolved: While MLRA reduces gradient variance, other variants might offer better efficiency or different stability characteristics
- What evidence would resolve it: Head-to-head comparisons of MLRA against alternative attention mechanisms on same tasks

## Limitations

- Weak theoretical justification for TEV as stability proxy - the mechanism relies on empirical correlations rather than formal proof
- Limited model scale testing - experiments only cover models up to 1.5B parameters, leaving scalability questions unanswered
- Narrow scope of attention variants - MLRA is compared primarily to σReparam without exploring other low-rank or efficient attention mechanisms

## Confidence

- Mechanism 1 (TEV as stability proxy): Low confidence - lacks theoretical justification and external validation
- Mechanism 2 (MLRA reducing instability): Medium confidence - empirical results support claims but theoretical understanding is incomplete
- Mechanism 3 (Pre-LN gradient explosion): Medium confidence - phenomenon is well-documented but specific sensitivity claims need validation

## Next Checks

1. Conduct systematic study measuring both TEV and actual gradient variance across multiple training runs with varying hyperparameters to quantify their relationship

2. Test MLRA on models with post-layer normalization and other attention mechanisms to determine whether variance reduction benefits are general or specific to pre-LN

3. Develop and validate theoretical model linking token embedding variability to gradient norms through mathematical properties of pre-LN residual connections