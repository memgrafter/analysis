---
ver: rpa2
title: Aligning Language Models with Demonstrated Feedback
arxiv_id: '2406.00888'
source_url: https://arxiv.org/abs/2406.00888
tags:
- ditto
- demonstrations
- arxiv
- policy
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DITTO, a method to align language models to
  specific individuals or tasks using very few demonstrations (less than 10). DITTO
  treats user demonstrations as preferred over model outputs and iteratively constructs
  pairwise preference data to fine-tune the model.
---

# Aligning Language Models with Demonstrated Feedback

## Quick Facts
- arXiv ID: 2406.00888
- Source URL: https://arxiv.org/abs/2406.00888
- Reference count: 35
- Primary result: DITTO outperforms few-shot prompting, supervised fine-tuning, and self-play methods by 19 percentage points using fewer than 10 demonstrations

## Executive Summary
DITTO is a method for aligning language models to specific individuals or tasks using very few demonstrations (less than 10). It treats user demonstrations as preferred over model outputs and iteratively constructs pairwise preference data to fine-tune the model. This approach generates cheap online comparison data, avoiding the need for large-scale preference annotation. Experiments on static benchmarks and a user study show DITTO outperforms baselines by an average of 19 percentage points in win rates, effectively capturing fine-grained style and task alignment while demonstrating superior sample efficiency.

## Method Summary
DITTO aligns language models to specific individuals or tasks using very few demonstrations by treating them as preferred over model outputs. The method iteratively constructs pairwise preference data through online comparison generation, where demonstrations are compared against model completions sampled from the current policy and previous checkpoints. This ranking-based approach allows the model to learn from implicit preferences encoded in demonstrations without requiring explicit pairwise preference annotations. The process involves initializing with supervised fine-tuning on demonstrations, then iteratively sampling comparisons and updating the policy using preference optimization.

## Key Results
- DITTO outperforms few-shot prompting, supervised fine-tuning, and self-play methods by 19 percentage points in win rates
- Effective with fewer than 10 demonstrations across news articles, emails, and blog posts
- Captures fine-grained style and task alignment with superior sample efficiency

## Why This Works (Mechanism)

### Mechanism 1: Online Comparison Generation
DITTO generates online comparison data by treating demonstrations as preferred over all model outputs, including intermediate checkpoints. For each demonstration, it samples completions from the current policy and constructs a ranking of policies (DE ⪰ D_t ⪰ D_{t-1} ⪰ ... ⪰ D_0). The core assumption is that the current policy's samples are inferior to expert demonstrations and the policy improves monotonically with each iteration. This fails if the policy doesn't improve monotonically or if demonstrations aren't actually preferred to model outputs.

### Mechanism 2: Extrapolation via Online Imitation Learning
DITTO can extrapolate beyond demonstrator performance by using online imitation learning principles. It's derived as an algorithm where the policy maximizes reward while the reward function is learned from comparisons between the current policy and expert demonstrations. The core assumption is that the optimization problem can be reparameterized to eliminate inner policy optimization, and the learned reward function can express any reward function. This fails if the reparameterization doesn't hold or if the learned reward function cannot express the true reward.

### Mechanism 3: Sample Efficiency Through Demonstrations
DITTO is more sample-efficient than collecting pairwise preferences because demonstrations directly encode user preferences. Instead of requiring many pairwise preferences, it uses a small number of demonstrations to generate comparisons. The core assumption is that demonstrations are more informative than pairwise preferences for capturing user preferences. This fails if demonstrations aren't more informative than pairwise preferences for the specific task, or if user preferences aren't well-captured by demonstrations.

## Foundational Learning

- **Concept**: Reinforcement Learning from Human Feedback (RLHF)
  - **Why needed here**: DITTO builds on RLHF principles by using preference optimization to align the model to user preferences.
  - **Quick check question**: What is the objective function used in RLHF to maximize expected reward subject to a KL-constraint?

- **Concept**: Inverse Reinforcement Learning (IRL)
  - **Why needed here**: DITTO can be derived as an online imitation learning algorithm, which is related to IRL. Understanding IRL helps in understanding how DITTO learns a reward function from demonstrations.
  - **Quick check question**: How does IRL differ from standard RL in terms of the learning objective?

- **Concept**: Bradley-Terry Model
  - **Why needed here**: DITTO uses the Bradley-Terry model to construct pairwise preferences between demonstrations and model outputs.
  - **Quick check question**: What is the Bradley-Terry model and how is it used in preference learning?

## Architecture Onboarding

- **Component map**: LLM -> Demonstrations -> Policy -> Ranking of policies -> Comparison generator -> Preference optimizer
- **Critical path**: 1) Sample completions from current policy for each demonstration. 2) Construct pairwise preferences between demonstrations and sampled completions. 3) Update policy using preference optimization. 4) Repeat until convergence.
- **Design tradeoffs**: Number of DITTO iterations vs. overfitting; Number of negative samples vs. runtime; Use of replay and interpolicy comparisons vs. sample efficiency
- **Failure signatures**: Policy not improving monotonically; Demonstrations not preferred to model outputs; Overfitting to demonstrations
- **First 3 experiments**: 1) Test DITTO on a simple text generation task with a small number of demonstrations. 2) Compare DITTO to SFT and few-shot prompting on a benchmark dataset. 3) Evaluate the impact of the number of DITTO iterations and negative samples on performance.

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but based on the content, several key open questions emerge:

### Open Question 1
- Question: How does DITTO's performance scale with different model sizes and architectures?
- Basis in paper: Inferred
- Why unresolved: The paper only evaluates DITTO on Mistral 7B and mentions exploring scaling as future work, but doesn't provide empirical results for other model sizes or architectures.
- What evidence would resolve it: Systematic experiments comparing DITTO's performance across various model sizes (e.g., 1B, 13B, 70B parameters) and different architectures (e.g., Llama, GPT, Claude).

### Open Question 2
- Question: What is the optimal strategy for selecting demonstrations to maximize DITTO's performance?
- Basis in paper: Inferred
- Why unresolved: While the paper explores demonstration cohesiveness, it doesn't provide concrete guidelines for demonstration selection strategies or how to handle diverse/unrepresentative demonstrations.
- What evidence would resolve it: Experiments comparing different demonstration selection strategies (e.g., diversity sampling, clustering-based selection, quality filtering) and their impact on downstream performance.

### Open Question 3
- Question: How does DITTO's performance compare to other preference learning methods when demonstrations are noisy or suboptimal?
- Basis in paper: Inferred
- Why unresolved: The paper assumes demonstrations are high-quality and doesn't test DITTO's robustness to noisy or suboptimal demonstrations.
- What evidence would resolve it: Comparative experiments between DITTO and baselines using demonstrations with varying quality levels (e.g., intentionally noisy demonstrations, demonstrations from multiple users with conflicting preferences).

### Open Question 4
- Question: What is the long-term stability of DITTO-adapted models across different domains and tasks?
- Basis in paper: Inferred
- Why unresolved: The paper focuses on short-term performance and doesn't examine how DITTO-adapted models maintain performance over extended periods or across diverse tasks.
- What evidence would resolve it: Longitudinal studies tracking DITTO model performance across multiple domains, tasks, and time periods, including potential degradation or adaptation to new contexts.

### Open Question 5
- Question: How does the choice of reference policy update strategy affect DITTO's performance and sample efficiency?
- Basis in paper: Explicit
- Why unresolved: The paper mentions that updating the reference policy can degrade performance but doesn't explore alternative strategies or provide theoretical justification for this behavior.
- What evidence would resolve it: Systematic experiments comparing different reference policy update strategies (e.g., delayed updates, selective updates based on performance metrics) and their impact on both performance and computational efficiency.

## Limitations
- Performance may vary significantly depending on task complexity and demonstration quality
- Assumes monotonic policy improvement which may not hold in all settings
- Limited evaluation on complex reasoning tasks beyond stylistic alignment

## Confidence
**High Confidence**: Sample efficiency claim is well-supported by experimental results showing superior performance with fewer than 10 demonstrations compared to baselines requiring thousands of preferences.

**Medium Confidence**: Extrapolation claim beyond demonstrator performance is supported by theoretical framework but lacks extensive empirical validation across diverse tasks.

**Medium Confidence**: Claim that demonstrations are more informative than pairwise preferences is reasonable but not conclusively proven through direct comparison.

## Next Checks
1. Test DITTO's performance when demonstrations contain varying levels of noise or when multiple demonstrations have conflicting preferences to validate robustness to real-world usage.

2. Evaluate how DITTO's performance scales with the number of demonstrations (e.g., 5, 10, 20, 50) and compare this scaling behavior against baselines to confirm sample efficiency across different data regimes.

3. Apply DITTO to a more complex task involving multi-step reasoning or planning (such as mathematical problem-solving or code generation) to test whether the method can capture and generalize beyond simple stylistic preferences.