---
ver: rpa2
title: 'Code-Switching Red-Teaming: LLM Evaluation for Safety and Multilingual Understanding'
arxiv_id: '2406.15481'
source_url: https://arxiv.org/abs/2406.15481
tags:
- csrt
- code-switching
- red-teaming
- languages
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CSRT (Code-Switching Red-Teaming), a method
  for evaluating large language models (LLMs) on both multilingual understanding and
  safety. CSRT generates synthetic code-switching prompts by combining up to 10 languages,
  using existing multilingual red-teaming benchmarks as input.
---

# Code-Switching Red-Teaming: LLM Evaluation for Safety and Multilingual Understanding

## Quick Facts
- arXiv ID: 2406.15481
- Source URL: https://arxiv.org/abs/2406.15481
- Authors: Haneul Yoo; Yongjin Yang; Hwaran Lee
- Reference count: 40
- Key result: CSRT achieves 46.7% higher attack success rate compared to English-only red-teaming

## Executive Summary
This paper introduces Code-Switching Red-Teaming (CSRT), a novel method for evaluating large language models on both multilingual understanding and safety. CSRT generates synthetic code-switching prompts by combining up to 10 languages from existing multilingual red-teaming benchmarks, creating a dataset of 315 prompts across six unsafe categories. The method demonstrates that language diversity can be leveraged to uncover safety vulnerabilities in LLMs, achieving significantly higher attack success rates than traditional monolingual approaches while also evaluating multilingual comprehension.

## Method Summary
CSRT generates code-switching prompts by combining parallel adversarial prompts from 10 languages (English, Chinese, Italian, Vietnamese, Arabic, Korean, Thai, Bengali, Swahili, Javanese) using GPT-4o. The method takes monolingual red-teaming data as input and automatically produces multilingual code-switched prompts that test both safety and language understanding. These prompts are then evaluated on ten state-of-the-art LLMs using GPT-4o as a judge, measuring attack success rate, refusal rate, and comprehension scores. The approach is fully automated and extensible, requiring only monolingual data as input for generation.

## Key Results
- CSRT achieves 46.7% higher attack success rate compared to standard English-only red-teaming
- Performance scales with language diversity, particularly benefiting from low-resource language inclusion
- Demonstrates superior multilingual comprehension while exposing safety vulnerabilities
- Ablation studies confirm that using more languages increases attack effectiveness

## Why This Works (Mechanism)
CSRT exploits the fact that code-switching creates linguistic complexity that can bypass monolingual safety filters while simultaneously testing multilingual understanding. By combining tokens from multiple languages, the method creates prompts that are difficult for models to fully comprehend, leading to reduced safety awareness. The approach leverages the correlation between language diversity and safety vulnerability, showing that models struggle to maintain safety standards when faced with multilingual input.

## Foundational Learning
- **Code-switching**: Alternating between multiple languages within a single conversation or prompt - needed to create linguistic complexity that tests both safety and multilingual understanding; quick check: verify prompts contain mixed language tokens
- **Red-teaming**: Systematic testing of model vulnerabilities through adversarial prompts - needed as the safety evaluation framework; quick check: confirm unsafe categories are properly represented
- **Attack Success Rate (ASR)**: Metric measuring how often adversarial prompts elicit harmful responses - needed to quantify safety effectiveness; quick check: verify ASR calculation methodology
- **Refusal Rate**: Metric measuring how often models decline to respond to prompts - needed to measure safety filter effectiveness; quick check: ensure refusal is properly distinguished from harmful response
- **Multilingual comprehension**: Model's ability to understand and process multiple languages - needed to validate that CSRT tests genuine multilingual understanding; quick check: verify comprehension scoring methodology

## Architecture Onboarding

**Component Map**: MultiJail prompts -> GPT-4o prompt generator -> CSRT dataset -> LLM evaluation -> GPT-4o judge -> ASR/RR/comprehension metrics

**Critical Path**: The generation of code-switching prompts (GPT-4o generator) -> evaluation on target LLMs -> scoring by GPT-4o judge is the critical path for obtaining CSRT metrics.

**Design Tradeoffs**: The use of GPT-4o for both generation and evaluation provides automation and scalability but introduces potential bias. The choice of 10 languages balances comprehensiveness with prompt coherence. The reliance on existing red-teaming benchmarks limits the method to available unsafe categories.

**Failure Signatures**: Low attack success rates may indicate insufficient code-switching complexity or ineffective input prompts. Inconsistent comprehension scores across languages suggest generation quality issues. High refusal rates across all models may indicate overly aggressive safety filters or problematic prompt formulation.

**First Experiments**:
1. Generate CSRT prompts using a subset of 3-4 languages to validate generation process before scaling to full 10-language combinations
2. Evaluate a single LLM (e.g., GPT-3.5-turbo) on CSRT dataset to verify evaluation pipeline and metric calculation
3. Compare ASR results between English-only prompts and basic 2-language code-switched prompts to establish baseline effectiveness

## Open Questions the Paper Calls Out
- **Open Question 1**: How does CSRT perform on low-resource language pairs not included in the original MultiJail dataset?
- **Open Question 2**: What is the impact of CSRT on model safety training when used as adversarial examples during fine-tuning?
- **Open Question 3**: How does CSRT performance scale with larger language models beyond those tested in the paper?

## Limitations
- The method relies heavily on GPT-4o for both prompt generation and evaluation, creating potential bias
- Specific system prompts and evaluation rubrics are not provided, limiting reproducibility
- Only tested on 10 languages from MultiJail dataset, leaving performance on other language combinations unknown
- The correlation between code-switching complexity and safety vulnerability may not hold for all model architectures

## Confidence
- Core methodology claims: High
- Quantitative results reproducibility: Medium
- Generalizability across different LLM architectures: Medium
- Specific implementation details for reproduction: Low

## Next Checks
1. Replicate the CSRT generation process with alternative parallel prompt sources beyond MultiJail to test generalizability
2. Conduct inter-annotator agreement studies using multiple judges (human and/or different LLM judges) to validate the consistency of ASR and comprehension metrics
3. Perform targeted experiments with specific low-resource language combinations to isolate their individual contributions to attack success rates