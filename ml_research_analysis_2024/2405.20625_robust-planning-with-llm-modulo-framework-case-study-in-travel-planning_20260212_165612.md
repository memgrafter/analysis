---
ver: rpa2
title: 'Robust Planning with LLM-Modulo Framework: Case Study in Travel Planning'
arxiv_id: '2405.20625'
source_url: https://arxiv.org/abs/2405.20625
tags:
- critics
- planning
- modulo
- llms
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work operationalizes the LLM-Modulo framework for the Travel
  Planning domain, leveraging Large Language Models (LLMs) as idea generators, translators,
  and critics to improve planning performance. The authors instantiate this framework
  on the Travel Planning benchmark, using GPT-3.5-Turbo and GPT-4-Turbo to generate
  and refine travel itineraries based on natural language queries.
---

# Robust Planning with LLM-Modulo Framework: Case Study in Travel Planning

## Quick Facts
- arXiv ID: 2405.20625
- Source URL: https://arxiv.org/abs/2405.20625
- Reference count: 11
- Primary result: 4.6x improvement in final pass rate for GPT-4-Turbo (20.6% vs. 4.4% baseline)

## Executive Summary
This work operationalizes the LLM-Modulo framework for travel planning, leveraging Large Language Models as idea generators, translators, and critics to improve planning performance. The authors instantiate this framework on the Travel Planning benchmark using GPT-3.5-Turbo and GPT-4-Turbo to generate and refine travel itineraries based on natural language queries. The study demonstrates significant improvements in planning performance, with GPT-4-Turbo achieving a 4.6x improvement in final pass rate and GPT-3.5-Turbo improving from 0% to 5% baseline performance.

## Method Summary
The authors implement the LLM-Modulo framework by decomposing the planning process into three key LLM roles: idea generation (generating multiple travel plan candidates), translation (converting natural language queries into structured planning problems), and criticism (refining and validating plans). They use GPT-3.5-Turbo and GPT-4-Turbo to handle these roles, with critic implementations extracted from LLMs and LLMs also serving as reformulators for structured plan representation. The framework is evaluated on the Travel Planning benchmark, measuring pass rates for generated travel itineraries against automated success criteria.

## Key Results
- GPT-4-Turbo final pass rate improved from 4.4% to 20.6% (4.6x improvement)
- GPT-3.5-Turbo baseline performance improved from 0% to 5%
- Demonstrated successful extraction of critic implementations from LLMs
- Showed effective use of LLMs as reformulators for structured plan representation

## Why This Works (Mechanism)
The LLM-Modulo framework improves planning by breaking down the complex task into specialized LLM roles that work iteratively. Idea generation provides diverse plan candidates, translation converts natural language requirements into actionable planning problems, and criticism refines and validates the output. This modular decomposition allows each LLM component to focus on its strengths while compensating for individual weaknesses through the iterative feedback loop.

## Foundational Learning
1. **LLM as Critic**: Why needed - To validate and refine plans through automated reasoning. Quick check - Does the critic consistently identify logical inconsistencies in generated plans?
2. **Multi-role LLM decomposition**: Why needed - Single LLMs struggle with complex planning tasks; decomposition leverages specialized capabilities. Quick check - Do individual roles (generator, translator, critic) each contribute measurably to performance gains?
3. **Structured plan representation**: Why needed - Natural language outputs require conversion to actionable formats. Quick check - Can the reformulator accurately convert between natural language and structured representations across different domains?

## Architecture Onboarding

Component Map: User Query -> Natural Language Translator -> Idea Generator -> Plan Critic -> Refined Plan

Critical Path: User Query → Natural Language Translator → Idea Generator → Plan Critic → Refined Plan

Design Tradeoffs:
- Multiple LLM calls vs. single prompt efficiency
- Automated vs. human validation of critic implementations
- Tradeoff between plan diversity (idea generation) and refinement time (criticism iterations)

Failure Signatures:
- Poor critic extraction leading to invalid plan refinements
- Translation errors causing mismatch between user intent and generated plans
- Idea generator producing plans outside user constraints

First Experiments:
1. Test critic extraction quality by manually validating critic outputs against ground truth plan quality assessments
2. Evaluate translation accuracy by comparing translated structured problems against original natural language queries
3. Measure diversity vs. quality tradeoff by varying the number of ideas generated before criticism

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on simulated queries rather than real human travelers
- Success metrics based on automated checks rather than human judgment of plan quality
- Framework tested only on two LLM models (GPT-3.5-Turbo and GPT-4-Turbo)
- Limited analysis of critic extraction generalizability beyond travel planning domain

## Confidence
- High confidence in core observation: LLM-Modulo improves planning performance compared to direct prompting
- Medium confidence in generalizability of critic extraction methodology across different domains
- Medium confidence in practical significance given evaluation methodology limitations

## Next Checks
1. Conduct user studies with real travelers to validate whether automated pass rates correlate with human-perceived plan quality
2. Test framework's performance across diverse planning domains (logistics, project management, resource allocation) to assess generalizability
3. Implement ablation studies to quantify individual contributions of each LLM-Modulo component (idea generation, translation, criticism) to overall performance