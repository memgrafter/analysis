---
ver: rpa2
title: Self-training from Self-memory in Data-to-text Generation
arxiv_id: '2401.10567'
source_url: https://arxiv.org/abs/2401.10567
tags:
- data
- training
- self-training
- arxiv
- self-mem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a novel training model, self-training from
  self-memory (STSM) in data-to-text generation (DTG), allowing the model to self-train
  on subsets, including self-memory as outputs inferred directly from the trained
  models and/or the new data. The quality of self-memory is validated by two models,
  data-to-text (D2T) and text-to-data (T2D), by two pre-defined conditions: (1) the
  appearance of all source values in the outputs of the D2T model and (2) the ability
  to convert back to source data in the outputs in the T2D model.'
---

# Self-training from Self-memory in Data-to-text Generation

## Quick Facts
- arXiv ID: 2401.10567
- Source URL: https://arxiv.org/abs/2401.10567
- Authors: Hoang-Thang Ta
- Reference count: 40
- With 30% of the dataset, STSM achieves competitive performance compared to full training in the same setup

## Executive Summary
This paper introduces a novel training model, self-training from self-memory (STSM), for data-to-text generation tasks. The method allows models to self-train on subsets of data, including self-memory as outputs inferred directly from trained models and/or new data. The quality of self-memory is validated by two models - data-to-text (D2T) and text-to-data (T2D) - using two conditions: the appearance of all source values in D2T outputs and the ability to convert back to source data in T2D outputs. The approach demonstrates competitive performance using only 30% of training data while offering generalization capabilities from subset memory and reducing training data volume.

## Method Summary
The STSM approach involves training D2T and T2D Transformer models on a subset of data, then generating outputs (Y') from the D2T model. These outputs are validated using the T2D model to ensure they can be converted back to the original data (X'). A greedy algorithm is used to generate shorter D2T outputs that contain all source values, creating optimized targets. These validated and optimized outputs serve as self-memory, which is combined with new data for subsequent self-training epochs. The process iterates, allowing the model to improve its performance while using significantly less training data than traditional approaches.

## Key Results
- STSM achieves competitive performance using only 30% of training data compared to full training
- The approach successfully validates self-memory quality through dual-model verification (D2T and T2D)
- Optimized targets generated through greedy algorithm improve training efficiency without sacrificing quality
- The method demonstrates generalization capabilities from subset memory while reducing training data requirements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-training from self-memory enables competitive performance with 30% of training data by using high-quality pseudo-labels inferred from the model itself.
- Mechanism: The method trains D2T and T2D models on a subset of data, generates outputs (Y'), validates them by ensuring all source values appear and the T2D model can reconstruct the input (X'), then uses these validated outputs as additional training examples.
- Core assumption: Outputs that preserve all source values and can be reconstructed by T2D are sufficiently high quality to serve as pseudo-labels for self-training.
- Evidence anchors:
  - [abstract] "The quality of self-memory is validated by two models, data-to-text (D2T) and text-to-data (T2D), by two pre-defined conditions: (1) the appearance of all source values in the outputs of the D2T model and (2) the ability to convert back to source data in the outputs in the T2D model."
  - [section] "We utilize a greedy algorithm to generate shorter D2T outputs if they contain all source values. Subsequently, we use the T2D model to confirm that these outputs can capture input relationships by demonstrating their capacity to convert text back into data."
  - [corpus] Weak - no direct citations in corpus about this specific validation mechanism.
- Break condition: If the T2D model cannot accurately reconstruct inputs from D2T outputs, the self-memory quality degrades and self-training fails.

### Mechanism 2
- Claim: Optimizing D2T outputs to be shorter while preserving all source values improves training efficiency without sacrificing quality.
- Mechanism: Algorithm 1 greedily selects sentences containing source values, ensuring each appears exactly once, creating more concise "optimized targets" that serve as higher-quality training examples.
- Core assumption: Shorter outputs that contain all source values are equally or more informative than longer ones, and the optimization preserves semantic content.
- Evidence anchors:
  - [section] "In DTG, an ideal target must capture all information in the input, usually source values and relationships between subjects and objects... With that sense, we believe that any generated target that satisfies these conditions and is shorter than the gold target should be chosen as the new gold target, or called the optimized target."
  - [section] "Table 2 displays two examples on DART and E2E NLG, each contains the optimized target and the gold target. The optimized target is more brief than the gold target and features a different word order."
  - [corpus] Weak - no corpus evidence about output optimization for DTG.
- Break condition: If the greedy algorithm removes context or relationships essential for understanding, the optimized targets become insufficient for training.

### Mechanism 3
- Claim: Training on fixed data subsets provides better performance than random subsets in STSM.
- Mechanism: Fixed subsets ensure consistent data distribution across epochs, while random subsets introduce variability that may disrupt learning patterns established in self-memory.
- Core assumption: Consistent data exposure across training epochs helps the model better integrate self-memory with new data.
- Evidence anchors:
  - [section] "The superiority between self-training on fixed data and random data remains uncertain. Nonetheless, opting for fixed data in training is more advantageous, particularly in scenarios involving continual learning with the introduction of new data."
  - [section] "self-mem + new data on fixed data emerged as the optimized method, achieving highly competitive results compared to the best method over T5-base."
  - [corpus] Weak - no direct corpus citations about fixed vs random data in self-training.
- Break condition: If the fixed subset lacks sufficient diversity or represents a biased sample, performance may degrade compared to random sampling.

## Foundational Learning

- Concept: Sequence-to-sequence architecture with encoder-decoder transformers
  - Why needed here: STSM relies on transformer-based D2T and T2D models for both generating outputs and validating self-memory quality
  - Quick check question: Can you explain how the encoder processes input triples and how the decoder generates text conditioned on encoder representations?

- Concept: Self-training and pseudo-label generation
  - Why needed here: The core innovation involves using model-generated outputs as additional training data, requiring understanding of when and how to trust pseudo-labels
  - Quick check question: What validation criteria ensure that self-generated outputs are of sufficient quality to use as training data?

- Concept: Cycle consistency in dual-task learning
  - Why needed here: The T2D model validates D2T outputs by converting them back to data, ensuring the generated text preserves all input relationships
  - Quick check question: How does the ability to reconstruct source data from generated text indicate that all relationships have been captured?

## Architecture Onboarding

- Component map: D2T Model -> T2D Model -> Self-Memory Generator -> Data Optimizer -> Training Controller
- Critical path:
  1. Initialize D2T and T2D models on initial subset
  2. Generate outputs (Y') from D2T on subset input (X)
  3. Validate outputs by converting Y' to X' with T2D
  4. Optimize Y' to Y'' if shorter and complete
  5. Filter pairs meeting validation criteria
  6. Combine filtered pairs with new data for next epoch
  7. Self-train D2T (and optionally T2D) on combined data
- Design tradeoffs:
  - Fixed vs random data subsets: Fixed ensures consistency but may introduce bias; random ensures diversity but may disrupt learning patterns
  - Self-training T2D or not: Training T2D improves validation capability but increases computational cost
  - Greedy optimization vs beam search: Greedy is faster but may miss better optimizations; beam search is slower but potentially finds better solutions
- Failure signatures:
  - T2D validation fails consistently: Indicates D2T model isn't capturing relationships properly
  - Self-memory quality doesn't improve over epochs: Suggests validation criteria are too lenient or data subsets are inadequate
  - Performance worse than baseline with full data: Indicates self-training is introducing noise or the subset selection is problematic
- First 3 experiments:
  1. Baseline comparison: Train full model on 100% data vs STSM on 30% data to verify competitive performance claim
  2. Validation effectiveness: Test whether T2D validation actually improves self-memory quality by comparing with random pseudo-labels
  3. Optimization impact: Compare performance using optimized targets vs original gold targets to measure benefit of the greedy algorithm

## Open Questions the Paper Calls Out

- How does the performance of STSM compare when using larger pre-trained models like BART-large or T5-large?
  - Basis in paper: [inferred] The paper notes that their model was only evaluated on small-scale pre-trained models (BART-base and T5-base) and suggests that testing on larger models is a future direction.
  - Why unresolved: The authors did not conduct experiments with larger pre-trained models, leaving uncertainty about whether the performance gains from STSM scale with model size.
  - What evidence would resolve it: Experimental results comparing STSM performance using BART-large and T5-large models against the baseline models would clarify the impact of model size on STSM's effectiveness.

- What is the optimal ratio of self-memory to new data for self-training in STSM?
  - Basis in paper: [explicit] The paper mentions that the optimal ratio between self-memory and new data for self-training is unknown and suggests further investigation into this aspect.
  - Why unresolved: The authors did not explore different ratios of self-memory to new data, so it is unclear whether the current equal distribution (30% each) is optimal.
  - What evidence would resolve it: Experiments varying the proportions of self-memory and new data in the self-training process would provide insights into the optimal ratio for maximizing performance.

- Does self-training on the T2D model significantly improve the quality of the D2T model's outputs?
  - Basis in paper: [explicit] The paper states that self-training on the T2D model does not contribute significantly to performance improvement, and it recommends excluding this self-training to conserve resources.
  - Why unresolved: While the authors observed no significant improvement, the exact impact of self-training on the T2D model remains unclear without further experimentation.
  - What evidence would resolve it: Detailed experiments comparing the performance of the D2T model with and without self-training on the T2D model would clarify the necessity and impact of this step.

## Limitations
- The effectiveness of the greedy optimization algorithm for creating shorter targets is not thoroughly validated with ablation studies
- The T2D validation mechanism may not be robust enough to catch all quality issues in self-memory, with no exploration of edge cases
- The choice of fixed data subsets over random sampling lacks empirical validation comparing both approaches directly

## Confidence

- **High confidence**: The core STSM framework and iterative training process are well-defined and reproducible
- **Medium confidence**: The performance claims (competitive results with 30% data) are supported by experimental results but could benefit from more extensive ablation studies
- **Low confidence**: The specific design choices (optimization algorithm details, validation thresholds, fixed vs random data selection) lack sufficient justification or empirical support

## Next Checks
1. Conduct ablation studies to quantify the impact of each component: test performance with and without the greedy optimization algorithm, with and without T2D validation, and with fixed vs random data subsets
2. Perform error analysis on self-memory quality by manually inspecting a sample of validated outputs to identify failure modes that automated metrics might miss
3. Test the approach on additional datasets beyond E2E NLG and DART to verify generalizability across different data-to-text generation tasks and domains