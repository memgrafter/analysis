---
ver: rpa2
title: 'Expanding FLORES+ Benchmark for more Low-Resource Settings: Portuguese-Emakhuwa
  Machine Translation Evaluation'
arxiv_id: '2408.11457'
source_url: https://arxiv.org/abs/2408.11457
tags:
- translation
- emakhuwa
- language
- devtest
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work expanded the FLORES+ evaluation set to include Emakhuwa,
  a low-resource Bantu language spoken by approximately 9 million people in Mozambique.
  The authors translated the dev and devtest sets from Portuguese to Emakhuwa, implementing
  a rigorous quality assurance process including post-editing and adequacy assessments.
---

# Expanding FLORES+ Benchmark for more Low-Resource Settings: Portuguese-Emakhuwa Machine Translation Evaluation

## Quick Facts
- arXiv ID: 2408.11457
- Source URL: https://arxiv.org/abs/2408.11457
- Reference count: 11
- Primary result: Expanded FLORES+ with Emakhuwa translations, revealing spelling inconsistencies pose significant challenges for MT

## Executive Summary
This work expands the FLORES+ evaluation set to include Emakhuwa, a low-resource Bantu language spoken by approximately 9 million people in Mozambique. The authors translated the dev and devtest sets from Portuguese to Emakhuwa, implementing a rigorous quality assurance process including post-editing and adequacy assessments. The resulting dataset contains multiple reference translations for each source sentence to account for orthographic variation. The authors present baseline results from training a Neural Machine Translation system and fine-tuning existing multilingual translation models, finding that spelling inconsistencies remain a significant obstacle for machine translation in Emakhuwa.

## Method Summary
The authors translated the FLORES+ dev and devtest sets from Portuguese to Emakhuwa using Matecat CAT tool, then applied post-editing and adequacy assessments to ensure quality. They generated multiple reference translations per source sentence to capture orthographic variation. For evaluation, they trained a baseline small Transformer model and fine-tuned several multilingual models (mT5, ByT5, M2M-100, NLLB, AfriByT5, AfriMT5) on the available training data. The evaluation used BLEU and chrF metrics, with the dataset publicly available at Hugging Face.

## Key Results
- Baseline models underperformed on Emakhuwa evaluation set, with BLEU scores remaining relatively low
- Fine-tuned ByT5 model achieved BLEU score of 14.1 and chrF score of 37.75, substantially improving over baseline
- Multiple reference translations increased BLEU scores by +0.24 to +0.54 on dev set and +0.3 on devtest set
- chrF scores consistently higher than BLEU scores, suggesting BLEU penalizes spelling variations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expanding FLORES+ with Emakhuwa creates a high-impact low-resource benchmark because Emakhuwa is spoken by ~9 million people yet lacks prior MT datasets.
- Mechanism: Adding a new language to an established multilingual benchmark increases coverage, enabling direct comparison of MT performance across languages and spotlighting low-resource gaps.
- Core assumption: Emakhuwa is truly low-resource (minimal digital presence, limited standardization).
- Evidence anchors:
  - [abstract] "Emakhuwa, a low-resource language widely spoken in Mozambique... spoken by approximately 9 million people."
  - [section 3] "Emakhuwa digital resources are scarce, and the spelling standards are still under development."
- Break condition: If Emakhuwa were to suddenly acquire abundant parallel corpora, the low-resource "novelty" benefit would diminish.

### Mechanism 2
- Claim: Multiple reference translations per source sentence improve evaluation robustness for languages with spelling inconsistencies.
- Mechanism: Multiple references capture orthographic variation, allowing evaluation metrics like BLEU/chrF to reflect true translation adequacy rather than penalizing legitimate spelling variants.
- Core assumption: Spelling inconsistencies in Emakhuwa are systematic enough that different translators will produce legitimately different spellings.
- Evidence anchors:
  - [section 3.1] "existing materials exhibit inconsistent spelling... different combinations of morphemes result in varied spellings of the same word."
  - [section 6] "Using multiple references... BLEU scores increased by +0.24 to +0.54 on the dev set and by +0.3 on the devtest set."
- Break condition: If orthographic variation were negligible, multiple references would add cost without evaluation benefit.

### Mechanism 3
- Claim: Fine-tuning multilingual models (especially tokenization-free ByT5) yields better performance than vanilla Transformers for Emakhuwa→Portuguese translation.
- Mechanism: Multilingual pre-training plus tokenization-free processing better handles morphologically rich, low-resource languages by leveraging cross-lingual transfer and avoiding subword segmentation issues.
- Core assumption: Emakhuwa's agglutinative morphology and orthographic variability are poorly handled by standard subword tokenization.
- Evidence anchors:
  - [section 5.2] "multilingual language models... enable knowledge transfer among related languages."
  - [section 6] "fine-tuned ByT5 model achieved a BLEU score of 14.1 and a ChrF score of 37.75... which marks a substantial improvement over the baseline."
- Break condition: If Emakhuwa had more consistent orthography and abundant data, vanilla Transformers might close the performance gap.

## Foundational Learning

- Concept: Low-resource language MT challenges
  - Why needed here: Explains why standard high-resource techniques fail and motivates specialized methods (multiple references, multilingual fine-tuning).
  - Quick check question: What are the two main types of scarcity that limit low-resource MT performance?

- Concept: Multilingual model fine-tuning
  - Why needed here: Central to the experimental approach; understanding transfer learning and tokenization choices is key to reproducing results.
  - Quick check question: Why might a tokenization-free model like ByT5 perform better on Emakhuwa than subword-based models?

- Concept: Evaluation metrics BLEU vs. chrF
  - Why needed here: Different sensitivity to orthographic variation; explains why chrF scores were higher despite lower BLEU.
  - Quick check question: Which metric is more sensitive to character-level spelling differences?

## Architecture Onboarding

- Component map: Portuguese source text → Matecat CAT tool → post-editing → DA scoring → multiple reference generation → training data preparation → model training/fine-tuning → evaluation
- Critical path: Translation → quality assurance (post-editing + DA) → dataset release → model training/fine-tuning → evaluation
- Design tradeoffs:
  - Multiple references increase annotation cost but improve evaluation robustness for inconsistent orthographies.
  - Tokenization-free models reduce segmentation errors but may be slower to train.
  - Fine-tuning multilingual models leverages transfer but risks cross-lingual interference.
- Failure signatures:
  - BLEU >> chrF suggests evaluation is penalizing legitimate spelling variants.
  - Low ICC for orthography scoring indicates annotator disagreement due to lack of standard.
  - High Matecat edit distance indicates translation quality issues.
- First 3 experiments:
  1. Train baseline Transformer (small config) on available Emakhuwa data; evaluate on dev/devtest.
  2. Fine-tune AfriByT5 on same data; compare BLEU/chrF to baseline.
  3. Fine-tune ByT5 (tokenization-free) and compare performance, especially on orthography-sensitive metrics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the correlation between automatic evaluation metrics (BLEU and ChrF) and human judgments for Emakhuwa translations, particularly regarding spelling inconsistencies?
- Basis in paper: [inferred] The paper notes that while BLEU scores remained relatively low for the pt→vmw direction, ChrF scores were consistently higher, suggesting BLEU may be disproportionately penalizing spelling variations in Emakhuwa. The authors explicitly state "further studies need to be done to assess the correlation of these automatic metrics with human evaluations."
- Why unresolved: The authors acknowledge this gap but did not conduct a detailed correlation study between automatic metrics and human assessments, leaving uncertainty about which metric better reflects translation quality for Emakhuwa.
- What evidence would resolve it: Conducting a correlation study between BLEU, ChrF, and human judgments on the Emakhuwa dataset, including analysis of how spelling variations affect each metric's scores relative to human perception of translation quality.

### Open Question 2
- Question: How do spelling inconsistencies in Emakhuwa affect downstream language technology applications beyond machine translation, such as information retrieval or text classification?
- Basis in paper: [explicit] The authors discuss how spelling inconsistencies lead to data sparsity, inflate vocabulary size, and reduce performance of language technologies, noting that "different combinations of morphemes result in varied spellings of the same word" and this "impairs the model's ability to learn the language's nuances effectively."
- Why unresolved: The paper focuses on machine translation evaluation but does not empirically test the impact of spelling variations on other NLP tasks or quantify the extent of performance degradation in applications beyond translation.
- What evidence would resolve it: Empirical studies comparing the performance of information retrieval, text classification, and other NLP tasks on standardized versus non-standardized Emakhuwa text, measuring the impact of spelling variations on task accuracy and model efficiency.

### Open Question 3
- Question: What are the most effective strategies for handling loanword adaptation in Emakhuwa to reduce spelling inconsistencies while maintaining linguistic authenticity?
- Basis in paper: [explicit] The authors identify that "Emakhuwa text corpora frequently contain Portuguese loanwords with inconsistent adaptations due to the absence of standardized guidelines" and note three adaptation patterns (phonetic, phonotactic alignment, or unchanged), but do not evaluate which approach yields the best results for machine translation.
- Why unresolved: While the paper documents the problem of inconsistent loanword adaptation, it does not test different standardization approaches or evaluate their impact on translation quality or language model performance.
- What evidence would resolve it: Comparative experiments testing machine translation systems trained on corpora with different loanword adaptation strategies (phonetic, phonotactic, or mixed approaches), measuring translation quality and assessing which approach produces the most consistent and effective results for language technology applications.

## Limitations
- Spelling inconsistencies in Emakhuwa create challenges for establishing gold standards and affect both model training and evaluation
- Relatively small evaluation set (1,212 sentences) may not capture full language diversity
- Baseline models' poor performance might reflect architectural limitations rather than inherent task difficulty

## Confidence
- High confidence: Dataset creation process is well-documented; Emakhuwa is confirmed as low-resource with spelling inconsistencies; multilingual models outperform vanilla Transformers
- Medium confidence: Multiple reference translations improve evaluation robustness; Emakhuwa's challenges are representative of other low-resource Bantu languages
- Low confidence: Poor baseline performance definitively reflects task difficulty; larger models wouldn't substantially improve results

## Next Checks
1. **Orthography Standardization Impact**: Train models using only the most standardized reference per sentence versus using all references, then compare performance to quantify the evaluation benefit of multiple references.

2. **Cross-Linguistic Comparison**: Apply the same evaluation methodology to another low-resource Bantu language (e.g., Tsonga or Siswati) to determine whether Emakhuwa's challenges are language-specific or representative of Bantu language MT challenges.

3. **Tokenization Sensitivity Analysis**: Systematically compare tokenization-free ByT5 performance against subword-tokenized models across different Emakhuwa text genres (news, religious, conversational) to determine which aspects of orthographic variation most impact performance.