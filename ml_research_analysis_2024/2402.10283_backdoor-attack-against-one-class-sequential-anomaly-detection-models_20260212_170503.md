---
ver: rpa2
title: Backdoor Attack against One-Class Sequential Anomaly Detection Models
arxiv_id: '2402.10283'
source_url: https://arxiv.org/abs/2402.10283
tags:
- backdoor
- detection
- perturbed
- anomaly
- benign
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a backdoor attack framework against one-class
  sequential anomaly detection models like Deep SVDD and OC4Seq. The attack has two
  stages: trigger generation, where imperceptible trigger patterns are crafted by
  perturbing normal sequences without adding anomalies, and backdoor injection, where
  learning objectives are added to drift perturbed samples toward the normal center
  and align their representations with benign ones.'
---

# Backdoor Attack against One-Class Sequential Anomaly Detection Models

## Quick Facts
- **arXiv ID**: 2402.10283
- **Source URL**: https://arxiv.org/abs/2402.10283
- **Reference count**: 18
- **Primary result**: Introduces a backdoor attack framework against one-class sequential anomaly detection models like Deep SVDD and OC4Seq, achieving up to 87% attack success rate while maintaining F1 > 95% benign performance.

## Executive Summary
This paper presents a novel backdoor attack framework targeting one-class sequential anomaly detection models. The attack operates in two stages: trigger generation, where imperceptible backdoor patterns are crafted by perturbing normal sequences without introducing detectable anomalies, and backdoor injection, where learning objectives are added to drift perturbed samples toward the normal center and align their representations with benign ones. The framework enables attackers to inject abnormal entries into sequences that will be misclassified as normal, while maintaining strong benign performance on anomaly detection tasks.

## Method Summary
The backdoor attack framework targets one-class sequential anomaly detection models (Deep SVDD and OC4Seq) through a two-stage approach. First, trigger generation creates perturbed sequences by replacing specific entries with normal entries from training data, establishing imperceptible backdoor patterns. Second, backdoor injection modifies the training objective to include perturbed sequence center drifting (aligning perturbed sample center with normal center) and representation drifting (maximizing mutual information between perturbed and benign representations). The infected models are trained on both benign and perturbed samples, enabling successful attack deployment where abnormal entries injected at placeholder positions are misclassified as normal.

## Key Results
- Infected models maintain strong benign performance (F1 > 95%) while achieving high attack success rates
- Attack success rates reach up to 87% for 6 injected anomalies
- Visualization confirms perturbed and poisoned samples cluster near the benign center, evading detection
- The attack significantly outperforms benign models in handling sequences with injected abnormal entries

## Why This Works (Mechanism)

### Mechanism 1: Trigger Generation
The attack creates imperceptible backdoor patterns by replacing specific entries with normal entries from training data. The core assumption is that the anomaly detection model will focus on subsequence patterns rather than specific entry positions, allowing abnormal entries to be injected at placeholder positions without detection. The trigger pattern is established based on the unchanged subsequence, making the perturbed samples remain normal while serving as backdoor triggers.

### Mechanism 2: Backdoor Injection
Two learning objectives are introduced to ensure backdoored samples are indistinguishable from benign ones. Perturbed sequence center drifting aligns the center of perturbed samples with the normal center in latent space, while representation drifting maximizes mutual information between perturbed and benign representations. This dual approach ensures that sequences with injected abnormal entries maintain similarity to normal samples in the feature space, preventing detection.

### Mechanism 3: Performance Balance
The framework achieves the delicate balance of maintaining high benign performance while enabling successful attacks by carefully tuning hyperparameters (α and β) and using imperceptible triggers. The infected model simultaneously optimizes for normal anomaly detection performance and backdoor objectives without significant performance degradation, demonstrating the effectiveness of the attack design.

## Foundational Learning

- **Mutual Information Maximization**: Used to ensure perturbed sequences maintain similarity to their benign counterparts in latent space, making it harder to distinguish backdoored samples. *Quick check: How does maximizing mutual information between perturbed and benign representations help prevent detection of backdoored samples?*

- **Hypersphere-based Anomaly Detection**: Deep SVDD and OC4Seq use hyperspheres to enclose normal samples, and the attack strategy relies on understanding how perturbations affect distances to the center. *Quick check: Why is it important that perturbed samples are close to the normal center in hypersphere-based anomaly detection models?*

- **Sequential Data Representation Learning**: The attack works with sequential anomaly detection models, requiring understanding of how sequences are represented and processed in models like LSTM/GRU. *Quick check: How does using the last hidden state of LSTM/GRU as the sequence representation affect the attack's effectiveness?*

## Architecture Onboarding

- **Component map**: Trigger Generation Module → Backdoor Injection Module → Attack Deployment Module → Model Training Pipeline
- **Critical path**: Trigger generation → Backdoor injection training → Model deployment → Attack execution
- **Design tradeoffs**: Number of placeholders (m) vs. attack success rate; Hyperparameter balance (α, β) vs. benign performance; Perturbation extent vs. detection evasion
- **Failure signatures**: Sudden drop in benign performance (F1 score below 90%); Low attack success rate despite proper trigger injection; Model overfitting to perturbed samples; Hypersphere boundary shifts significantly
- **First 3 experiments**: 1) Test trigger generation with different numbers of perturbed samples per benign sample (t parameter); 2) Vary the number of placeholders (m) to determine maximum abnormal entries that can be injected; 3) Test different hyperparameter combinations (α, β) to find optimal balance

## Open Questions the Paper Calls Out

### Open Question 1
How does the effectiveness of backdoor attacks scale with sequence length and the number of placeholders (m) beyond the tested values (m=6, sequence length=40)? The paper's experiments are limited to specific parameter values, and the impact of varying these parameters on attack effectiveness is not explored.

### Open Question 2
Can the proposed backdoor attack framework be adapted to target anomaly detection models that do not rely on distance-based one-class classification, such as those based on density estimation or reconstruction error? The paper focuses on distance-based models and does not investigate the applicability to other types of anomaly detection models.

### Open Question 3
How do the proposed backdoor attack framework and its components perform against real-world anomaly detection systems with complex data distributions and diverse anomaly types? The paper evaluates on controlled datasets but does not explore performance on real-world systems with more complex scenarios.

## Limitations

- The framework's effectiveness depends heavily on the careful balance of hyperparameters, which may be difficult to achieve in practice
- The attack assumes the availability of normal training data for trigger generation, limiting applicability when only limited or no benign samples are available
- The paper does not explore the framework's robustness against detection mechanisms specifically designed to identify backdoor attacks

## Confidence

- **High Confidence**: The overall two-stage attack framework and its conceptual validity against one-class anomaly detection models
- **Medium Confidence**: The effectiveness metrics (F1 > 95% benign performance, up to 87% ASR) based on reported experimental results
- **Medium Confidence**: The claim that trigger patterns remain imperceptible while enabling successful backdoor injection

## Next Checks

1. Implement and test the trigger generation mechanism with varying numbers of perturbed samples (t) and placeholder positions (m) to empirically determine the optimal balance between imperceptibility and attack success rate.

2. Validate the mutual information maximization implementation by testing different discriminator architectures and measuring their impact on the similarity between perturbed and benign representations.

3. Conduct ablation studies by removing each component of the backdoor injection (center drifting and representation drifting) to quantify their individual contributions to the overall attack effectiveness.