---
ver: rpa2
title: Dissecting Adversarial Robustness of Multimodal LM Agents
arxiv_id: '2406.12814'
source_url: https://arxiv.org/abs/2406.12814
tags:
- agent
- adversarial
- agents
- attack
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We study the robustness of multimodal language model agents against
  adversarial attacks. We build a new adversarial extension of VisualWebArena, a real
  web environment for agents, and manually annotate 200 targeted adversarial tasks.
---

# Dissecting Adversarial Robustness of Multimodal LM Agents

## Quick Facts
- arXiv ID: 2406.12814
- Source URL: https://arxiv.org/abs/2406.12814
- Authors: Chen Henry Wu; Rishi Shah; Jing Yu Koh; Ruslan Salakhutdinov; Daniel Fried; Aditi Raghunathan
- Reference count: 27
- Key outcome: We build a new adversarial extension of VisualWebArena and manually annotate 200 targeted adversarial tasks. We find that imperceptible perturbations to a single image can hijack latest agents to execute targeted adversarial goals with success rates up to 67%.

## Executive Summary
This paper studies the robustness of multimodal language model agents against adversarial attacks in real web environments. The authors build a new adversarial extension of VisualWebArena, a real web environment for agents, and manually annotate 200 targeted adversarial tasks. They propose the Agent Robustness Evaluation (ARE) framework, which views agents as graphs showing the flow of intermediate outputs between components, and decomposes robustness as the flow of adversarial information on the graph. They find that they can successfully break latest agents that use black-box frontier LMs, including those that perform reflection and tree search.

## Method Summary
The paper builds VisualWebArena-Adv with 200 manually annotated adversarial tasks including trigger images/texts and targeted adversarial goals. Three attack methods are implemented: text injection for black-box text access, captioner attack for white-box image access, and CLIP attack for black-box image access. The Agent Robustness Evaluation (ARE) framework decomposes agent robustness into measurable edge weights representing adversarial influence propagation through the agent graph. The framework quantifies how adversarial perturbations propagate through the system by computing edge weights λ(e) that bound the maximum attack success rate attributable to each intermediate output.

## Key Results
- Imperceptible perturbations to a single image (less than 5% of total web page pixels) can hijack agents to execute targeted adversarial goals with success rates up to 67%
- Inference-time compute that typically improves benign performance can open up new vulnerabilities and harm robustness
- An attacker can compromise the evaluator used by the reflexion agent and the value function of the tree search agent, which increases attack success relatively by 15% and 20%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ARE framework decomposes agent robustness into measurable edge weights representing adversarial influence propagation.
- Mechanism: By modeling the agent as a directed graph where nodes represent components and edges represent information flow, ARE quantifies how adversarial perturbations propagate through the system by computing edge weights λ(e) that bound the maximum attack success rate attributable to each intermediate output.
- Core assumption: Adversarial influence is additive and propagates independently through each edge, allowing decomposition into individual edge weights that can be computed separately.
- Evidence anchors:
  - [abstract] "ARE views the agent as a graph showing the flow of intermediate outputs between components and decomposes robustness as the flow of adversarial information on the graph."
  - [section 3.3] "We define the adversarial influence of an intermediate output c, AdvIn(c) ∈ [0, 1] as the tightest upper bound on the expected attack success rate if the edge takes value c and no further downstream component is attacked."
  - [corpus] Weak - no direct corpus evidence for this specific decomposition mechanism.

### Mechanism 2
- Claim: Inference-time compute components like evaluators and value functions can paradoxically increase agent vulnerability when attacked.
- Mechanism: When components that typically improve benign performance (like reflexion agents with evaluators or tree search agents with value functions) are compromised, they can actively bias the agent toward adversarial goals rather than blocking them, creating new attack surfaces.
- Core assumption: Components designed to improve reasoning or decision-making can be hijacked to serve adversarial purposes rather than their intended defensive role.
- Evidence anchors:
  - [abstract] "We find that inference-time compute that typically improves benign performance can open up new vulnerabilities and harm robustness."
  - [section 5.2] "Two key phenomena increase the ASR: (1) the evaluator more readily accepts adversarial actions... and (2) it is more likely to reject non-adversarial actions and produce adversarial reflections."
  - [section 5.3] "An attacked value function increases the ASR from 31% to 38%... a value function can get compromised and decreases the robustness by biasing the agent toward adversarial actions through adversarial scores."

### Mechanism 3
- Claim: CLIP model attacks can transfer to black-box large language models despite domain shift from image classification to multimodal reasoning.
- Mechanism: By optimizing perturbations on CLIP model encoders using contrastive loss between target and negative text embeddings, the attack finds perturbations that generalize to black-box LMs that weren't directly optimized against.
- Core assumption: CLIP models and black-box LMs share sufficient representational similarity in their image encoding spaces for adversarial perturbations to transfer effectively.
- Evidence anchors:
  - [section 4.3] "We make necessary modifications to their method to improve the performance in our targeted setting... We optimize the image perturbation δ to maximize: cos(E(i)x(x + δ), E(i)y(z)) − cos(E(i)x(x + δ), E(i)y(z−))"
  - [section 5.1] "We see that 38% of the captions generated by the black-box LM captioner are adversarial (as seen by λ of that edge). This result shows that attacks on CLIP models can generalize to black-box LMs."
  - [corpus] Weak - no direct corpus evidence for this specific CLIP-to-LM transfer mechanism.

## Foundational Learning

- Concept: Adversarial robustness evaluation in compound systems
  - Why needed here: Traditional robustness evaluation focuses on single models, but agents are compound systems with multiple interacting components that create complex attack surfaces
  - Quick check question: What key difference between chatbots and autonomous agents necessitates new robustness evaluation approaches?

- Concept: Graph-based system decomposition for security analysis
  - Why needed here: The ARE framework requires understanding how to model complex systems as graphs where nodes represent components and edges represent information flow for systematic security analysis
  - Quick check question: How does modeling an agent as a graph enable systematic decomposition of robustness into component-level contributions?

- Concept: Transferability of adversarial attacks across model architectures
  - Why needed here: Understanding when attacks on surrogate models (like CLIP) can generalize to target models (like black-box LMs) is crucial for developing effective black-box attacks
  - Quick check question: What factors determine whether adversarial perturbations optimized on one model architecture will transfer to a different architecture?

## Architecture Onboarding

- Component map: Base agent (policy model) → Captioner-augmented agent (captioner + policy model) → Reflexion agent (policy model + evaluator) → Tree search agent (policy model + value function + tree search)
- Critical path: Environment → Trigger injection → Component processing → Policy decision → Action execution → Goal achievement
- Design tradeoffs: Adding components improves benign performance but introduces new vulnerabilities; white-box components enable stronger attacks but black-box components are harder to attack
- Failure signatures: High ASR indicates successful attack propagation; edge weights showing λ > 0.5 indicate components vulnerable to attack; decreased ASR after adding components suggests defensive benefits
- First 3 experiments:
  1. Run base agent without attack to establish baseline benign success rate
  2. Apply text injection attack to policy model and measure ASR increase
  3. Add captioner component and test whether captioner attack achieves similar ASR to text injection attack

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the success rate of adversarial attacks scale with the number of components in an agent system? Are there diminishing returns or accelerating vulnerabilities as agents become more complex?
- Basis in paper: [inferred] The paper shows that adding components like evaluators and value functions can both help and harm robustness depending on whether they are attacked. This suggests a complex relationship between component count and overall vulnerability.
- Why unresolved: The paper only examines a limited set of agent configurations. The relationship between component count and attack success rate across diverse agent architectures remains unexplored.
- What evidence would resolve it: Empirical studies comparing attack success rates across agent systems with varying numbers and types of components, controlling for other variables like model size and task complexity.

### Open Question 2
- Question: What is the effectiveness of adversarial training specifically for multimodal agents in realistic environments, and how does it compare to unimodal approaches?
- Basis in paper: [explicit] The paper discusses several baseline defenses but finds limited gains, suggesting current approaches may be insufficient for the multimodal setting.
- Why unresolved: The paper does not explore adversarial training as a defense mechanism, leaving a gap in understanding whether this standard technique in adversarial robustness can be effectively adapted for multimodal agents.
- What evidence would resolve it: Comparative studies of adversarial training approaches for multimodal agents versus unimodal components, measuring both attack success rates and impact on benign performance.

### Open Question 3
- Question: How transferable are adversarial examples across different multimodal language models and architectures in agent systems?
- Basis in paper: [explicit] The paper demonstrates that CLIP attacks can generalize to black-box LMs, but the extent of this generalization across different model families and architectures is unclear.
- Why unresolved: The paper only tests a limited set of models (GPT-4V, Claude-3-Opus, Gemini-1.5-Pro, GPT