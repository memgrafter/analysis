---
ver: rpa2
title: 'Odyssey: Empowering Minecraft Agents with Open-World Skills'
arxiv_id: '2407.15325'
source_url: https://arxiv.org/abs/2407.15325
tags:
- task
- minecraft
- agent
- tasks
- open-world
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ODYSSEY introduces a framework that empowers LLM-based agents with
  open-world skills for efficient exploration in Minecraft. It combines a comprehensive
  skill library (40 primitive + 183 compositional skills), a fine-tuned LLaMA-3 model
  trained on 390k+ Minecraft Q&A entries, and a new benchmark with long-term planning,
  dynamic-immediate planning, and autonomous exploration tasks.
---

# Odyssey: Empowering Minecraft Agents with Open-World Skills

## Quick Facts
- arXiv ID: 2407.15325
- Source URL: https://arxiv.org/abs/2407.15325
- Authors: Shunyu Liu; Yaoru Li; Kongcheng Zhang; Zhenyu Cui; Wenkai Fang; Yuxuan Zheng; Tongya Zheng; Mingli Song
- Reference count: 40
- Key outcome: ODYSSEY introduces a framework that empowers LLM-based agents with open-world skills for efficient exploration in Minecraft, combining a comprehensive skill library with fine-tuned models and new benchmark tasks.

## Executive Summary
ODYSSEY presents a framework for empowering LLM-based agents with open-world skills in Minecraft, enabling efficient exploration and task completion. The system combines a comprehensive skill library (40 primitive + 183 compositional skills), fine-tuned LLaMA-3 models trained on 390k+ Minecraft Q&A entries, and a new benchmark with long-term planning, dynamic-immediate planning, and autonomous exploration tasks. The framework demonstrates significant improvements in agent efficiency, with MineMA-8B outperforming LLaMA-3-8B and MineMA-70B showing superior performance in dynamic planning tasks.

## Method Summary
The ODYSSEY framework employs an LLM planner-actor-critic architecture where the planner decomposes high-level goals into subgoals corresponding to specific skills from a comprehensive library. The actor retrieves and executes the most relevant skill code based on semantic similarity, while the critic provides feedback for strategy refinement through self-validation and self-reflection. The framework uses fine-tuned LLaMA-3 models (MineMA-8B and MineMA-70B) trained on Minecraft-specific Q&A data, enabling more effective planning and decision-making in the Minecraft environment.

## Key Results
- The skill library significantly improves agent efficiency, enabling MineMA-8B to outperform LLaMA-3-8B in success rates and completion times across tasks
- MineMA-70B shows superior performance over MineMA-8B in dynamic planning tasks
- The autonomous exploration task demonstrates consistent performance with GPT-4-based methods while using open-source models

## Why This Works (Mechanism)

### Mechanism 1
The open-world skill library improves agent efficiency by providing pre-built, composable skills that bypass the need for agents to learn basic programmatic tasks from scratch. The framework combines 40 primitive skills (direct Mineflayer API calls) with 183 compositional skills (higher-level abstractions) that can be recursively executed, automatically ensuring prerequisites like tools are crafted before attempting higher-level tasks.

### Mechanism 2
Fine-tuning LLaMA-3 on Minecraft-specific Q&A data improves agent performance compared to using the base model. The framework generates 390k+ instruction entries from Minecraft Wiki content, then uses LoRA fine-tuning to adapt LLaMA-3 to Minecraft domain knowledge, creating MineMA models that better understand Minecraft-specific contexts and generate more accurate plans.

### Mechanism 3
The LLM planner-actor-critic architecture enables effective task decomposition and execution in complex open-world environments. The planner breaks down high-level goals into subgoals corresponding to specific skills, the actor retrieves and executes the most relevant skill code, and the critic provides feedback for strategy refinement through self-validation and self-reflection.

## Foundational Learning

- Concept: Recursive skill decomposition and execution
  - Why needed here: Allows agents to handle complex tasks by automatically ensuring prerequisite skills are completed before executing higher-level skills
  - Quick check question: How does the framework ensure an agent has the right tools before attempting to mine diamond?

- Concept: Semantic similarity-based skill retrieval
  - Why needed here: Enables the agent to map natural language subgoals to executable skill code by finding the most relevant skills in the library
  - Quick check question: What method does the framework use to find the most appropriate skill for a given subgoal?

- Concept: Iterative planning and optimization
  - Why needed here: Allows agents to improve their strategies over time by learning from previous successes and failures
  - Quick check question: How does the framework use feedback from previous combat attempts to improve future planning?

## Architecture Onboarding

- Component map: Planner → Skill Retrieval → Actor → Execution → Critic → Feedback → Updated Planning
- Critical path: The system flows from goal decomposition through skill execution to strategy refinement in an iterative loop
- Design tradeoffs: Skill library comprehensiveness vs. maintenance overhead; model size (8B vs 70B) vs. computational cost and performance; fine-tuning data quality vs. development effort
- Failure signatures: Agent gets stuck in planning loops (planner issue); skills are not found or executed correctly (skill library/actor issue); agent performs poorly despite having correct plans (critic/feedback issue); performance degrades over time (model degradation or environment changes)
- First 3 experiments: 1) Test basic skill execution to verify primitive and compositional skills work correctly in isolation; 2) Test planner-subgoal mapping to verify the planner can decompose simple goals into appropriate subgoals; 3) Test full pipeline on simple task to verify planner → actor → critic → feedback loop works on a basic crafting task

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the fine-tuned LLaMA-3 models (MineMA-8B and MineMA-70B) compare to other state-of-the-art LLM-based agents in Minecraft, such as Voyager with GPT-4 or other recent approaches? The paper mentions some comparisons but lacks a comprehensive benchmark against all major LLM-based agents in Minecraft.

### Open Question 2
How does the recursive method for skill execution in the open-world skill library impact the agent's efficiency and adaptability in complex, multi-step tasks compared to non-recursive approaches? The paper mentions benefits but doesn't provide quantitative evidence or ablation studies specifically isolating the impact of the recursive method.

### Open Question 3
How does the size of the open-world skill library (number of primitive and compositional skills) affect the agent's performance and ability to handle diverse tasks in Minecraft? The paper presents a library with 40 primitive and 183 compositional skills but doesn't explore how performance might change with different library sizes.

## Limitations
- Model performance generalization is uncertain beyond the specific Minecraft benchmark tasks and versions
- Skill library coverage lacks systematic analysis of coverage gaps and fallback rates
- Evaluation benchmark scope may not capture the full complexity of open-world exploration with only three task types

## Confidence

**High Confidence Claims**:
- The skill library improves agent efficiency by reducing the need for basic programmatic learning
- The LLM planner-actor-critic architecture enables effective task decomposition
- Fine-tuning on Minecraft-specific Q&A data improves model performance

**Medium Confidence Claims**:
- MineMA-8B outperforms LLaMA-3-8B in success rates and completion times
- MineMA-70B shows superior performance over MineMA-8B in dynamic planning tasks
- The framework demonstrates consistent performance with GPT-4-based methods

**Low Confidence Claims**:
- The framework's performance will generalize to other open-world environments
- The skill library provides comprehensive coverage for most Minecraft tasks
- The LLM planner-actor-critic architecture is optimal for all open-world scenarios

## Next Checks

1. **Skill Library Coverage Analysis**: Conduct systematic testing to quantify the frequency of skill library coverage gaps across a broader range of Minecraft tasks, measuring fallback rates and execution efficiency when agents must generate code from scratch versus using pre-built skills.

2. **Cross-Environment Generalization Test**: Evaluate the framework's performance on alternative open-world environments (e.g., other sandbox games or simulated worlds) to assess whether the planning and skill execution mechanisms transfer beyond the Minecraft-specific context.

3. **Long-Term Performance Stability**: Implement extended testing periods (multiple hours/days) to monitor whether the agent's performance degrades over time due to model drift, skill library limitations, or environmental changes, particularly in the autonomous exploration task.