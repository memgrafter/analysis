---
ver: rpa2
title: Resilience of Entropy Model in Distributed Neural Networks
arxiv_id: '2403.00942'
source_url: https://arxiv.org/abs/2403.00942
tags:
- entropy
- compression
- size
- data
- resilience
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the resilience of entropy models in distributed
  deep neural networks (DNNs) to intentional and unintentional interference. Entropy
  coding has been introduced to reduce communication overhead by adaptively encoding
  latent representations into bit streams.
---

# Resilience of Entropy Model in Distributed Neural Networks

## Quick Facts
- arXiv ID: 2403.00942
- Source URL: https://arxiv.org/abs/2403.00942
- Reference count: 40
- Entropy attacks can increase communication overhead by up to 95% in distributed DNNs

## Executive Summary
This paper investigates the resilience of entropy models in distributed deep neural networks to intentional and unintentional interference. Entropy coding has been introduced to reduce communication overhead by adaptively encoding latent representations into bit streams. Through extensive experiments with 3 DNN architectures, 2 entropy models, and 4 rate-distortion trade-off factors, the paper demonstrates that entropy attacks can significantly increase communication overhead. To address this issue, the paper proposes a defense mechanism that disentangles compression features in frequency and spatial domains using object-aware total variation denoising.

## Method Summary
The method implements a distributed DNN architecture with a head model on mobile devices and a tail model on servers, using entropy coding for compression. The entropy model estimates probability distributions of latent representations to assign variable-length codes. Adversarial examples are generated using PGD attacks targeting both accuracy and entropy. The proposed defense applies object-aware total variation denoising to inputs before feeding them into the head and entropy model, using the entropy model's probability output as a soft mask to selectively remove high-frequency noise from non-object regions.

## Key Results
- Entropy attacks increase communication overhead by up to 95% across different architectures
- Proposed defense reduces transmission overhead of attacked inputs by about 9% compared to unperturbed data
- Defense maintains classification accuracy with only about 2% loss
- Shot noise increases data size by 65.31% on average, indicating entropy model learns different features than classification model

## Why This Works (Mechanism)

### Mechanism 1
The entropy model learns a different feature set than the task classifier, making compression vulnerable to high-frequency perturbations. The model assigns variable-length codes based on estimated probability distributions of latent representations, which depend on low-level texture and edge features rather than high-level semantics. Adversarial perturbations that alter high-frequency components can thus shift the entropy estimates and increase bit rates.

### Mechanism 2
Background regions are more vulnerable to entropy-targeted attacks because they carry less semantic weight for classification. The entropy model assigns higher bit rates to semantically important regions (e.g., objects) and compresses backgrounds. Attackers can increase overall bit rate by adding high-frequency noise to backgrounds without affecting classification, exploiting the mismatch between compression and task objectives.

### Mechanism 3
Object-aware total variation denoising can selectively remove high-frequency noise from non-object regions while preserving semantic features. By using the entropy model's probability output as a soft mask, TV denoising is applied more aggressively in low-probability (background) regions. This reduces high-frequency perturbations that inflate entropy estimates without harming object features critical for classification.

## Foundational Learning

- **Entropy coding and rate-distortion tradeoff**: Why needed here - The paper hinges on how entropy models estimate probability distributions to assign variable-length codes; understanding the rate-distortion tradeoff is key to grasping why perturbations affect bit rates. Quick check question: In entropy coding, what does minimizing the entropy of latent representations achieve in terms of compression?

- **Total variation denoising**: Why needed here - The defense mechanism relies on TV denoising to selectively remove high-frequency noise; knowing how TV measures image gradients and preserves edges is essential. Quick check question: What does the total variation of an image measure, and why is it useful for denoising?

- **Adversarial attack formulation in l∞ space**: Why needed here - The paper evaluates both classification-targeted and entropy-targeted PGD attacks; understanding the l∞ constraint and gradient-based optimization is crucial for interpreting attack results. Quick check question: In l∞ constrained attacks, what does the constraint ϵ control, and how does it differ from l₂ constraints?

## Architecture Onboarding

- **Component map**: Input image -> Head network -> Latent representation z -> Entropy model estimates PZ(z) -> Arithmetic encoding produces bit stream -> Bit stream transmitted to tail -> Tail decodes and completes inference

- **Critical path**: 1. Input image → Head network → Latent representation z 2. Entropy model estimates PZ(z) 3. Arithmetic encoding produces bit stream 4. Bit stream transmitted to tail 5. Tail decodes and completes inference 6. Defense module intercepts input, applies denoising, then proceeds with steps 1-5

- **Design tradeoffs**:
  - Compression vs. robustness: Aggressive compression (high β) improves bandwidth usage but may amplify sensitivity to perturbations
  - Denoising strength vs. accuracy: Stronger TV denoising reduces bit rate but risks smoothing out task-relevant high-frequency details
  - Mask accuracy vs. defense effectiveness: A better soft mask (from entropy model) improves selective denoising, but misalignment can hurt performance

- **Failure signatures**:
  - Bit rate spikes: Sudden increase in transmitted data size suggests entropy-targeted attack
  - Accuracy drop with modest bit rate change: Likely classification-targeted attack
  - Persistent high bit rate after denoising: Indicates weak mask alignment or attack targeting object regions

- **First 3 experiments**:
  1. Run clean ImageNet validation set through head+entropy model; record baseline bit rate and accuracy
  2. Apply PGD-E (entropy-targeted) with varying ϵ; measure bit rate increase and accuracy change
  3. Apply proposed object-aware TV denoising before head+entropy model; compare post-defense bit rate and accuracy to baseline

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed defense mechanism perform against adaptive attacks in real-world scenarios beyond ImageNet? The paper discusses adaptive attacks in Section 5.4, evaluating the defense against low frequency and regional attacks, but only within the ImageNet dataset. Testing the defense mechanism on a variety of real-world datasets and scenarios would provide insights into its generalizability and robustness.

### Open Question 2
Can the proposed defense mechanism be integrated with other existing defense techniques, such as adversarial training, to further enhance resilience? While the paper acknowledges the potential for integration, it does not explore or provide experimental results on combining the proposed defense with other techniques. Conducting experiments that combine the proposed defense mechanism with adversarial training would provide insights into potential synergies and improvements.

### Open Question 3
How does the proposed defense mechanism affect the overall computational efficiency and latency of the distributed DNN system? The paper focuses on reducing the transmission overhead and maintaining accuracy but does not discuss the impact on computational efficiency and latency. Conducting experiments to measure the computational efficiency and latency introduced by the defense mechanism would provide insights into its practical implications for real-world deployments.

## Limitations
- The entropy model's vulnerability to high-frequency perturbations is demonstrated empirically but lacks theoretical grounding on why compression features differ so strongly from classification features
- The effectiveness of the soft mask relies on the entropy model's probability map accurately identifying object regions, but no validation is provided on mask quality or alignment accuracy
- The proposed defense reduces bit rate by ~9% on attacked data but the comparison baseline suggests residual vulnerability may remain unexplained

## Confidence

- **High confidence**: Entropy attacks can increase communication overhead (empirical results show up to 95% increase across architectures)
- **Medium confidence**: Object-aware TV denoising reduces transmission overhead of attacked inputs while maintaining accuracy (9% reduction with 2% accuracy loss)
- **Medium confidence**: The entropy model learns different features than the task classifier, making it vulnerable to high-frequency perturbations (supported by shot noise experiments and correlation analysis)

## Next Checks

1. **Mask quality validation**: Quantify the alignment accuracy between entropy model probability maps and ground truth object masks on a subset of validation images

2. **Feature importance analysis**: Perform ablation studies removing high-frequency components to determine which frequency bands drive the bit rate increases under entropy-targeted attacks

3. **Defense robustness testing**: Evaluate the proposed defense against attacks specifically targeting object regions to verify the soft mask prevents oversmoothing of semantic features