---
ver: rpa2
title: 'Text-Free Multi-domain Graph Pre-training: Toward Graph Foundation Models'
arxiv_id: '2405.13934'
source_url: https://arxiv.org/abs/2405.13934
tags:
- graph
- domain
- pre-training
- domains
- multi-domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MDGPT, a text-free multi-domain graph pre-training
  framework designed to train universal graph models across diverse domains. The key
  challenges addressed are aligning multi-domain graphs with divergent characteristics
  and adapting pre-trained knowledge to downstream tasks in seen or unseen domains.
---

# Text-Free Multi-domain Graph Pre-training: Toward Graph Foundation Models

## Quick Facts
- arXiv ID: 2405.13934
- Source URL: https://arxiv.org/abs/2405.13934
- Reference count: 40
- Outperforms state-of-the-art baselines by up to 37.9% in one-shot node classification tasks

## Executive Summary
This paper introduces MDGPT, a text-free multi-domain graph pre-training framework designed to train universal graph models across diverse domains. The framework addresses the challenge of aligning multi-domain graphs with divergent characteristics through domain tokens and adapts pre-trained knowledge to downstream tasks using dual prompts (unifying and mixing). Extensive experiments demonstrate MDGPT's superior performance in both seen and unseen domains while maintaining high parameter efficiency.

## Method Summary
MDGPT uses domain tokens to align semantic spaces across different graph domains, where each domain token is a learnable vector that modifies dimension-aligned features via element-wise multiplication. The framework employs dual prompts for downstream adaptation: a unifying prompt aligns target domains with unified pre-trained knowledge, while a mixing prompt aggregates domain tokens for tailored domain-specific knowledge. Only prompts are updated during downstream adaptation, making the method highly parameter-efficient.

## Key Results
- Achieves up to 37.9% improvement over state-of-the-art baselines in one-shot node classification
- Demonstrates superior performance in both seen and unseen domains
- Shows highest parameter efficiency by updating only dual prompts during adaptation

## Why This Works (Mechanism)

### Mechanism 1
Domain tokens align semantic spaces across different graph domains by using learnable vectors associated with each source domain that modify dimension-aligned features via element-wise multiplication, creating unified feature spaces. Core assumption: element-wise multiplication with domain-specific vectors can bridge semantic gaps between different graph domains. Break condition: if domain tokens fail to capture domain-specific semantic patterns, they may introduce noise rather than alignment.

### Mechanism 2
Dual prompts enable effective adaptation to both seen and unseen domains through a unifying prompt that aligns target domain with unified pre-trained knowledge and a mixing prompt that aggregates domain tokens for tailored domain-specific knowledge. Core assumption: combining unified and domain-specific knowledge provides better adaptation than either alone. Break condition: if mixing coefficients fail to prioritize relevant source domains, adaptation quality degrades.

### Mechanism 3
Parameter-efficient prompting outperforms full fine-tuning by updating only prompts during downstream adaptation while keeping pre-trained model weights frozen. Core assumption: lightweight parameter updates can achieve comparable performance to full fine-tuning with fewer parameters. Break condition: if the task gap between pre-training and downstream tasks is too large, parameter-efficient methods may underperform.

## Foundational Learning

- Concept: Graph neural networks and their limitations
  - Why needed here: Understanding GNNs is essential for grasping why multi-domain pre-training is challenging and why MDGPT's approach matters
  - Quick check question: What are the key differences between conventional GNN training and MDGPT's multi-domain pre-training approach?

- Concept: Self-supervised learning and contrastive methods
  - Why needed here: The pre-training objective uses link prediction based on subgraph similarity, which requires understanding of self-supervised learning paradigms
  - Quick check question: How does the link prediction task in MDGPT differ from traditional supervised GNN training?

- Concept: Prompt learning and parameter-efficient fine-tuning
  - Why needed here: MDGPT's adaptation strategy relies on prompt tuning rather than full fine-tuning, which is a relatively new concept in graph learning
  - Quick check question: What are the trade-offs between using dual prompts versus full fine-tuning for downstream adaptation?

## Architecture Onboarding

- Component map: Dimension alignment module (SVD/MLP) -> Domain tokens (learnable vectors) -> Graph encoder (GCN/Transformer) -> Dual prompts (unifying and mixing) -> Downstream classifier
- Critical path: Dimension alignment → Domain token application → Multi-domain pre-training → Target domain alignment → Dual prompt application → Downstream task adaptation
- Design tradeoffs: Domain token dimension vs. semantic coverage (higher dimensions increase capacity but may amplify semantic gaps); Prompt complexity vs. parameter efficiency (more complex prompts may improve performance but reduce efficiency gains); Source domain selection (including too many domains may cause conflicts; too few limits knowledge transfer)
- Failure signatures: Poor performance on seen domains (may indicate domain token misalignment); Worse performance with more source domains (suggests interference rather than synergy); Inconsistent results across runs (could indicate unstable prompt tuning)
- First 3 experiments: 1) Test dimension alignment effectiveness: Pre-train with and without domain tokens on Cora using Citeseer as target; 2) Validate dual prompt contribution: Compare performance with only unifying prompt, only mixing prompt, and both; 3) Measure parameter efficiency: Compare accuracy vs. number of tunable parameters across GCN, GraphCL, GraphPrompt, and MDGPT

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of MDGPT change when applied to graphs with very high dimensional node features (e.g., thousands of dimensions) compared to the relatively low dimensional features used in the current experiments? The paper mentions dimension alignment using SVD or MLP with unified feature dimension set to 50, but does not explore scenarios with very high-dimensional original features. Experiments on datasets with high-dimensional features would provide evidence of how MDGPT scales with feature dimensionality.

### Open Question 2
What is the impact of using different types of graph neural network architectures (e.g., Graph Attention Networks, Graph Transformers) as the backbone in MDGPT compared to the 3-layer GCN used in the current experiments? The paper uses a 3-layer GCN as the base model but does not explore other GNN architectures. Comparative experiments using different GNN architectures would provide evidence of the architecture's impact on performance.

### Open Question 3
How does the performance of MDGPT vary with the number of source domains used during pre-training, especially when the number of domains is significantly larger than the five used in the current experiments? The paper uses five source domains for pre-training and shows performance improvements with more domains, but does not explore scenarios with a much larger number of domains. Experiments with a much larger number of source domains would provide evidence of how MDGPT scales with the number of domains.

### Open Question 4
How does the performance of MDGPT change when applied to graph classification tasks with a large number of classes (e.g., hundreds or thousands of classes) compared to the relatively small number of classes used in the current experiments? The paper conducts graph classification experiments on datasets with a relatively small number of classes and does not explore scenarios with a large number of classes. Experiments on graph classification tasks with a large number of classes would provide evidence of how MDGPT performs in such scenarios.

## Limitations

- Limited empirical validation of domain token effectiveness in bridging semantic gaps across truly heterogeneous domains
- Lack of ablation studies isolating the specific contribution of dual prompts versus other design choices
- Insufficient exploration of the limits of prompt-based adaptation and scenarios where it might fail

## Confidence

**High Confidence:** The experimental results demonstrating MDGPT's superior performance over baseline methods are well-supported by the presented data with accuracy improvements up to 37.9%.

**Medium Confidence:** The theoretical framework for dimension alignment and domain token usage is logically sound but requires further validation of actual effectiveness in bridging semantic gaps.

**Low Confidence:** The claims about parameter efficiency being sufficient for effective adaptation across all possible downstream tasks are not fully substantiated.

## Next Checks

1. **Domain Token Effectiveness Validation:** Conduct controlled experiments where MDGPT is pre-trained with and without domain tokens on pairs of graphs with known semantic differences, measuring alignment quality using domain similarity metrics before and after token application.

2. **Dual Prompt Contribution Isolation:** Design ablation experiments that systematically remove either the unifying prompt or mixing prompt while keeping all other components constant, comparing these variants against full dual prompt MDGPT and full fine-tuning baselines.

3. **Cross-Domain Transfer Robustness:** Test MDGPT on a challenging cross-domain scenario where source domains have minimal structural similarity to target domains, evaluating whether dual prompts can still provide meaningful adaptation when semantic gaps are large.