---
ver: rpa2
title: Interpretable mixture of experts for time series prediction under recurrent
  and non-recurrent conditions
arxiv_id: '2409.03282'
source_url: https://arxiv.org/abs/2409.03282
tags:
- traffic
- data
- non-recurrent
- conditions
- recurrent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of predicting traffic speed
  under both recurrent and non-recurrent conditions (e.g., incidents, accidents).
  The key innovation is a Mixture of Experts (MoE) model that leverages separate expert
  models for recurrent and non-recurrent conditions, specifically using Temporal Fusion
  Transformers (TFTs) as expert backbones.
---

# Interpretable mixture of experts for time series prediction under recurrent and non-recurrent conditions

## Quick Facts
- arXiv ID: 2409.03282
- Source URL: https://arxiv.org/abs/2409.03282
- Reference count: 37
- Primary result: MoE model achieves 11.16% SMAPE and 5.56 RMSE for traffic speed prediction under both recurrent and non-recurrent conditions

## Executive Summary
This paper introduces a Mixture of Experts (MoE) model for traffic speed prediction that addresses the challenge of handling both recurrent patterns (regular traffic flow) and non-recurrent conditions (incidents, accidents). The key innovation is using separate Temporal Fusion Transformer (TFT) expert models for each condition type, with a gating network that dynamically combines their outputs. The model incorporates multi-source data including traffic speed, incident reports, and weather data, and proposes a specialized training pipeline for the non-recurrent expert to handle limited data issues.

## Method Summary
The method employs a Mixture of Experts framework with two Temporal Fusion Transformer (TFT) models as experts - one trained on recurrent traffic patterns (incident-free) and another trained on non-recurrent patterns (with incidents). The non-recurrent expert is pre-trained on recurrent data then fine-tuned on incident data to address limited data issues. A gating network dynamically weights the expert outputs based on input features. The model integrates multi-source data (traffic speed, incidents from Waze, weather) processed with 5-minute temporal resolution. Training follows a two-stage process where the non-recurrent expert is first pre-trained on abundant recurrent data, then fine-tuned on scarce non-recurrent data.

## Key Results
- Achieves 11.16% Symmetric Mean Absolute Percentage Error (SMAPE) and 5.56 Root Mean Squared Error (RMSE)
- Outperforms baseline algorithms in predicting traffic speed during incidents
- Demonstrates superior performance in handling both recurrent and non-recurrent traffic conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The MoE model improves prediction accuracy by using separate expert models for recurrent and non-recurrent traffic conditions
- Mechanism: Recurrent and non-recurrent traffic conditions have fundamentally different patterns - recurrent conditions follow periodic patterns while non-recurrent conditions are volatile due to incidents. By using separate Temporal Fusion Transformer (TFT) models for each condition type, the model can specialize in learning the distinct characteristics of each pattern type rather than trying to learn a single model that handles both conditions.
- Core assumption: Recurrent and non-recurrent traffic conditions are sufficiently different that a single model cannot capture both patterns effectively
- Evidence anchors: Existing traffic speed prediction studies use one single model to learn all possible patterns from these drastically diverse conditions; non-recurrent conditions caused by incidents are volatile because incidents typically lead to sudden disruptions in the flow of traffic

### Mechanism 2
- Claim: The proposed training pipeline for non-recurrent models addresses limited data issues
- Mechanism: The non-recurrent expert model is first pre-trained on recurrent data (which is more abundant) to learn general traffic patterns, then fine-tuned on non-recurrent data. This allows the model to leverage the larger recurrent dataset while still specializing in non-recurrent patterns.
- Core assumption: Recurrent and non-recurrent conditions share some underlying traffic patterns that can be learned from recurrent data
- Evidence anchors: Non-recurrent conditions are not as prevalent as recurrent conditions, so the dataset size of non-recurrent conditions is relatively small; recurrent data contain some common and general traffic patterns that may be helpful for prediction in non-recurrent conditions

### Mechanism 3
- Claim: Multi-source data integration improves prediction accuracy by providing contextual information
- Mechanism: The model incorporates traffic speed data, incident reports, and weather data, all processed and aligned temporally and spatially. This provides richer context for predictions, allowing the model to understand how different factors (incidents, weather) affect traffic patterns differently in recurrent vs non-recurrent conditions.
- Core assumption: Incident and weather data contain predictive signal for traffic speed that complements historical traffic data
- Evidence anchors: Multi-source datasets including traffic speed, incident reports, and weather data are integrated and processed to be informative features; several types of incidents including accidents, road-closing events, and hazard floods are deemed critical and chosen for the study

## Foundational Learning

- Concept: Temporal Fusion Transformer (TFT) architecture
  - Why needed here: TFT is chosen as the expert model backbone because it demonstrated superior forecasting capability and interpretability compared to other deep learning models for time series prediction
  - Quick check question: What are the three main blocks within TFT and what is each responsible for?

- Concept: Mixture of Experts (MoE) framework
  - Why needed here: MoE allows combining specialized models (recurrent and non-recurrent experts) with a gating network that dynamically assigns inputs to the most appropriate expert, enabling the model to handle diverse traffic patterns
  - Quick check question: How does the gating network in MoE determine which expert(s) to utilize for a given input?

- Concept: Traffic condition classification (recurrent vs non-recurrent)
  - Why needed here: The model needs to distinguish between conditions with and without incidents to route predictions through the appropriate expert model
  - Quick check question: Based on the paper, what are the four conditions defined for classifying traffic patterns?

## Architecture Onboarding

- Component map: Multi-source data processing pipeline → Recurrent expert model (TFT) → Non-recurrent expert model (TFT) → Gating network → Weighted combination → Final output

- Critical path: Raw multi-source data → Data processing pipeline → Expert model selection → Expert prediction → Weighted combination → Final output

- Design tradeoffs:
  - Using separate expert models increases complexity but improves specialization
  - Pre-training on recurrent data helps with limited non-recurrent data but may introduce bias
  - Multi-source data integration adds complexity but provides richer context
  - TFT chosen for interpretability over potentially more complex black-box models

- Failure signatures:
  - If gating network fails, predictions may be dominated by wrong expert
  - If data alignment fails, features may be mismatched in time/space
  - If pre-training introduces too much bias, fine-tuning may not overcome it
  - If incident data is too sparse, non-recurrent expert may underperform

- First 3 experiments:
  1. Test expert model selection: Feed known recurrent and non-recurrent examples through the gating network and verify it selects the appropriate expert with high confidence
  2. Test data processing pipeline: Feed raw data through the processing pipeline and verify all features are correctly extracted, aligned, and normalized
  3. Test end-to-end prediction: Run the full model on a small test dataset and verify predictions are reasonable and improve upon baseline models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do incident severity levels impact the performance of the Mixture of Experts model?
- Basis in paper: The paper mentions future work could explore incorporating additional data sources, such as real-time incident severity information, to further enhance prediction accuracy.
- Why unresolved: The current model does not incorporate incident severity levels into its training or prediction process.
- What evidence would resolve it: Incorporating incident severity data into the model and comparing its performance with the current model would provide evidence.

### Open Question 2
- Question: What is the impact of using different expert model backbones on the performance of the Mixture of Experts model?
- Basis in paper: The paper mentions that while TFT is used as the expert model backbone, the choice of expert models is flexible.
- Why unresolved: The paper does not explore the performance of the model with different expert model backbones.
- What evidence would resolve it: Testing the model with different expert model backbones and comparing their performance would provide evidence.

### Open Question 3
- Question: How does the model's performance vary with different prediction horizons?
- Basis in paper: The paper mentions that in time series forecasting problems, how the prediction errors propagate along prediction horizons is of interest.
- Why unresolved: The paper only provides a general observation that errors increase along prediction horizons but does not provide a detailed analysis.
- What evidence would resolve it: A detailed analysis of the model's performance at different prediction horizons would provide evidence.

## Limitations

- Assumes recurrent and non-recurrent conditions are sufficiently different to warrant separate expert models without extensive quantitative validation
- Pre-training strategy for non-recurrent expert may introduce harmful biases if recurrent and non-recurrent patterns are too dissimilar
- Multi-source data integration adds complexity and potential failure points, particularly if incident or weather data quality is poor

## Confidence

- Mechanism 1 (separate expert models): Medium confidence - well-reasoned but needs empirical validation
- Mechanism 2 (pre-training strategy): Low-Medium confidence - theoretically sound but unproven in this context
- Mechanism 3 (multi-source integration): Medium confidence - common practice but implementation-dependent

## Next Checks

1. **Expert selection validation**: Feed a balanced test set of known recurrent and non-recurrent examples through the gating network and measure whether it correctly selects the appropriate expert with >90% accuracy

2. **Data transfer validation**: Compare model performance when training non-recurrent expert with and without pre-training on recurrent data to quantify the actual benefit of the proposed training pipeline

3. **Feature importance analysis**: Perform SHAP or similar interpretability analysis to verify that incident and weather features are indeed being used differently for recurrent vs non-recurrent predictions, and that the model is not simply memorizing patterns