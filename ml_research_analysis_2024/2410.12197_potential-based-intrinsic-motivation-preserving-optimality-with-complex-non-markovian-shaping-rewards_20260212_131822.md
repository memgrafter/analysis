---
ver: rpa2
title: 'Potential-Based Intrinsic Motivation: Preserving Optimality With Complex,
  Non-Markovian Shaping Rewards'
arxiv_id: '2410.12197'
source_url: https://arxiv.org/abs/2410.12197
tags:
- reward
- shaping
- optimal
- equation
- intrinsic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper extends potential-based reward shaping (PBRS) to arbitrary\
  \ reward functions, addressing the problem of intrinsic motivation (IM) reward shaping\
  \ altering optimal policies in reinforcement learning. The authors prove that adding\
  \ a shaping reward of the form Ft = \u03B3\u03A6t+1 \u2212 \u03A6t preserves optimal\
  \ policies under a more general set of functions than previously shown, and derive\
  \ a boundary condition for this preservation."
---

# Potential-Based Intrinsic Motivation: Preserving Optimality With Complex, Non-Markovian Shaping Rewards

## Quick Facts
- arXiv ID: 2410.12197
- Source URL: https://arxiv.org/abs/2410.12197
- Reference count: 13
- Key outcome: Extends potential-based reward shaping to arbitrary reward functions while preserving optimal policies, introducing PBIM and GRM methods that outperform baseline IM approaches in MiniGrid environments

## Executive Summary
This paper addresses a fundamental challenge in reinforcement learning where intrinsic motivation (IM) rewards can alter optimal policies. The authors extend potential-based reward shaping (PBRS) beyond its traditional Markovian constraints to handle arbitrary reward functions. They introduce two methods - Potential-Based Intrinsic Motivation (PBIM) and Generalized Reward Matching (GRM) - that convert complex IM rewards into potential-based forms while provably preserving optimal policies. The theoretical framework provides a more general boundary condition for optimality preservation than previous work.

## Method Summary
The paper presents two complementary approaches to preserve optimality when applying intrinsic motivation rewards. PBIM implements potential-based shaping by accounting for total discounted rewards at episode boundaries, effectively creating a potential function that captures cumulative intrinsic rewards. GRM uses a matching function that pairs rewards received at different time steps to construct an equivalent potential-based shaping reward. Both methods build on a generalized theorem proving that optimal policies are preserved under a broader class of potential functions than previously established, removing the requirement for Markovian state representations.

## Key Results
- GRM with D=10 consistently outperformed baseline IM methods in MiniGrid DoorKey and Cliff Walking environments
- Both PBIM and GRM successfully prevented convergence to suboptimal policies while potentially speeding up training
- In long Cliff Walking environments, PBIM and GRM with D=10 preserved optimal policies while other methods were distracted by intrinsic rewards
- The methods showed modest but consistent performance improvements across multiple hyperparameter settings

## Why This Works (Mechanism)
The key insight is that by converting arbitrary intrinsic motivation rewards into potential-based forms, the shaping rewards can guide exploration without changing the underlying optimal policy structure. The potential function effectively encodes the long-term value of states relative to the intrinsic reward structure, allowing the agent to learn faster while still converging to the same optimal behavior as the original reward function.

## Foundational Learning
- **Potential-based reward shaping**: A technique that modifies rewards to speed up learning while preserving optimal policies; needed to understand how to safely incorporate intrinsic motivation without breaking optimality guarantees
- **Markov property**: The assumption that state transitions depend only on current state and action; required to understand the limitations of traditional PBRS and why the generalization matters
- **Intrinsic motivation**: Rewards generated from agent's curiosity or exploration drive; important context for why complex, non-Markovian shaping rewards are needed in the first place
- **Discounted cumulative rewards**: The total reward accounting for future values; essential for understanding how PBIM handles episode boundaries and long-term reward structures
- **Optimal policy preservation**: The property that learning with shaped rewards converges to the same policy as the original rewards; critical for evaluating whether shaping methods are theoretically sound

## Architecture Onboarding
- **Component map**: IM reward generator -> Shaping function (PBIM/GRM) -> Potential-based reward -> Standard RL algorithm -> Policy
- **Critical path**: IM reward calculation → Potential function computation → Reward shaping → Value function update → Policy improvement
- **Design tradeoffs**: PBIM requires episode boundary knowledge but handles arbitrary IM rewards; GRM is more general but needs careful matching function selection
- **Failure signatures**: Suboptimal policy convergence indicates boundary condition violation; unstable learning suggests incorrect potential function calculation
- **First experiments**: 1) Verify PBIM preserves optimality in simple episodic tasks with known optimal policies; 2) Test GRM sensitivity to matching function parameter D; 3) Compare convergence rates against standard IM methods in MiniGrid environments

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical proofs rely on idealized assumptions about Markovian state representations that may not hold in practice
- Empirical evaluation limited to simple gridworld environments with small state spaces
- Performance gains are modest and require careful hyperparameter tuning, particularly for the matching function D in GRM
- Scalability to continuous control tasks with function approximation remains untested

## Confidence
- **High**: Mathematical proofs for generalized boundary condition appear sound within stated assumptions
- **Medium**: Practical implementation described is feasible but real-world performance may vary
- **Medium**: Empirical results demonstrate methods work as intended but generalization to complex domains uncertain

## Next Checks
1. Test PBIM and GRM in continuous control environments (MuJoCo or PyBullet) to assess scalability with function approximation
2. Evaluate sensitivity of GRM's matching function D across wider range of values and different IM reward types
3. Implement ablation studies comparing convergence rates and final performance against state-of-the-art reward shaping methods in benchmark RL suites