---
ver: rpa2
title: 'ADESSE: Advice Explanations in Complex Repeated Decision-Making Environments'
arxiv_id: '2405.20705'
source_url: https://arxiv.org/abs/2405.20705
tags:
- explanations
- adesse
- agent
- advice
- grid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents ADESSE, a novel approach for generating explanations
  about an intelligent agent that provides advice to a human decision-maker in complex
  repeated decision-making environments. The agent consists of two deep learning-based
  components: one for making predictions about the future, and the other for computing
  advised actions with deep reinforcement learning based on the predicted future and
  the current state.'
---

# ADESSE: Advice Explanations in Complex Repeated Decision-Making Environments

## Quick Facts
- arXiv ID: 2405.20705
- Source URL: https://arxiv.org/abs/2405.20705
- Reference count: 40
- One-line primary result: ADESSE generates smaller explanations using less time compared to baseline method, with user study showing significantly higher satisfaction, reward, and faster decision-making

## Executive Summary
ADESSE is a novel approach for generating explanations about an intelligent agent that provides advice to human decision-makers in complex repeated decision-making environments. The agent uses a two-component structure: a prediction component based on deep learning for forecasting future states, and a DRL component for computing advised actions. ADESSE leverages this structure to create explanations that integrate visual and textual information, improving human trust and decision-making. Computational experiments and an interactive user study demonstrate ADESSE's effectiveness in generating more efficient and informative explanations compared to baseline methods.

## Method Summary
ADESSE generates explanations by analyzing both the prediction and DRL components of an advice-giving agent. It uses SHAP to select top-ranked features for predictions, domain-specific index functions to summarize DRL inputs, and visualizes the trained DRL policy across the entire state space. The approach aims to provide comprehensive yet concise explanations that help humans understand both the agent's predictions and the reasoning behind its action recommendations, reducing cognitive load while maintaining informativeness.

## Key Results
- ADESSE generates smaller explanations using less time compared to LIME baseline method
- User study participants achieved higher average rewards (98.18 vs 90.18) with ADESSE explanations
- Participants were significantly more satisfied and made decisions faster with ADESSE explanations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** ADESSE improves human trust and decision-making by generating explanations that integrate both visual and textual information about the agent's two-component structure (prediction and DRL).
- **Mechanism:** The agent's two-component structure (prediction component for forecasting future states, DRL component for computing advised actions) is leveraged to create explanations that address both the "why" (prediction) and "how" (action selection) of the agent's advice. This dual-component approach allows ADESSE to provide more comprehensive explanations than black-box methods that treat the agent as a single unit.
- **Core assumption:** Human decision-makers benefit from understanding both the agent's predictions and the reasoning behind its action recommendations.
- **Evidence anchors:**
  - [abstract]: "ADESSE leverages the agent's two-component structure and generates explanations with visual and textual information, to improve the human's trust in the agent and thus better assist human decision-making."
  - [section 4.4]: "An explanation generated by ADESSE consists of three key elements: (1) a short list of top-ranked input features that contribute the most to the agent's prediction; (2) a heatmap visualizing domain-specific indices summarizing the DRL input features; and (3) arrows in various shades of gray overlaying the heatmap to illustrate a trained DRL policy with state importance."
- **Break condition:** If human decision-makers do not require understanding of the prediction component or find the visual elements overwhelming, the dual-component approach may not provide additional value over simpler explanations.

### Mechanism 2
- **Claim:** ADESSE reduces explanation complexity and cognitive load by selecting top-ranked features and using domain-specific indices rather than showing all input features.
- **Mechanism:** Instead of presenting all input features (which can be overwhelming), ADESSE uses SHAP to identify the most important features for predictions and domain-specific index functions to summarize DRL inputs. This reduces the explanation size while maintaining informativeness.
- **Core assumption:** Humans can make better decisions with fewer, more relevant features rather than comprehensive but overwhelming feature sets.
- **Evidence anchors:**
  - [section 4.1]: "To reduce the explanation size, we focus on selecting a short list of top-ranked features for an individual prediction output at a time."
  - [section 4.2]: "To explain the input of the agent's DRL component, we summarize the DRL input features using a domain-specific index function, rather than showing multiple saliency maps (i.e., one for each DRL input feature) as in baseline explanations."
  - [section 6.2]: "One of the reasons that participants were more satisfied with explanations generated by ADESSE, as indicated by the higher ratings on the explanation satisfaction scale, could due to the fact that explanations generated by ADESSE are more succinct and informative than baseline explanations."
- **Break condition:** If the feature selection process consistently misses important features or the domain-specific indices fail to capture relevant information, the reduced complexity could lead to incomplete or misleading explanations.

### Mechanism 3
- **Claim:** ADESSE improves decision-making efficiency by visualizing the trained DRL policy across the entire grid world rather than just the current state.
- **Mechanism:** By displaying arrows representing the optimal action in every grid cell with shades of gray indicating state importance, ADESSE provides a policy-level explanation that helps humans understand the agent's behavior patterns and anticipate future recommendations.
- **Core assumption:** Understanding the agent's policy across different states helps humans make better decisions than knowing only the current recommendation.
- **Evidence anchors:**
  - [section 4.3]: "To improve the human decision-maker's trust in the adviser agent, we visualize the trained DRL policy for the entire grid world rather than only displaying the agent's advice for the current grid cell."
  - [section 6.2]: "The participants achieved a higher average reward when being presented with explanations generated by ADESSE (M = 98.18, SD = 13.18) than baseline explanations (M = 90.18, SD = 18.13)."
- **Break condition:** If the policy visualization becomes too complex to interpret in larger state spaces or if the importance degrees don't accurately reflect the agent's actual decision-making priorities, this mechanism could confuse rather than help human decision-makers.

## Foundational Learning

- **Concept:** Shapley values and feature importance
  - **Why needed here:** ADESSE uses SHAP to compute feature contributions for selecting top-ranked features in predictions, which is fundamental to reducing explanation complexity while maintaining informativeness.
  - **Quick check question:** What property of Shapley values makes them more reliable than simpler feature importance methods like LIME for identifying top-ranked features?

- **Concept:** Deep reinforcement learning (DRL) and Q-learning
  - **Why needed here:** Understanding how DRL policies are trained and how Q-values determine action selection is essential for interpreting the arrows and importance degrees in ADESSE explanations.
  - **Quick check question:** How does the dueling DQN architecture differ from standard DQN, and why might this be relevant for computing state importance?

- **Concept:** Heatmap visualization and color encoding
  - **Why needed here:** ADESSE uses heatmaps with domain-specific indices and overlaid arrows; understanding effective color encoding is crucial for creating interpretable visualizations.
  - **Quick check question:** What considerations should be made when choosing color schemes for heatmaps to ensure accessibility for color-blind users?

## Architecture Onboarding

- **Component map:** State → Predictor → Prediction → DRL Agent → Advice → ADESSE Explainer → Explanation → Human Decision

- **Critical path:** State → Predictor → Prediction → DRL Agent → Advice → ADESSE Explainer → Explanation → Human Decision

- **Design tradeoffs:**
  - Explanation comprehensiveness vs. simplicity: ADESSE balances detailed information with cognitive load by selecting top features and using domain-specific indices
  - Computational efficiency vs. explanation quality: SHAP provides better feature importance guarantees than LIME but is slower; ADESSE uses SHAP selectively for feature selection
  - Global vs. local explanations: ADESSE provides policy-level explanations across the grid world rather than just local explanations for the current state

- **Failure signatures:**
  - If explanations consistently fail to improve human trust or decision-making, the feature selection process may be missing important features
  - If explanations take too long to generate, the SHAP computation for feature selection may be too computationally expensive
  - If humans find explanations confusing, the domain-specific indices or policy visualization may not be intuitive enough

- **First 3 experiments:**
  1. **Feature selection validation:** Compare ADESSE's top-ranked features with SHAP values for individual predictions to verify the feature selection process is working correctly
  2. **Domain index function validation:** Verify that the domain-specific index functions (ϕtaxi and ϕfire) produce sensible values across different scenarios and capture the intended relationships
  3. **Policy visualization validation:** Check that the arrows and importance degrees accurately reflect the DRL policy by comparing recommended actions with the policy's Q-values in various states

## Open Questions the Paper Calls Out

- **Question:** How do ADESSE explanations perform in environments with continuous state/action spaces, beyond the grid world setup used in the experiments?
  - **Basis in paper:** [explicit] The paper mentions this as a future direction, noting that deep learning is increasingly used for predicting blood glucose levels and computing insulin dosages via deep reinforcement learning.
  - **Why unresolved:** The current ADESSE approach relies on domain-specific indices and heatmap visualizations tailored to grid world environments. Adapting these components to continuous spaces would require new techniques for summarizing and visualizing high-dimensional state/action information.
  - **What evidence would resolve it:** Experiments applying ADESSE to a continuous control task (e.g., glucose management) and comparing its explanation effectiveness to baselines like LIME/SHAP.

- **Question:** How does the explanation size and generation time of ADESSE scale with the dimensionality of the prediction and DRL input features?
  - **Basis in paper:** [inferred] The experiments varied grid world size but not feature dimensionality. The paper states ADESSE generates smaller explanations faster than LIME, but doesn't analyze how this advantage changes with feature count.
  - **Why unresolved:** The proposed techniques (feature ranking, domain-specific indices, policy visualization) may have different scaling properties that aren't apparent from the grid size experiments.
  - **What evidence would resolve it:** Experiments varying the number of prediction and DRL input features while measuring explanation size and generation time for ADESSE vs. LIME.

- **Question:** What is the impact of different domain-specific index functions on explanation effectiveness for the DRL component?
  - **Basis in paper:** [explicit] The paper presents specific index functions for taxi (demand-supply ratio) and wildfire (fuel risk) environments but doesn't explore alternatives.
  - **Why unresolved:** The effectiveness of the index function depends on how well it captures relevant domain knowledge. Different formulations could better highlight important DRL inputs for different tasks.
  - **What evidence would resolve it:** User studies comparing explanation effectiveness using different index functions (e.g., variations of the demand-supply ratio) for the same environment.

## Limitations

- The paper demonstrates ADESSE's effectiveness only in grid-world environments (taxi and wildfire), limiting generalizability to other domains
- Computational efficiency claims are based on comparison with a single baseline (LIME), which may not represent the state-of-the-art in XAI methods
- The user study sample size (52 participants) may limit statistical power and the robustness of the findings

## Confidence

- Mechanism 1 (Dual-component structure): High - Well-supported by evidence from both computational experiments and user study
- Mechanism 2 (Feature selection and domain indices): Medium - Computational evidence is strong, but user study only indirectly supports this mechanism
- Mechanism 3 (Policy visualization): Medium - Supported by user study reward improvements, but the specific contribution of policy visualization versus other ADESSE components is unclear

## Next Checks

1. Test ADESSE in additional domains beyond grid-worlds (e.g., continuous state spaces or real-world decision-making tasks) to assess generalizability
2. Conduct ablation studies to isolate the contribution of each ADESSE component (feature selection, domain indices, policy visualization) to user outcomes
3. Measure the actual cognitive load imposed by ADESSE explanations using standardized scales to validate the claim that they reduce cognitive burden compared to baseline explanations