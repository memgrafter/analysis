---
ver: rpa2
title: 'LLMPhy: Complex Physical Reasoning Using Large Language Models and World Models'
arxiv_id: '2411.08027'
source_url: https://arxiv.org/abs/2411.08027
tags:
- object
- parameters
- physical
- glass
- physics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLMPhy introduces a zero-shot black-box optimization framework
  that synergizes large language models with physics engines for complex physical
  reasoning tasks. By combining LLM-based program synthesis with non-differentiable
  simulators, it iteratively estimates physical parameters and scene layouts without
  requiring gradient information.
---

# LLMPhy: Complex Physical Reasoning Using Large Language Models and World Models

## Quick Facts
- arXiv ID: 2411.08027
- Source URL: https://arxiv.org/abs/2411.08027
- Reference count: 40
- Zero-shot black-box optimization achieves 62% accuracy on TraySim dataset, outperforming Bayesian optimization and CMA-ES

## Executive Summary
LLMPhy introduces a novel framework that combines large language models with physics engines for complex physical reasoning tasks. The approach uses an iterative feedback loop where an LLM generates code to estimate physical parameters and scene layouts, which are then evaluated by a non-differentiable simulator. The error feedback is used to refine subsequent estimates, enabling zero-shot optimization without gradient information. Experiments on the TraySim dataset demonstrate superior performance in both continuous and discrete optimization spaces, with the framework excelling at predicting object stability after tray impacts.

## Method Summary
LLMPhy employs a two-phase approach to physical reasoning: Phase 1 estimates physical parameters (friction, damping, stiffness, inertia) by matching trajectories between predicted and actual object movements, while Phase 2 estimates scene layouts by reconstructing object positions, types, and colors. The framework uses iterative LLM-simulator feedback loops where the LLM generates executable code with parameter estimates, the simulator executes and returns trajectory errors, and these errors are fed back as prompts to refine subsequent estimates. The method operates in a zero-shot manner without requiring gradient information from the non-differentiable physics engine.

## Key Results
- Achieves 62% accuracy on TraySim dataset, significantly outperforming Bayesian optimization and CMA-ES baselines
- Demonstrates superior physical parameter estimation with faster convergence than traditional black-box optimization methods
- Excels in both continuous optimization (physical parameters) and discrete optimization (scene layout reconstruction)

## Why This Works (Mechanism)

### Mechanism 1
The iterative feedback loop between LLM and simulator enables zero-shot optimization of physical parameters without requiring gradient information. The LLM generates code with estimated physical parameters, the simulator executes and returns trajectory error, and the LLM uses this error as prompt to refine next parameter estimate. This relies on the LLM's ability to leverage world knowledge to sample reasonable physical parameters and learn from error feedback across iterations. Break condition occurs when LLM fails to improve parameter estimates or simulator error plateaus.

### Mechanism 2
Separating physics parameter estimation from scene layout reconstruction enables efficient optimization in both continuous and discrete spaces. Phase 1 optimizes continuous physical parameters while Phase 2 optimizes discrete layout parameters using different LLMs or prompts. This assumes physical parameters and scene layout can be estimated independently without significant interference. Break condition occurs when scene layout reconstruction fails due to incorrect physical parameters or vice versa.

### Mechanism 3
Program synthesis capability of LLMs bridges symbolic reasoning with physics engine execution. The LLM translates reasoning tasks into executable code that interfaces with physics engine APIs, abstracting away simulation complexity while maintaining physical reasoning. This relies on LLMs possessing sufficient program synthesis skills to generate valid physics engine code given appropriate prompts and examples. Break condition occurs when LLM generates invalid or non-functional code.

## Foundational Learning

- Concept: Black-box optimization without gradients
  - Why needed here: The physics engine is non-differentiable, requiring sampling-based optimization methods
  - Quick check question: Can you explain how Bayesian optimization differs from gradient-based optimization in terms of information requirements?

- Concept: Physics engine simulation of multi-body dynamics
  - Why needed here: The simulator computes realistic object interactions and trajectories based on physical parameters
  - Quick check question: What physical quantities are typically needed to simulate rigid body dynamics in a physics engine?

- Concept: Program synthesis for API interaction
  - Why needed here: LLM must generate valid code that calls physics engine APIs with correct parameters
  - Quick check question: How would you structure a prompt to guide LLM toward generating code that follows a specific API structure?

## Architecture Onboarding

- Component map:
  - LLM: Program synthesis and parameter estimation
  - Physics Engine: Trajectory computation and simulation
  - Prompt System: Error feedback and iteration control
  - Dataset: TraySim with task and auxiliary sequences

- Critical path:
  1. Extract trajectories from auxiliary sequence
  2. LLM generates initial parameter estimates
  3. Physics engine simulates trajectories
  4. Compute error and feed back to LLM
  5. Iterate until convergence
  6. Use estimated parameters for scene reconstruction
  7. Simulate final scene and predict stability

- Design tradeoffs:
  - Single vs multiple LLMs for different phases
  - Number of optimization iterations vs computation cost
  - Granularity of physical parameter adjustments
  - Error metric selection for feedback

- Failure signatures:
  - Trajectory error plateaus above threshold
  - LLM generates invalid code repeatedly
  - Physical parameters diverge from reasonable ranges
  - Scene reconstruction fails to match input images

- First 3 experiments:
  1. Test LLM code generation with simple physics parameters
  2. Verify trajectory error computation and feedback loop
  3. Validate scene reconstruction with ground truth parameters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would LLMPhy's performance scale to more complex physics parameters beyond the four attributes tested (friction, damping, stiffness, armature)?
- Basis in paper: The paper mentions using only four physical attributes for object classes and suggests testing scalability with additional object classes, but doesn't provide results for more complex physics models.
- Why unresolved: The current experiments use a simplified MSD model with limited parameters, leaving open how well the approach generalizes to more complex physical systems with additional degrees of freedom or parameters.
- What evidence would resolve it: Testing LLMPhy on simulations with more complex physics models (e.g., including rotational friction, elasticity, or non-linear damping) and comparing performance to traditional optimization methods.

### Open Question 2
- Question: What is the minimum number of in-context examples required for LLMPhy to achieve optimal performance?
- Basis in paper: The paper states that in-context examples are "essential for the LLM to restrict its generative skills" but doesn't provide an ablation study on the number or quality of examples needed.
- Why unresolved: The paper uses a fixed number of examples without exploring how performance varies with different quantities or qualities of examples, leaving uncertainty about the approach's data efficiency.
- What evidence would resolve it: Systematic experiments varying the number and quality of in-context examples while measuring performance, convergence speed, and robustness.

### Open Question 3
- Question: How does LLMPhy's black-box optimization compare to gradient-based methods when differentiable simulators are available?
- Basis in paper: The paper emphasizes that LLMPhy works with non-differentiable simulators and compares only to other black-box methods, but doesn't explore performance when gradient information is accessible.
- Why unresolved: The paper's focus on black-box optimization leaves open the question of whether the LLM-based approach provides advantages even when more efficient gradient-based optimization is possible.
- What evidence would resolve it: Direct comparison between LLMPhy and gradient-based optimization methods on differentiable physics engines, measuring both performance and computational efficiency.

## Limitations
- Performance claims rely heavily on the TraySim dataset, which may not capture real-world physical reasoning complexity
- Zero-shot nature means generalization to different physics engines or object types remains untested
- Computational cost of iterative optimization process is not thoroughly analyzed, potentially limiting practical deployment

## Confidence

**High Confidence**: The core mechanism of LLM-simulator feedback loop for parameter estimation is well-supported by experimental results and aligns with established program synthesis principles.

**Medium Confidence**: The separation of continuous and discrete optimization phases is reasonable but lacks extensive ablation studies to prove its necessity over joint optimization approaches.

**Low Confidence**: The claim of superior performance across all optimization scenarios (continuous and discrete) requires further validation on more diverse datasets beyond TraySim.

## Next Checks

1. Test LLMPhy on a different physics engine (e.g., Bullet or PyBullet) to verify engine-agnostic performance and identify potential API-specific dependencies.

2. Conduct ablation studies comparing single-phase vs two-phase optimization approaches to quantify the benefits of separating physical parameter and scene layout estimation.

3. Evaluate the computational cost per iteration and total optimization time across different problem complexities to establish practical deployment boundaries.