---
ver: rpa2
title: Confidence-aware Denoised Fine-tuning of Off-the-shelf Models for Certified
  Robustness
arxiv_id: '2411.08933'
source_url: https://arxiv.org/abs/2411.08933
tags:
- denoised
- images
- robustness
- certified
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes FT-CADIS, a fine-tuning scheme to enhance certified
  robustness of off-the-shelf classifiers within the denoised smoothing framework.
  The key insight is that the confidence of off-the-shelf classifiers can effectively
  identify hallucinated images created by the denoising process.
---

# Confidence-aware Denoised Fine-tuning of Off-the-shelf Models for Certified Robustness

## Quick Facts
- arXiv ID: 2411.08933
- Source URL: https://arxiv.org/abs/2411.08933
- Authors: Suhyeok Jang; Seojin Kim; Jinwoo Shin; Jongheon Jeong
- Reference count: 40
- Key outcome: FT-CADIS improves certified accuracy from 29.5% to 39.4% at ε=2.0 on ImageNet compared to the best baseline

## Executive Summary
This paper addresses the fundamental challenge of improving certified robustness for off-the-shelf classifiers within the denoised smoothing framework. The key insight is that confidence scores from pre-trained classifiers can effectively identify hallucinated images created during the denoising process, which are harmful for training. FT-CADIS introduces two novel confidence-aware losses that are selectively applied only to non-hallucinated images, preventing over-optimization of hallucinated samples while maximizing robustness. The method achieves state-of-the-art certified robustness across all ℓ2-adversary radii on CIFAR-10 and ImageNet while being parameter-efficient, requiring updates to only 1% of classifier parameters.

## Method Summary
FT-CADIS proposes a confidence-aware fine-tuning scheme that improves certified robustness of off-the-shelf classifiers within denoised smoothing. The method uses classifier confidence scores to identify hallucinated denoised images that have lost semantic correspondence to their original class. Two selective losses are then applied: a confidence-aware selective cross-entropy loss and a confidence-aware masked adversarial loss, both targeting only non-hallucinated images. The approach uses parameter-efficient LoRA fine-tuning (updating only 1% of parameters) to adapt the classifier to the denoised image distribution without corrupting it with hallucinated samples. The training procedure involves iterative confidence-based image selection and selective loss computation.

## Key Results
- FT-CADIS establishes state-of-the-art certified robustness among denoised smoothing methods across all ℓ2-adversary radii
- Certified accuracy improves from 29.5% to 39.4% at ε=2.0 on ImageNet compared to the best baseline
- Achieves 99% parameter reduction using LoRA while maintaining similar ACR (Average Certified Radius)
- Effective across multiple noise levels (σ=0.25, 0.50, 1.00) on both CIFAR-10 and ImageNet

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Confidence of pre-trained classifiers can effectively identify hallucinated denoised images
- Mechanism: Off-the-shelf classifiers trained on clean images assign low confidence scores to hallucinated denoised images that have lost semantic correspondence to their original class. This confidence signal serves as a proxy to distinguish hallucinated from non-hallucinated images.
- Core assumption: The pre-trained classifier's confidence on denoised images correlates with semantic preservation
- Evidence anchors:
  - [abstract]: "the confidence of off-the-shelf classifiers can effectively identify hallucinated images during denoised smoothing"
  - [section]: "we use the 'likelihood of denoised images', i.e., confidence, of the off-the-shelf classifier with respect to the originally assigned class as a proxy for determining whether an image is hallucinated"
- Break condition: If the pre-trained classifier's confidence distribution shifts significantly on denoised images, the proxy becomes unreliable

### Mechanism 2
- Claim: Fine-tuning with non-hallucinated images improves certified robustness
- Mechanism: By selectively applying cross-entropy loss only to non-hallucinated images (those maintaining original semantics), the classifier learns the denoised image distribution without being confused by hallucinated samples that would otherwise corrupt the training objective.
- Core assumption: Hallucinated images are harmful for learning the correct denoised image distribution
- Evidence anchors:
  - [abstract]: "Two losses are selectively applied only to non-hallucinated images, thereby ensuring that the overall training process avoids over-optimizing hallucinated samples"
  - [section]: "fine-tuning fclf with hallucinated samples...is harmful for the generalizability since Eq. (6) forces the classifierfclf to remember non-y-like hallucinated images as y"
- Break condition: If the hallucination detection threshold is too strict, too few images remain for effective training

### Mechanism 3
- Claim: Parameter-efficient fine-tuning (LoRA) can achieve comparable results to full fine-tuning
- Mechanism: By updating only 1% of classifier parameters through LoRA, FT-CADIS maintains the learned representations of the pre-trained classifier while adapting to the denoised image distribution, achieving state-of-the-art robustness with minimal parameter updates.
- Core assumption: The bulk of useful representations are preserved in frozen parameters
- Evidence anchors:
  - [abstract]: "such a fine-tuning can be done by merely updating a small fraction (i.e., 1%) of parameters of the classifier"
  - [section]: "Extensive experiments demonstrate that FT-CADIS has established the state-of-the-art certified robustness among denoised smoothing methods across all ℓ2-adversary radius in a variety of benchmarks, such as CIFAR-10 and ImageNet"
- Break condition: If critical parameters for robustness lie outside the fine-tuned subset, performance degrades

## Foundational Learning

- Concept: Randomized smoothing and certified robustness
  - Why needed here: FT-CADIS operates within the denoised smoothing framework, which is an extension of randomized smoothing
  - Quick check question: What is the relationship between the base classifier's accuracy on Gaussian-perturbed images and the certified radius in randomized smoothing?

- Concept: Adversarial training and PGD
  - Why needed here: FT-CADIS incorporates adversarial training concepts to improve robustness, specifically using projected gradient descent to create challenging samples
  - Quick check question: How does adversarial training improve robustness in the context of randomized smoothing?

- Concept: Diffusion models and denoising
  - Why needed here: FT-CADIS uses diffusion models as denoisers in the denoised smoothing framework, so understanding how they work is crucial
  - Quick check question: What is the key difference between score-based denoising and diffusion-based denoising?

## Architecture Onboarding

- Component map:
  Pre-trained classifier -> Diffusion denoiser -> Confidence-based hallucinated image detection -> Selective loss computation -> Parameter updates (LoRA)

- Critical path:
  1. Input clean image → apply Gaussian noise
  2. Noisy image → diffusion denoiser → denoised image
  3. Denoised image → pre-trained classifier → confidence scores
  4. Confidence scores → hallucinated vs non-hallucinated classification
  5. Non-hallucinated images → selective loss computation
  6. Loss computation → parameter updates (via LoRA)

- Design tradeoffs:
  - Number of noise samples M: Higher M improves robustness but increases computation
  - Attack radius ε: Larger ε creates stronger adversarial examples but may include hallucinated images
  - LoRA rank r: Higher rank improves fine-tuning capacity but reduces parameter efficiency
  - Confidence threshold: Stricter thresholds reduce hallucination contamination but may limit training data

- Failure signatures:
  - Degraded certified accuracy at high radii suggests hallucination contamination
  - Training instability indicates overly aggressive adversarial loss application
  - Poor clean accuracy suggests loss of generalization to clean images
  - Computational bottlenecks indicate insufficient optimization of M or attack parameters

- First 3 experiments:
  1. Baseline comparison: Run FT-CADIS with λ=0 (no adversarial loss) to isolate the effect of confidence-aware selective cross-entropy
  2. Hallucination sensitivity: Vary the confidence threshold for hallucination detection and measure impact on ACR
  3. Parameter efficiency: Compare full fine-tuning vs LoRA with different ranks to find optimal parameter efficiency tradeoff

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- Limited analysis of the trade-off between hallucination detection threshold and certified robustness
- Potential generalization issues when applying to datasets with significantly different characteristics than CIFAR-10 and ImageNet
- No systematic exploration of optimal LoRA rank for balancing efficiency and robustness

## Confidence

High confidence in the core insight about using classifier confidence to detect hallucinated images
Medium confidence in the parameter efficiency claims and generalization across architectures
Low confidence in the empirical robustness gains without more comprehensive ablations

## Next Checks

1. **Ablation study**: Run FT-CADIS with confidence-aware selective loss only (λ=0) vs adversarial loss only (confidence-aware loss disabled) to quantify individual contribution to robustness gains.

2. **Hallucination analysis**: For varying confidence thresholds, compute precision/recall of hallucination detection and measure corresponding impact on certified accuracy to find optimal threshold settings.

3. **Architecture generalization**: Apply FT-CADIS to different backbone architectures (e.g., ResNet, ConvNeXt) beyond ViT-B/16 to verify parameter efficiency and robustness gains generalize beyond the specific architecture tested.