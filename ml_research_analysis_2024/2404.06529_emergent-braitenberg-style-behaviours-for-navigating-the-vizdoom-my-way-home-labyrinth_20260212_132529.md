---
ver: rpa2
title: Emergent Braitenberg-style Behaviours for Navigating the ViZDoom `My Way Home'
  Labyrinth
arxiv_id: '2404.06529'
source_url: https://arxiv.org/abs/2404.06529
tags:
- agent
- room
- state
- task
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether simple Braitenberg-style reactive
  heuristics can be evolved for navigating a visually complex, partially observable
  labyrinth (ViZDoom's 'My Way Home' task). The tangled program graphs (TPG) genetic
  programming framework is employed, constrained to arithmetic operations and sparse
  state sampling (indexing <0.8% of the state space).
---

# Emergent Braitenberg-style Behaviours for Navigating the ViZDoom `My Way Home' Labyrinth

## Quick Facts
- arXiv ID: 2404.06529
- Source URL: https://arxiv.org/abs/2404.06529
- Authors: Caleidgh Bayer; Robert J. Smith; Malcolm I. Heywood
- Reference count: 20
- One-line primary result: Evolved TPG agents discover simple reactive navigation heuristics (slow rightward arcs, wall-following, alternating arcs) that outperform standard DQN baselines in a visually complex labyrinth task.

## Executive Summary
This paper demonstrates that simple Braitenberg-style reactive navigation heuristics can emerge from evolved tangled program graphs (TPG) in a visually complex, partially observable labyrinth environment. By constraining TPG to arithmetic operations and sparse pixel indexing (<0.8% of state space), the evolutionary process is biased toward discovering low-dimensional reactive features rather than complex state processing. The evolved agents consistently use rightward arcing and wall-following behaviors to navigate effectively across the maze, achieving superior generalization compared to a standard DQN baseline that fails beyond visually obvious goals.

## Method Summary
The method employs tangled program graphs (TPG), a genetic programming framework where teams of small linear programs evolve to solve navigation tasks. Each program uses only arithmetic operations (+, -, ÷, ×) on registers and indexes a tiny fraction of the 160×120 pixel visual state. Programs are organized into ensembles with context-action learner pairs, forming a modular decision graph. Evolution occurs in two phases: initial team evolution without graph building, followed by graph construction. The approach is tested on ViZDoom's 'My Way Home' labyrinth with sparse rewards, and compared against a DQN baseline using frame stacking and experience replay.

## Key Results
- TPG agents evolve consistent navigation heuristics: slowly arc rightward from any initial state, align parallel to walls when encountered, and alternate arc direction after wall-following to reorient.
- The approach achieves effective navigation across the labyrinth, outperforming a standard DQN baseline that fails to generalize beyond visually obvious goals.
- Programs index only 36.2 ± 9.6 pixels per decision (≤ 0.8% of state space), demonstrating that useful low-dimensional features can be discovered through random pixel indexing in high-dimensional image space.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Simple reactive heuristics can solve complex visual navigation tasks when the agent is constrained to index only a tiny fraction of the state space.
- Mechanism: The tangled program graphs (TPG) representation, by using pixel indexing rather than convolutional kernels, forces programs to discover low-dimensional features that are sufficient for the task, effectively reducing the partial observability problem to manageable reactive behaviors.
- Core assumption: That useful low-dimensional features can be found by random pixel indexing in the high-dimensional image space, without explicit feature engineering.
- Evidence anchors:
  - [abstract] "tangled program graphs ... only employs 0.8% of the state space"
  - [section 4.6] "programs would on average index 36.2 ± 9.6 pixels ... ≤ 0.8% of the state space indexed per decision"
  - [corpus] Weak - no directly comparable studies found
- Break condition: If the underlying task requires global or multi-scale features that cannot be captured by a few pixel samples, the approach will fail.

### Mechanism 2
- Claim: Emergent modularity in TPG allows simple reactive programs to combine into a coherent navigation policy without memory.
- Mechanism: Multiple small programs coevolve and are selectively activated by context programs, forming a modular decision graph that switches between reactive behaviors (e.g., wall-following, arc turns) based on the current visual input.
- Core assumption: That the task's environmental structure is regular enough that a set of simple reactive behaviors can be sequenced to solve it.
- Evidence anchors:
  - [abstract] "multiple programs that typically index a fraction of the available state space"
  - [section 4.4] "TPG places less emphasis on 'seeing' the goal ... in favour of corridor following and using rooms to re-orientate the agent"
  - [corpus] Weak - no directly comparable studies found
- Break condition: If the environment has irregular structure or requires memory of past states, the emergent modular approach will not generalize.

### Mechanism 3
- Claim: Limiting the TPG instruction set to arithmetic operations biases evolution toward discovering simple Braitenberg-style reactive rules.
- Mechanism: By excluding complex operations like convolutions or indexed memory, the search space is constrained to discover policies that react directly to pixel values rather than building abstract representations.
- Core assumption: That arithmetic operations on pixel values are sufficient to encode the necessary reactive behaviors for the navigation task.
- Evidence anchors:
  - [section 3.2.1] "programs define a sequence of instructions ... a set of arithmetic operations {+. − .÷, ×}"
  - [section 4.5] "the agent has to prioritize maximizing the average reward accumulation ... as opposed to picking up on local reward accumulation"
  - [corpus] Weak - no directly comparable studies found
- Break condition: If the task requires more complex transformations of the visual input than simple arithmetic can provide, the approach will fail.

## Foundational Learning

- Concept: Genetic Programming (GP)
  - Why needed here: TPG is a variant of GP that evolves programs instead of neural network weights; understanding GP basics is essential to grasp how TPG works.
  - Quick check question: In GP, what is the primary difference between linear GP and tree-based GP?

- Concept: Partial Observability
  - Why needed here: The ViZDoom task provides only a first-person view, so the agent cannot see the entire maze at once; understanding partial observability explains why memory-based methods usually dominate this task.
  - Quick check question: How does partial observability differ from full observability in reinforcement learning?

- Concept: Emergent Behavior
  - Why needed here: The paper's central claim is that complex navigation emerges from simple reactive rules; understanding emergence helps explain how simple components can create sophisticated overall behavior.
  - Quick check question: What distinguishes emergent behavior from simply complex behavior in multi-agent systems?

## Architecture Onboarding

- Component map:
  - ViZDoom environment -> 160×120 pixel visual state -> TPG system -> Arithmetic instruction set -> Context-action learner pairs -> Ensemble graph -> Actions

- Critical path:
  1. Initialize random programs with arithmetic instructions
  2. Evaluate ensembles by selecting winning context programs and executing corresponding action programs
  3. Evolve through selection, crossover, and mutation of ensembles and programs
  4. Build graph structure by replacing action programs with ensemble pointers
  5. Identify champion by testing all root ensembles on validation states

- Design tradeoffs:
  - Simplicity vs. expressivity: limiting to arithmetic operations reduces search space but may miss complex features
  - State indexing vs. full observation: indexing <1% of pixels makes the problem tractable but risks missing crucial information
  - Memoryless vs. memory-based: avoiding memory makes solutions simpler but may limit generalization

- Failure signatures:
  - Agents that get stuck in corners or loops
  - Performance that only works from specific starting positions
  - Solutions that index very few pixels but still fail to navigate
  - TPG graphs that grow very large without improving performance

- First 3 experiments:
  1. Run TPG with full state observation (no pixel indexing) to verify if simplicity constraint is necessary
  2. Test different arithmetic instruction sets (e.g., add modulo operations) to see impact on solution complexity
  3. Compare performance when initializing with different percentages of indexed pixels (e.g., 0.1%, 0.8%, 5%)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do asymmetric room/corridor sizes affect the ability to derive Braitenberg-style navigation heuristics?
- Basis in paper: [explicit] The paper explicitly states that future work will investigate the impact of asymmetric room/corridor sizes on deriving Braitenberg-style navigation heuristics.
- Why unresolved: This question is directly identified as a future research direction by the authors, indicating it has not been experimentally tested.
- What evidence would resolve it: Experimental results comparing the effectiveness of Braitenberg-style heuristics in symmetric vs. asymmetric labyrinth layouts, including metrics like success rate, path efficiency, and heuristic simplicity.

### Open Question 2
- Question: What is the impact of non-centered entry and exit points on the navigation heuristic?
- Basis in paper: [explicit] The authors mention investigating the effect of non-centered entry and exit points on the heuristic as part of future work.
- Why unresolved: This is another future research direction explicitly stated by the authors, suggesting it has not been explored.
- What evidence would resolve it: Comparative studies of navigation performance with centered vs. non-centered entry/exit points, analyzing changes in heuristic structure and success rates.

### Open Question 3
- Question: Can TPG discover Braitenberg-style heuristics for navigation tasks with more complex visual environments?
- Basis in paper: [inferred] While the paper demonstrates TPG's success in a specific ViZDoom task, it does not explore more complex visual environments.
- Why unresolved: The study is limited to the "My Way Home" labyrinth, and there's no evidence provided for TPG's performance in more visually complex scenarios.
- What evidence would resolve it: Experimental results showing TPG's ability to evolve effective navigation heuristics in environments with increased visual complexity, such as more varied textures, dynamic obstacles, or less structured layouts.

## Limitations
- The observed behavior may represent a local optimum specific to this particular labyrinth layout rather than general Braitenberg-style navigation.
- The DQN baseline comparison lacks detailed hyperparameter matching and doesn't explore alternative architectures that might better handle partial observability.
- The approach's effectiveness in more visually complex environments remains untested, limiting claims about general applicability.

## Confidence
- High confidence: Technical implementation and reproduction details are well-specified with clear constraints
- Medium confidence: Claims about emergent reactive heuristics are supported but generality across layouts untested
- Low confidence: Comparative advantage over DQN may be overstated due to baseline architecture limitations

## Next Checks
1. Test the evolved TPG agents on procedurally generated mazes with different layouts to verify if the Braitenberg-style behaviors generalize beyond the specific 'My Way Home' configuration.

2. Implement and compare against a memory-based TPG variant (allowing indexed memory operations) to determine if the memoryless constraint is truly beneficial or simply limiting.

3. Conduct ablation studies varying the pixel indexing percentage (0.1%, 0.8%, 5%) to quantify the tradeoff between state sampling and navigation performance, determining if 0.8% is optimal or merely sufficient.