---
ver: rpa2
title: Holistic Adversarially Robust Pruning
arxiv_id: '2412.14714'
source_url: https://arxiv.org/abs/2412.14714
tags:
- pruning
- compression
- sparsity
- harp
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a holistic method for adversarially robust pruning
  of neural networks that learns both how many parameters and which parameters to
  prune for each layer individually. The method, HARP, uses dynamic regularization
  and a step-wise incremental function to balance natural accuracy, adversarial robustness,
  and model compactness during fine-tuning.
---

# Holistic Adversarially Robust Pruning

## Quick Facts
- arXiv ID: 2412.14714
- Source URL: https://arxiv.org/abs/2412.14714
- Reference count: 40
- Primary result: Achieves 99.9% sparsity while preserving natural accuracy and adversarial robustness through non-uniform per-layer compression

## Executive Summary
This paper introduces HARP, a holistic method for adversarially robust pruning that learns both how many parameters and which parameters to prune for each layer individually. Unlike previous approaches that apply uniform compression across all layers, HARP uses dynamic regularization and a step-wise incremental function to balance natural accuracy, adversarial robustness, and model compactness during fine-tuning. The method outperforms prior approaches in maintaining both accuracy and robustness at extreme compression rates, demonstrating that non-uniform compression strategies are crucial for preserving adversarial robustness during aggressive pruning.

## Method Summary
HARP implements a three-stage pipeline: pre-training with adversarial training, pruning with dynamic regularization balancing robustness and compression objectives, and fine-tuning. The method learns per-layer compression rates through trainable parameters r(l) that are mapped to compression rates a(l) in the range [amin, 1]. During pruning, a step-wise incremental function adapts the regularization parameter γ to first favor robustness, then shift to compression, and finally balance both equally once target compression is reached. The approach uses straight-through estimation to enable gradient flow through the non-differentiable pruning mask operation, allowing end-to-end optimization of pruning decisions.

## Key Results
- Achieves 99.9% sparsity while maintaining both natural accuracy and adversarial robustness
- Non-uniform compression strategy outperforms uniform approaches by 1.5% to 4.5% on natural accuracy
- Reduces model size by up to 2000x compared to adversarially pre-trained models
- Outperforms state-of-the-art pruning methods across all evaluated metrics and datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning compression quotas per layer (r(l)) allows the method to adapt sparsity non-uniformly, preserving critical parameters in some layers while aggressively pruning others.
- Mechanism: The trainable parameter r(l) is mapped through an activation function g(r) to compression rates a(l) in the range [amin, 1], enabling the optimizer to search for the best per-layer sparsity configuration.
- Core assumption: Different layers have different sensitivities to pruning, and allowing per-layer rates will improve both accuracy and robustness versus uniform compression.
- Evidence anchors:
  - [abstract] "learn a global compression strategy that optimizes how many parameters (compression rate) and which parameters (scoring connections) to prune specific to each layer individually"
  - [section 3] "We measure the number of currently preserved (i.e., non-zero) weights across all L layers of the network... We thus limit the layers’ compression a(l) to [amin, 1]"
- Break condition: If g(r) is not properly constrained, layers may be removed entirely or sparsity may not reach the target.

### Mechanism 2
- Claim: Dynamic regularization with a step-wise incremental γ balances robustness and compression objectives during fine-tuning.
- Mechanism: Early epochs emphasize robustness (low γ), then shift to compression (high γ), and finally balance both equally once the target compression is reached.
- Core assumption: Robustness and compression are conflicting objectives, so prioritizing them in sequence leads to better final performance than balancing them from the start.
- Evidence anchors:
  - [abstract] "fine-tunes an existing model with dynamic regularization, that follows a step-wise incremental function balancing the different objectives. It starts by favoring robustness before shifting focus on reaching the target compression rate and only then handles the objectives equally"
  - [section 3] "we employ a step-wise incremental function to adapt the regularization parameter γ throughout the learning process"
- Break condition: If γ step sizes are too large or too small, the method may either oscillate or fail to reach the target compression.

### Mechanism 3
- Claim: Using straight-through estimation (STE) allows gradients to flow through the non-differentiable pruning mask operation.
- Mechanism: During backpropagation, gradients are assigned directly to importance scores S(l) and compression quotas r(l), bypassing the binarization step.
- Core assumption: STE provides a good approximation of the true gradient for mask learning, enabling end-to-end optimization of pruning decisions.
- Evidence anchors:
  - [section 3] "We, thus, follow the 'straight through estimation' (STE) strategy (Hubara et al., 2016) as proposed by Kusupati et al. (2020) to assign the updated gradients to the importance scores S(l) directly"
- Break condition: If STE introduces significant bias, learned pruning masks may be suboptimal, harming accuracy or robustness.

## Foundational Learning

- Concept: Adversarial training and its impact on model robustness
  - Why needed here: HARP builds on adversarially pre-trained models and evaluates robustness under multiple attack types; understanding this foundation is key to interpreting pruning effects.
  - Quick check question: What is the difference between PGD-AT and TRADES-AT in terms of how they optimize the trade-off between natural accuracy and adversarial robustness?

- Concept: Neural network pruning and structured vs unstructured approaches
  - Why needed here: The paper compares weight pruning (unstructured) and channel pruning (structured); knowing the trade-offs informs why HARP focuses on weights initially.
  - Quick check question: How does unstructured weight pruning differ from structured channel pruning in terms of hardware efficiency and accuracy trade-offs?

- Concept: Compression rate and sparsity definitions
  - Why needed here: The paper uses compression rate a (fraction preserved) while sparsity is defined inversely; mixing these up would lead to incorrect interpretation of results.
  - Quick check question: If a network has 99.9% sparsity, what is its compression rate a?

## Architecture Onboarding

- Component map:
  Pre-trained model parameters θ -> Trainable compression quotas r(l) and importance scores S(l) -> Pruning mask M -> Pruned model θ ⊙ M

- Critical path:
  1. Initialize r(l) and S(l) per layer
  2. Alternate between generating adversarial examples and updating r(l), S(l) via STE
  3. Increment γ over epochs to shift focus from robustness to compression
  4. Fine-tune pruned model to recover performance

- Design tradeoffs:
  - Uniform vs non-uniform compression: Uniform is simpler but harms performance; non-uniform requires per-layer optimization but yields better results.
  - Weight vs channel pruning: Weight pruning offers higher sparsity but less hardware efficiency; channel pruning is more hardware-friendly but harder to preserve robustness.

- Failure signatures:
  - If γ schedule is incorrect, the model may not reach target compression or may sacrifice too much robustness.
  - If STE approximation is poor, importance scores may not converge, leading to random or suboptimal masks.
  - If initialization of S(l) is too aggressive, early pruning may destroy critical features.

- First 3 experiments:
  1. Run HARP on a small VGG16 with CIFAR-10 at 99% sparsity, track natural accuracy and PGD-10 robustness across epochs to see if the γ schedule works.
  2. Compare HARP with a baseline uniform pruning strategy on the same setup to quantify the benefit of non-uniform compression.
  3. Evaluate the effect of different γ step sizes (e.g., 0.001, 0.01, 0.1) on convergence and final performance.

## Open Questions the Paper Calls Out

- Does the effectiveness of HARP's non-uniform compression strategy generalize across different neural network architectures beyond VGG16 and ResNet18?
- What is the theoretical foundation for why non-uniform compression strategies preserve adversarial robustness better than uniform strategies?
- How does HARP's performance scale with increasingly larger models and datasets beyond ImageNet?
- What is the relationship between the step size parameter γ and the trade-off between natural accuracy and adversarial robustness during the pruning process?

## Limitations

- Limited architectural exploration beyond VGG16 and ResNet18, leaving uncertainty about generalizability to other network architectures
- γ schedule parameters appear tuned to specific datasets without systematic analysis of hyperparameter sensitivity
- Relies on STE approximation without theoretical analysis of approximation error or empirical validation of gradient quality
- Does not address how HARP performs when pruning models trained with different adversarial methods than those used during pruning

## Confidence

- Non-uniform compression superiority: High - The ablation study clearly demonstrates that allowing per-layer compression rates outperforms uniform strategies across all metrics.
- Dynamic regularization effectiveness: Medium - While results support the approach, the specific γ schedule parameters are not fully justified and may be dataset/model-specific.
- STE approximation validity: Medium-Low - The paper relies on STE for end-to-end optimization but does not provide theoretical analysis of approximation error or empirical validation of gradient quality.

## Next Checks

1. Apply HARP to models trained with adversarial methods not used during pruning (e.g., prune a MART-trained model using PGD-AT) to test robustness transfer across training paradigms.
2. Conduct ablation studies varying the γ schedule parameters (step sizes, epochs) across different dataset sizes to identify optimal configurations.
3. Analyze layer-wise sensitivity to pruning by systematically testing HARP with constrained per-layer sparsity bounds to quantify each layer's contribution to overall robustness.