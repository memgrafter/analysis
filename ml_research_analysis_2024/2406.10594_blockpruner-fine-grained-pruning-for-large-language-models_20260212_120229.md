---
ver: rpa2
title: 'BlockPruner: Fine-grained Pruning for Large Language Models'
arxiv_id: '2406.10594'
source_url: https://arxiv.org/abs/2406.10594
tags:
- pruning
- uni00000013
- uni00000018
- block
- uni00000015
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BlockPruner addresses the problem of inefficient compression of
  large language models by focusing on fine-grained redundancies within transformer
  layers. It segments each layer into multi-head attention (MHA) and multi-layer perceptron
  (MLP) blocks, evaluates their importance using perplexity, and applies iterative
  heuristic pruning to remove less critical blocks.
---

# BlockPruner: Fine-grained Pruning for Large Language Models

## Quick Facts
- arXiv ID: 2406.10594
- Source URL: https://arxiv.org/abs/2406.10594
- Reference count: 11
- Primary result: BlockPruner achieves better downstream task performance and lower perplexity compared to state-of-the-art baselines while allowing for higher pruning ratios.

## Executive Summary
BlockPruner introduces a fine-grained approach to compressing large language models by targeting redundancies within transformer layers. Unlike previous methods that prune entire layers, BlockPruner segments each layer into multi-head attention (MHA) and multi-layer perceptron (MLP) blocks, evaluates their importance using perplexity, and applies iterative heuristic pruning to remove less critical blocks. The method demonstrates superior performance compared to state-of-the-art baselines across multiple model sizes and architectures, achieving higher pruning ratios while maintaining better downstream task performance and lower perplexity.

## Method Summary
BlockPruner addresses LLM compression by segmenting each Transformer layer into MHA and MLP blocks, then iteratively pruning the least important blocks based on perplexity measurements. The method uses a small calibration dataset (256 samples from Alpaca) to compute block importance scores by masking each block and measuring the resulting perplexity increase. The block with the smallest impact on perplexity is removed iteratively until the target pruning ratio is reached. The approach is evaluated on Llama2, Baichuan2, and Qwen1.5 model families (7B and 13B/14B variants) using zero-shot evaluation on five benchmarks (PIQA, WinoGrande, HellaSwag, ARC-e, ARC-c) and Wikitext2 for perplexity measurement.

## Key Results
- BlockPruner achieves lower perplexity than layer-level pruning methods at equivalent sparsity levels
- MHA blocks show higher redundancy than MLP blocks, enabling more aggressive pruning with less performance loss
- BlockPruner consistently outperforms state-of-the-art baselines (SliceGPT, LaCo, ShortGPT, Relative Magnitude) across all evaluated metrics and model sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-grained block pruning outperforms layer-level pruning by targeting redundancies within individual MHA and MLP sublayers.
- Mechanism: The model is segmented into MHA and MLP blocks per layer; each block's importance is evaluated using perplexity on a small calibration dataset, then the least important blocks are iteratively removed.
- Core assumption: Redundancy in LLMs exists at the block level, not just the layer level, and perplexity accurately reflects block importance.
- Evidence anchors:
  - [abstract]: "finer-grained pruning can be achieved by targeting redundancies in multi-head attention (MHA) and multi-layer perceptron (MLP) blocks"
  - [section]: "Building on these findings, we argue that finer-grained pruning can be effectively implemented in LLMs"
  - [corpus]: Weak; related work focuses on layer-wise pruning without fine-grained block metrics.
- Break condition: If perplexity fails to capture block importance (e.g., due to calibration data mismatch), iterative pruning may remove critical blocks.

### Mechanism 2
- Claim: Iterative pruning using perplexity better preserves model performance than one-shot pruning with local metrics.
- Mechanism: At each iteration, the block whose removal causes the smallest perplexity increase is removed, progressively refining the model while minimizing degradation.
- Core assumption: Perplexity reflects the global impact of block removal on model fluency and coherence.
- Evidence anchors:
  - [abstract]: "assesses the importance of these blocks using perplexity measures and applies a heuristic search for iterative pruning"
  - [section]: "we determine the importance score of each block by masking it and then computing the perplexity"
  - [corpus]: Weak; most pruning work uses local importance metrics, not global perplexity-based iterative removal.
- Break condition: If iterative pruning accumulates errors or removes too many blocks too quickly, performance may collapse.

### Mechanism 3
- Claim: MHA blocks are more redundant than MLP blocks, enabling more aggressive pruning of MHA for inference speed gains.
- Mechanism: Experiments show that early pruning stages remove mostly MHA blocks; their removal reduces KV cache size, speeding up inference.
- Core assumption: MHA modules contain higher redundancy than MLP modules in LLMs.
- Evidence anchors:
  - [section]: "pruning only the MHA blocks results in less performance loss compared to pruning MLP blocks"
  - [section]: "pruning at the layer level rather than the block level yields less robust performance"
  - [corpus]: Weak; redundancy differences between MHA and MLP are not well explored in prior literature.
- Break condition: If MHA pruning removes critical heads, model performance will sharply decline after a threshold.

## Foundational Learning

- Concept: Transformer architecture and residual blocks
  - Why needed here: Understanding how MHA and MLP form minimal residual blocks is key to segmenting layers for fine-grained pruning.
  - Quick check question: What is the residual connection in a Transformer layer and how do MHA and MLP form two distinct residual blocks?

- Concept: Perplexity as a language model quality metric
  - Why needed here: Perplexity is used to measure block importance; knowing its definition and role is critical for interpreting pruning results.
  - Quick check question: How is perplexity calculated for a sequence and why does it reflect model fluency?

- Concept: Structured pruning vs. unstructured pruning
  - Why needed here: BlockPruner is a structured pruning method; understanding the difference informs why block pruning is preferred over weight-level pruning.
  - Quick check question: What is the difference between structured and unstructured pruning and why does structured pruning require less hardware support?

## Architecture Onboarding

- Component map:
  Input embedding layer → stacked Transformer layers (each = MHA block + MLP block) → output projection layer
  BlockPruner operates on MHA and MLP blocks within each Transformer layer
  Uses perplexity computed on calibration dataset to score block importance

- Critical path:
  1. Split each Transformer layer into MHA and MLP blocks
  2. For each block, mask it, compute perplexity on calibration data
  3. Iteratively remove block with lowest importance until target ratio reached
  4. Validate performance on downstream benchmarks

- Design tradeoffs:
  - Fine-grained pruning vs. layer pruning: More pruning flexibility but higher computational cost for importance scoring
  - Perplexity vs. local metrics: Better global measure but requires full model inference for scoring
  - MHA vs. MLP pruning ratio: More MHA removal speeds up inference but risks losing attention capacity

- Failure signatures:
  - Sudden performance drop during pruning → over-pruned critical blocks
  - High perplexity after pruning → calibration data mismatch or too aggressive pruning
  - Minimal speedup despite high pruning ratio → pruning mostly MLP blocks (less impact on KV cache)

- First 3 experiments:
  1. Run BlockPruner on a small model (e.g., Llama2-7B) with 10% pruning; compare downstream task performance to baselines
  2. Vary the calibration dataset size (e.g., 64 vs. 256 samples) to observe impact on pruning quality
  3. Isolate MHA vs. MLP pruning to confirm differential redundancy and effect on inference speed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does BlockPruner's performance scale with increasingly larger language models (e.g., 30B+ parameters)?
- Basis in paper: [inferred] The paper mentions a limitation that due to computational resource constraints, they did not apply BlockPruner to prune larger models. However, they state that their approach is highly scalable and readily adaptable for pruning larger models in future research.
- Why unresolved: The authors explicitly acknowledge they haven't tested BlockPruner on models larger than 14B parameters, despite suggesting it should scale well.
- What evidence would resolve it: Experiments applying BlockPruner to models with 30B+ parameters showing whether the method maintains its effectiveness at larger scales.

### Open Question 2
- Question: Would incorporating other combinatorial optimization algorithms improve the pruning sequence identified by BlockPruner's iterative search?
- Basis in paper: [explicit] The authors state in the limitations section that "while our proposed pruning search algorithm is effective, other combinatorial optimization algorithms might identify superior pruning sequences."
- Why unresolved: The paper uses a specific heuristic iterative search but acknowledges other optimization approaches might be better without testing them.
- What evidence would resolve it: Comparative experiments applying alternative optimization algorithms (like beam search, genetic algorithms, or reinforcement learning) to the block pruning problem and measuring performance differences.

### Open Question 3
- Question: What is the optimal balance between MHA and MLP block pruning at different sparsity levels to maximize inference speedup while maintaining performance?
- Basis in paper: [explicit] The paper shows that BlockPruner tends to remove more MHA blocks at equivalent pruning ratios, which reduces KV cache usage and potentially accelerates inference, but doesn't explore the optimal balance.
- Why unresolved: While the paper observes that MHA blocks are more redundant and that pruning more MHA blocks speeds up inference, it doesn't systematically study the trade-off between MHA vs MLP pruning ratios.
- What evidence would resolve it: Controlled experiments varying the ratio of MHA to MLP blocks pruned at different sparsity levels, measuring both performance retention and inference speed to identify optimal ratios.

## Limitations

- The paper relies heavily on perplexity computed on a single calibration dataset (Alpaca, 256 samples) without ablation studies showing sensitivity to calibration data quality or size.
- Evaluation focuses only on zero-shot performance across five benchmarks, lacking fine-tuning experiments to test pruned models' adaptability.
- While claiming significant inference speed improvements from MHA pruning, the paper provides no empirical speed measurements or ablation studies isolating MHA vs. MLP pruning effects on latency.

## Confidence

**High Confidence**: The core mechanism of fine-grained block pruning (segmenting layers into MHA and MLP blocks, using perplexity for importance scoring) is well-specified and technically sound. The observation that BlockPruner achieves lower perplexity than layer-level methods on the same pruning ratio is directly supported by the reported results.

**Medium Confidence**: The claim that BlockPruner consistently outperforms all baselines across all metrics and model sizes is supported by the experimental tables, but the confidence is reduced by the lack of statistical significance testing and the limited number of baselines compared (4 methods). The generalizability to other model architectures and tasks remains untested.

**Low Confidence**: The assertion that MHA blocks contain significantly more redundancy than MLP blocks, enabling aggressive MHA pruning without performance loss, lacks sufficient ablation evidence. The paper shows pruning ratios by block type but doesn't isolate the performance impact of MHA-only vs. MLP-only pruning paths.

## Next Checks

1. **Calibration Data Sensitivity Analysis**: Run BlockPruner with varying calibration dataset sizes (16, 64, 256, 1024 samples) on a single model (e.g., Llama2-7B) and measure the variance in final perplexity and downstream task performance. This will quantify how robust the method is to calibration data quality.

2. **Ablation of MHA vs. MLP Pruning Impact**: Implement two variants of BlockPruner: one that only prunes MHA blocks and one that only prunes MLP blocks. Compare their performance curves and inference speed gains to validate the claimed differential redundancy between block types.

3. **Statistical Significance Testing**: Re-run all baseline comparisons with multiple seeds and compute statistical significance (e.g., paired t-tests) for the performance differences on each benchmark. Report confidence intervals for the average score metric to determine if observed improvements are statistically robust.