---
ver: rpa2
title: 'COPR: Continual Human Preference Learning via Optimal Policy Regularization'
arxiv_id: '2402.14228'
source_url: https://arxiv.org/abs/2402.14228
tags:
- learning
- copr
- human
- policy
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes COPR, a method for Continual Human Preference
  Learning in Large Language Models (LLMs) that addresses the challenge of adapting
  to evolving human preferences without forgetting historical preferences. COPR leverages
  the optimal policy theory to bypass complex RLHF pipelines, using sampling distributions
  as both demonstration and regularization constraints.
---

# COPR: Continual Human Preference Learning via Optimal Policy Regularization

## Quick Facts
- arXiv ID: 2402.14228
- Source URL: https://arxiv.org/abs/2402.14228
- Reference count: 40
- Authors: Han Zhang; Lin Gui; Yu Lei; Yuanzhao Zhai; Yehong Zhang; Yulan He; Hui Wang; Yue Yu; Kam-Fai Wong; Bin Liang; Ruifeng Xu

## Executive Summary
COPR is a novel method for Continual Human Preference Learning in Large Language Models (LLMs) that addresses the challenge of adapting to evolving human preferences without forgetting historical preferences. The method leverages optimal policy theory to bypass complex RLHF pipelines, using sampling distributions as both demonstration and regularization constraints. COPR employs a model-free reward function and Lagrangian Duality to dynamically balance learning new and old preferences. Experiments on a newly proposed benchmark demonstrate that COPR outperforms strong continual learning baselines across multiple metrics including reward-based, GPT-4, and human evaluations.

## Method Summary
COPR proposes a novel approach to continual human preference learning by fitting the sampling distribution of the optimal policy rather than directly learning the policy. The method uses historically optimal policies as regularization constraints to prevent catastrophic forgetting and employs a model-free reward function based on linear deterministic advantage to simplify the reward modeling step. The key innovation lies in using sampling distributions as both demonstration and regularization constraints, enabling the model to learn new preferences while retaining old ones. The method is trained on a series of tasks with human preference rankings, using a replay memory buffer to store historical data and Lagrangian Duality to dynamically balance the learning of new and old preferences.

## Key Results
- COPR outperforms strong continual learning baselines across multiple metrics including reward-based, GPT-4, and human evaluations.
- The method achieves comparable performance to the upper bound (Iterated RLHF) without retraining, demonstrating its effectiveness in preventing catastrophic forgetting.
- COPR shows robustness across different backbones, replay memory sizes, and task learning orders, highlighting its adaptability to various continual learning scenarios.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: COPR achieves continual alignment by fitting the sampling distribution of the optimal policy rather than directly learning the policy.
- Mechanism: The sampling distribution P*(y|Y_x) is defined as the relative probabilities of generating different responses under a given prompt x. By minimizing the KL-divergence between the current policy's sampling distribution and the optimal policy's sampling distribution, COPR learns to generate responses that align with human preferences while avoiding the complexity of estimating the partition function Z(x).
- Core assumption: Under sufficient sampling, fitting the sampling distribution P*(y|Y_x) is equivalent to directly learning the optimal policy Ï€*(y|x).
- Evidence anchors:
  - [abstract] "COPR utilizes a sampling distribution as a demonstration and regularization constraints for CL."
  - [section 3.1] "Inspired by Proposition 1, we propose approximating the sampling distribution P*t(y|Y_x)..."
  - [corpus] Weak evidence - no direct corpus support for this specific mechanism.
- Break condition: Insufficient sampling may lead to probability reduction and suboptimal learning.

### Mechanism 2
- Claim: COPR uses historically optimal policies as regularization constraints to prevent catastrophic forgetting.
- Mechanism: When learning a new task T_t, COPR minimizes the KL-divergence between the current policy's sampling distribution and the optimal policy's sampling distribution for all previous tasks T_i (i < t). This is achieved by using a replay memory buffer to store historical data and applying Lagrangian Duality to dynamically balance the learning of new and old preferences.
- Core assumption: Historical optimal policies can be effectively approximated using a replay memory buffer.
- Evidence anchors:
  - [abstract] "It adopts the Lagrangian Duality (LD) method to dynamically regularize the current policy based on the historically optimal policy, which prevents CF..."
  - [section 3.2.2] "To address this, we utilize historical sampling distributions {P*i(y|Y_x)|i = 1, 2, ..., t - 1} as regularization constraints..."
  - [corpus] Weak evidence - no direct corpus support for this specific mechanism.
- Break condition: Insufficient replay memory size may lead to increased forgetting of old preferences.

### Mechanism 3
- Claim: COPR employs a model-free reward function based on linear deterministic advantage to simplify the reward modeling step and avoid overfitting.
- Mechanism: Instead of learning a complex reward model, COPR uses a manually crafted reward function that assigns scores to each response in an arithmetic progression based on human ranking. This simplifies the learning process and reduces the risk of overfitting, especially in a continual learning scenario that requires repeated replay of historical data.
- Core assumption: The linear deterministic advantage function can effectively capture the relative preference information in the human ranking data.
- Evidence anchors:
  - [abstract] "It adopts the Lagrangian Duality (LD) method to dynamically regularize the current policy based on the historically optimal policy, which prevents CF and avoids over-emphasizing unbalanced objectives."
  - [section 3.3] "Linear deterministic advantage assigns scores to each response in an arithmetic progression, effectively translating the human preference order into rewards."
  - [corpus] Weak evidence - no direct corpus support for this specific mechanism.
- Break condition: The linear deterministic advantage function may not capture complex preference patterns in the human ranking data.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: COPR builds upon the RLHF framework to align LLMs with human preferences, but with modifications to enable continual learning.
  - Quick check question: What are the three main stages of the RLHF pipeline, and how does COPR modify each stage for continual learning?

- Concept: Continual Learning (CL)
  - Why needed here: COPR addresses the challenge of adapting LLMs to evolving human preferences over time without forgetting historical preferences, which is a key aspect of continual learning.
  - Quick check question: What are the two main challenges of applying RLHF to continual learning, and how does COPR address them?

- Concept: Catastrophic Forgetting (CF)
  - Why needed here: CF is a major concern in continual learning, and COPR specifically aims to prevent CF by using historical optimal policies as regularization constraints.
  - Quick check question: What is catastrophic forgetting, and how does COPR's use of historical optimal policies as regularization constraints help prevent it?

## Architecture Onboarding

- Component map: Sampling distribution estimator -> Replay memory buffer -> Lagrangian Duality optimizer
- Critical path:
  1. Initialize the model with the supervised fine-tuned model.
  2. For each new task, compute the loss function using the sampling distribution estimator and the replay memory buffer.
  3. Use the Lagrangian Duality optimizer to update the model parameters and the Lagrange multiplier.
  4. Store a subset of the new task's data in the replay memory buffer.
- Design tradeoffs:
  - Simplicity vs. expressiveness: COPR uses a simple linear deterministic advantage function to avoid overfitting, but this may not capture complex preference patterns.
  - Memory efficiency vs. forgetting: COPR uses a replay memory buffer to prevent forgetting, but the size of the buffer affects the amount of historical data that can be stored.
- Failure signatures:
  - Performance degradation on new tasks: This may indicate insufficient learning of new preferences due to overly strong regularization.
  - Performance degradation on old tasks: This may indicate catastrophic forgetting due to insufficient regularization or insufficient replay memory size.
- First 3 experiments:
  1. Train COPR on a single task and evaluate its performance on a held-out test set to verify that it can learn human preferences.
  2. Train COPR on two tasks sequentially and evaluate its performance on both tasks to verify that it can prevent catastrophic forgetting.
  3. Train COPR on multiple tasks with varying replay memory sizes and evaluate its performance to determine the optimal replay memory size for preventing forgetting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model-free reward function in COPR compare to learned reward models in terms of capturing complex human preferences?
- Basis in paper: [explicit] The paper derives a model-free reward function from the training loss of the Reward Model (RM) and claims it simplifies the complexity of the learning process.
- Why unresolved: The paper does not provide a direct comparison between the model-free reward function and learned reward models on complex human preference datasets.
- What evidence would resolve it: Experiments comparing COPR's model-free reward function against learned reward models on datasets with complex, nuanced human preferences.

### Open Question 2
- Question: What is the long-term performance of COPR in scenarios where human preferences evolve continuously over extended periods?
- Basis in paper: [inferred] The paper focuses on continual learning but does not provide experiments on extended timeframes or investigate the scalability of COPR to lifelong learning scenarios.
- Why unresolved: The experiments conducted cover a limited number of tasks and do not explore the scalability of COPR to scenarios with continuous preference evolution.
- What evidence would resolve it: Experiments testing COPR's performance on a continuous stream of evolving human preferences over extended periods, potentially using a simulated environment.

### Open Question 3
- Question: How does the sampling distribution approach in COPR handle situations where human preferences are contradictory or ambiguous?
- Basis in paper: [inferred] The paper does not explicitly address scenarios where human preferences are contradictory or ambiguous, which is a common occurrence in real-world applications.
- Why unresolved: The experiments conducted do not include datasets with contradictory or ambiguous human preferences, leaving the robustness of COPR in such scenarios unexplored.
- What evidence would resolve it: Experiments testing COPR's performance on datasets with contradictory or ambiguous human preferences, potentially including synthetic datasets designed to simulate such scenarios.

## Limitations

- The paper lacks extensive empirical validation on real-world, long-term continual learning scenarios, focusing on three datasets with limited task diversity.
- The human evaluation results, while promising, are based on a relatively small sample size, limiting the generalizability of the findings.
- The paper does not provide a detailed analysis of the computational overhead introduced by the Lagrangian Duality optimization and replay memory buffer, which could be significant for large-scale LLM deployments.

## Confidence

- **High**: The theoretical foundation of using sampling distributions as both demonstration and regularization constraints is well-established in the literature.
- **Medium**: The empirical results demonstrating COPR's effectiveness on the proposed benchmark are encouraging but limited in scope.
- **Low**: The paper does not provide a comprehensive analysis of the trade-offs between simplicity and expressiveness in the linear deterministic advantage function, nor does it explore the impact of varying replay memory sizes on performance.

## Next Checks

1. **Long-term Continual Learning Evaluation**: Extend the experiments to include more diverse and complex tasks, simulating real-world scenarios where human preferences evolve over extended periods. This will provide a more robust assessment of COPR's ability to prevent catastrophic forgetting in the long run.

2. **Computational Overhead Analysis**: Conduct a detailed analysis of the computational overhead introduced by the Lagrangian Duality optimization and replay memory buffer. This will help determine the scalability of COPR for large-scale LLM deployments and identify potential optimizations.

3. **Sensitivity Analysis of Hyperparameters**: Perform a comprehensive sensitivity analysis of the hyperparameters, particularly the Lagrange multiplier and the size of the replay memory buffer. This will provide insights into the robustness of COPR to hyperparameter choices and guide practitioners in tuning the method for their specific use cases.