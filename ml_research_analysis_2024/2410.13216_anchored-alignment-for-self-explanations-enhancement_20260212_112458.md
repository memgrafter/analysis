---
ver: rpa2
title: Anchored Alignment for Self-Explanations Enhancement
arxiv_id: '2410.13216'
source_url: https://arxiv.org/abs/2410.13216
tags:
- explanation
- preference
- pairs
- prompt
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study introduces a method to improve large language models''
  ability to explain their reasoning without requiring annotated rationale explanations.
  The approach involves three main components: assessing explanation quality, generating
  self-instruction datasets, and aligning the model.'
---

# Anchored Alignment for Self-Explanations Enhancement

## Quick Facts
- arXiv ID: 2410.13216
- Source URL: https://arxiv.org/abs/2410.13216
- Authors: Luis Felipe Villa-Arenas; Ata Nizamoglu; Qianli Wang; Sebastian Möller; Vera Schmitt
- Reference count: 31
- Primary result: Anchored Alignment improves self-explanation quality while maintaining classification accuracy

## Executive Summary
This paper introduces a method to enhance large language models' ability to generate self-explanations for classification tasks without requiring annotated rationale explanations. The approach uses three core components: evaluating generated explanations, creating self-instruction datasets, and aligning the model. A novel technique called Alignment with Anchor Preference Pairs enhances Direct Preference Optimization by categorizing model outputs into consistently correct, consistently incorrect, and variable groups, then applying tailored strategies to each. Experiments on four benchmark datasets demonstrate that this anchored approach significantly improves explanation quality while maintaining classification accuracy compared to other fine-tuning strategies.

## Method Summary
The method employs a three-component framework to enhance self-explanation capabilities. First, it fine-tunes a base model (Llama-3-8B-Instruct) on classification datasets to obtain a specialized model. Second, it generates self-instruction datasets using the fine-tuned model as a judge to evaluate its own explanations against predefined criteria. Third, it applies Direct Preference Optimization (DPO) with anchor-based preference pairs, where model responses are categorized into consistently correct, consistently incorrect, and variable groups. The anchored approach applies tailored strategies to each category during alignment, improving explanation quality while maintaining classification accuracy.

## Key Results
- Anchored Alignment method significantly improves explanation quality while maintaining accuracy
- Using anchor preference pairs outperforms self-alignment strategies relying solely on judge-based evaluations
- The approach achieves win rates of 57.9% in pairwise explanation quality comparisons on AQuA-Rat dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using anchor-based preference pairs improves explanation quality while maintaining classification accuracy.
- Mechanism: By categorizing model outputs into consistently correct, consistently incorrect, and variable groups, the method applies tailored strategies for each category. This ensures that preference pairs are constructed with clear quality distinctions, avoiding reinforcement of problematic behaviors.
- Core assumption: The classification task used as a probing mechanism provides a reliable anchor for evaluating model consistency on each input prompt.
- Evidence anchors:
  - [abstract] "Our experimental results demonstrate that this approach significantly improves explanation quality while maintaining accuracy compared to other fine-tuning strategies."
  - [section 3.3] "We introduce a method to enhance the selection of preference pairs by categorizing model responses into three groups: consistently correct, consistently incorrect, and variable."
- Break condition: If the classification task is not representative of the actual reasoning required for self-explanation, the anchor may lead to poor preference pair selection.

### Mechanism 2
- Claim: Self-instruction dataset generation with LLM-as-Judge evaluation creates a self-contained alignment process.
- Mechanism: The base model is used as the judge during dataset creation, evaluating its own generated explanations against predefined criteria. This eliminates the need for external human feedback while still providing quality signals for alignment.
- Core assumption: The base model has sufficient capability to accurately evaluate explanation quality according to the defined criteria.
- Evidence anchors:
  - [section 3.2] "During the creation of the self-instruct dataset, we employ Mbase as the judge (MJudge). This ensures that the model alignment process remains self-contained, without the need for external models, except for evaluation purposes."
  - [abstract] "Our approach integrates three core components: evaluating generated explanations, creating self-instruct datasets, and aligning the model."
- Break condition: If the base model's evaluation capability is insufficient, it may generate poor preference pairs, leading to ineffective alignment.

### Mechanism 3
- Claim: DPO alignment with anchor preference pairs mitigates the degradation of explanation quality caused by SFT.
- Mechanism: The DPO phase uses preference pairs that maintain clear quality distinctions, preventing the model from specializing solely on classification accuracy at the expense of explanation quality.
- Core assumption: The preference pairs selected through the anchor method provide better learning signals than rank-ordered pairs based solely on judge scores.
- Evidence anchors:
  - [abstract] "We show that using anchor preference pairs outperforms self-alignment strategies that rely solely on judge-based evaluations for preference pair selection."
  - [section 4.2.4] "This supports our design principle that tailoring strategies based on model behavior is crucial for improving the quality of self-instruct datasets and avoiding the reinforcement of problematic behavior."
- Break condition: If the anchor method fails to create meaningful preference pairs, the DPO phase may not effectively improve explanation quality.

## Foundational Learning

- Concept: Chain-of-Thought prompting
  - Why needed here: The method relies on prompting models to articulate reasoning before providing predictions, which is fundamental to generating self-explanations.
  - Quick check question: What is the purpose of having the model generate explanations before predictions in this framework?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO is the alignment technique used after creating the self-instruction dataset with anchor preference pairs.
  - Quick check question: How does DPO differ from standard supervised fine-tuning in this context?

- Concept: LLM-as-Judge evaluation framework
  - Why needed here: The method uses a more capable model to evaluate explanation quality based on predefined criteria.
  - Quick check question: What are the five evaluation criteria used by the judge model to assess self-explanations?

## Architecture Onboarding

- Component map:
  Base Model (Llama-3-8B-Instruct) → Supervised Fine-Tuning → MSFT → Self-Instruction Dataset Generation (with LLM-as-Judge) → Preference Pairs → DPO Alignment → MAnchor

- Critical path: Base Model → SFT → Self-Instruction Dataset → DPO Alignment → MAnchor
  The most critical components are the anchor-based preference pair selection and the DPO alignment phase.

- Design tradeoffs:
  - Using the base model as judge enables self-contained alignment but may limit evaluation quality
  - Smaller sample size (N=4) for preference pair selection balances computational cost with evaluation reliability
  - LoRA adapters reduce computational cost but may limit model capacity compared to full fine-tuning

- Failure signatures:
  - Poor classification accuracy improvement suggests issues with SFT or anchor selection
  - Low win rates in pairwise comparisons indicate ineffective explanation quality improvement
  - High variance in preference pair categories suggests inconsistent anchor behavior

- First 3 experiments:
  1. Verify that SFT improves classification accuracy on target datasets while measuring explanation quality degradation
  2. Test anchor-based preference pair selection by analyzing category distributions across datasets
  3. Compare MAnchor against MRank and MBase on both classification accuracy and explanation quality metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Alignment with Anchor Preference Pairs method compare to other preference selection strategies in terms of computational efficiency and scalability?
- Basis in paper: [inferred] The paper introduces the Alignment with Anchor Preference Pairs method but does not provide a detailed comparison of its computational efficiency or scalability with other preference selection strategies.
- Why unresolved: The paper focuses on the effectiveness of the method in improving explanation quality and maintaining classification accuracy but does not delve into the computational costs or scalability aspects.
- What evidence would resolve it: A comprehensive analysis comparing the computational resources and time required by the Alignment with Anchor Preference Pairs method versus other preference selection strategies would provide clarity.

### Open Question 2
- Question: Can the methodology for evaluating self-explanation quality be adapted for non-classification tasks, such as generative tasks or multi-modal tasks?
- Basis in paper: [explicit] The paper discusses the evaluation of self-explanation quality for classification tasks but does not explore its applicability to other types of tasks.
- Why unresolved: The criteria for evaluating self-explanations are tailored to classification tasks, and the paper does not address how these criteria might be adapted or extended to other task types.
- What evidence would resolve it: An exploration of the methodology's adaptability to different task types, including generative and multi-modal tasks, would demonstrate its versatility and potential limitations.

### Open Question 3
- Question: What are the long-term effects of using the base model as the judge during the self-instruct dataset creation, and how does this impact the model's ability to generalize to new tasks?
- Basis in paper: [explicit] The paper acknowledges that using the base model as the judge introduces a static evaluation process and suggests that iteratively enhancing the judge's capabilities could mitigate this issue.
- Why unresolved: The paper does not investigate the long-term effects of this approach on the model's generalization capabilities or explore alternative methods for improving the judge's effectiveness over time.
- What evidence would resolve it: Longitudinal studies examining the model's performance and generalization after multiple iterations of self-instruct dataset creation would provide insights into the long-term impacts and potential improvements.

## Limitations
- The use of base model as judge may introduce self-reinforcing bias in evaluation
- Preference pair selection with N=4 samples per prompt may not capture full quality distribution
- Results are limited to classification tasks where answers can be objectively verified

## Confidence
- **High confidence**: Anchored preference pairs improve explanation quality compared to judge-only methods
- **Medium confidence**: Mechanism explaining why anchored approach works better (categorization effectiveness)
- **Low confidence**: Generalizability beyond the four benchmark datasets used

## Next Checks
1. Apply anchored alignment method to a completely different reasoning domain (e.g., commonsense reasoning) to verify cross-domain generalization
2. Compare results when using increasingly capable judge models to determine if self-contained approach is optimal
3. Systematically vary preference pair sample size N from 2 to 10 to identify relationship between sample size and explanation quality improvement