---
ver: rpa2
title: 'On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer
  an Alternative to Human Translations?'
arxiv_id: '2406.14267'
source_url: https://arxiv.org/abs/2406.14267
tags:
- latn
- arab
- deva
- cyrl
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates multilingual NLP evaluation practices,
  highlighting limitations in current benchmarks' language coverage and representativeness.
  The authors propose machine translation as a viable alternative to human translation
  for large-scale evaluation, translating four classification datasets into 198 languages
  using NLLB.
---

# On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?

## Quick Facts
- **arXiv ID:** 2406.14267
- **Source URL:** https://arxiv.org/abs/2406.14267
- **Reference count:** 40
- **Primary result:** Machine translation can serve as a viable alternative to human translation for large-scale multilingual NLP evaluation, though low-resource language coverage tends to overestimate model performance by up to 8.7%

## Executive Summary
This paper investigates multilingual NLP evaluation practices by proposing machine translation as an alternative to human translation for large-scale evaluation across 198 languages. The authors translate four classification datasets using NLLB-3.3B and evaluate XLM-R, BLOOMz, and AYA-101 models to study the reliability of current evaluation practices. Their analysis reveals that while high- and mid-resource language subsets are generally representative, low-resource language coverage tends to overestimate model performance by up to 8.7%. Additionally, they find that simpler baselines like LSTMs can achieve surprisingly strong performance on unseen languages, suggesting current evaluation metrics may not fully capture the benefits of large-scale multilingual pretraining.

## Method Summary
The authors fine-tune XLM-R on English training data for four classification tasks (XNLI, PAWS-X, XCOPA, XStoryCloze) and evaluate zero-shot on machine-translated test sets across 198 languages. They also use zero-shot prompting for BLOOMz and AYA models. Machine translation quality is assessed using chrF++ scores and correlated with model performance. The evaluation compares performance across high-, mid-, low-resource, and unseen languages to assess representativeness and identify potential biases in current evaluation practices.

## Key Results
- Machine translation can serve as a viable alternative to human translation for large-scale multilingual evaluation
- Low-resource language coverage tends to overestimate model performance by up to 8.7% compared to high-resource languages
- Simpler baselines like LSTMs achieve surprisingly strong performance on unseen languages, questioning whether current evaluation metrics capture the full benefits of large-scale multilingual pretraining

## Why This Works (Mechanism)
The paper's approach works by leveraging machine translation to enable evaluation at unprecedented scale (198 languages) while maintaining reasonable quality through quality metrics like chrF++. The translation enables zero-shot evaluation across diverse languages without requiring human-translated test sets for each language. However, the mechanism is limited by translation quality variations, particularly for low-resource languages where sparse reference data leads to overestimation of model performance.

## Foundational Learning

**Multilingual Language Models**: Models trained on multiple languages simultaneously to capture cross-lingual patterns and transfer knowledge across languages. *Why needed:* Essential for understanding the evaluation of models that claim to work across many languages. *Quick check:* Verify that the model architecture includes shared parameters across languages.

**Zero-shot Evaluation**: Testing models on languages not seen during training without any fine-tuning on those languages. *Why needed:* The study specifically evaluates models' ability to generalize to unseen languages. *Quick check:* Confirm the model was only trained on English data for each task.

**Translationese**: Artifacts and patterns introduced by machine translation that can affect model performance differently than naturally occurring text. *Why needed:* Critical for interpreting performance differences between human and machine translations. *Quick check:* Compare model performance on naturally occurring vs. translated text in the same language.

## Architecture Onboarding

**Component Map**: NLLB-3.3B translation model -> Translated datasets -> MLMs (XLM-R, BLOOMz, AYA) -> Evaluation metrics (accuracy, chrF++)

**Critical Path**: Translate English datasets → Fine-tune XLM-R on English → Zero-shot evaluate on translated sets → Compute accuracy and chrF++ scores → Analyze performance across resource categories

**Design Tradeoffs**: Using machine translation enables evaluation at scale (198 languages) but introduces potential quality variations and translationese effects that may bias results, particularly for low-resource languages.

**Failure Signatures**: Inconsistent chrF++ scores across languages, performance gaps between resource categories, unexpected strong performance of simple baselines on unseen languages.

**First Experiments**:
1. Fine-tune XLM-R on English XNLI training data and evaluate on both human and machine-translated test sets for a small set of languages to quantify translation quality impact.
2. Compare accuracy scores between high-resource and low-resource languages to measure the 8.7% performance gap claimed in the paper.
3. Evaluate a simple LSTM baseline on unseen languages to reproduce the surprising effectiveness finding.

## Open Questions the Paper Calls Out
None

## Limitations
- Machine translation quality varies significantly across languages, particularly for low-resource languages where reference data may be sparse
- High- and mid-resource language subsets are generally representative, but low-resource coverage tends to overestimate performance by up to 8.7%
- The finding that simpler baselines like LSTMs can achieve strong performance on unseen languages suggests current evaluation metrics may not fully capture the benefits of large-scale multilingual pretraining

## Confidence

**High confidence**: The general finding that machine translation can serve as a viable alternative for large-scale evaluation across many languages

**Medium confidence**: The specific performance gaps between resource categories and the overestimation in low-resource languages

**Medium confidence**: The surprising effectiveness of simpler baselines on unseen languages

## Next Checks

1. **Translation Quality Validation**: Systematically compare model performance on human-translated vs. machine-translated subsets for a smaller set of languages to quantify the impact of translation quality on evaluation reliability.

2. **Benchmark Representativeness Audit**: Expand the analysis of language representativeness beyond the four datasets studied, particularly examining whether the "unseen" languages truly represent new linguistic phenomena or share similarities with seen languages.

3. **Artifact Sensitivity Test**: Design controlled experiments to isolate and measure "translationese" effects by comparing model performance on naturally occurring text vs. machine-translated text in the same languages.