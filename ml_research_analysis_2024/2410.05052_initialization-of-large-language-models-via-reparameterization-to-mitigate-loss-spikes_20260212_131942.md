---
ver: rpa2
title: Initialization of Large Language Models via Reparameterization to Mitigate
  Loss Spikes
arxiv_id: '2410.05052'
source_url: https://arxiv.org/abs/2410.05052
tags:
- training
- loss
- proposed
- wesar
- update
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses loss spikes, sudden divergences in loss during
  large language model pre-training, by identifying non-uniform parameter norms as
  a key cause. The authors propose Weight Scaling as Reparameterization (WeSaR), which
  introduces trainable gate parameters per matrix to uniformly scale parameters, thereby
  stabilizing training and enabling smaller initialization scales.
---

# Initialization of Large Language Models via Reparameterization to Mitigate Loss Spikes

## Quick Facts
- arXiv ID: 2410.05052
- Source URL: https://arxiv.org/abs/2410.05052
- Reference count: 40
- This paper proposes Weight Scaling as Reparameterization (WeSaR) to stabilize large language model training by introducing trainable gate parameters that ensure uniform parameter norms, reducing loss spikes and enabling faster convergence.

## Executive Summary
This paper addresses loss spikes, sudden divergences in loss during large language model pre-training, by identifying non-uniform parameter norms as a key cause. The authors propose Weight Scaling as Reparamization (WeSaR), which introduces trainable gate parameters per matrix to uniformly scale parameters, thereby stabilizing training and enabling smaller initialization scales. Experiments with 130M, 1.3B, and 13B parameter Transformer decoders show WeSaR reduces loss spikes, accelerates convergence, and achieves better perplexity than standard initialization and existing reparameterization methods. The approach improves both training stability and downstream task performance.

## Method Summary
WeSaR reparameterizes each parameter matrix W by introducing a trainable gate parameter α, creating virtual parameters α·W that are used in the forward pass. This allows actual parameters W to have uniform small norms while the gate parameters handle scaling needed for stable gradients. The method enables arbitrary small common standard deviation initialization (σ² = 4e-5 used in experiments) while maintaining stability through the reparameterization. During training, gradients flow through the virtual parameters α·W, and both W and α are updated by the optimizer. The approach is implemented on Transformer decoder architectures with embedding, self-attention (Wq, Wk, Wv, Wo), feed-forward (Wu, Wd), and prediction (Wp) layers, each with an associated gate parameter.

## Key Results
- WeSaR reduces loss spikes in 130M, 1.3B, and 13B parameter Transformer decoders during pre-training
- Enables arbitrary small initialization scale (σ² = 4e-5) while maintaining stability
- Achieves better perplexity than standard initialization and ablation studies (fixed α, different backbone initializations)
- Improves downstream task performance on SuperGLUE benchmark compared to baseline Small initialization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Non-uniform parameter norms lead to unstable training and loss spikes
- Mechanism: When parameters have different norms, those with smaller norms experience larger relative updates (higher ∥∆W∥/∥W∥), making them more sensitive to parameter changes and prone to instability
- Core assumption: The magnitude of parameter updates relative to parameter size determines training stability
- Evidence anchors:
  - [abstract] "parameters whose norm is smaller are more sensitive to the parameter update"
  - [section 3.3] "parameters whose norm is smaller than that of the others, the update ratios ∥∆W∥/∥W∥ are larger"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism

### Mechanism 2
- Claim: WeSaR stabilizes training by making parameter norms uniform through reparameterization
- Mechanism: By introducing trainable gate parameters α that scale each parameter matrix, WeSaR ensures all virtual parameters α·W· have appropriate norms for back-propagation while actual parameters W· have uniform small norms, eliminating sensitivity differences
- Core assumption: Uniform parameter norms eliminate sensitivity differences that cause instability
- Evidence anchors:
  - [abstract] "WeSaR introduces a gate parameter α ∈ R for each parameter matrix W and uses αW instead of W inside the model"
  - [section 4.2] "WeSaR relieves the actual parameters and their update of the restriction with respect to σ·"
  - [corpus] Weak - no direct corpus evidence for this specific reparameterization approach

### Mechanism 3
- Claim: WeSaR enables faster training by allowing smaller initialization scales while maintaining stability
- Mechanism: The reparameterization separates initialization scale from back-propagation requirements, allowing all parameters to be initialized with small common standard deviation σ while the gate parameters handle the scaling needed for stable gradients
- Core assumption: Smaller initialization scales lead to faster convergence when training remains stable
- Evidence anchors:
  - [abstract] "WeSaR enables an arbitrary small common standard deviation to set be for all parameters, which results in not only stable, but also accelerated, training"
  - [section 4.3] "Different from conventional initialization methods, WeSaR can set the common standard deviation σ to an arbitrary value"
  - [corpus] Weak - no direct corpus evidence for this specific initialization benefit

## Foundational Learning

- Concept: Back-propagation gradient scaling requirements
  - Why needed here: Understanding why parameter norms must be non-uniform in standard initialization is crucial for grasping the core problem WeSaR addresses
  - Quick check question: What mathematical requirement must each layer satisfy to avoid vanishing/exploding gradients?

- Concept: Update ratio as stability metric
  - Why needed here: The update ratio ∥∆W∥/∥W∥ is the key metric used to diagnose and measure training stability in this work
  - Quick check question: How does the update ratio differ between parameters with different norms under standard initialization?

- Concept: Reparameterization techniques
  - Why needed here: WeSaR is fundamentally a reparameterization method, so understanding existing approaches helps contextualize its novelty
  - Quick check question: How does weight normalization differ from the reparameterization approach used in WeSaR?

## Architecture Onboarding

- Component map: Embedding -> Self-Attention (Wq, Wk, Wv, Wo) -> Feed-Forward (Wu, Wd) -> Prediction (Wp)
- Critical path: Forward pass uses α·W· instead of W·, backward pass computes gradients through the virtual parameters, optimizer updates both W· and α·, gate parameters maintain appropriate scaling while actual parameters remain uniformly small
- Design tradeoffs: WeSaR adds one parameter per matrix (minimal overhead) but requires careful initialization of gate parameters; it enables smaller initialization but adds complexity to the training loop
- Failure signatures: Loss spikes reappear if gate parameters become unstable, training diverges if α· values grow too large or small, performance degrades if initialization of W· or α· is inappropriate
- First 3 experiments:
  1. Implement WeSaR on a small Transformer (130M params) and compare loss curves with standard initialization, focusing on early training stability
  2. Measure update ratios ∥∆W∥/∥W∥ across different parameter matrices to verify uniform scaling
  3. Test different initialization scales σ to find optimal values for rapid convergence while maintaining stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does WeSaR's stabilization mechanism extend to other neural network architectures beyond Transformers, such as CNNs or RNNs?
- Basis in paper: [inferred] The paper discusses non-uniform parameter norms causing loss spikes in Transformers and proposes WeSaR as a solution. The method involves introducing gate parameters to scale weights uniformly, which could theoretically apply to other architectures with similar gradient scaling issues.
- Why unresolved: The experiments only evaluated WeSaR on Transformer decoders, so its effectiveness on other architectures remains unknown.
- What evidence would resolve it: Empirical studies applying WeSaR to CNNs and RNNs showing improved stability and performance compared to standard initialization methods.

### Open Question 2
- Question: What is the theoretical limit of how small the standard deviation σ can be set in WeSaR before training becomes unstable?
- Basis in paper: [explicit] The paper states "Different from conventional initialization methods, WeSaR can set the common standard deviation σ to an arbitrary value" and experiments used σ² = 4e-5. However, it doesn't explore the theoretical or practical limits of this parameter.
- Why unresolved: The experiments used a specific value without systematically exploring the parameter space or establishing theoretical bounds.
- What evidence would resolve it: A comprehensive study varying σ across orders of magnitude, identifying the point where training stability degrades or performance drops significantly.

### Open Question 3
- Question: How does WeSaR's performance compare when used with newer activation functions like SwiGLU, which are commonly used in state-of-the-art LLMs?
- Basis in paper: [explicit] The paper notes "we did not use SWiGLU activation (Shazeer, 2020) in the feed-forward layers, as has been done in popular LLMs" and acknowledges "the effectiveness of SWiGLU remains controversial."
- Why unresolved: The experiments were conducted with standard GELU activation, leaving open questions about compatibility with alternative activation functions.
- What evidence would resolve it: Head-to-head comparisons of WeSaR with SwiGLU versus WeSaR with GELU on the same model architectures and datasets.

## Limitations

- Limited architectural scope: Experiments focus exclusively on Transformer decoder architectures without testing on encoder architectures, hybrid models, or non-Transformer architectures
- Sparse ablation studies: Lacks comprehensive ablations on norm choices, optimizer sensitivity, and sequence length effects
- Weak empirical grounding: While theoretical arguments are presented, direct corpus evidence validating the proposed mechanism is lacking

## Confidence

**High confidence**: The empirical demonstration that WeSaR reduces loss spikes and improves perplexity on the tested architectures (130M, 1.3B, 13B parameters). The quantitative results showing improved stability and faster convergence are well-supported by the experiments.

**Medium confidence**: The theoretical mechanism linking non-uniform parameter norms to training instability. While the mathematical reasoning is sound, the direct causal relationship lacks rigorous proof and corpus validation.

**Low confidence**: The claim that WeSaR would generalize to architectures beyond Transformer decoders without modification. The paper doesn't test this hypothesis, and different architectures may have different sensitivity patterns to parameter norm distributions.

## Next Checks

1. **Cross-architecture validation**: Implement and test WeSaR on a non-Transformer architecture (such as Mamba or RWKV) to verify the mechanism generalizes beyond the tested domain. This would validate whether the parameter norm uniformity principle applies universally or is architecture-specific.

2. **Mathematical formalization**: Rigorously prove the relationship between parameter norm uniformity and training stability by deriving exact bounds on the update ratio ∥∆W∥/∥W∥ as a function of parameter norm variance. This would strengthen the theoretical foundation and clarify the mechanism.

3. **Hyperparameter sensitivity analysis**: Systematically vary learning rates, batch sizes, and gradient clipping thresholds to identify the boundaries of WeSaR's effectiveness. This would reveal whether the method's benefits are robust across training configurations or require careful tuning.