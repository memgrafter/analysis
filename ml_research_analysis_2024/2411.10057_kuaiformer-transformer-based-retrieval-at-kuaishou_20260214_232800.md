---
ver: rpa2
title: 'KuaiFormer: Transformer-Based Retrieval at Kuaishou'
arxiv_id: '2411.10057'
source_url: https://arxiv.org/abs/2411.10057
tags:
- retrieval
- sequence
- user
- kuaiformer
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "KuaiFormer is a transformer-based retrieval framework deployed\
  \ in Kuaishou\u2019s short-video recommendation system. It replaces traditional\
  \ embedding-based retrieval with a transformer-driven Next Action Prediction paradigm,\
  \ enabling real-time multi-interest modeling."
---

# KuaiFormer: Transformer-Based Retrieval at Kuaishou

## Quick Facts
- arXiv ID: 2411.10057
- Source URL: https://arxiv.org/abs/2411.10057
- Reference count: 40
- Key outcome: 38% higher hit rate than strong baselines; +0.360% video watch time lift in online A/B tests

## Executive Summary
KuaiFormer is a transformer-based retrieval framework deployed in Kuaishou's short-video recommendation system. It replaces traditional embedding-based retrieval with a transformer-driven Next Action Prediction paradigm, enabling real-time multi-interest modeling. The system has served over 400 million daily active users since May 2024, delivering one of the largest retrieval improvements in the platform's history.

## Method Summary
KuaiFormer uses a transformer architecture with multi-interest query tokens, adaptive sequence compression, and smooth in-batch softmax with logQ correction. The model processes user behavior sequences of watched videos, extracting multiple interests through learnable query tokens and compressing long sequences to maintain efficiency. It's trained via online learning with minute-level updates and deployed using approximate nearest neighbor search for retrieval.

## Key Results
- 38% higher hit rate than strong baselines in offline experiments
- +0.360% video watch time lift in online A/B tests on Kuaishou Single Page
- Served over 400 million daily active users since May 2024

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stable training on billion-scale item sets using in-batch softmax with logQ correction and label smoothing
- Core assumption: Sampling bias in in-batch softmax degrades performance and can be corrected via logQ
- Evidence: Detailed derivation of Lsmooth logQ and discussion of in-batch sampling bias
- Break condition: If sampling bias is negligible or correction introduces instability

### Mechanism 2
- Claim: Multi-interest query tokens capture diverse user preferences more effectively than single-vector representations
- Core assumption: Users' interests are multi-faceted and can be represented by distinct vectors
- Evidence: Introduction of ð‘˜ learnable query tokens and their interaction via causal masking
- Break condition: If user interests are highly correlated, multiple tokens may introduce redundancy

### Mechanism 3
- Claim: Adaptive item compression reduces computation for long sequences while preserving accuracy
- Core assumption: Older interactions are less informative and can be coarsely modeled
- Evidence: Compression steps and experimental results showing 10% extra compute for 4x sequence length
- Break condition: If recent items are not more informative, compression may lose critical signals

## Foundational Learning

- **Softmax loss with logQ correction**
  - Why needed: To handle billion-scale item sets without full softmax while correcting sampling bias
  - Quick check: Why does in-batch softmax introduce sampling bias, and how does logQ correction address it?

- **Multi-head self-attention and causal masking**
  - Why needed: To model complex user behavior sequences and enable interest disentanglement
  - Quick check: How does causal masking differ from bidirectional attention, and why is it chosen for interest tokens?

- **Sequence compression via grouping**
  - Why needed: To scale transformer modeling to long sequences under strict latency constraints
  - Quick check: What is the trade-off between compression window size and information loss?

## Architecture Onboarding

- **Component map**: Input embeddings (one-hot + bucketed) â†’ Adaptive compression â†’ Multi-query token injection â†’ Stacked transformer layers â†’ LogQ-corrected softmax â†’ ANN retrieval
- **Critical path**: Embedding â†’ Compression â†’ Multi-query encoding â†’ Loss computation â†’ Parameter update â†’ ANN index refresh
- **Design tradeoffs**: Longer sequences vs. latency; more query tokens vs. compute; compression vs. accuracy; strict vs. smoothed labels
- **Failure signatures**: Accuracy drops when sequence length > 256; loss instability without logQ; poor online performance if query tokens misaligned
- **First 3 experiments**:
  1. Baseline: Run with sequence length 64, 1 query token, no compression; measure hit rate
  2. Compression test: Increase to length 256 with adaptive compression; compare accuracy and compute cost
  3. Multi-interest test: Add 4 query tokens with causal masking; measure improvement in HR@50 vs. single token baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does KuaiFormer's performance scale with extremely long user sequences (e.g., 10,000+ items) beyond the tested range of 64-256?
- Basis: The paper mentions that sequence length scaling from 64 to 256 showed diminishing returns
- Why unresolved: The paper only tested up to 256 sequence length
- What evidence would resolve it: Experiments comparing model accuracy at sequence lengths of 1,000, 5,000, and 10,000 items

### Open Question 2
- Question: What is the optimal number of query tokens for different user behavior patterns?
- Basis: The paper found diminishing returns in accuracy gains beyond 6 query tokens
- Why unresolved: The study used a fixed token count across all users
- What evidence would resolve it: A/B testing with adaptive token counts based on user interest diversity metrics

### Open Question 3
- Question: How does KuaiFormer's performance compare to traditional retrieval methods when deployed on less powerful hardware?
- Basis: The paper emphasizes the need for efficiency but only reports GPU-based performance metrics
- Why unresolved: Real-world deployment scenarios often involve heterogeneous hardware
- What evidence would resolve it: Benchmarking KuaiFormer's hit rate, latency, and throughput on CPU, mid-range GPUs, and edge devices

### Open Question 4
- Question: Can KuaiFormer's multi-interest extraction mechanism be extended to handle cross-modal user interests?
- Basis: The paper focuses on behavioral sequence modeling but doesn't explore integration with other modalities
- Why unresolved: Modern recommendation systems increasingly rely on multi-modal data
- What evidence would resolve it: Experiments integrating KuaiFormer with vision transformers or text encoders

## Limitations

- **Deployment Scale Verification**: Claims of 400M DAU and largest improvement rely on internal reporting without independent verification
- **Generalizability**: Framework optimized for short-video recommendation; performance on other domains not demonstrated
- **Sampling Bias Validation**: Necessity and effectiveness of logQ correction not empirically isolated through ablation studies
- **Multi-Interest Analysis**: No qualitative or quantitative analysis showing that query tokens learn distinct, interpretable interests

## Confidence

- **High**: Transformer architecture and multi-query token injection are standard and well-supported
- **Medium**: Adaptive sequence compression and its efficiency claims lack detailed ablation and sensitivity analysis
- **Low**: The claimed scale of impact and the necessity of logQ correction for billion-scale training are not independently verified

## Next Checks

1. **Ablation on LogQ Correction**: Run controlled experiments removing logQ correction and label smoothing to quantify their contributions
2. **Multi-Interest Token Analysis**: Perform qualitative analysis (attention visualization) and quantitative metrics to verify distinct interest capture
3. **Compression Trade-off Study**: Systematically vary compression window sizes and sequence lengths to establish the Pareto frontier