---
ver: rpa2
title: More RLHF, More Trust? On The Impact of Preference Alignment On Trustworthiness
arxiv_id: '2404.18870'
source_url: https://arxiv.org/abs/2404.18870
tags: []
core_contribution: "This study evaluates the impact of three popular reinforcement\
  \ learning from human feedback (RLHF) methods\u2014supervised fine-tuning (SFT),\
  \ proximal policy optimization (PPO), and direct preference optimization (DPO)\u2014\
  on the trustworthiness of large language models across five dimensions: toxicity,\
  \ stereotypical bias, machine ethics, truthfulness, and privacy. Using the Pythia\
  \ model suite and the Anthropic HH dataset for alignment, the experiments reveal\
  \ that PPO and SFT often increase toxicity and reduce truthfulness in larger models,\
  \ while DPO slightly improves toxicity but performs worse on ethics detection."
---

# More RLHF, More Trust? On The Impact of Preference Alignment On Trustworthiness

## Quick Facts
- arXiv ID: 2404.18870
- Source URL: https://arxiv.org/abs/2404.18870
- Reference count: 40
- RLHF methods (SFT, PPO, DPO) show mixed effects on model trustworthiness across toxicity, bias, ethics, truthfulness, and privacy dimensions

## Executive Summary
This study systematically evaluates how three popular reinforcement learning from human feedback (RLHF) methods—supervised fine-tuning (SFT), proximal policy optimization (PPO), and direct preference optimization (DPO)—affect the trustworthiness of large language models across five key dimensions. Using the Pythia model suite and Anthropic HH dataset, the experiments reveal that RLHF does not uniformly improve trustworthiness and that effects vary significantly by algorithm and model size. PPO and SFT often increase toxicity and reduce truthfulness in larger models, while DPO shows slightly better toxicity outcomes but worse ethics detection. All methods increase stereotypical bias, though DPO uniquely reduces privacy leakage. These findings challenge the assumption that RLHF inherently improves model trustworthiness and highlight the need for more nuanced alignment approaches.

## Method Summary
The researchers conducted controlled experiments using Pythia models of varying sizes (160M to 6.9B parameters) and three alignment algorithms: SFT, PPO, and DPO. They used the Anthropic HH dataset for alignment and evaluated models across five trustworthiness dimensions: toxicity, stereotypical bias, machine ethics, truthfulness, and privacy. The evaluation employed automated metrics including RealToxicityPrompts, BOLD, ETHICS, TruthfulQA, and privacy leakage tests. Models were trained with consistent hyperparameters across algorithms, and a random subsample of the alignment dataset was used for each training run to maintain comparability. The study measured performance both before and after alignment to isolate the effects of each RLHF method.

## Key Results
- PPO and SFT increase toxicity in larger models while DPO slightly reduces toxicity
- All three methods increase stereotypical bias, with DPO showing the smallest increase
- PPO and SFT reduce truthfulness, while DPO maintains more balanced performance
- Only DPO reduces privacy leakage, with other methods showing increased leakage
- Effects vary significantly by model size, with larger models showing more pronounced negative impacts on trustworthiness

## Why This Works (Mechanism)
Unknown: The paper does not explicitly explain the underlying mechanisms driving the observed effects of different RLHF methods on model trustworthiness. Assumption: The varying impacts may be related to how each algorithm optimizes for human preferences and how these preferences correlate with or diverge from trustworthiness metrics.

## Foundational Learning
- **Reinforcement Learning from Human Feedback (RLHF)**: Why needed: Core technique for aligning LLMs with human preferences; Quick check: Understand reward modeling and policy optimization basics
- **Proximal Policy Optimization (PPO)**: Why needed: Popular RL algorithm for stable policy updates; Quick check: Know clipped objective function and advantage estimation
- **Direct Preference Optimization (DPO)**: Why needed: Alternative to PPO that directly optimizes for human preferences; Quick check: Understand KL divergence minimization in preference space
- **Supervised Fine-Tuning (SFT)**: Why needed: Baseline alignment method using human demonstrations; Quick check: Know maximum likelihood training on preference data
- **Trustworthiness Metrics**: Why needed: Standardized evaluation of model safety and reliability; Quick check: Familiarity with toxicity, bias, ethics, truthfulness, and privacy benchmarks

## Architecture Onboarding

**Component Map:**
Pythia models (160M → 1.4B → 2.8B → 6.9B) -> Alignment Pipeline (SFT/PPO/DPO) -> Trustworthiness Evaluation Suite

**Critical Path:**
Model → Alignment Algorithm → Trustworthiness Metrics → Comparative Analysis

**Design Tradeoffs:**
Algorithm complexity vs. stability: PPO offers stable updates but complex implementation; DPO is simpler but may sacrifice some performance optimization; SFT is simplest but least effective at capturing nuanced preferences.

**Failure Signatures:**
- Increased toxicity scores post-alignment
- Degraded performance on truthfulness benchmarks
- Higher stereotypical bias in generated outputs
- Privacy leakage through model memorization
- Algorithm-specific degradation patterns (PPO/SFT worse for toxicity, DPO worse for ethics)

**First Experiments:**
1. Baseline evaluation of unaligned Pythia models across all trustworthiness dimensions
2. Comparative alignment of smallest model (160M) using all three algorithms
3. Stress test of largest model (6.9B) with single algorithm to observe size-dependent effects

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly call out specific open questions for future research.

## Limitations
- Results limited to Pythia model family, limiting generalizability to other architectures
- Single alignment dataset (Anthropic HH) may introduce dataset-specific biases
- Five trustworthiness dimensions examined do not capture full spectrum of ethical considerations
- Static evaluation metrics may not reflect real-world deployment scenarios
- No long-term stability assessment of alignment effects

## Confidence

**High confidence:**
- RLHF methods do not uniformly improve trustworthiness across all dimensions
- Effects vary by algorithm and model size

**Medium confidence:**
- PPO and SFT increase toxicity in larger models
- DPO's mixed performance across different trustworthiness dimensions

**Low confidence:**
- Generalizability to other model families and alignment datasets

## Next Checks
1. Replicate experiments using diverse model architectures (LLaMA, Mistral) and alternative alignment datasets to test robustness across different model families and data distributions
2. Conduct human evaluation studies to validate automated trustworthiness metrics, particularly for nuanced dimensions like machine ethics and privacy
3. Test long-term stability of RLHF effects by evaluating model behavior after extended deployment periods and exposure to diverse real-world inputs