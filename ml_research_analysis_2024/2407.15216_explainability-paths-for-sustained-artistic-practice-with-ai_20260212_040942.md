---
ver: rpa2
title: Explainability Paths for Sustained Artistic Practice with AI
arxiv_id: '2407.15216'
source_url: https://arxiv.org/abs/2407.15216
tags:
- training
- process
- generative
- datasets
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework for enhancing explainability in
  AI-driven generative audio systems, addressing the challenge of limited artist control
  and interpretability in current models. The authors advocate for human-scale models
  trained on artist-curated, small-scale datasets to foster deeper connections to
  training material.
---

# Explainability Paths for Sustained Artistic Practice with AI

## Quick Facts
- arXiv ID: 2407.15216
- Source URL: https://arxiv.org/abs/2407.15216
- Reference count: 17
- One-line primary result: Proposes a framework for enhancing explainability in AI-driven generative audio systems through small-scale artist-curated datasets, extended iterative processes, and interactive machine learning mapping.

## Executive Summary
This paper addresses the challenge of limited artist control and interpretability in AI-driven generative audio systems by proposing a framework that emphasizes human-scale models trained on artist-curated, small-scale datasets. The authors advocate for extending the iterative creative process beyond model inference to include data curation and training phases, and utilize interactive machine learning to map human performance spaces to model latent spaces. Through a case study using archival recordings, the paper demonstrates how these approaches enable artists to steer generative models with greater agency and predictability, resulting in more meaningful and sustained artistic practices with AI.

## Method Summary
The paper employs RA VE (a Variational Autoencoder for Fast and High-Quality Neural Audio Synthesis) to train generative audio models on archival recordings curated into three separate datasets based on historical and semantic content. The training process consists of a VAE phase (5 million steps) followed by a GAN phase (1-2 million steps). Real-time performance is implemented using interactive machine learning to map human performance spaces (facial landmarks and movements) to the model's latent space, allowing artists to define specific points of interest and control model output.

## Key Results
- Small-scale, artist-curated datasets foster deeper connections to training material and enable greater human agency in generative audio systems
- Extending the iterative process beyond inference to include data curation and training phases enhances explainability and artist control
- Interactive machine learning mapping between human performance space and model latent space enables steering of generative models with greater agency and predictability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Small-scale, artist-curated datasets enable greater human agency and explainability in generative audio systems.
- Mechanism: Artists gain intimate familiarity with small-scale datasets, allowing them to anticipate and influence model behavior more effectively.
- Core assumption: Artists require precise control and understanding of training data for sustained creative practice with AI.
- Evidence anchors:
  - [abstract] "human-scale models trained on artist-curated, small-scale datasets to foster deeper connections to training material"
  - [section 2] "artists usually find greater utility in models with a more narrow scope, facilitating a deeper connection to the training material"
  - [corpus] Weak evidence; corpus focuses on XAI in visual domains, not audio
- Break condition: If the small-scale dataset lacks sufficient diversity or quality to produce meaningful generative outputs.

### Mechanism 2
- Claim: Extending the iterative process beyond inference to include data curation and training phases enhances explainability.
- Mechanism: Iteration during data preparation, training, and implementation allows artists to continuously observe and adjust the model, compounding human agency and improving model reliability.
- Core assumption: Iterative processes facilitate the emergence of novel insights and are essential for artistic development.
- Evidence anchors:
  - [abstract] "extending the iterative creative process beyond model inference to include data curation and training phases"
  - [section 3] "Iteration is an essential process for artistic development. It facilitates the emergence of novel and meaningful insights"
  - [corpus] Weak evidence; corpus lacks focus on iterative creative processes in AI art
- Break condition: If the training process becomes too time-consuming or resource-intensive, disrupting the iterative cycle.

### Mechanism 3
- Claim: Interactive machine learning mapping between human performance space and model latent space enables steering of generative models.
- Mechanism: By mapping facial movements to latent space dimensions, artists can define specific points of interest and control model output in real-time, achieving causal gestures and long-term temporal coherence.
- Core assumption: Artists need to understand and control the axes and parameters of generative models during performance.
- Evidence anchors:
  - [abstract] "utilize interactive machine learning to map human performance spaces to model latent spaces"
  - [section 4] "we map the human performance space to the computer's latent space, using interactive machine learning as a mapping tool"
  - [corpus] Weak evidence; corpus lacks examples of interactive machine learning in audio generative models
- Break condition: If the mapping between human performance space and latent space is too complex or unintuitive.

## Foundational Learning

- Concept: Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs)
  - Why needed here: Understanding VAEs and GANs is crucial for grasping the training phases of the RAVE model used in the paper.
  - Quick check question: What are the primary differences between VAEs and GANs in terms of their training objectives and outputs?

- Concept: Interactive Machine Learning
  - Why needed here: Interactive machine learning is the key technique used to map human performance space to model latent space.
  - Quick check question: How does interactive machine learning differ from traditional supervised learning in terms of human involvement?

- Concept: Latent Space and Dimensionality
  - Why needed here: The concept of latent space and its dimensionality is central to understanding how models encode and decode information.
  - Quick check question: How does the choice of latent space dimensionality affect the model's ability to capture and reconstruct complex audio features?

## Architecture Onboarding

- Component map: Data Preparation -> Training (VAE -> GAN) -> Performance with interactive machine learning
- Critical path: Data preparation → Training (VAE → GAN) → Performance with interactive machine learning
- Design tradeoffs:
  - Small-scale datasets offer greater artist control but may limit model diversity
  - Real-time performance requires careful choice of latent space dimensionality
  - Interactive machine learning mapping adds complexity but enables precise control
- Failure signatures:
  - Poor model performance due to inadequate or inconsistent training data
  - Loss of control during performance due to unintuitive mapping between human space and latent space
  - Over-smoothing of output during extended GAN training
- First 3 experiments:
  1. Train a basic VAE model on a small, artist-curated audio dataset to understand data preparation and initial model behavior.
  2. Implement a simple interactive machine learning mapping between human gestures and latent space dimensions using a pre-trained model.
  3. Combine the VAE and interactive mapping components to create a basic real-time generative audio system, experimenting with different latent space dimensionalities.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the scale of the dataset impact the explainability and artist agency in generative audio models?
- Basis in paper: [explicit] The authors emphasize the importance of small-scale datasets for enhancing artist control and explainability, noting that larger datasets may reduce transparency and agency.
- Why unresolved: The paper discusses the benefits of small-scale datasets but does not provide a detailed comparative analysis of different dataset sizes and their effects on model behavior and artist agency.
- What evidence would resolve it: Conducting experiments with varying dataset sizes and measuring their impact on model explainability, artist control, and creative output would provide concrete evidence.

### Open Question 2
- Question: What are the long-term effects of using interactive machine learning to map human performance spaces to model latent spaces on artistic creativity?
- Basis in paper: [explicit] The authors propose using interactive machine learning as a mapping tool to enhance explainability and control over generative models.
- Why unresolved: While the paper outlines the potential benefits of this approach, it does not explore the long-term implications for artistic creativity and practice.
- What evidence would resolve it: Longitudinal studies tracking artists' creative processes and outputs over time when using this mapping technique would provide insights into its long-term effects.

### Open Question 3
- Question: How does the iterative process during model training compare to the iterative process during inference in terms of enhancing artist agency and model explainability?
- Basis in paper: [explicit] The authors advocate for extending the iterative process beyond inference to include training phases, suggesting it enhances artist agency and model explainability.
- Why unresolved: The paper does not provide a detailed comparison of the iterative processes during training and inference and their respective impacts on artist agency and model explainability.
- What evidence would resolve it: Comparative studies analyzing the differences in artist agency and model explainability between iterative processes during training and inference would provide clarity.

## Limitations
- The case study relies on archival recordings from a single source, limiting generalizability across different audio domains and cultural contexts.
- The interactive machine learning mapping between facial movements and latent space dimensions requires careful calibration that may not translate directly to other performance contexts or artists.
- The paper does not provide quantitative metrics for measuring explainability or human agency, relying instead on qualitative observations from artistic practice.

## Confidence

- High Confidence: The assertion that small-scale, artist-curated datasets enable greater human agency (supported by established principles in creative AI and interactive machine learning)
- Medium Confidence: The claim that extending iterative processes beyond inference enhances explainability (theoretically sound but lacks empirical validation in audio contexts)
- Low Confidence: The specific effectiveness of facial landmark-based performance mapping for generative audio control (limited evidence and highly context-dependent)

## Next Checks

1. **Dataset Diversity Validation**: Replicate the training and performance pipeline using audio datasets from multiple sources (e.g., music recordings, environmental sounds, spoken word) to assess whether the framework generalizes across different sonic domains and whether artist familiarity with training material consistently improves explainability.

2. **Quantitative Explainability Metrics**: Implement standardized metrics for measuring model interpretability and human agency, such as latent space traversal consistency, prediction error analysis, or user studies measuring artist understanding of model behavior, to provide empirical validation beyond qualitative observations.

3. **Alternative Performance Mapping Comparison**: Compare the facial landmark-based mapping approach with alternative interactive machine learning techniques (e.g., hand gesture recognition, audio input analysis, or eye tracking) to determine whether the proposed mapping method provides superior control and explainability or if the benefits are specific to the chosen interface.