---
ver: rpa2
title: Connecting Concept Convexity and Human-Machine Alignment in Deep Neural Networks
arxiv_id: '2409.06362'
source_url: https://arxiv.org/abs/2409.06362
tags:
- convexity
- alignment
- oooa
- correlation
- human-machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the relationship between convexity in neural
  network representations and human-machine alignment, motivated by cognitive science
  theories. Convexity is measured using a graph-based framework that assesses whether
  points within the same class form convex regions in latent space.
---

# Connecting Concept Convexity and Human-Machine Alignment in Deep Neural Networks

## Quick Facts
- arXiv ID: 2409.06362
- Source URL: https://arxiv.org/abs/2409.06362
- Reference count: 0
- One-line primary result: Strong positive correlation (R=0.91) between convexity and human-machine alignment in early layers of vision transformers

## Executive Summary
This study investigates the relationship between convexity in neural network representations and human-machine alignment, motivated by cognitive science theories. Using a graph-based framework to measure convexity and comparing model predictions with human judgments on odd-one-out tasks, the analysis covers pretrained and fine-tuned vision transformer models across multiple layers. Results show a strong positive correlation between convexity and alignment in early layers, but this relationship weakens or becomes negative in later layers for fine-tuned models. Fine-tuning consistently increases convexity but has inconsistent effects on alignment, suggesting a complex interplay between these properties that depends on training objectives and architectural choices.

## Method Summary
The study uses vision transformer models (ViT, BEiT, data2vec) in base and large variants, both pretrained and fine-tuned, to analyze representations from the THINGS dataset. For each layer, latent representations are extracted and used to compute two key metrics: graph convexity score (measuring whether same-class points form convex regions in latent space) and odd-one-out accuracy (comparing model similarity judgments with human judgments). The analysis examines correlation patterns across layers and investigates the causal relationship by applying a naive alignment transformation to improve alignment and measuring its effect on convexity.

## Key Results
- Strong positive correlation (R=0.91) between convexity and alignment in early layers, suggesting convex regions align with human-defined categories
- Fine-tuning increases convexity consistently but has inconsistent effects on alignment across different model types
- Applying alignment transformations increases convexity in pretrained models but not in fine-tuned ones
- Correlation weakens or becomes negative in later layers for fine-tuned models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Convexity and human-machine alignment are positively correlated in early layers because both are optimized for conceptual abstraction rather than task-specific fine-tuning.
- Mechanism: In early layers, the network is still building up general representations. Convexity ensures that similar inputs are grouped together in a way that mirrors human cognitive categories, while alignment ensures that these groupings match human similarity judgments. The dual optimization for abstraction drives both metrics upward.
- Core assumption: The network's early layers are not yet specialized for a downstream task, so both convexity and alignment reflect general perceptual similarity.
- Evidence anchors:
  - [abstract] "Our findings suggest that the convex regions formed in latent spaces of neural networks to some extent align with human-defined categories and reflect the similarity relations humans use in cognitive tasks."
  - [section] "In the first half of the models, which we interpret as the concept built-up phase, the correlation is strong."
  - [corpus] Weak - corpus focuses on concept extraction methods but not on convexity-alignment correlation in early layers.
- Break condition: If early layers are fine-tuned for a specific task, the correlation may break because task-specific features override general similarity structure.

### Mechanism 2
- Claim: Fine-tuning increases convexity but not consistently alignment because it prioritizes classification boundaries over perceptual similarity.
- Mechanism: Fine-tuning sharpens decision boundaries for classification, increasing the convexity of class regions (more compact, separable clusters). However, this sharpening may not preserve the similarity structure humans use, so alignment can drop or stay flat.
- Core assumption: Classification objectives and perceptual similarity are not perfectly aligned; maximizing classification accuracy can distort similarity relations.
- Evidence anchors:
  - [section] "While fine-tuning increases OOOA for ViT, it decreases OOOA for data2vec and has only a marginal impact on OOOA for BEiT."
  - [section] "This indicates a complex interplay between alignment and convexity, which potentially depends on factors such as training objectives."
  - [corpus] Weak - corpus papers discuss concept extraction but not the differential impact of fine-tuning on convexity vs alignment.
- Break condition: If the fine-tuning objective explicitly includes a similarity or alignment loss, the alignment may improve despite increased convexity.

### Mechanism 3
- Claim: Optimizing for alignment via latent space transformations increases convexity because aligning with human similarity judgments often means making representations more perceptually compact.
- Mechanism: The alignment transform (naive transform) adjusts the latent space to maximize agreement with human similarity judgments. This adjustment tends to pull similar examples closer and push dissimilar ones apart, which also increases the convexity of class regions.
- Core assumption: Human similarity judgments are based on perceptual features that naturally form convex regions in the representation space.
- Evidence anchors:
  - [section] "We find that improving the OOOA also leads to higher convexity of representations of the THINGS superclasses in pretrained models."
  - [section] "This relation is strongest in the last layers, but also holds in other layers of the models."
  - [corpus] Weak - corpus does not discuss the effect of alignment transforms on convexity.
- Break condition: If the alignment transform overfits to the specific triplets used for training, it may distort the global geometry and reduce convexity.

## Foundational Learning

- Concept: Graph convexity score
  - Why needed here: It provides a quantitative measure of how well class regions in latent space form convex sets, which is central to the study's hypothesis about alignment with human concepts.
  - Quick check question: If a class region is not convex, what does that imply about the shortest path between two points of the same class?
- Concept: Odd-one-out accuracy (OOOA)
  - Why needed here: It measures how well the model's similarity judgments match human judgments, serving as the empirical alignment metric.
  - Quick check question: What is the upper bound of OOOA and why?
- Concept: Nearest neighbor graph construction
  - Why needed here: It is used to compute the graph convexity score by approximating the manifold structure of the latent space.
  - Quick check question: Why is Euclidean distance used in constructing the nearest neighbor graph for convexity analysis?

## Architecture Onboarding

- Component map: THINGS dataset (images + human triplet judgments) -> Vision transformers (ViT, BEiT, data2vec) in base/large, pretrained/fine-tuned -> Graph convexity score, OOOA metrics -> Naive alignment transform (affine adjustment of latent space)
- Critical path:
  1. Extract latent representations after each transformer layer
  2. Compute OOOA using triplet judgments
  3. Compute graph convexity score using nearest neighbor graph
  4. Correlate OOOA and convexity across layers
  5. Apply naive transform to improve alignment
  6. Recompute convexity to assess causal effect
- Design tradeoffs:
  - Using 10 nearest neighbors for graph convexity balances local structure preservation with computational cost
  - Averaging convexity across classes simplifies analysis but may mask class-specific effects
  - Layer-wise correlation analysis reveals trends but requires many models to generalize
- Failure signatures:
  - High convexity but low alignment suggests the model groups by features humans don't use
  - Low convexity but high alignment suggests the model captures human similarity but in a non-convex way
  - Bell-shaped curves in pretrained models indicate reconstruction objectives affect later layers
- First 3 experiments:
  1. Replicate the correlation analysis on a new model architecture to test generalizability
  2. Apply the naive transform to intermediate layers (not just first/middle/last) to map the full effect
  3. Train a model with a combined loss for convexity and alignment to test causal optimization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific training conditions (architecture, objective, dataset) does convexity correlate most strongly with human-machine alignment?
- Basis in paper: [explicit] "preliminary results in Appendix A.3" show difference between self-supervised and supervised models
- Why unresolved: Current analysis is limited to vision transformers on ImageNet-21k/1k; no systematic exploration of other architectures (e.g., CNNs, diffusion models) or objectives (contrastive learning, masked prediction)
- What evidence would resolve it: Controlled experiments varying architecture, pretraining objective, and dataset while measuring both convexity and alignment across all layers

### Open Question 2
- Question: Is the relationship between convexity and alignment causal, or do they share a common underlying factor in model training?
- Basis in paper: [explicit] "optimizing for human-machine alignment through latent space transformations...can also increase the convexity" but "increasing convexity does not necessarily lead to higher alignment"
- Why unresolved: The paper shows bidirectional effects but cannot distinguish whether convexity causes alignment, alignment causes convexity, or both are influenced by model capacity/complexity
- What evidence would resolve it: Interventions that independently manipulate convexity (e.g., regularization, architectural constraints) and measure alignment effects, and vice versa

### Open Question 3
- Question: How does the convexity-alignment relationship evolve during the training process, and at which stages is it most relevant for generalization?
- Basis in paper: [inferred] "In the studied models, we find the highest human-machine alignment in the middle layers" and "convexity increases consistently for the first half of all evaluated models"
- Why unresolved: Analysis only examines pretrained and fully fine-tuned models, not training dynamics; unclear whether the relationship is most important during early concept formation or later refinement
- What evidence would resolve it: Tracking convexity and alignment metrics throughout training, correlating with validation performance and robustness measures at different stages

## Limitations
- The study is correlational in nature and cannot definitively prove causality between convexity and alignment
- Analysis is limited to vision transformers and the THINGS dataset, raising generalizability concerns
- Convexity metric relies on nearest-neighbor graph approximations that may not fully capture true manifold structure
- Does not investigate potential confounding factors such as batch normalization effects or attention patterns

## Confidence
- **High Confidence**: The positive correlation between convexity and alignment in early layers (R=0.91) is well-supported by the data across multiple models and appears robust. The observation that fine-tuning increases convexity but has inconsistent effects on alignment is clearly demonstrated.
- **Medium Confidence**: The claim that optimizing for alignment via transformation increases convexity in pretrained models is supported but limited to specific layers (first, middle, last). The assertion that this effect does not hold for fine-tuned models is based on fewer experiments and requires more systematic investigation.
- **Low Confidence**: The generalizability of these findings to other architectures and datasets is largely speculative, as the study does not test beyond vision transformers on the THINGS dataset. The interpretation of bell-shaped convexity curves in pretrained models as reflecting reconstruction objectives is plausible but not directly verified.

## Next Checks
1. **Cross-Architecture Validation**: Apply the same convexity-alignment analysis to convolutional neural networks and MLPs trained on standard vision benchmarks to test whether the observed correlation patterns hold across architectural paradigms.

2. **Causal Optimization Experiment**: Train a model with a combined loss function that explicitly optimizes for both convexity (using the graph convexity score as a regularizer) and alignment (using triplet loss on human judgments) to test whether simultaneous optimization improves both metrics and generalization performance.

3. **Class-Specific Analysis**: Perform a detailed analysis of which specific classes or superclasses drive the observed correlations, investigating whether certain semantic categories show stronger convexity-alignment relationships than others, potentially revealing systematic biases in how different concepts are represented.