---
ver: rpa2
title: Notes on Applicability of GPT-4 to Document Understanding
arxiv_id: '2405.18433'
source_url: https://arxiv.org/abs/2405.18433
tags:
- answer
- text
- question
- document
- turbo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the performance of GPT-4 family models on
  Document Visual Question Answering tasks. The authors conduct a systematic comparison
  across four datasets representing different document types (text-intensive, vision-intensive,
  and multi-page).
---

# Notes on Applicability of GPT-4 to Document Understanding

## Quick Facts
- arXiv ID: 2405.18433
- Source URL: https://arxiv.org/abs/2405.18433
- Authors: Łukasz Borchmann
- Reference count: 34
- Primary result: GPT-4 Vision Turbo outperforms text-only models when provided with both OCR-extracted text and document images

## Executive Summary
This paper systematically evaluates GPT-4 family models on Document Visual Question Answering tasks across four datasets representing different document types. The study compares text-only models (GPT-4 8K, GPT-4 32K, GPT-4 Turbo) with vision-enabled models (GPT-4 Vision Turbo) under various configurations including different OCR engines, image resolutions, and prompt variations. Results show that GPT-4 Vision Turbo achieves superior performance when both OCR text and document images are provided as input, while text-only models show signs of possible contamination with dataset-specific training data. The analysis reveals primacy bias in model performance and identifies specific categories where models struggle, providing practical guidance for optimizing document understanding applications.

## Method Summary
The study employs zero-shot evaluation of GPT-4 family models on four Document VQA datasets (DocVQA, InfographicsVQA, SlideVQA, DUDE). Models are tested with text-only input (OCR-extracted text), vision-only input (document images), and combined input (text + images). Performance is optimized through systematic prompt engineering using validation subsets, with comparisons across multiple OCR engines (Tesseract, Azure Cognitive Services, Amazon Textract) and image resolutions (512px, 1024px, 2048px, 4096px). The evaluation uses dataset-specific metrics including ANLS scores and exact match accuracy, followed by error analysis and contamination detection studies.

## Key Results
- GPT-4 Vision Turbo achieves state-of-the-art performance on SlideVQA and DUDE datasets when provided with both OCR text and document images
- Text-only GPT-4 models show contamination effects, with performance improvements of up to 3.8 points when dataset names are included in prompts
- Primacy bias is observed across all models, with highest scores when relevant information appears at the beginning of input documents
- Combined text+vision input consistently outperforms either modality alone for GPT-4 Vision Turbo

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 Vision Turbo achieves superior performance when both OCR-extracted text and document images are provided as input
- Mechanism: Combining visual and textual modalities enables the model to leverage both spatial layout information from images and precise textual content from OCR, compensating for each modality's individual limitations
- Core assumption: The model's architecture can effectively fuse multimodal inputs to enhance understanding beyond what either modality alone provides
- Evidence anchors:
  - [abstract]: "GPT-4 Vision Turbo performs well when one provides both text recognized by an external OCR engine and document images on the input"
  - [section]: "Though we could establish state-of-the-art performance on the SlideVQA and DUDE datasets, results achieved on the well-established task of DocVQA seem poor compared to scores reported in the literature"
  - [corpus]: Weak evidence - corpus contains related document understanding papers but no direct evidence about multimodal fusion mechanisms
- Break condition: If OCR quality degrades significantly or if the model's visual processing capacity becomes saturated with high-resolution images

### Mechanism 2
- Claim: Textual GPT-4 models show signs of contamination with dataset-specific training data
- Mechanism: The presence of dataset names in prompts (guided instruction) improves performance by triggering recall of memorized patterns from training data rather than genuine understanding
- Core assumption: The training corpus included some or all of the evaluation datasets, leading to memorization rather than true generalization
- Evidence anchors:
  - [abstract]: "Evaluation is followed by analyses that suggest possible contamination of textual GPT-4 models"
  - [section]: "Results reported in Table 4 indicate we could increase the performance of text-only models by adding the dataset name to the prompt by up to 3.8 points"
  - [corpus]: Weak evidence - corpus contains contamination-related papers but no direct evidence about this specific phenomenon
- Break condition: If the model performs equally well on truly unseen datasets or if the contamination effect disappears with more diverse training data

### Mechanism 3
- Claim: GPT-4 models exhibit primacy bias, performing better when relevant information appears at the beginning of input documents
- Mechanism: The model's attention mechanism or processing order prioritizes early tokens in the input sequence, leading to better utilization of information presented first
- Core assumption: The model's architecture processes input sequentially or gives disproportionate weight to initial tokens regardless of their actual relevance
- Evidence anchors:
  - [abstract]: "The analysis reveals primacy bias in model performance"
  - [section]: "Results in Figure 1 indicate the primacy bias for all of the models, i.e., achieved scores are highest when relevant information occurs at the beginning of the input"
  - [corpus]: Weak evidence - corpus contains related sequence processing papers but no direct evidence about this specific bias
- Break condition: If the model's performance becomes uniform across different information positions or if architectural changes eliminate the bias

## Foundational Learning

- Concept: Multimodal input processing
  - Why needed here: Understanding how GPT-4 Vision Turbo combines visual and textual information is crucial for optimizing document understanding performance
  - Quick check question: What specific architectural features enable the model to fuse image and text representations effectively?

- Concept: Zero-shot learning and prompt engineering
  - Why needed here: The study relies on prompt variations to optimize performance without fine-tuning, making understanding prompt sensitivity essential
  - Quick check question: How do minor changes in prompt phrasing affect model outputs in zero-shot settings?

- Concept: Document layout analysis
  - Why needed here: Document understanding requires comprehension of spatial relationships and formatting, which differs from plain text processing
  - Quick check question: What document features beyond text content are most critical for accurate question answering?

## Architecture Onboarding

- Component map: Document → OCR + Image → Prompt optimization → Model inference → Evaluation → Error analysis
- Critical path: OCR extraction and image preprocessing → prompt template selection and optimization → model inference with appropriate API calls → metric calculation and error categorization
- Design tradeoffs: Text-only vs. vision-only vs. combined input configurations; cost vs. accuracy tradeoffs between different GPT-4 variants
- Failure signatures: Poor performance with vision-only setup suggests need for OCR integration; performance degradation with lower resolution images indicates visual information importance
- First experiments: 1) Test OCR quality impact by comparing different OCR engines on same documents; 2) Evaluate primacy bias by reordering document content; 3) Verify contamination hypothesis by testing on truly unseen datasets

## Open Questions the Paper Calls Out

- Open Question 1: How does the performance of GPT-4 Vision models vary with different OCR input representations (e.g., plain text vs. structured layout information)?
- Open Question 2: What is the exact mechanism behind the primacy and recency biases observed in model performance across different document locations?
- Open Question 3: How does the contamination analysis methodology account for potential confounding factors like dataset-specific vocabulary or question-answering conventions?
- Open Question 4: What are the performance implications of fine-tuning GPT-4 family models on document understanding tasks compared to the zero-shot approach used in this study?
- Open Question 5: How do different confidence calibration methods compare in terms of reliability and practical utility for document understanding applications?

## Limitations

- Zero-shot evaluation approach may underestimate true model capabilities compared to fine-tuned alternatives
- Cost-efficiency considerations not addressed despite GPT-4 Vision Turbo being the most expensive model variant
- Contamination analysis remains suggestive rather than definitive, lacking direct evidence of training data memorization
- Focus on English-language documents limits generalizability to other languages and document formats

## Confidence

- High confidence: Performance benefits of combining OCR text with images (Mechanism 1), primacy bias findings (Mechanism 3)
- Medium confidence: Contamination hypothesis (Mechanism 2), optimal OCR and resolution recommendations
- Low confidence: Absolute performance rankings across datasets due to potential contamination effects

## Next Checks

1. **Contamination Verification Test**: Run the same evaluation protocol on a completely new, unpublished document VQA dataset to determine if the performance advantages observed with dataset name prompting persist, which would help distinguish between contamination effects and genuine prompt engineering benefits.

2. **Cost-Benefit Analysis**: Measure the actual token costs and inference times for each model variant across different input configurations (text-only, vision-only, text+vision) to provide practical deployment guidance beyond raw accuracy metrics.

3. **Cross-Lingual Generalization**: Test the optimized prompt configurations and model choices on non-English document datasets to assess the true generalization capabilities of the findings beyond the English-language evaluation corpus.