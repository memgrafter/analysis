---
ver: rpa2
title: 'AmazonQAC: A Large-Scale, Naturalistic Query Autocomplete Dataset'
arxiv_id: '2411.04129'
source_url: https://arxiv.org/abs/2411.04129
tags:
- search
- prefix
- term
- context
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces AmazonQAC, a large-scale dataset of 395M real-world
  query autocomplete examples from Amazon Search logs, including user-typed prefixes,
  final search terms, session IDs, and timestamps. This addresses the lack of realistic
  QAC datasets, as previous ones relied on synthetic prefixes.
---

# AmazonQAC: A Large-Scale, Naturalistic Query Autocomplete Dataset

## Quick Facts
- **arXiv ID**: 2411.04129
- **Source URL**: https://arxiv.org/abs/2411.04129
- **Reference count**: 11
- **Primary result**: Introduces AmazonQAC dataset with 395M real-world query autocomplete examples; finetuned LLMs achieve 37% Success@10, half the 69.8% theoretical upper bound

## Executive Summary
This paper introduces AmazonQAC, the first large-scale dataset of real-world query autocomplete (QAC) examples from Amazon Search logs. Unlike previous synthetic datasets, AmazonQAC contains actual user-typed prefixes, final search terms, session IDs, and timestamps, enabling realistic QAC research. The authors evaluate three approaches: prefix trees, semantic retrieval, and LLMs (both few-shot and finetuned), finding that finetuned LLMs incorporating session context perform best. However, even the best system achieves only 37% Success@10, indicating QAC remains a challenging problem. The dataset and baseline implementations are open-sourced to encourage further research.

## Method Summary
The authors created AmazonQAC by filtering Amazon Search logs with regex for PII, removing inappropriate content via LLM, and deduplicating examples. The dataset contains 395M examples of user-typed prefixes and final search terms, along with session IDs and timestamps. Three baseline approaches were evaluated: (1) Prefix Tree - trie-based exact matching with popularity ranking, (2) Semantic Retrieval - ColBERTv2 embeddings with prefix augmentation, and (3) LLM - both few-shot prompting (Mixtral-8x7B) and finetuned (Mistral-7B with LoRA). Models were evaluated using Success@10 and MRR@10 metrics on a 20K test set.

## Key Results
- Finetuned LLMs incorporating session context achieve 37% Success@10, outperforming both prefix trees and semantic retrieval
- 38% of user typing sequences exhibit non-linear patterns (deletions, substitutions, corrections)
- 13% of completions don't match their prefixes, validating need for semantic approaches beyond exact matching
- Best system achieves only 37% Success@10 compared to 69.8% theoretical upper bound

## Why This Works (Mechanism)

### Mechanism 1
Finetuned LLMs with context outperform both prefix trees and semantic retrieval by leveraging generative capacity and session history. The LLM is trained to map (prefix, context) → ranked suggestions, allowing it to capture semantic relationships and personalized patterns that static retrieval methods cannot. Core assumption: The training data distribution adequately represents the patterns needed to generalize to the test set.

### Mechanism 2
Real prefix sequences (not synthetic) enable modeling of non-linear typing patterns and spelling corrections. By providing actual user-typed prefix sequences, the dataset captures how users delete, substitute, and correct while typing, which synthetic methods cannot represent. Core assumption: Users' actual typing behavior is more informative than artificially constructed linear progressions.

### Mechanism 3
Context within a 1-hour window provides perfect information for disambiguation, creating an upper bound for performance. Recent search history reveals user intent, allowing systems to distinguish between ambiguous prefixes (e.g., "iphone" could mean case, repair, or phone). Core assumption: User search sessions exhibit temporal coherence where recent searches inform subsequent queries.

## Foundational Learning

- **Concept**: Query Autocomplete as a recommendation problem rather than pure prefix matching
  - Why needed here: The dataset shows 13% of completions don't match the prefix, requiring semantic understanding
  - Quick check question: Why does a 13% non-prefix match rate invalidate pure prefix-matching approaches?

- **Concept**: Temporal decay of search context relevance
  - Why needed here: The 1-hour window assumption creates the performance upper bound calculation
  - Quick check question: How would extending the context window to 24 hours affect the theoretical upper bound?

- **Concept**: Session-based user behavior modeling
  - Why needed here: Session IDs enable reconstruction of user search contexts that improve prediction accuracy
  - Quick check question: What happens to Success@10 when session context is removed from the finetuned LLM?

## Architecture Onboarding

- **Component map**: Raw logs → regex filtering → LLM filtering → dataset creation → Prefix Tree cache + Semantic Retriever fallback + LLM endpoint → Evaluation metrics
- **Critical path**: User types prefix → system retrieves from prefix tree → if insufficient results, triggers semantic retrieval → LLM used for final suggestions with context
- **Design tradeoffs**: Prefix trees offer speed but limited coverage; LLMs offer coverage but higher latency; semantic retrieval bridges the gap but requires index maintenance
- **Failure signatures**: Low Success@10 on test set indicates either distribution shift or insufficient model capacity; high prefix tree success with low LLM success suggests context isn't being properly utilized
- **First 3 experiments**:
  1. Compare Success@10 of prefix tree vs LLM on prefixes with no session context
  2. Measure impact of context window size (15min, 1hr, 4hr) on LLM performance
  3. Test semantic retriever's contribution by disabling it and measuring coverage loss

## Open Questions the Paper Calls Out

- **Open Question 1**: How does incorporating popularity information directly into the LLM's generation process affect its performance on shorter and more popular prefix/search term pairs?
- **Open Question 2**: How do different semantic retrieval models (beyond ColBERTv2) perform on AmazonQAC, and what impact do they have on handling cases where the final search term doesn't match the prefix?
- **Open Question 3**: How does the performance of QAC systems vary across different user demographics and search contexts beyond what is captured by session history?

## Limitations
- Dataset representativeness limited to single e-commerce platform, may not generalize to other search domains
- Temporal generalization concerns as user search behaviors may evolve over time
- Significant performance gap (37% vs 69.8%) suggests fundamental limitations in current approaches

## Confidence
- **High Confidence**: AmazonQAC dataset contains 395M real-world examples with session context; finetuned LLMs outperform both prefix trees and semantic retrieval methods; approximately 38% of user typing sequences exhibit non-linear patterns
- **Medium Confidence**: 13% of completions don't match their prefixes, validating need for semantic approaches; the 1-hour context window provides sufficient information for disambiguation; session-based context improves LLM performance by 6.2% Success@10
- **Low Confidence**: The theoretical upper bound of 69.8% accurately represents maximum achievable performance; the specific filtering methods used for PII and inappropriate content don't bias the dataset; results generalize to non-e-commerce search scenarios

## Next Checks
1. **Temporal Robustness Test**: Split the dataset into time-based folds (e.g., by month or season) and evaluate whether model performance degrades on later periods compared to earlier ones, indicating potential distribution shift.
2. **Context Window Sensitivity**: Systematically vary the context window size (15 minutes, 1 hour, 4 hours, 24 hours) to determine the optimal temporal scope for session-based predictions and validate the 1-hour assumption.
3. **Domain Transfer Evaluation**: Test the finetuned LLM on query autocomplete datasets from different domains (web search, academic search) to assess generalization beyond e-commerce scenarios.