---
ver: rpa2
title: 'Liar, Liar, Logical Mire: A Benchmark for Suppositional Reasoning in Large
  Language Models'
arxiv_id: '2406.12546'
source_url: https://arxiv.org/abs/2406.12546
tags:
- statement
- truth-teller
- liar
- error
- statements
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Knights and knaves puzzles require logical deduction to determine
  the identities of characters who either always tell the truth or always lie. The
  paper introduces TruthQuest, a benchmark with 2,400 problems of varying complexity
  based on these puzzles.
---

# Liar, Liar, Logical Mire: A Benchmark for Suppositional Reasoning in Large Language Models

## Quick Facts
- arXiv ID: 2406.12546
- Source URL: https://arxiv.org/abs/2406.12546
- Authors: Philipp Mondorf; Barbara Plank
- Reference count: 40
- Models struggle with knights and knaves puzzles, with performance declining as puzzle complexity increases

## Executive Summary
TruthQuest introduces a benchmark of 2,400 knights and knaves puzzles to evaluate large language models' suppositional reasoning abilities. The benchmark tests models' capacity to determine which characters always tell the truth versus always lie based on their statements. Evaluations across three model families show significant performance gaps, with all models struggling particularly on more complex puzzles involving multiple characters and logical relationships.

## Method Summary
The study generates knights and knaves puzzles using a bi-conditional encoding that uniquely constrains solutions. Models are evaluated using zero-shot, few-shot (4-shot, 8-shot), and chain-of-thought prompting on three model families: Llama 2, Llama 3, and Mixtral-8x7B. A regex-based conclusion extractor identifies model responses, with GPT-4 classifying errors into six categories when regex fails. The benchmark includes puzzles of varying complexity (3-6 characters) with different logical statement types.

## Key Results
- All models struggle significantly with knights and knaves puzzles, with performance declining as complexity increases
- Zero-shot prompting performs worse than chain-of-thought prompting across all model families
- Lower-performing models exhibit diverse reasoning errors (misunderstanding truth/lie concepts, logical operators), while higher-performing models mainly struggle with inferring implications of false statements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark isolates suppositional reasoning by requiring models to evaluate truth values of statements before drawing conclusions
- Mechanism: Each puzzle instance is encoded as a conjunction of bi-conditional operators linking character identities to their statements
- Core assumption: The logical form uniquely constrains the solution space so that any valid answer must satisfy the truth-value mapping across all characters
- Evidence anchors:
  - [abstract] "The challenge arises from the truth-telling or lying behavior, which influences the logical implications of each statement"
  - [section] "For a given puzzle with n characters, where P denotes the truth value of a character and Q is the character's logical claim, the puzzle can be expressed as a single conjunction using the bi-conditional operator..."
  - [corpus] Weak: No explicit discussion of bi-conditional encoding in neighbors

### Mechanism 2
- Claim: Error diversity in lower-performing models signals superficial pattern matching rather than genuine logical inference
- Mechanism: When models fail to grasp truth/lie concepts, they reproduce statements incorrectly, assume statements are true, or misunderstand logical operators
- Core assumption: The six error categories capture distinct reasoning failures that can be observed reliably in model outputs
- Evidence anchors:
  - [section] "Lower-performing models exhibit a diverse range of reasoning errors, frequently failing to grasp the concept of truth and lies"
  - [corpus] Moderate: Related works like "HardcoreLogic" and "PHANTOM RECALL" discuss template-based vs reasoning-based solutions

### Mechanism 3
- Claim: More capable models converge on a single failure mode (misunderstanding implications of false statements), indicating partial mastery of the core reasoning task
- Mechanism: Higher-performing models correctly identify truth/lie concepts and reproduce statements accurately, but still fail when required to infer all logical consequences of a false statement
- Core assumption: The "Misunderstanding logical operators" error type is the last barrier after basic concepts are mastered
- Evidence anchors:
  - [abstract] "more proficient models primarily struggle with accurately inferring the logical implications of potentially false statements"
  - [section] "the error distribution obtained through GPT-4 positively correlates with the distribution obtained via manual labeling"
  - [corpus] Weak: No neighbor explicitly discusses the progression from diverse to uniform error patterns

## Foundational Learning

- Concept: Bi-conditional logic (P ⇔ Q)
  - Why needed here: Each puzzle enforces that a character's truth value matches the truth of their statement; this is the core constraint to satisfy
  - Quick check question: If P is true and Q is false, what does P ⇔ Q evaluate to? (Answer: false)

- Concept: Disjunctive Normal Form (DNF) conversion
  - Why needed here: The paper converts the conjunction of bi-conditionals into DNF to enumerate all valid solutions; understanding DNF helps interpret puzzle solutions
  - Quick check question: Given Φ = (P1 ⇔ ¬P3) ∧ P2, what is the DNF form? (Answer: (P1 ∧ ¬P2 ∧ ¬P3) ∨ ... )

- Concept: Chain-of-thought prompting mechanics
  - Why needed here: The paper tests CoT vs zero-shot; understanding how CoT influences intermediate reasoning steps is key to interpreting performance gains
  - Quick check question: What is the main difference between zero-shot and CoT prompting in this study? (Answer: CoT explicitly asks the model to explain reasoning step-by-step before answering)

## Architecture Onboarding

- Component map: Prompt generator -> Model -> Conclusion extractor -> Error classifier -> Evaluator
- Critical path: Input problem -> LLM reasoning -> Parse final identity -> Compare to ground truth -> Record accuracy
- Design tradeoffs: Using regex parsing for conclusions trades robustness for speed; using GPT-4 for error classification trades cost for coverage
- Failure signatures: Regex fails -> fallback to LLM extraction; LLM extraction ambiguous -> manual annotation; error classification inconsistent -> human review
- First 3 experiments:
  1. Run a small subset (e.g., 10 puzzles) through the full pipeline to confirm parsing and evaluation logic
  2. Vary prompt wording (e.g., "knights/knaves" vs "jabbas/tettes") to test terminology invariance
  3. Compare zero-shot vs CoT performance on a balanced set of 3-, 4-, and 5-character puzzles

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the choice of terminology for truth-tellers and liars (e.g., knights/knaves vs. jabbas/tettes) impact model performance when puzzles are present in training data?
- Basis in paper: [explicit] The paper tested different terminology (knights/knaves, truth-tellers/liars, jabbas/tettes) and found no substantial impact on model performance
- Why unresolved: The study only tested scenarios where puzzles were unlikely to be present in training data. It's unclear if performance would differ if models had seen similar instances during training
- What evidence would resolve it: Testing models on instances that are likely to have been present in their training data, using different terminology variants

### Open Question 2
- Question: How would models perform on knights and knaves puzzles with multiple solutions or no solutions?
- Basis in paper: [inferred] The current benchmark only includes puzzles with a single, unique solution. The paper suggests future work could examine the impact of puzzles with multiple or no solutions
- Why unresolved: The study deliberately limited itself to single-solution puzzles to maintain consistency. The behavior of models on more ambiguous problems remains unknown
- What evidence would resolve it: Creating and testing models on knights and knaves puzzles with varying numbers of solutions (0, 1, multiple) and analyzing performance differences

### Open Question 3
- Question: Would more advanced prompting techniques like Tree-of-Thoughts or Graph-of-Thoughts improve model performance on complex knights and knaves puzzles?
- Basis in paper: [explicit] The paper notes that various prompting techniques were tested but suggests future research could explore more advanced methods like Tree-of-Thoughts or Graph-of-Thoughts
- Why unresolved: Only basic prompting techniques (zero-shot, few-shot, chain-of-thought) were evaluated. The potential benefits of more sophisticated prompting strategies remain untested
- What evidence would resolve it: Testing models using Tree-of-Thoughts and Graph-of-Thoughts prompting techniques on the same benchmark and comparing results to basic prompting approaches

## Limitations

- The study uses a relatively small sample size (40 puzzles per complexity level), limiting statistical power
- The bi-conditional encoding mechanism is asserted but not empirically validated for solution uniqueness
- Error classification relies on GPT-4's judgment, introducing potential subjectivity in categorizing reasoning failures

## Confidence

**High Confidence**: The observation that all evaluated models struggle with knights and knaves puzzles, and that zero-shot prompting performs worse than CoT prompting, is well-supported by the experimental results presented.

**Medium Confidence**: The claim that lower-performing models exhibit diverse reasoning errors while higher-performing models converge on a single failure mode is supported by the data but requires careful interpretation of the error classification results.

**Low Confidence**: The assertion that the bi-conditional encoding uniquely constrains the solution space and that this specific logical form is what makes the puzzles challenging is theoretically plausible but not empirically verified in the paper.

## Next Checks

1. **Solution Uniqueness Validation**: Run the puzzle generation algorithm on all 2,400 instances and verify that each has exactly one valid solution through automated constraint checking or SAT solver validation.

2. **Error Classification Reliability**: Conduct a human annotation study on a stratified sample of 100 model outputs to measure inter-rater reliability for the six error categories and compare against GPT-4's classifications.

3. **Complexity Gradient Analysis**: Perform power analysis to determine if the current sample sizes (40 puzzles per condition) are sufficient to detect meaningful performance differences across complexity levels, and if not, generate additional puzzles to achieve adequate statistical power.