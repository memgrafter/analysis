---
ver: rpa2
title: Image-text matching for large-scale book collections
arxiv_id: '2407.19812'
source_url: https://arxiv.org/abs/2407.19812
tags:
- book
- books
- matching
- target
- list
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of mapping books in images to
  entries in a book catalogue through a many-to-many matching process. The authors
  propose a two-stage approach that combines a state-of-the-art segmentation method
  (SAM) to detect book spines, commercial OCR to extract text, and CLIP embeddings
  for initial matching.
---

# Image-text matching for large-scale book collections

## Quick Facts
- arXiv ID: 2407.19812
- Source URL: https://arxiv.org/abs/2407.19812
- Authors: Artemis Llabrés; Arka Ujjal Dey; Dimosthenis Karatzas; Ernest Valveny
- Reference count: 21
- One-line primary result: Hungarian Algorithm achieves highest accuracy (0.641) on library inventory in combined detection-and-matching task

## Executive Summary
This paper addresses the problem of mapping books in images to entries in a book catalogue through a many-to-many matching process. The authors propose a two-stage approach that combines SAM segmentation to detect book spines, commercial OCR to extract text, and CLIP embeddings for initial matching, followed by refinement using either the Hungarian Algorithm or a BERT-based model. They introduce a new dataset of annotated bookshelf images from a Spanish public library and evaluate their methods on both closed-set (15k titles) and open-set (2.3M titles) scenarios. Results show that while the Hungarian method outperforms baselines on the smaller inventory, performance drops significantly with larger catalogues, highlighting scalability challenges.

## Method Summary
The authors present a two-stage matching approach for connecting detected books in images to catalogue entries. First, SAM segmentation detects book spines, commercial OCR extracts text, and CLIP embeddings compute similarity scores between OCR output and catalogue entries. Second, the Hungarian Algorithm or a BERT-based model refines these matches to handle collisions and noisy OCR input. The BERT model is fine-tuned on synthetic corrupted data to predict the best match or a "no match" token. The approach is evaluated on a new dataset of 285 bookshelf images containing 7,536 books, with two target lists: a library inventory of 15k books and a large open-set catalogue of 2.3M titles.

## Key Results
- Hungarian Algorithm achieves highest accuracy (0.641) on library inventory in combined detection-and-matching task
- BERT-based model shows lower accuracy but better scalability than Hungarian method
- Performance drops significantly with larger target lists, with Hungarian achieving only 0.188 accuracy on 2.3M catalogue
- Fuzzy string matching baseline performs worse than both proposed methods across all scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP embeddings enable fast initial filtering by computing cosine similarity between OCR-extracted text and book catalogue entries.
- Mechanism: CLIP's text encoder transforms OCR strings into high-dimensional vectors, which are compared via matrix multiplication to generate a similarity matrix. Each detected book is initially matched to the catalogue entry with the highest similarity score.
- Core assumption: OCR output, though noisy, preserves enough semantic content for CLIP embeddings to capture meaningful similarity to catalogue entries.
- Evidence anchors:
  - [abstract] "CLIP embeddings are used first for fast matching"
  - [section] "By computing the matrix multiplication BT T we obtain a similarity matrix S with shape NB × NT"
  - [corpus] Weak evidence; no direct mentions of CLIP in neighbor papers
- Break condition: If OCR noise is too high, embeddings will fail to align semantically, causing irrelevant matches.

### Mechanism 2
- Claim: Hungarian Algorithm resolves one-to-one assignment conflicts in CLIP's many-to-many output.
- Mechanism: After CLIP's initial filtering, the Hungarian Algorithm solves the assignment problem on the similarity matrix to find a globally optimal one-to-one matching, preventing multiple books from mapping to the same catalogue entry.
- Core assumption: CLIP's similarity scores are sufficiently accurate for the Hungarian Algorithm to produce meaningful refinements.
- Evidence anchors:
  - [abstract] "Hungarian Algorithm or a BERT-based model trained to cope with noisy OCR input and partial text matches"
  - [section] "the Hungarian Algorithm [9], which solves the assignment problem over CLIP's Similarity Matrix S"
  - [corpus] No corpus support for Hungarian Algorithm in this domain
- Break condition: Performance degrades as distractor count increases; Hungarian matching becomes computationally infeasible for large catalogues.

### Mechanism 3
- Claim: BERT-based model handles noisy OCR and detects non-matches by ranking top-K candidates.
- Mechanism: CLIP's similarity matrix filters the top K candidate catalogue entries per book; BERT embeds OCR text alongside these candidates and classifies the best match or outputs a "no match" token if none are suitable.
- Core assumption: Fine-tuning on corrupted synthetic data simulates real OCR noise and partial matches well enough for BERT to generalize.
- Evidence anchors:
  - [abstract] "BERT-based model finetuned to deal with noisy text inputs and partial text matches"
  - [section] "BERT should predict its index in the CLS token. Otherwise, it should predict the extra index that means that the book is not matched with any of the samples."
  - [corpus] No corpus support for BERT-based OCR matching
- Break condition: If synthetic corruption patterns differ significantly from real OCR errors, BERT's predictions degrade.

## Foundational Learning

- Concept: Many-to-many matching vs. retrieval
  - Why needed here: Standard retrieval assigns one query to one result; book spines may appear multiple times and must be matched collectively.
  - Quick check question: In a many-to-many problem, can two different queries map to the same target? (Yes)

- Concept: OCR noise handling
  - Why needed here: Book spines often contain abbreviations, missing characters, or reflections, so matching must tolerate imperfect text.
  - Quick check question: What type of string matching is used as a baseline for OCR-tolerant comparison? (Fuzzy/Levenshtein)

- Concept: Embedding similarity (cosine)
  - Why needed here: CLIP embeddings are compared via cosine similarity to rank book matches efficiently.
  - Quick check question: In a similarity matrix S, what does Si,j represent? (Cosine similarity between book i and catalogue entry j)

## Architecture Onboarding

- Component map: SAM segmentation -> OCR extraction -> CLIP embeddings -> Similarity matrix -> (Hungarian or BERT) refinement -> Final matches
- Critical path: OCR extraction -> CLIP similarity matrix -> Hungarian/BERT second stage
- Design tradeoffs:
  - Hungarian: optimal one-to-one but slow on large catalogues
  - BERT: scalable, handles no-match cases, but lower accuracy
  - CLIP: fast initial filter but prone to collisions
- Failure signatures:
  - High collision rate -> CLIP similarity matrix poorly discriminative
  - All books map to "not in list" -> BERT too conservative or training data mismatch
  - No matches found -> OCR too noisy for embeddings to capture semantics
- First 3 experiments:
  1. Run CLIP-only matching on a small held-out set; measure collision rate.
  2. Apply Hungarian refinement on same set; compare accuracy vs. speed.
  3. Fine-tune BERT on synthetic OCR corruption; evaluate no-match detection on partial spine text.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed BERT-based model compare to the Hungarian Algorithm approach when dealing with partial text matches and noisy OCR input in the matching-only task?
- Basis in paper: [explicit] The paper introduces a BERT-based model trained to handle noisy OCR input and partial text matches, but its performance is worse than the Hungarian Algorithm in the experiments.
- Why unresolved: The paper does not provide a detailed analysis of the BERT-based model's performance in handling partial text matches and noisy OCR input specifically.
- What evidence would resolve it: Conducting a detailed analysis of the BERT-based model's performance in handling partial text matches and noisy OCR input, and comparing it to the Hungarian Algorithm's performance in the same conditions.

### Open Question 2
- Question: What are the limitations of the CLIP embeddings in handling the many-to-many matching problem, and how can they be addressed?
- Basis in paper: [inferred] The paper uses CLIP embeddings as a first stage for fast matching, but mentions that the matching is done independently for each book, which can lead to collisions.
- Why unresolved: The paper does not provide a detailed analysis of the limitations of CLIP embeddings in handling the many-to-many matching problem.
- What evidence would resolve it: Conducting a detailed analysis of the limitations of CLIP embeddings in handling the many-to-many matching problem, and proposing solutions to address these limitations.

### Open Question 3
- Question: How does the performance of the proposed methods scale with the size of the target list, and what are the implications for real-world applications?
- Basis in paper: [explicit] The paper shows that the performance of the Hungarian Algorithm drops significantly with the larger target list of 2.3 million books.
- Why unresolved: The paper does not provide a detailed analysis of the performance of the proposed methods with larger target lists, and the implications for real-world applications.
- What evidence would resolve it: Conducting experiments with larger target lists, and analyzing the performance of the proposed methods and their implications for real-world applications.

## Limitations

- Scalability issues: Accuracy drops dramatically from 0.641 to 0.188 when expanding target list from 15k to 2.3M titles
- Commercial OCR dependency: Specific OCR system used is not specified, affecting reproducibility
- Computational constraints: Hungarian Algorithm becomes infeasible for large catalogues

## Confidence

- High confidence: The two-stage matching framework combining CLIP embeddings with Hungarian or BERT refinement is technically sound and well-implemented
- Medium confidence: The dataset creation process and annotation methodology are clearly described, though the use of weak annotations from SAM segmentation and commercial OCR may introduce noise
- Medium confidence: Performance comparisons between methods are valid, but the significant accuracy drop on larger catalogues raises questions about practical utility
- Low confidence: The scalability claims are undermined by the dramatic performance degradation observed in open-set scenarios

## Next Checks

1. Test the system's robustness to varying OCR quality by deliberately degrading OCR output and measuring matching accuracy degradation
2. Benchmark the computational cost and accuracy tradeoff of the Hungarian Algorithm versus BERT refinement across different catalogue sizes to establish practical limits
3. Evaluate the system's performance on books with partial spine visibility or non-standard formatting to assess real-world applicability beyond the controlled dataset