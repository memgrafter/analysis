---
ver: rpa2
title: On-Device Collaborative Language Modeling via a Mixture of Generalists and
  Specialists
arxiv_id: '2409.13931'
source_url: https://arxiv.org/abs/2409.13931
tags:
- user
- users
- routing
- experts
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CoMiGS, a novel method for on-device collaborative
  fine-tuning of LLMs using a mixture of generalist and specialist experts. The approach
  addresses both system heterogeneity (varying computational resources) and data heterogeneity
  (diverse local data distributions) among users.
---

# On-Device Collaborative Language Modeling via a Mixture of Generalists and Specialists

## Quick Facts
- arXiv ID: 2409.13931
- Source URL: https://arxiv.org/abs/2409.13931
- Authors: Dongyang Fan; Bettina Messmer; Nikita Doikov; Martin Jaggi
- Reference count: 22
- Primary result: CoMiGS achieves superior performance in balancing general and personalized knowledge for on-device collaborative fine-tuning of LLMs

## Executive Summary
This paper introduces CoMiGS, a novel method for on-device collaborative fine-tuning of large language models that addresses both system heterogeneity (varying computational resources) and data heterogeneity (diverse local data distributions) among users. The core innovation is a bi-level optimization formulation with a learnable routing network that dynamically assigns tokens to generalist and specialist experts at fine granularity. The approach allows high-resourced users to have more specialists while keeping generalist experts consistent across users for aggregation. Through extensive experiments on three datasets, CoMiGS demonstrates robust performance in balancing general and personalized knowledge while preventing overfitting.

## Method Summary
CoMiGS implements a mixture of experts (MoE) architecture where a learnable routing network assigns each input token to either generalist or specialist experts. The method uses bi-level optimization where expert parameters are updated using training loss while routing parameters are updated using validation loss. Generalist experts are shared across all users and aggregated during communication rounds, while specialist experts remain local to each user. The framework allows different numbers of specialists per user based on their computational resources, enabling personalized adaptation while maintaining collaborative benefits.

## Key Results
- CoMiGS consistently outperforms state-of-the-art baselines in scenarios with high data heterogeneity and varying computational resources
- The method demonstrates superior balance between general and personalized knowledge while showing robustness against overfitting
- Experimental results across three diverse datasets show effectiveness in both system and data heterogeneity scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The routing mechanism balances personalization and collaboration at the finest granularity by directing tokens to generalist or specialist experts based on their characteristics.
- Mechanism: A learnable routing network assigns each input token to either generalist or specialist experts, allowing different tokens within the same input sequence to receive different types of processing.
- Core assumption: Different tokens within a sequence have varying degrees of need for general vs. specialized knowledge, and a token-level routing approach is more effective than client-level collaboration.
- Evidence anchors:
  - [abstract]: "Central to our work is a learnable routing network that routes at a token level, balancing collaboration and personalization at the finest granularity."
  - [section 1]: "We take a novel approach by addressing it at a token level. Our approach leverages the powerful routing mechanism of MoEs, where the routing results are used to weigh generalist and specialist experts for each input token."
- Break condition: If the routing network fails to learn meaningful distinctions between tokens, or if token-level routing becomes computationally prohibitive for real-time applications.

### Mechanism 2
- Claim: Sharing generalist experts across users while keeping specialists localized addresses both system heterogeneity and data heterogeneity simultaneously.
- Mechanism: Generalist experts are aggregated across all users during communication rounds, providing shared knowledge, while specialist experts remain local to each user, adapting to their specific data distributions.
- Core assumption: There exists common knowledge across users (represented by generalist experts) while also requiring user-specific adaptation (represented by specialist experts).
- Evidence anchors:
  - [abstract]: "Our method shares generalist experts across users while localizing a varying number of specialist experts, thereby adapting to users' computational resources and preserving privacy."
  - [section 3.2]: "Since the objective in (1) is unattainable due to the unavailability of test loss during training, we approximate it using validation loss. We further reformulate our proxy objective as a bi-level optimization problem..."
- Break condition: If the distinction between generalist and specialist knowledge becomes unclear, or if the aggregation of generalist experts fails to converge across users.

### Mechanism 3
- Claim: The alternating update of expert and routing parameters using separate losses prevents overfitting while maintaining strong performance on target distributions.
- Mechanism: Expert parameters are updated using training loss while routing parameters are updated using validation loss, creating a separation between knowledge development and knowledge combination.
- Core assumption: Expert knowledge can be effectively developed from large training sets, while the optimal combination of that knowledge should remain target-specific and learned from validation data.
- Evidence anchors:
  - [section 3.2]: "The routing parameters Φ = {ϕi} are updated based on the validation loss, which reflects the target distribution (outer optimization), while the expert parameters Θ = θG ∪ {θS i } are updated using the training loss (inner optimization)."
  - [section 4.2.2]: "Notably, for out-of-domain tasks such as AG News, generalists play a more significant role early in training. By the end of training, the score distribution stabilizes, with generalists still contributing 30%-50% to next-token predictions..."
- Break condition: If the separation between training and validation sets becomes too small, or if the alternating updates fail to converge properly.

## Foundational Learning

- Concept: Mixture of Experts (MoE) architecture
  - Why needed here: Provides the framework for separating generalist and specialist knowledge while maintaining computational efficiency through conditional computation.
  - Quick check question: How does the routing network decide which expert(s) to activate for a given input token?

- Concept: Federated Learning with heterogeneous participants
  - Why needed here: Enables collaborative learning across devices with different computational resources and data distributions while preserving privacy.
  - Quick check question: What challenges arise when aggregating model parameters from participants with different model architectures?

- Concept: Bi-level optimization
  - Why needed here: Allows separate optimization of expert parameters (using training data) and routing parameters (using validation data), creating a principled approach to balancing personalization and collaboration.
  - Quick check question: What is the difference between the inner and outer optimization problems in a bi-level optimization framework?

## Architecture Onboarding

- Component map: Base LLM (GPT-2) with LoRA modules -> Routing network (one-layer MLP) -> Expert selection -> Parameter update -> Server aggregation
- Critical path: Token → Routing network → Expert selection → Parameter update → Aggregation
- Design tradeoffs:
  - More specialists provide better personalization but increase computational cost
  - More frequent routing updates improve adaptation but increase communication overhead
  - Larger validation sets improve routing accuracy but reduce training data
- Failure signatures:
  - Routing network collapses to always selecting the same expert
  - Generalist experts fail to converge across users
  - Overfitting on local data when specialists outnumber data complexity
- First 3 experiments:
  1. Compare performance with only generalists vs. only specialists to quantify their individual contributions
  2. Test routing effectiveness by analyzing token assignment patterns across different layers
  3. Evaluate resource adaptation by varying the number of specialists per user relative to their data size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CoMiGS scale when multiple generalist experts are used instead of just one?
- Basis in paper: [inferred] The paper states "While our paper focused on a single generalist, it is possible to include multiple generalists, and their impact on performance remains to be seen."
- Why unresolved: The paper only experiments with one generalist expert, leaving the effects of multiple generalists unexplored.
- What evidence would resolve it: Comparative experiments showing test perplexity and routing behavior with 1, 2, 3+ generalist experts while keeping specialist numbers constant.

### Open Question 2
- Question: How does CoMiGS perform when applied to collaborative instruction tuning of larger LLMs (e.g., 7B+ parameters) and evaluated on downstream tasks?
- Basis in paper: [explicit] The paper states "An interesting future direction to explore is adopting our framework for collaborative instruction tuning of larger LLMs and evaluating its performance on downstream tasks."
- Why unresolved: All experiments use a small 124M parameter GPT-2 model; scaling to larger models and task-specific evaluation is unexplored.
- What evidence would resolve it: Fine-tuning experiments on 1B+ parameter models with instruction tuning data, followed by evaluation on standard benchmarks (e.g., SuperGLUE, BBH).

### Open Question 3
- Question: What is the optimal routing update frequency (τ) and number of update steps for balancing communication efficiency and model performance?
- Basis in paper: [explicit] The paper states "Since the updates of Θ and Φ are disentangled, they do not need to be updated at the same frequency" and shows one sweep result, but doesn't provide a definitive recommendation.
- Why unresolved: The paper only sweeps over a limited range of routing update frequencies and steps, and only reports results for one dataset.
- What evidence would resolve it: Comprehensive sweeps across all three datasets with different routing update periods and step counts, showing the trade-off between test perplexity and communication rounds.

## Limitations
- The token-level routing mechanism's computational overhead is not fully quantified for practical deployment
- The method relies on having small validation sets locally, which may not be realistic in all scenarios
- Experiments focus on relatively clean, pre-defined dataset splits, potentially underestimating real-world performance degradation

## Confidence
**High Confidence**: The core claim that sharing generalist experts while localizing specialists can effectively balance collaboration and personalization is well-supported by experimental results.

**Medium Confidence**: The effectiveness of the token-level routing mechanism and bi-level optimization formulation, while theoretically sound, have weaker empirical support.

**Low Confidence**: The scalability claims regarding real-world deployment on heterogeneous devices are not thoroughly validated.

## Next Checks
1. **Routing Effectiveness Analysis**: Conduct detailed analysis of routing network behavior by visualizing token assignment patterns across different layers and input types, comparing token-level routing against client-level routing on the same experimental setup.

2. **Robustness to Data Scarcity**: Evaluate performance when local validation sets are extremely small or unavailable, testing alternative approaches for routing parameter updates such as using global validation metrics or unsupervised routing objectives.

3. **Real-World Deployment Simulation**: Create a simulation environment including realistic challenges such as device failures, network latency, and dynamic resource allocation, measuring performance under these conditions and comparing against simpler collaborative fine-tuning approaches.