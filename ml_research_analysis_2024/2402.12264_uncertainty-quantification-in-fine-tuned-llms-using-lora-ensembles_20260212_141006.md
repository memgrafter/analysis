---
ver: rpa2
title: Uncertainty quantification in fine-tuned LLMs using LoRA ensembles
arxiv_id: '2402.12264'
source_url: https://arxiv.org/abs/2402.12264
tags:
- uncertainty
- mmlu
- fine-tuning
- arxiv
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops principled uncertainty quantification for fine-tuned
  large language models using ensembles of low-rank adapters (LoRA). The method derives
  Bayesian posterior approximations by training multiple LoRA members, each adapting
  a pre-trained Mistral-7B model with different low-rank matrices added to attention
  layers.
---

# Uncertainty quantification in fine-tuned LLMs using LoRA ensembles

## Quick Facts
- **arXiv ID**: 2402.12264
- **Source URL**: https://arxiv.org/abs/2402.12264
- **Reference count**: 22
- **Primary result**: LoRA ensembles provide computationally efficient Bayesian posterior approximations for uncertainty quantification in fine-tuned LLMs

## Executive Summary
This paper develops principled uncertainty quantification for fine-tuned large language models using ensembles of low-rank adapters (LoRA). The method derives Bayesian posterior approximations by training multiple LoRA members, each adapting a pre-trained Mistral-7B model with different low-rank matrices added to attention layers. These ensembles are then used to compute epistemic uncertainty via mutual information and total uncertainty via predictive entropy across three multiple-choice QA datasets (CommonsenseQA, MMLU STEM, MMLU Social Studies). The key findings show that LoRA ensembles converge quickly—ensembles of size 5 provide similar uncertainty estimates to size 20—while remaining computationally efficient. During fine-tuning, the model becomes increasingly confident on correct answers, even in the overfitting regime, while maintaining higher uncertainty on incorrect predictions. This allows for filtering of uncertain (and often wrong) predictions at inference time. The method also reveals domain-specific differences: MMLU STEM questions remain consistently more uncertain ("known-uncertain") compared to MMLU Social Studies, which shows faster convergence to confident predictions. Overall, the approach provides a systematic way to monitor and interpret how fine-tuning reshapes model knowledge, improving trustworthiness and enabling better control over model behavior.

## Method Summary
The method uses LoRA ensembles to approximate the Bayesian posterior over fine-tuned model parameters. Multiple LoRA adapters are trained independently on the same dataset, each capturing a different point estimate in parameter space. The ensemble's predictive distribution is computed by averaging softmax outputs across all members, and uncertainty is quantified using predictive entropy (total uncertainty) and mutual information (epistemic uncertainty). The approach is applied to fine-tune Mistral-7B on CommonsenseQA, with validation on multiple-choice QA datasets. Key hyperparameters include LoRA rank r=8, ensemble size M=5, and L2 regularization λ/2=1. The method is evaluated using accuracy, NLL loss, ECE, and uncertainty dynamics through two-dimensional histograms of MI versus entropy.

## Key Results
- LoRA ensembles of size 5 provide comparable uncertainty estimates to ensembles of size 20, enabling computationally efficient uncertainty quantification
- During fine-tuning, correct answers become increasingly confident while incorrect predictions maintain higher mutual information, enabling filtering of uncertain (likely wrong) predictions
- MMLU STEM questions remain consistently more uncertain ("known-uncertain") compared to MMLU Social Studies, which shows faster convergence to confident predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LoRA ensembles provide a computationally efficient approximation to the Bayesian posterior over fine-tuned model parameters
- Mechanism: By training multiple LoRA adapters independently, each capturing a different point estimate in parameter space, the ensemble approximates a multimodal posterior. The diversity among ensemble members reflects epistemic uncertainty, and averaging predictions reduces overconfident errors
- Core assumption: The LoRA-adapted parameters, when ensembled, span a subspace rich enough to represent the posterior's modes without requiring full Bayesian sampling
- Evidence anchors:
  - [abstract] "We derive principled uncertainty quantification for fine-tuned LLMs with posterior approximations using computationally efficient low-rank adaptation ensembles."
  - [section 5.2] "We use LoRA ensembles to approximate the posterior p(θ|Dfine-tune)."
  - [corpus] Weak/no direct evidence that LoRA ensembles match full posterior sampling; stated as approximation
- Break condition: If the posterior is highly concentrated in a low-dimensional subspace not captured by LoRA ranks, or if ensemble diversity collapses due to insufficient regularization

### Mechanism 2
- Claim: Epistemic uncertainty (mutual information) distinguishes correctly answered questions from incorrect ones, even in the overfitting regime
- Mechanism: During fine-tuning, correct answers become increasingly confident (low entropy, low MI), while incorrect answers retain high MI, signaling the model's uncertainty about its mistakes. This separation allows filtering out uncertain (and likely wrong) predictions at inference
- Evidence anchors:
  - [abstract] "The model becomes increasingly confident on correct answers, even in the overfitting regime, while maintaining higher uncertainty on incorrect predictions."
  - [section 7.4] "almost all of these incorrect predictions are marked by high mutual information, placing them in the 'unknown' regime."
  - [corpus] No explicit experimental comparison to alternative uncertainty metrics; claim inferred from MI trends
- Break condition: If incorrect predictions start showing low MI due to memorization or dataset bias, or if MI becomes uninformative in highly overfitted regimes

### Mechanism 3
- Claim: LoRA ensemble size can be small (e.g., 5 members) without sacrificing uncertainty quality, reducing computational cost
- Mechanism: The loss curves and uncertainty metrics for ensembles of size 5, 10, and 20 converge closely, indicating that a small ensemble suffices to capture the essential posterior modes and their epistemic uncertainty
- Evidence anchors:
  - [section 7.3] "the loss curves for ensemble sizes M = 5, 10, and 20 converge closely, indicating that small ensembles (M = 5) provide posterior approximations comparable to those of larger ensembles (M = 20)."
  - [section 7.4] "Based on this observation, we use M = 5 ensembles for uncertainty quantification in the following section."
  - [corpus] No ablation on even smaller ensembles; assumption that 5 is near-optimal
- Break condition: If domain complexity or task difficulty increases such that more ensemble members are needed to capture diverse modes

## Foundational Learning

- Concept: Bayesian posterior inference and the role of ensembles in approximating it
  - Why needed here: The paper frames LoRA ensembles as a posterior approximation method, so understanding Bayesian inference and how ensembles can approximate multimodal posteriors is essential
  - Quick check question: In Bayesian inference, what does the posterior distribution represent, and how can an ensemble of models approximate it?

- Concept: Epistemic vs. aleatoric uncertainty and their quantification via entropy and mutual information
  - Why needed here: The paper uses these two metrics to analyze model behavior during fine-tuning; understanding their distinction and calculation is critical
  - Quick check question: What is the difference between predictive entropy and mutual information, and which captures epistemic uncertainty?

- Concept: Low-rank adaptation (LoRA) and its role in parameter-efficient fine-tuning
  - Why needed here: LoRA is the backbone of the proposed method; understanding how it modifies model weights with low-rank matrices is necessary to follow the experiments
  - Quick check question: How does LoRA decompose weight updates, and why does this reduce the number of trainable parameters?

## Architecture Onboarding

- Component map: Pre-trained Mistral-7B model -> LoRA adapters -> Ensemble manager -> Uncertainty calculator -> Dataset pipeline -> Evaluation module

- Critical path: 1) Initialize pre-trained model and LoRA adapters 2) Train each LoRA adapter independently on CQA training data 3) Collect predictions from all ensemble members on validation data 4) Compute average predictive distribution and mutual information 5) Calculate uncertainty metrics and evaluate model behavior

- Design tradeoffs:
  - LoRA rank (r=8 chosen) vs. adapter expressiveness: higher rank increases capacity but also computation
  - Ensemble size (M=5 chosen) vs. uncertainty quality: larger ensembles improve approximation but increase cost
  - Prior variance (λ=1 chosen) vs. adaptation flexibility: higher variance allows more deviation from pre-trained weights

- Failure signatures:
  - Low ensemble diversity: all members converge to similar predictions → MI collapses → epistemic uncertainty underestimated
  - Over-regularization: high λ forces adapters toward zero → little adaptation → poor performance
  - Under-regularization: low λ allows large deviations → overfitting and overconfident incorrect predictions

- First 3 experiments:
  1. Train a single LoRA adapter on CQA and evaluate its accuracy, entropy, and MI on validation data
  2. Train an ensemble of 5 LoRA adapters, compare loss curves and uncertainty metrics to the single adapter
  3. Vary ensemble size (M=1, 5, 10, 20) and measure convergence of uncertainty metrics and computational cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prior variances (λ⁻¹) affect the epistemic uncertainty estimates during fine-tuning, particularly in the overfitting regime?
- Basis in paper: [explicit] Section 5.3 discusses the selection of prior variance as a hyperparameter affecting regularization and model adaptation
- Why unresolved: The paper only explores λ/2 = 1 as optimal and doesn't systematically investigate how different prior variances influence uncertainty quantification across datasets
- What evidence would resolve it: Systematic ablation studies showing how varying prior variances affects epistemic uncertainty, calibration error, and the balance between retained and acquired knowledge across multiple fine-tuning scenarios

### Open Question 2
- Question: How does the LoRA ensemble posterior approximation compare quantitatively to HMC-derived posteriors for uncertainty estimation in fine-tuned LLMs?
- Basis in paper: [explicit] Section 5.4 explicitly acknowledges that HMC is the gold standard but cannot be run due to computational constraints, preventing direct comparison
- Why unresolved: The paper argues LoRA ensembles are effective but lacks quantitative validation against the theoretically ideal HMC posterior
- What evidence would resolve it: Direct comparison using smaller-scale models where HMC is tractable, measuring agreement, variance, and calibration metrics between LoRA ensembles and HMC posteriors

### Open Question 3
- Question: Does the observed retention of acquired knowledge during overfitting generalize across different fine-tuning objectives and model architectures beyond Mistral-7B and multiple-choice QA?
- Basis in paper: [explicit] Section 7.3 identifies unexpected retention of acquired knowledge during overfitting as a key finding but only tests on specific datasets and model
- Why unresolved: The paper's findings are limited to one model architecture and task type, leaving generalization uncertain
- What evidence would resolve it: Replication studies across diverse model families (different sizes, architectures), task types (generation, classification), and datasets to test the robustness of knowledge retention patterns during overfitting

## Limitations

- The uncertainty quantification approach relies on LoRA ensembles as a proxy for Bayesian posterior inference, but this approximation may not fully capture the true posterior, particularly in highly non-linear parameter spaces or when the LoRA rank is too low to represent critical modes
- The study focuses on multiple-choice QA tasks with fixed answer choices, which may not generalize to open-ended generation or tasks requiring nuanced reasoning
- The analysis assumes that ensemble diversity is maintained through independent training, but there's no explicit regularization to ensure diversity, potentially leading to collapsed posteriors in some settings

## Confidence

- **High Confidence**: The empirical findings about ensemble size requirements (M=5 sufficient) and the observed trend of increasing confidence on correct answers during fine-tuning are well-supported by the experimental data presented
- **Medium Confidence**: The mechanism linking LoRA ensemble diversity to epistemic uncertainty approximation is theoretically sound but lacks direct comparison to gold-standard Bayesian methods like HMC or SGLD
- **Medium Confidence**: The domain-specific uncertainty patterns (STEM vs Social Studies) are observed but may be influenced by dataset-specific factors not fully controlled for in the analysis

## Next Checks

1. Compare LoRA ensemble uncertainty estimates against Hamiltonian Monte Carlo or Stochastic Gradient Langevin Dynamics on a smaller model to validate the posterior approximation quality
2. Test the approach on open-ended generation tasks (e.g., summarization) to assess generalization beyond multiple-choice settings
3. Implement explicit ensemble diversity regularization (e.g., variance-based loss) and measure its impact on uncertainty calibration and ensemble performance