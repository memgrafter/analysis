---
ver: rpa2
title: Pseudo-Label Calibration Semi-supervised Multi-Modal Entity Alignment
arxiv_id: '2403.01203'
source_url: https://arxiv.org/abs/2403.01203
tags:
- alignment
- entity
- learning
- multi-modal
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multi-modal entity alignment (MMEA) for integrating
  multi-modal knowledge graphs. It introduces PCMEA, a pseudo-label calibration semi-supervised
  framework that extracts diverse embeddings, filters modal-specific noise via mutual
  information maximization, and leverages momentum-based contrastive learning with
  calibrated pseudo-labels.
---

# Pseudo-Label Calibration Semi-supervised Multi-Modal Entity Alignment

## Quick Facts
- arXiv ID: 2403.01203
- Source URL: https://arxiv.org/abs/2403.01203
- Reference count: 10
- Primary result: PCMEA achieves 6.76%-8.20% Hits@1 and 7.28%-8.58% MRR improvements over state-of-the-art baselines

## Executive Summary
This paper introduces PCMEA, a pseudo-label calibration semi-supervised framework for multi-modal entity alignment (MMEA) in multi-modal knowledge graphs. The framework extracts diverse embeddings from multiple modalities, filters modal-specific noise through mutual information maximization, and leverages momentum-based contrastive learning with calibrated pseudo-labels. Experiments on FB15K-DB15K and FB15K-YAGO15K datasets demonstrate significant performance improvements over existing baselines, particularly in low-label scenarios.

## Method Summary
PCMEA combines attention-guided multi-modal embedding extraction with mutual information maximization and momentum-based contrastive learning with pseudo-label calibration. The framework uses GAT for structural information, BOW/PLM for relations and attributes, and PVM for images, then fuses these through attention mechanisms. Mutual information maximization filters modal-specific noise while preserving cross-modal alignment signals. The momentum-based contrastive learning with calibrated pseudo-labels improves alignment by pulling aligned entities closer while reducing error propagation through selective incorporation of reliable pseudo-labels.

## Key Results
- PCMEA achieves 6.76%-8.20% Hits@1 improvement over state-of-the-art baselines
- MRR improvements of 7.28%-8.58% across different labeled data proportions
- 9.04%-23.13% relative improvement over strongest baseline MCLEA
- Ablation studies confirm effectiveness of cross-modal alignment, mutual information enhancement, and pseudo-label calibration

## Why This Works (Mechanism)

### Mechanism 1: Mutual Information Maximization
- Claim: Filters modal-specific noise while preserving cross-modal alignment signals
- Core assumption: Modal-specific noise is largely independent from cross-modal alignment signals
- Evidence anchors: Abstract states mutual information maximization "filters the modal-specific noise and augments modal-invariant commonality"
- Break condition: If modal-specific signals contain useful alignment information, this mechanism would incorrectly suppress them

### Mechanism 2: Momentum-based Contrastive Learning with Pseudo-label Calibration
- Claim: Improves alignment by pulling aligned entities closer while reducing error propagation
- Core assumption: Error propagation can be controlled through selective incorporation of pseudo-labels
- Evidence anchors: Abstract states combining pseudo-label calibration with momentum-based contrastive learning "improves the quality of pseudo-label and pulls aligned entities closer"
- Break condition: If pseudo-label calibration becomes too conservative, it may exclude useful training signals

### Mechanism 3: Cross-modal Knowledge Transfer
- Claim: Cross-modal alignment improves uni-modal representations
- Core assumption: Cross-modal alignment provides useful signals for improving individual modality representations
- Evidence anchors: Paper states minimizing Align Loss "reduces the difference between uni-modality and joint modality and realizes knowledge transfer"
- Break condition: If modalities are already well-aligned, additional alignment loss may introduce noise

## Foundational Learning

- **Knowledge Graph Embeddings**: Why needed - entire framework operates on embeddings of entities from multi-modal knowledge graphs. Quick check - What are key differences between structure-based and pre-trained language model embeddings for knowledge graph entities?

- **Mutual Information Estimation**: Why needed - framework uses MINE to maximize mutual information between modalities. Quick check - How does MINE differ from other mutual information estimation methods like Donsker-Varadhan?

- **Contrastive Learning**: Why needed - framework uses momentum-based contrastive learning to pull aligned entities closer. Quick check - What is the role of the momentum encoder in preventing representation collapse?

## Architecture Onboarding

- **Component map**: Encoder modules (GAT for structure, BOW/PLM for relations/attributes, PVM for images) → Attention weights → Joint embedding → MI maximization → Contrastive learning with pseudo-label calibration
- **Critical path**: Input → Multi-modal embeddings → Joint embedding → Loss computation (AL + MI + CL) → Parameter updates
- **Design tradeoffs**: Simplicity vs. complexity (weighted concatenation vs. sophisticated fusion), semi-supervised benefits vs. potential error propagation
- **Failure signatures**: Poor performance on image-heavy datasets, overfitting with limited labels, degradation when modalities have low correlation
- **First 3 experiments**:
  1. Test ablation removing MI maximization to verify its contribution to performance
  2. Test different momentum coefficients (κ) to find optimal stability-speed tradeoff
  3. Test the effect of pseudo-label calibration timing on early vs. late training performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PCMEA's performance scale with increasing size and complexity of multi-modal knowledge graphs beyond current datasets?
- Basis: Paper mentions experimental results on two benchmark datasets but doesn't explore performance on larger or more complex knowledge graphs
- Resolution needed: Experiments on larger datasets or synthetic datasets with varying complexity would provide insights into scalability

### Open Question 2
- Question: What is the impact of incorporating additional modalities (e.g., audio, video) on PCMEA's effectiveness?
- Basis: Paper focuses on visual, structural, relational, and attribute features but doesn't explore integration of other modalities
- Resolution needed: Testing PCMEA with datasets including additional modalities would demonstrate framework's adaptability

### Open Question 3
- Question: How does choice of pre-trained language models affect PCMEA performance in different languages or domains?
- Basis: Paper mentions using different pre-trained language models and notes they affect entity alignment to some extent
- Resolution needed: Experiments using models trained on different languages or specific domains would clarify influence on cross-lingual or domain-specific tasks

## Limitations

- Implementation details for mutual information estimator MINE and its integration with contrastive learning framework remain unspecified
- Limited discussion of failure cases and modality imbalance effects on performance
- No detailed ablation studies on individual contribution of each mechanism to overall performance

## Confidence

- **High confidence** in basic architecture description and reported experimental results
- **Medium confidence** in specific mechanisms, particularly mutual information maximization and pseudo-label calibration due to sparse implementation details
- **Low confidence** in scalability claims as no experiments were conducted on larger or more complex knowledge graphs

## Next Checks

1. Verify contribution of MI maximization by testing performance with and without this component across different dataset proportions
2. Experiment with different momentum coefficients (κ) to identify optimal stability-speed tradeoff for contrastive learning
3. Test pseudo-label calibration timing (early vs. late training incorporation) to optimize error propagation control