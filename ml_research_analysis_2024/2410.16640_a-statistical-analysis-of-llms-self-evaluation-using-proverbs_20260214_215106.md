---
ver: rpa2
title: A Statistical Analysis of LLMs' Self-Evaluation Using Proverbs
arxiv_id: '2410.16640'
source_url: https://arxiv.org/abs/2410.16640
tags:
- proverb
- does
- llms
- what
- mean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates the reliability of LLMs\u2019 self-evaluation\
  \ using a novel proverb reasoning task. The authors introduce a dataset of 600 proverbs\
  \ across three topics (gender, wisdom, society), with each proverb paired with a\
  \ semantically similar variant."
---

# A Statistical Analysis of LLMs' Self-Evaluation Using Proverbs

## Quick Facts
- arXiv ID: 2410.16640
- Source URL: https://arxiv.org/abs/2410.16640
- Reference count: 40
- This study introduces a novel method combining textual and numerical consistency analysis to evaluate LLMs' self-assessment reliability, successfully identifying failures in culturally nuanced content evaluation.

## Executive Summary
This study investigates the reliability of LLMs' self-evaluation using a novel proverb reasoning task. The authors introduce a dataset of 600 proverbs across three topics (gender, wisdom, society), with each proverb paired with a semantically similar variant. They propose a method combining textual consistency analysis using natural language inference and numerical consistency evaluation via the Siegel-Tukey statistical test. Their approach successfully identifies failures in LLMs' self-evaluation, revealing issues such as gender stereotyping, cultural misunderstandings, and lack of common-sense reasoning.

## Method Summary
The authors construct proverb pairs with similar intent but different wording across three topics (gender, wisdom, society). They generate 5 responses per proverb with scores (1-10) using multiple LLMs (ChatGPT, GPT-4, Claude-3, Llama-3). The method calculates textual consistency using DeBERTa-v3-large NLI model and applies Siegel-Tukey test to detect inconsistencies between score distributions and textual consistency scores across proverb pairs.

## Key Results
- The proposed method achieves low error rates (0-6%) across multiple LLMs for detecting scoring failures
- LLMs demonstrate gender stereotyping, with feminine terms leading to poorer understanding of proverb intent
- Cultural understanding limitations are revealed, with LLMs better comprehending popular English proverbs than their translated counterparts from other cultures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The combination of textual consistency via NLI and numerical consistency via Siegel-Tukey test can identify scoring failures where similar proverb prompts should yield consistent evaluations.
- Mechanism: The method constructs paired proverb prompts with similar intent but different wording. It then uses NLI to measure semantic similarity of responses and Siegel-Tukey test to detect variance in numerical scores. When the null hypothesis is rejected (indicating significant variance), the method flags potential scoring failures.
- Core assumption: Scoring failures are more likely when there are significant differences in either the variance of numerical scores or the variance of textual consistency scores between proverb pairs with similar meaning.
- Evidence anchors:
  - [abstract] "We propose tests to evaluate textual consistencies as well as numerical consistencies across similar proverbs"
  - [section 3.2] "We prefer to find a relationship between textual consistency and scoring failure"
  - [corpus] "Found 25 related papers (using 8)" - Weak corpus support for this specific mechanism
- Break condition: If the LLMs generate responses that are superficially different but semantically equivalent, the textual consistency measure may not detect the inconsistency, leading to false negatives.

### Mechanism 2
- Claim: Gender-specific word substitutions in proverb pairs can reveal gender stereotyping biases in LLMs.
- Mechanism: By constructing proverb pairs that differ only in gender terms (e.g., "man" vs "woman"), the method tests whether LLMs apply consistent reasoning across genders. Inconsistent evaluations or reasoning indicate gender bias.
- Core assumption: If an LLM has gender bias, it will produce different evaluations or reasoning quality when gender terms are swapped, even when the underlying meaning should remain constant.
- Evidence anchors:
  - [section 2] "For the topic of gender, we construct proverb pairs by changing all the gender words"
  - [section 4.1] "when the gender words were feminine, the intent of the proverb was seldom understood"
  - [corpus] Weak support - no directly related papers found
- Break condition: If the LLM recognizes gender terms but doesn't let them influence reasoning quality, this mechanism may not detect bias effectively.

### Mechanism 3
- Claim: Cross-cultural proverb pairs can reveal cultural understanding limitations in LLMs.
- Mechanism: The method uses proverb pairs from different cultures that convey similar meanings. Inconsistent evaluations or reasoning across these pairs indicate cultural bias or misunderstanding.
- Core assumption: LLMs should recognize and apply similar reasoning to proverb pairs from different cultures that convey the same underlying message.
- Evidence anchors:
  - [section 2] "For the topic on society, in order to examine if LLMs can understand cultural differences, we choose proverb pairs across different cultures"
  - [section 4.1] "meaning of proverbs across cultures are not necessarily well captured—popular ones in English are understood, but their English translated counterparts from other cultures are not necessarily understood correctly"
  - [corpus] "MasalBench: A Benchmark for Contextual and Cross-Cultural Understanding of Persian Proverbs in LLMs" provides some support
- Break condition: If the LLM has been primarily trained on Western cultural content, it may consistently fail to understand non-Western proverbs, making the method less effective at identifying specific failures.

## Foundational Learning

- Concept: Natural Language Inference (NLI)
  - Why needed here: NLI is used to measure textual consistency between responses to similar proverb prompts by determining if one response entails another.
  - Quick check question: Can you explain how an NLI model determines if one statement entails another, and why this is useful for measuring consistency in LLM responses?

- Concept: Non-parametric statistical tests (Siegel-Tukey)
  - Why needed here: Siegel-Tukey test is used to detect differences in variance between score distributions for similar proverb pairs, which indicates scoring failures.
  - Quick check question: What is the difference between parametric and non-parametric tests, and why might Siegel-Tukey be preferred over a t-test in this context?

- Concept: Proverb semantics and cultural context
  - Why needed here: Understanding how proverbs convey cultural wisdom and how their meaning can be preserved across linguistic variations is essential for constructing valid test pairs.
  - Quick check question: How would you construct two proverb pairs that convey the same meaning but use different cultural references, and why is this important for the test methodology?

## Architecture Onboarding

- Component map: Data generation (prompt templates → LLM API calls) → Response collection (5 responses + scores per prompt) → Consistency analysis (NLI scoring + Siegel-Tukey testing) → Error detection and manual validation
- Critical path: Prompt construction → LLM response generation → NLI consistency scoring → Siegel-Tukey statistical testing → Manual validation of flagged pairs
- Design tradeoffs: Using only 5 responses per prompt limits statistical power but reduces computational cost; manual proverb pair construction ensures quality but doesn't scale; the Siegel-Tukey test is distribution-free but may be less powerful than parametric alternatives
- Failure signatures: High error rates in specific topics (gender, wisdom, society) indicate LLM weaknesses; consistent rejection of null hypothesis across proverb pairs suggests systematic bias; low inter-annotator agreement indicates ambiguous evaluation criteria
- First 3 experiments:
  1. Run the complete pipeline on a small subset (10 proverb pairs) with one LLM to verify all components work end-to-end
  2. Test the sensitivity of the NLI consistency measure by using paraphrased versions of the same response to establish baseline consistency
  3. Verify the Siegel-Tukey test implementation by generating synthetic score distributions with known variance differences to confirm it correctly rejects the null hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Siegal-Tukey test detect all types of scoring failures across different numerical ranges beyond 1-10?
- Basis in paper: [explicit] The authors mention that when the score range was changed to 1-3, they observed highly skewed patterns and found it ambiguous for manual evaluation, suggesting the need to examine other score ranges.
- Why unresolved: The study only tested fixed ranges (1-10) and found issues with 1-3 range, but did not rigorously examine intermediate ranges or different ranges for various LLM models.
- What evidence would resolve it: Testing the Siegal-Tukey method across multiple score ranges (e.g., 1-5, 1-7, 1-3) with different LLM models and comparing error rates would show the robustness of the test across numerical scales.

### Open Question 2
- Question: How does cultural diversity in proverb selection affect the consistency detection capabilities of the proposed method?
- Basis in paper: [explicit] The authors note that LLMs better understand popular English proverbs but struggle with their translated counterparts from other cultures, suggesting cultural bias affects performance.
- Why unresolved: The study used a limited set of proverbs from specific cultures and did not systematically test how cultural diversity in the dataset affects error detection rates across different LLM models.
- What evidence would resolve it: Creating proverb datasets with varying levels of cultural diversity (monolingual vs. multilingual, same culture vs. cross-cultural) and testing consistency detection rates across different LLM models would reveal how cultural representation impacts the method's effectiveness.

### Open Question 3
- Question: What is the optimal number of responses and textual consistency samples needed to reliably detect scoring failures?
- Basis in paper: [explicit] The authors state that the study focused on analyzing fixed numbers of responses (5) and scores (5), and did not explore whether different sample sizes would affect detection accuracy.
- Why unresolved: The study used a fixed sample size of 5 responses per prompt, but did not investigate whether this number is sufficient or optimal for detecting inconsistencies, or whether fewer or more samples would improve reliability.
- What evidence would resolve it: Systematically varying the number of responses and textual consistency samples (e.g., 3, 5, 10) while measuring detection rates and statistical power would identify the minimum sample size needed for reliable failure detection.

## Limitations
- Manual proverb pair construction introduces potential subjectivity and doesn't scale to larger datasets
- Only 5 responses per prompt limits statistical power of the Siegel-Tukey test
- Manual verification of scoring failures is ambiguous, particularly for scores near threshold values

## Confidence
- **High Confidence**: The methodology for combining NLI-based textual consistency with Siegel-Tukey statistical testing to detect scoring failures is well-founded and theoretically sound. The identification of specific LLM weaknesses (gender stereotyping, cultural misunderstandings) through this approach is supported by the experimental results.
- **Medium Confidence**: The effectiveness of the approach across different LLM models (ChatGPT, GPT-4, Claude-3, Llama-3) is demonstrated, but the generalizability to other models or more diverse proverb sets requires further validation.
- **Low Confidence**: The assumption that manual proverb pair construction ensures quality without introducing bias is questionable, as is the reliability of the manual verification process for scoring failures.

## Next Checks
1. **Statistical Power Validation**: Conduct a power analysis to determine the minimum number of responses per proverb needed to reliably detect scoring failures using the Siegel-Tukey test, and compare results with the current 5-response setup.

2. **NLI Model Robustness**: Test the DeBERTa-v3-large NLI model's performance on a separate set of culturally nuanced proverb pairs to ensure its consistency measures are not biased by cultural unfamiliarity.

3. **Manual Verification Consistency**: Implement a double-blind manual verification process for scoring failures to quantify inter-annotator agreement and assess the reliability of the current manual evaluation method.