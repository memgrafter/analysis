---
ver: rpa2
title: Towards Brain Passage Retrieval -- An Investigation of EEG Query Representations
arxiv_id: '2412.06695'
source_url: https://arxiv.org/abs/2412.06695
tags:
- retrieval
- information
- neural
- signals
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DEEPER proposes direct EEG-to-passage retrieval without intermediate
  text translation, using a dual-encoder architecture to map neural signals and text
  into a shared semantic space. Experiments on the ZuCo dataset show 8.81% precision@5
  improvement over EEG-to-text baselines, with cross-modal alignment achieving 0.29
  cosine similarity.
---

# Towards Brain Passage Retrieval -- An Investigation of EEG Query Representations

## Quick Facts
- arXiv ID: 2412.06695
- Source URL: https://arxiv.org/abs/2412.06695
- Authors: Niall McGuire; Yashar Moshfeghi
- Reference count: 40
- Primary result: 8.81% precision@5 improvement over EEG-to-text baselines using direct EEG-to-passage retrieval

## Executive Summary
This paper introduces DEEPER, a novel approach for brain-based passage retrieval that bypasses traditional EEG-to-text translation by directly mapping neural signals to relevant documents. The method employs a dual-encoder architecture that aligns EEG features with passage embeddings in a shared semantic space, enabling end-to-end retrieval without intermediate text representation. Experiments demonstrate that direct neural-to-text retrieval outperforms approaches that first translate brain activity into text queries, with hard negative sampling providing substantial performance gains.

## Method Summary
DEEPER uses a dual-encoder architecture with two transformers: one encoding EEG signals into neural embeddings and another encoding passages into text embeddings. Both encoders are trained jointly using contrastive loss to align semantically similar pairs in a shared vector space. The model processes 1-second EEG windows at passage end points, using fixed-channel electrode selection (C3, C4, P3, P4, O1, O2) based on relevance to language processing. Training employs hard negative mining with top-k similarity sampling to improve discriminative power. The approach operates directly on raw EEG data without prior signal processing, using 16 electrode channels.

## Key Results
- 8.81% precision@5 improvement over EEG-to-text baselines
- Cross-modal alignment achieves 0.29 cosine similarity
- Hard negative sampling improves performance by 51.6% in Precision@1
- Performance near-parity with text-based retrieval when controlling for architecture and dataset size

## Why This Works (Mechanism)
Direct EEG-to-passage retrieval eliminates information loss from intermediate text translation steps. By mapping neural signals directly into the same semantic space as text embeddings, the model preserves fine-grained cognitive representations that may be lost during transcription to natural language. The dual-encoder architecture enables end-to-end optimization of the neural-to-text alignment, while hard negative mining ensures the model learns discriminative features that distinguish relevant from irrelevant passages.

## Foundational Learning
- **Contrastive learning**: Why needed - to align neural and text embeddings in shared space; Quick check - measure cosine similarity between matched pairs
- **Dual-encoder architecture**: Why needed - separate specialized encoders for different modalities; Quick check - verify independent training of each encoder
- **Hard negative mining**: Why needed - to improve discriminative learning; Quick check - examine top-k similarity scores for negative samples
- **Neural embeddings**: Why needed - compact representation of cognitive state; Quick check - visualize embedding distributions
- **Shared semantic space**: Why needed - enables direct similarity comparison; Quick check - validate retrieval ranking performance

## Architecture Onboarding

**Component map**: EEG signal -> EEG encoder -> Neural embedding <- [shared space] -> Text embedding <- Passage encoder -> Retrieved passages

**Critical path**: Raw EEG → EEG Encoder → Shared Space → Cosine Similarity → Ranked Retrieval

**Design tradeoffs**: Fixed vs. adaptive EEG window sizing; complete vs. electrode subset selection; raw vs. processed EEG features

**Failure signatures**: Low cross-modal alignment (cosine similarity < 0.2); performance near random baseline; sensitivity to electrode selection

**3 first experiments**:
1. Test retrieval with varying EEG window sizes (0.5s, 1s, 2s)
2. Evaluate performance with different electrode subsets
3. Compare hard vs. random negative sampling strategies

## Open Questions the Paper Calls Out
None specified in source material.

## Limitations
- Small dataset (21 participants, 36 passages) raises overfitting concerns
- Fixed 1-second EEG window may miss temporal dynamics of cognitive processing
- Substantial semantic gap between neural and text embeddings (0.29 cosine similarity)
- Performance significantly below text-only baselines in absolute terms

## Confidence
- **High confidence**: Dual-encoder architecture is technically sound and reproducible
- **Medium confidence**: Superiority of direct EEG-to-passage retrieval over EEG-to-text translation
- **Low confidence**: Generalizability to real-world retrieval scenarios beyond controlled experimental setting

## Next Checks
1. Test model generalization across multiple brain-imaging modalities (fMRI, MEG) and document types beyond Wikipedia
2. Evaluate retrieval performance with variable EEG window sizes and temporal offsets relative to reading completion
3. Conduct ablation studies isolating the contribution of neural signal features versus architectural improvements