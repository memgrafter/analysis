---
ver: rpa2
title: MLPs Learn In-Context on Regression and Classification Tasks
arxiv_id: '2405.15618'
source_url: https://arxiv.org/abs/2405.15618
tags:
- task
- mlps
- context
- in-context
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that multi-layer perceptrons (MLPs) can learn
  in-context as effectively as Transformers on synthetic regression and classification
  tasks, challenging the assumption that this ability is unique to attention-based
  architectures. The authors compare MLPs, MLP-Mixers, and Transformers across controlled
  in-context learning tasks, finding comparable performance when accounting for compute
  budget.
---

# MLPs Learn In-Context on Regression and Classification Tasks

## Quick Facts
- **arXiv ID**: 2405.15618
- **Source URL**: https://arxiv.org/abs/2405.15618
- **Reference count**: 40
- **Primary result**: MLPs can learn in-context as effectively as Transformers on synthetic regression and classification tasks

## Executive Summary
This paper challenges the prevailing assumption that in-context learning (ICL) is exclusive to attention-based architectures like Transformers. Through controlled experiments on synthetic regression and classification tasks, the authors demonstrate that multi-layer perceptrons (MLPs) and MLP-Mixers can perform ICL comparably to Transformers when matched for compute budget. The study reveals that MLPs actually outperform Transformers on classical relational reasoning tasks from psychology, showing superior compute efficiency and generalization. A key finding is that Transformers transition from in-weight learning to ICL at lower data diversity compared to MLPs, suggesting fundamental differences in how these architectures adapt to novel contexts.

## Method Summary
The authors conducted a comprehensive comparison of MLPs, MLP-Mixers, and Transformers across multiple in-context learning tasks. They designed synthetic regression and classification problems with controlled data distributions to systematically evaluate ICL performance. Models were trained and tested under identical compute budgets to ensure fair comparison. The study included classical relational reasoning tasks from psychology to assess generalization capabilities. Additionally, they analyzed the transition points between in-weight learning and in-context learning by varying data diversity levels across different architectures.

## Key Results
- MLPs perform ICL as effectively as Transformers on synthetic regression and classification tasks when matched for compute budget
- MLPs outperform Transformers on classical relational reasoning tasks, demonstrating better compute efficiency and generalization
- Transformers transition from in-weight to in-context learning at lower data diversity than MLPs

## Why This Works (Mechanism)
The paper suggests that in-context learning may not be an exclusive property of attention mechanisms, but rather a more general capability that can emerge in various neural network architectures when properly configured. The authors hypothesize that MLPs can capture task-relevant patterns through their depth and width, enabling them to adapt to new contexts without parameter updates. The superior performance of MLPs on relational reasoning tasks indicates they may be better at learning abstract relationships and patterns that generalize across contexts. The difference in transition points between in-weight and in-context learning suggests that MLPs require more diverse training data to discover and leverage in-context patterns effectively.

## Foundational Learning
- **In-context learning (ICL)**: The ability of models to learn new tasks from context examples without parameter updates. Critical for understanding how models generalize to unseen distributions.
- **In-weight learning**: Traditional parameter-based learning where models update weights during training. Quick check: Compare model performance on training vs. test distributions.
- **Compute budget matching**: Ensuring fair comparison by equalizing computational resources across different architectures. Quick check: Verify FLOPs are comparable across model comparisons.
- **Data diversity**: The variety and distribution of training examples. Quick check: Analyze training set statistics for coverage of relevant patterns.
- **Relational reasoning**: The ability to understand and apply relationships between entities. Quick check: Test model performance on pattern completion tasks.
- **Architectural capacity**: The ability of a network architecture to represent complex functions. Quick check: Measure performance vs. model size scaling.

## Architecture Onboarding
**Component Map**: Input -> MLP Layers -> Output Prediction
- MLP Layers: Fully connected layers with activation functions
- Output: Task-specific prediction head

**Critical Path**: Input features flow through sequential MLP layers, where each layer transforms representations to capture increasingly abstract patterns. The depth and width of these layers determine the model's capacity for in-context learning.

**Design Tradeoffs**: 
- Depth vs. width: Deeper networks may capture more complex patterns but risk overfitting; wider networks offer more parameter efficiency but may require more data
- Activation functions: Different non-linearities affect the model's ability to learn diverse patterns
- Layer normalization: Affects training stability and convergence speed

**Failure Signatures**: 
- Poor performance on out-of-distribution examples indicates insufficient capacity for in-context adaptation
- Overfitting to training contexts suggests inadequate regularization or excessive model complexity
- Inconsistent predictions across similar contexts reveal sensitivity to input ordering

**First Experiments**:
1. Vary MLP depth (2-8 layers) while keeping total parameters constant to identify optimal architecture
2. Test different activation functions (ReLU, GELU, Swish) to determine which best enables ICL
3. Compare performance on structured vs. unstructured data to assess pattern learning capabilities

## Open Questions the Paper Calls Out
The paper highlights the need to investigate whether MLP in-context learning capabilities extend to more complex, real-world tasks beyond synthetic datasets. Specifically, they call for research on natural language understanding benchmarks and multimodal tasks where Transformers have traditionally excelled. The authors also suggest exploring whether specific architectural modifications to MLPs could enhance their in-context learning abilities, particularly for tasks requiring sophisticated relational reasoning.

## Limitations
- Experiments are limited to synthetic regression and classification problems with only one real-world dataset (CIFAR-10)
- Architectural choices may influence results, as the study doesn't explore modifications that could enhance MLP ICL
- Compute budget matching may not fully capture practical deployment considerations and scaling properties of different architectures

## Confidence
- **High confidence**: MLPs can learn in-context on synthetic regression/classification tasks
- **Medium confidence**: MLPs outperform Transformers on classical relational reasoning tasks with better compute efficiency
- **Medium confidence**: MLPs require higher data diversity than Transformers to transition from in-weight to in-context learning

## Next Checks
1. Test MLP architectures on natural language understanding benchmarks (e.g., GLUE, SuperGLUE) to assess real-world applicability
2. Conduct ablation studies varying MLP depth, width, and activation functions to identify architectural factors enabling in-context learning
3. Evaluate performance on multimodal tasks (text-vision) to determine if findings extend beyond unimodal synthetic datasets