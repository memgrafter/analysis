---
ver: rpa2
title: Do LLMs have Consistent Values?
arxiv_id: '2407.12878'
source_url: https://arxiv.org/abs/2407.12878
tags:
- values
- value
- human
- llms
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) exhibit
  consistent human-like value systems. The researchers analyzed responses from GPT-4
  and Gemini Pro to a standardized value questionnaire across different prompting
  strategies.
---

# Do LLMs have Consistent Values?

## Quick Facts
- arXiv ID: 2407.12878
- Source URL: https://arxiv.org/abs/2407.12878
- Reference count: 40
- LLMs can produce consistent human-like value systems when prompted appropriately

## Executive Summary
This study investigates whether large language models exhibit consistent human-like value systems by analyzing responses from GPT-4 and Gemini Pro to a standardized value questionnaire. The researchers found that standard prompting produced inconsistent and unreliable responses, but when using specific prompts like "Value Anchor," the LLM responses showed strong agreement with human value hierarchies and correlations. The Value Anchor prompt achieved Spearman rank correlations above 0.77 with human value rankings and produced internal consistency metrics comparable to human samples.

## Method Summary
The researchers administered a standardized value questionnaire to GPT-4 and Gemini Pro using different prompting strategies. They compared the LLM responses against established human value hierarchies from psychological research. The study employed the Value Anchor prompt technique, which instructed models to answer as someone emphasizing a particular value, and measured both the agreement with human rankings and the internal consistency of the value structures produced by the models.

## Key Results
- Standard prompting produced inconsistent and unreliable value responses from LLMs
- Value Anchor prompt achieved Spearman rank correlations above 0.77 with human value rankings
- LLM responses using Value Anchor showed circular value structure patterns matching human psychology

## Why This Works (Mechanism)
The Value Anchor prompting strategy works by providing LLMs with a consistent reference frame for value assessment, reducing the context-dependent variability that occurs with standard prompting. By instructing the model to adopt a particular value perspective, the prompt creates a stable cognitive framework that produces more coherent and human-like value hierarchies.

## Foundational Learning
- Human value theory: Understanding Schwartz's circular value structure provides the theoretical framework for evaluating LLM value consistency. Quick check: Can you describe the circular pattern of human values?
- Prompt engineering principles: Knowledge of how different prompts affect model behavior is essential for designing effective assessment strategies. Quick check: What makes Value Anchor prompting more effective than standard prompting?
- Statistical correlation methods: Spearman rank correlation is used to measure agreement between LLM and human value rankings. Quick check: Why use rank correlation rather than linear correlation for value assessment?

## Architecture Onboarding
- Component map: User query -> Prompt processor -> LLM core -> Value extraction -> Correlation analysis
- Critical path: The prompt selection and processing stage is most critical, as it determines whether the model produces consistent or variable responses
- Design tradeoffs: Using Value Anchor prompts improves consistency but may reduce the model's ability to provide nuanced, context-dependent responses
- Failure signatures: Inconsistent value rankings across different runs, lack of correlation with human values, or failure to produce circular value patterns
- First experiments: 1) Test Value Anchor prompt with multiple LLMs, 2) Compare correlation strength across different value questionnaires, 3) Measure response consistency over multiple runs

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the research raises important issues about the generalizability of the Value Anchor prompting strategy to other LLMs and contexts, as well as the philosophical distinction between simulating versus possessing consistent values.

## Limitations
- Generalizability of Value Anchor prompting to other LLMs and contexts remains uncertain
- Reliance on a single questionnaire format may limit comprehensiveness of value assessment
- Distinction between simulating versus genuinely possessing consistent values remains unresolved

## Confidence
- High confidence in finding that standard prompting produces inconsistent LLM responses
- Medium confidence in Value Anchor prompt's ability to produce human-like value correlations
- Low confidence in claims about genuine value possession versus sophisticated simulation

## Next Checks
1. Test the Value Anchor prompting strategy across multiple LLMs (Claude, Llama, etc.) and different value assessment instruments to evaluate generalizability
2. Conduct longitudinal studies to assess whether LLM value responses remain consistent across time, contexts, and conversation histories
3. Design adversarial prompting experiments to probe for hidden value inconsistencies that may emerge under stress-testing conditions