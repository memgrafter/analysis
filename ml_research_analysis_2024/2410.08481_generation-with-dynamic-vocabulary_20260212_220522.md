---
ver: rpa2
title: Generation with Dynamic Vocabulary
arxiv_id: '2410.08481'
source_url: https://arxiv.org/abs/2410.08481
tags:
- phrases
- vocabulary
- phrase
- language
- dynamic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a dynamic vocabulary approach for language
  models, enabling them to incorporate arbitrary text spans as basic generation units
  alongside traditional tokens. The method employs a dynamic phrase encoder that maps
  phrases to the input space of the language model, allowing seamless integration
  without modifying the underlying architecture.
---

# Generation with Dynamic Vocabulary

## Quick Facts
- arXiv ID: 2410.08481
- Source URL: https://arxiv.org/abs/2410.08481
- Reference count: 25
- This paper introduces a dynamic vocabulary approach for language models, enabling them to incorporate arbitrary text spans as basic generation units alongside traditional tokens.

## Executive Summary
This paper proposes a dynamic vocabulary approach that allows language models to incorporate arbitrary text spans as basic generation units alongside traditional tokens. The method employs a dynamic phrase encoder that maps phrases to the input space of the language model, enabling seamless integration without modifying the underlying architecture. Training involves careful sample construction and negative sampling strategies to ensure proper learning of the phrase encoder. Experimental results show that the dynamic vocabulary improves generation quality (MAUVE score increased by 25% on WikiText-103) and efficiency (20% latency reduction) compared to standard language models. The approach also demonstrates strong performance in domain adaptation and citation generation tasks, with superior compression ratios and enhanced citation recall/precision.

## Method Summary
The method introduces a dynamic phrase encoder that maps arbitrary text spans (phrases) directly to the embedding space of the language model. This allows the model to output entire phrases as single units during generation, reducing the number of decoding steps needed and enabling more coherent, meaningful outputs. The training process includes not only in-batch and pre-batch negative phrases but also manually constructed informative negative phrases (prefixes and suffixes of the target phrase). The approach is evaluated on WikiText-103, LawMT, and ASQA datasets using metrics such as MAUVE score, Rep-n, diversity, perplexity, and latency.

## Key Results
- MAUVE score increased by 25% on WikiText-103 compared to standard language models
- 20% latency reduction achieved through atomic phrase generation
- Superior performance in domain adaptation and citation generation tasks with enhanced recall/precision

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic vocabulary improves generation quality and efficiency by allowing arbitrary text spans to be generated atomically rather than as sequences of tokens.
- Mechanism: The dynamic phrase encoder maps arbitrary text spans (phrases) directly to the embedding space of the language model. This allows the model to output entire phrases as single units during generation, reducing the number of decoding steps needed and enabling more coherent, meaningful outputs.
- Core assumption: Phrases can be effectively encoded and decoded without loss of semantic meaning, and the language model can learn to appropriately switch between generating tokens and phrases.
- Evidence anchors:
  - [abstract]: "These text spans act as basic generation bricks, akin to tokens in the traditional static vocabularies... the ability to generate multi-tokens atomically improve both generation quality and efficiency"
  - [section 2.4]: "Supporting arbitrary phrase setP and integrating V â€² with language models are two cruxes to implement dynamic vocabularies"
- Break condition: If the phrase encoder fails to properly distinguish phrases from their sub-phrases or cannot handle the increased vocabulary size, the model may generate nonsensical outputs or experience significant performance degradation.

### Mechanism 2
- Claim: Proper negative sampling is crucial for the phrase encoder to learn correct prefix and suffix distinctions.
- Mechanism: The training process includes not only in-batch and pre-batch negative phrases but also manually constructed informative negative phrases (prefixes and suffixes of the target phrase). This helps the model learn to distinguish between phrases and their sub-phrases, preventing decoding ambiguity.
- Core assumption: The model can effectively learn from these carefully constructed negative samples and generalize to unseen phrases.
- Evidence anchors:
  - [section 2.4]: "it is crucial to make the two properly interleaved in training samples... to prevent the learned model from either biased towards full static token outputs or towards full new phrase outputs"
  - [section 3.4]: "we have designed several negative sampling strategies and explored their influence on the generation... strong negative phrases are crucial for the model's generation quality"
- Break condition: If the negative sampling strategy is insufficient or incorrectly implemented, the model may fail to properly distinguish phrases, leading to generation errors and poor performance.

### Mechanism 3
- Claim: The plug-and-play nature of the dynamic vocabulary enables efficient domain adaptation without model retraining.
- Mechanism: By simply adding domain-specific phrases to the dynamic vocabulary, the model can adapt to new domains without fine-tuning the underlying language model. This is achieved by extracting relevant phrases from domain-specific documents and using them as additional generation units.
- Core assumption: The phrase encoder, once trained on a general corpus, can effectively encode and decode domain-specific phrases without additional training.
- Evidence anchors:
  - [abstract]: "The dynamic vocabulary can be deployed in a plug-and-play way, thus is attractive for various downstream applications... we demonstrate that dynamic vocabulary can be applied to different domains in a training-free manner"
  - [section 3.4]: "only equipped with dynamic vocabulary extracted on the target domain, the model can outperform the transformer fine-tuned on LawMT datasets"
- Break condition: If the domain-specific phrases are too different from those seen during training, the phrase encoder may fail to properly encode and decode them, limiting the effectiveness of the domain adaptation.

## Foundational Learning

- Concept: Autoregressive language modeling
  - Why needed here: The dynamic vocabulary approach builds upon standard autoregressive language modeling, extending it to handle variable-length phrases as generation units.
  - Quick check question: In an autoregressive language model, what is the relationship between the probability of the next token and the previous tokens in the sequence?

- Concept: Negative sampling in contrastive learning
  - Why needed here: Proper negative sampling is crucial for training the phrase encoder to distinguish between correct phrases and incorrect alternatives.
  - Quick check question: In contrastive learning, what is the purpose of including negative samples in the training objective?

- Concept: Sequence compression and tokenization
  - Why needed here: The dynamic vocabulary approach aims to compress information more efficiently by allowing multi-token phrases to be generated as single units, which relates to sequence compression techniques.
  - Quick check question: How does increasing the vocabulary size affect the number of tokens needed to represent a given sequence of text?

## Architecture Onboarding

- Component map: Language model (e.g., GPT-2) -> Dynamic phrase encoder -> Vocabulary expansion mechanism -> Negative sampling module

- Critical path:
  1. Extract phrases from input text or related documents
  2. Encode phrases using the dynamic phrase encoder
  3. Expand the language model's vocabulary with phrase embeddings
  4. Generate text using the expanded vocabulary, potentially outputting phrases as single units

- Design tradeoffs:
  - Using a causal transformer vs. an encoder for the phrase encoder: The causal transformer allows for more efficient negative sampling but may limit the ability to capture bidirectional context
  - Contextualized vs. non-contextualized phrase representations: Non-contextualized representations are simpler and more similar to token embeddings but may miss important contextual information
  - Size of dynamic vocabulary: Larger vocabularies allow for more phrase options but increase memory usage and computational cost

- Failure signatures:
  - Poor generation quality: The model may generate nonsensical outputs or struggle to switch between tokens and phrases
  - Slow generation speed: If the phrase encoder is inefficient or the dynamic vocabulary is too large, generation may be slower than expected
  - Memory issues: Very large dynamic vocabularies may cause out-of-memory errors during training or inference

- First 3 experiments:
  1. Compare generation quality (MAUVE score) of the base model vs. the model with dynamic vocabulary on a small dataset
  2. Evaluate the impact of different negative sampling strategies on phrase encoder training
  3. Test domain adaptation capabilities by applying the model with domain-specific phrases to a new domain without fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the dynamic vocabulary approach scale with increasingly larger language models beyond TinyLlama (1.1B parameters)?
- Basis in paper: [explicit] The paper evaluates the approach on GPT-2 and TinyLlama, noting superior performance on TinyLlama with 1.09% MAUVE and 21.46% Diversity improvements.
- Why unresolved: The experiments only cover models up to 1.1B parameters, leaving uncertainty about performance on much larger models like GPT-3 (175B) or beyond.
- What evidence would resolve it: Systematic experiments testing the dynamic vocabulary approach on progressively larger models (e.g., 10B, 100B, 175B parameters) while measuring MAUVE, diversity, and computational efficiency.

### Open Question 2
- Question: What is the optimal strategy for managing the dynamic vocabulary size during inference to balance performance and computational cost?
- Basis in paper: [inferred] The paper mentions that memory and computational overhead increase linearly with the number of phrases, and discusses potential strategies like parallel encoding and dynamic offloading of unused phrases.
- Why unresolved: The paper does not provide concrete guidelines or experiments on how to optimally size or update the dynamic vocabulary during inference across different tasks and domains.
- What evidence would resolve it: Empirical studies comparing different dynamic vocabulary management strategies (e.g., fixed vs. adaptive sizing, frequency-based pruning, context-aware updates) across various domains and tasks.

### Open Question 3
- Question: How does the quality of the phrase encoder impact the overall performance of the dynamic vocabulary approach?
- Basis in paper: [explicit] The paper uses a causal Transformer as the phrase encoder and mentions that the quality of negative phrases significantly impacts generation quality, but does not extensively explore different phrase encoder architectures or training strategies.
- Why unresolved: The paper uses a specific phrase encoder architecture without exploring alternatives or ablation studies on phrase encoder quality.
- What evidence would resolve it: Comparative experiments using different phrase encoder architectures (e.g., bidirectional vs. causal Transformers, different model sizes) and training strategies (e.g., different negative sampling methods, curriculum learning for phrases) to determine their impact on generation quality and efficiency.

## Limitations

- Weak empirical foundation: The corpus analysis reveals minimal connection to related work, with only 8 out of 25 related papers showing meaningful overlap, suggesting the approach may be less novel than claimed.
- Limited evaluation methodology: Heavy reliance on automatic metrics like MAUVE without stronger human evaluation validation, and lack of comparison against alternative vocabulary expansion approaches.
- Potentially misleading "training-free" claims: The phrase extraction process from domain-specific documents likely requires some form of training or fine-tuning, making the "training-free" characterization potentially misleading.

## Confidence

- Medium: The experimental results show consistent improvements across multiple metrics, but the evaluation methodology has limitations and the "training-free" domain adaptation claims deserve scrutiny.

## Next Checks

1. **Negative Sampling Ablation Study**: Systematically vary the proportion and quality of negative samples (prefixes, suffixes, unrelated phrases) to quantify their exact contribution to performance improvements. This would validate Mechanism 2 and help establish optimal negative sampling strategies.

2. **Cross-Domain Transfer Analysis**: Test the model's ability to handle domains with significantly different phrase distributions than the training data (e.g., medical or legal domains if trained on general web text). This would stress-test the plug-and-play adaptation claims and reveal limitations of the phrase encoder's generalization.

3. **Vocabulary Size Scalability Study**: Systematically increase the dynamic vocabulary size and measure the impact on memory usage, generation speed, and quality degradation. This would validate the efficiency claims and identify practical limits for real-world deployment.