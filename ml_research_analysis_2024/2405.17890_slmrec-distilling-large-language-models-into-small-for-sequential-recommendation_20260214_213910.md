---
ver: rpa2
title: 'SLMRec: Distilling Large Language Models into Small for Sequential Recommendation'
arxiv_id: '2405.17890'
source_url: https://arxiv.org/abs/2405.17890
tags:
- recommendation
- language
- arxiv
- knowledge
- sequential
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the necessity of large language models
  (LLMs) in sequential recommendation. Through extensive experiments, the authors
  find that most intermediate layers of LLMs are redundant for sequential recommendation
  tasks.
---

# SLMRec: Distilling Large Language Models into Small for Sequential Recommendation

## Quick Facts
- **arXiv ID:** 2405.17890
- **Source URL:** https://arxiv.org/abs/2405.17890
- **Reference count:** 40
- **Primary result:** Achieves competitive sequential recommendation performance using only 13% of LLM parameters with 6.6x/8.0x training/inference speedups

## Executive Summary
This paper challenges the necessity of large language models (LLMs) for sequential recommendation tasks by demonstrating that most intermediate layers are redundant. Through systematic ablation studies, the authors show that pruning these layers while maintaining early and late layers preserves strong recommendation performance. Building on this insight, they propose SLMRec, a knowledge distillation framework that transfers knowledge from a large LLM teacher to a small student model using layer-wise feature alignment and multiple supervision signals. The method achieves competitive performance with significantly reduced computational costs.

## Method Summary
SLMRec employs knowledge distillation to transfer knowledge from a large LLaMa-7B teacher model to a smaller LLaMa student model for sequential recommendation. The approach uses layer-wise feature distillation with cosine similarity and L2 norm losses to align intermediate representations between teacher and student. Additional supervision signals are added through LoRA adapters that provide task-specific guidance. The student model uses block-wise distillation where intermediate teacher representations are distilled to corresponding student blocks, enabling effective knowledge transfer despite having fewer layers than the teacher.

## Key Results
- Achieves competitive performance compared to LLM-based recommendation models while using only 13% of parameters
- Delivers up to 6.6x/8.0x speedups in training/inference time
- Ablation studies show that setting block number to 4 yields optimal performance
- Distillation losses (Dcos, Dnorm, Lms) contribute to effective knowledge transfer

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pruning most intermediate layers of LLMs preserves recommendation performance because the remaining layers contain sufficient representation power for sequential patterns.
- **Mechanism:** The model's representational capacity is concentrated in early and late layers, with middle layers being redundant for sequential recommendation tasks. Removing these layers reduces parameter count while maintaining accuracy.
- **Core assumption:** Sequential recommendation tasks require less complex feature extraction than general language modeling, making deeper architectures unnecessary.
- **Evidence anchors:** [abstract]: "most intermediate layers of LLMs are redundant" and "pruning the remaining layers can still maintain strong performance"; [section 2]: "an 8-layer E4Rec*8 can obtain nearly as informative user representations as a 24-layer E4Rec*24"
- **Break condition:** If the task requires capturing long-range dependencies or complex sequential patterns that benefit from deeper architectures, this mechanism would fail.

### Mechanism 2
- **Claim:** Knowledge distillation effectively transfers teacher model knowledge to smaller student models through feature alignment.
- **Mechanism:** By minimizing feature similarity loss (cosine similarity) and feature norm regularization (L2 distance) between teacher and student representations, the student learns to produce comparable intermediate representations despite having fewer layers.
- **Core assumption:** The teacher's intermediate representations contain valuable information that can be compressed and transferred to smaller models through proper alignment.
- **Evidence anchors:** [section 4]: "Dcos(Θt, Θs) = 1/b Σ ht_k · hs_k / (||ht_k||2 · ||hs_k||2)" and "Dnorm(Θt, Θs) = 1/b Σ ||ht_k - hs_k||2 2"; [abstract]: "knowledge distillation to transfer the knowledge from a large LLM teacher model to a small student model"
- **Break condition:** If the teacher model's knowledge cannot be effectively compressed or if the alignment objectives don't capture the essential information.

### Mechanism 3
- **Claim:** Multiple supervision signals help the student model acquire task-aware knowledge beyond simple representation alignment.
- **Mechanism:** Additional adapters reduce dimension and compute predictions on intermediate representations, providing task-specific supervision that guides the student toward domain-relevant knowledge.
- **Core assumption:** Task-specific supervision is necessary for the student to perform well on the sequential recommendation task, not just match teacher representations.
- **Evidence anchors:** [section 4]: "Lms(Θs, Wt) = 1/b Σ Lce(y, p̂mp)" and "steer the student model toward assimilating specific aspects of recommendation-related knowledge"; [abstract]: "multiple supervision signals are crafted to steer the student model toward acquiring task-aware knowledge"
- **Break condition:** If the additional supervision conflicts with representation alignment or if the adapters introduce too much complexity.

## Foundational Learning

- **Concept:** Knowledge Distillation
  - **Why needed here:** Enables transfer of knowledge from large teacher models to smaller student models, addressing the computational inefficiency of large LLMs
  - **Quick check question:** What are the three main loss components in SLMRec's knowledge distillation process?

- **Concept:** Sequential Recommendation
  - **Why needed here:** Understanding how user behavior sequences are modeled and predicted is crucial for grasping why LLMs can be applied to this task
  - **Quick check question:** How does SLMRec convert user action sequences into input for the LLM?

- **Concept:** Transformer Architecture
  - **Why needed here:** The LLM backbone uses transformer blocks, so understanding self-attention and layer stacking is essential for understanding the model
  - **Quick check question:** What is the role of the prompt template in E-LLMRec methods?

## Architecture Onboarding

- **Component map:** Teacher LLM (e.g., LLaMa-7B) → Student LLM (smaller LLaMa) → ID Embedding Layer → Linear Transformations → Prediction Head
- **Critical path:** Input sequence → ID embedding → tokenization → stacked decoder layers → final representation → dot product with item embeddings → prediction
- **Design tradeoffs:** Parameter efficiency vs. performance (SLMRec achieves 13% parameters with competitive performance), training speed vs. inference speed (6.6x/8.0x speedups), model simplicity vs. effectiveness (simple distillation vs. complex adaptations)
- **Failure signatures:** Significant performance drop when block number b is too small, failure to converge if λ parameters are poorly tuned, representation misalignment causing poor predictions
- **First 3 experiments:**
  1. Reproduce the layer truncation experiment from Figure 2 to verify that intermediate layers are indeed redundant
  2. Train SLMRec with different block numbers b to find optimal performance
  3. Test the ablation of each knowledge regularizer (Dcos, Dnorm, Lms) to understand their individual contributions

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do SLMRec's performance and efficiency scale with different LLM teacher model sizes beyond the 7B parameters used in the paper?
- **Basis in paper:** [inferred] The paper uses LLaMa-7B as the teacher model but does not explore scaling to larger or smaller teacher models, leaving open questions about optimal teacher size.
- **Why unresolved:** The authors only experimented with one teacher model size, so the relationship between teacher model size and student model performance/efficiency remains unclear.
- **What evidence would resolve it:** Experiments comparing SLMRec performance using teacher models of varying sizes (e.g., 3B, 13B, 70B parameters) while keeping the student model fixed.

### Open Question 2
- **Question:** What is the impact of different knowledge distillation techniques (e.g., logits distillation, contrastive distillation) compared to the feature-based approach used in SLMRec?
- **Basis in paper:** [explicit] The paper explicitly states they did not use logits-based distillation because they wanted the student to learn representation encoding rather than prediction, but did not compare against other distillation methods.
- **Why unresolved:** The authors chose one distillation approach but did not benchmark against alternatives that might yield better performance or efficiency.
- **What evidence would resolve it:** Head-to-head comparison of SLMRec against versions using logits distillation, contrastive distillation, and other KD variants on the same datasets.

### Open Question 3
- **Question:** How does SLMRec perform on cold-start scenarios with limited user interaction history?
- **Basis in paper:** [inferred] The paper focuses on standard sequential recommendation with sufficient interaction history but does not address cold-start problems or scenarios with very short sequences.
- **Why unresolved:** The experiments use datasets where users have multiple interactions, so performance on sparse or new user/item cases is unknown.
- **What evidence would resolve it:** Evaluation of SLMRec on datasets or subsets with limited interaction sequences, comparing performance degradation against baselines as sequence length decreases.

## Limitations

- Limited evaluation to Amazon18 dataset, raising questions about generalization to other domains and datasets
- Lack of comparison with alternative knowledge distillation methods that might offer different tradeoffs
- No exploration of how SLMRec performs with different teacher model sizes or architectures

## Confidence

**High Confidence Claims:**
- SLMRec achieves competitive performance compared to LLM-based recommendation models while using significantly fewer parameters (13%)
- The proposed layer-wise distillation approach with multiple supervision signals is effective for transferring knowledge from large to small models
- Computational advantages (training/inference speedups) are real and significant under the tested conditions

**Medium Confidence Claims:**
- Most intermediate layers of LLMs are redundant for sequential recommendation tasks (supported by ablation but limited to specific experimental setup)
- The specific combination of cosine similarity, L2 norm, and multi-supervision losses is optimal for knowledge distillation in this context
- The 6.6x/8.0x speedups are representative of general performance improvements

**Low Confidence Claims:**
- Claims about the general applicability of the findings to other sequential recommendation tasks or datasets
- Assertions about the theoretical necessity of pruning intermediate layers without broader empirical validation
- Claims about the superiority of SLMRec's distillation approach compared to alternative knowledge transfer methods

## Next Checks

1. **Dataset Generalization Test:** Evaluate SLMRec on additional sequential recommendation datasets (e.g., MovieLens, YooChoose) to verify that performance gains and computational advantages hold across different domains and data characteristics.

2. **Alternative Distillation Comparison:** Implement and compare SLMRec against other knowledge distillation methods (e.g., traditional KD, contrastive distillation) to establish whether the specific combination of losses and supervision signals provides unique advantages.

3. **Layer Redundancy Validation:** Conduct a systematic study of layer importance across different sequential recommendation tasks, using techniques like layer-wise relevance propagation or targeted ablation, to validate the claim that intermediate layers are universally redundant for this application domain.