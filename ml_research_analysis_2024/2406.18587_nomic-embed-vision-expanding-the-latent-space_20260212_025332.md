---
ver: rpa2
title: 'Nomic Embed Vision: Expanding the Latent Space'
arxiv_id: '2406.18587'
source_url: https://arxiv.org/abs/2406.18587
tags:
- text
- vision
- image
- encoder
- nomic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces nomic-embed-vision, a vision encoder that
  shares the same latent space as the previously released nomic-embed-text, forming
  a unified embedding space for vision, language, and multimodal tasks. The model
  is trained using Locked Text Tuning, where a frozen text encoder from nomic-embed-text
  is used alongside a vision encoder initialized from EV A02-ViT B/16.
---

# Nomic Embed Vision: Expanding the Latent Space

## Quick Facts
- arXiv ID: 2406.18587
- Source URL: https://arxiv.org/abs/2406.18587
- Reference count: 9
- Nomic Embed Vision achieves strong performance across multiple benchmarks, outperforming existing models like OpenAI CLIP and Jina CLIP on ImageNet zero-shot classification, retrieval tasks, and the DataComp benchmark suite.

## Executive Summary
This paper introduces nomic-embed-vision, a vision encoder that shares the same latent space as the previously released nomic-embed-text, forming a unified embedding space for vision, language, and multimodal tasks. The model is trained using Locked Text Tuning, where a frozen text encoder from nomic-embed-text is used alongside a vision encoder initialized from EVA02-ViT B/16. Training is performed on the DFN-2B dataset using contrastive learning with a large batch size of 65,536. The model achieves strong performance across multiple benchmarks, outperforming existing models like OpenAI CLIP and Jina CLIP on ImageNet zero-shot classification, retrieval tasks, and the DataComp benchmark suite.

## Method Summary
The paper introduces Locked Text Tuning, a method that freezes a high-quality text encoder while training a vision encoder from a pretrained checkpoint. This approach enables efficient transfer of multimodal alignment without sacrificing text embedding performance. The vision encoder is initialized from EVA02-ViT B/16 and trained on the DFN-2B dataset using contrastive learning with a batch size of 65,536. The model uses multi-head attention pooling to aggregate visual features into a single embedding.

## Key Results
- Outperforms OpenAI CLIP and Jina CLIP on ImageNet zero-shot classification
- Achieves strong performance on retrieval tasks (Flickr30k)
- Demonstrates the feasibility of enhancing a high-quality text embedder with multimodal capabilities while maintaining strong text embedding performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Freezing a high-quality text encoder while training a vision encoder from a pretrained checkpoint enables efficient transfer of multimodal alignment without sacrificing text embedding performance.
- Mechanism: By locking the text encoder weights and initializing the vision encoder from a pretrained model, the training process can focus solely on aligning the vision representations to the already optimized text space.
- Core assumption: The frozen text encoder's representations are sufficiently general and robust to serve as a stable reference for multimodal alignment.
- Evidence anchors:
  - [abstract]: "We adapt the LiT (Zhai et al., 2022) training recipe, and instead freeze the text encoder... enables us to maintain the performance of nomic-embed-text as well as unlock new multimodal latent space capabilities."
  - [section]: "We adapt the LiT (Zhai et al., 2022) training recipe, and instead freeze the text encoder... This enables us to maintain the performance of nomic-embed-text as well as unlock new multimodal latent space capabilities."

### Mechanism 2
- Claim: Large batch sizes (65,536) improve contrastive learning performance by providing more negative samples, leading to better discrimination between similar and dissimilar image-text pairs.
- Mechanism: Increasing the batch size in contrastive learning increases the number of negative samples available for each positive pair. This makes the model's learned representations more discriminative, as it must distinguish the positive pair from a larger pool of negatives.
- Core assumption: The computational infrastructure can handle the large batch size without running into memory or optimization issues.
- Evidence anchors:
  - [section]: "As shown in Figure 2, increasing the batch size leads to sizable improvements on ImageNet 0-shot accuracy. We choose to use 65,536 as this is the biggest batch size we can accommodate given our compute limitation."

### Mechanism 3
- Claim: Initializing the vision encoder from a broadly pretrained model (EVA02-ViT B/16) provides a strong visual foundation that improves both multimodal retrieval and zero-shot classification performance.
- Mechanism: Pretrained vision encoders have already learned rich visual representations from large-scale supervised or self-supervised tasks. Initializing from such a model gives the multimodal training a head start, as the vision encoder already has good feature extraction capabilities.
- Core assumption: The pretraining task and dataset of the vision encoder are sufficiently general to benefit multimodal alignment tasks.
- Evidence anchors:
  - [section]: "We find that more broadly pretrained vision encoders lead to better multimodal retrieval and Imagenet zero-shot results... the ViT B/16 released in (Fang et al., 2023b) performed the best across Imagenet zero-shot and Flickr retrieval, which leverages the unsupervised Masked Image Modeling (MIM) objective."

## Foundational Learning

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: The model is trained using contrastive learning with a large batch size of 65,536. Understanding how contrastive learning works and how the InfoNCE loss function encourages similar image-text pairs to have similar embeddings while pushing dissimilar pairs apart is crucial for understanding the training process.
  - Quick check question: What is the role of negative samples in contrastive learning, and how does increasing the batch size affect their availability?

- Concept: Vision transformer architectures and pretraining
  - Why needed here: The vision encoder is initialized from EVA02-ViT B/16, a vision transformer pretrained using Masked Image Modeling. Understanding vision transformer architectures, their pretraining objectives, and how pretraining affects downstream task performance is essential for grasping why this initialization choice was made.
  - Quick check question: How does Masked Image Modeling pretraining differ from supervised pretraining, and what advantages might it offer for multimodal tasks?

- Concept: Text embedding models and their evaluation
  - Why needed here: The paper aims to maintain strong text embedding performance while adding multimodal capabilities. Understanding how text embedding models are trained (e.g., contrastive learning on text pairs) and evaluated (e.g., on MTEB benchmarks) is important for appreciating the challenge of this goal.
  - Quick check question: What are the key differences between multimodal embedding models and text-only embedding models in terms of their training objectives and evaluation metrics?

## Architecture Onboarding

- Component map: Frozen text encoder (nomic-embed-text) -> Vision encoder (initialized from EVA02-ViT B/16) -> Contrastive loss layer -> Multi-head attention pooling -> Training pipeline

- Critical path:
  1. Load and preprocess image-text pairs from DFN-2B
  2. Pass images through the vision encoder
  3. Pass text through the frozen text encoder
  4. Compute contrastive loss between image and text embeddings
  5. Backpropagate through the vision encoder only
  6. Update vision encoder weights using AdamW optimizer

- Design tradeoffs:
  - Freezing text encoder vs. training both encoders: Freezing maintains text performance but limits flexibility in the embedding space
  - Large batch size vs. memory constraints: Larger batches improve performance but require more computational resources
  - Pretrained vision encoder initialization vs. training from scratch: Pretraining provides a strong starting point but may introduce biases from the pretraining task

- Failure signatures:
  - Poor multimodal retrieval performance: May indicate misalignment between vision and text encoders
  - Degradation in text embedding performance: Suggests the vision encoder is negatively affecting the frozen text encoder's representations
  - Unstable training or NaN losses: Could be caused by too large batch size or improper learning rate

- First 3 experiments:
  1. Verify that freezing the text encoder maintains its performance on MTEB benchmarks
  2. Test different vision encoder initialization choices (e.g., random, supervised ViT, EVA02-ViT) on a small subset of DFN-2B
  3. Evaluate the effect of different batch sizes (e.g., 16k, 32k, 65k) on ImageNet zero-shot accuracy using a small-scale training run

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Locked Text Tuning method result in better downstream performance compared to traditional CLIP-style training when evaluated on a wider range of multimodal tasks?
- Basis in paper: [explicit] The paper discusses the performance of Nomic Embed Vision trained using Locked Text Tuning and compares it to other models like OpenAI CLIP and Jina CLIP. However, it mentions that retrieval scores tend to be lower compared to CLIP-style models.
- Why unresolved: The paper provides some comparison but does not extensively evaluate the method across a broad spectrum of multimodal tasks, leaving the question of its overall effectiveness open.
- What evidence would resolve it: Comprehensive benchmarking of Locked Text Tuning against traditional CLIP-style training across diverse multimodal tasks and datasets would provide clarity.

### Open Question 2
- Question: Can the modality gap in multimodal embedding spaces be effectively closed using the Locked Text Tuning approach, and does this improve downstream performance?
- Basis in paper: [inferred] The paper notes that recent work suggests CLIP-style training is not sufficient for closing the modality gap, and refers to the embedding spaces as "unified" rather than "aligned."
- Why unresolved: The paper does not explore whether modifying the Locked Text Tuning approach could address the modality gap, leaving this potential improvement unexplored.
- What evidence would resolve it: Experimental results showing whether adjustments to Locked Text Tuning can reduce the modality gap and enhance performance on multimodal tasks would provide insights.

### Open Question 3
- Question: How does the performance of Nomic Embed Vision scale with larger batch sizes beyond 65,536, and is there a point of diminishing returns?
- Basis in paper: [explicit] The paper mentions that increasing batch size leads to improvements on ImageNet zero-shot accuracy but stops at 65,536 due to compute limitations.
- Why unresolved: The study does not investigate whether further increases in batch size continue to yield performance gains or if a plateau is reached.
- What evidence would resolve it: Training models with progressively larger batch sizes and evaluating their performance would determine if there is a limit to the benefits of increased batch size.

## Limitations

- The paper relies on the DFN-2B dataset, which is not publicly available, creating a significant barrier to direct reproduction.
- The specific training configuration details, particularly around the multi-head attention pooling layer and the exact augmentation pipeline, are not fully specified.
- While the model achieves strong performance on established benchmarks, the long-term stability of the text encoder's performance when deployed in production settings with diverse inputs is not addressed.

## Confidence

- Basic Locked Text Tuning methodology and its effectiveness in maintaining text embedding performance while enabling multimodal capabilities: High
- Choice of large batch sizes for contrastive learning: Medium
- Initialization from EVA02-ViT B/16 and its generalizability across different vision encoder architectures: Medium

## Next Checks

1. Conduct a systematic ablation study varying batch sizes (16k, 32k, 65k, 128k if compute allows) on a held-out validation set to quantify the relationship between batch size and ImageNet zero-shot accuracy beyond the current single data point.

2. Test the model's text embedding performance degradation over time by fine-tuning on a small multimodal dataset and measuring MTEB benchmark performance before and after fine-tuning, to verify the claimed stability of the frozen text encoder.

3. Evaluate the model's robustness to out-of-distribution inputs by testing on datasets with significant domain shift from the DFN-2B training data (e.g., medical images, satellite imagery) to assess the generalization limits of the learned embedding space.