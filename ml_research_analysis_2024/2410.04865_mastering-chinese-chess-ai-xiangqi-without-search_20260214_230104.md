---
ver: rpa2
title: Mastering Chinese Chess AI (Xiangqi) Without Search
arxiv_id: '2410.04865'
source_url: https://arxiv.org/abs/2410.04865
tags: []
core_contribution: The paper presents a Chinese Chess AI that achieves top 0.1% human
  performance without using search algorithms. The key innovation is combining supervised
  learning with reinforcement learning using a modified PPO algorithm.
---

# Mastering Chinese Chess AI (Xiangqi) Without Search

## Quick Facts
- arXiv ID: 2410.04865
- Source URL: https://arxiv.org/abs/2410.04865
- Authors: Yu Chen; Juntong Lin; Zhichao Shu
- Reference count: 23
- One-line primary result: Achieves top 0.1% human performance without search algorithms using Transformer architecture and rule-based features

## Executive Summary
This paper presents a Chinese Chess AI that achieves top 0.1% human performance without using search algorithms. The system combines supervised learning from human games with reinforcement learning using a modified PPO algorithm. The key innovations include incorporating rule-based features for move encoding, using Transformer architecture instead of CNNs, implementing a dynamic opponent pool to prevent training collapse, and introducing Value Estimation with Cutoff (VECT) for stable training. The resulting model achieves over 1000x faster inference than MCTS-based systems and demonstrates 92.5% win rate against a strong baseline AI.

## Method Summary
The approach uses a two-phase training process: first, supervised learning on 15 million human game records with auxiliary win-rate prediction; second, reinforcement learning with modified PPO using a dynamic opponent pool and VECT. The model architecture is a Vision Transformer that takes as input both the board state and rule-based features encoding valid moves for both players. Training incorporates opponent sampling based on win rates to maintain strategic diversity, and value estimation is truncated to prevent instability from extreme win/loss outcomes.

## Key Results
- Achieves 92.5% win rate against a strong baseline AlphaBeta AI
- Ranks in top 0.1% of human players on the Chinese Chess ladder
- Inference is 1000x faster than MCTS-based systems and 100x faster than AlphaBeta-based systems
- Ablation studies show rule-based features, Transformer architecture, dynamic opponent pools, and VECT all significantly improve performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rule-based move encoding accelerates learning by reducing search space complexity.
- Mechanism: By embedding possible moves for both players as explicit features, the model learns valid actions without exhaustive search, allowing faster convergence.
- Core assumption: Move validity and board context can be encoded as static features that guide neural network predictions.
- Evidence anchors: "Possible moves of both sides as features can greatly improve the training process"; includes move channels identifying which pieces can be moved and to which locations.

### Mechanism 2
- Claim: Transformer architecture outperforms CNN by capturing long-range spatial dependencies in Xiangqi.
- Mechanism: Attention-based self-attention layers allow the model to weigh relationships between distant pieces, crucial for strategy in a 10x9 board.
- Core assumption: Xiangqi's strategic complexity benefits more from attention mechanisms than from local convolutional filters.
- Evidence anchors: "The same parameter amount of Transformer architecture has a higher performance than CNN on Chinese chess"; huge improvement in transition from mod-ResNet to ViT.

### Mechanism 3
- Claim: Dynamic Opponent Pool (DOP) prevents training collapse by maintaining strategic diversity.
- Mechanism: Instead of pure self-play, the model faces a curated set of opponents with varying strategies, sampled based on win rate performance.
- Core assumption: Exposure to diverse strategies prevents overfitting to a single style and avoids cyclical traps.
- Evidence anchors: "Selective opponent pool, compared to pure self-play training, results in a faster improvement curve and a higher strength limit"; designed diverse pool of opponent models inspired by AlphaStar's league concept.

## Foundational Learning

- Concept: Reinforcement Learning (RL) with Proximal Policy Optimization (PPO)
  - Why needed here: Enables the model to improve from self-play without explicit search, learning optimal policies through reward signals.
  - Quick check question: What is the main advantage of PPO over standard policy gradient methods in this context?

- Concept: Supervised Learning Pretraining
  - Why needed here: Provides a strong initial policy learned from human games, reducing the exploration burden in RL.
  - Quick check question: How does pretraining on human games affect the convergence speed in RL?

- Concept: Value Estimation with Cutoff (VECT)
  - Why needed here: Stabilizes training by truncating advantage estimation, preventing value function volatility from extreme win/loss swings.
  - Quick check question: Why is truncating the trajectory beneficial compared to full Monte Carlo returns in Xiangqi?

## Architecture Onboarding

- Component map: Board state tensor (10x9x14) + Rule-based move channels (10x9x14) -> Vision Transformer (ViT) with patch size 1x1 -> Move policy (8100 logits) and win rate value head

- Critical path:
  1. Load and clean human game dataset
  2. Train supervised model with auxiliary win-rate prediction
  3. Initialize RL with supervised checkpoint
  4. Run RL with dynamic opponent pool and VECT
  5. Evaluate against baseline and human ladder

- Design tradeoffs:
  - ViT vs CNN: Higher accuracy but more memory usage
  - DOP vs self-play: Better diversity but increased complexity
  - VECT vs GAE: More stable but requires cutoff tuning

- Failure signatures:
  - Sudden drop in win rate: Likely VECT cutoff too aggressive or opponent pool misconfigured
  - Slow convergence: Check rule-based feature encoding or transformer patch size
  - Overfitting: Reduce model size or increase opponent pool diversity

- First 3 experiments:
  1. Train supervised model with and without rule-based move features; compare accuracy.
  2. Swap ViT backbone for mod-ResNet; measure win rate vs baseline.
  3. Run RL with pure self-play vs dynamic opponent pool; compare learning curves.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the transformer-based architecture and VECT method be effectively applied to other board games with different rule sets and strategic complexities, such as Shogi or Checkers?
- Basis in paper: [inferred] The paper demonstrates success with Chinese Chess and mentions that these methods "could, in principle, be extended or modified for other board games."
- Why unresolved: The paper only validates these methods on Chinese Chess. Different board games have varying rule complexities, piece movements, and strategic depths that may require architectural or algorithmic modifications.
- What evidence would resolve it: Experimental results showing the same architecture and training methods achieving competitive performance against established AI systems in other board games like Shogi, Checkers, or Chess.

### Open Question 2
- Question: What is the theoretical upper limit of performance for search-free AI systems in Chinese Chess, and how close does this approach get to that limit?
- Basis in paper: [explicit] The paper mentions achieving "top 0.1% of human players" but doesn't discuss the absolute ceiling of performance possible without search algorithms.
- Why unresolved: The paper establishes strong human-level performance but doesn't quantify what the theoretical maximum achievable performance might be without search, nor does it compare against the strongest possible search-based systems.
- What evidence would resolve it: Head-to-head comparisons against the strongest search-based Chinese Chess AIs, along with analysis of whether further architectural or algorithmic improvements could yield additional performance gains.

### Open Question 3
- Question: How does the model's performance degrade when playing against opponents with highly unconventional or novel strategies not present in the training data?
- Basis in paper: [inferred] The paper discusses using diverse opponent pools to improve robustness but doesn't test the model against truly novel strategies outside the distribution of human play.
- Why unresolved: The ablation studies focus on incremental improvements within known strategic paradigms. The model's generalization to completely novel play styles or unconventional opening moves remains untested.
- What evidence would resolve it: Systematic evaluation against opponents employing deliberately unusual strategies, including asymmetric piece configurations, non-standard openings, or intentionally suboptimal play patterns designed to probe the model's adaptability.

## Limitations

- Claims about top 0.1% human performance lack external validation and statistical significance testing
- Results are based on a single baseline system (AlphaBeta AI) without comparison to other published Xiangqi AIs
- The model's generalization to unconventional strategies not present in training data remains untested

## Confidence

- **High Confidence**: Supervised learning phase with rule-based move features showing improved convergence - supported by clear training curves and measurable accuracy gains.
- **Medium Confidence**: Transformer architecture outperforms CNN for Xiangqi - internal results show improvement but lack cross-validation against other Transformer variants.
- **Low Confidence**: Achieves top 0.1% human performance - based on single ladder evaluation without statistical testing or comparison to other published AIs.

## Next Checks

1. **External Benchmarking**: Test the trained model against multiple published Xiangqi AIs beyond the AlphaBeta baseline, including those using search algorithms, to verify the claimed performance advantage.

2. **Architecture Generalization**: Replace the Transformer backbone with different attention mechanisms (e.g., Swin Transformer, MLP-Mixer) while keeping rule-based features constant to isolate the contribution of attention architecture versus move encoding.

3. **Opponent Pool Robustness**: Systematically vary the diversity and size of the dynamic opponent pool to determine the optimal configuration and test whether the benefits persist under different opponent sampling strategies.