---
ver: rpa2
title: 'RewardBench: Evaluating Reward Models for Language Modeling'
arxiv_id: '2403.13787'
source_url: https://arxiv.org/abs/2403.13787
tags:
- reward
- arxiv
- bench
- contextualai
- archangel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces REWARD BENCH, the first comprehensive benchmark
  for evaluating reward models (RMs) used in RLHF alignment of language models. The
  benchmark consists of 2,958 manually verified prompt-chosen-rejected trios across
  five categories: Chat, Chat Hard, Safety, Reasoning, and Prior Sets.'
---

# RewardBench: Evaluating Reward Models for Language Modeling

## Quick Facts
- arXiv ID: 2403.13787
- Source URL: https://arxiv.org/abs/2403.13787
- Reference count: 40
- This paper introduces REWARD BENCH, the first comprehensive benchmark for evaluating reward models (RMs) used in RLHF alignment of language models.

## Executive Summary
This paper introduces REWARD BENCH, the first comprehensive benchmark for evaluating reward models (RMs) used in RLHF alignment of language models. The benchmark consists of 2,958 manually verified prompt-chosen-rejected trios across five categories: Chat, Chat Hard, Safety, Reasoning, and Prior Sets. It evaluates RMs on challenging, structured, and out-of-distribution queries, with subsets designed to test specific capabilities like handling trick questions, refusals, and code reasoning. The authors evaluate over 80 RMs, including classifiers and those trained via Direct Preference Optimization (DPO), revealing significant performance variations. Key findings include scaling benefits for DPO models, safety behavior trade-offs, and limitations in current preference datasets. REWARD BENCH provides a unified framework for RM evaluation, highlighting areas for improvement in RLHF training and alignment.

## Method Summary
The REWARD BENCH benchmark evaluates reward models using 2,958 manually verified prompt-chosen-rejected trios across five categories. For classifier models, training uses maximum likelihood loss to predict human preference probability. For DPO models, optimization directly uses log ratios of policy and reference model probabilities. Inference computes scores for prompt-chosen and prompt-rejected pairs, classifying as win if chosen score exceeds rejected score. Accuracy is calculated per subset and weighted averaged for the final REWARD BENCH score. The evaluation pipeline handles both classifier and DPO model types with different inference methods, prioritizing comprehensive evaluation over speed.

## Key Results
- DPO models generally outperform classifiers on challenging chat tasks due to better capture of preference structure
- Scaling reward model size shows monotonic improvement across Llama 2 models but less consistent gains for Qwen 1.5 due to out-of-distribution generalization challenges
- Performance varies significantly across different task categories, with some subsets showing 100% accuracy for small RMs while others plateau around 75% even for state-of-the-art models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reward models trained via DPO outperform classifiers on challenging chat tasks because they capture preference structure directly from policy outputs rather than learning a separate reward function.
- Mechanism: DPO models optimize for the log ratio of policy probabilities (π(y1|x)/πref(y1|x)) > π(y2|x)/πref(y2|x), which implicitly encodes preference signals during training. This creates a tighter coupling between policy and reward, reducing distributional mismatch.
- Core assumption: The reference model (πref) is a good proxy for the base model and captures the underlying preference structure better than pairwise classification.
- Evidence anchors:
  - [abstract] "We also evaluate popular chat models trained with Direct Policy Optimization (DPO) ... to ground recent debates on RLHF methods and showcase specific datasets where they fall short."
  - [section] "In general, Llama 2 shows a clear improvement with scaling across all sections of REWARD BENCH, but Qwen 1.5 shows less monotonic improvement, likely due to out of distribution generalization challenges."
- Break condition: If the reference model is poorly chosen or too dissimilar from the base model, the implicit reward signal becomes noisy and performance degrades below classifier-based approaches.

### Mechanism 2
- Claim: Scaling reward model size improves performance on reasoning and safety tasks because larger models capture more subtle distinctions in preference data.
- Mechanism: Larger parameter counts enable better representation of complex preference patterns, especially in code reasoning where single-token differences determine correctness, and in safety where nuanced context determines appropriate refusals.
- Core assumption: Preference data contains sufficient signal for larger models to learn meaningful distinctions, and training doesn't plateau due to dataset limitations.
- Evidence anchors:
  - [section] "Tab. 3 shows the impact of scaling across different Llama 2, via Tulu 2 (Ivison et al., 2023), and Qwen 1.5 versions. In general, Llama 2 shows a clear improvement with scaling across all sections of REWARD BENCH"
  - [section] "Tab. 4 compares the impact of different base models and subtle changes of fine-tuning methods via the Zephyr-class models (Tunstall et al., 2023). Each of these models are fine-tuned on the UltraFeedback dataset via DPO as the final stage"
- Break condition: When dataset size or quality becomes the limiting factor rather than model capacity, further scaling provides diminishing returns.

### Mechanism 3
- Claim: RewardBench's structure with varied difficulty subsets enables better differentiation between reward model capabilities than single-score benchmarks.
- Mechanism: By including subsets ranging from easy (100% accuracy achievable) to hard (performance plateaus around 75%), the benchmark reveals specific capability gaps and allows targeted improvements rather than just overall performance ranking.
- Core assumption: The subsets are well-designed to isolate specific capabilities (chat, reasoning, safety) and the manual verification ensures ground truth quality.
- Evidence anchors:
  - [section] "The benchmark is broken down into five sections from different subsets... The RewardBench dataset is released under the ODC-BY license"
  - [section] "Some subsets are solved by small RMs, reaching 100% accuracy, but others are harder to differentiate and still have state-of-the-art performance around 75%"
- Break condition: If subsets don't adequately isolate capabilities or if manual verification introduces bias, the benchmark fails to provide meaningful differentiation signals.

## Foundational Learning

- Concept: Bradley-Terry model for pairwise comparison
  - Why needed here: The reward model is trained to predict p*(y1 ≻ y2 | x) using this probabilistic model, which forms the foundation for how preference data is converted to reward signals.
  - Quick check question: How does the Bradley-Terry model transform pairwise preferences into a probability distribution over completions?

- Concept: KL regularization in DPO vs no regularization in classifiers
  - Why needed here: Understanding this difference explains why DPO models may generalize differently and have different failure modes compared to classifier-based reward models.
  - Quick check question: What regularization mechanism does DPO use that classifier-based RMs typically don't employ?

- Concept: Distribution shift in reward modeling
  - Why needed here: The benchmark shows significant performance variation across different data types, highlighting how reward models struggle with out-of-distribution examples, especially in the reasoning and safety categories.
  - Quick check question: Why do reward models trained on general chat data often perform poorly on specialized reasoning or safety tasks?

## Architecture Onboarding

- Component map: Data loading (2,958 prompt-chosen-rejected trios) -> Model inference (score computation for each pair) -> Accuracy calculation (comparison of chosen vs rejected scores) -> Aggregation (weighted averaging across subsets)
- Critical path: For each model evaluation, the most time-consuming step is inference across all 2,958 examples. The pipeline must handle different model architectures (classifier vs DPO) and quantization levels efficiently.
- Design tradeoffs: The benchmark prioritizes comprehensive evaluation over speed, using manual verification for ground truth but requiring substantial compute for evaluation. The weighted averaging approach balances different capability areas but may obscure model strengths in specific domains.
- Failure signatures: Models showing high variance across subsets indicate inconsistent generalization. Models with strong performance on easy subsets but poor on hard subsets reveal capability gaps. DPO models failing on prior test sets indicate reference model issues.
- First 3 experiments:
  1. Run evaluation on a small subset (e.g., AlpacaEval Easy) to verify inference pipeline works correctly for both classifier and DPO models
  2. Compare chosen vs rejected score distributions for a single model to understand reward function characteristics
  3. Test the impact of reference model choice on DPO performance by evaluating with an intentionally mismatched reference model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do reward model outputs correlate with downstream RLHF performance?
- Basis in paper: [explicit] The authors mention "Initial experiments with ranking RMs with best-of-N sampling and downstream training with PPO are underway" and note this as a crucial next step.
- Why unresolved: The paper focuses on RM evaluation but does not establish quantitative correlations between REWARD BENCH performance and actual RLHF training outcomes.
- What evidence would resolve it: Experiments measuring RLHF policy performance using RMs ranked differently on REWARD BENCH, tracking win rates and other RLHF metrics.

### Open Question 2
- Question: What is the optimal shape/distribution of reward model outputs for RL training?
- Basis in paper: [explicit] "Few RMs are Gaussian in their scores... Future work should identify a preferred RM output distribution for downstream RL training."
- Why unresolved: The paper observes that RM outputs vary widely in distribution but does not test which distributions work best for RL algorithms like PPO.
- What evidence would resolve it: Controlled experiments training RLHF policies with RMs having different output distributions (Gaussian, zero-centered, etc.) and comparing learning stability and final performance.

### Open Question 3
- Question: How do DPO models compare to classifiers when using reference-free inference?
- Basis in paper: [explicit] "When a reference model is unavailable or compute is constrained, an alternative approach in such cases would be to obtain a reference free reward: π(y1|x) > π(y2|x)."
- Why unresolved: The paper only tests DPO models with their intended reference models, leaving the performance of reference-free inference unexplored.
- What evidence would resolve it: Benchmarking DPO models using reference-free scoring methods on REWARD BENCH and comparing results to their reference-based performance.

## Limitations

- Potential data contamination as some evaluated models may have been trained on overlapping datasets
- Manual verification process may introduce subjective biases that affect model rankings
- Performance gap between classifier and DPO models suggests current preference datasets may be insufficient for capturing nuanced decision-making

## Confidence

High confidence in the core benchmark design and its ability to differentiate reward model capabilities across different task categories. Medium confidence in the interpretation of scaling benefits, as dataset quality limitations may confound capacity effects. Low confidence in absolute performance comparisons across models with unclear training data provenance and potential contamination.

## Next Checks

1. Replicate the benchmark evaluation using a subset of models with fully documented training data to verify that performance differences are not artifacts of data contamination.

2. Conduct ablation studies on the manual verification process by having multiple annotators review the same examples to quantify inter-annotator agreement and potential bias.

3. Test whether models that perform well on REWARD BENCH also demonstrate superior alignment in end-to-end RLHF fine-tuning experiments, validating the benchmark's predictive value for real-world alignment performance.