---
ver: rpa2
title: Can time series forecasting be automated? A benchmark and analysis
arxiv_id: '2407.16445'
source_url: https://arxiv.org/abs/2407.16445
tags:
- time
- series
- timeout
- forecasting
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive benchmark study comparing the
  performance of AutoGluon-Timeseries and sktime frameworks for time series forecasting.
  The study evaluates 18 forecasting methods across 36 diverse datasets from various
  domains, including tourism, banking, energy, economics, transportation, and health.
---

# Can time series forecasting be automated? A benchmark and analysis

## Quick Facts
- arXiv ID: 2407.16445
- Source URL: https://arxiv.org/abs/2407.16445
- Reference count: 7
- AutoGluon outperforms sktime across diverse datasets with ensemble methods showing consistent advantages

## Executive Summary
This benchmark study comprehensively compares AutoGluon-Timeseries and sktime frameworks for automated time series forecasting across 36 diverse datasets. The evaluation covers 18 forecasting methods using sMAPE and MASE metrics, revealing that AutoGluon generally outperforms sktime methods, particularly through its ensemble approach and PatchTST model. The study provides actionable insights for practitioners, demonstrating that while AutoGluon excels overall, sktime's Naive and StatsForecastAutoETS methods show competitive performance in specific scenarios like monthly and weekly datasets.

## Method Summary
The study evaluates 18 forecasting methods from AutoGluon-Timeseries and sktime frameworks across 36 datasets from various domains including tourism, banking, energy, and economics. Methods are assessed using sMAPE and MASE metrics with time limits of 600 and 3600 seconds. The benchmark includes individual model evaluation and ensemble approaches, with additional hyperparameter tuning experiments for sktime methods. Statistical tests (Friedman test, Wilcoxon Signed-Rank) validate performance differences between methods.

## Key Results
- AutoGluon's ensemble approach (WeightedEnsemble) consistently outperforms individual models across diverse datasets
- PatchTST shows particularly strong performance within AutoGluon, excelling at capturing local temporal patterns
- sktime's Naive and StatsForecastAutoETS methods perform competitively on monthly and weekly datasets
- Hyperparameter tuning improves some sktime methods but doesn't surpass AutoGluon's default performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: AutoGluon's ensemble approach with WeightedEnsemble consistently outperforms individual models across diverse datasets.
- **Mechanism**: By combining predictions from multiple models (DeepAR, PatchTST, RecursiveTabular, etc.) using forward selection, WeightedEnsemble leverages complementary strengths and mitigates individual model weaknesses.
- **Core assumption**: Different forecasting models capture different temporal patterns, and their errors are not perfectly correlated.
- **Evidence anchors**:
  - [abstract] "Results show that AutoGluon generally outperforms sktime methods, with RecursiveTabular and PatchTST being particularly effective within AutoGluon."
  - [section 5.1] "WeightedEnsemble, on the other hand, consistently outperformed other methods, indicating a significant improvement in overall model performance achieved through the combination of predictions from multiple models."
- **Break condition**: If the dataset exhibits highly uniform temporal patterns where a single model excels, the ensemble overhead may not justify marginal gains.

### Mechanism 2
- **Claim**: PatchTST's patch-based transformer architecture excels at capturing local semantic meaning in time series.
- **Mechanism**: By dividing time series into smaller patches and training on these segments individually, PatchTST can extract meaningful temporal relationships while maintaining computational efficiency.
- **Core assumption**: Local temporal patterns within time series segments contain predictive information that can be aggregated for accurate forecasting.
- **Evidence anchors**:
  - [section 2.2.1] "These patches help the model to extract local semantic meaning and capture meaningful temporal relationships by considering a set of time steps collectively rather than handling the time step individually."
  - [section 5.3] "PatchTST, when trained individually within AutoGluon, demonstrates particularly favorable results... This suggests that the state-of-the-art PatchTST approach excels in capturing and predicting time series patterns."
- **Break condition**: If time series patterns are too long-range or require global context, patch-based approaches may miss critical dependencies.

### Mechanism 3
- **Claim**: AutoGluon's preset-based configuration balances model quality and training time effectively.
- **Mechanism**: The "best quality" preset automatically selects appropriate models and hyperparameters based on the dataset characteristics, optimizing for forecast accuracy within time constraints.
- **Core assumption**: The preset configurations have been empirically tuned to perform well across diverse time series domains and patterns.
- **Evidence anchors**:
  - [section 4.3.1] "For AutoGluon, presets are set to 'best quality' as opposed to other presets... with the time limit set to 600 seconds and 3600 seconds, to prioritize both high-quality forecasts and robustness."
  - [section 5.1] "Throughout the experimentation, it was observed that the AutoGluon framework exhibited relatively consistent performance across varying training durations indicating longer training times do not necessarily yield a significant improvement in accuracy."
- **Break condition**: If the dataset requires specialized domain knowledge or contains extreme outliers, generic presets may underperform custom configurations.

## Foundational Learning

- **Time series decomposition (trend, seasonality, residuals)**:
  - Why needed here: Understanding how AutoGluon and sktime methods decompose and model these components is crucial for interpreting their performance differences across frequencies and domains.
  - Quick check question: How would you modify an ARIMA model to handle a time series with strong weekly seasonality but no trend?

- **Error metrics for time series (sMAPE, MASE)**:
  - Why needed here: The paper uses sMAPE and MASE to evaluate forecasting performance, requiring understanding of their calculation, interpretation, and limitations.
  - Quick check question: Why is MASE preferred over RMSE when comparing models across datasets with different scales?

- **AutoML pipeline components (preprocessing, model selection, hyperparameter tuning)**:
  - Why needed here: The comparison between AutoGluon's automated approach and sktime's manual configuration requires understanding of how these components interact.
  - Quick check question: What are the trade-offs between using random search versus grid search for hyperparameter tuning in time series forecasting?

## Architecture Onboarding

- **Component map**: Data loading and preprocessing -> TSF format conversion -> train/test split -> model execution (AutoGluon presets vs sktime methods) -> forecast generation -> sMAPE and MASE computation -> statistical analysis -> result interpretation
- **Critical path**: Dataset loading → model training (with time limits) → forecast generation → metric computation → statistical analysis → result interpretation
- **Design tradeoffs**: 
  - Time limit constraints vs. model accuracy (600s vs 3600s for AutoGluon)
  - Ensemble methods vs. individual model performance (WeightedEnsemble vs. PatchTST)
  - Default configurations vs. hyperparameter tuning (sktime methods)
- **Failure signatures**:
  - "Timeout" indicates models unable to complete training within time limits
  - "N/A" suggests model failures due to period constraints or non-positive data handling
  - Memory errors or segmentation faults during tuning experiments
- **First 3 experiments**:
  1. Run AutoGluon with "fast training" preset on M4 Monthly dataset to verify baseline performance and identify timeout patterns
  2. Execute sktime's Naive method with default settings on Tourism Monthly dataset to establish performance floor
  3. Implement simple ensemble of sktime's ExponentialSmoothing and StatsForecastAutoETS on FRED-MD dataset to test combination benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of AutoGluon-Timeseries compare to sktime methods when applied to large-scale datasets with thousands of time series?
- Basis in paper: [explicit] The paper mentions that AutoGluon-Timeseries and sktime methods were evaluated on various datasets, including the Kaggle Wikipedia Web Traffic Daily dataset with 145,063 time series. However, both frameworks encountered issues loading this dataset, leaving the comparison incomplete.
- Why unresolved: The inability to load and process the large-scale dataset prevents a direct comparison of the frameworks' performance on such data.
- What evidence would resolve it: Successful loading and processing of the large-scale dataset by both frameworks, followed by a comprehensive performance comparison using appropriate metrics.

### Open Question 2
- Question: How does hyperparameter tuning impact the performance of sktime methods compared to AutoGluon-Timeseries?
- Basis in paper: [explicit] The paper investigates the impact of hyperparameter tuning on sktime methods, finding that tuning improves performance for some methods but does not surpass AutoGluon-Timeseries results.
- Why unresolved: The study only explores a limited set of hyperparameters for sktime methods, and the comparison with AutoGluon-Timeseries is based on default settings.
- What evidence would resolve it: A comprehensive hyperparameter tuning study for sktime methods, exploring a wider range of hyperparameters and comparing the results to AutoGluon-Timeseries with optimized settings.

### Open Question 3
- Question: How do different ensembling techniques within AutoGluon-Timeseries affect the overall forecasting performance?
- Basis in paper: [explicit] The paper mentions that AutoGluon-Timeseries uses ensembling techniques for hyperparameter tuning, but does not delve into the specific impact of different ensembling methods on performance.
- Why unresolved: The study does not explore the effectiveness of various ensembling techniques within AutoGluon-Timeseries or compare them to other ensembling approaches.
- What evidence would resolve it: A comparative analysis of different ensembling techniques within AutoGluon-Timeseries, evaluating their impact on forecasting accuracy and robustness across diverse datasets.

## Limitations

- Time constraints (600s and 3600s) may not represent all real-world deployment scenarios, particularly for models requiring longer training times
- Evaluation focuses on point forecasts without considering probabilistic forecasting capabilities crucial for uncertainty quantification
- The study does not explore specialized domain knowledge requirements for extreme outliers or non-standard time series patterns

## Confidence

- AutoGluon's superior performance: High
- PatchTST effectiveness: Medium
- Hyperparameter tuning impact: Medium

## Next Checks

1. **Extended Time Horizon Analysis**: Re-run the benchmark with extended time limits (e.g., 12-24 hours) to determine if longer training yields significant accuracy improvements for specific methods
2. **Domain-Specific Validation**: Conduct focused experiments on datasets from critical domains (healthcare, finance) where forecasting errors have high-stakes consequences
3. **Probabilistic Forecasting Comparison**: Extend the benchmark to include probabilistic forecasting metrics (CRPS, quantile loss) to evaluate uncertainty quantification capabilities across frameworks