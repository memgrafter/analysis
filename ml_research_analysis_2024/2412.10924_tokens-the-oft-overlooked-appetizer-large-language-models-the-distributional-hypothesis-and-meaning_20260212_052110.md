---
ver: rpa2
title: 'Tokens, the oft-overlooked appetizer: Large language models, the distributional
  hypothesis, and meaning'
arxiv_id: '2412.10924'
source_url: https://arxiv.org/abs/2412.10924
tags:
- tokens
- words
- language
- which
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores tokenization's impact on large language models
  (LLMs), arguing it's an overlooked but crucial aspect of their architecture. The
  authors demonstrate that the distributional hypothesis is sufficient for human-like
  language performance in LLMs, and that current tokenization techniques create suboptimal
  semantic building blocks and obscure necessary distributional patterns.
---

# Tokens, the oft-overlooked appetizer: Large language models, the distributional hypothesis, and meaning

## Quick Facts
- arXiv ID: 2412.10924
- Source URL: https://arxiv.org/abs/2412.10924
- Reference count: 33
- Primary result: Tokenization algorithm objective functions shape LLM cognition despite appearing insulated from main model intelligence

## Executive Summary
This paper argues that tokenization is a critical yet overlooked component of LLM architecture that significantly impacts model cognition. The authors demonstrate that current tokenization techniques, particularly those optimized for orthographic efficiency like BPE, create suboptimal semantic building blocks that obscure necessary distributional patterns. They show that tokens can encode semantic, syntactic, and frequency information but also harbor bias and unwanted content from training data. The paper highlights how tokenization algorithms' objective functions impact internal representations and discusses implications for alignment, bias, and meaning construction in LLMs.

## Method Summary
The authors used Byte-Pair Encoding (BPE) tokenization to explore tokenization processes on datasets including wikitext and Alice in Wonderland. They analyzed vocabularies from various LLMs available on Hugging Face and tiktoken, examining token structures and content. Using a RoBERTa masked language model, they tracked information in exemplar token vectors as they moved through model layers, visualizing results with PCA and UMAP. The study examined how tokens passing through human-meaningful linguistic units affect semantic representations and how tokenization choices impact distributional pattern extraction.

## Key Results
- Tokenization algorithm objective functions shape LLM cognition despite being insulated from main model intelligence
- Current tokenization creates suboptimal semantic building blocks that obscure necessary distributional patterns
- The distributional hypothesis is sufficient for human-like language performance in LLMs

## Why This Works (Mechanism)

### Mechanism 1
Orthographic-efficiency-maximizing algorithms create tokens that carry semantic information from training data, which becomes part of the model's learned representations. This content shapes internal representations even though the tokenization process itself is not directly involved in inference.

### Mechanism 2
Popular tokenization strategies prioritize orthographic efficiency over semantic coherence, creating tokens that don't align with human-meaningful linguistic units. This forces the model to do extra work to extract meaningful distributions from suboptimal token boundaries.

### Mechanism 3
The 1D sequence of tokens provides syntagmatic information that the model can learn from. This syntagmatic foundation enables the model to build semantic representations and achieve human-like language performance without requiring direct access to supradiegetic linguistic information.

## Foundational Learning

- **Distributional Hypothesis (DH)**: Core theoretical framework explaining how language models can learn meaning from patterns of token usage rather than explicit definitions. *Quick check: Can you explain how "you shall know a word by the company it keeps" applies to token sequences in LLMs?*

- **Tokenization as semantic primitive creation**: Tokens serve dual roles as both the input units the model processes and the semantic building blocks for meaning construction. *Quick check: How does the choice of tokenization algorithm impact what kind of semantic information becomes available to the model?*

- **Supradiegetic vs diegetic linguistic information**: Distinguishes between information LLMs can access (diegetic, within the text) versus information they cannot access (supradiegetic, like pronunciation, physical form). *Quick check: What types of linguistic information are inherently inaccessible to LLMs due to their architecture?*

## Architecture Onboarding

- **Component map**: Tokenization → Embedding matrix → Model layers (24 RoBERTa blocks) → Output generation. Key insight: Tokenization happens before the model and its effects persist through the entire pipeline.
- **Critical path**: Raw text → BPE tokenization → Token vectors → Initial embedding → Layer-by-layer processing → Output prediction. Each step must preserve information quality for the next.
- **Design tradeoffs**: Orthographic efficiency vs semantic coherence, vocabulary size vs token length, character-level vs byte-level tokenization, deterministic vs probabilistic tokenization.
- **Failure signatures**: Suboptimal token boundaries causing poor distributional pattern extraction, vocabulary bias from training data quality, excessive token length reducing combinatorial power, insufficient vocabulary coverage for target domains.
- **First 3 experiments**:
  1. Compare model performance on same task using different tokenization algorithms (BPE vs WordPiece vs character-level) to measure impact of tokenization choices.
  2. Analyze token vector trajectories through model layers for polysemous words to identify how semantic distinctions emerge.
  3. Create controlled vocabulary variations (e.g., emphasizing word-like tokens vs subword tokens) and measure impact on downstream task performance.

## Open Questions the Paper Calls Out

### Open Question 1
How does tokenization algorithm's objective function impact LLM cognition beyond token selection, specifically affecting semantic representations? Current understanding focuses on token selection effects; mechanisms of how objective functions shape internal representations during training and inference need deeper investigation.

### Open Question 2
What is the optimal vocabulary size for tokenization that balances combinatorial power with efficiency, and how does this vary across languages? Current research focuses on maximizing efficiency without considering whether human-like linguistic structures might improve performance.

### Open Question 3
How can grounding strategies be effectively integrated with LLM architectures to enhance semantic depth without sacrificing the distributional hypothesis's benefits? Current grounding approaches often focus on specific tasks rather than general semantic enhancement.

## Limitations
- Analysis relies heavily on qualitative examination rather than quantitative performance comparisons across different tokenization strategies
- No systematic comparison of model performance using different tokenization algorithms on identical tasks
- Limited empirical validation of claims about how token vectors evolve through model layers

## Confidence

- **High confidence**: Tokenization algorithms create semantic content that persists through model training; current tokenization creates suboptimal semantic building blocks
- **Medium confidence**: The distributional hypothesis is sufficient for human-like language performance in LLMs; tokenization can encode bias and unwanted content
- **Low confidence**: The specific mechanism by which orthographic-efficiency-maximizing algorithms shape internal representations; the extent to which syntagmatic information alone suffices for MVP language performance

## Next Checks
1. **Quantitative ablation study**: Train identical models using BPE, WordPiece, and character-level tokenization on the same corpus, then compare performance on standard benchmarks to establish causal links between tokenization choices and capability

2. **Token vector trajectory validation**: Use controlled experiments with polysemous words and syntactic constructions to track how semantic distinctions emerge across model layers, validating the qualitative observations with quantitative clustering metrics

3. **Vocabulary bias analysis**: Systematically compare vocabulary content across different training corpora (fiction, news, technical documents) to quantify how training data characteristics translate into tokenization-induced bias and semantic gaps