---
ver: rpa2
title: Searching for internal symbols underlying deep learning
arxiv_id: '2405.20605'
source_url: https://arxiv.org/abs/2405.20605
tags:
- symbols
- dnns
- layers
- learning
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether deep neural networks (DNNs) develop
  abstract internal codes (termed "symbols") that could enhance decision-making reliability.
  The authors hypothesize that DNNs learn to encode contextual information in hidden
  layers through functional clusters, which can be extracted using segmentation models
  and unsupervised clustering.
---

# Searching for internal symbols underlying deep learning

## Quick Facts
- arXiv ID: 2405.20605
- Source URL: https://arxiv.org/abs/2405.20605
- Reference count: 40
- This study investigates whether DNNs develop abstract internal codes ("symbols") that could enhance decision-making reliability

## Executive Summary
This study investigates whether deep neural networks (DNNs) develop abstract internal codes ("symbols") that could enhance decision-making reliability. The authors hypothesize that DNNs learn to encode contextual information in hidden layers through functional clusters, which can be extracted using segmentation models and unsupervised clustering. By analyzing 5 ImageNet models (ResNet18, ResNet50, VGG19, DenseNet121, and ViT), they identified symbols associated with semantic meanings of inputs and found these symbols can predict DNNs' accuracy, detect out-of-distribution (OOD) examples, identify adversarial perturbations, and enable temporary learning of OOD examples. The proposed method achieved up to 80% accuracy in symbol-based predictions and demonstrated strong separation between normal and adversarial inputs (AUROC near 1.0).

## Method Summary
The authors propose a pipeline to extract and analyze internal symbols from DNN hidden layers. They use segmentation models with STCert to identify regions of interest (ROIs) in images, then apply ROI-pooling to extract 3x3 activation vectors from hidden layers. These high-dimensional vectors are reduced to 3D using UMAP, then clustered using X-means to identify symbols. The symbols are correlated with class labels to create correlation maps, which are then used for symbol-based predictions. The Expected Symbol Score (ESS) is calculated to measure confidence and detect OOD examples or adversarial perturbations.

## Key Results
- Symbol-based predictions achieved up to 80% accuracy in layer 4 of analyzed models
- ESS successfully distinguished between correct and incorrect predictions, with higher scores for correct predictions
- Symbols effectively detected OOD examples and adversarial perturbations, with AUROC near 1.0
- The methodology demonstrated the potential for using internal symbols to address DNN reliability issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hidden layers develop functional clusters that encode contextual information progressively from low-level to high-level features
- Mechanism: Earlier layers encode basic visual features while later layers encode semantic features, creating natural clustering of similar contexts
- Core assumption: All hidden layers share the same operating principle - mapping inputs with similar contextual information into neighboring outputs
- Evidence anchors: [abstract] "We hypothesize that DNNs can develop abstract codes that can be used to augment DNNs' decision-making" and [section 1] description of progressive encoding
- Break condition: If clustering analysis fails to reveal meaningful patterns or if functional clusters don't correlate with semantic meanings

### Mechanism 2
- Claim: Symbols extracted from hidden layers can predict DNNs' accuracy, detect OOD examples, and identify adversarial perturbations
- Mechanism: Symbols serve as abstract codes that capture the essence of what the network has learned
- Core assumption: Symbols are closely linked to the semantic meanings of inputs and DNNs' decision-making process
- Evidence anchors: [abstract] claims about symbol capabilities and [section 3.1] showing 80% symbol-based prediction accuracy
- Break condition: If symbol-based predictions perform no better than random chance or fail to distinguish between normal and abnormal inputs

### Mechanism 3
- Claim: Expected Symbol Score (ESS) can be used to measure DNNs' confidence in their predictions and detect OOD examples
- Mechanism: ESS represents the likelihood of observing certain symbols given a class label
- Core assumption: Hidden layer responses are synchronized when processing in-distribution examples but become desynchronized with OOD examples
- Evidence anchors: [section 3.2] showing higher ESS for correct predictions and [section 3.3] showing different ESS between in-distribution and OOD examples
- Break condition: If ESS fails to correlate with prediction accuracy or cannot distinguish between in-distribution and OOD examples

## Foundational Learning

- Concept: UMAP (Uniform Manifold Approximation and Projection)
  - Why needed here: Reduces high-dimensional hidden layer responses to 3-dimensional vectors for clustering analysis
  - Quick check question: What parameters are used in the UMAP analysis and why are they chosen?

- Concept: X-means clustering
  - Why needed here: Automatically discovers the optimal number of clusters (symbols) without requiring manual specification
  - Quick check question: How does X-means use Bayesian Information Criterion to determine the optimal number of clusters?

- Concept: ROI-pooling
  - Why needed here: Converts arbitrary-sized hidden layer activation maps into standardized 3-by-3 activation maps for consistent symbol extraction
  - Quick check question: Why is ROI-pooling necessary for extracting smooth activation vectors from hidden layers?

## Architecture Onboarding

- Component map: STCert (segmentation + cross-validation) → ROI extraction → Hidden layer response sampling → ROI-pooling → UMAP reduction → X-means clustering → Symbol analysis
- Critical path: STCert → ROI extraction → Hidden layer response sampling → Symbol extraction → Symbol-based prediction
- Design tradeoffs: Using segmentation models adds computational overhead but improves ROI accuracy; limiting maximum clusters to 1000 may miss some symbols but prevents overfitting
- Failure signatures: Low prediction accuracy (<50%), inability to distinguish between normal and OOD examples, symbols not correlating with semantic meanings
- First 3 experiments:
  1. Test symbol extraction on a simple dataset (e.g., CIFAR-10) to verify the pipeline works
  2. Compare symbol-based predictions using different numbers of clusters to find optimal setting
  3. Test ESS on a dataset with known OOD examples to validate OOD detection capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the identified internal symbols evolve during the training process of DNNs?
- Basis in paper: [inferred] The paper analyzes symbols in pre-trained models but doesn't examine their development over training epochs
- Why unresolved: The study focuses on extracting and analyzing symbols from trained models rather than monitoring their formation during training
- What evidence would resolve it: Tracking symbol emergence and evolution across training epochs using the same methodology

### Open Question 2
- Question: Can the proposed symbol-based methods for OOD detection and adversarial attack mitigation generalize to domains beyond computer vision?
- Basis in paper: [explicit] The study only tests on ImageNet models and doesn't explore applicability to other domains
- Why unresolved: The methodology and results are specific to image classification tasks with segmentation models
- What evidence would resolve it: Applying the symbol extraction and analysis framework to NLP, speech recognition, or other domains

### Open Question 3
- Question: What is the relationship between symbol complexity and model architecture (depth, width, attention mechanisms)?
- Basis in paper: [inferred] The study compares 5 different architectures but doesn't systematically analyze how architectural differences affect symbol properties
- Why unresolved: The analysis shows accuracy differences but doesn't explore underlying architectural factors
- What evidence would resolve it: Controlled experiments varying model architecture parameters while measuring symbol characteristics

## Limitations
- The study relies on unsupervised clustering without ground truth validation of what extracted symbols actually represent semantically
- The methodology uses a limited dataset (78 classes from ImageNet) and only 5 model architectures, potentially limiting generalizability
- The computational complexity of the proposed method, requiring segmentation models and extensive clustering, may limit practical applicability

## Confidence
- High Confidence: The methodology for extracting and clustering hidden layer activations is technically sound and well-implemented
- Medium Confidence: Claims about symbols representing semantic meanings and their utility for OOD detection and adversarial example identification are supported by experimental results but lack independent validation
- Low Confidence: Generalizability of findings to other datasets, model architectures, and real-world applications remains uncertain

## Next Checks
1. Conduct human evaluation studies to verify whether the extracted symbols correspond to interpretable semantic concepts beyond mere label correlation
2. Apply the symbol extraction method to diverse datasets (beyond ImageNet) and different model architectures to assess robustness and generalizability
3. Test the stability of extracted symbols across multiple training runs of the same model to ensure consistency and reliability of the approach