---
ver: rpa2
title: 'eXmY: A Data Type and Technique for Arbitrary Bit Precision Quantization'
arxiv_id: '2405.13938'
source_url: https://arxiv.org/abs/2405.13938
tags:
- exponent
- https
- arxiv
- quantization
- exmy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: eXmY introduces a new data type and technique for arbitrary bit-precision
  quantization in machine learning models. It supports arbitrary bit widths and formats
  (e.g., e3m3, e4m2), enabling flexible quantization tailored to different model components.
---

# eXmY: A Data Type and Technique for Arbitrary Bit Precision Quantization

## Quick Facts
- arXiv ID: 2405.13938
- Source URL: https://arxiv.org/abs/2405.13938
- Reference count: 40
- Primary result: Introduces eXmY format enabling arbitrary bit-precision quantization with per-row metadata maintaining model quality at lower bit widths

## Executive Summary
eXmY introduces a new data type and technique for arbitrary bit-precision quantization in machine learning models. It supports arbitrary bit widths and formats (e.g., e3m3, e4m2), enabling flexible quantization tailored to different model components. The method exploits the statistical distribution of exponents in tensors to reduce bit requirements while preserving model quality. eXmY includes software libraries for emulation, encoding, and decoding across frameworks (C++, TensorFlow, JAX, PAX), leveraging SIMD/vector instructions for performance. Evaluation on PaLM 2 S shows that e3m1 with per-row metadata maintains model quality for LLMs, even at lower bit widths.

## Method Summary
eXmY is a generalization of floating point format to arbitrary bit widths and formats. It uses 1 sign bit, X exponent bits, and Y mantissa bits, allowing combinations like e3m3 or e4m2. For non-power-of-two bit widths, a novel encoding scheme decomposes bits into power-of-2 segments (e.g., 7 = 4+2+1) and packs them into int32/int16/int8 containers for perfect compression. The technique includes metadata management (per-tensor or per-row) to optimize exponent scaling factors. Software libraries provide emulation, encoding/decoding, and framework integration with SIMD optimization.

## Key Results
- Achieves perfect compression and byte addressability for arbitrary bit widths through power-of-2 decomposition
- Maintains model quality for Large Language Models with e3m1 format and per-row metadata
- Reduces memory, network, and storage requirements while increasing multi-tenancy and compute acceleration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: eXmY enables arbitrary bit widths and formats, unlike fixed-power-of-two formats.
- Mechanism: The format uses 1 sign bit, X exponent bits, and Y mantissa bits, allowing combinations like e3m3 or e4m2. For example, 7-bit e3m3 gives 3 exponent and 3 mantissa bits.
- Core assumption: The distribution of exponents in tensors is sparse enough that fewer exponent bits can be used without significant quality loss.
- Evidence anchors:
  - [abstract] "it seamlessly supports 3, 5, 6, 7, 9 bit formats... For a specific bit width, say 7, it defines all possible formats e.g. e0m6, e1m5, e2m4, e3m3, e4m2, e5m1 and e6m0."
  - [section] "eXmY is a generalization of the floating point format to arbitrary bit widths and formats."
  - [corpus] Weak - no direct corpus evidence yet.
- Break condition: If the tensor's exponent distribution is not sparse (many values far from zero), the compression advantage disappears.

### Mechanism 2
- Claim: Perfect compression and byte addressability are achieved through power-of-two bit decomposition.
- Mechanism: 7-bit elements are split into 4+2+1 bits and packed into int32, int16, and int8 containers, so 8 elements use exactly 32+16+8 bits.
- Core assumption: The number of elements per block is a multiple of 8, which is almost always true in ML models.
- Evidence anchors:
  - [section] "We decompose the bits into power-of-2 segments i.e., 7 = 4 + 2 + 1... An array of shape (8R, C) gets packed into 3 arrays of int32, int16 and int8..."
  - [abstract] "For non-power of two bit widths e.g. 5, 6, 7, we created a novel encoding and decoding scheme which achieves perfect compression, byte addressability..."
  - [corpus] Weak - no direct corpus evidence yet.
- Break condition: If tensor shapes are not multiples of 8 in the packing dimension, the scheme needs padding or becomes inefficient.

### Mechanism 3
- Claim: Metadata granularity (per-row vs per-tensor) balances compression vs quality.
- Mechanism: Per-row metadata allows a tighter fit of exponent range for each row, preserving more dynamic range and reducing saturation.
- Core assumption: Different rows in a weight matrix have different exponent distributions, so per-row metadata improves accuracy.
- Evidence anchors:
  - [abstract] "eXmY is also a technique and exploits the statistical distribution of exponents in tensors."
  - [section] "e4mY with per tensor metadata and e3mY with per row or column metadata is quality neutral for a wide variety of Large Embedding Models (LEMs) and Large Language Models (LLMs)."
  - [corpus] Weak - no direct corpus evidence yet.
- Break condition: If metadata storage cost outweighs quality gain, per-tensor metadata may be preferable.

## Foundational Learning

- Concept: Floating-point representation (sign, exponent, mantissa)
  - Why needed here: eXmY is a generalization of floating-point to arbitrary bit widths; understanding the IEEE layout helps grasp why eXmY works.
  - Quick check question: What is the bias for a 3-bit exponent in eXmY if the smallest normal is 2^-1 and the range is [2^-1, 2^5]?

- Concept: Bit packing and SIMD vectorization
  - Why needed here: The codecs pack bits into int32/int16/int8 for efficient vectorized processing; knowing SIMD is essential to understand performance claims.
  - Quick check question: Why does decomposing 7 bits into 4+2+1 bits allow using standard int32/int16/int8 containers?

- Concept: Tensor quantization and PTQ vs QAT
  - Why needed here: eXmY can be applied to both post-training quantization and quantization-aware training; knowing the difference helps decide when to use it.
  - Quick check question: In PTQ, what is the typical metadata stored alongside quantized weights?

## Architecture Onboarding

- Component map: Emulation layer -> Metadata extraction -> Bit packing -> Storage -> (Inference) Bit unpacking -> Emulation -> Compute

- Critical path: Emulation → Metadata extraction → Bit packing → Storage → (Inference) Bit unpacking → Emulation → Compute

- Design tradeoffs:
  - More mantissa bits → better precision, higher memory
  - Per-row metadata → better quality, more metadata storage
  - Smaller block sizes → finer granularity, more metadata overhead

- Failure signatures:
  - NaNs/Inf values during inference: check metadata extraction or clamping logic
  - Quality drop after quantization: check exponent bias choice and metadata granularity
  - Performance slowdown: check that SIMD paths are used and data alignment is correct

- First 3 experiments:
  1. Quantize a small ResNet weight matrix to e3m1 with per-row metadata and measure accuracy drop.
  2. Pack/unpack a 7-bit eXmY tensor using the power-of-2 decomposition and verify round-trip equality.
  3. Compare inference latency with eXmY vs bfloat16 on a CPU with SIMD support.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal exponent bias for eXmY formats to maximize model quality while minimizing bit usage?
- Basis in paper: [explicit] The paper mentions that eXmY supports arbitrary exponent biases, unlike IEEE and OCP formats, and this is stored as metadata.
- Why unresolved: The paper does not provide empirical data on how different exponent biases affect model quality for various tasks and model architectures.
- What evidence would resolve it: Empirical studies comparing model quality across different exponent biases for various eXmY formats on diverse datasets and model architectures.

### Open Question 2
- Question: How does the exponent distribution vary across different model architectures and training techniques, and what implications does this have for eXmY quantization?
- Basis in paper: [explicit] The paper discovers a distribution of exponents in ML models and exploits it to reduce bit requirements, but only provides data for one PaLM-2 layer.
- Why unresolved: The paper only analyzes the exponent distribution for one specific model (PaLM-2), and it's unclear if this distribution generalizes to other architectures or if training techniques significantly alter it.
- What evidence would resolve it: Comprehensive analysis of exponent distributions across various model architectures, training techniques, and datasets, with implications for eXmY quantization strategies.

### Open Question 3
- Question: What are the trade-offs between different block sizes for metadata in eXmY, and how do these trade-offs vary across model types and quantization schemes?
- Basis in paper: [explicit] The paper mentions that eXmY does not constrain block size and that different block sizes may be optimal for different formats, but only provides limited evaluation on PaLM-2.
- Why unresolved: The paper only briefly mentions the impact of block size on model quality for a few specific cases (e.g., e2m1) and does not provide a comprehensive analysis of the trade-offs across different model types and quantization schemes.
- What evidence would resolve it: Systematic evaluation of the impact of block size on model quality across various model types, quantization schemes, and datasets, with guidelines for optimal block size selection.

## Limitations
- No direct corpus evidence showing exponent distributions are sparse enough to justify bit reductions
- Assumes tensor shapes are multiples of 8 for power-of-2 decomposition efficiency
- Performance claims regarding SIMD acceleration lack empirical demonstration across hardware platforms

## Confidence
- High confidence: The basic mechanism of eXmY (arbitrary bit-width floating point generalization) is well-founded and theoretically sound.
- Medium confidence: Quality preservation claims with per-row metadata are supported by PaLM 2 S evaluation but need broader validation.
- Low confidence: Performance claims regarding SIMD acceleration and production deployment benefits are stated but not empirically demonstrated.

## Next Checks
1. Analyze exponent distributions across multiple tensor types in diverse model architectures to verify sparsity assumption.
2. Measure metadata overhead of per-row versus per-tensor metadata across different model sizes to determine optimal granularity.
3. Implement SIMD-optimized encoding/decoding on different CPU architectures to validate performance claims.