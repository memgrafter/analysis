---
ver: rpa2
title: Failure Modes of LLMs for Causal Reasoning on Narratives
arxiv_id: '2410.23884'
source_url: https://arxiv.org/abs/2410.23884
tags:
- causal
- narrative
- graph
- narratives
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work examines how large language models perform causal reasoning
  on narratives, focusing on how their world knowledge and logical reasoning interact.
  The authors construct controlled synthetic and semi-synthetic datasets with underlying
  causal graphs, and test models on tasks like inferring causality between events
  and reconstructing the causal graph.
---

# Failure Modes of LLMs for Causal Reasoning on Narratives

## Quick Facts
- arXiv ID: 2410.23884
- Source URL: https://arxiv.org/abs/2410.23884
- Authors: Khurram Yamin; Shantanu Gupta; Gaurav R. Ghosal; Zachary C. Lipton; Bryan Wilder
- Reference count: 40
- Primary result: LLMs rely on positional ordering heuristics and parametric knowledge, leading to failures in causal reasoning; graph extraction improves robustness.

## Executive Summary
This paper investigates how large language models perform causal reasoning on narratives, revealing that models often rely on superficial heuristics like event order and memorized world knowledge rather than genuine causal understanding. The authors construct controlled synthetic and semi-synthetic datasets with underlying causal graphs and test models on tasks such as inferring causality between events and reconstructing causal graphs. They find that models fail when events are reordered or contradict common-sense, but explicitly asking the model to extract a causal graph before answering improves robustness, even in complex or reverse narrative orders. Performance degrades with longer narratives and complex graph structures, but graph extraction mitigates many issues.

## Method Summary
The authors create synthetic and semi-synthetic narratives linked to underlying causal graphs, using LLMs to generate events and verbalized stories. They test models (GPT-4o, Claude 3.5 Sonnet, Llama 3.1 8B) on causal reasoning tasks, employing various prompt strategies such as Standard QA, Chain-of-Thought, In-Context Learning, Explicit Causal Graph Extraction, and Narrative-Augmented Graph Extraction. The experiments compare LLM answers to ground truth causal graphs, analyzing performance across topological order, parametric knowledge conflicts, and graph complexity. Results show that positional biases and parametric knowledge shortcuts cause reasoning failures, while graph extraction improves accuracy.

## Key Results
- LLMs rely heavily on positional ordering heuristics, assuming earlier events are causes and later events are effects.
- Models default to parametric knowledge when it conflicts with narrative content, overriding explicit context.
- Extracting a causal graph before reasoning significantly improves LLM performance and robustness, even in reverse or complex narrative orders.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs rely on positional ordering heuristics when inferring causal relationships in narratives.
- Mechanism: When events are narrated in the same order as the underlying causal graph (forward topological order), models default to associating earlier events with causes and later events with effects. This heuristic breaks down in reverse ordering because the expected positional pattern is violated.
- Core assumption: The model treats narrative position as a strong signal for causal direction, overriding explicit content cues.
- Evidence anchors:
  - [abstract] "models often rely on superficial heuristics—for example, inferring causality from event order"
  - [section] "LLMs are influenced heavily by a prior that causes are likely to appear before effects in a narrative"
  - [corpus] Weak evidence—no direct citations but aligns with known LLM positional biases
- Break condition: When explicit causal markers (e.g., "because," "led to") are present, the positional heuristic may be overridden.

### Mechanism 2
- Claim: LLMs default to parametric knowledge when it conflicts with narrative content.
- Mechanism: If the model has a strong memorized association that event A causes event B, but the narrative implies the opposite, the model will still answer based on its parametric knowledge rather than the narrative.
- Core assumption: Parametric knowledge acts as a shortcut that is more salient than narrative context.
- Evidence anchors:
  - [abstract] "recalling memorized world knowledge without attending to context"
  - [section] "LLMs use their parametric causal knowledge...as a shortcut to answer causal questions"
  - [corpus] Weak evidence—relies on internal model behavior not externally validated
- Break condition: If the narrative explicitly contradicts parametric knowledge with strong linguistic cues, the model may adapt.

### Mechanism 3
- Claim: Isolating causal graph extraction from reasoning improves performance by preventing shortcut interference.
- Mechanism: When asked to first construct a causal graph from the narrative, the model engages in more deliberate reasoning. This extracted graph can then be used to answer causal questions without the noise of the original narrative triggering positional or parametric shortcuts.
- Core assumption: The act of graph construction forces the model to attend to all relationships in the narrative systematically.
- Evidence anchors:
  - [abstract] "simple reformulations of the task can elicit more robust reasoning behavior"
  - [section] "asking the LLM to extract the entire causal graph implied by the narrative results in a high degree of success"
  - [corpus] Weak evidence—experimental results not externally validated
- Break condition: If the graph extraction step is followed by narrative+graph joint reasoning, the benefits dissipate.

## Foundational Learning

- Concept: Causal chain graphs and topological ordering
  - Why needed here: The experiments manipulate whether the narrative matches the causal graph's topological order, which is central to the failure modes studied.
  - Quick check question: In a chain A→B→C, what would be the reverse topological order for narration?
- Concept: Parametric knowledge and its role in reasoning
  - Why needed here: The paper shows that models rely on memorized causal associations, which can conflict with narrative content.
  - Quick check question: If a model knows "smoking causes cancer" but reads a narrative where cancer causes smoking, which will it likely answer?
- Concept: Collider and fork structures in causal graphs
  - Why needed here: The paper tests performance on more complex graph structures beyond simple chains.
  - Quick check question: In a collider structure A→C←B, what is the causal relationship between A and B?

## Architecture Onboarding

- Component map: Synthetic narrative generator → Causal graph ground truth → LLM narrative creation → LLM causal reasoning task → Evaluation against ground truth
- Critical path: Generate events → Create causal graph → Generate narrative → Prompt LLM for causal graph extraction → Use extracted graph to answer causal questions → Compare with ground truth
- Design tradeoffs: Synthetic data offers control but may not reflect real-world complexity; real-world data is messier but more ecologically valid
- Failure signatures: Accuracy drops significantly in reverse topological order; performance degrades with narrative length; reliance on parametric knowledge over narrative content
- First 3 experiments:
  1. Generate a simple causal chain (e.g., A→B→C), create forward and reverse narratives, test LLM causal reasoning accuracy
  2. Create narratives with parametric knowledge conflicts (e.g., cancer→longer life), test whether LLM relies on parametric knowledge
  3. Test performance on narratives with collider structures, comparing simple vs complex graph reasoning accuracy

## Open Questions the Paper Calls Out

- Can we improve causal reasoning in LLMs by fine-tuning on synthetic causal reasoning tasks that explicitly target the identified failure modes (positional bias and parametric knowledge interference)?
- How do LLMs perform on causal reasoning tasks involving counterfactual scenarios, and do the identified failure modes (positional bias and parametric knowledge interference) persist in such settings?
- Does explicitly identifying and using the causal graph (Graph prompting) consistently improve LLM performance across all types of causal structures (chains, forks, colliders) and narrative complexities?

## Limitations

- The synthetic data generation process may introduce artifacts that don't generalize to naturally occurring text.
- The human verification process for narrative coherence is not fully specified, making it difficult to assess the quality threshold used.
- Performance differences between LLMs are not systematically analyzed.

## Confidence

- High confidence: Positional ordering heuristics affect LLM causal reasoning performance
- Medium confidence: Graph extraction as a reformulation strategy improves robustness
- Medium confidence: Parametric knowledge conflicts cause reasoning failures

## Next Checks

1. Replicate core experiments using naturally occurring narratives from news articles or literature to test generalization beyond synthetic data.
2. Systematically compare performance across different LLM architectures to identify whether failure modes are model-specific or universal.
3. Test the graph extraction strategy with human subjects to validate whether the improvement is due to the methodology or potential artifacts in LLM behavior.