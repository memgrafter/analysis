---
ver: rpa2
title: 'Adaptive Learn-then-Test: Statistically Valid and Efficient Hyperparameter
  Selection'
arxiv_id: '2409.15844'
source_url: https://arxiv.org/abs/2409.15844
tags:
- altt
- hyperparameter
- control
- testing
- fwer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces adaptive learn-then-test (aLTT), a hyperparameter
  selection method that provides finite-sample statistical guarantees on AI model
  performance. Unlike the existing learn-then-test (LTT) approach, which uses conventional
  p-value-based multiple hypothesis testing, aLTT implements sequential data-dependent
  testing with early termination using e-processes.
---

# Adaptive Learn-then-Test: Statistically Valid and Efficient Hyperparameter Selection

## Quick Facts
- arXiv ID: 2409.15844
- Source URL: https://arxiv.org/abs/2409.15844
- Authors: Matteo Zecchin; Sangwoo Park; Osvaldo Simeone
- Reference count: 24
- Primary result: aLTT achieves same performance as LTT with significantly fewer testing rounds using sequential data-dependent testing with e-processes

## Executive Summary
This paper introduces adaptive learn-then-test (aLTT), a hyperparameter selection method that provides finite-sample statistical guarantees on AI model performance. Unlike the existing learn-then-test (LTT) approach, which uses conventional p-value-based multiple hypothesis testing, aLTT implements sequential data-dependent testing with early termination using e-processes. The method is particularly effective for scenarios where testing is costly or presents safety risks.

The authors demonstrate aLTT's effectiveness in two practical applications: online policy selection for offline reinforcement learning and resource allocation for wireless engineering. In both cases, aLTT achieves the same performance as LTT while requiring only a fraction of the testing rounds. For instance, in the reinforcement learning task, aLTT achieves a true positive rate of up to 0.85 compared to LTT's 0.32 when using an adaptive acquisition policy.

## Method Summary
aLTT introduces a novel approach to hyperparameter selection by replacing conventional multiple hypothesis testing with sequential data-dependent testing using e-processes. The method adaptively selects hyperparameters while maintaining statistical validity through rigorous control of family-wise error rate (FWER) and false discovery rate (FDR). The key innovation lies in the use of e-processes for sequential testing, which allows for early termination of unpromising hyperparameter configurations while preserving statistical guarantees.

The algorithm operates by testing multiple hypotheses sequentially, where each hypothesis corresponds to a hyperparameter configuration. The e-process framework enables adaptive stopping rules that terminate tests when sufficient evidence has been gathered to either accept or reject a hypothesis, significantly reducing the number of testing rounds required compared to traditional methods.

## Key Results
- In reinforcement learning tasks, aLTT achieved true positive rates up to 0.85 compared to LTT's 0.32 with adaptive acquisition policy
- Achieved same statistical guarantees (FWER and FDR control) as LTT while requiring significantly fewer testing rounds
- Demonstrated effectiveness in both online policy selection for reinforcement learning and resource allocation for wireless engineering applications

## Why This Works (Mechanism)
aLTT works by leveraging e-processes for sequential hypothesis testing, which allows for data-dependent early stopping while maintaining statistical validity. The method adaptively focuses computational resources on promising hyperparameter configurations while quickly discarding poor ones, achieving efficiency gains without sacrificing statistical rigor.

## Foundational Learning
- **E-processes**: Sequential probability measures that maintain validity under optional stopping - needed for adaptive testing without compromising statistical guarantees
- **Family-wise error rate (FWER)**: Probability of making at least one false discovery among all hypotheses - crucial for controlling overall error in multiple testing
- **False discovery rate (FDR)**: Expected proportion of false discoveries among all rejected hypotheses - important metric for balancing discovery and error
- **Sequential testing**: Testing hypotheses one at a time with data-dependent stopping rules - enables computational efficiency through early termination
- **Hyperparameter selection**: Process of choosing optimal model parameters - fundamental to machine learning model performance

## Architecture Onboarding
- **Component map**: Hyperparameter configurations -> Sequential testing pipeline -> E-process evaluation -> Early stopping decision -> Selected configuration
- **Critical path**: Configuration selection -> Test initiation -> Data collection -> E-process update -> Stopping criterion evaluation -> Configuration acceptance/rejection
- **Design tradeoffs**: Statistical rigor vs. computational efficiency, early stopping vs. completeness of evaluation, adaptability vs. implementation complexity
- **Failure signatures**: Premature termination of good configurations, failure to achieve FWER/FDR control, computational overhead exceeding traditional methods
- **First experiments**: 1) Test on synthetic data with known optimal configurations, 2) Compare convergence rates with LTT on simple classification tasks, 3) Validate FWER control on controlled hyperparameter spaces

## Open Questions the Paper Calls Out
None

## Limitations
- Practical scalability to high-dimensional hyperparameter spaces remains uncertain, particularly for applications with dozens or hundreds of hyperparameters
- Implementation complexity of e-process-based sequential testing may present challenges in real-world deployment
- Performance improvements may vary significantly across different problem domains and hyperparameter spaces

## Confidence
- Statistical validity: High - rigorous FWER and FDR control demonstrated in tested scenarios
- Computational efficiency: Medium - relative performance improvements vary across domains
- Generalizability: Medium - strong evidence in tested domains but needs validation across diverse ML tasks

## Next Checks
1. Test aLTT on hyperparameter optimization problems with >50 hyperparameters to evaluate scalability limits and performance degradation
2. Implement the method across diverse ML domains (e.g., computer vision, NLP) to assess robustness across different model architectures
3. Conduct ablation studies comparing e-process-based early stopping with alternative sequential testing approaches under varying computational budgets