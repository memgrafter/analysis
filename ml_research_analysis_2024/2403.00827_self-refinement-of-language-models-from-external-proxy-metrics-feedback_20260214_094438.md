---
ver: rpa2
title: Self-Refinement of Language Models from External Proxy Metrics Feedback
arxiv_id: '2403.00827'
source_url: https://arxiv.org/abs/2403.00827
tags:
- response
- final
- metrics
- user
- initial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ProMiSe, an algorithm that enables language
  models to self-refine responses using external feedback from proxy metrics. The
  approach performs iterative refinement along specific principles like specificity,
  faithfulness, and relevance by leveraging few-shot exemplars and metric thresholding.
---

# Self-Refinement of Language Models from External Proxy Metrics Feedback

## Quick Facts
- arXiv ID: 2403.00827
- Source URL: https://arxiv.org/abs/2403.00827
- Authors: Keshav Ramji; Young-Suk Lee; Ramón Fernandez Astudillo; Md Arafat Sultan; Tahira Naseem; Asim Munawar; Radu Florian; Salim Roukos
- Reference count: 40
- One-line primary result: ProMiSe algorithm enables iterative self-refinement of language model responses using external proxy metrics, showing consistent improvements across multiple evaluation metrics on document-grounded question answering tasks.

## Executive Summary
This paper introduces ProMiSe, an algorithm that enables language models to self-refine responses using external feedback from proxy metrics. The approach performs iterative refinement along specific principles like specificity, faithfulness, and relevance by leveraging few-shot exemplars and metric thresholding. Experiments on MultiDoc2Dial and QuAC datasets with Flan-T5-XXL and Llama-2-13B-Chat models show consistent performance improvements across five automatic metrics and GPT-4-as-a-judge evaluations.

## Method Summary
ProMiSe is an iterative self-refinement algorithm that uses external proxy metrics to guide response improvement. The method begins with best-of-N sampling to generate multiple initial responses, which are evaluated against principle-specific metrics (ROUGE for specificity and faithfulness, WeCheck for factual consistency). Responses below metric thresholds are iteratively refined using few-shot exemplars targeting specific quality principles. The algorithm uses weighted metric scoring to determine improvement and continues refinement until all metrics meet thresholds or improvement falls below a stopping criterion. The approach is evaluated on document-grounded question answering and dialogue generation tasks.

## Key Results
- ProMiSe consistently improves performance across five automatic metrics (Rouge-L, BERT-Recall, BERT K-Precision, Recall, K-Precision) on MultiDoc2Dial and QuAC datasets
- GPT-4-as-a-judge evaluations confirm quality improvements from iterative refinement
- Synthetic dialogue data generated using ProMiSe shows higher quality and improves downstream fine-tuning results
- Both Flan-T5-XXL and Llama-2-13B-Chat models benefit from the self-refinement approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative refinement guided by principle-specific proxy metrics improves response quality.
- Mechanism: The algorithm evaluates initial responses against multiple quality metrics (e.g., ROUGE, WeCheck), and iteratively refines responses one principle at a time using few-shot exemplars when metrics fall below thresholds.
- Core assumption: Proxy metrics correlate with response quality and can serve as effective feedback signals for self-improvement.
- Evidence anchors:
  - [abstract] "ProMiSe leverages feedback on response quality through principle-specific proxy metrics, and iteratively refines its response one principle at a time"
  - [section] "Determining Improvement...The user assigns weights w = [w1, w2, ..., w|T |] for the respective metrics in T"
- Break condition: When all proxy metrics exceed their respective thresholds, or when weighted improvement fails to meet the λ threshold.

### Mechanism 2
- Claim: Best-of-N sampling combined with metric-based selection yields better initial responses.
- Mechanism: Multiple initial responses are generated and scored across proxy metrics, with the response scoring highest on the most metrics selected as the initial best response.
- Core assumption: Sampling diversity improves the chances of finding a response that meets multiple quality criteria simultaneously.
- Evidence anchors:
  - [section] "We perform Best-of-N sampling to yield a set of responses, Y0, from Language Model M...The initial generation prompt consists of an instruction and optional in-context demonstrations"
  - [section] "We determine the quality of the sampled responses based on a set of proxy metrics determined a priori...Each response in Y0 is scored with each metric mt in T"
- Break condition: When a response meets all metric thresholds or when refinement iterations are exhausted.

### Mechanism 3
- Claim: Combining multiple proxy metrics (ROUGE + WeCheck) provides complementary feedback for different quality dimensions.
- Mechanism: ROUGE metrics capture lexical/semantic overlap (specificity, faithfulness), while WeCheck captures factual consistency, providing a multi-faceted evaluation framework.
- Core assumption: Different proxy metrics capture different aspects of response quality, and combining them yields more comprehensive feedback than any single metric.
- Evidence anchors:
  - [section] "We select three ROUGE metrics intended to correspond to each of the three principles...WeCheck: Factual Consistency Checker"
  - [section] "If we use only WeCheck, rejection sampling is performed to yield the highest scoring response according to WeCheck"
- Break condition: When responses meet thresholds on all selected proxy metrics or when weighted improvement across metrics is insufficient.

## Foundational Learning

- Concept: Proxy metrics and their correlation with response quality
  - Why needed here: Understanding how different metrics (ROUGE, WeCheck) capture specific quality dimensions is crucial for designing effective refinement prompts
  - Quick check question: How does ROUGE-L between response and document differ from ROUGE-L between response and query in terms of quality dimensions captured?

- Concept: Few-shot learning and in-context exemplars
  - Why needed here: The algorithm relies on providing principle-specific refinement exemplars to guide the model's self-improvement process
  - Quick check question: What are the key components of an effective refinement exemplar for teaching specificity improvement?

- Concept: Metric thresholding and sufficiency criteria
  - Why needed here: Setting appropriate thresholds determines when refinement stops and affects the balance between quality and computational cost
  - Quick check question: How would setting overly high vs. overly low thresholds affect refinement outcomes and computational efficiency?

## Architecture Onboarding

- Component map: Input processor (documents, conversation history) → Initial response generator (Best-of-N sampling) → Proxy metric evaluator (ROUGE, WeCheck, custom metrics) → Refinement engine (iterative, principle-specific) → Threshold comparator (sufficiency check) → Output selector (final response choice)

- Critical path: Document/Context → Initial Response Generation → Proxy Metric Evaluation → Threshold Check → (If insufficient) Iterative Refinement → Final Response

- Design tradeoffs:
  - Number of initial samples (N) vs. computational cost
  - Number of proxy metrics vs. refinement specificity
  - Refinement iteration limit vs. quality improvement
  - Threshold stringency vs. refinement frequency

- Failure signatures:
  - Refinement loops without improvement (stuck in local optima)
  - Metric score improvements without quality gains
  - Excessive response length inflation
  - Loss of original context or relevance

- First 3 experiments:
  1. Vary the number of initial samples (N) from 5 to 50 and measure quality improvement and computational cost
  2. Test different combinations of proxy metrics (ROUGE-only, WeCheck-only, combined) on a validation set
  3. Experiment with different threshold values for the same metric set to find optimal balance between quality and efficiency

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- Generalizability across domains and tasks remains unclear as the method is only validated on document-grounded question answering and dialogue generation tasks
- Computational overhead from iterative refinement is not analyzed, making it difficult to assess practical efficiency trade-offs
- Potential for reward hacking and metric gaming is acknowledged but not systematically evaluated

## Confidence
- High confidence: The core mechanism of iterative refinement guided by external proxy metrics is well-supported by experimental results showing consistent improvements across multiple evaluation metrics and model sizes.
- Medium confidence: The effectiveness of combining ROUGE and WeCheck metrics for capturing complementary quality dimensions is supported by results but lacks deeper analysis of how these metrics interact.
- Low confidence: The claim that ProMiSe improves downstream fine-tuning results from synthetic dialogue data is mentioned but not deeply analyzed.

## Next Checks
1. **Domain generalization test**: Apply ProMiSe to at least two additional domains (e.g., code generation and creative writing) with appropriate domain-specific proxy metrics, and compare performance against baseline self-refinement methods to assess cross-domain robustness.

2. **Efficiency benchmarking**: Measure wall-clock time and computational resources for ProMiSe across different response lengths and document sizes, and compare against simple baseline methods to quantify the practical trade-off between quality gains and computational overhead.

3. **Metric correlation analysis**: Conduct ablation studies removing individual proxy metrics to determine whether ROUGE and WeCheck are truly complementary or if some metrics contribute negligible value, and test for evidence of reward hacking by analyzing whether metric improvements align with human judgments of quality.