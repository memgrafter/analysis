---
ver: rpa2
title: 'Reliable Evaluation of Attribution Maps in CNNs: A Perturbation-Based Approach'
arxiv_id: '2411.14946'
source_url: https://arxiv.org/abs/2411.14946
tags:
- evaluation
- attribution
- image
- maps
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel perturbation-based approach for evaluating
  attribution maps in CNNs, addressing the distributional shift problem inherent in
  widely used insertion/deletion metrics. By replacing pixel modifications with adversarial
  perturbations (specifically FGSM), the method provides a more robust evaluation
  framework that maintains closer proximity to the original image distribution.
---

# Reliable Evaluation of Attribution Maps in CNNs: A Perturbation-Based Approach

## Quick Facts
- arXiv ID: 2411.14946
- Source URL: https://arxiv.org/abs/2411.14946
- Reference count: 40
- Key outcome: Introduced perturbation-based approach for attribution map evaluation, achieving 96.7% monotonicity and 0.891 smoothness scores, significantly outperforming deletion/insertion metrics

## Executive Summary
This paper addresses the fundamental problem of distributional shift in attribution map evaluation by introducing a perturbation-based approach. Traditional deletion/insertion metrics suffer from unreliable score fluctuations because they drastically alter the image distribution through pixel masking or insertion. The proposed method replaces these operations with adversarial perturbations (specifically FGSM with ε = 1/255), which maintain closer proximity to the original image distribution while still enabling meaningful evaluation of attribution maps. The evaluation employs smoothness and monotonicity measures and demonstrates superior performance across 15 dataset-architecture combinations using 16 different attribution methods.

## Method Summary
The method replaces traditional pixel masking (deletion) and insertion operations with adversarial perturbations generated using FGSM. The perturbation magnitude is constrained to ε = 1/255 to minimize distribution shift while maintaining effectiveness. Attribution maps are evaluated by gradually undoing perturbations in the order specified by the attribution map values, measuring the resulting probability changes. The evaluation framework computes area under curve (AUC) values and assesses reliability through smoothness (standard deviation of first derivative) and monotonicity (percentage of monotonic increase/decrease) metrics. The approach is tested across 16 attribution methods, 3 datasets, and 5 architectures, totaling 15 dataset-architecture combinations.

## Key Results
- Achieved 96.7% monotonicity and 0.891 smoothness scores, significantly outperforming deletion/insertion metrics
- Demonstrated increased consistency across datasets and architectures with Kendall's τ correlation of 0.466 ± 0.252 compared to deletion (0.236 ± 0.409) and insertion (0.353 ± 0.431)
- SmoothGrad identified as the best-performing attribution map method
- Only metric to pass all sanity checks using baseline attribution maps (Canny edge detector and uniform random)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing pixel masking with adversarial perturbations reduces distribution shift, making attribution evaluation more reliable
- Mechanism: Traditional deletion/insertion methods alter the image distribution drastically by masking or inserting pixels. FGSM perturbations with small epsilon (±1/255) make minimal changes, staying close to the original image distribution. This allows the model to behave consistently across evaluations, reducing unreliable score fluctuations
- Core assumption: The model has seen perturbations of this magnitude (±1/255) during training, so the perturbed image remains within the training distribution
- Evidence anchors:
  - [abstract]: "Our method proposes to replace pixel modifications with adversarial perturbations, which provides a more robust evaluation framework."
  - [section]: "We opt for discrete images to remain as close as possible to the original image distribution... The FGSM attack mentioned earlier aligns with this requirement because it introduces ±1 perturbations for all pixels when ϵ = 1/255, satisfying the minimal perturbation criterion."
  - [corpus]: No relevant corpus evidence; this is a novel approach not covered in the neighbor papers
- Break condition: If the perturbation magnitude is too large (epsilon > 1/255), the distribution shift returns and evaluation scores become unreliable again

### Mechanism 2
- Claim: Adversarial perturbations ensure the attribution map evaluation remains monotonic and smooth, unlike deletion/insertion metrics
- Mechanism: When perturbations are applied systematically and undone in the order dictated by the attribution map, the probability change curve is smooth and monotonic. This contrasts with deletion/insertion curves, which show jumps and non-monotonic behavior due to abrupt pixel changes
- Core assumption: The adversarial perturbation correctly identifies regions that the model depends on for its prediction, so removing the perturbation in that order will gradually restore the original probability
- Evidence anchors:
  - [abstract]: "By using smoothness and monotonicity measures, we illustrate the effectiveness of our approach in correcting distribution shifts."
  - [section]: "Unlike Deletion and Insertion, our functions are smooth and monotonic... we define smoothness measures... the function's fluctuation by measuring the standard deviation of its first derivative."
  - [corpus]: No direct corpus evidence; smoothness is a novel evaluation metric introduced here
- Break condition: If the adversarial attack fails to change the model's prediction (low success rate), the monotonicity property degrades because perturbations don't meaningfully affect the probability

### Mechanism 3
- Claim: The perturbation-based metric is consistent across datasets and architectures, unlike deletion/insertion metrics
- Mechanism: Kendall's τ rank correlation between rankings from different dataset-architecture pairs is higher for the perturbation metric. This consistency arises because the perturbation method is less sensitive to dataset-specific artifacts and maintains the same evaluation behavior
- Core assumption: Attribution methods are general techniques that should perform similarly across different data and architectures, so a good evaluation metric should reflect that consistency
- Evidence anchors:
  - [abstract]: "Using Kendall's τ rank correlation coefficient, we show the increased consistency of our metric across 15 dataset-architecture combinations."
  - [section]: "We computed for all four metrics the pairwise Kendall's τ... the results can be seen in table 3... our metric is the most consistent of the four."
  - [corpus]: No relevant corpus evidence; this cross-dataset consistency analysis is unique to this paper
- Break condition: If the perturbation method is not applicable to certain architectures (e.g., transformers), consistency may break down

## Foundational Learning

- Concept: Adversarial perturbations (FGSM)
  - Why needed here: The core of the evaluation method relies on generating small perturbations that fool the model, allowing systematic evaluation of attribution maps
  - Quick check question: What is the formula for FGSM perturbation, and why is ℓ∞ norm used?
- Concept: Kendall's τ rank correlation
  - Why needed here: To measure the consistency of attribution map rankings across different datasets and architectures
  - Quick check question: How does Kendall's τ differ from Pearson correlation, and why is it more appropriate for ranking comparison?
- Concept: Monotonicity and smoothness in evaluation curves
  - Why needed here: These properties ensure that the evaluation metric is reliable and not subject to random fluctuations
  - Quick check question: How do you compute monotonicity and smoothness from an evaluation curve?

## Architecture Onboarding

- Component map: Pre-trained CNN model -> Attribution map generator -> Adversarial perturbation generator (FGSM) -> Evaluation curve builder -> Monotonicity/smoothness calculators -> Consistency analyzer (Kendall's τ)
- Critical path: Attribution map → Perturbation application → Probability measurement → AUC calculation → Consistency check
- Design tradeoffs: Using stronger attacks (PGD) increases success rate but may introduce more distribution shift. Smaller epsilon reduces shift but may fail to fool the model
- Failure signatures: If monotonicity drops below ~0.9, the perturbation is too strong. If smoothness increases, the distribution shift is significant. If Kendall's τ is low, the metric is inconsistent
- First 3 experiments:
  1. Implement FGSM perturbation on a small image set and verify that probability changes smoothly when undoing perturbations
  2. Compare monotonicity and smoothness of perturbation-based evaluation vs deletion/insertion on a single dataset
  3. Run Kendall's τ analysis across two dataset-architecture pairs to confirm consistency improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the perturbation-based evaluation method maintain its reliability and consistency when applied to transformer-based architectures and sequence-based models?
- Basis in paper: [inferred] The paper notes that "transformers and other sequence-based architectures could be considered for further work" and evaluates only CNN architectures
- Why unresolved: The evaluation framework has been validated only for convolutional neural networks, not for architectures that process sequential or non-grid-based data
- What evidence would resolve it: Testing the perturbation-based evaluation method on transformer models (e.g., ViT, BERT) across multiple datasets and comparing consistency scores (Kendall's τ) with those obtained from CNN evaluations

### Open Question 2
- Question: What is the optimal epsilon value (ϵ) that balances minimal distribution shift while maintaining high attack success rates across different datasets and model architectures?
- Basis in paper: [explicit] The paper discusses different epsilon values (1/255, 2/255, 4/255, 8/255) and their effects on monotonicity and smoothness, noting that higher values cause more distribution shift
- Why unresolved: The paper uses ϵ = 1/255 as a default but acknowledges trade-offs with stronger attacks, without identifying an optimal value that works universally
- What evidence would resolve it: Systematic evaluation of monotonicity and smoothness metrics across all dataset-architecture combinations for different epsilon values, identifying the value that maximizes both metrics while maintaining acceptable attack success rates

### Open Question 3
- Question: How do black-box adversarial attacks compare to gradient-based attacks (FGSM/PGD) in terms of maintaining original image distribution while evaluating attribution maps?
- Basis in paper: [explicit] The paper states that "opting for a black-box attack over a gradient-based approach like FGSM comes with the drawback of potentially producing images that are more distant from the data manifold" but does not test black-box attacks
- Why unresolved: The paper only uses gradient-based attacks and suggests black-box attacks might be worse but does not empirically validate this claim
- What evidence would resolve it: Comparative evaluation using both gradient-based and black-box attacks on the same attribution maps, measuring distribution shift through histogram comparison and evaluating the impact on monotonicity, smoothness, and consistency metrics

### Open Question 4
- Question: Can the perturbation-based evaluation method effectively handle cases where non-existence of objects determines the CNN's decision, as mentioned in the paper's limitations?
- Basis in paper: [explicit] The paper explicitly states that "A limitation of both our method and AMs in general is the case when non-existence of objects determines the CNN decision" and that both the evaluation function and attribution maps would fail in such cases
- Why unresolved: The paper acknowledges this limitation but does not propose solutions or test the method's behavior in such scenarios
- What evidence would resolve it: Testing the evaluation method on datasets with negative class definitions (e.g., anomaly detection, object absence detection) and analyzing whether the perturbation-based approach produces meaningful results or fails as predicted

## Limitations

- The method assumes that small adversarial perturbations (±1/255) remain within the model's training distribution, which may not hold for all architectures or datasets
- The consistency improvements measured by Kendall's τ, while significant, still show substantial variance (0.466 ± 0.252), indicating room for improvement
- The smoothness and monotonicity metrics are novel contributions that lack extensive validation against established benchmarks

## Confidence

- High confidence: The distribution shift problem with deletion/insertion metrics is well-established and the perturbation approach logically addresses this
- Medium confidence: The quantitative improvements in consistency (Kendall's τ) and the superiority of SmoothGrad require further validation across additional datasets and architectures
- Medium confidence: The smoothness and monotonicity metrics, while intuitive, need more extensive benchmarking against alternative evaluation frameworks

## Next Checks

1. Test the perturbation-based evaluation on transformer architectures to verify cross-architecture consistency claims
2. Implement ablation studies with varying perturbation magnitudes (ε = 0.5/255, 2/255) to determine the optimal balance between distribution shift and perturbation effectiveness
3. Compare the smoothness and monotonicity metrics against alternative curve-fitting approaches to validate their sensitivity and specificity