---
ver: rpa2
title: Collapse-Proof Non-Contrastive Self-Supervised Learning
arxiv_id: '2410.04959'
source_url: https://arxiv.org/abs/2410.04959
tags:
- learning
- collapse
- cplearn
- self-supervised
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CPLearn, a principled non-contrastive self-supervised
  learning approach that prevents training failure modes through a novel projector
  design and loss function. The method leverages hyperdimensional computing principles
  with large random orthogonal dictionaries to simultaneously decorrelate features
  and encourage clustering, without requiring contrastive objectives or complex heuristics.
---

# Collapse-Proof Non-Contrastive Self-Supervised Learning

## Quick Facts
- arXiv ID: 2410.04959
- Source URL: https://arxiv.org/abs/2410.04959
- Authors: Emanuele Sansone; Tim Lebailly; Tinne Tuytelaars
- Reference count: 40
- One-line primary result: Novel non-contrastive SSL method preventing training collapse through orthogonal dictionaries and principled loss function

## Executive Summary
This paper addresses the challenge of representation collapse in non-contrastive self-supervised learning by proposing CPLearn, a principled approach that leverages hyperdimensional computing principles with large random orthogonal dictionaries. The method simultaneously decorrelates features and encourages clustering without requiring contrastive objectives or complex heuristics. Through theoretical analysis and extensive experiments on SVHN, CIFAR-10/100, and ImageNet-100, CPLearn demonstrates strong performance on both clustering and linear classification tasks while guaranteeing avoidance of representation, cluster, dimensional, and intracluster collapses.

## Method Summary
CPLearn uses a projector with a frozen random orthogonal dictionary W ∈ {-1, 1}^{f×c} where c ≫ f, mapping embeddings to high-dimensional codes. The loss function combines invariance (ensuring augmented views map to same code) and prior-matching (distributing assignments uniformly across codes) terms. This approach guarantees avoidance of four types of collapse: representation (constant embeddings), cluster (few codes used), dimensional (low-rank covariance), and intracluster (high within-cluster variance). The method is trained using standard encoder backbones (ResNet variants, ViT) with Adam optimizer and specific learning rates for each dataset.

## Key Results
- CPLearn achieves strong performance on both clustering (NMI) and linear classification tasks across multiple datasets
- Increasing dictionary size systematically improves generalization, validating theoretical guarantees
- Outperforms existing feature decorrelation and cluster-based methods in non-contrastive SSL setting
- Guarantees avoidance of all four collapse modes through orthogonal dictionary design and principled loss formulation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Orthogonal frozen dictionary with large size prevents dimensional and intracluster collapses
- **Mechanism:** The projector maps representations to a high-dimensional code space via an orthogonal basis. This forces embeddings to align with dictionary codes, ensuring they span the full embedding space and cluster into distinct groups.
- **Core assumption:** Orthogonality of W ensures codes span embedding space; large c ≫ f provides sufficient basis vectors.
- **Evidence anchors:**
  - [abstract] "leverages hyperdimensional computing principles with large random orthogonal dictionaries to simultaneously decorrelate features and encourage clustering"
  - [section] "W ∈ {−1, 1}f ×c with elements drawn i.i.d. from a Rademacher distribution... codes are orthogonal to each other on average"
  - [corpus] Weak - no direct corpus evidence for orthogonal dictionary preventing collapses
- **Break condition:** If W is not quasi-orthogonal (e.g., due to poor random sampling), embeddings may not span full space, leading to dimensional collapse.

### Mechanism 2
- **Claim:** Loss function with invariance and prior-matching terms prevents representation and cluster collapses
- **Mechanism:** Invariance loss ensures augmented views map to same code; prior-matching loss distributes assignments uniformly across codes. Together they prevent constant embeddings and ensure balanced clustering.
- **Core assumption:** Global minima of loss correspond to hard assignments to distinct codes with uniform distribution.
- **Evidence anchors:**
  - [abstract] "minimizing invariance to augmentations while matching priors suffices to avoid representation and cluster collapses"
  - [section] "minimizing invariance to data augmentations while matching priors suffices to avoid representation and cluster collapses"
  - [corpus] Weak - no corpus evidence for specific loss formulation preventing collapses
- **Break condition:** If β is too small, prior-matching term dominates and causes cluster collapse; if too large, invariance term dominates and causes representation collapse.

### Mechanism 3
- **Claim:** Quasi-orthogonality from hyperdimensional computing enables overcomplete dictionaries
- **Mechanism:** Rademacher-distributed codes create a basis that is orthogonal on average, allowing c > f while maintaining decorrelation properties.
- **Core assumption:** Expected cosine similarity between codes is zero with variance 1/f, preserving quasi-orthogonality.
- **Evidence anchors:**
  - [abstract] "leverages hyperdimensional computing principles with large random orthogonal dictionaries"
  - [section] "EW {cos(wj, wj′)} = 0... V arW {cos(wj, wj′)} = 1/f"
  - [corpus] Weak - no corpus evidence for hyperdimensional computing connection
- **Break condition:** If f is too small relative to c, variance increases and quasi-orthogonality degrades, causing decorrelation failure.

## Foundational Learning

- **Concept:** Orthogonality and basis spanning
  - Why needed here: Ensures embeddings span full embedding space and align with dictionary codes
  - Quick check question: If W has rank < f, what happens to embedding decorrelation?
- **Concept:** Loss function convexity and global minima
  - Why needed here: Guarantees existence of minima that avoid collapses through KKT conditions
  - Quick check question: Why can't both invariance and prior-matching losses be minimized simultaneously when representations collapse?
- **Concept:** Hyperdimensional computing and random coding
  - Why needed here: Enables overcomplete dictionaries while maintaining quasi-orthogonality
  - Quick check question: How does increasing c affect the variance of cosine similarity between codes?

## Architecture Onboarding

- **Component map:** Input -> Backbone -> Projector (L2-norm + BN + Linear + Softmax) -> Code assignment -> Loss (invariance + prior-matching) -> Gradient backprop
- **Critical path:** Input → Backbone → Projector → Embedding → Code assignment → Loss computation → Gradient backprop
- **Design tradeoffs:** Larger c improves decorrelation but increases memory; orthogonal W ensures properties but requires careful initialization
- **Failure signatures:** Representation collapse (constant embeddings), cluster collapse (few codes used), dimensional collapse (low-rank covariance), intracluster collapse (high within-cluster variance)
- **First 3 experiments:**
  1. Test dimensional collapse detection: Compute singular value spectrum of embedding covariance with varying c
  2. Test cluster collapse detection: Monitor KL divergence between empirical and uniform code distributions
  3. Test decorrelation: Compute condition number of embedding covariance matrix across training epochs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CPLearn scale with extremely large dictionary sizes (e.g., c >> f²) in terms of both computational cost and downstream task performance?
- Basis in paper: [explicit] The paper demonstrates that increasing dictionary size improves performance, but focuses on moderate increases (c = f to c = 16384) and mentions future work on scalability.
- Why unresolved: The paper only tests up to c = 16384, and theoretical guarantees may not hold for extremely large c. Computational challenges and potential diminishing returns at very large scales are not explored.
- What evidence would resolve it: Experiments with dictionary sizes orders of magnitude larger than f, measuring both training time/storage requirements and downstream task performance (clustering accuracy, linear probing accuracy).

### Open Question 2
- Question: Can the quasi-orthogonal dictionary property be further improved by using more sophisticated random code generation methods beyond simple Rademacher distributions?
- Basis in paper: [explicit] The paper notes that "several ways to define random code vectors in HC" exist and chooses multiply-add-permute encoding, but leaves exploring other encodings for future work.
- Why unresolved: The paper only uses one random code generation method and does not compare against alternatives or theoretically analyze whether other methods could provide better orthogonality properties.
- What evidence would resolve it: Comparative experiments testing different random code generation methods (e.g., circulant codes, bipolar codes) and theoretical analysis of their orthogonality properties and impact on embedding quality.

### Open Question 3
- Question: How robust is CPLearn to non-uniform prior distributions (q ≠ uniform) and what is the optimal strategy for choosing q in different datasets?
- Basis in paper: [explicit] The paper mentions that "while the results of the Lemma are general and valid for any prior distribution q, our focus is mainly on the uniform setting" and leaves non-uniform case study to future work.
- Why unresolved: The theoretical analysis assumes uniform prior, and experiments only use uniform priors. The impact of different prior distributions on collapse avoidance and downstream performance is unexplored.
- What evidence would resolve it: Experiments comparing uniform vs non-uniform priors (e.g., class-balanced, data-driven priors) and theoretical analysis of how different priors affect the optimality conditions and collapse avoidance guarantees.

### Open Question 4
- Question: What is the relationship between the β hyperparameter and the trade-off between invariance and clustering in CPLearn, and how can this be optimized systematically?
- Basis in paper: [explicit] The paper notes that "excessively large values of β lead to cluster collapse" and conducts sensitivity analysis, but does not provide a systematic optimization strategy.
- Why unresolved: The paper only tests discrete β values and observes qualitative effects, but does not analyze the underlying trade-off or provide principled methods for hyperparameter selection.
- What evidence would resolve it: Theoretical analysis of the β-invariance-clustering trade-off and automated hyperparameter optimization methods (e.g., Bayesian optimization) that can adapt β during training based on collapse indicators.

## Limitations

- Theoretical guarantees rely heavily on quasi-orthogonality of random dictionary, but empirical evidence for preventing all collapse modes is limited
- Connection to hyperdimensional computing, while elegant, lacks extensive empirical validation showing specific benefits over other overcomplete dictionary methods
- Loss function formulation assumes global minima correspond to desired solutions, but relationship between local minima and collapse modes is not fully characterized

## Confidence

- **High confidence:** The orthogonal dictionary design with large c ≫ f prevents dimensional collapse by construction
- **Medium confidence:** The loss function prevents representation and cluster collapses based on theoretical analysis, but practical behavior with different β values needs more empirical validation
- **Low confidence:** The claim that increasing c systematically improves generalization is supported by CIFAR experiments but lacks theoretical backing and broader validation

## Next Checks

1. Perform comprehensive ablation study varying dictionary size c and sparsity λ to quantify their effects on each collapse mode across multiple datasets
2. Analyze the empirical distribution of singular values in embedding covariance matrices during training to verify dimensional collapse prevention across all experimental conditions
3. Conduct experiments with non-orthogonal dictionaries to measure the degradation in collapse prevention and quantify the importance of the quasi-orthogonality assumption