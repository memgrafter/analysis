---
ver: rpa2
title: 'Investigating Context-Faithfulness in Large Language Models: The Roles of
  Memory Strength and Evidence Style'
arxiv_id: '2409.10955'
source_url: https://arxiv.org/abs/2409.10955
tags:
- evidence
- llms
- memory
- question
- strength
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how Large Language Models (LLMs) handle
  conflicting information when retrieving external evidence. The authors propose measuring
  memory strength by evaluating the consistency of LLM responses to paraphrased versions
  of the same question, which previous work had not considered.
---

# Investigating Context-Faithfulness in Large Language Models: The Roles of Memory Strength and Evidence Style

## Quick Facts
- arXiv ID: 2409.10955
- Source URL: https://arxiv.org/abs/2409.10955
- Reference count: 40
- Primary result: Memory strength strongly influences context faithfulness in LLMs, with paraphrased evidence increasing receptiveness to external information.

## Executive Summary
This study investigates how Large Language Models handle conflicting information when retrieving external evidence, revealing that memory strength and evidence presentation style significantly affect context faithfulness. The authors introduce a novel approach to measure memory strength by evaluating LLM response consistency to paraphrased questions, which previous work had not considered. They demonstrate that stronger memory correlates with greater resistance to contradictory evidence, with GPT-4 relying on internal knowledge for nearly 50% of high-memory-strength questions. The research also shows that paraphrasing evidence substantially improves LLM receptiveness compared to simple repetition or indirect presentation, providing practical insights for improving retrieval-augmented generation systems.

## Method Summary
The study employs a multi-stage methodology using two datasets (popQA and Natural Questions) across six LLM models. Memory strength is quantified by generating seven paraphrased versions of each question, clustering LLM responses, and calculating consistency using negative entropy. Context faithfulness is evaluated through a multiple-choice format presenting memory answers (MA), counter-memory answers (CMA), and uncertain answers (UCT), with evidence generated in direct, paraphrased, and indirect styles. The approach includes rigorous entailment and contradiction checks using NLI models to ensure evidence quality, and examines how different memory strength groups respond to various evidence presentations.

## Key Results
- Memory strength strongly correlates with context faithfulness: stronger memories lead to greater resistance to contradictory evidence
- GPT-4 answered nearly 50% of high-memory-strength questions using internal knowledge despite contradictory evidence
- Paraphrased evidence significantly increases LLM receptiveness compared to simple repetition or indirect presentation
- Evidence style affects receptiveness: paraphrased versions of the same evidence outperform adding details or indirect presentation

## Why This Works (Mechanism)
The mechanism underlying these findings relates to how LLMs balance internal knowledge against external evidence. When memory strength is high (consistent responses to paraphrased questions), LLMs develop stronger confidence in their internal knowledge, making them more resistant to contradictory information. The effectiveness of paraphrasing stems from presenting familiar information in a novel format that bypasses the model's learned resistance to repeated or contradictory statements. This suggests LLMs treat paraphrased evidence as fresh information rather than recognizing it as semantically equivalent to previously seen content, thereby reducing the defensive response against contradiction.

## Foundational Learning
**Memory Strength Calculation**: Measuring consistency of LLM responses to paraphrased questions using negative entropy - needed to quantify how confidently an LLM holds certain knowledge; check by manually verifying answer clusters for coherence.
**Paraphrasing for Semantic Equivalence**: Generating multiple question variants while maintaining meaning - needed to test knowledge consistency; check using NLI models to confirm equivalence scores.
**Evidence Entailment Detection**: Using NLI models to verify evidence supports or contradicts answers - needed to ensure experimental control; check by testing NLI model on known entailment pairs.
**Context Faithfulness Metrics**: Ratio of memory answers, counter-memory answers, and uncertain answers - needed to quantify model receptiveness; check by validating against human judgments.
**Memory vs. Retrieval Trade-off**: Understanding when models prefer internal knowledge over external evidence - needed to improve RAG systems; check by varying evidence quality and quantity.

## Architecture Onboarding

**Component Map**: Question Paraphrasing -> Answer Clustering -> Memory Strength Calculation -> Evidence Generation -> Multiple Choice Format -> LLM Response -> Faithfulness Analysis

**Critical Path**: The critical path is Question Paraphrasing → Answer Clustering → Memory Strength Calculation → Evidence Generation → LLM Response → Faithfulness Analysis, as each step depends on the successful completion of the previous one.

**Design Tradeoffs**: The study trades computational efficiency for measurement precision by generating seven paraphrased questions per original question and using NLI models for semantic verification, which increases accuracy but also resource requirements.

**Failure Signatures**: LLMs failing to show context faithfulness would manifest as consistently choosing memory answers regardless of evidence quality, or showing no correlation between memory strength and resistance to contradictory evidence.

**3 First Experiments**:
1. Generate paraphrased questions for a subset of 50 questions and verify semantic equivalence using NLI models
2. Cluster answers for paraphrased questions to establish baseline memory strength distribution
3. Create evidence for 10 questions in direct and paraphrased styles and test on a single LLM model

## Open Questions the Paper Calls Out

**Open Question 1**: How does the reasoning ability of LLMs relate to their memory strength, and what mechanisms drive the observed discrepancies between comprehension and response selection?
- Basis in paper: [explicit] The paper mentions a case study where an LLM selected the memory answer (MA) despite its rationale indicating successful processing of the counter-memory answer (CMA), suggesting potential reasoning errors.
- Why unresolved: The paper identifies this as an interesting question for future work but does not investigate the relationship between reasoning ability and memory strength. The case study provides only anecdotal evidence of this phenomenon.
- What evidence would resolve it: Systematic experiments comparing LLM reasoning quality across different memory strength levels, using standardized reasoning benchmarks while controlling for memory strength, would clarify whether weaker reasoning ability contributes to memory dependence.

**Open Question 2**: Does the effectiveness of evidence paraphrasing vary across different types of knowledge conflicts, such as factual versus conceptual contradictions?
- Basis in paper: [inferred] The paper demonstrates that paraphrasing evidence significantly improves LLM receptiveness, but only tests this on factual question-answering tasks with entity-based conflicts.
- Why unresolved: The study focuses exclusively on extractive QA tasks where answers must appear in the evidence. The effectiveness of paraphrasing strategies for abstractive or conceptual knowledge conflicts remains untested.
- What evidence would resolve it: Experiments applying paraphrasing strategies to abstractive QA tasks, knowledge graph reasoning, or conceptual understanding tasks would reveal whether the technique generalizes beyond factual contradictions.

**Open Question 3**: What is the optimal evidence presentation strategy that balances persuasiveness with computational efficiency for large-scale retrieval-augmented generation systems?
- Basis in paper: [explicit] The paper compares multiple evidence styles (direct, paraphrased, indirect) and finds paraphrasing most effective, but notes that indirect evidence with additional details is less effective than paraphrasing.
- Why unresolved: The study evaluates different evidence styles but does not optimize for efficiency or explore hybrid approaches that might combine the benefits of multiple strategies while minimizing computational overhead.
- What evidence would resolve it: Empirical comparison of different evidence presentation strategies measured by both accuracy improvements and computational costs, including exploration of adaptive strategies that select evidence styles based on question characteristics, would identify optimal approaches.

## Limitations
- Exact versions of NLI model and LLMs used for paraphrasing and answer consistency checks are not specified
- Specific hyperparameters for clustering answers and exact entropy formula implementation details are not provided
- Generated paraphrased questions may not be semantically equivalent, potentially affecting memory strength calculation
- CMA-MA conflict detection may be inaccurate due to incorrect entity replacement or filtering

## Confidence
The confidence in reproducing this study is Medium due to several key uncertainties. The exact versions and implementation details of the NLI model and LLMs used for paraphrasing and answer clustering are not specified, which could significantly affect the generated paraphrased questions and memory strength calculations. Additionally, the specific hyperparameters for clustering answers (e.g., threshold for "Same" vs "Contradicted") and the exact entropy formula implementation are not provided, introducing variability in the results.

## Next Checks
1. Manually verify the semantic equivalence of a sample of generated paraphrased questions to ensure accurate memory strength calculation
2. Validate the CMA-MA conflict detection using the NLI model and ensure the alternative entity is not present in any MA
3. Test the correlation between memory strength and context faithfulness across different LLM models and evidence styles to confirm the robustness of the findings