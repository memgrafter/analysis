---
ver: rpa2
title: 'Evaluating Large Language Models on the GMAT: Implications for the Future
  of Business Education'
arxiv_id: '2401.02985'
source_url: https://arxiv.org/abs/2401.02985
tags:
- llms
- gmat
- gpt-4
- turbo
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors benchmark seven major Large Language Models (LLMs)
  on the Graduate Management Admission Test (GMAT) and find that the latest models,
  especially GPT-4 Turbo, significantly outperform human candidates. They use zero-shot
  prompting and compare models across quantitative and verbal reasoning sections,
  finding GPT-4 Turbo achieved an 85.07% average accuracy, surpassing top business
  school applicant scores.
---

# Evaluating Large Language Models on the GMAT: Implications for the Future of Business Education

## Quick Facts
- arXiv ID: 2401.02985
- Source URL: https://arxiv.org/abs/2401.02985
- Reference count: 40
- Large language models, especially GPT-4 Turbo, outperform human candidates on the GMAT, with GPT-4 Turbo achieving 85.07% accuracy.

## Executive Summary
This study benchmarks seven major large language models (LLMs) on the Graduate Management Admission Test (GMAT), finding that the latest models, particularly GPT-4 Turbo, significantly outperform human candidates. Using zero-shot prompting across quantitative and verbal reasoning sections, GPT-4 Turbo achieved an 85.07% average accuracy, surpassing top business school applicant scores. The authors demonstrate GPT-4 Turbo's tutoring capabilities through a case study involving explanation generation, response assessment, and counterfactual scenario creation. While highlighting the transformative potential of LLMs in education, the study also raises important concerns about accuracy, fairness, and equitable access to these technologies.

## Method Summary
The authors evaluated seven major large language models on the GMAT using zero-shot prompting, testing them across quantitative and verbal reasoning sections. They compared model performance against human test-takers and conducted a case study demonstrating GPT-4 Turbo's tutoring capabilities in explaining answers, assessing responses, and generating counterfactual scenarios. The study employed a controlled benchmarking approach to ensure standardized evaluation conditions.

## Key Results
- GPT-4 Turbo achieved 85.07% average accuracy on the GMAT, surpassing top business school applicant scores
- LLMs demonstrated superior performance compared to human candidates across both quantitative and verbal sections
- GPT-4 Turbo showed effective tutoring capabilities in explaining answers, assessing responses, and generating counterfactual scenarios

## Why This Works (Mechanism)
The superior performance of LLMs on the GMAT stems from their ability to process and synthesize complex information patterns at scale, leveraging their training on vast datasets to recognize question structures and apply learned reasoning strategies. Their success in quantitative and verbal sections reflects their capacity for both mathematical reasoning and language understanding, while their tutoring capabilities emerge from their ability to generate explanations, evaluate responses, and create alternative scenarios based on learned patterns.

## Foundational Learning
1. **Zero-shot prompting** - Why needed: Enables standardized evaluation without prior training on specific test questions. Quick check: Compare zero-shot results with few-shot prompting to establish performance baselines.
2. **Large language model architecture** - Why needed: Understanding how models process and generate responses is crucial for interpreting performance. Quick check: Examine attention mechanisms and transformer layers' role in reasoning tasks.
3. **GMAT test structure** - Why needed: Essential for contextualizing model performance relative to human benchmarks. Quick check: Map model strengths to specific GMAT question types and difficulty levels.

## Architecture Onboarding
**Component map:** Input prompt → LLM core processing → Output generation → Response evaluation
**Critical path:** Zero-shot prompt → Model inference → Accuracy calculation → Comparative analysis
**Design tradeoffs:** Standardized zero-shot evaluation vs. potential performance gains from fine-tuning; broad accessibility vs. potential bias concerns
**Failure signatures:** Pattern matching over true understanding; potential bias in generated responses; accuracy variations across question types
**First experiments:** 1) Human verification of random LLM responses, 2) Few-shot prompting comparison, 3) Bias pattern analysis across demographic contexts

## Open Questions the Paper Calls Out
The study acknowledges but does not fully address several critical questions, including the potential biases in model outputs, implications for equitable access to education, and the broader impact of LLM integration into educational assessment and tutoring systems.

## Limitations
- Zero-shot prompting may underestimate true LLM capabilities compared to fine-tuned or few-shot approaches
- Static GMAT question sets may not fully represent real-world reasoning challenges
- Absence of human verification for each LLM response introduces uncertainty in reported accuracy rates

## Confidence
- LLM performance comparison to human test-takers: Medium
- GPT-4 Turbo superiority: High
- Tutoring capability demonstration: Medium

## Next Checks
1. Conduct human verification of a random sample of LLM responses to validate accuracy rates and identify systematic errors
2. Test the same models using few-shot prompting to establish performance ceilings and compare against zero-shot results
3. Evaluate model outputs for demographic bias patterns across different question types and demographic contexts to assess fairness implications