---
ver: rpa2
title: What limits performance of weakly supervised deep learning for chest CT classification?
arxiv_id: '2402.04419'
source_url: https://arxiv.org/abs/2402.04419
tags:
- disease
- classification
- performance
- nodule
- multi-label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study examined the performance limitations of weakly supervised
  deep learning models for multi-label chest CT classification. The researchers evaluated
  model performance under three conditions: label noise tolerance, dataset size effects,
  and binary versus multi-label classification differences.'
---

# What limits performance of weakly supervised deep learning for chest CT classification?

## Quick Facts
- arXiv ID: 2402.04419
- Source URL: https://arxiv.org/abs/2402.04419
- Authors: Fakrul Islam Tushar; Vincent M. D'Anniballe; Geoffrey D. Rubin; Joseph Y. Lo
- Reference count: 40
- Primary result: Weakly supervised deep learning models tolerate up to 10% label noise and show performance plateau at 75% of training data for chest CT classification

## Executive Summary
This study systematically examines performance limitations of weakly supervised deep learning models for multi-label chest CT classification. The researchers investigated three key factors: label noise tolerance, dataset size effects, and binary versus multi-label classification differences. Their experiments reveal critical thresholds where model performance degrades or plateaus, providing practical guidance for medical imaging applications where perfect labels are unavailable and data resources are limited.

## Method Summary
The study used 5,044 body CT scans from Duke Health with 7,441 case-level disease labels extracted via rule-based algorithms. A 3D CNN with 4 resolution scales and residual blocks was trained using Adam optimizer (learning rate 0.0001) and weighted cross-entropy loss. Models were evaluated for label noise tolerance by adding synthetic noise, dataset size scaling from 5-100% of data, and binary versus multi-label classification performance using ROC AUC metrics. DenseVNet segmentation extracted lung patches before classification.

## Key Results
- Models tolerate up to 10% added label error before classification performance declines
- Classification accuracy improves with larger training datasets, plateauing at 75% of available data
- Binary classification outperforms multi-label classification for individual disease categories, influenced by disease co-occurrence patterns

## Why This Works (Mechanism)

### Mechanism 1
Weakly supervised learning with noisy labels can still extract meaningful patterns when noise remains below a critical threshold. The signal-to-noise ratio remains high enough that the model can learn discriminative features despite imperfect labels.

### Mechanism 2
Binary models simplify the learning task by focusing on single disease presence/absence, avoiding interference from co-occurring diseases. Disease co-occurrence patterns create feature overlap that confuses multi-label classifiers but doesn't affect binary classifiers.

### Mechanism 3
Deep learning models require sufficient data to learn generalizable patterns, but benefit diminishes after reaching a critical sample size. The dataset contains enough diversity that additional samples beyond 75% don't provide new information.

## Foundational Learning

- Concept: Receiver Operating Characteristic (ROC) and Area Under Curve (AUC) metrics
  - Why needed here: The paper uses AUC to evaluate classification performance across different experimental conditions
  - Quick check question: If a classifier has AUC = 0.9 for disease X, what does this tell you about its true positive and false positive rates across all thresholds?

- Concept: Rule-based annotation and label noise
  - Why needed here: Understanding how automated label extraction from text reports introduces noise is central to interpreting the results
  - Quick check question: If a rule-based system achieves 92% accuracy on manually validated labels, what is the maximum percentage of training labels that could be incorrect?

- Concept: Multi-label vs binary classification architectures
  - Why needed here: The paper compares these approaches to understand how co-occurrence affects learning
  - Quick check question: In a multi-label problem with 4 diseases, how many output nodes would the final classification layer have compared to a binary classifier?

## Architecture Onboarding

- Component map: CT volumes -> Resampling/normalization -> DenseVNet segmentation -> 3D CNN (4 resolution scales) -> Global average pooling -> Sigmoid outputs
- Critical path: Data preprocessing → Segmentation → Feature extraction (R-blocks) → Global pooling → Classification
- Design tradeoffs: 3D CNN captures spatial relationships but requires more parameters and training data than 2D alternatives
- Failure signatures: Poor performance on co-occurring diseases suggests feature confusion; plateauing at 75% data suggests architecture limitations
- First 3 experiments:
  1. Validate label noise tolerance by incrementally adding noise to single disease classes
  2. Test performance scaling with dataset size from 5% to 100% of available data
  3. Compare binary vs multi-label classification performance on the same validation set

## Open Questions the Paper Calls Out

### Open Question 1
What is the maximum level of label noise that weakly supervised deep learning models can tolerate before classification performance significantly degrades in medical imaging applications?
- Basis in paper: The study found models could endure up to 10% added label error before experiencing a decline in disease classification performance.
- Why unresolved: The paper only tested up to 10% added label error. It remains unknown whether higher levels of label noise would cause more severe performance degradation.
- What evidence would resolve it: Testing model performance with label noise levels exceeding 10% would determine the upper threshold of tolerable label noise.

### Open Question 2
How does the relationship between dataset size and model performance differ across various medical imaging tasks and disease categories?
- Basis in paper: The study showed disease classification performance steadily rose as the amount of training data increased for all disease classes, before experiencing a plateau at 75% of training data.
- Why unresolved: The paper only examined one specific medical imaging task (chest CT classification) and did not compare performance across different medical imaging modalities or disease categories.
- What evidence would resolve it: Conducting similar experiments with different medical imaging tasks and disease categories would reveal how the dataset size-performance relationship varies across applications.

### Open Question 3
What are the specific features that weakly supervised deep learning models learn when classifying diseases with high co-occurrence rates, and how can these features be interpreted?
- Basis in paper: The study found that binary classification outperformed multi-label classification for individual disease categories, but this result was influenced by disease co-occurrence patterns.
- Why unresolved: The paper did not investigate the specific features learned by the models or provide a detailed analysis of how co-occurrence affects feature learning.
- What evidence would resolve it: Applying explainable AI techniques to analyze the learned features and their relationship with disease co-occurrence would provide insights into the model's decision-making process.

## Limitations

- Findings are based on a single institutional dataset from Duke Health, limiting generalizability to other populations and scanner protocols
- Label noise tolerance was tested by artificially adding noise, but the real-world impact of systematic label errors from the rule-based extraction system was not fully characterized
- The plateau at 75% dataset size may reflect dataset-specific characteristics rather than universal deep learning behavior

## Confidence

- High confidence: Label noise tolerance finding (10% threshold) - directly measured with systematic experiments
- Medium confidence: Binary vs multi-label classification comparison - demonstrated but influenced by dataset-specific co-occurrence patterns
- Low confidence: Dataset size scaling results - plateau behavior may be specific to this particular dataset's diversity and noise characteristics

## Next Checks

1. Replicate the label noise tolerance experiment on an external multi-institutional dataset to verify the 10% threshold generalizes across different scanning protocols and patient populations
2. Conduct ablation studies varying the number of co-occurring diseases in the dataset to isolate whether binary classification benefits are truly from reduced feature interference versus other factors
3. Test the dataset size scaling relationship using datasets with different levels of label noise and diversity to determine whether the 75% plateau is architecture-dependent or dataset-dependent