---
ver: rpa2
title: On the Power of Decision Trees in Auto-Regressive Language Modeling
arxiv_id: '2409.19150'
source_url: https://arxiv.org/abs/2409.19150
tags:
- decision
- tree
- trees
- ardts
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Auto-regressive Decision Trees (ARDTs) are proposed for language
  modeling, combining tree-based models with auto-regressive generation. Theoretically,
  ARDTs can simulate automata, Turing machines, and sparse circuits using chain-of-thought
  intermediate tokens, with bounds on size, depth, and computational complexity.
---

# On the Power of Decision Trees in Auto-Regressive Language Modeling

## Quick Facts
- arXiv ID: 2409.19150
- Source URL: https://arxiv.org/abs/2409.19150
- Authors: Yulu Gan; Tomer Galanti; Tomo Poggio; Eran Malach
- Reference count: 39
- Key outcome: ARDT ensembles with 0.3M parameters generate coherent TinyStories text, outperforming a 1M-parameter Transformer on the same dataset

## Executive Summary
This paper introduces Auto-regressive Decision Trees (ARDTs) as a novel approach to language modeling that combines the interpretability of decision trees with the auto-regressive generation capabilities of neural models. ARDTs can theoretically simulate complex computational models like Turing machines and circuits using chain-of-thought intermediate tokens, with provable bounds on size, depth, and computational complexity. Empirically, ARDT ensembles demonstrate competitive performance on both text generation (TinyStories dataset) and reasoning tasks (BIG-Bench-Hard), often matching or exceeding much larger transformer models while maintaining interpretability and parameter efficiency.

## Method Summary
ARDTs generate text by iteratively predicting next tokens using weighted averages of word embeddings, where weights decay exponentially over time. The prediction model is an ensemble of decision trees (XGBoost) trained on sliding window contexts from the TinyStories dataset. For reasoning tasks, ARDTs leverage chain-of-thought intermediate tokens to perform complex computations, with decision trees making predictions based on both input and generated intermediate tokens. The approach combines Word2Vec embeddings with tree-based models to create interpretable, efficient language models that can match the performance of much larger neural architectures on certain tasks.

## Key Results
- ARDT ensemble with 0.3M parameters generates coherent TinyStories text, outperforming a 1M-parameter Transformer on grammar, creativity, consistency, and plot scores
- On BIG-Bench-Hard reasoning tasks, ARDTs achieve accuracy comparable to or exceeding models like InstructGPT and PaLM-540B
- Decision trees on top of transformer embeddings (GPT-2) achieve performance similar to general models, demonstrating the flexibility of tree-based approaches
- Theoretical analysis proves ARDTs can simulate automata, Turing machines, and sparse circuits with bounded size, depth, and runtime

## Why This Works (Mechanism)
ARDTs work by decomposing the language modeling task into interpretable decision boundaries learned by ensembles of trees. The auto-regressive nature allows sequential token prediction with context from previously generated tokens, while the chain-of-thought mechanism enables complex reasoning through intermediate computational steps. The weighted averaging of word embeddings with exponential decay creates a memory mechanism that captures relevant context while allowing the model to focus on recent information. This combination of interpretable tree structures with sequential generation creates a powerful yet explainable alternative to black-box neural networks.

## Foundational Learning

### Decision Trees
- **Why needed**: Form the core computational unit for ARDTs, providing interpretable decision boundaries
- **Quick check**: Understand how decision trees split on feature values to make predictions

### Auto-regressive Generation
- **Why needed**: Enables sequential token prediction where each step conditions on previous outputs
- **Quick check**: Verify you can implement next-token prediction in a loop with context updates

### Chain-of-Thought Reasoning
- **Why needed**: Allows ARDTs to perform complex computations through intermediate reasoning steps
- **Quick check**: Understand how intermediate tokens can encode computational states

### Word Embeddings
- **Why needed**: Provide dense vector representations that decision trees can operate on
- **Quick check**: Know how to train and use Word2Vec embeddings

### XGBoost Ensembles
- **Why needed**: Combine multiple decision trees to improve prediction accuracy and robustness
- **Quick check**: Understand how gradient boosting combines weak learners

## Architecture Onboarding

### Component Map
TinyStories Dataset -> Sliding Window Processing -> Word2Vec Embeddings -> XGBoost Ensemble -> Auto-regressive Generation Loop -> Text Output

### Critical Path
Context (weighted embeddings) -> XGBoost prediction -> Next token selection -> Context update (with exponential decay) -> Repeat

### Design Tradeoffs
- **Interpretability vs performance**: Trees provide explanations but may underperform neural nets on some tasks
- **Parameter efficiency vs expressiveness**: ARDTs use fewer parameters but may need more computation for complex tasks
- **Fixed embeddings vs learned representations**: Word2Vec is static while transformers learn embeddings

### Failure Signatures
- Poor generation quality indicates insufficient tree depth or ensemble size
- Overfitting shows as large gap between training and validation loss
- Incorrect reasoning suggests problems with intermediate token generation or tree training

### First Experiments
1. Train a single decision tree on TinyStories embeddings and evaluate next-token prediction accuracy
2. Compare XGBoost ensemble performance with varying numbers of trees and depths on validation loss
3. Generate text sequences with different exponential decay rates to observe impact on coherence

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can ARDTs be effectively scaled to larger datasets while maintaining their interpretability and efficiency advantages?
- **Basis in paper**: [inferred] The paper demonstrates ARDTs' capabilities on TinyStories and BIG-Bench-Hard, but these are relatively small-scale tasks compared to real-world language modeling.
- **Why unresolved**: The paper focuses on proving theoretical capabilities and showing initial empirical results, but does not explore scaling to larger datasets or more complex language tasks.
- **What evidence would resolve it**: Experiments showing ARDT performance on larger datasets like BookCorpus or Common Crawl, comparing parameter efficiency and inference speed to Transformers at scale.

### Open Question 2
- **Question**: What is the theoretical relationship between the number of intermediate tokens and the complexity of the function being computed in ARDTs?
- **Basis in paper**: [explicit] The paper provides bounds on size, depth, and runtime (measured by intermediate tokens) for simulating automata, Turing machines, and circuits, but does not establish a general relationship.
- **Why unresolved**: The analysis focuses on specific function classes with known computational structures, rather than establishing a general complexity measure for ARDTs.
- **What evidence would resolve it**: A formal proof relating the number of intermediate tokens to computational complexity classes (e.g., P vs NP) or empirical analysis across diverse function families.

### Open Question 3
- **Question**: How does the choice of word embedding method (e.g., Word2Vec vs. transformer-based embeddings) affect ARDT performance and interpretability?
- **Basis in paper**: [explicit] The paper uses Word2Vec embeddings and shows that using GPT-2 embeddings improves reasoning task performance, but does not systematically compare different embedding methods.
- **Why unresolved**: The paper presents preliminary results using different embeddings but does not conduct a comprehensive analysis of how embedding choice impacts ARDT capabilities.
- **What evidence would resolve it**: Systematic experiments comparing ARDT performance across multiple embedding methods (Word2Vec, GloVe, BERT, GPT) on the same tasks, measuring both accuracy and interpretability metrics.

### Open Question 4
- **Question**: Can ARDTs be combined with neural architectures to create hybrid models that leverage both interpretability and neural expressiveness?
- **Basis in paper**: [explicit] The paper shows that decision trees on top of transformer embeddings perform comparably to larger general models, suggesting potential for hybrid approaches.
- **Why unresolved**: While the paper demonstrates one hybrid approach, it does not explore the full design space of combining ARDTs with neural components.
- **What evidence would resolve it**: Experiments with various hybrid architectures (e.g., ARDTs as a post-processing layer, neural embeddings as input to ARDTs, or ARDTs within neural attention mechanisms) and analysis of their trade-offs.

## Limitations

- Theoretical analysis relies on chain-of-thought mechanisms that may not fully translate to practical implementation constraints
- Experimental validation limited to relatively small-scale datasets (TinyStories with 169K examples and 23 BIG-Bench-Hard tasks)
- Comparison to transformer models uses different scales (0.3M vs 1M parameters), making efficiency claims difficult to assess

## Confidence

- Theoretical framework and computational bounds: **High**
- TinyStories generation quality: **Medium**
- BIG-Bench-Hard reasoning performance: **Medium**
- Efficiency claims vs transformers: **Low**

## Next Checks

1. Reproduce the TinyStories generation results using the provided implementation details, varying the exponential decay rate Î± and ensemble size to assess sensitivity
2. Conduct ablation studies on the chain-of-thought intermediate tokens to verify their contribution to both generation quality and reasoning performance
3. Compare ARDTs against transformers of comparable parameter counts on BIG-Bench-Hard tasks to properly evaluate efficiency claims