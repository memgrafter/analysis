---
ver: rpa2
title: 'Rotated Runtime Smooth: Training-Free Activation Smoother for accurate INT4
  inference'
arxiv_id: '2409.20361'
source_url: https://arxiv.org/abs/2409.20361
tags:
- outliers
- runtime
- smooth
- quantization
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Rotated Runtime Smooth (RRS) is a training-free activation smoothing
  method for INT4 inference of large language models. It addresses the challenge of
  outliers in activations that hinder low-bit quantization by combining runtime smoothing
  of channel-wise outliers with rotation-based handling of spike outliers.
---

# Rotated Runtime Smooth: Training-Free Activation Smoother for accurate INT4 inference

## Quick Facts
- arXiv ID: 2409.20361
- Source URL: https://arxiv.org/abs/2409.20361
- Authors: Ke Yi; Zengke Liu; Jianwei Zhang; Chengyuan Li; Tong Zhang; Junyang Lin; Jingren Zhou
- Reference count: 12
- One-line primary result: Improves WikiText-2 perplexity from 57.33 to 6.66 for LLaMA3-70B under A4W4 quantization

## Executive Summary
Rotated Runtime Smooth (RRS) is a training-free activation smoothing method designed to enable accurate INT4 inference of large language models. The method addresses the challenge of outliers in activations that hinder low-bit quantization by combining runtime smoothing of channel-wise outliers with rotation-based handling of spike outliers. RRS achieves state-of-the-art performance across various model families including LLaMA and Qwen, while maintaining negligible overhead as a plug-and-play component.

## Method Summary
The Rotated Runtime Smooth method combines two key techniques to address outliers in activations during INT4 quantization. First, Runtime Smooth eliminates channel-wise outliers by normalizing activations with channel-wise maximums during runtime, implemented efficiently through GEMM kernel fusion with negligible overhead. Second, Rotation spreads spike outliers within tokens using Hadamard rotation matrices applied to both weights and activations, creating consistent smoothing scales that prevent "victims." The method reorders activations and weights by channel-wise maximums, groups them for efficient block-wise smoothing, and integrates all operations into the matrix multiplication pipeline.

## Key Results
- Improves WikiText-2 perplexity from 57.33 to 6.66 for LLaMA3-70B under A4W4 quantization
- Achieves state-of-the-art performance across various model families including LLaMA and Qwen
- Maintains negligible overhead as a plug-and-play component with no additional training required

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Runtime Smooth eliminates channel-wise outliers by dividing activations by their channel-wise maximums during runtime.
- Mechanism: The method computes smoothing scales as the per-channel maximum of activations, applies them in real-time, and integrates the process into the GEMM kernel with negligible overhead by reordering activations and weights.
- Core assumption: Channel-wise outliers persist across batches and can be effectively smoothed by normalizing with per-channel maxima without migrating outliers to weights.
- Evidence anchors:
  - [abstract]: "Runtime Smooth (RS) is introduced to eliminate channel-wise outliers by smoothing activations with channel-wise maximums during runtime."
  - [section 3.2]: "We reorder the activations and weights according to the magnitude of smoothing scales... The group size is configured to be identical to the block size."
  - [corpus]: Weak correlation with related works; no direct discussion of runtime smoothing in corpus.
- Break condition: If outliers are not channel-consistent or if activation statistics vary drastically between batches, the smoothing scale becomes ineffective.

### Mechanism 2
- Claim: Rotation spreads spike outliers within their token, creating consistent smoothing scales that prevent "victims."
- Mechanism: By applying a Hadamard rotation matrix to both weights and activations, spike outliers are distributed across the token, making the smoothing scale uniform and eliminating the creation of victims during quantization.
- Core assumption: Spike outliers, when rotated, maintain consistent values across channels, allowing for stable smoothing without abnormal scaling.
- Evidence anchors:
  - [abstract]: "The Rotation operation can narrow the gap between spike outliers and normal values, alleviating the effect of victims caused by channel-wise smoothing."
  - [section 3.3]: "The smoothing scales are more consistent across channels, leaving fewer victims since all elements are divided by such a consistent scale."
  - [corpus]: Weak correlation; no explicit mention of spike outlier handling via rotation in corpus.
- Break condition: If multiple spike outliers exist within a single token and are not uniformly distributed, the smoothing scale may still be inconsistent.

### Mechanism 3
- Claim: Reordering activations and weights by channel-wise maximums groups outliers and normal values, enabling efficient block-wise smoothing in GEMM kernels.
- Mechanism: Activations and weights are sorted based on their channel-wise maximums, grouped into blocks matching the GEMM block size, and smoothed with group-wise maxima, minimizing overhead.
- Core assumption: Sorting by channel-wise maximums is hardware-friendly and does not introduce significant overhead compared to baseline quantization.
- Evidence anchors:
  - [section 3.2]: "We reorder the activations and weights according to the magnitude of smoothing scales, validated as hardware-friendly with negligible cost."
  - [section 4.5]: "Runtime Smooth fused Kernel brings negligible overhead compared with A4W4 Per-Channel quantization."
  - [corpus]: Weak correlation; no direct evidence of sorting-based optimization in corpus.
- Break condition: If the overhead of sorting outweighs the benefits of block-wise smoothing, or if the hardware does not support efficient sorting.

## Foundational Learning

- Concept: Quantization and outlier impact
  - Why needed here: Understanding how outliers stretch quantization ranges and degrade accuracy is essential for grasping why smoothing methods are necessary.
  - Quick check question: How do outliers in activations affect the quantization range and effective bit usage for normal values?

- Concept: Channel-wise vs. spike outliers
  - Why needed here: Differentiating between these two types of outliers is crucial for understanding the dual approach of Runtime Smooth and Rotation.
  - Quick check question: What distinguishes channel-wise outliers from spike outliers in terms of their distribution and impact on quantization?

- Concept: Rotation matrices and their properties
  - Why needed here: Knowledge of orthogonal rotation matrices and their effect on data distribution is necessary to understand how rotation mitigates spike outliers.
  - Quick check question: How does applying a rotation matrix to activations and weights affect the distribution of outliers?

## Architecture Onboarding

- Component map:
  - Runtime Smooth operator -> Rotation operation -> GEMM kernel fusion -> Quantization modules

- Critical path:
  1. Reorder activations and weights by channel-wise maximums.
  2. Group activations and compute group-wise smoothing scales.
  3. Rotate weights and activations.
  4. Apply Runtime Smooth on rotated activations.
  5. Perform fused GEMM with smoothing and quantization.

- Design tradeoffs:
  - Runtime smoothing vs. offline smoothing: Runtime smoothing adapts to input statistics but introduces slight overhead; offline smoothing is fixed but may be mismatched.
  - Group size selection: Larger groups reduce overhead but may degrade smoothing effectiveness; smaller groups improve accuracy but increase computation.

- Failure signatures:
  - High perplexity or accuracy degradation indicates ineffective outlier smoothing.
  - Increased inference latency suggests overhead from sorting or smoothing operations.
  - Inconsistent results across batches may point to mismatched smoothing scales.

- First 3 experiments:
  1. Validate that Runtime Smooth reduces channel-wise outliers without significant overhead on a small model.
  2. Test the effectiveness of rotation in spreading spike outliers and preventing victims.
  3. Combine both methods and evaluate end-to-end accuracy and efficiency on a medium-sized model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the maximum number of spike outliers per token that Rotated Runtime Smooth can handle before accuracy degradation becomes significant?
- Basis in paper: [inferred] The paper mentions that having two outlier tokens in one activation could potentially trouble Rotated Runtime Smooth, suggesting there's a threshold for outlier tolerance.
- Why unresolved: The paper only briefly mentions this limitation and doesn't provide systematic experiments to determine the exact threshold or analyze the relationship between outlier count and accuracy degradation.
- What evidence would resolve it: Systematic experiments varying the number of spike outliers per token (1, 2, 3, 4+) and measuring the resulting accuracy/WikiText-2 perplexity degradation would establish clear limits.

### Open Question 2
- Question: How does Rotated Runtime Smooth perform on architectures beyond standard Transformers, such as those with gated linear units (GLU) or other non-standard activation patterns?
- Basis in paper: [explicit] The paper mentions that Down_Projector activations contain spike outliers due to SwiGLU functions, indicating the method has been tested on at least one non-standard activation pattern.
- Why unresolved: The paper only analyzes specific LLM components and doesn't explore performance on more diverse or complex architectures that might have different outlier patterns.
- What evidence would resolve it: Testing RRS on a variety of architectures including those with GLU, gated convolutional layers, or other non-standard components would reveal its generalizability.

### Open Question 3
- Question: What is the computational overhead of RRS compared to SmoothQuant when both are implemented with hardware-friendly optimizations?
- Basis in paper: [explicit] The paper states that RRS brings negligible overhead compared to the A4W4 baseline, but doesn't provide a direct comparison with SmoothQuant's optimized implementation.
- Why unresolved: While the paper compares RRS to SmoothQuant's baseline performance, it doesn't benchmark RRS against a hardware-optimized version of SmoothQuant.
- What evidence would resolve it: Direct benchmarking of hardware-optimized RRS versus hardware-optimized SmoothQuant implementations on the same hardware platform would provide a fair comparison of their computational costs.

## Limitations

- The method's effectiveness depends on channel-wise outliers being consistent across batches, which may not hold for all workloads or model architectures.
- The rotation-based approach assumes Hadamard rotation matrices will uniformly distribute outliers, which may not generalize well to models with different activation distributions.
- The claim of negligible overhead requires further validation across different hardware platforms and batch sizes beyond the tested configurations.

## Confidence

**High Confidence**: The core mechanism of channel-wise smoothing via runtime normalization is well-supported by empirical evidence and aligns with established quantization practices. The improvement from 57.33 to 6.66 perplexity on WikiText-2 is substantial and reproducible under controlled conditions.

**Medium Confidence**: The rotation-based handling of spike outliers is theoretically sound but lacks comprehensive validation across diverse model architectures and activation distributions. The claim of negligible overhead requires further testing on varied hardware configurations.

**Low Confidence**: The assumption that channel-wise outliers persist across batches may not hold for all use cases, particularly for models processing highly variable input data. The effectiveness of the method on non-language tasks remains untested.

## Next Checks

1. **Cross-Model Validation**: Test RRS on diverse model families beyond LLaMA and Qwen, including CodeLlama for code generation tasks and Mistral for long-context processing. Measure whether the 6.66 perplexity improvement generalizes or degrades on these architectures.

2. **Hardware Overhead Analysis**: Implement RRS on both NVIDIA A100 and AMD Instinct accelerators, measuring actual GEMM kernel execution time with varying batch sizes (1, 8, 32, 128). Compare against baseline quantization to verify the "negligible overhead" claim across platforms.

3. **Batch Consistency Stress Test**: Evaluate RRS performance on datasets with high input variability, such as random text generation or code completion tasks. Track whether smoothing scales become mismatched across batches and quantify the resulting accuracy degradation.