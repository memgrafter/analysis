---
ver: rpa2
title: How Does A Text Preprocessing Pipeline Affect Ontology Matching?
arxiv_id: '2411.03962'
source_url: https://arxiv.org/abs/2411.03962
tags:
- text
- preprocessing
- pipeline
- matching
- lemmatisation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how text preprocessing affects ontology
  matching accuracy. The authors find that Phase 1 methods (tokenization and normalization)
  improve matching, while Phase 2 methods (stop word removal and stemming/lemmatization)
  often reduce precision.
---

# How Does A Text Preprocessing Pipeline Affect Ontology Matching?

## Quick Facts
- **arXiv ID**: 2411.03962
- **Source URL**: https://arxiv.org/abs/2411.03962
- **Reference count**: 40
- **Primary result**: Phase 1 preprocessing methods improve matching accuracy, while Phase 2 methods often reduce precision, with context-based repair approaches showing significant improvements.

## Executive Summary
This paper investigates the impact of text preprocessing on ontology matching accuracy, revealing that different preprocessing phases have varying effects on matching performance. The authors systematically evaluate tokenization and normalization (Phase 1) versus stop word removal and stemming/lemmatization (Phase 2), finding that Phase 1 methods generally improve matching while Phase 2 methods can be detrimental. They propose a context-based pipeline repair approach using reserved word sets to eliminate false mappings, achieving significant improvements in precision and F1 scores. The study also explores large language model (LLM) approaches for matching and pipeline repair, demonstrating their potential for detecting false positives when combined with post-hoc correction.

## Method Summary
The authors conduct a comprehensive evaluation of text preprocessing pipelines on ontology matching using 15 ontology pairs. They systematically test four preprocessing phases: tokenization, normalization, stop word removal, and stemming/lemmatization. The evaluation employs the AgreementMakerLight (AML) matcher as the baseline system. For the context-based pipeline repair approach, they develop a method that uses reserved word sets to identify and eliminate false mappings. Additionally, they explore LLM-based matching using zero-shot and few-shot prompting strategies, including both embedding-based similarity and direct answer generation approaches. The experiments measure precision, recall, and F1 scores across different preprocessing configurations and compare results with and without pipeline repair mechanisms.

## Key Results
- Phase 1 preprocessing methods (tokenization and normalization) consistently improve matching accuracy
- Phase 2 methods (stop word removal and stemming/lemmatization) often reduce precision, with effects varying by domain context
- Context-based pipeline repair approach using reserved word sets significantly improves precision and F1 scores
- LLM-based approaches show promise for detecting false positives when combined with post-hoc correction mechanisms

## Why This Works (Mechanism)
The effectiveness of preprocessing pipelines in ontology matching depends on the nature of the textual data and the specific matching task. Phase 1 methods work by standardizing text representation, making it easier to identify lexical similarities between concepts. Tokenization breaks text into manageable units, while normalization (lowercasing, removing special characters) reduces variability in text representation. Phase 2 methods, however, can remove semantically important information or create ambiguity through aggressive stemming, leading to false matches. The context-based repair approach works by maintaining domain-specific knowledge through reserved word sets, allowing the system to recognize when certain terms should not be matched based on their contextual importance.

## Foundational Learning
- **Tokenization**: Breaking text into individual tokens or words - needed to create discrete units for comparison, quick check: verify tokens preserve meaningful word boundaries
- **Normalization**: Standardizing text representation through lowercasing and character removal - needed to reduce surface form variations, quick check: ensure normalization doesn't remove semantically important characters
- **Stop word removal**: Eliminating common words like "the", "and", "is" - needed to reduce noise in matching, quick check: verify stop words are truly non-informative in the domain context
- **Stemming/lemmatization**: Reducing words to their root forms - needed to handle morphological variations, quick check: ensure stemming doesn't create false equivalencies
- **Reserved word sets**: Domain-specific vocabulary that should be preserved - needed to maintain semantic integrity, quick check: validate reserved words against domain expertise
- **Context-based repair**: Using surrounding context to validate matches - needed to eliminate false positives, quick check: verify context captures relevant semantic relationships

## Architecture Onboarding

**Component Map**: Ontology pairs -> Preprocessing Pipeline -> Matcher (AML) -> Raw Matches -> Context-based Repair -> Final Matches

**Critical Path**: Text preprocessing → lexical matching → false positive detection → pipeline repair → final matching results

**Design Tradeoffs**: The system balances between aggressive preprocessing for efficiency versus preservation of semantic information for accuracy. The choice between Phase 1 and Phase 2 methods represents a tradeoff between computational simplicity and matching precision. The context-based repair adds computational overhead but significantly improves result quality by eliminating false positives that would otherwise propagate through the system.

**Failure Signatures**: Poor matching performance occurs when Phase 2 methods remove semantically important terms, when reserved word sets are incomplete or inaccurate, or when the context-based repair incorrectly identifies valid matches as false positives. LLM-based approaches may fail when prompts are ambiguous or when the model lacks domain-specific knowledge encoded in the reserved word sets.

**First Experiments**:
1. Test Phase 1 preprocessing methods individually on a small ontology pair to establish baseline improvements
2. Evaluate the impact of different reserved word set configurations on false positive elimination rates
3. Compare LLM-based matching with traditional AML matching on ontologies with varying complexity levels

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, though it implies several areas for future research including the scalability of LLM-based approaches to larger ontologies and the development of more sophisticated context-aware preprocessing strategies that can adapt to different domain requirements.

## Limitations
- Relatively small evaluation corpus of 15 ontology pairs may not capture full diversity of real-world scenarios
- Focus on lexical matching techniques may overlook semantic and structural matching approaches
- LLM-based approaches tested on limited scale and may face scalability challenges with larger ontologies
- Effects of preprocessing steps vary significantly by domain, limiting generalizability of recommendations

## Confidence
**High confidence**: Phase 1 preprocessing methods consistently improve matching accuracy across different ontology pairs.
**Medium confidence**: Phase 2 methods have variable effects that depend heavily on domain context and ontology characteristics.
**Medium confidence**: Context-based pipeline repair approach shows promising improvements, but results need replication across diverse datasets.

## Next Checks
1. Replicate the study across a larger, more diverse set of ontology pairs spanning multiple domains to validate the generalizability of the findings
2. Conduct a controlled experiment varying individual preprocessing steps while holding others constant to isolate their specific effects
3. Test the context-based pipeline repair approach on ontologies with different structural characteristics and complexity levels to evaluate its robustness