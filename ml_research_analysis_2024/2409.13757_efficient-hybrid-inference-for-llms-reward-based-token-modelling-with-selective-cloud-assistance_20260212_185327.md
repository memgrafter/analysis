---
ver: rpa2
title: 'Efficient Hybrid Inference for LLMs: Reward-Based Token Modelling with Selective
  Cloud Assistance'
arxiv_id: '2409.13757'
source_url: https://arxiv.org/abs/2409.13757
tags:
- cloud
- reward
- token
- threshold
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a hybrid inference approach for large language
  models (LLMs) that leverages a smaller language model (SLM) for initial token generation,
  with a reward model determining when to involve a costly cloud-based LLM for assistance.
  The reward model evaluates each token generated by the SLM, and only tokens below
  a predefined threshold trigger LLM involvement, reducing overall cloud LLM usage.
---

# Efficient Hybrid Inference for LLMs: Reward-Based Token Modelling with Selective Cloud Assistance

## Quick Facts
- arXiv ID: 2409.13757
- Source URL: https://arxiv.org/abs/2409.13757
- Authors: Adarsh MS; Jithin VG; Ditto PS
- Reference count: 14
- Primary result: Hybrid inference approach reduces cloud LLM usage by 35-98% while maintaining competitive accuracy

## Executive Summary
This paper introduces a hybrid inference approach for large language models (LLMs) that leverages a smaller language model (SLM) for initial token generation, with a reward model determining when to involve a costly cloud-based LLM for assistance. The reward model evaluates each token generated by the SLM, and only tokens below a predefined threshold trigger LLM involvement, reducing overall cloud LLM usage. Experiments using Qwen2 models showed significant reductions in cloud LLM activation while maintaining competitive accuracy. The approach achieved up to 87% accuracy in hybrid decoding on GSM8K with only 56% cloud LLM activation, and throughput ranged from 10.70 to 8.48 tokens/sec depending on the threshold.

## Method Summary
The hybrid inference approach uses an SLM as a candidate generator that produces tokens which are then evaluated by a reward model. The reward model, trained on paired comparisons between SLM and LLM-generated tokens, assigns scores based on alignment with the LLM's distribution. If the reward score exceeds a predefined threshold, the token is accepted from the SLM; otherwise, the cloud LLM generates the next token. The reward model is trained using PPO-style loss on synthetic data generated by comparing Qwen2 model variants, with training on 2x A100 GPUs and batch size 256. The system implements threshold-based acceptance/rejection of SLM tokens and evaluates performance on GSM8K, MMLU, MBPP, and CNN/DM datasets.

## Key Results
- Cloud LLM activation ratio reduced from 35% to 98% depending on threshold settings
- Achieved up to 87% accuracy on GSM8K with only 56% cloud LLM activation
- Throughput ranged from 10.70 to 8.48 tokens/sec depending on threshold
- Fine-grained control over efficiency-quality trade-off through reward threshold adjustment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token-level reward scoring enables selective routing of token generation between SLM and cloud LLM
- Mechanism: Each token generated by the SLM is evaluated using a reward model that compares the token's alignment with the cloud LLM's probability distribution. If the reward score exceeds a predefined threshold, the token is accepted; otherwise, the cloud LLM generates the next token
- Core assumption: The reward model can effectively distinguish between tokens that align well with the cloud LLM's distribution and those that do not
- Evidence anchors:
  - [abstract] "each token predicted by the SLM is evaluated against a reward score, and only when this score falls below a certain threshold is the cloud LLM consulted for assistance"
  - [section 2.1] "Our reward model is derived from pre-trained transformer-based language models... This model assigns scores to each token, providing the fine-grained control needed to manage the interaction between small language models (SLMs) and large language models (LLMs)"
  - [corpus] Weak evidence - corpus papers focus on similar hybrid routing but don't provide direct evidence for reward-based token-level scoring effectiveness

### Mechanism 2
- Claim: The reward threshold directly controls the trade-off between computational efficiency and output quality
- Mechanism: By adjusting the reward threshold, the system can dynamically control how many tokens are accepted from the SLM versus requiring cloud LLM assistance
- Core assumption: The relationship between reward threshold and cloud LLM activation ratio is predictable and consistent across different tasks and datasets
- Evidence anchors:
  - [section 3.1.1] "As observed in the evaluation results across various datasets, the cloud LLM activation ratio directly correlates with the reward threshold"
  - [section 3.1.2] "For the GSM8K dataset, a reward threshold of 1.0 achieves 66.48% accuracy in hybrid decoding... As the threshold increases to 2.0, hybrid accuracy improves to 74.75%... At a threshold of 4.0, hybrid accuracy peaks at 77.78%"
  - [corpus] Moderate evidence - corpus papers discuss threshold-based routing but don't provide specific quantitative evidence for this reward threshold effect

### Mechanism 3
- Claim: Speculative decoding architecture with SLM as candidate generator reduces overall inference time
- Mechanism: The SLM generates candidate tokens that are evaluated by the reward model. Only tokens below the threshold trigger cloud LLM involvement, allowing the faster SLM to handle most token generation while the cloud LLM provides assistance only when needed
- Core assumption: The SLM can generate candidate tokens quickly enough that the overall latency is lower than generating all tokens directly with the cloud LLM
- Evidence anchors:
  - [section 2.2] "Our approach adapts the speculative decoding framework by leveraging the SLM as the candidate generator model"
  - [section 3.1.3] "At lower reward thresholds (e.g., 1.0), the hybrid decoding throughput is significantly higher compared to higher thresholds"
  - [corpus] Moderate evidence - corpus papers mention speculative decoding but don't provide specific evidence for SLM-as-candidate-generator efficiency

## Foundational Learning

- Concept: Reinforcement Learning with Human Feedback (RLHF) and reward modeling
  - Why needed here: The paper's approach builds directly on RLHF reward models but adapts them for token-level alignment rather than human preference learning
  - Quick check question: What is the key difference between traditional RLHF reward models and the token-level reward models used in this paper?

- Concept: Speculative decoding architecture
  - Why needed here: The paper's approach is based on adapting speculative decoding, where a smaller model generates candidates that are verified by a larger model
  - Quick check question: How does speculative decoding differ from traditional autoregressive generation, and what are its potential benefits?

- Concept: Token-level vs sequence-level evaluation
  - Why needed here: The paper emphasizes per-token routing decisions rather than sequence-level decisions, which is a key innovation
  - Quick check question: What are the advantages and disadvantages of token-level routing compared to sequence-level routing in hybrid inference systems?

## Architecture Onboarding

- Component map: SLM -> Reward Model -> Cloud LLM -> Inference Engine
- Critical path:
  1. SLM generates candidate token
  2. Reward model evaluates token alignment
  3. If score >= threshold: accept token and continue with SLM
  4. If score < threshold: send prefix to cloud LLM, accept cloud LLM token
  5. Repeat until sequence completion

- Design tradeoffs:
  - Reward model size vs accuracy: Smaller models are faster but may be less accurate in token alignment assessment
  - Threshold value vs efficiency/quality: Higher thresholds improve quality but reduce efficiency
  - SLM vs cloud LLM capacity: The relative capabilities of the models affect routing effectiveness

- Failure signatures:
  - Low throughput with high cloud LLM activation: May indicate threshold is too high or reward model is too conservative
  - Degraded output quality: May indicate threshold is too low or reward model is too permissive
  - High latency: May indicate reward model evaluation is too slow or cloud LLM is being called too frequently

- First 3 experiments:
  1. Baseline test: Run SLM alone and cloud LLM alone on sample tasks to establish performance baselines for accuracy and throughput
  2. Threshold sweep: Test different reward thresholds (e.g., 1.0, 2.0, 4.0) on a representative dataset to observe the relationship between threshold, accuracy, and cloud LLM activation
  3. Reward model evaluation: Test the reward model's ability to distinguish between SLM and cloud LLM tokens using a validation set to ensure it's making accurate routing decisions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal reward threshold that balances accuracy and throughput across different datasets and tasks?
- Basis in paper: [explicit] The paper discusses how varying the reward threshold affects cloud LLM activation ratio, accuracy, and throughput, but does not identify an optimal threshold applicable across all scenarios
- Why unresolved: The optimal threshold depends on the specific task requirements, computational constraints, and the trade-off between accuracy and efficiency that each application prioritizes
- What evidence would resolve it: Systematic experiments across diverse datasets and tasks to identify threshold ranges that maximize accuracy while minimizing cloud LLM activation and maintaining acceptable throughput

### Open Question 2
- Question: How can the reward model be made more robust to length bias and other potential biases in token scoring?
- Basis in paper: [explicit] The paper acknowledges length bias in reward models and discusses chunking strategies to mitigate it, but does not explore comprehensive solutions to make the reward model more robust to various biases
- Why unresolved: While chunking addresses length bias, other biases may exist in the reward model's scoring, potentially leading to suboptimal routing decisions and inefficient use of computational resources
- What evidence would resolve it: Development and evaluation of advanced techniques to identify and mitigate various biases in the reward model, including length bias, through comprehensive testing and comparison with baseline methods

### Open Question 3
- Question: How does the proposed hybrid inference approach scale when multiple SLM-LLM pairs are involved?
- Basis in paper: [explicit] The paper mentions scalability concerns regarding managing and maintaining reward models for each SLM-LLM pair, but does not explore solutions or provide experimental evidence on multi-pair scenarios
- Why unresolved: The complexity of managing multiple reward models and ensuring their alignment with corresponding SLM-LLM pairs increases as the number of pairs grows, potentially affecting the efficiency and effectiveness of the hybrid inference approach
- What evidence would resolve it: Experimental studies and analysis of the hybrid inference approach's performance and scalability when multiple SLM-LLM pairs are involved, including resource utilization, accuracy, and latency comparisons with single-pair scenarios

## Limitations

- Reward model generalization to model families beyond Qwen2 is unverified
- Computational overhead of reward model evaluation at each token step is not thoroughly analyzed
- Dataset specificity limits applicability to diverse domains requiring specialized knowledge

## Confidence

**High Confidence**
- The hybrid decoding approach can reduce cloud LLM activation from 35% to 98% depending on threshold settings
- The reward threshold directly controls the trade-off between computational efficiency and output quality
- Speculative decoding with SLM as candidate generator reduces overall inference time

**Medium Confidence**
- The reward model can effectively distinguish between tokens that align well with the cloud LLM's distribution
- The SLM can generate candidate tokens quickly enough that the overall latency is lower than generating all tokens directly with the cloud LLM
- The approach offers fine-grained control over the balance between efficiency and response quality

**Low Confidence**
- The reward model's effectiveness generalizes to model families beyond Qwen2
- The computational overhead of reward model evaluation is negligible compared to efficiency gains
- The threshold calibration approach scales effectively to diverse, real-world applications

## Next Checks

1. **Cross-Model Family Validation**: Test the reward model trained on Qwen2 models with other model families (e.g., LLaMA, Mistral, GPT variants) to assess generalization capabilities. Measure both reward model accuracy and hybrid decoding performance across at least 3 different model families on standard benchmarks.

2. **Real-World Deployment Simulation**: Evaluate the approach on a diverse corpus of real-world prompts from production LLM applications (e.g., customer service, code generation, creative writing) rather than benchmark datasets. Track not just accuracy and throughput, but also user satisfaction metrics and edge case handling.

3. **Computational Overhead Analysis**: Measure the end-to-end latency breakdown of the hybrid decoding system, including SLM generation time, reward model evaluation time, and cloud LLM call time. Compare this to pure SLM and pure cloud LLM baselines across varying sequence lengths to determine the break-even point where hybrid decoding stops providing benefits.