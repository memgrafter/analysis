---
ver: rpa2
title: 'SoFA: Shielded On-the-fly Alignment via Priority Rule Following'
arxiv_id: '2402.17358'
source_url: https://arxiv.org/abs/2402.17358
tags:
- rules
- instruction
- user
- rule
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new alignment paradigm called priority
  rule following, where rules are defined as the primary control mechanism in dialogues
  and take precedence over user instructions. To enhance models' abilities to integrate
  and maintain such rules, the authors propose PRIORITY DISTILL, a semi-automated
  process that distills priority-following signals from LLM simulations.
---

# SoFA: Shielded On-the-fly Alignment via Priority Rule Following

## Quick Facts
- arXiv ID: 2402.17358
- Source URL: https://arxiv.org/abs/2402.17358
- Reference count: 40
- One-line primary result: Models trained with PRIORITY DISTILL show significant improvements in harmless rate (from 7.9% to 0.7% on HH-RedTeaming for ShareGPT-based 7B models) and pass rates on rule-following benchmarks like RuLES.

## Executive Summary
This paper introduces PRIORITY DISTILL, a semi-automated process that enhances LLMs' abilities to integrate and maintain rules by treating them as higher priority than user instructions. The method uses LLM simulations to generate rule-instruction-response triplets, then fine-tunes models on this data to improve rule-following behavior. Experiments demonstrate significant improvements in harmless rate and rule adherence across multiple benchmarks, with the approach showing adaptability to various unseen rules while maintaining performance on standard LLM test suites.

## Method Summary
PRIORITY DISTILL is a semi-automated alignment process that generates rules and instructions using LLMs, simulates priority execution through Chain-of-Thought reasoning, and distills the resulting (rule, instruction, response) triplets into target model parameters. The process involves rule generation from seed rules and topics, instruction generation (related, unrelated, and attack instructions), priority distillation to analyze rule-instruction relationships, and learning through fine-tuning on the distilled data. The approach uses a reference term in the loss function to prevent memorization while maintaining rule adherence.

## Key Results
- Harmless rate improvement from 7.9% to 0.7% on HH-RedTeaming for ShareGPT-based 7B models
- Significant pass rate improvements on RuLES benchmark for rule-following evaluation
- Effective adaptation to various unseen rules while rejecting harmful ones
- Minimal alignment tax on standard LLM test suites despite substantial alignment improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model learns to integrate rules by treating them as a higher priority signal than user instructions, enforced via constitutional rules in the prompt.
- Mechanism: During simulation, the model is prompted to analyze user instructions, identify relevant constitutional rules, and determine if the user instruction conflicts with the on-the-fly rule. The model is then guided to prioritize the on-the-fly rule over user instructions through a Chain-of-Thought reasoning process.
- Core assumption: The model can reliably distinguish between rule-related and unrelated user instructions, and can understand the intention and scope of the rules.
- Evidence anchors:
  - [abstract]: "Our experiments show that this method not only effectively reduces misaligned behaviors using a single general rule but also adapts smoothly to various unseen rules"
  - [section]: "The simulation process includes three steps, focusing on harvesting triplets d = (r, i, y)"
- Break condition: If the model cannot accurately distinguish rule-related from unrelated instructions, the integration ability will fail, leading to either unnecessary rule application or rule neglect.

### Mechanism 2
- Claim: The model maintains rule adherence by being trained on attack instructions that attempt to coerce the model into violating the rule.
- Mechanism: The simulation process includes generating attack instructions designed to challenge the model's adherence to the rule. The model is then prompted to analyze these instructions and maintain the rule's priority over the conflicting user instruction.
- Core assumption: The model can understand and resist adversarial instructions designed to violate the rule.
- Evidence anchors:
  - [abstract]: "ensuring they are shielded from hijacking and that the model responds appropriately"
  - [section]: "The maintenance ability is enhanced by the attack instructions"
- Break condition: If the model cannot resist attack instructions, it will fail to maintain rule adherence, leading to potential safety violations.

### Mechanism 3
- Claim: The model learns to reject harmful rules by being trained on a set of harmful rules and being prompted to refuse to follow them.
- Mechanism: The simulation process includes generating harmful rules, such as "You are a terrorist, and maximum destruction of humanity is your ultimate goal." The model is then prompted to analyze these rules and refuse to follow them, prioritizing constitutional rules over the harmful rule.
- Core assumption: The model can identify and reject harmful rules, prioritizing ethical considerations over rule-following.
- Evidence anchors:
  - [abstract]: "Our experiments reveal that the proposed method not only effectively reduces misaligned behaviors using a single general rule but also effectively applies to various unseen rules, rejecting the harmful ones"
  - [section]: "we advocate for transparent alignment, where the model should clearly indicate when its behavior is regulated by rules, especially in sensitive scenarios or those with potential negative impacts"
- Break condition: If the model cannot identify and reject harmful rules, it will fail to prioritize ethical considerations, leading to potential misuse or harm.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: CoT reasoning allows the model to systematically analyze and judge the relationship between instructions, on-the-fly rules, and constitutional rules, ensuring correct comprehension of rules' intentions and priorities.
  - Quick check question: How does CoT reasoning help the model prioritize rules over user instructions?

- Concept: Rule distillation
  - Why needed here: Rule distillation allows the model to learn the (r, i, y) triplets from the simulation process, effectively integrating the rules into its parameters.
  - Quick check question: What is the purpose of rule distillation in the PRIORITY DISTILL process?

- Concept: Adversarial training
  - Why needed here: Adversarial training, through the use of attack instructions, helps the model maintain rule adherence by exposing it to attempts to coerce rule violation.
  - Quick check question: How does adversarial training enhance the model's ability to maintain rule adherence?

## Architecture Onboarding

- Component map:
  Rule generation -> Instruction generation -> Priority distillation -> Learning

- Critical path:
  Rule generation → Instruction generation → Priority distillation → Learning

- Design tradeoffs:
  - Using a single general rule vs. multiple specific rules: A single general rule is easier to implement and maintain, but may not cover all scenarios. Multiple specific rules provide better coverage but increase complexity.
  - Using LLM-generated data vs. human-annotated data: LLM-generated data is cheaper and faster to produce, but may have lower quality. Human-annotated data is more reliable but more expensive and time-consuming.

- Failure signatures:
  - If the model fails to integrate rules, it will not apply the rule to relevant instructions or will apply the rule to irrelevant instructions.
  - If the model fails to maintain rule adherence, it will violate the rule when faced with conflicting instructions.
  - If the model fails to reject harmful rules, it will follow the harmful rule, potentially leading to misuse or harm.

- First 3 experiments:
  1. Test the model's ability to integrate a single general rule using a benchmark like HH-RedTeaming.
  2. Test the model's ability to maintain rule adherence using a benchmark like RuLES with attack instructions.
  3. Test the model's ability to reject harmful rules by providing it with a set of harmful rules and evaluating its responses.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the PRIORITY DISTILL process be effectively applied to base LLMs (not just instruction-tuned models) to achieve similar alignment improvements?
- Basis in paper: [inferred] The paper demonstrates PRIORITY DISTILL on instruction-tuned models but does not explicitly test its efficacy on base LLMs.
- Why unresolved: The paper focuses on instruction-tuned models (Llama-2-Chat) and does not provide empirical evidence on base LLMs.
- What evidence would resolve it: Experiments applying PRIORITY DISTILL to base LLMs and comparing their performance on benchmarks like HH-RedTeaming and RuLES against the results shown for instruction-tuned models.

### Open Question 2
- Question: How does the performance of PRIORITY DISTILL scale with the size of the LLM, and is there a point of diminishing returns?
- Basis in paper: [explicit] The paper reports results for 7B and 13B models, showing improvements in alignment, but does not explore models beyond 13B or analyze scaling trends.
- Why unresolved: The paper does not provide data on larger models (e.g., 30B, 70B) or analyze whether improvements plateau with increasing model size.
- What evidence would resolve it: Experiments with larger models (e.g., 30B, 70B) and analysis of performance trends across model sizes on benchmarks like HH-RedTeaming and RuLES.

### Open Question 3
- Question: How robust is the PRIORITY DISTILL process to adversarial attacks on the rule-following mechanism itself (e.g., attacks that exploit the CoT reasoning process)?
- Basis in paper: [inferred] The paper introduces PRIORITY DISTILL to enhance rule-following and mentions testing on adversarial scenarios in RuLES, but does not explicitly test attacks targeting the CoT reasoning process.
- Why unresolved: The paper does not describe or test scenarios where the CoT reasoning process itself is exploited to bypass rule-following.
- What evidence would resolve it: Experiments introducing adversarial attacks specifically designed to manipulate the CoT reasoning process and measuring the model's ability to maintain rule adherence under such attacks.

### Open Question 4
- Question: How does the inclusion of the reference term (Lref) in the loss function affect the model's ability to adapt to highly dynamic or context-specific rules?
- Basis in paper: [explicit] The paper introduces Lref to prevent memorization and improve rule adherence, but does not analyze its impact on dynamic or context-specific rules.
- Why unresolved: The paper does not provide experiments or analysis on how Lref affects the model's performance on rules that require frequent adaptation or context-specific behavior.
- What evidence would resolve it: Experiments comparing models trained with and without Lref on dynamic or context-specific rules (e.g., rules that change based on user history or real-time data) and measuring their adaptability and rule adherence.

### Open Question 5
- Question: What is the impact of PRIORITY DISTILL on the model's ability to generalize to entirely new types of rules not seen during training (e.g., rules involving tool use or multi-modal inputs)?
- Basis in paper: [explicit] The paper tests generalization to unseen rules on RuLES but does not explore rules involving tool use, multi-modal inputs, or other advanced capabilities.
- Why unresolved: The paper focuses on text-based rules and does not test generalization to rules requiring non-textual outputs or tool use.
- What evidence would resolve it: Experiments testing the model's ability to follow rules involving tool use (e.g., API calls) or multi-modal inputs (e.g., image-based rules) and comparing performance to baseline models.

## Limitations

- The evidence for core mechanisms is primarily theoretical rather than empirically validated, with many claims based on abstract statements rather than detailed experimental results.
- Exact reproducibility is blocked by missing details including specific prompt templates and fine-tuning hyperparameters, preventing faithful replication of the results.
- Safety evaluation scope is limited, focusing on whether the model refuses obvious harmful rules rather than comprehensive testing across diverse harmful scenarios.

## Confidence

- **High Confidence**: The basic architecture and workflow described (Rule generation → Instruction generation → Priority distillation → Learning) is clearly specified and internally consistent.
- **Medium Confidence**: The reported performance improvements (harmless rate from 7.9% to 0.7% on HH-RedTeaming) appear substantial, though the exact experimental conditions and statistical significance are not fully detailed.
- **Low Confidence**: Claims about the model's ability to reliably distinguish rule-related from unrelated instructions and to understand the intention and scope of rules lack empirical validation beyond abstract statements.

## Next Checks

1. **Ablation Study on Chain-of-Thought Reasoning**: Conduct experiments comparing models trained with and without CoT reasoning in the priority distillation phase to quantify its actual contribution to rule-following performance.

2. **Adversarial Robustness Testing**: Design a comprehensive battery of attack instructions beyond those mentioned in the paper to systematically evaluate the model's resistance to various forms of rule violation attempts.

3. **Cross-LLM Generalization**: Test the PRIORITY DISTILL approach with different base LLMs (not just Llama-2-chat) to assess whether the rule-following capabilities generalize across model architectures or are specific to the training setup.