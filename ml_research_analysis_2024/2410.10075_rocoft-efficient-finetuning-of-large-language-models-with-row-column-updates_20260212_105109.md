---
ver: rpa2
title: 'RoCoFT: Efficient Finetuning of Large Language Models with Row-Column Updates'
arxiv_id: '2410.10075'
source_url: https://arxiv.org/abs/2410.10075
tags:
- arxiv
- parameters
- rocoft
- tasks
- kernel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RoCoFT introduces a parameter-efficient fine-tuning method that
  updates only a small subset of rows or columns in the weight matrices of transformer-based
  language models. By avoiding low-rank decomposition and extra parameters, RoCoFT
  achieves competitive or superior performance compared to state-of-the-art methods
  like LoRA and AdaLoRA, while requiring fewer trainable parameters and reducing memory
  and computational overhead.
---

# RoCoFT: Efficient Finetuning of Large Language Models with Row-Column Updates

## Quick Facts
- arXiv ID: 2410.10075
- Source URL: https://arxiv.org/abs/2410.10075
- Reference count: 32
- Method achieves competitive or superior performance compared to state-of-the-art PEFT methods while using fewer trainable parameters

## Executive Summary
RoCoFT introduces a parameter-efficient fine-tuning method that updates only a small subset of rows or columns in the weight matrices of transformer-based language models. By avoiding low-rank decomposition and extra parameters, RoCoFT achieves competitive or superior performance compared to state-of-the-art methods like LoRA and AdaLoRA, while requiring fewer trainable parameters and reducing memory and computational overhead. Extensive experiments across medium-size models (e.g., RoBERTa, DeBERTa) and large-scale models (e.g., Bloom-7B, Llama2-7B/13B) on benchmarks like GLUE, SQuAD, XSum, and commonsense reasoning tasks demonstrate strong accuracy with significant parameter savings.

## Method Summary
RoCoFT modifies existing weight matrices by selecting and updating specific rows or columns while keeping the rest frozen as buffers, without introducing additional parameters. The method can be expressed as W = W0 + R or W = W0 + C, where R and C are restricted weight matrices with at most r nonzero rows or columns. This approach leverages Neural Tangent Kernel theory to explain why restricted parameter updates can achieve performance comparable to full fine-tuning, as the kernels induced by row/column updates closely approximate those from full-parameter training.

## Key Results
- Achieves competitive or superior performance to LoRA and AdaLoRA on multiple benchmarks while using fewer trainable parameters
- Demonstrates robustness to row/column selection strategies through ablation studies
- Shows NTK similarity between restricted and full parameter sets explains performance retention
- Effective across diverse model sizes from medium (RoBERTa, DeBERTa) to large-scale models (Bloom-7B, Llama2-7B/13B)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Finetuning a small subset of rows or columns preserves model capacity while reducing parameters.
- Mechanism: Updating only a few rows/columns introduces low-rank perturbations that adjust feature representations without changing overall model structure.
- Core assumption: Critical task-specific features are already encoded in pretrained weights, so sparse updates suffice.
- Evidence anchors:
  - [abstract] "by updating only a small subset of rows or columns in the weight matrices... achieves competitive or superior performance"
  - [section 3] "These can be expressed as W = W0 + R and W = W0 + C, where R and C are restricted weight matrices such that only at most r of the rows or columns are nonzero"
  - [corpus] No direct evidence for row-column update effectiveness found; mechanisms inferred from general fine-tuning literature.
- Break condition: If task requires substantial feature modification beyond what sparse row/column updates can provide, performance will degrade.

### Mechanism 2
- Claim: NTK similarity between restricted and full parameter sets explains performance retention.
- Mechanism: The Neural Tangent Kernel induced by row/column updates approximates the full NTK, maintaining learning dynamics.
- Core assumption: NTK framework applies to fine-tuning regime with limited parameter updates.
- Evidence anchors:
  - [abstract] "Neural Tangent Kernel analysis reveals that the kernels induced by RoCoFT closely match those from full fine-tuning"
  - [section 5] "We empirically demonstrate that our kernel, constructed using a restricted set of row and column parameters, are numerically close to the full-parameter kernel"
  - [corpus] No direct NTK comparison evidence found; mechanisms inferred from NTK literature.
- Break condition: If NTK approximation breaks down for complex tasks, performance will diverge from full fine-tuning.

### Mechanism 3
- Claim: Random or structured row/column selection provides sufficient parameter coverage for effective adaptation.
- Mechanism: Any sufficiently large set of row/column updates captures diverse directions in weight space needed for adaptation.
- Core assumption: Parameter importance is distributed across weight matrices, not concentrated in specific locations.
- Evidence anchors:
  - [abstract] "Ablation studies confirm the method's robustness to row/column selection strategies"
  - [section 6] "we demonstrate the robustness of our row and column selection method through a detailed comparison of four selection strategies: Max, Min, Mixed, and random"
  - [corpus] No direct ablation evidence found; mechanisms inferred from general PEFT literature.
- Break condition: If parameter importance is highly localized, random selection may miss critical updates.

## Foundational Learning

- Concept: Neural Tangent Kernel theory
  - Why needed here: Explains why restricted parameter updates can achieve full fine-tuning performance
  - Quick check question: What is the relationship between NTK and the learning dynamics of infinite-width networks?

- Concept: Low-rank matrix updates
  - Why needed here: Forms the mathematical basis for understanding how row/column modifications affect model behavior
  - Quick check question: How does a rank-r update differ from full-rank weight matrix modification?

- Concept: Parameter-efficient fine-tuning strategies
  - Why needed here: Provides context for comparing RoCoFT against existing methods like LoRA and adapters
  - Quick check question: What are the key differences between LoRA and RoCoFT in terms of parameter efficiency?

## Architecture Onboarding

- Component map: Weight matrix -> Frozen buffer (most rows/columns) + Trainable subset (selected rows/columns)
- Critical path: Select rows/columns → Update selected parameters → Forward pass uses concatenated frozen and trainable weights → Backward pass computes gradients only for trainable subset
- Design tradeoffs: Simplicity and efficiency vs. potential under-adaptation for tasks requiring distributed parameter changes; no additional memory overhead vs. limited expressivity compared to low-rank methods
- Failure signatures: Poor performance on tasks requiring extensive feature modification; sensitivity to rank selection; convergence issues if selected rows/columns don't capture task-relevant directions
- First 3 experiments:
  1. Compare RoCoFT rank-1 row/column updates against LoRA on a simple GLUE task (e.g., SST-2) to verify parameter efficiency claims
  2. Test different row/column selection strategies (random, max, min, mixed) on the same task to validate robustness claims
  3. Run NTK analysis comparing restricted kernels to full parameter kernels on a small dataset to verify theoretical claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of RoCoFT depend on the specific selection strategy for rows and columns, or is it robust to any selection method?
- Basis in paper: [explicit] The paper states "Extensive experiments and ablation studies on the design choices such as which and how many rows and columns to select" and shows that RoCoFT is "robust to any selection of rows and columns."
- Why unresolved: While the paper shows RoCoFT is robust to different selection strategies, it does not explore whether the method's effectiveness varies significantly between selection methods in extreme cases or specific tasks.
- What evidence would resolve it: A comprehensive study comparing RoCoFT's performance across a wider range of selection strategies (e.g., random, gradient-based, attention-based) on diverse and challenging tasks would clarify the dependency on selection methods.

### Open Question 2
- Question: Can RoCoFT be effectively combined with other parameter-efficient fine-tuning techniques, such as quantization or sparsity, to further improve efficiency?
- Basis in paper: [inferred] The paper mentions that "Future works include combining our RoCoFT method with quantization to achieve more compressed models during finetuning."
- Why unresolved: The paper does not explore the practical implementation or performance of RoCoFT when combined with other techniques like quantization or sparsity, leaving the potential benefits and challenges unclear.
- What evidence would resolve it: Experiments demonstrating the performance and efficiency gains (or trade-offs) of RoCoFT when combined with quantization or sparsity methods on various tasks and model sizes would provide clarity.

### Open Question 3
- Question: How does RoCoFT perform on tasks that require fine-grained, task-specific feature adaptation, compared to tasks where pretrained features are already sufficient?
- Basis in paper: [explicit] The paper notes that "the effectiveness of the approximating fine-tuning with NTK regression depends on the dataset's complexity and distributional properties" and that RoCoFT's NTK approximation diverges significantly on tasks like QNLI, MNLI, and QQP.
- Why unresolved: While the paper identifies that RoCoFT may underperform on certain tasks, it does not systematically analyze the characteristics of tasks where RoCoFT excels versus those where it struggles.
- What evidence would resolve it: A detailed analysis of RoCoFT's performance across tasks with varying degrees of task-specific feature requirements, along with insights into the underlying reasons for success or failure, would help clarify its applicability.

## Limitations

- NTK similarity claims lack rigorous statistical validation and comprehensive empirical evidence
- Performance comparisons with state-of-the-art methods may be sensitive to specific hyperparameter choices not fully specified
- Limited analysis of break conditions where row/column updates prove insufficient for complex tasks requiring extensive feature modification

## Confidence

- High Confidence: Claims about parameter efficiency gains (clearly demonstrated through trainable parameter counts)
- Medium Confidence: Performance claims relative to LoRA/AdaLoRA (results show competitive performance but with potential hyperparameter sensitivity)
- Low Confidence: NTK similarity claims (lack of rigorous statistical validation and comprehensive empirical evidence)

## Next Checks

1. Conduct statistical analysis comparing eigenvalue spectra distributions of full-parameter and restricted kernels across multiple datasets to quantify NTK similarity claims
2. Perform systematic hyperparameter sweeps for both RoCoFT and baseline methods to ensure fair performance comparisons
3. Test RoCoFT on tasks requiring substantial feature modification to identify break conditions where row/column updates prove insufficient