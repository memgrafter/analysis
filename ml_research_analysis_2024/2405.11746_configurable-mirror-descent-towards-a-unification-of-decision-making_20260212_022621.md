---
ver: rpa2
title: 'Configurable Mirror Descent: Towards a Unification of Decision Making'
arxiv_id: '2405.11746'
source_url: https://arxiv.org/abs/2405.11746
tags:
- uni00000013
- uni00000014
- uni00000011
- uni00000015
- uni00000017
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of developing a single algorithm
  capable of tackling all categories of decision-making problems, including single-agent,
  cooperative multi-agent, competitive multi-agent, and mixed cooperative and competitive
  multi-agent. The proposed method, Configurable Mirror Descent (CMD), introduces
  a generalized mirror descent (GMD) that considers multiple historical policies and
  works with a broader class of Bregman divergences.
---

# Configurable Mirror Descent: Towards a Unification of Decision Making

## Quick Facts
- **arXiv ID:** 2405.11746
- **Source URL:** https://arxiv.org/abs/2405.11746
- **Reference count:** 40
- **Key outcome:** Proposes Configurable Mirror Descent (CMD) to unify decision-making algorithms across single-agent, cooperative, competitive, and mixed multi-agent categories, demonstrating competitive or better performance than specialized baselines.

## Executive Summary
This paper addresses the challenge of developing a single algorithm capable of handling all decision-making categories. The authors propose Configurable Mirror Descent (CMD), which generalizes mirror descent algorithms by considering multiple historical policies and a broader class of Bregman divergences. A meta-controller dynamically adjusts hyper-parameters based on evaluation measures, enabling CMD to adapt to different solution concepts with minimal modifications. The authors also construct GAME BENCH, a comprehensive benchmark with 15 games across all decision-making categories, and demonstrate through extensive experiments that CMD achieves empirically competitive or better outcomes compared to specialized baselines.

## Method Summary
The proposed method consists of two main components: Generalized Mirror Descent (GMD) and a Configurable Mirror Descent (CMD) wrapper. GMD generalizes existing mirror descent algorithms by incorporating multiple previous policies into policy updates and solving Karush–Kuhn–Tucker conditions to enable exploration of broader Bregman divergences. CMD introduces a meta-controller that samples multiple hyper-parameter candidates, evaluates their performance using targeted evaluation measures, and updates parameters based on performance differences. The meta-controller uses zero-order optimization since evaluation measures are typically non-differentiable with respect to hyper-parameters. The method is designed to handle different decision-making categories by having each agent independently execute GMD with dynamically adjusted parameters.

## Key Results
- CMD achieves empirically competitive or better outcomes compared to specialized baselines across all decision-making categories
- The meta-controller effectively adapts hyper-parameters to different evaluation measures with minimal modifications
- GAME BENCH provides a comprehensive, academic-friendly benchmark covering all decision-making categories
- CMD successfully handles environments with different numbers of agents and relationships between agents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CMD's meta-controller dynamically adjusts hyper-parameters based on evaluation measures, enabling it to satisfy desiderata D3 and D4.
- Mechanism: The meta-controller samples multiple candidates of hyper-parameters, evaluates their performance using the targeted evaluation measure, and updates the parameters based on the sign of the performance difference (direction-guided update). This allows CMD to adapt to different solution concepts and evaluation measures with minimal modifications.
- Core assumption: The evaluation measure is non-differentiable with respect to the hyper-parameters, making gradient-based methods inapplicable.
- Evidence anchors:
  - [abstract]: "We propose the configurable mirror descent (CMD) where a meta-controller is introduced to dynamically adjust the hyper-parameters in GMD conditional on the evaluation measures."
  - [section]: "To address this issue, we propose a simple yet effective zero-order optimization method where the performance difference between two candidates is used to only determine the update direction of the hyper-parameters rather than the update magnitude."
  - [corpus]: Weak or missing evidence; this is a novel contribution not present in related works.
- Break condition: If the evaluation measure becomes differentiable with respect to the hyper-parameters, gradient-based methods may outperform the zero-order approach.

### Mechanism 2
- Claim: GMD generalizes existing mirror descent algorithms by considering multiple historical policies and a broader class of Bregman divergences.
- Mechanism: GMD incorporates multiple previous policies into the policy updating and solves the Karush–Kuhn–Tucker (KKT) conditions at each iteration, enabling exploration of a broader class of Bregman divergences beyond specific choices like KL divergence.
- Core assumption: The convex function ψ(π) can be decomposed into a sum of convex functions defined on individual action probabilities.
- Evidence anchors:
  - [abstract]: "We propose the generalized mirror descent (GMD), a generalization of MD variants, which considers multiple historical policies and works with a broader class of Bregman divergences."
  - [section]: "We relax this premise by addressing the KKT conditions at each iteration, enabling us to explore a broader class of Bregman divergence."
  - [corpus]: Weak or missing evidence; this generalization is not present in related works.
- Break condition: If the decomposition of ψ(π) is not valid for a given problem, the numerical method for computing the dual variable may fail.

### Mechanism 3
- Claim: CMD can effectively tackle all categories of decision-making problems by independently executing GMD for each agent.
- Mechanism: Each agent in the environment independently executes GMD with the given hyper-parameters, allowing CMD to handle different numbers of agents and relationships between agents (single-agent, cooperative, competitive, and mixed cooperative-competitive).
- Core assumption: The agents can execute GMD independently without requiring centralized information during execution.
- Evidence anchors:
  - [abstract]: "As GMD is adopted by each agent independently, it can be applied to different decision-making categories involving different numbers of agents and different relationships between agents (D1 and D2)."
  - [section]: "By incorporating the MC into GMD, we establish the CMD. Intuitively, CMD can be configured to apply to different evaluation measures and hence, can satisfy the desiderata D3 and D4 while only minimal modifications are required: specifying the MC's optimization objective L."
  - [corpus]: Weak or missing evidence; the ability to handle all categories independently is a novel contribution.
- Break condition: If the agents require centralized information for execution, the independent execution of GMD may not be feasible.

## Foundational Learning

- Concept: Mirror descent algorithm and its variants (MD, MMD)
  - Why needed here: Understanding the foundation of mirror descent is crucial for grasping the generalizations made by GMD and CMD.
  - Quick check question: What is the key difference between classic mirror descent and magnetic mirror descent (MMD)?

- Concept: Bregman divergences and their properties
  - Why needed here: CMD's ability to explore a broader class of Bregman divergences is a key differentiator, so understanding their properties is essential.
  - Quick check question: How does the choice of Bregman divergence affect the convergence and performance of mirror descent algorithms?

- Concept: Equilibrium concepts (Nash equilibrium, coarse correlated equilibrium) and evaluation measures (NashConv, CCEGap, social welfare)
  - Why needed here: CMD's ability to handle different solution concepts and evaluation measures is central to its design, so familiarity with these concepts is necessary.
  - Quick check question: What is the difference between Nash equilibrium and coarse correlated equilibrium, and when is each concept applicable?

## Architecture Onboarding

- Component map: Generalized Mirror Descent (GMD) -> Configurable Mirror Descent (CMD) -> Meta-controller -> GAME BENCH

- Critical path:
  1. Initialize GMD with initial policy, number of historical policies (M), and hyper-parameters (α)
  2. Execute GMD for each agent independently to update policies
  3. If CMD, use meta-controller to sample hyper-parameter candidates and evaluate their performance
  4. Update hyper-parameters based on the evaluation measure
  5. Repeat steps 2-4 for the desired number of iterations

- Design tradeoffs:
  - Computational cost vs. performance: CMD requires extra computational cost to evaluate multiple hyper-parameter candidates, but this allows for better adaptation to different evaluation measures
  - Flexibility vs. complexity: CMD's ability to handle different solution concepts and evaluation measures adds flexibility but also increases complexity compared to specialized algorithms

- Failure signatures:
  - Poor convergence: If CMD fails to converge or converges slowly, it may indicate issues with the meta-controller's hyper-parameter adjustment or the choice of Bregman divergence
  - Suboptimal performance: If CMD achieves suboptimal performance compared to specialized algorithms, it may suggest that the generalization comes at the cost of fine-tuning for specific problem classes

- First 3 experiments:
  1. Evaluate CMD's performance on a single-agent game (e.g., Kuhn-A) and compare it to existing single-agent algorithms (e.g., PPO)
  2. Test CMD's ability to handle cooperative multi-agent games (e.g., TinyHanabi-A) and assess its performance against cooperative MARL algorithms (e.g., QMIX)
  3. Assess CMD's effectiveness in competitive multi-agent games (e.g., Kuhn poker) and compare it to competitive game-solving algorithms (e.g., CFR)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical convergence rate of Configurable Mirror Descent (CMD) for different decision-making categories?
- Basis in paper: [inferred] The paper mentions that CMD requires a numerical method for policy updates rather than closed-form solutions, and theoretical analysis of CMD's behavior is identified as an interesting future problem.
- Why unresolved: The paper primarily evaluates CMD empirically across different games but does not provide theoretical convergence guarantees or rates for CMD in different decision-making scenarios.
- What evidence would resolve it: Mathematical proofs establishing convergence rates for CMD in various decision-making categories (single-agent, cooperative, competitive, mixed) would resolve this question.

### Open Question 2
- Question: Can neural Bregman divergences be effectively integrated into CMD to automatically select optimal divergences for different decision-making categories?
- Basis in paper: [explicit] The paper identifies this as a limitation and future direction, noting that CMD still requires mathematical formulation of convex functions ψ and exploring the entire space of convex functions could be an interesting direction.
- Why unresolved: While CMD can explore a broader class of Bregman divergences than previous methods, it still requires explicit convex function formulations and cannot automatically discover optimal divergences.
- What evidence would resolve it: Empirical demonstrations of CMD with neural Bregman divergences showing improved performance across multiple decision-making categories compared to fixed divergences would resolve this question.

### Open Question 3
- Question: How does the computational efficiency of CMD scale with the complexity of decision-making environments?
- Basis in paper: [explicit] The paper acknowledges that CMD requires evaluating multiple candidates at each iteration, creating computational overhead, and developing more computationally efficient hyperparameter updating methods is identified as a future direction.
- Why unresolved: While the paper analyzes running time for specific games in GAME BENCH, it doesn't provide theoretical scaling analysis or investigate how CMD performs as environment complexity increases.
- What evidence would resolve it: Systematic experiments varying environment complexity (number of agents, decision points, action space size) and measuring computational requirements, or theoretical analysis of CMD's computational complexity, would resolve this question.

## Limitations
- Computational overhead from evaluating multiple hyper-parameter candidates at each iteration
- Dependence on valid decomposition of convex functions for broader Bregman divergence exploration
- Limited theoretical analysis of convergence rates and scaling behavior

## Confidence
- **High Confidence**: The fundamental algorithmic framework of GMD and CMD is well-specified and theoretically grounded in mirror descent theory
- **Medium Confidence**: The empirical results demonstrate competitive performance across categories, though the benchmark selection may favor the proposed method
- **Low Confidence**: The meta-controller's effectiveness in dynamically adjusting hyper-parameters across all evaluation measures requires further validation in more diverse problem domains

## Next Checks
1. **Scalability Test**: Evaluate CMD's performance and computational overhead on games with larger state and action spaces to assess scalability limitations
2. **Hyperparameter Sensitivity**: Systematically vary the meta-controller's exploration parameters to quantify the impact on convergence speed and solution quality
3. **Cross-Category Transfer**: Test whether CMD's learned hyper-parameters from one decision-making category transfer effectively to another category without retraining