---
ver: rpa2
title: 'Nexus: Specialization meets Adaptability for Efficiently Training Mixture
  of Experts'
arxiv_id: '2408.15901'
source_url: https://arxiv.org/abs/2408.15901
tags:
- expert
- experts
- router
- dense
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Nexus, a novel MoE architecture that enables
  efficient upcycling of specialized dense expert models while maintaining adaptability.
  The key innovation is a router that projects domain embeddings to expert embeddings,
  allowing the model to flexibly add new experts trained on unseen data without requiring
  large-scale MoE retraining.
---

# Nexus: Specialization meets Adaptability for Efficiently Training Mixture of Experts

## Quick Facts
- **arXiv ID**: 2408.15901
- **Source URL**: https://arxiv.org/abs/2408.15901
- **Authors**: Nikolas Gritsch; Qizhen Zhang; Acyr Locatelli; Sara Hooker; Ahmet Üstün
- **Reference count**: 20
- **Primary result**: Upcycling specialized dense models into MoE with 2.1% relative gain (470M) and 1.6% relative gain (2.8B) while enabling efficient extension with new experts

## Executive Summary
Nexus introduces a novel MoE architecture that enables efficient upcycling of specialized dense expert models while maintaining adaptability. The key innovation is a router that projects domain embeddings to expert embeddings, allowing the model to flexibly add new experts trained on unseen data without requiring large-scale MoE retraining. This approach outperforms baseline methods in upcycling specialized models and enables efficient extension with new experts, showing 18.8% relative gain when finetuning on limited data.

## Method Summary
Nexus builds on upcycling by taking dense experts trained on specific domains and combining them into an MoE model. The key innovation is a learned projection that maps pre-computed domain embeddings to expert embeddings, which are then used by the router to compute routing probabilities. The original FFN layer of the seed model is used as a shared expert, always activated, while other experts are the FFN layers from the dense experts. This architecture allows flexible addition of new experts after initial upcycling through separately trained dense models, without requiring large-scale MoE training for unseen data domains.

## Key Results
- Upcycling specialized dense models achieves 2.1% relative gain on 470M parameter models and 1.6% relative gain on 2.8B parameter models
- Efficient extension with new experts shows 18.8% relative gain when finetuning on limited data
- Learned projection preserves domain relationships and maintains specialization, even for newly added experts
- Router learns to specialize experts to their corresponding domains while maintaining the seed model's general capabilities

## Why This Works (Mechanism)

### Mechanism 1
The router learns to map domain embeddings to expert embeddings via a learned projection layer, enabling specialization without requiring large-scale MoE training. Domain embeddings are pre-computed and fixed. A learned projection (two-layer MLP with SwiGLU activation) maps each domain embedding to an expert embedding. The router then computes routing probabilities by measuring the similarity between the input token representation and the expert embeddings.

### Mechanism 2
The learned projection preserves domain relationships after projection, allowing the router to maintain specialization. The projection layer is trained to map domain embeddings to expert embeddings while preserving the relative similarity structure between domains. This ensures that similar domains map to similar expert embeddings.

### Mechanism 3
The shared expert in the MoE layer preserves the capabilities of the seed model while allowing specialization in the routed experts. The original FFN layer of the seed model is used as the shared expert, which is always activated. The other experts are the FFN layers from the dense experts trained on specific domains.

## Foundational Learning

- **Mixture of Experts (MoE) architecture**
  - Why needed here: Understanding the basic MoE framework is essential for understanding how Nexus extends it with learned projections from domain to expert embeddings.
  - Quick check question: In a standard MoE, how is the router typically parameterized and what does it output?

- **Upcycling dense models into MoE**
  - Why needed here: Nexus builds on upcycling by using dense experts trained on specific domains and combining them into an MoE model.
  - Quick check question: What are the key steps in upcycling a dense model into an MoE model, and how does Nexus differ from standard upcycling?

- **Load balancing in MoE training**
  - Why needed here: Nexus is robust to different load balancing factors, which is important for understanding its training stability.
  - Quick check question: What is the purpose of the load balancing loss in MoE training, and how might it affect the router's behavior?

## Architecture Onboarding

- **Component map**:
  - Input representation (token intermediate representation) -> Router (with learned projection) -> Top-k experts -> Shared expert -> Output combination

- **Critical path**:
  1. Compute input representation (token intermediate representation)
  2. Compute expert embeddings by applying learned projection to domain embeddings
  3. Compute routing probabilities by measuring similarity between input representation and expert embeddings
  4. Select top-k experts based on routing probabilities
  5. Compute outputs of shared and selected routed experts
  6. Combine outputs (weighted sum for routed experts, direct addition for shared expert)

- **Design tradeoffs**:
  - Using learned projection from domain to expert embeddings vs. standard linear router: More inductive bias, potentially better specialization, but requires pre-computed domain embeddings
  - Shared expert vs. using seed model's FFN as routed expert: Better preservation of general capabilities, but may reduce parameter efficiency
  - Top-1 routing vs. top-k routing: Simpler, potentially more stable, but may reduce model capacity

- **Failure signatures**:
  - Poor performance on general tasks: May indicate that the shared expert is not adequately preserving general capabilities
  - Poor performance on domain-specific tasks: May indicate that the learned projection is not producing meaningful expert embeddings
  - High variance in routing probabilities: May indicate instability in the router or poor load balancing
  - Low activation of routed experts: May indicate that the learned projection is not producing expert embeddings that are sufficiently different from the shared expert

- **First 3 experiments**:
  1. Train a simple MoE model with the learned projection from domain to expert embeddings on a small dataset, and compare its performance to a standard MoE with a linear router.
  2. Train a dense expert on a specific domain, compute its domain embedding, and visualize the expert embedding produced by the learned projection.
  3. Extend the MoE model with a new expert trained on a new domain, and measure the routing probabilities for the new domain before and after fine-tuning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Nexus's performance scale with increasing numbers of experts beyond what was tested?
- Basis in paper: [inferred] The paper experiments with up to 6 routed experts and 1 shared expert, but does not explore the limits of scalability.
- Why unresolved: The paper focuses on demonstrating effectiveness with a moderate number of experts, leaving the scalability question open.
- What evidence would resolve it: Experiments testing Nexus with significantly larger numbers of experts (e.g., 10, 20, 50) and analyzing performance degradation or improvement.

### Open Question 2
- Question: How does Nexus's adaptive routing compare to other adaptive routing methods like AdaMix or M6's routing?
- Basis in paper: [explicit] The paper mentions related work on adaptive MoEs but does not directly compare Nexus to these specific methods.
- Why unresolved: While Nexus is shown to outperform linear router baselines, a direct comparison to other state-of-the-art adaptive routing methods is missing.
- What evidence would resolve it: Benchmarking Nexus against AdaMix, M6, and other adaptive routing methods on the same datasets and tasks.

### Open Question 3
- Question: Can the learned projection in Nexus be further improved by incorporating additional information beyond domain embeddings?
- Basis in paper: [explicit] The paper notes that the projection layer uses domain embeddings as input, but does not explore alternative or additional inputs.
- Why unresolved: The current projection relies solely on domain embeddings, leaving the question of whether other features could enhance the learned projection.
- What evidence would resolve it: Experiments testing Nexus with alternative or additional inputs to the projection layer, such as task embeddings, input token features, or intermediate representations.

## Limitations

- The entire architecture hinges on pre-computed domain embeddings that adequately capture the semantic characteristics of each expert's training data, but no quantitative evaluation is provided showing how domain embedding quality affects final performance.
- Limited ablation studies on key design choices like the two-layer projection depth, SwiGLU activation choice, and different domain embedding models.
- Results are shown primarily for 470M and 2.8B models, with efficient extension claim demonstrated only for one domain (Code).

## Confidence

**High confidence**: The core claim that learned projection from domain to expert embeddings can improve upon standard linear routers in MoE upcycling. The mechanism is well-specified and the experimental results show consistent improvements across multiple model scales and evaluation tasks.

**Medium confidence**: The claim about efficient extension with new experts. While the 18.8% relative gain is demonstrated, it's only shown for one domain (Code) and doesn't establish whether this efficiency scales to multiple new domains or different data regimes.

**Low confidence**: The claim that domain embeddings adequately capture semantic characteristics for all use cases. The paper doesn't provide quantitative analysis of domain embedding quality or its correlation with routing performance.

## Next Checks

1. **Domain embedding ablation**: Systematically vary the quality and type of domain embeddings (e.g., different embedding models, dimensionality, or learned vs. fixed embeddings) to quantify their impact on routing performance and specialization.

2. **Multi-expert extension scalability**: Test the efficient extension claim by adding multiple new experts (3-5) trained on diverse domains simultaneously, measuring both performance gains and training stability compared to full MoE retraining.

3. **Router architecture ablation**: Replace the learned projection with alternative architectures (single linear layer, different activation functions, deeper MLPs) to isolate whether the specific design choices or the general domain-to-expert mapping concept drives the performance improvements.