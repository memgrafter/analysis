---
ver: rpa2
title: Investigating Low-Cost LLM Annotation for~Spoken Dialogue Understanding Datasets
arxiv_id: '2406.13269'
source_url: https://arxiv.org/abs/2406.13269
tags:
- annotation
- dialogue
- annotations
- spoken
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of LLM-based semi-automatic annotation
  for enriching spoken dialogue datasets with fine-grained semantic representations.
  It addresses the gap between textual and spoken dialogue understanding by proposing
  structured contextual meaning representations tailored to the hotel booking domain.
---

# Investigating Low-Cost LLM Annotation for~Spoken Dialogue Understanding Datasets

## Quick Facts
- arXiv ID: 2406.13269
- Source URL: https://arxiv.org/abs/2406.13269
- Reference count: 27
- Primary result: Fine-tuned Mistral-7B with LoRA outperforms prompting approaches for semi-automatic annotation of spoken dialogue datasets

## Executive Summary
This paper presents a low-cost approach for annotating spoken dialogue datasets using fine-tuned large language models (LLMs). The method addresses the gap between textual and spoken dialogue understanding by creating fine-grained semantic representations for the hotel booking domain. Through human annotation, LoRA fine-tuning of Mistral-7B, grammar-constrained generation, and iterative correction, the approach demonstrates that automatic annotations can capture significant knowledge from human annotations while reducing manual effort.

## Method Summary
The method involves initial human annotation of a subset of dialogues, followed by LoRA fine-tuning of Mistral-7B on these annotations. Grammar-constrained decoding ensures valid ontology output while matching speaker transcription spans. A score estimator filters low-quality annotations for human correction, enabling iterative model retraining. The process uses smatch score for evaluation and balances annotation quality with human effort through threshold-based filtering.

## Key Results
- Fine-tuned Mistral-7B with LoRA outperforms prompting approaches on annotation quality metrics
- Grammar-constrained decoding improves structural quality but may reduce empty tree performance
- Iterative annotation with filtering and correction demonstrates promise for accelerating manual annotation
- Automatic annotations capture significant knowledge from human annotations for effective model training

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning Mistral-7B with LoRA enables domain-specific knowledge capture while preserving base model capabilities. The approach adapts large models on consumer GPUs by training only low-rank delta matrices. This allows effective learning from a relatively small set of high-quality human annotations (208 dialogues, 10% annotated by multiple annotators).

### Mechanism 2
Grammar-constrained decoding improves structural quality by restricting vocabulary at each decoding step to grammar-valid tokens and speaker transcription spans. This ensures generated annotations conform to ontology structure while matching actual speech content, though it may reduce performance on empty trees.

### Mechanism 3
Semi-automatic annotation with iterative human correction and model retraining achieves high-quality annotations at lower cost than full manual annotation. The process uses a score estimator to filter low-quality annotations, which are then corrected by humans and used to retrain the model in an iterative loop.

## Foundational Learning

- **Abstract Meaning Representation (AMR)**: Needed to create fine-grained semantic representations adapted for spoken dialogue. Quick check: What are the key components of an AMR graph, and how do they differ from traditional semantic role labeling?

- **Low-Rank Adaptation (LoRA)**: Enables efficient fine-tuning of large language models on consumer-grade hardware by training only low-rank matrices instead of full model weights. Quick check: How does LoRA's approach to parameter-efficient fine-tuning differ from other methods like adapters or prefix tuning?

- **Semantic Match (SMATCH) score**: Used to evaluate the quality of generated annotations by comparing them to human annotations, accounting for variable alignment and matching triples. Quick check: What are the limitations of SMATCH as an evaluation metric for semantic parsing tasks?

## Architecture Onboarding

- **Component map**: Human annotation interface -> LoRA fine-tuning pipeline (Mistral-7B) -> Grammar-constrained decoding module -> SMATCH evaluation metric -> Score estimator (SVR with sentence embeddings) -> Iterative annotation loop

- **Critical path**: 1. Initial human annotation of subset, 2. LoRA fine-tuning on annotated data, 3. Grammar-constrained generation of annotations, 4. SMATCH evaluation and score estimation, 5. Filtering and human correction of low-quality annotations, 6. Retraining with corrected annotations

- **Design tradeoffs**: Fine-tuning vs. prompting (customization vs. speed), grammar constraints vs. flexibility (quality vs. handling unusual cases), iteration depth vs. diminishing returns (quality vs. cost)

- **Failure signatures**: Poor SMATCH scores indicating structural mismatches, high ontology error rates suggesting ontology non-compliance, score estimator failing to identify low-quality annotations

- **First 3 experiments**: 1. Compare SMATCH scores of fine-tuned Mistral-7B vs. prompted commercial LLMs, 2. Evaluate grammar-constrained decoding impact on full vs. empty trees, 3. Test iterative annotation loop with different filtering thresholds

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of grammar-constrained decoding compare to unconstrained decoding on full versus empty semantic trees in spoken dialogue annotation? The paper presents findings that grammar-constrained decoding improves full tree annotations but reduces performance on empty trees, but does not provide detailed analysis of trade-offs or solutions.

### Open Question 2
What are the implications of using fine-tuned open-weight LLMs versus prompt-based commercial models for iterative annotation pipelines in terms of cost, efficiency, and annotation quality? The paper highlights that fine-tuning enables faster iterations but does not explore long-term cost implications or quality differences across linguistic contexts.

### Open Question 3
How transferable are the results of this fine-tuning approach to low-resource languages or multi-domain dialogue datasets? The paper acknowledges that the MEDIA dataset is single-domain French but does not provide empirical data on effectiveness in low-resource languages or multi-domain scenarios.

## Limitations
- Single domain (hotel booking) and language (French) limit generalizability to other domains or languages
- Evaluation relies heavily on SMATCH metric which may not fully capture semantic equivalence or practical utility
- Grammar-constrained approach may be too restrictive for complex dialogue phenomena or fail to generate empty trees when appropriate
- Iterative annotation loop is only demonstrated once, leaving questions about convergence properties and long-term effectiveness

## Confidence
- **High Confidence**: Fine-tuning Mistral-7B with LoRA outperforms prompting approaches, methodology for grammar-constrained decoding is clearly described
- **Medium Confidence**: Approach can accelerate manual annotation, score estimator effectiveness is demonstrated but needs more validation
- **Low Confidence**: Generalizability to other domains/languages, long-term effectiveness of iterative loop, impact of different filtering thresholds

## Next Checks
1. **Domain Generalization Test**: Apply the fine-tuning and grammar-constrained decoding approach to a different spoken dialogue dataset (e.g., medical consultations) to assess generalizability beyond hotel booking domain.

2. **Ablation Study on Grammar Constraints**: Conduct controlled experiment comparing annotation quality with and without grammar constraints across different dialogue complexity levels to determine if constraints are universally beneficial.

3. **Extended Iterative Loop Evaluation**: Implement multiple iterations of the annotation loop with varying filtering thresholds and measure convergence properties, quality improvements, and cost-effectiveness over time.