---
ver: rpa2
title: Adaptive $k$-nearest neighbor classifier based on the local estimation of the
  shape operator
arxiv_id: '2409.05084'
source_url: https://arxiv.org/abs/2409.05084
tags:
- curvature
- classifier
- k-nn
- algorithm
- kk-nn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel adaptive k-nearest neighbor (kK-NN)
  classifier that leverages local curvature information to dynamically adjust the
  neighborhood size for each sample. The method estimates the local Gaussian curvature
  using an approximation of the shape operator based on local covariance and Hessian
  matrices.
---

# Adaptive $k$-nearest neighbor classifier based on the local estimation of the shape operator

## Quick Facts
- arXiv ID: 2409.05084
- Source URL: https://arxiv.org/abs/2409.05084
- Authors: Alexandre Luís Magalhães Levada; Frank Nielsen; Michel Ferreira Cardia Haddad
- Reference count: 18
- One-line primary result: Curvature-based adaptive k-NN achieves superior balanced accuracy compared to standard k-NN and another adaptive k-NN algorithm across 30 real-world datasets

## Executive Summary
This paper introduces a novel adaptive k-nearest neighbor (kK-NN) classifier that leverages local curvature information to dynamically adjust the neighborhood size for each sample. The method estimates the local Gaussian curvature using an approximation of the shape operator based on local covariance and Hessian matrices. This curvature-based approach allows the classifier to use larger neighborhoods for low-curvature points and smaller neighborhoods for high-curvature points, resulting in superior balanced accuracy compared to both standard k-NN and another adaptive k-NN algorithm across 30 real-world datasets.

## Method Summary
The kK-NN classifier constructs a k-NN graph using k = log₂(n) neighbors, then estimates Gaussian curvature at each point using the shape operator computed from local covariance and Hessian matrices. Curvature values are normalized and quantized into 10 bins. For each point, the cᵢ farthest neighbors are pruned where cᵢ is the quantized curvature score, leaving only the nearest (k - cᵢ) neighbors for classification. This adaptive neighborhood adjustment improves classification accuracy by using larger neighborhoods in flat regions and smaller neighborhoods near complex boundaries.

## Key Results
- kK-NN demonstrates superior balanced accuracy compared to both standard k-NN and another adaptive k-NN algorithm across 30 real-world datasets
- The method shows particular strength when training data is limited, suggesting it can learn more discriminative functions with fewer samples
- The primary tradeoff is higher computational cost compared to standard k-NN due to O(n²m²) complexity for curvature estimation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local curvature estimation allows dynamic adjustment of neighborhood size per sample.
- Mechanism: The algorithm estimates Gaussian curvature at each data point using a discrete approximation of the shape operator (computed from local covariance and Hessian matrices). Points with low curvature get larger neighborhoods (flat regions), while high-curvature points get smaller neighborhoods (edges/ridges).
- Core assumption: Local Gaussian curvature accurately reflects how well a tangent plane approximates the data manifold at that point.
- Evidence anchors:
  - [abstract]: "The rationale is that points with low curvature could have larger neighborhoods (locally, the tangent space approximates well the underlying data shape), whereas points with high curvature could have smaller neighborhoods (locally, the tangent space is a loose approximation)."
  - [section]: "The intuition behind the kK-NN classifier is to explore the local curvature to define the size of the neighborhood k at each vertex of the k-NNG in an adaptive manner."
- Break condition: If the manifold assumption fails (e.g., data is not locally smooth) or curvature estimation is noisy, the adaptive adjustment may misclassify samples.

### Mechanism 2
- Claim: Quantized curvature scores prune edges in the k-NN graph to create sample-specific neighborhoods.
- Mechanism: After computing curvature for all points, values are normalized to [0,1] and quantized into 10 bins. For a point with score cᵢ, the cᵢ farthest neighbors are pruned, leaving only the nearest (k - cᵢ) neighbors. This reduces overfitting near complex boundaries and increases robustness to noise.
- Core assumption: Pruning based on curvature score reliably isolates outliers and reduces influence of high-curvature (potentially noisy) points.
- Evidence anchors:
  - [abstract]: "The curvature are then quantized into ten different scores... its farthest k − ci neighbors are pruned while assigning the most frequent label in the adjusted neighborhood."
  - [section]: "high curvature points are commonly related to outliers, being the kK-NN classifier capable of isolating such samples by drastically reducing its neighborhoods."
- Break condition: If quantization granularity is too coarse, distinct curvature regimes may be merged, reducing discriminative power.

### Mechanism 3
- Claim: Adaptive neighborhoods improve classification when training data is limited.
- Mechanism: By tailoring neighborhood size to local geometry, the classifier avoids oversmoothing in dense regions and undersmoothing near complex structures, learning more discriminative functions with fewer samples.
- Core assumption: Local geometry varies meaningfully across the dataset, and capturing this variation improves generalization under data scarcity.
- Evidence anchors:
  - [abstract]: "This is particularly evident when the number of samples in the training data is limited, suggesting that the kK-NN is capable of learning more discriminant functions with less data."
  - [section]: "The kK-NN classifier learns more discriminant decision functions while the number of training samples is substantially reduced."
- Break condition: If the local geometry is nearly uniform across the dataset, adaptive adjustment provides little benefit over fixed-k k-NN.

## Foundational Learning

- Concept: Differential geometry of manifolds and tangent spaces.
  - Why needed here: The method approximates local manifold structure to estimate curvature; understanding tangent spaces and curvature is essential to grasp why this works.
  - Quick check question: What does a high Gaussian curvature at a point indicate about the local geometry of the data manifold?

- Concept: Covariance and Hessian matrix estimation.
  - Why needed here: Curvature is estimated using the local covariance matrix (for metric tensor) and Hessian matrix (for second fundamental form). Knowing how to compute and interpret these matrices is critical.
  - Quick check question: How does the local covariance matrix approximate the metric tensor of a data manifold?

- Concept: Graph theory and k-NN graph construction.
  - Why needed here: The method builds a k-NN graph and prunes edges based on curvature; understanding graph connectivity and edge pruning is necessary to implement the algorithm.
  - Quick check question: What is the effect of pruning edges in a k-NN graph on the classification outcome?

## Architecture Onboarding

- Component map:
  - Input data -> k-NN graph construction -> Local covariance/Hessian estimation -> Shape operator computation -> Curvature calculation -> Quantization -> Edge pruning -> Classification (train/test)
- Critical path:
  - Building k-NN graph -> Curvature estimation -> Neighborhood adjustment -> Label assignment. Delays in curvature estimation directly impact runtime.
- Design tradeoffs:
  - Adaptive k improves accuracy and robustness but increases computational cost (O(n²m²) vs O(n log n) for regular k-NN). Tradeoff is accuracy vs speed.
- Failure signatures:
  - Degraded accuracy when data is nearly uniform (little curvature variation). High runtime for high-dimensional data. Sensitivity to noise in curvature estimation.
- First 3 experiments:
  1. Run kK-NN on a low-dimensional dataset with known manifold structure (e.g., Swiss roll) and visualize how neighborhoods adapt.
  2. Compare accuracy vs regular k-NN as training set size decreases on a synthetic dataset.
  3. Measure runtime scaling with number of features and samples to confirm O(n²m²) complexity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the computational complexity of the kK-NN algorithm scale with increasing dimensionality, and at what point does it become impractical compared to standard k-NN?
- Basis in paper: [explicit] The paper states "The computational complexity of the function kkNN-test is also dominated by the computation of the curvatures... the global cost is equivalent to the function employed in the training stage. In comparison with the regular k-NN and the adaptive k-NN, the proposed kK-NN classifier shows a higher computational cost."
- Why unresolved: The paper only provides asymptotic complexity analysis but does not empirically test the algorithm's performance across datasets with varying numbers of features. No specific dimensionality thresholds or performance benchmarks are provided.
- What evidence would resolve it: Empirical runtime comparisons of kK-NN versus k-NN across datasets with increasing dimensionality, including specific dimensionality thresholds where kK-NN becomes impractical.

### Open Question 2
- Question: How sensitive is the kK-NN classifier's performance to the choice of the initial k = log₂(n) parameter used for building the k-NN graph, and could alternative initial k values lead to better performance?
- Basis in paper: [explicit] The paper states "The curvature of all vertices of the k-NNG is computed using the shape operator based algorithm... using k = log₂(n), where n is the number of samples" but does not explore sensitivity to this parameter choice.
- Why unresolved: The paper adopts a fixed initial k value based on log₂(n) but does not investigate whether this is optimal or how performance varies with different initial k values.
- What evidence would resolve it: Systematic experiments varying the initial k parameter (e.g., k = 1, 3, 5, 10, log₂(n), 2log₂(n)) across multiple datasets to determine sensitivity and optimal ranges.

### Open Question 3
- Question: How does the kK-NN classifier perform on datasets with non-uniform class distributions or severe class imbalance, where curvature-based neighborhood adjustment might interact differently with minority class samples?
- Basis in paper: [inferred] While the paper mentions "Impact on class-imbalanced data" in the introduction, it does not specifically test the kK-NN classifier on imbalanced datasets or analyze its behavior in such scenarios.
- Why unresolved: The experimental evaluation focuses on balanced accuracy and does not explore performance on imbalanced datasets where curvature-based methods might have unique advantages or disadvantages.
- What evidence would resolve it: Experiments on deliberately imbalanced datasets (e.g., varying class ratios from 50:50 to 99:1) measuring not just balanced accuracy but also per-class performance metrics like precision, recall, and F1-score for minority classes.

## Limitations
- High computational complexity (O(n²m²)) makes the method impractical for very large datasets or high-dimensional features
- Performance depends on the assumption that local manifold structure exists and can be reliably estimated
- Limited empirical evaluation on datasets with severe class imbalance or non-uniform distributions

## Confidence
- High: The mechanism of adaptive neighborhood sizing based on curvature is well-justified and empirically validated.
- Medium: The claim that the method excels with limited training data is supported by results but needs more systematic analysis across varying sample sizes.
- Low: The exact implementation details of the competing adaptive k-NN algorithm from LeJeune et al. 2019 are not fully specified, making direct comparison uncertain.

## Next Checks
1. Systematically evaluate performance as training set size decreases from 100% to 10% on multiple datasets to quantify the claimed advantage with limited data.
2. Test the method on synthetic manifolds with known curvature profiles (e.g., Swiss roll, spheres) to verify curvature estimation accuracy and its impact on classification.
3. Implement a runtime profiling study to confirm the O(n²m²) complexity and identify bottlenecks for potential optimization.