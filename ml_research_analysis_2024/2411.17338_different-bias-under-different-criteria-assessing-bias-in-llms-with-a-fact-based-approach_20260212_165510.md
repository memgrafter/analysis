---
ver: rpa2
title: 'Different Bias Under Different Criteria: Assessing Bias in LLMs with a Fact-Based
  Approach'
arxiv_id: '2411.17338'
source_url: https://arxiv.org/abs/2411.17338
tags:
- bias
- criteria
- arxiv
- occupation
- gender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel metric called "statistical alignment"
  for assessing bias in large language models (LLMs) based on real-world demographic
  statistics rather than equality-based criteria. The authors conduct a human survey
  showing that people prefer LLM outputs that align with real-world demographic distributions.
---

# Different Bias Under Different Criteria: Assessing Bias in LLMs with a Fact-Based Approach

## Quick Facts
- arXiv ID: 2411.17338
- Source URL: https://arxiv.org/abs/2411.17338
- Reference count: 40
- This paper introduces a novel metric called "statistical alignment" for assessing bias in large language models based on real-world demographic statistics rather than equality-based criteria.

## Executive Summary
This paper introduces a novel metric called "statistical alignment" for assessing bias in large language models (LLMs) based on real-world demographic statistics rather than equality-based criteria. The authors conduct a human survey showing that people prefer LLM outputs that align with real-world demographic distributions. When evaluating various LLMs using their proposed metric alongside traditional equality-based metrics (balance and refusal), they find that model bias varies significantly depending on which criteria are used. The results highlight the importance of assessing bias from multiple perspectives, as models that appear unbiased under equality-based criteria may show different bias patterns when evaluated against factual demographic data.

## Method Summary
The authors evaluate LLM bias using three metrics: MB (balance metric), MR (refusal rate), and MS (statistical alignment). They use coreference resolution tasks from the WinoBias dataset and persona-based occupation selection tasks, collecting responses from various LLMs including Llama2, Llama3, Mistral, Qwen series, and GPT models. The MS metric measures how closely LLM outputs match real-world demographic distributions from US Bureau of Labor Statistics. Human survey data from 58 participants validates that people prefer responses aligned with demographic statistics.

## Key Results
- Different bias assessment criteria reveal different bias patterns in LLMs
- Human survey shows people prefer LLM outputs that align with real-world demographic distributions
- Models that appear unbiased under equality-based criteria may show different bias patterns when evaluated against factual demographic data
- RLHF and instruction-tuning can cause unpredictable shifts in statistical alignment

## Why This Works (Mechanism)

### Mechanism 1
The proposed metric aligns better with human preferences than equality-based metrics. The statistical alignment metric measures how closely LLM outputs match real-world demographic distributions, and human survey data shows people prefer responses that align with these distributions. Core assumption: Real-world demographic distributions represent the "natural" or preferred distribution of occupations by gender/age groups.

### Mechanism 2
Different bias assessment criteria reveal different bias patterns in LLMs. The study uses three distinct metrics (MB for balance, MR for refusal, MS for statistical alignment) that capture different aspects of bias, showing that models can appear unbiased under one metric while being biased under another. Core assumption: Bias is multi-dimensional and cannot be fully captured by a single metric.

### Mechanism 3
RLHF and instruction-tuning can change statistical alignment in unpredictable ways. The study shows that fine-tuning processes can either increase or decrease statistical alignment depending on the model and task, sometimes leading to anti-stereotypical responses that contradict real-world statistics. Core assumption: Fine-tuning processes can introduce unintended shifts in model behavior beyond the intended bias reduction.

## Foundational Learning

- Concept: Statistical alignment vs. balance metrics
  - Why needed here: Understanding the difference between these metrics is crucial for interpreting the study's results, as they capture fundamentally different notions of what constitutes "unbiased" behavior
  - Quick check question: If a model perfectly balances responses between male and female occupations (MB=0) but chooses occupations completely uncorrelated with real-world demographics, what would its MS score be?

- Concept: Coreference resolution tasks for bias detection
  - Why needed here: The study uses coreference resolution tasks with ambiguous pronouns to measure inherent bias without definitive answers, which is a key methodological innovation
  - Quick check question: Why did the researchers choose ambiguous sentences from WinoBias rather than unambiguous ones for their experiments?

- Concept: Persona-based bias assessment
  - Why needed here: The study introduces a novel persona-based task to assess how LLMs respond to demographic information, complementing the coreference resolution approach
  - Quick check question: How does assigning a persona to an LLM before asking it to choose an occupation differ from simply asking about pronoun referents in sentences?

## Architecture Onboarding

- Component map: Task generation (coreference resolution and persona assignment) → LLM response collection → Metric computation (MB, MR, MS) → Statistical alignment analysis
- Critical path: Task generation → LLM inference → Score calculation → Statistical alignment analysis
- Design tradeoffs: Using real-world statistics provides objective benchmarks but may perpetuate existing biases; using balance metrics provides egalitarian ideals but may ignore natural demographic distributions
- Failure signatures: Models with high MB but low MS may be artificially balanced without reflecting reality; models with high MS but high MB may reflect real-world biases while still treating groups unequally
- First 3 experiments:
  1. Run coreference resolution task on a base model and compute all three metrics to establish baseline bias patterns
  2. Apply the same task to an RLHF-tuned version of the model and compare metric changes to identify fine-tuning effects
  3. Conduct a small human survey with the same occupation pairs to validate that human preferences align with statistical alignment metric results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do statistical alignment metrics perform when applied to intersectional categories beyond simple binary gender and age groupings?
- Basis in paper: Explicit - The paper acknowledges limitations in only using binary gender and age categories
- Why unresolved: The current study's methodology is constrained by binary categorizations and U.S.-specific data
- What evidence would resolve it: Testing the statistical alignment metrics with multi-dimensional intersectional categories and applying the methodology to demographic data from multiple countries

### Open Question 2
- Question: What is the optimal balance between equality-based and fact-based criteria for bias assessment across different types of LLM applications?
- Basis in paper: Explicit - The paper demonstrates a trade-off between MB (balance) and MS (statistical alignment) scores
- Why unresolved: While the paper shows these metrics can vary inversely, it doesn't establish guidelines for when to prioritize one over the other
- What evidence would resolve it: Conducting application-specific evaluations to determine optimal weighting schemes between equality and statistical alignment metrics

### Open Question 3
- Question: How does the statistical alignment metric behave when applied to languages and cultures with fundamentally different demographic distributions than those in the United States?
- Basis in paper: Explicit - The paper acknowledges using U.S. demographic data
- Why unresolved: The paper doesn't explore whether the statistical alignment approach remains valid or requires adjustment when applied to countries with different cultural contexts
- What evidence would resolve it: Applying the same methodology to demographic data from countries with different cultural contexts

## Limitations

- The proposed statistical alignment metric depends on the quality and relevance of real-world demographic statistics, which may themselves reflect historical discrimination
- The human survey involved only 58 participants with limited demographic diversity, potentially not representing broader population preferences
- The paper does not provide a principled framework for determining which criterion should take precedence in different contexts

## Confidence

**High Confidence**: The finding that different bias assessment criteria reveal different bias patterns in LLMs is well-supported by the empirical results.

**Medium Confidence**: The claim that human preferences align with statistical alignment metrics is supported by survey data but limited by the small sample size.

**Low Confidence**: The assertion that RLHF and instruction-tuning can cause unpredictable shifts in statistical alignment is based on observed correlations but lacks deeper analysis of underlying mechanisms.

## Next Checks

1. **Demographic Validation of Survey Participants**: Replicate the human preference study with a larger, more diverse participant pool stratified by age, gender, race, and cultural background.

2. **Temporal Stability Analysis**: Track how MS, MB, and MR metrics change over time as LLMs are fine-tuned on different datasets using controlled fine-tuning experiments.

3. **Bias Impact Assessment**: Design an experiment comparing model outputs across the three metrics in real-world applications to measure downstream effects on different demographic groups.