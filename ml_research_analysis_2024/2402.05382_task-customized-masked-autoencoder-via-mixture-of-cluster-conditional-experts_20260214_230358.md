---
ver: rpa2
title: Task-customized Masked AutoEncoder via Mixture of Cluster-conditional Experts
arxiv_id: '2402.05382'
source_url: https://arxiv.org/abs/2402.05382
tags:
- moce
- experts
- expert
- pre-training
- downstream
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the negative transfer problem in Masked Autoencoders
  (MAE) where pre-training on irrelevant data can harm downstream task performance.
  The authors propose Mixture of Cluster-conditional Experts (MoCE), a novel paradigm
  that trains each expert with semantically relevant images through cluster-conditional
  gates.
---

# Task-customized Masked AutoEncoder via Mixture of Cluster-conditional Experts

## Quick Facts
- arXiv ID: 2402.05382
- Source URL: https://arxiv.org/abs/2402.05382
- Reference count: 19
- Primary result: MoCE improves MAE performance by 2.45% top-1 accuracy and achieves state-of-the-art results in detection and segmentation

## Executive Summary
This paper addresses the negative transfer problem in Masked Autoencoders (MAE) where pre-training on irrelevant data can harm downstream task performance. The authors propose Mixture of Cluster-conditional Experts (MoCE), a novel paradigm that trains each expert with semantically relevant images through cluster-conditional gates. Unlike vanilla MAE or TokenMoE, MoCE uses cluster embeddings to route tokens, ensuring each expert learns from semantically similar data. Experiments on 11 downstream tasks show MoCE outperforms vanilla MAE by 2.45% in top-1 accuracy and achieves state-of-the-art results in detection and segmentation.

## Method Summary
MoCE addresses negative transfer in MAE by training multiple experts specialized for semantically similar data clusters. The method consists of three stages: first, pre-training a dense MAE on full ImageNet and extracting features for all images; second, clustering the pre-training data using Sinkhorn-Knopp algorithm to create semantically meaningful partitions; third, initializing MoCE with the pre-trained MAE and training with cluster-conditional gates that route tokens based on cluster embeddings rather than low-level features. The model uses 8 experts and 256 clusters, with imbalance loss to stabilize training. For deployment, the best expert is selected by clustering downstream task images and choosing the cluster with maximum assignment.

## Key Results
- MoCE improves MAE performance by 2.45% in top-1 accuracy across 11 downstream tasks
- Achieves state-of-the-art results in detection and segmentation on ADE20K and COCO datasets
- Demonstrates better PSNR and training/testing efficiency compared to vanilla MAE and TokenMoE baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The negative transfer phenomenon occurs because MAE pre-trained on semantically irrelevant data can harm downstream task performance.
- Mechanism: When MAE is pre-trained on data with different semantic distributions than the downstream task, the irrelevant pre-training information impedes transfer performance.
- Core assumption: Downstream tasks have distinct semantic distributions that may not align with the full ImageNet pre-training data.
- Evidence anchors:
  - [abstract]: "when the various downstream tasks have data distributions different from the pre-training data, the semantically irrelevant pre-training information might result in negative transfer"
  - [section 3.1]: "MAE pre-trained with Split-A performs best on Aircraft, Cars, SUN397 and DTD, while the MAE pre-trained with Split-B performs best on Flowers, Food, Pets, Cifar-10 and Cifar-100"
  - [corpus]: No direct evidence found in corpus, weak support from related work on negative transfer in self-supervised learning
- Break condition: If downstream tasks have semantic distributions similar to the full ImageNet pre-training data, negative transfer would not occur.

### Mechanism 2
- Claim: TokenMoE fails to improve transfer performance because token routing is based on low-level pixel values rather than semantic information.
- Mechanism: TokenMoE uses pixel RGB values as reconstruction targets, causing tokens with similar pixel values to be routed to the same expert regardless of semantics.
- Core assumption: Self-supervised learning lacks semantic labels, so experts in TokenMoE differ more on low-level information than semantics.
- Evidence anchors:
  - [section 3.2]: "naive adoption of MoE to the MAE has inferior performance" and "tokens with similar pixel values tend to be routed to the same expert"
  - [abstract]: "unlike supervised pre-training, self-supervised learning lacks semantic labels, and thus the experts differ more on low-level information than semantics"
  - [corpus]: No direct evidence found in corpus, weak support from related work on vision transformer experts
- Break condition: If semantic information could be incorporated into token routing without supervision, TokenMoE might improve.

### Mechanism 3
- Claim: MoCE improves transfer by training each expert with semantically similar images through cluster-conditional gates.
- Mechanism: MoCE uses cluster embeddings to route tokens, ensuring each expert learns from semantically similar data rather than low-level features.
- Core assumption: Clustering pre-training data by semantics creates meaningful partitions that align with downstream task requirements.
- Evidence anchors:
  - [abstract]: "Different from the mixture of experts (MoE), our MoCE trains each expert only with semantically relevant images by using cluster-conditional gates"
  - [section 3.3]: "MoCE trains each expert in a semantic-aware manner" and "We select the largest cluster with assigned downstream images"
  - [corpus]: No direct evidence found in corpus, but related work on data clustering for self-supervised learning provides weak support
- Break condition: If clustering fails to capture meaningful semantic distinctions, MoCE would not provide customized pre-training.

## Foundational Learning

- Concept: Masked Autoencoder (MAE)
  - Why needed here: MAE is the base architecture being modified to address negative transfer
  - Quick check question: How does MAE achieve self-supervised learning through masked image modeling?

- Concept: Mixture of Experts (MoE)
  - Why needed here: MoE provides the multi-expert architecture that enables task-customized pre-training
  - Quick check question: What is the key difference between standard MoE and the cluster-conditional approach used in MoCE?

- Concept: Data Clustering
  - Why needed here: Clustering creates semantically meaningful partitions of pre-training data for expert specialization
  - Quick check question: How does the Sinkhorn-Knopp algorithm contribute to balanced clustering assignments?

## Architecture Onboarding

- Component map:
  Pre-trained MAE -> Feature Extraction -> Clustering Module -> Multi-expert Transformer Layers -> Cluster-conditional Gates -> Routing Mechanism -> Regularization Losses

- Critical path:
  1. Pre-train MAE on full ImageNet
  2. Extract features and cluster data semantically
  3. Initialize MoCE with expert specialization
  4. Train with cluster-conditional gates and regularization
  5. Deploy by selecting expert closest to downstream task

- Design tradeoffs:
  - Number of clusters vs. expert specialization granularity
  - Cluster size balance vs. semantic purity
  - Gate confidence regularization vs. routing flexibility
  - Model capacity vs. inference efficiency

- Failure signatures:
  - Poor downstream performance despite good pre-training metrics
  - Experts showing no semantic specialization in routing heatmaps
  - Cluster assignments that don't align with semantic similarities
  - Training instability due to gate confidence issues

- First 3 experiments:
  1. Verify negative transfer exists by comparing MAE pre-trained on different data subsets
  2. Test TokenMoE implementation to confirm it fails to improve transfer
  3. Validate MoCE clustering produces semantically meaningful partitions before full training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MoCE scale with different numbers of clusters beyond 256?
- Basis in paper: [inferred] The paper shows MoCE performance peaks at 256 clusters, but does not explore further scaling.
- Why unresolved: The paper only tests up to 512 clusters, showing performance saturation, but does not explore higher cluster counts to determine if there is a theoretical limit or optimal cluster count for different downstream tasks.
- What evidence would resolve it: Experiments testing MoCE with varying numbers of clusters (e.g., 1024, 2048) across multiple downstream tasks to determine the relationship between cluster count and performance.

### Open Question 2
- Question: Can the clustering methodology used in MoCE be applied to other self-supervised learning frameworks beyond MAE?
- Basis in paper: [explicit] The paper demonstrates MoCE's effectiveness with MAE but does not explore its applicability to other SSL methods like DINO or SimCLR.
- Why unresolved: While MoCE shows promise for MAE, the paper does not investigate whether the clustering approach can be generalized to other SSL architectures, which would broaden its impact.
- What evidence would resolve it: Experiments applying MoCE's clustering methodology to other SSL frameworks and comparing their performance against their vanilla counterparts.

### Open Question 3
- Question: How does the imbalance loss (Limbalance) affect the training dynamics and final performance of MoCE compared to using only importance and load losses?
- Basis in paper: [explicit] The paper introduces Limbalance to enhance gate confidence but does not provide a detailed ablation of its effects compared to using only importance and load losses.
- Why unresolved: While Limbalance is shown to improve performance, the paper does not quantify its specific contribution or compare its effects to using only the traditional importance and load losses.
- What evidence would resolve it: A controlled experiment comparing MoCE with and without Limbalance, using only importance and load losses, to measure the impact on performance and training dynamics.

## Limitations

- The paper lacks ablation studies on the number of clusters and experts, making it unclear what the optimal configuration is for different downstream tasks
- No comparison with other clustering methods or alternative routing mechanisms to establish the superiority of the cluster-conditional approach
- The selection of the 11th and 12th ViT layers for MoCE insertion is based on gradient magnitude, but this heuristic isn't validated across different model architectures

## Confidence

- **High confidence**: The existence of negative transfer phenomenon (supported by empirical evidence across multiple downstream tasks)
- **Medium confidence**: The effectiveness of MoCE in mitigating negative transfer (demonstrated on 11 tasks but lacks comprehensive ablation studies)
- **Medium confidence**: The superiority of cluster-conditional routing over token-based routing (logical argument supported but not exhaustively tested)

## Next Checks

1. Conduct ablation studies varying the number of clusters (e.g., 64, 128, 256, 512) to determine optimal granularity for different task types
2. Test alternative routing mechanisms that incorporate semantic information without clustering, such as using pre-trained image classifiers for routing decisions
3. Evaluate MoCE performance on additional downstream tasks beyond the 11 reported to assess generalizability across diverse domains