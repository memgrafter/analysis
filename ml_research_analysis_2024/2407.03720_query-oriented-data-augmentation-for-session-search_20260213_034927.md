---
ver: rpa2
title: Query-oriented Data Augmentation for Session Search
arxiv_id: '2407.03720'
source_url: https://arxiv.org/abs/2407.03720
tags:
- search
- query
- queries
- data
- session
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces query-oriented data augmentation to address
  limitations in existing session search training paradigms. The authors observe that
  current methods neglect the symmetric nature of relevance between search context
  and documents, failing to account for how document relevance varies with context
  changes.
---

# Query-oriented Data Augmentation for Session Search

## Quick Facts
- arXiv ID: 2407.03720
- Source URL: https://arxiv.org/abs/2407.03720
- Authors: Haonan Chen; Zhicheng Dou; Yutao Zhu; Ji-Rong Wen
- Reference count: 40
- Primary result: QASS achieves 2.94% improvement in NDCG@1 on AOL compared to strong baselines

## Executive Summary
This paper addresses limitations in session search training paradigms by introducing query-oriented data augmentation. The authors observe that existing methods neglect the symmetric nature of relevance between search context and documents, failing to account for how document relevance varies with context changes. Their proposed method, QASS, generates new training pairs by altering the current query through term-level modifications and query-level replacements, enabling models to learn how relevance shifts with context. Experiments on AOL and Tiangong-ST search logs demonstrate QASS outperforms existing models across multiple ranking metrics.

## Method Summary
QASS is a BERT-based ranking model that uses query-oriented data augmentation to generate negative training samples. The method alters the current query through three term-level strategies (masking, replacing, adding) and three query-level strategies (random queries, historical queries, ambiguous queries mined by document position similarity). These augmented pairs are used in a pairwise loss function with different score margins for varying difficulty levels. The model learns to distinguish between positive pairs (original context + clicked document) and negative pairs (altered context + same document), capturing the symmetric nature of relevance in session search.

## Key Results
- QASS outperforms existing models on both AOL and Tiangong-ST search logs
- Achieves 2.94% improvement in NDCG@1 on AOL compared to strong baselines
- Demonstrates consistent improvements across multiple ranking metrics (MAP, MRR, NDCG@1/3/5/10)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Altering the current query during training exposes the model to variations in search intent that naturally occur in real user behavior, leading to better generalization.
- Mechanism: By masking, replacing, or adding terms to the current query, the model learns to associate the same clicked document with subtly or substantially different search intents. This prevents the model from overfitting to exact query-document matches and encourages robust representation learning.
- Core assumption: The current query is the most important signal in the session context for determining relevance, and small changes in the query can meaningfully change the user's search intent.
- Evidence anchors:
  - [abstract] "We develop several strategies to alter the current query, resulting in new training data with varying degrees of difficulty."
  - [section] "We choose to alter the current query to generate new search contexts, thus our approach can be treated as a query-oriented data augmentation method."
  - [corpus] Found 25 related papers, average neighbor FMR=0.437, but none directly address query-level augmentation for session search—weak corpus evidence for this specific mechanism.
- Break condition: If the augmented queries become too dissimilar to the original, the training signal may become noisy or misleading, causing the model to associate incorrect search intents with documents.

### Mechanism 2
- Claim: Generating negative samples at varying difficulty levels stabilizes training and improves the model's ability to distinguish between relevant and irrelevant documents.
- Mechanism: Random queries create easy negatives, historical queries create medium-difficulty negatives (similar context), and ambiguous queries create hard negatives (similar clicked documents). This curriculum-like approach prevents the model from being overwhelmed by uniformly hard negatives early in training.
- Core assumption: The difficulty of a negative sample correlates with the similarity between the original and altered queries; more similar queries require more nuanced understanding to rank correctly.
- Evidence anchors:
  - [abstract] "Different score margins are applied to identify their difficulty and coordinate these augmented pairs."
  - [section] "Based on the similarity between q′c and qc, we can categorize the difficulty of the generated negative pairs into three levels."
  - [corpus] Weak evidence—no corpus neighbors directly discuss difficulty-based negative sampling for session search.
- Break condition: If the difficulty thresholds are set incorrectly, the model may focus too much on easy negatives (wasting capacity) or too much on hard negatives (leading to poor convergence).

### Mechanism 3
- Claim: Using ambiguous queries mined by tracking clicked document positions in other sessions provides informative negatives that are neither too easy nor false negatives.
- Mechanism: By ranking all documents for each query in the log and sampling a window around the clicked document, queries whose clicked documents fall near each other are considered ambiguous. Replacing the current query with these creates negatives that test fine-grained relevance distinctions.
- Core assumption: If a document is highly ranked for two different queries, those queries share a significant portion of search intent, making the document's relevance context-dependent.
- Evidence anchors:
  - [abstract] "Finally, we mine some ambiguous queries of the current query by some heuristics... more informative than other mined queries for learning users' search intents."
  - [section] "If the clicked document dc under the current query qc is ranked around the clicked document d′c under another query q′c... we can treat q′c as an ambiguous query of qc."
  - [corpus] No direct corpus evidence; assumption based on heuristic reasoning in the paper.
- Break condition: If the ranking model used to identify ambiguous queries is inaccurate, the mined negatives may be either too easy or false negatives, harming training.

## Foundational Learning

- Concept: Symmetric relevance in session search
  - Why needed here: The paper argues that existing methods only model relevance from the query side, ignoring that clicked documents also have an ideal context. Understanding this symmetry is key to why query augmentation helps.
  - Quick check question: If a document is clicked after query Q1, why might it also be relevant in a different session context with query Q2?

- Concept: Curriculum learning via difficulty-based negative sampling
  - Why needed here: The model uses varying score margins for negatives of different difficulty to coordinate training. This prevents overwhelming the model with uniformly hard negatives.
  - Quick check question: How does adjusting the margin between positive and negative scores help the model focus on harder examples over time?

- Concept: Contrastive learning in ranking
  - Why needed here: The model learns by pulling positive pairs (original context + clicked doc) closer and pushing negative pairs (altered context + same doc) apart in the representation space.
  - Quick check question: In a pairwise ranking loss, what happens to the model's embeddings if the margin is set too small versus too large?

## Architecture Onboarding

- Component map: Data augmentation module -> BERT encoder -> Scoring layer -> Training loop
- Critical path:
  1. Load session logs → 2. Generate augmented queries → 3. Encode sequences → 4. Compute pairwise loss → 5. Update BERT parameters
- Design tradeoffs:
  - Augmentation breadth vs. noise: More augmentation strategies increase robustness but risk introducing misleading negatives.
  - Difficulty calibration: Tighter margins for hard negatives make training more challenging but can improve discrimination.
  - Computational cost: Generating ambiguous queries requires a dense retriever pass over the entire corpus.
- Failure signatures:
  - Performance plateaus or degrades if augmented queries are too dissimilar to originals.
  - Model overfitting to augmented data if too many negatives are generated.
  - Slow convergence if margins are poorly calibrated (e.g., hard negatives with insufficient margin).
- First 3 experiments:
  1. Ablation: Train without term-level modification to measure its impact on fine-grained relevance learning.
  2. Margin sweep: Vary the score margins for easy/medium/hard negatives to find optimal stability.
  3. Ambiguous query quality: Compare model performance using ambiguous queries vs. random queries to validate their informativeness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the term-level modifications (masking, replacing, adding) specifically impact the model's understanding of query intent compared to more complex augmentation strategies?
- Basis in paper: [explicit] The authors discuss term-level modifications but acknowledge there are more sophisticated strategies to be designed, e.g., masking the word with the highest attention score.
- Why unresolved: The paper only explores basic term-level modifications and does not investigate more advanced strategies that could potentially yield better results.
- What evidence would resolve it: Conducting experiments comparing the current term-level modifications with more advanced strategies like attention-based word masking would provide insights into their relative effectiveness.

### Open Question 2
- Question: Can the proposed query-oriented data augmentation method be effectively applied to ad-hoc queries that lack session history?
- Basis in paper: [explicit] The authors acknowledge that the current implementation only works for queries with session history and suggest designing special augmentation strategies for ad-hoc queries in future work.
- Why unresolved: The paper does not explore how to handle ad-hoc queries, which are a significant portion of real-world search queries.
- What evidence would resolve it: Developing and testing augmentation strategies specifically designed for ad-hoc queries and evaluating their impact on model performance would provide a solution.

### Open Question 3
- Question: How would the performance of QASS change if it were implemented using different base models, such as the encoder of BART or other advanced embedding models like E5 or BGE?
- Basis in paper: [explicit] The authors suggest that QASS can be applied to other base models and mention exploring different models like BART, E5, or BGE in future work.
- Why unresolved: The paper only evaluates QASS using BERT as the base model, limiting the understanding of its performance across different architectures.
- What evidence would resolve it: Conducting experiments using QASS with various base models and comparing their performance would reveal the impact of different architectures on the method's effectiveness.

## Limitations

- The effectiveness of ambiguous query mining relies on heuristic assumptions about document ranking positions that may not generalize across different search domains
- The symmetric relevance argument is conceptually sound but unproven—we don't have evidence that models trained with this approach actually learn better representations of context-dependent relevance
- The paper only evaluates QASS using BERT as the base model, limiting understanding of its performance across different architectures

## Confidence

Our confidence in the proposed mechanisms is **Medium**. The core idea of query-oriented data augmentation is well-supported by the experimental results, showing 2.94% improvement in NDCG@1 on AOL compared to strong baselines. However, the corpus evidence for these specific mechanisms is weak—we found 25 related papers with average neighbor FMR=0.437, but none directly address query-level augmentation for session search.

Major uncertainties include the effectiveness of the ambiguous query mining strategy, which relies on heuristic assumptions about document ranking positions that may not generalize across different search domains. The claim that term-level modifications (masking, replacing, adding) meaningfully change search intent is supported by the experiments but lacks direct validation—we don't know if the augmented queries are semantically distinct enough to provide genuine learning signals.

The symmetric relevance argument is conceptually sound but unproven—we don't have evidence that models trained with this approach actually learn better representations of context-dependent relevance rather than just memorizing augmented patterns.

## Next Checks

1. **Ablation study on query augmentation strategies**: Systematically remove term-level modification and each query-level replacement strategy (random, historical, ambiguous) to quantify their individual contributions to performance gains.

2. **Cross-domain generalization test**: Evaluate QASS on a third search log from a different domain (e.g., academic search or product search) to assess whether the augmentation strategies generalize beyond web search.

3. **Human evaluation of augmented queries**: Have human annotators assess whether the term-level modifications and query replacements genuinely represent different search intents, validating the core assumption that these changes create meaningful negative samples.