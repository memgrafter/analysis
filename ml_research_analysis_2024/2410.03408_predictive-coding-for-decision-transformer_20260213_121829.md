---
ver: rpa2
title: Predictive Coding for Decision Transformer
arxiv_id: '2410.03408'
source_url: https://arxiv.org/abs/2410.03408
tags:
- learning
- policy
- offline
- predictive
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Predictive Coding for Decision Transformer
  (PCDT), a framework that addresses the limitations of Decision Transformer (DT)
  in offline goal-conditioned reinforcement learning, particularly in unstructured
  and suboptimal datasets. PCDT introduces predictive codings as conditioning variables
  instead of returns, enabling the agent to reason about future behaviors and improve
  generalization.
---

# Predictive Coding for Decision Transformer

## Quick Facts
- arXiv ID: 2410.03408
- Source URL: https://arxiv.org/abs/2410.03408
- Authors: Tung M. Luu; Donghoon Lee; Chang D. Yoo
- Reference count: 40
- Primary result: PCDT achieves 74.9 average normalized score, outperforming DT by 3.2x in long-horizon tasks requiring stitching capabilities

## Executive Summary
This paper introduces Predictive Coding for Decision Transformer (PCDT), a framework that addresses limitations of Decision Transformer in offline goal-conditioned reinforcement learning with unstructured and suboptimal datasets. PCDT replaces return-to-go conditioning with predictive codings learned through masked autoencoding, enabling agents to reason about future behaviors and improve generalization. The two-stage approach first learns predictive codings that capture trajectory history and future goals, then trains a policy network conditioned on these codings. Experimental results on eight datasets from AntMaze and FrankaKitchen environments demonstrate PCDT achieves performance on par with or surpassing existing value-based and transformer-based methods.

## Method Summary
PCDT employs a two-stage training approach for offline goal-conditioned reinforcement learning. In stage one, a trajectory autoencoder learns predictive codings through masked autoencoding, reconstructing input states and predicting future states up to horizon L. The encoder f_E takes past states and target goals, applies bidirectional masking, and produces latent codes that capture both trajectory context and desired goals. In stage two, a causal transformer policy π_θ is trained to predict actions conditioned on these pre-learned predictive codings. During inference, only the trajectory encoder is used to generate predictive codings from observed states and goals, which then guide action generation. This approach enables effective decision-making in sparse-reward environments and improves stitching capabilities in unstructured datasets.

## Key Results
- PCDT achieves 74.9 average normalized score across eight datasets from AntMaze and FrankaKitchen environments
- Outperforms DT by a factor of 3.2 in average performance for long-horizon tasks requiring stitching capabilities
- Shows significant improvements in real-world applications, validated on a physical 7-DOF Sawyer robot for goal-reaching tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Predictive codings encode both trajectory history and future goals in a single latent representation.
- Mechanism: The trajectory encoder f_E takes a sequence of past states plus a target goal, applies bidirectional masking, and produces latent codes z that summarize the entire sub-trajectory and desired goal. These z values then condition the policy π_θ to predict the next action.
- Core assumption: The bidirectional masked autoencoding training objective forces the encoder to learn a compressed representation that captures both past context and future goal information.
- Evidence anchors:
  - [abstract] "PCDT utilizes an architecture that extends the DT framework, conditioned on predictive codings, enabling decision-making based on both past and future factors"
  - [section IV-A] "These latent variables zi are expected to capture not only the historical context and task information (i.e., the desired goal) but also to be predictive, enabling reasoning over future states toward the desired goal"
  - [corpus] Weak - related works mention transformers but not predictive coding mechanisms specifically
- Break condition: If the bidirectional transformer fails to learn useful compressed representations, the predictive codings will not provide meaningful guidance to the policy.

### Mechanism 2
- Claim: Predictive codings enable stitching capability in unstructured and suboptimal datasets.
- Mechanism: By conditioning on latent codes that encode future state information, the policy can learn to combine sub-trajectories from different demonstrations effectively. The predictive codings provide guidance toward the goal even when the dataset lacks complete optimal trajectories.
- Core assumption: The trajectory encoder learns to predict future states, which provides the policy with information about how to progress toward the goal even in sparse-reward environments.
- Evidence anchors:
  - [abstract] "PCDT shows significant improvements in long-horizon tasks requiring stitching capabilities, outperforming DT by a factor of 3.2 in average performance"
  - [section IV-B] "To encourage predictive coding, in addition to reconstructing input states, future state prediction is concurrently performed from the latent codes"
  - [corpus] Weak - while related works mention stitching, they don't provide specific evidence about predictive coding's role
- Break condition: If the dataset lacks sufficient diversity or the future prediction component is ineffective, the stitching capability will degrade.

### Mechanism 3
- Claim: The two-stage training approach (autoencoder then policy) improves learning stability and effectiveness.
- Mechanism: First, the trajectory autoencoder learns to compress trajectories into predictive codings without requiring rewards or actions. Then, the policy network is trained to predict actions conditioned on these pre-learned codings. This separation prevents interference between representation learning and policy optimization.
- Core assumption: Learning the predictive codings first allows the policy to focus solely on action prediction without the added complexity of representation learning.
- Evidence anchors:
  - [section IV-B] "During policy learning and inference, only the trajectory encoder is used, applied to the unmasked observed states and the target goal to acquire predictive codings"
  - [section IV-A] "At training time, return-to-go sequences are computed from offline trajectories. During inference, this quantity guides the policy in generating actions to achieve the desired outcome"
  - [corpus] Weak - related works mention training approaches but not this specific two-stage separation
- Break condition: If the predictive codings learned in the first stage become stale or irrelevant to the current policy learning task, the separation becomes detrimental.

## Foundational Learning

- Concept: Masked autoencoding for representation learning
  - Why needed here: PCDT uses masked autoencoding to learn compressed trajectory representations without requiring rewards or actions, enabling training on larger, unlabeled datasets
  - Quick check question: How does random masking during training improve generalization of the trajectory encoder?

- Concept: Goal-conditioned reinforcement learning
  - Why needed here: The framework operates in goal-conditioned RL settings where the agent must reach arbitrary target states, making it suitable for multi-task learning
  - Quick check question: What is the difference between return-to-go conditioning and goal conditioning in transformer-based RL?

- Concept: Sequence modeling with transformers
  - Why needed here: PCDT leverages GPT-based transformers to model sequences of states, actions, and predictive codings as a supervised learning problem
  - Quick check question: Why does PCDT use sinusoidal positional encoding instead of timestep encoding?

## Architecture Onboarding

- Component map:
  Trajectory data -> Trajectory Autoencoder (Bidirectional transformer) -> Predictive codings z
  Predictive codings z + States + Actions -> Policy Network (Causal transformer) -> Next action

- Critical path:
  1. Sample trajectory segment and goal from dataset
  2. Generate predictive codings using trajectory encoder
  3. Feed predictive codings, states, and actions into policy transformer
  4. Predict next action and compute loss
  5. Update policy parameters only (encoder frozen during policy training)

- Design tradeoffs:
  - Two-stage training vs. end-to-end training: Separation provides stability but may miss fine-grained interactions
  - Predictive coding vs. return conditioning: Predictive codings work better in sparse-reward settings but require more complex representation learning
  - Future prediction length L: Longer horizons provide more guidance but increase computational cost and training difficulty

- Failure signatures:
  - Poor performance in stitching tasks: Indicates predictive codings aren't capturing sufficient future information
  - Sensitivity to hyperparameter choices: Suggests the two-stage training isn't properly decoupling representation and policy learning
  - Degradation in long-horizon tasks: May indicate the causal transformer can't effectively use the predictive codings

- First 3 experiments:
  1. Verify the trajectory autoencoder can reconstruct input states and predict future states with varying L values
  2. Test policy performance with and without predictive codings to confirm they provide useful guidance
  3. Evaluate stitching capability by measuring performance on datasets requiring sub-trajectory combination

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PCDT's performance scale with dataset size and diversity, particularly when using large amounts of reward-free data?
- Basis in paper: [explicit] The paper mentions that PCDT can leverage large amounts of action-free data (Section IV-B) and that predictive coding learning only requires states from the dataset, enabling the use of potentially large amounts of action-free data.
- Why unresolved: The paper does not provide experimental results demonstrating PCDT's performance with varying dataset sizes or using large amounts of reward-free data.
- What evidence would resolve it: Experiments showing PCDT's performance across different dataset sizes and compositions, including datasets with and without action information, would clarify the method's scalability and data efficiency.

### Open Question 2
- Question: What is the optimal length of future states (L) for different types of goal-conditioned tasks, and how does it affect the trade-off between prediction accuracy and computational efficiency?
- Basis in paper: [explicit] The paper investigates the effect of different lengths of future states (L) on agent performance (Section V-C) but does not provide a comprehensive analysis across different task types or examine computational efficiency.
- Why unresolved: While the paper shows that future prediction improves performance, it does not establish guidelines for selecting L based on task characteristics or analyze the computational trade-offs involved.
- What evidence would resolve it: A systematic study across various task types with different horizon lengths, along with computational complexity analysis, would help determine optimal L values and understand efficiency trade-offs.

### Open Question 3
- Question: How does PCDT perform in continuous goal spaces compared to discrete goal spaces, and what modifications might be needed for high-dimensional goal spaces?
- Basis in paper: [inferred] The paper evaluates PCDT on tasks with relatively low-dimensional state spaces (AntMaze, FrankaKitchen, and Sawyer robot) but does not explicitly test performance in high-dimensional or continuous goal spaces.
- Why unresolved: The paper does not address how PCDT would handle tasks with continuous or high-dimensional goal spaces, which are common in many real-world applications.
- What evidence would resolve it: Experiments testing PCDT on tasks with varying goal space dimensions and continuity properties, along with modifications to the predictive coding architecture for high-dimensional spaces, would clarify its applicability to different goal space types.

## Limitations
- The framework's performance depends heavily on the quality and diversity of the offline dataset, with limited ability to recover from highly suboptimal data
- The computational overhead of the two-stage training process may limit real-time applicability in resource-constrained settings
- The effectiveness of predictive codings in environments with complex, non-linear dynamics remains untested

## Confidence

**Confidence Labels:**
- High confidence: The two-stage training approach and overall performance improvements in experimental results
- Medium confidence: The effectiveness of predictive codings in enabling stitching capabilities across unstructured datasets
- Low confidence: The framework's scalability to environments with significantly different dynamics or higher-dimensional state spaces

## Next Checks

1. Test PCDT's performance when trained on progressively sparser datasets to identify the minimum dataset quality threshold for effective learning
2. Evaluate the framework's ability to generalize to environments with different reward structures or control frequencies
3. Conduct ablation studies on the future prediction length L to determine optimal trade-offs between guidance quality and computational cost