---
ver: rpa2
title: 'Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial Applications'
arxiv_id: '2410.21943'
source_url: https://arxiv.org/abs/2410.21943
tags:
- multimodal
- image
- text
- gpt-4v
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates multimodal RAG pipelines for industrial applications
  using both text and image data. The authors created a custom dataset of 100 question-answer
  pairs from industrial PDFs and compared single-modality (text-only, image-only)
  with multimodal approaches using two image processing strategies (multimodal embeddings
  vs.
---

# Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial Applications

## Quick Facts
- arXiv ID: 2410.21943
- Source URL: https://arxiv.org/abs/2410.21943
- Authors: Monica Riedler; Stefan Langer
- Reference count: 28
- Primary result: Multimodal RAG outperforms single-modality approaches in industrial applications, with image summaries slightly better than embeddings

## Executive Summary
This paper evaluates multimodal RAG pipelines for industrial applications using both text and image data from technical documents. The authors created a custom dataset of 100 question-answer pairs from industrial PDFs and compared single-modality (text-only, image-only) with multimodal approaches using two image processing strategies (multimodal embeddings vs. image summaries) and two models (GPT-4V, LLaVA). Results show multimodal RAG significantly outperforms single-modality approaches, with image summaries proving more effective than embeddings for retrieval. GPT-4V consistently outperforms LLaVA in accuracy, and using multiple images generally improves performance except for image faithfulness.

## Method Summary
The study extracts text chunks and images from 20 industrial PDF documents, creates a vector database using ChromaDB, and implements two image retrieval methods (CLIP embeddings and image summaries via multimodal LLM) alongside text retrieval using text-embedding-3-small. Experiments compare single-modality and multimodal RAG approaches with GPT-4V and LLaVA models, evaluating performance using LLM-as-a-Judge with metrics including Answer Correctness, Faithfulness, Relevancy, and Context Relevancy.

## Key Results
- Multimodal RAG outperforms single-modality RAG across all metrics
- Image summaries outperform multimodal embeddings for retrieval
- GPT-4V consistently outperforms LLaVA in accuracy
- Using multiple images improves performance across all metrics except Image Faithfulness
- Image retrieval remains more challenging than text retrieval

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal RAG outperforms single-modality RAG because combining text and image context provides complementary information
- Core assumption: Questions genuinely require both text and image information to answer correctly
- Evidence anchors: Abstract states "multimodal RAG can outperform single-modality RAG", section shows "Using both text and images yields significantly higher Answer Correctness scores"
- Break condition: If questions can be answered from text alone

### Mechanism 2
- Claim: Image summaries outperform multimodal embeddings because text embeddings capture semantic similarity better
- Core assumption: Text embedding models are better at capturing semantic relationships in industrial technical content
- Evidence anchors: Abstract states "leveraging textual summaries from images presents a more promising approach", section notes "image summaries approach appears to be more promising"
- Break condition: If multimodal embedding models improve significantly

### Mechanism 3
- Claim: GPT-4V outperforms LLaVA because it can process multiple images in a single prompt
- Core assumption: Relevant visual information is distributed across multiple images
- Evidence anchors: Abstract states "GPT-4V consistently outperforms LLaVA", section shows "employing multiple images in prompts generally improves performance"
- Break condition: If questions consistently require information from only one image

## Foundational Learning

- **Vector embeddings and similarity search**: RAG systems rely on embedding documents and queries in shared vector space to find relevant context. Quick check: What happens if you use cosine similarity vs L2 distance for your vector search?

- **Multimodal vs unimodal model architectures**: Understanding how models process different input types is crucial for designing effective multimodal RAG pipelines. Quick check: How does a multimodal model differ architecturally from a text-only model?

- **Evaluation metrics for RAG systems**: The study uses multiple metrics (Answer Correctness, Faithfulness, Relevancy) to comprehensively evaluate system performance. Quick check: Why might a system have high Answer Relevancy but low Text Faithfulness?

## Architecture Onboarding

- **Component map**: PDF extraction → text chunks + images → Embedding generation (text vs multimodal) → Vector database storage → Retrieval component (similarity search) → Context concatenation (query + retrieved context) → Answer generation (multimodal LLM) → Evaluation (LLM-as-a-Judge)

- **Critical path**: PDF extraction → Embedding generation → Vector database → Retrieval → Answer generation → Evaluation

- **Design tradeoffs**: Single vector store vs separate stores for text and images, CLIP embeddings vs text summaries for image processing, multiple images vs single image processing, GPT-4V vs LLaVA for answer generation

- **Failure signatures**: Low text context relevancy → retrieval is finding irrelevant documents, low image context relevancy → image processing/retrieval is failing, low faithfulness scores → model is hallucinating beyond retrieved context, large gap between gold context and RAG → retrieval quality needs improvement

- **First 3 experiments**: 
  1. Run baseline (no retrieval) to establish LLM's internal knowledge limits
  2. Test text-only RAG to verify retrieval works for textual content
  3. Test image-only RAG with both embedding strategies to compare image processing approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does multimodal RAG consistently outperform text-only RAG across different industrial domains and document types?
- Basis: Paper states "multimodal RAG can outperform single-modality RAG" but based on single industrial dataset
- Why unresolved: Experiments conducted on single dataset from industrial domain
- Evidence needed: Testing multimodal RAG on multiple industrial datasets from different domains and document types

### Open Question 2
- Question: What is the optimal number of images to include in multimodal RAG prompts?
- Basis: Paper notes "employing multiple images in prompts generally improves performance" but doesn't determine optimal number
- Why unresolved: Only tested single image vs multiple images without exploring intermediate values
- Evidence needed: Systematic testing with varying numbers of images to identify point of diminishing returns

### Open Question 3
- Question: How can image retrieval quality be improved to match text retrieval performance?
- Basis: Paper states "image retrieval poses a greater challenge than text retrieval"
- Why unresolved: Identifies weakness but doesn't propose specific solutions
- Evidence needed: Comparative analysis of different image retrieval methods and development of novel techniques

### Open Question 4
- Question: How do performance differences between GPT-4V and LLaVA translate to real-world applications?
- Basis: Paper notes "GPT-4V consistently outperforms LLaVA" but doesn't discuss practical implementation
- Why unresolved: Focuses solely on accuracy metrics without considering cost, speed, or deployment factors
- Evidence needed: Comprehensive evaluation including cost per query, response time, hardware requirements, and licensing terms

## Limitations

- Findings based on single custom dataset of 100 question-answer pairs from 20 industrial PDFs
- Image retrieval shows significantly lower performance than text retrieval
- LLM-as-a-Judge evaluation may introduce self-enhancement bias
- Image summary approach relies on quality of automatic image descriptions

## Confidence

**High Confidence**: Multimodal RAG outperforming single-modality approaches, GPT-4V vs LLaVA performance differences
**Medium Confidence**: Image summaries outperforming multimodal embeddings for retrieval
**Low Confidence**: Image faithfulness not improving with multiple images

## Next Checks

1. Conduct human evaluation validation of a subset of questions to validate LLM-as-a-Judge scores, particularly for image faithfulness and context relevancy metrics
2. Test multimodal RAG pipeline on different industrial documents (e.g., maintenance manuals from different industries) to assess cross-domain generalization
3. Perform detailed analysis of retrieval failures by examining specific cases where relevant context was not retrieved to identify root causes