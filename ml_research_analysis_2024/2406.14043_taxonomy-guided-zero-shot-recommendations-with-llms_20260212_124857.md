---
ver: rpa2
title: Taxonomy-Guided Zero-Shot Recommendations with LLMs
arxiv_id: '2406.14043'
source_url: https://arxiv.org/abs/2406.14043
tags:
- item
- recommendation
- llms
- taxonomy
- taxrec
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of deploying large language
  models (LLMs) for recommender systems, specifically tackling issues of limited prompt
  length, unstructured item information, and unconstrained generation. The proposed
  Taxonomy-guided Recommendation (TaxRec) framework introduces a two-step process:
  first, generating a taxonomy dictionary to systematically categorize and structure
  item information; second, using this structured taxonomy within LLM prompts to achieve
  controlled feature generation and efficient token utilization.'
---

# Taxonomy-Guided Zero-Shot Recommendations with LLMs

## Quick Facts
- arXiv ID: 2406.14043
- Source URL: https://arxiv.org/abs/2406.14043
- Reference count: 12
- Primary result: Taxonomy-guided framework achieves up to 0.300 Recall@10 on movie recommendations using GPT-4 without fine-tuning

## Executive Summary
This paper addresses the challenge of deploying large language models (LLMs) for recommender systems, specifically tackling issues of limited prompt length, unstructured item information, and unconstrained generation. The proposed Taxonomy-guided Recommendation (TaxRec) framework introduces a two-step process: first, generating a taxonomy dictionary to systematically categorize and structure item information; second, using this structured taxonomy within LLM prompts to achieve controlled feature generation and efficient token utilization. This approach enables zero-shot recommendations without requiring domain-specific fine-tuning. Experimental results on movie and book recommendation datasets demonstrate significant improvements over traditional zero-shot methods.

## Method Summary
TaxRec operates through a two-phase framework: (1) taxonomy categorization, where an LLM generates a domain-specific taxonomy dictionary and categorizes items based on their titles and descriptions, and (2) LLM-based recommendation, where the system processes user interaction history, generates feature-based recommendations using the taxonomy, and ranks items by matching recommended features against categorized item features. The approach leverages structured taxonomy to compress item information into feature sets, enabling efficient token utilization within LLM context limits while maintaining recommendation quality through controlled generation and reliable parsing of structured output.

## Key Results
- Achieved up to 0.300 Recall@10 on movie dataset and 0.265 Recall@10 on book dataset using GPT-4
- Outperformed traditional zero-shot methods significantly across all metrics (Recall@1, Recall@5, Recall@10, NDCG@1, NDCG@5, NDCG@10)
- Demonstrated token efficiency through structured taxonomy representation compared to full item descriptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The taxonomy dictionary reduces prompt length by replacing full item descriptions with condensed feature sets
- Mechanism: Instead of feeding millions of tokens representing full item descriptions into LLM prompts, the taxonomy dictionary provides a structured categorization system where items are represented by their feature attributes (genre, theme, language, etc.). This compression enables fitting candidate information within LLM context limits
- Core assumption: A well-designed taxonomy can capture sufficient item characteristics for meaningful recommendations while being more token-efficient than full descriptions
- Evidence anchors:
  - [abstract] "incorporating the taxonomy dictionary into LLM prompts, we achieve efficient token utilization and controlled feature generation"
  - [section] "The taxonomy dictionary is a condensed categorization of the whole item pool. Compared with adding all candidate items, adding the dictionary can greatly save the tokens"
  - [corpus] Weak evidence - related papers focus on LLM-based recommendations but don't address token efficiency through taxonomy
- Break condition: If the taxonomy fails to capture distinguishing features between items, token savings become meaningless as recommendations degrade

### Mechanism 2
- Claim: Structured taxonomy enables controlled generation and parsing of recommendations
- Mechanism: By constraining LLM output to follow taxonomy format (feature-value pairs), the system can reliably parse recommendations and match them against categorized items using simple matching algorithms rather than complex semantic similarity calculations
- Core assumption: LLMs can consistently generate structured output when explicitly prompted with format constraints
- Evidence anchors:
  - [section] "s is further parsed as the feature list F = [f1, f2, ..., f|F|] representing recommended features. Then the ranking score of each item i is calculated as: Score i = |iC ∩ F|"
  - [section] "PRecommendation also regularizes LLM's generation format as a list of features based on T"
  - [corpus] Weak evidence - related work focuses on LLM recommendations but doesn't address structured output parsing
- Break condition: If LLM-generated output deviates from expected format, parsing fails and the matching mechanism breaks

### Mechanism 3
- Claim: Taxonomy retrieval activates domain-specific knowledge within LLM parameters
- Mechanism: The taxonomy generation prompt explicitly retrieves domain knowledge (books, movies) from LLM parameters, creating a knowledge-rich representation that helps LLM understand item contexts better than raw titles alone
- Core assumption: LLMs contain sufficient domain knowledge within parameters to generate meaningful taxonomies without external data
- Evidence anchors:
  - [section] "we first retrieve an LLM to obtain a taxonomy dictionary that contains the categorization knowledge of a domain"
  - [section] "The categorized item pool IC is obtained by categorizing items in I with Equation 3"
  - [section] "Compared with the original vague book title, the enriched texts provide more detailed information to assist LLMs inference user's interests"
  - [corpus] Weak evidence - related papers explore LLM knowledge but don't specifically examine taxonomy-based knowledge retrieval
- Break condition: If LLM lacks domain knowledge, generated taxonomy will be incomplete or incorrect, reducing recommendation quality

## Foundational Learning

- Concept: Zero-shot learning in recommender systems
  - Why needed here: TAXREC operates without training on historical user-item interactions, relying entirely on LLM knowledge and item metadata
  - Quick check question: What distinguishes zero-shot from few-shot or fine-tuned approaches in recommender systems?

- Concept: Taxonomy construction and categorization
  - Why needed here: The system depends on generating meaningful category hierarchies that capture essential item attributes for recommendation
  - Quick check question: How does taxonomy granularity affect both token efficiency and recommendation accuracy?

- Concept: Prompt engineering for structured output
  - Why needed here: TAXREC requires LLMs to generate recommendations in specific feature-based formats rather than free-form text
  - Quick check question: What prompt techniques ensure consistent structured output from LLMs across different domains?

## Architecture Onboarding

- Component map:
  - Taxonomy Generation (one-time) -> Item Categorization (one-time) -> User Interaction Processing -> Recommendation Generation -> Feature Matching -> Ranking
- Critical path: Taxonomy Generation → Item Categorization → Recommendation Generation → Feature Matching → Ranking
- Design tradeoffs:
  - Taxonomy comprehensiveness vs. prompt length constraints
  - Feature granularity vs. matching accuracy
  - Prompt specificity vs. LLM generation flexibility
  - One-time cost vs. recommendation quality
- Failure signatures:
  - Low R@1/R@5/R@10 scores: taxonomy missing key attributes or LLM generating poor feature sets
  - Inconsistent performance across domains: LLM domain knowledge gaps
  - High variance between runs: unstable LLM generation despite structured prompts
  - Memory errors during processing: taxonomy too large for available context
- First 3 experiments:
  1. Test taxonomy generation quality by examining sample taxonomies across different domains and checking for completeness and logical structure
  2. Validate feature matching by manually inspecting recommended features against item features to ensure meaningful overlap calculations
  3. Benchmark token usage by comparing prompt lengths with and without taxonomy to quantify efficiency gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of TAXREC scale when applied to item pools that are orders of magnitude larger than those tested (e.g., millions of items)?
- Basis in paper: [inferred] The paper notes that the taxonomy dictionary approach helps alleviate the limited prompt length challenge, but doesn't explicitly test TAXREC on very large item pools.
- Why unresolved: The experiments were conducted on datasets with item pools of 1,682 (Movie) and 4,389 (Book), which are significantly smaller than typical real-world recommender systems.
- What evidence would resolve it: Experiments applying TAXREC to datasets with millions of items, measuring recall@N and NDCG@N at various taxonomy sizes, and comparing token usage and latency against baselines.

### Open Question 2
- Question: How robust is TAXREC when the taxonomy generation prompt produces incomplete or inaccurate domain knowledge?
- Basis in paper: [explicit] The paper mentions "the taxonomy generated via LLM prompts may lack completeness and scientific rigor" as a limitation.
- Why unresolved: The paper doesn't investigate how TAXREC performs when the taxonomy generation step produces imperfect taxonomies, or when domain knowledge is insufficient.
- What evidence would resolve it: Experiments where taxonomies are deliberately generated with missing categories or incorrect classifications, measuring performance degradation and identifying thresholds where TAXREC becomes ineffective.

### Open Question 3
- Question: Can TAXREC be effectively adapted to multi-modal item representations (images, videos) beyond just textual descriptions?
- Basis in paper: [inferred] The current TAXREC framework only handles textual item titles and descriptions, and the paper doesn't explore how it might handle multi-modal content.
- Why unresolved: Modern recommender systems increasingly incorporate visual and audio content, but TAXREC's taxonomy generation and categorization steps are text-only.
- What evidence would resolve it: Experiments integrating image/video embeddings into the taxonomy framework, developing cross-modal taxonomy generation prompts, and measuring performance improvements on multi-modal datasets.

## Limitations
- Limited generalization to domains beyond movies and books due to lack of cross-domain validation
- Dependence on LLM's inherent domain knowledge quality for taxonomy generation
- No quantitative comparison of token usage reduction provided

## Confidence

**High Confidence**: The core mechanism of using structured taxonomies to enable controlled LLM generation and efficient parsing shows theoretical soundness. The two-step framework architecture is clearly defined and the feature matching approach is straightforward and verifiable.

**Medium Confidence**: The experimental results showing improvements over baseline zero-shot methods are promising but limited to two datasets. The magnitude of improvements (e.g., 0.300 Recall@10 for movies) suggests the approach works well in tested conditions, but generalization claims require additional validation.

**Low Confidence**: Claims about the taxonomy's role in activating domain-specific LLM knowledge are based on qualitative observations rather than systematic evaluation. The paper doesn't measure what portion of recommendation quality comes from taxonomy structure versus LLM's inherent knowledge.

## Next Checks

1. **Cross-Domain Generalization Test**: Apply TAXREC to a third domain (e.g., music or product recommendations) and measure performance degradation. Compare taxonomy quality and recommendation accuracy against the movie and book domains to identify domain-specific limitations.

2. **Ablation Study on Taxonomy Structure**: Systematically vary taxonomy granularity (number of categories, depth of hierarchy) and measure the impact on token efficiency and recommendation quality. Identify optimal taxonomy characteristics for different recommendation scenarios.

3. **Token Usage Benchmarking**: Implement the full TAXREC pipeline and measure exact token counts for prompts with and without taxonomy across multiple recommendation rounds. Calculate percentage reduction in token usage and correlate with any performance trade-offs.