---
ver: rpa2
title: Deep Learning of Dynamic Systems using System Identification Toolbox(TM)
arxiv_id: '2409.07642'
source_url: https://arxiv.org/abs/2409.07642
tags:
- data
- system
- neural
- toolbox
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The System Identification Toolbox has been enhanced to integrate
  deep learning techniques for dynamic system modeling, enabling the use of neural
  networks as building blocks for nonlinear models. Key innovations include neural
  state-space models with auto-encoding for reduced-order modeling, support for neural
  networks in Nonlinear ARX and Hammerstein-Wiener structures, and feature selection
  capabilities using sparsity-inducing methods.
---

# Deep Learning of Dynamic Systems using System Identification Toolbox(TM)

## Quick Facts
- arXiv ID: 2409.07642
- Source URL: https://arxiv.org/abs/2409.07642
- Authors: Tianyu Dai; Khaled Aljanaideh; Rong Chen; Rajiv Singh; Alec Stothert; Lennart Ljung
- Reference count: 6
- Primary result: System Identification Toolbox enhanced with deep learning integration for nonlinear dynamic system modeling

## Executive Summary
This paper presents significant enhancements to MATLAB's System Identification Toolbox, enabling seamless integration of deep learning techniques for dynamic system modeling. The toolbox now supports neural networks as building blocks for nonlinear models including state-space, ARX, and Hammerstein-Wiener structures. Key innovations include auto-encoding neural state-space models for reduced-order modeling and feature selection capabilities using sparsity-inducing methods. The framework leverages auto-differentiation for efficient state estimation and gradient computation.

## Method Summary
The toolbox implements neural state-space models where neural networks parameterize state and output equations, supporting auto-encoding for dimensionality reduction by compressing states into latent spaces. Nonlinear ARX and Hammerstein-Wiener models can now use deep neural networks instead of traditional basis functions. The framework supports direct use of MATLAB data types and leverages auto-differentiation for Jacobian computation in Extended Kalman Filters. Training uses gradient-based optimization with solvers like ADAM and LBFGS, while feature selection employs proximal gradient algorithms with sparsity penalties.

## Key Results
- Neural state-space models achieve 93.77% fit for reduced-order modeling of two-tank systems
- Engine torque dynamics modeling achieves 87.52% model fit
- Robot arm dynamics modeling achieves 79.61% fit with Hammerstein-Wiener neural structures
- Auto-encoding enables effective dimensionality reduction while preserving system dynamics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Auto-encoding neural state-space models reduce model order while preserving dynamics.
- Mechanism: Auto-encoders compress high-dimensional regressor states into a lower-dimensional latent space that captures essential dynamics, then decode back for prediction.
- Core assumption: The latent space dimension can be chosen smaller than the original regressor set while still representing the system's dynamic behavior.
- Evidence anchors:
  - [abstract] "neural state-space models which can be extended with auto-encoding features that are particularly suited for reduced-order modeling of large systems"
  - [section] "we enable the use of an auto-encoder by setting the value of the LatentDim property to a finite number; this value indicates the dimension of the latent space"
  - [corpus] Weak - corpus contains related work on reduced-order modeling but no direct auto-encoder validation studies
- Break condition: If the latent dimension is too small to capture essential dynamics, the model will fail to represent system behavior accurately.

### Mechanism 2
- Claim: Neural networks can effectively replace traditional nonlinear function blocks in ARX and Hammerstein-Wiener models.
- Mechanism: Deep networks learn complex nonlinear mappings between regressors and outputs that traditional basis functions cannot capture, providing better generalization.
- Core assumption: The neural network architecture chosen is sufficient to represent the underlying system nonlinearity.
- Evidence anchors:
  - [abstract] "use of deep neural networks as building blocks of nonlinear models"
  - [section] "The most recent update allows the use of neural networks to represent the nonlinear input-to-output, or regressor-to-output mappings"
  - [corpus] Moderate - corpus shows several papers using neural networks for nonlinear system identification
- Break condition: If the network architecture is too simple or training data insufficient, the model will underfit and perform poorly.

### Mechanism 3
- Claim: Feature selection through sparsity-inducing methods improves model generalizability by reducing overfitting.
- Mechanism: Proximal gradient algorithms with sparsity penalties (L1, L0, log-sum) automatically identify and remove irrelevant regressors from the model.
- Core assumption: Not all candidate regressors contribute meaningfully to the model output, and sparsity methods can identify the relevant subset.
- Evidence anchors:
  - [abstract] "enable a direct use of raw numeric matrices and timetables for training models"
  - [section] "perform an estimation that utilizes sparsification options in order to automatically reduce the number of active model regressors"
  - [corpus] Weak - corpus contains sparse estimation methods but no direct validation for dynamic system identification
- Break condition: If sparsity penalty is too strong, the algorithm may remove important features, degrading model performance.

## Foundational Learning

- Concept: State-space representation of dynamic systems
  - Why needed here: Neural state-space models directly parameterize the state equations using neural networks, requiring understanding of how states evolve
  - Quick check question: What are the three fundamental equations that define a continuous-time state-space system?

- Concept: Auto-differentiation and gradient-based optimization
  - Why needed here: Training neural state-space models requires computing gradients of complex nested functions through the network and ODE solver
  - Quick check question: How does auto-differentiation differ from numerical differentiation in computing gradients for neural ODEs?

- Concept: Time-series data preprocessing and segmentation
  - Why needed here: The examples show splitting data into experiments to improve training efficiency and generalizability
  - Quick check question: What are the trade-offs between using longer vs shorter time segments for training neural dynamic models?

## Architecture Onboarding

- Component map: idNeuralStateSpace object → StateNetwork/OutputNetwork (dlnetwork) → Auto-encoder networks → Training options → nlssest()
- Critical path: Data → Model structure creation → Network initialization → Training options setup → nlssest() → Validation
- Design tradeoffs:
  - Model complexity vs training data requirements
  - Latent dimension choice vs model fidelity in reduced-order models
  - Solver choice (ADAM vs LBFGS) vs convergence speed
  - Experiment segmentation vs prediction horizon
- Failure signatures:
  - Poor fit to validation data indicates overfitting or insufficient model capacity
  - Training divergence suggests learning rate too high or poor initialization
  - Auto-encoder reconstruction error indicates latent dimension too small
- First 3 experiments:
  1. Train a basic neural state-space model on the two-tank system with known order to establish baseline performance
  2. Apply auto-encoding with varying latent dimensions to find the minimum dimension that maintains acceptable fit
  3. Compare neural ARX vs Hammerstein-Wiener structures on the robot arm dataset to understand structural differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical guarantees for generalizability and stability when using neural state-space models with auto-encoders for reduced-order modeling of large-scale systems?
- Basis in paper: [inferred] The paper discusses auto-encoders for reducing latent state dimensions but does not provide theoretical analysis of stability or generalization bounds.
- Why unresolved: The focus is on practical implementation and performance examples, with no mention of formal stability or generalization guarantees.
- What evidence would resolve it: Theoretical proofs of stability and generalization bounds for auto-encoder-based neural state-space models, validated through systematic experiments.

### Open Question 2
- Question: How does the choice of sparsity-inducing method (ℓ1, ℓ0, log-sum) affect the interpretability and accuracy of nonlinear ARX models in dynamic systems?
- Basis in paper: [explicit] The paper mentions three sparsity methods but does not compare their impact on interpretability and model accuracy.
- Why unresolved: The feature selection example demonstrates effectiveness but lacks comparative analysis of different sparsity measures.
- What evidence would resolve it: Empirical studies comparing model accuracy, interpretability, and computational efficiency across different sparsity-inducing methods for various dynamic systems.

### Open Question 3
- Question: What are the limitations of using auto-differentiation for Jacobian computation in Extended Kalman Filters for highly nonlinear systems?
- Basis in paper: [explicit] The paper states that auto-differentiation is now supported but does not discuss its limitations for highly nonlinear systems.
- Why unresolved: While auto-differentiation simplifies implementation, the paper does not address potential numerical issues or performance degradation in highly nonlinear scenarios.
- What evidence would resolve it: Comparative studies of auto-differentiation vs analytical Jacobians in Extended Kalman Filters for systems with varying degrees of nonlinearity, including convergence and accuracy analysis.

## Limitations
- Limited experimental validation for auto-encoding mechanism with only one example provided
- No comparative analysis against established system identification methods (NARMAX, subspace identification)
- Feature selection sparsity methods mentioned but not validated with concrete examples
- Computational efficiency compared to traditional approaches not discussed

## Confidence

- **High confidence**: Neural network integration as function approximators in ARX/Hammerstein-Wiener structures (well-established technique with multiple examples)
- **Medium confidence**: Neural state-space models with auto-differentiation (supported by examples but limited comparative analysis)
- **Low confidence**: Auto-encoding for reduced-order modeling and sparsity-based feature selection (minimal validation and no baseline comparisons)

## Next Checks

1. Compare the neural state-space model performance against traditional subspace identification methods (N4SID, MOESP) on the same datasets using identical validation metrics
2. Perform an ablation study varying the latent dimension in auto-encoding to quantify the trade-off between model order reduction and performance degradation
3. Evaluate the feature selection sparsity methods by measuring regressor count reduction and corresponding model performance changes across multiple datasets with known relevant features