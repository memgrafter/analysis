---
ver: rpa2
title: An Analysis of User Behaviors for Objectively Evaluating Spoken Dialogue Systems
arxiv_id: '2401.04867'
source_url: https://arxiv.org/abs/2401.04867
tags:
- dialogue
- evaluation
- behaviors
- user
- subjective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study proposes an objective evaluation method for spoken
  dialogue systems by analyzing users'' behaviors. The core idea is to correlate user
  behaviors during dialogue (e.g., utterance time, disfluencies, turn-taking) with
  subjective evaluation scores across three social dialogue tasks: attentive listening,
  job interview, and first-meeting conversation.'
---

# An Analysis of User Behaviors for Objectively Evaluating Spoken Dialogue Systems

## Quick Facts
- arXiv ID: 2401.04867
- Source URL: https://arxiv.org/abs/2401.04867
- Reference count: 27
- Core result: Achieved mean absolute error below evaluation score granularity (0.970, 0.953, and 0.683) for three social dialogue tasks

## Executive Summary
This study proposes an objective evaluation method for spoken dialogue systems by analyzing users' behaviors during dialogue. The core insight is that observable user behaviors—such as utterance time, disfluencies, and turn-taking patterns—can be correlated with subjective evaluation scores across different social dialogue tasks. The research demonstrates that user behaviors significantly differ by task type, with utterance-related behaviors being key indicators for user-centric tasks and turn-taking behaviors being more important for interactive tasks. The method achieved mean absolute error below the evaluation score granularity, demonstrating its feasibility as an objective evaluation framework.

## Method Summary
The study collected Japanese dialogue data for three social dialogue tasks (attentive listening, job interview, and first-meeting conversation) using android ERICA. User behavior features were manually extracted from transcribed dialogues, including utterance time, number of utterances, words, unique words, content words, backchannels, fillers, laughs, disfluencies, and average switching pause length. XGBoost regression models were trained with leave-one-out cross-validation, using these behavioral features to predict subjective evaluation scores on 7-point scales. SHAP analysis was employed to identify which behaviors were most predictive for each task type.

## Key Results
- The evaluation framework achieved mean absolute error below the evaluation score granularity (0.970, 0.953, and 0.683 for attentive listening, job interview, and first-meeting conversation respectively)
- Task-specific behavioral indicators were identified: utterance-related behaviors (number of utterances, words) were key for user-centric tasks, while turn-taking behaviors (average switch pause length) were more important for interactive tasks
- Disfluency patterns, particularly in job interview contexts, served as indicators of system effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: User behavior features can predict subjective evaluation scores with error below the evaluation score granularity.
- Mechanism: Regression models trained on observable user behaviors can capture patterns that correlate with subjective satisfaction, allowing objective evaluation.
- Core assumption: User behaviors systematically vary with subjective satisfaction and are measurable without system intervention.
- Evidence anchors: Cross-validation using XGBoost with 11 behavioral features achieved MAE below 1.0, which is the granularity of the 7-point scale.
- Break condition: If user behaviors become decoupled from satisfaction due to cultural shifts in communication norms or if granularity changes to finer scales.

### Mechanism 2
- Claim: Different dialogue tasks require different behavioral indicators for evaluation.
- Mechanism: Task characteristics (user-centric vs. interactive) determine which user behaviors are most predictive of subjective evaluation.
- Core assumption: Task structure influences how users interact and express satisfaction through behaviors.
- Evidence anchors: SHAP analysis showed number of utterances and words were key for attentive listening and job interview, while average switch pause length was more important for first-meeting conversation.
- Break condition: If new dialogue task types emerge that don't fit the user-centric vs. interactive dichotomy, or if task boundaries blur significantly.

### Mechanism 3
- Claim: Disfluency patterns can indicate system effectiveness in formal tasks.
- Mechanism: In formal dialogue contexts like job interviews, user disfluencies reflect cognitive load and stress related to system performance.
- Core assumption: Formal dialogue contexts create measurable psychological responses that manifest as disfluencies.
- Evidence anchors: SHAP values showed number of disfluencies was particularly important for job interview task evaluation.
- Break condition: If disfluency patterns become less reliable indicators due to changes in communication technology or cultural norms around formal speech.

## Foundational Learning

- Concept: SHAP (SHapley Additive exPlanations) values and their interpretation
  - Why needed here: The paper uses SHAP analysis to determine which user behaviors are most predictive of subjective evaluation scores, requiring understanding of how SHAP values indicate feature importance
  - Quick check question: If a behavioral feature has a high absolute SHAP value, what does this tell you about its relationship to the model's predictions?

- Concept: Regression modeling and cross-validation
  - Why needed here: The paper employs regression models (XGBoost) with leave-one-out cross-validation to validate the evaluation framework, requiring understanding of these techniques
  - Quick check question: Why is leave-one-out cross-validation particularly appropriate for this study's relatively small dataset?

- Concept: Turn-taking dynamics in human conversation
  - Why needed here: The paper analyzes turn-taking behaviors like average switch pause length, requiring understanding of typical human conversation patterns
  - Quick check question: What is the typical average switching pause length in human-human conversations compared to human-system dialogues?

## Architecture Onboarding

- Component map: Data collection (dialogue recording) -> Transcription -> Feature extraction (11 behavioral metrics) -> Regression modeling (XGBoost) -> SHAP analysis -> Validation (cross-validation)
- Critical path: Feature extraction -> Regression model training -> SHAP value calculation -> Error measurement
- Design tradeoffs: Manual transcription provides accurate behavioral features but limits scalability; automated transcription could enable larger-scale evaluation but may reduce accuracy
- Failure signatures: High MAE values (above 1.0) would indicate poor predictive performance; inconsistent SHAP patterns across similar dialogue tasks would suggest task classification issues
- First 3 experiments:
  1. Train XGBoost models on each task separately and compare MAE to ensure task-specific modeling is beneficial
  2. Perform ablation study by removing each behavioral feature to identify which features contribute most to prediction accuracy
  3. Test model performance when mixing dialogue types to determine if a unified model is feasible or if task separation is necessary

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed evaluation framework be extended to multimodal dialogue systems that incorporate visual or gestural cues?
- Basis in paper: The authors acknowledge that future research will explore additional modalities such as eye-gaze behaviors analyzed through image processing.
- Why unresolved: The current study focuses solely on verbal and acoustic behaviors, leaving the integration of visual/gestural cues unexplored.
- What evidence would resolve it: Empirical studies comparing evaluation performance with and without multimodal behavior data in diverse dialogue tasks.

### Open Question 2
- Question: Does the relationship between user behaviors and subjective evaluation scores generalize across different cultural contexts or languages?
- Basis in paper: The study uses Japanese dialogue data and participants, with no mention of cross-cultural validation.
- Why unresolved: Cultural norms significantly influence conversational behaviors (e.g., backchannel frequency, pause lengths), but the framework's generalizability remains untested.
- What evidence would resolve it: Replication of the study with multilingual datasets and diverse cultural groups showing consistent behavior-evaluation correlations.

### Open Question 3
- Question: What is the optimal balance between objective and subjective evaluation methods for comprehensive assessment of spoken dialogue systems?
- Basis in paper: The authors discuss the complementary nature of objective and subjective evaluations but do not provide a framework for their optimal integration.
- Why unresolved: While both methods have merits, guidelines for when and how to combine them in practice are lacking.
- What evidence would resolve it: Comparative studies demonstrating improved evaluation accuracy and reliability when combining both approaches versus using either alone.

## Limitations
- The dataset size is relatively small (69-86 dialogues per task), which may limit generalizability
- Behavioral features are extracted through manual transcription, limiting scalability and introducing potential human error
- The method's generalizability to non-Japanese dialogues or different cultural contexts remains unverified

## Confidence
- High confidence: The core finding that user behaviors can predict subjective evaluation scores with error below the granularity threshold
- Medium confidence: The task-specific patterns of which behaviors are most predictive
- Low confidence: The generalizability of these findings to other dialogue types, languages, or cultural contexts

## Next Checks
1. Test the model on an independent dataset collected from different users, dialogue partners, or cultural contexts to verify generalizability
2. Compare the objective evaluation results with additional subjective measures (e.g., physiological indicators like heart rate or eye tracking) to validate the behavioral indicators
3. Conduct an ablation study to determine the minimum set of behavioral features needed to maintain prediction accuracy below the granularity threshold