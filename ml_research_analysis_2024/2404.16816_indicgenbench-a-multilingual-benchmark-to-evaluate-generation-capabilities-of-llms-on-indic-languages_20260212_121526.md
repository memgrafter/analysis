---
ver: rpa2
title: 'IndicGenBench: A Multilingual Benchmark to Evaluate Generation Capabilities
  of LLMs on Indic Languages'
arxiv_id: '2404.16816'
source_url: https://arxiv.org/abs/2404.16816
tags:
- languages
- indic
- language
- performance
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IndicGenBench, the largest benchmark for
  evaluating large language models (LLMs) on user-facing generation tasks across 29
  Indic languages spanning 13 scripts and 4 language families. The benchmark extends
  existing datasets like CrossSum, FLORES, XQuAD, and XorQA to cover a broader set
  of Indic languages through human curation, providing multi-way parallel evaluation
  data for many underrepresented languages.
---

# IndicGenBench: A Multilingual Benchmark to Evaluate Generation Capabilities of LLMs on Indic Languages

## Quick Facts
- arXiv ID: 2404.16816
- Source URL: https://arxiv.org/abs/2404.16816
- Reference count: 22
- Primary result: Introduces the largest benchmark for evaluating LLMs on generation tasks across 29 Indic languages spanning 13 scripts and 4 language families.

## Executive Summary
This paper introduces IndicGenBench, a comprehensive multilingual benchmark designed to evaluate large language models on user-facing generation tasks across 29 Indic languages. The benchmark extends existing datasets like CrossSum, FLORES, XQuAD, and XorQA through human curation to create multi-way parallel evaluation data for many underrepresented languages. The authors evaluate a wide range of proprietary and open-source LLMs across various model sizes and training settings, finding that while PaLM-2 models perform best overall, significant performance gaps remain compared to English, indicating the need for further research on inclusive multilingual language models.

## Method Summary
The benchmark comprises five datasets (CrossSum-IN, FLORES-IN, XQuAD-IN, XORQA-IN-XX, XORQA-IN-EN) with training, dev, and test splits for all tasks. Evaluation uses Character-F1 (ChrF) for generation tasks and SQuAD-style Token-F1 for QA. The authors evaluate LLMs in one-shot, few-shot, and fine-tuning settings across model families including mT5, Gemma, BLOOM, LLaMA, GPT-3.5, GPT-4, and PaLM-2. Human translation extends existing multilingual benchmarks to cover 29 Indic languages, creating consistent evaluation conditions while varying only the language dimension.

## Key Results
- PaLM-2 models achieve the best performance across most tasks and languages
- Significant performance gaps persist between English and all Indic languages, even for high-resource languages like Hindi
- Fine-tuning with small parallel training data can close performance gaps for generation tasks
- Tokenization disparities create uneven performance due to context length limitations and in-context exemplar capacity
- Hindi in-context exemplars are more effective than English ones for prompting in Indic languages

## Why This Works (Mechanism)

### Mechanism 1
Extending existing multilingual benchmarks through human translation produces multi-way parallel data, enabling disentanglement of task vs. language performance gaps. By re-using source benchmarks (CrossSum, XQuAD, XorQA, FLORES) and translating into 29 Indic languages, the benchmark creates consistent evaluation conditions across tasks while varying only the language dimension. Translation quality is assumed to be high enough that observed performance differences are attributable to language rather than task understanding.

### Mechanism 2
Fine-tuning on small amounts of high-quality parallel training data can close performance gaps for generation tasks in low-resource languages. The benchmark provides small training sets for all tasks (except FLORES-IN), enabling parameter-efficient adaptation of LLMs to generate quality text in underrepresented languages. Even small amounts of parallel data are assumed to be sufficient to guide generation models when combined with pre-trained multilingual knowledge.

### Mechanism 3
Tokenization disparities across Indic languages create uneven performance due to context length limitations and in-context exemplar capacity. High token fertility (words broken into many subwords) in low-resource languages reduces the number of in-context examples that fit within model context windows, degrading few-shot performance. Model context windows are assumed to be the limiting factor for in-context learning performance.

## Foundational Learning

- **Cross-lingual generalization**: Understanding whether models can transfer knowledge from high-resource to low-resource languages is central to evaluating multilingual generation capabilities. Quick check: What evidence from the paper shows that Hindi exemplars are more effective than English ones for prompting in Indic languages?

- **Tokenization and subword segmentation**: Tokenization affects both model capacity usage and the number of in-context examples that can be provided, directly impacting few-shot performance. Quick check: How does token fertility differ between Tibetan and Pashto according to the analysis?

- **Few-shot vs. fine-tuning tradeoffs**: The paper compares these two adaptation methods to understand which is more effective for different task types and model sizes. Quick check: According to the results, which approach performs better for longer-form generation tasks like cross-lingual summarization?

## Architecture Onboarding

- **Component map**: IndicGenBench consists of five datasets (CrossSum-IN, FLORES-IN, XQuAD-IN, XORQA-IN-XX, XORQA-IN-EN) each with training, dev, and test splits; evaluation scripts for multiple metrics (ChrF for generation, Token-F1 for QA); and model comparison tables across different prompting strategies.

- **Critical path**: 1) Load evaluation dataset for a task, 2) Format model input using appropriate prompt template, 3) Run model inference with temperature=0 (or 0.1 for LLaMA), 4) Calculate evaluation metric, 5) Aggregate results across languages and model configurations.

- **Design tradeoffs**: Human translation ensures quality but may miss India-specific entities; small training sets enable fine-tuning but may be insufficient for some languages; focusing on generation tasks complements existing NLU benchmarks but leaves reasoning tasks for future work.

- **Failure signatures**: Low performance in medium/low resource languages may indicate tokenization issues, insufficient in-context exemplars, or fundamental model limitations; mixed-language outputs suggest the model is confused between related languages.

- **First 3 experiments**:
  1. Run one-shot evaluation of PaLM-2-L on CrossSum-IN across all 29 languages to establish baseline performance
  2. Test few-shot performance with Hindi vs. English exemplars for a medium-resource language like Punjabi
  3. Fine-tune mT5-XXL on XQuAD-IN training data and compare Token-F1 to few-shot prompting results

## Open Questions the Paper Calls Out

### Open Question 1
How does tokenization fertility across Indic languages impact LLM performance, and what alternative tokenization approaches could mitigate these disparities? The paper identifies token fertility as a concern but does not explore alternative tokenization approaches or quantify the exact impact on downstream task performance.

### Open Question 2
What is the relative importance of task-specific knowledge versus language understanding in cross-lingual LLM performance, and how can these components be disentangled? While the paper creates the capability to disentangle these factors through multi-way parallel data, it does not perform the analysis to determine the relative contribution of each component.

### Open Question 3
How does the relatedness between source and target languages in cross-lingual tasks affect LLM performance, and can this be leveraged to improve performance on low-resource languages? The paper observes that Hindi exemplars are more effective than English ones, but does not systematically study the relationship between language similarity and performance.

## Limitations

- The quality and consistency of human translations used to extend existing benchmarks to 29 Indic languages are not fully detailed, making it difficult to assess whether performance gaps are truly due to language characteristics versus translation artifacts.

- The evaluation focuses exclusively on generation tasks, leaving reasoning and comprehension capabilities unmeasured, which may provide an incomplete picture of model performance across the full spectrum of language understanding.

- The small training sets provided for fine-tuning may be insufficient for some low-resource languages, potentially limiting the validity of conclusions about fine-tuning effectiveness.

## Confidence

- **High Confidence**: The observation that PaLM-2 models outperform other LLMs across most tasks is well-supported by systematic evaluation across multiple model sizes and tasks.

- **Medium Confidence**: The claim that fine-tuning can close performance gaps for generation tasks in low-resource languages is partially supported, but the evidence is mixed.

- **Low Confidence**: The assertion that tokenization disparities are a primary driver of performance differences across languages is based on limited analysis and has not been empirically validated.

## Next Checks

1. Conduct a blind evaluation where native speakers rate translation quality across the 29 Indic languages to establish whether performance differences correlate with translation quality or genuine language understanding gaps.

2. Systematically vary the number of in-context exemplars for languages with different token fertility levels while keeping the total token budget constant, measuring the precise relationship between tokenization and few-shot performance.

3. Design controlled experiments comparing few-shot performance using exemplars from multiple source languages (English, Hindi, and related Indic languages) across different target languages to quantify the effectiveness of cross-lingual transfer.