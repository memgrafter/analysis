---
ver: rpa2
title: Can LLMs Learn by Teaching for Better Reasoning? A Preliminary Study
arxiv_id: '2406.14629'
source_url: https://arxiv.org/abs/2406.14629
tags:
- teacher
- score
- learning
- code
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  improve their reasoning capabilities through a teaching process, inspired by the
  human learning-by-teaching phenomenon. The authors propose three methods (M1, M2,
  M3) that simulate different levels of learning by teaching, each implemented in
  well-established LLM pipelines.
---

# Can LLMs Learn by Teaching for Better Reasoning? A Preliminary Study

## Quick Facts
- arXiv ID: 2406.14629
- Source URL: https://arxiv.org/abs/2406.14629
- Authors: Xuefei Ning; Zifu Wang; Shiyao Li; Zifu Wang; Zinan Lin; Peiran Yao; Tianyu Fu; Matthew B. Blaschko; Guohao Dai; Huazhong Yang; Yu Wang
- Reference count: 40
- Key outcome: LbT methods improve LLM reasoning, with M1 achieving up to 18.23% accuracy gain over baselines

## Executive Summary
This paper investigates whether LLMs can improve their reasoning capabilities through Learning by Teaching (LbT), inspired by the human learning-by-teaching phenomenon. The authors propose three methods (M1, M2, M3) that simulate different levels of learning by teaching, each implemented in well-established LLM pipelines. Experiments on mathematical reasoning, code synthesis, and text classification tasks show that these methods can improve LLM performance, with M1 achieving up to 18.23% accuracy improvement over strong baselines and demonstrating potential for weak-to-strong generalization.

## Method Summary
The paper proposes three Learning by Teaching methods that simulate different levels of human learning by teaching. M1 uses LbT-based scoring to select high-quality rationales for in-context learning by observing student feedback. M2 improves model capability through fine-tuning with preference data derived from student feedback. M3 iteratively refines teaching materials by analyzing student failure cases, mimicking higher-level learning by teaching. All methods use a teacher LLM to generate rationales for problem-solution pairs and student LLMs to provide feedback through performance on similar problems.

## Key Results
- M1 achieves up to 18.23% accuracy improvement over self-consistency and correctness-based scoring baselines on MATH dataset
- M2 shows potential for improving teacher model capability through preference optimization with LbT scores
- M3 demonstrates weak-to-strong generalization with modest improvements (2.3-3.6%) when teacher teaches multiple diverse students
- Multiple diverse students provide better feedback than single students or teacher alone

## Why This Works (Mechanism)

### Mechanism 1
- Teaching materials that make it easier for students to learn have clearer and more accurate logic when using ICL as the student's "learning" method.
- When LLMs learn via in-context examples, the quality of their reasoning is strongly influenced by the clarity and correctness of the teaching materials. If a rationale helps a student solve similar problems correctly, it likely contains well-structured and accurate logic.
- Core assumption: Students can effectively follow the reasoning steps in ICL examples and apply them to new but similar problems.
- Break condition: If TP and EP are not sufficiently similar, or if the ICL example is too complex for the student to follow, the LbT score will not reflect TR quality.

### Mechanism 2
- Strong teachers can improve even when teaching weaker students (weak-to-strong generalization).
- When a strong model teaches a weaker one, the feedback from the weaker student reveals gaps or ambiguities in the teaching material that the strong model may not notice on its own. By refining the teaching material based on this feedback, the strong model improves its own reasoning clarity.
- Core assumption: Weak students provide meaningful feedback that identifies flaws in teaching material logic.
- Break condition: If the student is too weak to follow any logic, or if the teacher cannot interpret the student's feedback, no improvement occurs.

### Mechanism 3
- Diversity in students might help: teaching multiple students could be better than teaching a single student or the teacher itself.
- Different students make different kinds of mistakes. By observing a diverse set of failures, the teacher can refine teaching materials in ways that address a broader range of misunderstandings, leading to more robust reasoning.
- Core assumption: Multiple students with different capabilities and knowledge backgrounds provide complementary feedback.
- Break condition: If students are too similar in their errors, diversity offers no advantage; if students are too weak, feedback becomes noise.

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: The student models in M1, M2, and M3 learn from exemplars without fine-tuning; ICL is the core learning mechanism.
  - Quick check question: If you provide an ICL example that solves "add two numbers" and ask the model to solve "multiply two numbers," will it likely succeed? (No—problems must be similar.)

- Concept: Chain-of-thought (CoT) reasoning
  - Why needed here: Detailed rationales help students follow the logic step-by-step, which is essential for the LbT-TMQ assumption to hold.
  - Quick check question: Does a single-line answer in an ICL example provide enough reasoning guidance for a student to solve a similar problem? (No—CoT is needed.)

- Concept: Preference optimization (e.g., DPO)
  - Why needed here: M2 uses LbT scores to create preference pairs for fine-tuning the teacher model; understanding DPO is key to grasping M2.
  - Quick check question: In DPO, what is compared to form a preference pair? (Two outputs, one preferred over the other based on a score or label.)

## Architecture Onboarding

- Component map: Teacher LLM -> LbT Scoring Module -> Student LLM(s) -> Exemplar Refinement Pipeline
- Critical path:
  1. Teacher generates diverse TR-TA pairs for TP.
  2. Each TR-TA is used as ICL to teach student on EPs.
  3. Student performance is scored → LbT score for TR-TA.
  4. Best TR-TA is selected (M1) or used for preference data (M2).
  5. For M3, teacher reflects on student failures and revises exemplars; repeat.
- Design tradeoffs:
  - Single vs. multiple students: multiple students increase diversity of feedback but add compute.
  - TR-only vs. TR+TA in ICL: TR+TA improves student following but increases exemplar length.
  - LbT scoring vs. self-evaluation: LbT scoring requires EP generation but better reflects logic quality.
- Failure signatures:
  - Low LbT score despite correct TA: TR logic is unclear or not generalizable.
  - High LbT score but TP/EP mismatch: student guessed or used unrelated reasoning.
  - No improvement over baselines: student cannot follow ICL logic, or TP/EP are too dissimilar.
- First 3 experiments:
  1. Run M1 on MATH with GPT-4o teacher and GPT-4o mini student; compare SC vs. LbT-based selection.
  2. Run M2 on MATH training set; fine-tune LLaMA3-8B with correctness-based DPO vs. LbT-based DPO.
  3. Run M3 on Logical Fallacy with LLaMA3-70B teacher and LLaMA3-8B + Mistral-7B students; measure F1 gain over iterations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the LbT-TMQ assumption hold across diverse task domains beyond mathematical reasoning, code synthesis, and logical reasoning?
- Basis in paper: [explicit] The authors state this assumption underlies M1 and M2, and verify it for the three chosen task domains.
- Why unresolved: The verification is limited to three task types. The assumption's generalizability to other domains (e.g., creative writing, dialogue) remains untested.
- What evidence would resolve it: Empirical validation of M1/M2 across diverse task domains, showing consistent improvements when the LbT-TMQ assumption holds.

### Open Question 2
- Question: What is the optimal balance between teacher strength and student diversity for maximizing LbT benefits?
- Basis in paper: [inferred] The authors observe that having multiple and diverse students improves teacher performance in M3, and discuss the potential of weak-to-strong generalization.
- Why unresolved: The experiments provide preliminary evidence but don't systematically explore the optimal teacher-student configurations for different LbT methods and task types.
- What evidence would resolve it: A comprehensive study varying teacher strength, student diversity, and task complexity to identify optimal configurations for each LbT method.

### Open Question 3
- Question: How can the computational overhead of LbT-based scoring be reduced while maintaining or improving performance gains?
- Basis in paper: [explicit] The authors acknowledge the additional inference cost of LbT-based scoring in M1 and M2, and suggest designing efficient inference algorithms as a future direction.
- Why unresolved: The paper doesn't explore specific techniques to reduce computational overhead, such as model compression, knowledge distillation, or efficient search strategies.
- What evidence would resolve it: Development and evaluation of techniques to reduce LbT-based scoring computational overhead, demonstrating maintained or improved performance gains with lower computational cost.

## Limitations

- Weak-to-strong generalization evidence is limited to modest improvements (2.3-3.6%) from a single teacher-student configuration
- LbT score validity requires TP and EPs to be sufficiently similar, but "sufficiently similar" is not systematically quantified
- Diversity benefits are claimed but not rigorously validated as coming from complementary error patterns versus other factors

## Confidence

**High confidence**: The core observation that LbT-based scoring can identify high-quality rationales for in-context learning (M1) is well-supported by MATH dataset results showing up to 18.23% improvement over baselines.

**Medium confidence**: The weak-to-strong generalization claim (M3) has moderate support but limited scope. The improvement is real but modest, and the evidence comes primarily from a single teacher-student configuration.

**Low confidence**: The claim about diversity benefits from multiple students lacks rigorous validation. The paper shows that multiple students help but doesn't establish that this comes from complementary feedback patterns versus other factors like increased computational resources.

## Next Checks

1. **LbT score robustness test**: Systematically vary the similarity between TP and EPs across multiple task domains and measure how LbT score correlation with rationale quality degrades as similarity decreases.

2. **Weak-to-strong generalization boundary**: Test M3 with progressively weaker students (down to near-random performance) to identify the minimum student capability required for meaningful teacher improvement.

3. **Diversity mechanism validation**: Design an experiment where multiple students are trained to make systematically different types of errors (rather than just random variation). Measure whether teacher improvement correlates with the diversity of error types rather than simply the number of students.