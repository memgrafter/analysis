---
ver: rpa2
title: 'ChatGPT Role-play Dataset: Analysis of User Motives and Model Naturalness'
arxiv_id: '2403.18121'
source_url: https://arxiv.org/abs/2403.18121
tags:
- chatgpt
- role-play
- human
- dataset
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the ChatGPT Role-play Dataset (CRD), the
  first dataset containing human-AI conversations annotated for user motives and model
  naturalness. The dataset comprises 85 conversations with 1,742 utterances collected
  in vanilla and role-play settings (boss and classmate).
---

# ChatGPT Role-play Dataset: Analysis of User Motives and Model Naturalness

## Quick Facts
- arXiv ID: 2403.18121
- Source URL: https://arxiv.org/abs/2403.18121
- Authors: Yufei Tao; Ameeta Agrawal; Judit Dombi; Tetyana Sydorenko; Jung In Lee
- Reference count: 0
- Primary result: Role-play prompts significantly improve ChatGPT's conversational naturalness compared to vanilla interactions

## Executive Summary
This paper introduces the ChatGPT Role-play Dataset (CRD), the first dataset containing human-AI conversations annotated for user motives and model naturalness. The dataset comprises 85 conversations with 1,742 utterances collected in vanilla and role-play settings (boss and classmate). Results show that role-play interactions were more natural than vanilla, with 52-47% natural responses versus 5.6% in vanilla. User motives varied across settings, with conversation being the most frequent motive. ChatGPT responses were consistently more verbose than human utterances (6.3x in vanilla, 1.7-2.4x in role-play). Perplexity scores indicated lower certainty in human responses, particularly in vanilla. Sentiment analysis revealed more positive human sentiment in role-play settings. The study demonstrates that humans desire more human-like interactions with AI and that role-play prompts improve naturalness of ChatGPT responses.

## Method Summary
The study collected 85 conversations (1,742 utterances) from 57 participants interacting with ChatGPT in vanilla and role-play settings (boss and classmate). Expert linguists manually annotated all utterances for user motives and model naturalness using a defined coding scheme. Statistical analysis was performed using NLTK for text processing, GPT-2 for perplexity scoring, and VADER for sentiment analysis. Interrater reliability was calculated using Fleiss' kappa.

## Key Results
- Role-play interactions showed significantly higher naturalness (52-47%) compared to vanilla (5.6%)
- User motives varied across settings, with conversation being the dominant motive in role-play
- ChatGPT responses were 6.3x more verbose than human utterances in vanilla, reducing to 1.7-2.4x in role-play
- Perplexity scores were consistently lower for ChatGPT responses, indicating higher certainty
- Human sentiment was more positive in role-play settings compared to vanilla

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Role-play prompts significantly improve ChatGPT's conversational naturalness compared to vanilla interactions.
- Mechanism: By constraining ChatGPT to adopt specific personas (boss, classmate), the model's responses become more contextually appropriate and human-like, reducing the frequency of "AI disclaimer" responses and increasing question-asking behavior.
- Core assumption: ChatGPT's conversational quality is highly sensitive to the prompt structure and role-play constraints.
- Evidence anchors:
  - [abstract]: "Results show that role-play interactions were more natural than vanilla, with 52-47% natural responses versus 5.6% in vanilla."
  - [section]: "The most frequent unnatural response in the vanilla dataset were labeled as 'AI' because it was an annoying for a human to hear 'as an AI' when they expected to have a conversation. 'As an AI' responses were almost completely missing from the role-playing datasets."
  - [corpus]: Weak evidence. The corpus shows related papers on role-play and human-like AI, but no direct comparison to vanilla settings.
- Break condition: If role-play prompts become too restrictive or the persona is poorly defined, ChatGPT may produce unnatural or contradictory responses.

### Mechanism 2
- Claim: User motives shift significantly between vanilla and role-play settings, with conversation being the dominant motive in role-play.
- Mechanism: Role-play prompts create a structured social context that naturally encourages conversational interaction, while vanilla settings lead to more exploratory and task-oriented motives.
- Core assumption: The social context provided by role-play prompts directly influences user behavior and interaction patterns.
- Evidence anchors:
  - [abstract]: "User motives varied across settings, with conversation being the most frequent motive."
  - [section]: "Conversational motives (Convo) are most frequent in all datasets (Figure 4, top), but particularly so in boss and classmate datasets."
  - [corpus]: Weak evidence. Corpus papers discuss user interactions with AI but don't specifically address motive shifts between settings.
- Break condition: If users perceive role-play as artificial or the persona is mismatched with their expectations, they may revert to task-oriented motives.

### Mechanism 3
- Claim: ChatGPT's verbosity remains consistently higher than human utterances regardless of setting, but the ratio decreases in role-play.
- Mechanism: The model's tendency to provide comprehensive responses is moderated by the conversational context, but not eliminated, suggesting inherent architectural bias toward detailed output.
- Core assumption: ChatGPT's response length is an inherent property that can be partially but not completely shaped by context.
- Evidence anchors:
  - [abstract]: "ChatGPT responses were consistently more verbose than human utterances (6.3x in vanilla, 1.7-2.4x in role-play)."
  - [section]: "ChatGPT was about 1.7 or 2.4 times wordier than humans in the role-play settings, and about 6.3 times wordier than humans in the vanilla setting."
  - [corpus]: Weak evidence. Corpus papers discuss AI verbosity but don't provide quantitative comparisons to human utterances.
- Break condition: If the model's architecture changes to prioritize brevity or if fine-tuning explicitly targets response length, this mechanism would break.

## Foundational Learning

- Concept: Gricean maxims of conversation
  - Why needed here: The paper evaluates model naturalness based on adherence to these cooperative principles (Quantity, Quality, Relevance, Manner).
  - Quick check question: Which Gricean maxim is violated when ChatGPT provides too much information in its responses?

- Concept: Perplexity as a language model evaluation metric
  - Why needed here: The study uses perplexity scores to compare certainty between human and AI-generated text.
  - Quick check question: What does a lower perplexity score indicate about a language model's performance?

- Concept: Sentiment analysis using VADER
  - Why needed here: The paper uses sentiment analysis to compare emotional tone between human and AI responses.
  - Quick check question: What type of text is VADER specifically designed to analyze effectively?

## Architecture Onboarding

- Component map: Data collection -> Expert annotation -> Statistical analysis -> Topic modeling -> Perplexity/sentiment analysis
- Critical path:
  1. Collect conversation data with varied prompts
  2. Expert annotation for user motives and naturalness
  3. Statistical analysis of utterance patterns
  4. Topic modeling to identify conversation themes
  5. Perplexity and sentiment analysis for deeper insights
- Design tradeoffs:
  - Manual annotation provides high-quality labels but limits dataset size
  - Role-play settings improve naturalness but may constrain natural interaction
  - Using expert annotators ensures reliability but increases cost and time
- Failure signatures:
  - Low interrater agreement indicates unclear annotation guidelines
  - High perplexity in human responses suggests short utterances or unusual language
  - Lack of questions from ChatGPT indicates failure to engage conversationally
- First 3 experiments:
  1. Compare interrater agreement scores across different annotator pairs
  2. Analyze perplexity scores for utterances of varying lengths
  3. Test correlation between question frequency and conversation length across different datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do individual user characteristics (age, native language, prior chatbot experience) affect the quality and naturalness of human-AI interactions in both vanilla and role-play settings?
- Basis in paper: [explicit] The paper notes participants varied in age, native language, and prior experience, but doesn't analyze how these characteristics affected interaction quality
- Why unresolved: The study collected demographic data but didn't perform statistical analysis correlating these factors with interaction outcomes
- What evidence would resolve it: Statistical analysis showing correlation between participant demographics and metrics like conversation length, naturalness scores, or perplexity scores

### Open Question 2
- Question: Would longer training conversations with feedback mechanisms improve ChatGPT's ability to generate more natural responses in both vanilla and role-play settings?
- Basis in paper: [inferred] The paper found ChatGPT was often unnatural, particularly in vanilla settings, and some participants explicitly requested more human-like responses
- Why unresolved: The study was cross-sectional and didn't test whether repeated interactions with feedback would improve model performance
- What evidence would resolve it: Experimental design comparing naturalness scores of ChatGPT responses after different numbers of interaction-feedback cycles

### Open Question 3
- Question: How do different role-play scenarios (varying social distance, power dynamics, and task complexity) affect the naturalness of ChatGPT responses and the quality of human-AI interaction?
- Basis in paper: [explicit] The paper used two role-play scenarios (boss and classmate) and found differences in interaction patterns, but didn't systematically vary other role-play parameters
- Why unresolved: The study only tested two specific role-play scenarios without varying the social and contextual parameters
- What evidence would resolve it: Comparative analysis of multiple role-play scenarios varying in social distance, power dynamics, and task complexity, measuring naturalness and interaction quality metrics

## Limitations

- Dataset Size Constraints: The study analyzed 85 conversations totaling 1,742 utterances, which provides meaningful patterns but limits generalizability and statistical power for individual user motives.
- Annotation Subjectivity: Manual annotation by expert linguists introduces potential subjectivity, with moderate interrater agreement (Fleiss' kappa = 0.51) suggesting some ambiguity in coding decisions.
- Temporal Generalization: Data was collected using ChatGPT versions from March 2023, limiting applicability to current or future model versions due to potential behavior changes.

## Confidence

- High Confidence: Role-play prompts significantly reduce "AI disclaimer" responses and increase conversational naturalness (5.6% vs 52-47% natural responses).
- Medium Confidence: User motive shifts across settings are reasonably supported but limited by dataset size; conversational motives dominate in role-play but relative frequencies of other motives need larger samples.
- Medium Confidence: ChatGPT's consistent verbosity advantage is well-supported numerically, but the underlying mechanism remains partially speculative.

## Next Checks

1. **Interrater Reliability Stress Test**: Re-analyze the 5 conversations initially rated by all three annotators using different reliability metrics (Cohen's kappa, Krippendorff's alpha) to verify the Fleiss' kappa score of 0.51 and identify specific categories with low agreement.

2. **Temporal Validation**: Collect new data using the same prompts with current ChatGPT versions to assess whether the observed patterns in naturalness and user motives persist after model updates.

3. **Prompt Variation Study**: Test additional role-play scenarios beyond boss and classmate (e.g., therapist, friend, colleague) to determine whether the improvements in naturalness generalize across different social contexts or are specific to the tested scenarios.