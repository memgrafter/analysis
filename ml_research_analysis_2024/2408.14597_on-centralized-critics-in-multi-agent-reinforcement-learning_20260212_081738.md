---
ver: rpa2
title: On Centralized Critics in Multi-Agent Reinforcement Learning
arxiv_id: '2408.14597'
source_url: https://arxiv.org/abs/2408.14597
tags:
- centralized
- critics
- policy
- critic
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive theoretical and empirical analysis
  of centralized critics in multi-agent reinforcement learning (MARL). It challenges
  common intuitions by showing that centralized critics are not strictly beneficial
  and may introduce bias and variance compared to decentralized critics.
---

# On Centralized Critics in Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2408.14597
- Source URL: https://arxiv.org/abs/2408.14597
- Reference count: 12
- One-line primary result: Centralized critics are not strictly beneficial and may introduce bias and variance compared to decentralized critics

## Executive Summary
This paper provides a comprehensive theoretical and empirical analysis of centralized critics in multi-agent reinforcement learning (MARL). It challenges common intuitions by showing that centralized critics are not strictly beneficial and may introduce bias and variance compared to decentralized critics. The authors prove that centralized and decentralized history-based critics have the same expected gradient, while state-based critics may incur bias and higher variance. They recommend using history-state-based critics as an alternative that is both unbiased and potentially easier to learn. The paper also highlights the deficiencies of popular benchmarks and provides practical insights for choosing the right critic type based on task characteristics.

## Method Summary
The paper analyzes four types of critic architectures in MARL: IAC (decentralized critic), IACC-H (centralized history-based critic), IACC-S (centralized state-based critic), and IACC-HS (centralized history-state-based critic). The authors provide theoretical analysis of bias and variance properties for each critic type, then empirically validate their findings across multiple domains including Climb Game, Dec-Tiger, SMAC, and Predator and Prey environments. The analysis uses policy gradient theorems to derive expected gradients and variance properties for each critic architecture.

## Key Results
- Centralized and decentralized history-based critics produce identical expected policy gradients
- State-based critics introduce bias and higher variance compared to history-based critics
- Centralized critics increase policy gradient variance due to Multi-Action and Multi-Observation Variance
- History-state-based critics offer a promising alternative that is both unbiased and potentially easier to learn
- Popular benchmarks like SMAC have limitations including excessive observations and state aliasing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Centralized critics do not improve cooperation in policy learning compared to decentralized critics.
- Mechanism: Both centralized and decentralized history-based critics produce the same expected policy gradient because the centralized critic's value is marginalized over other agents' histories and actions, which is equivalent to what decentralized critics do inherently.
- Core assumption: The critic models accurately represent their respective value functions (critic convergence assumption).
- Evidence anchors:
  - [abstract]: "we prove that critic centralization does not theoretically improve cooperation compared to decentralized critics from a policy learning perspective"
  - [section]: "Theorem 1. The IACC-H sample gradient is an unbiased estimate of the IAC sample gradient, i.e., gh = E [ˆgh] = E [ˆgi] = ∇θiJ."
  - [corpus]: No direct corpus evidence found for this specific mechanism.
- Break condition: If the critic models are not accurately learned or if the marginalization property does not hold due to approximation errors.

### Mechanism 2
- Claim: State-based critics introduce bias in policy gradients compared to history-based critics.
- Mechanism: State values alias over all histories that visit the state, losing information about uncertainty and information-gathering needs. This aliasing makes state values biased estimators of history values, leading to biased policy gradients.
- Core assumption: The environment requires active information gathering or the optimal policies are not purely reactive.
- Evidence anchors:
  - [abstract]: "we show that state-based critics may result in bias, making them theoretically inferior to history-based critics"
  - [section]: "Theorem 3. Qπ(s, a) may be a biased estimate ofQπ(h, a), i.e., the equality Qπ(h, a) = Es|h [Qπ(s, a)] cannot be assumed to hold."
  - [corpus]: No direct corpus evidence found for this specific mechanism.
- Break condition: If the environment is almost fully observable or reactive policies are optimal, making state and history values value-equivalent.

### Mechanism 3
- Claim: Centralized critics increase policy gradient variance compared to decentralized critics.
- Mechanism: The centralized critic treats other agents' histories and actions as random variables, introducing Multi-Action Variance (MAV) and Multi-Observation Variance (MOV). Decentralized critics already marginalize over these variables, avoiding this additional variance.
- Core assumption: The critic models accurately represent their respective value functions.
- Evidence anchors:
  - [abstract]: "we show that centralized critics (both history and state-based) result in higher variance"
  - [section]: "Theorem 2. The IACC-H sample gradient has variance greater or equal than that of the IAC sample gradient, i.e.,Var [ˆgh] ≥ Var [ˆgi]."
  - [corpus]: No direct corpus evidence found for this specific mechanism.
- Break condition: If the number of agents is small or the observation space is limited, reducing MOV impact.

## Foundational Learning

- Concept: Discounted visitation counts and probabilities
  - Why needed here: These are used to formalize the expected number of times states, histories, or state-history pairs are visited, which is crucial for defining value functions and policy gradients in MARL.
  - Quick check question: What is the relationship between discounted visitation counts η and discounted visitation probabilities ρ?

- Concept: Value functions for different information types (history, state, history-state)
  - Why needed here: Different critic types use different value functions, and understanding their properties (bias, variance) is central to the paper's analysis.
  - Quick check question: How does the history-state value function Qπ(h, s, a) differ from the history value function Qπ(h, a) in terms of information used?

- Concept: Policy gradient theorem in MARL
  - Why needed here: The paper analyzes policy gradients for different critic types, and the policy gradient theorem provides the foundation for these analyses.
  - Quick check question: What is the role of the value function in the policy gradient theorem, and how does it differ for centralized vs. decentralized critics?

## Architecture Onboarding

- Component map: Actor models (decentralized, one per agent) -> Centralized critic models (history-based, state-based, or history-state-based) -> Environment (partially observable)

- Critical path: 1. Sample environment episodes 2. Compute value estimates using centralized critic 3. Compute policy gradients for decentralized actors 4. Update actor and critic parameters

- Design tradeoffs:
  - Bias vs. variance: State-based critics have lower variance but higher bias; history-based critics have higher variance but lower bias
  - Scalability: Decentralized critics scale better with the number of agents; centralized critics have higher policy gradient variance
  - Ease of learning: State-based critics are easier to learn; history critics are more difficult to learn

- Failure signatures:
  - High variance in policy updates leading to unstable policies
  - Poor cooperation performance due to shadowed equilibria
  - Convergence to suboptimal policies due to biased value estimates

- First 3 experiments:
  1. Compare IAC (decentralized critic) and IACC-H (centralized history-based critic) on a simple matrix game (e.g., Climb Game) to verify their equivalence in policy gradients and potential cooperation pathologies.
  2. Test IACC-S (centralized state-based critic) on a partially observable environment (e.g., Dec-Tiger) to demonstrate the bias in policy gradients and its impact on performance.
  3. Evaluate IACC-HS (centralized history-state-based critic) on a complex environment (e.g., Predator and Prey) to show its robustness and performance advantages over other critic types.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the approximation error in critic models affect the bias-variance trade-off between centralized and decentralized critics in practice?
- Basis in paper: [explicit] Section 7.4 discusses approximation error and its impact on bias-variance trade-offs, but doesn't provide concrete evidence of how this affects real-world performance.
- Why unresolved: The paper acknowledges the existence of approximation error but doesn't quantify its impact on different critic types or provide empirical evidence of its effects.
- What evidence would resolve it: Controlled experiments comparing performance of different critic types across various levels of approximation error, or theoretical analysis of how approximation error propagates through the learning process.

### Open Question 2
- Question: Can the scalability issues of centralized critics be mitigated through improved representation learning or architectural innovations?
- Basis in paper: [inferred] Section 7.1.2 discusses scalability issues for centralized critics but doesn't explore potential solutions beyond noting the problem.
- Why unresolved: The paper identifies the scalability problem but doesn't investigate whether techniques like attention mechanisms, graph neural networks, or other representation learning approaches could address it.
- What evidence would resolve it: Experiments comparing centralized critics with and without advanced representation learning techniques on large-scale multi-agent tasks, or theoretical analysis of representation capacity requirements for centralized critics.

### Open Question 3
- Question: How do the theoretical bias-variance trade-offs manifest in non-stationary environments where policies are constantly changing?
- Basis in paper: [explicit] Section 7.4 mentions non-stationarity issues but doesn't analyze how they interact with the theoretical properties of different critic types.
- Why unresolved: The paper's theoretical analysis assumes fixed policies, but real-world applications involve constantly evolving policies, which may change the bias-variance dynamics.
- What evidence would resolve it: Empirical studies tracking the evolution of bias and variance over time in dynamic environments, or theoretical models incorporating policy adaptation into the bias-variance analysis.

## Limitations
- Theoretical analysis assumes critic convergence, which may not hold in practice with function approximation
- Empirical evaluation is limited to specific benchmark domains with known limitations
- Analysis focuses on fully observable state inputs for state-based critics, which may not be realistic
- Variance analysis is based on idealized conditions that may not fully capture practical training dynamics

## Confidence
- High Confidence: The theoretical proof that centralized and decentralized history-based critics produce equivalent policy gradients
- Medium Confidence: Claims about state-based critics introducing bias and centralized critics increasing variance, though relying on idealized assumptions
- Medium Confidence: Empirical validation across multiple domains supports theoretical findings, though benchmarks have limitations

## Next Checks
1. Scale Analysis: Systematically vary the number of agents in benchmark domains to quantify how Multi-Action Variance and Multi-Observation Variance scale with agent count, validating Theorem 2's variance bounds.

2. Partial Observability Stress Test: Design a controlled experiment where state-based critics must make decisions under varying levels of partial observability to empirically measure the bias introduced when the equality Qπ(h, a) = Es|h[Qπ(s, a)] does not hold.

3. Critic Convergence Impact: Implement experiments where critic learning is intentionally limited (e.g., by reducing training epochs) to measure how the convergence assumption affects the practical equivalence between centralized and decentralized history-based critics.