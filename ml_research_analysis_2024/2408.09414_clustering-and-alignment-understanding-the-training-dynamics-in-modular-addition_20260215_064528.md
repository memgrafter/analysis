---
ver: rpa2
title: 'Clustering and Alignment: Understanding the Training Dynamics in Modular Addition'
arxiv_id: '2408.09414'
source_url: https://arxiv.org/abs/2408.09414
tags:
- weight
- decay
- training
- embeddings
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the training dynamics of a simple transformer
  model on modular addition using 2D embeddings. The key finding is that embedding
  vectors self-organize into grid-like or circular structures during training, which
  are crucial for successful generalization.
---

# Clustering and Alignment: Understanding the Training Dynamics in Modular Addition

## Quick Facts
- arXiv ID: 2408.09414
- Source URL: https://arxiv.org/abs/2408.09414
- Reference count: 33
- One-line primary result: Embedding vectors self-organize into grid-like or circular structures during training, driven by clustering and alignment forces, with weight decay critical for enabling these forces.

## Executive Summary
This paper analyzes the training dynamics of a simple transformer model on modular addition using 2D embeddings. The key finding is that embedding vectors self-organize into grid-like or circular structures during training, which are crucial for successful generalization. The author proposes that this self-organization results from two interaction forces between embedding pairs: clustering (which pushes pair sums apart if they have different modular sums, or pulls them together if they have the same sum) and alignment (which encourages embedding vectors to align with classification boundaries). A particle simulation using these force equations successfully reproduces the observed structures. Weight decay is shown to be critical for enabling these forces by constraining the magnitudes of weights and biases in the classification layers. The author provides interactive visualizations and extensive empirical evidence, including quantitative measures of grid imperfections and their correlation with accuracy.

## Method Summary
The study uses a single-layer transformer with constant attention and 2D embeddings to solve modular addition (a + b mod N). The model processes N=17 by default, with 80% of all N(N+1)/2 pairs for training and 20% for validation. The embedding vectors undergo self-organization during training, forming grid-like or circular structures. A particle simulation with N=17 particles in 2D space models these dynamics using proposed clustering and alignment force equations. The study sweeps weight decay parameters (0.0, 0.3, 0.6, 1.0) and runs 100 random initializations for each condition, recording validation accuracy and final embedding positions.

## Key Results
- Embedding vectors organize into grid-like or circular structures during training, with these structures being crucial for generalization
- Weight decay constrains weights and biases in the linear layer, enabling the emergence of circular structures and reducing grid imperfections
- Particle simulation using clustering and alignment forces successfully reproduces the observed embedding structures
- Grid imperfections correlate negatively with validation accuracy, providing quantitative evidence of structure quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embedding vectors self-organize into grid or circular structures during training, which are crucial for successful generalization.
- Mechanism: During training, pair sums (outputs of constant attention) cluster based on their modular sum. This clustering is driven by classification boundaries that push pair sums with different modular sums apart and pull pair sums with the same modular sum together.
- Core assumption: The classification function can effectively separate pair sums based on their modular sums, and the embedding vectors can be updated to facilitate this separation.
- Evidence anchors:
  - [abstract] "embedding vectors tend to organize into grid-like or circular structures during training, which are crucial for successful generalization"
  - [section 5.1] "The pair sums tend to cluster based on their modular sum...the classifier must place a decision boundary between them. This decision boundary will create a gradient that will push pair sums away from each other."
  - [corpus] Weak evidence: corpus does not directly mention clustering mechanism, but papers like "Clustering Head: A Visual Case Study of the Training Dynamics in Transformers" suggest related work on clustering in transformer training.
- Break condition: If the classification function cannot effectively separate pair sums based on their modular sums, or if the embedding vectors cannot be updated to facilitate this separation, the clustering mechanism will fail.

### Mechanism 2
- Claim: Weight decay plays a crucial role in the emergence of circular structures and in reducing grid imperfections.
- Mechanism: Weight decay limits the magnitude of weights and biases in the linear layer, which forces the ReLU activations to remain close to the origin. This creates conditions where weight vectors align with pair sums, facilitating the emergence of circular structures.
- Core assumption: Weight decay is necessary to constrain the weights and biases in the linear layer, and this constraint is essential for the alignment mechanism to work.
- Evidence anchors:
  - [abstract] "weight decay is shown to be critical for enabling these forces by constraining the magnitudes of weights and biases in the classification layers"
  - [section 7] "Weight decay encourages the linear layer to have small weights and biases, which results in linear functions with small slopes and close to the origin...Under these conditions, in order to minimize the training loss, the weight vectors of the linear layer will tend to align with the pair sums."
  - [corpus] Weak evidence: corpus does not directly mention weight decay's role in alignment, but papers like "Survival of the Fittest Representation: A Case Study with Modular Addition" suggest related work on weight decay in transformer training.
- Break condition: If weight decay is not applied, the weights and biases in the linear layer could become too large, preventing the alignment mechanism from working and leading to overfitting.

### Mechanism 3
- Claim: The particle simulation using the proposed force equations successfully reproduces the observed structures in the trained transformer.
- Mechanism: The particle simulation models the training dynamics of the embedding vectors using forces that correspond to the clustering and alignment mechanisms. Particles self-organize into grids, circles, and imperfect grids, just like the embedding vectors in the trained transformer.
- Core assumption: The proposed force equations accurately capture the dynamics of the embedding vectors during training, and the particle simulation can effectively model these dynamics.
- Evidence anchors:
  - [abstract] "A particle simulation using these force equations successfully reproduces the observed structures"
  - [section 6] "We model the training dynamics of the embedding vectors using N particles...We observe that the particles self-organize into the same structures as the embeddings in the trained transformer: circles, grids, and imperfect grids."
  - [corpus] Weak evidence: corpus does not directly mention particle simulation, but papers like "Uncovering a Universal Abstract Algorithm for Modular Addition in Neural Networks" suggest related work on mechanistic explanations of transformer training.
- Break condition: If the proposed force equations do not accurately capture the dynamics of the embedding vectors, or if the particle simulation cannot effectively model these dynamics, the simulation will fail to reproduce the observed structures.

## Foundational Learning

- Concept: Modular arithmetic
  - Why needed here: The paper studies the problem of modular addition, which is a fundamental concept in modular arithmetic.
  - Quick check question: What is the result of 7 + 5 (mod 6)?
- Concept: Transformer architecture
  - Why needed here: The paper uses a simplified transformer model to study the training dynamics of modular addition.
  - Quick check question: What is the role of the attention mechanism in a transformer?
- Concept: Gradient descent optimization
  - Why needed here: The paper discusses how the embedding vectors are updated during training using gradient descent.
  - Quick check question: What is the update rule for a parameter Î¸ in gradient descent?

## Architecture Onboarding

- Component map:
  - Input tokens (a, b) -> Embedding layer (2D) -> Constant attention (sum) -> Linear layer (ReLU) -> Output layer (N classes) -> Cross-entropy loss
- Critical path:
  1. Input tokens are embedded into 2D vectors
  2. Constant attention sums the embedding vectors
  3. Linear layer applies transformation and ReLU activation
  4. Output layer maps to N possible sums
  5. Cross-entropy loss is computed
  6. Gradients are backpropagated
  7. Embedding vectors are updated using AdamW optimizer
- Design tradeoffs:
  - Using a simplified transformer architecture with constant attention and 2D embeddings for ease of visualization and analysis
  - Using weight decay to constrain the weights and biases in the linear layer
  - Using a small embedding dimension (2D) to facilitate visualization, but potentially limiting the model's capacity
- Failure signatures:
  - Embedding vectors do not self-organize into grid or circular structures
  - Validation accuracy does not improve during training
  - Weight decay does not effectively constrain the weights and biases in the linear layer
- First 3 experiments:
  1. Train the model without weight decay to observe the impact on the embedding structures and validation accuracy
  2. Vary the embedding dimension (e.g., 3D or 4D) to study the effect on the embedding structures and validation accuracy
  3. Use a different optimizer (e.g., SGD) to study the impact on the training dynamics and embedding structures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mathematical relationship between weight decay and the magnitude of weights and biases in the linear layer?
- Basis in paper: [explicit] The paper mentions that weight decay strongly limits the magnitude of weights and biases, with biases being limited more than weights, and shows a visualization of average magnitudes.
- Why unresolved: The paper only provides empirical observations and visualizations, but does not offer a theoretical framework or mathematical proof for this relationship.
- What evidence would resolve it: A mathematical proof or a more detailed analysis showing how weight decay affects the magnitude of weights and biases during training, possibly including the impact of learning rate and other hyperparameters.

### Open Question 2
- Question: How do the clustering and alignment forces generalize to higher-dimensional embedding spaces and more complex architectures?
- Basis in paper: [inferred] The paper discusses clustering and alignment forces in the context of 2D embeddings in a simple transformer model, but mentions that similar structures emerge in higher-dimensional embedding spaces in Appendix F.
- Why unresolved: The paper does not provide a theoretical framework for these forces in higher dimensions or more complex architectures, and the discussion is limited to empirical observations in a simplified setting.
- What evidence would resolve it: Mathematical proofs or extensive empirical studies showing how clustering and alignment forces operate in higher-dimensional spaces and more complex architectures, possibly including different types of attention mechanisms and neural network layers.

### Open Question 3
- Question: Can the particle simulation model be extended to provide more accurate predictions of training dynamics in larger, more complex models?
- Basis in paper: [explicit] The paper constructs a particle simulation using the proposed clustering and alignment forces to reproduce the observed structures in the transformer model.
- Why unresolved: The simulation is limited to the specific case of modular addition with a simple transformer, and it is unclear how well it would generalize to larger models or different problems.
- What evidence would resolve it: Extensions of the particle simulation model to larger, more complex architectures and problems, along with comparisons of the simulation's predictions to actual training dynamics in these settings.

## Limitations
- Dimensionality Constraint: Study focuses exclusively on 2D embeddings, which may not generalize to higher dimensions where structures could differ significantly
- Simplified Architecture: Analysis uses highly simplified transformer with constant attention, limiting generalizability to full transformer architectures
- Empirical Validation Gaps: Particle simulation lacks rigorous quantitative comparison with trained transformer embeddings and statistical validation

## Confidence
- High Confidence: Empirical observations of grid/circular structures in trained embeddings with clear visualizations and quantitative measures
- Medium Confidence: Clustering and alignment force mechanisms provide plausible explanation, supported by particle simulation but lacking rigorous validation
- Medium Confidence: Weight decay analysis showing its necessity is supported by experiments, though mechanistic explanation could be more rigorous

## Next Checks
1. Dimensionality Scaling Test: Replicate with 3D and 4D embeddings to determine whether clustering/alignment mechanisms generalize beyond 2D
2. Architecture Complexity Test: Replace constant attention with learned attention and add depth to assess mechanism persistence in realistic transformers
3. Cross-Problem Validation: Apply clustering/alignment analysis framework to other modular arithmetic problems and non-modular tasks to test generalizability