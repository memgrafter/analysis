---
ver: rpa2
title: Expressive Power of ReLU and Step Networks under Floating-Point Operations
arxiv_id: '2401.15121'
source_url: https://arxiv.org/abs/2401.15121
tags:
- have
- relu
- parameters
- lemma
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the expressive power of neural networks under
  floating-point operations, which are typically used in practice. The authors show
  that neural networks with ReLU and binary threshold activation functions can memorize
  any finite input/output pairs and approximate any continuous function within an
  arbitrary error, using a similar number of parameters as classical results assuming
  exact mathematical operations.
---

# Expressive Power of ReLU and Step Networks under Floating-Point Operations

## Quick Facts
- arXiv ID: 2401.15121
- Source URL: https://arxiv.org/abs/2401.15121
- Reference count: 26
- Neural networks with ReLU and Step activation functions can memorize any finite input/output pairs and approximate any continuous function within arbitrary error using O(n) and O(ω⁻¹f*(ε)⁻ᵈ) parameters respectively under floating-point arithmetic

## Executive Summary
This paper analyzes the expressive power of neural networks under floating-point operations, which are used in practice rather than idealized mathematical operations. The authors prove that neural networks with ReLU and binary threshold activation functions can memorize any finite input/output pairs and approximate any continuous function within arbitrary error using a similar number of parameters as classical results assuming exact mathematical operations. The analysis covers two types of floating-point representations: Fp with unbounded exponent and Fp,q with finite exponent, encompassing formats like IEEE 754 single-precision and bfloat16.

## Method Summary
The authors analyze expressive power by constructing specific neural network architectures for memorization and universal approximation under floating-point constraints. For memorization, they build indicator functions using Step networks (3 layers) that can distinguish between different input values. For universal approximation, they construct ReLU networks (4 layers) that partition the input space and approximate continuous functions piecewise. The analysis considers two floating-point formats: Fp with p-bit significands and unbounded exponents, and Fp,q with bounded q-bit exponents. The proofs rely on carefully designed network architectures that remain effective despite rounding errors inherent in floating-point arithmetic.

## Key Results
- Memorization: O(n) parameters sufficient for arbitrary n input/output pairs under floating-point arithmetic
- Universal approximation: O(ω⁻¹f*(ε)⁻ᵈ) parameters for ε-error approximation of continuous function f* in d dimensions
- Results hold for both Fp (unbounded exponent) and Fp,q (finite exponent) floating-point formats
- Covers practical floating-point formats including IEEE 754 single-precision and bfloat16

## Why This Works (Mechanism)
The expressive power is maintained under floating-point operations through careful architectural design that accounts for rounding errors. The Step network architecture uses piecewise constant functions that can distinguish between discrete values even with rounding, while the ReLU network partitions input space into small regions where continuous functions can be well-approximated. The key insight is that floating-point rounding errors don't fundamentally change the network's ability to create sharp transitions (for memorization) or fine partitions (for approximation) as long as the precision is sufficient.

## Foundational Learning
- **Floating-point arithmetic**: Understanding rounding modes and error propagation is essential because neural network computations involve many floating-point operations where errors accumulate
  - Why needed: The analysis must account for how floating-point rounding affects network computations
  - Quick check: Verify that basic floating-point operations (+, ×) with rounding to nearest produce results within 0.5 ulp of the exact value

- **Universal approximation theory**: Classical results show shallow networks can approximate continuous functions with O(ω⁻¹ε⁻ᵈ) parameters
  - Why needed: The paper builds on this foundation while showing similar bounds hold under floating-point constraints
  - Quick check: Confirm that classical universal approximation bounds match the paper's floating-point bounds when ω=1 (exact arithmetic)

- **Indicator function construction**: Building functions that output 1 for specific inputs and 0 otherwise is crucial for memorization
  - Why needed: Memorization requires networks to distinguish between different input values exactly
  - Quick check: Verify that the constructed Step network can produce exact 0/1 outputs for small test cases

## Architecture Onboarding
**Component map**: Input -> Step network (3 layers) OR ReLU network (4 layers) -> Output
**Critical path**: The ability to create sharp transitions (Step) or fine partitions (ReLU) despite floating-point rounding errors
**Design tradeoffs**: Shallow networks (3-4 layers) are used for theoretical clarity, but deeper networks might offer different parameter tradeoffs under floating-point constraints
**Failure signatures**: Memorization fails when rounding errors cause different inputs to map to the same floating-point value; approximation fails when partition regions become too coarse due to precision limits
**First experiments**: 
1. Implement floating-point arithmetic operations (addition, multiplication) with rounding to nearest for Fp and Fp,q formats
2. Test Step network memorization on 3-5 input/output pairs with exact equality checks
3. Verify ReLU network approximation of f(x)=x² on [0,1] with ε=0.01 error tolerance

## Open Questions the Paper Calls Out
**Open Question 1**: Does the number of parameters required for universal approximation under floating-point operations increase with deeper networks compared to shallow networks?
- Basis in paper: The paper shows similar parameter bounds for shallow networks under floating-point operations as classical results for shallow networks under exact operations, but doesn't explore deeper networks
- Why unresolved: The paper focuses on shallow networks (3-4 layers) and doesn't investigate how depth affects parameter complexity under floating-point operations
- What evidence would resolve it: Theoretical analysis showing parameter bounds for deep ReLU/Step networks under floating-point operations, or empirical studies comparing shallow vs. deep networks

**Open Question 2**: How does the choice of rounding mode in floating-point operations affect the expressive power of neural networks?
- Basis in paper: The paper assumes "round to nearest (ties to even)" rounding mode, but notes that other rounding modes exist
- Why unresolved: The paper only analyzes one rounding mode and doesn't explore how other rounding modes might impact memorization and universal approximation capabilities
- What evidence would resolve it: Theoretical analysis or empirical studies comparing neural network performance under different rounding modes

**Open Question 3**: How do the expressive power results change for neural networks using activation functions other than ReLU and binary threshold (Step) under floating-point operations?
- Basis in paper: The paper focuses on ReLU and Step activation functions but doesn't explore other common activation functions like sigmoid, tanh, or leaky ReLU
- Why unresolved: The paper only analyzes two specific activation functions and doesn't investigate whether results extend to other activation functions under floating-point operations
- What evidence would resolve it: Theoretical analysis or empirical studies showing memorization and universal approximation capabilities for various activation functions under floating-point arithmetic

## Limitations
- The paper focuses on shallow networks (3-4 layers) and doesn't explore how depth affects parameter complexity under floating-point constraints
- Exact parameter values for the constructed networks in proofs are not fully specified, requiring additional derivation for implementation
- The analysis assumes specific rounding modes and floating-point formats, leaving open questions about how different choices affect expressive power

## Confidence
- **High Confidence**: The theoretical framework for analyzing expressive power under floating-point constraints is sound and builds on established universal approximation theory
- **Medium Confidence**: The O(n) memorization bound follows similar logic to classical results, though floating-point implementation details require careful verification
- **Medium Confidence**: The universal approximation result with O(ω⁻¹f*(ε)⁻ᵈ) parameters represents a meaningful contribution, but practical implementation would need to verify specific network constructions

## Next Checks
1. Implement the exact 3-layer Step network architecture from Theorem 3.1 and verify it can perfectly memorize 3-5 input/output pairs under Fp/q arithmetic
2. Construct the 4-layer ReLU network from Theorem 3.3 and test its ability to approximate f(x)=x² on [0,1] with ε=0.01 error tolerance
3. Benchmark the actual parameter counts against the theoretical O(n) and O(ω⁻¹f*(ε)⁻ᵈ) bounds across different function complexities and approximation accuracies