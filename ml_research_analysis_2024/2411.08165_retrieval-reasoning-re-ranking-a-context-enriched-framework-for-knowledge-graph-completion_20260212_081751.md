---
ver: rpa2
title: 'Retrieval, Reasoning, Re-ranking: A Context-Enriched Framework for Knowledge
  Graph Completion'
arxiv_id: '2411.08165'
source_url: https://arxiv.org/abs/2411.08165
tags:
- entity
- knowledge
- kgr3
- county
- triple
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of knowledge graph completion
  by proposing a context-enriched framework named KGR3. The method combines retrieval
  of supporting triples and contextual information, reasoning via large language models,
  and re-ranking of candidate answers.
---

# Retrieval, Reasoning, Re-ranking: A Context-Enriched Framework for Knowledge Graph Completion

## Quick Facts
- arXiv ID: 2411.08165
- Source URL: https://arxiv.org/abs/2411.08165
- Authors: Muzhi Li; Cehao Yang; Chengjin Xu; Xuhui Jiang; Yiyan Qi; Jian Guo; Ho-fung Leung; Irwin King
- Reference count: 17
- One-line primary result: KGR3 achieves 12.3% and 5.6% absolute Hits@1 improvements on FB15k237 and WN18RR datasets respectively.

## Executive Summary
This paper introduces KGR3, a context-enriched framework for knowledge graph completion that addresses limitations of both embedding-based and text-based approaches. The method combines retrieval of supporting triples and entity contexts, reasoning via large language models, and re-ranking of candidate answers to bridge the semantic gap between KG triples and natural language. Experiments demonstrate consistent improvements across multiple base KGC models, with the best variant achieving significant absolute Hits@1 gains on standard benchmark datasets.

## Method Summary
KGR3 is a three-stage framework that enhances knowledge graph completion by integrating structural and semantic knowledge. The Retrieval module gathers supporting triples and entity contexts from Wikidata, while also collecting candidate answers from base KGC models. The Reasoning module uses LLM in-context learning to generate candidate answers from the retrieved information. The Re-ranking module employs supervised fine-tuning with LoRA adaptation to select the best answer from combined candidates, using hard negative samples to improve discrimination between similar entities.

## Key Results
- Achieves absolute Hits@1 improvements of 12.3% on FB15k237 and 5.6% on WN18RR
- Consistently improves performance across multiple base KGC models (TransE, RotatE, GIE, SimKGC, CoLE, NBF-Net)
- Outperforms existing embedding-based, text-based, and LLM-based approaches on standard KGC benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework bridges the semantic gap between KG triples and natural language by leveraging entity contexts from knowledge bases.
- Mechanism: Entity contexts (labels, descriptions, aliases) are retrieved from Wikidata and incorporated into the LLM's reasoning process, transforming structured triples into natural language that the LLM can process.
- Core assumption: The LLM has sufficient pre-training exposure to the entity contexts retrieved from Wikidata to understand and reason about them.
- Evidence anchors:
  - [abstract]: "Apart from triples, entity contexts (e.g., labels, descriptions, aliases) also play a significant role in augmenting KGs."
  - [section]: "We note that there is a significant semantic gap between structural triples and natural language sentences... we extract relevant contexts related to entities in the query triple and supporting triples from Wikidata knowledge base."
- Break condition: If the Wikidata knowledge base lacks entity contexts for specific entities in the KG, or if the LLM has not been pre-trained on these specific entity contexts, the semantic gap bridging will fail.

### Mechanism 2
- Claim: The Retrieval-Reasoning-Re-ranking pipeline combines structural and semantic knowledge to improve KGC performance beyond either approach alone.
- Mechanism: The Retrieval module gathers supporting triples and entity contexts, the Reasoning module uses LLM with in-context learning to generate candidate answers, and the Re-ranking module fine-tunes LLM to select the best answer from combined candidates.
- Core assumption: The structural knowledge from KG triples and semantic knowledge from entity contexts are complementary and can be effectively combined by LLM-based reasoning.
- Evidence anchors:
  - [abstract]: "KGR3 is composed of three modules... Retrieval, Reasoning, and Re-ranking."
  - [section]: "Motivated by the complementary nature of semantic and structural knowledge, we aim to exploit the candidate answer list generated by the LLM and the base KGC model to compose our final rankings."
- Break condition: If the base KGC model's candidate list does not contain the correct answer, or if the LLM cannot effectively combine the two knowledge sources during re-ranking, performance gains will be limited.

### Mechanism 3
- Claim: Supervised fine-tuning with hard negative samples improves the LLM's ability to distinguish between entities with similar properties.
- Mechanism: The Re-ranking module uses SFT with LoRA adaptation, where training samples are constructed by corrupting triples and sampling hard negative candidates that share the same relation as the ground truth.
- Core assumption: Including hard negative samples that share relations with the ground truth helps the LLM learn to differentiate between similar entities.
- Evidence anchors:
  - [section]: "Incorporating these hard negative samples helps the LLM to distinguish between different entities with the same property, which is crucially important since candidate entities suggested by base KGC models usually yield similar characteristics."
- Break condition: If the hard negative sampling strategy does not effectively capture the challenging distinctions between similar entities, or if the LLM cannot learn from these distinctions during SFT, the re-ranking performance will suffer.

## Foundational Learning

- Concept: Knowledge Graph Structure and Completion
  - Why needed here: Understanding the basic structure of KGs (triples in form of (head, relation, tail)) and the KGC task (predicting missing entities) is fundamental to grasping the problem KGR3 addresses.
  - Quick check question: What is the standard format of a knowledge graph triple, and what does the KGC task aim to accomplish?

- Concept: Embedding-based vs Text-based KGC Methods
  - Why needed here: The paper positions KGR3 as addressing limitations of both embedding-based methods (vulnerability to specious patterns and long-tail entities) and text-based methods (semantic gap between triples and natural language).
  - Quick check question: What are the key limitations of embedding-based and text-based KGC methods that KGR3 aims to overcome?

- Concept: Large Language Model Capabilities (In-context Learning and Fine-tuning)
  - Why needed here: KGR3 relies on LLM's in-context learning for reasoning and supervised fine-tuning (SFT) with LoRA for re-ranking, so understanding these LLM capabilities is essential.
  - Quick check question: How does in-context learning differ from supervised fine-tuning in the context of using LLMs for KGC?

## Architecture Onboarding

- Component map:
  - Retrieval Module: Supporting triple retrieval, textual context retrieval, candidate answer retrieval
  - Reasoning Module: Supporting triple demonstrations, context-aware reasoning
  - Re-ranking Module: Supervised fine-tuning with hard negatives, candidate selection and final ranking

- Critical path: Query triple → Retrieval module (supporting triples + contexts + base KGC candidates) → Reasoning module (LLM generates answers) → Re-ranking module (SFT LLM selects final answer) → Output ranked entities

- Design tradeoffs:
  - Using Wikidata contexts vs. relying solely on KG triples: Provides semantic understanding but depends on external knowledge base availability
  - In-context learning vs. fine-tuning: IL is cheaper but SFT with hard negatives improves discrimination
  - Candidate subset selection vs. full KG ranking: Computationally feasible but upper-bounded by base KGC model's coverage

- Failure signatures:
  - Low Hits@1 despite improvements in Hits@3/10: Suggests correct answers exist in candidate pool but LLM struggles to identify top answer
  - Performance similar to base KGC model: Indicates retrieval or reasoning module failing to add value
  - Significant performance drop on specific relations: May indicate semantic gap not being bridged for those relation types

- First 3 experiments:
  1. Implement and test just the Retrieval module with a simple base KGC model to verify context extraction and candidate generation works
  2. Add the Reasoning module (IL only) and compare against Retrieval-only to measure IL contribution
  3. Add the Re-ranking module (SFT) and compare against Reasoning-only to measure SFT contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does KGR3 perform on KGC tasks under an inductive setting where entities in test triples do not appear in training triples?
- Basis in paper: [explicit] The authors explicitly state that KGR3 cannot handle the KGC task under an inductive setting and plan to tackle unseen entities in future work.
- Why unresolved: The paper only evaluates KGR3 under a transductive setting where test entities are present in training data.
- What evidence would resolve it: Experiments comparing KGR3's performance on datasets with held-out entities that are not present in the training graph.

### Open Question 2
- Question: What is the impact of different entity context sources (beyond Wikidata labels, descriptions, and aliases) on KGR3's performance?
- Basis in paper: [inferred] The paper uses Wikidata contexts but doesn't explore other knowledge bases or context types like images, structured attributes, or cross-lingual descriptions.
- Why unresolved: The authors only extract specific types of contextual information from Wikidata without exploring the full range of available entity contexts.
- What evidence would resolve it: Comparative experiments using different knowledge bases (e.g., DBpedia, YAGO) or different types of entity contexts (e.g., images, structured attributes, cross-lingual descriptions).

### Open Question 3
- Question: How does the performance of KGR3 scale with increasing KG size and entity count?
- Basis in paper: [inferred] While KGR3 is evaluated on FB15k237 and WN18RR, the paper doesn't examine how performance changes with larger KGs or how computational costs scale.
- Why unresolved: The experiments are limited to relatively small benchmark datasets without analysis of scaling behavior.
- What evidence would resolve it: Experiments on progressively larger KGs (e.g., FB15k, YAGO3-10) showing performance trends and computational scaling patterns.

### Open Question 4
- Question: Can KGR3's re-ranking strategy be extended to handle multi-hop reasoning tasks beyond simple KGC?
- Basis in paper: [inferred] The re-ranking mechanism shows effectiveness for KGC, but the paper doesn't explore its applicability to more complex reasoning tasks like path queries or logical rule inference.
- Why unresolved: The evaluation focuses solely on standard KGC metrics without testing the framework's generalization to other reasoning scenarios.
- What evidence would resolve it: Experiments applying KGR3 to multi-hop reasoning benchmarks or complex query answering tasks involving logical compositions of relations.

## Limitations

- Performance upper-bounded by base KGC model's candidate coverage, creating fundamental limitation
- Cannot handle inductive KGC settings with unseen entities, limiting real-world applicability
- Reliance on Wikidata contexts introduces dependency on external knowledge base availability and coverage

## Confidence

- **High confidence**: The framework's core architecture (Retrieval-Reasoning-Re-ranking) is well-specified and demonstrates consistent improvements across multiple base models on both benchmark datasets.
- **Medium confidence**: The semantic gap bridging mechanism via entity contexts is theoretically sound but depends heavily on Wikidata's coverage and the LLM's pre-training exposure to those contexts.
- **Medium confidence**: The SFT with hard negative sampling approach is supported by results but the specific implementation details and sampling strategy remain unclear.

## Next Checks

1. **Ablation study on context sources**: Test KGR3 performance using only KG triples versus Wikidata contexts versus combined sources to quantify the contribution of external context retrieval.
2. **Stress test on long-tail entities**: Evaluate performance degradation on entities with minimal Wikidata contexts or those absent from Wikidata to identify framework limitations.
3. **Cross-dataset generalization**: Apply the trained KGR3 models to different KG datasets or domains to assess whether the semantic gap bridging generalizes beyond the training distribution.