---
ver: rpa2
title: 'Careless Whisper: Speech-to-Text Hallucination Harms'
arxiv_id: '2402.08021'
source_url: https://arxiv.org/abs/2402.08021
tags:
- hallucinations
- audio
- whisper
- aphasia
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a study of hallucinations in OpenAI's Whisper
  speech-to-text API, finding that approximately 1% of transcriptions contain entire
  hallucinated phrases or sentences not present in the audio. Through thematic analysis
  of 312 hallucinated transcriptions, 38% were found to contain explicit harms like
  perpetuating violence, making inaccurate associations, or implying false authority.
---

# Careless Whisper: Speech-to-Text Hallucination Harms

## Quick Facts
- arXiv ID: 2402.08021
- Source URL: https://arxiv.org/abs/2402.08021
- Reference count: 40
- 1% of Whisper transcriptions contain entire hallucinated phrases not present in audio

## Executive Summary
This paper presents a systematic evaluation of OpenAI's Whisper speech-to-text API, revealing that approximately 1% of transcriptions contain entire hallucinated phrases or sentences not present in the source audio. Through thematic analysis of 312 hallucinated transcriptions, the researchers identified explicit harms in 38% of cases, including perpetuating violence, making inaccurate associations, and implying false authority. The study found that hallucinations disproportionately affect individuals with aphasia, occurring at 1.7% versus 1.2% for control speakers. The research suggests that Whisper's generative AI modeling approach and certain speech patterns contribute to these hallucinations, raising serious concerns about the reliability and potential harm of speech-to-text systems for vulnerable populations.

## Method Summary
The study analyzed 13,140 audio segments from the AphasiaBank dataset, running each through OpenAI's Whisper API multiple times to capture non-deterministic hallucinations. Researchers segmented continuous speech into roughly sentence-level utterances (~10 seconds each), compared Whisper transcriptions with ground truth, and identified hallucinated segments using multi-token differences between API runs. Hallucinations were categorized thematically, and logistic regression was used to analyze factors contributing to hallucination rates, particularly focusing on non-vocal durations and speech characteristics.

## Key Results
- Whisper exhibits approximately 1% hallucination rate across all tested audio segments
- Hallucinations disproportionately affect individuals with aphasia (1.7% vs 1.2% for controls)
- 38% of identified hallucinations contained explicit harms including violence, inaccurate associations, or false authority
- Hallucinations are more likely when audio contains longer non-vocal durations, a common symptom of aphasia

## Why This Works (Mechanism)

### Mechanism 1
Whisper hallucinations are caused by generative AI modeling patterns inherited from language models. Whisper's end-to-end architecture over-relies on generative language modeling, leading to hallucinated text even with default sampling temperature. Core assumption: Whisper's training included large-scale data from sources like YouTube containing non-speech content and authority-based language.

### Mechanism 2
Longer non-vocal durations in speech lead to increased hallucinations. Whisper generates hallucinated text when encountering longer pauses or silence, defaulting to generative output instead of waiting for speech. Core assumption: Individuals with aphasia have longer non-vocal durations, making them more susceptible to hallucinations.

### Mechanism 3
Hallucinations are OpenAI-specific and not present in other speech-to-text systems. Whisper's hallucinations result from OpenAI's unique approach to integrating generative AI into speech-to-text, not replicated in competing systems. Core assumption: Competing systems don't use generative language models in the same manner.

## Foundational Learning

- Voice Activity Detection (VAD): Needed to measure non-vocal durations linked to hallucination rates. Quick check: What tool or method can identify periods of silence or low-energy audio in a speech file?
- Thematic coding: Needed to categorize types of harms arising from hallucinations. Quick check: How can you systematically categorize qualitative data to identify recurring themes or patterns?
- Logistic regression: Needed to statistically analyze factors contributing to hallucinations. Quick check: What type of statistical model would you use to predict a binary outcome based on multiple independent variables?

## Architecture Onboarding

- Component map: Whisper API -> VAD tools (PyAnnote, Silero) -> Thematic coding framework -> Logistic regression model
- Critical path: 1. Segment audio into utterances. 2. Run segments through Whisper API. 3. Detect hallucinations using VAD and transcription comparison. 4. Categorize hallucinations thematically. 5. Analyze factors using regression.
- Design tradeoffs: Generative AI increases hallucination risk but improves accuracy; segmenting reduces hallucination risk but may miss context; VAD tools provide insights but add overhead.
- Failure signatures: High hallucination rates in audio with long non-vocal durations; disproportionate rates for speech impairments; inconsistent transcription outputs.
- First 3 experiments: 1. Run audio through Whisper with varying sampling temperatures. 2. Compare Whisper versus non-generative systems for same segments. 3. Analyze correlation between non-vocal durations and hallucination rates.

## Open Questions the Paper Calls Out

### Open Question 1
What is the specific mechanism by which Whisper's generative AI modeling leads to hallucinations, and how does this differ from competing systems? The paper hypothesizes that over-reliance on OpenAI's language modeling advancements causes hallucinations but doesn't prove causation or identify exact technical mechanisms.

### Open Question 2
How do Whisper hallucinations affect downstream applications like hiring, legal proceedings, or medical records, and what are long-term consequences for individuals with aphasia? The paper discusses potential harms but lacks empirical data on actual impacts in real-world applications.

### Open Question 3
Can Whisper's hallucination rate be reduced through model fine-tuning on speech with more disfluencies, and what would be the optimal approach? The paper suggests this as a potential solution but doesn't test it through actual model fine-tuning or experimentation.

## Limitations
- Analysis limited to single aphasia dataset, potentially limiting generalizability to other speech impairments
- Classification relied on human coders without detailed inter-rater reliability metrics
- Measured hallucination rates at segment level, potentially missing hallucinations spanning multiple segments

## Confidence
- High Confidence: 1% overall hallucination rate and disproportionate rates for aphasia speakers
- Medium Confidence: Correlation between non-vocal durations and hallucinations; thematic analysis of harms
- Low Confidence: Hypothesis about generative AI modeling being the specific cause; absence of evidence in competing systems

## Next Checks
1. Obtain technical documentation or conduct controlled experiments to verify whether Whisper's hallucination patterns are caused by generative modeling versus other architectural factors.
2. Expand testing to include speakers with other speech impairments and diverse demographic groups to verify aphasia-specific findings generalize.
3. Conduct systematic testing of additional speech-to-text systems using identical test datasets to determine whether Whisper's hallucination rates are unique.