---
ver: rpa2
title: Sustainable techniques to improve Data Quality for training image-based explanatory
  models for Recommender Systems
arxiv_id: '2407.06740'
source_url: https://arxiv.org/abs/2407.06740
tags:
- data
- training
- explainability
- techniques
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the challenge of improving visual explainability
  in recommender systems while maintaining sustainability. The authors develop three
  novel techniques focusing on data quality: Positive-Unlabeled Learning to reduce
  label noise, transform-based data augmentation, and text-to-image generative data
  augmentation.'
---

# Sustainable techniques to improve Data Quality for training image-based explanatory models for Recommender Systems

## Quick Facts
- arXiv ID: 2407.06740
- Source URL: https://arxiv.org/abs/2407.06740
- Reference count: 23
- Three novel techniques increase performance by ~5% in Recall@10, NDCG@10, and AUC-ROC

## Executive Summary
This work addresses the challenge of improving visual explainability in recommender systems while maintaining sustainability. The authors develop three novel techniques focusing on data quality: Positive-Unlabeled Learning to reduce label noise, transform-based data augmentation, and text-to-image generative data augmentation. These methods are applied to three state-of-the-art explainability models (ELVis, MF-ELVis, BRIE) using real-world restaurant recommendation datasets. The integrated techniques increase performance by approximately 5% in relevant ranking metrics (Recall@10, NDCG@10, AUC-ROC) while maintaining computational sustainability. The PU Learning approach enables more efficient training on refined datasets with reduced noise, while data augmentation addresses cold-start problems for low-activity users.

## Method Summary
The method combines three data quality enhancement techniques: Positive-Unlabeled Learning for selecting reliable negative training examples, transform-based data augmentation, and text-to-image generative augmentation. These are integrated with three explainability models (ELVis, MF-ELVis, BRIE) trained on user-image interaction data from TripAdvisor restaurant recommendation datasets. The PU Learning technique uses a Rocchio classifier with cosine similarity threshold to identify reliable negative examples per user, while data augmentation addresses cold-start problems through geometric transforms and generated images from user reviews.

## Key Results
- Performance improvements of approximately 5% in Recall@10, NDCG@10, and AUC-ROC metrics
- PU Learning reduces training time by using smaller refined datasets with reduced noise
- Computational sustainability maintained as techniques only affect training data preparation, not inference
- Long-term sustainability impact minimal due to relatively small computational overhead compared to inference-time emissions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PU Learning reduces label noise by identifying reliable negative examples for each user
- Mechanism: User-personalized two-step PU Learning approach creates refined negative training sets (RNu) by finding centroid of user's images and filtering out outliers using similarity thresholds
- Core assumption: Positive class is smooth (good explanations are similar to user's own images) and separable from negative class (bad explanations differ from user's images)
- Evidence anchors:
  - [section] "To select reliable negative examples for each user we used discriminators based on a Rocchio classifier... The selection of user-wise reliable negatives RNu of each user is illustrated in Figure 2 and proceeds as follows: 1. Find the centroid of the embedding projections ofu's images... 2. Discriminate reliable negatives through a similarity threshold... 3. Admit unlabelled user-image pairs (u, p) with p /∈ P u to RNu as reliable negative examples"
  - [abstract] "1) selection of reliable negative training examples using Positive-unlabelled Learning"
  - [corpus] Weak evidence - related papers focus on uncertainty quantification and graph explainability, not PU Learning for recommender systems
- Break condition: If the separability assumption fails (user's good explanations are not distinguishable from bad ones), the PU Learning approach will incorrectly classify negatives as positives

### Mechanism 2
- Claim: Data augmentation addresses cold-start problems for low-activity users
- Mechanism: Two approaches - transform-based augmentation (geometric transforms, cutouts, blur) and text-to-image generation from user reviews - enrich training data for users with few uploaded images
- Core assumption: Generated or transformed images are coherent with user preferences and maintain semantic validity as explanations
- Evidence anchors:
  - [section] "We also propose the use of generative models to directly create new good explanation image data for less active users... For each useru with |Pu| < n reviews, we consider the set of textual reviews Ru... Using a generative text-to-image model G, and until an activity threshold|Pu| = n is reached, we iteratively generate images pgen = G(T )"
  - [abstract] "2) transform-based data augmentation, and 3) text-to-image generative-based data augmentation"
  - [corpus] Weak evidence - related papers discuss explainability but not data augmentation for cold-start problems in recommender systems
- Break condition: If generated images don't match user preferences or if transforms create unrealistic representations, the augmented data may harm model performance

### Mechanism 3
- Claim: Computational sustainability is maintained through minimal training overhead
- Mechanism: Techniques only affect training data preparation and model training (not inference), with PU Learning reducing training time by using smaller refined datasets, while data augmentation has manageable overhead compared to inference emissions
- Core assumption: Training-time computational cost is negligible compared to long-term inference emissions
- Evidence anchors:
  - [abstract] "The PU Learning approach enables more efficient training on refined datasets with reduced noise, while data augmentation addresses cold-start problems... the long-term sustainability impact is minimal due to the relatively small computational overhead compared to inference-time emissions"
  - [section] "Data Augmentation is a popular approach to increase the availability of coherent training data without massive, costly data gathering... the consequent lower labelling noise and increased quality of training data leads to more effective and efficient training"
  - [corpus] Weak evidence - related papers discuss explainability but not sustainability metrics or carbon emissions
- Break condition: If inference volume is low or if the techniques significantly increase training computational requirements, the sustainability benefits may be negated

## Foundational Learning

- Concept: Positive-Unlabeled Learning
  - Why needed here: The recommender system data contains only positive examples (images uploaded by users) with no explicit negative examples, creating noisy training data
  - Quick check question: What is the fundamental challenge PU Learning addresses in recommender systems with user-uploaded images?

- Concept: Data Augmentation
  - Why needed here: Most users have few uploaded images, creating cold-start problems that prevent effective model training
  - Quick check question: How does data augmentation help overcome the sparsity problem in user-image recommendation data?

- Concept: Sustainability Metrics
  - Why needed here: The work explicitly measures carbon emissions and computational costs to ensure improvements don't compromise environmental sustainability
  - Quick check question: Why is it important to measure both training and inference emissions when evaluating the sustainability of recommender system techniques?

## Architecture Onboarding

- Component map:
  - User embedding module (maps user IDs to d-dimensional vectors)
  - Image embedding module (pretrained ResNet/ViT for image projection)
  - Similarity computation module (inner product or MLP for user-image compatibility)
  - PU Learning module (user-wise negative example selection)
  - Data augmentation module (transform-based and generative augmentation)
  - Training pipeline (integrates refined datasets with base explainability models)

- Critical path: User ID → User embedding → Image embedding → Similarity computation → Ranking → Explanation output
- Design tradeoffs: PU Learning vs. data augmentation complexity, computational overhead vs. performance gains, transformation realism vs. data diversity
- Failure signatures: Performance degradation when assumptions about data separability or user preferences are violated, increased computational costs outweighing sustainability benefits
- First 3 experiments:
  1. Implement PU Learning module and evaluate negative example selection quality on a small dataset
  2. Test transform-based augmentation on low-activity users and measure performance improvement
  3. Evaluate text-to-image generation quality and its impact on model performance for cold-start users

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would a comprehensive hyperparameter optimization of the explainability models affect the effectiveness of the proposed data quality techniques?
- Basis in paper: [explicit] The authors note that they only re-optimized the number of training epochs used on the datasets refined with their techniques, and acknowledge that "a proper hyper-parameter re-optimization of the used explainability models may [improve] the effectiveness of our techniques."
- Why unresolved: The experiments used existing hyperparameter settings from previous works without full re-optimization for the modified datasets, potentially limiting the observed performance gains.
- What evidence would resolve it: Re-running the experiments with grid search or Bayesian optimization to find optimal hyperparameters for each model-variant combination (ELVis, MF-ELVis, BRIE with PU Learning, transform-based augmentation, and generative augmentation) across all three datasets.

### Open Question 2
- Question: How effective would the proposed techniques be when evaluated on datasets containing both image-based and text-based reviews?
- Basis in paper: [explicit] The authors identify that "the design of PU Learning assumptions beyond single-prototype characterizations of users, which may be needed for users with diverse explanatory tastes" and suggest this as future work, implying current methods may not handle diverse user preferences well.
- Why unresolved: The current evaluation only uses image-based datasets where users upload photos, but many real-world recommendation systems include both images and text reviews.
- What evidence would resolve it: Testing the techniques on datasets that contain both image and text reviews (like the full TripAdvisor dataset mentioned as having 100M reviews per year) to evaluate whether the generative text-to-image augmentation performs better when text reviews are available.

### Open Question 3
- Question: What is the long-term sustainability impact of the proposed techniques when scaled to large-scale recommendation systems with millions of users?
- Basis in paper: [explicit] The authors state that "our techniques do not have effects on model inference" and show that training-time emissions become "negligible compared to inference-time emissions" in their restaurant recommendation context.
- Why unresolved: While the current results show minimal training overhead, the scalability to massive systems with different inference patterns is not established.
- What evidence would resolve it: Measuring the cumulative training and inference emissions across millions of users and items, particularly analyzing whether the generative data augmentation becomes unsustainable at scale due to the computational cost of generating images for low-activity users.

## Limitations

- Limited generalizability across different recommendation domains and model types
- Sustainability analysis lacks concrete carbon emission measurements
- PU Learning parameters and thresholds are not extensively validated across different datasets

## Confidence

- Performance improvements (5% gain): High confidence
- PU Learning effectiveness: Medium confidence
- Sustainability claims: Low-Medium confidence

## Next Checks

1. Validate PU Learning effectiveness across different similarity threshold values and evaluate robustness to varying data distributions
2. Conduct controlled experiments measuring actual carbon emissions during training and inference phases
3. Test the techniques on diverse recommendation domains (e.g., movies, products) to assess generalizability