---
ver: rpa2
title: 'BERTCaps: BERT Capsule for Persian Multi-Domain Sentiment Analysis'
arxiv_id: '2412.05591'
source_url: https://arxiv.org/abs/2412.05591
tags:
- sentiment
- data
- domain
- used
- approaches
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes BERTCaps, a novel deep learning approach for
  Persian multi-domain sentiment analysis. The method combines BERT for instance representation
  and Capsule Network for feature learning, addressing the challenge of poor applicability
  of sentiment models to different domains.
---

# BERTCaps: BERT Capsule for Persian Multi-Domain Sentiment Analysis

## Quick Facts
- arXiv ID: 2412.05591
- Source URL: https://arxiv.org/abs/2412.05591
- Reference count: 15
- BERTCaps achieves 0.9712 accuracy in binary sentiment classification and 0.8509 accuracy in multi-class domain classification on the Digikala dataset

## Executive Summary
This paper introduces BERTCaps, a novel deep learning approach for Persian multi-domain sentiment analysis that combines BERT for contextual representation with Capsule Networks for hierarchical feature learning. The model addresses the challenge of poor applicability of sentiment models across different domains by leveraging BERT's contextualized embeddings while using Capsule Networks to capture spatial relationships and domain-specific features. The approach is evaluated on the Digikala dataset containing 10 domains with both positive and negative polarity, demonstrating superior performance compared to baseline models including CNN-Multi Channel, Character level CNN, NeuroSent, Bi-GRUCapsule, and Bi-IndRNNCapsule.

## Method Summary
BERTCaps combines BERT for instance representation with a Capsule Network structure for feature learning. The model uses BERT to generate contextualized embeddings of input text, which are then processed through an attention-based representation module that aggregates weighted contributions from all BERT hidden states into capsule vectors. These capsule representations are mapped to sentiment and domain probabilities through fully connected layers, and a reconstruction module provides auxiliary supervision by minimizing reconstruction error between original and predicted representations. The model is trained on the Digikala dataset with 50,799 comments across 10 domains, using 80% for training and 20% for testing.

## Key Results
- BERTCaps achieves 0.9712 accuracy in binary sentiment classification
- BERTCaps achieves 0.8509 accuracy in multi-class domain classification
- Outperforms baseline models including CNN-Multi Channel, Character level CNN, NeuroSent, Bi-GRUCapsule, and Bi-IndRNNCapsule

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BERTCaps leverages BERT's contextual embeddings to preserve semantic richness across domains while Capsule Networks capture hierarchical relationships in feature space
- Mechanism: BERT encodes input text into contextualized vectors that maintain syntactic and semantic nuances. These vectors feed into a Capsule Network, which preserves spatial relationships and learns pose parameters for feature transformation. The attention-based representation module ensures each capsule receives weighted contributions from the full BERT output, while the probability module maps capsule representations to sentiment and domain predictions via learned transformations
- Core assumption: Domain-specific linguistic features are sufficiently captured by BERT embeddings for the Capsule Network to differentiate sentiment and domain classes without requiring explicit domain labels during training
- Evidence anchors:
  - [abstract] "In this approach, BERT was used for Instance representation, and Capsule Structure was used to learn the extracted graphs."
  - [section] "BERT was used for feature representation, and Capsule was used for feature learning."
- Break condition: If BERT embeddings lose domain-specific signals due to insufficient training data per domain, the Capsule Network cannot reconstruct meaningful sentiment or domain distinctions, leading to performance degradation

### Mechanism 2
- Claim: The reconstruction module enforces internal consistency by minimizing reconstruction error between original and predicted sentiment/domain representations
- Mechanism: After computing activation probabilities, the model reconstructs sentiment and domain representations by element-wise multiplying capsule representations with their activation probabilities. These reconstructions are then compared to the original inputs using reconstruction loss, encouraging capsules to encode discriminative features
- Core assumption: The reconstructed representations can serve as effective supervisory signals for the capsule network, reinforcing the learning of features that are both predictive and reconstructible
- Evidence anchors:
  - [section] "The Reconstruction module multiplies the activation probability of the capsule ð‘ð‘– by the representation vector of the capsule ð‘£ð‘–, which results in the reconstructed representation vector ð‘Ÿð‘–."
  - [section] "By minimizing the reconstruction error between the reconstructed emotion representation and the original emotion representation and the reconstruction error between the reconstructed domain representation and the original domain representation, we can increase the prediction accuracy of the model."
- Break condition: If reconstruction targets are noisy or poorly aligned with classification objectives, the auxiliary loss may distract from primary task learning, especially in data-scarce domains

### Mechanism 3
- Claim: Attention-based capsule representation dynamically weights BERT outputs, enabling domain-adaptive feature aggregation
- Mechanism: Each capsule computes attention scores over all BERT hidden states using learned weight vectors. The weighted sum forms the capsule's representation, allowing different capsules to focus on distinct aspects of the input text relevant to specific domains or sentiment classes
- Core assumption: Different domains exhibit characteristic feature importance patterns that can be captured by distinct attention weight vectors per capsule
- Evidence anchors:
  - [section] "Let ð‘¤ð‘– be the weight vector for the ð‘–-th capsule. With these interpretations, the attention score ð›¼ð‘– is calculated as follows: ð›¼ð‘– = softmax(ð» â‹… ð‘¤ð‘–)"
  - [section] "By summing the outputs of the hidden layer of the BERT encoder, each capsule obtains a vector representation corresponding to a category of emotions [6]."
- Break condition: If domain differences are subtle or overlapping, attention weights may fail to discriminate, leading to similar capsule representations across domains and poor classification

## Foundational Learning

- Concept: Bidirectional contextual embeddings
  - Why needed here: BERT provides context-aware representations that capture domain-specific linguistic patterns necessary for accurate multi-domain sentiment classification
  - Quick check question: What is the difference between BERT's bidirectional training and traditional unidirectional language models in terms of context capture?

- Concept: Capsule Networks and dynamic routing
  - Why needed here: Capsules preserve spatial relationships and learn pose parameters, enabling hierarchical feature learning that adapts to different domains without fixed convolution filters
  - Quick check question: How do capsule networks differ from CNNs in handling viewpoint changes or domain shifts in text data?

- Concept: Attention mechanisms in deep learning
  - Why needed here: Attention allows the model to selectively focus on relevant parts of BERT's output for each capsule, enabling domain-adaptive feature aggregation
  - Quick check question: In the BERTCaps architecture, how does the attention mechanism contribute to each capsule's representation?

## Architecture Onboarding

- Component map: Text -> BERT Encoder -> Attention-weighted Capsule Representation -> Probability Mapping -> Classification Output
- Critical path: Text â†’ BERT â†’ Attention-weighted Capsule Representation â†’ Probability Mapping â†’ Classification Output
- Design tradeoffs: BERTCaps balances between using pre-trained BERT for rich representations and training capsule-specific parameters for domain adaptation. The reconstruction module adds computational overhead but improves feature learning. Attention weighting increases parameter count but enables domain-specific focus
- Failure signatures:
  - Poor domain classification accuracy may indicate insufficient attention differentiation between domains
  - Sentiment classification failure could suggest BERT embeddings aren't capturing domain-invariant sentiment features
  - High training but low test accuracy may indicate overfitting, especially in domains with limited data
- First 3 experiments:
  1. Ablation study: Remove reconstruction module to assess its contribution to accuracy
  2. Attention visualization: Examine attention weights to verify domain-specific feature focus
  3. Cross-domain evaluation: Test model trained on one domain and evaluated on others to measure domain generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does BERTCaps perform when dealing with unbalanced sentiment classes in multi-domain sentiment analysis?
- Basis in paper: [inferred] The paper mentions that the Digikala dataset is unbalanced, with more positive samples than negative ones, and suggests using data augmentation techniques and class-sensitive functions to handle this challenge
- Why unresolved: The paper does not provide results or a detailed analysis of BERTCaps' performance on unbalanced classes
- What evidence would resolve it: Experimental results comparing BERTCaps' performance on balanced vs. unbalanced datasets, and the effectiveness of proposed data augmentation and class-sensitive techniques

### Open Question 2
- Question: What is the impact of different parameter optimization techniques (e.g., PSO, GWO, IGWO) on the performance of BERTCaps in multi-domain sentiment analysis?
- Basis in paper: [explicit] The paper mentions that using PSO, GWO, and IGWO could be helpful for optimizing the parameter space in future works
- Why unresolved: The paper does not explore or provide results on the impact of these optimization techniques
- What evidence would resolve it: Comparative studies showing the performance of BERTCaps with different parameter optimization techniques applied

### Open Question 3
- Question: How does the performance of BERTCaps compare to other state-of-the-art models in cross-domain sentiment analysis tasks?
- Basis in paper: [explicit] The paper compares BERTCaps with several baseline models but does not mention comparisons with other state-of-the-art cross-domain models
- Why unresolved: The paper does not include comparisons with other recent or advanced cross-domain sentiment analysis models
- What evidence would resolve it: Experimental results comparing BERTCaps with other state-of-the-art cross-domain sentiment analysis models on the same or similar datasets

## Limitations

- Limited Comparative Evaluation: Lacks comparison with state-of-the-art transformer-based approaches like XLNet, RoBERTa, or domain-adapted BERT variants
- Domain Generalization Unverified: No evidence about the model's ability to generalize to unseen domains or perform well when trained on subset of domains and tested on novel ones
- Persian-Specific Constraints: Approach is tailored for Persian language with specific preprocessing using Hazm, effectiveness for other languages remains unknown

## Confidence

- High Confidence: The core claim that BERTCaps achieves high accuracy (0.9712 for sentiment, 0.8509 for domain classification) is well-supported by the experimental results on the Digikala dataset
- Medium Confidence: The assertion that combining BERT with Capsule Networks provides benefits over using BERT alone is plausible given the architecture design, but direct ablation studies comparing BERTCaps to BERT-only approaches are not provided
- Low Confidence: Claims about the specific advantages of Capsule Networks over other neural architectures (CNNs, RNNs) for capturing hierarchical relationships in this domain are not rigorously tested against alternatives

## Next Checks

1. **Ablation Study Implementation** - Remove the Capsule Network component and compare BERTCaps performance against a pure BERT baseline to quantify the actual contribution of the Capsule Network to the reported accuracy gains

2. **Cross-Domain Generalization Test** - Train the model on 7-8 domains and evaluate on the remaining 2-3 unseen domains to assess whether the attention-based capsule representation truly enables domain adaptation or merely memorizes domain-specific patterns

3. **Reconstruction Module Analysis** - Conduct controlled experiments where the reconstruction loss is varied or removed entirely to determine its actual impact on classification performance and whether it justifies the additional computational overhead