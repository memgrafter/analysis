---
ver: rpa2
title: 'Beyond Surface Structure: A Causal Assessment of LLMs'' Comprehension Ability'
arxiv_id: '2411.19456'
source_url: https://arxiv.org/abs/2411.19456
tags:
- structure
- deep
- surface
- causal
- adce
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) truly
  comprehend deep structure or merely rely on surface structure for problem-solving.
  The authors propose a novel causal mediation analysis framework to quantify LLMs'
  understanding of deep and surface structures.
---

# Beyond Surface Structure: A Causal Assessment of LLMs' Comprehension Ability

## Quick Facts
- **arXiv ID**: 2411.19456
- **Source URL**: https://arxiv.org/abs/2411.19456
- **Reference count**: 40
- **Primary result**: Introduces ADCE and AICE metrics to quantify LLMs' deep vs surface structure comprehension, showing most LLMs exhibit growing deep structure understanding with scale

## Executive Summary
This paper addresses a fundamental question in LLM evaluation: do models truly understand deep structure or merely exploit surface patterns? The authors propose a novel causal mediation analysis framework that quantifies LLMs' comprehension of deep structure (direct causal effect) versus surface structure (indirect causal effect). Due to non-estimability of original causal effects, they develop approximated DCE (ADCE) and approximated ICE (AICE) as practical surrogates. Experiments across five tasks reveal that closed-source models like GPT rely more on deep structure while open-source models like Llama are more surface-sensitive, with sensitivity decreasing as model scale increases.

## Method Summary
The paper formulates deep structure comprehension as direct causal effect (DCE) and surface structure comprehension as indirect causal effect (ICE) within a causal mediation analysis framework. Since original DCE and ICE are non-estimable, the authors propose approximated DCE (ADCE) and approximated ICE (AICE) as proxies. The method involves generating intervention data through Mask and Rephrase strategies, running LLM inference on these interventions, and computing ADCE/AICE using indicator functions. ADCE is shown to measure both sufficiency and necessity of deep structure changes in output variations, offering a bidirectional evaluation that outperforms accuracy in spurious correlation scenarios.

## Key Results
- Most LLMs exhibit deep structure comprehension ability that grows with prediction accuracy
- Closed-source models (GPT) rely more on deep structure while open-source models (Llama) are more surface-sensitive
- Surface sensitivity decreases as model scale increases
- ADCE outperforms accuracy in spurious correlation scenarios by better reflecting reliance on deep structure over spurious surface features

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: ADCE isolates and quantifies deep structure comprehension independent of surface structure effects
- **Mechanism**: Uses causal mediation analysis to measure direct causal effect of deep structure on outputs while controlling for surface structure through approximated ICE
- **Core assumption**: Deep structure changes cause output variations independent of surface structure
- **Evidence anchors**: [abstract], [section 3.2]
- **Break condition**: If deep and surface structures are fundamentally inseparable

### Mechanism 2
- **Claim**: ADCE provides bidirectional evaluation measuring both sufficiency and necessity of deep structure changes
- **Mechanism**: Combines probability of sufficiency and probability of necessity to capture necessary and sufficient relationships
- **Core assumption**: Model outputs are monotonic with respect to deep structure changes
- **Evidence anchors**: [abstract], [section 3.4]
- **Break condition**: If model outputs are non-monotonic with respect to deep structure changes

### Mechanism 3
- **Claim**: ADCE outperforms accuracy in spurious correlation scenarios
- **Mechanism**: Measures actual causal relationship between deep structure and outputs, avoiding overfitting to surface patterns
- **Core assumption**: Models can achieve high accuracy through spurious correlations without true deep structure understanding
- **Evidence anchors**: [abstract], [section 4.5]
- **Break condition**: If spurious correlations do not exist or models do not rely on them

## Foundational Learning

- **Concept**: Causal mediation analysis
  - Why needed here: To decompose total effect of inputs on outputs into direct (deep structure) and indirect (surface structure) effects
  - Quick check question: What are the three key assumptions (positivity, consistency, sequential ignorability) required for valid causal mediation analysis?

- **Concept**: Potential outcomes framework
  - Why needed here: To define causal effects as expected differences between outcomes under different intervention conditions
  - Quick check question: How does the potential outcomes framework handle the counterfactual nature of causal effects?

- **Concept**: Monotonicity assumption
  - Why needed here: To enable computation of probability of sufficiency and necessity from observable data
  - Quick check question: What happens to the ADCE calculation if the monotonicity assumption is violated?

## Architecture Onboarding

- **Component map**: Input processing -> Deep structure extraction -> Surface structure extraction -> Intervention generation -> LLM inference -> Effect calculation
- **Critical path**: 
  1. Generate intervention data (TE and AICE)
  2. Run LLM inference on intervention data
  3. Calculate ADCE and AICE metrics
  4. Compare across models and tasks
- **Design tradeoffs**: 
  - Tradeoff between intervention strength and surface structure similarity
  - Balance between comprehensive evaluation and computational cost of multiple inference runs
- **Failure signatures**:
  - Zero ADCE values indicating lack of deep structure understanding
  - Negative ADCE values suggesting surface structure dependency
  - High variance in ADCE across similar samples indicating intervention quality issues
- **First 3 experiments**:
  1. Run baseline evaluation on random weight model to verify ADCE detects lack of deep structure comprehension
  2. Compare ADCE and accuracy on synthetic data with known causal effects to validate metric accuracy
  3. Test intervention strategy effectiveness by measuring surface structure similarity between TE and AICE samples

## Open Questions the Paper Calls Out
None

## Limitations
- Separability assumption between deep and surface structures may not hold in real-world scenarios
- Monotonicity assumption required for ADCE computation may be violated in non-monotonic relationships
- Intervention strategies are task-specific and may not generalize across diverse domains

## Confidence

**High confidence** in theoretical framework and mathematical formulation of ADCE. The causal mediation analysis approach is well-grounded in existing literature.

**Medium confidence** in practical implementation and experimental results. Methodology is sound but relies on specific intervention strategies and approximation methods.

**Low confidence** in generalizability across diverse language tasks and model architectures. Current evaluation focuses on limited tasks and known strong-performing model families.

## Next Checks

1. **Robustness to intervention quality**: Systematically vary intervention strength and measure ADCE stability to assess sensitivity to intervention quality versus genuine comprehension differences.

2. **Cross-domain generalization**: Apply framework to naturalistic language tasks beyond curated benchmarks to evaluate performance in less controlled settings.

3. **Model-agnostic validation**: Test framework with broader range of model architectures including smaller models and specialized domain models to establish consistent ADCE performance across diverse model types.