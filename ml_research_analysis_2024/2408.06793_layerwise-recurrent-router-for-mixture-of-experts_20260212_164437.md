---
ver: rpa2
title: Layerwise Recurrent Router for Mixture-of-Experts
arxiv_id: '2408.06793'
source_url: https://arxiv.org/abs/2408.06793
tags:
- rmoe
- router
- smoe
- layer
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the parameter inefficiency in large language
  models using Mixture-of-Experts (MoE) architecture. It introduces the Layerwise
  Recurrent Router for Mixture-of-Experts (RMoE), which uses a Gated Recurrent Unit
  (GRU) to establish dependencies between routing decisions across consecutive layers,
  leveraging historical routing information to improve token-expert combinations.
---

# Layerwise Recurrent Router for Mixture-of-Experts

## Quick Facts
- **arXiv ID**: 2408.06793
- **Source URL**: https://arxiv.org/abs/2408.06793
- **Reference count**: 40
- **Primary result**: RMoE consistently outperforms baseline MoE models across various model sizes, architectures, datasets, and training settings with negligible additional computational costs.

## Executive Summary
The paper addresses parameter inefficiency in large language models using Mixture-of-Experts (MoE) architecture by introducing the Layerwise Recurrent Router (RMoE). RMoE uses a Gated Recurrent Unit (GRU) to establish dependencies between routing decisions across consecutive layers, leveraging historical routing information to improve token-expert combinations. The method is orthogonal to existing MoE designs and can be seamlessly integrated with them. Extensive empirical evaluations show that RMoE consistently outperforms baseline models across various model sizes, architectures, datasets, and training settings, with negligible additional computational costs.

## Method Summary
RMoE introduces a layerwise recurrent router that captures historical routing information using a shared GRU across layers. Each token's hidden state is projected to a lower dimension, combined with the previous layer's GRU output, and processed through a shared GRU to produce the current GRU output, which is then used by the router to select experts. This approach establishes dependencies between routing decisions across consecutive layers, enabling cross-layer information sharing that improves expert selection and diversity. The method is designed to be orthogonal to existing MoE architectures, allowing seamless integration with other MoE designs like XMoE.

## Key Results
- RMoE consistently outperforms baseline MoE models across multiple model sizes (0.91B to 15B parameters)
- Performance gains are attributed to effective cross-layer information sharing and improved expert selection diversity
- RMoE achieves better balance between exploration and exploitation in expert selection, leading to moderate gating scores
- The additional computational costs are negligible, with only ~3.5M additional parameters for 0.91B model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layerwise recurrence enables cross-layer information sharing that improves expert selection.
- Mechanism: The GRU in RMoE captures routing decisions from previous layers and feeds them into the current layer's router, providing historical context for expert selection.
- Core assumption: Router decisions at different layers are interdependent and can benefit from shared routing history.
- Evidence anchors: [abstract]: "RMoE leverages a Gated Recurrent Unit (GRU) to establish dependencies between routing decisions across consecutive layers." [section]: "When ablated on the layerwise recurrence in RMoE, the performance largely drops, even worse than SMoE. Both results suggest that the layerwise recurrence is the main contributor."

### Mechanism 2
- Claim: Recurrent gradient flow improves router optimization through additional backpropagation paths.
- Mechanism: The GRU introduces an extra gradient path from language modeling loss through hidden states to router parameters, providing richer gradient signals.
- Core assumption: Additional gradient paths help overcome optimization challenges in sparse MoE training.
- Evidence anchors: [section]: "While RMoE-NP's MI is largely smaller than RMoE, it still surpasses the three baseline methods. The reason can be the shared GRU in Eq. 5."

### Mechanism 3
- Claim: Layerwise recurrence achieves better exploration-exploitation balance in expert selection.
- Mechanism: By incorporating historical routing information, RMoE encourages moderate flat gating scores rather than sharp or random distributions.
- Core assumption: Balanced exploration-exploitation leads to more effective expert utilization.
- Evidence anchors: [section]: "RMoE, with unique cross-layer information sharing, has high entropy for many tokens while low entropy for a few tokens. These moderate gating scores can achieve a better balance between exploration and exploitation."

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: RMoE is a router enhancement specifically for MoE models, understanding MoE fundamentals is essential.
  - Quick check question: How does sparse MoE activation reduce computational cost compared to dense models?

- Concept: Recurrent Neural Networks (RNNs) and Gated Recurrent Units (GRUs)
  - Why needed here: RMoE uses GRUs to capture sequential dependencies between layer-wise routing decisions.
  - Quick check question: What is the key difference between standard RNNs and GRUs that makes GRUs suitable for this application?

- Concept: Mutual Information (MI) and information theory concepts
  - Why needed here: The paper uses MI to measure cross-layer information sharing between routers.
  - Quick check question: How does mutual information between two random variables quantify their statistical dependence?

## Architecture Onboarding

- Component map:
  Input hidden state → Layerwise projector (separate for each layer) → GRU (shared across layers) → Router → Expert selection → Output

- Critical path: Hidden state → Projector → GRU → Router → Expert selection
  - Performance bottleneck: GRU computation, though shown to be negligible in practice
  - Memory impact: Additional projector and GRU parameters (~3.5M for 0.91B model)

- Design tradeoffs:
  - Separate projectors per layer vs shared projector: Layerwise projectors perform better but increase parameter count
  - GRU dimension choice: 128 dimensions shown effective, larger dimensions don't improve performance
  - Orthogonality to other MoE methods: RMoE can be combined with XMoE, etc., providing flexibility

- Failure signatures:
  - Poor performance when removing layerwise recurrence (RMoE-NP)
  - Degradation when detaching GRU hidden states from gradient computation
  - Overfitting when using excessively large GRU dimensions

- First 3 experiments:
  1. Ablation study: Replace RMoE with standard linear router to establish baseline performance
  2. Cross-layer MI analysis: Measure mutual information between router distributions across layers
  3. Entropy distribution analysis: Compare gating score entropy across different router configurations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Layerwise Recurrent Router (RMoE) affect the exploration-exploitation trade-off during different training phases of Mixture-of-Experts models?
- Basis in paper: [explicit] The paper discusses that RMoE enables moderate flat gating scores, which can achieve a better balance between exploration and exploitation. It also mentions that the router should actively explore more new expert combinations at the early stage of learning.
- Why unresolved: While the paper shows that RMoE has higher entropy in gating scores compared to baselines, it doesn't provide a detailed analysis of how this balance changes throughout the training process or how it impacts the model's performance at different stages.
- What evidence would resolve it: Detailed experiments tracking the entropy of gating scores and model performance across different training epochs would help understand the dynamics of exploration-exploitation trade-off in RMoE.

### Open Question 2
- Question: What is the impact of different recurrent cell types (e.g., GRU, LSTM, simple RNN) on the performance of the Layerwise Recurrent Router in Mixture-of-Experts models?
- Basis in paper: [explicit] The paper mentions that the GRU router performs best among different recurrent cell types tested, but it doesn't provide a comprehensive comparison of how different cell types affect the model's performance.
- Why unresolved: The paper only briefly mentions the comparison between GRU, RNN, and LSTM, but doesn't delve into the specific reasons why GRU performs better or how other recurrent cell types might affect the model's performance.
- What evidence would resolve it: A more detailed comparison of different recurrent cell types, including their impact on model performance, training dynamics, and cross-layer information sharing, would provide insights into the optimal choice of recurrent cell for RMoE.

### Open Question 3
- Question: How does the Layerwise Recurrent Router affect the model's ability to generalize to unseen data or tasks?
- Basis in paper: [inferred] The paper focuses on the performance of RMoE on specific tasks and datasets, but doesn't explicitly address its generalization capabilities to new, unseen data or tasks.
- Why unresolved: While the paper demonstrates improved performance on the tested tasks, it doesn't provide evidence of how RMoE's cross-layer information sharing and enhanced expert selection generalize to broader or more diverse datasets and tasks.
- What evidence would resolve it: Experiments testing RMoE's performance on a wide range of tasks, including those not seen during training, and comparing its generalization capabilities to baseline models would provide insights into its ability to transfer knowledge across different domains.

## Limitations

- The ablation studies are primarily conducted on smaller 0.91B parameter models, with scaling behavior to truly massive models remaining an open question
- The paper claims RMoE is "orthogonal to existing MoE designs" but demonstrates compatibility only through brief mention of XMoE integration rather than comprehensive experiments
- The attribution of gains to specific mechanisms (cross-layer information sharing, improved expert diversity) is supported by proxy metrics but doesn't directly prove causal relationships to improved performance

## Confidence

**High Confidence**: The core claim that layerwise recurrence improves routing decisions and model performance is well-supported by controlled ablation studies showing consistent gains across multiple datasets and model sizes.

**Medium Confidence**: The attribution of gains to specific mechanisms (cross-layer information sharing, improved expert diversity, better exploration-exploitation balance) is supported by proxy metrics like mutual information and entropy analysis, but these don't directly prove causal relationships to improved performance.

**Low Confidence**: The claim about orthogonal compatibility with other MoE methods lacks comprehensive validation, and the assertion that RMoE achieves "better balance between exploration and exploitation" is inferred from entropy patterns rather than directly measured or proven to improve task performance.

## Next Checks

**Check 1**: Perform a direct comparison of RMoE against other modern router architectures (e.g., PCoR, Top2Gating, RoutedAttention) on the same benchmarks with identical training configurations.

**Check 2**: Conduct an experiment isolating the exploration-exploitation effects by training models with controlled entropy levels (using temperature scaling on gating scores) to determine whether the moderate entropy distribution in RMoE directly causes the performance improvements.

**Check 3**: Scale RMoE to truly large models (e.g., 70B+ parameters) and evaluate whether the performance gains scale proportionally, diminish, or disappear at scale.