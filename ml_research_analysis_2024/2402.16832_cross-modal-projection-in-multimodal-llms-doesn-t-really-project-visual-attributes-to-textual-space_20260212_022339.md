---
ver: rpa2
title: Cross-Modal Projection in Multimodal LLMs Doesn't Really Project Visual Attributes
  to Textual Space
arxiv_id: '2402.16832'
source_url: https://arxiv.org/abs/2402.16832
tags:
- domain-specific
- image
- projection
- mllm
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether fine-tuning multimodal large language
  models (MLLMs) like LLaVA-1.5 on domain-specific tasks (agriculture, textures, dermatology,
  humanitarian) improves the projection layer's ability to extract relevant visual
  features. Through experiments with 4 datasets under 2 fine-tuning strategies (projection-only
  vs.
---

# Cross-Modal Projection in Multimodal LLMs Doesn't Really Project Visual Attributes to Textual Space

## Quick Facts
- arXiv ID: 2402.16832
- Source URL: https://arxiv.org/abs/2402.16832
- Reference count: 11
- Primary result: Fine-tuning MLLMs improves performance, but domain-specific visual attributes are predominantly modeled by LLM parameters rather than the projection layer

## Executive Summary
This study investigates whether fine-tuning multimodal large language models (MLLMs) like LLaVA-1.5 on domain-specific tasks improves the projection layer's ability to extract relevant visual features. Through experiments with 4 datasets under 2 fine-tuning strategies (projection-only vs. end-to-end), the authors find that while MLLM performance improves after fine-tuning, the post-projection image representations do not become richer in domain-specific features. Instead, the domain-specific visual attributes are predominantly modeled by the LLM parameters themselves, even when only the projection layer is fine-tuned. This suggests that the projection layer primarily serves to interface with the LLM rather than actively extracting domain-specific visual features.

## Method Summary
The study evaluates LLaVA-1.5 on four domain-specific datasets (agriculture, textures, dermatology, humanitarian) using two fine-tuning strategies: fine-tuning only the cross-modal projection MLP (projection-only) or fine-tuning both projection and LLM parameters end-to-end. Each strategy was applied for 1 epoch with default hyperparameters. Performance was measured using macro-averaged F1 score and accuracy, while domain-specific richness was estimated by training independent MLPs on post-projection representations.

## Key Results
- End-to-end fine-tuning (FT-E2E) consistently outperforms projection-only fine-tuning (FT-Proj) across all datasets
- Post-projection representations show no improvement in domain-specific feature richness despite fine-tuning
- Domain-specific visual capabilities are primarily modeled by LLM parameters rather than the projection layer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The projection layer does not learn to extract domain-specific visual features even when fine-tuned.
- Mechanism: During fine-tuning, the projection layer adapts to facilitate better use of frozen LLM parameters for domain-specific tasks, but does not capture domain-specific attributes itself. The domain-specific visual capabilities are modeled by the LLM parameters, whether frozen or not.
- Core assumption: The projection layer primarily serves to interface with the LLM rather than actively extracting domain-specific visual features.
- Evidence anchors:
  - [abstract] "Our results indicate that the domain-specific visual attributes are modeled by the LLM, even when only the projection is fine-tuned."
  - [section] "This indicates that as MLLMs are fine-tuned to classify domain-specific images, the identification of domain-specific image attributes occurs in the LLM parameters, whether frozen or not."
  - [corpus] Weak evidence - no direct corpus support found for this specific mechanism.
- Break condition: If the projection layer were to show improved domain-specific richness after fine-tuning, or if the LLM parameters were shown to not be responsible for domain-specific visual capabilities.

### Mechanism 2
- Claim: The LLM parameters can model visual data with minimal assistance from the cross-modal projection.
- Mechanism: The LLM has inherent multimodal capabilities, allowing it to detect visual attributes even with minimal feature extraction from the projection layer. The projection layer's role is to facilitate communication between visual and textual representations rather than extracting meaningful visual features.
- Core assumption: Deep neural networks can be inherently multimodal, and LLMs can model visual data without extensive visual feature extraction.
- Evidence anchors:
  - [abstract] "Our results indicate that the domain-specific visual attributes are modeled by the LLM, even when only the projection is fine-tuned."
  - [section] "More broadly, our results add to the existing evidence that deep neural networks can be inherently multimodal (Goh et al., 2021; Schwettmann et al., 2023), and LLMs could model visual data with minimal assistance from the cross-modal projection."
  - [corpus] Weak evidence - no direct corpus support found for this specific mechanism.
- Break condition: If the LLM parameters were shown to be insufficient for modeling visual data without extensive visual feature extraction from the projection layer.

### Mechanism 3
- Claim: Fine-tuning the entire MLLM (projection + LLM) is more effective than fine-tuning the projection alone for domain-specific tasks.
- Mechanism: The greater representational space of the entire MLLM (approximately 7B parameters) compared to the projection alone (approximately 20M parameters) allows for more effective learning of domain-specific visual capabilities.
- Core assumption: A larger representational space enables better learning of domain-specific features.
- Evidence anchors:
  - [abstract] "The greater effectiveness of the FT-E2E can be attributed to greater representational space ( ~ 7B) over FT-Proj (~ 20M)."
  - [section] "With these observations, next, we focus on investigating the role of projection in capturing domain-specific image attributes."
  - [corpus] Weak evidence - no direct corpus support found for this specific mechanism.
- Break condition: If fine-tuning the projection alone were shown to be as effective as fine-tuning the entire MLLM for domain-specific tasks.

## Foundational Learning

- Concept: Multimodal Large Language Models (MLLMs)
  - Why needed here: Understanding the architecture and components of MLLMs is crucial for interpreting the results of this study.
  - Quick check question: What are the two major modules of current open-source MLLMs?

- Concept: Cross-modal projection
  - Why needed here: The study focuses on the role of the cross-modal projection layer in MLLMs, making it essential to understand its function and limitations.
  - Quick check question: What is the primary role of the cross-modal projection layer in MLLMs according to this study?

- Concept: Domain-specific fine-tuning
  - Why needed here: The study investigates the effects of domain-specific fine-tuning on MLLMs, so understanding this concept is crucial for interpreting the results.
  - Quick check question: What are the two fine-tuning strategies used in this study to improve domain-specific visual capabilities of MLLMs?

## Architecture Onboarding

- Component map:
  Image encoder (CLIP) -> Cross-modal projection (MLP) -> LLM (LLaMA-2-7B) -> Text encoder

- Critical path:
  1. Image is encoded by CLIP
  2. Image features are projected to align with LLM space
  3. LLM processes projected image representation and text tokens
  4. LLM generates output

- Design tradeoffs:
  - Large representational space of LLM vs. small projection layer
  - Freezing vs. fine-tuning LLM parameters
  - General-purpose vs. domain-specific capabilities

- Failure signatures:
  - Poor domain-specific performance despite fine-tuning
  - Lack of improvement in post-projection representation richness
  - Overfitting to specific domain at the cost of general capabilities

- First 3 experiments:
  1. Fine-tune only the projection layer on a domain-specific dataset and evaluate performance
  2. Fine-tune the entire MLLM (projection + LLM) on the same dataset and compare performance
  3. Train an independent MLP on the post-projection representation to estimate domain-specific richness and compare across different fine-tuning strategies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the projection layer in MLLMs serve any purpose beyond providing an interface to the LLM, or could the vision encoder embeddings be directly input to the LLM?
- Basis in paper: [explicit] The authors find that domain-specific visual attributes are predominantly modeled by the LLM parameters, even when only the projection layer is fine-tuned, suggesting the projection primarily serves as an interface rather than extracting domain-specific features.
- Why unresolved: The paper only tests one type of projection (MLP) and doesn't explore whether different projection architectures or direct LLM input could perform better at extracting domain-specific visual features.
- What evidence would resolve it: Comparative experiments testing different projection architectures (e.g., linear layers, attention mechanisms) and direct LLM input with CLIP embeddings, measuring both domain-specific performance and post-projection feature richness.

### Open Question 2
- Question: Are there specific types of visual features or concepts that are better extracted by the projection layer versus the LLM parameters in MLLMs?
- Basis in paper: [inferred] The paper shows that domain-specific visual attributes are modeled by the LLM rather than the projection, but doesn't investigate whether certain types of visual features might be better handled by the projection.
- Why unresolved: The study focuses on overall domain-specific performance and feature richness without analyzing which specific visual attributes are being extracted by each component.
- What evidence would resolve it: Detailed analysis of which visual features/concepts are captured by the projection versus LLM using techniques like concept activation vectors or feature attribution methods.

### Open Question 3
- Question: How does the size and capacity of the projection layer relative to the LLM affect its ability to extract domain-specific visual features?
- Basis in paper: [inferred] The paper notes that the projection layer has ~20M parameters while the LLM has ~7B parameters, and that the projection captures fewer domain-specific features, but doesn't explore whether increasing projection capacity would improve feature extraction.
- Why unresolved: The study uses a fixed projection architecture without varying its size or capacity relative to the LLM to test its impact on feature extraction.
- What evidence would resolve it: Experiments with projection layers of varying sizes and capacities, comparing their ability to extract domain-specific features while controlling for other factors.

### Open Question 4
- Question: Do MLLMs develop multimodal neurons for domain-specific visual concepts when fine-tuned, similar to the multimodal neurons found in CLIP?
- Basis in paper: [explicit] The discussion section mentions existing literature on multimodal neurons in neural networks and notes that the study shows acquired domain-specific visual capabilities rely on LLM parameters.
- Why unresolved: The paper doesn't investigate the internal mechanisms by which the LLM develops domain-specific visual capabilities, such as the emergence of multimodal neurons.
- What evidence would resolve it: Neuron-level analysis of the LLM before and after fine-tuning to identify multimodal neurons that respond to domain-specific visual concepts.

## Limitations

- The study only evaluates one MLLM architecture (LLaVA-1.5) and four specific domain datasets, limiting generalizability
- Feature richness is measured indirectly through MLP training performance rather than direct feature attribution analysis
- Fine-tuning duration was relatively short (1 epoch), potentially missing longer-term adaptation patterns

## Confidence

- **High Confidence:** The empirical finding that end-to-end fine-tuning consistently outperforms projection-only fine-tuning across all datasets
- **Medium Confidence:** The conclusion that domain-specific visual attributes are predominantly modeled by LLM parameters rather than the projection layer
- **Low Confidence:** The broader claim about inherent multimodality of deep neural networks based on these results

## Next Checks

1. **Cross-Architecture Validation:** Repeat the fine-tuning and evaluation experiments using different MLLM architectures (e.g., GPT-4V, Flamingo, BLIP-2) to determine whether the observed pattern of LLM-dominated domain-specific modeling is architecture-dependent or a general phenomenon.

2. **Extended Training Analysis:** Conduct fine-tuning experiments with multiple epochs (5-10 epochs) and varied learning rates to determine whether the projection layer's ability to extract domain-specific features improves with longer training, potentially revealing delayed adaptation patterns.

3. **Feature Attribution Study:** Perform direct feature attribution analysis (e.g., Integrated Gradients, SHAP values) on the projection layer outputs to quantify the contribution of different visual features to domain-specific classification decisions, providing more direct evidence about what the projection layer captures.