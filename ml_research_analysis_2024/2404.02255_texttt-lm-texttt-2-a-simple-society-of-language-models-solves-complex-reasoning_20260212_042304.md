---
ver: rpa2
title: '$\texttt{LM}^\texttt{2}$: A Simple Society of Language Models Solves Complex
  Reasoning'
arxiv_id: '2404.02255'
source_url: https://arxiv.org/abs/2404.02255
tags:
- question
- solver
- decomposer
- reasoning
- verifier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'LM2 proposes a multi-LLM coordination framework that modularizes
  reasoning into three specialized models: a solver, a decomposer, and a verifier.
  The decomposer identifies key concepts and generates step-by-step subproblems tailored
  to the solver''s responses, while the verifier provides fine-grained feedback to
  guide the reasoning process.'
---

# $\texttt{LM}^\texttt{2}$: A Simple Society of Language Models Solves Complex Reasoning

## Quick Facts
- arXiv ID: 2404.02255
- Source URL: https://arxiv.org/abs/2404.02255
- Authors: Gurusha Juneja; Subhabrata Dutta; Tanmoy Chakraborty
- Reference count: 30
- LM2 achieves state-of-the-art performance across multiple reasoning benchmarks, outperforming the best baseline by 8.1% on MATH dataset

## Executive Summary
LM2 introduces a multi-LLM coordination framework that modularizes complex reasoning into three specialized models: a solver, a decomposer, and a verifier. Using GPT-3.5 as the solver and finetuned LLaMA-2 models for decomposition and verification, LM2 achieves state-of-the-art performance across multiple reasoning benchmarks. The framework demonstrates strong generalization to out-of-domain tasks, improving over baselines by 9.7% on MedQA and 7.71% on JEEBench. LM2's structured reasoning template and verifier-guided decomposition enable more efficient token usage compared to prior methods.

## Method Summary
LM2 separates complex reasoning into three specialized models: a solver generates solutions, a decomposer generates step-by-step subproblems and identifies key concepts, and a verifier provides fine-grained feedback on solver errors. The framework uses GPT-3.5 as a frozen solver and finetuned LLaMA-2 models for decomposition and verification. The decomposer learns through policy optimization to coordinate with the solver and verifier, generating subproblems that adapt based on intermediate results and feedback. A structured reasoning template coordinates the three models through a defined input-output framework, enabling more efficient token usage compared to prior methods.

## Key Results
- LM2 outperforms the best baseline by 8.1% in absolute accuracy on the MATH dataset
- Achieves strong generalization to out-of-domain tasks, improving over baselines by 9.7% on MedQA and 7.71% on JEEBench
- Ablation studies show verifier and concept generation are critical for performance, particularly on out-of-distribution tasks
- Structured reasoning template and verifier-guided decomposition enable more efficient token usage compared to prior methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modular coordination between solver, decomposer, and verifier improves reasoning performance.
- Mechanism: LM2 separates reasoning into three specialized models: solver generates solutions, decomposer generates step-by-step subproblems and identifies key concepts, verifier provides fine-grained feedback on solver errors. This modular structure enables targeted improvements at each reasoning stage.
- Core assumption: Different reasoning capabilities (conceptual understanding, procedural execution, error detection) can be effectively separated into specialized models rather than handled by a single monolithic model.
- Evidence anchors:
  - [abstract] "LM2 modularizes the decomposition, solution, and verification into three different language models"
  - [section] "LM2 is built upon three separate LMs, each dedicated to three different components of complex multistep reasoning"
  - [corpus] Weak - corpus mentions related multi-agent approaches but doesn't directly validate LM2's specific modular separation claim
- Break condition: If any component cannot effectively specialize or coordinate with others, performance degrades significantly

### Mechanism 2
- Claim: Verifier-guided feedback enables adaptive subproblem generation.
- Mechanism: The decomposer generates each subproblem depending on the solver's answers to prior subproblems and verifier feedback. This creates a dynamic reasoning chain that adapts based on intermediate results rather than following a fixed decomposition plan.
- Core assumption: Solver performance on subproblems can be reliably assessed to guide subsequent decomposition decisions.
- Evidence anchors:
  - [abstract] "depending upon the feedback from the verifier, the reasoning context is constructed using the subproblems and the solutions"
  - [section] "the decomposer generates each subproblem depending on the solver's answers to prior subproblems, along with the verifier's feedback"
  - [corpus] Weak - corpus shows related adaptive reasoning but doesn't specifically validate verifier-guided adaptation
- Break condition: If verifier feedback is inaccurate or decomposer cannot effectively use feedback, the adaptive process fails

### Mechanism 3
- Claim: Structured reasoning template improves token efficiency and performance.
- Mechanism: LM2 employs a novel structured answering template that coordinates the three models through a defined input-output framework. This template enables more efficient token usage compared to prior methods by reducing redundant generation.
- Core assumption: A predefined structured template can effectively coordinate multiple specialized models better than ad-hoc prompting.
- Evidence anchors:
  - [abstract] "LM2's structured reasoning template and verifier-guided decomposition enable more efficient token usage compared to prior methods"
  - [section] "we make use of a structured, step-by-step input-output framework"
  - [corpus] Weak - corpus mentions related structured approaches but doesn't specifically validate LM2's template efficiency
- Break condition: If the template constrains necessary reasoning flexibility or introduces coordination overhead

## Foundational Learning

- Concept: Policy gradient methods for training decomposer coordination
  - Why needed here: The decomposer needs to learn how to generate subproblems that work with the solver's capabilities and verifier feedback, which requires optimization beyond supervised learning
  - Quick check question: What reward function would encourage the decomposer to generate subproblems that lead to correct final answers?

- Concept: Fine-grained error classification for verification
  - Why needed here: The verifier needs to provide specific feedback types (conceptual, computational, procedural, etc.) to guide the decomposer effectively
  - Quick check question: How would you design a classification system to distinguish between conceptual and procedural errors in mathematical reasoning?

- Concept: Modular reasoning decomposition
  - Why needed here: Complex reasoning tasks benefit from breaking down into subproblems rather than attempting direct solution
  - Quick check question: What criteria would you use to determine when a problem should be decomposed versus solved directly?

## Architecture Onboarding

- Component map: Question → Decomposer (concepts + subproblem) → Solver (sub-answer) → Verifier (error classification) → Decomposer (next subproblem or final answer)
- Critical path: Question → Decomposer (concepts + subproblem) → Solver (sub-answer) → Verifier (error classification) → Decomposer (next subproblem or final answer)
- Design tradeoffs:
  - Using frozen GPT-3.5 as solver preserves world knowledge but limits customization
  - Two-stage finetuning (supervised then policy) adds training complexity but improves coordination
  - Fine-grained verifier classification provides better feedback but requires more training data
- Failure signatures:
  - Poor concept generation → solver lacks necessary background knowledge
  - Verifier misclassifications → decomposer receives incorrect feedback signals
  - Token inefficiency → coordination overhead outweighs benefits
- First 3 experiments:
  1. Test individual components: run solver alone, decomposer alone, and verifier alone on benchmark problems to establish baseline capabilities
  2. Test simple coordination: implement basic solver-decomposer loop without verifier to validate basic decomposition works
  3. Test full pipeline on simple problems: run complete LM2 on straightforward math problems to verify all components coordinate correctly before scaling to complex tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LM2 compare to using a single large language model like GPT-4 for complex reasoning tasks?
- Basis in paper: [explicit] The paper mentions that existing methods typically use either well-curated prompting techniques on large LLMs like GPT-4 or finetuning smaller LLMs, but LM2 proposes a novel approach using three separate LMs coordinated via policy learning.
- Why unresolved: While the paper shows LM2 outperforms baselines on various datasets, it doesn't directly compare its performance to using a single powerful LLM like GPT-4 for the same tasks. This comparison would help quantify the benefits of the multi-LLM approach over a monolithic one.
- What evidence would resolve it: An experiment comparing LM2's performance on the MATH, JEEBench, and MedQA datasets against GPT-4 using similar prompting or finetuning techniques would provide direct evidence of the relative effectiveness of the two approaches.

### Open Question 2
- Question: How does the performance of LM2 scale with the size of the language models used for the decomposer, solver, and verifier components?
- Basis in paper: [inferred] The paper uses GPT-3.5 as the solver and finetuned LLaMA-2 models for the decomposer and verifier, but doesn't explore how the performance changes with different model sizes.
- Why unresolved: Understanding the impact of model size on each component's performance could provide insights into the optimal architecture for LM2 and guide decisions about resource allocation in practical applications.
- What evidence would resolve it: Experiments varying the size of the decomposer, solver, and verifier models (e.g., using smaller or larger LLaMA models, different GPT models) and measuring the impact on accuracy across the MATH, JEEBench, and MedQA datasets would reveal how performance scales with model size.

### Open Question 3
- Question: How does LM2's performance compare to other approaches that use multiple specialized models for complex reasoning, such as DSP (Demonstrate-Search-Predict)?
- Basis in paper: [explicit] The paper mentions DSP as a related work that relies on coordination between a generative LM and a retrieval model, but doesn't provide a direct comparison to LM2's performance.
- Why unresolved: A direct comparison would help establish the relative strengths and weaknesses of LM2 compared to other multi-model approaches and provide a clearer picture of its position in the landscape of complex reasoning techniques.
- What evidence would resolve it: An experiment comparing LM2's performance to DSP on the MATH, JEEBench, and MedQA datasets using similar evaluation metrics would provide direct evidence of their relative effectiveness.

### Open Question 4
- Question: How does the verifier's fine-grained feedback mechanism contribute to LM2's improved performance compared to binary correct/incorrect feedback?
- Basis in paper: [explicit] The paper introduces a verifier that provides nuanced feedback with nine different classes of mistakes, contrasting it with prior methods that use binary feedback.
- Why unresolved: While the paper mentions the benefits of fine-grained feedback, it doesn't provide quantitative evidence of its contribution to the overall performance improvement. Understanding this contribution would help justify the added complexity of the multi-class feedback system.
- What evidence would resolve it: An ablation study comparing LM2's performance with and without the fine-grained feedback (e.g., using only binary feedback) on the MATH, JEEBench, and MedQA datasets would quantify the impact of the nuanced feedback mechanism.

### Open Question 5
- Question: How does LM2 handle out-of-distribution tasks that require knowledge beyond mathematical reasoning, such as common sense or world knowledge?
- Basis in paper: [explicit] The paper demonstrates LM2's performance on out-of-domain tasks like MedQA and JEEBench Chemistry, but doesn't provide a detailed analysis of how it handles tasks requiring diverse types of knowledge.
- Why unresolved: Understanding LM2's capabilities and limitations when dealing with various types of knowledge beyond mathematical reasoning would help assess its potential applicability to a wider range of complex reasoning tasks.
- What evidence would resolve it: Experiments testing LM2 on a diverse set of out-of-distribution tasks that require different types of knowledge (e.g., common sense reasoning, world knowledge, logical reasoning) and analyzing its performance on each would reveal its strengths and weaknesses in handling various knowledge domains.

## Limitations
- Performance improvements come with increased computational overhead from coordinating three separate models
- Two-stage finetuning approach for decomposer and verifier adds training complexity without clear justification
- Exact hyperparameters and training procedures for fine-tuned LLaMA-2 models are unspecified

## Confidence
- High confidence: The modular framework design and basic coordination mechanism are well-specified and reproducible
- Medium confidence: The claimed performance improvements are supported by results, but the comparison baselines and training details are not fully specified
- Medium confidence: The token efficiency claims require careful validation as they depend on specific implementation details not fully disclosed

## Next Checks
1. Component Isolation Test: Run each component (solver, decomposer, verifier) independently on benchmark problems to establish baseline capabilities and identify potential coordination bottlenecks
2. Cross-Domain Transfer: Evaluate LM2 on reasoning tasks from completely different domains (e.g., legal reasoning, social science analysis) to test generalization beyond STEM benchmarks
3. Resource Efficiency Analysis: Measure actual token usage, latency, and computational costs compared to baseline approaches to verify the claimed efficiency improvements hold in practice