---
ver: rpa2
title: SCaRL- A Synthetic Multi-Modal Dataset for Autonomous Driving
arxiv_id: '2405.17030'
source_url: https://arxiv.org/abs/2405.17030
tags:
- radar
- data
- dataset
- scarl
- lidar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SCaRL provides the first multi-modal dataset with synchronized
  synthetic data from coherent lidar and MIMO radar sensors, addressing the lack of
  complete sensor suites in existing autonomous driving datasets. The dataset uses
  CARLA simulator to generate synchronized data from RGB, semantic/instance, and depth
  cameras; radar raw data and range-Doppler-Azimuth/Elevation maps; and 3D point clouds/2D
  maps from coherent lidar.
---

# SCaRL- A Synthetic Multi-Modal Dataset for Autonomous Driving

## Quick Facts
- arXiv ID: 2405.17030
- Source URL: https://arxiv.org/abs/2405.17030
- Reference count: 0
- Primary result: First multi-modal dataset with synchronized synthetic data from coherent lidar and MIMO radar sensors for autonomous driving

## Executive Summary
SCaRL introduces the first synthetic multi-modal dataset featuring synchronized data from coherent lidar and MIMO radar sensors, addressing the lack of complete sensor suites in existing autonomous driving datasets. Generated using the CARLA simulator, SCaRL provides 140,000 time-synchronized frames from 6 sensor suites around an ego vehicle in 7 diverse scenarios. The dataset enables training and validation of autonomous driving solutions while reducing the need for extensive real-world annotated datasets and improving training convergence through transfer learning.

## Method Summary
The SCaRL dataset leverages the CARLA simulator to generate synthetic multi-modal sensor data from an ego vehicle equipped with 6 sensor suites. Custom sensor implementations for coherent lidar and MIMO FMCW radar are built on CARLA's ray-casting physics engine. The dataset includes RGB, semantic/instance, and depth cameras; raw radar data and range-Doppler-Azimuth/Elevation maps; and 3D point clouds/2D maps from coherent lidar with Doppler information. Data frames are time-synchronized and aligned across all sensor modalities, covering diverse urban scenarios with dynamic traffic conditions.

## Key Results
- First dataset to include synthetic synchronized data from coherent lidar and MIMO radar sensors
- 140,000 time-synchronized data frames from complete sensor suites (RGB, semantic/depth cameras, lidar, and radar)
- Enables training and validation of autonomous driving solutions while reducing real-world annotation costs
- Supports transfer learning from simulation to real-world data for improved training convergence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic multi-modal data from synchronized sensors improves deep learning model robustness and training convergence for autonomous driving perception tasks.
- Mechanism: The dataset provides synchronized, high-diversity sensor data covering complete sensor suites in complex dynamic scenarios, eliminating the need for costly real-world annotation and sensor calibration while enabling transfer learning from simulation to real-world data.
- Core assumption: The simulated sensor data sufficiently captures the statistical properties and physical characteristics of real-world sensor outputs, and the synchronization is accurate enough to support sensor fusion algorithms.
- Evidence anchors:
  - [abstract] "SCaRL provides synchronized Synthetic data from RGB, semantic/instance, and depth Cameras; Range-Doppler-Azimuth/Elevation maps and raw data from Radar; and 3D point clouds/2D maps of semantic, depth and Doppler data from coherent Lidar."
  - [section] "SCaRL is the first dataset to include synthetic synchronized data from coherent Lidar and MIMO radar sensors."
  - [corpus] Weak evidence - related papers discuss multi-modal datasets but don't directly address the specific synchronized coherent lidar and MIMO radar combination or the simulation-based transfer learning benefits.
- Break condition: If the simulation model fails to accurately represent real-world sensor physics (e.g., material reflection properties, multi-path propagation, noise characteristics), or if the synchronization errors exceed the tolerance required for effective sensor fusion algorithms.

### Mechanism 2
- Claim: Using CARLA simulator with custom sensor implementations enables generation of diverse, complex, and dynamic scenarios that would be expensive or dangerous to capture in real-world measurements.
- Mechanism: The CARLA platform provides customizable urban layouts, dynamic actors (vehicles, pedestrians), and environmental conditions that can be programmatically controlled to generate varied traffic scenarios. The custom sensor implementations (coherent lidar, MIMO FMCW radar) are built on CARLA's ray-casting physics engine to produce realistic sensor outputs.
- Core assumption: CARLA's physics engine and asset library accurately represent real-world urban driving environments and target behaviors, and the custom sensor implementations correctly model the underlying physical principles of coherent lidar and MIMO FMCW radar systems.
- Evidence anchors:
  - [section] "SCaRL is a large dataset based on the CARLA Simulator, which provides data for diverse, dynamic scenarios and traffic conditions."
  - [section] "CARLA provides open digital assets (urban layouts, buildings, vehicles, pedestrians, street signs) that can be used freely."
  - [section] "To simulate this lidar, we use the rotating lidar implemented in CARLA. These sensors obtain depth and semantic information from the casted ray at the point of incidence on the target."
  - [corpus] Weak evidence - related papers mention simulation platforms but don't specifically address CARLA's capabilities for coherent lidar and MIMO radar simulation.
- Break condition: If CARLA's physics engine has significant limitations in modeling real-world phenomena (e.g., weather effects, material properties, dynamic target behaviors) or if the custom sensor implementations have fundamental modeling errors that compromise data quality.

### Mechanism 3
- Claim: The dataset's large size (140,000 time-synchronized frames) and complete sensor suite coverage enable effective training and validation of sensor fusion algorithms while reducing overfitting risk.
- Mechanism: The extensive dataset provides sufficient data diversity across multiple sensor modalities and viewpoints, enabling neural networks to learn robust feature representations that generalize across different scenarios and sensor configurations. The complete sensor suite allows development of algorithms that leverage complementary information from all available sensors.
- Core assumption: The dataset's diversity and size are sufficient to capture the full range of real-world driving scenarios and sensor variations, and the sensor synchronization maintains temporal alignment across all modalities.
- Evidence anchors:
  - [abstract] "SCaRL is a large dataset based on the CARLA Simulator, which provides data for diverse, dynamic scenarios and traffic conditions."
  - [section] "The SCaRL dataset consists of 140000 time synchronized and aligned data frames from 6x RGB, 6x semantic/instance segmentation, 6x depth cameras, 6x lidar, and 6x MIMO FMCW radar sensors."
  - [section] "SCaRL provides synchronized sensor data covering multiple viewpoints by enabling customizable sensor placement at arbitrary locations on a pseudo-real ego vehicle."
  - [corpus] Weak evidence - related papers discuss dataset size but don't specifically address the benefits of complete sensor suite coverage for sensor fusion algorithm development.
- Break condition: If the dataset's diversity is insufficient to cover critical edge cases or if the sensor synchronization introduces temporal misalignments that degrade sensor fusion performance.

## Foundational Learning

- Concept: FMCW radar signal processing and MIMO array processing
  - Why needed here: The dataset includes raw radar data and range-Doppler-Azimuth/Elevation maps that require understanding of FMCW radar principles and MIMO processing techniques for proper interpretation and algorithm development
  - Quick check question: What is the relationship between beat frequency, target range, and chirp slope in an FMCW radar system?

- Concept: Coherent lidar physics and signal processing
  - Why needed here: The dataset includes coherent lidar point clouds with Doppler information, requiring understanding of FMCW optical carrier principles and signal processing for range, velocity, and intensity estimation
  - Quick check question: How does the angle of incidence affect the reflected power in a coherent lidar system, and how is this modeled in the simulation?

- Concept: Sensor synchronization and calibration
  - Why needed here: The dataset emphasizes synchronized data collection across multiple sensor types, requiring understanding of timing alignment, coordinate transformations, and calibration procedures for multi-modal sensor fusion
  - Quick check question: What are the key challenges in synchronizing data from cameras, lidars, and radars operating at different scan rates and with different latencies?

## Architecture Onboarding

- Component map:
  Data generation pipeline: CARLA simulator → custom sensor modules → synchronization → data transformation → storage
  Sensor implementations: RGB/semantic/depth cameras, coherent lidar (with Doppler), MIMO FMCW radar
  Data processing: Ray casting physics engine, signal processing for radar/lidar, coordinate transformations, noise modeling
  Storage format: Time-synchronized frames with multiple sensor modalities per frame

- Critical path:
  1. CARLA scenario setup and execution
  2. Custom sensor data acquisition
  3. Synchronization and alignment
  4. Data transformation and noise injection
  5. Storage and indexing

- Design tradeoffs:
  - Simulation accuracy vs. computational efficiency: Higher fidelity physics models improve data realism but increase processing time
  - Sensor diversity vs. dataset size: Including more sensor types increases data complexity but may reduce the number of scenarios that can be processed within resource constraints
  - Noise modeling vs. data cleanliness: Adding realistic noise improves transfer learning but may complicate algorithm development

- Failure signatures:
  - Synchronization errors: Temporal misalignment between sensor modalities, visible as inconsistent object positions across different sensor types
  - Physics model inaccuracies: Unrealistic sensor outputs, such as incorrect Doppler shifts or range measurements that don't match expected values
  - Data quality issues: Missing data, corrupted frames, or inconsistent coordinate transformations

- First 3 experiments:
  1. Verify sensor synchronization by checking temporal alignment of static objects across different sensor modalities in a simple scenario
  2. Validate radar signal processing by comparing simulated range-Doppler maps with analytical expectations for simple target configurations
  3. Test data consistency by loading and visualizing a small subset of frames across all sensor modalities to ensure proper coordinate transformations and data integrity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the quantitative impact of synthetic data domain shift on deep learning model performance when transferring from SCaRL to real-world datasets?
- Basis in paper: [explicit] The paper mentions that synthetic data is subject to assumptions like AWGN and linear motion models, requiring domain adaptation when using SCaRL to train models for real-world inference.
- Why unresolved: The paper acknowledges domain adaptation is needed but does not provide quantitative results on performance degradation or the effectiveness of different domain adaptation techniques.
- What evidence would resolve it: Empirical studies comparing model performance on real-world datasets after training on SCaRL versus training on real-world data, along with ablation studies on different domain adaptation techniques.

### Open Question 2
- Question: How does the performance of sensor fusion algorithms trained on SCaRL compare to those trained on real-world multi-modal datasets in terms of accuracy and robustness?
- Basis in paper: [explicit] The paper states that SCaRL encourages the development of sensor fusion algorithms but does not provide any performance comparisons with real-world datasets.
- Why unresolved: The paper focuses on dataset generation methodology rather than empirical evaluation of algorithms trained on the dataset.
- What evidence would resolve it: Benchmark studies comparing sensor fusion algorithms trained on SCaRL versus real-world datasets across standard metrics like object detection accuracy, tracking performance, and robustness to challenging conditions.

### Open Question 3
- Question: What is the optimal balance between simulation complexity (e.g., modeling multipath propagation, target materials) and computational efficiency for generating high-quality synthetic radar data?
- Basis in paper: [inferred] The paper mentions future work will include modeling target material, multi-path propagation, and free-path loss, suggesting current limitations in simulation realism.
- Why unresolved: The paper acknowledges these limitations but does not explore the trade-offs between simulation fidelity and computational cost or determine what level of detail is necessary for effective model training.
- What evidence would resolve it: Systematic studies varying levels of simulation complexity and measuring their impact on downstream model performance, along with computational cost analysis to identify the optimal complexity level.

## Limitations
- Limited validation of simulation accuracy against real-world sensor measurements
- Potential domain gap between synthetic and real sensor data requiring domain adaptation
- Computational complexity of high-fidelity sensor simulations

## Confidence
- High confidence: The dataset provides synchronized multi-modal data from 6 sensor suites covering RGB, semantic/depth cameras, lidar, and radar
- Medium confidence: The CARLA-based simulation can generate diverse and complex urban scenarios
- Low confidence: The synthetic data quality is sufficient for training robust autonomous driving perception systems without extensive domain adaptation

## Next Checks
1. **Synchronization Accuracy Verification**: Perform a quantitative analysis of the temporal alignment between different sensor modalities by measuring the cross-correlation of signals from static objects in controlled scenarios, ensuring the synchronization error is below the threshold required for effective sensor fusion algorithms.

2. **Radar Signal Processing Validation**: Compare the simulated FMCW radar range-Doppler maps with analytical predictions for canonical scenarios (single point target, multiple stationary targets, single moving target) to verify that the custom radar implementation correctly models the underlying physics and signal processing chain.

3. **Domain Gap Assessment**: Conduct a feature distribution analysis comparing SCaRL synthetic data with real-world sensor data (where available) to quantify the domain shift and assess the potential impact on transfer learning performance for object detection and classification tasks.