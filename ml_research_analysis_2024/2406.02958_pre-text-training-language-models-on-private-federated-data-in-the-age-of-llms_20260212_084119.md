---
ver: rpa2
title: 'PrE-Text: Training Language Models on Private Federated Data in the Age of
  LLMs'
arxiv_id: '2406.02958'
source_url: https://arxiv.org/abs/2406.02958
tags:
- data
- private
- arxiv
- pre-text
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes PrE-Text, a method for generating differentially
  private (DP) synthetic text data to replace on-device training in federated learning
  settings. The core idea is to use a multi-round evolution process with DP noise
  and mask-filling models to generate high-quality synthetic data, followed by an
  expansion phase using LLMs to generate more training samples.
---

# PrE-Text: Training Language Models on Private Federated Data in the Age of LLMs

## Quick Facts
- arXiv ID: 2406.02958
- Source URL: https://arxiv.org/abs/2406.02958
- Reference count: 40
- One-line primary result: PrE-Text achieves better language model performance on private federated data than on-device training, with 9× fewer rounds, 6× less client computation, and 100× less communication per round under practical privacy budgets.

## Executive Summary
This paper introduces PrE-Text, a method for generating differentially private (DP) synthetic text data to replace on-device training in federated learning settings. The core idea is to use a multi-round evolution process with DP noise and mask-filling models to generate high-quality synthetic data, followed by an expansion phase using LLMs to generate more training samples. The paper shows that training small models on PrE-Text synthetic data outperforms on-device training under practical privacy budgets (ε=1.29, ε=7.58), achieving this with 9× fewer rounds, 6× less client computation per round, and 100× less communication per round. Additionally, finetuning large language models (LLMs) on PrE-Text synthetic data improves LLM performance on private data across the same privacy budgets. The results suggest that training on DP synthetic data can be a better option than training models on-device on private distributed data.

## Method Summary
PrE-Text generates DP synthetic text data through a two-phase process: (1) iterative DP synthetic seed collection using Private Evolution (PE) adapted for text, and (2) single-shot synthetic seed expansion using large language models (LLMs) to generate more training samples. The method starts with an initial population of samples, then iteratively applies the Variation function using a mask-filling model (RoBERTa-large) and Selection function with DP noise to evolve the population. The surviving samples are then expanded using an LLM (LLaMA-2-7B) to generate additional synthetic data. This synthetic data is used to train small language models, which are shown to outperform on-device training in terms of accuracy and efficiency under practical privacy regimes.

## Key Results
- PrE-Text achieves better next-token prediction accuracy than on-device training under privacy budgets ε=1.29 and ε=7.58.
- Training small models on PrE-Text synthetic data requires 9× fewer rounds, 6× less client computation, and 100× less communication per round compared to on-device training.
- Finetuning large language models (LLMs) on PrE-Text synthetic data improves LLM performance on private data across the same privacy budgets.

## Why This Works (Mechanism)
PrE-Text works by generating high-quality DP synthetic data that captures the essential patterns and structure of the private federated data without exposing individual user information. The iterative evolution process with DP noise ensures that the synthetic data maintains diversity and fidelity to the original data distribution. The use of LLMs for expansion allows for efficient generation of large amounts of synthetic data that can be used to train or finetune models effectively. By replacing on-device training with synthetic data training, PrE-Text significantly reduces the communication, computation, and round requirements while still achieving competitive or better performance.

## Foundational Learning
- **Federated Learning**: A distributed machine learning approach where models are trained on decentralized data without exchanging raw data. Needed to understand the context of training models on private, distributed user data. Quick check: Verify that the paper mentions the use of federated learning techniques and the challenges associated with on-device training.
- **Differential Privacy**: A privacy-preserving technique that adds noise to data or model updates to protect individual user information. Needed to ensure that the synthetic data generation process does not leak private information. Quick check: Confirm that the paper discusses the use of DP mechanisms, such as Gaussian or Laplace noise, to protect user privacy.
- **Synthetic Data Generation**: The process of creating artificial data that mimics the statistical properties of real data. Needed to understand how PrE-Text generates DP synthetic data for training models. Quick check: Ensure that the paper explains the two-phase process of synthetic seed collection and expansion using LLMs.
- **Language Models**: Neural networks trained on large text corpora to understand and generate human-like text. Needed to comprehend the models used for mask-filling, expansion, and finetuning in PrE-Text. Quick check: Verify that the paper mentions the specific language models used, such as RoBERTa-large and LLaMA-2-7B.
- **Privacy Budgets (ε)**: A parameter that quantifies the amount of privacy loss in DP mechanisms. Needed to understand the trade-off between model performance and privacy in PrE-Text. Quick check: Confirm that the paper discusses the privacy budgets used (ε=1.29, ε=7.58) and their impact on model performance.

## Architecture Onboarding
- **Component Map**: Private federated datasets -> PrE-Text (PE + Expand) -> DP synthetic data -> Small model training / LLM finetuning -> Performance evaluation
- **Critical Path**: The critical path in PrE-Text involves the iterative evolution process using PE to generate high-quality synthetic seeds, followed by the expansion phase using LLMs to create more training samples. The quality of the synthetic data directly impacts the performance of the downstream models.
- **Design Tradeoffs**: PrE-Text trades off some model performance for significant gains in communication, computation, and round efficiency compared to on-device training. The use of LLMs for expansion allows for efficient generation of large amounts of synthetic data but may introduce some biases or limitations compared to the original data distribution.
- **Failure Signatures**: Poor quality of synthetic data leading to subpar model performance, inefficient communication and computation due to incorrect implementation of secure aggregation or DP noise addition, and potential biases introduced by the initial population of samples or the LLM expansion process.
- **First Experiments**: 1) Implement and validate the Variation function's mask-filling process to ensure synthetic data quality. 2) Experiment with different privacy budgets and dataset sizes to test the robustness of the efficiency gains. 3) Conduct a user study to evaluate the realism and diversity of the generated synthetic data compared to the original private data.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the performance of PrE-Text scale with the number of synthetic samples generated during the expansion phase, and what is the optimal number of samples for different types of datasets?
- Basis in paper: [explicit] The paper mentions that the quality of the downstream model mostly improves log-linearly with the amount of synthetic data, but performance starts getting diminishing returns at around 1M samples for the CODE dataset, and even decreases in performance at 2M samples.
- Why unresolved: The paper does not provide a comprehensive analysis of the scaling behavior across all datasets or a theoretical framework for determining the optimal number of synthetic samples.
- What evidence would resolve it: A detailed study across a wider range of datasets and a theoretical analysis of the trade-offs between synthetic data size and model performance.

### Open Question 2
- Question: What are the potential biases introduced by the initial population of samples in PrE-Text, and how do these biases affect the quality and diversity of the generated synthetic data?
- Basis in paper: [inferred] The paper states that the initial population of samples can come from various sources as long as they do not contain private information, but does not discuss the potential biases this might introduce.
- Why unresolved: The impact of the initial population on the synthetic data generation process is not thoroughly investigated, and there is no mention of methods to mitigate potential biases.
- What evidence would resolve it: An analysis of how different initial populations affect the synthetic data quality and diversity, and experiments testing the impact of biased initial populations.

### Open Question 3
- Question: How does PrE-Text compare to other synthetic data generation methods in terms of computational efficiency and scalability when dealing with larger datasets or more complex models?
- Basis in paper: [explicit] The paper highlights the computational and communication efficiency of PrE-Text compared to on-device training baselines, but does not compare it to other synthetic data generation methods.
- Why unresolved: The paper focuses on the advantages of PrE-Text over on-device training but does not provide a comprehensive comparison with other synthetic data generation approaches.
- What evidence would resolve it: A comparative study of PrE-Text with other synthetic data generation methods in terms of computational efficiency and scalability, including experiments with larger datasets and more complex models.

## Limitations
- The paper's results are based on simulated federated settings, which may not fully capture the complexities of real-world deployments.
- The confidence in the LLM finetuning results is Medium, as the improvements are demonstrated on private data but the generalizability to other domains or tasks is unclear.
- The paper does not provide a comprehensive comparison of PrE-Text with other synthetic data generation methods in terms of computational efficiency and scalability.

## Confidence
- **High**: Efficiency gains (9× fewer rounds, 6× less client computation, 100× less communication) compared to on-device training.
- **Medium**: Performance improvements over on-device training under specific privacy budgets and dataset splits.
- **Medium**: Improvements in LLM finetuning on private data, but generalizability to other domains or tasks is unclear.

## Next Checks
1. Implement and validate the Variation function's mask-filling process to ensure synthetic data quality.
2. Experiment with different privacy budgets and dataset sizes to test the robustness of the efficiency gains.
3. Conduct a user study to evaluate the realism and diversity of the generated synthetic data compared to the original private data.