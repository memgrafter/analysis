---
ver: rpa2
title: Zero-Shot End-To-End Spoken Question Answering In Medical Domain
arxiv_id: '2406.05876'
source_url: https://arxiv.org/abs/2406.05876
tags:
- audio
- whisper
- zero-shot
- speech
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a novel zero-shot end-to-end approach for
  spoken question answering (SQA) in the medical domain. The method leverages large
  language models (LLMs) to directly process speech inputs, bypassing the need for
  separate speech recognition and language model components.
---

# Zero-Shot End-To-End Spoken Question Answering In Medical Domain

## Quick Facts
- arXiv ID: 2406.05876
- Source URL: https://arxiv.org/abs/2406.05876
- Reference count: 0
- Zero-shot SQA approach achieves comparable accuracy to cascade models using 14.7× fewer parameters

## Executive Summary
This study introduces a novel zero-shot end-to-end approach for spoken question answering in the medical domain. The method leverages large language models to directly process speech inputs, bypassing the need for separate speech recognition and language model components. A new benchmark dataset of 8 medical tasks with 48 hours of synthetic audio was created from existing textual question-answering corpora. The proposed end-to-end models, including Whisper, CLAP, and SpeechGPT, were evaluated against traditional cascade systems.

## Method Summary
The study proposes a zero-shot end-to-end SQA approach that directly processes speech inputs without separate ASR and LLM components. A synthetic audio dataset was created from medical QA corpora (MMLU, MedQA, MedMCQA) using text-to-speech synthesis. End-to-end models (Whisper, CLAP, SpeechGPT) were implemented with audio-text entailment scoring to select answers from multiple-choice options. These were compared against cascade baselines using ASR followed by LLM. The approach requires only 14.7× fewer parameters than traditional systems while maintaining comparable accuracy.

## Key Results
- End-to-end approach improves average accuracy by 0.5% compared to cascade models
- Requires up to 14.7 times fewer resources than combined 1.3B parameter LLM with 1.55B parameter ASR model
- Analysis reveals Whisper concentrates information in final layers while HuBERT and WavLM integrate information from broader layer ranges

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: End-to-end audio-text entailment bypasses cascade error accumulation.
- **Mechanism**: By encoding both audio and text in the same model space, the system directly compares modality representations rather than relying on ASR transcription that may introduce noise.
- **Core assumption**: The model can align audio and text embeddings without external supervision.
- **Evidence anchors**:
  - [abstract] "Our study introduces a novel zero-shot SQA approach, compared to traditional cascade systems."
  - [section] "Our proposed zero-shot audio-text entailment method is integrated into the four previously mentioned models, aiming to assess the likelihood of a textual sequence matching an audio recording."
- **Break condition**: If audio-text embedding spaces are misaligned, the entailment score will not correlate with true semantic match, leading to degraded accuracy.

### Mechanism 2
- **Claim**: Resource efficiency comes from shared encoder parameters rather than separate ASR+LLM.
- **Mechanism**: A single model processes raw audio and directly outputs answer probabilities, eliminating the need for a separate speech recognizer and large language model.
- **Core assumption**: The end-to-end model's parameters are sufficient to capture both acoustic and linguistic features needed for SQA.
- **Evidence anchors**:
  - [abstract] "our approach requires up to 14.7 times fewer resources than a combined 1.3B parameters LLM with a 1.55B parameters ASR model"
- **Break condition**: If the single model cannot model the full acoustic-linguistic pipeline, performance will drop below cascade systems despite fewer parameters.

### Mechanism 3
- **Claim**: Layer-wise information distribution analysis reveals which encoder layers carry task-relevant features.
- **Mechanism**: By fine-tuning audio encoders with weighted layer representations, the system can identify and prioritize layers that best capture semantic content for SQA.
- **Core assumption**: Different audio models encode information in distinct layer patterns; understanding these patterns can guide better model design.
- **Evidence anchors**:
  - [abstract] "The study also analyzed the information distribution across encoder layers, finding that models like Whisper concentrate information in final layers, while others like HuBERT and WavLM integrate information from a broader range of layers."
  - [section] "As depicted in Figure 1, illustrating cumulative weights across encoder layers, Whisper models exhibit a propensity to concentrate information in the final layers"
- **Break condition**: If the weighting scheme does not capture true task-relevant features, fine-tuning will not improve performance and may overfit to training data.

## Foundational Learning

- **Concept: Audio-Text Embedding Alignment**
  - Why needed here: End-to-end models must map audio and text into comparable semantic spaces for entailment scoring.
  - Quick check question: How does the model compute similarity between an audio embedding and a text embedding?

- **Concept: Zero-Shot Classification via Log-Probability Scoring**
  - Why needed here: The method ranks answer options by comparing log probabilities without fine-tuning on task-specific data.
  - Quick check question: What normalization is applied to log probabilities before selecting the final answer?

- **Concept: Layer-Wise Feature Importance**
  - Why needed here: Understanding which encoder layers contribute most to SQA performance can guide model design and fine-tuning.
  - Quick check question: Which layer(s) show the highest cumulative weight for Whisper versus HuBERT in this study?

## Architecture Onboarding

- **Component map**: Audio encoder → embedding projection → entailment scoring → answer selection
- **Critical path**: Audio signal → encoder layers → weighted combination → classification head → answer output
- **Design tradeoffs**: Fewer parameters vs. possible accuracy loss; zero-shot flexibility vs. potential need for adaptation data
- **Failure signatures**: High WER in ASR cascade stage vs. low entailment scores in end-to-end stage; layer weights concentrated on irrelevant layers
- **First 3 experiments**:
  1. Compare end-to-end entailment accuracy vs. cascade ASR+LLM baseline on MedMCQA subset
  2. Vary the number of weighted layers and observe impact on performance for Whisper vs. HuBERT
  3. Test entailment with truncated audio (≤30s) to confirm compliance with Whisper constraints

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does scaling end-to-end models compare to traditional cascade systems in terms of accuracy and efficiency?
- Basis in paper: [explicit] The paper mentions that end-to-end models can be enhanced by scaling with better quality SQA data and increasing the number of parameters to see if they follow scaling laws similar to LLMs.
- Why unresolved: The paper does not provide experimental data on scaling end-to-end models, and the authors suggest that further research is needed to determine if end-to-end models follow similar scaling laws as LLMs.
- What evidence would resolve it: Experimental results comparing the performance of scaled end-to-end models with traditional cascade systems, including accuracy, efficiency, and resource utilization metrics.

### Open Question 2
- Question: What is the impact of speaker diversity on the accuracy of synthetic audio datasets for SQA tasks?
- Basis in paper: [explicit] The paper acknowledges that using limited speaker variety for synthetic audio may reduce accuracy compared to natural speech, affecting response precision.
- Why unresolved: The paper does not provide experimental data on the impact of speaker diversity on the accuracy of synthetic audio datasets, and the authors suggest that this is an area for further research.
- What evidence would resolve it: Experimental results comparing the accuracy of SQA models trained on synthetic audio datasets with varying speaker diversity, using both synthetic and natural speech as inputs.

### Open Question 3
- Question: How do end-to-end models perform in multilingual contexts for SQA tasks?
- Basis in paper: [explicit] The paper mentions that their study neglects multilingual contexts, highlighting the need for additional exploration in diverse linguistic settings.
- Why unresolved: The paper does not provide experimental data on the performance of end-to-end models in multilingual contexts, and the authors suggest that this is an area for further research.
- What evidence would resolve it: Experimental results comparing the performance of end-to-end models in multilingual contexts, using SQA datasets in different languages and evaluating their accuracy and efficiency.

## Limitations
- Synthetic audio dataset may not capture real clinical acoustic variability and complexity
- Evaluation limited to accuracy without detailed error analysis or bias consideration
- Layer-wise analysis conclusions are primarily descriptive without clear functional validation

## Confidence
- **High confidence**: Parameter efficiency claim (14.7× fewer resources) is directly supported by model specifications and comparisons
- **Medium confidence**: Comparable accuracy claim is supported by experimental results, though limited to synthetic data and 8 medical tasks
- **Low confidence**: Layer-wise information distribution analysis conclusions are primarily descriptive without clear functional validation

## Next Checks
1. Validate the end-to-end approach on real clinical speech data with diverse accents, background noise, and spontaneous speech patterns to test generalization beyond synthetic audio
2. Conduct comprehensive ablation studies removing the layer-wise weighting mechanism to quantify its actual contribution to performance improvements
3. Perform error analysis comparing specific failure modes between end-to-end and cascade approaches, particularly examining cases where ASR errors might propagate versus cases where end-to-end models fail due to limited training data