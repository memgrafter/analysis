---
ver: rpa2
title: Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)
arxiv_id: '2402.10376'
source_url: https://arxiv.org/abs/2402.10376
tags:
- concept
- clip
- splice
- concepts
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SpLiCE, a method to decompose dense CLIP image
  embeddings into sparse, interpretable concept embeddings. The method leverages the
  linear structure of CLIP's latent space to approximate image embeddings as sparse,
  nonnegative linear combinations of human-interpretable concepts from an overcomplete
  vocabulary.
---

# Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)

## Quick Facts
- arXiv ID: 2402.10376
- Source URL: https://arxiv.org/abs/2402.10376
- Authors: Usha Bhalla; Alex Oesterling; Suraj Srinivas; Flavio P. Calmon; Himabindu Lakkaraju
- Reference count: 40
- One-line primary result: SpLiCE decomposes CLIP embeddings into sparse, interpretable concept embeddings while maintaining zero-shot classification accuracy

## Executive Summary
SpLiCE introduces a task-agnostic method to transform dense CLIP image embeddings into sparse, interpretable concept embeddings. The approach leverages the linear structure of CLIP's latent space by solving a sparse recovery problem that finds non-negative linear combinations of human-interpretable concepts. Unlike previous approaches, SpLiCE requires no training, making it a drop-in replacement for dense CLIP representations. The method maintains high zero-shot classification accuracy while achieving sparse decompositions (5-20 concepts per image) and provides interpretable explanations through human-understandable concepts.

## Method Summary
SpLiCE is a task-agnostic method that transforms CLIP image embeddings into sparse linear combinations of human-interpretable concepts. It formulates the problem as sparse recovery, using an overcomplete concept vocabulary from LAION-400m captions. The method mean-centers CLIP embeddings to align image and text modalities, then solves a non-negative Lasso regression to find sparse concept weights. SpLiCE requires no training and serves as a drop-in replacement for dense CLIP representations, maintaining performance while providing interpretability.

## Key Results
- Maintains high zero-shot classification accuracy while achieving sparse decompositions (5-20 concepts per image)
- Outperforms learned and random dictionaries on semantic tasks while providing human-interpretable concepts
- Reveals CLIP encodes both semantic and non-semantic information, with SpLiCE decompositions capturing primarily semantic content
- User study shows participants preferred SpLiCE explanations over existing concept bottleneck methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP embeddings can be decomposed into sparse, non-negative linear combinations of human-interpretable concepts
- Mechanism: The method exploits the linear structure of CLIP's latent space, where semantic concepts are approximately additive. By solving a sparse recovery problem with an overcomplete concept vocabulary, SpLiCE finds sparse weight vectors that reconstruct CLIP embeddings while maintaining interpretability.
- Core assumption: CLIP representations encode semantic information in a linear, additive manner, and non-semantic components can be ignored for task performance
- Evidence anchors:
  - [abstract]: "We formulate this problem as one of sparse recovery and propose a novel method, Sparse Linear Concept Embeddings, for transforming CLIP representations into sparse linear combinations of human-interpretable concepts."
  - [section 3]: "Sufficient Conditions for Sparse Decomposition" lists five assumptions including linearity in concept space and semantic alignment between image and text encoders
  - [corpus]: Weak - only 1 of 8 related papers mentions sparse decomposition of CLIP embeddings

### Mechanism 2
- Claim: Using an overcomplete, non-negative concept dictionary improves interpretability while maintaining performance
- Mechanism: The overcomplete dictionary ensures all possible concepts are covered, while non-negativity prevents "negative" concepts that lack semantic meaning. This combination allows for sparse, constructive decompositions that are easier for humans to interpret.
- Core assumption: Negative concept weights are less interpretable than positive ones, and an overcomplete dictionary can span the semantic space of CLIP
- Evidence anchors:
  - [section 4.1]: "we require the weights of decompositions to be strictly nonnegative, to avoid having 'negative' concept weights which do not always carry semantic meaning" and "we construct an overcomplete dictionary containing a wide range of concepts, including antonyms"
  - [section 5.3]: Ablation study shows that non-negativity and overcomplete dictionary are essential for interpretability and performance
  - [corpus]: Weak - no direct evidence about dictionary design choices in related papers

### Mechanism 3
- Claim: Modality alignment through mean-centering enables successful decomposition of CLIP image embeddings using text concepts
- Mechanism: CLIP's image and text embeddings lie on different cones in the unit sphere. Mean-centering aligns these modalities by shifting them to a common origin, allowing non-negative decomposition of images into text concepts.
- Core assumption: The modality gap between CLIP's image and text embeddings prevents direct non-negative decomposition, and mean-centering effectively resolves this
- Evidence anchors:
  - [section 4.1]: "We empirically find that CLIP image and text embeddings exist on two cones... Not only does this prevent nonnegative decomposition, it also violates Assumption 4 from Section 3. To rectify this, we mean-center CLIP images"
  - [section A.4]: Empirical evidence showing that after centering, cosine similarity distributions become centered around zero
  - [corpus]: Weak - only 1 of 8 related papers mentions modality gap in CLIP

## Foundational Learning

- Concept: Linear representation hypothesis
  - Why needed here: The method relies on CLIP's embeddings being approximately linear functions of semantic concepts
  - Quick check question: If two images each contain concept A and concept B respectively, will their joint embedding equal the average of their individual embeddings?

- Concept: Sparse recovery and ℓ1 regularization
  - Why needed here: The optimization problem uses ℓ1 penalty to encourage sparsity in concept weights
  - Quick check question: What is the relationship between ℓ0 norm minimization and ℓ1 norm minimization in convex optimization?

- Concept: Modality gap in multimodal models
  - Why needed here: The method must address the different distributions of CLIP's image and text embeddings
  - Quick check question: If image and text embeddings lie on different cones in the unit sphere, what geometric operation can align them?

## Architecture Onboarding

- Component map:
  CLIP image encoder -> CLIP text encoder -> Concept vocabulary construction -> Mean-centering -> ADMM-based LASSO solver -> Sparse concept weights

- Critical path:
  1. Embed concept vocabulary with CLIP text encoder
  2. Mean-center both image embeddings and concept vocabulary
  3. Solve sparse recovery problem for each image
  4. (Optional) Train linear probes on decomposed representations

- Design tradeoffs:
  - Vocabulary size vs. computational cost
  - Sparsity level vs. reconstruction accuracy
  - Non-negativity constraint vs. expressive power
  - Mean-centering vs. potential loss of discriminative information

- Failure signatures:
  - High cosine reconstruction error but good task performance (indicates missing non-semantic information)
  - Poor zero-shot accuracy after decomposition (indicates loss of critical semantic information)
  - Decomposition concepts don't match human annotations (indicates vocabulary mismatch)

- First 3 experiments:
  1. Sanity check linearity: Combine two images/texts and verify their joint embedding equals the average of individual embeddings
  2. Ablation on vocabulary: Compare performance using semantic vs. random vs. learned dictionaries
  3. Modality alignment test: Measure cosine similarity distributions before and after mean-centering

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the linear representation hypothesis hold up for CLIP embeddings beyond simple image-text pairs?
- Basis in paper: [explicit] The paper investigates the linearity of CLIP embeddings by testing if a composite image embedding equals the average of its components.
- Why unresolved: The experiments only use basic compositions (image concatenation, text appending) and a limited set of datasets.
- What evidence would resolve it: Comprehensive experiments using diverse image manipulations, text combinations, and cross-modal tasks to validate linearity across a broader range of inputs.

### Open Question 2
- Question: Can SpLiCE be effectively applied to datasets with more complex or abstract concepts?
- Basis in paper: [inferred] The method uses a concept vocabulary based on common 1- and 2-word phrases, which may not capture nuanced or abstract ideas.
- Why unresolved: The experiments focus on concrete objects and scenes, leaving the performance on abstract or multifaceted concepts untested.
- What evidence would resolve it: Testing SpLiCE on datasets with abstract themes, emotional content, or complex social scenarios to assess its interpretability and performance.

### Open Question 3
- Question: How does the choice of concept vocabulary size and composition affect SpLiCE's performance?
- Basis in paper: [explicit] The paper uses a vocabulary of 10,000 one-word and 5,000 two-word concepts from LAION-400m, but doesn't explore the impact of varying these parameters.
- Why unresolved: The study doesn't investigate how different vocabulary sizes or sources (e.g., other datasets, expert-curated lists) influence decomposition quality and downstream task performance.
- What evidence would resolve it: Systematic ablation studies varying vocabulary size, diversity, and source, measuring impacts on sparsity, interpretability, and task accuracy.

## Limitations
- The linear representation hypothesis is largely empirical and untested beyond the specific datasets used
- Method performance depends heavily on quality and coverage of pre-defined concept vocabulary
- Optimal sparsity level and its relationship to downstream performance across diverse tasks remains unclear
- Mean-centering may not generalize to all CLIP model variants or newer versions

## Confidence
- High confidence: SpLiCE can produce sparse, interpretable concept embeddings that maintain reasonable zero-shot classification accuracy
- Medium confidence: The non-negativity constraint and overcomplete dictionary design choices are optimal for interpretability and performance
- Low confidence: The claim that CLIP encodes both semantic and non-semantic information, with SpLiCE capturing primarily semantic content, is primarily based on observation rather than rigorous analysis

## Next Checks
1. **Linearity verification experiment**: Systematically test the linear additivity hypothesis by combining multiple images and measuring whether their joint embedding equals the average of individual embeddings across different concept combinations
2. **Vocabulary coverage analysis**: Quantify the relationship between concept vocabulary size/coverage and decomposition performance, including an ablation study varying vocabulary quality (semantic vs. random concepts)
3. **Modality gap generalization**: Test whether mean-centering works consistently across different CLIP model variants (OpenCLIP, CLIP-ViT, CLIP-ResNet) and whether the cone structure of embeddings is preserved across versions