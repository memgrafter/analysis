---
ver: rpa2
title: DeepSeek-V3 Technical Report
arxiv_id: '2412.19437'
source_url: https://arxiv.org/abs/2412.19437
tags:
- training
- deepseek-v3
- arxiv
- performance
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeepSeek-V3 is a 671B parameter Mixture-of-Experts language model
  with 37B active parameters per token. It uses Multi-Head Latent Attention and DeepSeekMoE
  for efficient training and inference, and introduces an auxiliary-loss-free load
  balancing strategy.
---

# DeepSeek-V3 Technical Report

## Quick Facts
- arXiv ID: 2412.19437
- Source URL: https://arxiv.org/abs/2412.19437
- Authors: DeepSeek-AI; Aixin Liu; Bei Feng; Bing Xue; Bingxuan Wang; Bochao Wu; Chengda Lu; Chenggang Zhao; Chengqi Deng; Chenyu Zhang; Chong Ruan; Damai Dai; Daya Guo; Dejian Yang; Deli Chen; Dongjie Ji; Erhang Li; Fangyun Lin; Fucong Dai; Fuli Luo; Guangbo Hao; Guanting Chen; Guowei Li; H. Zhang; Han Bao; Hanwei Xu; Haocheng Wang; Haowei Zhang; Honghui Ding; Huajian Xin; Huazuo Gao; Hui Li; Hui Qu; J. L. Cai; Jian Liang; Jianzhong Guo; Jiaqi Ni; Jiashi Li; Jiawei Wang; Jin Chen; Jingchang Chen; Jingyang Yuan; Junjie Qiu; Junlong Li; Junxiao Song; Kai Dong; Kai Hu; Kaige Gao; Kang Guan; Kexin Huang; Kuai Yu; Lean Wang; Lecong Zhang; Lei Xu; Leyi Xia; Liang Zhao; Litong Wang; Liyue Zhang; Meng Li; Miaojun Wang; Mingchuan Zhang; Minghua Zhang; Minghui Tang; Mingming Li; Ning Tian; Panpan Huang; Peiyi Wang; Peng Zhang; Qiancheng Wang; Qihao Zhu; Qinyu Chen; Qiushi Du; R. J. Chen; R. L. Jin; Ruiqi Ge; Ruisong Zhang; Ruizhe Pan; Runji Wang; Runxin Xu; Ruoyu Zhang; Ruyi Chen; S. S. Li; Shanghao Lu; Shangyan Zhou; Shanhuang Chen; Shaoqing Wu; Shengfeng Ye; Shengfeng Ye; Shirong Ma; Shiyu Wang; Shuang Zhou; Shuiping Yu; Shunfeng Zhou; Shuting Pan; T. Wang; Tao Yun; Tian Pei; Tianyu Sun; W. L. Xiao; Wangding Zeng; Wanjia Zhao; Wei An; Wen Liu; Wenfeng Liang; Wenjun Gao; Wenqin Yu; Wentao Zhang; X. Q. Li; Xiangyue Jin; Xianzu Wang; Xiao Bi; Xiaodong Liu; Xiaohan Wang; Xiaojin Shen; Xiaokang Chen; Xiaokang Zhang; Xiaosha Chen; Xiaotao Nie; Xiaowen Sun; Xiaoxiang Wang; Xin Cheng; Xin Liu; Xin Xie; Xingchao Liu; Xingkai Yu; Xinnan Song; Xinxia Shan; Xinyi Zhou; Xinyu Yang; Xinyuan Li; Xuecheng Su; Xuheng Lin; Y. K. Li; Y. Q. Wang; Y. X. Wei; Y. X. Zhu; Yang Zhang; Yanhong Xu; Yanhong Xu; Yanping Huang; Yao Li; Yao Zhao; Yaofeng Sun; Yaohui Li; Yaohui Wang; Yi Yu; Yi Zheng; Yichao Zhang; Yifan Shi; Yiliang Xiong; Ying He; Ying Tang; Yishi Piao; Yisong Wang; Yixuan Tan; Yiyang Ma; Yiyuan Liu; Yongqiang Guo; Yu Wu; Yuan Ou; Yuchen Zhu; Yuduan Wang; Yue Gong; Yuheng Zou; Yujia He; Yukun Zha; Yunfan Xiong; Yunxian Ma; Yuting Yan; Yuxiang Luo; Yuxiang You; Yuxuan Liu; Yuyang Zhou; Z. F. Wu; Z. Z. Ren; Zehui Ren; Zhangli Sha; Zhe Fu; Zhean Xu; Zhen Huang; Zhen Zhang; Zhenda Xie; Zhengyan Zhang; Zhewen Hao; Zhibin Gou; Zhicheng Ma; Zhigang Yan; Zhihong Shao; Zhipeng Xu; Zhiyu Wu; Zhongyu Zhang; Zhuoshu Li; Zihui Gu; Zijia Zhu; Zijun Liu; Zilin Li; Ziwei Xie; Ziyang Song; Ziyi Gao; Zizheng Pan
- Reference count: 40

## Executive Summary
DeepSeek-V3 is a 671B parameter Mixture-of-Experts (MoE) language model that achieves state-of-the-art performance among open-source models while maintaining economical training costs. The model employs 37B active parameters per token through fine-grained expert routing, with 1 shared expert and 256 routed experts. Key innovations include an auxiliary-loss-free load balancing strategy and a multi-token prediction objective that together enhance both training efficiency and model performance. The model is trained on 14.8 trillion tokens using FP8 mixed precision across 2048 H800 GPUs, achieving breakthrough results on benchmarks including MMLU, MMLU-Pro, GPQA, and various math and code tasks.

## Method Summary
DeepSeek-V3 implements a Mixture-of-Experts architecture with Multi-Head Latent Attention (MLA) and DeepSeekMoE innovations. The model activates only 37B parameters per token from a total of 671B through fine-grained expert routing with 1 shared and 256 routed experts. A key innovation is the auxiliary-loss-free load balancing strategy that dynamically adjusts bias per expert based on load monitoring, avoiding explicit auxiliary loss penalties. The training objective extends to multi-token prediction (MTP), predicting multiple future tokens at each position to densify training signals. The model is trained using FP8 mixed precision across 2048 H800 GPUs with 14.8 trillion diverse tokens, followed by supervised fine-tuning and reinforcement learning stages using Group Relative Policy Optimization.

## Key Results
- Achieves state-of-the-art performance among open-source models on MMLU, MMLU-Pro, GPQA, and math/code benchmarks
- Activates only 37B parameters per token from 671B total, enabling efficient computation
- Trained with economical costs of only 2.788M H800 GPU hours (14.8 trillion tokens)
- Outperforms competitors on established benchmarks while maintaining competitive inference efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DeepSeek-V3 achieves strong performance while activating only 37B out of 671B total parameters.
- Mechanism: Mixture-of-Experts (MoE) architecture routes each token to a subset of experts, reducing computational load while maintaining model capacity.
- Core assumption: Expert specialization patterns emerge naturally during training without heavy auxiliary loss penalties.
- Evidence anchors:
  - [abstract]: "DeepSeek-V3 is a 671B parameter Mixture-of-Experts language model with 37B active parameters per token."
  - [section]: "Compared with traditional MoE architectures like GShard, DeepSeekMoE uses finer-grained experts and isolates some experts as shared ones."
  - [corpus]: "Mixture of Experts (MoE) models have emerged as a promising paradigm for scaling language models efficiently by activating only a subset of parameters for each input token."
- Break condition: Routing collapse occurs if load balancing is not maintained, leading to performance degradation.

### Mechanism 2
- Claim: Auxiliary-loss-free load balancing strategy improves model performance.
- Mechanism: Dynamic bias adjustment per expert based on load monitoring, avoiding explicit auxiliary loss penalties.
- Core assumption: Load balancing can be achieved through soft constraints rather than hard auxiliary losses.
- Evidence anchors:
  - [abstract]: "DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance."
  - [section]: "To achieve a better trade-off between load balance and model performance, we pioneer an auxiliary-loss-free load balancing strategy (Wang et al., 2024a) to ensure load balance."
  - [corpus]: "Skywork-MoE: A Deep Dive into Training Techniques for Mixture-of-Experts Language Models" discusses alternative load balancing methods.
- Break condition: If bias update speed is too high or too low, experts may become over-specialized or under-utilized.

### Mechanism 3
- Claim: Multi-Token Prediction (MTP) objective enhances overall model performance.
- Mechanism: Predicting multiple future tokens densifies training signals and enables speculative decoding.
- Core assumption: MTP does not compromise the accuracy of single-token prediction while improving data efficiency.
- Evidence anchors:
  - [abstract]: "DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance."
  - [section]: "Inspired by Gloeckle et al. (2024), we investigate and set a Multi-Token Prediction (MTP) objective for DeepSeek-V3, which extends the prediction scope to multiple future tokens at each position."
  - [corpus]: "dots.llm1 Technical Report" mentions multi-token prediction as a performance enhancement technique.
- Break condition: If MTP modules are not properly discarded during inference, they may introduce unnecessary computational overhead.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: MoE allows scaling to extremely large models while maintaining computational efficiency by activating only a subset of parameters per token.
  - Quick check question: How does MoE differ from traditional dense architectures in terms of parameter activation and computational cost?

- Concept: Load balancing strategies
  - Why needed here: Proper load balancing ensures all experts are utilized effectively, preventing routing collapse and maintaining performance.
  - Quick check question: What are the trade-offs between auxiliary-loss-based and auxiliary-loss-free load balancing approaches?

- Concept: Multi-Token Prediction (MTP)
  - Why needed here: MTP densifies training signals and enables speculative decoding, improving both training efficiency and inference speed.
  - Quick check question: How does MTP maintain the causal chain of predictions while predicting multiple future tokens?

## Architecture Onboarding

- Component map:
  Base Transformer layers with MLA (Multi-Head Latent Attention) → MoE layers with 1 shared expert and 256 routed experts → MTP modules for multi-token prediction → Auxiliary-loss-free load balancing mechanism → FP8 mixed precision training framework

- Critical path:
  Token embedding → MLA attention → MoE routing → FFN → MTP prediction → output head
  Load balancing bias updates occur after each training step

- Design tradeoffs:
  Fine-grained experts vs. computational overhead in all-to-all communication
  Auxiliary-loss-free balancing vs. potential for extreme sequence-wise imbalance
  MTP for training vs. discarding modules during inference

- Failure signatures:
  Routing collapse indicated by extremely unbalanced expert loads
  Training instability from improper quantization or precision settings
  Performance degradation from excessive auxiliary loss penalties

- First 3 experiments:
  1. Test auxiliary-loss-free load balancing on a small MoE model with controlled token distributions
  2. Validate MTP effectiveness by comparing single-token vs. multi-token prediction performance
  3. Benchmark FP8 training stability against BF16 baseline on a medium-scale MoE model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the multi-token prediction (MTP) strategy scale with deeper context lengths, and what is the optimal depth for maximizing model performance without introducing significant computational overhead?
- Basis in paper: [explicit] The paper investigates a Multi-Token Prediction (MTP) objective and proves it beneficial to model performance. It also mentions that MTP modules are used to predict multiple future tokens at each position.
- Why unresolved: The paper only implements a 1-depth MTP module and does not explore deeper contexts or the trade-offs between performance gains and computational costs at different depths.
- What evidence would resolve it: Experimental results comparing models with varying MTP depths (e.g., 1, 2, 3, 4) on the same benchmarks, showing performance and computational overhead trade-offs.

### Open Question 2
- Question: What are the long-term effects of the auxiliary-loss-free load balancing strategy on expert specialization and model performance, especially in dynamic or evolving datasets?
- Basis in paper: [explicit] The paper pioneers an auxiliary-loss-free strategy for load balancing, which minimizes performance degradation. It also discusses expert specialization patterns observed in ablation studies.
- Why unresolved: The paper provides short-term ablation studies but does not explore how this strategy performs over extended training periods or with dynamic datasets.
- What evidence would resolve it: Long-term training experiments with varying datasets, tracking expert specialization and model performance metrics over time.

### Open Question 3
- Question: How does the fine-grained quantization strategy (tile- and block-wise) impact model performance and training stability across different hardware architectures and tensor core implementations?
- Basis in paper: [explicit] The paper introduces a fine-grained quantization method to mitigate quantization errors caused by feature outliers and discusses its effectiveness in FP8 training.
- Why unresolved: The paper validates the strategy on NVIDIA H800 GPUs but does not explore its performance on other hardware architectures or tensor core implementations.
- What evidence would resolve it: Comparative experiments across different GPU architectures (e.g., NVIDIA A100, AMD MI300X) and tensor core implementations, measuring quantization accuracy and training stability.

## Limitations

- Token quality and diversity: The exact composition, domain distribution, and quality control mechanisms of the 14.8 trillion training tokens are not specified, making it difficult to assess whether performance improvements stem from model architecture innovations versus data quality advantages.

- Benchmark transparency: Several benchmark results are presented without detailed methodology, including whether results represent single-seed or multiple-seed averages, temperature sampling details, or post-processing methods.

- Reproducibility constraints: The report mentions specific hardware configurations but does not provide exact hyperparameter settings, learning rate schedules, or batch size configurations that would enable faithful reproduction.

## Confidence

**High Confidence (95%+)**: Claims about model architecture specifications (671B total parameters, 37B active parameters, MLA implementation, MoE routing strategy) are supported by detailed technical descriptions and architectural diagrams.

**Medium Confidence (70-90%)**: Performance claims on established benchmarks (MMLU, HumanEval, GPQA) are reasonably credible as these benchmarks have standardized evaluation protocols, though exact implementation details remain unclear.

**Low Confidence (50-70%)**: Claims about training efficiency (2.788M H800 GPU hours) and cost-effectiveness are difficult to verify without knowing the exact training configuration, optimization techniques, and potential optimizations that may not be documented.

## Next Checks

**Validation Check 1**: Implement and evaluate the auxiliary-loss-free load balancing strategy on a medium-scale MoE model (1-10B parameters) using controlled token distributions. Compare performance and load distribution against both traditional auxiliary-loss approaches and a baseline dense model. This would validate whether the load balancing mechanism generalizes beyond the full-scale model.

**Validation Check 2**: Conduct ablation studies on the Multi-Token Prediction objective by training identical models with single-token vs. multi-token prediction objectives on the same data distribution. Measure both training efficiency gains and any degradation in single-token inference performance to verify the claimed benefits without hidden costs.

**Validation Check 3**: Perform systematic testing of FP8 training stability by training the same model architecture with FP8, BF16, and mixed precision across various batch sizes and sequence lengths. Quantify the trade-off between computational efficiency and numerical precision errors to verify the claimed training stability advantages.