---
ver: rpa2
title: 'Images are Achilles'' Heel of Alignment: Exploiting Visual Vulnerabilities
  for Jailbreaking Multimodal Large Language Models'
arxiv_id: '2403.09792'
source_url: https://arxiv.org/abs/2403.09792
tags:
- harmful
- image
- mllms
- alignment
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically studies the harmlessness alignment of
  multimodal large language models (MLLMs) and identifies that images are a critical
  vulnerability. The authors propose HADES, a novel jailbreak method that hides harmful
  textual content in images and amplifies their harmfulness using typography, LLM-optimized
  image generation, and adversarial noise.
---

# Images are Achilles' Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2403.09792
- Source URL: https://arxiv.org/abs/2403.09792
- Reference count: 40
- Key outcome: HADES achieves 90.26% ASR on LLaVA-1.5 and 71.60% on Gemini Pro Vision by exploiting visual vulnerabilities in MLLM alignment

## Executive Summary
This paper systematically studies the harmlessness alignment of multimodal large language models (MLLMs) and identifies images as a critical vulnerability. The authors propose HADES, a novel jailbreak method that hides harmful textual content in images and amplifies their harmfulness using typography, LLM-optimized image generation, and adversarial noise. Experiments show HADES achieves an average Attack Success Rate (ASR) of 90.26% on LLaVA-1.5 and 71.60% on Gemini Pro Vision, demonstrating significant jailbreak effectiveness across both open- and closed-source MLLMs.

## Method Summary
The paper introduces HADES, a three-stage jailbreak method targeting MLLM visual vulnerabilities. First, harmful keywords are hidden in images using typography and text-to-image pointers to bypass text-based content filters. Second, image harmfulness is amplified through LLM-optimized image generation using diffusion models and prompt refinement. Finally, adversarial noise is added via gradient updates to further optimize attack effectiveness. The method is evaluated on a dataset of 750 harmful instructions across 5 scenarios (Violence, Financial, Privacy, Self-Harm, Animal) using both open-source models (LLaVA-1.5, LLaVA-1.5 LoRA, MiniGPT-v2, MiniGPT-4) and closed-source models (Gemini Pro Vision, GPT-4V). Attack success is measured using Beaver-dam-7B as a harmfulness judging model.

## Key Results
- HADES achieves 90.26% average ASR on LLaVA-1.5 across all harmful categories
- Closed-source model Gemini Pro Vision shows 71.60% average ASR under HADES attack
- Ablation studies show typography, LLM-optimized generation, and adversarial noise each contribute to attack effectiveness

## Why This Works (Mechanism)
MLLMs inherit harmlessness alignment from text-only LLMs but face unique vulnerabilities when processing visual information. The paper identifies that current MLLMs lack robust visual harmlessness alignment, allowing attackers to bypass text filters by hiding harmful content in images. Typography can effectively conceal harmful keywords from text-based detection while remaining interpretable by the MLLM's visual processing. LLM-optimized image generation ensures the visual content aligns with the harmful intent, and adversarial noise further optimizes the attack to overcome any remaining defenses.

## Foundational Learning
- Multimodal alignment: Why needed - MLLMs must align both visual and textual modalities to avoid harmful outputs. Quick check - Verify MLLM can process and understand both image and text inputs correctly.
- Typography recognition: Why needed - Hidden text in images must be readable by MLLM OCR systems. Quick check - Test if MLLM can accurately extract text from typography-based attacks.
- Adversarial optimization: Why needed - Fine-tuning images to maximize harmful responses requires gradient-based methods. Quick check - Verify adversarial images produce stronger harmful responses than unmodified versions.

## Architecture Onboarding

**Component map:** Harmful instruction -> Typography generation -> LLM-optimized image generation -> Adversarial noise optimization -> MLLM input -> Harmful output generation

**Critical path:** The most critical path is the transformation from harmful text to visual representation, ensuring the MLLM can both see and understand the harmful content while bypassing text-based filters.

**Design tradeoffs:** The method trades off between visual stealth (hiding harmful content) and effectiveness (ensuring MLLM recognizes and acts on harmful content). More obvious typography may be more effective but less stealthy.

**Failure signatures:** 
- Low ASR indicates MLLM's visual processing fails to recognize harmful content
- Inconsistent ASR across categories suggests some types of visual harm are harder to convey
- High ASR on blank images indicates model biases toward generating harmful content without clear harmful cues

**3 first experiments:**
1. Test ASR of HADES on LLaVA-1.5 with only typography component (no image generation or noise)
2. Evaluate ASR when harmful keywords are visible in text but paired with optimized images
3. Measure ASR difference between standard and LLM-optimized diffusion model outputs

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- The evaluation relies on Beaver-dam-7B as a harmfulness judge, introducing potential model-dependent biases
- Complete prompt engineering details and adversarial optimization procedures are not fully specified
- Results may not generalize to MLLMs with stronger visual grounding or different architectural approaches

## Confidence
- **High confidence** in the core finding that images represent a vulnerability in MLLM alignment, supported by consistent results across multiple models
- **Medium confidence** in the specific HADES methodology and its relative effectiveness, given the methodological gaps
- **Medium confidence** in the generalizability of results beyond the tested models and categories

## Next Checks
1. Conduct ablation studies to isolate the contribution of each HADES component (typography, LLM-optimized generation, adversarial noise) to the overall attack success rate
2. Test HADES effectiveness on additional MLLM architectures not evaluated in the original study, particularly those with different visual grounding capabilities
3. Implement independent verification of the adversarial noise optimization procedure with full technical specifications to assess reproducibility of the attack methodology