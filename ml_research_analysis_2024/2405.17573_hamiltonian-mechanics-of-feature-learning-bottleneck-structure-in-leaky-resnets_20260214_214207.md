---
ver: rpa2
title: 'Hamiltonian Mechanics of Feature Learning: Bottleneck Structure in Leaky ResNets'
arxiv_id: '2405.17573'
source_url: https://arxiv.org/abs/2405.17573
tags:
- should
- which
- representation
- answer
- bottleneck
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies Leaky ResNets, which interpolate between traditional
  ResNets and fully connected networks, focusing on the infinite depth limit. The
  authors analyze "representation geodesics" - continuous paths in representation
  space that minimize the network parameter norm.
---

# Hamiltonian Mechanics of Feature Learning: Bottleneck Structure in Leaky ResNets

## Quick Facts
- **arXiv ID**: 2405.17573
- **Source URL**: https://arxiv.org/abs/2405.17573
- **Authors**: Arthur Jacot; Alexandre Kaiser
- **Reference count**: 40
- **Key outcome**: Analyzes Leaky ResNets in infinite depth limit, revealing bottleneck structure in representation space governed by competing kinetic and potential energy forces

## Executive Summary
This paper studies Leaky ResNets through the lens of Hamiltonian mechanics, analyzing how representations evolve in the infinite depth limit. The authors formulate the problem of finding optimal representation paths as minimizing a functional that balances kinetic energy (favoring smooth layer transitions) against potential energy (measured by the "Cost of Identity"). Their key finding is that for large effective depths, the potential energy dominates, leading to a bottleneck structure where representations rapidly jump to a low-dimensional subspace, evolve slowly there, and then jump to the output. This theoretical insight motivates an adaptive layer step-size training method that accounts for the separation of timescales between these dynamics.

## Method Summary
The authors analyze Leaky ResNets by considering the infinite depth limit and studying "representation geodesics" - continuous paths in representation space that minimize the network parameter norm. They reformulate this optimization problem using Lagrangian and Hamiltonian mechanics, where the functional to minimize contains kinetic energy (favoring small layer derivatives) and potential energy (favoring low-dimensional representations). The potential energy is quantified by the "Cost of Identity" (COI), which measures how well a representation can be approximated by an identity transformation. Through this analysis, they predict that for large effective depths, the potential energy dominates, leading to a bottleneck structure in the representation space.

## Key Results
- In the infinite depth limit, Leaky ResNets exhibit a bottleneck structure where representations rapidly transition to low-dimensional subspaces
- The bottleneck structure emerges from competition between kinetic energy (smooth layer transitions) and potential energy (low-dimensional representations)
- The proposed adaptive layer step-size training method improves performance by accounting for the separation of timescales in the dynamics
- Experiments on synthetic data demonstrate the emergence of bottleneck structures and validate the adaptive discretization approach

## Why This Works (Mechanism)
The paper's mechanism relies on the infinite depth limit of Leaky ResNets, where the network becomes a continuous-time dynamical system. In this limit, finding optimal representations becomes equivalent to finding geodesics in representation space, which can be analyzed using Lagrangian and Hamiltonian mechanics. The key insight is that the functional to minimize contains two competing terms: kinetic energy that penalizes rapid changes in layer outputs, and potential energy that penalizes high-dimensional representations (measured by COI). For large effective depths, the potential energy term dominates, forcing representations to rapidly jump to low-dimensional subspaces where they can evolve slowly before jumping to the output. This creates the observed bottleneck structure.

## Foundational Learning
- **Representation geodesics**: Continuous paths in representation space that minimize network parameter norm. Why needed: Provides geometric framework for analyzing infinite-depth networks. Quick check: Verify that geodesics satisfy Euler-Lagrange equations derived from the Lagrangian.
- **Lagrangian/Hamiltonian mechanics**: Mathematical framework for analyzing optimization problems with energy-like functionals. Why needed: Enables analysis of competing forces (kinetic vs potential energy) in representation evolution. Quick check: Confirm Hamiltonian formulation correctly captures the dynamics of Leaky ResNets.
- **Cost of Identity (COI)**: Metric measuring how well a representation can be approximated by identity transformation. Why needed: Quantifies potential energy that favors low-dimensional representations. Quick check: Verify COI computation matches theoretical definition for simple cases.

## Architecture Onboarding

**Component map**: Leaky ResNet layers -> Representation space -> Output layer

**Critical path**: Input → Leaky ResNet blocks → Bottleneck representation → Output prediction

**Design tradeoffs**: 
- Traditional ResNets: Identity mappings, no bottleneck structure
- Fully connected networks: High-dimensional representations, potential overfitting
- Leaky ResNets: Interpolates between extremes, controlled dimensionality reduction

**Failure signatures**:
- Bottleneck too restrictive: Underfitting, poor representation capacity
- Bottleneck too loose: Loss of efficiency gains, ineffective dimensionality reduction
- Incorrect timescale separation: Suboptimal adaptive step-size performance

**First experiments**:
1. Train Leaky ResNet on synthetic data to visualize bottleneck structure emergence
2. Compare adaptive layer step-size vs fixed step-size training on simple classification tasks
3. Vary leak parameter to study transition between ResNets and fully connected behavior

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis relies on infinite depth limit, which may not accurately capture finite-depth network behavior
- COI metric assumes specific form of potential energy that may not generalize across different architectures
- Separation of timescales assumption needs empirical validation across diverse datasets and architectures
- Adaptive layer step-size method requires testing on real-world problems beyond synthetic data

## Confidence
- **High confidence**: Mathematical formulation of representation geodesics and reformulation using Lagrangian/Hamiltonian mechanics
- **Medium confidence**: Theoretical prediction of bottleneck structure emerging from potential energy dominance
- **Low confidence**: Practical effectiveness of proposed adaptive training scheme without broader empirical validation

## Next Checks
1. Test adaptive layer step-size method on real-world datasets (CIFAR-10, ImageNet) to evaluate performance impact
2. Conduct ablation studies varying COI formulation to assess sensitivity and generalizability
3. Perform experiments with finite-depth Leaky ResNets to quantify deviation from infinite-depth predictions and establish practical depth thresholds