---
ver: rpa2
title: 'FisherMask: Enhancing Neural Network Labeling Efficiency in Image Classification
  Using Fisher Information'
arxiv_id: '2411.05752'
source_url: https://arxiv.org/abs/2411.05752
tags:
- information
- samples
- data
- fisher
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FisherMask is an active learning approach that uses Fisher information
  to identify the most informative samples for training deep neural networks. The
  method constructs a sparse network mask by selecting the k weights with the highest
  Fisher information values, focusing on the network's middle layers to capture critical
  details about unlabeled data samples.
---

# FisherMask: Enhancing Neural Network Labeling Efficiency in Image Classification Using Fisher Information

## Quick Facts
- arXiv ID: 2411.05752
- Source URL: https://arxiv.org/abs/2411.05752
- Authors: Shreen Gul; Mohamed Elmahallawy; Sanjay Madria; Ardhendu Tripathy
- Reference count: 24
- One-line primary result: FisherMask achieves significant gains in labeling efficiency for image classification by using Fisher information to identify the most informative samples, outperforming state-of-the-art methods especially in imbalanced and low-data regimes.

## Executive Summary
FisherMask is an active learning approach that leverages Fisher information to identify the most informative samples for training deep neural networks. By constructing a sparse network mask that selects the k weights with the highest Fisher information values, FisherMask focuses on parameters that have the greatest impact on model predictions. The method specifically utilizes information from the network's middle layers to identify influential samples, capturing more diverse and discriminative features throughout the network. Experiments on CIFAR-10 and FashionMNIST demonstrate that FisherMask significantly outperforms state-of-the-art methods, achieving substantial gains in labeling efficiency, particularly under imbalanced settings and in low-data regimes.

## Method Summary
FisherMask constructs a sparse network mask by computing the Fisher information matrix for the entire network and selecting the k weights with the highest Fisher information values. The method uses middle layers of the network (rather than just the final layer) to capture critical details about unlabeled data samples. To improve computational efficiency, FisherMask approximates updates to the Fisher information matrices and their inverses using the Woodbury identity and trace rotation techniques. The approach is model-agnostic and particularly effective in low-data regimes where it consistently outperforms baselines like entropy sampling, margin sampling, and k-center greedy methods.

## Key Results
- FisherMask significantly outperforms state-of-the-art active learning methods on CIFAR-10 and FashionMNIST datasets
- The approach achieves substantial gains in labeling efficiency, especially under imbalanced settings
- FisherMask consistently outperforms baselines in low-data regimes, requiring fewer labeled samples to achieve comparable accuracy
- A sparsity level of 0.002 was found optimal for constructing the FisherMask while maintaining computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FisherMask identifies the most informative samples by focusing on network parameters with the highest Fisher information values.
- Mechanism: The method computes the Fisher information matrix for the entire network and creates a sparse mask by selecting the k weights with the highest Fisher information values. This mask captures critical details about unlabeled data samples by prioritizing parameters that have the greatest impact on model predictions.
- Core assumption: Parameters with higher Fisher information values are more important for model learning and thus selecting samples that activate these parameters will be more informative.
- Evidence anchors:
  - [abstract] "FisherMask is an active learning approach that uses Fisher information to identify the most informative samples for training deep neural networks."
  - [section] "The FIM is calculated from this probability vector, and the top k parameters with the largest Fisher information values are selected."
- Break condition: If Fisher information values do not correlate with parameter importance for the specific dataset or model architecture being used.

### Mechanism 2
- Claim: Using middle layers of the network provides better sample selection than only using the last layer.
- Mechanism: FisherMask leverages information from the network's middle layers to identify influential samples, rather than relying solely on the final layer as in BAIT. This approach captures more diverse and discriminative features throughout the network.
- Core assumption: Middle layers contain more informative features about data samples than just the final classification layer.
- Evidence anchors:
  - [abstract] "FisherMask specifically utilizes the information from the network's middle layers to identify influential samples."
  - [section] "Our approach differs from BAIT [6] in how we utilize Fisher information matrices to choose the optimal data points. Unlike BAIT, which relies solely on Fisher information matrices from the last layer of the network, we consider weights across intermediate layers of the network."
- Break condition: If the middle layers do not contain more informative features than the final layer for the specific classification task.

### Mechanism 3
- Claim: The sparsity constraint (selecting top k parameters) improves computational efficiency while maintaining selection quality.
- Mechanism: By filtering the Fisher information matrix to only include the top k parameters (with sparsity levels like 0.002 tested), FisherMask reduces computational complexity while still capturing the most important parameter-sample relationships.
- Core assumption: A small subset of highly informative parameters can represent the most important sample selection information.
- Evidence anchors:
  - [section] "To speed up computations, we approximate the updates to the Fisher information matrices and their inverses using the Woodbury identity and trace rotation techniques."
  - [section] "We tested multiple sparsity levels for constructing the FisherMask, including 0.01, 0.005, 0.002, and 0.001. Our experiments demonstrated that a sparsity level of 0.002 was optimal."
- Break condition: If the sparsity level is too aggressive and excludes important parameters, leading to poor sample selection.

## Foundational Learning

- Fisher Information:
  - Why needed here: Fisher information quantifies how much information an observable random variable reveals about an unknown parameter in a distribution, which is fundamental to understanding parameter sensitivity and sample informativeness.
  - Quick check question: How does Fisher information relate to the curvature of the log-likelihood function with respect to model parameters?

- Active Learning Strategies:
  - Why needed here: Understanding different AL approaches (uncertainty-based vs diversity-based) provides context for why FisherMask's hybrid approach is effective.
  - Quick check question: What are the main limitations of pure uncertainty-based approaches like entropy sampling in active learning?

- Neural Network Architecture:
  - Why needed here: Knowledge of ResNet-18 structure and how different layers capture features is crucial for understanding why middle layer information is valuable.
  - Quick check question: In a ResNet architecture, which layers typically capture more abstract/high-level features versus more detailed/low-level features?

## Architecture Onboarding

- Component map:
  Input: Unlabeled dataset S -> Core: Fisher information matrix computation, parameter selection (top k), sample selection optimization -> Output: Labeled subset C for model training
  Dependencies: PyTorch implementation, ResNet-18 model, Fisher information calculation utilities

- Critical path:
  1. Compute Fisher information for all parameters
  2. Select top k parameters based on Fisher values
  3. Use these parameters to identify most informative samples
  4. Query labels for selected samples
  5. Retrain model with newly labeled data

- Design tradeoffs:
  - Sparsity level (k): Higher values improve sample selection quality but increase computation; lower values improve speed but may miss important samples
  - Layer selection: Including more layers captures more information but increases complexity; middle layers offer a good balance
  - Batch size: Larger batches reduce AL round frequency but may include redundant samples

- Failure signatures:
  - Poor performance on certain datasets may indicate Fisher information isn't capturing relevant parameter importance
  - Computational bottlenecks suggest the sparsity level needs adjustment
  - Convergence issues might indicate insufficient diversity in selected samples

- First 3 experiments:
  1. Run FisherMask with different sparsity levels (0.01, 0.005, 0.002) on CIFAR-10 to find optimal k
  2. Compare FisherMask performance using only last layer vs. middle layers on FashionMNIST
  3. Test FisherMask on a simple dataset (like MNIST) with very few initial labels to verify low-data regime effectiveness

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the experimental scope and methodology, several important questions emerge regarding the generalizability and limitations of FisherMask.

## Limitations

- The computational efficiency claims are moderate confidence as the exact computational overhead for computing Fisher information matrices across all network layers is not fully quantified.
- The claim that FisherMask is "model-agnostic" is low confidence since experiments were only conducted on ResNet-18 architecture.
- The paper lacks ablation studies comparing different layer combinations to quantify the specific contribution of middle layer information.

## Confidence

- Mechanism 1 (Fisher information parameter selection): High
- Mechanism 2 (Middle layer advantage): Medium
- Mechanism 3 (Sparsity efficiency): Medium
- Model-agnostic claim: Low

## Next Checks

1. Conduct ablation studies testing FisherMask with different layer combinations (last layer only, middle layers only, full network) to quantify the specific contribution of middle layer information.

2. Test FisherMask on a non-ResNet architecture (e.g., Vision Transformer) to validate the model-agnostic claim and identify any architecture-specific limitations.

3. Implement FisherMask on a completely different task domain (e.g., text classification or tabular data) to assess cross-domain generalizability beyond image classification.