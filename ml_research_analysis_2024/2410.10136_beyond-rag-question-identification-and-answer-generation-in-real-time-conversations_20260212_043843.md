---
ver: rpa2
title: 'Beyond-RAG: Question Identification and Answer Generation in Real-Time Conversations'
arxiv_id: '2410.10136'
source_url: https://arxiv.org/abs/2410.10136
tags:
- questions
- customer
- system
- agent
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an enhanced decision support system that goes
  beyond traditional RAG approaches by integrating real-time question identification
  with FAQ retrieval and RAG-based generation. The system automatically identifies
  customer queries in real-time conversations, retrieving answers from a pre-existing
  FAQ database when possible, or using RAG for non-FAQ questions.
---

# Beyond-RAG: Question Identification and Answer Generation in Real-Time Conversations

## Quick Facts
- arXiv ID: 2410.10136
- Source URL: https://arxiv.org/abs/2410.10136
- Authors: Garima Agrawal; Sashank Gummuluri; Cosimo Spera
- Reference count: 5
- This paper presents an enhanced decision support system that goes beyond traditional RAG approaches by integrating real-time question identification with FAQ retrieval and RAG-based generation.

## Executive Summary
This paper introduces a novel decision support system that automatically identifies customer queries in real-time conversations and retrieves answers from a pre-existing FAQ database when possible, or uses RAG for non-FAQ questions. The system, deployed in Minerva CQ's AI-powered human-agent assist solution, significantly reduces reliance on manual queries and delivers responses to agents within 2 seconds. By combining FAQ retrieval with RAG generation through parallel processing threads, the approach achieves substantial improvements in operational efficiency, reduced average handling time (AHT), and cost savings by minimizing unnecessary RAG API calls.

## Method Summary
The system employs an LLM-agentic workflow to automatically generate FAQs from historical transcripts when no predefined FAQ database exists. For real-time conversations, it uses two parallel Claude-3-Haiku models - one for FAQ database matching and one for dynamic question generation. The system processes conversation turns using a rolling window approach, invoking models either automatically or through manual triggers. When a customer query matches an FAQ, the system retrieves the answer directly from the FAQ database; otherwise, it generates answers via RAG. The automated FAQ generation process analyzes approximately 30,000 historical call transcripts using five coordinated LLM agents to identify common questions and create a structured, evolving FAQ database.

## Key Results
- Reduces reliance on manual queries, providing responses to agents within 2 seconds
- Achieves 70% reduction in RAG queries by utilizing FAQ database retrieval
- Demonstrates significant improvements in operational efficiency and reduced average handling time (AHT)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parallel matching and generation threads reduce latency compared to sequential LLM-based approaches.
- Mechanism: Two Claude-3-Haiku models run concurrently—one for FAQ database matching and one for dynamic question generation—allowing faster response delivery within 2-4 seconds.
- Core assumption: Smaller LLMs like Claude-3-Haiku can handle the task with sufficient accuracy while maintaining low latency.
- Evidence anchors:
  - [section]: "Based on cost analysis and latency simulations, we selected the approach using two parallel threads of Claude-3-Haiku, one for matching and the other for generating questions...with a latency of 2-4 seconds."
  - [abstract]: "Our approach reduces reliance on manual queries, providing responses to agents within 2 seconds."
- Break condition: If the parallel processing introduces synchronization overhead or if one thread consistently fails to produce relevant results, overall system latency would increase.

### Mechanism 2
- Claim: FAQ database retrieval reduces RAG model calls, lowering operational costs.
- Mechanism: When a customer query matches an FAQ, the system retrieves the answer directly from the FAQ database instead of invoking the RAG model, saving API costs and reducing response time.
- Core assumption: Frequently asked questions can be accurately identified and their answers stored for direct retrieval.
- Evidence anchors:
  - [section]: "If the query matches an FAQ, the system retrieves the answer directly from the FAQ database; otherwise, it generates answers via RAG."
  - [abstract]: "If the query matches an FAQ, the answer is retrieved from the pre-existing FAQ database, by-passing the need for RAG retrieval."
- Break condition: If the FAQ database becomes outdated or incomplete, agents may frequently fall back to RAG, negating cost savings.

### Mechanism 3
- Claim: Automated FAQ generation from historical transcripts ensures comprehensive and up-to-date FAQ coverage.
- Mechanism: An LLM-agentic workflow analyzes historical call transcripts, identifies common questions using clustering and filtering, and creates a structured FAQ database that evolves with new transcripts.
- Core assumption: Historical transcripts contain sufficient patterns of frequently asked questions that can be algorithmically extracted and clustered.
- Evidence anchors:
  - [section]: "We employed a set of LLM agents...historical call transcripts...used as the source data...For our experiments, we used a large dataset containing approximately 30,000 calls."
  - [abstract]: "We also introduce an automated LLM-agentic workflow to identify FAQs from historical transcripts when no predefined FAQs exist."
- Break condition: If the clustering algorithm fails to group similar questions accurately or if the LLM agents miss context-specific nuances, the generated FAQ database may be incomplete or irrelevant.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: The system uses RAG for non-FAQ questions, so understanding how RAG combines retrieval with LLM generation is essential.
  - Quick check question: How does RAG differ from standard LLM query-response in terms of document grounding and context retrieval?

- Concept: FAQ database management and vector search
  - Why needed here: The system maintains a persistent FAQ database and uses vector search for matching, requiring knowledge of embedding techniques and similarity search.
  - Quick check question: What are the trade-offs between exact keyword matching and semantic vector search for FAQ retrieval?

- Concept: LLM-agentic workflows and LangGraph orchestration
  - Why needed here: The FAQ generation process uses multiple LLM agents orchestrated through LangGraph, so understanding agent-based workflows is critical.
  - Quick check question: How do LangGraph agents coordinate to process large datasets while maintaining context and avoiding token limits?

## Architecture Onboarding

- Component map:
  Real-time conversation interface -> FAQ model -> Match thread (FAQ database search) + Generate thread (Claude-3-Haiku) -> Agent selection -> FAQ retrieval or RAG pipeline -> Answer delivery
  Historical transcript processor -> LLM-agentic workflow (5 agents) -> FAQ database -> Vector storage

- Critical path:
  1. Conversation turn arrives
  2. FAQ model triggered by rolling window or manual button
  3. Parallel Match and Generate threads execute
  4. Agent selects from 6 suggested questions
  5. System retrieves answer from FAQ database or invokes RAG
  6. Answer displayed to agent

- Design tradeoffs:
  - Two smaller LLMs (Claude-3-Haiku) vs. one large LLM: Lower cost and latency but requires synchronization
  - Rolling window invocation vs. continuous monitoring: Reduced computational load but potential delay in assistance
  - FAQ database vs. pure RAG: Faster responses and lower costs for common queries but requires maintenance

- Failure signatures:
  - High latency (>4 seconds): Likely parallel thread synchronization issues or LLM model unavailability
  - Irrelevant question suggestions: FAQ database mismatch or generate thread prompt engineering issues
  - Frequent RAG fallback: Outdated FAQ database or insufficient FAQ coverage

- First 3 experiments:
  1. Simulate conversation with predefined FAQs and measure Match thread accuracy and latency
  2. Test Generate thread with Claude-3-Haiku on sample transcripts to evaluate question relevance and generation speed
  3. Deploy both threads in parallel on a test conversation and measure end-to-end response time and agent satisfaction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal frequency and timing for invoking the FAQ model during real-time conversations to maximize accuracy while minimizing latency?
- Basis in paper: [inferred] The paper discusses using a fixed rolling window approach for invoking the model but acknowledges the challenge of determining the optimal moment during conversations.
- Why unresolved: The paper mentions using a fixed rolling window approach but does not provide specific data on the optimal frequency or timing for model invocation.
- What evidence would resolve it: Empirical data comparing different invocation frequencies and timing strategies, showing their impact on accuracy, latency, and overall system performance.

### Open Question 2
- Question: How does the system perform when deployed across different industries with varying types of customer queries and knowledge bases?
- Basis in paper: [explicit] The paper mentions that future research will explore the scalability of the system across various industries but does not provide any current data on cross-industry performance.
- Why unresolved: The system has only been deployed in a customer contact center setting, and there is no data on its performance in other industry contexts.
- What evidence would resolve it: Performance metrics and user feedback from deployments in diverse industries such as healthcare, finance, or technical support.

### Open Question 3
- Question: What is the long-term impact of the FAQ model on agent training and knowledge retention, and how does it affect the overall skill development of human agents?
- Basis in paper: [inferred] The paper focuses on immediate operational benefits but does not address the potential long-term effects on agent skill development and knowledge retention.
- Why unresolved: The paper does not include any longitudinal studies or data on how continuous use of the FAQ model affects agent performance and learning over extended periods.
- What evidence would resolve it: Longitudinal studies comparing agent performance, knowledge retention, and skill development between teams using the FAQ model and those using traditional methods over several months or years.

## Limitations

- Unknown Prompt Engineering: The specific prompt templates for Claude-3-Haiku and the LLM agents are not provided, which could significantly impact system performance and question matching accuracy.
- Real-World Deployment Validation: While the system shows promise in controlled experiments, long-term deployment data (beyond pilot testing) is needed to assess sustained performance and maintenance requirements.
- Cost-Benefit Trade-offs: The claimed 70% reduction in RAG queries needs verification against actual operational data, as this depends heavily on FAQ coverage quality and customer query distribution.

## Confidence

- High Confidence: The core mechanism of parallel processing with Claude-3-Haiku and the fundamental approach of FAQ retrieval vs RAG fallback is well-supported by the described architecture and experimental results.
- Medium Confidence: The automated FAQ generation workflow is theoretically sound, but its effectiveness depends heavily on implementation details not fully specified in the paper.
- Low Confidence: The claimed 2-second response time and 70% RAG query reduction need real-world validation across diverse customer service scenarios.

## Next Checks

1. **Performance Benchmarking**: Deploy the system in a live customer service environment for 30 days and measure actual response times, FAQ matching accuracy, and RAG fallback rates across different query types and volumes.

2. **FAQ Generation Quality Audit**: Conduct a comprehensive evaluation of the automated FAQ generation process by comparing generated FAQs against manually curated ones, measuring both coverage and relevance scores.

3. **Cost Analysis Validation**: Track actual API usage costs and compute the real-world cost savings from FAQ retrieval vs RAG generation, accounting for FAQ maintenance overhead and LLM model updates.