---
ver: rpa2
title: 'IPEval: A Bilingual Intellectual Property Agency Consultation Evaluation Benchmark
  for Large Language Models'
arxiv_id: '2406.12386'
source_url: https://arxiv.org/abs/2406.12386
tags:
- chinese
- data
- questions
- performance
- patent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces IPEval, the first bilingual benchmark for
  evaluating Large Language Models (LLMs) on intellectual property (IP) agency and
  consulting tasks. The benchmark consists of 2,657 multiple-choice questions across
  four dimensions: creation, application, protection, and management of IP, covering
  eight knowledge areas including patents, trademarks, and copyrights.'
---

# IPEval: A Bilingual Intellectual Property Agency Consultation Evaluation Benchmark for Large Language Models

## Quick Facts
- arXiv ID: 2406.12386
- Source URL: https://arxiv.org/abs/2406.12386
- Reference count: 40
- Primary result: First bilingual benchmark evaluating LLMs on IP agency tasks, revealing regional knowledge gaps and superior performance of general-purpose models over specialized legal LLMs

## Executive Summary
This paper introduces IPEval, a bilingual benchmark designed to evaluate Large Language Models on intellectual property agency and consulting tasks. The benchmark comprises 2,657 multiple-choice questions covering four dimensions (creation, application, protection, management) and eight knowledge areas across Chinese and English. Three evaluation methods (zero-shot, 5-few-shot, Chain of Thought) were applied to 15 models, revealing that general-purpose models outperform specialized legal domain models. The study highlights the need for LLMs to understand regional and temporal differences in IP laws.

## Method Summary
The IPEval benchmark was constructed using questions from USPTO and CNIPA patent bar exams, ensuring regional specificity and temporal relevance. The dataset includes questions from multiple years (1997-2003 for US, 2012-2019 for China) to capture legal evolution. Three evaluation strategies were employed: zero-shot prompting, 5-few-shot learning, and Chain of Thought reasoning. Fifteen LLMs were tested, including both open-source and closed-source models, with performance measured across Chinese and English subsets of the benchmark.

## Key Results
- General-purpose models (GPT series, Qwen series) significantly outperform specialized legal domain models (fuzi-mingcha, MoZi) on IP tasks
- English-centric models perform better on English questions, while Chinese-centric models excel on Chinese questions
- Few-shot prompting strategy yielded the largest performance improvements across models, with some seeing up to 38.6% gains
- The benchmark successfully exposes regional knowledge gaps in LLMs, with models showing clear performance differences based on training data language

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bilingual evaluation exposes regional knowledge gaps in LLMs
- Mechanism: The benchmark uses separate Chinese and English datasets sourced from CNIPA and USPTO exams. This forces models to demonstrate mastery of region-specific IP laws rather than relying on translated or cross-lingual shortcuts
- Core assumption: IP laws differ meaningfully between jurisdictions and models cannot trivially transfer knowledge across languages without true legal understanding
- Evidence anchors:
  - [abstract] "Results show that English-centric models like GPT series and Qwen series perform better in English tests, while Chinese-centric models like Qwen series outperform GPT-4 in Chinese tests"
  - [section 2.1] "We collected data from both the USPTO and CNIPA patent bar exams to ensure the benchmark's regional specificity"
  - [corpus] FMR scores show high relevance to bilingual IP evaluation papers, indicating focused topic alignment
- Break condition: If the model's training data contains extensive cross-lingual legal translation pairs, the regional distinction may be eroded

### Mechanism 2
- Claim: Temporal alignment in data selection ensures legal currency testing
- Mechanism: Exam questions are collected across multiple years (1997-2003 for US, 2012-2019 for China), reflecting evolving IP laws. This forces models to adapt to legal changes over time rather than relying on static knowledge
- Core assumption: IP laws change annually and models must demonstrate awareness of these updates
- Evidence anchors:
  - [abstract] "We also analyze the models' capabilities in terms of the regional and temporal aspects of IP, emphasizing that IP domain LLMs need to clearly understand the differences in IP laws across different regions and their dynamic changes over time"
  - [section 2.1] "Additionally, as these exams are conducted annually and updated with regulatory changes, gathering exam questions from multiple consecutive years naturally ensures the benchmark's temporal relevance"
  - [corpus] No explicit temporal citation data in corpus, but related papers focus on IP benchmarking which implies temporal relevance
- Break condition: If models rely heavily on static pre-training data without exposure to recent legal updates, performance will degrade on newer questions

### Mechanism 3
- Claim: Multiple evaluation strategies (zero-shot, few-shot, CoT) reveal different reasoning strengths
- Mechanism: By applying three prompting strategies, the benchmark distinguishes between models that excel at direct recall versus those that benefit from structured reasoning or example-based learning
- Core assumption: Different LLMs have varying optimal prompting paradigms for complex legal reasoning
- Evidence anchors:
  - [abstract] "Evaluation methods include zero-shot, 5-few-shot, and Chain of Thought (CoT) for seven LLM types, predominantly in English or Chinese"
  - [section 3.3] "Most models significantly improved under the 5-few-shot strategy; for instance, Baichuan2-7B-Chat saw a 38.6% performance increase in Chinese questions with this approach"
  - [corpus] No direct corpus evidence for prompting strategies, but high FMR to IP benchmarking papers suggests alignment
- Break condition: If models are trained with instruction-tuning that already optimizes for these specific strategies, the differentiation may be less meaningful

## Foundational Learning

- Concept: Legal reasoning versus factual recall
  - Why needed here: IP consulting tasks require both knowledge application and logical inference about infringement, protection methods, and procedural compliance
  - Quick check question: Can the model distinguish between a correct legal outcome based on facts versus merely repeating memorized rules?

- Concept: Multilingual competence in technical domains
  - Why needed here: The benchmark tests whether models truly understand IP concepts in both Chinese and English, not just translation ability
  - Quick check question: Does the model perform consistently on equivalent questions in both languages, or does performance drop significantly in one language?

- Concept: Temporal legal awareness
  - Why needed here: IP laws evolve, and models must demonstrate awareness of jurisdiction-specific changes over time
  - Quick check question: Can the model correctly answer questions about legal provisions that have been amended or replaced in recent years?

## Architecture Onboarding

- Component map: Data pipeline: PDF parsing → JSON extraction → manual verification → annotation → benchmark assembly; Model interface: API endpoints for each model type (open-source and closed-source); Evaluation engine: Question routing, answer validation, score aggregation; Analysis module: Performance breakdown by dimension, field, language, and prompting strategy
- Critical path: Data collection → annotation → prompt template creation → model evaluation → result aggregation → analysis
- Design tradeoffs:
  - Multiple-choice format ensures objective scoring but may miss nuanced understanding
  - Bilingual design captures regional differences but doubles data complexity
  - Three prompting strategies provide comprehensive evaluation but increase experimental overhead
- Failure signatures:
  - Low accuracy across all strategies suggests fundamental knowledge gaps
  - High variance between prompting strategies indicates sensitivity to presentation format
  - Language-specific performance gaps reveal inadequate multilingual training
- First 3 experiments:
  1. Run zero-shot evaluation on a small subset of questions to establish baseline performance
  2. Test few-shot prompting with 5 examples to measure improvement potential
  3. Apply CoT prompting to identify reasoning bottlenecks and compare against other strategies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between general-purpose and specialized domain knowledge in LLMs for IP consulting tasks?
- Basis in paper: [explicit] The paper shows that specialized legal domain LLMs (fuzi-mingcha, MoZi) significantly lag behind general-purpose models (GPT series, Qwen series) in IP performance, despite being fine-tuned on legal corpora
- Why unresolved: The study demonstrates this performance gap but does not explore the underlying reasons or determine the optimal knowledge distribution
- What evidence would resolve it: Comparative studies testing various blends of general and specialized IP knowledge in LLMs, measuring performance on IP consulting tasks

### Open Question 2
- Question: How can regional differences in IP laws be effectively incorporated into LLM training to improve performance on geographically-specific tasks?
- Basis in paper: [explicit] The paper notes that models trained in specific regions (e.g., Qwen series for Chinese IP, GPT series for US IP) perform better on regional tasks, but also highlights the need for models to understand regional differences
- Why unresolved: While the paper identifies regional performance differences, it does not provide a method for effectively incorporating regional IP knowledge into LLM training
- What evidence would resolve it: Development and testing of LLMs trained on regionally-diverse IP corpora, measuring performance on geographically-specific IP consulting tasks

### Open Question 3
- Question: What is the impact of temporal dynamics in IP laws on LLM performance, and how can models be adapted to account for evolving legal frameworks?
- Basis in paper: [explicit] The paper discusses the temporal locality of IP laws and the need for models to understand dynamic changes, but does not explore the specific impact on LLM performance
- Why unresolved: The study acknowledges the importance of temporal aspects but does not quantify the performance impact or provide adaptation strategies
- What evidence would resolve it: Longitudinal studies tracking LLM performance on IP tasks over time, correlating with changes in IP laws and regulations

## Limitations

- The exact prompt templates and few-shot examples are not fully disclosed, making precise replication challenging
- Model performance may be influenced by undisclosed factors such as training data composition and instruction-tuning approaches
- The benchmark's temporal coverage (1997-2003 for US, 2012-2019 for China) may not fully capture recent legal changes affecting current IP practice

## Confidence

- **High confidence**: The benchmark's construction methodology (bilingual design, multiple evaluation strategies, comprehensive question coverage) is well-documented and reproducible
- **Medium confidence**: The performance ranking of models is reliable, though absolute accuracy scores may vary with different prompt implementations
- **Low confidence**: Claims about temporal legal awareness are based on question age rather than direct testing of legal updates, requiring additional validation

## Next Checks

1. Conduct temporal validation by creating a subset of questions specifically targeting post-2019 legal changes to test whether models can handle recent IP law modifications
2. Implement cross-linguistic equivalence testing by creating parallel Chinese-English question pairs with identical legal content to verify true bilingual competence versus translation artifacts
3. Test domain-specialized legal models (e.g., trained specifically on IP law) against the benchmark to determine whether general-purpose LLMs truly outperform specialized alternatives or if the gap reflects training methodology differences