---
ver: rpa2
title: 'SNED: Superposition Network Architecture Search for Efficient Video Diffusion
  Model'
arxiv_id: '2406.00195'
source_url: https://arxiv.org/abs/2406.00195
tags:
- video
- diffusion
- training
- search
- supernet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SNED, a superposition network architecture
  search method for efficient video diffusion models. SNED addresses the challenge
  of high computational costs and large model sizes in video diffusion models by employing
  a weight-sharing supernet training paradigm.
---

# SNED: Superposition Network Architecture Search for Efficient Video Diffusion Model

## Quick Facts
- arXiv ID: 2406.00195
- Source URL: https://arxiv.org/abs/2406.00195
- Reference count: 40
- Primary result: SNED achieves consistent video generation results across resolutions from 64×64 to 256×256 with model sizes ranging from 640M to 1.6B parameters for pixel-space video diffusion models.

## Executive Summary
This paper introduces SNED, a superposition network architecture search method for efficient video diffusion models. SNED addresses the challenge of high computational costs and large model sizes in video diffusion models by employing a weight-sharing supernet training paradigm. The method allows for dynamic computation cost sampling and enables a single supernet to manage different resolutions through a "super-position training" mechanism. SNED expands the search space with dynamic channel and fine-grained dynamic block options and includes a supernet training sampling warmup strategy. The approach is compatible with both pixel-space and latent-space video diffusion models.

## Method Summary
SNED implements a one-shot neural architecture search solution using a weight-sharing supernet that can generate multiple subnetworks with different computational costs and resolutions. The supernet is trained with dynamic cost sampling, randomly selecting architectures from a search space that includes dynamic channel ratios (40%-100%) and fine-grained dynamic blocks (component dropping). Super-position training allows the supernet to handle multiple resolution targets through weight-sharing. After training, specific subnets can be extracted without additional training. The method is validated on both pixel-space and latent-space video diffusion models.

## Key Results
- Achieves consistent video generation results across resolutions from 64×64 to 256×256
- Model sizes range from 640M to 1.6B parameters for pixel-space video diffusion models
- Demonstrates compatibility with both pixel-space and latent-space video diffusion architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supernet training with dynamic cost sampling enables a single model to efficiently represent multiple subnetworks with different computational costs and resolutions.
- Mechanism: The supernet is trained once with weight-sharing across all candidate architectures. During training, subnets are randomly sampled from the search space (including dynamic channels and fine-grained dynamic blocks), allowing the supernet to learn shared parameters that generalize across different model sizes and resolutions. After training, specific subnets can be extracted without additional training.
- Core assumption: Weight-sharing across diverse architectures does not compromise performance of individual subnets if properly sampled during training.
- Evidence anchors:
  - [abstract] "Our method employs a supernet training paradigm that targets various model cost and resolution options using a weight-sharing method."
  - [section] "We implement a one-shot neural architecture search solution, enabling dynamic computation cost sampling. This means that once the supernet is trained, it achieves the differentiation of computational costs across various subnets within the supernet."
  - [corpus] Weak evidence - corpus contains related pixel-space diffusion work but no direct supernet NAS studies for video diffusion.

### Mechanism 2
- Claim: Super-position training allows a single supernet to handle multiple resolution targets through weight-sharing.
- Mechanism: During each training iteration, both a subnet architecture and a resolution target are randomly sampled. The supernet learns to adapt to different resolutions while maintaining shared weights, eliminating the need to train separate models for each resolution.
- Core assumption: The same set of weights can effectively process different resolutions if the training distribution includes sufficient resolution diversity.
- Evidence anchors:
  - [abstract] "we introduce the concept of 'super-position training' into our supernet training process. This breakthrough allows a singular supernet model to effectively manage different resolutions"
  - [section] "This breakthrough allows a singular supernet model to effectively manage different resolutions, offering a versatile solution for handling diverse resolution requirements."
  - [corpus] Weak evidence - related work on multi-resolution training exists but not specifically for video diffusion supernets.

### Mechanism 3
- Claim: Dynamic channel and fine-grained dynamic block search spaces expand the optimization potential for video diffusion models.
- Mechanism: The search space includes multiple channel reduction ratios (40%-100%) for each layer and the ability to drop specific components (ResBlock, temporal self-attention, cross-attention, spatial attention) within diffusion blocks. This allows fine-grained control over computational cost versus quality trade-offs.
- Core assumption: Reducing channels and dropping blocks can significantly reduce computation without proportionally degrading output quality.
- Evidence anchors:
  - [section] "Our dynamic cost search space includes the dynamic channel space and the fine-grained dynamic block space."
  - [section] "To expand our search space during the supernet training and investigate the potential of the video diffusion model, we add the fine-grained dynamic block search process inside each diffusion block."
  - [corpus] Weak evidence - dynamic channel pruning is well-studied for classification but less so for generative models.

## Foundational Learning

- Concept: Neural Architecture Search (NAS) fundamentals
  - Why needed here: Understanding one-shot NAS and weight-sharing paradigms is essential to grasp how SNED trains a supernet that can produce multiple subnetworks.
  - Quick check question: What distinguishes one-shot NAS from traditional NAS approaches, and why is it more efficient?

- Concept: Diffusion models and video generation architectures
  - Why needed here: SNED builds on video diffusion models, so understanding the basic U-Net architecture, noise prediction, and temporal modeling is crucial.
  - Quick check question: How do spatial and temporal attention mechanisms differ in video diffusion models compared to image diffusion models?

- Concept: Supernet training and sampling strategies
  - Why needed here: The effectiveness of SNED depends on proper sampling during supernet training to ensure all architectures in the search space are adequately represented.
  - Quick check question: What is the purpose of the "supernet training sampling warmup" strategy, and how might it improve training stability?

## Architecture Onboarding

- Component map:
  Supernet (weight-sharing) -> Dynamic cost sampling -> Search space (dynamic channels + fine-grained dynamic blocks) -> Resolution management (super-position training) -> Extracted subnetworks

- Critical path: Supernet training → Subnetwork extraction → Resolution-specific deployment
  - First train the supernet with proper sampling strategy
  - Extract desired subnetworks based on computational constraints
  - Deploy with appropriate resolution handling

- Design tradeoffs:
  - Search space breadth vs. training efficiency: Larger search spaces provide more options but require more training iterations
  - Resolution diversity vs. model quality: Training on more resolutions may reduce per-resolution performance
  - Channel reduction vs. quality: More aggressive reduction saves computation but may degrade generation quality

- Failure signatures:
  - Poor performance on specific resolutions: Indicates insufficient resolution diversity during training
  - Inconsistent quality across different model sizes: Suggests sampling bias during supernet training
  - Slow convergence: May indicate inadequate warmup strategy or overly complex search space

- First 3 experiments:
  1. Train a basic supernet with limited search space (only 100% and 50% channels) on a single resolution to verify the core mechanism works
  2. Add resolution diversity (e.g., 64×64 and 128×128) to test super-position training effectiveness
  3. Expand to full search space with dynamic channels and fine-grained blocks to evaluate quality versus computational cost trade-offs

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but raises several implicit ones through its methodology and claims, particularly regarding the generalizability of the supernet training approach to other architectures and tasks beyond the tested video diffusion models.

## Limitations
- The paper lacks specific architectural details about the supernet design and implementation of the diffusion model components, making exact reproduction challenging.
- The use of an internal dataset for pixel-space model evaluation prevents direct comparison with other methods.
- The effectiveness of super-position training across resolutions relies on the assumption that shared weights can adequately capture resolution-specific features, which may not hold for very divergent resolutions.

## Confidence

### Major Uncertainties
- High confidence: The core concept of using weight-sharing supernet training for architecture search in video diffusion models is technically sound and well-supported by the proposed mechanisms.
- Medium confidence: The effectiveness of super-position training for resolution management depends on specific architectural choices and may vary based on resolution differences.
- Medium confidence: The claim that dynamic channels and fine-grained block dropping significantly reduce computational costs without quality degradation needs empirical validation across diverse use cases.

## Next Checks

1. Conduct ablation studies to quantify the contribution of super-position training versus traditional multi-resolution training approaches for video diffusion models.
2. Evaluate the performance gap between models trained with the proposed sampling warmup strategy versus standard random sampling to validate its necessity.
3. Test the extracted subnetworks on out-of-distribution video content to assess generalization beyond the training datasets and validate the robustness of the supernet training approach.