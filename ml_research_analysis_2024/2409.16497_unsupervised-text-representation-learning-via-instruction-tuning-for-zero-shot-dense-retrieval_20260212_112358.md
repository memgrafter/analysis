---
ver: rpa2
title: Unsupervised Text Representation Learning via Instruction-Tuning for Zero-Shot
  Dense Retrieval
arxiv_id: '2409.16497'
source_url: https://arxiv.org/abs/2409.16497
tags:
- corpus
- representation
- retrieval
- queries
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an unsupervised dense retrieval method via
  instruction-tuning large language models (LLMs) for text representation learning.
  The approach generates synthetic queries using pre-trained LLMs, filters them for
  quality, and instruction-tunes the model on these synthetic data.
---

# Unsupervised Text Representation Learning via Instruction-Tuning for Zero-Shot Dense Retrieval

## Quick Facts
- arXiv ID: 2409.16497
- Source URL: https://arxiv.org/abs/2409.16497
- Reference count: 40
- Key outcome: Proposed unsupervised dense retrieval method significantly improves zero-shot retrieval performance, achieving 3.34%-3.50% absolute NDCG@10 gains for FLAN-T5 models and outperforming three competitive dense retrievers by 1.96%-9.52% absolute NDCG@10 despite using models at least 38% smaller.

## Executive Summary
This paper introduces an unsupervised approach to dense retrieval that leverages instruction-tuning of large language models to generate synthetic queries for corpus representation learning. The method generates synthetic queries using a pre-trained LLM, filters them for quality, and instruction-tunes the model on these synthetic data. It then augments corpus representations by combining original corpus embeddings with embeddings of relevant synthetic queries, based on the Rao-Blackwell theorem. The approach significantly improves zero-shot retrieval performance across three English and one German IR datasets without requiring labeled query-corpus pairs.

## Method Summary
The method follows a three-stage pipeline: (1) Generate synthetic queries for each corpus document using a pre-trained LLM like FLAN-T5 with instructions to create questions and keywords, then filter the generated queries; (2) Instruction-tune the LLM on the filtered synthetic queries to improve its instruction-following ability; (3) Generate new synthetic queries using the instruction-tuned model, encode both original corpus and synthetic queries, and create an augmented corpus representation by weighted averaging of the two embeddings. The corpus is split into sentences for fine-grained semantic matching, and retrieval is performed using cosine similarity between query and corpus representations.

## Key Results
- The proposed method increases open-box FLAN-T5 model variations by 3.34%-3.50% in absolute NDCG@10
- Outperforms three competitive dense retrievers (docTTTTTquery, Contriever, SPARTA) by 1.96%-9.52% absolute NDCG@10
- Uses models at least 38% smaller than competing methods
- Sentence-level multi-representation outperforms corpus-level single representation across all datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic queries generated by an instruct-tuned LLM produce embeddings that, when combined with the original corpus embedding, yield a better representation than either alone.
- Mechanism: The Rao-Blackwell theorem is applied to estimate the expected query embedding from a corpus embedding. By conditioning on synthetic query embeddings as sufficient statistics, the corpus representation is improved.
- Core assumption: The synthetic queries generated are relevant to the corpus and their embeddings capture additional semantic information that the corpus embedding alone does not.
- Evidence anchors:
  - [abstract] "We demonstrate the corpus representation can be augmented by the representations of relevant synthetic queries generated by the instruct-tuned LLM founded on the Rao-Blackwell theorem."
  - [section] "According to Rao–Blackwell Theorem: If g(X) is any kind of estimator of a parameter θ, then the conditional expectation of g(X) given T (X), namely E(g(x)|T (x)), where T is a sufficient statistic, is typically a better estimator of θ, and is never worse."
  - [corpus] Weak evidence - The paper shows improved retrieval metrics but does not directly verify that synthetic query embeddings are truly "relevant" or that they capture the claimed additional semantic information.

### Mechanism 2
- Claim: Instruction-tuning the LLM on its own synthetic queries improves the quality of future synthetic queries, leading to better corpus representation.
- Mechanism: The LLM is fine-tuned on high-quality synthetic query-corpus pairs it generated itself, learning to follow instructions better and produce more relevant synthetic queries.
- Core assumption: The initial synthetic queries, even if filtered, contain enough signal for the LLM to learn from and improve its instruction-following ability.
- Evidence anchors:
  - [abstract] "We apply filters to gate the synthetic data quality and instruction-tune the pre-trained LLM (and its variant versions) on the filtered output."
  - [section] "While LLM demonstrates reasonable text generation capabilities, its ability to follow specific instructions can be honed. Instruction-tuning focuses on training a model to precisely follow the provided instructions."
  - [corpus] Weak evidence - The paper shows improved retrieval metrics after instruction-tuning but does not directly verify that the synthetic queries generated after instruction-tuning are of higher quality than those generated before.

### Mechanism 3
- Claim: Using a sentence-level multi-representation for the corpus captures fine-grained semantic interactions better than a single corpus-level representation.
- Mechanism: The corpus is split into sentences, each encoded separately, and the retrieval system can match any sentence to the query, effectively retrieving the entire corpus if any sentence matches.
- Core assumption: Splitting the corpus into sentences does not lose critical context and that sentence-level matching is sufficient for effective retrieval.
- Evidence anchors:
  - [section] "To help the encoder capture the fine-grained semantic interaction between queries and corpus, we divide each corpora into multiple sentences using the PunktSentenceTokenizer... and use the sentence level multi representation for the corpora, meaning that if any of the sentence is retrieved, the passage is retrieved."
  - [section] "We evaluate whether the sentence level multi-representation can capture the semantic interaction between the corpora and the query. Results for FLAN-T5 models using encoder-only representation are shown in Table 2. The sentence level multi-representation embedding technique outperforms the corpora level single representation by a large margin across all datasets."
  - [corpus] Strong evidence - The paper provides quantitative results showing sentence-level indexing outperforms corpus-level indexing.

## Foundational Learning

- Concept: The Rao-Blackwell theorem and its application to estimation.
  - Why needed here: The paper's core innovation relies on applying the Rao-Blackwell theorem to improve corpus representation by using synthetic query embeddings as sufficient statistics.
  - Quick check question: What does the Rao-Blackwell theorem state about the relationship between an estimator and its conditional expectation given a sufficient statistic?

- Concept: Instruction-tuning and its effect on model generalization.
  - Why needed here: The paper's method involves instruction-tuning an LLM on its own synthetic data to improve its ability to generate relevant queries.
  - Quick check question: How does instruction-tuning differ from standard fine-tuning, and why might it lead to better generalization on unseen tasks?

- Concept: Dense retrieval and the dual-encoder architecture.
  - Why needed here: The paper's method is applied within the context of dense retrieval using a dual-encoder framework.
  - Quick check question: In a dual-encoder dense retrieval system, how are query and corpus representations typically generated and compared?

## Architecture Onboarding

- Component map: Pre-trained LLM -> Synthetic query generator -> Quality filter -> Instruct-tuned LLM -> Corpus representation generator -> Retrieval system

- Critical path:
  1. Generate synthetic queries for each corpus using the pre-trained LLM.
  2. Filter the synthetic queries for quality.
  3. Instruction-tune the LLM on the filtered synthetic queries.
  4. Generate new synthetic queries for each corpus using the instruct-tuned LLM.
  5. Encode the corpus and synthetic queries into embeddings.
  6. Combine the embeddings using a weighted average to form the final corpus representation.
  7. Use the retrieval system to match queries to corpus representations.

- Design tradeoffs:
  - Quality vs. quantity of synthetic queries: Generating more queries might capture more semantic nuances but could also introduce more noise.
  - Weighting of corpus vs. synthetic query embeddings: The optimal weighting might depend on the dataset and the quality of the synthetic queries.
  - Sentence-level vs. corpus-level representation: Sentence-level might capture finer details but could lose some context.

- Failure signatures:
  - If the retrieval performance does not improve after instruction-tuning, it might indicate that the synthetic queries are not of sufficient quality or that the instruction-tuning process is not effective.
  - If the retrieval performance degrades significantly when using the augmented corpus representation, it might indicate that the synthetic queries are introducing too much noise or are not relevant to the corpus.

- First 3 experiments:
  1. Compare the retrieval performance of the original corpus embedding vs. the augmented corpus representation on a small subset of the test data.
  2. Analyze the quality of synthetic queries generated before and after instruction-tuning to verify that instruction-tuning improves query quality.
  3. Experiment with different weighting schemes for combining corpus and synthetic query embeddings to find the optimal weighting.

## Open Questions the Paper Calls Out

- How to improve the quality of synthetic queries through more advanced filtering techniques.
- How to scale the approach to larger corpora while maintaining performance.
- How to incorporate multiple instruction types beyond keyword summarization and question generation.

## Limitations
- The method's reliance on synthetic query generation introduces potential quality issues, and the filtering criteria beyond basic heuristics remain unspecified.
- The application of the Rao-Blackwell theorem is applied in a novel context without rigorous validation of its assumptions.
- The paper does not directly verify that synthetic queries generated after instruction-tuning are of higher quality than those generated before.

## Confidence
- **High Confidence**: The improvement in retrieval metrics (NDCG@10, MRR@100, Recall@100) compared to baseline models.
- **Medium Confidence**: The effectiveness of instruction-tuning in improving query generation quality.
- **Low Confidence**: The theoretical foundation of applying the Rao-Blackwell theorem to corpus representation augmentation.

## Next Checks
1. Conduct a human evaluation study comparing synthetic queries generated before and after instruction-tuning to directly verify improvements in query quality and relevance.
2. Design experiments to test whether synthetic query embeddings satisfy the sufficient statistic condition required by the Rao-Blackwell theorem for corpus representation improvement.
3. Perform a comprehensive grid search over weighting parameters for combining corpus and synthetic query embeddings, and analyze how performance varies across different datasets.