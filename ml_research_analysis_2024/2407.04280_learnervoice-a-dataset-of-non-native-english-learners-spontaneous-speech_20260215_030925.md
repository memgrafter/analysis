---
ver: rpa2
title: 'LearnerVoice: A Dataset of Non-Native English Learners'' Spontaneous Speech'
arxiv_id: '2407.04280'
source_url: https://arxiv.org/abs/2407.04280
tags:
- speech
- oice
- features
- learnerv
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LearnerVoice is a 50-hour spontaneous speech dataset of Korean
  English learners, containing significantly more disfluencies and grammatical errors
  than native speech datasets. Fine-tuning Whisper-small.en on LearnerVoice reduces
  WER by 44.2% on L2 learner speech, with a 48.1% decrease in L2S-related errors compared
  to 19.2% for non-L2S errors.
---

# LearnerVoice: A Dataset of Non-Native English Learners' Spontaneous Speech

## Quick Facts
- arXiv ID: 2407.04280
- Source URL: https://arxiv.org/abs/2407.04280
- Reference count: 0
- Key outcome: Fine-tuning Whisper-small.en on LearnerVoice reduces WER by 44.2% on L2 learner speech

## Executive Summary
This paper introduces LearnerVoice, a 50-hour spontaneous speech dataset of Korean English learners containing significantly more disfluencies and grammatical errors than native speech datasets. The authors fine-tune Whisper-small.en on this L2 data and demonstrate substantial improvements in ASR performance, reducing WER by 44.2% compared to the vanilla model. The dataset explicitly captures L2-specific features like filler words, self-repairs, and ungrammatical expressions, enabling ASR models to better handle spontaneous non-native speech.

## Method Summary
The authors collected 50.04 hours of spontaneous speech from 58 Korean L2 English learners across IELTS bands 4-9. They recruited annotators trained to recognize L2-specific features and transcribed the speech with explicit labeling of filler words, self-repairs, and ungrammatical expressions. The dataset was then used to fine-tune Whisper-small.en with AdamW optimizer (learning rate 1e-5) for 2.04 epochs with gradient accumulation every 2 steps. Performance was evaluated using WER reduction and detailed error type analysis comparing L2-specific versus general errors.

## Key Results
- Fine-tuning Whisper-small.en on LearnerVoice achieves 10.26% WER, 44.2% lower than vanilla Whisper-small.en
- L2S-related errors decreased by 48.1% compared to 19.2% reduction for non-L2S errors
- The dataset contains significantly more disfluencies and grammatical errors than native speech datasets like Switchboard

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning Whisper-small.en on L2 learner data significantly reduces WER by exposing the model to L2S-specific acoustic and linguistic patterns. The fine-tuning process adjusts the model's weights to better recognize learner-specific pronunciation patterns, disfluencies, and ungrammatical structures that differ from native speech. The model can generalize from 50 hours of L2 data to improve performance on unseen L2 speech. Break condition: If the L2 data is too homogeneous (e.g., only Korean learners), the model may not generalize well to other L1 backgrounds.

### Mechanism 2
The dataset's explicit transcription of L2S features (filler words, self-repairs, ungrammatical expressions) provides the model with explicit supervision for these phenomena. Annotators were trained to capture L2S features, and the fine-tuned model learns to output these features rather than correcting them away, aligning with the L2 speaker's actual production. Accurate transcription of L2S features is critical for model training. Break condition: If the model's architecture cannot represent the L2S features in its output space, fine-tuning will not help.

### Mechanism 3
The distribution of L2 learners across proficiency bands ensures the dataset covers a range of L2S feature frequencies, improving model robustness. By including learners from IELTS bands 4-9, the model is exposed to varying degrees of disfluency and grammatical complexity, preventing overfitting to only heavily accented or simple speech. A diverse proficiency range in training data leads to better generalization. Break condition: If the proficiency bands are not representative of the target deployment population, performance may degrade.

## Foundational Learning

- **Concept**: Spontaneous speech disfluencies (filler words, self-repairs, false starts)
  - Why needed here: The dataset's primary novelty is its focus on L2S features, which are more prevalent in spontaneous speech than in read speech.
  - Quick check question: What are the three main types of disfluencies commonly found in spontaneous speech?

- **Concept**: ASR error taxonomy (e.g., Pronunciation/Accent, Ungrammatical Expression, Hallucination)
  - Why needed here: The paper uses a 9-type taxonomy to analyze model errors, requiring understanding of what each error type means and how it relates to L2S features.
  - Quick check question: How does an "Ungrammatical Expression" error differ from a "Pronunciation/Accent" error in ASR output?

- **Concept**: WER (Word Error Rate) and its calculation
  - Why needed here: WER is the primary metric used to evaluate model performance before and after fine-tuning.
  - Quick check question: If a model's output has 10 errors in a 100-word reference, what is the WER?

## Architecture Onboarding

- **Component map**: Whisper-small.en (244M parameters, English-only) → fine-tuning pipeline → L2S feature-aware ASR
- **Critical path**: Data collection → annotation (L2S feature transcription) → fine-tuning (on 50h L2 data) → evaluation (WER reduction, error type analysis)
- **Design tradeoffs**: Multilingual vs. monolingual models (whisper-large-v3 performed worse due to accent confusion), dataset size vs. diversity (50h is modest but covers proficiency range)
- **Failure signatures**: WER improvement plateaus, error type distribution unchanged, model overfits to Korean accents only
- **First 3 experiments**:
  1. Evaluate vanilla Whisper-small.en on L2 test set to establish baseline WER.
  2. Fine-tune on L2 data for 1-2 epochs, monitor validation loss and WER on L2 validation set.
  3. Perform error tagging on fine-tuned model outputs to verify L2S error reduction.

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of ASR systems vary across different L2 proficiency levels (e.g., IELTS bands 4-9) when transcribing spontaneous speech? The dataset contains L2 learners distributed across IELTS bands 4-9, but the paper does not analyze performance variations across proficiency levels. ASR performance metrics broken down by learner proficiency levels would show how proficiency impacts recognition accuracy and which error types are most affected.

### Open Question 2
How do L2S feature distributions and ASR error patterns differ across various spontaneous speech topics (e.g., daily life, business/economics, current affairs/politics, culture/sports)? The dataset includes diverse topics for spontaneous speech, but the paper does not analyze how topic affects L2S feature prevalence or ASR performance. Analysis of L2S feature frequencies and ASR error distributions across different speech topics would reveal topic-specific challenges.

### Open Question 3
How does the distribution of L2S features in spontaneous speech compare to that in other spontaneous speech contexts (e.g., interviews, meetings, classroom discussions)? The paper compares L2S features in the dataset to Switchboard (native spontaneous speech interviews) and Librispeech (native read speech), but does not compare to other spontaneous speech contexts. Comparative analysis of L2S feature distributions across multiple spontaneous speech contexts would contextualize the L2S feature prevalence in the dataset.

## Limitations
- Dataset contains only Korean L2 learners, limiting generalizability to other L1 backgrounds
- Relatively modest 50-hour size may not provide sufficient coverage for all L2S phenomena
- Evaluation constrained to one base model (Whisper-small.en), preventing assessment of architecture transferability

## Confidence

*High Confidence Claims* (Supported by direct experimental evidence):
- Fine-tuning Whisper-small.en on LearnerVoice reduces WER by 44.2% on Korean L2 speech
- L2S-related error types show 48.1% reduction versus 19.2% for non-L2S errors
- The dataset contains significantly more disfluencies and grammatical errors than native speech datasets

*Medium Confidence Claims* (Supported by indirect evidence or reasonable extrapolation):
- The dataset captures universal L2S features that would benefit non-Korean learners
- The 50-hour size is sufficient to improve generalization across proficiency bands
- L2S feature annotation methodology is robust and reproducible

## Next Checks

1. **Cross-L1 Generalization Test**: Evaluate the fine-tuned model on spontaneous L2 speech from learners of different L1 backgrounds (e.g., Spanish, Chinese) to verify whether the Korean-specific adaptations generalize beyond the training population.

2. **Architecture Transferability Study**: Fine-tune a different ASR architecture (e.g., Conformer, Wav2Vec2) on the same LearnerVoice data to determine whether the WER improvements are model-specific or represent general benefits of L2S-aware training data.

3. **Longitudinal Performance Analysis**: Track model performance over multiple training epochs and varying dataset sizes to identify optimal training duration and determine whether additional L2 data would yield further improvements or diminishing returns.