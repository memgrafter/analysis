---
ver: rpa2
title: The Unreasonable Ineffectiveness of the Deeper Layers
arxiv_id: '2403.17887'
source_url: https://arxiv.org/abs/2403.17887
tags:
- layers
- pruning
- arxiv
- layer
- healing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how knowledge is stored in large language
  model (LLM) weights by examining layer pruning. The authors propose a method that
  identifies the optimal block of layers to prune based on similarity between layer
  representations, followed by a small amount of finetuning ("healing") using QLoRA.
---

# The Unreasonable Ineffectiveness of the Deeper Layers

## Quick Facts
- arXiv ID: 2403.17887
- Source URL: https://arxiv.org/abs/2403.17887
- Reference count: 40
- The paper finds that removing up to half of the deepest layers from large language models results in minimal performance degradation on question-answering benchmarks.

## Executive Summary
This paper investigates how knowledge is stored in large language model weights by examining layer pruning. The authors propose a method that identifies the optimal block of layers to prune based on similarity between layer representations, followed by a small amount of finetuning ("healing") using QLoRA. Surprisingly, they find that removing up to half of the deepest layers from models like Llama-2-70B results in minimal degradation of performance on question-answering benchmarks such as MMLU and BoolQ. This suggests that current pretraining methods may not fully utilize the parameters in deeper layers, or that shallow layers play a critical role in knowledge storage.

## Method Summary
The method identifies optimal layers to prune by computing angular distances between layer representations on a neutral pretraining dataset. The block with minimum angular distance is removed, and the model is healed using QLoRA finetuning on C4 data. Performance is evaluated on MMLU and BoolQ benchmarks, with normalized C4 validation loss also tracked.

## Key Results
- Removing up to half of the deepest layers from Llama-2-70B causes minimal degradation on MMLU and BoolQ
- The healing process with QLoRA restores performance to near-unpruned levels
- A sharp phase transition occurs in MMLU performance when pruning more than 50% of layers, but this disappears after healing
- Normalized C4 validation loss shows a smooth decay with increasing pruning, decoupling from QA accuracy metrics

## Why This Works (Mechanism)

### Mechanism 1
Deep layers can be pruned because their representations change slowly relative to shallow layers. The residual structure means each layer's output is a sum over all previous layer transformations plus the embedded input. If deeper layers produce similar outputs to their inputs, removing them has minimal impact.

Core assumption: Layer representations converge to a slowly changing function after some initial layers.

Evidence anchors: Abstract noting robustness to layer deletion implies either pretraining doesn't leverage deeper layers or shallow layers are critical; corpus reference to "Curse of Depth in Large Language Models."

Break condition: If layer representations change substantially between consecutive layers, pruning deep layers would cause significant performance degradation.

### Mechanism 2
Performance on knowledge-intensive tasks depends primarily on shallow layers that store and retrieve information. Shallow layers encode the essential knowledge needed for question-answering, while deeper layers handle more complex reasoning or refinement that isn't critical for these specific tasks.

Core assumption: Knowledge storage and retrieval mechanisms are localized primarily in early/middle layers.

Evidence anchors: Abstract showing minimal degradation on QA benchmarks after removing deep layers; corpus reference to "Exploring Concept Depth: How Large Language Models Acquire Knowledge and Concept at Different Layers?"

Break condition: If knowledge is delocalized across layers or reasoning tasks also depend heavily on shallow layers.

### Mechanism 3
Angular distance between layer representations can identify optimal blocks for pruning by finding regions where outputs are similar to inputs. By measuring angular distance between the input to layer ℓ and the input to layer ℓ+n, we can identify blocks where the transformation is minimal.

Core assumption: Angular distance is a reliable metric for identifying functionally redundant layers.

Evidence anchors: Abstract noting optimal block identification via similarity across layers; section describing angular distance computation between layer inputs.

Break condition: If angular distance doesn't correlate with functional redundancy.

## Foundational Learning

- Concept: Residual networks and layer decomposition
  - Why needed here: Understanding how the output is a sum over all layer transformations explains why removing similar layers has minimal impact
  - Quick check question: In a residual network with L layers, how is the final output x(L) expressed in terms of individual layer contributions?

- Concept: Angular distance and representation similarity
  - Why needed here: The pruning algorithm relies on measuring similarity between layer representations to identify redundant blocks
  - Quick check question: What mathematical operation computes the angular distance between two vectors, and why is it preferred over simple Euclidean distance for high-dimensional embeddings?

- Concept: Parameter-efficient fine-tuning (PEFT) methods
  - Why needed here: The "healing" process uses QLoRA to restore performance after pruning
  - Quick check question: How does LoRA reduce the number of trainable parameters compared to full fine-tuning, and what architectural changes does it make to the transformer?

## Architecture Onboarding

- Component map: Layer representation extraction -> Angular distance computation across layer pairs -> Optimal block identification via minimum distance search -> Layer removal by creating new module lists -> Healing via QLoRA fine-tuning on C4 data

- Critical path: The angular distance computation and block identification, as errors here directly determine which layers get pruned and whether the resulting model maintains performance.

- Design tradeoffs: The system trades precision (optimal block selection via similarity) against simplicity (dropping deepest layers heuristic), and trades healing quality (full fine-tuning vs PEFT) against computational cost.

- Failure signatures: Poor performance after pruning indicates either suboptimal block selection, insufficient healing, or violation of the slow-change assumption.

- First 3 experiments:
  1. Measure angular distances across all layer pairs for a small model to verify the slow-change pattern in deep layers exists.
  2. Implement simple deep-layer pruning (dropping last n-1 layers) and verify it preserves MMLU performance to establish baseline effectiveness.
  3. Compare similarity-informed pruning vs simple heuristic on MMLU to quantify benefit of optimal block selection.

## Open Questions the Paper Calls Out

### Open Question 1
What are better layer-pruning strategies that can maintain performance on reasoning tasks? The authors note this as open, observing that reasoning tasks like GSM8K show immediate degradation with any amount of pruning. Experiments comparing various pruning strategies on reasoning tasks would resolve this.

### Open Question 2
How does the pretraining process affect the ability to prune layers without significant performance loss? The authors discuss that current pretraining methods may not be fully leveraging deeper layers. Comparative studies of layer pruning across models with different pretraining regimes would provide evidence.

### Open Question 3
Why does healing eliminate the sharp phase transition in loss but not in QA accuracy? The authors observe this phenomenon but don't offer a theoretical explanation for why finetuning can smooth out loss curves while leaving accuracy curves with sharp transitions intact.

## Limitations
- Results may not generalize beyond Llama-2 and Qwen model families to other architectures or training paradigms
- Evaluation focuses primarily on question-answering tasks, leaving open whether deeper layers are more critical for reasoning, coding, or long-form generation
- The mechanism explaining why shallow layers store knowledge is plausible but could alternatively indicate deep layers are redundant rather than shallow layers being uniquely important

## Confidence

- **High Confidence**: Empirical observation that deep layers can be removed with minimal performance degradation on MMLU and BoolQ
- **Medium Confidence**: Proposed mechanism that angular distance identifies optimal pruning targets
- **Medium Confidence**: Interpretation that knowledge is primarily stored in shallow layers

## Next Checks

1. Apply the pruning method to models from different families (e.g., GPT-3, BLOOM) to test whether shallow-layer knowledge storage pattern holds universally

2. Test pruned models on a broader range of tasks including mathematical reasoning (GSM8K), code generation (HumanEval), and long-form generation

3. Conduct ablation studies that remove individual layers rather than blocks to map precisely which layers contain the most critical knowledge for different capabilities