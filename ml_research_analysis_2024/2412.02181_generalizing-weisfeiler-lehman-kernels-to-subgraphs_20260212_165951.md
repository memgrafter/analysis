---
ver: rpa2
title: Generalizing Weisfeiler-Lehman Kernels to Subgraphs
arxiv_id: '2412.02181'
source_url: https://arxiv.org/abs/2412.02181
tags:
- subgraph
- wlks
- graph
- subgraphs
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WLKS is a graph kernel method for subgraph representation learning
  that generalizes the Weisfeiler-Lehman algorithm to subgraphs by applying it on
  induced k-hop neighborhoods. The key insight is that combining WL histograms from
  multiple k-hop levels captures richer structural information than using a single
  k value, as we prove that (k+1)-hop histograms are not strictly more expressive
  than k-hop ones.
---

# Generalizing Weisfeiler-Lehman Kernels to Subgraphs

## Quick Facts
- arXiv ID: 2412.02181
- Source URL: https://arxiv.org/abs/2412.02181
- Authors: Dongkwan Kim; Alice Oh
- Reference count: 16
- Primary result: WLKS significantly outperforms state-of-the-art models on five real-world datasets with 0.01-0.25× faster training times without requiring GPUs or extensive hyperparameter tuning.

## Executive Summary
WLKS is a graph kernel method for subgraph representation learning that generalizes the Weisfeiler-Lehman algorithm to subgraphs by applying it on induced k-hop neighborhoods. The key insight is that combining WL histograms from multiple k-hop levels captures richer structural information than using a single k value, as we prove that (k+1)-hop histograms are not strictly more expressive than k-hop ones. Our approach uses k=0 and k=D (diameter) to balance expressiveness and efficiency, avoiding expensive neighborhood sampling. Experiments on eight real-world and synthetic benchmarks show WLKS significantly outperforms state-of-the-art models on five datasets while achieving 0.01-0.25× faster training times and not requiring GPUs, pre-computation, or extensive hyperparameter tuning.

## Method Summary
WLKS generalizes the Weisfeiler-Lehman algorithm to subgraph representation by computing WL histograms on induced k-hop neighborhoods of subgraphs within a global graph. The method strategically combines kernel matrices from k=0 (subgraph as-is) and k=D (full k-hop neighborhood up to graph diameter) using fixed weights. WLKS can optionally incorporate continuous node features through a linear combination with feature-based kernels. The resulting kernel matrix is used to train an SVM classifier for subgraph classification tasks. The approach avoids neighborhood sampling and GPU requirements by limiting k to two specific values, achieving significant efficiency gains over GNN-based methods.

## Key Results
- WLKS achieves significantly higher micro F1-scores than state-of-the-art models on five of eight benchmark datasets
- Training times are 0.01-0.25× faster than GNN-based models across all real-world datasets
- The method does not require GPUs, pre-computation, or extensive hyperparameter tuning
- Performance gains are particularly pronounced on real-world datasets (PPI-BP, HPO-Neuro, HPO-Metab, EM-User)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining WL histograms from multiple k-hop levels captures richer structural information than using a single k value.
- Mechanism: WLKS linearly combines kernel matrices across different k-hop levels to encode both local and global subgraph structures.
- Core assumption: Structural information captured at different k values is complementary rather than redundant.
- Evidence anchors:
  - [abstract]: "We combine kernels across different k-hop levels to capture richer structural information that is not fully encoded in existing models."
  - [section 3.3]: "Combining kernel matrices across multiple k-hop levels can capture richer structural information around subgraphs."
  - [corpus]: Weak evidence - no direct mention of WLKS combining multiple k values, but related work discusses k-hop GNNs.
- Break condition: If structural information at different k values is redundant or if k=0 and k=D already capture all necessary information, combining additional k values would add computational overhead without benefit.

### Mechanism 2
- Claim: (k+1)-hop WL histograms are not strictly more expressive than k-hop ones for subgraph representation.
- Mechanism: The WL algorithm's output for k-hop neighborhoods provides sufficient structural information that (k+1)-hop neighborhoods don't necessarily improve.
- Core assumption: Structural information captured at k-hop level is sufficient for distinguishing subgraph patterns.
- Evidence anchors:
  - [abstract]: "We theoretically demonstrate that WL histograms of the (k+1)-hop are not strictly more expressive than those of k-hop in distinguishing isomorphic structures."
  - [section 3.3]: Proposition 3.2 shows that non-equivalent colorings of two subgraphs in WLS k+1 do not guarantee non-equivalent colorings in WLS k.
  - [corpus]: Weak evidence - related work mentions k-hop message passing but doesn't specifically discuss expressiveness limitations.
- Break condition: If certain subgraph patterns require larger k values to distinguish, then limiting to k=0 and k=D would miss important structural distinctions.

### Mechanism 3
- Claim: WLKS achieves better efficiency than GNN-based models by avoiding neighborhood sampling and GPU requirements.
- Mechanism: By selecting k=0 and k=D, WLKS eliminates the need for neighborhood sampling and can be computed using SVMs without GPUs.
- Core assumption: The computational complexity of WLKS-{0,D} is significantly lower than deep GNNs for subgraph tasks.
- Evidence anchors:
  - [abstract]: "achieving 0.01-0.25× faster training times and not requiring GPUs, pre-computation, or extensive hyperparameter tuning."
  - [section 5]: "WLKS demonstrates significantly faster training and inference times across all real-world datasets compared to other models."
  - [section 3.4]: "We strategically limit the choice of k to two specific values: k=0 and k=D, where D is the diameter of the global graph G."
- Break condition: If the dataset size or subgraph complexity makes the WLKS computation on the entire graph prohibitive, or if GPU acceleration becomes necessary for scalability.

## Foundational Learning

- Concept: Weisfeiler-Lehman (WL) graph isomorphism test
  - Why needed here: WLKS is based on the WL algorithm, so understanding how it works is fundamental to understanding the method
  - Quick check question: What does the WL algorithm do at each iteration to refine node labels?

- Concept: Graph kernels and positive semi-definite matrices
  - Why needed here: WLKS creates kernel matrices that must be valid (positive semi-definite) for SVM training
  - Quick check question: Why must a kernel matrix be positive semi-definite for use in SVM?

- Concept: Subgraph representation learning
  - Why needed here: The paper addresses subgraph-level tasks, which differ from node or graph-level tasks
  - Quick check question: How does subgraph representation learning differ from node representation learning in terms of the structures that need to be captured?

## Architecture Onboarding

- Component map: Global graph G -> k-hop neighborhood extraction -> WL algorithm -> Histogram generation -> Kernel matrix computation -> Linear combination across k values -> SVM classifier

- Critical path:
  1. Extract k-hop neighborhoods for all subgraphs
  2. Apply WL algorithm to generate histograms
  3. Compute kernel matrix via inner products
  4. Combine kernels if using multiple k values
  5. Train SVM with kernel matrix

- Design tradeoffs:
  - Expressiveness vs efficiency: Using k=0 and k=D balances these but may miss intermediate structures
  - Fixed vs learned kernel weights: Using fixed weights (α=1) simplifies implementation but may miss optimal combinations
  - Structural vs feature information: Linearly combining with feature kernels adds expressiveness but may dilute structural information

- Failure signatures:
  - Poor performance on datasets requiring fine-grained structural distinctions between subgraphs
  - High computational cost when graph diameter D is very large
  - Suboptimal results when subgraph labels depend heavily on intermediate k-hop structures

- First 3 experiments:
  1. Run WLKS-{0} on a small dataset to verify basic functionality and check kernel matrix computation
  2. Compare WLKS-{0} vs WLKS-{0,D} on a medium-sized dataset to verify the benefit of combining k values
  3. Test WLKS-{0,D} vs a GNN baseline on a real-world dataset to verify the efficiency claims and performance gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific structural conditions (graph diameter, subgraph density, etc.) does WLKS-{0,D} outperform WLKS with intermediate k values (1<k<D-1)?
- Basis in paper: [explicit] The paper mentions that WLKS-{0,D} balances expressiveness and efficiency by avoiding neighborhood sampling, but also notes that intermediate k values may capture important substructures in datasets with large diameters, and that PPI-BP (diameter 8) performs as well with k=2 as with k=D.
- Why unresolved: The paper doesn't provide a systematic analysis of when different k combinations are optimal. The authors suggest that task nature and structural requirements should guide k selection, but don't characterize these relationships.
- What evidence would resolve it: Empirical studies varying graph diameter, subgraph density, and k values across diverse datasets to identify patterns where different k combinations excel.

### Open Question 2
- Question: Which specific node labeling methods and kernel types (beyond RWSE with linear combination) most effectively enhance WLKS expressiveness for different subgraph tasks?
- Basis in paper: [explicit] The paper discusses various node labeling approaches (zero-one labeling, SEAL's double-radius labeling, Distance Encoding, RWSE) and notes that RWSE improved performance on Cut-Ratio but not other datasets, suggesting labeling effectiveness varies by task.
- Why unresolved: The authors acknowledge this as future work, noting that determining which labeling methods, aggregations, and kernels best complement WLKS remains unexplored.
- What evidence would resolve it: Systematic experiments testing multiple labeling schemes (DE, RWSE variants, structural encodings) combined with different kernel types across diverse subgraph tasks to identify optimal combinations.

### Open Question 3
- Question: What is the theoretical expressiveness limit of WLKS compared to GNNs with similar computational complexity?
- Basis in paper: [explicit] The paper establishes that (k+1)-hop WL histograms aren't strictly more expressive than k-hop ones (Proposition 3.2), and demonstrates WLKS-{0,D} achieves competitive performance with 0.01-0.25× training time versus GNNs, but doesn't provide a formal expressiveness comparison.
- Why unresolved: While the paper shows practical effectiveness and efficiency advantages, it doesn't formally characterize the expressiveness trade-offs between WLKS and GNNs with equivalent computational budgets.
- What evidence would resolve it: Formal proofs or empirical studies establishing the representational capacity of WLKS relative to GNNs when constrained to similar computational resources, potentially using graph isomorphism testing or distinguishing power metrics.

## Limitations

- Performance on synthetic datasets (Density, Coreness, Component) is mixed, suggesting potential limitations with certain graph patterns
- Theoretical expressiveness proof relies on standard WL algorithm assumptions that may not hold for all graph structures or alternative coloring schemes
- Scalability to graphs with very large diameters is uncertain, as computational overhead increases with diameter D

## Confidence

- **High confidence**: The efficiency claims comparing WLKS to GNN-based models are well-supported by experimental results showing significantly faster training times without GPU requirements. The theoretical analysis of WL histogram expressiveness limitations is rigorous and clearly demonstrated.
- **Medium confidence**: The claim that combining k=0 and k=D hop levels captures optimal structural information balances the tradeoff between expressiveness and efficiency. While the theoretical justification is sound, the empirical validation is limited to specific datasets, and alternative k value combinations are not explored.
- **Low confidence**: The generalizability of WLKS to graphs with very large diameters or highly heterogeneous subgraph structures is uncertain. The paper does not provide extensive validation on extremely large-scale graphs or explore failure modes when the diameter D becomes prohibitively large.

## Next Checks

1. **Ablation study on k values**: Systematically evaluate WLKS performance using different combinations of k values (e.g., k=0,1,D/2,D) to quantify the marginal benefit of including k=0 and k=D versus intermediate k values, and determine if the fixed combination is truly optimal.

2. **Scalability analysis on large-diameter graphs**: Test WLKS on graphs with diameters ranging from 10 to 100+ to measure the computational overhead and identify the threshold where the method becomes impractical, comparing against alternative approaches that use neighborhood sampling.

3. **Robustness to graph heterogeneity**: Evaluate WLKS on datasets with highly heterogeneous subgraph structures (varying sizes, densities, and patterns) to assess whether the method maintains performance advantages across diverse graph types, particularly on the synthetic datasets where current results are mixed.