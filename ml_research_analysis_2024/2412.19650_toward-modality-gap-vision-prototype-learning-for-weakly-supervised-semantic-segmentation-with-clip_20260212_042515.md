---
ver: rpa2
title: 'Toward Modality Gap: Vision Prototype Learning for Weakly-supervised Semantic
  Segmentation with CLIP'
arxiv_id: '2412.19650'
source_url: https://arxiv.org/abs/2412.19650
tags:
- text
- vision
- prototypes
- semantic
- segmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the modality gap problem in weakly supervised
  semantic segmentation (WSSS) using CLIP. The core method, Vision Prototype Learning
  (VPL), learns vision prototypes in the vision space with the help of text prototypes
  to capture high-quality localization maps and proposes a regional semantic contrast
  module to align object regions with vision prototypes.
---

# Toward Modality Gap: Vision Prototype Learning for Weakly-supervised Semantic Segmentation with CLIP

## Quick Facts
- arXiv ID: 2412.19650
- Source URL: https://arxiv.org/abs/2412.19650
- Authors: Zhongxing Xu; Feilong Tang; Zhe Chen; Yingxue Su; Zhiyi Zhao; Ge Zhang; Jionglong Su; Zongyuan Ge
- Reference count: 10
- Primary result: Achieves 78.5% mIoU on VOC 2012 val and 49.2% mIoU on COCO 2014 val using Vision Prototype Learning with CLIP

## Executive Summary
This paper addresses the modality gap problem in weakly supervised semantic segmentation (WSSS) using CLIP. The core method, Vision Prototype Learning (VPL), learns class-specific vision prototypes in the vision space with the help of text prototypes to capture high-quality localization maps. The framework achieves state-of-the-art performance on PASCAL VOC 2012 and MS COCO 2014 benchmarks by effectively aligning vision prototypes with pixel-level features through KL-divergence minimization and regional semantic contrast.

## Method Summary
The proposed framework learns vision prototypes in vision space (rather than using text prototypes directly) to reduce the modality gap and improve localization accuracy. It employs a two-phase approach: Phase 1 learns vision prototypes via KL-divergence regularization with text prototypes using a larger temperature (τI=0.03) than CLIP text embeddings; Phase 2 refines pseudo-masks with a decoder and Regional Semantic Contrast (RSC) loss. The method uses frozen CLIP ViT-B-16 encoders and trains a decoder with SGD to generate segmentation masks.

## Key Results
- Achieves 78.5% mIoU on PASCAL VOC 2012 validation set
- Achieves 49.2% mIoU on MS COCO 2014 validation set
- Outperforms existing CLIP-based WSSS methods including CLIP-ES and CLIP-CPAL
- Demonstrates effective reduction of modality gap through vision prototype learning

## Why This Works (Mechanism)

### Mechanism 1
Learning vision prototypes in vision space reduces the modality gap and improves localization accuracy. The framework learns class-specific vision prototypes by minimizing KL-divergence between text prototypes and vision prototypes, aligning vision prototypes more closely with pixel-level features.

### Mechanism 2
Regional Semantic Contrast (RSC) module enhances alignment between object regions and vision prototypes by contrasting region embeddings with corresponding vision prototypes, suppressing negative masks from unrelated texts.

### Mechanism 3
Temperature tuning (τI > τT) helps identify optimal vision prototypes within the text space by smoothing the probability distribution, narrowing the modality gap and improving localization.

## Foundational Learning

- Concept: Modality Gap in Multi-modal Learning
  - Why needed here: Understanding the inherent differences between text and vision spaces in CLIP is crucial for appreciating why VPL is necessary
  - Quick check question: What causes the modality gap in CLIP and why does it affect segmentation performance?

- Concept: Contrastive Learning and KL-Divergence
  - Why needed here: VPL uses KL-divergence to align text and vision prototypes, and understanding contrastive learning is essential for grasping how CLIP works
  - Quick check question: How does minimizing KL-divergence between text and vision prototypes help reduce the modality gap?

- Concept: Class Activation Maps (CAM) and Weakly Supervised Learning
  - Why needed here: CAM is the foundation for generating localization maps in WSSS, and VPL builds upon this to improve segmentation
  - Quick check question: How do CAMs work in weakly supervised semantic segmentation and why are they limited without VPL?

## Architecture Onboarding

- Component map: CLIP encoders (frozen) -> Vision Prototype Learning module -> Regional Semantic Contrast module -> Decoder -> Post-processing
- Critical path: CLIP encoders → Vision Prototype Learning → Regional Semantic Contrast → Decoder → Post-processing → Segmentation output
- Design tradeoffs:
  - Using frozen CLIP encoders vs. fine-tuning: Frozen encoders maintain pre-trained knowledge but may limit adaptation
  - Temperature tuning (τI > τT): Helps reduce modality gap but may affect probability distribution
  - KL-divergence vs. other alignment methods: KL-divergence provides probabilistic alignment but may be sensitive to noise
- Failure signatures:
  - Poor segmentation results despite high mIoU: May indicate overfitting to validation set
  - GradCAMs not capturing complete object regions: May indicate insufficient vision prototype learning
  - Noisy pseudo-masks after post-processing: May indicate issues with regional semantic contrast
- First 3 experiments:
  1. Test VPL with different temperature values (τI) to find optimal setting
  2. Compare segmentation performance with and without Regional Semantic Contrast module
  3. Evaluate impact of different text prompt strategies on vision prototype learning

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis of modality gap reduction remains somewhat abstract
- Temperature tuning approach lacks extensive ablation studies across different settings
- Specific implementation details of Regional Semantic Contrast module not fully specified

## Confidence

- **High Confidence**: Empirical results showing state-of-the-art performance (78.5% mIoU on VOC 2012, 49.2% mIoU on COCO 2014) are well-documented and reproducible
- **Medium Confidence**: Theoretical claims about modality gap reduction and temperature tuning effectiveness are plausible but need more rigorous analysis
- **Low Confidence**: Implementation details of RSC module and exact prompt ensemble strategy are not fully specified

## Next Checks

1. **Temperature Sensitivity Analysis**: Conduct experiments with varying temperature values (τI) to determine optimal setting and assess robustness across different datasets
2. **Ablation Study of RSC Module**: Compare segmentation performance with and without Regional Semantic Contrast module to quantify its contribution
3. **Prompt Strategy Evaluation**: Test different text prompt strategies and threshold values (Φ) for seed mask generation to determine their impact on vision prototype learning and final segmentation accuracy