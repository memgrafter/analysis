---
ver: rpa2
title: 'Multilingual E5 Text Embeddings: A Technical Report'
arxiv_id: '2402.05672'
source_url: https://arxiv.org/abs/2402.05672
tags:
- linguistics
- multilingual
- association
- text
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This technical report presents multilingual E5 text embedding models,
  which extend the English E5 models through a two-stage training procedure involving
  contrastive pre-training on 1 billion multilingual text pairs followed by supervised
  fine-tuning on 1.6 million labeled data. Three model sizes (small/base/large) are
  released, along with an instruction-tuned variant.
---

# Multilingual E5 Text Embeddings: A Technical Report

## Quick Facts
- arXiv ID: 2402.05672
- Source URL: https://arxiv.org/abs/2402.05672
- Authors: Liang Wang; Nan Yang; Xiaolong Huang; Linjun Yang; Rangan Majumder; Furu Wei
- Reference count: 26
- Key outcome: Multilingual E5 text embedding models achieve strong performance on English tasks, surpassing previous multilingual models and English-only models of comparable sizes on the MTEB benchmark

## Executive Summary
This technical report presents multilingual E5 text embedding models (mE5) that extend the English E5 models through a two-stage training procedure. The models undergo contrastive pre-training on 1 billion multilingual text pairs followed by supervised fine-tuning on 1.6 million labeled data. Three model sizes (small/base/large) are released, along with an instruction-tuned variant. The models demonstrate strong performance on English tasks, surpassing previous multilingual models and English-only models of comparable sizes on the MTEB benchmark, while also showing competitive performance on multilingual retrieval tasks across 16 languages and bitext mining across 100+ languages.

## Method Summary
The multilingual E5 text embedding models are trained using a two-stage procedure. First, contrastive pre-training is performed on 1 billion multilingual text pairs from diverse sources including Wikipedia, mC4, multilingual CC News, NLLB, Reddit, S2ORC, Stackexchange, xP3, and miscellaneous SBERT data. This stage uses a large batch size (32k) and InfoNCE loss with in-batch negatives. Second, supervised fine-tuning is applied using 1.6 million labeled data from MS-MARCO, NQ, TriviaQA, SQuAD, NLI, ELI5, DuReader, Fever, HotpotQA, Quora, Mr. TyDi, and MIRACL. The fine-tuning stage incorporates mined hard negatives and knowledge distillation from a cross-encoder model. An instruction-tuned variant (mE5-large-instruct) is also trained using 500k synthetic data generated by GPT-3.5/4 with 150k unique instructions covering 93 languages.

## Key Results
- Achieves strong performance on English tasks, surpassing previous multilingual models and English-only models of comparable sizes on MTEB benchmark
- Demonstrates competitive performance on multilingual retrieval tasks across 16 languages in MIRACL benchmark
- Shows effectiveness for bitext mining across 100+ languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual contrastive pre-training on diverse text pairs enables effective cross-lingual alignment
- Mechanism: The model learns to map semantically similar text pairs close together in embedding space, creating a shared semantic space across languages
- Core assumption: Weakly-supervised text pairs contain sufficient semantic signal for meaningful contrastive learning
- Evidence anchors: [abstract] states "contrastive pre-training on 1 billion multilingual text pairs"; [section] describes data mixture including translation pairs, titles/abstracts, and comments/responses across 35 European languages; [corpus] shows related work also uses contrastive pre-training on multilingual data

### Mechanism 2
- Claim: Instruction-tuning with synthetic data improves task-specific embedding quality
- Mechanism: Adding task descriptions helps the model understand the semantic similarity task better, producing more discriminative embeddings for retrieval
- Core assumption: Synthetic instruction data covers the task distribution adequately
- Evidence anchors: [abstract] introduces "instruction-tuned embedding model" whose "performance is on par with state-of-the-art, English-only models"; [section] describes using "500k synthetic data generated by GPT-3.5/4" with "150k unique instructions and covers 93 languages"; [corpus] shows other multilingual models also use instruction tuning

### Mechanism 3
- Claim: Two-stage training (pre-training + supervised fine-tuning) creates better embeddings than single-stage approaches
- Mechanism: Pre-training establishes general semantic understanding, while fine-tuning adapts to specific retrieval tasks using labeled data and hard negatives
- Core assumption: The combination of weakly-supervised and supervised learning provides complementary benefits
- Evidence anchors: [abstract] describes "two-stage training procedure involving contrastive pre-training... followed by supervised fine-tuning"; [section] details fine-tuning stage uses "mined hard negatives and knowledge distillation from a cross-encoder model"; [corpus] shows other successful embedding models use similar two-stage approaches

## Foundational Learning

- Contrastive learning:
  - Why needed here: Enables the model to learn semantic similarity without explicit labels by pulling positive pairs together and pushing negative pairs apart
  - Quick check question: What happens to embeddings of semantically similar text pairs after contrastive training?

- Multilingual representation learning:
  - Why needed here: Allows the model to handle multiple languages in a single embedding space, crucial for cross-lingual retrieval
  - Quick check question: How does the model handle languages with different scripts or word orders?

- Hard negative mining:
  - Why needed here: Improves retrieval quality by explicitly training the model to distinguish between semantically similar but different passages
  - Quick check question: Why are hard negatives more effective than random negatives in training embedding models?

## Architecture Onboarding

- Component map:
  - Text encoder (XLM-Roberta or MiniLM backbone) -> Contrastive loss layer (InfoNCE with in-batch negatives) -> Fine-tuning pipeline (labeled datasets + hard negatives + knowledge distillation) -> Instruction template system (for synthetic data generation)

- Critical path:
  1. Load pre-trained multilingual backbone
  2. Apply contrastive pre-training on text pairs
  3. Fine-tune with labeled data and hard negatives
  4. (Optional) Apply instruction tuning with synthetic data
  5. Export for inference

- Design tradeoffs:
  - Model size vs. inference efficiency (small/base/large variants)
  - Pre-training data diversity vs. computational cost
  - Instruction tuning benefits vs. synthetic data quality
  - Hard negative mining benefits vs. additional complexity

- Failure signatures:
  - Poor multilingual performance: Check data distribution across languages
  - Weak retrieval metrics: Verify hard negative mining implementation
  - Slow inference: Consider smaller model variant or optimization
  - Instruction tuning not helping: Check synthetic data quality and relevance

- First 3 experiments:
  1. Compare single-stage vs. two-stage training on MTEB English subset
  2. Test different negative sampling strategies (random vs. hard negatives)
  3. Evaluate instruction tuning impact by training with/without synthetic data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the multilingual E5 text embedding models perform on languages not included in the MIRACL benchmark, particularly low-resource languages?
- Basis in paper: [inferred] The paper evaluates performance on 16 languages in the MIRACL benchmark and mentions the models' capability across "100+ languages" for bitext mining, but does not provide detailed performance metrics for a broader range of languages.
- Why unresolved: The evaluation focuses on a subset of languages, leaving the performance on a wider variety of languages, especially low-resource ones, unexplored.
- What evidence would resolve it: Conducting comprehensive evaluations across a more diverse set of languages, including low-resource ones, and reporting detailed performance metrics for each.

### Open Question 2
- Question: What is the impact of the synthetic data generated by GPT-3.5/4 on the performance of the instruction-tuned embedding model across different languages?
- Basis in paper: [explicit] The paper mentions the inclusion of 500k synthetic data generated by GPT-3.5/4 for the mE5-large-instruct model and notes expanded language coverage, but does not detail the impact on performance across different languages.
- Why unresolved: While the synthetic data's role in expanding language coverage is mentioned, its specific impact on model performance across various languages is not quantified or analyzed.
- What evidence would resolve it: Detailed performance comparisons of the instruction-tuned model with and without synthetic data across multiple languages, highlighting improvements or regressions.

### Open Question 3
- Question: How do the multilingual E5 text embedding models compare to monolingual models in terms of computational efficiency and storage requirements when deployed in real-world applications?
- Basis in paper: [inferred] The paper discusses the balance between inference efficiency and embedding quality, and mentions faster inference and reduced storage costs for smaller models, but does not provide a direct comparison with monolingual models in practical scenarios.
- Why unresolved: The paper provides theoretical insights into the trade-offs between model size and efficiency but lacks empirical data on real-world deployment scenarios comparing multilingual and monolingual models.
- What evidence would resolve it: Empirical studies comparing the computational efficiency, storage requirements, and performance of multilingual versus monolingual models in real-world applications across different tasks and languages.

## Limitations
- The quality and diversity of the 500k synthetic examples generated by GPT-3.5/4 for instruction tuning remain uncertain
- The two-stage training procedure requires substantial computational resources (1 billion contrastive pairs, 1.6 million labeled examples)
- Performance gains from instruction tuning versus standard fine-tuning are not thoroughly quantified

## Confidence

**High Confidence:** The effectiveness of the two-stage training procedure (contrastive pre-training + supervised fine-tuning) for multilingual text embeddings. This claim is supported by strong performance on MTEB English tasks and systematic comparison with baseline approaches.

**Medium Confidence:** The claim that the models achieve "competitive performance on multilingual retrieval tasks across 16 languages." While MIRACL benchmark results are presented, the report lacks detailed analysis of performance variance across language families and script types.

**Medium Confidence:** The assertion that instruction tuning with synthetic data produces embeddings "on par with state-of-the-art, English-only models." This claim is based on MTEB benchmark comparisons, but the report doesn't adequately address whether synthetic instruction data adequately captures the task distribution across all 93 languages.

## Next Checks

1. **Synthetic Data Quality Analysis**: Conduct a systematic evaluation of the synthetic instruction data quality by sampling and manually reviewing examples across different language families. Compare the distribution of synthetic instructions against real user queries in multilingual settings to identify potential gaps.

2. **Cross-Lingual Transfer Analysis**: Perform ablation studies to quantify the contribution of each language family to overall multilingual performance. Test whether languages from underrepresented families (e.g., languages using non-Latin scripts) show degraded performance compared to Indo-European languages.

3. **Computational Efficiency Benchmarking**: Measure inference latency and memory usage across the three model sizes (small/base/large) on representative hardware. Compare these metrics against previous multilingual models to quantify the practical deployment trade-offs.