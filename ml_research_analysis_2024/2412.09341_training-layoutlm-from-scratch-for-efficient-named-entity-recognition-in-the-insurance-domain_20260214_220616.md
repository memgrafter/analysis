---
ver: rpa2
title: Training LayoutLM from Scratch for Efficient Named-Entity Recognition in the
  Insurance Domain
arxiv_id: '2412.09341'
source_url: https://arxiv.org/abs/2412.09341
tags: []
core_contribution: This paper addresses the challenge of named-entity recognition
  (NER) in the insurance domain, where domain-specific data is scarce due to privacy
  constraints. The authors propose training LayoutLM from scratch using the DOCILE
  dataset of invoices, which is semantically and structurally similar to insurance
  pay statements, instead of using generic pre-trained models.
---

# Training LayoutLM from Scratch for Efficient Named-Entity Recognition in the Insurance Domain

## Quick Facts
- arXiv ID: 2412.09341
- Source URL: https://arxiv.org/abs/2412.09341
- Reference count: 28
- Key outcome: Training LayoutLM from scratch on domain-similar invoices (DOCILE) achieves higher F1-scores (64.74%) on insurance pay statements than using generic pre-trained models (62.31%), with reduced variance and faster inference using 6 layers.

## Executive Summary
This paper addresses the challenge of named-entity recognition (NER) in the insurance domain, where domain-specific data is scarce due to privacy constraints. The authors propose training LayoutLM from scratch using the DOCILE dataset of invoices, which is semantically and structurally similar to insurance pay statements, instead of using generic pre-trained models. They evaluate their approach on a novel PAYSLIPS dataset of anonymized insurance pay statements. Results show that their model achieves higher F1-scores (64.74%) compared to the original LayoutLM (62.31%) and exhibits lower variance. Additionally, they demonstrate that using only 6 layers of LayoutLM maintains competitive performance while reducing inference time by nearly half.

## Method Summary
The authors pre-train LayoutLM from scratch on the DOCILE dataset of invoices using Masked Language Modeling (MLM) loss for 5 epochs. They then fine-tune the pre-trained models on the PAYSLIPS dataset of anonymized insurance pay statements using a BIO-tagging approach for 10 epochs. The performance is evaluated using F1-score and compared against the original LayoutLM pre-trained on IIT-CDIP. The authors also experiment with different layer configurations (1, 2, 6, and 12 layers) to assess the trade-off between performance and inference time.

## Key Results
- Pre-training LayoutLM on DOCILE achieves higher F1-scores (64.74%) on PAYSLIPS compared to using the original LayoutLM pre-trained on IIT-CDIP (62.31%).
- The model pre-trained on DOCILE exhibits lower variance in F1-scores across fine-tuning runs compared to the original LayoutLM.
- Using only 6 layers of LayoutLM maintains competitive performance while reducing inference time by nearly half.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training on domain-similar documents (DOCILE) improves NER performance on insurance pay statements compared to generic pre-training (IIT-CDIP).
- Mechanism: The model learns domain-specific layout and textual patterns that are semantically and structurally similar to the target task, reducing domain mismatch.
- Core assumption: Layout and semantic similarity between DOCILE and PAYSLIPS is sufficient for transfer learning benefits.
- Evidence anchors:
  - [abstract] "using domain-relevant documents improves results on a named-entity recognition (NER) problem using a novel dataset of anonymized insurance-related financial documents"
  - [section 4.2] "we observed that LAYOUT LM tends to under-perform on our internal data. We suspect IIT-CDIP documents are too different in form and content from insurance documents"
  - [corpus] Weak: No direct corpus neighbor supports this mechanism specifically; the related papers focus on different domains (historical documents, biomedical, etc.)
- Break condition: If the domain similarity between DOCILE and PAYSLIPS is insufficient, or if DOCILE lacks critical layout patterns present in PAYSLIPS.

### Mechanism 2
- Claim: Reducing the number of LAYOUTLM layers from 12 to 6 maintains competitive performance while significantly reducing inference time.
- Mechanism: The self-attentive layers in LAYOUTLM exhibit diminishing returns beyond a certain depth for this specific NER task, allowing for aggressive pruning.
- Core assumption: The essential information for this NER task is captured in the first 6 layers, and deeper layers provide marginal gains.
- Evidence anchors:
  - [abstract] "using only 6 layers of LayoutLM maintains competitive performance while reducing inference time by nearly half"
  - [section 5.1] "when pre-training on DOCILE using only 6 layers, we achieve comparable scores to the off-the-shelf LAYOUT LM model, while dividing the inference time by almost 2"
  - [section 5.1] "On PAYSLIPS, when pre-training on DOCILE using only 6 layers, we achieve comparable scores to the off-the-shelf LAYOUT LM model"
  - [corpus] Weak: No direct corpus neighbor supports this mechanism specifically; related papers focus on different architectural changes.
- Break condition: If the task complexity increases or if the essential features require deeper representations that are not captured in the first 6 layers.

### Mechanism 3
- Claim: The variance in model performance is lower when pre-training on DOCILE compared to IIT-CDIP.
- Mechanism: Domain-relevant pre-training provides a more stable starting point for fine-tuning, reducing sensitivity to random initialization and data shuffling.
- Core assumption: The DOCILE pre-training provides better inductive biases for the PAYSLIPS task, leading to more consistent optimization trajectories.
- Evidence anchors:
  - [section 5.1] "Moreover, we observe that our pre-trained model exhibits a way lower variance between fine-tuning runs"
  - [section 5.1] "LAYOUT LM BASE DOCILE 58.30 ± 1.52 64.74 ± 2.92" vs "LAYOUT LM BASE IIT-CDIP 58.35 ± 1.63 62.31 ± 5.13"
  - [corpus] Weak: No direct corpus neighbor supports this mechanism specifically.
- Break condition: If the dataset size increases significantly, reducing the impact of initialization, or if the task becomes more complex and requires more diverse pre-training data.

## Foundational Learning

- Concept: Domain adaptation through pre-training
  - Why needed here: Generic pre-trained models struggle with specialized domains due to domain mismatch between training data and downstream tasks.
  - Quick check question: What is the primary reason why generic pre-trained models may underperform on specialized domain tasks like insurance NER?

- Concept: Self-attention and transformer architecture
  - Why needed here: LAYOUTLM uses self-attention to capture both textual and spatial information in documents, which is crucial for understanding document layout.
  - Quick check question: How does LAYOUTLM extend BERT's positional embeddings to incorporate spatial information from documents?

- Concept: Statistical significance testing for small datasets
  - Why needed here: The PAYSLIPS dataset is small, making it essential to validate that performance differences are not due to randomness.
  - Quick check question: What statistical method is used to test the significance of F1-score differences between models on small test sets?

## Architecture Onboarding

- Component map:
  - Input: Tokenized words + 2D positional embeddings (6-tuples of discretized coordinates and sizes)
  - Core: Self-attentive transformer layers (12, 6, 2, or 1 layer variants)
  - Output: Linear projection to vocabulary space for MLM loss
  - Loss: Masked Language Modeling (MLM) during pre-training, BIO tagging for NER during fine-tuning

- Critical path:
  1. Pre-train LAYOUTLM from scratch on DOCILE using MLM loss
  2. Fine-tune on PAYSLIPS using BIO tagging approach
  3. Evaluate using labeled F1-score

- Design tradeoffs:
  - Using DOCILE vs IIT-CDIP: Smaller dataset but domain-relevant vs larger but generic
  - Layer count: More layers = better performance but slower inference vs fewer layers = faster but potentially lower performance
  - Image embeddings: Including vs excluding image embeddings (excluded due to slowing down without significant impact)

- Failure signatures:
  - Low F1 scores across all models: Dataset annotation issues or task definition problems
  - High variance between runs: Insufficient pre-training data or unstable fine-tuning procedure
  - Minimal difference between DOCILE and IIT-CDIP pre-training: Domain similarity insufficient or pre-training data too small

- First 3 experiments:
  1. Pre-train LAYOUTLM on DOCILE for 5 epochs with MLM loss, batch size 80, learning rate 5e-5
  2. Fine-tune on PAYSLIPS for 10 epochs with batch size 16, learning rate 5e-5
  3. Repeat experiment with 6-layer LAYOUTLM variant and compare inference times

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LayoutLM models pre-trained on DOCILE compare to models pre-trained on other domain-specific datasets of similar size?
- Basis in paper: [inferred] The paper shows that pre-training LayoutLM on DOCILE improves performance on PAYSLIPS compared to pre-training on IIT-CDIP, but doesn't compare against other domain-specific datasets.
- Why unresolved: The authors only tested pre-training on IIT-CDIP and DOCILE, leaving the question of how DOCILE specifically compares to other potential domain-relevant datasets unanswered.
- What evidence would resolve it: Experimental results comparing LayoutLM models pre-trained on DOCILE against models pre-trained on other domain-specific datasets of similar size and composition.

### Open Question 2
- Question: What is the impact of using different OCR systems or preprocessing techniques on the performance of LayoutLM models for NER in insurance documents?
- Basis in paper: [inferred] The paper mentions that documents were processed through an in-house OCR solution, but doesn't explore the effect of different OCR systems or preprocessing methods on model performance.
- Why unresolved: The authors don't provide information on how the choice of OCR system or preprocessing techniques might affect the model's ability to perform NER on insurance documents.
- What evidence would resolve it: Comparative results showing the performance of LayoutLM models using different OCR systems or preprocessing techniques on the same dataset.

### Open Question 3
- Question: How does the performance of LayoutLM models for NER in insurance documents generalize to other types of financial documents or other specialized domains?
- Basis in paper: [explicit] The authors discuss the challenges of domain mismatch and the scarcity of in-domain data, but only test their approach on insurance pay statements.
- Why unresolved: The paper focuses on a specific domain (insurance) and a specific type of document (pay statements), without exploring how well the approach generalizes to other financial documents or specialized domains.
- What evidence would resolve it: Experimental results demonstrating the performance of LayoutLM models pre-trained on DOCILE or similar datasets on other types of financial documents or specialized domains.

### Open Question 4
- Question: What is the optimal number of layers for LayoutLM models when balancing computational efficiency and NER performance across different domains?
- Basis in paper: [explicit] The authors show that using 6 layers instead of 12 maintains competitive performance while reducing inference time, but don't explore other layer configurations or their performance across different domains.
- Why unresolved: The paper only tests a few layer configurations (1, 2, 6, and 12 layers) and doesn't provide a comprehensive analysis of the trade-off between computational efficiency and performance across different domains.
- What evidence would resolve it: A systematic study exploring the performance of LayoutLM models with different numbers of layers on various domains, providing insights into the optimal configuration for balancing efficiency and performance.

## Limitations
- The proprietary nature of the PAYSLIPS and DOCILE datasets limits reproducibility and external validation of the results.
- The domain similarity between DOCILE and PAYSLIPS is based on structural and semantic reasoning rather than quantitative measurement, introducing uncertainty about the true extent of domain transfer benefits.
- The generalizability of findings beyond insurance pay statements remains uncertain due to the specialized nature of the datasets and the lack of testing on other domain-specific document types.

## Confidence
- High Confidence: The observation that LAYOUTLM underperforms on PAYSLIPS compared to generic documents (IIT-CDIP) is well-supported by empirical evidence (F1 scores of 62.31% vs 64.74% for DOCILE pre-training). The variance reduction claim (2.92 vs 5.13 standard deviation in F1) is also strongly supported by reported metrics.
- Medium Confidence: The inference that domain-relevant pre-training improves performance is logically sound but relies on the assumption that DOCILE is sufficiently similar to PAYSLIPS. The paper provides reasoning but no quantitative validation of this similarity. The claim about layer reduction maintaining performance is supported by results but only tested on one specific dataset.
- Low Confidence: The generalizability of findings beyond insurance pay statements remains uncertain due to the specialized nature of the datasets and the lack of testing on other domain-specific document types.

## Next Checks
1. **Quantitative Domain Similarity Analysis**: Measure and report specific similarity metrics (e.g., layout complexity, entity distribution overlap, textual similarity scores) between DOCILE and PAYSLIPS to substantiate the claimed domain relevance.
2. **Cross-Domain Generalization Test**: Evaluate the 6-layer LAYOUTLM pre-trained on DOCILE on at least one additional domain-specific document type (e.g., medical bills, bank statements) to assess generalizability beyond insurance documents.
3. **Ablation Study on Layer Reduction**: Systematically test the performance of LAYOUTLM variants with 3, 4, 8, and 10 layers to better understand the relationship between depth and performance, providing clearer guidance on optimal layer selection for similar tasks.