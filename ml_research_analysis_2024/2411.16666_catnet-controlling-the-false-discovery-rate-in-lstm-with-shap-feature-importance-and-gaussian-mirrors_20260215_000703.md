---
ver: rpa2
title: 'CatNet: Controlling the False Discovery Rate in LSTM with SHAP Feature Importance
  and Gaussian Mirrors'
arxiv_id: '2411.16666'
source_url: https://arxiv.org/abs/2411.16666
tags:
- feature
- value
- features
- importance
- shap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CatNet introduces a new framework for controlling the False Discovery
  Rate in LSTM models by combining SHAP feature importance with Gaussian Mirrors.
  The method addresses the challenge of interpreting complex neural networks by quantifying
  feature importance through the derivative of SHAP values, which captures global
  feature contributions while accounting for feature correlations.
---

# CatNet: Controlling the False Discovery Rate in LSTM with SHAP Feature Importance and Gaussian Mirrors

## Quick Facts
- **arXiv ID**: 2411.16666
- **Source URL**: https://arxiv.org/abs/2411.16666
- **Reference count**: 40
- **Primary result**: CatNet framework controls FDR in LSTM feature selection while maintaining high statistical power across different model settings and dimensions

## Executive Summary
CatNet introduces a novel framework for controlling the False Discovery Rate in LSTM models by combining SHAP feature importance with Gaussian Mirrors. The method addresses the challenge of interpreting complex neural networks by quantifying feature importance through the derivative of SHAP values, which captures global feature contributions while accounting for feature correlations. To handle time-series correlations that can cause multicollinearity in LSTM training, CatNet extends the kernel-based dependence measure to include time-series cross-correlations. Evaluations using both simulated and real-world data show that CatNet effectively controls FDR while maintaining high statistical power across different model settings and dimensions.

## Method Summary
CatNet combines SHAP feature importance, Gaussian Mirrors, and an extended kernel-based dependence measure for time-series data to control FDR in LSTM feature selection. The method computes SHAP values for each feature in the trained LSTM, calculates derivatives of these SHAP values as feature importance vectors, and constructs mirror variables using a time-series kernel that accounts for temporal cross-correlations. Mirror statistics are computed using inner products and L1-norms, and FDR control is applied to select significant features. The approach is evaluated on both simulated Brownian Motion data and real-world stock market data, comparing performance against baseline LSTM and original Gaussian Mirror methods.

## Key Results
- CatNet effectively controls FDR at preset levels while maintaining high statistical power across different model settings and dimensions
- The method outperforms traditional LSTM models in stock price prediction while successfully identifying significant market-driving factors
- Time-series kernel dependence measure reduces variance in FDR estimates compared to standard Gaussian Mirror, though improvements in mean FDR are modest

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SHAP derivatives provide a global, path-aware feature importance metric that captures nonlinear correlations between features in LSTM models.
- Mechanism: Instead of using simple partial derivatives which assume feature independence, SHAP values compute the weighted average contribution of each feature across all possible feature coalitions. Taking the derivative of these SHAP values along the feature's path through the input space captures how the feature's importance changes with its value while accounting for correlations with other features.
- Core assumption: The SHAP value function is smooth enough to allow meaningful derivatives to be computed along the feature's path.
- Evidence anchors:
  - [abstract]: "CatNet employs the derivative of SHAP values to quantify the feature importance"
  - [section 2.4.2]: "we can use the partial derivative of the SHAP value relative to the input feature as a more reliable importance metric"
  - [corpus]: Weak evidence - no direct citations about SHAP derivatives in LSTMs found in neighbors
- Break condition: If the SHAP value function is not smooth or if the feature space is too sparse to compute meaningful derivatives along paths.

### Mechanism 2
- Claim: The time-series kernel dependence measure effectively prevents multicollinearity issues in LSTM training by capturing temporal cross-correlations.
- Mechanism: Traditional HSIC only captures instantaneous correlations between features. The extended time-series kernel computes HSIC for time-lagged pairs and takes a weighted sum, ensuring that mirror variables created for FDR control are uncorrelated both in current time and across the temporal dimension that LSTMs explicitly model.
- Core assumption: The weighted sum of time-lagged HSIC values adequately captures all temporal dependencies that could cause multicollinearity in LSTM weight updates.
- Evidence anchors:
  - [section 2.5]: "we extend the Kernel-based Dependence Measure to consider the time-series cross-correlation of input features"
  - [section 3.2]: "the time-series kernel does not clearly improve the mean value of FDR and power, but it clearly reduces the variance"
  - [corpus]: Weak evidence - no direct citations about time-series kernel dependence in LSTMs found in neighbors
- Break condition: If temporal dependencies are non-stationary or if the look-back window is too large for the weighted sum to capture all relevant correlations.

### Mechanism 3
- Claim: The vector-formed mirror statistic using inner product and L1-norm maintains FDR control properties while handling the vector nature of SHAP derivatives.
- Mechanism: Instead of scalar feature importance values, SHAP derivatives are vectors representing importance across all input values. The mirror statistic uses the normalized inner product (capturing directional alignment) multiplied by the maximum L1-norm (capturing magnitude), creating a symmetric statistic around zero for null features while being large for non-null features.
- Core assumption: The null feature SHAP derivatives are symmetric around zero in both direction and magnitude, making the inner product symmetric.
- Evidence anchors:
  - [section 2.3]: "we consider using the Inner Product (Standardized by L2-norm) as a substitute for the sgn function"
  - [section 2.6]: "For a null feature j, the expectation of the SHAP value in all values of xj is zero"
  - [corpus]: Weak evidence - no direct citations about vector mirror statistics in LSTMs found in neighbors
- Break condition: If the null feature SHAP derivatives are not symmetric around zero due to model bias or if the inner product fails to capture the relevant directional information.

## Foundational Learning

- Concept: SHAP (SHapley Additive exPlanations) values and their properties
  - Why needed here: Understanding SHAP is essential because CatNet uses SHAP derivatives as the feature importance metric, which is a key innovation over previous methods
  - Quick check question: What property of SHAP values ensures that their sum equals the difference between model prediction and expected prediction?

- Concept: False Discovery Rate (FDR) control and multiple testing
  - Why needed here: The entire CatNet framework is built around controlling FDR in feature selection for LSTM models, so understanding FDR concepts is fundamental
  - Quick check question: How does the Benjamini-Hochberg procedure control FDR, and why is it difficult to apply directly to neural networks?

- Concept: Kernel-based dependence measures and HSIC
  - Why needed here: The time-series kernel dependence measure is crucial for preventing multicollinearity in the Gaussian Mirror construction, which is essential for stable LSTM training
  - Quick check question: What is the relationship between HSIC with linear kernels and Pearson correlation coefficient?

## Architecture Onboarding

- Component map:
  Data preprocessing -> LSTM training -> SHAP value computation -> SHAP derivative calculation -> Time-series kernel dependence measure -> Mirror variable construction -> Mirror statistic computation -> FDR control thresholding -> Feature selection
  Key components: LSTM model, SHAP explainer, kernel dependence calculator, mirror statistic generator

- Critical path:
  1. Train LSTM with original features
  2. Compute SHAP values for all features
  3. Calculate SHAP derivatives as feature importance vectors
  4. Construct mirror variables using time-series kernel dependence
  5. Compute mirror statistics using inner product and norms
  6. Apply FDR control to select significant features

- Design tradeoffs:
  - SHAP vs. partial derivatives: SHAP captures global importance but is computationally expensive; partial derivatives are fast but assume independence
  - Time-series kernel vs. standard kernel: Time-series kernel prevents multicollinearity but adds computational overhead
  - Vector mirror statistic vs. scalar: Vector approach handles nonlinear importance but requires more complex computation

- Failure signatures:
  - High variance in FDR estimates: Likely due to multicollinearity not being fully addressed
  - Mirror statistics taking large negative values: Indicates model instability or unexpected feature correlations
  - Low power in linear link functions: May indicate LSTM overfitting to linear relationships

- First 3 experiments:
  1. Run CatNet on a simple linear model with known ground truth to verify FDR control properties
  2. Test time-series kernel dependence measure on correlated time series to ensure it captures temporal dependencies
  3. Compare SHAP derivatives vs. partial derivatives on a nonlinear synthetic dataset to validate the global importance claim

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How should the weights for the time-series kernel dependence measure be optimally chosen?
- Basis in paper: Explicit - The paper notes that the weights w_τ in the kernel dependence measure are chosen empirically as w_τ = a · exp(-1/10 · τ), but acknowledges this is not an optimal choice and that finding an explicit formula would be valuable.
- Why unresolved: The paper demonstrates that different weight choices can affect the stability of the mirror statistic, but does not provide a systematic method for determining optimal weights based on the LSTM architecture or data characteristics.
- What evidence would resolve it: Empirical studies comparing different weighting schemes across various LSTM configurations and datasets, or theoretical analysis linking optimal weights to properties of LSTM memory and attention mechanisms.

### Open Question 2
- Question: Can CatNet be effectively extended to transformer models and other attention-based architectures?
- Basis in paper: Explicit - The authors state that CatNet "can be easily extended to other time-series prediction models such as RNN and GRU" and "may also be generalized to Attention models with minor changes," but do not provide experimental validation.
- Why unresolved: The paper focuses on LSTM specifically, and while the framework seems applicable to attention mechanisms, the interaction between SHAP values, Gaussian Mirrors, and attention weights requires further investigation.
- What evidence would resolve it: Implementation and evaluation of CatNet on transformer architectures like BERT or GPT for sequential data tasks, comparing performance to standard feature importance methods.

### Open Question 3
- Question: What is the theoretical relationship between the FDR control achieved by CatNet and the choice of link function in LSTM models?
- Basis in paper: Inferred - The paper shows empirically that different link functions (linear, sinusoidal, arcsin) affect FDR control and power, but does not provide theoretical analysis of why these differences occur or how to predict them.
- Why unresolved: While the authors observe that nonlinear link functions maintain higher power than linear ones, they do not explain the underlying statistical properties that cause this difference or provide guidance on choosing appropriate link functions for FDR control.
- What evidence would resolve it: Theoretical analysis connecting the curvature and smoothness properties of different link functions to the stability of the mirror statistic and resulting FDR control, potentially using techniques from nonparametric statistics.

## Limitations
- SHAP derivatives rely on untested assumptions about the smoothness of SHAP value functions in neural networks
- Time-series kernel dependence measure shows only modest improvements in FDR control while adding computational complexity
- Vector-formed mirror statistic introduces additional assumptions about symmetry of null feature SHAP derivatives that may not hold in practice

## Confidence
- **High Confidence**: The overall framework of combining SHAP with Gaussian Mirrors for FDR control in feature selection
- **Medium Confidence**: The specific implementation of time-series kernel dependence measure and its effectiveness in preventing multicollinearity
- **Low Confidence**: The claim that SHAP derivatives capture nonlinear feature correlations better than partial derivatives

## Next Checks
1. **Ground Truth Verification**: Apply CatNet to a simple linear model with known feature importance to verify that it correctly controls FDR and identifies true features
2. **SHAP Derivative Validation**: Compare SHAP derivatives vs. partial derivatives on a nonlinear synthetic dataset with known correlation structure
3. **Computational Complexity Analysis**: Benchmark the runtime and memory requirements of CatNet vs. standard LSTM with feature selection on datasets of increasing size