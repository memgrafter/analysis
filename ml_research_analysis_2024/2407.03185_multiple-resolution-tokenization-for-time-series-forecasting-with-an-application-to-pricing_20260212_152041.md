---
ver: rpa2
title: Multiple-Resolution Tokenization for Time Series Forecasting with an Application
  to Pricing
arxiv_id: '2407.03185'
source_url: https://arxiv.org/abs/2407.03185
tags:
- series
- which
- time
- tokens
- auxiliary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Multiple-Resolution Tokenization (MRT), a transformer-based
  architecture for time series forecasting that uses multi-scale tokenisation and
  explicit modelling of auxiliary variables. MRT divides past data and auxiliary information
  into tokens at multiple resolutions, processes them separately, and combines them
  using self-attention.
---

# Multiple-Resolution Tokenization for Time Series Forecasting with an Application to Pricing

## Quick Facts
- arXiv ID: 2407.03185
- Source URL: https://arxiv.org/abs/2407.03185
- Authors: Egon Peršak; Miguel F. Anjos; Sebastian Lautz; Aleksandar Kolev
- Reference count: 10
- Key outcome: MRT outperforms existing architectures on MSE/MAE metrics for pricing forecasting

## Executive Summary
This paper introduces Multiple-Resolution Tokenization (MRT), a transformer-based architecture for time series forecasting that uses multi-scale tokenization and explicit modeling of auxiliary variables. MRT divides past data and auxiliary information into tokens at multiple resolutions, processes them separately, and combines them using self-attention. A novel output head uses reverse splitting to scale efficiently with more tokens. Applied to a real markdown pricing problem at a large retailer, MRT outperforms in-house models and existing architectures like PatchTST and DLinear on MSE and MAE metrics. Ablations on a public retail dataset show multi-resolution tokenization improves performance over single-resolution approaches, and including time-varying known tokens (e.g., price) further boosts results.

## Method Summary
MRT is a transformer-based architecture that tokenizes past data at multiple resolutions (K={1,2,3,4,6,8}) and processes auxiliary variables separately. The architecture concatenates MRP tokens (multiple resolution patching of past data), auxiliary tokens (static, time-varying known, and cross-series), and passes them through a single self-attention layer. A reverse splitting output head projects MRP tokens back to forecast segments proportionally, reducing parameters and encouraging token specialization. The model is trained using Adam optimizer with learning rate 0.0003, MSE loss, batch size 128, and early stopping after 3 epochs without improvement.

## Key Results
- MRT achieves 9.15% MSE improvement over PatchTST on real markdown pricing data
- MRT achieves 7.41% MSE improvement over DLinear on real markdown pricing data
- On Favorita dataset, MRT with multiple resolutions outperforms single-resolution baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiple-resolution tokenization allows the model to capture relevant dynamics at different scales by explicitly modeling pairwise relationships across all tokens in the same attention mechanism.
- Mechanism: By splitting past data into tokens at multiple resolutions and concatenating them with auxiliary variable tokens before passing to a single self-attention layer, MRT enables cross-resolution interactions and allows each token to attend to others at different granularities.
- Core assumption: Different scales of temporal patterns (e.g., short-term spikes vs long-term trends) are informative and can be better learned when modeled jointly rather than in isolation.
- Evidence anchors:
  - [abstract]: "Our architecture aims to learn effective representations at many scales across all available data simultaneously."
  - [section 3.1]: Describes the multiple resolution patching and the creation of a single attention context with tokens from all resolutions.
  - [corpus]: TOKON and FinTSBridge also emphasize the importance of scale-specific tokenization; TOKON specifically mentions token-based normalization which implies the importance of how tokens are formed.

### Mechanism 2
- Claim: Processing auxiliary variables separately from past data prevents auxiliary dominance and reduces overfitting by learning contextual representations independent of time series noise.
- Mechanism: Auxiliary variables are tokenized into base, time-varying known, and static tokens, which are processed separately from MRP tokens. They are then concatenated with MRP tokens and compressed before entering the transformer, ensuring their influence is balanced.
- Core assumption: Auxiliary variables carry significant, separate information that can improve forecasting if properly conditioned and not overwhelmed by past data tokens.
- Evidence anchors:
  - [section 3.2]: Explains the separate tokenisation and compression of auxiliary tokens.
  - [section 5.2.2]: Shows ablation results where including TVKT improves performance, indicating auxiliary information is useful.
  - [corpus]: TOKON also focuses on normalization for auxiliary data, suggesting the difficulty of incorporating such information.

### Mechanism 3
- Claim: The reverse splitting output head enables specialized output tokens and scales efficiently with increased token count by projecting MRP tokens back to forecast segments proportionally.
- Mechanism: Instead of flattening all tokens, reverse splitting takes the MRP tokens, projects each according to its resolution, and sums the resulting forecast segments, reducing parameters and encouraging token specialization.
- Core assumption: Different resolutions capture different aspects of the time series, and specialized output tokens for each resolution improve forecast quality and reduce overfitting.
- Evidence anchors:
  - [section 3.4]: Describes reverse splitting and its favorable scaling properties.
  - [section 5.2.2]: Implies that different module combinations (with/without auxiliary tokens) affect performance, suggesting the importance of token specialization.

## Foundational Learning

- Concept: Time series tokenisation
  - Why needed here: Traditional single-observation tokens are insufficient; multi-resolution patching captures dynamics at multiple scales, which is critical for complex forecasting tasks like pricing.
  - Quick check question: What is the main difference between single-resolution and multiple-resolution tokenisation in MRT?

- Concept: Transformer self-attention
  - Why needed here: MRT relies on self-attention to model pairwise relationships between tokens at different resolutions and from different data types, enabling complex conditional leading representations.
  - Quick check question: Why does MRT pass all tokens (MRP, auxiliary, cross-series) to the same attention mechanism instead of separate ones?

- Concept: Auxiliary variable handling
  - Why needed here: Pricing problems depend heavily on controllable variables like price; separate tokenization and compression prevent auxiliary variables from overwhelming the model and allow explicit conditioning.
  - Quick check question: How does MRT prevent auxiliary variable tokens from dominating the transformer input?

## Architecture Onboarding

- Component map: Input (past data, auxiliary variables) -> MRP module (multiple resolution patching) -> Auxiliary modules (base tokenisation + compression for TVKT and ST) -> Channel mixer (extracts cross-series tokens) -> Concatenation (all tokens combined) -> Transformer (self-attention blocks) -> Reverse splitting output head (specialized projections per resolution) -> Forecast output

- Critical path: 1. Tokenise past data via MRP 2. Tokenise auxiliary variables separately 3. Concatenate all tokens 4. Pass to transformer 5. Apply reverse splitting output head 6. Compute loss and backpropagate

- Design tradeoffs:
  - More resolutions → more tokens → higher computation but richer representations
  - Separate auxiliary tokenisation → reduced overfitting risk but increased complexity
  - Reverse splitting → parameter efficiency but requires careful resolution selection
  - Channel independence → easier scaling but potential loss of cross-series information unless mixer used

- Failure signatures:
  - Overfitting: High variance in training, poor generalization on validation
  - Poor resolution choice: No improvement over single resolution, high computation waste
  - Auxiliary dominance: Model ignores past data, relies too much on auxiliary variables
  - Scaling issues: Memory errors, slow training with many tokens

- First 3 experiments:
  1. Train MRT on Favorita with resolution set {1,2,4,16} and no auxiliary tokens; compare to PatchTST baseline.
  2. Add TVKT to MRT on Favorita; observe if performance improves and check for overfitting signs.
  3. Vary resolution set sizes on Favorita; identify which set yields best validation performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MRT scale with increasing lookback horizons (l) beyond the tested values?
- Basis in paper: [explicit] The authors mention that the real markdown problem dataset has irregularly spaced sampling and varying series lengths, requiring padding for shorter sequences. They set l to the longest sequence minus the fixed forecast horizon.
- Why unresolved: The experiments conducted only tested specific lookback horizon values (28 and 37 for the real markdown problem, 32 for the Favorita dataset). The paper does not explore how MRT's performance changes with significantly longer lookback horizons.
- What evidence would resolve it: Conducting experiments with MRT on datasets with varying and increasingly longer lookback horizons, comparing its performance against baselines like PatchTST and DLinear, would provide insights into its scalability and effectiveness for problems requiring extensive historical context.

### Open Question 2
- Question: What is the optimal strategy for selecting the resolution set K in MRT, and how does its composition affect model performance?
- Basis in paper: [explicit] The authors discuss the challenge of selecting a resolution set K, noting the exponential number of possible combinations and the impact on the number of tokens. They conducted ablation studies on the Favorita dataset with different resolution sets.
- Why unresolved: While the ablation studies show that multiple resolutions generally improve performance, the optimal composition of the resolution set (e.g., which specific values of k to include) remains unclear. The paper does not provide a definitive strategy for selecting K.
- What evidence would resolve it: Developing a systematic approach for selecting the resolution set K, potentially based on the characteristics of the time series data (e.g., frequency of observations, presence of seasonality), and validating this approach across multiple datasets would help determine the optimal strategy.

### Open Question 3
- Question: How does MRT handle missing data in time-varying known variables, and what are the implications for its performance?
- Basis in paper: [explicit] The authors mention that they assign a separate learnable vector embedding for each variable in cases the value has not been observed. They also discuss handling missing values in the context of numerical embeddings.
- Why unresolved: The paper does not provide a detailed analysis of how MRT's performance is affected by different patterns and proportions of missing data in time-varying known variables. The effectiveness of their approach for handling missing data is not thoroughly evaluated.
- What evidence would resolve it: Conducting experiments where different proportions and patterns of missing data are introduced in the time-varying known variables, and comparing MRT's performance against baselines under these conditions, would provide insights into its robustness and effectiveness in handling missing data.

## Limitations

- The lack of transparency regarding internal models used for comparison in real markdown pricing experiments makes it impossible to assess whether improvements stem from MRT's design or better hyperparameter tuning.
- The computational overhead of multi-resolution tokenization is acknowledged but not quantified, leaving open questions about scalability to longer sequences or higher resolutions.
- The evidence for auxiliary variable handling preventing dominance is indirect—based on ablation results rather than controlled experiments isolating this effect.

## Confidence

**High Confidence**: MRT's architecture design (multiple-resolution tokenization, separate auxiliary processing, reverse splitting output) is correctly implemented and follows established transformer principles. The MSE/MAE improvements on the Favorita dataset are directly measurable and reproducible.

**Medium Confidence**: The mechanism by which multi-resolution tokenization captures multi-scale dynamics is theoretically plausible and supported by ablation results, but the evidence is indirect and could be confounded by other architectural choices.

**Low Confidence**: The claim that separate auxiliary processing prevents overfitting is weakly supported. The ablation showing TVKT inclusion improves performance does not prove the separation mechanism is responsible, as the improvement could result from any auxiliary variable inclusion strategy.

## Next Checks

1. **Controlled Ablation on Auxiliary Processing**: Create a variant of MRT that processes auxiliary variables jointly with MRP tokens (no separate compression) while keeping all other components identical. Compare validation performance to determine if separate processing provides measurable benefits beyond simple inclusion.

2. **Resolution Sensitivity Analysis**: Systematically vary the resolution set (e.g., test {1,2,4}, {2,4,8}, {1,2,4,8,16}) on the Favorita dataset and measure both performance and computational overhead. This will quantify the trade-off between resolution diversity and efficiency.

3. **Internal Model Reconstruction**: Attempt to reconstruct plausible internal model architectures based on the problem description and compare their performance to MRT under identical training conditions. This would validate whether MRT's improvements are architecture-specific or due to training advantages.