---
ver: rpa2
title: Music Emotion Prediction Using Recurrent Neural Networks
arxiv_id: '2405.06747'
source_url: https://arxiv.org/abs/2405.06747
tags:
- dataset
- rnns
- accuracy
- neural
- music
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study applied recurrent neural networks to predict music\
  \ emotions using Russell\u2019s Emotion Quadrant, aiming to improve music recommendation\
  \ and therapeutic systems. Three RNN variants\u2014RNN, Bidirectional RNN, and LSTM\u2014\
  were trained on datasets ranging from 900 to 14,000 audio clips, using 14 audio\
  \ features extracted via Librosa."
---

# Music Emotion Prediction Using Recurrent Neural Networks

## Quick Facts
- arXiv ID: 2405.06747
- Source URL: https://arxiv.org/abs/2405.06747
- Reference count: 5
- Primary result: RNNs outperform traditional baselines for music emotion prediction, with data augmentation improving accuracy by ~20%

## Executive Summary
This study applies recurrent neural networks to predict music emotions using Russell's Emotion Quadrant, aiming to improve music recommendation and therapeutic systems. Three RNN variants—RNN, Bidirectional RNN, and LSTM—were trained on datasets ranging from 900 to 14,000 audio clips, using 14 audio features extracted via Librosa. Results showed that simpler RNNs performed comparably to or better than more complex models on smaller datasets (e.g., 53.33% accuracy for RNNs), while Bidirectional RNNs and LSTMs improved on larger datasets. Data augmentation significantly boosted accuracy (e.g., RNNs improved by 20%). Overall, neural networks outperformed traditional baselines, especially on larger datasets, highlighting their potential for personalized music emotion recognition.

## Method Summary
The study employs three RNN variants (RNN, Bidirectional RNN, and LSTM) trained on music audio clips with 14 audio features extracted using Librosa. Models were trained across datasets of varying sizes (900 to 14,000 samples) with data augmentation techniques (noise injection, time shifting, pitch changing). Cross-entropy loss and AdamW optimization were used, with early stopping and 5-fold cross-validation. Performance was evaluated using classification accuracy and compared against traditional baseline classifiers including Logistic Regression, SVM, Random Forest, and others.

## Key Results
- Simpler RNNs achieved 53.33% accuracy on 900-sample dataset, outperforming complex models
- Data augmentation improved RNN accuracy by approximately 20%
- LSTMs showed superior performance on larger datasets (14,000 samples) compared to smaller ones
- Neural networks consistently outperformed traditional baselines, especially on larger datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Recurrent neural networks (RNNs) capture temporal dynamics in music that influence emotional perception.
- Mechanism: RNNs process sequential audio features over time, maintaining a hidden state that integrates information from previous time steps, which aligns with how emotions unfold in music.
- Core assumption: The emotional content of music is inherently sequential and cannot be captured by static feature analysis alone.
- Evidence anchors:
  - [abstract] "Results showed that simpler RNNs performed comparably to or better than more complex models on smaller datasets"
  - [section 4.3.1] "RNNs possess a form of 'memory' that captures information from previous computations"
  - [corpus] Weak - corpus papers focus on emotion-aware recommendation rather than RNN mechanism specifics
- Break condition: If emotional cues in music are predominantly local rather than sequential, or if temporal dependencies are too long for RNNs to maintain effectively.

### Mechanism 2
- Claim: Data augmentation improves model performance by increasing dataset diversity and reducing overfitting.
- Mechanism: Augmentation techniques (noise injection, time shifting, pitch changing) create synthetic variations of existing audio clips, effectively expanding the training set and exposing the model to more diverse examples.
- Core assumption: The augmented data preserves the emotional content of the original clips while adding realistic variation.
- Evidence anchors:
  - [abstract] "Data augmentation significantly boosted accuracy (e.g., RNNs improved by 20%)"
  - [section 5.2.2] "we implemented a 5-fold cross-validation approach during training for all three models"
  - [corpus] Weak - corpus focuses on emotion recognition but not specifically on augmentation techniques
- Break condition: If augmentation introduces artifacts that change the emotional content or if the model overfits to augmented patterns rather than genuine emotional features.

### Mechanism 3
- Claim: Model complexity should scale with dataset size for optimal performance.
- Mechanism: Smaller datasets benefit from simpler models (like basic RNNs) that generalize better, while larger datasets can support more complex architectures (like LSTM) that capture intricate patterns.
- Core assumption: The relationship between model complexity and dataset size follows a sweet spot where neither underfitting nor overfitting occurs.
- Evidence anchors:
  - [abstract] "simpler RNNs performed comparably to or better than more complex models on smaller datasets"
  - [section 6.3] "From Table 3 we can clearly see the test accuracy of RNNs decreased as the dataset becomes larger"
  - [corpus] Weak - corpus papers focus on emotion-aware recommendation but don't explicitly discuss this scaling relationship
- Break condition: If the optimal model complexity doesn't follow this pattern, or if other factors (like feature quality) dominate the performance relationship.

## Foundational Learning

- Recurrent neural networks (RNNs)
  - Why needed here: RNNs are designed to process sequential data like audio features over time, which is essential for capturing how emotions evolve in music.
  - Quick check question: How does an RNN's hidden state differ from a standard neural network's output layer?

- Feature extraction with Librosa
  - Why needed here: Audio features like chroma, mel-spectrogram, and MFCCs capture the acoustic properties that correlate with emotional content in music.
  - Quick check question: What is the difference between chroma_stft and chroma_cqt features in Librosa?

- Cross-entropy loss
  - Why needed here: Cross-entropy measures the difference between predicted probabilities and true labels, making it ideal for multi-class emotion classification.
  - Quick check question: Why is cross-entropy preferred over mean squared error for classification tasks?

## Architecture Onboarding

- Component map:
  Audio files -> Librosa feature extraction -> shape normalization -> RNN/BRNN/LSTM -> cross-entropy loss -> AdamW optimization -> gradient clipping -> evaluation

- Critical path:
  1. Load and preprocess audio data with Librosa
  2. Convert features to appropriate tensor shapes
  3. Select model architecture based on dataset size
  4. Train with early stopping and cross-validation
  5. Evaluate and compare against baselines

- Design tradeoffs:
  - Simpler RNNs vs. complex LSTMs: simpler models generalize better on small datasets but may miss nuanced patterns
  - Data augmentation: increases diversity but risks introducing artifacts
  - Model complexity scaling: must balance underfitting on small datasets with overfitting on large ones

- Failure signatures:
  - Training accuracy much higher than validation accuracy: overfitting
  - Both accuracies plateau at low values: underfitting or poor feature extraction
  - Sudden drops in accuracy during training: gradient explosion or data issues

- First 3 experiments:
  1. Compare basic RNN vs. LSTM on the 900-sample dataset to verify simpler models perform better on small data
  2. Apply augmentation to the 900-sample dataset and retrain to measure performance improvement
  3. Scale up to the 14,000-sample dataset and test if LSTM outperforms simpler architectures as hypothesized

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different RNN architectures (RNN, BRNN, LSTM) perform when applied to music emotion recognition with datasets of varying sizes and complexities?
- Basis in paper: [explicit] The paper explicitly compares RNN, BRNN, and LSTM models across different dataset sizes (900, 3,600, and 14,000 audio clips), noting performance differences.
- Why unresolved: While the paper provides initial results, it does not fully explore the impact of dataset size and complexity on the performance of these models. It also does not investigate other RNN variants or configurations that might yield better results.
- What evidence would resolve it: Systematic experiments comparing multiple RNN architectures across a broader range of dataset sizes and complexities, including deeper or more intricate model structures, would provide clearer insights into their performance in music emotion recognition.

### Open Question 2
- Question: What is the optimal feature set and extraction method for improving music emotion recognition using RNNs?
- Basis in paper: [inferred] The paper uses 14 audio features extracted via Librosa, but it does not explore alternative feature sets or extraction methods that might enhance model performance.
- Why unresolved: The choice of features and extraction methods can significantly impact model performance, yet the paper does not investigate other potential feature sets or methods beyond the 14 features used.
- What evidence would resolve it: Comparative studies using different feature sets and extraction methods, such as novel audio features or domain-specific features, would help identify the most effective approach for music emotion recognition.

### Open Question 3
- Question: How do data augmentation techniques affect the performance of RNNs in music emotion recognition, and which techniques are most effective?
- Basis in paper: [explicit] The paper applies noise injection, time shifting, and pitch changing for data augmentation, showing significant improvements in accuracy.
- Why unresolved: While the paper demonstrates the effectiveness of these techniques, it does not explore other augmentation methods or quantify their relative contributions to performance gains.
- What evidence would resolve it: Experiments testing a wider range of data augmentation techniques and their individual and combined effects on model performance would provide a clearer understanding of their impact on music emotion recognition.

## Limitations
- Weak grounding for key mechanisms, particularly regarding data augmentation effectiveness and model complexity scaling
- Preprocessing pipeline details underspecified, especially feature reshaping into (n_samples, 204, 1295) format
- Hyperparameter choices for RNN architectures not fully disclosed, limiting reproducibility

## Confidence
- High confidence: RNNs outperform traditional baselines, especially on larger datasets
- Medium confidence: Data augmentation improves accuracy by approximately 20%
- Low confidence: The proposed scaling relationship between model complexity and dataset size

## Next Checks
1. Replicate the augmentation experiment by training the same RNN architecture with and without augmentation on a small dataset to verify the 20% improvement claim
2. Test the scaling hypothesis by systematically comparing model performance across multiple dataset sizes (e.g., 1k, 5k, 10k, 14k samples) using identical hyperparameters
3. Conduct ablation studies to isolate which audio features contribute most to emotion prediction accuracy