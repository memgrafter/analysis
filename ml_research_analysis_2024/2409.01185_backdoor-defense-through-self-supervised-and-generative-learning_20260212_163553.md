---
ver: rpa2
title: Backdoor Defense through Self-Supervised and Generative Learning
arxiv_id: '2409.01185'
source_url: https://arxiv.org/abs/2409.01185
tags:
- backdoor
- samples
- poisoned
- defense
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a backdoor defense method called GSSD that uses
  generative modeling in self-supervised feature space to detect and cleanse poisoned
  training data. The core idea is to train per-class normalizing flows on self-supervised
  embeddings and identify poisoned samples by their low likelihood under their labeled
  class.
---

# Backdoor Defense through Self-Supervised and Generative Learning

## Quick Facts
- arXiv ID: 2409.01185
- Source URL: https://arxiv.org/abs/2409.01185
- Authors: Ivan Sabolić; Ivan Grubišić; Siniša Šegvić
- Reference count: 40
- Primary result: GSSD achieves ASR below 1% across all attacks while maintaining high accuracy (92.6% average)

## Executive Summary
This paper proposes GSSD, a backdoor defense method that uses generative modeling in self-supervised feature space to detect and cleanse poisoned training data. The approach trains per-class normalizing flows on self-supervised embeddings and identifies poisoned samples by their low likelihood under their labeled class. Experiments demonstrate that GSSD outperforms state-of-the-art defenses across multiple datasets and attack types, achieving near-zero attack success rates while maintaining high classification accuracy.

## Method Summary
GSSD employs a two-stage approach to backdoor defense. First, it uses self-supervised learning (SimCLR) to extract feature representations that are more robust to backdoor attacks than supervised features. Per-class normalizing flows are then trained on these features to model class densities in the latent space. The method identifies poisoned samples by detecting those with low likelihood under their labeled class, splits data into clean, poisoned, and uncertain subsets, trains a classifier on the clean subset, and fine-tunes on relabeled poisoned samples to remove backdoor associations.

## Key Results
- Achieves ASR below 1% across all evaluated attacks on CIFAR-10, ImageNet-30, and VGGFace2-30
- Maintains high accuracy (92.6% average) while defending against BadNets, Blend, WaNet, LC, ISSBA, Adap-Patch, and Adap-Blend attacks
- Outperforms state-of-the-art defenses including NAD, ABL, DBD, and ASD
- Particularly effective against adaptive attacks designed to evade latent separability defenses

## Why This Works (Mechanism)

### Mechanism 1
Generative modeling in self-supervised feature space can detect poisoned samples by assigning them low likelihood under their labeled class. Per-class normalizing flows are trained on self-supervised embeddings, and poisoned samples are identified by their low density under the class they are labeled as, relative to other classes. The core assumption is that self-supervised embeddings of poisoned samples either resemble their original class (non-disruptive) or are displaced from the clean data manifold (disruptive), making them detectable by generative models. Evidence shows that these representations get either preserved or heavily disturbed under recent backdoor attacks, allowing per-class generative models to detect poisoned data. Break condition: If self-supervised embeddings of poisoned samples do not separate from clean samples in feature space, generative detection fails.

### Mechanism 2
Self-supervised representations are more robust to backdoor attacks than supervised representations because they rely on class-agnostic features. Self-supervised learning avoids learning shortcut associations between triggers and target labels, preserving semantic content of images. The core assumption is that self-supervised models are less affected by poisoned labels due to strong augmentations and contrastive objectives that disregard labels. Evidence includes the paper's conjecture that backdoor defense has a better chance of success if it relies more on class-agnostic features rather than discriminative features derived from potentially poisoned labels. Break condition: If self-supervised representations still learn backdoor associations, the defense fails.

### Mechanism 3
Fine-tuning on relabeled poisoned samples removes the backdoor by breaking the association between triggers and target labels. After filtering suspicious samples, the model is fine-tuned on data relabeled using generative classification, turning triggers into benign augmentations. The core assumption is that once triggers are no longer associated with target labels, they function as data augmentation rather than backdoor triggers. Evidence shows that training on cleansed data ensures high performance on benign inputs, while fine-tuning on relabeled data ensures the complete removal of the backdoor. Break condition: If relabeling is inaccurate or if the model still learns backdoor behavior, the backdoor persists.

## Foundational Learning

- **Self-supervised learning and contrastive objectives**: Why needed here - Self-supervised features are more robust to backdoor attacks and provide semantic representations that are less affected by poisoned labels. Quick check question: How does contrastive learning help preserve semantic content while avoiding shortcut associations?

- **Generative modeling with normalizing flows**: Why needed here - Per-class densities in feature space allow detection of poisoned samples by their low likelihood under their labeled class. Quick check question: Why are normalizing flows effective for modeling densities in self-supervised feature space compared to other generative models?

- **Generative classification for relabeling**: Why needed here - Allows recovery of original labels for poisoned samples by finding the class that assigns highest density to their features. Quick check question: How does generative classification differ from discriminative classification in handling poisoned samples?

## Architecture Onboarding

- **Component map**: Self-supervised feature extractor (SimCLR) → Per-class normalizing flows → Target class detection → Sample filtering → Discriminative classifier training → Fine-tuning on relabeled data
- **Critical path**: Self-supervised training → Generative modeling → Target class detection → Sample filtering → Final model training
- **Design tradeoffs**: Computationally expensive self-supervised pre-training vs. robustness to backdoor attacks; choice between generative and discriminative classifiers
- **Failure signatures**: Inability to detect target classes (extremely low poisoning rates), inaccurate relabeling, failure to break backdoor association
- **First 3 experiments**:
  1. Test self-supervised feature robustness by comparing L2 distances between clean and poisoned embeddings
  2. Validate target class detection by checking SyND and SyD scores on synthetic poisoned data
  3. Evaluate relabeling accuracy by comparing generated labels with original pre-poisoning labels

## Open Questions the Paper Calls Out

### Open Question 1
How robust is the proposed defense against adaptive attacks that are specifically designed to minimize the distance between poisoned samples and clean samples of the target class in the self-supervised feature space? The paper discusses an adaptive attack scenario where the attacker tries to fool the surrogate self-supervised model by increasing similarity between poisoned samples and clean samples of the target class, but the evaluation is limited to a specific scenario with a less stealthy trigger pattern. Conducting comprehensive experiments with various adaptive attack strategies that balance stealthiness and effectiveness in the feature space would provide insights into the defense's robustness.

### Open Question 2
What is the impact of using different self-supervised learning methods, such as BYOL or SimSiam, on the performance of the proposed defense? The paper primarily uses SimCLR for self-supervised learning and mentions that CLIP performs well, but does not explore other self-supervised learning methods. Different self-supervised learning methods may have varying effects on the feature space and the defense's ability to detect poisoned samples. Experimenting with various self-supervised learning methods and comparing their impact on the defense's performance would provide insights into the robustness across different feature extraction techniques.

### Open Question 3
How does the proposed defense perform when the poisoning rate is extremely low, such as 0.1%, and what are the limitations of the current approach in such scenarios? The paper acknowledges that the defense fails to detect the target class in the case of an extremely low poisoning rate, such as 0.1%, and mentions that other state-of-the-art methods also struggle against such attacks. Conducting extensive experiments with varying poisoning rates, including extremely low rates, and analyzing the defense's performance and limitations would provide insights into the robustness in challenging scenarios.

## Limitations

- The paper does not fully characterize when self-supervised features fail to preserve semantic content under poisoning
- Adaptive attack variants designed specifically to evade generative modeling in feature space are not extensively tested
- The computational cost of training per-class normalizing flows on large-scale datasets is not discussed

## Confidence

- **High confidence**: Core mechanism of using generative modeling in self-supervised feature space for backdoor detection
- **Medium confidence**: Robustness to adaptive attacks, as only a limited set of adaptive variants were tested
- **Medium confidence**: Relabeling approach, as the paper assumes generative classification can recover true labels but doesn't extensively validate this assumption

## Next Checks

1. Test GSSD on adaptive attacks specifically designed to minimize the L2 distance between poisoned and clean target class samples in the self-supervised feature space
2. Evaluate the impact of varying self-supervised model architectures (beyond SimCLR) on the effectiveness of the defense
3. Analyze the computational overhead of training per-class normalizing flows and its scalability to larger datasets