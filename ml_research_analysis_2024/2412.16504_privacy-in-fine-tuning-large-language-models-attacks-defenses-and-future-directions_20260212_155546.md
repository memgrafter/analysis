---
ver: rpa2
title: 'Privacy in Fine-tuning Large Language Models: Attacks, Defenses, and Future
  Directions'
arxiv_id: '2412.16504'
source_url: https://arxiv.org/abs/2412.16504
tags:
- privacy
- ne-tuning
- data
- attacks
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey comprehensively examines privacy risks in fine-tuning
  large language models (LLMs), focusing on three main attack types: membership inference,
  data extraction, and backdoor attacks. The authors analyze how different fine-tuning
  methods (full fine-tuning, LoRA, adapters, prompt-tuning) and pre-trained models
  influence privacy vulnerabilities.'
---

# Privacy in Fine-tuning Large Language Models: Attacks, Defenses, and Future Directions

## Quick Facts
- arXiv ID: 2412.16504
- Source URL: https://arxiv.org/abs/2412.16504
- Authors: Hao Du; Shang Liu; Lele Zheng; Yang Cao; Atsuyoshi Nakamura; Lei Chen
- Reference count: 40
- One-line primary result: Comprehensive survey examining privacy risks in LLM fine-tuning across three attack types and multiple defense mechanisms

## Executive Summary
This survey comprehensively examines privacy risks in fine-tuning large language models (LLMs), focusing on three main attack types: membership inference, data extraction, and backdoor attacks. The authors analyze how different fine-tuning methods (full fine-tuning, LoRA, adapters, prompt-tuning) and pre-trained models influence privacy vulnerabilities. They review defense mechanisms including differential privacy, federated learning, knowledge unlearning, and data anonymization, discussing their effectiveness and limitations. The survey identifies key gaps in existing research, particularly regarding the unique characteristics of fine-tuning data, the impact of parameter-efficient fine-tuning methods on privacy risks, and the potential vulnerabilities introduced by malicious pre-trained models.

## Method Summary
The paper employs a comprehensive literature review methodology to analyze privacy risks in LLM fine-tuning. The authors systematically examine existing research on privacy attacks (membership inference, data extraction, backdoor attacks) and defense mechanisms (differential privacy, federated learning, knowledge unlearning, data anonymization). They categorize findings based on different fine-tuning methods (full fine-tuning, LoRA, adapters, prompt-tuning) and pre-trained model characteristics. The survey identifies research gaps and proposes future directions, synthesizing insights from 40 referenced papers to provide a holistic view of the current state of privacy-preserving fine-tuning techniques.

## Key Results
- Fine-tuning methods create distinct privacy vulnerability profiles, with full fine-tuning being more susceptible to membership inference attacks than parameter-efficient methods
- Pre-trained model backdoor integration can amplify privacy attack success when compromised models are used for fine-tuning
- Public accessibility of pre-trained models creates reference point vulnerabilities that enable comparison-based attacks
- Current defense mechanisms struggle to comprehensively address diverse privacy attacks while maintaining model utility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Different fine-tuning methods create distinct privacy vulnerability profiles
- Mechanism: Full fine-tuning (FFT) updates all parameters, maximizing model adaptation but increasing susceptibility to membership inference attacks. Parameter-efficient fine-tuning (PEFT) methods like LoRA, Adapters, and Prompt-tuning freeze most parameters, reducing attack surface but potentially creating unique vulnerabilities in the trainable components
- Core assumption: The number and distribution of trainable parameters directly correlates with privacy risk exposure
- Evidence anchors:
  - [abstract] "how different fine-tuning methods (full fine-tuning, LoRA, adapters, prompt-tuning) and pre-trained models influence privacy vulnerabilities"
  - [section 3.1] "the fine-tuning method impacts privacy vulnerability, with head fine-tuning being more vulnerable to MIA compared to full fine-tuning or adapter fine-tuning"
  - [corpus] Weak evidence - corpus does not specifically address fine-tuning method differences
- Break condition: If PEFT methods achieve comparable performance to FFT with significantly fewer parameters, the correlation between parameter count and privacy risk may weaken

### Mechanism 2
- Claim: Pre-trained model backdoor integration amplifies privacy attack success
- Mechanism: Adversaries can inject malicious triggers into pre-trained models during pre-training phase. When these compromised models are used for fine-tuning, the backdoors enhance the effectiveness of subsequent privacy attacks, particularly membership inference and data extraction attacks
- Core assumption: Backdoors introduced during pre-training persist through fine-tuning and can be leveraged to amplify downstream attacks
- Evidence anchors:
  - [section 3.1] "injecting a backdoor into a pre-trained model before fine-tuning substantially enhances the effectiveness of MIAs"
  - [section 3.3] "PreCurious attack framework, where the attacker fine-tunes a legitimate pre-trained model using auxiliary data to create an adversarially initialized model"
  - [section 5.1] "Adversaries can release maliciously modified pre-trained models, embedding backdoors or vulnerabilities"
- Break condition: If fine-tuning sufficiently modifies the model architecture to overwrite or neutralize pre-existing backdoors, the amplification effect diminishes

### Mechanism 3
- Claim: Public accessibility of pre-trained models creates reference point vulnerabilities
- Mechanism: The widespread availability of open-source pre-trained models provides attackers with reliable reference points for comparison-based attacks. This accessibility enables techniques like SPV-MIA (Self-Prompting Membership Inference) and data extraction methods that leverage base model snapshots
- Core assumption: Attackers can reliably obtain functionally equivalent pre-trained models to serve as attack reference points
- Evidence anchors:
  - [section 3.1] "SPV-MIA, a novel technique that eliminates the need for attackers to access an external reference dataset" (implying reference models are typically available)
  - [section 3.2] "adversaries can easily acquire and use them as reference points or further fine-tune them to facilitate attacks"
  - [section 5.1] "since most pre-trained models are publicly available, adversaries can easily acquire and use them as reference points"
- Break condition: If model providers implement robust access controls or watermarking mechanisms that prevent unauthorized model acquisition, reference point attacks become less feasible

## Foundational Learning

- Concept: Differential Privacy (DP) fundamentals
  - Why needed here: DP mechanisms form the foundation of many privacy-preserving fine-tuning defenses
  - Quick check question: What is the primary privacy guarantee provided by DP, and how does it differ from simple data anonymization?

- Concept: Federated Learning (FL) principles
  - Why needed here: FL represents a distributed approach to privacy-preserving model training that can be applied to fine-tuning scenarios
  - Quick check question: How does FL preserve privacy compared to centralized training, and what are its key limitations?

- Concept: Parameter-Efficient Fine-Tuning (PEFT) methods
  - Why needed here: Understanding PEFT is crucial for evaluating how different fine-tuning approaches affect privacy vulnerability
  - Quick check question: What is the fundamental trade-off between parameter efficiency and model adaptation capability in PEFT methods?

## Architecture Onboarding

- Component map: Pre-trained model -> Fine-tuning dataset -> Fine-tuning method -> Fine-tuned model
- Critical path: Pre-trained model acquisition -> Fine-tuning method selection -> Defense mechanism implementation -> Privacy risk assessment -> Model deployment
- Design tradeoffs:
  - Utility vs. Privacy: Stronger privacy protections typically reduce model performance
  - Efficiency vs. Security: More comprehensive defenses require greater computational resources
  - Accessibility vs. Protection: Open model access enables broader use but increases attack surface
- Failure signatures:
  - Unexpected model behavior indicating potential backdoor activation
  - Degradation in model performance following privacy defense implementation
  - Anomalous gradient patterns suggesting data extraction attempts
  - Membership inference attack success rates exceeding baseline expectations
- First 3 experiments:
  1. Compare membership inference attack success rates across FFT, LoRA, and Adapter fine-tuning methods using identical datasets and pre-trained models
  2. Evaluate the effectiveness of DP-SGD versus PEFT with DP mechanisms in preserving utility while reducing privacy risk
  3. Test backdoor injection vulnerability by attempting to implant and detect triggers across different fine-tuning scenarios and pre-trained model sources

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different parameter-efficient fine-tuning (PEFT) methods (e.g., LoRA, Adapters, Prefix-tuning) differentially impact privacy vulnerabilities compared to full fine-tuning?
- Basis in paper: [explicit] The authors explicitly state that "privacy implications of different PEFTs remain underexplored" and note that "critical questions unanswered: How do various fine-tuning methods impact privacy risks?"
- Why unresolved: While the paper surveys privacy attacks and defenses, it does not provide a systematic comparison of how different PEFT methods (LoRA, Adapters, Prefix-tuning, etc.) specifically affect privacy vulnerabilities compared to full fine-tuning. The varying computational efficiencies and parameter modifications of these methods may create different attack surfaces.
- What evidence would resolve it: Controlled experiments comparing privacy attack success rates (membership inference, data extraction, backdoor attacks) across multiple PEFT methods and full fine-tuning on identical datasets and pre-trained models, with consistent privacy metrics and attack methodologies.

### Open Question 2
- Question: What are the unique privacy risks introduced by the specialized characteristics of fine-tuning datasets (narrower domain coverage, task-specific distributions, higher data sensitivity) compared to pre-training data?
- Basis in paper: [explicit] The authors identify this as a gap, stating "Fine-tuning datasets differ significantly from the general-purpose data used in pre-training" and noting "These characteristics introduce unique privacy risks."
- Why unresolved: Existing privacy research on LLMs has primarily focused on pre-training data characteristics and general model behavior. The specialized nature of fine-tuning datasets - often containing domain-specific terminology and sensitive personal information - creates distinct vulnerabilities that have not been systematically studied.
- What evidence would resolve it: Empirical studies comparing attack success rates on fine-tuning datasets versus pre-training datasets with similar sizes and distributions, analyzing how domain specificity and data sensitivity correlate with privacy vulnerabilities, and developing metrics that capture these dataset-specific privacy risks.

### Open Question 3
- Question: How can we develop comprehensive privacy metrics that evaluate defenses against diverse attack types (membership inference, data extraction, backdoor attacks) specifically in fine-tuning settings?
- Basis in paper: [explicit] The authors note that "there is a lack of unified metrics for evaluating effectiveness of defenses across diverse scenarios" and that "Current defense methods struggle to comprehensively address diverse privacy attacks."
- Why unresolved: The paper surveys multiple defense mechanisms (differential privacy, federated learning, knowledge unlearning, data anonymization) but acknowledges that these approaches have different strengths against different attack types. There is no standardized framework for evaluating overall defense effectiveness in the context of fine-tuning.
- What evidence would resolve it: Development and validation of a comprehensive evaluation framework that includes benchmark datasets, standardized attack methodologies, and unified metrics (e.g., privacy-utility trade-off curves, attack success rate thresholds) specifically designed for fine-tuning scenarios.

## Limitations
- The analysis relies heavily on existing literature without original empirical validation of proposed mechanisms
- The survey's conclusions about defense mechanism effectiveness are based on reported results from individual studies with different evaluation protocols
- The paper does not provide quantitative analysis of how different fine-tuning methods specifically impact privacy risk metrics across standardized benchmarks

## Confidence
- **High confidence**: Identification and categorization of privacy attack types and general overview of defense mechanisms
- **Medium confidence**: Relationships between fine-tuning methods and privacy vulnerability profiles based on theoretical reasoning
- **Low confidence**: Specific claims about pre-trained model backdoor amplification effects and comparative effectiveness of different defense mechanisms

## Next Checks
1. Conduct controlled experiments comparing membership inference attack success rates across FFT, LoRA, and Adapter fine-tuning methods using identical datasets, pre-trained models, and evaluation protocols
2. Perform systematic comparison of DP-SGD versus PEFT with DP mechanisms, measuring both privacy risk reduction and utility preservation across multiple model architectures
3. Design experiments to test backdoor persistence and amplification effects by injecting known triggers during pre-training and evaluating their impact on privacy attacks after various fine-tuning scenarios