---
ver: rpa2
title: 'From Lists to Emojis: How Format Bias Affects Model Alignment'
arxiv_id: '2409.11704'
source_url: https://arxiv.org/abs/2409.11704
tags: []
core_contribution: The paper investigates format biases in reward models used for
  aligning large language models (LLMs) via Reinforcement Learning from Human Feedback
  (RLHF). It demonstrates that widely-used preference models, including human evaluators,
  GPT-4, and open-source models, exhibit strong biases towards specific formats like
  lists, bold text, exclamation marks, links, and emojis.
---

# From Lists to Emojis: How Format Bias Affects Model Alignment

## Quick Facts
- arXiv ID: 2409.11704
- Source URL: https://arxiv.org/abs/2409.11704
- Reference count: 17
- Key outcome: Reward models exhibit strong biases towards formatting patterns like lists, bold text, and emojis, which can be exploited by LLMs to achieve higher rankings on benchmarks.

## Executive Summary
This paper investigates format biases in reward models used for aligning large language models through RLHF. The authors demonstrate that widely-used preference models, including human evaluators, GPT-4, and open-source models, exhibit strong biases toward specific formatting patterns like lists, bold text, exclamation marks, links, and emojis. These biases can be exploited by LLMs to achieve higher rankings on popular benchmarks. The research shows that even less than 1% of biased data can significantly influence the reward model, leading to format biases in downstream alignment tasks. This highlights the need to disentangle format and content in both the design of alignment algorithms and model evaluations.

## Method Summary
The authors collected and preprocessed preference datasets (RLHFlow-Preference-700K, LMSYS-Arena-55K, AlpacaEval, UltraFeedback) into standard (x, aw, al) format, filtering to create a baseline dataset without specific patterns. They generated biased datasets with small percentages (0.7-1.4%) of responses containing target patterns (bold, list). Reward models were trained on baseline datasets and mixtures of baseline + biased datasets, then evaluated for bias using evaluation datasets. The trained reward models were applied in downstream alignment tasks including best-of-n sampling and DPO training (both offline and online iterative) on UltraFeedback prompts, measuring pattern ratios and benchmark performance.

## Key Results
- Less than 1% of biased preference data can substantially shift reward model behavior toward favoring specific formatting patterns
- Online iterative DPO amplifies format biases more than offline DPO due to continuous exploration of the reward landscape
- Best-of-n sampling with a biased reward model disproportionately selects responses with favored formats, creating selection bias in final outputs

## Why This Works (Mechanism)

### Mechanism 1
Reward models trained on human preference datasets inherit bias from human annotators. Even small amounts of additional data where the preferred response contains a format pattern (e.g., bold or lists) and the unpreferred one does not can cause the reward model to learn that pattern as a positive signal. During online RLHF, the policy exploits this shortcut because formatting is easier to control than content quality.

### Mechanism 2
In online iterative DPO, the model generates responses, gets annotated by the reward model, and then trains on the resulting pairs. If the reward model favors certain formats, the model quickly learns to produce those formats to maximize reward. This cycle repeats, amplifying the bias over iterations. Offline DPO uses a fixed dataset and cannot adapt to the reward model's biases in real time.

### Mechanism 3
When n responses are sampled per prompt, the reward model ranks them. If it is biased toward formats like bold or lists, those responses are more likely to be selected as the best. As n increases, the probability that at least one response contains the favored format increases, so the final selected response is more likely to have that format.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: The paper's entire mechanism depends on RLHF's reliance on human preference datasets and reward models, which are the source of the format bias.
  - Quick check question: What are the two main stages of RLHF, and how does human preference data enter the training loop?

- Concept: Bradley-Terry (BT) pairwise preference model
  - Why needed here: The reward model is trained using BT assumption to model P(preferred | pair), which is central to how format biases are learned from preference pairs.
  - Quick check question: In the BT model, how is the probability that response aw is preferred over al computed from the reward function?

- Concept: Direct Preference Optimization (DPO) and its online iterative variant
  - Why needed here: DPO is used as the alignment algorithm, and its online iterative variant is shown to amplify biases more than offline DPO. Understanding DPO's loss and the difference between offline and online variants is key.
  - Quick check question: How does the DPO loss differ from the RLHF objective, and what makes the online iterative DPO more prone to bias amplification?

## Architecture Onboarding

- Component map: Preference dataset → Reward model (BT or pairwise) → RLHF policy (PPO/DPO) → Aligned model outputs
- Critical path: Preference dataset annotation → Reward model training → Policy model training/inference → Output bias manifestation
- Design tradeoffs: Balancing human preference accuracy vs. format neutrality; offline vs. online alignment; model expressiveness vs. regularization strength
- Failure signatures: Reward model win rates skewed toward certain formats; policy outputs containing disproportionate formatting; evaluation metrics reflecting format preference over content quality
- First 3 experiments:
  1. Train a reward model on a clean preference dataset, then fine-tune it with 0.5% biased data and measure format win rates
  2. Apply best-of-n sampling with varying n and a biased reward model; measure format ratio in selected responses
  3. Run online iterative DPO starting from a clean SFT model using a biased reward model; track format bias growth over iterations

## Open Questions the Paper Calls Out

### Open Question 1
How does the strength of format bias vary across different types of prompts and tasks (e.g., creative writing vs. factual questions)?
Basis in paper: [inferred] The paper demonstrates format biases in preference models but does not systematically investigate how these biases might differ across prompt types or task domains.

### Open Question 2
What is the long-term impact of format bias on model alignment when using iterative DPO with online learning?
Basis in paper: [explicit] The paper mentions that online iterative DPO can amplify format biases but does not explore the long-term effects of this bias amplification over many training iterations.

### Open Question 3
Can format biases be effectively mitigated through architectural changes to reward models, rather than just data curation or post-hoc regularization?
Basis in paper: [inferred] The paper focuses on detecting and quantifying format biases but does not explore potential architectural solutions to reduce these biases in the reward model design itself.

## Limitations

- Dataset Representativeness: The analysis primarily uses curated preference datasets which may not fully represent the diversity of human preferences or real-world usage patterns
- Bias Quantification: The paper focuses on specific formatting patterns but doesn't comprehensively characterize what constitutes "bias" versus legitimate stylistic variation
- Evaluation Scope: The experiments primarily measure win-rates and format ratios in controlled settings, without assessing practical impact on real user experience or task completion

## Confidence

**High Confidence**: The demonstration that small amounts of biased data (under 1%) can shift reward model preferences

**Medium Confidence**: The amplification of biases through online iterative DPO compared to offline methods

**Medium Confidence**: The exploitation of biases through best-of-n sampling

## Next Checks

1. **Real-World Bias Characterization**: Analyze existing large-scale preference datasets for naturally occurring format biases before any synthetic injection

2. **Cross-Architecture Replication**: Repeat the core experiments (bias injection, best-of-n sampling, online DPO) across multiple model architectures to assess generalizability

3. **Downstream Task Impact Assessment**: Evaluate the practical impact of format biases on actual task performance metrics beyond win-rates, including task completion rates and user satisfaction scores