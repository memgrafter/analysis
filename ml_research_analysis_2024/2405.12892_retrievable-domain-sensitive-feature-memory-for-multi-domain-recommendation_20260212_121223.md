---
ver: rpa2
title: Retrievable Domain-Sensitive Feature Memory for Multi-Domain Recommendation
arxiv_id: '2405.12892'
source_url: https://arxiv.org/abs/2405.12892
tags:
- features
- feature
- domain-sensitive
- domain
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multi-domain ad recommendation,
  where models need to capture both commonalities and distinctions across different
  domains. The authors identify that existing methods may neglect domain-sensitive
  features, which are crucial for modeling domain distinctions.
---

# Retrievable Domain-Sensitive Feature Memory for Multi-Domain Recommendation

## Quick Facts
- arXiv ID: 2405.12892
- Source URL: https://arxiv.org/abs/2405.12892
- Authors: Yuang Zhao; Zhaocheng Du; Qinglin Jia; Linxuan Zhang; Zhenhua Dong; Ruiming Tang
- Reference count: 40
- One-line primary result: 8.168% RPM increase in online A/B test

## Executive Summary
This paper addresses the challenge of multi-domain ad recommendation by identifying that existing methods may neglect domain-sensitive features crucial for modeling domain distinctions. The authors propose a domain-sensitive feature attribution method that identifies features with significant inter-domain differences in both distributions and effects on model predictions. They design a retrievable domain-sensitive feature memory architecture that extracts domain-specific information from these features for the model to retrieve and integrate. The method demonstrates significant improvements over state-of-the-art baselines, with 0.8484 AUC improvement on industrial data and 0.6272 on public data, plus an 8.168% RPM increase in online A/B testing.

## Method Summary
The method identifies domain-sensitive features using Integrated Gradients to measure their effects on model predictions, combined with JS divergence or Wasserstein distance to quantify distribution differences across domains. These features are processed through a Domain-Sensitive Feature Memory architecture consisting of an Extractor (a DNN processing sensitive features) and Retrievers (linear attention modules) that integrate domain-specific information into the base model. The architecture uses linear attention instead of softmax attention to reduce computational complexity from O(n²*d) to O(n*d²). The final prediction combines logits from both the base and auxiliary networks, with extensive experiments validating effectiveness on both industrial and public datasets.

## Key Results
- 0.8484 AUC improvement on industrial dataset compared to state-of-the-art baselines
- 0.6272 AUC improvement on Ali-CCP public dataset
- 8.168% RPM and 2.86% eCPM increases in online A/B test

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-sensitive features capture inter-domain distinctions more effectively than general multi-domain modeling architectures.
- Mechanism: By identifying features with high inter-domain differences in both distribution and model effect, the system explicitly emphasizes features that reflect domain distinctions.
- Core assumption: Inter-domain differences are primarily reflected in a small subset of features rather than being distributed uniformly across all features.
- Evidence anchors:
  - [abstract] "Experiments demonstrate that existing multi-domain modeling methods may neglect domain-sensitive features, indicating insufficient learning of domain distinctions."
  - [section] "We find through experiments that they may neglect domain-sensitive features, thus affecting the learning of domain distinctions."
  - [corpus] Weak evidence; related papers focus on different aspects of multi-domain learning without directly addressing feature distribution differences.
- Break condition: If domain distinctions are not primarily captured by feature distributions but rather by complex feature interactions or higher-order patterns that cannot be isolated to individual features.

### Mechanism 2
- Claim: The Domain-Sensitive Feature Memory architecture improves multi-domain recommendation performance by extracting and retrieving domain-specific information.
- Mechanism: The architecture uses an Extractor to process domain-sensitive features and generate domain-specific information, which is then retrieved by the base model through linear attention mechanisms.
- Core assumption: Domain-specific information extracted from domain-sensitive features can be effectively retrieved and integrated into the base model to improve predictions.
- Evidence anchors:
  - [abstract] "we design a memory architecture that extracts domain-specific information from domain-sensitive features for the model to retrieve and integrate"
  - [section] "It takes domain-sensitive features as the input and extracts domain-specific information for the model to retrieve and utilize"
  - [corpus] Weak evidence; related papers don't directly address memory architectures for feature extraction and retrieval.
- Break condition: If the extracted domain-specific information is not useful for improving predictions or if the retrieval mechanism fails to properly integrate this information into the base model.

### Mechanism 3
- Claim: Linear attention reduces computational complexity while maintaining effectiveness for domain-specific information retrieval.
- Mechanism: By replacing standard softmax attention with linear attention, the system achieves O(n*d²) complexity instead of O(n²*d), where n is sequence length and d is token size.
- Core assumption: Linear attention can approximate softmax attention effectively enough for the retrieval task while providing significant computational savings.
- Evidence anchors:
  - [section] "the linear attention [17] as a substitute, which is defined by... Let n be the sequence length and d be the token size, Eq. (15) allows a complexity of O(n*d²) while the complexity of softmax attention is O(n²*d)"
  - [corpus] Weak evidence; related papers don't directly compare linear and softmax attention in this context.
- Break condition: If linear attention fails to capture important attention patterns that softmax attention would have captured, leading to degraded retrieval performance.

## Foundational Learning

- Concept: Feature attribution methods (Integrated Gradients)
  - Why needed here: To measure the effect of each feature on model predictions and identify domain-sensitive features based on both distribution differences and attribution scores.
  - Quick check question: How does Integrated Gradients calculate feature importance compared to simpler methods like gradients or occlusion?

- Concept: Domain adaptation and multi-domain learning paradigms
  - Why needed here: To understand the context of existing approaches (hard-sharing vs soft-sharing) and why the proposed method offers advantages by focusing on feature distributions.
  - Quick check question: What are the key differences between hard-sharing and soft-sharing approaches in multi-domain recommendation?

- Concept: Attention mechanisms and their computational complexity
  - Why needed here: To understand why linear attention is chosen over softmax attention and how it affects the overall system design and performance.
  - Quick check question: What is the computational complexity of softmax attention and how does linear attention reduce this complexity?

## Architecture Onboarding

- Component map: Embedding Layer -> Base DNN and Domain-Sensitive Feature Extractor -> Retrievers (linear attention) -> Logit Combination
- Critical path:
  1. Input features → Embedding layer → Base DNN and Extractor
  2. Extractor processes domain-sensitive features → Generates domain-specific information
  3. Retrievers apply linear attention between towers → Integrate domain-specific information
  4. Combine logits from base and auxiliary networks → Final prediction
- Design tradeoffs:
  - Computational efficiency vs. model expressiveness: Linear attention reduces complexity but may miss some attention patterns
  - Feature selection granularity: Using top-k domain-sensitive features vs. all features affects performance and complexity
  - Auxiliary loss contribution: Balancing the contribution of the auxiliary logit in the final output
- Failure signatures:
  - Poor performance on tail domains despite improvements on dominant domain
  - Computational overhead that prevents online deployment
  - Sensitivity to the choice of domain-sensitive features (poor performance with random feature selection)
  - Degradation in performance when removing retriever components
- First 3 experiments:
  1. Compare AUC performance with and without domain-sensitive feature attribution on a multi-domain dataset
  2. Test different numbers of domain-sensitive features (top-1, top-3, top-5) to find optimal selection
  3. Measure computational efficiency (FLOPs) with linear attention vs. softmax attention in the retriever modules

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed domain-sensitive feature attribution method perform when applied to sequential features compared to scalar features, and what are the specific challenges in extending the attribution method to sequential features?
- Basis in paper: [explicit] The paper discusses extending the attribution method to sequential features (Section 4.1.4) but does not provide experimental results comparing performance on sequential vs. scalar features.
- Why unresolved: The paper focuses on the methodology and shows results using both feature types but doesn't explicitly compare the effectiveness of the attribution method on sequential versus scalar features.
- What evidence would resolve it: Experiments comparing attribution accuracy and impact on model performance when using sequential vs. scalar features as input to the domain-sensitive feature memory architecture.

### Open Question 2
- Question: What is the optimal number of domain-sensitive features to select for the memory architecture, and how does this selection affect model performance and computational efficiency?
- Basis in paper: [explicit] The paper mentions selecting top-k domain-sensitive features but doesn't systematically investigate the impact of different k values on performance.
- Why unresolved: While the paper demonstrates effectiveness with specific k values (top-5 for industrial, top-1 for public dataset), it doesn't explore the sensitivity of results to different numbers of selected features.
- What evidence would resolve it: Systematic experiments varying the number of selected domain-sensitive features and measuring the trade-off between performance improvement and computational cost.

### Open Question 3
- Question: How does the performance of the proposed method scale with increasing number of domains, and what are the limitations of the approach in extremely multi-domain scenarios?
- Basis in paper: [inferred] The paper evaluates on datasets with 3-4 domains but doesn't test on scenarios with significantly more domains, leaving scalability concerns unaddressed.
- Why unresolved: The experimental validation is limited to moderate numbers of domains, and the paper doesn't discuss theoretical or practical limitations when scaling to hundreds of domains.
- What evidence would resolve it: Experiments on datasets with varying numbers of domains (from few to hundreds) measuring performance degradation, computational complexity growth, and potential architectural modifications needed for scalability.

## Limitations

- Limited scalability testing: The method is only validated on datasets with 3-4 domains, leaving uncertainty about performance in scenarios with hundreds of domains.
- Proprietary data dependency: The industrial dataset details remain undisclosed, making independent verification of the 8.168% RPM improvement difficult.
- Assumption of feature-based distinctions: The method assumes domain differences are primarily captured by individual feature distributions rather than complex feature interactions.

## Confidence

**High confidence**: The computational efficiency claims for linear attention (O(n*d²) vs O(n²*d)) are mathematically sound and well-established in the literature. The experimental methodology for measuring domain-sensitivity using Integrated Gradients combined with JS/Wasserstein distances follows established practices.

**Medium confidence**: The claim that existing multi-domain methods "neglect domain-sensitive features" is based on observed performance gaps but lacks direct ablation studies showing how specific prior architectures fail to capture these features. The 0.8484 AUC improvement on industrial data and 0.6272 on public data are credible given the methodology, though exact baselines and comparison conditions could affect magnitude.

**Low confidence**: The online A/B test results showing 8.168% RPM and 2.86% eCPM improvements are difficult to verify without access to the industrial dataset and detailed experimental setup. The claim that these improvements stem specifically from capturing domain-sensitive features rather than general model improvements lacks rigorous isolation.

## Next Checks

1. **Ablation study on feature selection**: Systematically vary the number of domain-sensitive features (k=1,3,5,10) and measure performance degradation when using random feature subsets versus the proposed domain-sensitive selection to validate that feature selection quality drives improvements.

2. **Direct comparison with hard/soft-sharing baselines**: Implement and compare against specific hard-sharing and soft-sharing architectures on the same datasets using identical feature sets and hyperparameters to quantify the exact contribution of the domain-sensitive feature memory component.

3. **Cross-domain generalization test**: Train the model on a subset of domains and evaluate on held-out domains to measure whether the domain-sensitive feature memory generalizes to unseen domains or overfits to specific domain characteristics.