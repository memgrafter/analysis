---
ver: rpa2
title: Towards Scalable Foundation Models for Digital Dermatology
arxiv_id: '2411.05514'
source_url: https://arxiv.org/abs/2411.05514
tags:
- foundation
- images
- tasks
- dermatology
- vit-t
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores domain-specific foundation models for digital
  dermatology to address the challenge of limited high-quality labeled data. The authors
  pre-train models using self-supervised learning on over 240,000 dermatological images
  and evaluate them across 12 downstream diagnostic tasks.
---

# Towards Scalable Foundation Models for Digital Dermatology

## Quick Facts
- arXiv ID: 2411.05514
- Source URL: https://arxiv.org/abs/2411.05514
- Reference count: 25
- Key outcome: Domain-specific SSL pre-training outperforms ImageNet pre-training and approaches performance of models 50× larger

## Executive Summary
This paper addresses the challenge of limited labeled dermatological data by developing domain-specific foundation models using self-supervised learning on over 240,000 images. The authors evaluate their approach across 12 downstream diagnostic tasks, demonstrating that models pre-trained with DINO and iBOT outperform domain-agnostic ImageNet pre-trained models. The work emphasizes developing smaller, more efficient models suitable for resource-limited clinical settings while achieving comparable performance to much larger models like MONET. The study also shows significant label efficiency gains, reducing the need for annotated samples by a factor of 2 on average.

## Method Summary
The authors pre-train vision transformer and ResNet models using self-supervised learning methods (DINO, iBOT, SimCLR, BYOL) on 242,039 dermatological images from multiple public and private datasets. They evaluate these models on 12 downstream dermatological diagnostic tasks using frozen kNN and linear classifiers, comparing against ImageNet pre-trained baselines and the larger MONET model. The pre-training uses standard SSL augmentations at 224×224 resolution for 100 epochs, with downstream evaluation including low-data scenarios to assess label efficiency.

## Key Results
- Domain-specific SSL models (DINO, iBOT) outperform ImageNet pre-trained models across 12 dermatological tasks
- Smaller models (ViT-T, 5M parameters) approach performance of MONET (ViT-L, 304M parameters)
- DINO model reduces needed annotated samples by factor of 2 on average for downstream tasks
- Models generalize well to tasks not overlapping with pre-training data

## Why This Works (Mechanism)

### Mechanism 1
Domain-specific pre-training with SSL methods like DINO or iBOT yields more effective feature representations for dermatological diagnosis than domain-agnostic ImageNet pre-training. SSL methods learn visual representations without labeled data by creating artificial supervised objectives (e.g., contrastive loss, teacher-student architecture) that exploit invariances in dermatological images, leading to features that generalize better across diverse skin conditions and imaging modalities.

### Mechanism 2
Smaller models (e.g., ViT-T) trained with domain-specific SSL can achieve performance comparable to much larger models (e.g., MONET) for dermatological tasks. Efficient SSL methods combined with domain-specific knowledge allow smaller models to learn more relevant representations, reducing the need for model size to capture performance.

### Mechanism 3
Domain-specific SSL pre-training improves label efficiency, reducing the number of annotated samples needed for downstream tasks. Pre-trained representations capture more relevant visual information for dermatological diagnosis, allowing classifiers to achieve higher performance with fewer labeled examples.

## Foundational Learning

- **Concept**: Self-Supervised Learning (SSL) methods
  - Why needed here: To learn meaningful visual representations from large unlabeled dermatological image datasets without relying on expensive expert annotations
  - Quick check question: What is the main difference between supervised and self-supervised learning in the context of image representation learning?

- **Concept**: Transfer Learning
  - Why needed here: To adapt pre-trained models to specific dermatological tasks with limited labeled data, leveraging knowledge gained from large-scale pre-training
  - Quick check question: How does fine-tuning differ from using frozen features in transfer learning?

- **Concept**: Vision Transformers (ViTs)
  - Why needed here: To capture global context and local details in dermatological images, which is crucial for accurate diagnosis across diverse skin conditions and imaging modalities
  - Quick check question: What are the key architectural differences between ViTs and traditional CNNs?

## Architecture Onboarding

- **Component map**: Pre-training data collection -> SSL method selection (DINO, iBOT, etc.) -> Model architecture choice (ViT-T vs ResNet-50) -> Hyperparameter tuning -> Downstream task evaluation (frozen kNN/linear classifiers, low-data scenarios)
- **Critical path**: Curate diverse unlabeled dermatological images -> Implement chosen SSL method with appropriate hyperparameters -> Train model until convergence -> Evaluate on suite of downstream tasks -> Compare against baselines (ImageNet, MONET)
- **Design tradeoffs**: Model size vs. performance (ViT-T smaller but competitive vs. MONET larger but more resource-intensive), choice of SSL method (DINO vs. iBOT vs. SimCLR), use of frozen features vs. fine-tuning
- **Failure signatures**: Poor generalization across diverse skin types or imaging modalities, overfitting to pre-training data, failure to converge during SSL training, low performance on downstream tasks despite pre-training
- **First 3 experiments**:
  1. Implement and train a basic DINO model on a subset of the pre-training data to verify the pipeline works
  2. Compare DINO and iBOT on a single downstream task to assess which SSL method performs better
  3. Evaluate label efficiency by training classifiers on increasing amounts of labeled data for a representative task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do domain-specific foundation models perform on diverse skin tones beyond Fitzpatrick types I-III, particularly in populations underrepresented in current datasets?
- Basis in paper: The paper mentions evaluating on Fitzpatrick17k which includes Fitzpatrick types I-L, and discusses the importance of equitable representation across skin tones, but doesn't specifically address performance on underrepresented populations
- Why unresolved: The study primarily evaluates on Fitzpatrick17k which focuses on Fitzpatrick types I-L, and while it mentions evaluating on PASSION (Sub-Saharan African patients), it doesn't provide detailed performance analysis for populations with darker skin tones
- What evidence would resolve it: Performance metrics broken down by specific Fitzpatrick types IV-VI and other underrepresented skin tones across all 12 downstream tasks

### Open Question 2
- Question: What is the optimal balance between model size and performance for resource-constrained clinical settings, and how does this vary across different dermatological diagnostic tasks?
- Basis in paper: The paper emphasizes developing smaller models suitable for resource-limited settings and shows that DINO (ViT-T with 5M parameters) approaches MONET's (ViT-L with 304M parameters) performance, but doesn't explore the full spectrum of model sizes
- Why unresolved: The study only compares one small model (ViT-T) against one large model (ViT-L) and doesn't investigate intermediate sizes or task-specific optimal model sizes
- What evidence would resolve it: Performance curves comparing multiple model sizes (e.g., ViT-S, ViT-B, ViT-L) across all 12 tasks to identify optimal size-performance trade-offs

### Open Question 3
- Question: How well do the pre-trained models generalize to rare dermatological conditions not present in the pre-training dataset?
- Basis in paper: The authors note that some tasks did not overlap with pre-training data and evaluate generalization, but don't specifically analyze performance on rare conditions
- Why unresolved: The study focuses on common diagnostic tasks but doesn't investigate model performance on rare conditions that would benefit most from foundation models in clinical practice
- What evidence would resolve it: Detailed performance analysis on rare conditions and comparison of label efficiency for rare vs. common conditions

### Open Question 4
- Question: How do the foundation models perform on multi-modal dermatological data (e.g., combining clinical images with dermoscopic images or patient metadata)?
- Basis in paper: The paper mentions evaluating on both clinical and dermoscopic images separately but doesn't explore multi-modal integration
- Why unresolved: The study treats different image modalities as separate tasks rather than exploring how foundation models can integrate multi-modal information for improved diagnosis
- What evidence would resolve it: Performance comparison of uni-modal vs. multi-modal models and analysis of which combinations provide the most benefit

## Limitations

- Evaluation focuses primarily on frozen feature extraction and linear classification scenarios, which may not represent real-world clinical deployment where fine-tuning is typically required
- Comparison against MONET doesn't account for potential architectural differences beyond representation quality
- Study relies on mix of public and private datasets, with private Basel collection potentially introducing bias that limits generalizability

## Confidence

- **High Confidence**: The claim that domain-specific SSL pre-training outperforms ImageNet pre-training for dermatological tasks is well-supported by presented experimental results across 12 downstream tasks
- **Medium Confidence**: The assertion that smaller models can match larger models (50× size difference) in performance is supported but would benefit from additional ablation studies on different model architectures and sizes
- **Medium Confidence**: The label efficiency improvement claim (2× reduction in annotated samples) is demonstrated but only evaluated in controlled scenarios, requiring validation in more diverse real-world settings

## Next Checks

1. **Fine-tuning validation**: Evaluate the pre-trained models using full fine-tuning rather than just frozen features to assess real-world applicability and compare against state-of-the-art fine-tuned models

2. **Cross-institutional validation**: Test model performance on external dermatology datasets not used in pre-training or initial evaluation to verify generalizability across different clinical settings and imaging equipment

3. **Clinical utility assessment**: Conduct a user study with dermatologists to evaluate whether the performance gains translate to meaningful improvements in diagnostic accuracy and workflow efficiency in clinical practice