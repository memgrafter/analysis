---
ver: rpa2
title: 'FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs'
arxiv_id: '2402.05904'
source_url: https://arxiv.org/abs/2402.05904
tags:
- claim
- claims
- llms
- matching
- fact-checking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of misinformation spread on social
  media by introducing FACT-GPT, a system that uses Large Language Models (LLMs) to
  automate the claim matching stage of fact-checking. The core method involves training
  specialized LLMs on synthetic datasets to identify social media content that aligns
  with, contradicts, or is irrelevant to previously debunked claims.
---

# FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs

## Quick Facts
- arXiv ID: 2402.05904
- Source URL: https://arxiv.org/abs/2402.05904
- Reference count: 28
- One-line primary result: Fine-tuned smaller LLMs can match larger models' accuracy in claim matching tasks, achieving F1 scores of 0.83, 0.67, and 0.44 for entailment, neutral, and contradiction classes respectively.

## Executive Summary
This paper addresses the critical challenge of misinformation spread on social media by introducing FACT-GPT, a system that automates the claim matching stage of fact-checking using Large Language Models (LLMs). The core innovation involves training specialized LLMs on synthetic datasets to identify social media content that aligns with, contradicts, or is irrelevant to previously debunked claims. The research demonstrates that fine-tuned smaller LLMs can achieve accuracy comparable to larger models in identifying related claims, with particular success in entailment detection (F1=0.83) while showing more modest performance in contradiction detection (F1=0.44).

## Method Summary
The method employs a three-step approach: first, synthetic training data is generated using LLMs (GPT-4, GPT-3.5-Turbo, and Llama-2-70b) to create tweets that either support, contradict, or are neutral to a collection of 1,225 debunked COVID-19-related claims. Second, smaller LLMs (GPT-3.5-Turbo, Llama-2-13b, and Llama-2-7b) are fine-tuned on these synthetic datasets using the textual entailment framework, treating claim matching as a three-class classification problem. Finally, the fine-tuned models are evaluated against human-annotated test data consisting of 1,225 pairs of tweets and claims, with performance measured using F1 scores for each class.

## Key Results
- Fine-tuned smaller LLMs achieved F1 scores of 0.83 for entailment, 0.67 for neutral, and 0.44 for contradiction classes
- Models fine-tuned on synthetic datasets outperformed their pre-trained versions
- Specialized LLMs matched the accuracy of larger models in identifying related claims, closely mirroring human judgment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuned smaller LLMs can match the accuracy of larger models in claim matching tasks.
- Mechanism: Smaller models are trained on synthetic datasets that are specifically tailored to the claim matching task, allowing them to learn task-specific patterns efficiently.
- Core assumption: The quality and relevance of synthetic training data are sufficient to compensate for the smaller model size.
- Evidence anchors:
  - [abstract] "Our evaluation shows that our specialized LLMs can match the accuracy of larger models in identifying related claims, closely mirroring human judgment."
  - [section] "Notably, models fine-tuned on synthetic datasets exhibited superior performance in comparison to the pre-trained versions."

### Mechanism 2
- Claim: Claim matching can be effectively modeled as a three-class textual entailment task (Entailment, Neutral, Contradiction).
- Mechanism: By framing the problem as textual entailment, the model leverages established NLP techniques for understanding semantic relationships between claims and posts.
- Core assumption: The semantic relationships in claim matching align well with the definitions of entailment, neutral, and contradiction used in NLP.
- Evidence anchors:
  - [section] "Claim matching tasks can be configured in various forms including but not limited to textual entailment [16], ranking [15, 22], and binary detection tasks [13]."

### Mechanism 3
- Claim: Synthetic data generation using LLMs improves model performance by providing balanced and diverse training examples.
- Mechanism: LLMs generate synthetic examples across all three classes (Entailment, Neutral, Contradiction) in equal proportions, addressing class imbalance issues.
- Core assumption: LLMs can generate realistic and diverse examples that accurately represent the complexity of real-world claim matching scenarios.
- Evidence anchors:
  - [section] "Using a collection of debunked claims as a basis, we generated tweets that either supported, were neutral to, or contradicted these claims."
  - [section] "A total of 3,675 synthetic tweets were generated from each model, ensuring an equal distribution across all three categories."

## Foundational Learning

- Concept: Textual Entailment and Semantic Relationships
  - Why needed here: Understanding how to model relationships between claims and social media posts as entailment, neutral, or contradiction is fundamental to the claim matching task.
  - Quick check question: Can you explain the difference between entailment and contradiction in the context of claim matching?

- Concept: Synthetic Data Generation and Augmentation
  - Why needed here: Creating synthetic training data is crucial for addressing class imbalance and providing diverse examples for model training.
  - Quick check question: How would you design a prompt to generate synthetic examples for the contradiction class?

- Concept: Model Fine-tuning and Transfer Learning
  - Why needed here: Fine-tuning pre-trained LLMs on task-specific synthetic data is the core approach for achieving high performance with smaller models.
  - Quick check question: What are the key differences between fine-tuning and prompt engineering when adapting LLMs to a new task?

## Architecture Onboarding

- Component map: Synthetic Data Generator (LLMs) -> Fine-tuning Framework -> Claim Matching Model -> Evaluation Pipeline
- Critical path: Synthetic Data Generation → Model Fine-tuning → Claim Matching Inference
- Design tradeoffs:
  - Model size vs. computational cost: Smaller models are more efficient but may require higher-quality synthetic data
  - Synthetic vs. real data: Synthetic data addresses class imbalance but may lack real-world complexity
  - Generic vs. task-specific models: Fine-tuning allows adaptation but may reduce generalization
- Failure signatures:
  - Overfitting to synthetic patterns: Model performs well on validation data but poorly on real-world claims
  - Class imbalance issues: Model struggles with the contradiction class despite balanced synthetic training data
  - Prompt sensitivity: Model performance varies significantly with minor changes in input prompts
- First 3 experiments:
  1. Compare the performance of fine-tuned models on synthetic validation data vs. human-annotated test data
  2. Ablation study: Train models on synthetic data generated by different LLMs (GPT-4 vs. GPT-3.5-Turbo vs. Llama-2) and compare performance
  3. Class-wise analysis: Evaluate model performance separately on entailment, neutral, and contradiction classes to identify specific weaknesses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can FACT-GPT's performance in detecting contradictory posts be improved, given the importance of rebuttals in curbing the spread of misinformation?
- Basis in paper: [explicit] The paper states that the models struggle with categorizing posts that contradict debunked claims and emphasizes the importance of rebuttals in preventing the spread of misinformation.
- Why unresolved: The paper does not provide specific methods or approaches to enhance the detection of contradictory posts.
- What evidence would resolve it: Experimental results showing improved F1 scores for contradiction detection after implementing new methods or approaches.

### Open Question 2
- Question: What are the potential methods for data synthesis and augmentation that could further optimize FACT-GPT's performance?
- Basis in paper: [explicit] The paper suggests that future studies should focus on discovering different methods for data synthesis and augmentation to optimize FACT-GPT.
- Why unresolved: The paper does not provide specific methods or techniques for data synthesis and augmentation.
- What evidence would resolve it: Comparative studies demonstrating the effectiveness of various data synthesis and augmentation methods on FACT-GPT's performance.

### Open Question 3
- Question: How can natural language explanation (NLE) capabilities be integrated within GPT models to enhance transparency in the fact-checking process?
- Basis in paper: [explicit] The paper mentions that exploring the integration of NLE capabilities within GPT models can further enhance transparency.
- Why unresolved: The paper does not provide details on how to implement NLE capabilities or the potential impact on model performance.
- What evidence would resolve it: Experimental results showing improved transparency and model performance after integrating NLE capabilities.

## Limitations

- The evaluation relies on human annotations for a relatively small dataset of 1,225 claim-post pairs, which may not capture the full complexity of real-world misinformation
- The F1 score of 0.44 for the contradiction class indicates significant limitations in detecting posts that contradict debunked claims, a critical function for fact-checking
- The study does not evaluate model performance on unseen claims or claims from different domains beyond COVID-19, raising questions about generalizability

## Confidence

**High Confidence**: The finding that fine-tuned smaller LLMs can achieve comparable accuracy to larger models (F1 scores of 0.83, 0.67, and 0.44 for entailment, neutral, and contradiction respectively) is well-supported by the experimental results and controlled comparisons.

**Medium Confidence**: The claim that synthetic data generation significantly improves model performance has moderate support, though the study doesn't provide direct comparisons with models trained only on real data or with different ratios of synthetic to real data.

**Low Confidence**: The assertion that FACT-GPT "closely mirrors human judgment" is based on limited human annotations and doesn't account for potential disagreements among human annotators or variations in judgment criteria.

## Next Checks

1. Test the trained models on an independent dataset of claim-post pairs from different domains (e.g., climate change, elections) to assess generalizability beyond COVID-19 claims.

2. Conduct detailed analysis of where the model disagrees with human annotators, particularly focusing on the contradiction class where performance is weakest, to identify systematic biases or failure patterns.

3. Evaluate the system's performance when deployed in a semi-automated fact-checking workflow, measuring factors like annotation time savings, false positive/negative rates, and fact-checker satisfaction with model suggestions.