---
ver: rpa2
title: 'DART: Denoising Autoregressive Transformer for Scalable Text-to-Image Generation'
arxiv_id: '2410.08159'
source_url: https://arxiv.org/abs/2410.08159
tags:
- dart
- generation
- diffusion
- image
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DART, a transformer-based model that unifies
  autoregressive and diffusion modeling within a non-Markovian framework. Unlike traditional
  diffusion models that rely on Markovian assumptions, DART leverages the full generation
  trajectory to iteratively denoise image patches spatially and spectrally using an
  autoregressive transformer architecture.
---

# DART: Denoising Autoregressive Transformer for Scalable Text-to-Image Generation

## Quick Facts
- arXiv ID: 2410.08159
- Source URL: https://arxiv.org/abs/2410.08159
- Reference count: 29
- This paper proposes DART, a transformer-based model that unifies autoregressive and diffusion modeling within a non-Markovian framework, achieving competitive performance on both class-conditioned (ImageNet) and text-to-image generation tasks.

## Executive Summary
DART introduces a novel approach to text-to-image generation by combining diffusion modeling with autoregressive transformers in a non-Markovian framework. Unlike traditional diffusion models that rely on Markovian assumptions and only condition on the previous step, DART leverages the full generation trajectory to iteratively denoise image patches spatially and spectrally. The approach introduces two key improvements: DART-AR adds token-level autoregressive modeling for finer control and improved quality, while DART-FM employs flow-based refinement to enhance expressiveness and smoothness between denoising steps. The model achieves competitive performance with fewer denoising steps compared to traditional diffusion models, offering a scalable and efficient alternative for text-to-image generation.

## Method Summary
DART is a transformer-based model that unifies autoregressive and diffusion modeling through a non-Markovian framework. The core approach involves using a transformer architecture to predict image patches autoregressively while conditioning on the full generation trajectory rather than just the previous step. The model operates by iteratively denoising image patches through spatial and spectral processing using rotary positional embeddings and causal attention masks. DART-AR adds token-level autoregressive modeling for improved quality, while DART-FM introduces flow-based refinement using flow matching instead of Gaussian assumptions. The model is trained with a denoising objective and uses classifier-free guidance for conditioning on text or class labels.

## Key Results
- DART-FM achieves an FID of 11.12 on COCO text-to-image generation
- DART uses only 16 denoising steps compared to 250+ steps in traditional diffusion models
- The model achieves competitive performance on both class-conditioned (ImageNet) and text-to-image generation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DART's non-Markovian formulation allows the model to leverage the full generation trajectory, improving denoising efficiency.
- Mechanism: Instead of relying only on the previous step's information (Markovian), DART conditions denoising on all future noisy steps, enabling richer context and more accurate denoising predictions.
- Core assumption: The full trajectory provides more informative signals than just the immediate prior step.
- Evidence anchors:
  - [abstract] "We argue that the Markovian property limits the model's ability to fully utilize the generation trajectory, leading to inefficiencies during training and inference."
  - [section 3.1] "The non-Markovian formulation in DART enables the model to leverage the full generative trajectory during training and inference, while retaining the progressive modeling benefits of diffusion models"

### Mechanism 2
- Claim: Token-level autoregressive modeling (DART-AR) captures dependencies between image tokens, enabling finer control and improved quality.
- Mechanism: By decomposing denoising into autoregressive steps at the token level, DART-AR ensures tokens are not generated independently, which is strictly stronger than the original DART.
- Core assumption: Token independence assumption in standard diffusion is too simplistic for complex image structures.
- Evidence anchors:
  - [section 3.2] "the independent Gaussian assumption of pθ(xt−1|xt:T ) is inaccurate to approximate the complex true distribution of q(xt−1|xt:T )"
  - [section 3.2] "The autoregressive decomposition ensures each tokens are not independent, which is strictly stronger than the original DART"

### Mechanism 3
- Claim: Flow-based refinement (DART-FM) enhances model expressiveness and smooths transitions between denoising steps.
- Mechanism: Instead of Gaussian modeling at each step, DART-FM applies a continuous flow network over multiple iterations to bridge the gap between intermediate predictions and the target denoised image.
- Core assumption: Gaussian assumptions are insufficient to capture complex distributions in image denoising.
- Evidence anchors:
  - [section 3.2] "Alternatively, we can improve the expressiveness of pθ(xt−1|xt:T ) by abandoning the Gaussian assumption, similar to Li et al. (2024)"
  - [section 3.2] "We train vϕ via flow matching (Liu et al., 2022; Lipman et al., 2023; Albergo et al., 2023) due to its simplicity"

## Foundational Learning

- Concept: Non-Markovian diffusion vs Markovian diffusion
  - Why needed here: Understanding the fundamental shift from standard diffusion that only conditions on previous step to DART's full trajectory conditioning
  - Quick check question: What key limitation of Markovian diffusion does DART address?

- Concept: Autoregressive modeling in image generation
  - Why needed here: DART uses transformer architecture to predict image patches autoregressively, requiring understanding of how this differs from diffusion
  - Quick check question: How does DART's autoregressive approach differ from traditional pixel-level autoregressive models like PixelCNN?

- Concept: Flow matching vs diffusion
  - Why needed here: DART-FM uses flow matching for refinement, which is a different paradigm from diffusion that requires understanding
  - Quick check question: What is the key difference between flow matching and diffusion in terms of how they model the generation process?

## Architecture Onboarding

- Component map: Input → Patch embedding → Transformer blocks with causal attention → Output prediction → (Optional) Flow network refinement → Next step generation
- Critical path: Input → Patch embedding → Transformer blocks with causal attention → Output prediction → (Optional) Flow network refinement → Next step generation
- Design tradeoffs:
  - DART vs DART-AR: Better quality vs higher inference cost
  - DART vs DART-FM: Simpler implementation vs potentially better expressiveness
  - Number of noise levels: Fewer levels = faster but lower quality
- Failure signatures:
  - Poor FID scores: May indicate insufficient model capacity or training issues
  - High inference time: Likely with DART-AR due to many autoregressive steps
  - Memory errors: Could be from too many noise levels or high resolution
- First 3 experiments:
  1. Train baseline DART with 16 noise levels on ImageNet 256x256, measure FID
  2. Compare DART vs DART-AR on same data to verify quality improvement claim
  3. Test different numbers of noise levels (4, 8, 16) to find efficiency-quality tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal non-Markovian noise schedule that maximizes the signal-to-noise ratio while maintaining computational efficiency in DART?
- Basis in paper: Explicit. The paper discusses non-Markovian diffusion formulation and mentions Proposition 1 about achieving maximal SNR, but does not explore complex non-Markovian processes beyond independent noising.
- Why unresolved: The paper only explores the simplest independent noising process and defers exploration of more complex non-Markovian processes to future work. The optimal balance between SNR maximization and computational efficiency remains unknown.
- What evidence would resolve it: Empirical comparison of different non-Markovian noise schedules (beyond the simple independent case) showing their impact on generation quality, training efficiency, and final model performance metrics like FID.

### Open Question 2
- Question: How does DART's performance scale with model size compared to diffusion models when trained for equivalent FLOPs?
- Basis in paper: Inferred. The paper mentions that baseline models like DiT are trained for 7M iterations versus DART's 500k iterations, and that DART-FM achieves better FID than DiT(16) despite this difference, but doesn't provide a direct FLOPs-matched comparison.
- Why unresolved: The paper acknowledges FLOPs differences between DART and baselines but doesn't conduct controlled experiments matching training compute budgets. This leaves uncertainty about DART's true efficiency advantage.
- What evidence would resolve it: Training DART and comparable diffusion models for matched FLOPs budgets and measuring the resulting performance trade-offs across different model scales.

### Open Question 3
- Question: What is the theoretical relationship between the number of denoising steps (T) and generation quality in DART compared to traditional diffusion models?
- Basis in paper: Explicit. The paper notes that DART uses T=16 steps while diffusion models often use 250+ steps, and mentions that DART can generate plausible samples with as few as 4 noise levels, but doesn't provide systematic analysis of this relationship.
- Why unresolved: While the paper demonstrates DART works with fewer steps than diffusion models, it doesn't establish the precise scaling laws or provide theoretical justification for why fewer steps suffice in the non-Markovian framework.
- What evidence would resolve it: Systematic experiments varying T in DART while measuring quality metrics, and theoretical analysis explaining why non-Markovian modeling enables quality preservation with fewer steps.

## Limitations

- The computational efficiency claims need more thorough validation, particularly regarding the memory overhead from storing full generation trajectories
- The flow-based refinement (DART-FM) introduces complexity without clear evidence that it outperforms simpler alternatives like increased model capacity or longer training
- Real-world deployment considerations and scalability to higher resolutions beyond 256x256 are not demonstrated

## Confidence

**High Confidence** (Theoretical Foundation):
- The mathematical derivation of the non-Markovian diffusion framework is correct and follows established principles
- The autoregressive decomposition in DART-AR is logically sound
- The flow matching approach in DART-FM is technically valid

**Medium Confidence** (Empirical Claims):
- Performance improvements over existing models are demonstrated but limited to specific benchmarks
- The claimed efficiency gains need more comprehensive comparison across different hardware setups
- The quality differences between DART variants require more systematic evaluation

**Low Confidence** (Practical Implementation):
- Real-world deployment considerations are not addressed
- Scalability to higher resolutions beyond 256x256 is not demonstrated
- The computational overhead of the full trajectory conditioning in practical scenarios is unclear

## Next Checks

1. **Memory Efficiency Validation**: Measure and compare the memory usage of DART's full trajectory conditioning versus traditional Markovian diffusion across different batch sizes and sequence lengths to quantify the practical overhead claims.

2. **Cross-Resolution Scalability**: Train and evaluate DART on 512x512 resolution datasets to validate whether the performance gains and efficiency benefits scale beyond the 256x256 resolution used in the paper.

3. **Ablation Study on Trajectory Length**: Systematically vary the number of denoising steps and trajectory length to determine the optimal tradeoff between generation quality and computational efficiency, particularly for the DART-AR variant where the autoregressive nature may compound costs.