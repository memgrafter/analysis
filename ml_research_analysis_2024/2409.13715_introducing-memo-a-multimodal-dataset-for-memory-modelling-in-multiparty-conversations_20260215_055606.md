---
ver: rpa2
title: 'Introducing MeMo: A Multimodal Dataset for Memory Modelling in Multiparty
  Conversations'
arxiv_id: '2409.13715'
source_url: https://arxiv.org/abs/2409.13715
tags:
- memory
- participants
- data
- group
- conversational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the MeMo corpus, a multimodal dataset of
  small-group discussions annotated with participants' memory retention reports. The
  corpus includes 31 hours of conversations on Covid-19, repeated 3 times over 2 weeks,
  with video, audio, and behavioral annotations.
---

# Introducing MeMo: A Multimodal Dataset for Memory Modelling in Multiparty Conversations

## Quick Facts
- arXiv ID: 2409.13715
- Source URL: https://arxiv.org/abs/2409.13715
- Authors: Maria Tsfasman; Bernd Dudzik; Kristian Fenech; Andras Lorincz; Catholijn M. Jonker; Catharine Oertel
- Reference count: 40
- Primary result: First multimodal dataset for memory modeling in multiparty conversations, enabling computational approaches to understanding memory retention in group discussions

## Executive Summary
This paper introduces the MeMo corpus, a multimodal dataset designed to advance research in conversational memory modeling within small-group discussions. The corpus contains 31 hours of video-recorded conversations on Covid-19 topics, conducted across three sessions over two weeks with 54 participants. What distinguishes MeMo is its combination of free-recall memory tasks with precise first-party annotations mapping memories to exact video timestamps, along with multimodal features including eye gaze, prosody, and body pose. The dataset enables both the study of memory encoding and retention dynamics and the development of computational models for predicting memorable moments in multiparty conversations.

## Method Summary
The MeMo corpus was created through structured online discussions using Zoom, where participants engaged in 45-60 minute conversations on Covid-19 topics across three sessions separated by 3-4 day intervals. Each session was video and audio recorded, and participants completed questionnaires before and after discussions measuring interpersonal closeness and group perception. Memory retention was assessed through free recall tasks immediately after each session (short-term memory) and again 3-4 days later (long-term memory). Participants then mapped their recalled memories to specific time segments in the recorded conversation, creating precise ground truth annotations. The dataset includes extracted features such as transcripts, eye gaze patterns, prosody, body pose, and facial action units, all synchronized and preprocessed for research use.

## Key Results
- First multimodal corpus specifically designed for memory modeling in multiparty conversations
- Demonstrates prediction of group memorability levels using eye-gaze features with balanced accuracy scores of 0.42-0.43
- Provides 31 hours of conversation data with precise memory annotations mapped to video timestamps
- Enables investigation of both encoding (what makes moments memorable) and retention (what is remembered over time) in group discussions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Self-reported memory labels anchored to exact video timestamps improve ground truth accuracy over third-party annotations.
- **Mechanism**: Participants first recall memories in free text, then map those memories to exact start/end times in the recorded conversation. This eliminates ambiguity about which conversational event a memory refers to.
- **Core assumption**: Participants can accurately identify the video segment corresponding to their memory recall.
- **Evidence anchors**:
  - [abstract] "combining a free-recall task with a subsequent first-party annotation of the recorded memorable moments to a video time frame"
  - [section] "participants themselves did the assignment of their self-reports to specific events that happened within the conversation"
  - [corpus] Weak - the paper does not report validation that participant annotations are accurate.
- **Break Condition**: If participants cannot accurately locate the video segment corresponding to their memory, the ground truth quality degrades.

### Mechanism 2
- **Claim**: Multi-session structure over 2 weeks allows modeling of memory retention dynamics.
- **Mechanism**: By collecting memory reports immediately after each session and again 3-4 days later, the dataset captures both short-term encoding and longer-term retention patterns.
- **Core assumption**: The 3-4 day interval is sufficient to observe meaningful forgetting.
- **Evidence anchors**:
  - [abstract] "repeated 3 times over 2 weeks" and "short-term and long-term memory reports"
  - [section] "memory reports collected immediately after interactions for short-term memory and after 3-4 days for longer-term memory"
  - [corpus] Weak - the paper does not provide empirical validation of forgetting curves from this specific dataset.
- **Break Condition**: If the 3-4 day interval is too short or too long relative to actual forgetting patterns, retention modeling will be less accurate.

### Mechanism 3
- **Claim**: Ecological validity of home-based video calls preserves natural conversational dynamics for memory modeling.
- **Mechanism**: Recording conversations in participants' homes using their own devices and typical video conferencing software maintains natural interaction patterns compared to lab settings.
- **Core assumption**: The video call format sufficiently captures the conversational dynamics that would occur in person.
- **Evidence anchors**:
  - [section] "All discussion sessions happened online, through a Zoom video-call platform... in the comfort of their own homes"
  - [section] "preserve a natural conversational setting typical to real-world environments"
  - [corpus] Weak - the paper does not empirically compare memory patterns between video calls and in-person conversations.
- **Break Condition**: If video call dynamics differ significantly from in-person conversations in ways that affect memory encoding, the model's generalizability to face-to-face settings may be limited.

## Foundational Learning

- **Concept**: Free recall vs. recognition memory tasks
  - Why needed here: The dataset uses free recall tasks to capture memory content, requiring understanding of how this differs from recognition-based approaches
  - Quick check question: What is the key difference between free recall and recognition memory tasks in terms of participant cognitive effort?

- **Concept**: Primacy and recency effects in memory
  - Why needed here: The dataset structure allows investigation of whether memories are more likely to be encoded at the beginning or end of conversations
  - Quick check question: How might primacy and recency effects manifest differently in multi-hour conversations versus short word lists?

- **Concept**: Theory of Mind in multiparty conversations
  - Why needed here: The dataset involves group discussions where participants must track multiple conversational threads and social dynamics
  - Quick check question: What cognitive challenges arise when tracking conversational memory across multiple participants simultaneously?

## Architecture Onboarding

- **Component map**: Data collection (Zoom recordings, questionnaires) -> Preprocessing (audio synchronization, pseudo-anonymization, calibration removal) -> Feature extraction (transcripts, eye gaze, prosody, body pose, facial action units) -> Memory annotation mapping (free recall reports to video timestamps) -> Modeling components (encoding prediction, retention prediction, retrieval modeling)

- **Critical path**: Data collection → preprocessing → feature extraction → memory annotation → model training → evaluation

- **Design tradeoffs**: 
  - Video call vs. in-person recording (ecological validity vs. conversational dynamics)
  - Self-reported vs. third-party memory annotations (accuracy vs. potential bias)
  - Multiple sessions vs. single session (longitudinal insights vs. participant fatigue)

- **Failure signatures**:
  - Poor audio synchronization leading to misaligned features
  - Inaccurate memory timestamp mapping due to participant fatigue
  - Insufficient participant diversity limiting model generalizability

- **First 3 experiments**:
  1. Validate memory annotation accuracy by comparing participant timestamp mappings against expert annotations on a subset
  2. Test encoding prediction baseline using eye gaze features as in the paper's example
  3. Explore session dependency by training models with and without session identifiers to quantify temporal effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do individual differences in memory encoding and retention affect the development of social bonds within groups over repeated interactions?
- Basis in paper: [explicit] The paper discusses how memory plays a crucial role in shaping social bonds and fostering relationship building, and mentions that understanding conversational memory can provide information on the long-term development of social connections within a group.
- Why unresolved: While the paper introduces the MeMo corpus to study conversational memory, it does not provide specific findings on how individual memory differences impact social bond development over time.
- What evidence would resolve it: Longitudinal analysis of the MeMo corpus data, correlating individual memory encoding/retention patterns with changes in IOS scores and group perception measures across sessions.

### Open Question 2
- Question: What specific conversational features or patterns are most predictive of memorable moments across different demographic groups?
- Basis in paper: [explicit] The paper introduces the MeMo corpus and mentions a preliminary study using eye-gaze features to predict group memorability levels, but does not provide detailed analysis of specific conversational features.
- Why unresolved: The paper presents the dataset and demonstrates its potential for memory prediction, but does not explore in-depth which conversational features are most predictive of memorability.
- What evidence would resolve it: Detailed analysis of the MeMo corpus data, examining correlations between various conversational features (e.g., linguistic patterns, non-verbal behaviors) and memorability across different demographic groups.

### Open Question 3
- Question: How does the topic of conversation influence memory encoding and retention in small-group discussions?
- Basis in paper: [explicit] The paper mentions that the MeMo corpus focuses on Covid-19 discussions and notes that the topic was chosen for its relevance, but does not explore how topic choice affects memory processes.
- Why unresolved: The paper introduces the dataset and its design choices but does not provide findings on how the specific topic of Covid-19 influences memory encoding and retention compared to other potential topics.
- What evidence would resolve it: Comparative analysis of memory encoding and retention patterns in the MeMo corpus data with similar datasets focused on different conversation topics, or replication of the study with varied topics.

## Limitations
- **Annotation accuracy uncertainty**: The paper does not validate the reliability of participants' self-reported memory timestamp mappings, creating uncertainty about ground truth quality.
- **Ecological generalizability gap**: Without empirical comparison between video call and in-person conversation memory patterns, it's unclear how well findings transfer to face-to-face settings.
- **Feature extraction validation**: The paper describes feature types but lacks detailed validation of extraction methods, raising questions about feature quality and consistency.

## Confidence
- **High Confidence**: The dataset creation methodology (multi-session design, free recall task structure, multimodal feature collection) is clearly specified and follows established practices in memory research and multimodal interaction analysis.
- **Medium Confidence**: The claim that this is "the first multimodal corpus for memory modeling in multiparty conversations" is plausible given the unique combination of features, but would require comprehensive literature review to verify definitively.
- **Low Confidence**: The empirical validation of memory annotation accuracy and the comparison of video vs. in-person memory dynamics are not provided, making claims about annotation quality and ecological validity uncertain.

## Next Checks
1. **Annotation Accuracy Validation**: Conduct a validation study where a subset of memory annotations are independently verified by multiple annotators to establish inter-rater reliability and quantify annotation accuracy.
2. **Feature Extraction Replication**: Replicate the feature extraction pipeline on a small subset of the data using open-source tools to verify that the described features can be consistently extracted and meet quality standards.
3. **Memory Dynamics Analysis**: Analyze the actual forgetting curves in the dataset to empirically verify that the 3-4 day interval captures meaningful retention patterns, rather than assuming this based on general memory research principles.