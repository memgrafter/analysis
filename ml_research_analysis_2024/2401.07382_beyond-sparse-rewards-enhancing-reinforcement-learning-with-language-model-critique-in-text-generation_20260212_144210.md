---
ver: rpa2
title: 'Beyond Sparse Rewards: Enhancing Reinforcement Learning with Language Model
  Critique in Text Generation'
arxiv_id: '2401.07382'
source_url: https://arxiv.org/abs/2401.07382
tags:
- reward
- learning
- rewards
- policy
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of sparse reward signals in
  reinforcement learning for text generation, where typically only a single reward
  is provided for an entire output. The proposed method, RELC, utilizes the critique
  capability of large language models (LLMs) to generate intermediate-step rewards
  during training.
---

# Beyond Sparse Rewards: Enhancing Reinforcement Learning with Language Model Critique in Text Generation

## Quick Facts
- arXiv ID: 2401.07382
- Source URL: https://arxiv.org/abs/2401.07382
- Reference count: 40
- Primary result: RELC significantly improves sample efficiency and performance in RL-based text generation by using LLM-generated intermediate rewards

## Executive Summary
This paper addresses the challenge of sparse reward signals in reinforcement learning for text generation, where typically only a single reward is provided for an entire output. The proposed method, RELC, utilizes the critique capability of large language models (LLMs) to generate intermediate-step rewards during training. It couples a policy model with a critic model that evaluates segments of the policy's output and produces token or span-level rewards. These intrinsic rewards are combined with extrinsic rewards in reinforcement learning training. Experiments on three text generation tasks—sentiment control, language model detoxification, and summarization—demonstrate that incorporating LLM-generated intrinsic rewards significantly improves both sample efficiency and overall performance of the policy model, as measured by both automatic and human evaluation metrics.

## Method Summary
RELC combines a policy model (which generates text) with a critic model (which evaluates the output and generates intermediate rewards). The critic LLM evaluates segments of the policy's output and translates this evaluation into intrinsic rewards that are combined with extrinsic rewards from the environment. The policy model is optimized using PPO to maximize the weighted sum of these rewards. The framework supports two settings: using a smaller policy model with a stronger critic, or using a single model for both roles (self-critique). The method aims to provide dense feedback signals at each generation step, addressing the credit assignment problem in sparse reward environments.

## Key Results
- RELC achieves superior performance compared to standard PPO baselines across sentiment control, detoxification, and summarization tasks
- The method demonstrates improved sample efficiency, requiring fewer training iterations to reach target performance
- Human evaluations confirm that RELC-generated text is preferred over baseline methods in terms of coherence, factuality, and overall quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The critic model provides token or span-level rewards that help with credit assignment in sparse reward environments.
- Mechanism: The critic LLM evaluates segments of the policy model's output and translates this evaluation into intrinsic rewards that are combined with extrinsic rewards. This provides dense feedback signals at each generation step.
- Core assumption: The critic LLM can accurately identify which tokens or spans contribute to the overall quality of the generated text and provide meaningful feedback.
- Evidence anchors:
  - [abstract] "The proposed method, RELC, utilizes the critique capability of large language models (LLMs) to generate intermediate-step rewards during training. It couples a policy model with a critic model that evaluates segments of the policy's output and produces token or span-level rewards."
  - [section] "The task of the critic model is to pinpoint the tokens or segments in the policy's output that directly contribute to receiving the environment's reward."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.507, average citations=0.6. Top related titles: Unveiling the Significance of Toddler-Inspired Reward Transition in Goal-Oriented Reinforcement Learning, Navigate the Unknown: Enhancing LLM Reasoning with Intrinsic Motivation Guided Exploration, Intrinsic Reward Policy Optimization for Sparse-Reward Environments.

### Mechanism 2
- Claim: Combining intrinsic and extrinsic rewards improves sample efficiency and final performance compared to using extrinsic rewards alone.
- Mechanism: The policy model is optimized to maximize the weighted sum of intrinsic and extrinsic rewards: J(θ)RELC = Eτ∼πθ[∑t=0T γt(α1rex + α2rin)]. This provides immediate feedback during generation rather than waiting for the final reward.
- Core assumption: The intrinsic rewards are well-aligned with the extrinsic rewards, so optimizing for both leads to better overall performance.
- Evidence anchors:
  - [abstract] "Incorporating artificial intrinsic rewards significantly improve both sample efficiency and the overall performance of the policy model"
  - [section] "We label the rewards generated by the critic model as 'intrinsic rewards' to differentiate them from the reward signals provided by the environment. The final reward is defined as the weighted sum of extrinsic and intrinsic rewards"
  - [corpus] Weak evidence. No specific citations found in corpus about combining intrinsic and extrinsic rewards for RL training.

### Mechanism 3
- Claim: Using a stronger critic model than policy model (or same model in self-critique setting) enables effective training even when the policy model is smaller.
- Mechanism: The framework allows for two settings: one where the policy model is smaller and paired with a more powerful critic model, and another where a single language model fulfills both roles. This flexibility enables effective training across different resource constraints.
- Core assumption: The critic model (whether stronger or same as policy) has sufficient capability to provide meaningful critique that improves the policy model's performance.
- Evidence anchors:
  - [abstract] "We investigate this approach under two different settings: one where the policy model is smaller and is paired with a more powerful critic model, and another where a single language model fulfills both roles."
  - [section] "We explicitly define an RL agent as the integration of 1) a policy model responsible for output generation, and 2) a critic model that tasked with assessing the quality of the outputs produced by the policy model."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.507, average citations=0.6. Top related titles: Unveiling the Significance of Toddler-Inspired Reward Transition in Goal-Oriented Reinforcement Learning, Navigate the Unknown: Enhancing LLM Reasoning with Intrinsic Motivation Guided Exploration, Intrinsic Reward Policy Optimization for Sparse-Reward Environments.

## Foundational Learning

- Concept: Reinforcement Learning (RL) fundamentals including MDPs, policy gradients, and actor-critic methods
  - Why needed here: The entire framework is built on RL principles, with the policy model being trained via policy gradient methods and the critic providing additional reward signals
  - Quick check question: What is the difference between value-based and policy-based RL methods, and which category does this framework fall into?

- Concept: Credit assignment problem in sequential decision making
  - Why needed here: The core motivation for this work is addressing the credit assignment problem that arises from sparse rewards in text generation tasks
  - Quick check question: Why is credit assignment particularly challenging in text generation tasks with sparse rewards?

- Concept: Large language model capabilities and prompt engineering
  - Why needed here: The critic model is an LLM that needs to be effectively prompted to provide meaningful critique of the policy model's outputs
  - Quick check question: What are the key considerations when designing prompts for an LLM to act as a critic for text generation?

## Architecture Onboarding

- Component map:
  - Policy model -> Text generation
  - Critic model -> Evaluation and intrinsic reward generation
  - Environment -> Extrinsic reward provision
  - Reward combiner -> Weighted sum of intrinsic and extrinsic rewards
  - RL optimizer -> Policy model training (PPO)

- Critical path:
  1. Policy model generates text given input
  2. Critic model evaluates text and generates intrinsic rewards
  3. Environment provides extrinsic reward for complete output
  4. Rewards are combined and used to update policy model

- Design tradeoffs:
  - Using a stronger critic than policy enables better learning but increases computational cost
  - Self-critique setting reduces resources needed but may be less effective
  - Token-level vs span-level rewards affects granularity of feedback
  - Weighting between intrinsic and extrinsic rewards (α1 and α2) requires tuning

- Failure signatures:
  - Poor performance on extrinsic reward despite training: Intrinsic rewards may be misaligned
  - High variance in learning: Critic may be providing inconsistent feedback
  - No improvement over baseline: Either critic is too weak or prompt engineering needs refinement

- First 3 experiments:
  1. Implement basic PPO with only extrinsic rewards as baseline
  2. Add simple heuristic intrinsic rewards (e.g., reward for each token generated) to verify reward combination mechanism
  3. Implement full RELC with LLM-generated intrinsic rewards using a simple task like sentiment control

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but several important directions are implied throughout the discussion, particularly regarding scaling the method to larger models and maintaining critic alignment as the policy improves.

## Limitations
- The computational overhead of running a large critic model during training may limit practical deployment, particularly when the critic is substantially stronger than the policy model
- The long-term generalization of models trained with intrinsic rewards is not explored - it's unclear whether performance gains persist when evaluating on truly unseen data distributions
- The exact prompt engineering methodology for the critic model remains underspecified, making it difficult to reproduce or adapt the results

## Confidence
- **High Confidence**: The core mechanism of using LLM-generated intrinsic rewards to address sparse reward problems in RL-based text generation is well-supported by the experimental results
- **Medium Confidence**: The generalizability of RELC across different task types (sentiment control, detoxification, summarization) suggests the approach has broader applicability
- **Medium Confidence**: The claim about improved sample efficiency is supported by the experiments, but the magnitude of improvement may depend heavily on task-specific factors and critic model capabilities

## Next Checks
1. **Reward Alignment Analysis**: Conduct a systematic analysis comparing the critic's intrinsic rewards with the extrinsic rewards across different generation steps. This would validate whether the intrinsic rewards are truly aligned with the task objectives and help identify any systematic biases in the critic's evaluations.

2. **Resource Efficiency Benchmark**: Measure and report the wall-clock training time and computational overhead (FLOPs, memory usage) when using a strong critic model versus a weaker one or the self-critique setting. This would quantify the practical deployment costs and help determine optimal resource allocation strategies.

3. **Long-term Generalization Test**: Evaluate RELC-trained models after a period of no training (e.g., 1-2 weeks) to assess whether the intrinsic rewards lead to more robust learning that persists over time. Compare this with baseline models trained only on extrinsic rewards to determine if there are differences in knowledge retention or catastrophic forgetting.