---
ver: rpa2
title: 'CJST: CTC Compressor based Joint Speech and Text Training for Decoder-Only
  ASR'
arxiv_id: '2411.07607'
source_url: https://arxiv.org/abs/2411.07607
tags:
- speech
- text
- compressor
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a joint speech and text training (CJST) framework
  for decoder-only ASR models, leveraging the CTC compressor to align speech and text
  modalities. The approach utilizes sequence compression, forced peaky alignment,
  and CTC class embeddings to inject text information effectively without duration
  handling.
---

# CJST: CTC Compressor based Joint Speech and Text Training for Decoder-Only ASR

## Quick Facts
- **arXiv ID**: 2411.07607
- **Source URL**: https://arxiv.org/abs/2411.07607
- **Reference count**: 32
- **Primary result**: CTC Compressor based Joint Speech and Text Training (CJST) achieves consistent improvements in both in-domain and cross-domain ASR settings without requiring external language models.

## Executive Summary
This paper introduces a joint speech and text training (CJST) framework for decoder-only ASR models, leveraging the CTC compressor to align speech and text modalities. The approach utilizes sequence compression, forced peaky alignment, and CTC class embeddings to inject text information effectively without duration handling. Experiments on Librispeech and TED-LIUM2 datasets demonstrate that CJST achieves consistent improvements in both in-domain and cross-domain scenarios. Comprehensive evaluation of CTC compressor modes reveals that blank probability removal with a high threshold provides the most robust performance across clean and noisy data conditions. The proposed method leads to relative improvements of approximately 6% in cross-domain evaluation and further enhances in-domain performance, achieving the best results without requiring external language models.

## Method Summary
The CJST framework integrates speech and text modalities through a CTC compressor that aligns acoustic and linguistic sequences. The method employs sequence compression to reduce computational overhead, forced peaky alignment to ensure sharp phoneme boundaries, and CTC class embeddings to inject text information directly into the decoder. By avoiding explicit duration modeling, the approach simplifies training while maintaining strong alignment between modalities. The CTC compressor's blank probability removal with a high threshold emerges as the optimal mode for robust performance across varying acoustic conditions.

## Key Results
- CJST achieves consistent improvements in both in-domain and cross-domain ASR settings without requiring external language models.
- Relative improvements of approximately 6% are observed in cross-domain evaluation.
- Blank probability removal with a high threshold in the CTC compressor provides the most robust performance across clean and noisy data conditions.

## Why This Works (Mechanism)
The effectiveness of CJST stems from its ability to align speech and text modalities without explicit duration modeling. By leveraging sequence compression and forced peaky alignment, the framework reduces computational overhead while maintaining sharp phoneme boundaries. The CTC class embeddings inject linguistic information directly into the decoder, enhancing its ability to predict accurate transcriptions. The blank probability removal with a high threshold in the CTC compressor ensures robust performance by filtering out ambiguous alignments, particularly in noisy conditions.

## Foundational Learning
- **CTC Compressor**: Why needed - Aligns speech and text sequences efficiently. Quick check - Verify alignment quality using WER metrics.
- **Sequence Compression**: Why needed - Reduces computational overhead. Quick check - Compare training time and memory usage with and without compression.
- **Forced Peaky Alignment**: Why needed - Ensures sharp phoneme boundaries. Quick check - Analyze alignment sharpness using visualization tools.
- **CTC Class Embeddings**: Why needed - Injects linguistic information into the decoder. Quick check - Evaluate decoder performance with and without embeddings.

## Architecture Onboarding
- **Component Map**: Speech input -> CTC Compressor -> Sequence Compression -> Forced Peaky Alignment -> CTC Class Embeddings -> Decoder -> Text output
- **Critical Path**: Speech input -> CTC Compressor -> Decoder -> Text output
- **Design Tradeoffs**: Avoids explicit duration modeling for simplicity but may limit effectiveness for languages with complex temporal dependencies.
- **Failure Signatures**: Poor performance in noisy conditions if blank probability threshold is set too low; alignment issues if sequence compression is too aggressive.
- **First Experiments**: 1) Test CTC compressor modes on clean and noisy data. 2) Evaluate sequence compression impact on training efficiency. 3) Analyze decoder performance with and without CTC class embeddings.

## Open Questions the Paper Calls Out
None

## Limitations
- The approach relies heavily on the CTC compressor's performance, which is dependent on specific hyperparameters like the blank probability threshold.
- The absence of explicit duration modeling could limit effectiveness for languages with complex temporal dependencies.
- Claims about out-of-domain robustness are based on limited cross-domain evaluation and may not generalize to broader acoustic or linguistic variations.

## Confidence
- **High**: Overall improvement trends on standard benchmarks (Librispeech and TED-LIUM2).
- **Medium**: Generalizability of CTC compressor's mode selection across datasets and model sizes.
- **Low**: Claims about out-of-domain robustness beyond tested cross-domain setup.

## Next Checks
1. Test the CJST framework with varying model sizes and architectures (e.g., non-Transformer decoders) to assess scalability and architectural dependence.
2. Evaluate the approach on multilingual and low-resource datasets to determine cross-linguistic applicability, especially given the lack of duration modeling.
3. Conduct ablation studies isolating the contributions of each CTC compressor mode under controlled noise and channel variability to better understand robustness mechanisms.