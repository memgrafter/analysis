---
ver: rpa2
title: Planning and Editing What You Retrieve for Enhanced Tool Learning
arxiv_id: '2404.00450'
source_url: https://arxiv.org/abs/2404.00450
tags:
- tool
- tools
- user
- retrieval
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PLUTO, a framework that enhances tool retrieval
  for large language models (LLMs) by combining planning, learning, and understanding.
  The key idea is to decompose complex user queries into focused sub-queries (Plan-and-Retrieve)
  and enrich tool descriptions using user scenarios (Edit-and-Ground).
---

# Planning and Editing What You Retrieve for Enhanced Tool Learning

## Quick Facts
- **arXiv ID:** 2404.00450
- **Source URL:** https://arxiv.org/abs/2404.00450
- **Reference count:** 7
- **Primary result:** PLUTO framework achieves 7.3% absolute improvement in recall and 9.3% in NDCG over state-of-the-art baselines for tool retrieval

## Executive Summary
This paper introduces PLUTO, a framework that enhances tool retrieval for large language models by combining planning, learning, and understanding. The key idea is to decompose complex user queries into focused sub-queries and enrich tool descriptions using user scenarios. PLUTO significantly improves recall and NDCG in tool retrieval tasks compared to state-of-the-art models and achieves higher pass rates in downstream tool execution tasks.

## Method Summary
PLUTO coordinates two paradigms: Plan-and-Retrieve (P&R) and Edit-and-Ground (E&G). P&R uses an LLM to decompose complex queries into actionable sub-queries, then retrieves relevant tools using a neural retriever followed by LLM-based selection. E&G employs an iterative optimization process where tool descriptions are enriched based on user queries and retrieval performance. The framework operates in alternating phases where P&R handles immediate query processing while E&G proactively optimizes underperforming tool descriptions.

## Key Results
- Achieves 7.3% absolute improvement in recall and 9.3% in NDCG over state-of-the-art baselines
- Demonstrates 92% pass rate in tool execution tasks versus 86% for best baseline
- Shows consistent improvements across both Non-Finetuned and Finetuned settings on ToolBench dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing complex queries into focused sub-queries improves retrieval accuracy by enabling more precise matching between user intent and tool descriptions.
- Core assumption: Breaking down complex queries into simpler, focused components allows the retrieval system to better match tools to user needs by reducing ambiguity and semantic gap.
- Evidence anchors: [abstract] "The P&R paradigm consists of a neural retrieval module for shortlisting relevant tools and an LLM-based query planner that decomposes complex queries into actionable tasks"; [section] "Plan. In the Plan stage, a LLM-based planner autoregressively decomposes the user query Q into sub-queries q1, q2, ..., qn."
- Break condition: If the LLM cannot effectively decompose complex queries into meaningful sub-queries, the retrieval accuracy will not improve and may even degrade.

### Mechanism 2
- Claim: Enriching tool descriptions using user scenarios and LLM knowledge bridges the semantic gap between user queries and tool functionalities.
- Core assumption: User scenarios provide valuable context that can be used to enrich tool descriptions, making them more aligned with real-world applications and improving retrieval effectiveness.
- Evidence anchors: [abstract] "The E&G paradigm utilizes LLMs to enrich tool descriptions based on user scenarios"; [section] "Our second contribution is the proposal of Edit-and-Ground paradigm that utilizes user queries' rich contextual information and LLM's extensive world knowledge for enriching descriptions of tool functionalities."
- Break condition: If the LLM fails to accurately understand tool functionalities or user scenarios, the enriched descriptions may not improve retrieval and could introduce misleading information.

### Mechanism 3
- Claim: Combining planning, retrieval, and description enrichment creates a feedback loop that continuously improves tool selection effectiveness.
- Core assumption: The two paradigms work synergistically - P&R benefits from better tool descriptions created by E&G, while E&G is guided by the retrieval performance data from P&R.
- Evidence anchors: [section] "Our PLUTO framework employs strategic coordination of the Plan-and-Retrieve (P&R) and Edit-and-Ground (E&G) paradigms, phased to optimize the process of tool retrieval"; [section] "During the optimization phase, P&R and E&G operate alternatively."
- Break condition: If the coordination between paradigms is not properly implemented, the feedback loop may not function effectively, limiting overall system performance.

## Foundational Learning

- **Concept: Dense retrieval using neural embeddings**
  - Why needed here: PLUTO uses DPR and Contriever as backbone retrievers, which rely on dense vector representations for matching queries to tool descriptions
  - Quick check question: How do dense retrievers differ from sparse retrievers like BM25 in terms of matching query-tool pairs?

- **Concept: Query decomposition and planning strategies**
  - Why needed here: The P&R paradigm relies on autoregressive query decomposition to break down complex user queries into focused sub-queries
  - Quick check question: What is the purpose of using K-means clustering to select sub-queries during the planning stage?

- **Concept: Tool description optimization using LLMs**
  - Why needed here: The E&G paradigm uses LLMs to enrich tool descriptions based on user scenarios, requiring understanding of both tool functionality and contextual use cases
  - Quick check question: How does filtering out specific entities from user queries help prevent overfitting during the description optimization process?

## Architecture Onboarding

- **Component map:** Query Input → LLM Planner (decomposition) → Neural Retriever (candidate retrieval) → LLM Predictor (final selection) → Tool Set Output
- **Critical path:** Query Input → LLM Planner → Neural Retriever → LLM Predictor → Tool Set Output
- **Design tradeoffs:** Using LLM for planning adds computational cost but improves query decomposition quality; Entity filtering during description optimization balances specificity with generalization; Multiple optimization rounds improve quality but increase training time
- **Failure signatures:** Poor query decomposition leading to retrieval results that are too broad or miss relevant tools; Ineffective description optimization leaving tools underrepresented; Coordination breakdown between P&R and E&G paradigms
- **First 3 experiments:** 1) Test query decomposition with simple vs. complex queries to verify planner effectiveness; 2) Evaluate description optimization by comparing retrieval performance before and after E&G processing; 3) Measure coordination effectiveness by running P&R and E&G in alternating vs. parallel modes

## Open Questions the Paper Calls Out
Based on the paper, here are some open research questions:

### Open Question 1
- Question: How can the PLUTO framework be extended to handle tool retrieval in multilingual settings?
- Basis in paper: The paper mentions that the current study is limited to English language datasets
- Why unresolved: The paper does not explore the challenges and potential solutions for adapting PLUTO to multilingual tool retrieval scenarios
- What evidence would resolve it: Experiments demonstrating the effectiveness of PLUTO on multilingual tool retrieval datasets, along with an analysis of the unique challenges and solutions for handling different languages

### Open Question 2
- Question: Can the Edit-and-Ground paradigm be applied to optimize tool descriptions in real-time, as new tools are introduced or existing ones are updated?
- Basis in paper: The paper mentions that the Edit-and-Ground paradigm is executed in multiple rounds, but it does not discuss the possibility of real-time optimization
- Why unresolved: The paper does not explore the feasibility and benefits of applying the Edit-and-Ground paradigm to continuously update tool descriptions as the tool ecosystem evolves
- What evidence would resolve it: Experiments showing the performance gains of real-time optimization of tool descriptions using the Edit-and-Ground paradigm, along with a discussion of the challenges and potential solutions for implementing such a system

### Open Question 3
- Question: How can the PLUTO framework be adapted to handle tool retrieval in domains with highly specialized or domain-specific tools?
- Basis in paper: The paper mentions that the ToolBench dataset covers a wide range of tool categories, but it does not explore the challenges and potential solutions for handling highly specialized domains
- Why unresolved: The paper does not discuss the specific considerations and modifications required to adapt PLUTO for tool retrieval in domains with highly specialized or domain-specific tools
- What evidence would resolve it: Experiments demonstrating the effectiveness of PLUTO on tool retrieval tasks in specialized domains, along with an analysis of the unique challenges and solutions for handling domain-specific tools

## Limitations
- Computational overhead from LLM-based query planning and tool prediction may not scale efficiently to production environments
- Reliance on powerful LLM APIs (ChatGPT) may limit deployment in scenarios without API access
- Entity filtering strategy in description optimization may remove contextually important information

## Confidence
- **Mechanism 1:** Medium - Clear theoretical benefits but relies heavily on automated metrics rather than extensive human evaluation
- **Mechanism 2:** Medium - Supported by retrieval metric improvements but long-term stability of description optimizations remains unclear
- **Mechanism 3:** Medium - The synergistic coordination is theoretically sound but practical implementation challenges may limit effectiveness

## Next Checks
1. **Query Decomposition Quality Analysis:** Conduct human evaluation studies to assess whether LLM-generated sub-queries accurately capture user intent and improve over using original complex queries directly

2. **Description Optimization Stability:** Implement long-term tracking of tool descriptions after E&G optimization to measure stability over time and test performance across different query distributions

3. **Scalability Benchmarking:** Measure end-to-end latency and computational costs of the complete PLUTO pipeline under varying query loads to quantify practical trade-offs between improved accuracy and increased resource requirements