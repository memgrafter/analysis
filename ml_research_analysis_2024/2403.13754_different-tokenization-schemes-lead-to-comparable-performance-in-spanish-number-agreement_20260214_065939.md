---
ver: rpa2
title: Different Tokenization Schemes Lead to Comparable Performance in Spanish Number
  Agreement
arxiv_id: '2403.13754'
source_url: https://arxiv.org/abs/2403.13754
tags:
- tokenization
- plural
- nouns
- language
- agreement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the impact of different tokenization schemes
  on Spanish number agreement in language models. The authors examine single-token,
  morphemically-tokenized, and non-morphemically-tokenized plural nouns using a masked
  article prediction task.
---

# Different Tokenization Schemes Lead to Comparable Performance in Spanish Number Agreement

## Quick Facts
- arXiv ID: 2403.13754
- Source URL: https://arxiv.org/abs/2403.13754
- Reference count: 7
- Primary result: All tokenization schemes (single-token, morphemically-tokenized, non-morphemically-tokenized) achieve near-ceiling accuracy for Spanish number agreement, with artificial morphemic tokenization still allowing successful performance.

## Executive Summary
This study investigates how different tokenization schemes affect Spanish number agreement in language models. Using a masked article prediction task with the BETO Spanish BERT model, the authors examine single-token, morphemically-tokenized, and non-morphemically-tokenized plural nouns. They find that all tokenization schemes perform similarly, achieving near-ceiling accuracy for article agreement. Notably, artificially inducing morphemic tokenization for words not originally tokenized this way still allows successful task performance, though slightly less accurately than original tokenization. Exploratory analysis shows that different plural tokenizations have similar distributions along the embedding axis that maximally distinguishes singular and plural nouns, suggesting morphologically-aligned tokenization is viable but not strictly required for good performance.

## Method Summary
The study uses the BETO Spanish BERT model (110M parameters, 12 layers) trained on approximately 3B words. Spanish plural nouns and their singular form lemmas were extracted from the AnCora Treebanks and categorized into one-token, multi-token morphemic, and multi-token non-morphemic lists. The masked article prediction task assessed agreement by taking the logarithm of relative probability of plural vs. singular articles. Artificial morphemic tokenization was created by concatenating affix tokens to singular noun tokens. Linear mixed-effects models predicted Log Odds with various fixed effects and random intercepts, while Linear Discriminant Analysis identified axes separating singular and plural nouns and different plural tokenization types.

## Key Results
- All tokenization schemes achieve near-ceiling accuracy for Spanish number agreement in the masked article prediction task
- Artificially induced morphemic tokenization leads to successful task performance, though slightly less accurate than original tokenization
- Different plural tokenizations have similar distributions along the embedding axis that maximally distinguishes singular and plural nouns
- Word frequency strongly correlates with tokenization scheme assignment during model training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Morphologically-aligned tokenization is not strictly required for Spanish number agreement performance, as models can generalize morphological patterns to unseen token sequences.
- Mechanism: The model learns abstract morphological representations that are transferable across different tokenization schemes. When presented with artificially induced morphemic tokenization, the model applies learned patterns to maintain high agreement accuracy.
- Core assumption: The model's internal representations capture morphological regularities that transcend specific tokenization boundaries.
- Evidence anchors:
  - [abstract]: "Artificially inducing morphemic tokenization for words not originally tokenized this way still allows successful task performance"
  - [section]: "Artificial tokenization schemes, where we coerce an initially single-token or non-morphemically-tokenized plural into a morphemic representation, leads to successful task performance"
  - [corpus]: Weak - no direct corpus evidence of morphological generalization mechanisms
- Break condition: If the model cannot generalize morphological patterns beyond its training tokenization, artificially induced morphemic tokenization would fail to maintain high accuracy.

### Mechanism 2
- Claim: Frequency of wordforms influences tokenization scheme selection, which in turn affects agreement performance.
- Mechanism: More frequent words are more likely to be assigned single-token representations during tokenizer training, while less frequent words are decomposed into subword units. This frequency-based tokenization affects the model's ability to predict correct articles.
- Core assumption: Tokenizer training prioritizes frequent words for single-token assignment, creating a frequency-tokenization correlation that impacts downstream performance.
- Evidence anchors:
  - [section]: "Using oral frequency measures... we examined the relationship between a wordform's frequency and how it was tokenized. A linear model predicting Log Frequency from Tokenization Scheme explained significant variance"
  - [section]: "the frequency of a wordform was likely a major factor in how it was tokenized"
  - [corpus]: Weak - frequency analysis is based on a specific corpus but no broader corpus evidence provided
- Break condition: If frequency effects are not present in the training data or if the tokenizer uses different criteria for token assignment, the correlation between frequency and tokenization scheme would break down.

### Mechanism 3
- Claim: Language model embeddings for different plural tokenizations have similar distributions along the embedding space axis that maximally distinguishes singular and plural nouns.
- Mechanism: The model learns a common embedding space where singular and plural forms are linearly separable, regardless of tokenization scheme. This shared axis of separation allows the model to apply the same agreement mechanism across different tokenizations.
- Core assumption: The embedding space contains axes that capture morphological number distinctions independent of tokenization boundaries.
- Evidence anchors:
  - [section]: "We find axes with high overlap between all plural forms (regardless of tokenization scheme) and high discriminability between plural and singular forms"
  - [section]: "all types of plurals... patterned together and were not linearly discriminable along this axis"
  - [corpus]: Weak - no corpus evidence of embedding space structure
- Break condition: If the embedding space does not contain shared axes for morphological distinctions, different tokenization schemes would pattern differently and agreement performance would vary significantly.

## Foundational Learning

- Concept: Linear Discriminant Analysis (LDA)
  - Why needed here: LDA is used to identify axes in the embedding space that maximally separate singular and plural forms, revealing how the model represents morphological distinctions across different tokenizations.
  - Quick check question: What does LDA compute when given n sets of representations? (Answer: n-1 directions that maximize separation between the sets)

- Concept: Masked Language Model Prediction
  - Why needed here: The study uses masked article prediction to assess number agreement, where the model predicts masked articles based on noun representations learned through different tokenization schemes.
  - Quick check question: How does the log-odds calculation work for article prediction in this study? (Answer: Log-odds = log(P(plural article)/P(singular article)), where positive values indicate higher probability of plural article)

- Concept: Subword Tokenization and Morphological Boundaries
  - Why needed here: Understanding how different tokenization schemes (single-token, morphemic, non-morphemic) affect the model's ability to learn and apply morphological patterns is central to the study's investigation.
  - Quick check question: What distinguishes morphemically-tokenized plurals from non-morphemically-tokenized plurals in this study? (Answer: Morphemically-tokenized follow morpheme boundaries like 'naranja'+'##s', while non-morphemically-tokenized do not like 'neuro'+'##nas')

## Architecture Onboarding

- Component map:
  Tokenizer -> BETO BERT model (110M parameters, 12 layers) -> Embedding Space (768-dimensions) -> Masked Article Prediction -> Linear Mixed-Effects Models & LDA Analysis

- Critical path:
  1. Extract plural nouns and categorize by tokenization scheme
  2. Create artificial morphemic tokenizations for non-morphemic and single-token words
  3. Run masked article prediction task for all tokenization variants
  4. Analyze agreement performance using linear mixed-effects models
  5. Compare embeddings using LDA to identify shared representation axes

- Design tradeoffs:
  - Single-token vs. subword tokenization: Single-token offers semantic precision but requires more computational resources and may not generalize to unseen words
  - Original vs. artificial tokenization: Original tokenization performs slightly better but artificial tokenization still works, suggesting model generalization
  - Layer selection for embeddings: Using last 4 layers balances task-relevant information with computational efficiency

- Failure signatures:
  - Agreement accuracy drops significantly for artificially-tokenized words
  - LDA shows no shared axes between different tokenization schemes
  - Mixed-effects models show strong interactions between tokenization scheme and word number
  - Frequency effects disappear when controlling for tokenization scheme

- First 3 experiments:
  1. Replicate the masked article prediction task with a different Spanish BERT model to test generalizability across architectures
  2. Test agreement performance on a different morphological phenomenon (e.g., gender agreement) to see if findings extend beyond number
  3. Apply the artificial tokenization procedure to a morphologically-rich language like Turkish to test cross-linguistic applicability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do different tokenization schemes rely on distinct neural mechanisms for number agreement, or do they utilize the same underlying processing pathways?
- Basis in paper: [inferred] The paper notes that similar agreement performance across tokenization schemes "could indicate multiple different agreement mechanisms in the model" and suggests future work applying causal interventions on different embedding axes.
- Why unresolved: The study only analyzed correlations between tokenization schemes and agreement performance; it did not conduct causal interventions to determine whether the same neural sub-networks are involved in number agreement across different tokenization types.
- What evidence would resolve it: Causal intervention experiments (e.g., as in Mueller et al., 2022) that manipulate specific embedding axes identified through LDA could reveal whether the same neural sub-networks are responsible for number agreement across different tokenization schemes.

### Open Question 2
- Question: How generalizable are the findings on Spanish number agreement to other morphosyntactic phenomena and languages with different morphological structures?
- Basis in paper: [explicit] The authors explicitly state "A key limitation of the current work is scope" and suggest considering "additional morphosyntactic phenomena, additional languages, and a larger range of language models or tokenization schemes."
- Why unresolved: The study only examined one specific morphosyntactic phenomenon (number agreement in Spanish plurals) using one language model (BETO). The effects of tokenization on other morphosyntactic rules and in languages with different morphological structures remain unknown.
- What evidence would resolve it: Replicating the study design with other morphosyntactic phenomena (e.g., case agreement, gender agreement) and in languages with different morphological typologies (e.g., agglutinative languages like Turkish or fusional languages like Russian) would reveal the generalizability of the findings.

### Open Question 3
- Question: Does morphological tokenization consistently improve performance across all frequency ranges of words, or is the effect frequency-dependent?
- Basis in paper: [inferred] The authors found that word frequency was correlated with tokenization scheme and that "the language model made better predictions for more frequent nouns than less frequent nouns." However, they did not directly test whether morphological tokenization improves performance across all frequency ranges.
- Why unresolved: The study only examined the overall effect of tokenization scheme on agreement performance, without considering whether this effect varies across different frequency ranges of words.
- What evidence would resolve it: Analyzing the effect of tokenization scheme on agreement performance separately for high-frequency, medium-frequency, and low-frequency words would reveal whether morphological tokenization provides consistent benefits across all frequency ranges or if the effect is frequency-dependent.

## Limitations

- Frequency effects may confound tokenization scheme effects, as word frequency strongly correlates with tokenization assignment during BERT training
- Artificial tokenization procedure may not perfectly mimic original tokenizer learning dynamics, potentially underestimating benefits of morphologically-aligned tokenization
- Study focuses solely on number agreement for plural nouns in Spanish, limiting generalizability to other morphological phenomena and languages

## Confidence

**High Confidence**: The claim that all tokenization schemes achieve near-ceiling accuracy for Spanish number agreement is well-supported by experimental results and statistical analyses.

**Medium Confidence**: The assertion that morphologically-aligned tokenization is viable but not strictly required is supported by the data but requires additional validation across different languages and phenomena.

**Low Confidence**: The broader implications about morphological generalization across tokenization schemes and languages are speculative and require further research to validate.

## Next Checks

1. **Cross-linguistic validation**: Apply the artificial tokenization procedure to morphologically-rich languages with different morphological systems (e.g., Turkish with its agglutinative morphology or Finnish with its extensive case system) to test whether the observed generalization patterns hold across language families.

2. **Multi-phenomenon testing**: Extend the agreement task beyond number to include gender agreement, case marking, and verbal agreement to determine whether the tokenization independence generalizes to other morphological dependencies and whether certain phenomena show stronger tokenization effects than others.

3. **Temporal analysis of tokenization effects**: Train Spanish BERT models with controlled tokenization schemes (all morpheme-aligned vs. all non-morpheme-aligned) from scratch to establish whether the observed pattern reflects the training procedure or is an emergent property of language model learning, controlling for frequency effects through matched corpora.