---
ver: rpa2
title: 'FaithBench: A Diverse Hallucination Benchmark for Summarization by Modern
  LLMs'
arxiv_id: '2410.13210'
source_url: https://arxiv.org/abs/2410.13210
tags:
- hallucination
- llms
- hallucinations
- passage
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FaithBench, a benchmark for evaluating hallucinations
  in LLM-generated summaries. FaithBench focuses on challenging hallucination samples
  where popular detection models disagree, annotated by human experts.
---

# FaithBench: A Diverse Hallucination Benchmark for Summarization by Modern LLMs

## Quick Facts
- arXiv ID: 2410.13210
- Source URL: https://arxiv.org/abs/2410.13210
- Reference count: 7
- This paper introduces FaithBench, a benchmark for evaluating hallucinations in LLM-generated summaries, showing that even the best detection models achieve only around 50% accuracy on challenging samples.

## Executive Summary
FaithBench is a new benchmark designed to evaluate hallucinations in LLM-generated summaries by focusing on challenging samples where popular detection models disagree. The benchmark uses human expert annotations at the span level with a nuanced taxonomy distinguishing unwanted, questionable, and benign hallucinations. Testing 10 modern LLMs from 8 different families, FaithBench reveals that current hallucination detection models struggle with difficult cases, achieving only around 50% accuracy, while GPT-4o and GPT-3.5-Turbo produce the least hallucinations among evaluated models.

## Method Summary
The FaithBench benchmark is constructed by selecting samples from Vectara's Hallucination Leaderboard where multiple state-of-the-art hallucination detection models disagree on their outputs. Human experts then provide span-level annotations using a three-tier taxonomy (unwanted, questionable, benign) through a semantic cross-checking tool that links summary spans to passage spans. The benchmark evaluates 10 LLMs from 8 different families, calculating inter-annotator agreement using Krippendorff's alpha and benchmarking detection models against the human-annotated ground truth.

## Key Results
- All tested hallucination detection models achieve only around 50% accuracy on FaithBench, confirming its challenging nature
- GPT-4o and GPT-3.5-Turbo produce the least hallucinations, with unwanted hallucination rates of 5.8% and 6.5% respectively
- Inter-annotator agreement is high (0.748) for unwanted hallucinations but drops significantly when including questionable and benign categories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FaithBench focuses on challenging hallucination samples where detection models disagree, providing a more rigorous evaluation.
- Mechanism: By filtering for samples where multiple state-of-the-art hallucination detection models produce conflicting results, FaithBench ensures that only the most difficult cases are included. This approach leverages the disagreement among detectors as a proxy for sample difficulty.
- Core assumption: Popular hallucination detection models are reasonably accurate and their disagreements indicate genuine ambiguity in the text.
- Evidence anchors: [abstract] "FaithBench focuses on challenging hallucination samples where popular detection models disagree"; [section 2.3] "To make the best use of our annotators' time, we focus on labeling challenging samples where hallucination detectors disagree the most"

### Mechanism 2
- Claim: Human expert annotation with span-level granularity and a nuanced taxonomy improves hallucination evaluation accuracy.
- Mechanism: Expert annotators with published work in hallucination detection provide fine-grained span-level labels using a three-tier taxonomy (unwanted, questionable, benign) rather than simple binary classification. This captures the subjectivity and nuance in hallucination perception.
- Core assumption: Human experts can reliably distinguish between hallucination types and provide consistent span-level annotations when using clear guidelines and discussion protocols.
- Evidence anchors: [abstract] "ground truth annotations by human experts" and "span level using a nuanced taxonomy"; [section 3.1] "IAA for the 'consistent' and 'unwanted' classes is 0.748"

### Mechanism 3
- Claim: Testing across diverse LLM families and sizes provides more comprehensive hallucination evaluation.
- Mechanism: By including 10 LLMs from 8 different families and selecting different model sizes within families, FaithBench captures variation in hallucination patterns that might be missed by testing only popular or large models.
- Core assumption: Different LLM architectures, training approaches, and model sizes produce meaningfully different hallucination patterns that warrant separate evaluation.
- Evidence anchors: [abstract] "10 modern LLMs from 8 different families"; [section 2.3] "we restrict the benchmark to eight of the most anecdotally popular LLM families"

## Foundational Learning

- Concept: Inter-annotator agreement (IAA) metrics like Krippendorff's alpha
  - Why needed here: To measure the reliability and consistency of human annotations, particularly important given the subjective nature of hallucination labeling
  - Quick check question: If two annotators label 100 samples and agree on 70, what is their observed agreement percentage?

- Concept: Hallucination detection as a classification problem
  - Why needed here: Understanding how hallucination detection models work is crucial for interpreting why FaithBench filters samples based on detector disagreement
  - Quick check question: What is the difference between precision and recall in the context of hallucination detection?

- Concept: Taxonomy design and label granularity
  - Why needed here: The nuanced three-tier taxonomy (unwanted, questionable, benign) is central to FaithBench's approach, requiring understanding of how label granularity affects evaluation quality
  - Quick check question: What are the trade-offs between using binary vs. multi-class labeling for hallucination detection?

## Architecture Onboarding

- Component map: Vectara Hallucination Leaderboard → Sample filtering → Human annotation → Ground truth database
- Critical path: 1. Sample selection based on detector disagreement; 2. Expert annotation with span-level granularity; 3. IAA calculation and annotation quality verification; 4. Model ranking and detection model benchmarking
- Design tradeoffs: Sample diversity vs. annotation effort; Expert annotation quality vs. scalability; Granular taxonomy vs. annotation consistency
- Failure signatures: Low IAA values indicating annotation inconsistency; Detector disagreement not correlating with actual difficulty; Systematic bias in sample selection or annotation
- First 3 experiments: 1. Compare IAA values when using different taxonomy granularities; 2. Test whether detector disagreement correlates with human expert difficulty ratings; 3. Evaluate how sample difficulty affects LLM ranking stability across different subsets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different hallucination types (benign, questionable, unwanted) affect downstream task performance in RAG applications?
- Basis in paper: [explicit] The paper introduces a taxonomy distinguishing between benign, questionable, and unwanted hallucinations, noting that benign hallucinations may be acceptable or even welcomed by users.
- Why unresolved: The paper evaluates hallucination types but does not measure their impact on actual RAG system performance or user satisfaction.
- What evidence would resolve it: Experiments measuring retrieval accuracy, answer relevance, and user satisfaction scores when summaries contain different hallucination types.

### Open Question 2
- Question: How does hallucination frequency vary across different domains or topic areas?
- Basis in paper: [inferred] The paper notes that sports-related samples were challenging due to annotators' unfamiliarity with European sports terminology, suggesting domain-specific knowledge requirements.
- Why unresolved: The paper does not systematically analyze hallucination patterns across different domains or content types.
- What evidence would resolve it: Comparative analysis of hallucination rates and types across multiple domains with domain-specific annotators.

### Open Question 3
- Question: What is the relationship between model size and hallucination frequency within the same model family?
- Basis in paper: [explicit] The paper intentionally selected different-sized models but could not study this systematically due to annotation constraints.
- Why unresolved: Limited coverage of multiple sizes within each model family prevents drawing conclusions about scaling effects.
- What evidence would resolve it: Systematic evaluation of multiple model sizes from each family with consistent annotation methodology.

## Limitations
- Heavy reliance on human expert annotation introduces subjectivity and potential bias, with IAA dropping significantly for nuanced categories
- Sample selection based on detector disagreement may introduce selection bias and not represent real-world hallucination distributions
- Limited coverage of model sizes within families prevents systematic study of scaling effects on hallucination patterns

## Confidence

**Confidence: Low** - The core limitation is the heavy reliance on human expert annotation for ground truth, which introduces subjectivity and potential bias. The IAA scores of 0.748 for consistent/unwanted labels drop significantly when including questionable and benign categories.

**Confidence: Medium** - The benchmark's focus on challenging samples may not fully represent the distribution of hallucination types in real-world summarization tasks. The filtering process could introduce selection bias.

**Confidence: High** - The finding that even the best detection models achieve only ~50% accuracy on FaithBench is robust and well-supported. The benchmark methodology is clearly specified and reproducible.

## Next Checks

1. **IAA Stability Analysis**: Replicate the annotation process with a larger pool of annotators (at least 10) to determine whether the current IAA scores are representative or if they improve with more diverse perspectives.

2. **Detector Disagreement Correlation**: Systematically test whether samples selected based on detector disagreement actually correlate with human-perceived difficulty by having independent annotators rate difficulty without knowing the detector disagreement status.

3. **Cross-Domain Generalization**: Evaluate whether FaithBench's findings about hallucination patterns generalize to domains beyond the original passages by testing the same LLMs on different content types and measuring whether ranking patterns hold.