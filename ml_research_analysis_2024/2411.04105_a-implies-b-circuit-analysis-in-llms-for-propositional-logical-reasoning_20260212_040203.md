---
ver: rpa2
title: 'A Implies B: Circuit Analysis in LLMs for Propositional Logical Reasoning'
arxiv_id: '2411.04105'
source_url: https://arxiv.org/abs/2411.04105
tags:
- attention
- heads
- query
- answer
- chain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a circuit analysis of three large language models
  (Mistral-7B, Gemma-2-9B, and Gemma-2-27B) to understand how they solve propositional
  logic reasoning problems. The authors study a minimal propositional logic problem
  where models must infer the truth value of a variable given rules and facts.
---

# A Implies B: Circuit Analysis in LLMs for Propositional Logical Reasoning

## Quick Facts
- arXiv ID: 2411.04105
- Source URL: https://arxiv.org/abs/2411.04105
- Reference count: 40
- Models studied: Mistral-7B, Gemma-2-9B, Gemma-2-27B for propositional logic reasoning

## Executive Summary
This paper presents a circuit analysis of three large language models to understand how they solve propositional logic reasoning problems. Using causal mediation analysis, the authors identify four distinct families of attention heads that form specialized reasoning circuits: queried-rule locating heads, queried-rule mover heads, fact-processing heads, and decision heads. The study reveals that models perform "lazy reasoning" - core circuits activate primarily after seeing the Question token rather than pre-processing all input facts upfront. The analysis achieves accuracy rates of 70-86% on logic problems across different model sizes.

## Method Summary
The authors use causal mediation analysis with activation patching experiments to identify reasoning circuits in pre-trained LLMs. They swap QUERY tokens, rule locations, and fact values to measure indirect effects of attention head components. The analysis focuses on QUERY-based experiments to uncover the sequential reasoning pathway "QUERY→Relevant Rule→Relevant Facts→Decision." They verify circuit sufficiency by replacing the complement of identified circuits with counterfactual signals and checking output stability. The study examines attention patterns of each head family to confirm their specialized roles in the reasoning process.

## Key Results
- All three models employ four distinct families of attention heads with specialized roles for propositional logic reasoning
- Models perform "lazy reasoning" - core circuits activate primarily after seeing the Question token rather than pre-processing all input facts upfront
- The same circuit components are reused when the model needs to invoke rules later in constructing its proof
- Analysis achieves accuracy rates of 70-86% on logic problems across different model sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model uses four distinct families of attention heads with specialized roles for propositional logic reasoning
- Mechanism: Attention heads form specialized circuits: queried-rule locating heads identify which rule is being queried, queried-rule mover heads transport relevant rule information, fact-processing heads extract relevant facts, and decision heads determine the final answer
- Core assumption: The reasoning process can be decomposed into sequential modular steps rather than distributed computation
- Evidence anchors:
  - [abstract] "all three models employ four distinct families of attention heads with specialized roles: queried-rule locating heads, queried-rule mover heads, fact-processing heads, and decision heads"
  - [section] "the model must execute the causal chain 'QUERY→Relevant Rule→Relevant fact(s)→Decision'"
  - [corpus] Weak - no direct corpus evidence found for this specific modular decomposition
- Break condition: If attention patterns show distributed processing across all parameters rather than localized circuits, or if different models use completely different reasoning strategies

### Mechanism 2
- Claim: Models perform "lazy reasoning" - core circuits activate primarily after seeing the Question token
- Mechanism: The model doesn't pre-process all input facts upfront. Instead, it waits until the Question token appears, then activates specialized attention heads to locate the relevant rule and process facts
- Core assumption: The QUERY token serves as a trigger for the reasoning process, activating dormant circuits
- Evidence anchors:
  - [abstract] "core circuits activate primarily after seeing the Question token rather than pre-processing all input facts upfront"
  - [section] "it primarily activates after seeing the Question and the ultimate query variable"
  - [corpus] Weak - corpus doesn't directly support this specific timing behavior
- Break condition: If pre-processing activity is observed before the Question token, or if the model uses different activation patterns for different problem types

### Mechanism 3
- Claim: Same circuit components are reused when the model needs to invoke rules later in constructing its proof
- Mechanism: The queried-rule locating heads and decision heads that identify the initial rule and answer also serve as first and last reasoning steps when invoking rules later in the proof construction
- Core assumption: Circuit components are modular and reusable across different stages of reasoning
- Evidence anchors:
  - [abstract] "the same circuit components are reused when the model needs to invoke rules later in constructing their proof"
  - [section] "when the model needs to invoke a relevant rule (from the input) later to construct their output proof, it uses the same heads (but other input positions) to retrieve the location of the rule"
  - [corpus] Weak - corpus doesn't provide evidence for this specific reuse pattern
- Break condition: If different stages of reasoning require completely different circuit components, or if component reuse breaks down for more complex problems

## Foundational Learning

- Concept: Causal mediation analysis
  - Why needed here: This is the primary method for identifying which model components are causally important for reasoning tasks
  - Quick check question: How does measuring indirect effects help identify important model components?

- Concept: Attention head functionality
  - Why needed here: Understanding how different attention heads process information is crucial for interpreting the reasoning circuit
  - Quick check question: What's the difference between query, key, and value activations in attention heads?

- Concept: Modular reasoning circuits
  - Why needed here: The paper's core finding is that reasoning happens through specialized, localized components rather than distributed processing
  - Quick check question: Why is modularity important for understanding how models solve reasoning problems?

## Architecture Onboarding

- Component map: QUERY token → Queried-rule locating heads → Queried-rule mover heads → Fact-processing heads → Decision heads → Output. The process is triggered by the QUERY token and proceeds sequentially through the specialized attention head families.

- Critical path: QUERY token → Queried-rule locating heads → Queried-rule mover heads → Fact-processing heads → Decision heads → Output. The process is triggered by the QUERY token and proceeds sequentially through the specialized attention head families.

- Design tradeoffs: Using specialized attention heads provides modularity and interpretability but may limit flexibility. The "lazy reasoning" approach saves computation but requires proper triggering mechanisms. Circuit reuse saves parameters but may create dependencies between reasoning stages.

- Failure signatures: If any attention head family is missing or malfunctioning, the reasoning chain breaks. Poor accuracy on logic problems suggests circuit component failure. If attention patterns show distributed processing rather than localization, the modular circuit hypothesis fails.

- First 3 experiments:
  1. Verify the four attention head families by performing QUERY-based intervention experiments and measuring logit differences
  2. Test circuit sufficiency by replacing the complement of the identified circuit with counterfactual signals and checking output stability
  3. Analyze attention patterns of each head family to confirm their specialized roles in the reasoning process

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the precise roles of MLP-0 in the reasoning circuit, given that it shows higher intervened logit differences compared to other MLPs?
- Basis in paper: [explicit] The paper notes that MLP-0 has higher intervened logit difference but treats it as a "nonlinear token embedding" rather than a complex processing unit
- Why unresolved: The analysis focuses on attention heads and doesn't fully characterize MLP-0's specific contribution to the reasoning process
- What evidence would resolve it: Detailed causal intervention experiments isolating MLP-0's effects, or ablation studies showing its impact on reasoning accuracy

### Open Question 2
- Question: How do the reasoning circuits identified in this study scale with problem complexity, such as longer logical chains or more variables?
- Basis in paper: [inferred] The authors note computational constraints limited their analysis of Gemma-2-27B and suggest verifying component importance for longer chains as a future direction
- Why unresolved: The study focused on a minimal problem with length-2 chains, and didn't systematically test how circuits adapt to more complex problems
- What evidence would resolve it: Testing the same models on progressively longer chains while tracking circuit component activation patterns and necessity

### Open Question 3
- Question: What mechanistic differences exist between the reasoning processes of small (3-layer) models trained on the task versus pre-trained LLMs of similar size?
- Basis in paper: [explicit] The paper contrasts small models with 3-layer transformers trained specifically on the task versus pre-trained models, noting small models show "intermingled reasoning" and are "non-lazy" in processing
- Why unresolved: While the paper observes differences, it doesn't provide a complete mechanistic comparison of how these architectures fundamentally differ in their reasoning strategies
- What evidence would resolve it: Comprehensive circuit analysis of both model types on identical problems, comparing component modularity, processing order, and information flow patterns

### Open Question 4
- Question: How universal are the four-circuit families (queried-rule locating, mover, fact-processing, and decision heads) across different reasoning tasks and model architectures?
- Basis in paper: [inferred] The paper finds similar circuits across three different models but notes this could be an "emergent trait" rather than a universal mechanism
- Why unresolved: The analysis only covers one specific type of logical reasoning problem, making it unclear whether these components would appear in other reasoning domains
- What evidence would resolve it: Testing whether the same circuit components emerge when models solve different types of reasoning problems (arithmetic, syllogistic reasoning, etc.) or when using different model families

### Open Question 5
- Question: What is the exact nature of the "routing signal" (hroute) in the small transformer model and how does it generalize to larger architectures?
- Basis in paper: [explicit] The analysis of small transformers reveals a "routing" mechanism at the QUERY position that determines whether layer 3 treats the problem as linear or LogOp chain
- Why unresolved: The paper identifies this mechanism in small models but doesn't explore whether pre-trained LLMs use similar routing strategies or if the concept scales to deeper architectures
- What evidence would resolve it: Probing experiments in pre-trained models to detect analogous routing mechanisms, or systematic intervention studies to verify the existence and function of such signals

## Limitations

- Weak corpus evidence supporting the specific modular decomposition of reasoning circuits
- Limited generalizability of findings to more complex reasoning tasks beyond propositional logic
- Focus on relatively small models (3-27B parameters) raises questions about applicability to larger frontier models

## Confidence

- **High Confidence**: The identification of four distinct attention head families and their specialized roles is well-supported by the causal mediation analysis results and attention pattern visualizations
- **Medium Confidence**: The "lazy reasoning" hypothesis and circuit reuse claims are supported by the data but rely more heavily on timing-based observations and pattern matching
- **Low Confidence**: The claim that this modular circuit architecture represents a general principle for LLM reasoning is not well-supported by external literature

## Next Checks

1. **Cross-model circuit comparison**: Test whether the same four attention head families appear in other reasoning tasks (e.g., syllogistic inference, arithmetic) and different model architectures (RNNs, CNNs) to validate the generality of the modular circuit hypothesis.

2. **Circuit robustness under perturbation**: Systematically remove or disable individual attention head families to measure the impact on reasoning accuracy and observe whether alternative circuit pathways emerge, testing the sufficiency and necessity of the identified components.

3. **Temporal activation analysis**: Track attention head activations throughout the entire input processing pipeline (before, during, and after Question token appearance) using high-frequency monitoring to confirm the "lazy reasoning" timing claims and identify any pre-processing activities.