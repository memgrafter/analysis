---
ver: rpa2
title: 'Reward Modeling with Ordinal Feedback: Wisdom of the Crowd'
arxiv_id: '2411.12843'
source_url: https://arxiv.org/abs/2411.12843
tags:
- feedback
- loss
- ordinal
- learning
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies learning reward models from ordinal feedback\
  \ in human preference data, generalizing beyond binary feedback to handle richer,\
  \ more nuanced preference labels. It introduces a marginal unbiasedness condition\u2014\
  rooted in the \"wisdom of the crowd\" concept\u2014that connects ordinal feedback\
  \ to the underlying population-level preference probabilities."
---

# Reward Modeling with Ordinal Feedback: Wisdom of the Crowd

## Quick Facts
- **arXiv ID:** 2411.12843
- **Source URL:** https://arxiv.org/abs/2411.12843
- **Authors:** Shang Liu; Yu Pan; Guanting Chen; Xiaocheng Li
- **Reference count:** 40
- **Key outcome:** Ordinal feedback improves reward model learning by reducing Rademacher complexity and generalization bounds

## Executive Summary
This paper introduces a framework for learning reward models from ordinal human preference feedback, extending beyond binary labels to richer, fine-grained preference labels. The authors establish a marginal unbiasedness condition—rooted in the "wisdom of the crowd" concept—that connects ordinal feedback to population-level preference probabilities. Under this condition, they prove that ordinal feedback reduces Rademacher complexity, leading to better statistical learning. Experiments with Llama-3.2-1B and Gemma-2-2B models demonstrate that fine-grained feedback improves both in-distribution and out-of-distribution performance, and that mixing tied preference samples with untied ones further boosts reward model learning.

## Method Summary
The method involves fine-tuning pre-trained language models (Llama-3.2-1B and Gemma-2-2B) with ordinal preference feedback using cross-entropy loss. Data consists of preference pairs (prompt, response1, response2, ordinal label) from Skywork-Reward-Preference-80K-v0.2, with annotations generated by a 5-level oracle model. The framework handles ordinal feedback values beyond binary {0,1}, naturally accommodating tied samples by assigning intermediate values. Training uses scaled oracle preference scores (temperature T = 20/3) to simulate fine-grained feedback, with experiments testing different feedback granularities (binary, 3-level, 5-level) and proportions of tied samples (0%, 25%, 50%, 75%, 100%).

## Key Results
- More fine-grained ordinal feedback (5-level > 3-level > binary) leads to better reward model learning performance
- Incorporating a certain proportion of tied preference samples (25-75%) improves reward model learning compared to 0%-tied data
- Both in-distribution and out-of-distribution accuracy improve with ordinal feedback compared to binary feedback
- The learning objective naturally handles tied samples by assigning intermediate values (e.g., 0.5)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Marginal unbiasedness assumption generalizes binary feedback and enables richer ordinal feedback.
- **Mechanism:** Under the "wisdom of the crowd" assumption, the expected ordinal feedback matches the oracle preference probability, extending the binary case where feedback is a Bernoulli variable.
- **Core assumption:** The marginal unbiasedness condition—human annotators provide unbiased estimates of the population-level preference probability.
- **Evidence anchors:**
  - [abstract]: "We first identify a marginal unbiasedness condition, which generalizes the assumption of the BT model in the existing binary feedback setting. The condition validates itself via the sociological concept of the wisdom of the crowd."
  - [section]: "We assume the ordinal feedback Z defined in Definition 2.1 satisfies E[Z|(x, y1, y2)] = zoracle(x, y1, y2) for any (x, y1, y2) ∈ X × Y 2."
  - [corpus]: Weak—corpus neighbors don't directly discuss this unbiasedness assumption, but they mention preference learning from crowds.
- **Break condition:** If annotators systematically bias their feedback (e.g., due to fatigue, training inconsistency, or misunderstanding the scale), the marginal unbiasedness fails and the probability model becomes invalid.

### Mechanism 2
- **Claim:** Ordinal feedback reduces Rademacher complexity, leading to better statistical learning.
- **Mechanism:** More fine-grained ordinal feedback creates a hierarchical expectation structure where the feedback variable is a coupling of less granular versions. This reduces the Rademacher complexity, improving generalization bounds.
- **Core assumption:** The loss function satisfies the affinity condition (expected loss equals loss evaluated at expected feedback).
- **Evidence anchors:**
  - [abstract]: "We prove the statistical benefits of ordinal feedback in terms of reducing the Rademacher complexity compared to the case of binary feedback."
  - [section]: "For any two ordinal feedback systems Z and Z ′ taking values in Z and Z ′ such that Z is a hierarchical expectation of Z ′, we have RadZ,n(ℓ ◦ H) ≤ RadZ ′,n(ℓ ◦ H)."
  - [corpus]: Weak—corpus doesn't mention Rademacher complexity, but related works discuss ranking and preference learning.
- **Break condition:** If the affinity condition fails (e.g., using a non-affine loss function), the Rademacher complexity reduction argument breaks down.

### Mechanism 3
- **Claim:** Including tied samples in training improves reward model learning.
- **Mechanism:** The learning objective (2) naturally handles tied samples by assigning them intermediate values (e.g., 0.5), and mixing tied and untied samples creates a smoother loss landscape and better generalization.
- **Core assumption:** The objective function (2) can handle ordinal feedback values beyond binary {0,1}.
- **Evidence anchors:**
  - [abstract]: "Further experiments show that incorporating a certain proportion of samples with tied preference boosts RM learning."
  - [section]: "The framework also sheds light on writing guidance for human annotators."
  - [corpus]: Weak—corpus neighbors don't discuss tied samples specifically, but they mention preference strength and ordinal comparisons.
- **Break condition:** If too many samples are tied (100% tied), the reward model collapses to constant values, failing to learn meaningful preferences.

## Foundational Learning

- **Concept:** Bradley-Terry model and binary preference learning
  - Why needed here: The paper builds on the BT model as the foundation for reward modeling, then generalizes it to ordinal feedback.
  - Quick check question: How does the BT model connect pairwise preferences to reward values?

- **Concept:** Rademacher complexity and generalization bounds
  - Why needed here: The theoretical advantage of ordinal feedback is proven through reduced Rademacher complexity, which directly affects generalization.
  - Quick check question: What does reduced Rademacher complexity imply for a model's ability to generalize?

- **Concept:** Hierarchical expectation and coupling in probability theory
  - Why needed here: The proof that ordinal feedback reduces Rademacher complexity relies on constructing hierarchical expectations between different feedback granularities.
  - Quick check question: How does a hierarchical expectation coupling between two feedback systems work?

## Architecture Onboarding

- **Component map:** Data pipeline (prompts/responses → ordinal feedback annotation → storage) → Reward model (LLM fine-tuning with objective 2) → Training loop (batch sampling → ordinal loss computation → parameter updates) → Evaluation (ID/OOD accuracy, oracle CE loss)

- **Critical path:**
  1. Data annotation with ordinal feedback (crucial for model quality)
  2. Reward model training with objective (2) (must handle non-binary values)
  3. Evaluation using oracle labels (measures true performance)

- **Design tradeoffs:**
  - Granularity vs. annotation cost: More feedback levels give better learning but cost more annotation time
  - Bias vs. variance: Tied samples reduce variance but too many cause bias (constant rewards)
  - Model complexity vs. generalization: Finer feedback reduces Rademacher complexity but may require more data

- **Failure signatures:**
  - Model collapse to constant values: Too many tied samples or poor feedback calibration
  - Overfitting to training data: Insufficient ordinal granularity or poor affinity condition
  - Annotation bias: Systematic deviation from marginal unbiasedness assumption

- **First 3 experiments:**
  1. Compare binary vs 3-level vs 5-level feedback on same dataset to verify learning improvement
  2. Test different proportions of tied samples (0%, 25%, 50%, 75%, 100%) to find optimal mix
  3. Validate generalization by testing on out-of-distribution data after training with ordinal feedback

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the ordinal feedback values in practice relate to the "wisdom of the crowd" assumption when individual annotators may systematically differ from population-level preferences?
- Basis in paper: [explicit] The paper identifies the marginal unbiasedness condition (Assumption 3.1) which validates the probability setup of ordinal feedback systems, and states this is rooted in the sociological concept of the wisdom of the crowd. It explains that individual annotators have no access to the population preference $z^{\text{oracle}}$, but their annotation can be viewed as an unbiased random realization of $z^{\text{oracle}}$.
- Why unresolved: The paper assumes annotators provide unbiased estimations on a population level, but doesn't empirically validate whether individual annotators' systematic biases cancel out as assumed. Real-world annotator behavior might deviate from this ideal.
- What evidence would resolve it: Empirical studies measuring individual annotator bias versus population-level preferences, or experiments testing whether the marginal unbiasedness holds across different annotator populations and feedback systems.

### Open Question 2
- Question: What is the optimal proportion of tied samples to include in training data for maximum reward model performance?
- Basis in paper: [explicit] Section 5.3 reports experiments mixing different proportions of tied data (0%, 25%, 50%, 75%, 100%) and finds that mixing a proportion of tied data leads to better performance than the case of 0%-tied data. However, it notes that the 100%-tied setting fails significantly.
- Why unresolved: While the paper shows that some tied samples improve performance, it doesn't identify the optimal proportion. The results show 25%, 50%, and 75% all perform better than 0%, but doesn't determine which is best.
- What evidence would resolve it: Additional experiments testing more granular proportions (e.g., 10%, 20%, 30%, etc.) to identify the peak performance point, or theoretical analysis of why a certain proportion of tied samples provides regularization benefits.

### Open Question 3
- Question: How does the choice of ordinal feedback granularity (e.g., 3-level vs 5-level) interact with model architecture and size in determining reward model performance?
- Basis in paper: [explicit] The paper compares oracle, 5-level, 3-level, and binary feedback systems and finds that more fine-grained feedback leads to better reward learning. It also experiments with llama-3.2-1b and gemma-2-2b models.
- Why unresolved: The experiments use relatively small models (1B and 2B parameters). The interaction between feedback granularity and model capacity is unexplored - larger models might benefit differently from ordinal feedback granularity.
- What evidence would resolve it: Experiments testing the same ordinal feedback systems across models of varying sizes (e.g., 7B, 13B, 70B parameters) to determine whether the benefits of fine-grained feedback scale with model capacity.

## Limitations
- The framework depends critically on the marginal unbiasedness assumption, which may not hold in real-world annotation scenarios with systematic biases
- The theoretical benefits require the affinity condition for loss functions, limiting applicability to specific loss choices
- The practical benefit of ordinal feedback depends on having sufficiently calibrated annotators or oracle models to provide meaningful ordinal distinctions

## Confidence
- **High confidence:** The theoretical framework connecting ordinal feedback to reduced Rademacher complexity is mathematically sound, given the assumptions.
- **Medium confidence:** The experimental results showing improved performance with ordinal feedback are compelling, but the specific implementation details for generating oracle labels may limit exact reproducibility.
- **Low confidence:** The generalizability of findings to scenarios where the marginal unbiasedness assumption is violated remains uncertain.

## Next Checks
1. Test the reward model learning under systematic annotation bias by introducing controlled bias into the ordinal feedback and measuring degradation in performance to validate the importance of the unbiasedness assumption.
2. Compare the Rademacher complexity empirically across binary, 3-level, and 5-level feedback settings using the same dataset and model architecture to verify the theoretical prediction of reduced complexity with finer feedback.
3. Evaluate the optimal proportion of tied samples by training models with varying tied sample ratios (0% to 100%) and measuring the tradeoff between bias and variance in reward predictions.