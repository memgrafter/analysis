---
ver: rpa2
title: A Comprehensive Evaluation of Quantization Strategies for Large Language Models
arxiv_id: '2402.16775'
source_url: https://arxiv.org/abs/2402.16775
tags:
- gptq
- spqr
- llms
- quantized
- int8
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a comprehensive evaluation of quantization
  strategies for large language models (LLMs) across three key dimensions: knowledge
  & capacity, alignment, and efficiency. Through extensive experiments on ten diverse
  benchmarks, the study assesses the performance of quantized LLMs with varying parameter
  scales (7B, 14B, 72B) and bit-widths (8-bit, 4-bit, 3-bit, 2-bit).'
---

# A Comprehensive Evaluation of Quantization Strategies for Large Language Models

## Quick Facts
- arXiv ID: 2402.16775
- Source URL: https://arxiv.org/abs/2402.16775
- Reference count: 40
- One-line primary result: 4-bit quantization can retain performance comparable to non-quantized models on most benchmarks, while perplexity serves as a reliable proxy metric.

## Executive Summary
This paper provides a comprehensive evaluation of quantization strategies for large language models (LLMs) across three key dimensions: knowledge & capacity, alignment, and efficiency. Through extensive experiments on ten diverse benchmarks, the study assesses the performance of quantized LLMs with varying parameter scales (7B, 14B, 72B) and bit-widths (8-bit, 4-bit, 3-bit, 2-bit). The results show that 4-bit quantization can retain performance comparable to non-quantized models on most benchmarks, while perplexity serves as a reliable proxy metric for quantized LLMs. Furthermore, the study finds that quantized LLMs with larger parameter scales can outperform smaller non-quantized models.

## Method Summary
The study evaluates quantization strategies using Qwen-Chat series models (7B, 14B, 72B parameters) with post-training quantization (PTQ) methods including LLM.int8(), GPTQ, and SpQR across bit-widths from 8-bit down to 2-bit. The evaluation framework assesses knowledge & capacity through benchmarks like MMLU and GSM8K, alignment through FollowBench and TruthfulQA, and efficiency through memory consumption and decoding speed measurements. Calibration data from Taori et al. (2023) and Peng et al. (2023) datasets is used for quantization, with comprehensive comparisons against BFloat16 baselines across all dimensions.

## Key Results
- 4-bit quantization retains performance comparable to non-quantized models on most benchmarks
- Perplexity correlates strongly with benchmark performance (Pearson coefficient 0.7895)
- Larger parameter scale quantized models can outperform smaller non-quantized models at similar memory consumption
- 2-bit GPTQ quantization fails completely while SpQR succeeds through outlier isolation
- Quantization can lead to slower inference speeds, particularly for 3-bit and 2-bit methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Isolating outlier weights and keeping them in high precision enables effective extreme-bit quantization.
- Mechanism: SpQR uses bilevel quantization with small group sizes and isolates outlier weights by maintaining them in 16-bit precision during computation. This protects large-magnitude values that would otherwise cause severe quantization errors in low-bit formats.
- Core assumption: Outlier weights cause most quantization error in low-bit quantization and can be identified effectively.
- Evidence anchors:
  - [abstract]: "identifying isolating outlier weights as a key factor enabling SpQR to effectively quantize LLMs to an extreme 2-bit level"
  - [section]: "the isolation of outlier weights and their preservation in high precision is indispensable for SpQR to effectively quantize LLMs to an extreme level of 2 bits"
  - [corpus]: Weak - no direct corpus support for this specific mechanism

### Mechanism 2
- Claim: Perplexity serves as a reliable proxy metric for quantized LLM performance on downstream benchmarks.
- Mechanism: Lower perplexity indicates better language modeling capability, which correlates with better performance across knowledge, capacity, and alignment benchmarks. The relationship holds across different quantization levels (8-bit, 4-bit, 3-bit).
- Core assumption: Language modeling quality (measured by perplexity) is predictive of performance on diverse downstream tasks.
- Evidence anchors:
  - [abstract]: "perplexity can serve as a proxy metric for quantized LLMs on most benchmarks"
  - [section]: "there is a strong correlation between perplexity and the performance of quantized LLMs. The average absolute value of the Pearson correlation coefficient is notably high at 0.7895"
  - [corpus]: Weak - corpus shows related work on quantization evaluation but no direct evidence for perplexity correlation

### Mechanism 3
- Claim: Larger parameter scale quantized models can outperform smaller non-quantized models at similar memory consumption.
- Mechanism: When memory consumption is equalized through quantization, the increased model capacity from larger parameter scales provides performance benefits that outweigh quantization degradation, particularly for knowledge-intensive tasks.
- Core assumption: The performance gains from larger model capacity dominate the performance losses from quantization when memory is constrained.
- Evidence anchors:
  - [abstract]: "quantized LLMs with larger parameter scales can outperform smaller LLMs"
  - [section]: "LLMs quantized to lower bit precision with a larger parameter scale can be preferred over LLMs with a smaller parameter scale, considering their performance capabilities"
  - [corpus]: Weak - corpus mentions compressed LLMs but not this specific scaling relationship

## Foundational Learning

- Concept: Post-training quantization (PTQ) vs quantization-aware training (QAT)
  - Why needed here: The paper focuses exclusively on PTQ methods, which is critical context for understanding the engineering tradeoffs discussed
  - Quick check question: What is the key difference between PTQ and QAT, and why does the paper focus on PTQ?

- Concept: Group-wise quantization and bilevel quantization
  - Why needed here: SpQR's effectiveness relies on these techniques - small group sizes and two-stage quantization process
  - Quick check question: How does bilevel quantization differ from standard quantization, and why might smaller group sizes help with outlier handling?

- Concept: Correlation coefficients and statistical significance
  - Why needed here: The paper reports Pearson correlation coefficients and uses statistical significance markers (p < 0.05) to validate findings
  - Quick check question: What does a Pearson correlation coefficient of 0.7895 indicate about the relationship between perplexity and benchmark performance?

## Architecture Onboarding

- Component map: Qwen-Chat models (7B, 14B, 72B) -> Quantization methods (LLM.int8(), GPTQ, SpQR) -> Bit-widths (8-bit, 4-bit, 3-bit, 2-bit) -> Evaluation framework (knowledge & capacity, alignment, efficiency benchmarks) -> Performance metrics (accuracy, BLEU, ROUGE, bias scores, perplexity, memory, speed)

- Critical path: For a new quantized model evaluation: 1) Apply quantization using chosen method, 2) Measure memory and speed, 3) Compute perplexity on held-out data, 4) Run benchmarks across all three dimensions, 5) Compare against BFloat16 baseline and other quantization levels.

- Design tradeoffs: Memory vs speed tradeoff in quantization implementation - SpQR simulates low-bit quantization in high precision for compatibility but doesn't save memory. Engineering effort vs performance - manual low-precision kernel implementation could improve speed but requires significant hardware expertise.

- Failure signatures: 2-bit GPTQ quantization produces incoherent text (indicated by extreme perplexity >38,000). SpQR with large group sizes (128) or without outlier isolation shows significantly increased perplexity. Memory savings without speed improvements suggest implementation issues.

- First 3 experiments:
  1. Replicate the perplexity correlation experiment - measure perplexity on WikiText2, C4, and PTB for 4-bit and 3-bit quantized models, then compute Pearson correlation with benchmark performance
  2. Test outlier isolation sensitivity - run SpQR with and without outlier isolation on a small model (7B), measure perplexity and a few benchmark tasks
  3. Memory-speed tradeoff analysis - compare LLM.int8() vs GPTQ on same model, measure both memory consumption and tokens/second, analyze if memory savings come with speed penalties

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between perplexity and task performance for quantized LLMs across different bit-widths?
- Basis in paper: [explicit] The paper shows a strong correlation (Pearson coefficient 0.7895) between perplexity and benchmark performance, but notes that 4-bit models perform comparably to non-quantized models despite increased perplexity.
- Why unresolved: The relationship appears non-linear and discontinuous, with certain bit-widths showing performance plateaus despite increasing perplexity.
- What evidence would resolve it: Controlled experiments varying bit-widths systematically while measuring both perplexity and task performance across diverse benchmarks.

### Open Question 2
- Question: How do outlier weights vary across different LLM architectures and training regimes when quantized to extreme bit-widths?
- Basis in paper: [explicit] The paper identifies outlier isolation as crucial for 2-bit quantization success in Qwen models, with outlier proportions ranging from 0.003% (8-bit) to 11.336% (2-bit).
- Why unresolved: The analysis is limited to Qwen models; it's unclear if these outlier patterns generalize to other architectures like LLaMA or GPT.
- What evidence would resolve it: Comparative analysis of outlier distributions across multiple LLM architectures at various quantization levels.

### Open Question 3
- Question: What engineering optimizations could make low-bit quantization practical without sacrificing speed or accuracy?
- Basis in paper: [inferred] The paper notes that current low-bit quantization methods require significant engineering effort and hardware support, with inference speeds often slower than non-quantized models except at 4-bit.
- Why unresolved: The paper identifies the problem but doesn't propose specific solutions for the engineering challenges.
- What evidence would resolve it: Performance comparisons of different hardware acceleration strategies and custom kernel implementations for low-bit quantization across various GPU architectures.

## Limitations
- Focus on Qwen-Chat models limits generalizability to other LLM architectures
- Lack of quantization-aware training (QAT) evaluation may miss important performance differences
- Limited hardware platform testing (A100 80GB SXM only) may not represent broader deployment scenarios

## Confidence

**High Confidence**: The correlation between perplexity and benchmark performance (Pearson correlation coefficient of 0.7895) is well-supported by the extensive experimental results across ten diverse benchmarks. The finding that 4-bit quantization can retain performance comparable to non-quantized models is also highly reliable given the systematic evaluation.

**Medium Confidence**: The claim that larger parameter scale quantized models can outperform smaller non-quantized models at similar memory consumption is supported but limited by the lack of 3-bit and 2-bit results for the 72B model. The importance of outlier weight isolation for 2-bit quantization is demonstrated but could benefit from additional ablation studies.

**Low Confidence**: The assertion that substantial engineering efforts and hardware support are required to balance memory and decoding speed is largely theoretical, as the paper doesn't provide concrete implementation comparisons or hardware-specific optimizations beyond noting the observed speed penalties.

## Next Checks
1. **Replication of scaling relationship**: Test the 72B model with 3-bit and 2-bit quantization to validate whether the scaling advantage holds across all bit-widths, particularly at the extreme 2-bit level where engineering trade-offs become most critical.

2. **Cross-architecture generalization**: Apply the same evaluation framework to other LLM architectures (e.g., LLaMA, Mistral) to determine if the observed performance patterns and perplexity correlation generalize beyond Qwen-Chat models.

3. **Hardware platform comparison**: Evaluate quantization performance on different GPU architectures (e.g., H100, consumer GPUs) and CPU inference to assess whether the observed speed-memory tradeoffs are hardware-dependent or consistent across platforms.