---
ver: rpa2
title: 'SLIM-RAFT: A Novel Fine-Tuning Approach to Improve Cross-Linguistic Performance
  for Mercosur Common Nomenclature'
arxiv_id: '2408.03936'
source_url: https://arxiv.org/abs/2408.03936
tags:
- language
- portuguese
- product
- slim-raft
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes SLIM-RAFT, a simplified retrieval-augmented
  fine-tuning approach for domain-specific applications using smaller Portuguese language
  models. The method applies a streamlined chain-of-thought framework to NCM (Mercosur
  Common Nomenclature) classification, avoiding the complexity and cost of traditional
  RAFT by using focused training documents rather than full-length content.
---

# SLIM-RAFT: A Novel Fine-Tuning Approach to Improve Cross-Linguistic Performance for Mercosur Common Nomenclature

## Quick Facts
- **arXiv ID:** 2408.03936
- **Source URL:** https://arxiv.org/abs/2408.03936
- **Reference count:** 8
- **Primary result:** SLIM-RAFT fine-tuning of a 160M parameter TeenyTineLLaMA model achieves 8.63 average score on NCM classification vs 0.2 for base model and 4.5 for ChatGPT-4

## Executive Summary
This study proposes SLIM-RAFT, a simplified retrieval-augmented fine-tuning approach for domain-specific applications using smaller Portuguese language models. The method applies a streamlined chain-of-thought framework to NCM (Mercosur Common Nomenclature) classification, avoiding the complexity and cost of traditional RAFT by using focused training documents rather than full-length content. Using a 160M parameter TeenyTineLLaMA model as the base, SLIM-RAFT significantly outperforms both the original TeenyTineLLaMa (0.2 vs. 8.63 average score) and ChatGPT-4 (4.5) on a 100-question NCM classification task, achieving an average score of 8.63 with a standard deviation of 2.30. The results demonstrate that task-specific fine-tuning of compact models can surpass general-purpose LLMs in specialized domains while maintaining low computational costs.

## Method Summary
SLIM-RAFT simplifies the traditional RAFT approach by using focused training documents containing only relevant logical arguments instead of full-length content with distractors. The method employs a 160M parameter TeenyTineLLaMA model as the base, fine-tuned on NCM domain data with chain-of-thought reasoning. The training process involves creating question-answer pairs with domain expert input, generating variations using an LLM, and populating a question-answer set mask with NCM database samples. The simplified chain-of-thought approach uses logical argument sequences rather than full document contexts, making training more efficient while retaining reasoning capability.

## Key Results
- SLIM-RAFT achieves 8.63 average score on 100-question NCM classification task vs 0.2 for base TeenyTineLLaMA and 4.5 for ChatGPT-4
- Standard deviation of 2.30 indicates consistent performance across test questions
- Task-specific fine-tuning enables better handling of context-dependent abbreviations and terminology than general translation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SLIM-RAFT outperforms larger LLMs on specialized tasks by combining task-specific fine-tuning with a simplified chain-of-thought approach
- Mechanism: Fine-tuning a compact Portuguese LLM on focused domain dataset learns domain-specific semantic relationships rather than relying on general language understanding
- Core assumption: Domain-specific fine-tuning on compact models can outperform general-purpose LLMs when the task requires specialized knowledge
- Evidence anchors: Significant performance improvement (0.2 to 8.63) on NCM classification task; comparison showing ChatGPT-4 scored only 4.5/10

### Mechanism 2
- Claim: SLIM-RAFT achieves efficiency by simplifying training base construction compared to original RAFT
- Mechanism: Uses brief, focused documents containing only relevant logical arguments instead of full-length documents with distractors
- Core assumption: Chain-of-thought reasoning can be effectively implemented using condensed logical arguments
- Evidence anchors: Description of simplified approach using focused documents; elimination of need for second powerful LLM to generate training base

### Mechanism 3
- Claim: Task-specific fine-tuning enables better handling of context-dependent abbreviations and terminology
- Mechanism: Training on domain-specific examples with context-rich Portuguese descriptions and corresponding NCM codes
- Core assumption: Specialized training on domain-specific terminology provides better performance than general language models
- Evidence anchors: Explanation of context resolution importance; TRANSFORMER-based algorithms' ability to understand context

## Foundational Learning

- Concept: Chain-of-thought reasoning
  - Why needed here: Enables step-by-step logical reasoning rather than pattern matching for complex classification tasks
  - Quick check question: Can the model correctly explain its reasoning process for classifying a product when asked to show its work?

- Concept: Domain-specific fine-tuning
  - Why needed here: General-purpose LLMs lack specialized knowledge required for NCM classification involving complex tax and regulatory relationships
  - Quick check question: Does the model show significant performance improvement after fine-tuning compared to base model on domain-specific tasks?

- Concept: Context resolution in multilingual settings
  - Why needed here: Portuguese product descriptions contain abbreviations requiring understanding of local context and business practices
  - Quick check question: Can the model correctly disambiguate similar product descriptions that differ only in contextual details?

## Architecture Onboarding

- Component map: Base LLM (TeenyTineLLaMA 160M) → Fine-tuning pipeline → SLIM-RAFT model → Evaluation system → ChatGPT-4 evaluator
- Critical path: Data preparation → Fine-tuning → Evaluation → Deployment
- Design tradeoffs: Model size vs. performance vs. computational cost; training data complexity vs. model capability
- Failure signatures: Inability to resolve context-dependent terms; poor performance on unseen product descriptions; incorrect classification of similar products
- First 3 experiments:
  1. Fine-tune the base model on a small subset of NCM data and measure performance improvement
  2. Test the model's ability to handle context-dependent abbreviations using a validation set
  3. Evaluate the simplified chain-of-thought approach by comparing reasoning quality with full RAFT approach on sample tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SLIM-RAFT performance compare to other fine-tuning techniques like LoRa when applied to larger LLMs such as LLaMA 3 for multilingual tasks?
- Basis in paper: [inferred] Paper suggests future research will focus on applying SLIM-RAFT to larger LLMs and comparing with other techniques like LoRa
- Why unresolved: Only tested on 160M parameter model without comparison to other fine-tuning methods on larger models
- What evidence would resolve it: Comparative studies of SLIM-RAFT vs. LoRa or other techniques on larger multilingual models

### Open Question 2
- Question: What is the impact of using domain expert to develop reference lines of reasoning vs automated methods?
- Basis in paper: [explicit] Paper states domain expert involvement is beneficial and necessary for developing reference lines of reasoning
- Why unresolved: No empirical evidence on effectiveness of domain expert involvement versus automated methods
- What evidence would resolve it: Studies comparing performance of models trained with domain expert input vs automated methods

### Open Question 3
- Question: How does SLIM-RAFT performance vary when applied to other HS applications or multilingual contexts?
- Basis in paper: [explicit] Paper mentions methodology can be adapted for HS applications worldwide and suggests future research on multilingual tasks
- Why unresolved: Only demonstrates effectiveness on NCM classification task without exploring other applications
- What evidence would resolve it: Empirical studies applying SLIM-RAFT to various HS applications and multilingual contexts

## Limitations
- Evaluation relies on single custom 100-question dataset which may not generalize to broader NCM classification tasks
- Comparison methodology with ChatGPT-4 is flawed as it evaluates fine-tuned 160M parameter model against general-purpose 1.8T parameter system
- Methodology section lacks sufficient technical detail for replication of key implementation aspects

## Confidence

**High Confidence**: Core finding that task-specific fine-tuning improves performance on specialized classification tasks compared to untrained base models. Well-documented significant performance jump from 0.2 to 8.63 for TeenyTineLLaMA.

**Medium Confidence**: Claim that SLIM-RAFT outperforms ChatGPT-4 on NCM classification. Results show this on specific dataset but comparison methodology is flawed and generalizability remains unproven.

**Low Confidence**: Broader claims about efficiency and scalability. Study lacks comprehensive cost comparisons and domain expert involvement raises questions about practical deployment at scale.

## Next Checks

1. **External Dataset Validation**: Test SLIM-RAFT on separate, independently curated NCM classification dataset to verify performance gains generalize beyond original evaluation set

2. **Cost-Benefit Analysis**: Conduct detailed computational cost analysis comparing SLIM-RAFT fine-tuning vs inference costs of general-purpose LLMs, including time-to-train, inference latency, and expert labor costs

3. **Cross-Domain Transferability**: Apply SLIM-RAFT methodology to different domain-specific classification task (e.g., medical coding or legal document classification) to test generalizability beyond NCM applications