---
ver: rpa2
title: Fine-Tuning Large Language Models with User-Level Differential Privacy
arxiv_id: '2407.07737'
source_url: https://arxiv.org/abs/2407.07737
tags:
- compute
- privacy
- guls
- user-level
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work examines user-level differential privacy for fine-tuning
  large language models. It compares two methods: example-level sampling with per-example
  clipping (ELS) and user-level sampling with per-user clipping (ULS).'
---

# Fine-Tuning Large Language Models with User-Level Differential Privacy

## Quick Facts
- arXiv ID: 2407.07737
- Source URL: https://arxiv.org/abs/2407.07737
- Reference count: 40
- Primary result: User-level sampling with per-user clipping (ULS) generally outperforms example-level sampling with per-example clipping (ELS) when users have diverse gradients, particularly for strong privacy requirements or large compute budgets.

## Executive Summary
This work examines user-level differential privacy for fine-tuning large language models, comparing two methods: example-level sampling with per-example clipping (ELS) and user-level sampling with per-user clipping (ULS). A novel accounting technique is developed for ELS, providing tight privacy bounds. Experiments across synthetic mean estimation and real language modeling tasks show ULS generally outperforms ELS when users have diverse gradients, particularly for strong privacy requirements or large compute budgets. The authors demonstrate scalability to models with hundreds of millions of parameters and datasets with hundreds of thousands of users, and provide practical heuristics for parameter selection.

## Method Summary
The paper proposes DP-SGD-ELS (example-level sampling with per-example clipping) and DP-SGD-ULS (user-level sampling with per-user clipping) for fine-tuning LLMs with user-level differential privacy. ELS samples individual examples and adds noise per example, while ULS samples users, aggregates their gradients, clips at the user level, and adds noise per user. A novel Mixture-of-Gaussians (MoG) accountant is derived for ELS to compute tight privacy bounds. Both methods are evaluated under fixed compute budgets, with group sizes configured using median heuristics and an estimate-and-double algorithm. The methods are implemented using tf.data pipelines (ELS) and Dataset Grouper with FAX (ULS) for parallelization.

## Key Results
- ULS generally yields better utility than ELS when each user has a diverse collection of examples, especially under strong privacy requirements or large compute budgets
- The novel MoG accountant provides tighter privacy bounds for ELS compared to generic group privacy reductions
- The methods scale to models with hundreds of millions of parameters and datasets with hundreds of thousands of users
- Practical heuristics for configuring group sizes and other hyperparameters are provided

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ULS outperforms ELS when users have diverse gradients and strong privacy/compute budgets.
- Mechanism: ULS clips and adds noise at the user level, reducing per-user gradient variance when gradients are diverse. ELS clips and adds noise per-example, which does not leverage user-level diversity.
- Core assumption: Per-user gradients have lower variance than per-example gradients when users exhibit diverse language patterns.
- Evidence anchors:
  - [abstract] "we show that while ELS can outperform ULS in specific settings, ULS generally yields better results when each user has a diverse collection of examples"
  - [section 3] "While (7) is not entirely predictive of the relative performance of ELS and ULS... it is a useful way to compare the methods"
  - [corpus] Weak - corpus lacks direct gradient diversity measurements
- Break condition: If per-user gradients are not diverse (e.g., all users write similarly), ELS may outperform ULS due to better example-level sampling.

### Mechanism 2
- Claim: The novel MoG accountant provides tighter privacy bounds for ELS than generic reductions.
- Mechanism: MoG mechanisms directly model the privacy loss distribution for ELS, avoiding the exponential blow-up from generic group privacy reductions.
- Core assumption: The mixture of Gaussians accurately captures the privacy loss distribution for ELS under subsampling.
- Evidence anchors:
  - [abstract] "We derive a novel user-level DP accountant that allows us to compute provably tight privacy guarantees for ELS"
  - [section 2] "By leveraging the Mixture-of-Gaussians (MoG) mechanisms [27], we derive the optimal user-level DP accounting for ELS"
  - [corpus] Weak - corpus does not detail the MoG accounting derivation
- Break condition: If the MoG approximation breaks down for extreme privacy parameters, the accountant may become loose.

### Mechanism 3
- Claim: Fixed compute budgets enable fair comparison between ELS and ULS.
- Mechanism: Normalizing by total gradient computations removes algorithmic differences in data processing, isolating the effect of sampling/clipping strategies.
- Core assumption: Computational cost is dominated by gradient computations, not communication or other overheads.
- Evidence anchors:
  - [abstract] "We focus on their relative performance under a fixed compute budget"
  - [section 2] "For simplicity, we assume the computation cost is equal to the total number of gradient computations"
  - [corpus] Weak - corpus does not validate the computational cost assumption
- Break condition: If communication or other operations dominate runtime, fixed compute budgets may not enable fair comparison.

## Foundational Learning

- Concept: Differential privacy fundamentals (ε, δ guarantees, composition, amplification by subsampling)
  - Why needed here: Understanding how ELS and ULS achieve user-level DP and how privacy budgets are computed
  - Quick check question: What is the relationship between example-level DP and user-level DP in this work?

- Concept: Privacy accounting techniques (Renyi DP, PLD, MoG mechanisms)
  - Why needed here: Interpreting the novel accountant for ELS and comparing privacy bounds
  - Quick check question: How does the MoG accountant differ from generic group privacy reductions?

- Concept: Gradient clipping and noise addition in DP-SGD
  - Why needed here: Understanding how ELS and ULS implement DP-SGD at different granularities
  - Quick check question: What is the difference between per-example and per-user gradient clipping?

## Architecture Onboarding

- Component map:
  - ELS: Example sampling → Per-example clipping → Per-example noise addition → Model update
  - ULS: User sampling → Per-user gradient averaging → Per-user clipping → Per-user noise addition → Model update
  - Shared: Pre-trained model initialization, dataset partitioning, DP accounting, learning rate/clip norm tuning

- Critical path:
  1. Preprocess datasets into user partitions
  2. Configure group sizes (GELS for ELS, GULS for ULS) based on median heuristic
  3. Compute noise multipliers using DP accountants for target (ε, δ)
  4. Train with fixed compute budget, tracking validation loss
  5. Evaluate on test set

- Design tradeoffs:
  - ELS: Simpler implementation, but may require larger noise for strong privacy
  - ULS: Better privacy-utility for diverse users, but needs user-level gradient averaging
  - Both: Trade-off between group size (privacy vs utility) and compute budget

- Failure signatures:
  - ELS underperforms: Users have diverse gradients, strong privacy needed, large compute budget
  - ULS underperforms: Users have similar gradients, weak privacy, small compute budget
  - Both underperform: Poor group size choice, inadequate tuning of learning rate/clip norm

- First 3 experiments:
  1. Run ELS and ULS on synthetic mean estimation with varying gradient diversity to verify theoretical predictions
  2. Fine-tune on Stack Overflow with ε=4, varying compute budgets to compare ELS vs ULS
  3. Test the median heuristic for GELS/GULS by sweeping group sizes and measuring validation loss

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what precise conditions does ULS outperform ELS in terms of utility, and can this be theoretically predicted?
- Basis in paper: [explicit] The paper shows ULS generally outperforms ELS when users have diverse gradients, particularly for strong privacy requirements or large compute budgets, but notes a general theoretical comparison is lacking.
- Why unresolved: The paper lacks a general theorem comparing ELS and ULS utility across different settings, especially without Lipschitz loss functions. The analysis is primarily empirical.
- What evidence would resolve it: A formal theoretical analysis comparing ELS and ULS utility across various settings, including non-Lipschitz loss functions and different gradient diversity scenarios, would resolve this question.

### Open Question 2
- Question: How does the performance of ELS and ULS scale with model size, and what are the scaling laws for user-level DP fine-tuning?
- Basis in paper: [explicit] The paper demonstrates scalability to models with hundreds of millions of parameters but focuses on exploring hyperparameter configurations rather than maximally scaling up model size due to compute constraints.
- Why unresolved: The work did not investigate the model scaling behavior of ELS and ULS due to compute limitations, leaving the scaling laws for user-level DP fine-tuning unknown.
- What evidence would resolve it: Experiments scaling ELS and ULS to larger models (billions of parameters) while maintaining user-level DP guarantees would provide insights into the scaling laws and performance trade-offs.

### Open Question 3
- Question: What is the optimal strategy for selecting group sizes GELS and GULS in practice, and how sensitive are the algorithms to these hyperparameters?
- Basis in paper: [explicit] The paper provides practical heuristics for configuring group sizes based on median user dataset size and an estimate-and-double algorithm, but acknowledges these may not generalize to all settings.
- Why unresolved: While the paper offers useful heuristics, it is unclear if these strategies are optimal across diverse datasets and settings. The sensitivity of ELS and ULS to group size choices is not fully characterized.
- What evidence would resolve it: A comprehensive study evaluating the performance of ELS and ULS across a wide range of datasets with varying user dataset size distributions and gradient characteristics would reveal the optimal group size strategies and their sensitivity.

## Limitations

- The theoretical analysis relies on assumptions about gradient diversity that are not fully validated empirically.
- The comparison between ELS and ULS is based on fixed compute budgets, but the assumption that computation cost is dominated by gradient computations may not hold in practice.
- The novel MoG accountant, while providing tighter bounds, lacks detailed validation of its approximation accuracy for extreme privacy parameters.
- The study focuses on specific datasets (Stack Overflow, CC-News) and model sizes (350M parameters), which may limit generalizability.

## Confidence

- **High Confidence**: The empirical demonstration that ULS outperforms ELS under diverse user gradients and strong privacy requirements.
- **Medium Confidence**: The theoretical analysis predicting when ELS or ULS will perform better based on gradient diversity.
- **Medium Confidence**: The effectiveness of the MoG accountant for ELS.

## Next Checks

1. **Gradient Diversity Validation**: Measure and report the actual gradient diversity across users in the Stack Overflow and CC-News datasets. Compute per-user gradient norms and their variance to quantify the assumption underlying the ULS advantage.

2. **MoG Accountant Accuracy**: Compare the privacy bounds computed by the MoG accountant against numerical composition methods for extreme privacy parameters (ε < 1). Report the gap between analytical and numerical bounds across a range of δ values.

3. **Communication Overhead Analysis**: Measure the wall-clock time breakdown for ELS and ULS fine-tuning, including communication time for gradient aggregation. Verify whether fixed compute budgets based on gradient computations provide fair comparison in distributed settings.