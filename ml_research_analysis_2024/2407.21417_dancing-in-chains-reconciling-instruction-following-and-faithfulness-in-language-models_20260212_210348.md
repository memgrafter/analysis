---
ver: rpa2
title: 'Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language
  Models'
arxiv_id: '2407.21417'
source_url: https://arxiv.org/abs/2407.21417
tags:
- datasets
- instruction
- following
- training
- your
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Modern language models face a trade-off between instruction following
  and faithfulness when trained on mixed datasets. To address this, the authors propose
  Rejection Sampling for Continued Self-instruction Tuning (RESET), which samples
  model generations with varied decoding settings, evaluates them using external judges,
  and selects top-rated outputs for continued fine-tuning.
---

# Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models

## Quick Facts
- arXiv ID: 2407.21417
- Source URL: https://arxiv.org/abs/2407.21417
- Reference count: 8
- Modern LMs face trade-off between instruction following and faithfulness when trained on mixed datasets

## Executive Summary
Modern language models struggle with a fundamental trade-off: improving instruction-following ability often comes at the cost of faithfulness to source contexts, and vice versa. This paper introduces Rejection Sampling for Continued Self-instruction Tuning (RESET), a method that generates diverse candidate responses using varied decoding parameters, evaluates them with external judges, and selects top-rated outputs for continued fine-tuning. RESET achieves significant improvements in both instruction-following and faithfulness metrics while using substantially less data than traditional multi-task learning approaches.

## Method Summary
The method works in two stages: first, a base model is fine-tuned using standard multi-task learning on mixed instruction-following and context-dependent datasets. Then, RESET generates multiple candidate outputs per example using varied temperature and top-k decoding settings, which are evaluated by external judges (GPT-4 or ChatGPT) for both instruction-following and faithfulness. The top-rated generations are collected into a high-quality dataset (2k-8k examples) and used to fine-tune the model for one epoch with a small learning rate, avoiding overfitting while preserving gains in both capabilities.

## Key Results
- RESET improves faithfulness by up to +18.8% over multi-task learning baselines while maintaining instruction-following ability
- Training with high-quality, yet substantially smaller data (three-fold less) yields superior results
- The method is complementary to other alignment techniques and demonstrates consistent improvements across multiple model architectures (LLaMA-7B, Vicuna-7B)

## Why This Works (Mechanism)

### Mechanism 1
Sampling with varied decoding parameters introduces response diversity that captures orthogonal aspects of instruction-following and faithfulness. The LM generates multiple candidate outputs for each example by varying temperature and top-k, ensuring the sample set contains both creative, instruction-aligned responses and conservative, context-grounded ones. The LM's internal representations encode both capabilities; different decoding settings selectively activate these dimensions.

### Mechanism 2
Weighted scoring that upweights the instruction-following or faithfulness component depending on dataset type yields balanced capability retention. Scores are combined as `score = stask + 2.0*(Iinstr*sinstr + Ifaith*sfaith)`, boosting the relevant dimension for each dataset type before ranking. Different dataset types have distinct alignment priorities; weighting allows a single judge set to capture both.

### Mechanism 3
Rejection sampling on self-generated candidates plus lightweight continued fine-tuning yields larger gains than full MTL on the original data. Instead of mixing and retraining on all examples, the method collects high-scoring self-generated examples and fine-tunes for one epoch with a small LR. High-quality, targeted examples are more effective than quantity; small LR prevents forgetting.

## Foundational Learning

- **Concept**: Objective misalignment in multi-task training
  - Why needed here: The paper explicitly shows that mixing instruction-following and context-dependent tasks causes a trade-off
  - Quick check question: If a model is fine-tuned on both open-ended QA and extractive QA datasets, which capability is likely to degrade and why?

- **Concept**: Rejection sampling / preference learning
  - Why needed here: RESET is fundamentally a rejection-sampling approach
  - Quick check question: In rejection sampling, why is it better to use a generative model to propose candidates rather than random sampling from the data distribution?

- **Concept**: Decoding hyperparameters (temperature, top-k) and their effect on diversity
  - Why needed here: The method varies these to produce diverse outputs
  - Quick check question: What happens to the entropy of the output distribution as temperature increases from 0.1 to 0.7?

## Architecture Onboarding

- **Component map**: Base LM -> Decoder sampler -> External judge pipeline -> Scoring aggregator -> Continued fine-tuning stage -> Evaluation harness

- **Critical path**:
  1. Load fine-tuned checkpoint (MTL baseline)
  2. Sample generations with varied decoding settings per example
  3. Run external judges to obtain scores
  4. Compute weighted scores and select top-1 per example
  5. Assemble curated dataset (2k-8k examples)
  6. Fine-tune checkpoint for 1 epoch with small LR
  7. Evaluate on seen/unseen test sets

- **Design tradeoffs**:
  - Sampling many generations increases compute but improves diversity; fewer samples save cost but risk bias
  - Using weaker judge (ChatGPT) for RESET-S trades recall for precision; stronger judge (GPT-4) is more expensive but may yield higher quality
  - Single-epoch fine-tuning avoids overfitting but may underfit if dataset is too small

- **Failure signatures**:
  - Instruction-following score drops while faithfulness stays high → decoder diversity insufficient or weighting too conservative
  - Faithfulness drops while instruction-following high → external judge bias or weighted scoring over-privileges open-ended instruction following
  - No improvement over MTL → sampling step produced redundant outputs; external judges inconsistent; learning rate too low

- **First 3 experiments**:
  1. Run MTL baseline on a small mixed dataset, verify trade-off pattern
  2. Implement RESET with fixed decoding settings (e.g., only temperature=0.5) and compare
  3. Add varied decoding, external judging, and weighted scoring; verify gains over baseline

## Open Questions the Paper Calls Out

- **Open Question 1**: How does RESET performance scale with model size and parameter count?
  - Basis: Inferred from discussion of LLaMA-7B and Vicuna-7B results
  - Why unresolved: Only tests RESET on 7B parameter models
  - What evidence would resolve it: Testing RESET on larger models (e.g., LLaMA-13B, LLaMA-70B)

- **Open Question 2**: What is the optimal ratio of instruction-following to context-dependent data in multi-task learning?
  - Basis: Explicit mention that "one natural mitigation strategy is to use multi-task learning (MTL) by mixing datasets"
  - Why unresolved: Uses equal mixing but doesn't explore optimal ratios
  - What evidence would resolve it: Systematic experiments varying the proportion of each dataset type

- **Open Question 3**: How does RESET compare to other alignment methods like RLHF or DPO?
  - Basis: Explicit mention that "RESET is complimentary to different instruct-tuning techniques"
  - Why unresolved: Only compares RESET to vanilla MTL
  - What evidence would resolve it: Direct comparison with RLHF, DPO, and other preference learning methods

## Limitations
- Success hinges on quality and consistency of external judge evaluations, which are not fully characterized
- Benefits demonstrated only on specific model sizes (7B parameters) and datasets; generalization untested
- Method assumes varied decoding yields meaningful diversity, but distribution of generated samples is not analyzed

## Confidence
- **High confidence**: The observed trade-off between instruction-following and faithfulness in multi-task learning settings
- **Medium confidence**: The scalability of RESET to larger models or more diverse datasets
- **Low confidence**: The long-term stability of the improvements and out-of-domain performance

## Next Checks
1. Measure inter-judge agreement (e.g., using Cohen's kappa) on a subset of RESET-generated samples to quantify judge consistency
2. Test RESET with reduced decoding diversity (e.g., only temperature variation) to determine minimum diversity required for gains
3. Apply RESET to a held-out domain (e.g., biomedical QA) not present in original training or evaluation sets to assess generalization