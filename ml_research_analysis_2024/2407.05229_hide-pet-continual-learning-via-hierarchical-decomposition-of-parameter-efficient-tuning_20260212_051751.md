---
ver: rpa2
title: 'HiDe-PET: Continual Learning via Hierarchical Decomposition of Parameter-Efficient
  Tuning'
arxiv_id: '2407.05229'
source_url: https://arxiv.org/abs/2407.05229
tags:
- learning
- performance
- tasks
- parameters
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses continual learning (CL) with pre-trained models
  (PTMs) by keeping the backbone frozen and introducing parameter-efficient tuning
  (PET) techniques to optimize representation learning. The core method, HiDe-PET,
  provides a unified framework that explicitly optimizes the CL objective decomposed
  into three hierarchical components: within-task prediction (WTP), task-identity
  inference (TII), and task-adaptive prediction (TAP).'
---

# HiDe-PET: Continual Learning via Hierarchical Decomposition of Parameter-Efficient Tuning

## Quick Facts
- **arXiv ID:** 2407.05229
- **Source URL:** https://arxiv.org/abs/2407.05229
- **Reference count:** 40
- **Primary result:** Achieves final average accuracies of 91.21% on CIFAR-100, 79.32% on ImageNet-R, 88.76% on CUB-200, and 69.65% on Cars-196 across various CL scenarios

## Executive Summary
HiDe-PET introduces a unified framework for continual learning with pre-trained models by explicitly decomposing the learning objective into three hierarchical components: within-task prediction, task-identity inference, and task-adaptive prediction. The method keeps the backbone frozen while using parameter-efficient tuning techniques (Prompt, Adapter, LoRA) to optimize representation learning. This approach demonstrates remarkably superior performance over strong baselines across multiple image classification benchmarks, showing strong generality across different PET techniques and pre-trained checkpoints.

## Method Summary
HiDe-PET provides a unified framework that explicitly optimizes the continual learning objective through hierarchical decomposition into three components: within-task prediction (WTP) using task-specific parameters, task-identity inference (TII) using task-shared parameters, and task-adaptive prediction (TAP) that preserves statistical information of pre-trained representations. The method keeps the backbone frozen and employs parameter-efficient tuning techniques to optimize these components, addressing the challenge of catastrophic forgetting while maintaining computational efficiency.

## Key Results
- Achieves final average accuracies of 91.21% on CIFAR-100, 79.32% on ImageNet-R, 88.76% on CUB-200, and 69.65% on Cars-196
- Demonstrates remarkably superior performance over strong baselines across various CL scenarios
- Shows strong generality across different PET techniques (Prompt, Adapter, LoRA), pre-trained checkpoints, and CL benchmarks

## Why This Works (Mechanism)
HiDe-PET works by explicitly decomposing the continual learning objective into hierarchical components that address different aspects of the learning problem. The within-task prediction component handles task-specific classification using task-specific parameters, while the task-identity inference component uses shared parameters across tasks to recognize task identity. The task-adaptive prediction component preserves statistical information from pre-trained representations to maintain knowledge transfer. This hierarchical approach allows the model to adapt to new tasks while preserving knowledge from previous tasks, effectively mitigating catastrophic forgetting.

## Foundational Learning
- **Parameter-efficient tuning (PET):** Techniques like Prompt, Adapter, and LoRA that modify model behavior with minimal parameter updates, crucial for computational efficiency in CL
  - *Why needed:* Enables efficient adaptation to new tasks without full fine-tuning
  - *Quick check:* Verify parameter count is <5% of total model parameters

- **Continual learning (CL) with pre-trained models:** Framework for learning from sequential tasks while maintaining performance on previous tasks using frozen backbones
  - *Why needed:* Addresses catastrophic forgetting while leveraging pre-trained knowledge
  - *Quick check:* Ensure backbone parameters remain frozen throughout training

- **Hierarchical objective decomposition:** Breaking down the learning objective into WTP, TII, and TAP components
  - *Why needed:* Provides explicit optimization targets for different CL challenges
  - *Quick check:* Verify each component contributes to final accuracy through ablation

- **Task-identity inference:** Recognizing which task a sample belongs to using shared parameters
  - *Why needed:* Enables proper routing of inputs to appropriate task-specific parameters
  - *Quick check:* Measure task classification accuracy during inference

- **Statistical preservation of pre-trained representations:** Maintaining statistical properties from original pre-training
  - *Why needed:* Ensures knowledge transfer and prevents degradation of learned representations
  - *Quick check:* Compare feature distribution statistics across tasks

## Architecture Onboarding

**Component Map:** Backbone (frozen) -> PET module (Prompt/Adapter/LoRA) -> WTP layer -> TII layer -> TAP layer -> Output

**Critical Path:** Input -> Backbone feature extraction -> Task identity inference -> Within-task prediction with task-specific parameters -> Output classification

**Design Tradeoffs:** 
- Frozen backbone provides stability but limits adaptation capability
- Parameter-efficient tuning balances adaptation needs with computational constraints
- Hierarchical decomposition adds complexity but provides explicit optimization targets
- Task-shared vs task-specific parameters tradeoff between knowledge transfer and task specialization

**Failure Signatures:**
- Catastrophic forgetting indicates insufficient preservation of previous task knowledge
- Poor task-identity inference suggests inadequate TII component performance
- Degraded feature representations indicate issues with statistical preservation in TAP
- Computational inefficiency suggests PET technique is not properly optimized

**3 First Experiments:**
1. Test backbone freezing by attempting to update backbone parameters and verifying they remain unchanged
2. Validate task-identity inference by measuring classification accuracy on task recognition
3. Compare performance with and without each hierarchical component (WTP, TII, TAP) through ablation studies

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Evaluation primarily focuses on image classification benchmarks using ViT models, raising questions about generalizability to other domains and architectures
- The practical necessity and optimal balance of all three hierarchical components (WTP, TII, TAP) could be questioned
- Comparison methodology with baselines may not specify identical hyperparameter settings across all methods

## Confidence
- **Cross-domain applicability:** Medium - limited evaluation to image classification tasks
- **Hierarchical component necessity:** Medium - ablation studies show contribution but optimal balance unclear
- **Fairness of comparative evaluation:** Medium - methodology for baseline comparisons could be more detailed

## Next Checks
1. Evaluate HiDe-PET on non-vision tasks (e.g., text classification, sentiment analysis) to verify cross-domain effectiveness and identify any domain-specific limitations

2. Conduct extensive ablation studies varying the relative weighting of WTP, TII, and TAP components across different task sequences to determine optimal configurations and identify potential component redundancy

3. Test the framework's robustness to catastrophic forgetting under extreme conditions: very long task sequences (50+ tasks), high intra-task variance, and scenarios where tasks are revisited after long intervals