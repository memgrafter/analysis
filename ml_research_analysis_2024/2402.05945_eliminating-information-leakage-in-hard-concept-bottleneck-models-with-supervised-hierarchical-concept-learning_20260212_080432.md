---
ver: rpa2
title: Eliminating Information Leakage in Hard Concept Bottleneck Models with Supervised,
  Hierarchical Concept Learning
arxiv_id: '2402.05945'
source_url: https://arxiv.org/abs/2402.05945
tags:
- concepts
- concept
- cbms
- label
- supcbm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses information leakage in Concept Bottleneck
  Models (CBMs), where unintended information beyond the concepts is exploited for
  label prediction, undermining interpretability and intervention. The authors propose
  SUPCBM, a new CBM paradigm that introduces label supervision during concept prediction
  and constructs a hierarchical concept set with perceptual and descriptive concepts.
---

# Eliminating Information Leakage in Hard Concept Bottleneck Models with Supervised, Hierarchical Concept Learning

## Quick Facts
- arXiv ID: 2402.05945
- Source URL: https://arxiv.org/abs/2402.05945
- Authors: Ao Sun; Yuanyuan Yuan; Pingchuan Ma; Shuai Wang
- Reference count: 9
- Primary result: SUPCBM outperforms state-of-the-art CBMs on all datasets and reaches 85.98% accuracy on CUB-Bird, close to the 86.41% achieved by the feature-based model.

## Executive Summary
This paper addresses information leakage in Concept Bottleneck Models (CBMs), where unintended information beyond the concepts is exploited for label prediction, undermining interpretability and intervention. The authors propose SUPCBM, a new CBM paradigm that introduces label supervision during concept prediction and constructs a hierarchical concept set with perceptual and descriptive concepts. SUPCBM uses an intervention matrix to determine which concepts should be involved in predicting each class label, and a concept pooling layer to select the most important concepts for each input. The method significantly reduces information leakage compared to previous CBMs and achieves comparable performance to vanilla feature-based models on diverse datasets (CIFAR-10/100, CUB-Bird, HAM10000).

## Method Summary
SUPCBM introduces a hierarchical concept set construction process using GPT to generate perceptual and descriptive concepts, along with an intervention matrix that determines concept-label relationships. During training, the CB layer is supervised with both concept and label information, forcing it to predict only concepts relevant to the ground truth label. Concept pooling selects the top-k most similar descriptive concepts for each perceptual concept. Inference involves multiplying concept predictions with the intervention matrix to obtain final label predictions. The method combines these elements to eliminate both soft and hard information leakage while maintaining interpretability and performance.

## Key Results
- SUPCBM outperforms state-of-the-art CBMs on all evaluated datasets (CIFAR-10/100, CUB-Bird, HAM10000)
- Achieves 85.98% accuracy on CUB-Bird, close to the 86.41% achieved by the feature-based model
- Significantly reduces information leakage compared to previous CBMs while maintaining interpretability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Label supervision during concept prediction reduces information leakage by forcing the CB layer to only predict concepts relevant to the ground truth label.
- Mechanism: During training, concepts are annotated only with those involved in predicting the ground truth label (determined by the intervention matrix). The CB layer is trained to predict only these relevant concepts, preventing it from learning irrelevant patterns that could leak unintended information.
- Core assumption: The intervention matrix correctly identifies which concepts are relevant for each class label.
- Evidence anchors:
  - [abstract]: "We fuse them into a unified form by additionally supervising the concept prediction with class labels."
  - [section]: "Unlike previous CBMs that treat concept prediction and label prediction as two separate tasks, we fuse them into a unified form by additionally supervising the concept prediction with class labels."
  - [corpus]: Weak - no direct evidence about this specific mechanism in corpus papers.
- Break condition: If the intervention matrix is incorrectly constructed, the CB layer may be forced to ignore relevant concepts or include irrelevant ones, reducing performance and potentially introducing information leakage.

### Mechanism 2
- Claim: The intervention matrix eliminates soft information leakage by ensuring class prediction only relies on concepts uniquely relevant to each class.
- Mechanism: The intervention matrix is a binary matrix indicating which concepts should be involved in predicting each class. During prediction, the final label confidence is computed as the sum of probabilities of involved concepts. This means classes are only distinguished when different concepts are presented, preventing the model from using probability distributions to encode class information.
- Core assumption: The intervention matrix accurately captures the concept-label relationships.
- Evidence anchors:
  - [abstract]: "The intervention matrix is formed when constructing the concept set; it is implemented as a sparse binary matrix of shape #concepts × #classes, where the (i, j)-th entry indicates whether the i-th concept should be leveraged to recognize the j-th class."
  - [section]: "We maintain an intervention matrix to determine which concepts should be involved for a given class label... When training SUPCBM, the CB layer is forced to only predict concepts that are relevant to the ground truth label, which is jointly decided by the intervention matrix."
  - [corpus]: Weak - no direct evidence about this specific mechanism in corpus papers.
- Break condition: If concepts are not truly unique to classes (i.e., shared concepts exist between classes), the intervention matrix approach may not fully eliminate information leakage.

### Mechanism 3
- Claim: Concept pooling reduces hard information leakage by selecting only the most important descriptive concepts for each input.
- Mechanism: After computing similarities between the input and all relevant concepts, concept pooling selects the top-k most similar descriptive concepts for each perceptual concept. This prevents irrelevant concepts from contributing to the label prediction.
- Core assumption: The most similar descriptive concepts are indeed the most relevant for the input.
- Evidence anchors:
  - [abstract]: "a concept pooling layer (which further selects the most important concepts for each input; see Sec. 3.2)."
  - [section]: "we choose to annotate x with those most important concepts. We name our selection procedure as concept pooling... we choose those having the top-k similarity as the ground truth concepts."
  - [corpus]: Weak - no direct evidence about this specific mechanism in corpus papers.
- Break condition: If the similarity measure is not well-aligned with concept relevance, concept pooling may select irrelevant concepts, reducing performance and potentially introducing information leakage.

## Foundational Learning

- Concept: Concept Bottleneck Models (CBMs)
  - Why needed here: Understanding CBMs is fundamental to understanding SUPCBM's improvements and why information leakage is a problem.
  - Quick check question: What are the two key benefits CBMs deliver, and how do soft and hard CBMs differ in their approach?

- Concept: Information Leakage in CBMs
  - Why needed here: Understanding information leakage is crucial to understanding why SUPCBM was developed and how it addresses this issue.
  - Quick check question: What are the two types of information leakage mentioned in the paper, and how do they undermine CBM interpretability?

- Concept: Hierarchical Concept Sets
  - Why needed here: Understanding hierarchical concept sets is important for understanding SUPCBM's concept construction approach and its benefits.
  - Quick check question: How does SUPCBM structure its concept set, and what is the purpose of having both perceptual and descriptive concepts?

## Architecture Onboarding

- Component map: input image -> backbone model -> CB layer -> concept pooling -> concept prediction -> intervention matrix multiplication -> final label prediction
- Critical path: The critical path is: input image → backbone model → CB layer → concept pooling → concept prediction → intervention matrix multiplication → final label prediction.
- Design tradeoffs: SUPCBM trades some complexity (additional components like concept pooling and intervention matrix) for improved interpretability and reduced information leakage. It also requires human effort to define the initial concept set, though this is partially automated with GPT.
- Failure signatures: Performance degradation when the concept set is not well-aligned with the data, or when the intervention matrix incorrectly identifies concept-label relationships. Information leakage may persist if concepts are not truly unique to classes.
- First 3 experiments:
  1. Train SUPCBM on a simple dataset (e.g., CIFAR-10) and evaluate performance and information leakage compared to a baseline CBM.
  2. Vary the number of concepts in the hierarchical concept set and observe the impact on performance and interpretability.
  3. Modify the intervention matrix to incorrectly identify concept-label relationships and observe the impact on performance and information leakage.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed intervention matrix perform when applied to classes that share a large number of concepts?
- Basis in paper: [explicit] The paper discusses the case where classes share concepts (S ≠ ∅) and provides three cases to analyze, but does not provide empirical evidence on how the intervention matrix performs in scenarios with large concept overlap.
- Why unresolved: The paper provides theoretical justification but lacks empirical evidence on the effectiveness of the intervention matrix in scenarios with large concept overlap.
- What evidence would resolve it: Empirical results showing the performance of the intervention matrix when applied to classes with a large number of shared concepts would resolve this question.

### Open Question 2
- Question: How does the concept pooling mechanism handle cases where the most important concepts are not among the top-k selected concepts?
- Basis in paper: [explicit] The paper introduces the concept pooling mechanism to select the most important concepts, but does not discuss how it handles cases where the most important concepts are not among the top-k selected concepts.
- Why unresolved: The paper does not provide information on how the concept pooling mechanism handles cases where the most important concepts are not among the top-k selected concepts.
- What evidence would resolve it: Empirical results showing the performance of the concept pooling mechanism when the most important concepts are not among the top-k selected concepts would resolve this question.

### Open Question 3
- Question: How does the proposed SUPCBM perform when applied to datasets with a large number of classes and a small number of training samples per class?
- Basis in paper: [explicit] The paper evaluates the performance of SUPCBM on datasets with a varying number of classes, but does not specifically discuss the performance on datasets with a large number of classes and a small number of training samples per class.
- Why unresolved: The paper does not provide information on how SUPCBM performs when applied to datasets with a large number of classes and a small number of training samples per class.
- What evidence would resolve it: Empirical results showing the performance of SUPCBM when applied to datasets with a large number of classes and a small number of training samples per class would resolve this question.

## Limitations

- The effectiveness of the intervention matrix depends heavily on correctly identifying unique concept-label relationships, which may not hold in datasets with overlapping concepts between classes.
- The concept pooling mechanism's reliance on similarity measures may not always accurately identify the most relevant concepts for each input.
- The method requires human effort to define the initial concept set, which may be challenging for complex datasets or domains.

## Confidence

- **High Confidence**: The core architecture and training methodology are sound, and SUPCBM demonstrates improved performance over existing CBMs on benchmark datasets.
- **Medium Confidence**: The claims about reducing information leakage are supported by the proposed mechanisms, but the evaluation of information leakage reduction could be more comprehensive.
- **Low Confidence**: The long-term effectiveness of SUPCBM across diverse real-world applications and its robustness to concept set quality variations remain uncertain.

## Next Checks

1. **Intervention Matrix Robustness**: Systematically test SUPCBM performance when the intervention matrix is deliberately constructed with errors (e.g., missing relevant concepts or including irrelevant ones) to quantify the impact on information leakage and model performance.

2. **Cross-Dataset Generalizability**: Evaluate SUPCBM on datasets with different concept structures (e.g., overlapping concepts between classes) to test its effectiveness in eliminating information leakage when the core assumption of unique class-concept relationships is violated.

3. **Concept Pooling Quality Assessment**: Conduct a detailed analysis of the similarity measure used in concept pooling, comparing its ability to select relevant concepts against ground truth annotations across multiple datasets and concept types.