---
ver: rpa2
title: Scalable Ensembling For Mitigating Reward Overoptimisation
arxiv_id: '2406.01013'
source_url: https://arxiv.org/abs/2406.01013
tags:
- reward
- learning
- ensemble
- work
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of reward overoptimization in Reinforcement
  Learning from Human Feedback (RLHF), where policies tend to overfit learned proxy
  reward models past an inflection point of utility. The authors propose a scalable
  ensembling approach using a shared encoder but separate linear heads for reward
  models.
---

# Scalable Ensembling For Mitigating Reward Overoptimisation

## Quick Facts
- arXiv ID: 2406.01013
- Source URL: https://arxiv.org/abs/2406.01013
- Reference count: 9
- One-line primary result: Multi-head reward models with shared encoder achieve similar overoptimization mitigation as full ensembles while requiring significantly less memory and training time

## Executive Summary
This paper addresses reward overoptimization in Reinforcement Learning from Human Feedback (RLHF), where policies trained on proxy reward models overfit and degrade in true utility past an inflection point. The authors propose a scalable multi-head reward model architecture using a shared encoder with separate linear heads, which captures diverse reward representations while sharing common linguistic features. The approach uses a pessimistic minimum function across heads to prevent overoptimization and requires only one epoch of training per head compared to three epochs for full ensemble methods, achieving similar performance with significant computational savings.

## Method Summary
The method uses a shared encoder backbone with multiple linear heads for reward modeling, trained on pairwise preference data using Bradley-Terry loss. Instead of a single reward head, the model incorporates multiple heads that learn distinct reward functions from the same shared feature representation. During policy optimization with PPO, the minimum reward across all heads is used as a pessimistic estimator to prevent overoptimization. This architecture requires only one epoch of training per head compared to three epochs for traditional full ensemble approaches, while maintaining similar mitigation of reward overoptimization as measured by gold reward and KL divergence from the base policy.

## Key Results
- Multi-head reward models mitigate overoptimization as effectively as full ensembles while requiring only one epoch per head versus three epochs
- The shared encoder architecture achieves similar performance to separate models with tremendous savings in memory and training time
- Minimum aggregation across ensemble members provides better calibration and uncertainty capture compared to mean or maximum objectives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The multi-head architecture with shared encoder but separate linear heads captures diverse reward representations while sharing common linguistic features.
- Mechanism: Separate linear heads learn distinct reward functions from shared feature representation, with the shared encoder ensuring efficient use of common features while separate heads enable diversity in reward predictions. Taking the minimum creates a pessimistic estimator that prevents overoptimization.
- Core assumption: Separate linear heads will learn sufficiently diverse reward functions even with shared features, and taking the minimum provides effective regularization.
- Evidence anchors: [abstract] states similar performance to full ensemble with savings in memory and time; [section 3.2.1] describes the multi-head structure; corpus analysis found 25 related papers but limited direct evidence on multi-head reward models.

### Mechanism 2
- Claim: The pessimistic minimum function over ensemble members provides better calibration and prevents overoptimistic reward predictions.
- Mechanism: Taking the minimum reward across ensemble members creates a conservative reward estimate that helps prevent the policy from exploiting spurious patterns and reduces reward hacking.
- Core assumption: The minimum function is an effective way to capture epistemic uncertainty that is well-calibrated with respect to the true reward.
- Evidence anchors: [abstract] mentions taking the minimum over the ensemble; [section 4.6] shows enhanced ability to capture uncertainty with minimum objective; corpus analysis found some evidence from RL literature on pessimistic estimates.

### Mechanism 3
- Claim: The multi-head approach requires fewer training epochs per head compared to full ensemble training while achieving similar performance.
- Mechanism: Sharing encoder parameters reduces computational cost, with the paper showing only one epoch per head is needed versus three for full ensemble, suggesting more efficient learning dynamics.
- Core assumption: The shared encoder provides sufficient feature representation for all heads, and training efficiency translates to similar or better performance with fewer epochs.
- Evidence anchors: [abstract] states one epoch per head vs three for full ensemble; [section 4.6] confirms multi-head approach needs only one epoch; corpus analysis found limited direct evidence on training efficiency comparisons.

## Foundational Learning

- Concept: Reward Model Overoptimization
  - Why needed here: Understanding why policies overfit to proxy reward models is essential to grasp the problem this paper addresses, showing training reaches an inflection point where increasing proxy reward degrades gold reward performance.
  - Quick check question: What phenomenon occurs when policies trained on proxy reward models show degraded performance on gold reward models despite both being consistent with human labels?

- Concept: Ensemble Methods in RLHF
  - Why needed here: The paper builds on ensemble approaches but proposes a more efficient variant, making it crucial to understand how traditional ensembles work in RLHF contexts.
  - Quick check question: How do traditional ensemble approaches in RLHF differ from the multi-head approach proposed in this paper in terms of computational requirements?

- Concept: Proximal Policy Optimization (PPO)
  - Why needed here: PPO is the RL algorithm used to train policies against the reward models, and understanding its objective and KL constraint is important for the experimental setup.
  - Quick check question: What is the role of the KL constraint in PPO, and why is it particularly important when training language models?

## Architecture Onboarding

- Component map:
  Base encoder -> Multiple linear heads -> Minimum function -> PPO trainer

- Critical path:
  1. SFT model → shared encoder initialization
  2. Reward learning → train multiple linear heads on shared encoder
  3. Minimum aggregation → compute pessimistic reward estimate
  4. PPO training → optimize policy using ensemble reward

- Design tradeoffs:
  - Memory efficiency vs. ensemble diversity: Multi-head uses less memory than full ensemble but may have less diversity than completely separate models
  - Training efficiency vs. performance: One epoch per head vs. three for full ensemble, with similar performance outcomes
  - Pessimism vs. reward accuracy: Minimum function prevents overoptimization but may be overly conservative

- Failure signatures:
  - All heads converge to similar values (indicating lack of diversity)
  - Performance matches or is worse than single reward model
  - High variance in head predictions (indicating training instability)
  - PPO fails to learn or shows poor convergence

- First 3 experiments:
  1. Train single reward model vs. multi-head with 2 heads on small dataset to verify minimum function changes behavior
  2. Test multi-head with varying numbers of heads (2, 3, 5) to find optimal ensemble size
  3. Compare training dynamics: one epoch per head vs. three epochs for full ensemble on same computational budget

## Open Questions the Paper Calls Out

- Question: What is the optimal number of linear heads in the multi-head reward model architecture for mitigating reward overoptimization?
  - Basis in paper: [explicit] The paper states "Our experiments suggest that a smaller number of heads (3) might be optimal for mitigating overoptimisation" and discusses how "An increase in the number of heads beyond a certain threshold might introduce more noise than beneficial diversity."
  - Why unresolved: While the paper suggests 3 heads may be optimal based on their experiments, they do not provide a rigorous analysis of how performance scales with the number of heads.
  - What evidence would resolve it: A systematic study varying the number of heads from 1 to 10+ and measuring performance on both gold and proxy rewards across multiple datasets and model sizes.

- Question: How does the performance of the multi-head reward model compare to other ensemble methods like Bayesian uncertainty estimation or LoRA-based approaches?
  - Basis in paper: [inferred] The paper mentions concurrent work on Bayesian uncertainty estimation and LoRA approaches, but only compares to a full ensemble with separate models.
  - Why unresolved: The paper only compares against one specific baseline (full ensemble), while other recent approaches exist but are not directly compared.
  - What evidence would resolve it: Head-to-head comparisons of multi-head, Bayesian, and LoRA approaches on the same datasets and models, measuring both performance and computational efficiency.

- Question: Does the multi-head reward model architecture provide benefits beyond mitigating overoptimization, such as improved calibration or robustness to label noise?
  - Basis in paper: [explicit] The paper discusses calibration analysis showing the multi-head model "demonstrate[s] an enhanced ability to capture uncertainty in the reward signals."
  - Why unresolved: While the paper shows improved calibration, they don't explore whether this translates to robustness to label noise or other failure modes.
  - What evidence would resolve it: Experiments testing the multi-head model's performance on increasingly noisy datasets and measuring both overoptimization and other metrics like calibration error.

## Limitations

- The evaluation focuses primarily on gold reward and KL divergence metrics without extensive human preference studies to validate practical utility gains
- Results are demonstrated on relatively small language models (1.3B parameters) compared to modern frontier models, leaving questions about scalability to larger architectures
- Claims about training efficiency are somewhat limited by lack of ablation studies on optimal head count and comparison with other ensemble variants

## Confidence

- **High confidence**: The multi-head architecture successfully mitigates reward overoptimization as demonstrated by tracking gold reward vs KL divergence curves
- **Medium confidence**: The claimed training efficiency gains (1 epoch vs 3 epochs) are well-supported but would benefit from broader architectural comparisons
- **Medium confidence**: The minimum aggregation function provides better calibration, though the evidence is primarily from synthetic uncertainty assessment rather than real-world preference data

## Next Checks

1. Conduct ablation studies varying the number of heads (2, 3, 5, 10) to determine optimal ensemble size and test whether diversity degrades with more heads sharing the same encoder
2. Compare the multi-head approach against alternative ensemble methods (bootstrap aggregation, Monte Carlo dropout) using identical computational budgets to isolate efficiency benefits
3. Run human preference evaluations on policy outputs to verify that the theoretical improvements in gold reward and KL divergence translate to practical quality gains in user experience