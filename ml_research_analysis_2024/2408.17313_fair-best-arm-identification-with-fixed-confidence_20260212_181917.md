---
ver: rpa2
title: Fair Best Arm Identification with Fixed Confidence
arxiv_id: '2408.17313'
source_url: https://arxiv.org/abs/2408.17313
tags:
- fairness
- fair
- constraints
- psum
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Fair Best Arm Identification (F-BAI), a novel
  framework that integrates fairness constraints into the classical BAI problem. The
  proposed F-TAS algorithm achieves optimal sample complexity while satisfying fairness
  constraints, which can be either pre-specified or model-dependent.
---

# Fair Best Arm Identification with Fixed Confidence

## Quick Facts
- arXiv ID: 2408.17313
- Source URL: https://arxiv.org/abs/2408.17313
- Authors: Alessio Russo; Filippo Vannella
- Reference count: 40
- Primary result: F-TAS algorithm achieves optimal sample complexity while satisfying fairness constraints in fair best arm identification

## Executive Summary
This paper introduces Fair Best Arm Identification (F-BAI), a novel framework that integrates fairness constraints into the classical best arm identification problem. The authors propose F-TAS, a Track-and-Stop algorithm that provably matches the sample complexity lower bound while ensuring fairness constraints are satisfied. The framework allows for both pre-specified and model-dependent fairness constraints, and establishes a sample complexity lower bound that quantifies the price of fairness. Experimental results demonstrate F-TAS outperforms baseline algorithms in both synthetic and wireless scheduling scenarios.

## Method Summary
The F-TAS algorithm combines a sampling rule that tracks the optimal allocation vector with a stopping rule based on a generalized-likelihood ratio test. The algorithm estimates the optimal allocation in each round using current empirical means, then takes a convex combination with a constant exploration policy to ensure sufficient exploration. A stopping rule using a GLRT test and threshold function decides when to stop and output the best arm estimate. The framework allows for both pre-specified fairness constraints and model-dependent constraints that depend on the true parameter vector.

## Key Results
- F-TAS achieves optimal sample complexity matching the lower bound while satisfying fairness constraints
- The price of fairness scales as (1-psum)^(-1) or (pmin)^(-1) depending on constraint type
- F-TAS outperforms TAS and UNIFORM_FAIR baselines in both synthetic and wireless scheduling experiments
- The algorithm ensures fairness for all rounds with pre-specified constraints and asymptotically for model-dependent constraints

## Why This Works (Mechanism)

### Mechanism 1
The F-TAS algorithm achieves optimal sample complexity while satisfying fairness constraints by tracking the optimal allocation vector w* that solves the lower bound optimization problem (3). The algorithm estimates the optimal allocation in each round using the current empirical mean ˆθ(t), then takes a convex combination with a constant exploration policy πc to ensure each arm is sampled sufficiently often. This tracking approach asymptotically converges to the optimal allocation w* that minimizes sample complexity while respecting fairness constraints.

### Mechanism 2
The sample complexity lower bound quantifies the price of fairness, showing how fairness constraints increase the sample complexity compared to unconstrained BAI. The lower bound optimization problem (3) includes a clipped simplex Σp that accounts for fairness constraints wa ≥ pa. This constrained optimization yields a higher characteristic time T*_p ≥ T* compared to the unconstrained case, quantifying the additional samples needed for fairness.

### Mechanism 3
The stopping rule using a generalized-likelihood ratio test (GLRT) Z(t) and a threshold function β(δ,t) ensures δ-PAC guarantees while maintaining fairness. The GLRT Z(t) = t/2T*_p(t) compares the current sample size to the optimal characteristic time. The threshold function β(δ,t) increases with t and decreases with δ, ensuring the algorithm stops with high probability when the best arm is identified.

## Foundational Learning

- **Concept:** Multi-armed bandit problem with fixed confidence
  - Why needed here: The fair BAI problem extends the classical MAB framework, so understanding the basic MAB setup is essential.
  - Quick check question: In a MAB problem with K arms, what is the objective and how is it typically measured?

- **Concept:** Best arm identification (BAI) with fixed confidence
  - Why needed here: Fair BAI is a variant of BAI, so understanding the classical BAI problem is crucial.
  - Quick check question: What is the difference between regret minimization and best arm identification in the MAB setting?

- **Concept:** Fairness constraints in sequential decision-making
  - Why needed here: Fair BAI introduces fairness constraints, so understanding how fairness is defined and measured in sequential settings is important.
  - Quick check question: What are some common notions of fairness in machine learning, and how might they apply to bandit problems?

## Architecture Onboarding

- **Component map:**
  - Sampling rule: Tracks optimal allocation w* using empirical estimates and convex combination with exploration policy
  - Stopping rule: Uses GLRT Z(t) and threshold function β(δ,t) to decide when to stop
  - Decision rule: Outputs the arm with highest empirical mean at stopping time
  - Fairness constraints: Either pre-specified (pa) or θ-dependent (pa(θ))

- **Critical path:**
  1. Initialize algorithm with fairness vector p and confidence δ
  2. In each round t:
     a. Compute optimal allocation w*_p(t) using current empirical estimates
     b. Sample arm according to convex combination of w*_p(t) and exploration policy
     c. Update empirical estimates and allocation counts
     d. Compute GLRT Z(t) and compare to threshold β(δ,t)
     e. If Z(t) ≥ β(δ,t), stop and output best arm estimate
  3. Return the arm with highest empirical mean at stopping time

- **Design tradeoffs:**
  - Exploration vs. exploitation: The convex combination with exploration policy ensures sufficient exploration but may increase sample complexity
  - Pre-specified vs. θ-dependent fairness: Pre-specified constraints are easier to satisfy but less flexible, while θ-dependent constraints are more flexible but require asymptotic guarantees
  - Sample complexity vs. fairness violation: Stricter fairness constraints may increase sample complexity but reduce fairness violations

- **Failure signatures:**
  - High fairness violations: May indicate insufficient exploration or overly restrictive fairness constraints
  - Excessive sample complexity: May indicate poor tracking of optimal allocation or overly conservative stopping rule
  - Failure to identify best arm: May indicate insufficient confidence parameter δ or poor calibration of threshold function

- **First 3 experiments:**
  1. Test F-TAS on a simple 2-armed bandit with pre-specified fairness constraints to verify basic functionality and fairness guarantees
  2. Vary the fairness parameter psum to observe the impact on sample complexity and fairness violations
  3. Compare F-TAS to baseline algorithms (TAS and UNIFORM_FAIR) on a larger bandit instance to evaluate performance gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the F-BAI framework be extended to combinatorial bandits where multiple arms can be selected simultaneously?
- Basis in paper: [inferred] The authors mention this as a future research direction, noting that analyzing their fair bandit framework for such combinatorial bandit structures is out of the scope of this paper.
- Why unresolved: The current F-TAS algorithm is designed for single arm selection per round. Extending it to combinatorial settings would require new algorithmic approaches to handle the increased complexity of selecting multiple arms while maintaining fairness constraints.
- What evidence would resolve it: A proof-of-concept algorithm that extends F-TAS to combinatorial settings, along with experimental results demonstrating its effectiveness in maintaining fairness while identifying the best arm combination.

### Open Question 2
- Question: Can the F-BAI framework be adapted to incorporate counterfactual fairness, where decisions are evaluated based on how they would change if an individual belonged to a different group?
- Basis in paper: [inferred] The authors discuss various fairness concepts in the literature but do not explore counterfactual fairness within their framework.
- Why unresolved: Counterfactual fairness requires considering hypothetical scenarios where an individual's group membership is changed, which is not directly addressed by the current fairness constraints in F-BAI.
- What evidence would resolve it: A formal definition of counterfactual fairness within the F-BAI framework, along with theoretical analysis and experimental results showing how it impacts sample complexity and fairness guarantees.

### Open Question 3
- Question: How does the price of fairness scale in the case of large-scale bandit problems with a very large number of arms (K >> 1)?
- Basis in paper: [inferred] The authors provide bounds on the price of fairness but do not extensively explore the behavior in large-scale settings.
- Why unresolved: The current analysis focuses on general bounds, but the behavior of the price of fairness in large-scale scenarios, where the number of arms is extremely large, remains unclear.
- What evidence would resolve it: Theoretical analysis and experimental results demonstrating the scaling of the price of fairness in large-scale bandit problems, potentially revealing new insights or patterns not captured by the current bounds.

## Limitations
- The framework assumes Gaussian reward distributions, limiting applicability to other reward types
- The computational complexity of solving the optimization problem (3) in each round is not addressed
- The fairness metrics used may not capture all relevant notions of fairness in practice

## Confidence
- **High confidence:** Theoretical framework and sample complexity bounds
- **Medium confidence:** Experimental results demonstrating F-TAS's superior performance
- **Low confidence:** Claim that F-TAS achieves asymptotically fair sampling for θ-dependent constraints

## Next Checks
1. Implement F-TAS with sub-Gaussian reward distributions (e.g., Bernoulli) to assess the robustness of the theoretical guarantees and experimental performance.
2. Conduct ablation studies to quantify the impact of the forced exploration schedule and the tracking of the optimal allocation on sample complexity and fairness violations.
3. Evaluate F-TAS on a real-world wireless scheduling dataset to validate the practical applicability of the framework and compare against domain-specific scheduling algorithms.