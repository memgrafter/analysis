---
ver: rpa2
title: 'Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates'
arxiv_id: '2410.07137'
source_url: https://arxiv.org/abs/2410.07137
tags:
- output
- response
- preprint
- outputs
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Automatic LLM benchmarks like AlpacaEval 2.0, Arena-Hard-Auto,\
  \ and MT-Bench use LLM-based auto-annotators to evaluate language models cost-effectively.\
  \ This work demonstrates that even a null model outputting a constant irrelevant\
  \ response can achieve high win rates\u201486.5% LC win rate on AlpacaEval 2.0,\
  \ 83.0 on Arena-Hard-Auto, and 9.55 on MT-Bench\u2014by exploiting structural weaknesses\
  \ in the evaluation templates."
---

# Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates

## Quick Facts
- arXiv ID: 2410.07137
- Source URL: https://arxiv.org/abs/2410.07137
- Reference count: 40
- Primary result: Null models can achieve high win rates on automatic LLM benchmarks by exploiting structural weaknesses in evaluation templates

## Executive Summary
This paper demonstrates that automatic LLM benchmarks like AlpacaEval 2.0, Arena-Hard-Auto, and MT-Bench can be systematically cheated by null models that output constant, irrelevant responses. The authors show that even without access to private benchmark instructions, a transferable adversarial prefix combined with a structured cheating response can achieve remarkably high win rates—86.5% on AlpacaEval 2.0, 83.0% on Arena-Hard-Auto, and 9.55 on MT-Bench. The vulnerability stems from structural weaknesses in how auto-annotators parse evaluation templates, which can be exploited through random search optimization of adversarial prefixes.

## Method Summary
The method exploits structural weaknesses in automatic LLM benchmark evaluation templates by crafting structured cheating responses that override original instruction-output triplets. The authors use random search optimization to create transferable adversarial prefixes from public instruction sets, assuming benchmark instructions are private. The approach combines syntactic manipulation of template parsing with position bias exploitation, where responses in certain positions receive preferential treatment from auto-annotators. The optimized null model is then evaluated across multiple benchmarks using both GPT-4 and open-source auto-annotators.

## Key Results
- Null models achieved 86.5% LC win rate on AlpacaEval 2.0, 83.0% on Arena-Hard-Auto, and 9.55 score on MT-Bench
- Cheating response is transferable without access to private benchmark instructions
- Random search optimization of adversarial prefixes enhances cheating effectiveness
- Position bias in auto-annotators can be exploited by strategic response placement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The structured response exploits LLM auto-annotators' syntactic analysis weakness in evaluation templates.
- Mechanism: The response overrides the original instruction-output triplet with a fabricated one, exploiting position bias to guide auto-annotator judgment.
- Core assumption: LLMs parsing the template become confused when the structured response replaces the expected format.
- Evidence anchors:
  - [abstract] "we show that even a 'null model' that always outputs a constant response (irrelevant to input instructions) can cheat automatic benchmarks and achieve top-ranked win rates"
  - [section] "our cheating strategy involves replacing the original comparison with a misleading one, which disrupts the auto-annotator's syntactic analysis of the evaluation template"
  - [corpus] Weak - related papers don't address template parsing vulnerabilities
- Break condition: If auto-annotator template parsing becomes robust to structural manipulation or if position bias is eliminated.

### Mechanism 2
- Claim: Random search optimization of adversarial prefixes enhances cheating effectiveness through transferable prefixes.
- Mechanism: A transferable prefix is crafted using public instructions and optimized through token-level random search to minimize aggregated loss across instructions.
- Core assumption: A prefix optimized on public data will generalize to private benchmark instructions.
- Evidence anchors:
  - [abstract] "we assume that the instructions of these benchmarks...are private and cannot be accessed. Thus, we craft a transferable prefix using a public set of instructions"
  - [section] "We utilize an RS algorithm to optimize the adversarial prefix...by sampling modifications and selecting the variant that minimizes the aggregated loss across multiple instructions"
  - [corpus] Weak - no direct evidence about prefix transferability in related work
- Break condition: If random search optimization fails to produce transferable prefixes or if benchmarks use completely different instruction distributions.

### Mechanism 3
- Claim: Position bias in auto-annotators causes preference for responses in certain positions within evaluation templates.
- Mechanism: When the structured response is placed in default position, auto-annotator predicts "M"; when swapped, it predicts "m" due to overwriting behavior.
- Core assumption: Auto-annotators have inherent position bias that can be exploited through strategic response placement.
- Evidence anchors:
  - [section] "when the submitted response is positioned last, the annotator predicts 'M'. Conversely, when it appears in the first position, the annotator predicts 'm'"
  - [section] "Regarding the positional bias, the Llama-3-8B-Instruct shows little position bias as the probabilities for both default and swapped positions are fairly close. In contrast, Llama-3-70B-Instruct shows a clear positional bias"
  - [corpus] Weak - related papers don't discuss position bias exploitation
- Break condition: If position bias is eliminated through benchmark design changes or if auto-annotators become immune to positional manipulation.

## Foundational Learning

- Concept: Evaluation template structure and parsing
  - Why needed here: Understanding how LLMs parse evaluation templates is crucial for identifying vulnerabilities that can be exploited
  - Quick check question: What happens when the structured response replaces the original instruction-output triplet in the evaluation template?

- Concept: Random search optimization techniques
  - Why needed here: The paper uses random search to optimize adversarial prefixes, requiring understanding of optimization algorithms
  - Quick check question: How does the random search algorithm select the best prefix variant from sampled modifications?

- Concept: Transferability of adversarial examples
  - Why needed here: The paper assumes prefixes optimized on public data will transfer to private benchmark instructions
  - Quick check question: What factors determine whether an adversarial prefix optimized on one instruction set will work on a different instruction set?

## Architecture Onboarding

- Component map: Null model generator -> Auto-annotator interface -> Random search optimizer -> Evaluation metrics
- Critical path: Structured response generation → Random search optimization → Auto-annotator querying → Win rate calculation
- Design tradeoffs: Manual crafting of structured responses vs. automated generation; optimization on public vs. private instructions
- Failure signatures: Low win rates indicate ineffective structured responses or poor prefix optimization; high variance suggests instability in the optimization process
- First 3 experiments:
  1. Test basic structured response effectiveness on AlpacaEval 2.0 without optimization
  2. Implement random search optimization on a small instruction set and measure improvement
  3. Compare transferability by testing optimized prefixes on held-out instructions vs. training instructions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust are automatic LLM benchmarks against more sophisticated adversarial attacks beyond the structured null model approach?
- Basis in paper: [explicit] The authors note that while their experiments are proof-of-concept, a determined adversary could leverage LLMs to generate more subtle and imperceptible cheating responses.
- Why unresolved: The current study only demonstrates the effectiveness of a structured null model response, but does not explore more complex adversarial strategies that could be employed by a sophisticated attacker.
- What evidence would resolve it: Experiments testing the effectiveness of various advanced adversarial techniques (e.g., adversarial suffix optimization, jailbreaking, prompt injection) against multiple automatic LLM benchmarks would provide insights into the robustness of these benchmarks.

### Open Question 2
- Question: Can template paraphrasing or other anti-cheating mechanisms effectively prevent the exploitation of structural weaknesses in automatic LLM benchmarks?
- Basis in paper: [explicit] The authors tested template paraphrasing as a potential defense mechanism but found that it was insufficient to prevent the cheat from achieving high win rates.
- Why unresolved: While the authors demonstrate the ineffectiveness of template paraphrasing, they do not explore other potential anti-cheating mechanisms or combinations of defenses that could be more effective.
- What evidence would resolve it: Comprehensive testing of various anti-cheating mechanisms (e.g., adversarial training, anomaly detection, human-in-the-loop evaluation) against a range of adversarial attacks would provide insights into the most effective defenses.

### Open Question 3
- Question: How do the vulnerabilities of automatic LLM benchmarks vary across different model sizes and architectures?
- Basis in paper: [inferred] The authors observe that the structured response is less effective on smaller Llama-3 models compared to GPT-4, suggesting that model size and architecture may play a role in susceptibility to adversarial attacks.
- Why unresolved: The study only tests the cheat against a limited set of models (GPT-4, Llama-3-8B, Llama-3-70B) and does not explore how the vulnerabilities may vary across a wider range of model sizes and architectures.
- What evidence would resolve it: Systematic testing of the cheat against a diverse set of models with varying sizes and architectures (e.g., GPT-3.5, Claude, PaLM, BLOOM) would provide insights into the relationship between model characteristics and vulnerability to adversarial attacks.

## Limitations

- The transferability assumption remains largely untested empirically - prefixes optimized on public data may not generalize well to private benchmark instructions
- The paper doesn't explore whether auto-annotator providers could easily patch these vulnerabilities through template parsing modifications or adversarial training
- Limited testing across different model architectures and sizes prevents understanding how vulnerabilities vary with model characteristics

## Confidence

- **Medium**: The exploitation of template parsing vulnerabilities - while the structured response concept is sound, the specific implementation details and effectiveness across different auto-annotator architectures need verification
- **Low**: The random search optimization's effectiveness and transferability - insufficient evidence provided about optimization convergence and cross-instruction generalization
- **Medium**: Position bias exploitation - preliminary evidence exists but needs broader testing across more auto-annotator models and template variations

## Next Checks

1. **Transferability Stress Test**: Measure prefix effectiveness degradation when trained on subsets of UltraFeedback vs. full dataset, and when tested on instruction distributions significantly different from training data

2. **Template Robustness Evaluation**: Systematically modify evaluation template parsing logic (e.g., add content validation, remove position bias) and measure impact on cheating success rates

3. **Cross-Annotator Consistency Analysis**: Test whether cheating responses optimized for one auto-annotator (e.g., GPT-4) maintain effectiveness across different auto-annotator architectures (e.g., Llama-3 variants, Claude)