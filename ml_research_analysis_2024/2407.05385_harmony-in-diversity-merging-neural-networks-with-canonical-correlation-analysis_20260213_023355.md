---
ver: rpa2
title: 'Harmony in Diversity: Merging Neural Networks with Canonical Correlation Analysis'
arxiv_id: '2407.05385'
source_url: https://arxiv.org/abs/2407.05385
tags:
- merging
- merge
- layer
- neurons
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of merging neural network models
  by combining their parameters without significant loss in performance. The authors
  propose CCA Merge, a novel method based on Canonical Correlation Analysis (CCA)
  that aims to maximize correlations between linear combinations of model features,
  rather than relying on restrictive one-to-one neuron mappings.
---

# Harmony in Diversity: Merging Neural Networks with Canonical Correlation Analysis

## Quick Facts
- arXiv ID: 2407.05385
- Source URL: https://arxiv.org/abs/2407.05385
- Reference count: 30
- Primary result: CCA Merge outperforms permutation-based methods for merging neural networks, achieving higher accuracy and stability, especially when combining multiple models

## Executive Summary
This paper introduces CCA Merge, a novel method for merging neural network models that uses Canonical Correlation Analysis (CCA) to align representations in a maximally correlated space, rather than relying on restrictive one-to-one neuron mappings. By finding transformations that align features distributed across multiple neurons, CCA Merge can effectively combine models trained on the same or different data splits. Experiments on CIFAR10, CIFAR100, and ImageNet demonstrate that CCA Merge significantly outperforms existing permutation-based methods, particularly in the challenging task of merging multiple models.

## Method Summary
CCA Merge aligns neural network features by computing projection matrices using CCA on neuron activations from merging layers of models being combined. The method identifies maximally correlated linear combinations of neurons across models, applies transformations to align them in a common representation space, then averages the aligned parameters. This approach captures distributed features rather than enforcing exact neuron correspondence, enabling better alignment of complementary information learned by different models.

## Key Results
- CCA Merge achieves higher accuracy than permutation-based methods when merging pairs of VGG11 models on CIFAR10
- Maintains superior performance across different model widths (×1, ×2, ×4, ×8) with more stable results
- Significantly outperforms past methods in multi-model merging, maintaining accuracy even as the number of models increases
- Successfully combines models trained on disjoint data splits, beating second-best methods by approximately 4% and 2% on first and second data splits respectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CCA Merge finds better alignments between neural network features by modeling correlations between linear combinations of neurons rather than enforcing one-to-one neuron mappings.
- Mechanism: CCA identifies projection matrices that transform neuron activations into a common space where linear combinations of neurons are maximally correlated, allowing distributed features to be matched across models.
- Core assumption: Useful features learned by neural networks can be represented as linear combinations of neurons rather than requiring exact one-to-one correspondence.
- Evidence anchors:
  - [abstract]: "CCA Merge, a novel method based on Canonical Correlation Analysis (CCA) that aims to maximize correlations between linear combinations of model features"
  - [section 3.2]: "CCA finds projection matrices PA i and PB i which bring the neural activations XA i and XB i respectively from their original representation spaces into a new, common, representation space"
- Break condition: If neural network features are inherently discrete and require exact one-to-one correspondence, or if linear combinations identified by CCA do not correspond to meaningful features.

### Mechanism 2
- Claim: CCA Merge maintains better performance when merging multiple models because it creates more consistent indirect matchings between neurons across the model population.
- Mechanism: When merging multiple models, CCA Merge creates transformations that are more stable across different reference choices, keeping indirect matching between models closer to direct matching compared to permutation-based methods.
- Core assumption: Consistent indirect and direct matchings indicate that CCA Merge is finding stable, meaningful feature correspondences across the model population.
- Evidence anchors:
  - [section 4.4]: "CCA Merge generates fewer feature mismatches in the multi-model setting, explaining in part its success over permutation-based methods"
  - [section 4.4]: "the Frobenius norm of the differences between the direct and indirect matching matrices is significantly lower for CCA Merge than for a permutation-based method"
- Break condition: If stability of indirect vs direct matchings does not correlate with actual model performance, or if feature correspondences identified by CCA are not meaningful for the task.

### Mechanism 3
- Claim: CCA Merge can successfully combine learned features from models trained on disjoint data splits by finding common representations that capture complementary information.
- Mechanism: When models are trained on different subsets of data, they learn different but complementary features. CCA Merge identifies the common representation space where these complementary features align, allowing successful merging.
- Core assumption: Models trained on disjoint data splits learn complementary features that can be aligned in a common representation space.
- Evidence anchors:
  - [section 4.5]: "CCA Merge outperforms the other methods, beating the second best method by approximately 4% and approximately 2% on the first and second data splits respectively"
- Break condition: If models trained on disjoint data splits learn fundamentally incompatible features that cannot be meaningfully aligned, or if complementary information cannot be combined without destroying task-relevant representations.

## Foundational Learning

- Concept: Canonical Correlation Analysis (CCA)
  - Why needed here: CCA is the core mathematical technique that enables finding maximally correlated linear combinations of neurons across different models, essential for the alignment process.
  - Quick check question: What is the optimization objective of CCA when aligning two sets of neural activations?

- Concept: Neural network invariance to permutations
  - Why needed here: Understanding that neural networks are invariant to neuron order is crucial for recognizing why permutation-based methods work and why CCA Merge's approach is different.
  - Quick check question: Why can we permute neurons within a layer without changing the function learned by a neural network?

- Concept: Linear mode connectivity
  - Why needed here: This concept explains why direct parameter averaging typically fails and why alignment methods like CCA Merge are necessary for successful model merging.
  - Quick check question: What condition must be satisfied for two neural network minima to be linearly mode connected?

## Architecture Onboarding

- Component map: Model loading -> Feature alignment (CCA Merge) -> Merging component -> Evaluation component
- Critical path: Load models → Compute activations on shared dataset → Compute CCA projection matrices → Apply transformations to align models → Average aligned parameters → Evaluate merged model performance
- Design tradeoffs: CCA Merge trades computational cost (requires computing activations on shared data) for alignment flexibility (can align distributed features rather than just one-to-one neurons). This is more expensive than permutation-based methods but yields better performance, especially for multiple models.
- Failure signatures: Poor performance may indicate: insufficient data for computing reliable CCA statistics, incompatible model architectures, or features that cannot be meaningfully aligned across models. Memory issues may occur with very wide models or large activation matrices.
- First 3 experiments:
  1. Merge two VGG11 models trained on CIFAR10 using CCA Merge with varying regularization parameters (γ) to find optimal alignment.
  2. Compare CCA Merge performance against Direct Averaging and Permute for VGG11 models of different widths (×1, ×2, ×4, ×8).
  3. Test CCA Merge on merging three ResNet20 models trained on CIFAR100 to verify multi-model merging stability.

## Open Questions the Paper Calls Out
- What is the optimal regularization parameter γ for CCA Merge across different architectures and datasets?
- How does CCA Merge perform when merging models with different architectures?
- What is the relationship between the number of neurons matched indirectly through a reference model versus directly in multi-model merging?

## Limitations
- Computational complexity scales quadratically with layer width due to correlation matrix computations
- Requires access to model activations on shared data, which may not always be available
- Regularization parameter γ selection requires validation on extra models, potentially limiting practical applicability

## Confidence
- Mechanism 1 (distributed feature alignment via CCA): High confidence - well-supported by mathematical formulation and experimental results
- Mechanism 2 (multi-model merging stability): Medium confidence - supported by indirect vs direct matching analysis, but correlation with actual performance could be stronger
- Mechanism 3 (disjoint data merging): Low confidence - limited experimental validation; only tested on two data splits with moderate improvements

## Next Checks
1. Conduct runtime and memory complexity analysis comparing CCA Merge against permutation-based methods across different model widths and layer sizes
2. Test sensitivity of CCA Merge performance to the regularization parameter γ across multiple random seeds and data splits
3. Evaluate CCA Merge on models trained with fundamentally different architectures or on completely disjoint label sets to test the limits of feature alignment capability