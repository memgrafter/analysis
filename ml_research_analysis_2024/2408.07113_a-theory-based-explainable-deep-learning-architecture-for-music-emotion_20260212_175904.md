---
ver: rpa2
title: A Theory-Based Explainable Deep Learning Architecture for Music Emotion
arxiv_id: '2408.07113'
source_url: https://arxiv.org/abs/2408.07113
tags:
- emotion
- music
- features
- filters
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a theory-based, explainable deep learning convolutional
  neural network (CNN) classifier to predict the time-varying emotional response to
  music. Novel CNN filters leverage the frequency harmonics structure from acoustic
  physics known to impact the perception of musical features.
---

# A Theory-Based Explainable Deep Learning Architecture for Music Emotion

## Quick Facts
- arXiv ID: 2408.07113
- Source URL: https://arxiv.org/abs/2408.07113
- Reference count: 27
- Primary result: Theory-based CNN with harmonics filters achieves comparable performance to atheoretical models (F1=0.51 vs 0.48) while being more parsimonious

## Executive Summary
This paper develops a theory-based explainable deep learning CNN classifier for music emotion prediction that leverages acoustic physics principles. The novel approach uses harmonics-based filters that directly model consonance and dissonance patterns known to impact emotional response, providing both competitive predictive performance and enhanced interpretability through Grad-CAM visualizations. The model achieves comparable results to atheoretical deep learning approaches while using significantly fewer parameters, demonstrating that incorporating domain knowledge about music theory can improve both efficiency and explainability in emotion classification tasks.

## Method Summary
The method combines audio preprocessing with a custom CNN architecture featuring harmonics-based filters. Raw audio waveforms (6 seconds) are converted to mel spectrograms (256 mel bands) using STFT with 4,096 window size and 512 hop length. The CNN uses 12 pitch-class-specific filters (A through G#) with non-contiguous frequency bands designed to capture harmonic relationships. Each filter has 32 channels, followed by batch normalization, ReLU activation, average pooling, dropout, and concatenation. The architecture produces probability distributions across four valence-arousal quadrants (Q1: Exuberance, Q2: Anxiety, Q3: Sadness, Q4: Contentment), trained using cross-entropy loss with 10-fold cross-validation on combined Soundtracks (360 excerpts) and DEAM (1,802 excerpts) datasets.

## Key Results
- Theory-based CNN achieves F1-score of 0.51, comparable to atheoretical models (0.48) and better than handcrafted features
- Model uses 100,000 trainable parameters versus 5-10 million for atheoretical CNN variants
- Grad-CAM visualizations show expected consonance patterns (high brightness for Q4, low for Q2) supporting the harmonics-emotion theory
- In digital advertising application, theory-based model produces lower skip rates and higher brand recall than atheoretical alternatives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Harmonics-based CNN filters capture non-contiguous frequency relationships critical for emotional prediction
- Mechanism: Filters focus on specific harmonic frequency bands (integer multiples of fundamental frequency) to model consonance/dissonance features
- Core assumption: Emotional response to music is strongly linked to consonance/dissonance determined by harmonic alignment
- Evidence anchors: Abstract mentions harmonics explainability; section details non-contiguous filter construction; corpus evidence is weak
- Break condition: If emotional response is not primarily driven by consonance/dissonance patterns

### Mechanism 2
- Claim: Grad-CAM visualizations reveal consonance patterns aligning with emotional classification
- Mechanism: Brightness values in heatmaps show higher values for Q4 (positive valence, low arousal) and lower for Q2 (negative valence, high arousal)
- Core assumption: Grad-CAM brightness accurately reflects harmonic contributions to emotion classification
- Evidence anchors: Section expects high brightness for Q4 and low for Q2; Grad-CAM implementation details provided; corpus evidence is weak
- Break condition: If Grad-CAM brightness does not accurately represent harmonic contributions

### Mechanism 3
- Claim: Theory-based model achieves comparable performance with greater parsimony
- Mechanism: Domain knowledge about harmonics enables more efficient learning with fewer parameters
- Core assumption: Domain knowledge can be effectively incorporated to improve efficiency without sacrificing performance
- Evidence anchors: Abstract states model is more parsimonious with comparable performance; section compares parameter counts (100k vs 5-10 million); corpus evidence is weak
- Break condition: If incorporating domain knowledge does not lead to more efficient learning

## Foundational Learning

- Fourier Transform and audio signals
  - Why needed: Understanding frequency decomposition is fundamental to harmonics importance
  - Quick check: What is the relationship between a sound wave and its frequency spectrum according to Fourier theory?

- Mel Spectrogram and human auditory perception
  - Why needed: Mel scale reflects human hearing, crucial for model processing
  - Quick check: How does mel scale differ from linear frequency scale, and why is this transformation important?

- Convolutional Neural Networks for non-image data
  - Why needed: Understanding CNN adaptation for music motivates harmonics-based filters
  - Quick check: What are key differences between applying CNNs to images versus music spectrograms?

## Architecture Onboarding

- Component map: Raw audio waveform -> STFT spectrogram -> Mel spectrogram -> Harmonics filters (12 pitch classes, 32 channels each) -> Batch normalization -> ReLU -> Average pooling -> Dropout -> Concatenation -> Max pooling -> Dropout -> Fully connected layer -> Softmax

- Critical path: 1) Audio preprocessing (waveform â†’ mel spectrogram) 2) Harmonics filter application (pitch class-specific blinders) 3) CNN feature extraction and classification 4) Grad-CAM visualization for explainability

- Design tradeoffs: Parsimony vs Performance (fewer parameters vs maintained accuracy), Explainability vs Complexity (better interpretability requires complex filter design), Audio length vs Temporal modeling (6-second clips balance context with efficiency)

- Failure signatures: Poor predictive performance despite theoretical soundness, Grad-CAM visualizations not aligning with expected patterns, overfitting on training data

- First 3 experiments: 1) Compare harmonics-based model with and without harmonics structure 2) Generate Grad-CAM visualizations for diverse music clips and verify alignment with consonance/dissonance 3) Evaluate model performance on out-of-sample data from different genres or cultures

## Open Questions the Paper Calls Out

- How does incorporating listener demographic heterogeneity (age, gender, cultural background) impact predictive accuracy and explainability? Authors note privacy concerns prevented inclusion but suggest it could improve performance.

- What is the optimal duration of audio clips for emotion classification using this architecture? Authors use 6-second clips for ad application but acknowledge method applies to any duration with architectural modifications.

- How does the model perform on music genres outside Western tonal music? Authors focus on Western music and acknowledge limitation, noting model could be adapted but not exploring non-Western music performance.

## Limitations

- The harmonics-based mechanism relies heavily on the assumption that consonance/dissonance patterns are primary drivers of emotional response, which may not hold across all genres or cultures.

- Grad-CAM visualization technique lacks direct validation that brightness values accurately reflect harmonic contributions to emotion classification.

- Model's performance comparison with atheoretical models shows comparable results but doesn't conclusively demonstrate superiority of theory-based approach.

## Confidence

- High confidence: Overall methodology for combining audio preprocessing, CNN architecture, and evaluation metrics is sound and well-documented.

- Medium confidence: Claim that theory-based model is more parsimonious than atheoretical models, based on parameter counts.

- Low confidence: Specific mechanisms by which harmonics-based filters capture emotional features, and interpretation of Grad-CAM visualizations for explainability.

## Next Checks

1. Conduct ablation studies comparing harmonics-based model with versions having random or frequency-agnostic filters to isolate contribution of theory-based design.

2. Perform cross-cultural validation using datasets from different musical traditions to test generalizability of harmonics-emotion relationship.

3. Implement user study to correlate Grad-CAM visualizations with human perception of consonance/dissonance in music, validating explainability claims.