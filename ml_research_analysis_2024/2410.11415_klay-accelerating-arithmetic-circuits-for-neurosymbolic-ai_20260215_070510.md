---
ver: rpa2
title: 'KLay: Accelerating Arithmetic Circuits for Neurosymbolic AI'
arxiv_id: '2410.11415'
source_url: https://arxiv.org/abs/2410.11415
tags:
- circuits
- circuit
- nodes
- node
- arithmetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KLay addresses the challenge of efficiently evaluating arithmetic
  circuits on modern AI accelerators like GPUs. The core idea is to represent arithmetic
  circuits as "knowledge layers" (KLay), which are organized into layers that can
  be evaluated sequentially using indexing and scatter-reduce operations already present
  in popular tensor libraries.
---

# KLay: Accelerating Arithmetic Circuits for Neurosymbolic AI

## Quick Facts
- arXiv ID: 2410.11415
- Source URL: https://arxiv.org/abs/2410.11415
- Authors: Jaron Maene; Vincent Derkinderen; Pedro Zuidberg Dos Martires
- Reference count: 19
- Primary result: KLay achieves speedups of multiple orders of magnitude over existing methods, both on CPU and GPU, making it feasible to scale neurosymbolic AI to larger real-world applications.

## Executive Summary
KLay addresses the challenge of efficiently evaluating arithmetic circuits on modern AI accelerators like GPUs for neurosymbolic AI applications. The core innovation is representing arithmetic circuits as "knowledge layers" (KLay), which can be evaluated sequentially using indexing and scatter-reduce operations already present in popular tensor libraries. By transforming irregular circuit structures into layered representations, KLay enables efficient parallelization on GPUs. Experiments demonstrate significant runtime improvements on both synthetic benchmarks and neurosymbolic tasks, with KLay outperforming baselines by up to four orders of magnitude in some cases.

## Method Summary
KLay represents arithmetic circuits as layers that can be evaluated sequentially using indexing and scatter-reduce operations from tensor libraries like PyTorch and JAX. The approach consists of two key algorithms: one for mapping traditional circuit representations to KLay and another for deduplicating nodes via Merkle hashing. The layerization algorithm assigns each node a height, groups nodes by height into layers, and introduces unary nodes to ensure all children are in the immediately preceding layer. This transformation allows each layer to be computed using only indexing and scatter-reduce operations, avoiding the need for custom CUDA kernels. The deduplication algorithm identifies and merges equivalent sub-circuits, reducing redundant computations and improving performance.

## Key Results
- KLay achieves speedups of multiple orders of magnitude over existing methods on both CPU and GPU
- On large circuits, KLay on GPU outperforms all baselines with over one order of magnitude improvement
- Significant runtime improvements demonstrated on synthetic benchmarks and neurosymbolic tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Representing arithmetic circuits as layers of indexing and scatter-reduce operations enables efficient GPU parallelization.
- **Mechanism:** The layerization algorithm assigns each node in a d-DNNF circuit a height, groups nodes by height into layers, and introduces unary nodes to ensure all children are in the immediately preceding layer. This allows each layer to be computed sequentially using only indexing and scatter-reduce operations, which are already optimized in tensor libraries.
- **Core assumption:** Indexing and scatter-reduce operations can be efficiently fused and parallelized on modern GPUs.
- **Evidence anchors:**
  - [abstract] "The main advantage of KLay is that it reduces arithmetic circuit evaluations to index and scatter operations – operations already present in popular tensor libraries."
  - [section 4] "To compute Nl, we first select relevant values from Nl−1 using as index Sl, giving us El = Nl−1[Sl]."
- **Break condition:** If scatter-reduce operations cannot be fused efficiently, or if the layerization introduces excessive overhead.

### Mechanism 2
- **Claim:** Deduplication of nodes via Merkle hashing reduces redundant computations and improves performance.
- **Mechanism:** Each node is assigned a unique Merkle hash based on its children's hashes, allowing equivalent sub-circuits to be identified and merged.
- **Core assumption:** Merkle hashes can uniquely identify equivalent sub-circuits without collisions.
- **Evidence anchors:**
  - [section 3.2] "We show that this is possible in linear time. Theorem 1. Given a set of circuits, all identical sub-circuits can be identified in linear expected time complexity."
  - [section 3.2] "We realize Theorem 1 using Merkle hashes... For each internal node, we combine the hashes of the children using a permutation invariant function."
- **Break condition:** If Merkle hashing collisions occur, or if the overhead of computing hashes outweighs deduplication benefits.

### Mechanism 3
- **Claim:** Layerization and tensorization make arithmetic circuits tractable on hardware accelerators by converting irregular sparsity into structured parallelism.
- **Mechanism:** The transformation from linked nodes to layered indices and reduction indices (Sl and Rl) allows the circuit to be evaluated as a sequence of parallelizable tensor operations.
- **Core assumption:** The structured layer-by-layer evaluation is more parallelizable than naive node-by-node traversal.
- **Evidence anchors:**
  - [abstract] "KLay achieves speedups of multiple orders of magnitude over the state of the art."
  - [section 4] "By imposing an order on the input nodes and by providing the set of vectors {S1 . . . SL} and {R1 . . . RL}, we entirely characterize an arithmetic circuit."
  - [section 6.1] "Our results in Figure 6 indicate that on large circuits, KLay on GPU outperforms all baselines with over one order of magnitude."
- **Break condition:** If the layerization overhead is too high, or if the structured parallelism does not translate to actual GPU speedup.

## Foundational Learning

- **Concept:** d-DNNF (deterministic decomposable negation normal form)
  - **Why needed here:** Arithmetic circuits are derived from d-DNNF circuits, and the layerization algorithm exploits the alternating ∧/∨ gate structure of d-DNNF.
  - **Quick check question:** What are the two key properties of d-DNNF circuits that enable efficient probabilistic inference?

- **Concept:** Merkle hashing
  - **Why needed here:** Used to deduplicate equivalent sub-circuits in multi-rooted circuits and during layerization.
  - **Quick check question:** How does Merkle hashing ensure that equivalent sub-circuits are identified without false positives?

- **Concept:** Indexing and scatter-reduce operations in tensor libraries
  - **Why needed here:** These operations are the core of KLay's efficient evaluation on GPUs.
  - **Quick check question:** Why are indexing and scatter-reduce operations more efficient than custom CUDA kernels for this application?

## Architecture Onboarding

- **Component map:** Input d-DNNF circuit -> Layerization module -> Tensorization module -> Evaluation module -> Deduplication module
- **Critical path:** Layerization → Tensorization → Evaluation
- **Design tradeoffs:**
  - Layerization overhead vs. parallelism: Introducing unary nodes adds nodes but enables layer-by-layer evaluation
  - Deduplication accuracy vs. hashing cost: Perfect hashing avoids collisions but may be expensive
  - Flexibility vs. performance: Supporting both PyTorch and JAX increases flexibility but may limit low-level optimizations
- **Failure signatures:**
  - Slow performance: Layerization introduces too many unary nodes, or scatter-reduce operations are not fused
  - Memory issues: Merkle hashing table grows too large, or tensor intermediates are too big
  - Incorrect results: Deduplication merges non-equivalent nodes due to hash collisions
- **First 3 experiments:**
  1. **Layerization sanity check:** Take a small d-DNNF circuit, run the layerization algorithm, and verify that nodes are grouped by height and unary nodes are correctly inserted.
  2. **Tensorization check:** Convert the layered circuit to Sl and Rl vectors, and manually verify that the tensor operations reproduce the original circuit evaluation.
  3. **GPU speedup test:** Compare runtime of naive node-by-node evaluation vs. KLay on a small arithmetic circuit, ensuring GPU acceleration is realized.

## Open Questions the Paper Calls Out
- How does KLay's performance scale with circuit depth and width beyond the tested benchmarks?
- Can KLay's layerization and deduplication techniques be applied to other types of probabilistic circuits beyond d-DNNF and SDDs?
- What is the impact of KLay's preprocessing overhead on end-to-end neurosymbolic training time for different problem sizes?
- How does KLay's performance compare to specialized hardware accelerators (FPGAs, ASICs) for arithmetic circuit evaluation?

## Limitations
- The paper lacks direct empirical evidence from tensor library optimization literature supporting the efficiency of indexing and scatter-reduce operations for arithmetic circuit evaluation.
- Merkle hashing efficiency claims are supported only by a theoretical theorem without corpus evidence of practical collision resistance.
- The evaluation focuses primarily on runtime speedups without addressing potential accuracy degradation from deduplication or memory overhead from layerization.
- No discussion of how KLay handles dynamic circuits where input distributions change between evaluations.

## Confidence
- High confidence in the core algorithmic contributions (layerization and tensorization frameworks)
- Medium confidence in the claimed speedups, as experimental results show orders-of-magnitude improvements but lack comparison to all relevant baselines
- Low confidence in the claims about tensor library optimization efficiency, as no direct evidence is provided

## Next Checks
1. Benchmark KLay against specialized GPU kernels for arithmetic circuits on both synthetic and real-world neurosymbolic tasks to verify claimed speedups
2. Conduct a memory usage analysis comparing KLay's layerization and deduplication overhead against naive implementations
3. Test KLay's robustness to hash collisions by deliberately introducing controlled collisions and measuring impact on evaluation correctness