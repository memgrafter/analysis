---
ver: rpa2
title: 'ACCORD: Closing the Commonsense Measurability Gap'
arxiv_id: '2406.02804'
source_url: https://arxiv.org/abs/2406.02804
tags:
- reasoning
- answer
- suppose
- pairing
- csqa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ACCORD addresses the construct validity gap in commonsense reasoning
  benchmarks by disentangling reasoning from parroting through controlled anti-factual
  contexts. The framework formalizes reasoning hops and distractors using ConceptNet-derived
  templates, generating benchmarks of arbitrary complexity without additional human
  effort.
---

# ACCORD: Closing the Commonsense Measurability Gap

## Quick Facts
- arXiv ID: 2406.02804
- Source URL: https://arxiv.org/abs/2406.02804
- Reference count: 40
- One-line primary result: ACCORD reveals LLMs rely on inductive biases rather than genuine reasoning by disentangling commonsense grounding from reasoning using controlled anti-factual contexts

## Executive Summary
ACCORD addresses the construct validity gap in commonsense reasoning benchmarks by disentangling reasoning from parroting through controlled anti-factual contexts. The framework formalizes reasoning hops and distractors using ConceptNet-derived templates, generating benchmarks of arbitrary complexity without additional human effort. On ACCORDCSQA, state-of-the-art LLMs (GPT-4o, Llama-3-70B, Mixtral-8x22B) showed factual reasoning significantly outperforming anti-factual reasoning, with the latter degrading below random chance as reasoning hops increased. This performance gap highlights how LLMs rely on inductive biases rather than genuine reasoning, while the ability to precisely measure hops and distractors reveals that LLMs filter distractors more effectively than they reason through multiple hops.

## Method Summary
ACCORD formalizes commonsense reasoning by mapping reasoning skills to ConceptNet relations and constructing reasoning templates. The framework generates reasoning trees with specified complexity (reasoning hops and distractors), pairs them to CSQA instances, and applies anti-factual grounding by negating statements that are implausible under the default world model. The resulting ACCORDCSQA benchmark uses zero-shot evaluation with JSON-formatted prompts to measure LLM performance on both factual and anti-factual reasoning tasks, revealing the extent to which LLMs rely on inductive biases versus genuine reasoning.

## Key Results
- Factual reasoning accuracy: 0.57 ± 0.02, Anti-factual reasoning accuracy: 0.33 ± 0.02 (random chance: 0.25)
- Performance gap between factual and anti-factual reasoning widens with increasing reasoning hops
- Distractors have negligible effect compared to reasoning hops on LLM performance
- LLMs filter distractors more effectively than they reason through multiple hops

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: ACCORD disentangles commonsense grounding and reasoning by using anti-factual contexts that are implausible under the default world model (wdef) of LLM training data.
- **Mechanism**: By constructing reasoning tasks where the context contradicts wdef, LLMs cannot rely on inductive biases from parametric knowledge to bypass reasoning. The anti-factual grounding ensures the LLM must engage with the provided context rather than default to memorized facts.
- **Core assumption**: LLMs are trained on data that largely conforms to wdef, making anti-factual scenarios effective at mitigating context unfaithfulness.
- **Evidence anchors**:
  - [abstract]: "ACCORD introduces formal elements to commonsense reasoning to explicitly control and quantify reasoning complexity beyond the typical 1 or 2 hops."
  - [section]: "Since CSQA derives from ConceptNet, we employ it as our grounding KB. Essentially, since ConceptNet is a factual KB, finding concept triples—e.g., part_of(outer space, watch)—not in ConceptNet enables us to argue that they are anti-factual."
  - [corpus]: Weak evidence - the corpus does not provide direct validation of the anti-factual grounding mechanism.
- **Break condition**: If the LLM can still bypass reasoning by exploiting subtle correlations or if the anti-factual grounding is not sufficiently implausible under wdef.

### Mechanism 2
- **Claim**: ACCORD controls reasoning complexity by formalizing commonsense reasoning into composable reasoning skills and templates.
- **Mechanism**: By mapping commonsense reasoning skills to ConceptNet relations and constructing reasoning templates, ACCORD creates a structured framework where reasoning hops and distractors can be precisely quantified. This allows for automated generation of benchmarks of arbitrary complexity.
- **Core assumption**: The reasoning skills identified by Talmor et al. (2019) and their mapping to ConceptNet relations are sufficient to capture the essential elements of commonsense reasoning.
- **Evidence anchors**:
  - [abstract]: "ACCORD introduces formal elements to commonsense reasoning to explicitly control and quantify reasoning complexity beyond the typical 1 or 2 hops."
  - [section]: "Each reasoning skill is a recurring pattern of commonsense reasoning. For example, the spatial reasoning skill is the pattern of understanding which objects commonsensically appear at which locations..."
  - [corpus]: Moderate evidence - the corpus shows that the approach is novel in applying formal reasoning elements to commonsense reasoning.
- **Break condition**: If the formalization fails to capture important aspects of commonsense reasoning or if the mapping to ConceptNet relations is not comprehensive enough.

### Mechanism 3
- **Claim**: ACCORD mitigates dataset artifacts and spurious shortcuts by carefully controlling the generation and validation of reasoning paths.
- **Mechanism**: Through tree duplication and negation, ACCORD ensures that each answer choice is equally represented in the context, preventing the LLM from exploiting lexical matching or other dataset artifacts. The reduction matrix ensures that reasoning paths are valid and sound.
- **Core assumption**: The reduction matrix correctly captures all valid compositions of reasoning skills, and the tree duplication process effectively neutralizes lexical matching biases.
- **Evidence anchors**:
  - [abstract]: "Uniquely, ACCORD can automatically generate benchmarks of arbitrary reasoning complexity, and so it scales with future LLM improvements."
  - [section]: "To mitigate this potential bias, we duplicate the paired tree, once for each ai, which ensures each is present in Caf while holding all else equal."
  - [corpus]: Moderate evidence - the corpus shows that the approach is designed to address construct validity concerns in commonsense reasoning benchmarks.
- **Break condition**: If the reduction matrix is incomplete or if the tree duplication process introduces other biases or confounds.

## Foundational Learning

- **Concept**: Formal reasoning vs. commonsense reasoning
  - **Why needed here**: Understanding the distinction is crucial for grasping ACCORD's approach of formalizing commonsense reasoning elements.
  - **Quick check question**: What is the key difference between formal reasoning (which follows systematic rules) and commonsense reasoning (which relies on intuition and worldly experience)?

- **Concept**: Counterfactuals and anti-factuals
  - **Why needed here**: ACCORD uses anti-factual contexts to mitigate context unfaithfulness in LLMs. Understanding the difference between hypothetical and anti-factual counterfactuals is essential.
  - **Quick check question**: What is the difference between a hypothetical counterfactual (which runs counter to a specific scenario but is plausible under wdef) and an anti-factual counterfactual (which is implausible under wdef)?

- **Concept**: Reasoning hops and distractors
  - **Why needed here**: ACCORD quantifies reasoning complexity by measuring the number of reasoning hops and distractors in a reasoning path. Understanding these concepts is key to interpreting ACCORD's results.
  - **Quick check question**: In the context of ACCORD, what is the difference between a reasoning hop (a step in the reasoning process) and a distractor (an irrelevant statement in the context)?

## Architecture Onboarding

- **Component map**: CSQA instance → Reasoning skill identification → Reasoning tree generation → Tree pairing → Reasoning path computation → Anti-factual grounding → Benchmark generation

- **Critical path**: The critical path for generating an ACCORD benchmark involves: (1) selecting a CSQA instance and identifying its reasoning skill, (2) generating all possible reasoning trees up to a specified size, (3) pairing the CSQA instance to relevant trees, (4) computing reasoning paths and identifying hops and distractors, and (5) anti-factually grounding variables and negating statements to control for answer selection.

- **Design tradeoffs**: ACCORD prioritizes rigorous quantification of reasoning complexity and mitigation of construct validity concerns over naturalness of the generated instances. This results in benchmarks that resemble logic puzzles more than natural language questions, but ensures that the reasoning complexity is explicitly controlled and measurable.

- **Failure signatures**: Potential failures in ACCORD could include: (1) incomplete or incorrect reduction matrix, leading to invalid reasoning paths, (2) insufficient anti-factual grounding, allowing LLMs to bypass reasoning, (3) overfitting to the specific structure of CSQA, limiting generalizability, and (4) excessive computational complexity, hindering scalability.

- **First 3 experiments**:
  1. Verify that ACCORD can correctly identify and pair reasoning trees to CSQA instances based on their reasoning skills.
  2. Test the reduction matrix to ensure that it correctly identifies valid compositions of reasoning skills and generates sound reasoning paths.
  3. Evaluate the effectiveness of the anti-factual grounding mechanism by checking if the generated contexts are indeed implausible under wdef and if they successfully mitigate context unfaithfulness in LLMs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of reasoning hops (n) and distractors (d) for maximizing LLM performance on ACCORDCSQA?
- Basis in paper: [inferred] The paper states that LLM performance degrades with increasing reasoning hops and that distractors have a negligible effect compared to reasoning hops.
- Why unresolved: The paper only tests a limited range of reasoning hops and distractors. It is unclear what the optimal balance is for maximizing LLM performance.
- What evidence would resolve it: Testing a wider range of reasoning hops and distractors to find the optimal balance for LLM performance.

### Open Question 2
- Question: How does the quality of the ConceptNet grounding affect the performance of LLMs on ACCORDCSQA?
- Basis in paper: [explicit] The paper mentions that ACCORD relies on ConceptNet for grounding variables, but also acknowledges that ConceptNet is noisy and may contain inconsistencies.
- Why unresolved: The paper does not provide a detailed analysis of how the quality of ConceptNet grounding affects LLM performance.
- What evidence would resolve it: Conducting experiments with different versions of ConceptNet or other knowledge bases to assess the impact of grounding quality on LLM performance.

### Open Question 3
- Question: Can ACCORD be extended to other types of reasoning tasks beyond commonsense reasoning?
- Basis in paper: [inferred] The paper mentions that ACCORD borrows from formal reasoning to partially formalize commonsense reasoning. It also states that ACCORD is designed to be applicable to other commonsense reasoning datasets.
- Why unresolved: The paper only applies ACCORD to CSQA. It is unclear whether ACCORD can be successfully extended to other types of reasoning tasks.
- What evidence would resolve it: Applying ACCORD to other reasoning tasks and datasets to assess its generalizability.

## Limitations
- Reliance on ConceptNet as grounding knowledge base introduces breadth and depth constraints
- Anti-factual grounding assumes missing ConceptNet triples are implausible, which may not always hold true
- Benchmarks resemble logic puzzles rather than natural language questions, potentially limiting ecological validity

## Confidence
- **High confidence**: The core claim that ACCORD successfully disentangles reasoning from inductive bias is well-supported by the significant performance gap between factual and anti-factual reasoning tasks across all tested LLMs.
- **Medium confidence**: The assertion that LLMs filter distractors more effectively than they reason through multiple hops, while supported by the data, requires additional validation across diverse reasoning domains.
- **Medium confidence**: The scalability claims depend on the assumption that the reduction matrix remains computationally tractable as reasoning complexity increases, which needs empirical verification.

## Next Checks
1. **Generalization Test**: Apply ACCORD to a non-ConceptNet grounded dataset (e.g., medical or legal reasoning) to verify if the anti-factual grounding mechanism generalizes beyond commonsense knowledge.
2. **Reasoning Skill Coverage**: Conduct an ablation study removing each of the eight reasoning skills to quantify their individual contribution to the benchmark's difficulty and identify potential blind spots in the formalization.
3. **Human Performance Baseline**: Establish human expert performance on ACCORDCSQA across factual and anti-factual conditions to determine if the observed LLM performance gaps reflect genuine reasoning limitations or artifact of the evaluation methodology.