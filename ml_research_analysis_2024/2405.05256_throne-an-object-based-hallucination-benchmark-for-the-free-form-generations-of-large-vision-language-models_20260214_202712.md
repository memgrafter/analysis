---
ver: rpa2
title: 'THRONE: An Object-based Hallucination Benchmark for the Free-form Generations
  of Large Vision-Language Models'
arxiv_id: '2405.05256'
source_url: https://arxiv.org/abs/2405.05256
tags:
- hallucinations
- throne
- type
- image
- pope
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces THRONE, a new benchmark for evaluating Type
  I hallucinations (open-ended free-form responses) in Large Vision-Language Models
  (LVLMs). THRONE uses language models to judge the existence of objects in LVLM responses,
  providing a more accurate and accessible evaluation than existing methods like CHAIR.
---

# THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models

## Quick Facts
- arXiv ID: 2405.05256
- Source URL: https://arxiv.org/abs/2405.05256
- Reference count: 40
- Key outcome: Introduces THRONE benchmark for Type I hallucinations in LVLMs, achieving 86.1% class-wise F0.5-score using data augmentation

## Executive Summary
This paper introduces THRONE, a novel benchmark for evaluating Type I hallucinations (object-based free-form responses) in Large Vision-Language Models (LVLMs). The benchmark leverages language models to judge the existence of objects in LVLM responses, providing a more accurate and accessible evaluation method compared to existing benchmarks like CHAIR. The study reveals that improvements in traditional evaluation metrics do not necessarily translate to reductions in Type I hallucinations, and that current methods like POPE underestimate Type II hallucinations. The authors also propose a simple data augmentation technique that significantly reduces both hallucination types.

## Method Summary
The THRONE benchmark addresses limitations in existing hallucination evaluation methods by using language models to judge object existence in LVLM responses. The evaluation process involves human-labeled object existence as ground truth, with language models assessing whether mentioned objects actually appear in images. The benchmark specifically targets Type I hallucinations through open-ended free-form responses. A novel data augmentation method is introduced that improves hallucination reduction by training LVLMs to better align with ground truth object presence. The evaluation framework compares THRONE against existing benchmarks (CHAIR and POPE) to demonstrate its effectiveness and identify shortcomings in current hallucination measurement approaches.

## Key Results
- Improvements in traditional benchmarks do not correlate with reduced Type I hallucinations
- POPE significantly underestimates the prevalence of Type II hallucinations
- Proposed data augmentation method achieves 86.1% class-wise F0.5-score on THRONE
- Language model-based judging provides more accurate evaluation than existing methods

## Why This Works (Mechanism)
The benchmark works by leveraging language models' ability to understand context and semantics when evaluating object existence in LVLM responses. By using language models as judges rather than relying on fixed templates or rule-based systems, THRONE can capture nuanced interpretations of object presence that align more closely with human judgment. The data augmentation method works by exposing LVLMs to more diverse training examples that emphasize correct object identification, thereby reducing the tendency to hallucinate objects that aren't present in images.

## Foundational Learning
- **Type I vs Type II hallucinations**: Why needed - distinguishes between object-based and factual hallucinations; Quick check - verify that evaluation methods are appropriate for the hallucination type being measured
- **Language model judging**: Why needed - provides flexible, context-aware evaluation; Quick check - assess judge model consistency across different object types
- **Data augmentation for hallucination reduction**: Why needed - addresses fundamental model training limitations; Quick check - measure hallucination reduction across multiple model architectures
- **Benchmark interoperability**: Why needed - ensures evaluation methods can be compared and validated; Quick check - test same models across different hallucination benchmarks

## Architecture Onboarding
- **Component map**: LVLM (generator) -> Language model judge -> THRONE evaluation framework
- **Critical path**: Image input → LVLM generation → Object extraction → Language model judging → Score calculation
- **Design tradeoffs**: Automated judging vs human evaluation (speed vs nuance), benchmark specificity vs generalizability, model complexity vs evaluation accessibility
- **Failure signatures**: Judge model hallucinations affecting scores, ambiguous object presence leading to inconsistent judgments, benchmark overfitting to specific LVLM architectures
- **First experiments**: 1) Compare THRONE scores across different judge model sizes, 2) Validate benchmark consistency with human evaluation on sample responses, 3) Test data augmentation effectiveness on multiple LVLM architectures

## Open Questions the Paper Calls Out
The paper highlights several open questions regarding the generalizability of its evaluation methodology and the long-term effectiveness of proposed hallucination reduction techniques. Key questions include how well language model-based judging captures human judgment nuances, whether the benchmark construction process adequately handles context-dependent object interpretations, and how the proposed data augmentation method performs across diverse LVLM architectures beyond those tested.

## Limitations
- Language model-based judging introduces potential bias from the judge model's own hallucination tendencies
- Benchmark assumes human-labeled object existence provides definitive ground truth, potentially missing context-dependent interpretations
- Focus on Type I hallucinations leaves Type II hallucinations with less rigorous evaluation through POPE
- Data augmentation method effectiveness may not generalize across different LVLM architectures

## Confidence
- **High confidence**: Demonstration that improvements in existing benchmarks don't correlate with reduced Type I hallucinations is well-supported
- **Medium confidence**: Claim that POPE underestimates Type II hallucinations is plausible but needs broader validation
- **Medium confidence**: Effectiveness of data augmentation method is demonstrated but long-term generalization remains untested

## Next Checks
1. Test language model judge across multiple judge models (different architectures and sizes) to assess consistency and potential judge-dependent bias
2. Conduct human evaluation studies comparing THRONE's automated scoring against human judgment on object existence
3. Evaluate data augmentation method across diverse LVLM architectures beyond those tested to assess generalizability