---
ver: rpa2
title: 'Federated Continual Learning Goes Online: Uncertainty-Aware Memory Management
  for Vision Tasks and Beyond'
arxiv_id: '2405.18925'
source_url: https://arxiv.org/abs/2405.18925
tags:
- learning
- data
- tasks
- uncertainty
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework for online Federated Continual
  Learning (FCL), addressing catastrophic forgetting in dynamic, real-world settings.
  Unlike existing generative-based approaches limited to offline scenarios and vision
  tasks, the proposed method employs an uncertainty-aware memory management strategy
  applicable to various data modalities.
---

# Federated Continual Learning Goes Online: Uncertainty-Aware Memory Management for Vision Tasks and Beyond

## Quick Facts
- arXiv ID: 2405.18925
- Source URL: https://arxiv.org/abs/2405.18925
- Authors: Giuseppe Serra; Florian Buettner
- Reference count: 40
- One-line primary result: Novel uncertainty-aware memory management approach for online federated continual learning outperforms generative replay and standard baselines while being applicable across data modalities.

## Executive Summary
This paper introduces a novel framework for online Federated Continual Learning (FCL) that addresses catastrophic forgetting in dynamic, real-world settings. Unlike existing generative-based approaches limited to offline scenarios and vision tasks, the proposed method employs an uncertainty-aware memory management strategy applicable to various data modalities. The core idea involves using Bregman Information (BI) as an estimator of epistemic uncertainty to selectively store and replay representative samples from a local memory buffer. This approach outperforms state-of-the-art methods, including generative replay and standard baselines, in reducing catastrophic forgetting while maintaining competitive communication efficiency.

## Method Summary
The method introduces uncertainty-aware memory management for online federated continual learning, using Bregman Information (BI) as an epistemic uncertainty estimator. Local clients maintain memory buffers of representative samples, which are selectively stored based on BI values and randomly sampled for replay during training. The approach includes a burn-in period before communication and limits communication rounds per task to balance model stability and plasticity. BI is computed using test-time augmentation (TTA) to reduce computational overhead, and the memory update strategy ensures class-balanced representation while minimizing uncertainty in stored samples.

## Key Results
- Outperforms state-of-the-art methods including generative replay and standard baselines in reducing catastrophic forgetting
- Demonstrates effectiveness across diverse data modalities including biomedical images and text
- Shows competitive communication efficiency compared to generative approaches while being more flexible

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using Bregman Information (BI) as an uncertainty estimator effectively identifies representative samples for memory replay, reducing catastrophic forgetting.
- Mechanism: BI measures epistemic uncertainty in the logit space by quantifying the variance of predictions across perturbed inputs. Low BI indicates samples where the model has high confidence about the data generating process, making them ideal for replay.
- Core assumption: Samples with low epistemic uncertainty are more representative of their class and thus better for mitigating forgetting.
- Evidence anchors:
  - [abstract]: "we suggest using an estimator based on the Bregman Information (BI) to compute the model's variance at the sample level"
  - [section]: "we leverage a recently proposed bias-variance decomposition of the cross-entropy loss (Gruber & Buettner, 2023) and estimate the Bregman Information (BI) as the variance term in logit space"
  - [corpus]: Weak - related works focus on uncertainty estimation but not specifically BI in federated settings

### Mechanism 2
- Claim: Online federated continual learning with memory buffers outperforms generative replay approaches in dynamic, data-streaming scenarios.
- Mechanism: Memory buffers store a small set of representative samples from each class, which are randomly sampled for replay during training. This approach is computationally efficient and doesn't require training generative models on limited data.
- Core assumption: In online settings where data arrives in mini-batches, memory-based approaches are more practical than generative methods that require multiple epochs over complete datasets.
- Evidence anchors:
  - [abstract]: "The majority of the current approaches in FCL propose generative-based solutions... However, this setting requires multiple training epochs over the data, implying an offline setting"
  - [section]: "the majority of the proposed approaches exploit generative models... A substantial drawback of such generative approaches is that they are tailored to image data and it is not clear how they could translate to other data modalities"
  - [corpus]: Moderate - related works discuss memory-based approaches but don't directly compare online vs offline scenarios

### Mechanism 3
- Claim: The proposed communication strategy with burn-in periods and limited communication rounds balances model stability and plasticity in online federated settings.
- Mechanism: Local models train independently for a burn-in period before participating in communication rounds, preventing instability from early parameter averaging. Communication occurs after processing q consecutive mini-batches rather than after each batch.
- Core assumption: Early in task learning, frequent communication with unstable local models degrades global model performance.
- Evidence anchors:
  - [section]: "we propose to set a burn-in period. During this period, the local model learns independently for a certain number of batches without sharing and receiving information from the others"
  - [section]: "we propose to limit the number of communications rounds per task... we let the local models learn for q consecutive mini-batches before actively participating to the communication round"
  - [corpus]: Weak - related works don't specifically address burn-in periods in federated continual learning

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: The entire paper addresses this problem in federated settings
  - Quick check question: What happens to a model's performance on previous tasks when it learns new tasks without any mitigation strategy?

- Concept: Epistemic vs aleatoric uncertainty
  - Why needed here: The paper uses epistemic uncertainty (measured by BI) to select representative samples
  - Quick check question: Which type of uncertainty (epistemic or aleatoric) can be reduced by gathering more data?

- Concept: Bias-variance decomposition of cross-entropy loss
  - Why needed here: BI is derived from this decomposition as the variance term
  - Quick check question: In the bias-variance decomposition, what does the variance term represent in terms of model behavior?

## Architecture Onboarding

- Component map:
  Local clients -> Local models + Memory buffers -> Uncertainty estimator -> Server (aggregates parameters) -> Global model distribution

- Critical path:
  1. Client receives mini-batch of new data
  2. Client updates local model on new data + sampled memory
  3. Client updates memory using BI-based uncertainty
  4. After burn-in + q batches, client shares parameters
  5. Server aggregates parameters (class-balanced if needed)
  6. Server distributes updated global model

- Design tradeoffs:
  - Memory buffer size vs storage constraints: Larger buffers provide better replay but increase memory usage
  - Burn-in period length vs early learning speed: Longer burn-in improves stability but delays knowledge sharing
  - Communication frequency (q) vs model freshness: More frequent updates provide fresher models but increase communication cost

- Failure signatures:
  - Memory buffer contains unrepresentative samples → BI estimation fails or augmentation set is inadequate
  - Model shows poor performance on initial tasks → Burn-in period too short or q too large
  - High variance across clients → Communication frequency too low or class imbalance not properly handled

- First 3 experiments:
  1. Baseline comparison: Run FedAvg, ER, and proposed method on CIFAR-10 with 5 tasks, measure average accuracy and forgetting
  2. Ablation study: Test different uncertainty metrics (LC, MS, RC, EN, BI) on CIFAR-100 to validate BI's effectiveness
  3. Cross-domain validation: Apply method to CRC-Tissue and KC-Cell datasets to verify performance on biomedical images with class imbalance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the BI-based uncertainty estimator compare to more sophisticated Bayesian methods like MC Dropout or Deep Ensembles when computational resources are not a constraint?
- Basis in paper: [explicit] The paper mentions that BI estimates can be computed using deep ensembles or test-time augmentation (TTA), but TTA is chosen to reduce computational overhead. The paper also states that BI is "only asymptotically unbiased and underestimates the theoretical quantity."
- Why unresolved: The paper does not provide a direct comparison between BI and Bayesian methods like MC Dropout or Deep Ensembles under the same computational constraints. It only mentions TTA as a computationally efficient alternative.
- What evidence would resolve it: An empirical study comparing the performance of BI using TTA against MC Dropout and Deep Ensembles on the same datasets, measuring both accuracy and computational efficiency.

### Open Question 2
- Question: Can the BI-based uncertainty estimator be effectively applied to non-image modalities like audio or video data, where data augmentation strategies might be more complex?
- Basis in paper: [explicit] The paper demonstrates the effectiveness of BI on text data using Gaussian noise on latent representations. It also mentions that BI is "applicable across different data modalities."
- Why unresolved: While the paper shows success on text data, it does not explore more complex modalities like audio or video, where appropriate augmentation strategies might be less straightforward.
- What evidence would resolve it: Experiments applying the BI-based approach to audio and video datasets, with appropriate augmentation strategies for these modalities, and comparing the results to baseline methods.

### Open Question 3
- Question: How does the choice of augmentation strategy impact the performance of the BI-based uncertainty estimator, and is there an optimal set of augmentations for different data modalities?
- Basis in paper: [explicit] The paper includes an ablation study on the effect of the augmentation set on performance, showing that removing augmentations decreases performance. It also mentions relying on "the large body of literature on TTA" for choosing augmentations.
- Why unresolved: The ablation study only explores removing augmentations randomly and does not investigate the impact of different augmentation strategies or their combinations for different modalities.
- What evidence would resolve it: A systematic study comparing different augmentation strategies and their combinations for various data modalities, measuring their impact on BI estimation accuracy and overall model performance.

### Open Question 4
- Question: What is the theoretical relationship between the BI-based uncertainty estimator and other uncertainty measures like entropy or least confidence, and under what conditions do they diverge in their recommendations for memory management?
- Basis in paper: [explicit] The paper compares BI to other uncertainty measures (LC, MS, RC, EN) and highlights that BI does not require normalization or rely on the largest activation value, unlike the others. It also mentions that BI captures epistemic uncertainty while others mostly capture aleatoric uncertainty.
- Why unresolved: While the paper provides a qualitative comparison and highlights differences in what aspects of uncertainty they capture, it does not provide a theoretical analysis of the relationship between these measures or under what conditions they would diverge in their recommendations.
- What evidence would resolve it: A theoretical analysis of the relationship between BI and other uncertainty measures, including conditions under which they would recommend different samples for memory management, supported by empirical evidence on synthetic datasets where the ground truth uncertainty is known.

## Limitations

- Performance in heterogeneous federated environments with significant local data distribution differences remains less certain
- Memory update mechanism lacks specific implementation details for sample replacement when memory is full
- Communication strategy (burn-in + limited rounds) needs empirical validation as optimal for online FCL

## Confidence

- **High confidence**: The mechanism for using memory buffers with random replay for preventing catastrophic forgetting in online settings is well-established and theoretically sound.
- **Medium confidence**: The claim that BI specifically outperforms other uncertainty metrics (LC, MS, RC, EN) in federated continual learning requires further validation, as the paper only shows BI's effectiveness in centralized settings.
- **Low confidence**: The assertion that the proposed communication strategy (burn-in + limited rounds) is optimal for online FCL needs empirical validation, as related works don't specifically address this design choice.

## Next Checks

1. Test the method when clients have non-IID data distributions to evaluate BI's robustness in realistic federated settings
2. Implement and test different strategies for the `UPDATE MEMORY` function, particularly how samples are selected for replacement when memory is full
3. Conduct a systematic ablation study varying the burn-in period and jump parameter q to identify optimal values across different task complexities