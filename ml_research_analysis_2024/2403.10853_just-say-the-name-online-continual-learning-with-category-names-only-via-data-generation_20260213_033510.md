---
ver: rpa2
title: 'Just Say the Name: Online Continual Learning with Category Names Only via
  Data Generation'
arxiv_id: '2403.10853'
source_url: https://arxiv.org/abs/2403.10853
tags:
- data
- images
- arxiv
- prompt
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of continual learning (CL) in
  scenarios where only category names are provided without any data samples. Existing
  CL methods rely on human supervision, which is costly and time-consuming, especially
  in real-time scenarios.
---

# Just Say the Name: Online Continual Learning with Category Names Only via Data Generation

## Quick Facts
- arXiv ID: 2403.10853
- Source URL: https://arxiv.org/abs/2403.10853
- Reference count: 40
- Primary result: GenCL achieves 9% and 13% improvements in AAUC on PACS OOD domain compared to models trained with web-scraped and manually annotated data, respectively

## Executive Summary
This paper addresses the challenge of continual learning when only category names are provided without any data samples. Existing CL methods require costly human supervision, particularly problematic in real-time scenarios. The authors propose Generative name-only Continual Learning (GenCL), a framework that leverages text-to-image generative models to create training data from category names. GenCL employs hierarchical recurrent prompt generation (HIRPG) to ensure diversity in generated images and a complexity-guided data ensemble method (CONAN) for efficient sample selection. The approach significantly outperforms prior arts on class-incremental learning and multi-modal visual concept-incremental learning tasks.

## Method Summary
GenCL is a framework for name-only continual learning that generates training data from category names using text-to-image models. The method consists of three main components: HIRPG for diverse prompt generation using LLMs with negative example conditioning, CONAN for complexity-guided coreset selection using relative Mahalanobis distance, and a continual learner with episodic memory replay. The framework operates online, generating and selecting samples for each incoming concept without requiring stored real data, thus avoiding privacy concerns while maintaining performance through replay of generated samples.

## Key Results
- GenCL achieves 9% and 13% improvements in AAUC on PACS OOD domain compared to models trained with web-scraped and manually annotated data, respectively
- Significant performance gains over prior arts including models trained with fully supervised data
- Demonstrates effectiveness on both class-incremental learning and multi-modal visual concept-incremental learning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt diversity via negative example augmentation drives image variety
- Mechanism: HIRPG feeds previously generated prompts as negative examples into an LLM, constraining each new prompt to be semantically distinct from earlier ones through recursive conditioning
- Core assumption: LLMs respond to negative constraints and maintain semantic coherence under recursive conditioning
- Evidence anchors: [abstract] "HIRPG generates diverse text prompts using a large language model (LLM) by iteratively providing previously generated prompts as negative examples"; [section 4.1] iterative prompt creation with negative examples
- Break condition: LLM stops generating semantically distinct prompts; negative examples fail to reduce overlap

### Mechanism 2
- Claim: Complexity-guided ensemble sampling improves both diversity and class-representativeness
- Mechanism: CONAN uses relative Mahalanobis distance (RMD) to rank samples by their distance from both class prototypes and global prototypes, with high RMD samples more likely to be selected
- Core assumption: High RMD scores correspond to samples that increase diversity while maintaining class relevance
- Evidence anchors: [abstract] "CONAN selects samples with minimal overlap from multiple generative models... based on their complexity"; [section 4.3] RMD-based probabilistic coreset selection
- Break condition: RMD fails to correlate with true sample difficulty; probabilistic sampling collapses to uniform selection

### Mechanism 3
- Claim: Episodic memory replay mitigates forgetting without compromising privacy
- Mechanism: GenCL stores previously generated samples in episodic memory for replay during continual learning, avoiding real-data storage privacy issues
- Core assumption: Generated samples retain sufficient fidelity for replay; privacy benefits of generated data over real data
- Evidence anchors: [abstract] "storing samples in episodic memory is free from data privacy issues"; [section 4] episodic memory for efficiency and privacy
- Break condition: Generated samples degrade too much for effective replay; memory storage becomes a bottleneck

## Foundational Learning

- Concept: Text-to-image (T2I) generative models and prompt engineering
  - Why needed here: Core of GenCL relies on generating concept-specific images from text prompts; understanding how prompts control output diversity is essential
  - Quick check question: How does adding a background descriptor to a prompt affect the diversity and realism of generated images?

- Concept: Continual learning setups (class-incremental, task-incremental)
  - Why needed here: GenCL operates in name-only continual learning; knowing how CIL differs from standard supervised learning is critical for architecture design
  - Quick check question: What is the primary challenge of class-incremental learning that episodic memory replay aims to solve?

- Concept: Mahalanobis distance and its use in sample selection
  - Why needed here: CONAN uses RMD for complexity-based sampling; understanding Mahalanobis distance is necessary to grasp how samples are ranked
  - Quick check question: Why is Mahalanobis distance preferred over Euclidean distance when measuring sample complexity in feature space?

## Architecture Onboarding

- Component map: Concept arrival -> Prompt Generation Module (ψ) -> Generator Set (G) -> Ensembler (∆) -> Continual Learner (fθ) -> Episodic Memory (M)
- Critical path: Concept arrival → Prompt generation → Image generation → Coreset selection → Model update → Memory update
- Design tradeoffs:
  - More generators → higher diversity but higher compute cost
  - Deeper HIRPG hierarchy → more diversity but longer context and potential lost-in-the-middle
  - Larger coreset → better performance but slower adaptation
  - Larger episodic memory → better forgetting mitigation but higher storage cost
- Failure signatures:
  - Low RMD variance → poor diversity in ensemble
  - High overlap between prompts → redundant image generation
  - Memory overflow → degraded replay effectiveness
  - Model collapse → poor concept generalization
- First 3 experiments:
  1. Test HIRPG vs baseline prompt generation on diversity metrics with a fixed generator
  2. Validate CONAN coreset selection by comparing AAUC with and without ensembling on a simple dataset
  3. Integrate all components in a small-scale class-incremental task and measure forgetting via episodic replay ablation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality and diversity of images generated by GenCL compare to those obtained through web-scraping, particularly in terms of realism and relevance to the given concepts?
- Basis in paper: [explicit] The paper discusses limitations of web-scraped data and claims GenCL generates more diverse and controllable images
- Why unresolved: The paper provides qualitative comparisons and metrics like Recognizability and Diversity, but lacks comprehensive quantitative comparison of image quality and relevance
- What evidence would resolve it: A study comparing image quality metrics (e.g., FID, IS) and human evaluations of relevance and realism between GenCL-generated and web-scraped images for the same concepts

### Open Question 2
- Question: What is the impact of using different text-to-image generative models within GenCL on the overall performance and diversity of the generated images?
- Basis in paper: [explicit] The paper mentions using multiple T2I models but does not provide detailed analysis of their individual contributions
- Why unresolved: The paper does not analyze performance when using different combinations or subsets of T2I models, nor compare generated images from each model
- What evidence would resolve it: An ablation study comparing performance of GenCL using different T2I models individually and in various combinations, along with qualitative and quantitative analysis of generated images

### Open Question 3
- Question: How does the proposed hierarchical recurrent prompt generation method (HIRPG) compare to other prompt generation techniques in terms of prompt diversity and image quality?
- Basis in paper: [explicit] The paper introduces HIRPG and claims it outperforms existing prompt generation methods in terms of image diversity and recognizability
- Why unresolved: While the paper provides comparisons with other prompt generation methods, it does not delve into specific aspects of prompt diversity and how it affects image quality
- What evidence would resolve it: A detailed analysis of diversity of generated prompts using metrics like semantic diversity and prompt novelty, along with comparison of image quality metrics for images generated using different prompt generation methods

### Open Question 4
- Question: How does the complexity-guided data ensemble method (CONAN) perform compared to other data selection techniques in terms of computational efficiency and sample representativeness?
- Basis in paper: [explicit] The paper introduces CONAN and claims it outperforms other data ensemble methods in terms of efficiency and diversity
- Why unresolved: The paper provides comparison with other data ensemble methods but does not analyze computational efficiency in detail or explore impact on representativeness of selected samples
- What evidence would resolve it: A comparison of computational time and memory requirements of CONAN with other data ensemble methods, along with analysis of representativeness of selected samples using metrics like coverage and diversity

## Limitations
- Heavy reliance on proprietary LLM APIs for prompt generation creates reproducibility and cost barriers
- Privacy benefits of generated data storage over real data lack direct empirical validation
- Effectiveness depends on specific LLM and generative model behaviors that may not generalize

## Confidence
- **High confidence**: The overall framework design and empirical methodology are sound; the paper clearly articulates the problem of name-only continual learning and provides comprehensive evaluation
- **Medium confidence**: The effectiveness of HIRPG and CONAN mechanisms, while demonstrated empirically, depends on specific LLM and generative model behaviors that may not generalize
- **Low confidence**: The privacy benefits claimed for generated data storage over real data lack direct empirical validation; the paper asserts this advantage without comparative analysis

## Next Checks
1. **Diversity validation**: Conduct controlled experiments comparing HIRPG's negative example conditioning against random prompt generation across multiple LLM models to quantify the actual contribution of hierarchical diversity
2. **Ensemble robustness test**: Evaluate CONAN's coreset selection by measuring performance degradation when systematically removing high-RMD samples versus low-RMD samples from the ensemble
3. **Privacy impact assessment**: Design a study comparing the privacy implications of storing generated samples versus real samples, measuring both data utility and privacy leakage across different storage scenarios