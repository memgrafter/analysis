---
ver: rpa2
title: 'AUEB-Archimedes at RIRAG-2025: Is obligation concatenation really all you
  need?'
arxiv_id: '2412.11567'
source_url: https://arxiv.org/abs/2412.11567
tags:
- answer
- repass
- obligations
- passages
- answers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents systems for the RIRAG-2025 shared task, which
  requires answering regulatory questions by retrieving relevant passages and generating
  answers evaluated using RePASs, a reference-free metric. The authors propose three
  systems that use a combination of BM25 and domain-specific neural retrievers with
  a reranker for passage retrieval.
---

# AUEB-Archimedes at RIRAG-2025: Is obligation concatenation really all you need?

## Quick Facts
- arXiv ID: 2412.11567
- Source URL: https://arxiv.org/abs/2412.11567
- Reference count: 10
- Primary result: Achieves 0.947 RePASs score by concatenating obligation sentences, revealing metric vulnerability

## Executive Summary
This paper presents systems for the RIRAG-2025 shared task, which requires answering regulatory questions by retrieving relevant passages and generating answers evaluated using RePASs, a reference-free metric. The authors propose three systems that combine BM25 with domain-specific neural retrievers and a reranker for passage retrieval. Their answer generation approaches range from direct obligation concatenation (achieving suspiciously high scores) to iterative refinement using RePASs as a verifier. The work demonstrates both the effectiveness of their retrieval approach and the vulnerability of RePASs to adversarial exploitation through obligation concatenation.

## Method Summary
The systems use a three-stage approach: passage retrieval combines BM25 with two domain-specific embedding models (voyage-law-2, voyage-finance-2) using Rank Fusion and a reranker; answer generation includes three variants (NOC: concatenating obligations, LOC: LLM rewriting obligations, VRR: iterative refinement using RePASs); evaluation uses the RePASs metric measuring obligation coverage, entailment, and contradiction. The VRR system iteratively refines answers by removing contradictions and adding missing obligations, achieving the best balance of coherence and performance.

## Key Results
- NOC system achieves 0.947 RePASs score by directly concatenating obligation sentences, though answers are incoherent
- VRR system achieves 0.639 RePASs score through iterative refinement while producing more plausible answers
- Domain-specific neural retrievers (VL2, VF2) outperform generic models and BM25 in passage retrieval
- RePASs vulnerability allows near-perfect scores through simple obligation concatenation, questioning metric robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Concatenating extracted "obligation" sentences directly into an answer achieves high RePASs scores by exploiting the metric's obligation extraction component.
- Mechanism: The RePASs metric includes a neural LegalBERT model that extracts "obligation" sentences from retrieved passages. When answers are formed by simply concatenating these extracted obligation sentences, they automatically align well with the metric's obligation extraction, leading to high obligation coverage scores.
- Core assumption: RePASs's obligation extraction component will identify the concatenated obligation sentences as valid obligations, and these sentences will not trigger high contradiction scores.
- Evidence anchors:
  - [abstract]: "we achieve a dubiously high score (0.947), even though the answers are directly extracted from the retrieved passages and are not actually generated answers"
  - [section 4.3]: "From the definition of RePASs (Section 2), this answer should get an almost perfect obligation score. Additionally, we expect a low contradiction score, as obligations should not conflict."
  - [corpus]: Weak - no direct evidence found in corpus about this specific exploitation mechanism
- Break condition: If the obligation extraction component changes to require more contextual coherence, or if the concatenated obligations contain contradictions that trigger the NLI model's contradiction detection.

### Mechanism 2
- Claim: Iterative refinement using RePASs as a verifier improves answer quality by reducing contradictions and increasing obligation coverage.
- Mechanism: The VRR system generates multiple answer candidates, uses RePASs to select the best one, then iteratively refines it by identifying and removing sentences with high contradiction scores and adding missing obligations from the retrieved passages.
- Core assumption: RePASs accurately identifies contradictions and missing obligations, and the refinement steps progressively improve the answer quality.
- Evidence anchors:
  - [abstract]: "Our third system works by a) generating multiple candidate answers and using RePASs to select the best answer, and b) iteratively refining the selected answer by reducing contradictions and covering more obligations"
  - [section 4.5.2]: "To remove contradictions: a) we compute the average contradiction score over all the answers... and b) we remove the sentences of the answer that get a contradiction score higher than the average."
  - [section 4.5.2]: "To locate missing obligations, we extract obligations from the retrieved passages and the current answer. Obligations from the retrieved passages that are not covered... are missing obligations."
  - [corpus]: Weak - no direct evidence found in corpus about this iterative refinement approach
- Break condition: If RePASs becomes unreliable as a verifier, or if the iterative refinement introduces more contradictions than it removes.

### Mechanism 3
- Claim: Using domain-specific embedding models (VL2, VF2) combined with BM25 and a reranker improves retrieval performance compared to generic models.
- Mechanism: The system combines BM25 for exact term matching with two domain-specific neural retrievers (voyage-law-2 for legal domain and voyage-finance-2 for finance domain), then applies a reranker to the top-N results.
- Core assumption: Domain-specific models capture domain-specific semantics better than generic models, and combining them with BM25 captures both semantic similarity and exact term matching.
- Evidence anchors:
  - [section 3.1]: "We experiment with BM25... and three of the best text embedding models: text-embedding-3-large (OL3) from OpenAI... voyage-law-2 (VL2), and voyage-finance-2 (VF2) from Voyage."
  - [section 3.1]: "we expand Rank Fusion to handle three retrievers instead of two, as follows. f(p) = aˆsx(p) + bˆsy(p) + (1-(a+b))ˆsz(p)"
  - [table 1]: Shows domain-specific models (VL2, VF2) outperform BM25 and generic OL3
  - [table 2]: Shows triple Rank Fusion with BM25, VL2, VF2 performs best
  - [corpus]: Weak - no direct evidence found in corpus about this specific combination approach
- Break condition: If the domain-specific models become outdated or if the reranking step introduces noise that degrades performance.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG) systems
  - Why needed here: The task requires answering regulatory questions by retrieving relevant passages and generating answers, which is the core functionality of RAG systems
  - Quick check question: How does RAG differ from traditional question answering systems that rely solely on pre-trained knowledge?

- Concept: Reference-free evaluation metrics for RAG
  - Why needed here: RePASs is a reference-free metric that evaluates answers without ground truth references, which is crucial for this task's evaluation
  - Quick check question: What are the advantages and disadvantages of reference-free metrics compared to reference-based metrics?

- Concept: Rank Fusion techniques for combining multiple retrievers
  - Why needed here: The system uses Rank Fusion to combine BM25 with two domain-specific neural retrievers, which is a key architectural decision
  - Quick check question: How does Rank Fusion handle the different scoring scales of different retrieval models?

## Architecture Onboarding

- Component map: Passage Retrieval (BM25 + VL2 + VF2 + reranker) → Answer Generation (NOC/LOC/VRR) → RePASs Evaluation
- Critical path: Retrieve passages → Extract obligations → Generate answer → Evaluate with RePASs → (for VRR: iterate refinement)
- Design tradeoffs: NOC achieves high scores but produces incoherent answers; LOC improves readability but reduces scores; VRR balances coherence and performance but requires iterative processing
- Failure signatures: NOC fails when RePASs changes its obligation extraction; VRR fails when RePASs becomes unreliable as a verifier; retrieval fails when domain-specific models don't capture relevant semantics
- First 3 experiments:
  1. Compare single retrieval models (BM25, OL3, VL2, VF2) on public test set
  2. Compare Rank Fusion configurations on public test set
  3. Investigate effect of reranking top-N passages for different N values on Recall@10

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can RePASs be made more robust against adversarial attacks that exploit its obligation extraction component?
- Basis in paper: [explicit] The authors demonstrate that their NOC system achieves near-perfect RePASs scores by directly concatenating obligation sentences, even though the answers are incoherent and don't actually answer the questions.
- Why unresolved: The paper shows the vulnerability but doesn't propose solutions to make RePASs more robust against such exploitation.
- What evidence would resolve it: Developing and testing modifications to RePASs that penalize verbatim obligation extraction or require semantic coherence between obligation sentences would show whether the metric can be made more robust.

### Open Question 2
- Question: How does the performance of VRR compare to human experts when evaluated with reference-based metrics rather than RePASs?
- Basis in paper: [inferred] The authors note that VRR achieves the highest RePASs score among systems that don't exceed human performance, suggesting that systems with super-human RePASs scores may be gaming the metric. They also show that VRR produces coherent answers.
- Why unresolved: The paper only compares systems using RePASs, which may be susceptible to adversarial exploitation as demonstrated by NOC.
- What evidence would resolve it: Evaluating VRR and other systems using reference-based metrics like ROUGE or BLEU on a set of questions with human-written reference answers would show how they truly compare to human performance.

### Open Question 3
- Question: Would incorporating a contradiction detection mechanism during the obligation extraction phase improve the overall quality and RePASs score of generated answers?
- Basis in paper: [explicit] The authors observe that NOC achieves excellent contradiction scores because obligations should not conflict, but VRR requires iterative refinement steps to reduce contradictions after generation.
- Why unresolved: The paper treats obligation extraction and contradiction handling as separate steps, with contradiction handling only applied after answer generation in VRR.
- What evidence would resolve it: Modifying the obligation extraction process to filter out or flag potentially contradictory obligations, then comparing the RePASs scores and coherence of answers generated from these filtered obligations versus the original unfiltered set would show if early contradiction detection is beneficial.

## Limitations
- The vulnerability of RePASs to exploitation through obligation concatenation raises questions about the metric's robustness for evaluating answer quality, as high scores may not reflect coherent or useful answers
- GPU-dependent variations in RePASs scoring (noted by the authors) introduce reproducibility challenges for validation
- The effectiveness of the iterative refinement approach depends heavily on RePASs being a reliable verifier, which may not hold if the metric changes or becomes less reliable

## Confidence
- **High Confidence**: Retrieval performance improvements from domain-specific models and Rank Fusion - supported by quantitative results in Tables 1 and 2
- **Medium Confidence**: VRR system's iterative refinement approach - theoretically sound but depends on RePASs reliability as a verifier
- **Low Confidence**: RePASs metric vulnerability - while demonstrated, the extent to which this represents a general weakness versus task-specific artifact is unclear

## Next Checks
1. **RePASs Metric Robustness Test**: Systematically evaluate whether obligation concatenation continues to yield high scores when using different NLI models or modified obligation extraction components
2. **GPU Variation Investigation**: Document RePASs score variations across different GPU configurations to quantify the reproducibility challenge
3. **Human Evaluation Validation**: Conduct human evaluation of NOC, LOC, and VRR system outputs to determine if RePASs scores correlate with actual answer quality and coherence