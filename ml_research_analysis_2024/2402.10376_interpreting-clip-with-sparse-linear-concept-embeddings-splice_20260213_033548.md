---
ver: rpa2
title: Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)
arxiv_id: '2402.10376'
source_url: https://arxiv.org/abs/2402.10376
tags:
- concept
- clip
- splice
- concepts
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The authors introduce SpLiCE, a method for decomposing CLIP\u2019\
  s dense embeddings into sparse, interpretable combinations of human-readable concepts.\
  \ They formulate the problem as sparse recovery and enforce non-negativity and sparsity\
  \ to maintain interpretability."
---

# Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)

## Quick Facts
- **arXiv ID**: 2402.10376
- **Source URL**: https://arxiv.org/abs/2402.10376
- **Reference count**: 40
- **Primary result**: SpLiCE decomposes CLIP embeddings into sparse, interpretable concept combinations while maintaining high zero-shot accuracy and enabling bias detection and model editing.

## Executive Summary
This paper introduces SpLiCE, a method for decomposing CLIP's dense image embeddings into sparse, interpretable linear combinations of human-readable concepts. The approach formulates decomposition as a sparse recovery problem using LASSO with non-negativity and sparsity constraints, enabling task-agnostic interpretation without retraining. Experiments across multiple datasets demonstrate that SpLiCE maintains high zero-shot accuracy while producing semantically meaningful, highly sparse decompositions. The method enables two key applications: detecting spurious correlations in datasets (e.g., gender bias in CIFAR100) and editing models by intervening on concept representations.

## Method Summary
SpLiCE decomposes CLIP image embeddings into sparse linear combinations of concepts by solving a LASSO optimization problem with non-negativity constraints. The method uses an overcomplete vocabulary derived from LAION captions (10k single-word + 5k two-word concepts), centers image embeddings to align modalities, and solves the ℓ0-constrained problem using ADMM-based optimization. The approach is task-agnostic and can replace or explain CLIP representations without training. Key components include CLIP encoders, LAION concept vocabulary, LASSO solver with non-negativity, and modality alignment via mean-centering.

## Key Results
- Maintains high zero-shot accuracy and retrieval performance while producing highly sparse (l0 norm 5-20) and semantically meaningful decompositions
- Successfully detects spurious correlations in datasets, such as gender bias in CIFAR100
- Enables model editing through intervention on concept representations with equivalent performance to linear probe modifications
- Ablation studies confirm non-negativity, modality alignment, and large semantic dictionary are essential for performance

## Why This Works (Mechanism)

### Mechanism 1
Dense CLIP embeddings can be decomposed into sparse linear combinations of human-interpretable concepts using LASSO with ℓ1 penalty and non-negativity constraints. This works because CLIP embeddings are approximately linear in concept space and capture semantic content without non-semantic noise.

### Mechanism 2
SpLiCE maintains high performance while yielding sparse decompositions because the overcomplete LAION vocabulary spans possible embeddings and modality alignment by mean-centering enables nonnegative decomposition. The LAION vocabulary is sufficiently diverse and alignment effectively resolves distribution shifts.

### Mechanism 3
SpLiCE enables bias detection and model editing because interpretable decompositions accurately reflect semantic content, allowing users to identify spurious correlations and intervene on concept representations to modify model behavior.

## Foundational Learning

- **Sparse recovery and LASSO optimization**: Used to enforce sparsity and non-negativity in concept decompositions. Quick check: How does LASSO with ℓ1 penalty enforce sparsity in the solution?
- **Linear representation hypothesis**: CLIP embeddings are approximately linear in concept space, enabling decomposition into sparse linear combinations. Quick check: What evidence supports the linear representation hypothesis for CLIP embeddings?
- **Modality alignment**: Mean-centering aligns image and text embeddings to enable nonnegative decomposition over text-based vocabulary. Quick check: How does mean-centering align image and text embeddings in CLIP?

## Architecture Onboarding

- **Component map**: CLIP image/text encoders -> LAION concept vocabulary -> LASSO solver with non-negativity -> Modality alignment (mean-centering)
- **Critical path**: Embed image with CLIP → align modalities by mean-centering → solve LASSO optimization to find sparse concept weights → construct interpretable concept decomposition
- **Design tradeoffs**: Vocabulary size and sparsity level affect interpretability-accuracy tradeoff; larger vocabularies and higher sparsity may improve accuracy but reduce interpretability
- **Failure signatures**: LASSO solver convergence failure leads to inaccurate or non-sparse decompositions; ineffective modality alignment prevents nonnegative decomposition
- **First 3 experiments**: 1) Verify LASSO with non-negativity recovers known sparse linear combinations, 2) Check modality alignment improves cosine similarity between image/text embeddings, 3) Test interpretability-accuracy tradeoff by varying vocabulary size and sparsity level

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions beyond the scope of the current work, focusing instead on demonstrating the core capabilities of SpLiCE.

## Limitations

- Strong assumption that CLIP embeddings are approximately linear in concept space may not hold for all domains
- Decomposition quality heavily dependent on LAION vocabulary completeness and potential biases
- Modality alignment via mean-centering is heuristic and may not fully resolve distribution shifts
- Reliance on cosine similarity may not comprehensively capture perceptual or semantic fidelity

## Confidence

- **High**: Ability to produce sparse, interpretable concept decompositions maintaining zero-shot accuracy (directly demonstrated across multiple datasets)
- **Medium**: Claims regarding spurious correlation detection and model editing capabilities (shown in specific cases but may not generalize)
- **Low**: Broader interpretability claims, particularly regarding user studies (less rigorously validated and subjective)

## Next Checks

1. Evaluate SpLiCE on out-of-distribution datasets (medical imaging, satellite imagery) to test generality of linear representation hypothesis
2. Conduct controlled user study comparing SpLiCE explanations to ground-truth annotations for semantic content
3. Test robustness to concept vocabulary size and composition by systematically varying number and diversity of concepts