---
ver: rpa2
title: 'Unified Uncertainties: Combining Input, Data and Model Uncertainty into a
  Single Formulation'
arxiv_id: '2406.18787'
source_url: https://arxiv.org/abs/2406.18787
tags:
- uncertainty
- input
- epistemic
- data
- aleatoric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to propagate input uncertainty through
  neural networks, estimating input, data, and model uncertainty simultaneously. The
  approach uses either Monte Carlo sampling or a Taylor approximation to propagate
  input uncertainty through the model, combined with existing uncertainty estimation
  methods for epistemic uncertainty.
---

# Unified Uncertainties: Combining Input, Data and Model Uncertainty into a Single Formulation

## Quick Facts
- arXiv ID: 2406.18787
- Source URL: https://arxiv.org/abs/2406.18787
- Reference count: 19
- One-line primary result: Proposes a unified framework for simultaneously estimating input, aleatoric, and epistemic uncertainty in neural networks using either Monte Carlo sampling or Taylor approximation for input uncertainty propagation

## Executive Summary
This paper introduces a novel formulation for combining input uncertainty (IU), aleatoric uncertainty (AU), and epistemic uncertainty (EU) into a single uncertainty estimation framework for neural networks. The method propagates input uncertainty through the network using either Monte Carlo sampling or a Taylor approximation, then combines it with existing epistemic uncertainty estimation techniques. Experiments on synthetic datasets demonstrate that this unified approach produces more stable decision boundaries and better uncertainty estimates compared to traditional Monte Carlo sampling, particularly under high input noise conditions.

## Method Summary
The unified uncertainty formulation combines three types of uncertainty estimation in a two-step process: first applying an epistemic uncertainty estimation method (MC-Dropout, MC-DropConnect, ensembles, or Flipout) to generate M samples, then propagating input uncertainty through these samples using either Monte Carlo sampling (NM forward passes) or a Taylor approximation (M forward passes with Jacobian computation). The Taylor approximation computes output uncertainty as the propagated input uncertainty transformed by the network's Jacobian matrix, while Monte Carlo sampling directly estimates output uncertainty through repeated sampling. This approach claims to transform input uncertainty into model (epistemic) uncertainty at the output, creating a unified representation of all uncertainty sources.

## Key Results
- The Taylor approximation method for IU propagation is computationally efficient (O(M)) compared to Monte Carlo sampling (O(NM))
- Input uncertainty, when propagated through the model, transforms into model uncertainty at the outputs
- The unified formulation produces more stable decision boundaries and better uncertainty estimates under high input noise compared to Monte Carlo sampling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Input uncertainty (IU) propagates through the network and transforms into model (epistemic) uncertainty at the output
- Mechanism: The Taylor approximation propagates IU through the network's Jacobian, effectively treating input noise as a source of parameter uncertainty during inference
- Core assumption: The network's learned function can represent and transform input noise into uncertainty about its own predictions
- Evidence anchors:
  - [abstract]: "we argue that input uncertainty, when propagated through the model, results in model uncertainty at the outputs"
  - [section 2.1]: "When propagating IU through a model, the output uncertainties σo_inp and σo_epi are EUs, as they correspond to uncertainty due to the model's equations"
  - [corpus]: Weak - no direct mention of this specific transformation, but related work on input uncertainty propagation exists.
- Break condition: If the network's learned representation cannot distinguish between true uncertainty and input noise, or if the Jacobian approximation fails for highly nonlinear layers.

### Mechanism 2
- Claim: The Taylor approximation method for IU propagation is computationally efficient compared to Monte Carlo sampling
- Mechanism: The Taylor approximation only requires computing the Jacobian matrix once and propagating uncertainty analytically, while MC sampling requires multiple forward passes
- Core assumption: The first-order Taylor approximation is sufficiently accurate for the network's activation functions and input noise levels
- Evidence anchors:
  - [section 2]: "This makes sampling infeasible on large models and datasets, while using the Taylor approximation incurs minimal additional cost"
  - [section 2.1]: "For IU propagation using a Taylor approximation, the cost is O(M), as only EU estimation contribute to increased costs"
  - [corpus]: Weak - no direct mention of computational efficiency comparisons.
- Break condition: If the network has highly nonlinear activation functions where higher-order terms in the Taylor expansion are significant, or if input noise is very large relative to the input scale.

### Mechanism 3
- Claim: The unified formulation allows simultaneous estimation of aleatoric, epistemic, and input uncertainty
- Mechanism: By first applying epistemic uncertainty estimation (via MC-Dropout, ensembles, etc.) and then propagating input uncertainty through the resulting samples, the method creates a combined uncertainty representation
- Core assumption: The epistemic uncertainty estimation method produces meaningful samples that can be used for further uncertainty propagation
- Evidence anchors:
  - [abstract]: "a novel formulation combining AU, EU, and IU into a single model and uncertainty estimation method"
  - [section 2.1]: "We combine these by first applying the EU estimation method, and then the IU propagation method"
  - [corpus]: Weak - no direct mention of unified formulations combining all three uncertainty types.
- Break condition: If the epistemic uncertainty estimation method is poorly calibrated or produces samples that are not representative of the true parameter uncertainty.

## Foundational Learning

- Concept: Bayesian Neural Networks and their approximations (MC-Dropout, MC-DropConnect, Flipout)
  - Why needed here: These methods provide the foundation for estimating epistemic uncertainty, which is a key component of the unified formulation
  - Quick check question: How does MC-Dropout approximate a Bayesian Neural Network, and what is the role of the dropout probability?

- Concept: Uncertainty propagation through neural networks (Taylor approximation vs Monte Carlo sampling)
  - Why needed here: The paper proposes a specific method for propagating input uncertainty through the network, which is central to the unified formulation
  - Quick check question: What are the advantages and disadvantages of using a Taylor approximation versus Monte Carlo sampling for uncertainty propagation?

- Concept: Aleatoric vs epistemic uncertainty
  - Why needed here: The paper explicitly distinguishes between these two types of uncertainty and proposes a method for estimating both simultaneously
  - Quick check question: What is the fundamental difference between aleatoric and epistemic uncertainty, and why is it important to distinguish between them in machine learning?

## Architecture Onboarding

- Component map: Input -> 4-layer MLP with ReLU -> Epistemic uncertainty estimation (MC-Dropout, MC-DropConnect, Ensemble, Flipout) -> Output with combined uncertainty
- Critical path:
  1. Input data with uncertainty is fed into the network
  2. Epistemic uncertainty estimation is applied (M forward passes)
  3. Input uncertainty is propagated through the network (using Taylor approximation or Monte Carlo sampling)
  4. Output uncertainty is computed and combined with aleatoric uncertainty (if applicable)
  5. Loss is computed and backpropagated to update network parameters

- Design tradeoffs:
  - Taylor approximation vs Monte Carlo sampling for IU propagation: Computational efficiency vs accuracy
  - Choice of epistemic uncertainty estimation method: Computational cost vs quality of uncertainty estimates
  - Network architecture: Depth and width vs ability to capture complex relationships between input and output uncertainty

- Failure signatures:
  - Decision boundary becomes noisy or loses shape under high input uncertainty (indicates IU propagation failure)
  - Predicted input uncertainty does not increase with ground truth input uncertainty (indicates IU estimation failure)
  - Epistemic uncertainty does not increase under high input uncertainty (indicates EU estimation failure)

- First 3 experiments:
  1. Train a simple network on the two moons dataset with added input noise, using both Taylor approximation and Monte Carlo sampling for IU propagation. Compare decision boundaries and uncertainty estimates.
  2. Train a network on a synthetic regression dataset with known input and output uncertainty. Compare the predicted uncertainties to the ground truth.
  3. Train a network on a real-world dataset with known sensor noise characteristics. Evaluate the performance of the unified uncertainty formulation compared to a baseline without input uncertainty.

## Open Questions the Paper Calls Out

The paper acknowledges several limitations and open questions:
- The experiments only test with input Gaussian noise, while real-world corruptions might follow distributions that considerably deviate from Gaussian noise
- The computational complexity analysis is theoretical and based on a simple four-layer MLP; practical performance on large-scale models may differ
- The method's effectiveness depends on the accuracy of the Jacobian approximation, which may break down for highly nonlinear activation functions or very large input noise

## Limitations
- Method effectiveness depends heavily on Jacobian approximation accuracy, which may fail for highly nonlinear activation functions
- Computational efficiency gains are theoretically sound but not empirically validated across different architectures
- Unified formulation assumes epistemic uncertainty methods produce representative samples for further propagation

## Confidence
- High confidence in theoretical framework and mathematical formulation
- Medium confidence in empirical validation due to limited experiments on synthetic datasets only
- Medium confidence in computational efficiency claims without broader benchmarking

## Next Checks
1. Test the method on real-world datasets with known sensor noise characteristics to validate practical applicability
2. Compare uncertainty estimates against ground truth on synthetic regression problems with controlled noise levels
3. Evaluate the breakdown point where Taylor approximation accuracy degrades compared to Monte Carlo sampling