---
ver: rpa2
title: Amortized Posterior Sampling with Diffusion Prior Distillation
arxiv_id: '2407.17907'
source_url: https://arxiv.org/abs/2407.17907
tags:
- diffusion
- inverse
- posterior
- data
- prior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Amortized Posterior Sampling (APS), a novel
  variational inference approach for efficient posterior sampling in inverse problems.
  The method trains a conditional flow model to minimize the divergence between the
  variational distribution and the posterior distribution implicitly defined by a
  diffusion model.
---

# Amortized Posterior Sampling with Diffusion Prior Distillation

## Quick Facts
- arXiv ID: 2407.17907
- Source URL: https://arxiv.org/abs/2407.17907
- Authors: Abbas Mammadov; Hyungjin Chung; Jong Chul Ye
- Reference count: 18
- Primary result: APS achieves 23.37 PSNR on CelebA denoising in 0.0021 seconds, compared to 16.95 seconds for previous methods

## Executive Summary
This paper proposes Amortized Posterior Sampling (APS), a novel variational inference approach for efficient posterior sampling in inverse problems. The method trains a conditional flow model to minimize the divergence between the variational distribution and the posterior distribution implicitly defined by a diffusion model. This results in a powerful, amortized sampler capable of generating diverse posterior samples with a single neural function evaluation, generalizing across various measurements. Unlike existing methods, APS is unsupervised, requires no paired training data, and is applicable to both Euclidean and non-Euclidean domains.

## Method Summary
APS trains a conditional normalizing flow to approximate the posterior distribution in inverse problems. The method uses a diffusion model as a prior and optimizes the flow parameters to minimize KL divergence between the variational distribution and the true posterior. The key innovation is the use of an ELBO approximation to avoid expensive probability-flow ODE solves during training, while still maintaining the ability to generate diverse posterior samples with a single forward pass through the network. The approach is demonstrated across image restoration, manifold signal reconstruction, and climate data imputation tasks.

## Key Results
- Achieves 23.37 PSNR on CelebA denoising in 0.0021 seconds (vs 16.95 seconds for previous methods)
- Outperforms existing approaches in computational efficiency while maintaining competitive reconstruction quality
- Demonstrates generalization across measurements without re-training for specific conditions

## Why This Works (Mechanism)

### Mechanism 1
Amortized conditional normalizing flows can learn a universal posterior sampler that generalizes across all measurements without re-training. The variational inference objective is modified to include the measurement y as conditioning input to the flow model, enabling a single conditional flow to map noise vectors and measurements to posterior samples across the entire dataset.

### Mechanism 2
The ELBO approximation for log p_θ(x₀) enables efficient training without expensive PF-ODE solves. Instead of exactly computing log p_θ(x₀) by solving the probability-flow ODE, the method uses an evidence lower bound based on denoising score matching, which can be estimated using the pre-trained diffusion model without solving ODEs.

### Mechanism 3
Single NFE posterior sampling achieves computational efficiency while maintaining competitive quality through parallelizable diverse sampling. Once trained, the conditional flow model can generate multiple posterior samples by inputting different noise vectors z ~ N(0,I) concatenated with the measurement y, requiring only a single forward pass through the network.

## Foundational Learning

- Concept: Variational Inference and KL Divergence
  - Why needed here: The method frames posterior sampling as minimizing KL divergence between the variational distribution q_ϕ(x₀|y) and the true posterior p_θ(x₀|y)
  - Quick check question: What does minimizing KL(q_ϕ||p_θ) encourage the variational distribution to do?

- Concept: Normalizing Flows and Invertible Transformations
  - Why needed here: The proposal distribution is a conditional normalizing flow that must be invertible to compute likelihoods efficiently and allow easy sampling by transforming noise vectors
  - Quick check question: Why is invertibility important for the flow model in this context?

- Concept: Diffusion Models and Score Matching
  - Why needed here: The diffusion model provides the prior distribution through its score function, and training involves denoising score matching to approximate likelihood under this prior
  - Quick check question: How does denoising score matching relate to the ELBO approximation used here?

## Architecture Onboarding

- Component map: y,z → G_ϕ → x₀ → (optional) evaluation of log p_θ(x₀) via ELBO
- Critical path: Measurement y and noise vector z are input to conditional flow G_ϕ to produce posterior sample x₀
- Design tradeoffs:
  - Single vs. multiple NFE: Sacrifices some reconstruction quality for massive speed gains
  - Flow capacity vs. generalization: Larger flow models may capture more complex posteriors but could overfit
  - ELBO tightness vs. computational cost: Tighter bounds require more accurate score function estimation but increase training complexity
- Failure signatures:
  - Mode collapse: Generated samples lack diversity, indicating flow model cannot capture posterior variation
  - Poor generalization: Performance degrades significantly on measurements not seen during training
  - Training instability: ELBO optimization fails to converge or produces unrealistic samples
- First 3 experiments:
  1. Implement and train the conditional flow on synthetic data with known posteriors to verify amortization works
  2. Test the trained model on held-out measurements to confirm generalization capability
  3. Compare single NFE sampling quality against iterative baselines on a simple inverse problem like denoising

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions but identifies areas for future work including exploring alternative flow architectures and optimizing network design choices.

## Limitations
- Computational advantage comes with potential trade-off in reconstruction quality compared to iterative methods
- Performance depends heavily on quality and expressiveness of pre-trained diffusion model serving as prior distribution
- Generalization to highly diverse or out-of-distribution measurements may be limited

## Confidence
- High confidence: Computational efficiency claims are well-supported by reported inference times and comparison with iterative baselines
- Medium confidence: Generalization capabilities across different measurements are demonstrated but could benefit from more extensive testing
- Medium confidence: Unsupervised nature and lack of paired training data requirements are theoretically sound but would benefit from more ablation studies

## Next Checks
1. Conduct ablation study on flow capacity by systematically varying number of flow steps and coupling layers to determine minimum architecture required for acceptable performance
2. Evaluate APS on measurements with significantly different characteristics from training data (extreme noise levels or unusual degradation patterns) to assess robustness limits
3. Measure gap between ELBO approximation and true diffusion prior likelihood on validation samples to quantify impact of computational shortcut on training accuracy