---
ver: rpa2
title: 'TPP-Gaze: Modelling Gaze Dynamics in Space and Time with Neural Temporal Point
  Processes'
arxiv_id: '2410.23409'
source_url: https://arxiv.org/abs/2410.23409
tags:
- tpp-gaze
- scanpath
- deepgazeiii
- fixation
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TPP-Gaze introduces a neural temporal point process model for predicting
  human scanpaths that jointly learns fixation positions and durations. The method
  integrates deep learning with point process theory, using a CNN backbone to extract
  image semantics and either a GRU or Transformer to encode fixation history.
---

# TPP-Gaze: Modelling Gaze Dynamics in Space and Time with Neural Temporal Point Processes

## Quick Facts
- arXiv ID: 2410.23409
- Source URL: https://arxiv.org/abs/2410.23409
- Reference count: 40
- Primary result: Neural Temporal Point Process model for predicting human scanpaths that jointly learns fixation positions and durations, outperforming state-of-the-art approaches

## Executive Summary
TPP-Gaze introduces a novel approach to modeling human scanpath dynamics by treating fixations as realizations of marked temporal point processes. The method integrates deep learning with point process theory, using a CNN backbone to extract image semantics and either a GRU or Transformer to encode fixation history. By jointly modeling both fixation positions and durations, TPP-Gaze achieves superior performance across five eye-tracking datasets, with KL divergence values around 0.03-0.06 and string edit distances around 7-17.

## Method Summary
TPP-Gaze models scanpaths as sequences of events where each fixation consists of a 2D position and duration. The model uses a DenseNet201 backbone with CoordConv layers to extract semantic image representations, which are combined with history embeddings from a GRU or Transformer encoder. Fixation positions are modeled using a Gaussian mixture model (GMM) while durations use a log-Gaussian mixture model (LGMM). The model is trained using negative log-likelihood of the joint distribution of positions and durations, and can be extended to visual search tasks by incorporating a readout network.

## Key Results
- Outperforms state-of-the-art methods in scanpath prediction metrics (MM, SM, SS) with KL divergence values around 0.03-0.06
- Achieves string edit distances around 7-17 across five datasets (COCO-FreeView, MIT1003, OSIE, NUSEF, FiFa)
- Strong saliency prediction performance with AUC values around 0.84-0.88
- Successfully extended to visual search tasks with improved return fixation analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TPP-Gaze jointly models fixation positions and durations by treating scanpaths as realizations of marked temporal point processes.
- Mechanism: The model uses a neural TPP framework where the conditional joint distribution of the next fixation position and duration is parameterized via deep learning. Fixation positions are modeled as a 2D Gaussian mixture model (GMM) and durations as a log-Gaussian mixture model (LGMM), both conditioned on the image semantics and history embedding.
- Core assumption: Scanpaths can be represented as sequences of events (fixations) with continuous marks (positions) and inter-event times (durations), aligning with the mathematical structure of marked TPPs.
- Evidence anchors:
  - [abstract]: "we present TPP-Gaze, a novel and principled approach to model scanpath dynamics based on Neural Temporal Point Process (TPP), that jointly learns the temporal dynamics of fixations position and duration"
  - [section 3.1]: "We model the conditional dependence of the distribution pθ(τn+1|hn, zj) on both past events and stimulus by concatenating the history embedding and semantic vectors into a context vector cj,n"
- Break condition: If the assumption that fixations are discrete events with independent positions and durations fails (e.g., strong dependencies between consecutive fixations), the GMM/LGMM parameterization may become insufficient.

### Mechanism 2
- Claim: The semantic representation of the image is extracted via a CNN backbone with CoordConv layers to preserve spatial information.
- Mechanism: A DenseNet201 is used to extract multi-scale feature maps from the image, which are then passed through CoordConv layers to provide the CNN access to spatial coordinates, followed by 1x1 convolutions and a linear layer to produce the semantic representation zj.
- Core assumption: Spatial information is critical for accurately predicting fixation locations, and standard convolutions without coordinate information are insufficient for this task.
- Evidence anchors:
  - [section 3.1]: "we adopt a CoordConv layer [41] to give convolutions access to their own input coordinates. This results in a 2, 051 channels volume which is fed as input to 3 layers of 1 × 1 convolutions, followed by a linear layer mapping to zj"
  - [corpus]: No direct evidence found in corpus neighbors; the claim relies on the original CoordConv paper.
- Break condition: If the semantic representation becomes too abstract or loses spatial detail, the model's ability to predict precise fixation locations will degrade.

### Mechanism 3
- Claim: History is encoded using either a GRU or Transformer to capture nonlinear dependencies over both marks and timings from past events.
- Mechanism: The sequence of past events (fixation positions and durations) is fed into a GRU or Transformer encoder, which produces a history embedding hn that captures the influence of past events on the next fixation.
- Core assumption: The future fixation depends nonlinearly on the history of past fixations, and this dependency can be effectively captured by recurrent or self-attention mechanisms.
- Evidence anchors:
  - [section 3.1]: "the pair (rFn , τn) representing the event occurring at the time tn with fixation position rFn and duration τn = tn − tn−1, is fed as the input into either a GRU or a Transformer encoder"
  - [section 2.1]: "Neural TPPs employ either Recurrent Neural Networks (RNNs) and their variants (e.g., LSTM, GRU) [24, 48, 55] or Transformer encoders [64, 65] to model the nonlinear dependency over both the markers and the timings from past events"
- Break condition: If the history encoding becomes too short or too long, it may fail to capture relevant temporal dependencies or overfit to noise.

## Foundational Learning

- Concept: Marked Temporal Point Processes (TPPs)
  - Why needed here: Scanpaths are naturally modeled as sequences of events with continuous marks (positions) and inter-event times (durations), which aligns with the mathematical structure of marked TPPs.
  - Quick check question: What is the difference between a standard TPP and a marked TPP, and why is the marked version necessary for scanpath modeling?

- Concept: Gaussian Mixture Models (GMMs) and Log-Gaussian Mixture Models (LGMMs)
  - Why needed here: GMMs are used to model the 2D fixation positions, while LGMMs model the fixation durations, allowing the model to capture the multimodal nature of human attention.
  - Quick check question: How do GMMs and LGMMs differ in their parameterization and what types of data are each best suited for?

- Concept: Semantic image representation and CoordConv
  - Why needed here: The model needs to extract meaningful features from the image that can guide fixation predictions, and CoordConv helps preserve spatial information that is critical for this task.
  - Quick check question: Why might standard convolutions fail at mapping from Cartesian coordinates to fixation locations, and how does CoordConv address this issue?

## Architecture Onboarding

- Component map: Image → DenseNet201 → CoordConv → 1x1 convs → zj → concatenate with hn → GMM/LGMM parameters → sample next fixation and duration
- Critical path: Image → CNN backbone → CoordConv → 1x1 convs → zj → concatenate with hn → GMM/LGMM parameters → sample next fixation and duration
- Design tradeoffs:
  - CNN backbone choice: DenseNet201 vs ResNet50 - DenseNet201 provides better performance but may be more computationally expensive
  - History encoding: GRU vs Transformer - Transformer may capture longer-range dependencies but is more computationally intensive
  - GMM/LGMM components: Number of mixture components affects model expressiveness and computational cost
- Failure signatures:
  - Poor scanpath similarity scores (MM, SM, SS) indicate issues with position prediction
  - Inaccurate fixation duration predictions indicate issues with duration modeling
  - High KL divergence between real and simulated distributions indicates overall model failure
- First 3 experiments:
  1. Train with only position prediction (fix duration to constant) to isolate the impact of the semantic representation
  2. Train with only duration prediction (fix position to center of image) to isolate the impact of history encoding
  3. Compare GRU vs Transformer for history encoding to understand the impact of different architectures on performance

## Open Questions the Paper Calls Out

The paper does not explicitly call out any open questions in the abstract or introduction. However, based on the methodology and results presented, several questions arise regarding the model's scalability, robustness to different viewing conditions, and computational efficiency.

## Limitations

- Strong parametric assumptions: The GMM/LGMM parameterization assumes unimodal or mildly multimodal distributions, which may not capture complex fixation patterns in all scenarios.
- Computational requirements: The model requires significant computational resources due to the deep learning components and the need to sample from GMM/LGMM distributions during inference.
- Limited evaluation of visual search extension: While the paper mentions the ability to extend TPP-Gaze to visual search tasks, this capability is only briefly mentioned and not extensively evaluated.

## Confidence

- Scanpath prediction performance: High
- Semantic representation and history encoding mechanisms: Medium
- Visual search extension: Low

## Next Checks

1. Conduct ablation studies to isolate the impact of the semantic representation (e.g., by comparing with a baseline that uses a simpler CNN backbone without CoordConv).
2. Perform a sensitivity analysis on the number of GMM/LGMM components to determine the optimal balance between model expressiveness and computational cost.
3. Evaluate the model's performance on a more diverse set of eye-tracking datasets, including those with different viewing conditions and tasks, to assess its generalizability.