---
ver: rpa2
title: Artificial Scientific Discovery
arxiv_id: '2411.11672'
source_url: https://arxiv.org/abs/2411.11672
tags:
- learning
- asif
- training
- page
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This thesis explores the development of artificial scientists,
  examining their knowledge discovery and communication capabilities. It presents
  three key contributions: Olivaw, an Othello-playing AI demonstrating autonomous
  learning without human knowledge; Explanatory Learning, a framework formalizing
  scientific explanation and symbol interpretation; and ASIF, a method for creating
  multimodal models without training by leveraging pre-trained unimodal encoders.'
---

# Artificial Scientific Discovery

## Quick Facts
- arXiv ID: 2411.11672
- Source URL: https://arxiv.org/abs/2411.11672
- Reference count: 0
- This thesis explores the development of artificial scientists, examining their knowledge discovery and communication capabilities.

## Executive Summary
This thesis investigates the development of artificial scientists, focusing on their ability to autonomously discover knowledge and communicate findings. It presents three key contributions: Olivaw, an Othello-playing AI demonstrating autonomous learning without human knowledge; Explanatory Learning, a framework formalizing scientific explanation and symbol interpretation; and ASIF, a method for creating multimodal models without training by leveraging pre-trained unimodal encoders. The work argues that true artificial scientists must both discover novel insights and communicate them effectively, proposing ASIF as a powerful baseline for foundation multimodal models that achieves competitive performance with traditional methods while enabling rapid model updates and interpretable representations.

## Method Summary
The thesis introduces ASIF (Aligned Semantic Interpreters with Few examples), a method for creating multimodal models without training by aligning pre-trained unimodal encoders using relative representations and coupled data. The approach computes embeddings for image-text pairs using pre-trained encoders, constructs relative representations by comparing each sample against the dataset, and uses these processed representations to assign labels in zero-shot classification tasks. The method requires only a small collection of image-text pairs and two frozen pre-trained encoders, avoiding the need for contrastive training while achieving competitive performance with established models like CLIP.

## Key Results
- ASIF achieves competitive zero-shot classification performance on standard vision datasets (ImageNet, CIFAR-100) using two frozen pre-trained encoders and a fraction of the image-text pairs used by CLIP.
- The framework demonstrates interpretable representations that allow rapid model updates by adding or removing image-text pairs without retraining.
- Explanatory Learning and Critical Rationalist Networks successfully learn symbol interpretation from observation-explanation pairs, enabling autonomous discovery of new explanations.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: An artificial scientist requires both autonomous discovery and communicative capability to be truly functional.
- Mechanism: The thesis traces a progression from purely self-play learning (Olivaw) to communicative learning (Explanatory Learning) to transparent multimodal integration (ASIF), showing that knowledge must be interpretable and transmittable to be valuable.
- Core assumption: Knowledge that cannot be communicated is not functionally knowledge for a collaborative scientific enterprise.
- Evidence anchors:
  - [abstract] "This thesis explores the development of artificial scientists, examining their knowledge discovery and communication capabilities."
  - [section 2.8] "Olivaw’s discovered Othello knowledge remains trapped in the weights of its neural network, and is only able to produce board evaluations... This is a fundamental limitation of the AlphaGo Zero paradigm as a model for a scientist."

### Mechanism 2
- Claim: Symbol interpretation and meaning attribution can be learned autonomously from paired observation-explanation data without pre-coded grammars.
- Mechanism: Explanatory Learning (EL) replaces rigid interpreters with learned ones, using a small set of explanation-observation pairs to build a dynamic map between symbols and their meanings, enabling discovery of new explanations.
- Core assumption: The ability to interpret symbols is not innate but learnable from data, and this interpretation is central to scientific reasoning.
- Evidence anchors:
  - [section 3.2] "We formulate the challenge of creating a machine that masters a language as the problem of learning an interpreter from a collection of examples in the form explanation-observations."
  - [section 3.4] "Our Critical Rationalist Networks (CRNs) tackle the EL scientist problem... They are formed by two independently trained models: a stochastic Conjecture Generator and a learned Interpreter."

### Mechanism 3
- Claim: Multimodal alignment can be achieved without contrastive training by leveraging relative representations induced by coupled data.
- Mechanism: ASIF uses pre-trained unimodal encoders and a small set of image-text pairs to construct interpretable, sparse relative representations, enabling zero-shot classification without tuning neural weights.
- Core assumption: Captions of similar images are themselves similar, and this semantic coherence can be exploited to create a common space.
- Evidence anchors:
  - [section 4.1] "The key insight is that captions of similar images are themselves similar... a representation crafted using just similarities to ground-truth multimodal pairs is quasi mode-invariant."
  - [section 4.4.2] "Remarkably, we achieve competitive performance with CLIP and LiT using two frozen pretrained encoders and a fraction of the image-text pairs."

## Foundational Learning
- Concept: AlphaGo Zero and reinforcement learning self-play
  - Why needed here: Provides the baseline for autonomous discovery without human knowledge, motivating the communication gap explored in the thesis.
  - Quick check question: How does AlphaGo Zero generate training data, and what is the role of the Monte Carlo Tree Search?

- Concept: Interpretable machine learning and explainability
  - Why needed here: Central to the thesis's argument that an artificial scientist must not only discover but also explain; informs the design of CRNs and ASIF.
  - Quick check question: What distinguishes a learned interpreter from a pre-coded one in the context of symbol interpretation?

- Concept: Multimodal representation learning and contrastive training
  - Why needed here: Background for understanding how models like CLIP work and why ASIF's training-free approach is novel and significant.
  - Quick check question: What is the purpose of aligning image and text embeddings in a common space, and how is this typically achieved?

## Architecture Onboarding
- Component map: Olivaw (RL agent with MCTS + neural oracle) → Explanatory Learning (CG + I models) → ASIF (pre-trained encoders + relative representation construction)
- Critical path: Data generation (self-play or paired examples) → Model training (RL, EL, or none for ASIF) → Inference (MCTS, conjecture testing, or relative representation comparison)
- Design tradeoffs: Autonomy vs. communicability (Olivaw vs. EL), training cost vs. interpretability (CLIP vs. ASIF), flexibility vs. performance (learned vs. hardcoded interpreters)
- Failure signatures: Inability to generate novel, verifiable insights (too rigid), poor generalization across symbol sets (insufficient learning), degraded performance with domain shift (overfitting to training data)
- First 3 experiments:
  1. Implement Olivaw-style MCTS with a simple neural network oracle on a small board game; verify learning progression.
  2. Build a CRN prototype for a toy EL problem (e.g., simple rule discovery); test NRS and T-Acc.
  3. Construct an ASIF model using pre-trained encoders and a small image-text dataset; evaluate zero-shot classification vs. CLIP.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can artificial scientists be developed to effectively communicate their discoveries to humans?
- Basis in paper: [explicit] The thesis identifies this as a key challenge, noting that Olivaw mastered Othello but could not communicate its knowledge, and discusses the need for autonomous symbol interpretation in Explanatory Learning.
- Why unresolved: While Explanatory Learning and Critical Rationalist Networks are proposed as frameworks, the practical implementation and effectiveness of these models in real-world scientific communication remain untested and require further research.
- What evidence would resolve it: Demonstrating that an artificial scientist model can not only discover knowledge but also explain it in a way that is understandable and useful to human scientists, validated through peer review or practical application.

### Open Question 2
- Question: What is the role of memory and retrieval in machine learning, and how can they be effectively integrated with learning algorithms?
- Basis in paper: [explicit] The ASIF model demonstrates that a common space can be created without training by using relative representations and a collection of coupled data, raising questions about the effectiveness of storing information only in the weights of a model.
- Why unresolved: While ASIF shows promise, the long-term implications of combining learning representations with external memories are not fully understood, and further research is needed to optimize this approach and explore its potential applications.
- What evidence would resolve it: Empirical studies comparing the performance of models that integrate memory and retrieval with traditional learning algorithms, demonstrating clear advantages in terms of efficiency, interpretability, or generalization.

### Open Question 3
- Question: How can Large Language Models be improved to better align with scientific practice and overcome their limitations in terms of truth, ignorance awareness, and critical assessment?
- Basis in paper: [explicit] The thesis discusses the tension between the functioning of LLMs and the principles of scientific practice, highlighting their propensity for hallucination, inability to gauge their own ignorance, and uncritical acceptance of input data.
- Why unresolved: While the Symbol Interpretation Task reveals the shortcomings of current LLMs, developing effective strategies to address these issues and improve their alignment with scientific reasoning remains a significant challenge.
- What evidence would resolve it: Demonstrating that an improved LLM can accurately assess its own knowledge limitations, critically evaluate input data, and generate scientifically valid explanations, validated through rigorous testing and peer review.

## Limitations
- The communication requirement for scientific knowledge may be overstated, as some discoveries might be verifiable without full interpretability.
- Explanatory Learning depends heavily on the availability of paired observation-explanation data, which may not exist for many scientific domains.
- ASIF's performance advantage relies on the semantic coherence assumption between captions of similar images, which may break down in specialized or abstract domains.

## Confidence
- **High Confidence**: The fundamental limitation of AlphaGo Zero as a model for scientific discovery (Olivaw cannot communicate its knowledge) is well-established and uncontroversial.
- **Medium Confidence**: The claim that symbol interpretation can be learned autonomously is supported by the CRN experiments, but the framework's scalability to complex scientific languages remains unproven.
- **Low Confidence**: ASIF's competitive performance against CLIP and LiT is impressive, but the experiments rely on specific pre-trained encoders and datasets; generalization to other domains or encoder pairs is uncertain.

## Next Checks
1. **Domain Transfer Test for ASIF**: Apply ASIF to a non-visual scientific domain (e.g., protein structure prediction with text descriptions) to test the robustness of relative representations beyond standard image classification tasks.
2. **Explainability Benchmark for EL**: Design a benchmark where CRNs must discover and communicate novel physical laws from simulated experiments, measuring both discovery accuracy and explanation quality against human scientists.
3. **Minimum Data Threshold for EL**: Systematically vary the number of observation-explanation pairs in the EL framework to determine the minimum data requirement for successful symbol learning, informing practical deployment constraints.