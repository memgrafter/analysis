---
ver: rpa2
title: Exploring Optimal Transport-Based Multi-Grained Alignments for Text-Molecule
  Retrieval
arxiv_id: '2411.11875'
source_url: https://arxiv.org/abs/2411.11875
tags:
- molecule
- retrieval
- representations
- alignments
- motif
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses cross-modal text-molecule retrieval, a critical
  task in bioinformatics for identifying molecular candidates from textual descriptions.
  Existing approaches often overlook detailed molecular substructure information,
  limiting retrieval precision.
---

# Exploring Optimal Transport-Based Multi-Grained Alignments for Text-Molecule Retrieval

## Quick Facts
- arXiv ID: 2411.11875
- Source URL: https://arxiv.org/abs/2411.11875
- Reference count: 35
- Hits@1 score of 66.5% on ChEBI-20 text-molecule retrieval, outperforming AMAN by 17.1%

## Executive Summary
This paper addresses the challenge of cross-modal text-molecule retrieval in bioinformatics by proposing ORMA (Optimal Transport-based Multi-grained Alignments), a novel model that leverages multi-grained representations of both text and molecules. The approach models molecules as hierarchical heterogeneous graphs (atom, motif, and molecule nodes) and uses optimal transport to align tokens with motifs, creating rich multi-token representations. Experimental results on ChEBI-20 and PCdes datasets demonstrate significant performance improvements over state-of-the-art methods, achieving substantial gains in retrieval accuracy through three-level contrastive learning at token-atom, multi-token-motif, and sentence-molecule scales.

## Method Summary
ORMA employs a hierarchical approach to cross-modal text-molecule retrieval by constructing multi-grained representations for both modalities. For text, it uses SciBERT to generate token-level and sentence-level representations. For molecules, it builds hierarchical heterogeneous graphs with atom, motif, and molecule nodes, where motifs are extracted using the BRICS algorithm. The key innovation is applying optimal transport to align tokens with motifs, creating multi-token representations that integrate multiple token alignments with their corresponding motifs. Contrastive learning is then used to refine cross-modal alignments at three scales: token-atom, multi-token-motif, and sentence-molecule. This multi-level alignment strategy captures both fine-grained substructure information and global semantic relationships between text descriptions and molecular structures.

## Key Results
- Achieves 66.5% Hits@1 score on ChEBI-20 text-molecule retrieval, outperforming AMAN by 17.1%
- Achieves 61.6% Hits@1 score on molecule-text retrieval, outperforming AMAN by 15.0%
- Demonstrates consistent performance improvements across multiple metrics (Hits@10, MRR, Mean Rank) on both ChEBI-20 and PCdes datasets

## Why This Works (Mechanism)
The method works by capturing hierarchical molecular information through multi-grained representations and aligning them with corresponding text representations using optimal transport. By decomposing molecules into atoms, motifs, and whole molecules, ORMA can match text descriptions to both specific functional groups and overall molecular structure. The optimal transport-based alignment between tokens and motifs creates rich multi-token representations that bridge the gap between textual descriptions and molecular substructures, while the three-level contrastive learning framework ensures both local and global semantic consistency across modalities.

## Foundational Learning
- **Hierarchical molecular graph construction**: Molecules are modeled as heterogeneous graphs with atom, motif, and molecule nodes to capture multi-scale structural information. Why needed: Single-level representations miss important substructure details that are crucial for accurate retrieval.
- **BRICS algorithm for motif extraction**: BRICS (Breaking of Retrosynthetically Interesting Chemical Substructures) decomposes molecules into meaningful functional groups. Quick check: Verify motif quality by measuring coverage of known functional groups in benchmark molecules.
- **Optimal transport for cross-modal alignment**: OT provides a principled way to align tokens with motifs by minimizing transport costs between distributions. Quick check: Compare OT alignment quality against simpler methods like cosine similarity on alignment accuracy.
- **Multi-grained contrastive learning**: Three-level contrastive learning (token-atom, multi-token-motif, sentence-molecule) captures both fine-grained and global semantic relationships. Quick check: Analyze performance impact by removing each contrastive level individually.
- **SciBERT text encoding**: Domain-specific BERT variant for scientific text provides better molecular context understanding. Quick check: Compare performance with general BERT variants on domain-specific vocabulary.
- **GCN molecule encoding**: Graph Convolutional Networks capture local molecular structure and atom interactions. Quick check: Validate feature propagation by analyzing attention weights on known functional groups.

## Architecture Onboarding
- **Component map**: Text (SciBERT) -> Token embeddings + Sentence embeddings -> Optimal Transport -> Multi-token representations -> Contrastive learning; Molecule (GCN) -> Atom, motif, molecule nodes -> Graph embeddings -> Contrastive learning
- **Critical path**: Text encoding → Optimal transport alignment → Multi-token representation → Contrastive learning → Retrieval
- **Design tradeoffs**: Hierarchical graph vs flat representation (better substructure capture vs complexity), Optimal transport vs attention (principled alignment vs computational efficiency), Three-level vs two-level contrastive learning (richer semantics vs training complexity)
- **Failure signatures**: Poor retrieval if motif extraction misses key functional groups; Ineffective alignment if token-motif correspondences are unclear; Low performance if contrastive learning weights are unbalanced
- **First experiments**: 1) Validate motif extraction quality on known functional groups, 2) Test optimal transport alignment with synthetic token-motif pairs, 3) Evaluate single-level contrastive learning before adding complexity

## Open Questions the Paper Calls Out
- **Open Question 1**: How does ORMA's performance scale with increasing dataset size, and what are the limits of its effectiveness on extremely large molecular databases? The paper demonstrates strong performance on ChEBI-20 and PCdes datasets but does not explore performance on larger or more diverse molecular databases. Testing ORMA on larger datasets like PubChem or ChEMBL, and analyzing performance metrics (e.g., Hits@1, MRR) as dataset size grows, would provide insights into scalability and limitations.
- **Open Question 2**: How does the Optimal Transport