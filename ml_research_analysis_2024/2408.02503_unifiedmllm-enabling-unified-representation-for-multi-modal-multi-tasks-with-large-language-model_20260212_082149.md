---
ver: rpa2
title: 'UnifiedMLLM: Enabling Unified Representation for Multi-modal Multi-tasks With
  Large Language Model'
arxiv_id: '2408.02503'
source_url: https://arxiv.org/abs/2408.02503
tags:
- tasks
- image
- arxiv
- task
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UnifiedMLLM, a unified multi-modal model
  that handles various tasks through a common representation using task tokens and
  grounding tokens. The model employs a task router to activate corresponding expert
  models for task execution, decoupling the LLM from downstream tasks to enhance scalability.
---

# UnifiedMLLM: Enabling Unified Representation for Multi-modal Multi-tasks With Large Language Model

## Quick Facts
- arXiv ID: 2408.02503
- Source URL: https://arxiv.org/abs/2408.02503
- Reference count: 17
- Primary result: Achieves 26.667 PSNR, 0.808 SSIM, and 21.104 CLIP Score on reasoning editing tasks

## Executive Summary
UnifiedMLLM introduces a unified multi-modal model that handles various tasks through a common representation using task tokens and grounding tokens. The model employs a task router to activate corresponding expert models for task execution, decoupling the LLM from downstream tasks to enhance scalability. A three-stage training strategy is used: modality-perception pretraining, task adaptation tuning, and multi-task LoRA MoE tuning. The model is trained on a task-specific dataset and a 100k multi-turn multi-task dataset covering complex scenarios. UnifiedMLLM demonstrates superior performance across multiple multi-modal tasks, including reasoning editing, layout-based image generation, and multi-modality generation, outperforming existing methods.

## Method Summary
UnifiedMLLM employs a three-stage training strategy: first modality-perception pretraining, then task adaptation tuning, and finally multi-task LoRA MoE tuning. The model uses task tokens and grounding tokens to represent different tasks and regions, with a task router that activates corresponding expert models (such as Stable Diffusion and InstructPix2Pix) for task execution. The architecture decouples the LLM from expert models, allowing seamless integration of different tasks across multiple modalities without additional training. The model processes image, video, and audio inputs through modality-specific encoders and adapters before routing to the appropriate expert models.

## Key Results
- Achieves 26.667 PSNR, 0.808 SSIM, and 21.104 CLIP Score on reasoning editing tasks
- Demonstrates superior performance across multiple multi-modal tasks compared to existing methods
- Shows exceptional scalability and generality through unified representation enabling seamless task integration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unified representation via task tokens and grounding tokens enables seamless integration of multiple tasks without additional training
- Mechanism: The model outputs special tokens indicating task type and region, which are routed to corresponding expert models, decoupling LLM from downstream tasks
- Core assumption: Task tokens can uniquely encode both task type and spatial region in a unified format
- Evidence anchors:
  - [abstract] "Our model also outputs task tokens and grounding tokens, serving as indicators of task types and task granularity."
  - [section] "To achieve this, we expand the vocabularies of the LLM and introduce multiple task-specific tokens and grounding tokens, which appear in pairs (e.g., <Edit></Edit>). The content between task tokens indicates the task to be executed, while the content between grounding tokens contains region-relative coordinates expressed in text format."
  - [corpus] Weak - no direct citations found for task token routing mechanisms
- Break condition: Task tokens cannot uniquely encode complex or novel tasks, or expert models fail to handle routed tasks correctly

### Mechanism 2
- Claim: Three-stage training strategy preserves general knowledge while enhancing task-specific capabilities
- Mechanism: First stage trains modality perception, second stage adapts to tasks with frozen encoders, third stage fine-tunes with LoRA-MoE while freezing all parameters except LoRA
- Core assumption: Freezing backbone prevents catastrophic forgetting while LoRA-MoE allows task adaptation
- Evidence anchors:
  - [abstract] "Employing a three-stage training strategy, we equip our model with robust reasoning and task processing capabilities while preserving its generalization capacity and knowledge reservoir."
  - [section] "We adopt a strategy where the backbone of the model is frozen to preserve its capabilities. Additionally, multiple expert models are introduced to handle various downstream tasks. We employ LoRA as the structure for the expert models..."
  - [corpus] Weak - no direct citations found for this specific three-stage training with LoRA-MoE combination
- Break condition: Freezing parameters too early prevents necessary fine-tuning, or LoRA parameters become insufficient for complex task adaptation

### Mechanism 3
- Claim: Decoupling LLM from expert models reduces training costs and improves scalability
- Mechanism: LLM generates task descriptions and regions, then task router activates external expert models for execution, avoiding LLM retraining for new tasks
- Core assumption: Expert models can handle diverse tasks efficiently when properly routed
- Evidence anchors:
  - [abstract] "This representation approach facilitates seamless integration of different tasks across multiple modalities. Furthermore, decoupling the LLM from the subsequent expert models not only reduces training costs but also ensures excellent scalability."
  - [section] "The task router will activate the corresponding expert model to perform the task based on the special tokens. This representation approach facilitates seamless integration of different tasks across multiple modalities."
  - [corpus] Weak - no direct citations found for this specific routing architecture
- Break condition: Expert models become bottlenecks or cannot handle novel tasks, or routing becomes too complex to maintain

## Foundational Learning

- Concept: Multi-modal feature extraction and fusion
  - Why needed here: The model must process image, video, and audio inputs through different encoders before combining them for LLM input
  - Quick check question: What are the three modalities supported and their respective encoders?

- Concept: Mixture of Experts (MoE) and LoRA integration
  - Why needed here: Third training stage uses LoRA-MoE to add task-specific capabilities without forgetting general knowledge
  - Quick check question: How does LoRA-MoE differ from standard MoE in parameter efficiency?

- Concept: Task routing and token-based instruction
  - Why needed here: The model must interpret special tokens to activate appropriate expert models
  - Quick check question: What is the format of task tokens and grounding tokens, and how are they paired?

## Architecture Onboarding

- Component map: Image encoder (ViT-L/14) → Image adapter → LLM, Video encoder (Q-Former) → Video adapter → LLM, Audio encoder → Audio adapter → LLM, LLM → Task router → Expert models (Stable Diffusion, InstructPix2Pix, etc.)
- Critical path: Input modality → Encoder → Adapter → LLM → Task tokens/Grounding tokens → Task router → Expert model → Output
- Design tradeoffs: Decoupling enables scalability but adds routing complexity; LoRA-MoE reduces training cost but may limit expressivity
- Failure signatures: Incorrect routing (wrong expert activated), poor task understanding (wrong tokens generated), expert model failure (output not usable)
- First 3 experiments:
  1. Test single modality input through full pipeline to verify encoder-adapter-LLM flow
  2. Generate task tokens for a known task and verify routing to correct expert
  3. Train on small task-specific dataset and measure performance on held-out samples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the task router handle ambiguity when multiple expert models could potentially address a given task?
- Basis in paper: [explicit] The paper describes a task router that routes task tokens to corresponding expert models but does not detail how it resolves ambiguity when multiple experts could handle a task
- Why unresolved: The paper does not provide specific details on the decision-making process of the task router when faced with ambiguous tasks
- What evidence would resolve it: A detailed description of the task router's decision-making algorithm, including how it prioritizes or selects among multiple potential expert models for a given task

### Open Question 2
- Question: What are the specific limitations of using external expert models in terms of scalability and performance, and how do these compare to an end-to-end trainable multi-modal system?
- Basis in paper: [explicit] The paper acknowledges that the model primarily relies on external models to accomplish various multi-modal tasks, which constrains the scope and effectiveness of the model
- Why unresolved: The paper does not provide a detailed comparison of the limitations and performance trade-offs between using external expert models versus an end-to-end trainable system
- What evidence would resolve it: Empirical studies comparing the performance and scalability of the current approach with external expert models to a hypothetical end-to-end trainable multi-modal system

### Open Question 3
- Question: How does the model handle interleaved multi-modal inputs and outputs, and what are the challenges associated with this capability?
- Basis in paper: [explicit] The paper mentions that the model mainly focuses on processing single-modal inputs and that effectively handling multi-modal information simultaneously or interleaved is a challenge
- Why unresolved: The paper does not provide a detailed exploration of the model's capabilities or limitations in handling interleaved multi-modal inputs and outputs
- What evidence would resolve it: Experimental results demonstrating the model's performance on tasks involving interleaved multi-modal inputs and outputs, along with an analysis of the challenges and potential solutions

## Limitations

- Weak citation support for core architectural innovations including task token routing, three-stage LoRA-MoE training, and expert model decoupling
- Exceptional performance metrics (26.667 PSNR, 0.808 SSIM) lack baseline comparisons on standardized benchmarks
- Reliance on external expert models introduces uncontrolled performance dependencies and scalability concerns

## Confidence

**High Confidence**: The general concept of unified multi-modal representation and the three-stage training strategy are well-grounded in existing literature. The modular architecture separating modality encoders, LLM, and expert models follows established design patterns.

**Medium Confidence**: The specific implementation of task tokens and grounding tokens as a unified representation mechanism is plausible but lacks direct empirical validation. The performance improvements over existing methods are claimed but not independently verified.

**Low Confidence**: The scalability claims regarding expert model decoupling and the effectiveness of the LoRA-MoE integration for preserving general knowledge while enabling task-specific adaptation remain largely theoretical without extensive ablation studies or stress tests.

## Next Checks

1. **Ablation Study on Training Stages**: Remove the three-stage training strategy and train the model end-to-end on the same datasets. Compare performance to determine if the staged approach provides measurable benefits in knowledge preservation and task adaptation.

2. **Expert Model Dependency Analysis**: Replace the expert models (Stable Diffusion, InstructPix2Pix) with simpler baselines or random models. Test whether the task routing and token generation still function correctly, isolating the contribution of expert model quality versus routing mechanism effectiveness.

3. **Generalization Test on Novel Tasks**: Design and implement a completely new task not present in the training datasets. Evaluate whether the task token system can correctly route to an appropriate (potentially new) expert model, testing the claimed scalability and generality of the decoupled architecture.