---
ver: rpa2
title: Conditioning diffusion models by explicit forward-backward bridging
arxiv_id: '2405.13794'
source_url: https://arxiv.org/abs/2405.13794
tags:
- diffusion
- conditional
- sample
- samples
- given
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a new method, called forward-backward bridging\
  \ (FBB), for exact conditional sampling within diffusion models, addressing the\
  \ problem of sampling from \u03C0(x|y) given a joint diffusion model \u03C0(x,y).\
  \ The key idea is to treat the conditioning problem as an inference problem on an\
  \ augmented space corresponding to a partial SDE bridge."
---

# Conditioning diffusion models by explicit forward-backward bridging

## Quick Facts
- arXiv ID: 2405.13794
- Source URL: https://arxiv.org/abs/2405.13794
- Reference count: 40
- Key outcome: New method for exact conditional sampling within diffusion models using forward-backward bridging

## Executive Summary
This paper introduces forward-backward bridging (FBB), a method for exact conditional sampling from diffusion models. The approach treats conditioning as an inference problem on an augmented space corresponding to a partial SDE bridge, alternating between forward noising and conditional sequential Monte Carlo (CSMC) to form a Gibbs sampler. Unlike existing methods, FBB provides asymptotically exact samples while correcting for biases from finite particles and finite time horizons.

## Method Summary
The method addresses conditional sampling from π(x|y) given a joint diffusion model π(x,y) by formulating it as an inference problem on an augmented space. It alternates between forward noising (generating trajectories via SDE simulation) and backward CSMC (sampling conditional trajectories), forming a Gibbs sampler. For separable Gaussian dynamics, the method can be adapted to run in-place with zero memory cost using a pseudo-marginal approach based on particle Markov chain Monte Carlo (PMCMC) with preconditioned Crank-Nicolson proposals.

## Key Results
- FBB provides asymptotically exact conditional sampling within the approximate diffusion model
- The method corrects for biases arising from finite particle numbers and finite time horizons
- Demonstrates improved sample quality and performance compared to existing approaches on synthetic and real data examples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditioning diffusion models via forward-backward bridging enables exact conditional sampling within the approximate diffusion model
- Mechanism: The method treats the conditioning problem as an inference problem on an augmented space corresponding to a partial SDE bridge, alternating between forward noising and CSMC to form a Gibbs sampler
- Core assumption: The joint diffusion model π(x,y) is correct and can be trusted; the noising process is tractable and separable
- Evidence anchors: Abstract states it "express[es] exact conditional simulation within the approximate diffusion model as an inference problem on an augmented space"; section claims it "does not introduce any additional approximation to the unconditional diffusion model aside from the Monte Carlo error"
- Break condition: If the forward noising process is not tractable or separable, or if the joint model π(x,y) contains errors

### Mechanism 2
- Claim: The Gibbs sampler formed by alternating forward noising and backward CSMC corrects for biases from finite particles and finite time horizons
- Mechanism: By treating conditioning as inference on the joint p(x0, xT, yT | Y0 = y), the method leverages dynamic structure to iteratively refine samples through forward and conditional backward distributions
- Core assumption: The particle filter CSMC algorithm is properly implemented with sufficient particles N
- Evidence anchors: Abstract notes it "does not introduce any additional approximation to the unconditional diffusion model aside from the Monte Carlo error"; section describes taking "a more principled approach of treating the conditioning problem as an inference problem on the joint"
- Break condition: If N is too small to adequately approximate the filtering distribution, or if ergodicity is violated

### Mechanism 3
- Claim: When the noising process is separable and Gaussian, the method can be adapted to run in-place at zero memory cost using a pseudo-marginal approach
- Mechanism: The particle filter returns an unbiased estimate of the marginal likelihood, used in a Metropolis-Hastings step to sample from the posterior distribution of the path X0:K
- Core assumption: The forward noising process follows the form dY = A_t Y dt + Σ_t dW with block diagonal Σ_t and A_t, and πT ≡ πref
- Evidence anchors: Section states "When the forward noising process is given by an SDE of the form (7), the conditional trajectories F(· | xT, yT, x0, y0) are tractable"; describes implementing "a pseudo-marginal counterpart of Section 2, known as the particle Markov chain Monte Carlo (PMCMC) method"
- Break condition: If the forward noising process is not separable or Gaussian, or if πT ≠ πref

## Foundational Learning

- Concept: Diffusion models and Schrödinger bridges
  - Why needed here: The method builds on these foundations to perform conditional sampling
  - Quick check question: Can you explain the difference between denoising diffusion models and Schrödinger bridges?

- Concept: Particle filters and conditional sequential Monte Carlo (CSMC)
  - Why needed here: CSMC is the key algorithm for implementing the backward pass in the Gibbs sampler
  - Quick check question: What is the difference between a standard particle filter and CSMC?

- Concept: Markov chain Monte Carlo (MCMC) and Gibbs sampling
  - Why needed here: The overall method forms a Gibbs sampler by alternating between forward and backward passes
  - Quick check question: How does a Gibbs sampler differ from other MCMC methods like Metropolis-Hastings?

## Architecture Onboarding

- Component map:
  Forward noising process (SDE simulation) -> Conditional sequential Monte Carlo (CSMC) algorithm -> Gibbs sampler framework (alternating passes) -> (Optional) Pseudo-marginal MCMC for separable cases

- Critical path:
  1. Initialize with unconditional diffusion model π(x,y)
  2. Perform forward noising pass to generate trajectory Y0:K
  3. Run CSMC algorithm to sample from backward distribution
  4. Alternate between forward and backward passes to form Gibbs sampler
  5. (Optional) Use pseudo-marginal approach for separable cases to reduce memory

- Design tradeoffs:
  - Memory vs. accuracy: Storing full trajectories vs. online bridge simulation
  - Particle count vs. computational cost: More particles reduce bias but increase computation
  - Gibbs iterations vs. convergence: More iterations improve sample quality but increase runtime

- Failure signatures:
  - Particle coalescence in backward pass (all particles collapse to single point)
  - Poor acceptance rates in pseudo-marginal approach (high variance in likelihood estimates)
  - Slow mixing in Gibbs sampler (high autocorrelation between samples)

- First 3 experiments:
  1. Test Gibbs-CSMC on synthetic Gaussian process regression with known ground truth
  2. Compare PF vs Gibbs-CSMC on non-separable Schrödinger bridge model
  3. Apply method to MNIST inpainting with different particle counts and observe convergence

## Open Questions the Paper Calls Out

- How can we mitigate the variance of the log-likelihood estimation in PMCMC to improve its acceptance rate and reduce the need for careful calibration of the PCN parameter δ?
- What are the theoretical guarantees for the convergence of Gibbs-CSMC to the true conditional distribution π(x|y), and how do these guarantees depend on the number of particles N and the time horizon T?
- How can we extend the FBB framework to handle nonlinear likelihood models, which are more common in practice than the linear Gaussian models considered in the paper?

## Limitations

- The method's computational cost scales poorly with particle count and dimensionality, limiting practical applicability to large-scale problems
- Pseudo-marginal adaptation for separable dynamics may suffer from poor mixing due to high variance in likelihood estimates
- The paper focuses on linear Gaussian likelihood models, with extension to nonlinear models remaining an open challenge

## Confidence

- Confidence in core theoretical claims: High - the connection between conditional sampling and inference on augmented SDE spaces is well-founded
- Confidence in empirical demonstrations: Medium - results are compelling but limited in scope to relatively small-scale problems
- Confidence in practical applicability for large-scale problems: Low - significant engineering challenges remain unaddressed

## Next Checks

1. Test FBB on higher-resolution images (e.g., 256x256) with varying particle counts to quantify the scaling behavior and identify break points
2. Compare mixing times and effective sample sizes between Gibbs-CSMC and simpler conditioning approaches on a controlled synthetic benchmark
3. Implement the pseudo-marginal version and systematically evaluate how variance in likelihood estimates affects acceptance rates and sample quality across different problem scales