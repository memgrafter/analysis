---
ver: rpa2
title: 'Rethinking Tokenization: Crafting Better Tokenizers for Large Language Models'
arxiv_id: '2403.00417'
source_url: https://arxiv.org/abs/2403.00417
tags:
- language
- tokenizers
- cognitive
- tokens
- types
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving tokenizer performance
  for large language models (LLMs), particularly for non-Latin languages like Chinese.
  The core idea is to draw inspiration from the cognitive science principle of "Least
  Effort," which suggests that humans naturally seek to reduce cognitive effort.
---

# Rethinking Tokenization: Crafting Better Tokenizers for Large Language Models

## Quick Facts
- arXiv ID: 2403.00417
- Source URL: https://arxiv.org/abs/2403.00417
- Reference count: 0
- Primary result: LiB tokenizer outperforms existing word and BPE tokenizers by reducing both token count and vocabulary size while improving BPC scores and cognitive alignment

## Executive Summary
This paper proposes the Less-is-Better (LiB) model, a new approach for large language model tokenizers inspired by the cognitive science principle of "Least Effort." The LiB model autonomously learns an integrated vocabulary consisting of subwords, words, and multiword expressions (MWEs), effectively reducing both the numbers of tokens and types. Comparative evaluations show that the LiB tokenizer outperforms existing word and Byte Pair Encoding (BPE) tokenizers, presenting an innovative method for tokenizer development. The LiB model's units are consistent with human cognitive units and perform better in Bits-per-character scores, suggesting the value of the Principle of Least Effort in enhancing LLM performance.

## Method Summary
The LiB model implements a cognitive approach to tokenizer development based on the Principle of Least Effort. It uses a dual mechanism consisting of a "Memorizer" that iteratively merges adjacent tokens into longer units, and a "Forgetter" that prunes less useful vocabulary types. This process continues until reaching a steady state where both token count and vocabulary size are minimized. The model is trained on text corpora in different languages (e.g., English, Chinese) to learn integrated units spanning subwords, words, and multiword expressions. Performance is evaluated using metrics such as Bits-per-character (BPC) scores and comparisons with existing tokenization approaches.

## Key Results
- LiB tokenizer outperforms existing word and BPE tokenizers in reducing both token count and vocabulary size
- LiB units show better Bits-per-character scores compared to baseline tokenizers
- LiB's learned units demonstrate cognitive alignment with human processing patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LiB reduces both token count and vocabulary size by learning integrated units spanning subwords, words, and MWEs
- Mechanism: The model iteratively merges adjacent tokens while simultaneously pruning low-utility types, balancing two competing pressures: minimizing token sequences (working memory load) and minimizing types (long-term memory load)
- Core assumption: Human cognitive processing naturally optimizes for minimal combined burden of token sequences and vocabulary size
- Evidence anchors:
  - [abstract] "The LiB model can autonomously learn an integrated vocabulary consisting of subwords, words, and MWEs, which effectively reduces both the numbers of tokens and types"
  - [section] "The 'Memorizer' continuously merges adjacent tokens in the corpus into new (longer) units... Conversely, the 'Forgetter' removes less useful 'junk' units from the vocabulary"
  - [corpus] Weak: Corpus neighbors focus on tokenizer evaluation frameworks and morphology but do not provide direct empirical validation of LiB's dual reduction mechanism
- Break condition: If merged units no longer reduce overall token count or if pruning removes too many types needed for language coverage

### Mechanism 2
- Claim: Cognitive plausibility of LiB units correlates with human reading behavior
- Mechanism: LiB units align with human cognitive chunking during reading, making them more efficient for language processing than purely statistical subword methods
- Core assumption: Human cognitive chunking reflects optimal language processing units that can be approximated by unsupervised learning
- Evidence anchors:
  - [abstract] "The LiB model's units are consistent with human cognitive units and perform better in Bits-per-character scores"
  - [section] "The units learned by LiB can be used to predict the eye fixation patterns of human readers, suggesting that the model's units are consistent with human cognitive units"
  - [corpus] Weak: Corpus neighbors discuss tokenizer evaluation but do not address cognitive alignment or eye-tracking validation
- Break condition: If LiB units fail to predict human reading patterns across diverse languages or text types

### Mechanism 3
- Claim: LiB implements Principle of Least Effort by balancing immediate (token reduction) and future (type reduction) cognitive burdens
- Mechanism: The model achieves equilibrium between "Memorizer" and "Forgetter" forces, optimizing for minimal combined cognitive load across processing time scales
- Core assumption: Human language processing follows Zipf's Principle of Least Effort, which can be operationalized through token/type balancing
- Evidence anchors:
  - [abstract] "Based on this principle, the paper proposes that the Less-is-Better (LiB) model could be a new approach for LLM tokenizer"
  - [section] "Applying PLE in language processing suggests minimizing cognitive burden in language learning and usage... Fewer tokens can lessen the burden of working memory storage and information decoding steps; Fewer types can alleviate the burden of long-term memory storage and retrieval"
  - [corpus] Weak: Corpus neighbors focus on tokenizer algorithms but do not explicitly discuss cognitive science principles like Least Effort
- Break condition: If the balance between token and type reduction fails to improve language model performance or becomes unstable during training

## Foundational Learning

- Concept: Subword tokenization (BPE, WordPiece, Unigram)
  - Why needed here: Understanding how current tokenizers work and their limitations is essential to appreciate LiB's innovations
  - Quick check question: What problem does subword tokenization solve that word-level tokenization cannot?

- Concept: Multiword Expressions (MWEs) and their linguistic significance
  - Why needed here: LiB specifically integrates MWEs with subwords and words, requiring understanding of why MWEs matter in language processing
  - Quick check question: Why do MWEs like "kick the bucket" pose challenges for standard tokenizers?

- Concept: Principle of Least Effort (Zipf) and its application to cognitive science
  - Why needed here: LiB is explicitly built on this principle, making it crucial to understand how it applies to tokenization
  - Quick check question: How does balancing immediate and future cognitive burdens relate to tokenizer design?

## Architecture Onboarding

- Component map: Input corpus processor -> Initial tokenization -> Memorizer/Forgetter iterations -> Stable vocabulary -> Evaluation (BPC scores, human alignment)
- Critical path: Corpus → Initial tokenization → Memorizer/Forgetter iterations → Stable vocabulary → Evaluation (BPC scores, human alignment)
- Design tradeoffs:
  - Granularity vs. coverage: Smaller units capture more text but increase token count
  - Frequency vs. utility: High-frequency types may not reduce token count effectively
  - Computational cost vs. quality: More iterations improve vocabulary but increase training time
  - Language specificity vs. generalization: Language-specific patterns may limit cross-lingual transfer
- Failure signatures:
  - Vocabulary explosion without token reduction
  - Token count remains similar to baseline tokenizers
  - Loss of important linguistic distinctions
  - Poor performance on downstream language tasks
  - Failure to generalize across different language types
- First 3 experiments:
  1. Compare LiB vs. BPE tokenization on a Chinese corpus, measuring token count, vocabulary size, and BPC scores
  2. Test LiB's ability to capture known MWEs in English (idioms, collocations) versus standard subword tokenizers
  3. Evaluate eye-tracking prediction accuracy using LiB units versus baseline tokenizations on a reading corpus

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the LiB model's performance compare to other tokenization approaches when applied to large language models (LLMs) in terms of downstream task performance?
- Basis in paper: [inferred] The paper mentions that LiB performs better in Bits-per-character scores and that evaluations on simple language models show promising results, but does not test LiB as a tokenizer for large language models.
- Why unresolved: The paper only provides preliminary results showing LiB's potential, but does not implement LiB as a tokenizer for large language models or test its performance on downstream tasks.
- What evidence would resolve it: Implementing LiB as a tokenizer for large language models and evaluating its performance on a range of downstream tasks, such as text classification, question answering, and language generation, compared to other tokenization approaches like BPE and WordPiece.

### Open Question 2
- Question: How can the LiB model be optimized for computational efficiency while maintaining its cognitive plausibility?
- Basis in paper: [explicit] The paper mentions that LiB is a cognitive model and has not been optimized for language models, suggesting a need for optimization.
- Why unresolved: The paper introduces LiB as a cognitive model but does not address the computational challenges of implementing it in large language models.
- What evidence would resolve it: Developing efficient algorithms and techniques to optimize the LiB model for large-scale language model training and inference, while preserving its ability to learn cognitive units that balance tokens and types.

### Open Question 3
- Question: How can the selection of multiword expressions (MWEs) be automated to optimize tokenizer performance?
- Basis in paper: [inferred] The paper discusses the potential benefits of incorporating MWEs into tokenizers but does not provide a concrete method for selecting MWEs.
- Why unresolved: The paper highlights the importance of MWEs but does not address the challenge of automatically identifying and selecting MWEs that would be most beneficial for tokenizer performance.
- What evidence would resolve it: Developing algorithms or techniques that can automatically identify and select MWEs based on their frequency, semantic importance, and potential to reduce the number of tokens and types in a given corpus.

## Limitations
- The cognitive alignment claims lack direct experimental validation through eye-tracking studies or psycholinguistic experiments
- Cross-linguistic robustness across morphologically diverse languages has not been systematically evaluated
- Computational complexity and scalability of the iterative Memorizer/Forgetter process remain unclear

## Confidence

- **High Confidence**: The technical mechanism of LiB (the Memorizer/Forgetter iterative process) is clearly specified and implementable. The goal of reducing both token count and vocabulary size through integrated subword/word/MWE learning is well-defined.
- **Medium Confidence**: The empirical performance improvements on BPC scores and token reduction are likely valid based on the described methodology, though specific quantitative results are not provided in the summary.
- **Low Confidence**: Claims about cognitive alignment with human processing and eye fixation prediction are not supported by direct experimental evidence in the available information.

## Next Checks

1. **Eye-Tracking Validation**: Conduct a controlled experiment comparing human reading patterns (fixation durations, saccade lengths) on texts tokenized with LiB versus BPE, measuring whether LiB units actually align with natural reading behavior across multiple subjects and text types.

2. **Cross-Linguistic Robustness Test**: Implement LiB on morphologically diverse languages (e.g., Turkish, Finnish, Vietnamese) and systematically evaluate whether the token/type reduction mechanism maintains effectiveness across different morphological typologies, comparing performance against language-specific tokenizers.

3. **Scalability Analysis**: Measure the computational cost (time and memory) of LiB training relative to BPE across different corpus sizes (from 10MB to 10GB), and evaluate whether the quality improvements justify the computational overhead in practical deployment scenarios.