---
ver: rpa2
title: The Prevalence of Neural Collapse in Neural Multivariate Regression
arxiv_id: '2409.04180'
source_url: https://arxiv.org/abs/2409.04180
tags:
- neural
- collapse
- regression
- nrc3
- epoch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper demonstrates that neural regression collapse (NRC) is
  a prevalent phenomenon in multivariate regression tasks, extending the concept of
  neural collapse beyond classification problems. Through extensive experiments across
  six diverse datasets (including robotic control, autonomous driving, and age prediction)
  and multiple network architectures (MLP and ResNet), the authors empirically establish
  three key properties of NRC: feature vectors collapse to the subspace spanned by
  principal components of the feature vectors (NRC1), feature vectors also collapse
  to the subspace spanned by the weight vectors (NRC2), and the Gram matrix of the
  weight vectors converges to a specific form dependent on the covariance matrix of
  targets (NRC3).'
---

# The Prevalence of Neural Collapse in Neural Multivariate Regression

## Quick Facts
- arXiv ID: 2409.04180
- Source URL: https://arxiv.org/abs/2409.04180
- Reference count: 40
- Primary result: Neural regression collapse (NRC) extends neural collapse from classification to regression tasks

## Executive Summary
This paper demonstrates that neural regression collapse (NRC) is a prevalent phenomenon in multivariate regression tasks, extending the concept of neural collapse beyond classification problems. Through extensive experiments across six diverse datasets and multiple network architectures, the authors empirically establish three key properties of NRC: feature vectors collapse to subspaces defined by principal components of features and weight vectors, and the Gram matrix of weight vectors converges to a specific form dependent on target covariance. The authors provide theoretical justification using the Unconstrained Feature Model (UFM), showing that NRC emerges when regularization parameters are strictly positive. This work reveals that neural collapse is a universal behavior in deep learning that extends to both classification and regression tasks.

## Method Summary
The paper investigates neural regression collapse through experiments on six datasets (Swimmer, Reacher, Hopper, CARLA 2D/1D, UTKFace) using MLP and ResNet architectures. Models are trained with L2 loss and weight decay regularization, with NRC metrics computed throughout training. The theoretical framework uses UFM, treating last-layer features as free variables during optimization. NRC properties are measured through three metrics: NRC1 (feature vector collapse to principal component subspace), NRC2 (collapse to weight vector subspace), and NRC3 (Gram matrix convergence). The experiments systematically vary regularization strength and target dimensions to characterize the collapse behavior.

## Key Results
- NRC emerges consistently across six diverse datasets spanning robotic control, autonomous driving, and age prediction tasks
- NRC properties (NRC1-NRC3) are observed in both MLP and ResNet architectures
- Theoretical analysis using UFM confirms NRC emergence when regularization parameters are strictly positive, with no collapse when regularization is zero
- The Gram matrix of weight vectors converges to a form involving the square root of the target covariance matrix Σ^(1/2)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neural Regression Collapse (NRC) emerges when regularization parameters in the Unconstrained Feature Model (UFM) are strictly positive.
- Mechanism: The UFM treats last-layer feature vectors as free variables during optimization. When regularization (λW, λH > 0) is applied, the optimization problem forces the feature vectors to collapse to subspaces defined by the principal components of the features and the weight vectors.
- Core assumption: The regularization parameters must be strictly positive; zero regularization prevents collapse.
- Evidence anchors:
  - [abstract] "We show that when the regularization parameters in the UFM model are strictly positive, then (NRC1)-(NRC3) also emerge as solutions in the UFM optimization problem."
  - [section 4.4] "From Theorem 4.3, when there is no regularization, the feature vectors do not collapse."
- Break condition: Setting λW = λH = 0 eliminates collapse, as shown in both theory and experiments.

### Mechanism 2
- Claim: NRC3 describes a specific geometric structure in the weight matrix that depends on the covariance matrix of targets.
- Mechanism: The Gram matrix of weight vectors converges to a specific form involving Σ^(1/2) - γ^(1/2)In, where Σ is the target covariance matrix and γ is a constant determined by regularization strength.
- Core assumption: The target covariance matrix Σ has full rank and positive definite.
- Evidence anchors:
  - [abstract] "The Gram matrix for the weight vectors converges to a specific functional form that depends on the covariance matrix of the targets."
  - [section 4.2] "NRC3 → 0 indicates that angles between the rows in W are influenced by Σ^(1/2)."
- Break condition: If targets are uncorrelated (Σ diagonal), NRC3 → 0 implies orthogonal weight vectors; otherwise angles depend on Σ^(1/2).

### Mechanism 3
- Claim: Feature vectors collapse to the subspace spanned by the n principal components of the feature vectors, where n is the dimension of the targets.
- Mechanism: During training, the optimization process drives feature vectors toward a lower-dimensional representation that captures the essential variation in the data, specifically the principal components corresponding to the target dimension.
- Core assumption: n << d (target dimension much smaller than feature dimension).
- Evidence anchors:
  - [abstract] "The last-layer feature vectors collapse to the subspace spanned by the n principal components of the feature vectors, where n is the dimension of the targets."
  - [section 3.1] "NRC1 = 1/M ∑||hi - proj(hi|HPCA_n)||²₂ → 0 indicates that there is feature-vector collapse."
- Break condition: If n approaches d, the collapse becomes less pronounced as the subspace approaches the full feature space.

## Foundational Learning

- Concept: Principal Component Analysis (PCA)
  - Why needed here: NRC1 requires understanding how feature vectors collapse to the subspace spanned by the n principal components of the feature vectors.
  - Quick check question: Given a matrix H of feature vectors, how would you compute the n principal components that define the collapsed subspace?

- Concept: Unconstrained Feature Model (UFM)
  - Why needed here: The theoretical explanation of NRC relies on treating last-layer features as free optimization variables rather than fixed inputs.
  - Quick check question: In the UFM framework, what is the key difference between how features are treated compared to standard neural network training?

- Concept: Covariance matrix properties
  - Why needed here: NRC3 depends on the square root of the target covariance matrix Σ^(1/2) and its eigenvalues.
  - Quick check question: If the target covariance matrix Σ has eigenvalues λ1 ≥ λ2 ≥ ... ≥ λn, what is the relationship between these eigenvalues and the regularization parameter c for which collapse occurs?

## Architecture Onboarding

- Component map:
  - Feature extractor (MLP/ResNet layers) -> Last linear layer (W matrix) -> Bias vector (b) -> Regularization terms (λW, λH) -> Loss function (L2 loss with regularization)

- Critical path:
  1. Forward pass: x → hθ(x) → Whθ(x) + b
  2. Compute L2 loss: ½M||Wh + b1ᵀ - Y||²F
  3. Add regularization: λH||H||²F/M + λW||W||²F
  4. Backpropagation through all parameters
  5. Monitor NRC metrics during training

- Design tradeoffs:
  - Regularization strength vs. collapse severity: Higher regularization → stronger collapse
  - Feature dimension d vs. target dimension n: Collapse more pronounced when n << d
  - Network depth: Deeper networks may exhibit different collapse dynamics

- Failure signatures:
  - No collapse when λW = λH = 0 (training diverges or becomes unstable)
  - Excessive collapse leading to poor generalization (over-regularization)
  - Collapse occurring too early in training (may indicate optimization issues)

- First 3 experiments:
  1. Train with λW = λH = 0 and verify no NRC1-NRC3 emergence
  2. Train with small positive regularization (e.g., λW = λH = 1e-5) and observe phase transition
  3. Vary target dimension n while keeping feature dimension d fixed to study collapse behavior

## Open Questions the Paper Calls Out

- Question: What is the relationship between neural regression collapse and model generalization?
  - Basis in paper: [explicit] "However, it is worth acknowledging that while we have gained a better understanding of the model behavior of deep regression models in the terminal phase of training, we have not addressed the connection between neural regression collapse and model generalization."
  - Why unresolved: The paper focuses on empirically establishing NRC properties and providing theoretical explanation via UFM, but does not investigate how NRC relates to generalization performance or generalization bounds.
  - What evidence would resolve it: Empirical studies comparing generalization performance of models exhibiting strong NRC vs weak/none NRC, or theoretical analysis deriving generalization bounds that explicitly depend on NRC properties.

## Limitations

- Limited to L2 loss regression tasks, leaving open questions about whether NRC occurs with other loss functions
- Theoretical connection between UFM solutions and practical neural network training could be further strengthened
- Does not explore imbalanced target distributions where some target components may be rare

## Confidence

- NRC as a general phenomenon: High - Extensive experiments across six diverse datasets and multiple architectures show consistent emergence of NRC properties
- Theoretical justification via UFM: Medium - The mathematical proofs are sound, but the assumptions about regularization parameters may not fully capture practical training dynamics
- Practical implications for model design: Medium - While the findings suggest potential benefits for regularization strategies, the impact on downstream task performance needs more exploration

## Next Checks

1. **Zero regularization boundary**: Systematically verify that NRC phenomena disappear when both λW and λH are exactly zero across all experimental conditions, particularly for networks trained to convergence
2. **Target dimension scaling**: Conduct experiments systematically varying the target dimension n while keeping feature dimension d fixed to better characterize the threshold behavior of NRC1 collapse
3. **Alternative architectures**: Test NRC emergence in transformer-based architectures and convolutional networks with different depths to establish whether the phenomenon is architecture-independent or specific to the studied models