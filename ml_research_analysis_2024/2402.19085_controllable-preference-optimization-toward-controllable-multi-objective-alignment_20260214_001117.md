---
ver: rpa2
title: 'Controllable Preference Optimization: Toward Controllable Multi-Objective
  Alignment'
arxiv_id: '2402.19085'
source_url: https://arxiv.org/abs/2402.19085
tags:
- preference
- helpfulness
- honesty
- arxiv
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the alignment tax problem in multi-objective
  alignment of large language models (LLMs), where optimizing for one objective (e.g.,
  helpfulness) can degrade performance on others (e.g., honesty or harmlessness).
  The authors propose Controllable Preference Optimization (CPO), which introduces
  explicit preference conditions as input tokens to guide the model's optimization
  direction.
---

# Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment

## Quick Facts
- arXiv ID: 2402.19085
- Source URL: https://arxiv.org/abs/2402.19085
- Authors: Yiju Guo; Ganqu Cui; Lifan Yuan; Ning Ding; Zexu Sun; Bowen Sun; Huimin Chen; Ruobing Xie; Jie Zhou; Yankai Lin; Zhiyuan Liu; Maosong Sun
- Reference count: 40
- One-line primary result: Introduces CPO framework with CPSFT and CDPO stages to achieve controllable multi-objective alignment while mitigating alignment tax

## Executive Summary
This paper addresses the alignment tax problem in multi-objective alignment of large language models (LLMs), where optimizing for one objective (e.g., helpfulness) can degrade performance on others (e.g., honesty or harmlessness). The authors propose Controllable Preference Optimization (CPO), which introduces explicit preference conditions as input tokens to guide the model's optimization direction. CPO consists of two stages: controllable preference supervised fine-tuning (CPSFT) and controllable direct preference optimization (CDPO). Experiments on Mistral-7B show that CPO achieves better controllability and performance across helpfulness, honesty, and harmlessness metrics compared to baseline methods like DPO and PPO, effectively mitigating alignment tax while preserving flexibility across objectives.

## Method Summary
CPO introduces explicit preference conditions as input tokens to guide multi-objective alignment. The method operates in two stages: CPSFT first trains the model to interpret preference tokens like `<Helpfulness:5>` by appending them to prompts during supervised fine-tuning. CDPO then refines the model using pairwise preference data with a reward function that penalizes deviation from specified conditions for controlled objectives while maximizing others. This staged approach allows selective optimization of objectives rather than simultaneous maximization of all, reducing conflicts inherent in multi-objective alignment.

## Key Results
- CPO achieves better controllability and performance across helpfulness, honesty, and harmlessness metrics compared to baseline methods like DPO and PPO
- The two-stage training pipeline (CPSFT → CDPO) effectively mitigates alignment tax while preserving flexibility across objectives
- Experiments on Mistral-7B demonstrate that CPO can selectively optimize for specific objectives without degrading performance on uncontrolled objectives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Introducing explicit preference tokens as input conditions enables the model to disambiguate multi-objective optimization by reducing the effective dimensionality of the alignment problem.
- Mechanism: By appending tokens like `<Helpfulness:5>` directly into the prompt, the model learns to condition its output generation on these tokens during CPSFT. During CDPO, the reward function is reshaped so that controlled objectives are penalized for deviation from the specified condition, while uncontrolled objectives are optimized normally. This reduces conflict between objectives by narrowing the optimization direction to a subspace defined by the given conditions.
- Core assumption: The model can correctly interpret and act on structured preference tokens without additional task-specific training beyond exposure in fine-tuning data.
- Evidence anchors:
  - [abstract] "explicitly specifies preference scores for different objectives, thereby guiding the model to generate responses that meet the requirements."
  - [section 2.1] "Ti(θ, c) = −|Pi(θ) − ci|, if i-th objective is controlled, Pi(θ), otherwise."
  - [corpus] Weak: No direct mention of token interpretability; this is inferred from the success of the approach.

### Mechanism 2
- Claim: The two-stage training pipeline (CPSFT → CDPO) allows the model to first learn structured preference conditioning, then refine its behavior through pairwise preference optimization under those conditions.
- Mechanism: CPSFT stage teaches the model to generate text conditioned on explicit preference tokens by expanding the input context. CDPO stage then fine-tunes the model using preference pairs, where the reward incorporates the deviation from the specified condition for controlled objectives and maximizes others. This staged approach decouples learning preference interpretation from learning preference ranking.
- Core assumption: Separating preference conditioning (CPSFT) from preference optimization (CDPO) improves learning stability compared to joint optimization.
- Evidence anchors:
  - [section 2.2.1] "CPSFT... enables LLMs to control the preferences of their generations."
  - [section 2.2.2] "CDPO controls a subset of value preferences while maximizing other preferences."
  - [corpus] Weak: No explicit comparison to single-stage approaches; effectiveness is inferred from reported results.

### Mechanism 3
- Claim: By framing alignment as conditional multi-objective optimization, CPO mitigates the alignment tax by allowing selective maximization of objectives rather than simultaneous maximization of all.
- Mechanism: The reward function in CDPO is designed so that when a preference condition is provided, the corresponding objective is optimized toward that condition, while others are maximized freely. This selective approach avoids the conflicts inherent in optimizing all objectives to their maxima simultaneously.
- Core assumption: Alignment trade-offs arise primarily from simultaneous optimization pressure on conflicting objectives, and removing some of that pressure reduces the trade-off.
- Evidence anchors:
  - [abstract] "mitigating the impact of the alignment tax and achieving improvements in multi-objective alignment."
  - [section 2.1] "we propose transforming human value alignment into a conditional multi-objective optimization problem."
  - [corpus] Weak: No direct causal evidence; the claim is supported by comparative performance results.

## Foundational Learning

- Concept: Multi-objective optimization and Pareto optimality
  - Why needed here: The paper explicitly frames alignment as a multi-objective problem where improving one objective can degrade others, and proposes conditional optimization as a way to navigate this trade-off.
  - Quick check question: If you have two conflicting objectives, can you find a solution that improves both simultaneously? Why or why not?

- Concept: Reinforcement learning from human feedback (RLHF) and preference optimization
  - Why needed here: CPO builds on DPO, which is a preference optimization method derived from RLHF, so understanding the reward modeling and policy optimization framework is essential.
  - Quick check question: In preference optimization, how is the reward function typically inferred from pairwise preference data?

- Concept: Supervised fine-tuning (SFT) with conditional inputs
  - Why needed here: CPSFT is a variant of SFT that conditions the model on explicit preference tokens, so understanding how conditioning affects generation is key.
  - Quick check question: How does adding structured tokens to the input affect the model's output distribution during fine-tuning?

## Architecture Onboarding

- Component map:
  Base LLM (Mistral-7B) -> CPSFT module -> CDPO module -> Preference token parser -> Reward estimator -> Data pipeline

- Critical path:
  1. Prepare CPSFT dataset with preference tokens appended to inputs
  2. Run CPSFT fine-tuning (3 epochs, lr=1e-5)
  3. Construct CDPO preference pairs with multi-preference rewards
  4. Run CDPO fine-tuning (3 epochs, lr=5e-7)
  5. Evaluate controllability and multi-objective alignment

- Design tradeoffs:
  - Token conditioning vs. parameter-efficient modules: Tokens are simpler to implement but less flexible than learned modules
  - Single-stage vs. two-stage training: Two-stage may improve stability but doubles training time
  - Fixed vs. adaptive weights (λ, ω): Fixed weights are easier to tune but may not generalize across tasks

- Failure signatures:
  - Model ignores preference tokens → CPSFT failed to establish conditioning
  - Controllability degrades with multiple conditions → Reward shaping in CDPO is incorrect
  - Performance drops on uncontrolled objectives → λ and ω weights are misbalanced

- First 3 experiments:
  1. Verify CPSFT learns to generate different responses for different preference tokens (qualitative check)
  2. Test CDPO with single-objective control to ensure reward shaping works (quantitative metrics on one objective)
  3. Evaluate multi-objective controllability with two conditions active to confirm trade-off mitigation (comparative analysis across 3H)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between controllability and performance maximization across different alignment objectives?
- Basis in paper: Explicit - Section 4.2 discusses sensitivity analysis on hyperparameters λ (controllability weight) and ω (objective importance weight)
- Why unresolved: The paper shows that excessive control can degrade generative capacity, but doesn't provide a universal optimal ratio for all scenarios
- What evidence would resolve it: Empirical testing across diverse tasks and objectives to establish optimal λ and ω ranges that balance controllability with generation quality

### Open Question 2
- Question: How does CPO's controllability mechanism affect model robustness against adversarial attacks?
- Basis in paper: Inferred - The paper mentions misuse risks where adversarial users might guide models to generate harmful content
- Why unresolved: While CPO improves controllability, the security implications of explicit preference tokens haven't been thoroughly explored
- What evidence would resolve it: Security evaluations testing whether CPO's preference tokens can be exploited for jailbreaking or other adversarial purposes

### Open Question 3
- Question: Can CPO's framework be extended beyond the "3H" objectives to handle more complex multi-objective alignment scenarios?
- Basis in paper: Explicit - The paper acknowledges that "human preferences are more than sophisticated" and "aligning AI systems with these preferences requires a nuanced understanding that extends beyond the '3H' framework"
- Why unresolved: The paper only demonstrates CPO with helpfulness, honesty, and harmlessness objectives
- What evidence would resolve it: Testing CPO with additional objectives like fairness, privacy, or task-specific goals to validate framework extensibility

### Open Question 4
- Question: What is the long-term impact of preference token conditioning on model generalization capabilities?
- Basis in paper: Inferred - The paper mentions that "excessive control can impact the generative capacity of the model" but doesn't examine long-term effects
- Why unresolved: The trade-off between controllability and creative generation capacity over extended use hasn't been characterized
- What evidence would resolve it: Longitudinal studies comparing CPO models with baseline models on generation tasks after extended deployment periods

### Open Question 5
- Question: How does CPO's performance scale with model size and complexity?
- Basis in paper: Inferred - The paper uses Mistral-7B as base model but doesn't examine performance across different model scales
- Why unresolved: The effectiveness of CPO's preference token mechanism may vary with model size and architecture
- What evidence would resolve it: Comparative experiments running CPO across different model sizes (e.g., 1B, 13B, 70B parameters) to identify scaling patterns

## Limitations

- Dataset Construction Details: The paper references UltraSafety and HH-RLHF datasets but provides limited details on their construction methodology, affecting reproducibility.
- Scalability to Complex Objectives: CPO's effectiveness on more nuanced or abstract alignment objectives beyond the three well-defined "3H" objectives remains untested.
- Generalization Across Model Architectures: Experiments are conducted exclusively on Mistral-7B, with unknown effectiveness on larger models or different architectural families.

## Confidence

**High Confidence**: The claim that introducing explicit preference tokens enables controllable multi-objective alignment is supported by both theoretical framing and empirical results.

**Medium Confidence**: The claim that CPO effectively mitigates alignment tax is supported by comparative results against DPO and PPO baselines, though the extent of tax reduction requires further validation.

**Low Confidence**: The claim that CPO can handle an arbitrary number of objectives without performance degradation is theoretical, with no experiments testing scenarios beyond three objectives.

## Next Checks

1. **Ablation Study on Training Stages**: Conduct experiments comparing CPO with only CPSFT, only CDPO, and joint training to isolate the contribution of each stage to controllability and performance.

2. **Objective Scalability Test**: Evaluate CPO on a synthetic task with 5-7 diverse objectives to assess how performance degrades as objective count increases and whether the alignment tax reappears with more complex multi-objective scenarios.

3. **Cross-Model Architecture Validation**: Implement CPO on a different base model (e.g., Llama-2 13B or GPT-NeoXT-Chat-Base) to verify that the preference token conditioning generalizes across architectures.