---
ver: rpa2
title: Data Selection via Optimal Control for Language Models
arxiv_id: '2410.07064'
source_url: https://arxiv.org/abs/2410.07064
tags:
- data
- selection
- arxiv
- https
- pre-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces PDS, a principled framework for pre-training\
  \ data selection in language models, formulated via optimal control theory using\
  \ Pontryagin\u2019s Maximum Principle. PDS estimates data quality scores in a proxy\
  \ setting and transfers them to large corpora using a learned scorer, enabling offline\
  \ data selection before LM training."
---

# Data Selection via Optimal Control for Language Models

## Quick Facts
- arXiv ID: 2410.07064
- Source URL: https://arxiv.org/abs/2410.07064
- Reference count: 40
- Models achieve 2× speed-up in pre-training and outperform strong baselines across 160M–1.7B parameters on diverse downstream tasks

## Executive Summary
This work introduces PDS (Principled Data Selection), a novel framework for pre-training data selection in language models using optimal control theory. PDS formulates data selection as an optimal control problem and leverages Pontryagin's Maximum Principle to estimate data quality scores in a proxy setting. These scores are then transferred to large corpora via a learned scorer, enabling offline data selection before LM training begins.

The framework demonstrates significant improvements over traditional data selection methods, achieving 2× faster pre-training while maintaining or improving downstream performance. PDS also improves data utilization by reducing pre-training data requirements by 1.8×, addressing the growing concern of web-crawled corpus exhaustion. The approach shows consistent benefits across different model sizes (160M-1.7B parameters) and diverse downstream tasks, with scaling law extrapolations suggesting applicability to models up to ~400B parameters trained on ~10T tokens.

## Method Summary
PDS introduces a principled framework that formulates pre-training data selection as an optimal control problem. The core innovation lies in using Pontryagin's Maximum Principle to optimize the data selection process by estimating data quality scores in a proxy setting. These scores are then transferred to the large target corpus through a learned scoring function, enabling offline data selection before the actual language model training begins. The framework consists of two main phases: first, estimating data quality scores using a proxy task that approximates the downstream performance; second, learning a scorer that can transfer these quality estimates to the full pre-training corpus. This approach allows for principled, theoretically-grounded data selection that can be applied before the expensive LM training process starts.

## Key Results
- Achieves 2× speed-up in pre-training compared to standard approaches
- Consistently outperforms strong baselines across model sizes from 160M to 1.7B parameters
- Reduces pre-training data demand by 1.8× while maintaining performance

## Why This Works (Mechanism)
None

## Foundational Learning
- **Pontryagin's Maximum Principle**: A fundamental result in optimal control theory that provides necessary conditions for optimality in dynamic systems. Why needed: Forms the theoretical foundation for the data selection optimization problem. Quick check: Verify the Hamiltonian maximization condition holds for the data selection dynamics.
- **Proxy Task Learning**: Using smaller-scale tasks to estimate data quality that transfers to larger tasks. Why needed: Enables efficient estimation of data quality without full-scale training. Quick check: Validate proxy task correlation with downstream performance across domains.
- **Data Quality Scoring**: Learning a function that maps data samples to quality scores. Why needed: Provides a mechanism to rank and select pre-training data. Quick check: Ensure score distribution is stable across different corpus subsets.

## Architecture Onboarding
- **Component Map**: Proxy Task -> Quality Score Estimator -> Scorer Trainer -> Data Selection Filter -> LM Pre-training
- **Critical Path**: The scoring phase (proxy task + quality estimator) must complete before LM pre-training can begin, creating a sequential bottleneck.
- **Design Tradeoffs**: Balance between proxy task fidelity and computational cost versus the benefits of better data selection.
- **Failure Signatures**: Poor proxy task design leads to scores that don't transfer; overfitting in the scorer causes poor generalization to new data.
- **First Experiments**: 1) Validate proxy task correlation with downstream performance, 2) Test scorer transferability across domains, 3) Measure pre-training speed-up vs. data reduction tradeoff

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes existence of high-quality proxy tasks that accurately reflect downstream performance
- Scaling law extrapolations to 400B+ parameter models remain unverified empirically
- Computational overhead during proxy scoring phase may limit practicality

## Confidence
- **High Confidence**: Mathematical framework using Pontryagin's Maximum Principle is theoretically sound; 2× speed-up and performance improvements are well-supported
- **Medium Confidence**: Extrapolation to extreme-scale models relies on scaling laws that may not capture all PDS dynamics
- **Low Confidence**: Claims about mitigating data exhaustion assume continued scarcity of high-quality pre-training data

## Next Checks
1. **Extreme-Scale Validation**: Implement and validate PDS on models exceeding 10B parameters with training on >100B tokens to empirically verify scaling law extrapolations
2. **Domain Transfer Robustness**: Systematically test PDS performance when proxy tasks and target domains are mismatched to quantify cross-domain transfer limits
3. **Dynamic Data Stream Adaptation**: Evaluate PDS in online/continual learning settings with streaming data to test adaptability to changing distributions