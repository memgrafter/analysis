---
ver: rpa2
title: 'Qsnail: A Questionnaire Dataset for Sequential Question Generation'
arxiv_id: '2402.14272'
source_url: https://arxiv.org/abs/2402.14272
tags:
- questions
- research
- questionnaire
- question
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Qsnail is the first dataset for questionnaire generation, addressing
  the lack of quality data in this sequential question generation task. It contains
  13,168 human-written questionnaires with 184,854 question-option pairs across 11
  domains, collected from online platforms and filtered for quality.
---

# Qsnail: A Questionnaire Dataset for Sequential Question Generation

## Quick Facts
- arXiv ID: 2402.14272
- Source URL: https://arxiv.org/abs/2402.14272
- Authors: Yan Lei; Liang Pang; Yuanzhuo Wang; Huawei Shen; Xueqi Cheng
- Reference count: 0
- Primary result: First dataset for questionnaire generation with 13,168 human-written questionnaires across 11 domains

## Executive Summary
Qsnail addresses the lack of quality data for questionnaire generation by introducing the first specialized dataset containing 13,168 human-written questionnaires with 184,854 question-option pairs. The dataset spans 11 domains and is collected from online platforms with rigorous quality filtering. The study evaluates retrieval models, traditional generative models, and large language models on this sequential question generation task, revealing significant performance gaps between machine-generated and human-written questionnaires. While LLMs show better relevance to research topics, they struggle with diversity and specificity requirements.

## Method Summary
The paper formulates questionnaire generation as a task where models generate sequential questions and options given research topics and intents. The Qsnail dataset is collected from Wenjuanxing and Tencent Wenjuan platforms, containing 13,168 questionnaires filtered for quality. Models evaluated include BM25 retrieval, GPT-2 fine-tuning, and LLMs (ChatGPT, ChatGLM-6B, Vicuna-7B). Evaluation uses automatic metrics (Rouge-L, semantic similarity, repetition scores, diversity, BLEU) and human evaluation across six dimensions. The paper also explores outline-first prompt methods and fine-tuning approaches to improve performance.

## Key Results
- Retrieval models (BM25) and traditional generative models (GPT-2) perform poorly in relevance and specificity compared to human-written questionnaires
- LLMs show better relevance to research topics but exhibit significant gaps in diversity and specificity
- Proposed outline-first prompt and fine-tuning approaches improve specificity and rationality of generated questionnaires
- Questionnaires generated by LLMs still fall short of human-written ones, highlighting the challenge of this task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dataset provides structured constraints that enable systematic evaluation of questionnaire generation models
- Mechanism: Explicit constraints at question, option, and overall levels allow automated and human evaluation metrics to measure model performance against these requirements
- Core assumption: These constraints are both necessary and sufficient for high-quality questionnaires
- Evidence anchors: [abstract] mentions structured format requirements; [section] details individual question constraints

### Mechanism 2
- Claim: LLMs show better relevance but significant gaps in diversity and specificity compared to human-written questionnaires
- Mechanism: LLMs leverage pretraining on diverse corpora for topic relevance but struggle with specific structural constraints required for questionnaire quality
- Core assumption: Pretraining data includes sufficient coverage of questionnaire-like structures for relevance but not for diversity and specificity
- Evidence anchors: [abstract] notes LLM limitations in diversity and specificity; [section] attributes structured format relevance to extensive pretraining

### Mechanism 3
- Claim: Outline-first prompt method and fine-tuning approaches improve specificity and rationality of generated questionnaires
- Mechanism: Breaking down generation into outline creation followed by detailed content generation mirrors human writing processes, while fine-tuning adapts models to domain-specific constraints
- Core assumption: Human-like generation processes transfer effectively to machine learning models
- Evidence anchors: [abstract] mentions improvements from outline-first prompt and fine-tuning; [section] describes the human writing process approach

## Foundational Learning

- Concept: Question generation task formulation
  - Why needed here: Understanding how research topics and intents map to sequential questions is fundamental to grasping the dataset's purpose
  - Quick check question: What are the three main components of a questionnaire generation task according to the paper?

- Concept: Evaluation metrics for questionnaire quality
  - Why needed here: The paper introduces novel automatic and human evaluation metrics that are critical for assessing model performance
  - Quick check question: What are the three levels at which evaluation metrics are defined in this work?

- Concept: Large language model capabilities and limitations
  - Why needed here: The paper extensively evaluates different LLMs and their performance characteristics on this task
  - Quick check question: According to the results, in which aspects do LLMs still lag behind human performance?

## Architecture Onboarding

- Component map: Data collection pipeline (web crawling, filtering, intent reconstruction) -> Model implementations (retrieval, traditional generative, LLMs) -> Evaluation framework (automatic metrics, human evaluation) -> Enhancement methods (outline-first prompt, fine-tuning)
- Critical path: Data collection → Model training/evaluation → Performance analysis → Enhancement implementation
- Design tradeoffs: Comprehensive constraints vs. model complexity, automated vs. human evaluation, general vs. domain-specific approaches
- Failure signatures: Poor relevance indicates inadequate topic understanding, low specificity suggests repetition issues, low rationality points to constraint violations
- First 3 experiments:
  1. Run baseline BM25 retrieval on research topics and compare to human-written questionnaires
  2. Evaluate GPT-2 fine-tuning on Qsnail dataset using proposed automatic metrics
  3. Compare Vicuna-7B performance with and without outline-first prompt on same test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed automatic evaluation metrics correlate with human evaluations in assessing questionnaire quality?
- Basis in paper: [explicit] The paper mentions that automatic metrics align closely with human evaluations for relevance and specificity at the question level, but notes limitations in fully capturing option quality and overall structure
- Why unresolved: While the paper establishes a correlation, it doesn't provide detailed quantitative analysis of the strength of this correlation across all evaluation dimensions
- What evidence would resolve it: A comprehensive statistical analysis showing correlation coefficients between each automatic metric and its corresponding human evaluation dimension across the entire dataset

### Open Question 2
- Question: How does the performance of large language models vary across different questionnaire domains?
- Basis in paper: [inferred] The paper shows overall performance differences between LLMs and humans but doesn't analyze domain-specific variations in detail
- Why unresolved: The paper provides domain distribution statistics but doesn't report how LLM performance differs across these domains
- What evidence would resolve it: Domain-specific evaluation results comparing LLM and human performance metrics for each of the 11 application domains

### Open Question 3
- Question: What is the optimal balance between providing research topic versus research intents to language models for questionnaire generation?
- Basis in paper: [explicit] The paper tests variations of input (topic only, topic+intents, topic+intents+outline) and notes that more information generally improves performance
- Why unresolved: The paper doesn't systematically explore the optimal information granularity or combination of inputs
- What evidence would resolve it: A detailed ablation study testing various combinations and granularities of input information to determine which yields the best questionnaire quality

## Limitations

- The evaluation methodology relies heavily on both automatic metrics and human judgments, with potential biases in the human evaluation process
- The dataset, while substantial at 13,168 questionnaires, may not fully capture the diversity of real-world questionnaire needs across all domains
- Performance gap between human-written and LLM-generated questionnaires suggests fundamental challenges in capturing nuanced understanding required for high-quality questionnaire design

## Confidence

- **High Confidence**: The dataset creation methodology and basic evaluation framework are well-documented and reproducible
- **Medium Confidence**: The claim that LLMs show better relevance but significant gaps in diversity and specificity is supported by the data, but evaluation metrics may not fully capture all aspects of questionnaire quality
- **Low Confidence**: The assertion that the proposed constraints are both necessary and sufficient for high-quality questionnaires is not fully validated

## Next Checks

1. **Ablation Study on Constraints**: Systematically remove individual constraints from the evaluation framework to determine which are truly essential versus nice-to-have for questionnaire quality assessment

2. **Cross-Domain Generalization**: Test the best-performing models (Vicuna-7B with outline-first prompt) on questionnaires from domains not present in the training data to assess generalization capabilities beyond the 11 domains in Qsnail

3. **Longitudinal Performance Tracking**: Conduct a follow-up study where the same questionnaire generation models are evaluated after several months of additional pretraining on more recent data to determine if performance gaps with human-written questionnaires narrow over time