---
ver: rpa2
title: Spontaneous Style Text-to-Speech Synthesis with Controllable Spontaneous Behaviors
  Based on Language Models
arxiv_id: '2407.13509'
source_url: https://arxiv.org/abs/2407.13509
tags:
- spontaneous
- speech
- prosody
- behavior
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of generating natural spontaneous
  speech by proposing a novel language model-based TTS system. The method introduces
  explicit modeling of diverse spontaneous behaviors (19 types categorized into disfluency,
  interjections, and non-speech sounds) using syntactic-aware embeddings and fine-grained
  prosody representations.
---

# Spontaneous Style Text-to-Speech Synthesis with Controllable Spontaneous Behaviors Based on Language Models

## Quick Facts
- arXiv ID: 2407.13509
- Source URL: https://arxiv.org/abs/2407.13509
- Reference count: 0
- Primary result: PN-MOS 4.09, LN-MOS 4.05, MCD 4.879 on spontaneous speech synthesis with controllable behaviors

## Executive Summary
This paper addresses the challenge of generating natural spontaneous speech by proposing a novel language model-based TTS system. The method introduces explicit modeling of diverse spontaneous behaviors (19 types categorized into disfluency, interjections, and non-speech sounds) using syntactic-aware embeddings and fine-grained prosody representations. A syntactic encoder captures behavior-specific syntactic positions, while a prosody extractor and predictor model subtle prosody variations. The model is pre-trained on large-scale data and fine-tuned on a high-quality spontaneous speech corpus. Experimental results show significant improvements over baselines: PN-MOS 4.09 (vs. 3.30 for V ALL-E) and LN-MOS 4.05 (vs. 3.32 for V ALL-E), with MCD reduced to 4.879 (vs. 5.291 for V ALL-E). The system effectively controls spontaneous behaviors and captures prosody variations, producing speech closer to human performance.

## Method Summary
The system builds on V ALL-E, a language model-based TTS that encodes speech into discrete tokens. It introduces a syntactic-aware spontaneous behavior encoder that processes 19 behavior types using syntactic structure information to create behavior embeddings (L-embeddings). A prosody extractor extracts frame-level and phoneme-level prosody representations from mel-spectrograms, and a prosody predictor generates behavior-aware prosody embeddings (P-embeddings) from text, behaviors, and syntactic information. The model uses a three-stage fine-tuning process: pre-training on 10k hours of multi-domain data, joint training of backbone, behavior encoder, label predictor, and prosody extractor, training the prosody predictor alone, then joint training of prosody predictor and acoustic model with frozen extractor.

## Key Results
- PN-MOS of 4.09 compared to 3.30 for V ALL-E baseline
- LN-MOS of 4.05 compared to 3.32 for V ALL-E baseline
- MCD reduced to 4.879 from 5.291 for V ALL-E baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Syntactic-aware spontaneous behavior encoder improves alignment between spontaneous behaviors and text features by explicitly modeling behavior-specific syntactic positions.
- Mechanism: The encoder expands character-level spontaneous labels to phoneme-level, then uses syntactic structure information (index and count of behaviors in sentences/subsentences) combined with label embeddings. This creates L-embeddings that are added to text embeddings, ensuring each text token gets the correct spontaneous behavior representation.
- Core assumption: Spontaneous behaviors have consistent syntactic positions that correlate with their pragmatic functions, and modeling these positions helps the model learn more accurate behavior-text associations.
- Evidence anchors:
  - [abstract] "We systematically categorize and uniformly model diverse spontaneous behaviors" and "explicit modeling of diverse spontaneous behaviors (19 types) using syntactic-aware embeddings"
  - [section] "We obtain the syntactic structure for each spontaneous behavior label at the character level" and "The syntactic positions of spontaneous behaviors correspond to their various pragmatic functions"
  - [corpus] Weak - the corpus provides related papers but no direct evidence about syntactic positions of spontaneous behaviors
- Break condition: If spontaneous behaviors don't have consistent syntactic positions or if the pragmatic function doesn't correlate with syntactic position, the explicit modeling would provide no benefit and could add noise.

### Mechanism 2
- Claim: Fine-grained prosody modeling with spontaneous behavior and linguistic context improves prosody prediction accuracy.
- Mechanism: The model uses a prosody extractor to extract frame-level and phoneme-level prosody representations from mel-spectrograms, then uses multi-head attention with L-embeddings as query to generate behavior-aware prosody embeddings (P-embeddings). An autoregressive prosody predictor then predicts P-embeddings from text, spontaneous behaviors, and syntactic information.
- Core assumption: Spontaneous behaviors significantly influence prosody patterns, and combining spontaneous behavior labels with linguistic context provides sufficient information for accurate prosody prediction.
- Evidence anchors:
  - [abstract] "fine-grained prosody modeling is introduced to enhance the model's ability to capture subtle prosody variations in spontaneous speech"
  - [section] "Spontaneous behavior significantly influences the prosody of spontaneous speech" and "we propose an autoregressive prosody predictor based on a language model consisting of transformer decoders that takes in these information and predicts P-embeddings"
  - [corpus] Weak - no direct corpus evidence about the relationship between spontaneous behaviors and prosody patterns
- Break condition: If spontaneous behaviors don't have predictable effects on prosody or if the linguistic context doesn't provide enough information for accurate prediction, the prosody modeling would fail to improve naturalness.

### Mechanism 3
- Claim: Pre-training on large-scale data followed by fine-tuning on high-quality spontaneous corpus enables learning both semantic understanding and spontaneous speech patterns.
- Mechanism: The backbone V ALL-E is pre-trained on 10k hours of multi-domain data to learn general semantic understanding and speech generation capabilities, then fine-tuned on 5.4 hours of high-quality spontaneous speech with explicit behavior annotations to specialize in spontaneous speech synthesis.
- Core assumption: Large-scale pre-training provides generalizable speech synthesis capabilities that can be effectively adapted to the specific domain of spontaneous speech through fine-tuning.
- Evidence anchors:
  - [abstract] "Recent language model-based TTS systems can be trained on large, diverse, and low-quality speech datasets, resulting in highly natural synthesized speech" and "We pre-train the backbone model described in section 2.1 on a large-scale dataset"
  - [section] "Firstly, we pre-train the backbone model described in section 2.1 on a large-scale dataset, enabling it to acquire strong semantic understanding and the ability to generate expressive speech"
  - [corpus] Weak - no direct evidence about the effectiveness of pre-training on large datasets for TTS adaptation
- Break condition: If the large-scale pre-training data is too different from spontaneous speech domain or if the fine-tuning dataset is too small to overcome domain differences, the adaptation would fail.

## Foundational Learning

- Concept: Language model-based TTS with discrete speech tokens
  - Why needed here: The paper uses V ALL-E which encodes speech waveforms into discrete tokens using neural audio codecs, allowing the model to treat speech generation as a language modeling task
  - Quick check question: How does the discrete token representation enable the model to leverage language modeling techniques for speech synthesis?

- Concept: Spontaneous speech behavior taxonomy and annotation
  - Why needed here: The system explicitly models 19 types of spontaneous behaviors categorized into disfluency, interjections, and non-speech sounds, requiring understanding of linguistic characteristics and acoustic features
  - Quick check question: What are the three major categories of spontaneous behaviors and how do they differ in their linguistic and acoustic characteristics?

- Concept: Fine-grained prosody modeling and prediction
  - Why needed here: The system introduces explicit prosody modeling using mel-spectrogram extraction and prediction from linguistic and behavioral context, requiring understanding of prosody representation and modeling techniques
  - Quick check question: How does the spontaneous prosody extractor convert mel-spectrograms into fine-grained prosody representations suitable for prediction?

## Architecture Onboarding

- Component map:
  - Text Encoder: Converts phoneme sequence to text embeddings
  - Syntactic-aware Spontaneous Behavior Encoder: Processes spontaneous behavior labels and syntactic information to create L-embeddings
  - Label Predictor: NAR transformer decoder that predicts spontaneous behavior sequence from text embeddings
  - Spontaneous Prosody Extractor: Convolution layers and multi-head attention to extract fine-grained prosody representations
  - LM-based Prosody Predictor: AR transformer decoder that predicts P-embeddings from text, behaviors, and syntactic information
  - Acoustic Decoder (V ALL-E): AR/NAR transformer decoder that generates acoustic tokens from text and acoustic prompts
  - Neural Audio Codec: Encodes/decodes audio waveform to/from discrete tokens

- Critical path: Text → Text Encoder → L-embeddings + Text Embeddings → Acoustic Decoder (with P-embeddings as condition) → Acoustic Tokens → Neural Audio Codec → Audio Waveform

- Design tradeoffs:
  - Explicit behavior modeling vs. implicit learning: The system chooses explicit modeling of 19 behavior types with syntactic information rather than learning behaviors implicitly from data
  - Fine-grained prosody vs. global style: The system uses fine-grained prosody representations extracted from mel-spectrograms rather than global style embeddings
  - AR vs. NAR components: The system uses AR predictor for prosody (better for capturing dependencies) and NAR predictor for labels (faster inference)

- Failure signatures:
  - Poor spontaneous behavior naturalness (high LN-MOS error): Indicates problems with behavior encoder or label predictor
  - Poor overall prosody naturalness (high PN-MOS error): Indicates problems with prosody extractor or predictor
  - Low MCD scores: Indicates poor acoustic quality, likely from the acoustic decoder or codec issues
  - Inconsistent behavior control: Indicates problems with L-embedding integration or behavior label prediction

- First 3 experiments:
  1. Test behavior prediction accuracy: Feed text through the system and check if the predicted spontaneous behavior labels match ground truth on the test set
  2. Test prosody prediction quality: Extract P-embeddings from ground truth mel-spectrograms and compare with predicted P-embeddings using MSE loss
  3. Test acoustic quality: Generate speech from text with and without spontaneous behavior labels and compare MCD scores to baseline models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model handle out-of-vocabulary spontaneous behaviors not present in the training data?
- Basis in paper: [inferred] The paper mentions 19 specific spontaneous behaviors but doesn't address generalization to unseen behaviors or novel expressions during inference.
- Why unresolved: The paper focuses on modeling 19 predefined spontaneous behaviors and uses a label predictor for behavior prediction, but doesn't discuss handling behaviors outside this taxonomy or emergent spontaneous expressions.
- What evidence would resolve it: Experimental results showing the model's performance on spontaneous behaviors not seen during training, or analysis of behavior prediction accuracy for novel expressions.

### Open Question 2
- Question: What is the computational overhead of the syntactic-aware spontaneous behavior encoder compared to the baseline V ALL-E model?
- Basis in paper: [inferred] The paper introduces additional components (syntactic encoder, label predictor, prosody extractor/predictor) but doesn't provide runtime or parameter count comparisons between the proposed model and baselines.
- Why unresolved: While the paper demonstrates improved speech quality, it doesn't quantify the efficiency trade-offs in terms of model size, inference speed, or training time.
- What evidence would resolve it: Detailed analysis of model parameters, inference latency measurements, and training time comparisons between the proposed model and baseline approaches.

### Open Question 3
- Question: How does the model's performance generalize across different speakers and recording conditions?
- Basis in paper: [explicit] The paper only evaluates on a single female Mandarin speaker dataset, though it mentions potential for multi-domain data in pre-training.
- Why unresolved: The experiments are limited to one speaker and don't address speaker adaptation, cross-speaker generalization, or robustness to varying recording environments.
- What evidence would resolve it: Results from multi-speaker experiments, cross-speaker adaptation studies, or robustness tests under different recording conditions and noise levels.

## Limitations

- The system relies on expensive manual annotation of 19 spontaneous behavior types, creating scalability challenges for larger datasets or other languages
- The 5.4-hour fine-tuning corpus is relatively small for speech synthesis, raising concerns about generalization to diverse spontaneous speech patterns
- The evaluation focuses primarily on naturalness metrics without thoroughly assessing the controllability aspect of spontaneous behavior generation

## Confidence

- Syntactic-aware behavior encoder: Low confidence - underspecified implementation details create uncertainty about the actual contribution of syntactic modeling
- Fine-grained prosody modeling: Medium confidence - sufficient detail provided but evaluation focuses on overall naturalness rather than prosody quality isolation
- Pre-training and fine-tuning strategy: Medium confidence - well-described process but lacks ablation studies showing individual stage contributions

## Next Checks

1. **Ablation study of syntactic modeling**: Remove the syntactic-aware behavior encoder and replace it with a simple behavior embedding approach, then compare PN-MOS and LN-MOS scores to isolate the contribution of syntactic modeling to overall performance.

2. **Behavior controllability assessment**: Systematically vary the spontaneous behavior labels in the input and measure how well the generated speech matches the specified behaviors using both subjective preference tests and objective acoustic feature analysis.

3. **Cross-domain generalization test**: Evaluate the model on spontaneous speech from different domains or speakers not represented in the fine-tuning corpus, measuring PN-MOS, LN-MOS, and MCD to assess generalization beyond the training data.