---
ver: rpa2
title: Condensed Data Expansion Using Model Inversion for Knowledge Distillation
arxiv_id: '2408.13850'
source_url: https://arxiv.org/abs/2408.13850
tags:
- samples
- condensed
- data
- synthetic
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of limited utility in condensed
  datasets when used for knowledge distillation (KD). The authors propose a method
  that expands condensed datasets using model inversion, generating synthetic data
  that enriches the compact representation and improves KD performance.
---

# Condensed Data Expansion Using Model Inversion for Knowledge Distillation

## Quick Facts
- **arXiv ID**: 2408.13850
- **Source URL**: https://arxiv.org/abs/2408.13850
- **Reference count**: 8
- **Primary result**: Expands condensed datasets using model inversion to improve knowledge distillation accuracy by up to 11.4%

## Executive Summary
This paper addresses the challenge of limited utility in condensed datasets when used for knowledge distillation (KD). The authors propose a method that expands condensed datasets using model inversion, generating synthetic data that enriches the compact representation and improves KD performance. Their approach leverages condensed samples as prototypes to guide the model inversion process, ensuring the generated synthetic data aligns closely with the underlying data distribution. Experiments demonstrate significant improvements in KD accuracy compared to using condensed datasets alone and outperform standard model inversion-based KD methods by up to 11.4% across various datasets and model architectures. The method remains effective even with as few as one condensed sample per class and can enhance performance in few-shot scenarios.

## Method Summary
The method expands condensed datasets by generating synthetic data through model inversion conditioned on condensed samples. A generator creates synthetic samples while a class-conditional discriminator aligns the feature distributions of synthetic and condensed data. The synthetic samples are combined with the original condensed dataset, and a student model is trained through knowledge distillation using this expanded dataset. The approach addresses the limited utility of condensed datasets in KD by enriching their representation while maintaining alignment with the original data distribution.

## Key Results
- Improves KD accuracy by up to 11.4% compared to standard model inversion-based KD methods
- Outperforms using condensed datasets alone for knowledge distillation
- Effective with as few as one condensed sample per class
- Maintains performance improvements across CIFAR-10, CIFAR-100, and ImageNet-200 datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using condensed samples as prototypes during model inversion improves the alignment between synthetic and real feature distributions.
- Mechanism: The method conditions the model inversion process to generate synthetic data that mimics the feature distribution of condensed samples, which act as compressed representations of the original dataset. This alignment is enforced through a feature discriminator that distinguishes between real (condensed) and synthetic features.
- Core assumption: The condensed samples retain sufficient distributional information to guide synthetic data generation toward the original training distribution.
- Evidence anchors:
  - [abstract] "Our method utilizes a small set of condensed samples to query the teacher model and extract more realistic impressions related to the target dataset."
  - [section] "This constraint is enforced through the inclusion of a feature discriminator in our model inversion framework."
- Break condition: If the condensed samples lose too much information during compression, the feature alignment would fail to capture the true data distribution.

### Mechanism 2
- Claim: Class-specific alignment using a conditional discriminator prevents the model from neglecting class information during synthetic data generation.
- Mechanism: The discriminator not only predicts whether a feature is from condensed or synthetic data but also determines the class it belongs to. This is achieved by pairing real features with their labels and synthetic features with pseudo-labels, plus adding "fake" inputs with mismatched class labels to prevent the discriminator from ignoring class information.
- Core assumption: Class-specific feature alignment is necessary for the synthetic data to be useful for knowledge distillation across all classes.
- Evidence anchors:
  - [section] "To enforce this, we present three different types of inputs to the discriminator... This further changes our discriminator objective to the following."
  - [abstract] "Our method demonstrates significant gains in KD accuracy compared to using condensed datasets alone..."
- Break condition: If the discriminator overfits to the limited condensed samples, it may produce poor class-specific alignment.

### Mechanism 3
- Claim: Combining expanded condensed datasets with iteratively refined synthetic samples during distillation improves student model accuracy compared to using either data source alone.
- Mechanism: The method expands the condensed dataset by adding synthetic samples generated through model inversion, then trains the student model on randomly sampled batches from this union. This provides both the condensed samples' compact representation and the synthetic samples' enriched diversity.
- Core assumption: The combination of condensed and synthetic data provides complementary information that neither source alone can fully capture.
- Evidence anchors:
  - [section] "In deciding on the layer index at which the feature alignment will be employed, we considered the type of image features encoded by different parts of the teacher model."
  - [abstract] "Our method demonstrates significant gains in KD accuracy compared to using condensed datasets alone..."
- Break condition: If the synthetic samples diverge too far from the condensed samples' distribution, the combination may introduce noise rather than useful information.

## Foundational Learning

- **Concept: Model Inversion**
  - Why needed here: The method relies on model inversion to generate synthetic data that mimics the training distribution without access to the original dataset.
  - Quick check question: What is the primary objective of model inversion in the context of data-free knowledge distillation?

- **Concept: Knowledge Distillation**
  - Why needed here: The method uses knowledge distillation to transfer knowledge from a pre-trained teacher model to a student model using the expanded dataset.
  - Quick check question: How does knowledge distillation differ from standard supervised learning in terms of the information available to the student model?

- **Concept: Dataset Condensation**
  - Why needed here: The method uses condensed datasets as prototypes to guide the model inversion process, so understanding how condensation works is essential.
  - Quick check question: What is the primary advantage of using condensed datasets over communicating real training samples?

## Architecture Onboarding

- **Component map**: Generator -> Teacher (feature extraction) -> Discriminator -> Student model
- **Critical path**: 1. Initialize synthetic dataset with condensed samples 2. Generate new synthetic batch via condensed sample-guided model inversion 3. Add new synthetic batch to dataset 4. Randomly sample batch from expanded dataset 5. Perform knowledge distillation step 6. Repeat until convergence
- **Design tradeoffs**: Using more condensed samples improves alignment but increases memory usage; deeper feature layers provide better semantic alignment but may be noisier; more complex discriminator architectures risk overfitting to limited condensed samples
- **Failure signatures**: Student accuracy plateaus below baseline when synthetic samples diverge from condensed distribution; discriminator loss becomes unstable, indicating poor feature alignment; generator produces unrealistic samples despite feature alignment
- **First 3 experiments**: 1. Train student using only condensed samples (baseline) vs expanded dataset with 10 spc 2. Compare feature distributions using t-SNE visualization for CMI vs CMI* methods 3. Test class-specific alignment by measuring per-class student accuracy improvements

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- The paper's claims about improved KD performance rely on assumptions about condensed samples retaining sufficient distributional information that requires empirical validation
- The specific contribution of class-specific alignment to performance improvements is not independently verified through ablation studies
- Reported improvements come from comparisons against baseline KD methods using condensed data, but do not compare against state-of-the-art data-free KD methods

## Confidence
- **High confidence**: The method's basic architecture and training procedure are clearly specified
- **Medium confidence**: The claim that condensed samples can effectively guide model inversion is reasonable but requires empirical validation of feature alignment quality
- **Low confidence**: The specific contribution of class-specific alignment to performance improvements is not independently verified

## Next Checks
1. Conduct ablation studies to isolate the contribution of feature alignment versus class-specific alignment components
2. Measure information preservation in condensed samples using quantitative metrics (e.g., mutual information between original and condensed distributions)
3. Compare performance against state-of-the-art data-free KD methods that do not use condensed samples as a baseline