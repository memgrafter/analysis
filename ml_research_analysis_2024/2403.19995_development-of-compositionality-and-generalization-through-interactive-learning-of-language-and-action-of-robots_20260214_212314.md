---
ver: rpa2
title: Development of Compositionality and Generalization through Interactive Learning
  of Language and Action of Robots
arxiv_id: '2403.19995'
source_url: https://arxiv.org/abs/2403.19995
tags:
- visual
- group
- language
- page
- visuo-proprioceptive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how compositionality in language and action
  can co-develop through interactions in a robotic learning system. The authors propose
  a brain-inspired neural network model integrating vision, proprioception, and language
  within a predictive coding and active inference framework.
---

# Development of Compositionality and Generalization through Interactive Learning of Language and Action of Robots

## Quick Facts
- arXiv ID: 2403.19995
- Source URL: https://arxiv.org/abs/2403.19995
- Reference count: 40
- One-line primary result: A brain-inspired neural network model integrating vision, proprioception, and language within a predictive coding and active inference framework shows that generalization to unlearned verb-noun compositions improves significantly (up to 76%) when training task variation increases.

## Executive Summary
This study investigates how compositionality in language and action can co-develop through interactions in a robotic learning system. The authors propose a brain-inspired neural network model integrating vision, proprioception, and language within a predictive coding and active inference framework. The model learns to generate goal-directed visuo-motor sequences from linguistic instructions by minimizing free energy during training. Key results show that generalization performance for learning unlearned verb-noun compositions improves significantly (up to 76%) when training task variation increases, from 9 to 40 compositional combinations.

## Method Summary
The model integrates vision, proprioception, and language through a parametric bias (PB) vector in an associative PV-RNN layer. It learns to generate goal-directed visuo-motor sequences from linguistic instructions by minimizing free energy during training. The system uses a physical robot (Torobo Arm) to perform object manipulation tasks on colored cubic blocks, with data including 64x64 RGB video, joint angle trajectories, and corresponding linguistic descriptions. The model is trained end-to-end using the ADAM optimizer to minimize a composite loss function including reconstruction errors and KL divergence terms.

## Key Results
- Generalization performance for unlearned verb-noun compositions improves significantly (up to 76%) when training task variation increases from 9 to 40 compositional combinations
- Visual attention and working memory modules are essential for accurate visuo-motor sequence generation
- The linguistic latent state representation develops compositional structure influenced by sensorimotor learning, with similar action categories clustering together

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compositionality in language and action co-develops through shared latent state representation.
- Mechanism: The parametric bias (PB) vector in the associative PV-RNN layer binds linguistic goals to visuo-proprioceptive sequences. As the model minimizes free energy during training, the PB space develops a compositional structure where similar action categories cluster together, enabling generalization to unlearned verb-noun compositions.
- Core assumption: The free-energy principle framework, specifically predictive coding and active inference, allows for the emergence of compositional structure in the latent state space through sensorimotor learning.
- Evidence anchors:
  - [abstract] "We attribute this to self-organized compositional structures in linguistic latent state space being influenced significantly by sensorimotor learning."
  - [section] "We find that structural relationships between learned compositions are extrapolated by the model to generalize to unlearned compositions of nouns and verbs."
  - [corpus] Weak - No direct evidence from corpus, but related work on symbol emergence through language games in collective predictive coding supports the general idea.
- Break condition: If the PB space fails to develop consistent topological relationships among different concepts combining actions and object nouns, or if the model cannot extrapolate this structure to unlearned compositions.

### Mechanism 2
- Claim: Visual attention and working memory modules are essential for accurate visuo-motor sequence generation.
- Mechanism: Visual attention dynamically adjusts the pixel density in different regions of the image, allowing the model to focus on and predict the visual appearance of manipulated objects in greater detail. Working memory modules (VWM-1 and VWM-2) save and read visual images, enabling the model to ground language to visuo-proprioceptive behavioral sequences.
- Core assumption: Structural visual information processing, facilitated by attention and working memory, interacts with compositional linguistic information processing to enhance generalization.
- Evidence anchors:
  - [abstract] "Ablation studies show that visual attention and working memory are essential to accurately generate visuo-proprioceptive sequences to achieve linguistically represented goals."
  - [section] "By performing ablation studies we find that generalization performance in learning is significantly reduced when either visual working memory or visual attention is deleted from the model."
  - [corpus] Weak - No direct evidence from corpus, but related work on visual attention and working memory in embodied language development supports the general idea.
- Break condition: If removing either the visual attention or working memory modules does not significantly impact the model's ability to generate accurate visuo-motor sequence predictions.

### Mechanism 3
- Claim: Increasing task composition variation in training improves generalization performance.
- Mechanism: As the model is trained on a wider variety of verb-noun compositions, the latent state representation develops more consistent relational structures among different concepts. This allows the model to better extrapolate to unlearned compositions.
- Core assumption: The amount of experience required for generalization in learning is proportional to the summation of the number of elements across all dimensions, rather than their product.
- Evidence anchors:
  - [abstract] "Our results show that generalization in learning to unlearned verb-noun compositions, is significantly enhanced when training variations of task composition are increased."
  - [section] "Generalization performance in learning unlearned compositions improves with increased vocabulary, as well as the training ratio."
  - [corpus] Weak - No direct evidence from corpus, but related work on the poverty of stimulus problem and the need for compositional generalization in language models supports the general idea.
- Break condition: If increasing the number of variations in task composition does not lead to a significant improvement in generalization performance, or if the model fails to develop consistent relational structures in the latent state representation.

## Foundational Learning

- Concept: Free-energy principle and active inference
  - Why needed here: This framework provides the theoretical foundation for how the model learns to minimize prediction error and generate goal-directed actions based on linguistic goals.
  - Quick check question: How does the free-energy principle relate to predictive coding and active inference in the context of this model?
- Concept: Parametric bias (PB) vectors
  - Why needed here: PB vectors are used to bind linguistic goals to visuo-proprioceptive sequences and to extract compositional structure from the data.
  - Quick check question: How do PB vectors influence the learning of associations between vision, proprioception, and linguistics in this model?
- Concept: Visual attention and working memory
  - Why needed here: These mechanisms allow the model to focus on relevant parts of the visual input and to maintain information about the manipulated objects, which is crucial for grounding language to visuo-proprioceptive behavior.
  - Quick check question: How do visual attention and working memory modules contribute to the model's ability to generate accurate visuo-motor sequence predictions?

## Architecture Onboarding

- Component map: Vision network (convLSTM with attention) -> Associative PV-RNN -> Language network (LSTM with PB) -> Goal-directed planning -> Visuo-proprioceptive sequence generation
- Critical path: Vision and proprioception inputs → Associative PV-RNN → Language network → Goal-directed planning → Visuo-proprioceptive sequence generation
- Design tradeoffs:
  - Complexity vs. generalization: More complex models with larger latent state spaces may have better generalization capabilities but also higher computational costs.
  - Task-specific vs. general: The current model is designed for specific object manipulation tasks, but the architecture could be extended to handle more general scenarios.
  - Real-time vs. offline: The model currently generates visuo-motor plans offline, but future work could focus on enabling real-time planning for physical robots.
- Failure signatures:
  - Poor generalization to unlearned compositions: This may indicate that the latent state representation has not developed sufficient compositional structure.
  - Inaccurate visuo-motor predictions: This could be due to issues with the visual attention or working memory modules, or with the integration of vision and proprioception in the associative network.
  - Inability to infer linguistic goals from observed sequences: This may suggest problems with the goal-directed planning mechanism or with the model's ability to understand the relationship between visuo-proprioceptive behavior and language.
- First 3 experiments:
  1. Ablation study: Remove the visual attention or working memory modules and evaluate the impact on visuo-motor prediction accuracy and generalization performance.
  2. Composition variation study: Train the model on different numbers of verb-noun compositions and assess the effect on generalization to unlearned compositions.
  3. Goal inference study: Test the model's ability to infer linguistic goals from observed visuo-proprioceptive sequences and compare the performance with visuo-motor planning given linguistic goals.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model's performance scale with the dimensionality of the compositional space (e.g., adding modifiers like adjectives and adverbs)?
- Basis in paper: [explicit] The authors mention this as a potential future direction, stating "If the dimension of composition increases to include not only verbs and object nouns, but also various modifiers such as adverbs and adjectives, and each dimension consists of hundreds of elements..."
- Why unresolved: The current study only evaluates a limited compositional space (verbs and object nouns). The model's ability to generalize to more complex compositions is unknown.
- What evidence would resolve it: Conducting experiments with a larger compositional space, including modifiers, and evaluating the model's performance and generalization capabilities.

### Open Question 2
- Question: How does the model's performance compare to large language models (LLMs) in terms of compositionality and generalization?
- Basis in paper: [explicit] The authors discuss the limitations of LLMs, stating "While the current study showed that generalization performance of approximately 75% success can be achieved with a training ratio of 60%, specifically in the case of 5x8 composition, this result offers a minimal potential solution to the poverty of stimulus problem."
- Why unresolved: The paper does not provide a direct comparison between the proposed model and LLMs. It's unclear how the model's compositionality and generalization capabilities compare to those of LLMs.
- What evidence would resolve it: Conducting experiments comparing the proposed model's performance to that of LLMs on tasks requiring compositionality and generalization.

### Open Question 3
- Question: How does the model's performance change with different visual attention mechanisms or working memory architectures?
- Basis in paper: [explicit] The authors conduct ablation studies on the visual attention and working memory modules, showing their importance for accurate visuo-proprioceptive predictions.
- Why unresolved: The ablation studies only explore a limited set of configurations. It's unclear how the model's performance would change with different visual attention mechanisms or working memory architectures.
- What evidence would resolve it: Conducting experiments with different visual attention mechanisms and working memory architectures, and evaluating the model's performance and generalization capabilities.

## Limitations
- Dataset size (only 200 examples per composition) may limit ability to handle more complex linguistic structures or longer-term planning tasks
- Model's performance on inferring linguistic goals from observed sequences (accuracy 0.763-0.833) lags behind its performance on the inverse task
- Specific mechanisms by which visual attention and working memory contribute to compositional learning remain underspecified

## Confidence

**High confidence**: The core finding that increasing task composition variation improves generalization (up to 76% improvement) is well-supported by the experimental results across multiple training ratios and vocabulary sizes. The ablation study results showing the importance of visual attention and working memory modules are also robust.

**Medium confidence**: The claim that compositional structures emerge in the latent state space through sensorimotor learning is supported by qualitative visualizations, but quantitative measures of compositional structure development are lacking. The mechanism by which free energy minimization leads to compositional generalization could benefit from more rigorous mathematical analysis.

**Low confidence**: The generalization to completely unseen object positions (U-P) shows more variable performance (0.667-0.825 accuracy), and the difference between this and composition generalization is not fully explained. The model's ability to scale to more complex linguistic structures beyond simple verb-noun compositions remains unproven.

## Next Checks

1. **Stress test compositionality**: Systematically vary the complexity of linguistic instructions (adding adjectives, prepositional phrases, or temporal relations) and measure degradation in generalization performance to identify the model's compositional boundaries.

2. **Cross-dataset validation**: Train the model on one set of object manipulation tasks and test generalization on entirely different physical objects or manipulation primitives to assess transfer of compositional knowledge.

3. **Latent space analysis**: Quantitatively measure the development of compositional structure in the parametric bias space during training using metrics like clustering quality, interpolation smoothness, or systematicity tests to complement the qualitative visualizations.