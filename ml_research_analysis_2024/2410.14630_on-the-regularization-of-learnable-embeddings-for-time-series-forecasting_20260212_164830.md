---
ver: rpa2
title: On the Regularization of Learnable Embeddings for Time Series Forecasting
arxiv_id: '2410.14630'
source_url: https://arxiv.org/abs/2410.14630
tags:
- learning
- regularization
- time
- series
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates regularization methods for learnable embeddings\
  \ in hybrid global-local time series forecasting architectures. The authors evaluate\
  \ six regularization strategies\u2014L1/L2 penalties, dropout, clustering, variational\
  \ regularization, and a novel forgetting approach\u2014across three model architectures\
  \ and six real-world datasets."
---

# On the Regularization of Learnable Embeddings for Time Series Forecasting

## Quick Facts
- arXiv ID: 2410.14630
- Source URL: https://arxiv.org/abs/2410.14630
- Authors: Luca Butera; Giovanni De Felice; Andrea Cini; Cesare Alippi
- Reference count: 40
- Regularizing embeddings consistently improves forecasting accuracy in hybrid global-local architectures

## Executive Summary
This paper investigates regularization methods for learnable embeddings in hybrid global-local time series forecasting architectures. The authors evaluate six regularization strategies—L1/L2 penalties, dropout, clustering, variational regularization, and a novel forgetting approach—across three model architectures and six real-world datasets. Regularizing embeddings consistently improves forecasting accuracy in both transductive and transfer learning settings. The best-performing methods are those that actively perturb embedding values during training (dropout, variational regularization, and forgetting), which the authors attribute to preventing co-adaptation between local and global model parameters.

## Method Summary
The paper evaluates three hybrid architectures (RNN, STGNN, STAtt) with learnable local embeddings regularized using six strategies: L1/L2 penalties, dropout, clustering, variational regularization, and forgetting regularization. Models are trained on six real-world datasets with 70%/10%/20% train/validation/test splits. Hyperparameter search is performed over learning rates [0.00025, 0.00075, 0.0015, 0.003] and hidden sizes [32, 64, 128, 256]. The forgetting regularization periodically resets embeddings during training to prevent memorization.

## Key Results
- Regularization of embeddings consistently improves forecasting accuracy across all three architectures and six datasets
- Perturbation-based methods (dropout, variational regularization, forgetting) outperform structural methods (L1/L2 penalties, clustering)
- Forgetting regularization demonstrates robust performance across different configurations
- Regularization benefits are most pronounced in transfer learning settings with new sequences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embedding regularization prevents co-adaptation between local and global model parameters.
- Mechanism: By perturbing embeddings during training (e.g., dropout, variational sampling, forgetting resets), the global processing blocks cannot rely on specific embedding values, forcing them to learn more generalizable representations.
- Core assumption: Local embeddings tend to degenerate into sequence identifiers when learned end-to-end without regularization.
- Evidence anchors:
  - [abstract] "methods that actively perturb embedding values during training (dropout, variational regularization, and forgetting), which the authors attribute to preventing co-adaptation between local and global model parameters"
  - [section] "embedding regularization should be adopted as standard practice in hybrid forecasting architectures, as it provides consistent performance gains"
  - [corpus] Weak evidence - corpus papers focus on LLM alignment and multimodal approaches, not embedding regularization mechanisms
- Break condition: If embeddings are not used as sequence identifiers in the first place, or if global model capacity is already sufficient to handle local dynamics without embedding specialization.

### Mechanism 2
- Claim: Perturbation-based regularization methods outperform structural regularization methods.
- Mechanism: Active value perturbation during training (random zeroing, sampling, resetting) creates robustness that static structural constraints (L1/L2 penalties, clustering) cannot achieve.
- Core assumption: The benefit comes specifically from preventing the model from relying on particular embedding values rather than from imposing structure on the embedding space.
- Evidence anchors:
  - [abstract] "methods attempting to prevent the co-adaptation of local and global parameters by means of embeddings perturbation are particularly effective"
  - [section] "methods that attempt to prevent the co-adaptation of the local and global blocks by perturbing the embeddings at training time are consistently among the best-performing approaches"
  - [corpus] Missing evidence - corpus does not contain papers discussing perturbation vs structural regularization trade-offs
- Break condition: If the primary issue is overfitting rather than co-adaptation, or if the embedding space structure itself is more important than preventing value-specific reliance.

### Mechanism 3
- Claim: Forgetting regularization works by periodically resetting embeddings to prevent memorization.
- Mechanism: Periodic resetting of embeddings during training forces the model to relearn representations rather than memorize training sequences, improving generalization and transferability.
- Core assumption: Memorization of individual sequence characteristics harms the model's ability to generalize to new sequences.
- Evidence anchors:
  - [abstract] "forgetting regularization, which periodically resets embeddings during training, demonstrates robust performance across different configurations"
  - [section] "resetting local parameters to regularize the training of a related time series forecasting model has never been explored in the literature"
  - [corpus] Missing evidence - corpus does not contain papers on forgetting regularization for embeddings
- Break condition: If the learning rate is too high, making periodic resets disruptive rather than beneficial, or if the model requires long-term memory of specific sequences.

## Foundational Learning

- Concept: Transductive vs transfer learning settings
  - Why needed here: The paper evaluates regularization in both settings, showing different methods work better in each context
  - Quick check question: In transductive learning, are we forecasting the same sequences seen during training, while in transfer learning we forecast entirely new sequences?

- Concept: Hybrid global-local architectures
  - Why needed here: Understanding how local embeddings interact with global processing blocks is crucial for grasping why regularization is needed
  - Quick check question: Does the global model process all sequences with shared parameters while local embeddings provide sequence-specific customization?

- Concept: Co-adaptation in deep learning
  - Why needed here: The paper's core argument is that co-adaptation between local and global parameters is the problem regularization solves
  - Quick check question: Does co-adaptation refer to local and global parameters becoming overly dependent on each other during joint training?

## Architecture Onboarding

- Component map: Embedding → Encoder → Propagation layers → Decoder → Forecast
- Critical path: Local embeddings are concatenated with sequence data in the encoder, propagated through shared global layers, then decoded to produce forecasts
- Design tradeoffs: Larger embedding size provides more capacity but increases overfitting risk; more complex global models reduce need for embeddings but lose sequence-specific adaptation
- Failure signatures: Overfitting on individual sequences, poor performance on new sequences (transfer learning), embeddings acting as simple identifiers rather than meaningful representations
- First 3 experiments:
  1. Implement baseline STGNN without embeddings, then add embeddings without regularization, measure performance gain
  2. Add dropout regularization to embeddings only, compare with dropout applied to entire model
  3. Implement forgetting regularization with 20-epoch reset period, evaluate on METR-LA dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of embedding regularization vary with the number of input time series?
- Basis in paper: [inferred] The paper mentions that future research could explore how the efficacy of different regularization methods varies with the number of input time series.
- Why unresolved: The paper does not provide experiments or analysis on how regularization effectiveness changes with dataset size (number of time series).
- What evidence would resolve it: Systematic experiments varying the number of time series across datasets while keeping other factors constant, measuring how different regularization methods perform.

### Open Question 2
- Question: What is the optimal combination strategy for multiple regularization techniques?
- Basis in paper: [explicit] The paper mentions that combining different regularization techniques can be beneficial but improvements appear case-dependent without clear patterns.
- Why unresolved: While the paper tests combinations (L2/dropout with clustering/variational/forgetting), it does not systematically explore all possible combinations or identify optimal strategies.
- What evidence would resolve it: A comprehensive grid search over all possible regularization combinations and their hyperparameters to identify optimal strategies for different scenarios.

### Open Question 3
- Question: How can we quantitatively characterize co-adaptation between global and local parameters?
- Basis in paper: [explicit] The paper states that future research could explore analytical methods to quantitatively characterize co-adaptation, for which only indirect signs can be identified.
- Why unresolved: The paper only provides indirect evidence of co-adaptation (through perturbation experiments) but lacks quantitative measures of this phenomenon.
- What evidence would resolve it: Development of metrics that directly measure the degree of interdependence between global and local parameters, validated through controlled experiments.

## Limitations
- Forgetting regularization mechanism lacks direct empirical evidence beyond performance comparisons
- The claim that embeddings "degenerate into sequence identifiers" when unregularized is presented without quantitative evidence
- The paper does not explore whether regularization benefits are additive or if they address different failure modes

## Confidence
- High confidence: Regularization of embeddings improves forecasting performance compared to no regularization
- Medium confidence: Perturbation-based methods are superior to structural methods
- Medium confidence: The benefits are due to preventing co-adaptation between local and global parameters

## Next Checks
1. **Ablation on forgetting timing**: Systematically vary the reset period (1, 10, 50, 100, 200 epochs) and measure performance to determine optimal reset frequency and confirm the mechanism is about preventing memorization
2. **Embedding visualization study**: Track and visualize embedding evolution during training with and without regularization, measuring sequence clustering and diversity to directly test the "sequence identifier degeneration" hypothesis
3. **Combination experiments**: Test combinations of regularization methods (e.g., dropout + variational, L1 + clustering) to determine if regularization benefits are complementary or if certain methods address the same underlying problem