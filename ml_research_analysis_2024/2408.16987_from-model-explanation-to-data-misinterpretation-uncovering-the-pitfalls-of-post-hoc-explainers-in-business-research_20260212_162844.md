---
ver: rpa2
title: 'From Model Explanation to Data Misinterpretation: Uncovering the Pitfalls
  of Post Hoc Explainers in Business Research'
arxiv_id: '2408.16987'
source_url: https://arxiv.org/abs/2408.16987
tags:
- shap
- lime
- explanations
- features
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the validity of using post hoc explainers
  (SHAP and LIME) in business research to draw inferences about data rather than the
  models they explain. Through extensive experiments, the authors find that these
  explainers typically fail to align with true marginal effects in the data, even
  in ideal conditions with linear ground truth models and high-performing machine
  learning models.
---

# From Model Explanation to Data Misinterpretation: Uncovering the Pitfalls of Post Hoc Explainers in Business Research

## Quick Facts
- arXiv ID: 2408.16987
- Source URL: https://arxiv.org/abs/2408.16987
- Reference count: 40
- Key outcome: Post hoc explainers (SHAP and LIME) typically fail to align with true marginal effects in data, even in ideal conditions with linear ground truth models.

## Executive Summary
This study investigates whether post hoc explainers like SHAP and LIME can reliably capture true marginal effects in data rather than just model behavior. Through extensive experiments with synthetic datasets and machine learning models, the authors demonstrate that these explainers systematically fail to align with ground truth marginal effects, even when model performance is high and features are independent. The misalignment manifests in directionality, concordance, and relevance metrics, suggesting fundamental limitations in the mathematical formulation of these explanation methods. The authors propose mitigation strategies but ultimately conclude that post hoc explainers should not be used to validate hypotheses about data relationships.

## Method Summary
The research employs a simulation-based approach using synthetic datasets with known linear ground truth models. Machine learning models (primarily neural networks) are trained on these datasets, then SHAP and LIME are applied to generate explanations. Three metrics evaluate explanation quality: directionality (sign agreement with true effects), concordance (ranking agreement), and relevance (identification of top-k important features). The study systematically varies factors like feature correlation, model complexity, and explainer parameters to understand their impact on data-alignment. Various mitigation strategies including normalization, parameter tuning, and ensemble methods are tested to improve explanation quality.

## Key Results
- SHAP and LIME explanations show poor alignment with true marginal effects across all three metrics (directionality, concordance, relevance)
- Feature correlation significantly degrades explanation quality, particularly for SHAP
- Higher model predictive performance improves explanation data-alignment but doesn't resolve the fundamental misalignment
- Even with mitigation strategies, post hoc explainers cannot reliably recover true marginal effects in the data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Post hoc explainers like SHAP and LIME produce explanations that are theoretically incapable of recovering true marginal effects in the data.
- Mechanism: The mathematical formulation of Shapley values (SHAP) and LIME's weighted ridge regression inherently distorts the relationship between feature importance and marginal effects. For SHAP, the Shapley value formula e_SHAP(x_j) = β_j(x_j - x̄_j) shows that importance values depend on the difference between feature value and mean, not the marginal effect β_j directly. For LIME, the use of discretized features and weighted ridge regression introduces approximation errors that prevent recovery of β.
- Core assumption: The ground truth relationship between features and target is linear and known.
- Evidence anchors:
  - [abstract] "We show that SHAP estimates situational importance... Although the computation of situational importance for a feature involves its marginal effect, the information it contributes can be destroyed in the process."
  - [section 5.3.1] "SHAP estimates situational importance (Strumbelj and Kononenko 2014), which expresses the consequence of a feature taking on a value different from its mean. Although the computation of situational importance for a feature involves its marginal effect, the information it contributes can be destroyed in the process."

### Mechanism 2
- Claim: The predictive performance of the black box model being explained significantly impacts the data-alignment of post hoc explanations.
- Mechanism: When a machine learning model M has high predictive accuracy on the data, its internal representation of feature importance is more likely to align with the true marginal effects in the ground truth model G. This occurs because M has successfully learned the correct mapping from features to target. Conversely, when M has poor predictive performance, its internal feature importance representation diverges from the true marginal effects.
- Core assumption: High predictive accuracy indicates that M has captured the correct relationship between features and target.
- Evidence anchors:
  - [section 5.3.2] "We investigate the relationship between the predictive power of M and the alignment of explanations of M by comparing SHAP and LIME explanations of NNs M1 with ROC AUC approximately equal to 0.60 and M3 with AUC equal to 0.99."

### Mechanism 3
- Claim: Feature independence in the data significantly improves the data-alignment of post hoc explanations.
- Mechanism: When features are mutually independent, post hoc explainers can more accurately isolate the individual contribution of each feature to the target variable. However, when features are correlated, the explainers struggle to disentangle the individual effects, leading to misinterpretation of feature importance. This occurs because correlated features create multiple possible mappings from input to output that achieve similar predictive performance.
- Core assumption: The ground truth model G uses features independently to determine the target.
- Evidence anchors:
  - [section 5.3.3] "We generate a modified version of the dataset from Section 4.1 with mutually independent features... In this revised dataset, 'Credit Score' is independent of all other features rather than being derived from 'Credit Utilization,' 'Credit Inquiries,' 'Delinquencies,' and 'Credit History.'"

## Foundational Learning

- Concept: Marginal effects in statistical modeling
  - Why needed here: The paper's core investigation is whether post hoc explainers can recover true marginal effects in the data, so understanding what marginal effects are and how they differ from feature importance is fundamental.
  - Quick check question: What is the difference between a marginal effect and a feature importance score in the context of explaining a predictive model?

- Concept: Shapley values and game theory
  - Why needed here: SHAP explanations are based on Shapley values from game theory, and understanding their mathematical properties is crucial for understanding why they fail to capture marginal effects.
  - Quick check question: How does the formula for Shapley values (e_SHAP(x_j) = β_j(x_j - x̄_j)) differ from simply reporting the marginal effect β_j?

- Concept: Law of large numbers and its application to LIME
  - Why needed here: The paper's theoretical analysis of LIME relies on understanding how weighted ridge regression behaves as sample size increases, which requires knowledge of the law of large numbers.
  - Quick check question: According to Theorem 1, what happens to LIME coefficients as the number of samples N tends to infinity when explaining a linear model?

## Architecture Onboarding

- Component map: Data generation -> Model training -> Explanation generation (SHAP/LIME) -> Evaluation (directionality, concordance, relevance) -> Mitigation strategies -> Statistical testing

- Critical path: 1. Generate synthetic dataset with known ground truth linear model, 2. Train ML model(s) on the dataset, 3. Apply post hoc explainers (SHAP/LIME) to the trained models, 4. Evaluate explanation quality using directionality, concordance, and relevance metrics, 5. Apply mitigation strategies if needed, 6. Compare results with and without mitigation

- Design tradeoffs:
  - Synthetic vs. real data: Synthetic data allows control over ground truth but may not capture real-world complexities
  - Model complexity: More complex models may achieve better predictive performance but produce less interpretable explanations
  - Number of samples in LIME: Higher N improves approximation quality but increases computational cost
  - Parameter settings: Default LIME/SHAP parameters are convenient but may not optimize data-alignment

- Failure signatures:
  - Directionality near 0.5 for features with strong true marginal effects (random guessing)
  - Low concordance values indicating poor feature ranking agreement with ground truth
  - Relevance values far below 1.0 showing failure to identify important features
  - No improvement from mitigation strategies despite theoretical justification

- First 3 experiments:
  1. Generate a simple linear dataset with 5 independent features and known ground truth coefficients, train a neural network, and evaluate SHAP/LIME explanations using all three metrics.
  2. Repeat experiment 1 but with correlated features to observe the impact on explanation quality.
  3. Apply SHAP normalization to the results from experiment 1 and compare against unnormalized results to verify the theoretical prediction about improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can post hoc explainers be designed to reliably capture true marginal effects in data, especially when features are correlated or numerous?
- Basis in paper: [explicit] The authors discuss the misalignment of SHAP and LIME with true marginal effects and identify contributing factors such as correlated features, model predictive performance, and explainer settings. They propose mitigation strategies but acknowledge these do not fully resolve the issue.
- Why unresolved: The paper shows that even with mitigation strategies, post hoc explainers like SHAP and LIME struggle to align with true marginal effects, especially in complex scenarios. The inherent limitations of these methods in capturing true data relationships remain.
- What evidence would resolve it: Development and validation of new explainers or modifications to existing ones that consistently achieve high data-alignment across diverse datasets and scenarios, including those with correlated features and high dimensionality.

### Open Question 2
- Question: What are the trade-offs between model complexity and the data-alignment of post hoc explanations?
- Basis in paper: [explicit] The authors find that simpler models tend to produce more data-aligned explanations than complex models with equivalent predictive performance, suggesting a trade-off between complexity and explanation quality.
- Why unresolved: While the paper suggests that simpler models may be preferable for obtaining data-aligned explanations, the optimal balance between model complexity and explanation quality is not fully explored or quantified.
- What evidence would resolve it: Empirical studies that systematically vary model complexity and measure the resulting data-alignment of post hoc explanations across multiple datasets and scenarios.

### Open Question 3
- Question: How can the Rashomon effect be mitigated in the context of post hoc explanations to ensure that explanations reflect true data relationships rather than model-specific artifacts?
- Basis in paper: [explicit] The authors discuss the Rashomon effect, where multiple models with similar predictive performance can capture different relationships between features and the target variable, complicating the interpretation of post hoc explanations.
- Why unresolved: The paper acknowledges the challenge posed by the Rashomon effect but does not provide a definitive solution for ensuring that post hoc explanations reflect true data relationships rather than model-specific artifacts.
- What evidence would resolve it: Development and validation of methods that aggregate or combine explanations from multiple models to identify robust, model-agnostic insights about the true data relationships.

## Limitations
- Experiments rely exclusively on synthetic datasets with known linear ground truth models, which may not capture real-world business data complexities
- The study focuses only on SHAP and LIME, leaving open questions about whether other explanation methods might perform better
- Mitigation strategies tested showed modest improvements that may not be practically sufficient for business applications

## Confidence

**High confidence**: The core finding that SHAP and LIME fail to align with true marginal effects, supported by extensive experiments across multiple datasets and model architectures

**Medium confidence**: The identification of specific factors (feature correlation, model performance) affecting explanation quality, though these relationships may be more nuanced in real data

**Medium confidence**: The proposed mitigation strategies, as they show improvement but may not be practically sufficient

## Next Checks

1. Test the findings on real-world business datasets with documented causal relationships to verify if synthetic data limitations affect the conclusions
2. Evaluate additional explanation methods (e.g., counterfactual explanations, attention-based methods) to determine if the limitations are specific to SHAP/LIME or apply more broadly
3. Investigate whether domain-specific knowledge or feature engineering can improve the data-alignment of post hoc explanations in practical business scenarios