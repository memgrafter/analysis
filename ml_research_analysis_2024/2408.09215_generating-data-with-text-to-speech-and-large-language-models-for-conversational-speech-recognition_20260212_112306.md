---
ver: rpa2
title: Generating Data with Text-to-Speech and Large-Language Models for Conversational
  Speech Recognition
arxiv_id: '2408.09215'
source_url: https://arxiv.org/abs/2408.09215
tags:
- data
- speech
- multi-speaker
- synthetic
- fisher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a method for generating synthetic conversational
  speech data for training multi-speaker ASR models using LLMs for transcript generation
  and a conversational multi-speaker TTS model (Parakeet) for speech synthesis. They
  demonstrate this approach on Fisher and Mixer 6 Speech datasets, showing that synthetic
  data generated by this method significantly outperforms traditional multi-speaker
  simulation techniques based on LibriSpeech.
---

# Generating Data with Text-to-Speech and Large-Language Models for Conversational Speech Recognition

## Quick Facts
- arXiv ID: 2408.09215
- Source URL: https://arxiv.org/abs/2408.09215
- Reference count: 0
- Key outcome: Synthetic conversational speech data generated using LLMs and multi-speaker TTS (Parakeet) significantly outperforms traditional methods on Fisher and Mixer 6 Speech datasets

## Executive Summary
This paper proposes a method for generating synthetic conversational speech data to train multi-speaker automatic speech recognition (ASR) models. The approach uses large language models (LLMs) to generate conversational transcripts and a conversational multi-speaker text-to-speech (TTS) model (Parakeet) to synthesize speech. The authors demonstrate that this method significantly outperforms traditional multi-speaker simulation techniques based on LibriSpeech, particularly when in-domain data is scarce. On Fisher Corpus, using 80 hours of synthetic data with Parakeet achieves 20.41% cpWER compared to 34.37% for traditional methods, though still higher than the 13.76% achieved with real Fisher training data.

## Method Summary
The authors generate synthetic conversational speech data using a pipeline that combines LLM-generated transcripts and multi-speaker TTS synthesis. They use Llama 3 8B to generate conversational transcripts from few-shot examples of multi-speaker podcast data, then synthesize speech using Parakeet TTS (trained on 60,000 hours of multi-speaker podcast data). The synthetic audio and transcripts are used to fine-tune Whisper medium with serialized output training (SOT) objective and LoRA adapters. The approach is evaluated on Fisher and Mixer 6 Speech datasets using cpWER and MIMO-WER metrics.

## Key Results
- On Fisher Corpus, 80 hours of synthetic data with Parakeet achieves 20.41% cpWER vs 34.37% for traditional LibriSpeech-based methods
- With only 2.3 hours of synthetic data, the method achieves results comparable to using 80 hours of external real data
- Parakeet's conversational synthesis outperforms xTTS, suggesting turn-taking and para-linguistic subtleties matter for multi-speaker ASR
- Similar improvements observed on Mixer 6 Speech dataset

## Why This Works (Mechanism)

### Mechanism 1
LLM-generated conversational transcripts maintain conversational structure better than concatenated single-speaker utterances. The LLM is prompted with few-shot examples from multi-speaker podcast data, learning to generate realistic turn-taking patterns and para-linguistic elements. Core assumption: LLMs can learn conversational dynamics from few-shot examples without explicit modeling of turn-taking. Break condition: If the LLM prompt examples don't contain diverse conversational patterns, the generated transcripts will lack realistic turn-taking dynamics.

### Mechanism 2
Parakeet's training on 60,000 hours of multi-speaker podcast data enables it to synthesize conversational speech with natural prosody. The model learns speaker-specific voice characteristics and conversational prosody patterns from the training data. Core assumption: Training data diversity is sufficient to capture the space of conversational speech patterns. Break condition: If the training data lacks diversity in speaker interactions or contains too many scripted conversations, the model will fail to generalize to natural conversational speech.

### Mechanism 3
Fine-tuning Whisper with serialized output training (SOT) objective enables effective multi-speaker ASR from synthesized conversations. The SOT objective allows the model to learn to separate and transcribe multiple speakers in serialized format. Core assumption: Whisper's pre-training provides sufficient general speech understanding to adapt to multi-speaker scenarios with limited data. Break condition: If the pre-trained Whisper model lacks sufficient multi-speaker examples in its training data, fine-tuning will not effectively learn speaker separation.

## Foundational Learning

- **Few-shot prompting in LLMs**: Why needed - The LLM generates conversational transcripts from minimal examples, requiring understanding of how to structure effective prompts. Quick check - What happens to LLM output quality if you increase the number of few-shot examples from 8 to 32?

- **Text-to-speech synthesis for multi-speaker scenarios**: Why needed - Parakeet must generate natural-sounding speech for multiple speakers in conversation, requiring understanding of speaker conditioning and prosody control. Quick check - How does Parakeet handle speaker identity transitions between utterances in a conversation?

- **Serialized output training (SOT) for multi-speaker ASR**: Why needed - The ASR model must learn to transcribe multiple speakers from a single audio stream, requiring understanding of SOT objectives and speaker separation. Quick check - What is the difference between SOT and traditional ASR training objectives when handling multi-speaker audio?

## Architecture Onboarding

- **Component map**: LLM transcript generation (Llama 3 8B) → Parakeet TTS synthesis → Whisper SOT fine-tuning → evaluation on real test data
- **Critical path**: LLM transcript generation → Parakeet speech synthesis → Whisper SOT fine-tuning → evaluation on real test data
- **Design tradeoffs**:
  - Parakeet vs. xTTS: Parakeet natively handles conversational dynamics but may have quality limitations; xTTS offers higher quality single-speaker synthesis but requires manual stitching
  - LLM prompt examples: More examples may improve transcript quality but increase generation cost; fewer examples may lead to less diverse conversations
  - Synthetic data quantity: More data may improve robustness but could introduce domain mismatch if not diverse enough
- **Failure signatures**: Poor cpWER scores likely indicate issues with Parakeet's conversational synthesis or Whisper's SOT adaptation; high variance in results may suggest instability in LLM transcript generation or TTS synthesis; degradation with more synthetic data suggests domain mismatch
- **First 3 experiments**:
  1. Generate 10 synthetic conversations using LLM + Parakeet and manually verify transcript quality and speech naturalness
  2. Fine-tune Whisper on 1 hour of synthetic data and evaluate on a small subset of real data to check for catastrophic forgetting
  3. Compare Parakeet vs. xTTS synthetic data performance on a held-out validation set to verify the conversational advantage

## Open Questions the Paper Calls Out

### Open Question 1
Can synthetic data generation using LLMs and TTS models bridge the performance gap with real in-domain data for multi-speaker ASR? The authors state that "there remains a substantial gap compared to using in-domain data (Fisher)" and that "this gap cannot be bridged solely by scaling the amount of synthetic data." Experiments demonstrating that increasing the quality and quantity of synthetic data can achieve performance comparable to real in-domain data would resolve this.

### Open Question 2
How important are turn-taking and para-linguistic subtleties in multi-speaker conversational speech for ASR performance? The authors note that "turn-taking and para-linguistics may play a considerable role for multi-talker ASR" based on Parakeet outperforming xTTS. Ablation studies isolating the effects of turn-taking and para-linguistic features on ASR performance would resolve this.

### Open Question 3
Can synthetic data augmentation improve multi-speaker ASR performance when combined with limited in-domain data? The authors attempted to combine 50 hours of synthetic LLM-generated data with 50 hours of Fisher training data but observed "negligible or no improvement over using solely the in-domain data." Experiments exploring different strategies for combining synthetic and real data would resolve this.

## Limitations
- The evaluation doesn't provide detailed ablation studies showing individual contributions of LLM-generated transcripts versus Parakeet's conversational synthesis capabilities
- The synthetic data generation pipeline has multiple moving parts making it difficult to isolate which component drives the improvements
- The comparison with traditional multi-speaker simulation doesn't account for potential differences in conversation length or speaker diversity between datasets

## Confidence
- **High confidence**: The synthetic data generated using LLM transcripts and Parakeet TTS significantly outperforms traditional multi-speaker simulation methods on both Fisher and Mixer 6 Speech datasets
- **Medium confidence**: Parakeet's conversational synthesis provides advantages over xTTS for multi-speaker scenarios, though the comparison doesn't fully control for transcript quality differences
- **Medium confidence**: The method is particularly effective when in-domain data is scarce, showing competitive performance with 2.3 hours of synthetic data versus 80 hours of real data

## Next Checks
1. **Ablation study on data generation components**: Generate synthetic data using Parakeet TTS with real transcripts and compare performance against Parakeet TTS with LLM transcripts to isolate contributions
2. **Speaker diversity analysis**: Analyze synthetic conversations for speaker turn distribution, conversation length, and speaker diversity compared to real conversations to quantify conversational dynamics capture
3. **Cross-dataset generalization test**: Fine-tune on synthetic data generated from one dataset (e.g., Fisher) and evaluate on another (e.g., Mixer 6 Speech) to assess whether the method learns general conversational patterns or dataset-specific characteristics