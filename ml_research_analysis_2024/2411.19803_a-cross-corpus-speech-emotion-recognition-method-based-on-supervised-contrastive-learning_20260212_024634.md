---
ver: rpa2
title: A Cross-Corpus Speech Emotion Recognition Method Based on Supervised Contrastive
  Learning
arxiv_id: '2411.19803'
source_url: https://arxiv.org/abs/2411.19803
tags:
- speech
- learning
- emotion
- datasets
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of speech emotion recognition
  (SER) under limited labeled data and poor generalization across different corpora.
  It proposes a cross-corpus SER method using supervised contrastive learning.
---

# A Cross-Corpus Speech Emotion Recognition Method Based on Supervised Contrastive Learning

## Quick Facts
- arXiv ID: 2411.19803
- Source URL: https://arxiv.org/abs/2411.19803
- Authors: Xiang minjie
- Reference count: 0
- Primary result: WavLM-based model achieves 77.41% UA on IEMOCAP and 96.49% UA on CASIA, outperforming state-of-the-art

## Executive Summary
This paper addresses the challenge of cross-corpus speech emotion recognition (SER) under limited labeled data and poor generalization across different corpora. The proposed method employs a two-stage fine-tuning process using supervised contrastive learning to improve cross-corpus generalization. First, a self-supervised speech representation model is fine-tuned using contrastive learning across multiple datasets to learn emotion-discriminative features. Then, a classifier is fine-tuned on the target dataset. Experimental results show significant improvements over state-of-the-art methods on both English IEMOCAP and Chinese CASIA datasets.

## Method Summary
The method involves two-stage fine-tuning: first, a pre-trained self-supervised speech representation model (Hubert or WavLM) is fine-tuned using supervised contrastive learning across multiple datasets to learn emotion-discriminative features. The contrastive loss (InfoNCE) treats samples from the same emotion as positive pairs and different emotions as negative pairs. In the second stage, a classifier is fine-tuned on the target dataset using the pre-trained, emotion-discriminative features. The model achieves 77.41% UA on IEMOCAP and 96.49% UA on CASIA using WavLM-base.

## Key Results
- WavLM-based model achieves 77.41% unweighted accuracy on IEMOCAP dataset
- WavLM-based model achieves 96.49% unweighted accuracy on CASIA dataset
- Outperforms state-of-the-art results on both IEMOCAP and CASIA datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage fine-tuning improves cross-corpus generalization by first learning emotion-discriminative features across multiple datasets and then adapting to the target dataset
- Mechanism: Supervised contrastive learning optimizes the speech representation model by treating same-emotion samples (regardless of language) as positive pairs and different-emotion samples as negative pairs, learning features invariant to language differences
- Core assumption: Emotion-relevant features can be learned from multiple datasets and generalized across languages
- Evidence anchors: Abstract states two-stage fine-tuning process; section describes contrastive learning with same-emotion samples as positive pairs
- Break condition: If emotion-relevant features are not transferable due to significant domain shifts or contrastive learning fails to learn meaningful emotion-discriminative features

### Mechanism 2
- Claim: Supervised contrastive learning effectively improves the speech representation model's ability to distinguish between different emotions
- Mechanism: Contrastive loss (InfoNCE) minimizes distance between positive pairs (same emotion) and maximizes distance between negative pairs (different emotions), encouraging feature space where same-emotion samples are close together
- Core assumption: Emotion information is encoded in the speech representation model in a way that can be exploited by contrastive learning
- Evidence anchors: Abstract mentions supervised contrastive learning; section provides formula for contrast loss
- Break condition: If contrastive learning objective is not aligned with emotion recognition task or positive/negative pairs are not well-defined

### Mechanism 3
- Claim: Self-supervised speech representation models provide a strong foundation for emotion recognition
- Mechanism: Pre-trained self-supervised models like WavLM learn general speech features on large unlabeled datasets, which can be fine-tuned for emotion recognition with less labeled data
- Core assumption: General speech features learned by self-supervised models are useful for emotion recognition
- Evidence anchors: Abstract shows WavLM-based model outperforms state-of-the-art; section mentions self-supervised models trained on hundreds of hours of speech data
- Break condition: If self-supervised features are not relevant to emotion recognition or fine-tuning corrupts pre-trained representations

## Foundational Learning

- Concept: Self-supervised learning
  - Why needed here: Provides strong starting point for emotion recognition, reducing need for large amounts of labeled data
  - Quick check question: What are the advantages of using self-supervised learning for speech representation compared to supervised learning?

- Concept: Contrastive learning
  - Why needed here: Helps model learn feature space where samples of same emotion are close together and different emotions are far apart, improving emotion recognition performance
  - Quick check question: How does the contrastive loss function encourage the model to learn discriminative features?

- Concept: Fine-tuning
  - Why needed here: Allows model to adapt pre-trained representations to specific task of emotion recognition, leveraging learned features while adapting to target dataset
  - Quick check question: What are the different strategies for fine-tuning a pre-trained model, and when would you use each strategy?

## Architecture Onboarding

- Component map:
  Speech input -> Speech Encoder -> Feature Projection -> Feature Encoder (Transformer) -> Self-Attention Pooling -> Classifier -> Emotion prediction

- Critical path:
  1. Speech input → Speech Encoder → Feature Projection → Feature Encoder → Self-Attention Pooling → Classifier → Emotion prediction
  2. Contrastive learning objective (first stage): Positive and negative pairs are sampled, and model is optimized to minimize contrastive loss and cosine margin loss

- Design tradeoffs:
  - Model size vs. performance: Larger models like WavLM may achieve better performance but are more computationally expensive
  - Fine-tuning strategy: Fine-tuning all parameters may lead to overfitting on small datasets, while freezing some parameters may limit model's ability to adapt to target dataset
  - Contrastive learning hyperparameters: Choice of positive/negative pairs, batch size, and learning rate can significantly impact model's performance

- Failure signatures:
  - Overfitting: High performance on training set but low performance on validation set
  - Underfitting: Low performance on both training and validation sets
  - Poor generalization: High performance on one dataset but low performance on another dataset

- First 3 experiments:
  1. Fine-tune WavLM directly on IEMOCAP and CASIA datasets (baseline)
  2. Fine-tune WavLM using supervised contrastive learning on both datasets, then fine-tune classifier on IEMOCAP
  3. Fine-tune WavLM using supervised contrastive learning on both datasets, then fine-tune classifier on CASIA

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method perform on more than two SER datasets, and what is the optimal number of datasets for fine-tuning?
- Basis in paper: [explicit] Paper states "In order to simplify the experiment, only two SER datasets are considered in this paper" and suggests future work on "fine-tuning more datasets to allow the model to learn more"
- Why unresolved: Paper only tested method on two datasets (IEMOCAP and CASIA), so performance on additional datasets and optimal number of datasets is unknown
- What evidence would resolve it: Experiments testing method on three or more SER datasets and analyzing performance changes as number of datasets increases

### Open Question 2
- Question: How does performance of proposed method compare to other state-of-the-art models on additional SER datasets?
- Basis in paper: [explicit] Paper only compares performance on IEMOCAP and CASIA datasets, stating WavLM-based model outperformed state-of-the-art on these two datasets
- Why unresolved: Paper does not provide comparisons to state-of-the-art models on other SER datasets
- What evidence would resolve it: Experiments comparing proposed method to other state-of-the-art models on additional SER datasets

### Open Question 3
- Question: How does the proposed method perform on SER datasets in languages other than English and Chinese?
- Basis in paper: [explicit] Paper only uses English (IEMOCAP) and Chinese (CASIA) datasets, stating only two SER datasets are considered
- Why unresolved: Paper does not test method on SER datasets in other languages
- What evidence would resolve it: Experiments testing proposed method on SER datasets in languages other than English and Chinese

### Open Question 4
- Question: How does the proposed method perform with smaller or larger model sizes compared to base versions used in paper?
- Basis in paper: [explicit] Paper states speech representation models are base versions with about 95M parameters and mentions future work on training model with smaller parameters using model distillation
- Why unresolved: Paper only tests base versions of models and does not explore performance of smaller or larger model sizes
- What evidence would resolve it: Experiments testing proposed method with smaller and larger versions of models (e.g., small, large, or huge versions of WavLM or Hubert)

## Limitations
- Limited evaluation on only two datasets (IEMOCAP and CASIA), restricting generalizability assessment
- Lack of detailed implementation specifications, particularly for self-attention pooling layer and sampling strategy for positive/negative pairs
- Uncertainty about reproducibility without additional implementation guidance

## Confidence
- High confidence in overall methodology and two-stage fine-tuning approach
- Medium confidence in specific implementation details of contrastive learning components
- Low confidence in reproducibility without additional implementation guidance

## Next Checks
1. Implement a simplified version of the self-attention pooling layer based on standard formulations to test if core contrastive learning approach works without exact Safari et al. implementation
2. Create controlled experiments varying the sampling strategy for positive and negative pairs to understand sensitivity of method to this design choice
3. Test the method on an additional third corpus (e.g., MSP-Improv or another multilingual dataset) to validate cross-corpus generalization claims beyond two evaluated datasets