---
ver: rpa2
title: 'Holistic Capability Preservation: Towards Compact Yet Comprehensive Reasoning
  Models'
arxiv_id: '2504.07158'
source_url: https://arxiv.org/abs/2504.07158
tags:
- data
- reasoning
- dataset
- capabilities
- general
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Ring-Lite-Distill is a lightweight reasoning model with only 2.75
  billion activated parameters, derived from the open-source MoE LLM Ling-Lite. It
  achieves reasoning performance comparable to DeepSeek-R1-Distill-Qwen-7B while significantly
  surpassing it in general capabilities like instruction following, tool use, and
  knowledge retention.
---

# Holistic Capability Preservation: Towards Compact Yet Comprehensive Reasoning Models

## Quick Facts
- arXiv ID: 2504.07158
- Source URL: https://arxiv.org/abs/2504.07158
- Reference count: 2
- Ring-Lite-Distill achieves reasoning performance comparable to DeepSeek-R1-Distill-Qwen-7B while significantly improving general capabilities (IFEval: 70.5 vs 37.3)

## Executive Summary
Ring-Lite-Distill is a lightweight reasoning model with 2.75 billion activated parameters, derived from the open-source MoE LLM Ling-Lite. It achieves reasoning performance comparable to DeepSeek-R1-Distill-Qwen-7B while significantly surpassing it in general capabilities like instruction following, tool use, and knowledge retention. The model was developed using a two-stage supervised fine-tuning approach combined with direct preference optimization, leveraging a carefully curated dataset of 2.59 million reasoning samples and 890K general-purpose samples. Evaluation results show Ring-Lite-Distill maintains strong reasoning performance (average 66.2) while achieving substantial improvements in general capabilities (average 70.5 vs 37.3 for DeepSeek-R1-Distill-Qwen-7B).

## Method Summary
Ring-Lite-Distill was developed through a two-stage supervised fine-tuning approach on the Ling-Lite MoE base model, followed by direct preference optimization. The first SFT stage enhanced reasoning capabilities using 2.59 million reasoning samples with learning rate 4e-5 and batch size 256. The second stage recovered general capabilities using a hybrid dataset of 890K samples (340K general task samples + 550K reasoning samples) with learning rate 1e-5 and batch size 128. DPO training with 23K samples addressed structural formatting issues. The approach balances reasoning enhancement with general capability preservation through careful data curation and hierarchical taxonomy organization.

## Key Results
- Reasoning performance comparable to DeepSeek-R1-Distill-Qwen-7B (average 66.2 vs 63.7)
- Significant improvement in general capabilities (IFEval: 70.5 vs 37.3)
- Maintains strong performance across diverse benchmarks including AIME24, MATH-500, LiveCodeBench, and MMLU
- Achieves parameter efficiency with only 2.75 billion activated parameters

## Why This Works (Mechanism)

### Mechanism 1
Ring-Lite-Distill preserves general capabilities during reasoning enhancement through carefully balanced two-stage training. The approach first improves reasoning capabilities with a large reasoning dataset (2.59M samples), then recovers general capabilities using a hybrid dataset (890K samples) that combines general-purpose samples with selectively sampled reasoning samples. This balanced approach ensures coverage across reasoning tasks of varying difficulty levels while preserving generic capabilities like instruction following, tool use, and knowledge retention.

### Mechanism 2
The data taxonomy system enables systematic capability development by ensuring comprehensive coverage. Hierarchical classification (CLC framework → domain → subtopics) allows targeted data synthesis and identifies gaps, ensuring the model develops reasoning abilities across all relevant domains rather than over-specializing. This framework serves two primary purposes: identifying data strengths and weaknesses to guide subsequent collection, generation, and model training strategies.

### Mechanism 3
DPO with structural focus addresses format-specific weaknesses that SFT alone cannot resolve. The 23K-sample DPO dataset, augmented with 3K human-curated pairs, specifically targets formatting issues like proper <tool_call>-<tool_call> tag pairing and output redundancy that persist after SFT training. Following DPO application, the model shows notable improvement in reducing formatting errors from 92 to 49 total errors.

## Foundational Learning

- **Concept: Mixture-of-Experts (MoE) architecture**
  - Why needed here: Ring-Lite-Distill maintains parameter efficiency (2.75B activated parameters) through MoE, requiring understanding of how expert routing affects capability preservation during fine-tuning
  - Quick check question: How does expert routing in MoE models differ from dense models when balancing reasoning vs general capabilities?

- **Concept: Supervised Fine-Tuning (SFT) with balanced datasets**
  - Why needed here: The two-stage SFT approach requires understanding how dataset composition affects capability trade-offs during training
  - Quick check question: What happens to general capabilities if reasoning data dominates the SFT dataset beyond the optimal ratio?

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: DPO is used to address specific formatting issues that SFT cannot resolve, requiring understanding of preference-based vs. loss-based training
  - Quick check question: How does DPO's focus on preference pairs enable correction of structural issues that standard SFT might miss?

## Architecture Onboarding

- **Component map**: MoE base model (Ling-Lite) → Two-stage SFT (Reasoning-oriented → Generalization-oriented) → DPO refinement → Final Ring-Lite-Distill
- **Critical path**: Data curation → Two-stage SFT → DPO → Evaluation → Deployment
- **Design tradeoffs**: Parameter efficiency (2.75B activated) vs. reasoning performance; reasoning capability vs. general capability preservation
- **Failure signatures**: Degradation in general capabilities (IFEval, Teval scores drop); formatting issues (missing/redundant </tool_call>-danThink tags); reasoning performance plateau
- **First 3 experiments**:
  1. Compare one-stage vs. two-stage SFT on general capability retention using IFEval/Teval benchmarks
  2. Test different ratios of reasoning-to-general data in Stage 2 SFT to find optimal balance
  3. Evaluate DPO effectiveness by measuring formatting error reduction before/after optimization

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal balance between reasoning data and general capability data during fine-tuning to prevent degradation of either capability? The paper discusses using a hybrid dataset of 890K samples combining 340K general task samples with 550K reasoning task samples, but this was determined through experimentation rather than systematic optimization. The optimal ratio may vary based on model size or initial capabilities.

### Open Question 2
How do the data curation and training techniques scale to larger model architectures beyond the 2.75B activated parameters used in Ring-Lite-Distill? The techniques were developed and validated on a relatively small MoE model, and larger models may have different optimization landscapes, capacity for capability preservation, and data requirements.

### Open Question 3
What is the relationship between the hierarchical data taxonomy system and the emergence of specific capabilities during training? While the taxonomy is used to guide data curation, the paper does not analyze whether certain taxonomy levels or domain distributions are more critical for developing reasoning versus general capabilities, or how this might inform future data collection strategies.

## Limitations

- Reliance on synthetic data generation using DeepSeek-R1 for answer distillation introduces potential quality control challenges
- Evaluation focuses primarily on performance metrics without comprehensive analysis of failure modes or edge cases
- Two-stage training approach requires careful tuning of reasoning-to-general data ratio, which isn't fully explored across different parameter ranges
- Long-term stability and robustness of preserved general capabilities during extended deployment remain untested

## Confidence

- **High Confidence**: The core claim that Ring-Lite-Distill achieves comparable reasoning performance to DeepSeek-R1-Distill-Qwen-7B while significantly improving general capabilities is well-supported by comprehensive benchmark results across multiple domains.
- **Medium Confidence**: The effectiveness of the two-stage SFT approach in preserving general capabilities during reasoning enhancement is demonstrated, though the optimal data ratio and training parameters may vary with different base models or task distributions.
- **Medium Confidence**: The DPO's effectiveness in correcting structural formatting issues is supported by error reduction data, but the generalizability of this approach to other formatting problems or model architectures requires further validation.

## Next Checks

1. **Data Contamination Analysis**: Conduct a comprehensive contamination check against all evaluation benchmarks to ensure the reported performance improvements aren't inflated by data overlap, particularly for the reasoning benchmarks (AIME24, GPQA Diamond, MATH-500).

2. **General Capability Stability Testing**: Evaluate the model's general capabilities (IFEval, Teval, BFCL_v2, MMLU) after extended inference sessions or under varying computational constraints to verify that capability preservation holds under realistic deployment conditions.

3. **Transfer Learning Assessment**: Test the model's ability to learn new tasks or adapt to domain shifts using few-shot or zero-shot learning scenarios to validate that the preserved general capabilities translate to true versatility rather than just benchmark performance.