---
ver: rpa2
title: 'PartIR: Composing SPMD Partitioning Strategies for Machine Learning'
arxiv_id: '2401.11202'
source_url: https://arxiv.org/abs/2401.11202
tags:
- tensor
- partir
- x8xf32
- loop
- sharding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PartIR is a system for composing sharding strategies for distributed
  training of neural networks. It provides a schedule-based API that allows users
  to incrementally compose partitioning strategies via a series of manual or automatic
  tactics.
---

# PartIR: Composing SPMD Partitioning Strategies for Machine Learning

## Quick Facts
- arXiv ID: 2401.11202
- Source URL: https://arxiv.org/abs/2401.11202
- Reference count: 40
- PartIR achieves comparable performance to state-of-the-art tools, is predictable and composable, and reduces manual decisions through propagation

## Executive Summary
PartIR is a system for composing sharding strategies for distributed training of neural networks. It provides a schedule-based API that allows users to incrementally compose partitioning strategies via manual or automatic tactics. PartIR decouples the model code from sharding strategies, making it easier to change and debug partitioning decisions. The system uses an MLIR-based compiler stack that translates tactics into compiler actions, introducing and propagating functional loops and slicing operations.

## Method Summary
PartIR uses a schedule-based API to compose sharding strategies incrementally. Users specify tactics (e.g., ManualPartition, AutomaticPartition) that apply local rewrites and invoke propagation. The system maintains a Tile Mapping Registry (TMR) encoding linear algebra homomorphism rewrite rules. Propagation matches tiling context against TMR entries to insert appropriate loops or reductions. PartIR generates device-local SPMD code with collective communication primitives, which is then compiled to XLA.

## Key Results
- Achieves comparable performance to state-of-the-art tools on benchmark models
- Provides predictable and composable sharding strategies
- Effectively reduces manual decisions through propagation system

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Incremental sharding tactics avoid undoing previous decisions, enabling stable incremental construction.
- **Mechanism:** Each tactic applies a local rewrite and invokes propagation, which uses linear algebra homomorphisms to infer additional necessary sharding steps. The system never rewinds earlier decisions.
- **Core assumption:** The order of tactics reflects priority; conflicts are rare because earlier decisions dominate later ones.
- **Evidence anchors:** [abstract] "These tactics can never undo sharding decisions introduced by previous tactics in the same schedule."

### Mechanism 2
- **Claim:** Decoupling model code from sharding strategies simplifies maintenance and testing.
- **Mechanism:** Sharding is expressed entirely in the schedule outside the model code; the model is compiled to StableHLO once, and PartIR operates on that IR.
- **Core assumption:** The compiler can accurately infer sharding propagation without model-internal annotations.
- **Evidence anchors:** [abstract] "Importantly, the tactics are specified separately from the model code, making them easy to change."

### Mechanism 3
- **Claim:** Propagation using linear algebra homomorphisms automatically infers correct sharding for downstream ops.
- **Mechanism:** PartIR maintains a Tile Mapping Registry (TMR) that encodes how each tensor op transforms when operands are sliced. Propagation matches current tiling context against TMR entries.
- **Core assumption:** Every tensor op's tiling behavior can be expressed as a finite set of LAH rewrite rules in the TMR.
- **Evidence anchors:** [section 5.2.1] "The TMR contains, for every tensor operation with n inputs, a set of specifications..."

## Foundational Learning

- **Concept:** SPMD (Single Program, Multiple Data) execution model
  - Why needed here: PartIR generates device-local code for SPMD; understanding that each device runs the same program on different data slices is key to reasoning about collectives and memory layout.
  - Quick check question: If a matmul is tiled along dimension 0 on axis "B", what collective ensures all devices see the same right operand?

- **Concept:** Linear algebra homomorphisms (LAHs)
  - Why needed here: LAHs justify the program equivalences used in propagation; they guarantee that tiling and reduction operations preserve program semantics.
  - Quick check question: Given a matrix multiplication A @ B, if A is sliced on its first dimension, what tiling rule applies to B?

- **Concept:** Collective communication primitives (AllReduce, AllGather, ReduceScatter, AllToAll)
  - Why needed here: PartIR:HLO emits these ops; understanding their semantics and how they map to mesh axes is essential for performance debugging.
  - Quick check question: If a tensor is partitioned across mesh axis "M" and then summed, which collective replaces the sum?

## Architecture Onboarding

- **Component map:** JAX frontend -> StableHLO IR -> PartIR:Core -> PartIR:Temporal -> PartIR:HLO -> XLA backend
- **Critical path:** Model -> JAX tracing -> StableHLO -> PartIR:Core rewriting -> PartIR:HLO lowering -> XLA compilation -> execution
- **Design tradeoffs:**
  - Incremental rewrites vs. single-pass annotation propagation: PartIR sacrifices some efficiency for predictability and conflict avoidance
  - Rule-based propagation vs. cost-based heuristics: PartIR avoids complex cost modeling but may miss optimal strategies
  - Device-mesh abstraction vs. explicit device IDs: PartIR abstracts away device IDs, making it portable but less fine-grained control
- **Failure signatures:**
  - Propagation blocks -> check TMR entries for ops involved; missing rules cause stalls
  - Memory overflow -> check for unintended replication; use atomic to force replication
  - Incorrect collectives -> inspect intermediate PartIR:HLO IR; ensure tiling context matches intended strategy
- **First 3 experiments:**
  1. Simple matmul chain with batch parallelism on 4 devices; inspect generated loops and AllReduce count
  2. Add model parallelism to the above; verify Megatron-style AllReduce placement and parameter sharding
  3. Apply Z3 tactic to batch+model parallelism; check for AllGather insertion and parameter gradient reduction behavior

## Open Questions the Paper Calls Out

- **Open Question 1:** How does PartIR handle padding and spatial partitioning for convolutional neural networks?
- **Open Question 2:** How does PartIR resolve conflicts when multiple tiling actions are applied to the same dimension across different mesh axes?
- **Open Question 3:** How does PartIR handle reshape operations when the mesh axis spans over a larger number of devices than the tensor dimensions?
- **Open Question 4:** How does PartIR's automatic partitioning tactic perform compared to manual partitioning in terms of runtime and memory usage?
- **Open Question 5:** How does PartIR handle model-internal annotations when propagating sharding decisions across the module?

## Limitations

- TMR completeness for arbitrary neural network ops remains untested
- Scaling behavior to production-scale clusters (256+ devices) is not explored
- Compilation overhead comparison to other systems is not benchmarked

## Confidence

- **High Confidence:** Decoupling model code from sharding strategies is clearly demonstrated
- **Medium Confidence:** Propagation mechanism using linear algebra homomorphisms is theoretically sound but TMR completeness is uncertain
- **Low Confidence:** Claims about scalability to large clusters and production workloads are not validated

## Next Checks

1. **TMR Stress Test:** Apply PartIR to a model with diverse tensor operations (einsum, batch normalization, attention variants) and verify propagation succeeds without manual TMR extensions

2. **Scaling Study:** Evaluate PartIR on a large transformer (T5-XXL or GPT-3 small) across a 16x16 or larger mesh, measuring strong scaling efficiency and comparing against cost-model-driven systems

3. **Cross-Backend Portability:** Compile the same PartIR schedule to different backends (XLA, TensorRT, or simulated backend) and verify correctness and efficiency on both GPU and TPU hardware