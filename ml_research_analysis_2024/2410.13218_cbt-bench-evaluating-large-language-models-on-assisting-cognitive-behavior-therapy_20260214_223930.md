---
ver: rpa2
title: 'CBT-Bench: Evaluating Large Language Models on Assisting Cognitive Behavior
  Therapy'
arxiv_id: '2410.13218'
source_url: https://arxiv.org/abs/2410.13218
tags: []
core_contribution: 'The paper introduces CBT-BENCH, a benchmark for evaluating Large
  Language Models (LLMs) in assisting cognitive behavioral therapy (CBT). CBT-BENCH
  consists of three levels: basic CBT knowledge acquisition (multiple-choice questions),
  cognitive model understanding (classification tasks for cognitive distortions and
  core beliefs), and therapeutic response generation (generating responses to patient
  speech).'
---

# CBT-Bench: Evaluating Large Language Models on Assisting Cognitive Behavior Therapy

## Quick Facts
- arXiv ID: 2410.13216
- Source URL: https://arxiv.org/abs/2410.13216
- Reference count: 36
- Primary result: CBT-BENCH reveals LLMs excel at basic CBT knowledge but struggle with complex therapeutic scenarios requiring deep cognitive understanding

## Executive Summary
This paper introduces CBT-BENCH, a comprehensive benchmark for evaluating Large Language Models (LLMs) in assisting cognitive behavioral therapy (CBT). The benchmark consists of three levels: basic CBT knowledge acquisition through multiple-choice questions, cognitive model understanding through classification tasks for cognitive distortions and core beliefs, and therapeutic response generation to patient speech. Through experiments with six popular LLMs across five newly constructed datasets, the research reveals that while models perform well on basic CBT knowledge, they struggle significantly with complex real-world scenarios requiring deep analysis of patients' cognitive structures and generating effective therapeutic responses.

## Method Summary
The benchmark evaluates LLMs across three levels: Level I uses multiple-choice questions on CBT knowledge (CBT-QA dataset), Level II tests cognitive model understanding through classification tasks (CBT-CD for cognitive distortions, CBT-PC for primary core beliefs, and CBT-FC for fine-grained core beliefs), and Level III assesses therapeutic response generation through deliberate practice exercises (CBT-DP dataset). Six LLMs were tested using zero-shot or few-shot learning approaches, with classification tasks cast as multiple-choice questions and therapeutic responses evaluated through expert pairwise comparison rather than automated metrics.

## Key Results
- Models achieve high accuracy on basic CBT knowledge questions but show limited scaling benefits for cognitive model understanding
- Performance on fine-grained cognitive disorder and core belief classification remains poor, even for larger models
- Therapeutic response generation reveals LLMs tend toward rigid, formulaic responses lacking the flexibility and empathy of human therapists
- GPT-4o performs best overall but still struggles with complex therapeutic scenarios requiring deep cognitive analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large LLMs are better at answering CBT knowledge questions because they store more CBT-related knowledge.
- Mechanism: Larger models have been trained on more diverse data, increasing their probability of having encountered CBT concepts during pretraining.
- Core assumption: CBT knowledge is widely available on the web and thus included in pretraining data.
- Evidence anchors:
  - [abstract] "models of large sizes are better at answering CBT knowledge questions"
  - [section] "Large models could achieve higher accuracies on CBT-QA than small models. This could be attributed to larger models storing more knowledge."
  - [corpus] No direct evidence in corpus; this is an inference based on general LLM scaling behavior.
- Break condition: If CBT knowledge is not well-represented in pretraining data, model size advantage disappears.

### Mechanism 2
- Claim: Simply increasing model size does not enhance understanding of cognitive models.
- Mechanism: Understanding cognitive models requires specialized reasoning and domain expertise that does not scale linearly with model parameters.
- Core assumption: Cognitive model understanding involves complex reasoning about patient beliefs and thought patterns that generic language model scaling does not address.
- Evidence anchors:
  - [abstract] "simply making the models larger could not enhance their understanding ability of the cognitive model"
  - [section] "Mistral-v0.3-7B and Llama-3.1-7B these two small models get the best F1 scores on CBT-CD and CBT-PC, outperforming models of large sizes"
  - [corpus] No direct evidence in corpus; this is based on experimental results.
- Break condition: If specialized fine-tuning or architectural modifications are applied, scaling benefits might emerge.

### Mechanism 3
- Claim: Current LLMs struggle with detecting fine-grained cognitive disorders or core beliefs because these tasks are inherently subjective and complex.
- Mechanism: Fine-grained classification requires nuanced understanding of language and context that current LLMs have not mastered, combined with inherent subjectivity in human annotation.
- Core assumption: Even human experts achieve relatively low agreement scores on these tasks, indicating inherent difficulty.
- Evidence anchors:
  - [abstract] "Current LLMs struggle with detecting fine-grained cognitive disorders or core beliefs"
  - [section] "The models generally perform poorly on these two datasets which are very difficult even for professional therapists"
  - [corpus] No direct evidence in corpus; this is based on experimental results and human expert performance comparisons.
- Break condition: If training data becomes significantly larger and more diverse, or if models receive specialized training for these specific tasks.

## Foundational Learning

- Concept: Cognitive Behavioral Therapy (CBT) basic knowledge
  - Why needed here: Understanding CBT fundamentals is prerequisite for building effective evaluation benchmarks
  - Quick check question: What are the three primary core beliefs in CBT?

- Concept: Cognitive model understanding
  - Why needed here: Core to evaluating LLM capabilities in therapeutic assistance
  - Quick check question: How do automatic thoughts, intermediate beliefs, and core beliefs relate in CBT?

- Concept: Deliberate Practice methodology
  - Why needed here: Framework for constructing realistic therapeutic response evaluation tasks
  - Quick check question: What are the three difficulty levels used in CBT-DP?

## Architecture Onboarding

- Component map: CBT-BENCH consists of three levels - knowledge acquisition (CBT-QA) -> cognitive model understanding (CBT-CD, CBT-PC, CBT-FC) -> therapeutic response generation (CBT-DP)

- Critical path: Data annotation → Prompt engineering → Model inference → Expert evaluation → Analysis. The annotation and expert evaluation steps are the most time-consuming and expensive.

- Design tradeoffs: Using deliberate practice as proxy for real sessions balances privacy concerns with evaluation realism. Classification tasks are framed as multiple-choice to leverage LLM strengths. Expert pairwise comparison is used instead of automatic metrics for generation tasks due to lack of gold standards.

- Failure signatures: Poor performance on fine-grained classification indicates lack of nuanced understanding. Rigid, formulaic responses in generation tasks indicate lack of therapeutic flexibility. Better performance on basic knowledge than complex tasks indicates gap between memorization and application.

- First 3 experiments:
  1. Evaluate same models on CBT-QA with different prompt formats to identify optimal question-answering approach
  2. Test transfer learning by fine-tuning smaller models on CBT knowledge before evaluation
  3. Compare pairwise evaluation results with automated metrics to assess validity of expert judgments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would LLMs perform on CBT-BENCH if trained with targeted mental health domain knowledge versus general pre-training?
- Basis in paper: [inferred] The paper notes that simply increasing model size doesn't enhance understanding of cognitive models, and suggests domain expertise should receive attention.
- Why unresolved: The experiments only tested existing LLMs without specialized training on mental health or CBT content.
- What evidence would resolve it: Comparing performance of LLMs fine-tuned on mental health datasets versus general LLMs on CBT-BENCH tasks.

### Open Question 2
- Question: Would more extensive real CBT session data improve LLM performance on therapeutic response generation beyond what deliberate practice exercises provide?
- Basis in paper: [explicit] The paper acknowledges privacy constraints limit real CBT session data collection and uses deliberate practice as a proxy.
- Why unresolved: The current benchmark relies on deliberate practice exercises rather than actual therapy sessions.
- What evidence would resolve it: Testing LLM performance on actual recorded therapy sessions (with appropriate privacy protections) versus deliberate practice exercises.

### Open Question 3
- Question: How can LLMs better balance professional CBT terminology with patient-centered empathy in therapeutic responses?
- Basis in paper: [explicit] The paper found GPT-4o tends to use professional CBT terminology to challenge clients, while human responses focus more on emotional support and perspective-taking.
- Why unresolved: The experiments identified this gap but didn't explore methods to address it.
- What evidence would resolve it: Testing different prompting strategies or fine-tuning approaches that emphasize empathy and patient perspective alongside CBT techniques.

## Limitations

- Privacy constraints limited collection of real CBT session data, requiring use of deliberate practice exercises as proxy
- Expert-based evaluation methodology introduces subjectivity that may affect reproducibility and generalizability
- Benchmark focuses on English-language CBT knowledge, limiting applicability to diverse cultural contexts
- Inherent subjectivity in cognitive distortion and core belief classification tasks makes even expert agreement challenging

## Confidence

- **High Confidence:** Basic CBT knowledge acquisition performance by LLMs is well-established and shows clear scaling benefits with model size. The experimental methodology for Level I tasks is straightforward and reproducible.
- **Medium Confidence:** The classification task results for cognitive distortion and core belief identification are reliable, though the inherent subjectivity of these tasks introduces some uncertainty. The pairwise comparison methodology for therapeutic response generation is reasonable but depends on expert judgment quality.
- **Low Confidence:** The generalization of these findings to real-world clinical settings is uncertain, as the benchmark cannot fully capture the complexity and nuance of actual therapeutic interactions. The effectiveness of current evaluation metrics for measuring therapeutic competence remains questionable.

## Next Checks

1. **Cross-cultural validation:** Test the benchmark with LLMs fine-tuned on CBT knowledge from different cultural contexts to assess whether the identified limitations are universal or culturally specific.

2. **Longitudinal performance analysis:** Evaluate whether repeated exposure to therapeutic scenarios through training improves LLM performance on fine-grained classification tasks, distinguishing between fundamental limitations and training data constraints.

3. **Expert clinical validation:** Conduct blind studies where human clinicians evaluate LLM-generated therapeutic responses without knowing their source, comparing these assessments with the benchmark's pairwise comparison results to validate the evaluation methodology.