---
ver: rpa2
title: 'Tx-LLM: A Large Language Model for Therapeutics'
arxiv_id: '2406.06316'
source_url: https://arxiv.org/abs/2406.06316
tags:
- smiles
- datasets
- auroc
- drug
- tx-llm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Tx-LLM is a generalist large language model fine-tuned from PaLM-2
  to encode biochemical knowledge across diverse therapeutic modalities. It is trained
  on 709 datasets covering 66 tasks spanning the drug discovery pipeline, using string
  representations of small molecules, proteins, nucleic acids, cell lines, and diseases.
---

# Tx-LLM: A Large Language Model for Therapeutics

## Quick Facts
- arXiv ID: 2406.06316
- Source URL: https://arxiv.org/abs/2406.06316
- Reference count: 40
- Tx-LLM achieves competitive performance with state-of-the-art on 43 out of 66 tasks and exceeds SOTA on 22 tasks

## Executive Summary
Tx-LLM is a generalist large language model fine-tuned from PaLM-2 to encode biochemical knowledge across diverse therapeutic modalities. Trained on 709 datasets covering 66 tasks spanning the drug discovery pipeline, it uses string representations of small molecules, proteins, nucleic acids, cell lines, and diseases. The model achieves competitive performance with state-of-the-art on most tasks, particularly excelling at tasks combining molecular SMILES with text. Tx-LLM demonstrates evidence of positive transfer between diverse drug types and shows potential as an end-to-end tool for therapeutic development.

## Method Summary
Tx-LLM is developed by fine-tuning PaLM-2 (S and M sizes) on the TxT collection of 709 datasets targeting 66 therapeutic tasks from the Therapeutics Data Commons. The training uses dataset mixture ratios proportional to datapoint counts, with prompts constructed from instructions, context, questions, and answers. The model is evaluated against state-of-the-art benchmarks across various metrics including AUROC, AUPRC, accuracy, and correlation measures for diverse therapeutic prediction tasks.

## Key Results
- Achieves competitive performance with SOTA on 43 out of 66 tasks
- Exceeds SOTA performance on 22 tasks, particularly those combining SMILES with text features
- Demonstrates evidence of positive transfer between tasks with diverse drug types (proteins/nucleic acids and small molecules)
- Shows improved performance on small molecule tasks when trained on all datasets versus small molecules only

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tx-LLM benefits from context learned during LLM pretraining for tasks involving SMILES + text features.
- Mechanism: PaLM-2's pretraining encodes biomedical knowledge that transfers to TDC tasks combining SMILES with disease/cell line names.
- Core assumption: Text representations in TDC are sufficiently similar to pretraining corpus to transfer effectively.
- Evidence anchors:
  - "Tx-LLM is particularly powerful and exceeds best-in-class performance on average for tasks combining molecular SMILES representations with text"
  - "This performance may be due to the text representations for diseases and cell lines, both because text is a natural representation for a LLM and because the base LLM may have learned context about these in its pretraining."
- Break condition: If pretraining corpus lacks relevant disease/cell line context or TDC text features are highly specialized.

### Mechanism 2
- Claim: Positive transfer occurs between tasks with diverse drug types and small molecule datasets.
- Mechanism: Training on diverse data types improves general biochemical reasoning ability that benefits small molecule tasks.
- Core assumption: Underlying biochemical relationships are similar enough across drug types to capture transferable patterns.
- Evidence anchors:
  - "We observe evidence of positive transfer between tasks with diverse drug types (e.g., tasks involving small molecules and tasks involving proteins)..."
  - "The model trained on all datasets performs better than the model trained on small molecule datasets when evaluated on 43 out of 56 small molecule datasets."
- Break condition: If protein/nucleic acid sequences capture fundamentally different information not useful for small molecules.

### Mechanism 3
- Claim: Tx-LLM's generalist design allows it to serve as an end-to-end tool across the drug discovery pipeline.
- Mechanism: Single model trained on diverse tasks can handle multiple pipeline stages without switching models.
- Core assumption: Tasks are sufficiently compatible to be represented in a single model without catastrophic interference.
- Evidence anchors:
  - "We believe Tx-LLM represents an important step towards LLMs encoding biochemical knowledge and could have a future role as an end-to-end tool across the drug discovery development pipeline."
  - "Tx-LLM can be effective at identifying genes associated with type 2 diabetes, predicting binding affinities of many small molecules against the protein target..."
- Break condition: If task interference is severe or certain pipeline stages require specialized architectures.

## Foundational Learning

- Concept: SMILES notation for small molecules
  - Why needed here: Tx-LLM processes small molecules as SMILES strings; understanding SMILES is essential for interpreting inputs and outputs.
  - Quick check question: Can you convert the molecule "water" into its SMILES representation?

- Concept: Amino acid and nucleotide sequences
  - Why needed here: Proteins and nucleic acids are represented as sequences in Tx-LLM; understanding sequence notation is essential for interpreting inputs and outputs.
  - Quick check question: What is the single-letter code for the amino acid "Glycine"?

- Concept: Basic drug discovery pipeline stages
  - Why needed here: Tx-LLM is designed to support multiple pipeline stages; understanding these stages helps in contextualizing model outputs and limitations.
  - Quick check question: What is the typical order of stages in drug discovery, from early to late?

## Architecture Onboarding

- Component map: PaLM-2 base LLM -> TxT instruction tuning dataset -> Fine-tuning process -> Evaluation pipeline
- Critical path: PaLM-2 base → TxT dataset assembly → Fine-tuning → Evaluation on TDC → Analysis of SOTA comparison and transfer effects
- Design tradeoffs:
  - Generalist vs. specialist: Tx-LLM trades task-specific optimization for broad coverage
  - Scale vs. efficiency: Larger models (M) outperform smaller (S) but at higher cost
  - Zero-shot vs. few-shot: Few-shot prompting can help but adds prompt complexity
- Failure signatures:
  - Poor performance on SMILES-only tasks suggests SMILES limitations vs. graph-based models
  - Context removal degrades performance, indicating context importance
  - Data contamination (overlap with pretraining) could inflate performance metrics
- First 3 experiments:
  1. Ablation: Train Tx-LLM (S) on small molecule datasets only vs. all datasets; compare performance on small molecule tasks to measure transfer benefit.
  2. Prompting: Compare 0-shot, 1-shot, 5-shot, and 10-shot random vs. KNN prompts on a binary classification task to measure in-context learning impact.
  3. Context: Train Tx-LLM (S) with and without task-specific context in prompts; compare performance to measure context importance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between general-purpose training and domain-specific fine-tuning for LLMs in therapeutic development?
- Basis in paper: The paper compares performances of models trained on diverse datasets versus models trained only on small molecule datasets, finding evidence of positive transfer.
- Why unresolved: The study only examines a limited range of model sizes and fine-tuning strategies, and the optimal balance may depend on the specific therapeutic task or modality.
- What evidence would resolve it: Systematic ablation studies varying the proportion of general-purpose vs. domain-specific training data, across multiple model sizes and architectures, for a wide range of therapeutic tasks.

### Open Question 2
- Question: How does the performance of Tx-LLM compare to other specialized models for individual therapeutic tasks?
- Basis in paper: The paper demonstrates that Tx-LLM achieves competitive or state-of-the-art performance on many therapeutic tasks, but it does not directly compare against specialized models for individual tasks.
- Why unresolved: The paper focuses on the generalist capabilities of Tx-LLM, but a direct comparison with specialized models would provide a more complete understanding of its strengths and weaknesses.
- What evidence would resolve it: Benchmarking Tx-LLM against the best specialized models for individual therapeutic tasks, using standardized datasets and evaluation metrics.

### Open Question 3
- Question: Can Tx-LLM be used to generate novel therapeutic candidates with desired properties, or is it limited to predicting properties of existing therapeutics?
- Basis in paper: The paper demonstrates Tx-LLM's ability to predict various properties of therapeutics, but it does not explore its potential for generating novel therapeutics.
- Why unresolved: The paper focuses on the predictive capabilities of Tx-LLM, but its generative potential for therapeutic development remains unexplored.
- What evidence would resolve it: Training Tx-LLM to generate novel therapeutic candidates with desired properties, and validating the generated candidates through experimental testing or computational screening.

## Limitations

- Data contamination between PaLM-2 pretraining and TDC datasets may inflate performance metrics, particularly for tasks involving molecule names rather than SMILES strings
- String-based representation approach may underperform compared to specialized graph neural networks for small molecule tasks
- Evaluation focuses heavily on TDC benchmarks, which may not fully represent real-world therapeutic development challenges

## Confidence

**High confidence**: Tx-LLM achieves competitive performance with state-of-the-art on 43 out of 66 tasks, with clear evidence of context transfer benefits for SMILES + text tasks. The generalist approach and positive transfer between molecular modalities are well-supported by experimental results.

**Medium confidence**: The claim that Tx-LLM could serve as an end-to-end tool across the drug discovery pipeline is promising but based primarily on benchmark performance rather than real-world deployment evidence. The extent of positive transfer between diverse drug types, while demonstrated, may vary with different dataset compositions.

**Low confidence**: The assertion that Tx-LLM represents "an important step towards LLMs encoding biochemical knowledge" is more speculative, as the paper doesn't establish long-term learning capabilities or compare against other biochemical knowledge encoding approaches.

## Next Checks

1. **Data Contamination Analysis**: Systematically analyze overlap between PaLM-2 pretraining corpus and TDC datasets by comparing molecule names, disease names, and cell line identifiers to quantify potential contamination effects on performance metrics.

2. **Cross-Modality Transfer Validation**: Design controlled experiments training Tx-LLM on only small molecule datasets versus all datasets, then evaluate on held-out protein/nucleic acid tasks to isolate and measure true cross-modality transfer effects.

3. **Real-World Pipeline Testing**: Implement Tx-LLM in a complete drug discovery workflow, from target identification through lead optimization, measuring actual development time and success rate improvements compared to traditional computational approaches.