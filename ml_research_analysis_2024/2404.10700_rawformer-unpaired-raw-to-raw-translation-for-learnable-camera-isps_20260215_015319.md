---
ver: rpa2
title: 'Rawformer: Unpaired Raw-to-Raw Translation for Learnable Camera ISPs'
arxiv_id: '2404.10700'
source_url: https://arxiv.org/abs/2404.10700
tags:
- images
- camera
- image
- translation
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Rawformer, a fully unsupervised Transformer-based
  encoder-decoder method for raw-to-raw image translation across different camera
  models. The proposed method addresses the challenge of generalizing learnable ISPs
  to new cameras without requiring paired datasets.
---

# Rawformer: Unpaired Raw-to-Raw Translation for Learnable Camera ISPs

## Quick Facts
- arXiv ID: 2404.10700
- Source URL: https://arxiv.org/abs/2404.10700
- Authors: Georgy Perevozchikov; Nancy Mehta; Mahmoud Afifi; Radu Timofte
- Reference count: 40
- Rawformer achieves up to 12 dB improvement over previous techniques for raw-to-raw image translation across different camera models

## Executive Summary
Rawformer introduces a fully unsupervised Transformer-based method for translating raw images between different camera models without requiring paired datasets. The method addresses the challenge of generalizing learnable ISPs to new cameras by learning camera characteristics in raw space. By incorporating contextual-scale aware downsampler and upsampler blocks, a novel cross-domain attention-driven discriminator, and a style modulator, Rawformer demonstrates state-of-the-art performance on real camera datasets while enabling pre-trained neural ISPs to process raw images from unseen cameras.

## Method Summary
Rawformer is a Transformer-based encoder-decoder architecture that performs unpaired raw-to-raw image translation between different camera models. The generator consists of contextual-scale aware downsampler (CSAD) and upsampler (CSAU) blocks that efficiently encode global and local semantic correlations. A style modulator using Vision Transformer extracts target camera characteristics from the source domain. The training framework employs two generators and two discriminators with cycle consistency loss, initialized with self-supervised pre-training on image inpainting tasks. The method is trained on raw images segmented into 224×224 crops from multiple camera datasets.

## Key Results
- Achieves up to 12 dB improvement in PSNR over previous techniques for raw-to-raw translation
- Demonstrates superior performance on real camera datasets including Raw-to-Raw and NUS datasets
- Enables pre-trained neural ISPs to process raw images from new cameras without re-training
- Shows 26 milliseconds per frame inference time on GPU and approximately 1 second on CPU

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rawformer enables unpaired raw-to-raw translation by leveraging global and local contextual information without requiring paired raw-sRGB datasets.
- Mechanism: The model uses a Transformer-based encoder-decoder architecture with contextual-scale aware downsampler (CSAD) and upsampler (CSAU) blocks. These blocks employ condensed query attention (CQA) and scale perceptive feed-forward networks (SPFN) to efficiently encode both global and local semantic correlations between source and target domains. The cross-domain attention-guided discriminator with caching mechanism stabilizes training and improves translation quality.
- Core assumption: Raw images from different cameras can be meaningfully mapped in raw space without paired examples, by learning the intrinsic camera characteristics through unsupervised learning.
- Evidence anchors:
  - [abstract] "Rawformer incorporates contextual-scale aware downsampler and upsampler blocks, a novel cross-domain attention-driven discriminator, and a style modulator to efficiently translate raw images between cameras."
  - [section 3] "We present a fully unsupervised Transformer-based raw-to-raw method, dubbed Rawformer, that maps raw images between camera models by efficiently encoding their global and semantic correlations."
  - [corpus] Weak evidence - no direct citations about unpaired raw-to-raw translation in corpus.
- Break condition: If camera sensor characteristics are too dissimilar or the raw data lacks sufficient common structure, the model may fail to learn meaningful mappings without paired data.

### Mechanism 2
- Claim: Self-supervised pre-training on image inpainting improves raw translation accuracy by initializing the generator to better preserve details.
- Mechanism: The generator is first trained to reconstruct 32×32 pixel patches with 40% random masking using a pixel-wise loss function. This pre-training phase teaches the network to maintain fine details and structural integrity, which transfers to the raw translation task where preserving content from the source domain is crucial.
- Core assumption: Learning to reconstruct partially occluded images develops features that are useful for translating between raw domains while preserving image content.
- Evidence anchors:
  - [section 4.2] "This self-supervised pre-training was shown to improve the raw mapping results (see Table 1)."
  - [section 3.3] "Inspired by the effective fine-tuning capabilities demonstrated by the pre-trained BERT model for the text completion task, we initially trained each generator network in a self-supervised fashion for image completion."
  - [corpus] No direct evidence in corpus about pre-training benefits for raw-to-raw translation.
- Break condition: If the pre-training task is too different from the target task, or if the network overfits to the inpainting task, the benefits may not transfer effectively.

### Mechanism 3
- Claim: The style modulator enables camera-specific style adaptation by learning target camera characteristics directly from the source domain.
- Mechanism: A Vision Transformer (ViT) network processes tokens from the generator's bottleneck along with a trainable style token to extract the latent style of the target camera. This style vector is then applied at each CSAU block to adjust feature maps, allowing the model to flexibly adapt to different camera characteristics without requiring separate training per camera.
- Core assumption: Camera characteristics can be effectively captured as a latent style vector that can be applied across different image features during translation.
- Evidence anchors:
  - [section 3.1] "To augment the expressiveness of Rawformer for handling camera perturbations, we design a style modulator in the generator using extended pixel-wise ViT."
  - [section 3.1] "This addition of style token at each stage in the decoder helps in the flexible adjustment of the feature maps."
  - [corpus] No direct evidence in corpus about style modulators for camera adaptation.
- Break condition: If camera characteristics are too complex or multimodal to be captured by a single style vector, or if the style vector overfits to training camera characteristics, the model may fail to generalize to truly unseen cameras.

## Foundational Learning

- Concept: Transformer architectures and self-attention mechanisms
  - Why needed here: Rawformer uses Transformer blocks (CQA) to capture long-range dependencies in raw images, which is crucial for understanding global semantic correlations between camera domains.
  - Quick check question: How does the condensed query attention (CQA) block reduce computational complexity compared to standard self-attention?

- Concept: Generative Adversarial Networks (GANs) and cycle consistency
  - Why needed here: The training framework uses two generators and two discriminators with cycle consistency loss to ensure that translated images maintain content while adapting to the target domain's style.
  - Quick check question: What role does the cache mechanism in the discriminator play in stabilizing training and preventing mode collapse?

- Concept: Camera ISP pipelines and raw image processing
- Why needed here: Understanding how traditional ISPs work and the challenges of raw image characteristics across different cameras is essential for appreciating why raw-to-raw translation is valuable and challenging.
  - Quick check question: What are the key differences in raw image formation between different camera sensors that make raw-to-raw translation necessary?

## Architecture Onboarding

- Component map:
  Generator network (GA→B and GB→A) -> CSAD blocks (encoder) -> Style modulator -> CSAU blocks (decoder) -> translated raw output

- Critical path: Raw input → CSAD blocks (encoder) → Style modulator → CSAU blocks (decoder) → translated raw output

- Design tradeoffs:
  - Using Transformer blocks vs. CNN blocks: Higher computational cost but better global context modeling
  - Self-supervised pre-training vs. direct training: Better initialization but longer total training time
  - Style modulator vs. camera-specific training: More flexible but potentially less accurate for extreme camera differences
  - Cache mechanism vs. larger batch size: Computationally efficient but may have limited historical context

- Failure signatures:
  - Content loss: If cycle consistency loss is high, the model may be losing important content during translation
  - Mode collapse: If discriminator loss becomes too low, the generators may be producing limited variations
  - Style mismatch: If identity loss is high, the translated images may not properly adopt target camera characteristics
  - Slow convergence: If losses plateau early, the model architecture or hyperparameters may need adjustment

- First 3 experiments:
  1. Test the basic encoder-decoder architecture without CSAD/CSAU blocks on a small subset of the Raw-to-Raw dataset to establish baseline performance
  2. Add CSAD blocks to the encoder and measure improvement in translation quality and training stability
  3. Implement the CQA block and compare computational efficiency and translation quality against standard self-attention

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of spatial compression (condensing) operation affect the performance of the CQA block? Specifically, why is condensing the query vector more effective than condensing the key or value vectors?
- Basis in paper: [explicit] The paper mentions that they spatially compress the query vector in the CQA block, but also conducts an ablation study comparing different spatial compression operations on Q, K, and V vectors.
- Why unresolved: The paper doesn't provide a detailed explanation for why condensing the query vector is more effective. It only presents the results of the ablation study.
- What evidence would resolve it: A theoretical analysis or empirical study demonstrating the impact of condensing different attention vectors (Q, K, V) on the performance of the CQA block and the overall model.

### Open Question 2
- Question: How does the self-supervised pre-training of the generator network impact the performance of Rawformer for raw-to-raw translation? Is this pre-training necessary for achieving state-of-the-art results?
- Basis in paper: [explicit] The paper mentions that they pre-train the generator network using a self-supervised image inpainting task before fine-tuning it for raw-to-raw translation. They also conduct an ablation study comparing results with and without this pre-training.
- Why unresolved: The paper doesn't provide a detailed analysis of the impact of this pre-training on the model's performance or whether it is necessary for achieving state-of-the-art results.
- What evidence would resolve it: A study comparing the performance of Rawformer with and without self-supervised pre-training on various datasets and tasks, including raw-to-raw translation.

### Open Question 3
- Question: How can the inference time of Rawformer be further optimized for real-time applications on devices with limited computational power, such as smartphones?
- Basis in paper: [explicit] The paper mentions that Rawformer runs at 26 milliseconds per frame on a GPU and approximately 1 second on a CPU, which may be impractical for real-time rendering on devices with limited computational power. They also present results of optimized versions of the model using quantization techniques.
- Why unresolved: The paper doesn't explore further optimization techniques or provide a detailed analysis of the trade-offs between model accuracy and inference time for real-time applications.
- What evidence would resolve it: A study exploring various optimization techniques, such as model compression, knowledge distillation, or specialized hardware acceleration, to reduce the inference time of Rawformer while maintaining its accuracy for real-time applications.

## Limitations
- Limited validation on truly unseen camera models beyond the tested dataset combinations
- Significant computational overhead from 1000 total training epochs (500 pre-training + 500 fine-tuning)
- Unclear generalization capability to extreme lighting conditions or highly dissimilar camera sensors
- Style modulator may not capture complex multimodal camera characteristics with a single style vector

## Confidence

- High confidence: The basic Transformer architecture with CSAD/CSAU blocks and their individual components (CQA, SPFN) - these are well-specified and tested
- Medium confidence: The overall raw-to-raw translation performance and superiority over baselines - results are strong but dataset-specific
- Low confidence: Generalization to truly unseen camera models and extreme capture conditions - limited validation beyond tested scenarios

## Next Checks

1. Test Rawformer on a new camera pair not included in any training data to verify true generalization capability
2. Conduct ablation studies removing the pre-training phase to quantify its actual contribution to final performance
3. Evaluate performance degradation under extreme lighting conditions (very low light, high dynamic range) to assess robustness limits