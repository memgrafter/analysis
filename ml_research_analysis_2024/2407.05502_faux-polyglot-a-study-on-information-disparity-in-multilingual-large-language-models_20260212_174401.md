---
ver: rpa2
title: 'Faux Polyglot: A Study on Information Disparity in Multilingual Large Language
  Models'
arxiv_id: '2407.05502'
source_url: https://arxiv.org/abs/2407.05502
tags:
- language
- information
- documents
- queries
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines information disparity in multilingual LLMs,
  specifically focusing on retrieval-augmented generation (RAG) systems. The researchers
  created a synthetic multilingual dataset with conflicting facts and perspectives
  across five languages (English, Hindi, German, Arabic, and Chinese) to evaluate
  how language affects information retrieval and generation.
---

# Faux Polyglot: A Study on Information Disparity in Multilingual Large Language Models

## Quick Facts
- **arXiv ID**: 2407.05502
- **Source URL**: https://arxiv.org/abs/2407.05502
- **Reference count**: 17
- **Primary result**: Multilingual LLMs systematically prefer documents in the same language as the query (68% for retrieval, 42% for generation), with high-resource language bias when native language documents are unavailable.

## Executive Summary
This study examines information disparity in multilingual retrieval-augmented generation (RAG) systems, revealing systematic linguistic bias in how LLMs retrieve and generate information across languages. The researchers created a synthetic multilingual dataset with conflicting facts and perspectives across five languages to evaluate language preference in both retrieval and generation stages. Their findings demonstrate that LLMs show strong preference for native language documents (68% retrieval preference) and high-resource languages when native documents are unavailable, creating potential filter bubbles and reinforcing dominant cultural narratives. This work highlights the need for improved multilingual information systems that can provide equitable access to diverse perspectives across languages.

## Method Summary
The researchers created a synthetic multilingual dataset with 170 documents in five languages (English, Hindi, German, Arabic, Chinese) containing conflicting facts about fictional festivals and war premises. They generated 27 factual and 16 opinion-based queries, translated into all target languages. The retrieval phase used multilingual embedding models to rank documents by cosine similarity, while the generation phase used LLMs with top documents as context. They evaluated retrieval using top-10 document language distribution and generation using information overlap metrics (BERTScore/keyword matching) against reference answers from original and alternate document sets.

## Key Results
- Retrieval models showed 68% preference for documents in the same language as the query (Native Language preference)
- When Native Language documents were unavailable, LLMs preferred high-resource languages during generation (English > German > Chinese > Hindi > Arabic)
- Language preference existed across both factual (57% native preference) and opinion-based queries
- Query type affected generation preferences but not retrieval preferences
- Systematic bias created potential filter bubbles, reinforcing dominant cultural narratives

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Retrieval models systematically prefer documents in the same language as the query (Native Language Preference)
- **Mechanism**: Embedding models use cosine similarity to match query embeddings with document embeddings, and embeddings for same-language content are more aligned due to training patterns
- **Core assumption**: The embedding space preserves language-specific semantic relationships
- **Evidence anchors**: [abstract] "LLMs displayed systemic bias towards information in the same language as the query language in both document retrieval and answer generation", [section] "there was a strong preference for Native Language document retrieval, with an average z-score of 1.03 for Native Language versus -0.25 for Foreign Language"

### Mechanism 2
- **Claim**: When Native Language documents are unavailable, LLMs prefer high-resource languages for generation
- **Mechanism**: LLMs' training data distribution favors high-resource languages, creating stronger language representations that are more likely to be selected when no native option exists
- **Core assumption**: Model training data reflects resource availability bias
- **Evidence anchors**: [abstract] "in scenarios where no information is in the language of the query, LLMs prefer documents in high-resource languages during generation", [section] "en (M = 49.28%, SE = 1.14) > de (M = 44.48% SE = 1.45) > zh (M = 42.05% SE = 1.07) > hi (M = 40.77% SE = 0.59) > ar (M = 39.77% SE = 1.15)"

### Mechanism 3
- **Claim**: Query type affects language preference in generation but not retrieval
- **Mechanism**: Different query types create different semantic contexts that influence the model's selection of information sources
- **Core assumption**: Model's source selection process varies with query complexity and specificity
- **Evidence anchors**: [section] "For factual information, we observed a preference for Native Language documents over Foreign Language documents (Broad: Mnative = 57.05%, SE = 6.39% vs Mf oreign = 25.38%, SE = 3.75%)", [section] "In the opinion-based queries, the results indicated a consistent preference for Native Documents for both query types"

## Foundational Learning

- **Concept**: Cross-language information retrieval (CLIR) systems
  - **Why needed here**: Understanding how retrieval systems handle multilingual documents is fundamental to analyzing language preference bias
  - **Quick check question**: How do traditional CLIR systems handle document ranking when queries and documents are in different languages?

- **Concept**: Embedding similarity and cosine similarity metrics
  - **Why needed here**: The study relies heavily on cosine similarity scores between query and document embeddings to determine retrieval preference
  - **Quick check question**: What does a high cosine similarity score between a query embedding and document embedding indicate about their semantic relationship?

- **Concept**: Multilingual model training data distribution
  - **Why needed here**: Understanding how training data imbalance affects model behavior is crucial for interpreting language preference results
  - **Quick check question**: How might imbalanced training data across languages influence a model's performance and preferences in multilingual tasks?

## Architecture Onboarding

- **Component map**: Query embedding → Document ranking via cosine similarity → Top-k document selection → Context assembly → Answer generation → Reference comparison
- **Critical path**: Query embedding → Document ranking via cosine similarity → Top-k document selection → Context assembly → Answer generation → Reference comparison
- **Design tradeoffs**: Language-specific vs. language-agnostic embeddings, retrieval accuracy vs. diversity, computational cost vs. multilingual coverage
- **Failure signatures**: Systematic preference for native languages in retrieval, dominance of high-resource languages in generation, failure to combine information from multiple sources
- **First 3 experiments**:
  1. Test retrieval bias by querying with same content in different languages and measuring document language distribution
  2. Evaluate generation bias by providing mixed-language contexts and analyzing source attribution
  3. Assess query type effects by comparing broad vs. specific queries across languages for both retrieval and generation stages

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do cultural differences impact information preference in multilingual LLMs beyond language-based preferences?
- **Basis in paper**: [inferred] The authors note they focused on language differences rather than cultural differences, acknowledging that people speaking the same language may have diverse narratives
- **Why unresolved**: The study design only examined 5 languages and didn't control for cultural factors that might influence how information is processed and preferred
- **What evidence would resolve it**: Comparative studies using same-language, different-culture document pairs and corresponding queries would help isolate cultural versus linguistic effects

### Open Question 2
- **Question**: Would retrieval-augmented generation systems with diverse document ranking strategies (beyond cosine similarity) show reduced linguistic bias in information retrieval?
- **Basis in paper**: [explicit] The authors mention they only focused on cosine similarity-based retrieval and suggest that other architectures like summarization and reranking were not evaluated
- **Why unresolved**: The study only tested one retrieval architecture, limiting understanding of whether linguistic bias is inherent to RAG systems or specific to the retrieval method
- **What evidence would resolve it**: Comparative experiments testing diverse retrieval methods (reranking, diversity-aware ranking, query expansion) would reveal if architectural choices can mitigate linguistic bias

### Open Question 3
- **Question**: How does pre-training data composition influence multilingual LLMs' preference for high-resource languages during cross-language retrieval and generation?
- **Basis in paper**: [explicit] The authors acknowledge they didn't extensively probe the effect of pre-training data bias and note that models like Llama and GPT-4 have predominantly English pre-training data
- **Why unresolved**: The study didn't analyze the relationship between pre-training corpus composition and observed linguistic preferences in the RAG setting
- **What evidence would resolve it**: Analysis correlating pre-training data language distribution with retrieval/generation preferences across different models would establish causation between pre-training bias and linguistic preference

## Limitations

- Synthetic dataset approach may not capture real-world information-seeking scenarios where users have existing background knowledge
- Exclusive reliance on English reference answers for evaluation creates potential bias in scoring mechanism
- Manual determination of evaluation thresholds (α and θ) introduces subjectivity in information overlap measurement

## Confidence

- **High confidence**: The systematic preference for native language documents in retrieval (68%) is supported by multiple statistical measures and consistent across query types
- **Medium confidence**: The generation stage preference for high-resource languages when native documents are unavailable shows statistical significance but relies on specific evaluation thresholds that were determined manually
- **Medium confidence**: The claim about linguistic preference creating filter bubbles is theoretically sound but would benefit from real-world case studies to validate the practical impact

## Next Checks

1. **Replication with real-world multilingual datasets**: Test the same methodology using existing multilingual knowledge bases (like Wikipedia in multiple languages) to validate whether synthetic dataset findings generalize to real information-seeking scenarios

2. **Cross-encoder vs. bi-encoder evaluation**: Compare retrieval results using different document ranking approaches to determine if the native language preference is consistent across retrieval architectures

3. **Human evaluation of generation quality**: Conduct expert human evaluation of generated answers to verify that the BERTScore and keyword matching metrics accurately capture information quality and perspective diversity across languages