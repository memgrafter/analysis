---
ver: rpa2
title: 'MAIDCRL: Semi-centralized Multi-Agent Influence Dense-CNN Reinforcement Learning'
arxiv_id: '2402.07890'
source_url: https://arxiv.org/abs/2402.07890
tags:
- learning
- maidcrl
- maidrl
- scenarios
- multi-agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study extends MAIDRL with convolutional layers, creating MAIDCRL,
  to improve multi-agent reinforcement learning in StarCraft II. By incorporating
  convolutional neural networks into the DenseNet architecture and using agent influence
  maps, the model extracts spatial features to enhance decision-making.
---

# MAIDCRL: Semi-centralized Multi-Agent Influence Dense-CNN Reinforcement Learning

## Quick Facts
- arXiv ID: 2402.07890
- Source URL: https://arxiv.org/abs/2402.07890
- Reference count: 12
- Primary result: MAIDCRL achieves higher average rewards and faster learning in StarCraft II SMAC scenarios compared to MAIDRL

## Executive Summary
MAIDCRL extends the MAIDRL framework by integrating convolutional neural networks into the DenseNet architecture, enabling effective spatial feature extraction from agent influence maps. This enhancement significantly improves multi-agent reinforcement learning performance in StarCraft II, particularly in complex scenarios requiring coordinated strategies. The model demonstrates superior learning speed, higher rewards, and greater robustness compared to its predecessor.

## Method Summary
MAIDCRL is built upon the StarCraft Multi-Agent Challenge (SMAC) framework, incorporating convolutional layers into the DenseNet architecture to process agent influence maps (AIMs) aggregated into multi-agent influence maps (MAIMs). The model uses a semi-centralized Actor-Critic (A2C) approach, where a centralized critic evaluates joint actions while decentralized actors make local decisions. Training involves 1600 episodes across 31 random seeds for each SMAC scenario (3m, 8m, 25m, 2s3z), with performance measured by average episode reward, total wins, and episodes to first win.

## Key Results
- MAIDCRL achieves higher average rewards and faster learning compared to MAIDRL in all tested SMAC scenarios.
- The model demonstrates greater robustness and more effective collaborative strategies, such as prioritizing target selection and repositioning after damage.
- MAIDCRL outperforms MAIDRL in both homogeneous (3m, 8m, 25m) and heterogeneous (2s3z) environments.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The convolutional layers in MAIDCRL extract spatial features from the agent influence map (MAIM), improving decision-making accuracy.
- Mechanism: CNN layers with 32 filters, kernel size 3, stride 1, and elu activation capture local spatial patterns in MAIM, then concatenate these features into DenseNet blocks for enriched state representation.
- Core assumption: Spatial dependencies in influence maps are relevant to cooperative behavior and can be encoded effectively by standard CNN architectures.
- Evidence anchors:
  - [abstract] "incorporating convolutional layers into the deep model architecture, and evaluate the performance on both homogeneous and heterogeneous scenarios."
  - [section] "Multiple convolutional layers have been incorporated in the new architecture with 32 filters for each layer, a stride rate of 1, and a kernel size of 3 to extract spatial features on MAIM."
  - [corpus] Weak or missing: No corpus paper explicitly confirms CNN's effect on MARL influence maps in this domain.
- Break condition: If spatial correlations in MAIM are not task-relevant, CNN layers may add unnecessary computation without benefit.

### Mechanism 2
- Claim: The semi-centralized learning structure balances exploration efficiency with decentralized execution, leading to faster convergence.
- Mechanism: Centralized critic evaluates joint actions while decentralized actors decide locally; ε-soft annealing (ε0=1→0) ensures initial exploration and later exploitation.
- Core assumption: The non-stationarity in multi-agent settings can be partially mitigated by joint value estimation while retaining decentralized control for scalability.
- Evidence anchors:
  - [abstract] "significantly improved the learning performance and achieved a faster learning rate compared to the existing MAIDRL"
  - [section] "A2C design contains one single controller that manages each agent individually based on the agent's observation"
  - [corpus] Weak or missing: No corpus neighbor discusses semi-centralized A2C specifically.
- Break condition: If the joint critic becomes too complex relative to agent count, learning instability or slowdown may occur.

### Mechanism 3
- Claim: Agent influence maps (AIM) aggregated into MAIM provide global context that improves collaborative target selection and repositioning.
- Mechanism: AIMs from each agent are spatially aligned into a 64×64 global map; this shared context encourages agents to prioritize group attacks and coordinated movement.
- Core assumption: Local observations alone are insufficient for complex coordination tasks; global influence data enhances cooperative strategies.
- Evidence anchors:
  - [abstract] "extracts spatial features to enhance decision-making" and "prioritizing target selection and repositioning after damage"
  - [section] "AIMs from all the agents on the map and generate a Multi-Agent Influence Map (MAIM)"
  - [corpus] Weak or missing: No corpus paper provides direct evidence for AIM-driven coordination.
- Break condition: If MAIM aggregation loses critical local information or introduces noise, coordination benefits may degrade.

## Foundational Learning

- Concept: Markov Game modeling for multi-agent RL
  - Why needed here: The SMAC environment is a multi-agent extension of MDPs; agents must handle shared rewards and joint state transitions.
  - Quick check question: What changes in the Markov property when multiple agents interact in the same environment?

- Concept: Convolutional Neural Network basics
  - Why needed here: CNN layers are used to extract spatial features from MAIM; understanding kernels, strides, and pooling is essential to grasp their role.
  - Quick check question: How does a 3x3 kernel with stride 1 and no padding affect the output spatial dimensions?

- Concept: Actor-Critic framework (A2C)
  - Why needed here: MAIDCRL uses separate actor and critic networks; the critic evaluates joint actions while actors act locally.
  - Quick check question: In A2C, why do we use the advantage function (Q-V) rather than raw Q-values for policy gradient updates?

## Architecture Onboarding

- Component map:
  Input: Agent observations + MAIM (64×64) -> CNN Encoder: 32 filters, kernel=3, stride=1, elu, max-pool 2×2, dropout [0.1, 0.5] -> DenseNet Blocks: Three groups, each with three 256-neuron dense layers -> Actor Head: Policy output per agent -> Critic Head: Joint-action value estimation

- Critical path:
  1. Collect MAIM from all agents
  2. Pass through CNN feature extractor
  3. Concatenate with agent-specific observations
  4. Forward through DenseNet blocks
  5. Separate actor and critic predictions
  6. Update using A2C gradients

- Design tradeoffs:
  - CNN vs. MLP: CNNs capture spatial correlations but add computation; MLPs are simpler but may miss spatial patterns.
  - Centralized critic vs. decentralized: Centralized critic improves credit assignment but increases complexity; decentralized scales better but may suffer from non-stationarity.
  - MAIM dimensionality: 64×64 chosen for balance between detail and computational tractability.

- Failure signatures:
  - Training collapse: If MAIM aggregation is noisy, agents may learn conflicting policies.
  - Slow convergence: If CNN feature extraction is misaligned with task structure, no learning benefit appears.
  - Overfitting to local views: If ε-soft annealing is too aggressive, exploration may be insufficient for discovering global coordination strategies.

- First 3 experiments:
  1. Run MAIDCRL vs. MAIDRL on 3m scenario; verify higher average reward and faster first-win episode count.
  2. Test MAIDCRL on 25m without MAIM; confirm performance drop to isolate MAIM contribution.
  3. Vary MAIM dimensionality (32×32, 64×64, 128×128) and measure impact on learning speed and robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of influence map dimensionality (64x64) impact the performance of MAIDCRL in different SMAC scenarios?
- Basis in paper: [explicit] The paper mentions that the 64x64 dimensionality "outperformed other dimensions on MAIDCRL."
- Why unresolved: The paper does not provide a comparison of performance across different influence map dimensionalities, nor does it explain why 64x64 was optimal.
- What evidence would resolve it: Experiments comparing MAIDCRL performance using different influence map dimensionalities (e.g., 32x32, 64x64, 128x128) across various SMAC scenarios would clarify the impact of dimensionality on performance.

### Open Question 2
- Question: What are the specific contributions of the convolutional layers in improving the learning performance and speed of MAIDCRL?
- Basis in paper: [explicit] The paper states that MAIDCRL "significantly improved the learning performance and achieved a faster learning rate compared to the existing MAIDRL" by incorporating convolutional layers.
- Why unresolved: The paper does not provide a detailed analysis of how the convolutional layers contribute to the improvements in performance and learning speed.
- What evidence would resolve it: A detailed ablation study comparing MAIDCRL with and without convolutional layers, analyzing the impact on performance metrics and learning curves, would elucidate the specific contributions of the convolutional layers.

### Open Question 3
- Question: How does MAIDCRL generalize to more complex heterogeneous environments with larger maps and more diverse unit types?
- Basis in paper: [inferred] The paper mentions that "further investigation is required to test our model in a wider range of heterogeneous environments containing more complicated maps."
- Why unresolved: The paper only evaluates MAIDCRL on a limited set of SMAC scenarios (3m, 8m, 25m, 2s3z) and does not explore more complex heterogeneous environments.
- What evidence would resolve it: Experiments testing MAIDCRL on more complex SMAC scenarios with larger maps, more diverse unit types, and different terrain features would demonstrate the model's generalization capabilities.

## Limitations
- The evidence supporting MAIDCRL's superiority is primarily derived from the paper's own experiments, with no direct corpus support for the specific mechanisms.
- Major uncertainties include the exact DenseNet architecture, CNN hyperparameters, and whether observed gains are due to spatial feature extraction or other factors.
- The claim of robustness is supported only by qualitative descriptions of emergent behaviors, not statistical robustness tests.

## Confidence
- Mechanism 1 (CNN on MAIM): Low - No corpus paper explicitly confirms CNN's effect on MARL influence maps in this domain.
- Mechanism 2 (Semi-centralized A2C): Low - No corpus neighbor discusses semi-centralized A2C specifically.
- Mechanism 3 (AIM-driven coordination): Low - No corpus paper provides direct evidence for AIM-driven coordination.
- Overall claim (MAIDCRL improves performance): Medium - Reported results are consistent within the study but lack external validation.

## Next Checks
1. Reproduce MAIDCRL vs. MAIDRL on 3m scenario to verify higher average reward and faster learning.
2. Test MAIDCRL without MAIM to isolate the contribution of spatial feature extraction.
3. Vary MAIM dimensionality (32×32, 64×64, 128×128) and measure impact on learning speed and robustness.