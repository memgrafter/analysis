---
ver: rpa2
title: 'GASP: Efficient Black-Box Generation of Adversarial Suffixes for Jailbreaking
  LLMs'
arxiv_id: '2411.14133'
source_url: https://arxiv.org/abs/2411.14133
tags:
- adversarial
- prompts
- harmful
- suffixes
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GASP, a framework for generating adversarial
  suffixes to jailbreak large language models (LLMs) in a fully black-box setting.
  The method uses Latent Bayesian Optimization (LBO) to explore continuous embedding
  spaces, balancing exploration and exploitation to efficiently find effective suffixes.
---

# GASP: Efficient Black-Box Generation of Adversarial Suffixes for Jailbreaking LLMs

## Quick Facts
- arXiv ID: 2411.14133
- Source URL: https://arxiv.org/abs/2411.14133
- Reference count: 40
- Primary result: GASP achieves up to 100% ASR@10 on certain LLMs while being 1.75x faster than AdvPrompter

## Executive Summary
GASP introduces a novel black-box framework for generating adversarial suffixes to jailbreak large language models. The method leverages Latent Bayesian Optimization (LBO) to efficiently explore continuous embedding spaces, combined with a custom evaluator (GASPEval) and Odds-Ratio Preference Optimization (ORPO) to iteratively refine suffixes. The approach achieves significantly higher attack success rates across multiple LLMs compared to baselines while maintaining superior readability and faster inference times.

## Method Summary
GASP trains a SuffixLLM on the AdvSuffixes dataset containing 519 harmful instructions with adversarial suffixes, then refines it using LBO and ORPO. The framework operates in a black-box setting where it only queries the target model and receives responses. LBO maps discrete token sequences to continuous embeddings, enabling smooth optimization while GASPEval evaluates responses using 21 comprehensive questions. ORPO then fine-tunes the suffix generator based on successful examples, balancing attack efficacy with prompt coherence.

## Key Results
- Achieves ASR@10 scores of up to 100% on certain models
- Outperforms baselines (GCG, AutoDAN, AdvPrompter) by 10-30% in ASR
- 1.75x faster training time compared to AdvPrompter
- Maintains superior readability while achieving high attack success
- Low API costs for attacking closed-source models like GPT-4o

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GASP generates adversarial suffixes that are both effective and human-readable by leveraging Latent Bayesian Optimization (LBO) in a continuous embedding space.
- Mechanism: LBO maps discrete token sequences to continuous embeddings, enabling smooth optimization of suffix effectiveness while maintaining readability constraints through iterative refinement with GASPEval.
- Core assumption: The continuous embedding space preserves meaningful relationships between token sequences that correlate with both attack success and human readability.
- Evidence anchors: [abstract], [section], [corpus]

### Mechanism 2
- Claim: The dual feedback mechanism using GASPEval and ORPO creates a self-reinforcing loop that continuously improves suffix generation.
- Mechanism: GASPEval evaluates LLM responses using 21 comprehensive questions to identify truly harmful content, while ORPO uses odds-ratio preference optimization to fine-tune the suffix generator based on successful examples.
- Core assumption: The combination of external evaluation and preference optimization can effectively align the suffix generator with the target model's specific vulnerabilities.
- Evidence anchors: [abstract], [section], [corpus]

### Mechanism 3
- Claim: Pre-training on AdvSuffixes provides a generic adversarial distribution that can be efficiently specialized to target models through LBO and ORPO.
- Mechanism: Initial training on a diverse dataset of adversarial suffixes creates a baseline distribution, which is then refined through Bayesian optimization and preference optimization to match the target model's specific response patterns.
- Core assumption: A generic adversarial distribution can serve as a useful starting point that accelerates convergence when specialized to individual models.
- Evidence anchors: [section], [section], [corpus]

## Foundational Learning

- **Concept**: Bayesian Optimization in Continuous Spaces
  - Why needed here: Traditional discrete optimization over token spaces is computationally intractable; Bayesian optimization provides efficient exploration of the continuous embedding space.
  - Quick check question: How does the Expected Improvement acquisition function balance exploration and exploitation in the embedding space?

- **Concept**: Adversarial Suffix Generation vs. Complete Prompt Generation
  - Why needed here: Suffix-based attacks are more efficient and targeted than generating entire prompts, leveraging the autoregressive nature of LLMs.
  - Quick check question: Why is appending suffixes more effective than generating complete adversarial prompts in the context of LLM safety mechanisms?

- **Concept**: Human Readability Constraints in Adversarial Attacks
  - Why needed here: Maintaining natural language coherence is essential for evading perplexity-based filters and ensuring real-world applicability.
  - Quick check question: How does the readability constraint (pnat(x + e) ≥ λ) affect the optimization landscape compared to unconstrained attacks?

## Architecture Onboarding

- **Component map**: TargetLLM ←→ GASPEval ←→ LBO ←→ SuffixLLM ←→ ORPO ←→ AdvSuffixes dataset
- **Critical path**: AdvSuffixes → SuffixLLM pre-training → LBO optimization → GASPEval evaluation → ORPO fine-tuning → TargetLLM jailbreak
- **Design tradeoffs**: Computational efficiency vs. attack success rate; readability vs. effectiveness; black-box constraints vs. white-box optimization capabilities
- **Failure signatures**: Low ASR indicates either poor embedding space mapping, ineffective evaluator, or insufficient pre-training diversity; high inference time suggests inefficient optimization parameters
- **First 3 experiments**:
  1. Run pre-training on AdvSuffixes and verify baseline suffix generation quality
  2. Test LBO with simple acquisition function on a single TargetLLM to validate embedding space optimization
  3. Evaluate GASPEval accuracy by comparing its scores against manual assessment on sample responses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GASP's Latent Bayesian Optimization (LBO) perform compared to alternative acquisition functions (e.g., Probability of Improvement, Lower Confidence Bound) in terms of adversarial success rate and query efficiency?
- Basis in paper: Explicit discussion of acquisition functions and their comparative analysis in Section F.3.
- Why unresolved: While the paper shows that Expected Improvement (EI) outperforms other acquisition functions, the analysis is limited to a specific set of models (LLaMA-3.1-8B and LLaMA-3-8B).
- What evidence would resolve it: Comparative experiments using multiple acquisition functions (EI, PI, LCB) across diverse LLMs and adversarial prompts, measuring ASR@1, ASR@10, and query efficiency metrics.

### Open Question 2
- Question: How does GASP's performance scale with increasing suffix length and complexity, and what is the impact on human readability and adversarial success rates?
- Basis in paper: Explicit mention of Max. Suffix Length hyperparameter (25) in Table 5 and discussions on readability in Section 4.3.
- Why unresolved: The paper does not explore the trade-offs between suffix length, readability, and adversarial success rates.
- What evidence would resolve it: Experiments varying suffix length (e.g., 10, 25, 50 tokens) and measuring ASR@1, ASR@10, and readability scores, with analysis of the trade-offs between these metrics.

### Open Question 3
- Question: How robust is GASP against evolving safety mechanisms in LLMs, such as dynamic prompt filtering or context-aware adversarial detection?
- Basis in paper: Inferred from the discussion of GASP's adaptability and the mention of future work testing against defenses like SmoothLLM and NeMo-Guardrails in Section A.
- Why unresolved: The paper does not evaluate GASP's performance against advanced defensive strategies that may dynamically adapt to adversarial prompts.
- What evidence would resolve it: Experiments testing GASP against LLMs with dynamic safety mechanisms (e.g., SmoothLLM, NeMo-Guardrails) and measuring changes in ASR@1 and ASR@10 over time or across different iterations of the defense mechanisms.

### Open Question 4
- Question: What is the impact of GASP's training data distribution on its generalization performance, and how does it compare to frameworks trained on more diverse or domain-specific datasets?
- Basis in paper: Explicit mention of AdvSuffixes dataset and its split (75% pre-train, 25% fine-tune) in Section 4, and discussions on out-of-distribution evaluation in Section 4.1.
- Why unresolved: The paper evaluates GASP on a fixed dataset split and does not explore how different training data distributions affect its generalization performance.
- What evidence would resolve it: Experiments training GASP on different dataset distributions (e.g., varying the pre-train/fine-tune split, using domain-specific adversarial suffixes) and measuring ASR@1, ASR@10 on both in-distribution and out-of-distribution evaluation sets.

## Limitations
- Evaluator dependency on GASPEval's accuracy and potential blind spots in its 21-question assessment
- Uncertainty about embedding space fidelity and preservation of semantic relationships
- Limited testing across newer LLM architectures with different safety mechanisms
- Incomplete cost-benefit analysis across different attack scenarios and model sizes

## Confidence
- **High Confidence**: Claims about GASP achieving higher ASR than baseline methods
- **Medium Confidence**: Efficiency claims (1.75x faster training, faster inference) and cost-effectiveness analysis
- **Low Confidence**: Assertion that generic AdvSuffixes distribution provides meaningful acceleration for target-specific optimization

## Next Checks
1. **Evaluator Ablation Study**: Remove GASPEval and replace it with random selection or a simpler heuristic to quantify how much of GASP's performance improvement comes from the evaluator versus the optimization algorithm itself.

2. **Embedding Space Quality Analysis**: Conduct a systematic study of the embedding space by comparing nearest neighbors in the continuous space versus discrete token space, measuring how often the optimization produces semantically coherent transitions.

3. **Cross-Architecture Generalization Test**: Evaluate GASP on a wider range of LLM architectures (including newer models like Claude, Gemini, and open models with different design philosophies) to assess whether the pre-trained AdvSuffixes distribution provides consistent benefits across diverse safety mechanisms.