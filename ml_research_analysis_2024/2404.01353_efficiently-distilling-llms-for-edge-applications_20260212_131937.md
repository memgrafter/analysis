---
ver: rpa2
title: Efficiently Distilling LLMs for Edge Applications
arxiv_id: '2404.01353'
source_url: https://arxiv.org/abs/2404.01353
tags:
- arxiv
- mlfs
- loss
- training
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MLFS is a novel parameter-efficient fine-tuning approach for supernet\
  \ training of LLMs, enabling efficient edge deployment. It employs a multistage\
  \ training process with low-rank matrix decomposition, reducing trainable parameters\
  \ from d\xB2 to 6rd."
---

# Efficiently Distilling LLMs for Edge Applications

## Quick Facts
- arXiv ID: 2404.01353
- Source URL: https://arxiv.org/abs/2404.01353
- Reference count: 40
- MLFS achieves 1/4 model size and 1/3 latency compression for encoder models

## Executive Summary
MLFS (Multistage Low-rank Fine-tuning of Super-transformers) is a novel parameter-efficient fine-tuning approach for supernet training of LLMs that enables efficient edge deployment. It uses a multistage training process with low-rank matrix decomposition to reduce trainable parameters from d² to 6rd, incorporating gradient scaling and knowledge distillation to improve convergence and performance. The method achieves comparable or better performance than existing methods for encoder models at significantly reduced model sizes and latencies, while providing substantial training time savings for decoder models.

## Method Summary
MLFS employs a three-stage training process: stage-0 fine-tunes the teacher model, stage-1 samples sub-transformers with varying widths, and stage-2 samples both width and depth. The method uses low-rank decomposition matrices (As and Bs) with rank r to replace full weight updates, reducing trainable parameters from d² to 6rd. Knowledge distillation transfers both output logits and intermediate features from the teacher to sub-transformers, while gradient scaling balances convergence rates across different-sized sub-transformers by weighting their loss contributions relative to the largest sub-transformer.

## Key Results
- Encoder models achieve comparable or better performance than existing methods at 1/4 the size and 1/3 the latency
- Decoder models show limited compression but significantly reduced training time through slicing from larger pre-trained teachers
- Optimal rank selection (r=8) and gradient scaling effectively improve convergence and performance
- MLFS provides 5.7x compression for encoder models while maintaining competitive accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MLFS achieves efficient compression of encoder models to 1/4 the size and 1/3 the latency of the teacher by using low-rank decomposition matrices shared across sub-transformers.
- Mechanism: By replacing full weight updates with rank-decomposition matrices (As and Bs) of rank r, the number of trainable parameters drops from d² to 6rd. Shared low-rank matrices across all sub-transformers in a stage allow simultaneous fine-tuning of multiple architectures without redundant weight updates.
- Core assumption: The rank r can be chosen small (e.g., r=8) while still preserving model expressiveness.
- Evidence anchors:
  - [abstract] "MLFS employs a multistage training process with low-rank matrix decomposition, reducing trainable parameters from d² to 6rd."
  - [section] "The number of parameters to be learnt in the MLFS approach is 6rd. In contrast, full fine-tuning requires updating d² parameters at every iteration."
- Break condition: If rank r is chosen too small, the compressed models will lose representational capacity, leading to significant performance drops.

### Mechanism 2
- Claim: Gradient scaling accelerates convergence of smaller sub-transformers by weighting their loss contributions relative to the largest sub-transformer (maxnet).
- Mechanism: Smaller sub-transformers have fewer trainable parameters, so their gradients are scaled up by a factor proportional to (n1/nj)^γ, where n1 is the parameter count of the maxnet and nj of sub-transformer j. This balances convergence rates across sub-transformers.
- Core assumption: Convergence rates are inversely proportional to the number of trainable parameters.
- Evidence anchors:
  - [abstract] "MLFS incorporates gradient scaling and knowledge distillation to improve convergence and performance."
  - [section] "We scale the losses using (n1/nj)γ, γ ≥ 1 so that training losses of smaller sub-transformer models converge at a rate similar to that of larger sub-transformer configurations."
- Break condition: If γ is set too high, gradient explosions may occur, destabilizing training.

### Mechanism 3
- Claim: Knowledge distillation from the teacher (maxnet) to all sub-transformers improves their performance by transferring both output logits and intermediate features.
- Mechanism: Distillation loss combines cross-entropy with KD from teacher logits and feature-based distillation. Features are projected to a low-dimensional space using shared projection matrices, reducing computational overhead.
- Core assumption: The teacher model has already learned useful representations that can be transferred to smaller architectures.
- Evidence anchors:
  - [abstract] "MLFS incorporates gradient scaling and knowledge distillation to improve convergence and performance."
  - [section] "Knowledge distillation is straightforward in a fixed-network fine-tuning setting. However, it is less so when fine-tuning a supernet..."
- Break condition: If the teacher is not well-trained or the task is dissimilar, distillation may introduce harmful bias.

## Foundational Learning

- Concept: Low-rank matrix decomposition
  - Why needed here: Enables parameter-efficient fine-tuning by approximating large weight matrices with small low-rank factors.
  - Quick check question: What is the formula for the number of parameters in a low-rank decomposition of a d×d matrix with rank r?

- Concept: Knowledge distillation
  - Why needed here: Allows smaller sub-transformers to learn from a larger pre-trained teacher, improving their performance despite compression.
  - Quick check question: In distillation, what two types of knowledge are transferred from teacher to student?

- Concept: Supernet training with weight sharing
  - Why needed here: Enables training of multiple subnetworks of different sizes simultaneously with shared weights, reducing overall training cost.
  - Quick check question: How does weight sharing affect the gradients computed for each subnetwork during training?

## Architecture Onboarding

- Component map:
  Teacher model -> Supernet with maxnet configuration -> Low-rank matrices (As, Bs) -> Sub-transformers -> Gradient scaling module -> Knowledge distillation loss module -> Feature projection matrices

- Critical path:
  1. Initialize teacher and supernet
  2. For each stage, sample sub-transformers and compute forward pass
  3. Apply gradient scaling and compute distillation losses
  4. Backpropagate only through low-rank matrices
  5. Update low-rank matrices and repeat

- Design tradeoffs:
  - Rank r: Higher r improves performance but increases parameters and computation.
  - Gradient scaling γ: Higher γ speeds up small subnet convergence but risks instability.
  - Distillation factor α: Higher α relies more on teacher knowledge but may slow task-specific learning.

- Failure signatures:
  - Underfitting: Low-rank decomposition rank too small, leading to poor performance.
  - Training instability: Gradient scaling factor too high, causing exploding gradients.
  - Suboptimal convergence: Distillation factor too high, preventing adaptation to target task.

- First 3 experiments:
  1. Train MLFS with rank r=4 on a small encoder task and compare performance to full fine-tuning.
  2. Vary gradient scaling γ (1, 2, 4) and observe impact on convergence speed of smallest subnet.
  3. Disable knowledge distillation (α=0) and measure performance drop to confirm its benefit.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MLFS compare to other parameter-efficient fine-tuning methods like LoRA or QLoRA in terms of computational efficiency and performance on edge devices?
- Basis in paper: [inferred] The paper mentions that PEFT methods like LoRA are not applicable to supernet training due to implications on weight-shared subnetworks, and that MLFS bridges this gap.
- Why unresolved: The paper does not provide a direct comparison between MLFS and other PEFT methods in terms of computational efficiency and performance on edge devices.
- What evidence would resolve it: A comparative study between MLFS and other PEFT methods, including LoRA and QLoRA, on the same edge device hardware and tasks, would provide insights into their relative performance and efficiency.

### Open Question 2
- Question: What are the specific hardware requirements and limitations for deploying MLFS-compressed models on edge devices?
- Basis in paper: [inferred] The paper discusses the need for efficient fine-tuning of LLMs for edge devices with limited memory and computational power, but does not provide specific hardware requirements or limitations for MLFS-compressed models.
- Why unresolved: The paper focuses on the algorithmic aspects of MLFS and its performance, but does not delve into the specific hardware constraints and requirements for deploying the compressed models on edge devices.
- What evidence would resolve it: A detailed analysis of the hardware requirements and limitations for deploying MLFS-compressed models on various edge devices, including memory, processing power, and storage constraints, would provide insights into the practical feasibility of the approach.

### Open Question 3
- Question: How does the choice of rank r in the low-rank matrices affect the performance and compression ratio of MLFS-compressed models?
- Basis in paper: [explicit] The paper mentions that the rank r is a hyperparameter that affects the number of trainable parameters and performance, and provides an ablation study on the impact of rank r on encoder models.
- Why unresolved: While the paper provides an ablation study on the impact of rank r on encoder models, it does not explore the relationship between rank r, performance, and compression ratio in detail, especially for decoder models.
- What evidence would resolve it: A comprehensive study on the impact of rank r on the performance and compression ratio of MLFS-compressed models, including both encoder and decoder models, would provide insights into the optimal choice of rank r for different scenarios.

## Limitations
- Encoder models show clear compression benefits while decoder compression is more limited
- Feature distillation implementation details are vague, particularly regarding layer mapping and projection dimensions
- Paper doesn't fully explain why decoder compression is limited or propose solutions

## Confidence

**High confidence**: The core mechanism of low-rank decomposition reducing parameters from d² to 6rd, and the multistage training approach with width/depth sampling are well-supported by the technical description and consistent with established supernet training methods.

**Medium confidence**: The gradient scaling mechanism and its impact on convergence rates is theoretically sound, but the specific choice of γ=2 for encoders and γ=1 for decoders lacks thorough justification. The ablation study shows benefits but doesn't explore the full hyperparameter space.

**Low confidence**: The decoder compression results are less convincing than encoder results, and the paper doesn't adequately explain this discrepancy or propose solutions. The feature distillation implementation details are insufficient for complete reproduction.

## Next Checks

1. **Decoder compression optimization**: Systematically vary rank r (4, 8, 16) and distillation factor α (0.5, 0.9, 1.0) for decoder models to identify settings that improve compression beyond the reported 5.7x, and analyze whether different configurations yield better HumanEval performance.

2. **Gradient scaling hyperparameter sweep**: For both encoder and decoder tasks, conduct a comprehensive grid search over γ values (0.5, 1, 1.5, 2, 3) and measure convergence speed and final accuracy to determine optimal settings and validate the theoretical convergence rate assumptions.

3. **Feature distillation implementation verification**: Implement multiple variants of feature distillation (different projection dimensions dlow, alternative layer mapping strategies gj) and measure their impact on encoder performance to validate the effectiveness of the proposed feature distillation approach and identify optimal configurations.