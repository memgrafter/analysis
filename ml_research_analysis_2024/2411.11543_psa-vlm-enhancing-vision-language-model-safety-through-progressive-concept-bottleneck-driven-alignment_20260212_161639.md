---
ver: rpa2
title: 'PSA-VLM: Enhancing Vision-Language Model Safety through Progressive Concept-Bottleneck-Driven
  Alignment'
arxiv_id: '2411.11543'
source_url: https://arxiv.org/abs/2411.11543
tags:
- safety
- content
- image
- answer
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of vision-language models
  (VLMs) to bypassing safety mechanisms through visual content, which can lead to
  the generation of harmful or inappropriate outputs. The proposed PSA-VLM method
  introduces a progressive, concept-driven safety alignment strategy using the Concept
  Bottleneck Model framework, incorporating safety modules (Safety Projector, Safety
  Tokens, and Safety Head) to monitor and intervene on safety-critical features.
---

# PSA-VLM: Enhancing Vision-Language Model Safety through Progressive Concept-Bottleneck-Driven Alignment

## Quick Facts
- arXiv ID: 2411.11543
- Source URL: https://arxiv.org/abs/2411.11543
- Authors: Zhendong Liu; Yuanbi Nie; Yingshui Tan; Jiaheng Liu; Xiangyu Yue; Qiushi Cui; Chongjun Wang; Xiaoyong Zhu; Bo Zheng
- Reference count: 40
- One-line primary result: PSA-VLM achieves state-of-the-art safety performance on RTVLM benchmark with scores of 8.26 (7B) and 8.46 (13B), significantly improving safety alignment while maintaining general performance.

## Executive Summary
PSA-VLM addresses the vulnerability of vision-language models (VLMs) to bypassing safety mechanisms through visual content. The method introduces a progressive, concept-driven safety alignment strategy using the Concept Bottleneck Model framework, incorporating specialized safety modules to monitor and intervene on safety-critical features. By training in two stages—first extracting safety concepts, then fine-tuning with these concepts—PSA-VLM achieves superior safety performance while maintaining general capabilities.

## Method Summary
PSA-VLM employs a progressive concept-based alignment strategy using the Concept Bottleneck Model framework. The method trains specialized safety modules (Safety Projector, Safety Tokens, and Safety Head) to extract and classify safety concepts from visual inputs. Training occurs in two stages: first, safety modules are trained to recognize safety concepts from visual features; second, the language model is fine-tuned using these safety-aligned representations. This approach enables explicit safety awareness and intervention capabilities while maintaining the model's general performance.

## Key Results
- Achieves state-of-the-art safety performance on RTVLM benchmark: 8.26 (7B model) and 8.46 (13B model)
- Demonstrates significant improvements in safety alignment while maintaining general performance
- Shows resilience to adversarial attacks through visual safety alignment strategy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Safety projector isolates high-risk visual features and transforms them into safety-aligned representations
- Mechanism: The safety projector `gϕ` takes raw visual features `ho` from the vision encoder and maps them into safety-aligned features `hs`, separate from general feature extraction
- Core assumption: The safety projector can effectively distinguish and isolate high-risk visual features from general visual features
- Evidence anchors:
  - [abstract] "By aligning model predictions with specific safety concepts, we improve defenses against risky images"
  - [section] "Here, the safety projector gϕ isolates high-risk features, enhancing the model's response to potential risks"
- Break condition: If the safety projector cannot effectively isolate high-risk features, or if it introduces significant noise into the general feature extraction pipeline

### Mechanism 2
- Claim: Safety tokens create explicit safety awareness within the model by signaling unsafe visual inputs
- Mechanism: Trainable safety tokens `st` categorize visual inputs as safe or unsafe and are concatenated with visual features to form safety-embedded representations
- Core assumption: The safety tokens can effectively signal and categorize unsafe visual content to the model
- Evidence anchors:
  - [abstract] "By aligning model predictions with specific safety concepts, we improve defenses against risky images"
  - [section] "To embed safety awareness directly within the model, we introduce trainable safety tokens st that categorize visual inputs as safe or unsafe"
- Break condition: If the safety tokens fail to accurately categorize unsafe content or if they become too general to provide meaningful safety guidance

### Mechanism 3
- Claim: Safety head provides concept-level classification and intervention capabilities through cross-attention
- Mechanism: The safety head uses cross-attention mechanism to generate attention-modulated features `hattn` from safety-aligned visual features `hs_comb`, then classifies these features into safety types and levels
- Core assumption: The cross-attention mechanism can effectively interpret safety-aligned features and classify them into meaningful safety categories
- Evidence anchors:
  - [abstract] "By aligning model predictions with specific safety concepts, we improve defenses against risky images"
  - [section] "Leveraging the VLM's native cross-attention capabilities, the safety head identifies both safety types (e.g., pornography, politics) and levels (e.g., high, medium, low risk)"
- Break condition: If the safety head fails to accurately classify safety types and levels, or if it becomes too complex to maintain efficient inference

## Foundational Learning

- Concept: Concept Bottleneck Model (CBM) framework
  - Why needed here: Provides interpretable, concept-level control over model outputs by incorporating a layer of human-interpretable concepts between input and output
  - Quick check question: How does CBM differ from traditional end-to-end training approaches?

- Concept: Progressive two-stage training
  - Why needed here: Allows for efficient initial safety module training before fine-tuning the larger language model, reducing computational costs
  - Quick check question: What are the advantages of separating safety module training from LLM fine-tuning?

- Concept: Safety concept classification and grading
  - Why needed here: Enables nuanced control over different types and levels of unsafe content, allowing for more flexible safety management
  - Quick check question: How do the six categories and three levels of safety concepts work together to provide comprehensive safety coverage?

## Architecture Onboarding

- Component map: Vision encoder → Safety projector → Safety tokens → Safety head → LLM
- Critical path: Vision encoder → Safety projector → Safety tokens → Safety head → LLM
- Design tradeoffs:
  - Additional safety modules add complexity but provide better safety control
  - Two-stage training increases implementation complexity but reduces computational costs
  - Safety tokens provide explicit safety awareness but require additional trainable parameters
- Failure signatures:
  - Safety projector fails to isolate high-risk features
  - Safety tokens incorrectly categorize safe content as unsafe
  - Safety head produces incorrect safety classifications
  - LLM fails to properly integrate safety concepts during fine-tuning
- First 3 experiments:
  1. Test safety projector isolation effectiveness by comparing high-risk feature extraction with and without the safety projector
  2. Evaluate safety token categorization accuracy on a diverse set of safe and unsafe images
  3. Measure safety head classification performance across all safety types and levels using a balanced test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PSA-VLM perform against sophisticated adversarial attacks designed specifically to bypass safety modules?
- Basis in paper: [explicit] The paper mentions that PSA-VLM's visual safety alignment strategy shows resilience to attacks but may be less effective against sophisticated adversarial attacks
- Why unresolved: The paper does not provide empirical results or detailed analysis of PSA-VLM's performance against such attacks
- What evidence would resolve it: Experimental results showing PSA-VLM's performance on adversarial examples specifically crafted to bypass safety modules

### Open Question 2
- Question: What is the impact of exaggerated safety behaviors on PSA-VLM's general performance in real-world applications?
- Basis in paper: [inferred] The paper mentions that PSA-VLM occasionally identified non-threatening data as risky during inference, displaying false positives
- Why unresolved: The paper does not provide quantitative analysis of the frequency or impact of these false positives on user experience
- What evidence would resolve it: User studies or A/B testing showing how PSA-VLM's safety filters affect user satisfaction and task completion rates

### Open Question 3
- Question: How does the ratio of clean to unsafe samples affect PSA-VLM's long-term performance and generalization capabilities?
- Basis in paper: [explicit] The paper discusses that the ratio of clean to unclean samples is crucial and experiments with different ratios
- Why unresolved: The paper only tests a limited range of ratios and does not explore long-term performance or effects on diverse datasets
- What evidence would resolve it: Extended experiments with various clean/unsafe sample ratios across multiple diverse datasets and over longer training periods

## Limitations

- Safety module architecture specifics are not fully detailed, making exact reproduction challenging
- Several safety datasets are close-sourced or require application access, limiting independent verification
- Safety concept granularity and cultural bias implications are not thoroughly explored

## Confidence

- High confidence: The core methodology of using progressive concept-driven alignment through the CBM framework is well-grounded in existing literature
- Medium confidence: The claimed performance improvements on the RTVLM benchmark are impressive but evaluation methodology lacks sufficient detail for full verification
- Low confidence: Long-term safety implications and potential for adversarial attacks against safety mechanisms are not thoroughly explored

## Next Checks

- Validation Check 1: Implement ablation studies removing each safety module component (safety projector, safety tokens, safety head) to quantify their individual contributions to safety performance
- Validation Check 2: Conduct cross-cultural validation by testing the model on safety datasets from diverse cultural contexts not represented in original training data
- Validation Check 3: Design adversarial attacks specifically targeting safety mechanisms, such as adversarial image perturbations or prompt engineering designed to bypass safety projector or confuse safety tokens