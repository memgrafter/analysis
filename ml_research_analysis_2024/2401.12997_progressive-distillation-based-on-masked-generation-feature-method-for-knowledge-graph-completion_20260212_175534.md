---
ver: rpa2
title: Progressive Distillation Based on Masked Generation Feature Method for Knowledge
  Graph Completion
arxiv_id: '2401.12997'
source_url: https://arxiv.org/abs/2401.12997
tags:
- knowledge
- distillation
- teacher
- information
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a progressive distillation method based on masked
  generation features for knowledge graph completion. The method aims to significantly
  reduce the complexity of pre-trained language models while maintaining performance.
---

# Progressive Distillation Based on Masked Generation Feature Method for Knowledge Graph Completion

## Quick Facts
- arXiv ID: 2401.12997
- Source URL: https://arxiv.org/abs/2401.12997
- Reference count: 10
- Key outcome: Progressive distillation method reduces model parameters by up to 56.7% while maintaining performance on knowledge graph completion

## Executive Summary
This paper proposes a progressive distillation method based on masked generation features for knowledge graph completion. The approach significantly reduces the complexity of pre-trained language models while maintaining performance by introducing masked generation feature distillation to learn richer representation information from teacher models, and a progressive distillation strategy to efficiently transfer knowledge to student models with fewer parameters. Experiments on WN18RR and FB15K-237 datasets demonstrate state-of-the-art performance on one dataset and substantial parameter reduction on the other.

## Method Summary
The method employs a two-stage process: pre-distillation using masked generation feature distillation (MGFD) to enhance baseline performance, followed by progressive distillation with multi-grade student models. The teacher model (BERT) generates masked feature vectors at inference time, capturing semantic information from both input and inferred tokens. Student models with decreasing parameters (12, 9, 6, 3 layers) and mask rates (20%, 10%, 5%, 0%) learn from three supervision signals: true labels, teacher scores, and masked generation features. The total loss combines these three components with balancing weights α and β.

## Key Results
- Achieved state-of-the-art performance on WN18RR dataset
- Reduced model parameters by up to 56.7% while maintaining performance on FB15K-237
- Demonstrated effectiveness of masked generation feature distillation for learning richer representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Masked generation feature distillation allows the student model to learn richer representations by transferring information about inferred entities.
- Mechanism: The teacher model generates masked feature vectors at inference time, capturing semantic information not only from input tokens but also from tokens inferred through deep network operations.
- Core assumption: Masked feature vectors from teacher model contain meaningful information about inferred entities.
- Evidence anchors:
  - [abstract] "masked generation feature distillation to learn richer representation information from teacher models"
  - [section] "Our proposed MGFD not only incorporates the semantic information within the input token set but also utilizes the inferential operations of deep networks to obtain semantic information from the inferred token set."

### Mechanism 2
- Claim: Progressive distillation with decreasing mask rate and model parameters enables efficient knowledge transfer between teacher and student models with different representational capacities.
- Mechanism: Gradually reducing both mask ratio and model parameters from higher to lower grades bridges the representational gap between teacher and student.
- Core assumption: Representational gap can be effectively bridged through gradual parameter and mask rate reduction.
- Evidence anchors:
  - [abstract] "progressive distillation strategy to efficiently transfer knowledge to student models with fewer parameters"
  - [section] "By gradually reducing the mask ratio and model parameters, it ensures that teacher model knowledge is effectively transferred to student model."

### Mechanism 3
- Claim: Combination of multiple supervision signals (true labels, teacher scores, and masked generation features) enables better learning than single supervision.
- Mechanism: Student model learns from three sources simultaneously: classification loss from true labels, distillation loss from teacher scores, and masked feature distillation loss.
- Core assumption: Different supervision signals provide complementary information that, when combined, improve learning efficiency and performance.
- Evidence anchors:
  - [section] "Overall, total loss comprises the three components above. α and β are used to balance the model's ability to capture both the global and local information of the triplets"
  - [section] "true label distillation is essential in the distillation process" and "The teacher model encodes the global features of the triplets"

## Foundational Learning

- Concept: Knowledge Graph Completion (KGC)
  - Why needed here: The paper is about improving KGC models, so understanding the task is fundamental
  - Quick check question: What is the difference between structure-based and description-based KGC methods?

- Concept: Knowledge Distillation
  - Why needed here: The entire method is built on knowledge distillation principles
  - Quick check question: What is the difference between logits distillation and feature distillation?

- Concept: Pre-trained Language Models (PLMs)
  - Why needed here: The method uses BERT as the base model and introduces masking techniques inspired by masked language modeling
  - Quick check question: How does masked language modeling in BERT work, and why might it inspire masked generation feature distillation?

## Architecture Onboarding

- Component map: Input triples and text descriptions → Masking module → Teacher model (BERT encoder with scoring) → Student models (multi-grade) → MGFD module → Scoring modules → Loss aggregation → Parameter update

- Critical path: Input → Masking → Teacher encoding → Student encoding → Feature comparison (MGFD) + Score comparison + Label comparison → Loss aggregation → Parameter update

- Design tradeoffs:
  - Mask rate vs. accuracy: Higher mask rates improve robustness but may reduce precision
  - Parameter reduction vs. performance: More aggressive compression leads to greater parameter savings but more performance degradation
  - Number of student grades: More grades enable smoother transitions but increase training complexity

- Failure signatures:
  - Poor performance on Hits@1 but good on Hits@10: Likely issue with teacher model giving wrong answers due to too many relations per entity
  - Performance degradation despite parameter reduction: Mask rate reduction may be too aggressive
  - No improvement over baseline: MGFD module may not be effective for the specific dataset

- First 3 experiments:
  1. Verify baseline model performance on WN18RR and FB15K-237
  2. Implement and test MGFD module with fixed mask rate on pre-distillation stage
  3. Implement progressive distillation with decreasing mask rates and parameters across multiple student grades

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the mask rate and mask positions be adaptively selected during the distillation stage to improve knowledge transfer efficiency?
- Basis in paper: [explicit] The authors mention exploring adaptive selection of mask rate and mask positions during the distillation stage in their conclusion.
- Why unresolved: The paper does not provide specific methods or results for adaptive selection of mask rate and positions.
- What evidence would resolve it: Experimental results comparing different adaptive strategies for mask rate and position selection, showing improvements in knowledge transfer efficiency and model performance.

### Open Question 2
- Question: What are the optimal loss function weights (α and β) for balancing global and local triplet information in different knowledge graph completion tasks?
- Basis in paper: [explicit] The authors mention that α and β are used to balance the model's ability to capture both global and local information, but they are determined through grid search.
- Why unresolved: The paper does not provide a systematic approach to determine optimal weights for different tasks.
- What evidence would resolve it: A comprehensive study showing the impact of different α and β values on model performance across various knowledge graph completion tasks, with recommendations for optimal settings.

### Open Question 3
- Question: How does the performance of PMD compare to other knowledge distillation methods specifically designed for description-based KGC models?
- Basis in paper: [inferred] The paper focuses on PMD's performance compared to the baseline and structure-based methods, but does not directly compare it to other distillation methods for description-based KGC.
- Why unresolved: The paper does not include comparisons with other distillation methods tailored for description-based KGC.
- What evidence would resolve it: Experimental results comparing PMD to other distillation methods specifically designed for description-based KGC, showing relative performance and efficiency gains.

## Limitations
- The effectiveness of masked generation features depends on teacher model's ability to generate meaningful semantic information from inferred entities, which is not directly validated
- Progressive distillation strategy's effectiveness depends on careful selection of mask rates and parameter reductions, but sensitivity analysis is limited
- Results are reported on only two datasets (WN18RR and FB15K-237), which may not generalize to knowledge graphs with different characteristics

## Confidence

- **High confidence**: The overall experimental methodology and evaluation protocol (using standard KGC metrics MRR and Hits@k) are sound and well-established in the literature. The reported parameter reduction of up to 56.7% is verifiable through model architecture specifications.

- **Medium confidence**: The three-stage knowledge distillation framework (true label, score, and masked feature distillation) is theoretically plausible, but the specific implementation details of how masked generation features are computed and used in the distillation loss are not fully specified, making exact reproduction challenging.

- **Low confidence**: The mechanism by which masked generation features capture richer semantic information and the specific design choices for progressive reduction (12→9→6→3 layers, 20%→10%→5%→0% mask rates) are presented without sufficient ablation studies to justify these particular configurations over alternatives.

## Next Checks

1. **Ablation study on masked generation features**: Train the model without the MGFD component to quantify its specific contribution to performance improvements beyond standard knowledge distillation.

2. **Hyperparameter sensitivity analysis**: Systematically vary the mask rates (e.g., 30%, 15%, 10%, 0%) and layer reductions (e.g., 10→8→6→4 instead of 12→9→6→3) to determine the optimal progressive distillation schedule and identify whether the reported configuration is truly optimal.

3. **Cross-dataset generalization test**: Evaluate the method on a third knowledge graph dataset with different properties (such as NELL-995 or Kinship) to assess whether the approach generalizes beyond WN18RR and FB15K-237, particularly testing its robustness on datasets with higher entity in-degree distributions where masking might cause teacher model failures.