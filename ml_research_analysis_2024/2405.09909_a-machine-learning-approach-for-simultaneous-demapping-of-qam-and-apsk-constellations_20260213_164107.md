---
ver: rpa2
title: A Machine Learning Approach for Simultaneous Demapping of QAM and APSK Constellations
arxiv_id: '2405.09909'
source_url: https://arxiv.org/abs/2405.09909
tags:
- constellations
- apsk
- representation
- constellation
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a probabilistic framework enabling a single
  deep neural network (DNN) demapper to simultaneously handle multiple QAM and APSK
  constellations without hardcoding bit mappings. The key innovation is separating
  symbol probabilities from bit mappings, allowing the network to be independent of
  specific mappings and reducing output size by exploiting hierarchical relationships
  in constellation families.
---

# A Machine Learning Approach for Simultaneous Demapping of QAM and APSK Constellations

## Quick Facts
- arXiv ID: 2405.09909
- Source URL: https://arxiv.org/abs/2405.09909
- Reference count: 17
- One-line primary result: Single DNN demapper achieves BER within 0.5 dB of optimal bound for 13 diverse QAM/APSK constellations

## Executive Summary
This paper introduces a probabilistic framework enabling a single deep neural network to simultaneously demap multiple QAM and APSK constellations without hardcoding specific bit mappings. The key innovation is separating symbol probability estimation from bit mapping, allowing the network to handle arbitrary mappings while exploiting hierarchical structures in constellation families to reduce output dimensionality. The framework demonstrates that joint training on diverse constellations can achieve performance matching or exceeding constellation-specific models.

## Method Summary
The approach uses a DNN to predict symbol probabilities directly, which are then mapped to bit probabilities through a flexible bit mapping function. This decouples the learning of channel and constellation structure from specific bit assignments. The framework exploits hierarchical relationships in QAM and APSK constellations - QAM's square structure allows interleaved in-phase and quadrature bits, while certain APSK constellations with phase offsets can share quadrant identification bits. The DNN architecture consists of two hidden layers (256 units each, ReLU, batch norm) producing LLRs for representation bits, which are converted to symbol probabilities and then to bit probabilities via hierarchical rules. Training uses binary cross-entropy loss with numerical stability mechanisms including the LogSumExp trick and clipping.

## Key Results
- Single DNN demapper achieves BER within 0.5 dB of optimal hard-decision bound under AWGN for all tested constellations
- Performance matches or exceeds constellation-specific models trained separately
- Hierarchical representation reduces output dimensionality (e.g., 12 vs 24 bits for certain APSK/QPSK pairs) while maintaining accuracy
- Framework generalizes to arbitrary bit mappings without loss of performance

## Why This Works (Mechanism)

### Mechanism 1
Separating symbol probabilities from bit mappings allows a single DNN to handle multiple constellations without hardcoding. By predicting symbol probabilities directly, the DNN can later apply any bit mapping function to convert symbols to bits, decoupling the learning of channel and constellation structure from specific bit assignments.

### Mechanism 2
Hierarchical structure in QAM and APSK constellations can be exploited to reduce DNN outputs without losing accuracy. QAM's square structure allows encoding each point using interleaved in-phase and quadrature bits, while certain APSK constellations can share decision boundaries by splitting quadrants hierarchically.

### Mechanism 3
Joint training on multiple constellations doesn't degrade performance because shared bits correspond to shared decision boundaries. By only sharing bits that represent identical physical decisions (e.g., quadrant identification), the model avoids conflicting objectives while separate bits handle constellation-specific details.

## Foundational Learning

- **Symbol demapping vs bit demapping**: Understanding the difference between predicting symbols directly and predicting bits is key to grasping why the framework is more flexible. Quick check: Why is it easier to generalize a symbol-level model to different bit mappings than a bit-level model?

- **Hierarchical decision boundaries**: The framework relies on recognizing and exploiting hierarchical structure in constellations to reduce output dimensionality. Quick check: How does the square structure of QAM allow it to be represented with fewer bits than a flat symbol enumeration?

- **LogSumExp trick and numerical stability**: The framework mentions using LSE to avoid numerical instability when summing symbol probabilities. Quick check: What problem does the LogSumExp trick solve when converting symbol probabilities to bit probabilities?

## Architecture Onboarding

- **Component map**: I/Q samples → DNN (hidden layers: 256 units, ReLU, batch norm) → representation LLRs → symbol probabilities (via hierarchical rules) → bit probabilities (via Mb mapping) → loss (binary cross-entropy)

- **Critical path**: I/Q samples → DNN → representation LLRs → symbol probabilities → bit probabilities → loss

- **Design tradeoffs**: More shared bits reduces model size but requires constellations to have compatible structure; separate representations increase flexibility but require more outputs; joint training may improve generalization but could slow convergence

- **Failure signatures**: BER curves plateau well above optimal bound; training loss decreases but validation performance is poor; model works well on training constellations but fails on new ones

- **First 3 experiments**:
  1. Train on 256-QAM only, test on smaller QAM constellations to verify hierarchical generalization
  2. Add one APSK constellation with π/8 offset, check if shared quadrant bits improve performance
  3. Train on mixed QAM/APSK set, measure BER vs number of shared bits to find optimal sharing level

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed probabilistic framework scale to constellation families beyond QAM and APSK, such as non-regular or higher-dimensional constellations? The paper focuses on QAM and APSK but notes the framework is modular and applicable to any deep-learning receiver pipeline. Empirical results on other constellation families would resolve this uncertainty.

### Open Question 2
What is the computational complexity of the DNN demapper compared to traditional demappers, especially when handling multiple constellations? The paper mentions potential computational complexity analysis but does not provide detailed comparisons. A detailed complexity analysis would quantify the tradeoff in computational resources.

### Open Question 3
How does the performance of the joint DNN demapper change under more complex channel conditions, such as fading or interference? The paper evaluates under AWGN but notes the need for testing under more complicated channel effects. Performance under realistic channel conditions remains uncertain.

## Limitations
- Performance claims assume AWGN channels with no evaluation for fading or interference scenarios
- Numerical stability mechanisms (LogSumExp trick and clipping) are mentioned but implementation details are not specified
- Dependence on hierarchical structure limits applicability to constellations lacking regular geometric patterns

## Confidence

**High confidence**: The fundamental separation of symbol probabilities from bit mappings is theoretically sound and the performance improvement over separate DNNs is well-supported. The concept that symbol-level modeling is more flexible than bit-level modeling is established.

**Medium confidence**: The hierarchical exploitation of constellation structure is supported by geometric analysis but depends on the assumption that decision boundaries align sufficiently across constellations. The 0.5 dB performance claim is based on simulations but requires independent verification.

**Low confidence**: The numerical stability mechanisms are mentioned but not detailed, and the performance in non-AWGN scenarios is entirely unknown. The exact impact of joint training versus separate training on convergence speed and generalization is not quantified.

## Next Checks

1. **Numerical Stability Verification**: Implement the LogSumExp trick and clipping mechanisms as described, then verify that training does not produce numerical instability (NaNs or inf values) across the full range of signal-to-noise ratios and constellation combinations.

2. **Cross-Constellation Generalization Test**: Train on a subset of 8 constellations (QPSK, 16-QAM, 64-QAM, 256-QAM, and four APSK variants), then evaluate BER on two held-out constellations (e.g., 32-QAM and a different APSK pattern) to verify the framework's generalization capability.

3. **Hierarchical Compression Validation**: Systematically vary the number of shared bits in the hierarchical representation and measure the tradeoff between model size reduction and BER performance, particularly focusing on whether the claimed 12-bit compression for certain constellation sets maintains the 0.5 dB bound.