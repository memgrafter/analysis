---
ver: rpa2
title: Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as
  Science Communicators
arxiv_id: '2409.14037'
source_url: https://arxiv.org/abs/2409.14037
tags:
- turbo
- reasoning
- responses
- score
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SCiPS-QA, a novel dataset of 742 complex Yes/No
  scientific questions spanning Physics, Chemistry, Mathematics, and other STEM fields,
  designed to test LLMs' scientific reasoning and answerability awareness. The dataset
  includes both closed questions with definitive answers and open problems lacking
  scientific consensus.
---

# Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators

## Quick Facts
- arXiv ID: 2409.14037
- Source URL: https://arxiv.org/abs/2409.14037
- Authors: Prasoon Bajpai; Niladri Chatterjee; Subhabrata Dutta; Tanmoy Chakraborty
- Reference count: 40
- Key outcome: Llama-3-70B matches or exceeds GPT-4 Turbo in accuracy (MACC: 0.693 vs 0.646), while most models struggle with open problems; human evaluators frequently deceived by confident but incorrect responses

## Executive Summary
This paper introduces SCiPS-QA, a novel dataset of 742 complex Yes/No scientific questions spanning Physics, Chemistry, Mathematics, and other STEM fields, designed to test LLMs' scientific reasoning and answerability awareness. The authors benchmark 16 LLMs (3 proprietary: GPT-4 Turbo, GPT-3.5 Turbo, text-davinci-003; 13 open-source: Llama-2/3, Mistral variants) using comprehensive evaluation metrics including accuracy, consistency, hallucination detection via SelfCheckGPT, and human evaluation of response convincingness. Key findings show that Llama-3-70B matches or exceeds GPT-4 Turbo in accuracy, while most models struggle with open problems. Proprietary models fail to reliably verify their own responses, and human evaluators are frequently deceived by incorrect but confident responses, highlighting risks in deploying LLMs as science communicators.

## Method Summary
The authors created SCiPS-QA, a dataset of 742 Yes/No scientific questions including both closed questions with definitive answers and open problems lacking scientific consensus. They benchmarked 16 LLMs using few-shot prompting with temperature 0.0 for main responses and temperature 1.0 for 10 stochastic responses per question. Evaluation employed SelfCheckGPT for hallucination detection (comparing main responses to stochastic samples via BERTScore, NLI, and prompt-based methods), GPT-3.5 Turbo as an evaluator for reasoning passages, and human evaluation of convincingness. Metrics included MACC (Main Response Accuracy), MSACC (Major Stochastic Response Accuracy), VSR (Variation in Stochastic Responses), CMACC/CMSACC for closed questions, and OMACC/OMSACC for open questions.

## Key Results
- Llama-3-70B achieves MACC of 0.693 vs GPT-4 Turbo's 0.646, with superior performance on closed questions (CMACC: 0.780 vs 0.750)
- Most models struggle with open problems, showing poor OMACC and OMSACC scores
- Human evaluators frequently assign high convince-factor scores to incorrect but confident responses
- Proprietary models fail to reliably verify their own responses, with GPT-4 Turbo performing worse than GPT-3.5 Turbo in this task

## Why This Works (Mechanism)

### Mechanism 1
SelfCheckGPT detects hallucinations by comparing main responses to 10 stochastic samples generated at high temperature. For each sentence in the main response, it calculates maximum semantic similarity (BERTScore) or logical entailment (NLI) across all sentences in stochastic passages. Low similarity or high contradiction implies hallucination. This works under the assumption that stochastic responses capture the true distribution of plausible continuations, and the main response deviates when it hallucinates.

### Mechanism 2
LLM-based evaluators (GPT-3.5 Turbo) assign convincingness and factuality scores by reading reasoning passages and outputting scores on 1-5 scales. High scores indicate persuasiveness or factual accuracy. This relies on the LLM's internal knowledge and reasoning ability to distinguish correct from incorrect reasoning. The method works when the evaluator has sufficient domain knowledge and is not biased by the fluency of explanations.

### Mechanism 3
Human evaluators are deceived by confident but incorrect responses because they rely on the fluency and structure of explanations rather than independent fact verification. When reading questions and reasoning (with or without answers), humans assign convince-factor scores that often remain high even for incorrect responses. This mechanism works because humans tend to trust well-structured, confident explanations without verifying underlying facts, especially when not domain experts.

## Foundational Learning

- **Stochastic response sampling and temperature decoding**: Used to generate multiple plausible continuations for hallucination detection. Quick check: What is the effect of increasing temperature from 0.0 to 1.0 on LLM output diversity?
- **Semantic similarity metrics (BERTScore, NLI)**: Core to SelfCheckGPT's hallucination scoring mechanism. Quick check: How does BERTScore differ from simple cosine similarity in measuring semantic closeness?
- **Prompt engineering for zero-shot LLM evaluation**: Used to get LLMs to rate convincingness, factuality, and information mismatch without fine-tuning. Quick check: What prompt elements influence the scale and consistency of LLM-generated scores?

## Architecture Onboarding

- **Component map**: SCiPS-QA dataset → LLM response collection (main + 10 stochastic) → Evaluation module → SelfCheckGPT (BERTScore/NLI/Prompt) → LLM evaluator (GPT-3.5 Turbo) → Human evaluation → Storage
- **Critical path**: 1) Load SCiPS-QA question → 2) Generate main response at temp=0.0 → 3) Generate 10 stochastic responses at temp=1.0 → 4) Run SelfCheckGPT variants → 5) Run LLM evaluator on main response → 6) Store results
- **Design tradeoffs**: Sampling 10 stochastic responses increases hallucination detection reliability but raises compute cost; using GPT-3.5 Turbo as evaluator is cheaper than GPT-4 Turbo but may be less accurate; including answers in prompts can boost human deception but improves fact-checking context
- **Failure signatures**: High invalid response rate → prompt or model capability issue; uniform low hallucination scores across all models → stochastic responses not diverse enough; LLM evaluator scores not correlated with correctness → evaluator lacks domain knowledge or is biased
- **First 3 experiments**: 1) Run a single SCiPS-QA question through the pipeline and inspect raw main and stochastic responses → 2) Compare SelfCheckGPT BERTScore vs NLI outputs for one question → 3) Test LLM evaluator on known correct vs incorrect reasoning pair

## Open Questions the Paper Calls Out

### Open Question 1
Can instruction-tuned LLMs like Llama-3-70B-instruct outperform proprietary models like GPT-4 Turbo in scientific reasoning tasks when properly evaluated? The paper shows Llama-3-70B-instruct achieving MACC of 0.693 vs GPT-4 Turbo's 0.646 and superior performance on closed questions (CMACC: 0.780 vs 0.750), but broader testing across diverse scientific domains with larger sample sizes is needed.

### Open Question 2
What training methodology improvements could help LLMs better recognize and abstain from answering open scientific problems? The paper identifies that most LLMs struggle with OMACC and OMSACC scores, showing poor ability to recognize unanswerable questions, but doesn't explore specific training techniques like curriculum learning or synthetic data generation.

### Open Question 3
How can hallucination detection methods be improved to reliably identify incorrect reasoning in complex scientific responses? Current SelfCheckGPT variants failed to conclusively detect hallucinations in GPT models despite their incorrect responses, and human evaluation is unreliable, indicating need for new frameworks potentially incorporating external knowledge bases.

## Limitations

- SelfCheckGPT relies on the assumption that high-temperature stochastic responses represent a valid "truth distribution," which may not hold if all samples are also hallucinated
- Human evaluation is based on a small sample size and may not generalize across broader scientific domains or expertise levels
- The paper doesn't provide extensive ablation studies on different verification prompts or model variants to support claims about proprietary models failing to verify their own responses

## Confidence

**High Confidence**: 
- Llama-3-70B achieving comparable or superior accuracy to GPT-4 Turbo on closed questions
- Most models performing poorly on open problems
- Human evaluators frequently rating incorrect but fluent responses as convincing

**Medium Confidence**: 
- SelfCheckGPT's effectiveness in detecting hallucinations
- LLM evaluator (GPT-3.5 Turbo) scores reflecting true factuality

**Low Confidence**: 
- Generalizability of human evaluation results beyond sampled questions
- Claims about proprietary models failing to verify their own responses

## Next Checks

1. **Stochastic Response Diversity Validation**: Generate 50 stochastic responses (instead of 10) for a subset of questions and measure hallucination score stability to test robustness to sampling noise.

2. **Human Expert Validation**: Repeat human evaluation with domain experts (e.g., physics PhDs) on a stratified sample of correct and incorrect responses to quantify deception extent and compare against original results.

3. **SelfCheckGPT Threshold Sensitivity**: Systematically vary BERTScore and NLI thresholds used in SelfCheckGPT and measure impact on hallucination detection accuracy to clarify whether current thresholds are optimal or arbitrary.