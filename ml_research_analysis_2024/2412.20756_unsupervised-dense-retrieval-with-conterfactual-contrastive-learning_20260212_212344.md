---
ver: rpa2
title: Unsupervised dense retrieval with conterfactual contrastive learning
arxiv_id: '2412.20756'
source_url: https://arxiv.org/abs/2412.20756
tags:
- retrieval
- counterfactual
- document
- learning
- passage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of making dense retrieval models
  more robust and interpretable by improving their sensitivity to fine-grained relevance
  signals. The authors propose a counterfactual contrastive learning method that enhances
  the model's ability to distinguish key passages determining document relevance while
  remaining stable to changes in irrelevant passages.
---

# Unsupervised dense retrieval with conterfactual contrastive learning

## Quick Facts
- arXiv ID: 2412.20756
- Source URL: https://arxiv.org/abs/2412.20756
- Reference count: 40
- One-line primary result: The method improves dense retrieval robustness and interpretability by making models sensitive to key passage modifications while stable to irrelevant changes.

## Executive Summary
This paper addresses the challenge of making dense retrieval models more robust and interpretable by improving their sensitivity to fine-grained relevance signals. The authors propose a counterfactual contrastive learning method that enhances the model's ability to distinguish key passages determining document relevance while remaining stable to changes in irrelevant passages. They extract key passages using a Shapley value-based method without relying on passage-level relevance annotations. The approach introduces counterfactual documents as hard negatives and pseudo-positives during training, improving both key passage extraction (MRR@10p) and robustness against adversarial attacks.

## Method Summary
The method introduces counterfactual contrastive learning for dense retrieval by first extracting key passages using Shapley value computation, then generating counterfactual documents through various operations (deletion, modification, replacement, adversarial). These counterfactual documents serve as training examples with different roles - as hard negatives or pseudo-positives - depending on their semantic relationship to the original document. The training combines classic dense retrieval losses with counterfactual-specific losses, using dynamic hyperparameter adjustment through coupling learning. The approach is evaluated on MSMARCO-doc and MSMARCO-passage datasets with pre-trained models like DPR, ANCE, colBERT, and ME-BERT.

## Key Results
- Significant improvements in key passage extraction performance (MRR@10p) compared to baseline models
- Enhanced robustness against adversarial attacks, surpassing state-of-the-art anti-attack methods
- Maintained or slightly improved document retrieval performance while improving interpretability
- Effective key passage extraction without requiring passage-level relevance annotations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Counterfactual contrastive learning improves dense retrieval robustness by making models sensitive to key passage modifications while stable to irrelevant changes.
- Mechanism: The method constructs counterfactual documents by removing or modifying key passages (identified via Shapley value), then trains the model to distinguish these from regular positive and negative documents. This forces the model to learn which passages determine relevance.
- Core assumption: The Shapley value accurately captures the importance of passages in determining relevance scores for dense retrieval models.
- Evidence anchors:
  - [abstract] "Our method can extract key passages without reliance on the passage-level relevance annotations."
  - [section 3.2] "We propose a counterfactual passage extraction method based on the Shapley value [59]."
  - [corpus] Weak - corpus doesn't directly address Shapley value effectiveness.
- Break condition: If Shapley value computation becomes too computationally expensive for large documents, or if the assumption that passage importance is additive fails for dense retrieval models.

### Mechanism 2
- Claim: Using adversarial counterfactual documents as training examples improves robustness against actual attacks.
- Mechanism: The method creates adversarial counterfactual documents by making minimal global modifications (word replacements) that preserve semantics but aim to fool the retrieval model. Training with these examples teaches the model to recognize and resist such manipulations.
- Core assumption: Adversarial modifications that preserve semantic meaning but alter relevance scoring can be constructed and identified effectively.
- Evidence anchors:
  - [abstract] "Moreover, the regularized dense retrieval models exhibit heightened robustness against adversarial attacks, surpassing the state-of-the-art anti-attack methods."
  - [section 3.3] "We construct the adversarial counterfactual documents ð‘‘ð‘Žð‘‘ð‘£ as: ð‘‘ð‘Žð‘‘ð‘£ = ð‘Žð‘Ÿð‘” max... "
  - [corpus] Weak - corpus doesn't directly address adversarial training effectiveness.
- Break condition: If adversarial examples become too complex to generate efficiently, or if the model learns to overfit to specific attack patterns rather than developing general robustness.

### Mechanism 3
- Claim: Dynamic hyperparameter adjustment through coupling learning optimizes the balance between different contrastive loss components.
- Mechanism: The coupling learning method [33] dynamically adjusts weights for different loss terms during training, allowing the model to find optimal balance between learning from positive examples, counterfactual negatives, and adversarial examples.
- Core assumption: The coupling learning approach can effectively balance multiple competing training objectives in dense retrieval.
- Evidence anchors:
  - [section 3.3] "Employing the couple learning method [33] to dynamically adjust the hyperparameters."
  - [section 5.2] "Results from models trained with MSMARCO passage labels indicate that the coupling learning method [33] yields the best performance."
  - [corpus] Weak - corpus doesn't directly address coupling learning effectiveness.
- Break condition: If the dynamic adjustment becomes unstable during training, or if the coupling learning method doesn't generalize well across different datasets.

## Foundational Learning

- Concept: Shapley value computation in cooperative game theory
  - Why needed here: Used to quantify the importance of individual passages in determining document relevance scores
  - Quick check question: Can you explain how Shapley value accounts for all possible combinations of passages when determining individual passage importance?

- Concept: Contrastive learning objectives
  - Why needed here: Forms the basis for training the model to distinguish between different types of documents (positive, counterfactual, negative)
  - Quick check question: What's the key difference between treating counterfactual documents as hard negatives versus pseudo-positives in the loss function?

- Concept: Adversarial attack construction
  - Why needed here: Understanding how to create effective adversarial examples that preserve semantics while manipulating retrieval scores
  - Quick check question: How does the ðœ– parameter in the adversarial construction equation control the difficulty of the attack?

## Architecture Onboarding

- Component map:
  Document preprocessing pipeline (passage segmentation) -> Shapley value calculator -> Counterfactual document generator (deletion/modification/replacement/adversarial) -> Dense retrieval model (DPR, ANCE, etc.) -> Loss function combiner with dynamic weights -> Training loop with hard negative mining

- Critical path:
  1. Segment documents into passages
  2. Calculate Shapley values for each passage
  3. Generate counterfactual documents
  4. Train dense retrieval model with contrastive losses
  5. Evaluate robustness against attacks

- Design tradeoffs:
  - Window size vs. computational efficiency in passage segmentation
  - Type of counterfactual construction (deletion vs. modification vs. adversarial) vs. training effectiveness
  - Dynamic vs. static hyperparameter adjustment

- Failure signatures:
  - High variance in relevance scores across similar documents
  - Poor performance on adversarial attacks despite good training metrics
  - Shapley values that don't correlate with human judgment of passage importance

- First 3 experiments:
  1. Compare Shapley-based key passage extraction against simple ranking change method on a small dataset
  2. Test different counterfactual construction methods (deletion vs. modification) on model robustness
  3. Evaluate the impact of different loss weight strategies (relevance score vs. Shapley value vs. coupling learning) on key passage extraction performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the efficiency and scalability of counterfactual learning methods be improved for large document collections?
- Basis in paper: [explicit] The authors acknowledge that "The efficiency and scalability of our methods need careful consideration when applied to large document collections" and that "Handling the computational challenges for such large-scale deployments is an area that requires further research."
- Why unresolved: The paper demonstrates the effectiveness of counterfactual learning on smaller datasets like MSMARCO-doc, but does not address the computational complexity when scaling to much larger document collections. The Shapley value calculation in particular requires sampling different passage combinations, which becomes computationally expensive with longer documents.
- What evidence would resolve it: Comparative studies showing runtime and memory usage of the proposed method versus traditional dense retrieval on large-scale datasets (e.g., web-scale collections), along with proposed optimizations or approximations for the Shapley value computation.

### Open Question 2
- Question: How robust is the counterfactual learning approach against evolving adversarial attack techniques beyond the ones tested?
- Basis in paper: [explicit] The authors state that "as adversarial attack techniques continue to evolve, our model may not be fully robust against new and sophisticated attacks" and plan to "focus on these aspects to enhance the practicality and robustness of our models" in future work.
- Why unresolved: While the paper demonstrates improved robustness against four specific attack methods (TS, PRADA, PAT, MCARA), adversarial attacks are constantly evolving. The paper does not test against more sophisticated attacks or provide theoretical guarantees of robustness against future attacks.
- What evidence would resolve it: Empirical testing against emerging attack techniques, analysis of failure cases when faced with new attack types, and/or theoretical analysis proving bounds on model vulnerability to unseen attacks.

### Open Question 3
- Question: What is the optimal trade-off between using human-labeled passage relevance data versus synthetic data generated by LLMs for training counterfactual models?
- Basis in paper: [explicit] The authors explore two training setups: one using Shapley-value-based key passage extraction and another using GPT-3.5 to label relevant passages, noting that "recognizing the challenge of obtaining high-quality passage-level relevance labels and the associated time and cost implications, we also conduct experiments generated by gpt-3.5."
- Why unresolved: The paper shows that both approaches yield improvements but does not systematically compare the quality of models trained with human-labeled data versus LLM-generated labels, nor does it explore hybrid approaches or the impact of label quality on downstream performance.
- What evidence would resolve it: Head-to-head comparisons of model performance when trained with varying proportions of human-labeled versus LLM-labeled data, analysis of the cost-benefit trade-off, and evaluation of techniques to improve the quality of synthetic labels.

## Limitations

- Computational cost of Shapley value calculation becomes prohibitive for large documents
- Effectiveness depends heavily on quality of passage segmentation and assumption about passage importance additivity
- Limited evaluation against evolving adversarial attack techniques beyond the tested methods
- Dynamic hyperparameter adjustment may not generalize well across different datasets

## Confidence

Medium confidence

- Shapley value effectiveness: Medium
- Adversarial training generalization: Low
- Coupling learning adaptability: Medium
- Scalability to large collections: Low

## Next Checks

1. Verify Shapley value computation accuracy by comparing extracted key passages against human-annotated importance rankings on a small subset of documents
2. Test model robustness against additional attack methods not covered in the paper, such as semantic-preserving synonym replacement attacks
3. Evaluate the computational overhead of counterfactual training by measuring training time and memory usage compared to standard dense retrieval training