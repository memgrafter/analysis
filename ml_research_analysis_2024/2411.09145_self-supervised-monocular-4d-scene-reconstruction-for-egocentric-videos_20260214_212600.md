---
ver: rpa2
title: Self-Supervised Monocular 4D Scene Reconstruction for Egocentric Videos
arxiv_id: '2411.09145'
source_url: https://arxiv.org/abs/2411.09145
tags:
- video
- camera
- reconstruction
- depth
- scene
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents UniHOI, a unified model for fast, dense, and
  generalizable 4D reconstruction of egocentric hand-object interaction (HOI) scenes.
  The model jointly estimates camera intrinsic, camera poses, and video depth in a
  feed-forward manner, optimizing their consistency through end-to-end supervision
  in 3D space.
---

# Self-Supervised Monocular 4D Scene Reconstruction for Egocentric Videos

## Quick Facts
- **arXiv ID:** 2411.09145
- **Source URL:** https://arxiv.org/abs/2411.09145
- **Reference count:** 40
- **Primary result:** UniHOI achieves state-of-the-art dense pointclouds sequence reconstruction for egocentric hand-object interaction scenes without labeled data

## Executive Summary
This paper presents UniHOI, a unified self-supervised model for 4D reconstruction of egocentric hand-object interaction scenes. The model jointly estimates camera intrinsic parameters, camera poses, and video depth from monocular videos, optimizing their consistency end-to-end in 3D space. Trained solely on large-scale unlabeled HOI video datasets and leveraging priors from foundation vision models, UniHOI overcomes the scarcity of labeled data while achieving state-of-the-art performance in dense pointclouds reconstruction and long-term 3D scene flow recovery.

## Method Summary
UniHOI is a unified model that jointly predicts camera intrinsic, camera poses, and video depth for egocentric HOI videos. The model uses a frozen UniDepth encoder with video adaptors (Unet3D + Transformer) to process 4 consecutive frames, outputting per-frame depth, camera intrinsic, and procrustes-alignment confidence maps. Camera poses are estimated through weighted procrustes alignment using these confidence maps to filter unreliable points. The model is trained end-to-end using 3D space supervision, leveraging foundation model priors (UniDepth for depth, EgoHOS for HOI masks, GMFlow for optical flow, CoTracker for tracking) to overcome the lack of labeled HOI data.

## Key Results
- Achieves state-of-the-art performance in dense pointclouds sequence reconstruction with superior 3D Chamfer Distance and F-score metrics
- Excels in long-term 3D scene flow recovery with strong zero-shot generalization to unseen HOI scenes
- Demonstrates 30x speedup over optimization-based methods while maintaining high reconstruction quality

## Why This Works (Mechanism)

### Mechanism 1
- Joint optimization of camera intrinsic, poses, and video depth in a unified model improves reconstruction consistency by reducing module-wise error accumulation
- The model simultaneously predicts all geometric variables and optimizes them end-to-end using 3D space supervision, aligning pointclouds across frames via optical flow and tracking priors
- Core assumption: Consistency loss in 3D space can effectively align predictions across different frames without requiring explicit labeled correspondences
- Break condition: If priors from pretrained models are inaccurate or if the scene contains complex occlusions, joint optimization may amplify errors rather than reduce them

### Mechanism 2
- Leveraging pretrained vision models as supervision enables effective training without labeled HOI data by providing geometric and motion priors
- The model uses depth predictions from UniDepth, HOI masks from EgoHOS, and optical flow from GMFlow as pseudo-labels to supervise learning of depth, camera poses, and confidence maps
- Core assumption: Pretrained models provide accurate enough priors to guide training of the unified model in absence of direct labels
- Break condition: If domain gap between pretrained models' training data and HOI videos is too large, priors may mislead training process

### Mechanism 3
- Reformulating camera pose estimation as confidence-weighted procrustes alignment problem improves robustness to dynamic regions and occlusions
- Instead of directly predicting camera poses, the model predicts confidence maps that indicate reliability of each pixel for alignment, filtering out unreliable points from dynamic regions, scene edges, and inaccurate optical flow
- Core assumption: Confidence maps can effectively identify and downweight unreliable regions for pose estimation, leading to more stable alignment across frames
- Break condition: If confidence maps themselves are inaccurate or if dynamic regions are too large, weighted alignment may still fail to produce reliable camera poses

## Foundational Learning

- **Self-supervised learning from unlabeled video data**: Why needed - Labeled HOI datasets are scarce, making it necessary to leverage large-scale unlabeled videos for training. Quick check - Can the model learn to reconstruct 3D scenes accurately without explicit depth or pose labels, relying only on geometric consistency and pretrained model priors?

- **Camera pose estimation via procrustes alignment**: Why needed - Directly predicting camera poses from images is challenging due to dynamic scenes and occlusions; procrustes alignment provides a differentiable way to estimate poses from pointclouds. Quick check - Does the weighted procrustes formulation with confidence maps improve pose estimation accuracy compared to direct prediction methods, especially in dynamic HOI scenes?

- **3D scene flow and long-term tracking for temporal consistency**: Why needed - Ensuring scale and geometry consistency across frames is crucial for accurate 4D reconstruction; 3D flow and tracking provide necessary temporal correspondences. Quick check - Can the model maintain consistent scale and geometry across frames by aligning pointclouds using 3D flow and tracking priors, even when camera and objects are moving?

## Architecture Onboarding

- **Component map**: Input video frames -> UniDepth encoder (frozen) -> Video adaptors (Unet3D + Transformer) -> Decoders -> Confidence maps -> Weighted procrustes alignment -> Camera poses -> Unprojection -> Pointclouds reconstruction -> 3D consistency loss

- **Critical path**: Video frames → Encoder → Adaptors → Decoders → Confidence maps → Weighted procrustes alignment → Camera poses → Pointclouds reconstruction → 3D consistency loss

- **Design tradeoffs**: Using frozen UniDepth encoder retains pretrained knowledge but limits adaptability to HOI-specific features; predicting confidence maps instead of direct poses adds complexity but improves robustness to dynamic regions; relying on pretrained model priors avoids labeled data but introduces domain gap risks

- **Failure signatures**: Dynamic part distortion (size of dynamic objects varies noticeably across frames), static part misalignment (boundaries of static regions become misaligned over time), low FPS failure (reconstruction quality degrades with sparse views due to optical flow limitations), large dynamic region failure (model struggles when dynamic portion of image is too large or contains too many moving objects)

- **First 3 experiments**: 1) Verify that the model can reconstruct static scenes accurately by testing on datasets with known camera poses and no hand-object interaction; 2) Test the robustness to dynamic regions by introducing synthetic motion in a controlled scene and measuring the accuracy of the confidence maps and pose estimates; 3) Evaluate the impact of the video adaptors by comparing the performance with and without them on a small HOI dataset with labeled data

## Open Questions the Paper Calls Out

None

## Limitations

- Performance degrades on videos with low FPS due to limitations of the optical flow module
- Static part misalignment and dynamic part distortion may occur due to inconsistencies in UniDepth shape regularization
- The approach may struggle when the dynamic portion of the image is too large or contains too many moving objects

## Confidence

- **Reconstruction quality claims (3D Chamfer Distance, F-score)**: Medium - supported by comprehensive evaluation across multiple datasets
- **Zero-shot generalization claims**: Low - validation datasets may share similar characteristics with training data
- **30x speedup claim**: High - well-supported by architecture design

## Next Checks

1. **Domain Gap Assessment**: Test UniHOI on HOI videos from domains significantly different from training data (medical, industrial, or outdoor settings) to validate zero-shot generalization claims and identify failure modes related to domain adaptation

2. **Prior Quality Impact**: Systematically evaluate how accuracy of each foundation model prior (UniDepth, EgoHOS, GMFlow) affects final reconstruction quality by replacing them with alternatives or ground truth where available

3. **Dynamic Scene Robustness**: Create controlled experiments with varying levels of dynamic motion and occlusion to quantify effectiveness of confidence-weighted procrustes alignment and identify threshold beyond which method fails