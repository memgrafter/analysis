---
ver: rpa2
title: 'Extracting PAC Decision Trees from Black Box Binary Classifiers: The Gender
  Bias Case Study on BERT-based Language Models'
arxiv_id: '2412.10513'
source_url: https://arxiv.org/abs/2412.10513
tags:
- decision
- trees
- error
- training
- tree
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the use of the Probably Approximately Correct
  (PAC) framework to provide theoretical guarantees of fidelity for decision trees
  extracted from AI models. The authors adapt a decision tree algorithm to ensure
  PAC guarantees under specific conditions, focusing on binary classification and
  extracting decision trees from BERT-based language models.
---

# Extracting PAC Decision Trees from Black Box Binary Classifiers: The Gender Bias Case Study on BERT-based Language Models

## Quick Facts
- arXiv ID: 2412.10513
- Source URL: https://arxiv.org/abs/2412.10513
- Authors: Ana Ozaki; Roberto Confalonieri; Ricardo Guimarães; Anders Imenes
- Reference count: 40
- One-line primary result: Decision trees extracted with PAC guarantees reveal occupational gender bias in BERT-based language models

## Executive Summary
This paper presents TREPAC, a decision tree extraction algorithm that provides Probably Approximately Correct (PAC) guarantees when approximating black-box binary classifiers. The authors adapt the standard PAC learning framework to account for training error tolerance, establishing theoretical bounds on sample size requirements. The method is applied to BERT-based language models to analyze occupational gender bias in pronoun prediction tasks, with the extracted decision trees providing both quantitative guarantees and enhanced interpretability compared to previous approaches.

## Method Summary
TREPAC extracts decision trees from black-box binary classifiers by using membership queries to classify examples and build the tree structure. The algorithm uses binary entropy as the splitting criterion and incorporates PAC guarantees through Theorem 8, which provides sample size bounds that allow for training error tolerance. The method was applied to BERT-base, BERT-large, RoBERTa-base, and RoBERTa-large models, querying them to predict pronouns in sentences about occupations with various birth periods and locations. The decision trees were extracted with parameters ε=0.2 (error tolerance) and δ=0.1 (confidence level).

## Key Results
- Decision trees with PAC guarantees successfully extracted from BERT-based models, with training errors consistently below the theoretical upper bounds
- Revealed occupational gender bias patterns, with "businessman/businesswoman" being the most discriminating occupation for pronoun prediction
- Decision tree format enhanced visualization of bias patterns compared to previous studies, showing occupations as the most relevant features for pronoun prediction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PAC learning guarantees can be adapted to decision tree extraction from black-box binary classifiers by bounding the training error.
- Mechanism: Theorem 8 provides a sample size bound that allows for a hypothesis to misclassify up to k examples in the training set while still maintaining PAC guarantees. This is achieved by adapting the standard PAC framework to account for training error tolerance.
- Core assumption: The hypothesis space is finite and the target function belongs to the hypothesis space (realizability assumption).
- Evidence anchors:
  - [abstract] "We adapt a decision tree algorithm to ensure a PAC guarantee under certain conditions."
  - [section] "Theorem 8(Sample Size with Training Error). Let H be a finite hypothesis space from a concept class. Let δ, ϵ ∈ (0,1/2) and m, k ∈ N, m ≥ k/ϵ. If m ≥ 1/ϵ [ln(|H|(k+1)ϵ^(1-k)/δ) + k] + k, then, for any distribution D, and for any t ∈ H, with probability of at least 1−δ over the choice of an i.i.d. sample S of size m, we have that error(T, T⋆,D) is smaller than ϵ for every h ∈ H with error S (h) ≤ k/m."
  - [corpus] Weak evidence; the related papers focus on decision tree extraction but do not explicitly mention PAC guarantees or training error bounds.
- Break condition: If the hypothesis space is infinite or the realizability assumption is violated, the PAC guarantee may not hold.

### Mechanism 2
- Claim: TREPAC algorithm uses membership queries to classify examples and builds a decision tree with PAC guarantees.
- Mechanism: TREPAC iteratively expands the decision tree by selecting the best split based on binary entropy, using a membership oracle to classify examples. The algorithm stops when the training error is below a predefined bound or the tree size limit is reached.
- Core assumption: The membership oracle can accurately classify examples according to the black-box model.
- Evidence anchors:
  - [abstract] "We adapt a decision tree algorithm to ensure a PAC guarantee under certain conditions."
  - [section] "TREPAC is a tree induction algorithm that extracts decision trees from binary classifiers, seen as oracles."
  - [corpus] Weak evidence; the related papers focus on decision tree extraction but do not explicitly mention membership queries or binary entropy as the splitting criterion.
- Break condition: If the membership oracle is inaccurate or the binary entropy criterion is not suitable for the data, the decision tree may not accurately represent the black-box model.

### Mechanism 3
- Claim: Decision trees extracted using TREPAC provide additional insights into the behavior of BERT-based language models, particularly regarding occupational gender bias.
- Mechanism: The decision tree format allows for visualization of which features (occupations, birth periods, locations) are most relevant for pronoun prediction, revealing patterns of bias in the language models.
- Core assumption: The extracted decision tree accurately approximates the behavior of the black-box model.
- Evidence anchors:
  - [abstract] "Our results indicate occupational gender bias in these models, which confirm previous results in the literature. Additionally, the decision tree format enhances the visualization of which occupations are most impacted by social bias."
  - [section] "Regarding the format, the decision trees facilitate the visualization of which features are most relevant for pronoun prediction, with the occupations being the most relevant ones."
  - [corpus] Weak evidence; the related papers focus on decision tree extraction but do not explicitly mention the application to BERT-based language models or the analysis of occupational gender bias.
- Break condition: If the decision tree does not accurately approximate the black-box model, the insights gained may be misleading or incorrect.

## Foundational Learning

- Concept: PAC learning framework
  - Why needed here: Provides a theoretical foundation for guaranteeing the fidelity of decision trees extracted from black-box models.
  - Quick check question: What is the relationship between sample size, error tolerance, and confidence level in the PAC framework?

- Concept: Decision tree algorithms (e.g., CART, C4.5)
  - Why needed here: Used as a basis for TREPAC, which adapts these algorithms to incorporate PAC guarantees.
  - Quick check question: How do decision tree algorithms select the best split at each node?

- Concept: Membership queries
  - Why needed here: Used by TREPAC to classify examples and build the decision tree.
  - Quick check question: What is the difference between a membership query and a classification query?

## Architecture Onboarding

- Component map: Black-box model -> Membership oracle -> TREPAC algorithm -> Decision tree
- Critical path: 1. Create training set using membership queries 2. Run TREPAC algorithm to extract decision tree 3. Analyze decision tree for insights into black-box model behavior
- Design tradeoffs:
  - Sample size vs. error tolerance: Larger sample sizes allow for higher error tolerance while maintaining PAC guarantees.
  - Tree size vs. accuracy: Larger trees may provide more accurate approximations but are more complex to interpret.
- Failure signatures:
  - High training error: Indicates that the decision tree does not accurately approximate the black-box model.
  - Low fidelity: Indicates that the decision tree does not provide meaningful insights into the black-box model behavior.
- First 3 experiments:
  1. Run TREPAC with different values of k (error tolerance) and n (tree size) to observe the effect on training error and decision tree complexity.
  2. Compare the decision trees extracted from different BERT-based models (e.g., BERT-base, BERT-large, RoBERTa-base, RoBERTa-large) to identify patterns of bias.
  3. Analyze the decision trees to extract rules and quantify the frequency of biased predictions for different occupations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the TREPAC algorithm's performance change when applied to multi-class classification tasks beyond binary pronoun prediction?
- Basis in paper: [inferred] The paper mentions that TREPAC was tested on BERT-based models for binary classification (pronoun prediction) and suggests extending experiments to other case studies and multi-classification tasks.
- Why unresolved: The current implementation and theoretical guarantees are specifically designed for binary classification, and extending them to multi-class scenarios would require significant modifications to the algorithm and theoretical framework.
- What evidence would resolve it: Experimental results showing TREPAC's effectiveness on multi-class classification tasks with PAC guarantees, including comparisons with existing multi-class decision tree algorithms.

### Open Question 2
- Question: What is the optimal sampling strategy that satisfies the constraints of the leaves within the PAC framework?
- Basis in paper: [explicit] The authors mention that studying a sampling strategy that satisfies the constraints of the leaves within the PAC framework seems promising but would be complex to analyze.
- Why unresolved: The current TREPAC algorithm uses random sampling, and a more sophisticated sampling strategy that targets specific leaves could potentially improve efficiency and accuracy.
- What evidence would resolve it: A modified TREPAC algorithm incorporating a targeted sampling strategy with theoretical analysis showing improved sample complexity or decision tree accuracy compared to random sampling.

### Open Question 3
- Question: How does the choice of splitting criterion affect the extracted decision tree's fidelity and interpretability?
- Basis in paper: [explicit] The paper mentions that TREPAC uses binary entropy as the splitting criterion, but does not explore other criteria.
- Why unresolved: Different splitting criteria may lead to different decision trees with varying levels of fidelity and interpretability, and the optimal criterion may depend on the specific task and data distribution.
- What evidence would resolve it: Experimental results comparing TREPAC with different splitting criteria (e.g., Gini impurity, misclassification error) on various tasks, showing the impact on fidelity, interpretability, and PAC guarantees.

## Limitations

- The work relies heavily on the realizability assumption which may not hold in practice when extracting trees from complex black-box models like BERT
- The finite hypothesis space requirement is critical but may be violated when dealing with high-dimensional feature spaces or complex decision boundaries
- The membership oracle's accuracy is assumed perfect but real-world black-box models may produce inconsistent predictions under similar inputs

## Confidence

- High Confidence: The PAC framework adaptation and theoretical guarantees (Theorem 8) - these follow established learning theory principles and are mathematically rigorous
- Medium Confidence: The decision tree extraction methodology (TREPAC algorithm) - while theoretically sound, practical implementation details and potential edge cases are not fully explored
- Medium Confidence: The validation of occupational gender bias findings - the results align with previous work but the decision tree format's added value for visualization and insight generation could be more thoroughly quantified

## Next Checks

1. Test TREPAC on synthetic datasets where the ground truth decision boundary is known to verify that extracted trees maintain PAC guarantees under controlled conditions and that Theorem 8's sample size bounds are tight in practice

2. Evaluate TREPAC's sensitivity to the realizability assumption by intentionally using black-box models with decision boundaries outside the hypothesis space and measuring degradation in PAC guarantees

3. Conduct ablation studies comparing TREPAC with other decision tree extraction methods (LIME, SHAP, DTExtract) on the same BERT-based models to quantify the specific advantages of PAC guarantees in terms of fidelity and interpretability