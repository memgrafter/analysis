---
ver: rpa2
title: Human-Agent Cooperation in Games under Incomplete Information through Natural
  Language Communication
arxiv_id: '2405.14173'
source_url: https://arxiv.org/abs/2405.14173
tags:
- player
- game
- information
- human
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a shared-control game framework where two
  players collectively control a single token to achieve a common objective under
  incomplete information. The authors address the challenge of human-agent cooperation
  in such games by proposing a communication-based approach that combines a language
  module and a planning module.
---

# Human-Agent Cooperation in Games under Incomplete Information through Natural Language Communication

## Quick Facts
- arXiv ID: 2405.14173
- Source URL: https://arxiv.org/abs/2405.14173
- Reference count: 13
- One-line primary result: Communication-enabled agents significantly improve cooperation efficiency in shared-control games under incomplete information, reaching treasures in fewer turns and less time compared to mute agents.

## Executive Summary
This paper introduces a shared-control game framework where two players collectively control a single token to achieve a common objective under incomplete information. The authors address the challenge of human-agent cooperation by proposing a communication-based approach that combines a language module and a planning module. The language module translates natural language messages into intent flags, while the asymmetric information-set Monte Carlo tree search with flag exchange (AISMCTS-F) algorithm uses these flags to compute optimal actions. Human subject experiments in a Gnomes at Night testbed show that communication-enabled agents significantly improve cooperation efficiency, reaching treasures in fewer turns and less time compared to mute agents, though human-human cooperation remains more efficient.

## Method Summary
The authors propose a two-module framework for human-agent cooperation in shared-control games under incomplete information. The language module uses a large language model with few-shot prompting to translate natural language messages into compact intent flags (e.g., "right", "down", "Inquiry"). The planning module implements AISMCTS-F, which maintains a hidden information dictionary to record actions rejected by the human player and uses this information to improve decision-making. The algorithm incorporates communicated flags to prioritize actions matching human intent and avoids actions in the hidden dictionary when simulating the human's moves.

## Key Results
- Communication-enabled agents reached treasures in significantly fewer turns and less time compared to mute agents in human subject experiments
- The language module achieved 74.32% accuracy in intent flag classification, outperforming direct action prediction (53.31%)
- Human-human cooperation remained more efficient than human-agent cooperation, but communication narrowed the efficiency gap

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Communication via natural language reduces information asymmetry between players in the shared-control game.
- **Mechanism:** The language module translates natural language messages into compact intent flags, which the planning module uses to inform Monte Carlo tree search, allowing the ego player to infer the human's preferences and hidden transition constraints.
- **Core assumption:** Intent flags are sufficiently expressive to represent relevant human intent without overwhelming the planning module.
- **Evidence anchors:** Abstract states communication narrows information gaps; section describes flag translation process.
- **Break condition:** If the language model fails to accurately parse intent from natural language, the flags will misrepresent the human's actual intent.

### Mechanism 2
- **Claim:** AISMCTS-F improves decision-making by incorporating inferred hidden information about the human player's transition function.
- **Mechanism:** The ego player maintains a hidden information dictionary that records actions the human player has rejected in each state, avoiding these actions during tree expansion.
- **Core assumption:** Rejection actions are reliable indicators of infeasibility in the human's transition function.
- **Evidence anchors:** Section describes hidden information dictionary and its use in reducing branching factor.
- **Break condition:** If the human player makes mistakes or changes strategies mid-game, the dictionary will contain false positives.

### Mechanism 3
- **Claim:** Using intent flags as an intermediary between natural language and planning is more accurate than directly predicting next actions.
- **Mechanism:** The language module first classifies messages into intent flags, which are then mapped to actions by the planning module, separating semantic understanding from strategic planning.
- **Core assumption:** The mapping from intent flags to actions is deterministic and context-independent within the game state.
- **Evidence anchors:** Section reports 74.32% accuracy for flag prediction vs. 53.31% for direct action prediction.
- **Break condition:** If the intent-action mapping becomes ambiguous in complex states, the flag-based policy may misalign with human intent.

## Foundational Learning

- **Concept:** Monte Carlo Tree Search (MCTS) with Upper Confidence Bound (UCB) selection
  - **Why needed here:** MCTS explores large decision spaces efficiently by balancing exploration and exploitation, crucial for turn-based games with incomplete information.
  - **Quick check question:** What is the role of the exploration parameter c in the UCB formula?

- **Concept:** Information-set MCTS (ISMCTS) for incomplete-information games
  - **Why needed here:** Standard MCTS assumes perfect information; ISMCTS maintains separate trees for each player's information set, handling hidden transitions.
  - **Quick check question:** How does ISMCTS handle actions that are invalid from the opponent's perspective?

- **Concept:** Large language model (LLM) few-shot prompting for intent classification
  - **Why needed here:** LLMs can generalize intent from natural language messages without hand-crafted parsers, enabling flexible communication.
  - **Quick check question:** What is the difference between zero-shot, one-shot, and few-shot prompting?

## Architecture Onboarding

- **Component map:** Language Module (LLM + Prompting) → Flag Space → Planning Module (AISMCTS-F) → Action Output; Shared State + Hidden Information Dictionary (Ω) ↔ Planning Module
- **Critical path:** Message → Flag (Language Module) → Policy Update (AISMCTS-F) → Action Selection
- **Design tradeoffs:** Flag granularity vs. parsing accuracy; simulation depth vs. runtime; rejection feedback vs. privacy
- **Failure signatures:** Persistent Reject flags with no pattern; high variance in turn counts; agent repeatedly suggests invalid actions
- **First 3 experiments:**
  1. **Flag accuracy test:** Run LLM on held-out message-action pairs, measure classification accuracy vs. direct prediction.
  2. **Ω sensitivity test:** Simulate games where human makes intentional mistakes; measure impact on agent's policy quality.
  3. **Communication volume test:** Compare turns and time with/without Inquiry flags to assess value of active questioning.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do communication patterns differ between human-human and human-agent interactions in shared-control games?
- **Basis in paper:** The paper observes distinct communication patterns, noting agents send around twice as many messages of longer lengths than human partners.
- **Why unresolved:** The paper only qualitatively observes these differences without quantifying or characterizing the specific patterns.
- **What evidence would resolve it:** Detailed quantitative analysis of message frequency, content, timing, and effectiveness across different interaction types with statistical comparisons.

### Open Question 2
- **Question:** What communication mechanisms would enable cooperation in shared-control games requiring strategy-making over longer time horizons?
- **Basis in paper:** Round 5 revealed that unique treasure positions necessitate more complex strategy coordination than one-step intent communication allows.
- **Why unresolved:** The current approach only supports one-step intent communication, and the paper acknowledges this limitation without proposing specific extensions.
- **What evidence would resolve it:** Design and evaluation of communication protocols that can encode and exchange multi-step strategic plans.

### Open Question 3
- **Question:** How can language processing modules be made more robust to human errors and parsing inaccuracies in intent flag extraction?
- **Basis in paper:** The paper identifies the need for methods more robust to human errors and inaccuracies from language parsing.
- **Why unresolved:** The current approach relies on a simple few-shot prompting approach that shows good but imperfect performance.
- **What evidence would resolve it:** Comparison of multiple language processing approaches showing improved accuracy and robustness in intent flag extraction.

## Limitations
- Limited real-world game diversity with experiments confined to a single 4x4 grid testbed
- No long-term memory modeling beyond tracking rejection actions in a single game session
- LLM dependency risks with limited detail on prompt engineering or handling ambiguous messages

## Confidence
- **High confidence:** Core mechanism of using intent flags to reduce information asymmetry is well-supported by experimental results
- **Medium confidence:** Superiority of flag-based prediction over direct action prediction relies on limited evaluation data
- **Low confidence:** Generalization to games with continuous or multi-dimensional state spaces remains untested

## Next Checks
1. **Cross-game transfer test:** Evaluate the communication framework on at least two additional games with different incomplete-information structures to assess generalizability.
2. **Dynamic strategy adaptation test:** Implement a decay mechanism for rejection actions in Ω and measure agent performance when human players intentionally change strategies mid-game.
3. **Human perception study:** Conduct post-game surveys to measure human trust, satisfaction, and perceived agency when playing with the agent versus human partners.