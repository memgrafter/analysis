---
ver: rpa2
title: Chain-of-Jailbreak Attack for Image Generation Models via Editing Step by Step
arxiv_id: '2410.03869'
source_url: https://arxiv.org/abs/2410.03869
tags:
- image
- generation
- attack
- safety
- generate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Chain-of-Jailbreak (CoJ), a novel jailbreaking
  method for text-based image generation models that bypasses safety safeguards by
  decomposing malicious queries into harmless-looking sub-queries and iteratively
  editing images through multiple steps. Experiments on four widely-used image generation
  services (GPT-4V, GPT-4o, Gemini 1.5, and Gemini 1.5 Pro) demonstrate that CoJ achieves
  a jailbreak success rate of over 60%, significantly outperforming other methods
  (14%).
---

# Chain-of-Jailbreak Attack for Image Generation Models via Editing Step by Step

## Quick Facts
- arXiv ID: 2410.03869
- Source URL: https://arxiv.org/abs/2410.03869
- Authors: Wenxuan Wang; Kuiyi Gao; Youliang Yuan; Jen-tse Huang; Qiuzhi Liu; Shuai Wang; Wenxiang Jiao; Zhaopeng Tu
- Reference count: 40
- One-line primary result: Chain-of-Jailbreak attack achieves over 60% jailbreak success rate on four widely-used image generation services, outperforming other methods by 46 percentage points.

## Executive Summary
This paper introduces Chain-of-Jailbreak (CoJ), a novel jailbreaking method for text-based image generation models that bypasses safety safeguards by decomposing malicious queries into harmless-looking sub-queries and iteratively editing images through multiple steps. The method demonstrates significant effectiveness, achieving jailbreak success rates over 60% on four major image generation services (GPT-4V, GPT-4o, Gemini 1.5, and Gemini 1.5 Pro), compared to just 14% for other methods. The study also proposes Think-Twice Prompting, a defense strategy that improves model safety by prompting models to internally simulate and examine content before generation, successfully defending against over 95% of CoJ attacks.

## Method Summary
Chain-of-Jailbreak works by decomposing a harmful query into multiple sub-queries using three edit operations (delete-then-insert, insert-then-delete, change-then-change-back) applied at word, character, or image levels. The attack intentionally creates seemingly harmless intermediate steps that ultimately lead to generating harmful content. The method uses Mistral-Large-2 to automatically decompose queries with human validation. For defense, Think-Twice Prompting prompts the model to describe and evaluate the safety of content before generation, forcing consideration of actual content rather than literal prompt text.

## Key Results
- CoJ attack achieves jailbreak success rate of over 60% on four widely-used image generation services
- CoJ outperforms other methods by 46 percentage points (60% vs 14% success rate)
- Think-Twice Prompting defense successfully blocks over 95% of CoJ attacks
- Insert-then-delete operation is the most effective jailbreak strategy
- CoJ-Bench dataset introduced for reproducible evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-Jailbreak works by decomposing a harmful query into a sequence of seemingly harmless sub-queries, which can bypass safety filters.
- Mechanism: The method applies edit operations (insert, delete, change) at word, character, or image level to transform the original malicious query into multiple safe-looking steps that ultimately lead to generating harmful content.
- Core assumption: Safety filters in image generation models focus primarily on the immediate content of each individual query rather than analyzing the full conversation context or potential sequence of edits.
- Evidence anchors:
  - [abstract] "we intentionally decompose the query into multiple sub-queries"
  - [section 2.2] "CoJ attack intentionally decomposes the original query into a sequence of sub-queries"
- Break condition: Safety filters implement conversation context analysis or can detect harmful content across multiple sequential prompts.

### Mechanism 2
- Claim: Insert-then-delete operation is the most effective jailbreak strategy.
- Mechanism: By first adding benign content and then removing it, the intermediate steps appear safe to the model's filters while still achieving the harmful final output.
- Core assumption: Safety filters are less likely to flag queries that add harmless elements compared to those that directly modify sensitive keywords.
- Evidence anchors:
  - [section 4.3] "insert-then-delete can bypass the safeguard of models with the highest success rate"
- Break condition: Safety filters learn to track semantic meaning across edit sequences rather than just analyzing individual prompts.

### Mechanism 3
- Claim: The defense method "Think-Twice Prompting" works by making the model explicitly describe and evaluate the safety of content before generation.
- Mechanism: The model is prompted to first describe what it will generate and determine if it's safe, which forces consideration of the actual content rather than just the literal prompt text.
- Core assumption: Models can be prompted to perform internal safety checks when explicitly asked to evaluate content before generation.
- Evidence anchors:
  - [abstract] "prompting models to internally simulate and examine content before generation"
  - [section 4.4] "we prompt the image generation to imagine and describe the image it is going to generate"
- Break condition: Models develop stronger internal safety alignment that operates independently of external prompting.

## Foundational Learning

- Concept: Edit Distance (Levenshtein Distance)
  - Why needed here: The attack methods are based on edit operations (insert, delete, change) that mirror the concepts in edit distance theory.
  - Quick check question: If the original prompt is "generate GPT will destroy the world" and the first edit inserts "not" after "will", what is the edit distance between the original and intermediate prompt?

- Concept: Diffusion Models
  - Why needed here: Understanding how image generation models work is crucial for understanding both attack and defense mechanisms.
  - Quick check question: What is the fundamental difference between how diffusion models generate images versus traditional GANs?

- Concept: Multimodal Safety Alignment
  - Why needed here: The paper deals with safety alignment specifically for multimodal models that handle both text and images.
  - Quick check question: How does safety alignment for multimodal models differ from safety alignment for text-only models?

## Architecture Onboarding

- Component map: Input Parser -> Edit Operation Engine -> Safety Filter Interface -> Image Generation -> Output Evaluator -> Defense Layer
- Critical path: Query → Decomposition → Sub-query Generation → Safety Filter → Image Generation → Output Evaluation
- Design tradeoffs:
  - Query efficiency vs. success rate: More decomposition steps increase success rate but also increase query count
  - Defense overhead vs. safety: "Think-Twice Prompting" adds generation time but significantly improves safety
  - Granularity of edit operations: Word-level edits are more effective but character-level may be needed for certain cases
- Failure signatures:
  - Defense failure: Generated content bypasses "Think-Twice Prompting" by describing harmless content while generating harmful images
  - Attack failure: Decomposition results in semantic drift from the original malicious intent
  - Model failure: Target model rejects all decomposed queries even when they appear harmless
- First 3 experiments:
  1. Test decomposition effectiveness: Take a known malicious prompt and verify it can be decomposed into harmless-looking sub-queries that still generate harmful content
  2. Evaluate defense effectiveness: Apply "Think-Twice Prompting" to decomposed queries and measure reduction in successful jailbreaks
  3. Measure edit operation impact: Compare success rates of insert-then-delete, delete-then-insert, and change-then-change-back strategies on the same set of malicious prompts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of decomposition steps for balancing jailbreak success rate and query efficiency?
- Basis in paper: [explicit] The paper shows that increasing editing steps from 2 to 5 improves jailbreak success rate, but also increases query costs and energy/time consumption.
- Why unresolved: The paper acknowledges this as a research question but doesn't provide a definitive answer on the optimal trade-off point between effectiveness and efficiency.
- What evidence would resolve it: Comparative analysis across different step counts showing the point where marginal gains in success rate diminish relative to increased costs.

### Open Question 2
- Question: Can defense methods like Think-Twice Prompting be implemented without requiring image description generation?
- Basis in paper: [explicit] The paper notes that the current defense method is "not efficient enough" because it requires generating image descriptions before generation.
- Why unresolved: The paper identifies this as a limitation but doesn't propose or evaluate alternative, more efficient defense mechanisms.
- What evidence would resolve it: Development and evaluation of alternative defense strategies that achieve similar protection levels without the computational overhead of image description generation.

### Open Question 3
- Question: How effective would CoJ attack be against image generation models with improved safety mechanisms?
- Basis in paper: [explicit] The paper tested CoJ against four models but excluded Midjourney and Stable Diffusion due to their weak safeguards, suggesting room for testing against more robust models.
- Why unresolved: The paper only evaluated current widely-used models and acknowledges that future models with better safety alignment would be worth testing.
- What evidence would resolve it: Application of CoJ attack to newer models with enhanced safety mechanisms to measure effectiveness against improved safeguards.

## Limitations
- Reliance on black-box testing of commercial image generation services prevents full verification of internal mechanisms
- Evaluation depends on a relatively small sample size (150 seed queries across 9 scenarios)
- Manual validation process introduces potential subjectivity in determining harmful content
- Defense mechanism's effectiveness against adaptive attacks beyond simple decomposition remains untested

## Confidence
**High Confidence Claims:**
- Chain-of-Jailbreak can successfully bypass safety filters on current image generation models (empirical results with JSR > 60%)
- The Think-Twice Prompting defense significantly reduces jailbreak success rates (empirical results with >95% effectiveness)

**Medium Confidence Claims:**
- Insert-then-delete is the most effective attack strategy (based on comparative results, but with limited exploration of alternative strategies)
- The safety filter vulnerability stems primarily from context-blind analysis (inferred from attack success, but not directly validated through ablation studies)

**Low Confidence Claims:**
- The generalizability of CoJ-Bench to all possible jailbreak scenarios (evaluated on a specific dataset of 150 queries)
- Long-term effectiveness of Think-Twice Prompting against evolving attack strategies (tested only against CoJ attack)

## Next Checks
1. Design an experiment where the safety filter explicitly tracks conversation history across multiple prompts to test whether CoJ effectiveness decreases when context is considered.
2. Test whether Think-Twice Prompting can defend against attacks that use more sophisticated deception techniques beyond simple decomposition, such as semantic camouflage or multi-turn dialogue strategies.
3. Evaluate the CoJ attack and Think-Twice defense on additional open-source image generation models (e.g., Stable Diffusion, DALL-E 2) to assess whether the findings generalize beyond the four commercial services tested.