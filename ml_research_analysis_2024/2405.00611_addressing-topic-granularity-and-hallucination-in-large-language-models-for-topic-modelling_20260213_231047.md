---
ver: rpa2
title: Addressing Topic Granularity and Hallucination in Large Language Models for
  Topic Modelling
arxiv_id: '2405.00611'
source_url: https://arxiv.org/abs/2405.00611
tags:
- topics
- topic
- seed
- llms
- modelling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses two critical challenges in LLM-based topic
  modelling: topic granularity control and hallucination mitigation. The authors introduce
  TopicMistral, a fine-tuned Mistral-7B model trained via Direct Preference Optimisation
  (DPO) using a reconstruction pipeline that doesn''t require human annotation.'
---

# Addressing Topic Granularity and Hallucination in Large Language Models for Topic Modelling

## Quick Facts
- arXiv ID: 2405.00611
- Source URL: https://arxiv.org/abs/2405.00611
- Reference count: 13
- Primary result: TopicMistral fine-tuned model reduces hallucinated topics from 13-49% to 0-5% and generates 85-116 unique topics versus 1,440-2,625 for baseline models

## Executive Summary
This paper addresses two critical challenges in LLM-based topic modelling: topic granularity control and hallucination mitigation. The authors introduce TopicMistral, a fine-tuned Mistral-7B model trained via Direct Preference Optimisation (DPO) using a reconstruction pipeline that doesn't require human annotation. Their approach significantly improves topic coherence and reduces hallucinated topics compared to off-the-shelf LLMs. On benchmark datasets, TopicMistral generates 85-116 unique topics versus 1,440-2,625 for baseline models, with higher Mutual Information scores (0.531-0.577 vs 0.315-0.360) indicating better alignment with human expectations.

## Method Summary
TopicMistral uses a reconstruction pipeline to modify raw LLM outputs without human annotation. The pipeline identifies near-duplicate topics using cosine similarity and replaces them with canonical forms, while replacing hallucinated topics with "No related topics" responses. The model is fine-tuned using Direct Preference Optimisation (DPO) with preference pairs created from accepted and rejected topics. Dynamic seed topics improve granularity by adapting the seed topic list based on generated topic statistics. The approach is evaluated on 20 News Groups, Wiki, and Bills datasets using metrics including unique topics count, similarity scores, mutual information, and hallucination rates.

## Key Results
- TopicMistral generates 85-116 unique topics versus 1,440-2,625 for baseline models
- Mutual Information scores improve from 0.315-0.360 to 0.531-0.577
- Hallucination rates reduce from 13-49% to 0-5% across adversarial prompt settings
- The reconstruction pipeline successfully merges near-duplicate topics while preserving distinct ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TopicMistral's DPO fine-tuning pipeline reduces hallucinated topics by modifying raw LLM outputs through a reconstruction process.
- Mechanism: The reconstruction pipeline identifies near-duplicate topics using cosine similarity and replaces them with canonical forms, while replacing hallucinated topics with "No related topics" responses.
- Core assumption: Raw LLM outputs contain both near-duplicate topics and hallucinated topics that can be systematically identified and corrected.
- Evidence anchors: [abstract] "Our approach does not rely on traditional human annotation to rank preferred answers but employs a reconstruction pipeline to modify raw topics generated by LLMs" [section 3.3] "For each of the topics TSub,i from TSub, we then use a sentence-transformer to identify the topic clusters, which contain near-duplicate topics from T, by computing cosine similarity"
- Break condition: If cosine similarity thresholds are poorly calibrated, the reconstruction pipeline may incorrectly merge distinct topics or fail to identify near-duplicates.

### Mechanism 2
- Claim: Dynamic seed topics improve topic granularity by adapting the seed topic list based on generated topic statistics.
- Mechanism: After generating topics for N documents, the top 10 most frequent topics replace the initial seed topics, guiding future generations toward more domain-appropriate granularity.
- Core assumption: Topic statistics from early document generations reflect the domain's appropriate granularity level.
- Evidence anchors: [section 3.4] "The pipeline first initialises with a set of initial seed topics... After generating topics for N documents using an TopicMistral, the seed topics are replaced with the top 10 most frequent topics" [section 5] "Dynamic Seed Topics: For all datasets, the use of dynamic seed topics can further reduce the number of unique topics with less similar N values"
- Break condition: If N is set too low, early topic statistics may not represent the domain well; if too high, the system adapts too slowly.

### Mechanism 3
- Claim: Attention weight shift analysis reveals that TopicMistral reduces hallucination by decreasing attention aggregation from topic instructions to prediction tokens in deep layers.
- Mechanism: TopicMistral learns to place less emphasis on hallucinated topic information during the attention propagation process, reducing the likelihood of generating hallucinated topics.
- Core assumption: Attention weight patterns in deep layers correlate with hallucination tendencies and can be modified through fine-tuning.
- Evidence anchors: [section 6.2] "To further investigate the reasons behind TopicMistral generating fewer hallucinations, we conduct an analysis of the attention weight shift from the topic instruction to the next prediction token within the hidden layers" [figure 4] Shows decreasing attention weights in TopicMistral's deep layers compared to base model
- Break condition: If the relationship between attention weights and hallucinations is not causal but merely correlational, this mechanism may not generalize.

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: Provides a simpler alternative to RLHF for fine-tuning LLMs based on preference data without requiring explicit reward modeling
  - Quick check question: What is the key difference between DPO's implicit reward and RLHF's explicit reward model?

- Concept: Cosine similarity for semantic clustering
  - Why needed here: Enables automated identification of near-duplicate topics without human annotation
  - Quick check question: What threshold value did the authors use to determine topic similarity, and how was this determined?

- Concept: Parameter-efficient fine-tuning (PEFT)
  - Why needed here: Allows fine-tuning of large models (7B parameters) within limited computational resources (12GB GPU memory)
  - Quick check question: What PEFT method did the authors use, and what was the maximum GPU memory requirement for full fine-tuning?

## Architecture Onboarding

- Component map: Document corpus and initial seed topics → Reconstruction pipeline (topic clustering and replacement) → DPO training (fine-tuning with preference pairs) → Dynamic adaptation (seed topic updating) → Topic generation
- Critical path: Document → Reconstruction pipeline → DPO training → Dynamic adaptation → Topic generation
- Design tradeoffs:
  - Using 30 candidate topics balances computational efficiency against comprehensive topic coverage
  - Setting cosine similarity threshold at 0.55 provides reasonable balance between merging near-duplicates and preserving distinct topics
  - Limiting dynamic seed topic updates to every 20 documents balances adaptation speed against stability
- Failure signatures:
  - If # of Unique Topics increases significantly after fine-tuning, the reconstruction pipeline may be incorrectly merging distinct topics
  - If Mutual Information decreases while other metrics improve, the model may be generating more coherent topics but less aligned with human expectations
  - If Hallucination% increases when using granularity description prompts, the fine-tuning may not generalize well to complex prompts
- First 3 experiments:
  1. Run baseline evaluation on 20NG dataset with Granularity Description prompt only - expect 2,414-2,625 unique topics
  2. Apply reconstruction pipeline to same dataset and measure reduction in unique topics and similarity scores
  3. Fine-tune TopicMistral using DPO with dynamic seed topics and re-evaluate on held-out test sets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TopicMistral's performance compare to fine-tuned LLMs specifically designed for topic modeling tasks?
- Basis in paper: [inferred] The paper compares TopicMistral against off-the-shelf LLMs and traditional topic modeling approaches, but does not compare it to LLMs that have been fine-tuned for topic modeling.
- Why unresolved: The paper does not provide any information about LLMs that have been specifically fine-tuned for topic modeling.
- What evidence would resolve it: Comparative experiments between TopicMistral and LLMs that have been fine-tuned for topic modeling would provide insights into the relative effectiveness of the two approaches.

### Open Question 2
- Question: What is the impact of using different sentence transformers on the quality of the reconstructed topics?
- Basis in paper: [explicit] The paper mentions using a specific sentence transformer model ("all-MiniLM-L6-v2") for topic clustering and cosine similarity calculations, but does not explore the impact of using different models.
- Why unresolved: The paper does not provide any information about the sensitivity of the results to the choice of sentence transformer model.
- What evidence would resolve it: Experiments using different sentence transformer models and comparing the results would provide insights into the impact of model choice on topic quality.

### Open Question 3
- Question: How does the dynamic seed topic approach perform on datasets with very different domain characteristics?
- Basis in paper: [explicit] The paper mentions using dynamic seed topics to improve topic granularity, but only evaluates it on a limited set of datasets.
- Why unresolved: The paper does not provide any information about the generalizability of the dynamic seed topic approach to diverse datasets.
- What evidence would resolve it: Experiments evaluating the dynamic seed topic approach on a wide range of datasets with varying domain characteristics would provide insights into its generalizability.

## Limitations
- The reconstruction pipeline's effectiveness depends heavily on hyperparameter choices (cosine similarity threshold of 0.55) without sensitivity analysis
- Evaluation relies primarily on proxy metrics rather than direct human evaluation of topic quality
- Dynamic seed topic mechanism has unclear convergence properties and may not generalize across diverse domains

## Confidence
- TopicMistral significantly reduces hallucinated topics: Medium confidence - supported by strong experimental results but limited to specific adversarial prompt settings
- Reconstruction pipeline effectively identifies and merges near-duplicate topics: Low confidence - mechanism described but threshold sensitivity not explored
- Dynamic seed topics improve granularity control: Medium confidence - shows improvement on tested datasets but generalizability uncertain
- DPO fine-tuning without human annotation is effective: Medium confidence - successful on used datasets but may not generalize to domains with different topic distributions

## Next Checks
1. **Hyperparameter sensitivity analysis**: Systematically vary the cosine similarity threshold (0.4-0.8) and cluster size threshold (2-5) to determine robustness of the reconstruction pipeline across different document types and topic distributions.

2. **Cross-domain generalization test**: Evaluate TopicMistral on datasets from different domains (e.g., medical literature, legal documents, social media) to assess whether the dynamic seed topic mechanism and reconstruction pipeline generalize beyond the tested domains.

3. **Human evaluation of topic quality**: Conduct blinded human assessments comparing TopicMistral-generated topics against baseline models and human-generated topics, focusing on coherence, interpretability, and domain relevance rather than proxy metrics.