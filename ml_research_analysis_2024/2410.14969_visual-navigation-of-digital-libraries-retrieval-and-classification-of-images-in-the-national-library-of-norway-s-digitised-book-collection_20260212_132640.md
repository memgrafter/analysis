---
ver: rpa2
title: 'Visual Navigation of Digital Libraries: Retrieval and Classification of Images
  in the National Library of Norway''s Digitised Book Collection'
arxiv_id: '2410.14969'
source_url: https://arxiv.org/abs/2410.14969
tags:
- image
- retrieval
- images
- search
- clip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a proof-of-concept image search application
  for exploring visual elements in the National Library of Norway's digitised pre-1900
  book collection. The system extracts images from ALTO-XML files using IIIF API endpoints
  and employs modern computer vision models (ViT, CLIP, and SigLIP) to compute embeddings
  for image retrieval and classification tasks.
---

# Visual Navigation of Digital Libraries: Retrieval and Classification of Images in the National Library of Norway's Digitised Book Collection

## Quick Facts
- **arXiv ID**: 2410.14969
- **Source URL**: https://arxiv.org/abs/2410.14969
- **Reference count**: 36
- **Primary result**: SigLIP embeddings outperform CLIP and ViT for image retrieval and classification in digitized historical book collections

## Executive Summary
This work presents a proof-of-concept image search application for exploring visual elements in the National Library of Norway's digitized pre-1900 book collection. The system extracts images from ALTO-XML files using IIIF API endpoints and employs modern computer vision models (ViT, CLIP, and SigLIP) to compute embeddings for image retrieval and classification tasks. Results show that SigLIP embeddings slightly outperform CLIP and ViT for both retrieval and classification, with SigLIP achieving 94% top-10 accuracy for exact image retrieval and 96% F1-score for image classification. The classification model can identify irrelevant graphical elements (blank pages and segmentation anomalies), potentially saving up to 40% storage space.

## Method Summary
The system extracts graphical elements from ALTO-XML files using IIIF API endpoints, then computes embeddings using pre-trained ViT, CLIP, and SigLIP models from HuggingFace Transformers. These embeddings are indexed using a cosine similarity-based HNSW index in a Qdrant vector database for efficient retrieval. For classification, logistic regression models are fine-tuned on embeddings to identify segmentation anomalies and blank pages. The application supports both context-based text search and image-based similarity search through a Flask/HTMX frontend backed by a FastAPI backend.

## Key Results
- SigLIP embeddings achieved 94% top-10 accuracy for exact image retrieval, slightly outperforming CLIP and ViT
- Image classification model obtained 96% F1-score for identifying segmentation anomalies and blank pages
- The system can potentially save up to 40% storage space by filtering out irrelevant graphical elements
- HNSW vector search enabled efficient dense vector search for high-dimensional embeddings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SigLIP embeddings outperform CLIP and ViT for both retrieval and classification in this domain.
- Mechanism: SigLIP uses a sigmoid loss during pre-training, which better aligns image and text embeddings for zero-shot tasks, especially on out-of-sample data like historical images.
- Core assumption: The training data distribution of SigLIP is sufficiently diverse to handle historical book illustrations despite domain shift.
- Evidence anchors: [abstract] "Results show that SigLIP embeddings slightly outperform CLIP and ViT for both retrieval and classification"; [section] "SigLIP performed slightly better than ViT and CLIP and retrieved 94% of the target images"

### Mechanism 2
- Claim: HNSW index enables efficient dense vector search for image retrieval.
- Mechanism: HNSW creates a navigable small world graph that allows logarithmic-time approximate nearest neighbor search in high-dimensional embedding space.
- Core assumption: The embedding space preserves semantic similarity that correlates with human-perceived image similarity.
- Evidence anchors: [section] "Qdrant database...used a cosine similarity-based HNSW index"

### Mechanism 3
- Claim: Fine-tuning logistic regression on embeddings enables classification of segmentation anomalies.
- Mechanism: Logistic regression learns a linear decision boundary in embedding space, transferring knowledge from pre-trained models to domain-specific classification.
- Core assumption: The embedding space contains sufficient discriminative information for classification tasks.
- Evidence anchors: [section] "logistic regression models (using scikit-learn v1.5.0) to classify images based on their embedding vectors"; [section] "obtained a cross-validated F1 score of 96%"

## Foundational Learning

- **Concept**: Vision Transformers (ViT)
  - Why needed here: Understanding how ViT processes images into embeddings for comparison with other models
  - Quick check question: What is the input size and output dimension for ViT-base in this application?

- **Concept**: Contrastive Learning
  - Why needed here: Understanding how CLIP and SigLIP align image and text embeddings through contrastive loss
  - Quick check question: How does contrastive loss differ from sigmoid loss in multimodal pre-training?

- **Concept**: Approximate Nearest Neighbor Search
  - Why needed here: Understanding why HNSW is used instead of brute-force search for efficiency
  - Quick check question: What is the time complexity difference between HNSW and brute-force search?

## Architecture Onboarding

- **Component map**: ALTO-XML parser → IIIF image fetcher → Embedding generator (ViT/CLIP/SigLIP) → Qdrant vector database → FastAPI backend → Flask/HTMX frontend
- **Critical path**: 1) Image extraction from ALTO-XML and IIIF API, 2) Embedding computation using pre-trained models, 3) Vector database indexing and search, 4) API serving and frontend display
- **Design tradeoffs**: Embedding size vs. storage (ViT/SigLIP 768-dim vs CLIP 512-dim), preprocessing (aspect ratio preservation vs resizing), on-premise hosting vs cloud scalability
- **Failure signatures**: Poor retrieval (poor embeddings or HNSW parameters), high classification error (insufficient discriminative features), API latency (poorly indexed database or insufficient compute)
- **First 3 experiments**: 1) Compare retrieval accuracy using different embedding models on held-out test set, 2) Evaluate classification performance by varying logistic regression regularization, 3) Test HNSW index performance with different parameter settings

## Open Questions the Paper Calls Out

- **Open Question 1**: How do CLIP and SigLIP embeddings perform for zero-shot classification of old illustrations compared to fine-tuned models?
- **Open Question 2**: What is the impact of aspect ratio distortion during image preprocessing on retrieval accuracy for historical illustrations?
- **Open Question 3**: How do deep learning-based embeddings compare to traditional hashing-based methods for exact image retrieval in digitized book collections?
- **Open Question 4**: What biases exist in the CLIP and SigLIP embeddings when applied to historical illustrations from Norwegian books?

## Limitations
- The study relies on a relatively small manually labeled dataset of 2000 images
- Performance evaluation is conducted on the same dataset without cross-domain validation
- Storage savings calculation assumes all identified anomalies are irrelevant
- Limited comparison to other state-of-the-art models beyond ViT, CLIP, and SigLIP

## Confidence

- **High Confidence**: Retrieval accuracy results (94% top-10 accuracy for SigLIP)
- **Medium Confidence**: Classification performance (96% F1-score)
- **Low Confidence**: Superiority of SigLIP over CLIP and ViT for this specific domain

## Next Checks

1. **Cross-domain validation**: Test the same embedding models and classification approach on a different historical book collection from another institution to verify generalizability of the performance improvements.

2. **Ablation study on preprocessing**: Systematically evaluate how different image preprocessing pipelines (aspect ratio handling, resizing methods) affect retrieval and classification performance for each embedding model.

3. **Long-term storage impact analysis**: Conduct a user study to determine whether automatically filtered "anomalies" actually contain useful information for researchers, validating the claimed storage savings against potential loss of scholarly value.