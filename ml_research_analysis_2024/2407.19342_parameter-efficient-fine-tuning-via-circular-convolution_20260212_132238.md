---
ver: rpa2
title: Parameter-Efficient Fine-Tuning via Circular Convolution
arxiv_id: '2407.19342'
source_url: https://arxiv.org/abs/2407.19342
tags:
- arxiv
- lora
- preprint
- language
- convolution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Parameter-efficient fine-tuning (PEFT) methods like LoRA reduce
  trainable parameters in large models but suffer from limited rank and performance
  constraints. This work introduces Circular Convolution Adaptation (C3A), which uses
  circular convolution and circulant matrices to represent delta weights.
---

# Parameter-Efficient Fine-Tuning via Circular Convolution

## Quick Facts
- arXiv ID: 2407.19342
- Source URL: https://arxiv.org/abs/2407.19342
- Authors: Aochuan Chen; Jiashun Cheng; Zijing Liu; Ziqi Gao; Fugee Tsung; Yu Li; Jia Li
- Reference count: 29
- One-line primary result: Introduces Circular Convolution Adaptation (C3A) that decouples parameter count from adaptation rank, achieving superior performance to LoRA with fewer parameters

## Executive Summary
This paper introduces Circular Convolution Adaptation (C3A), a novel parameter-efficient fine-tuning method that uses circular convolution and circulant matrices to represent delta weights. Unlike existing methods like LoRA that constrain adaptation rank through low-rank decomposition, C3A enables higher-rank adaptation without increasing parameter count by leveraging the mathematical properties of circulant matrices. The method achieves superior performance on multiple benchmarks including GLUE, commonsense reasoning, and code/math reasoning tasks while using fewer parameters and less memory than LoRA and its variants.

## Method Summary
C3A represents weight updates as circulant matrices generated by circular convolution kernels, which are then diagonalized using FFT for efficient computation. The method extends to block-circular convolution for handling non-square weight matrices and customizable parameter counts. By decoupling parameter count from adaptation rank, C3A achieves higher expressiveness than LoRA while maintaining computational efficiency through FFT-based operations. The approach is validated across multiple model sizes and task domains, demonstrating consistent improvements over baseline PEFT methods.

## Key Results
- C3A consistently outperforms LoRA and variants on GLUE, commonsense reasoning, and code/math reasoning tasks
- Achieves higher rank adaptation without increasing parameter count compared to LoRA's low-rank constraints
- Demonstrates effective scaling across different model sizes (LLaMA2-7B, LLaMA3-8B) and data quantities
- Provides better parameter efficiency and lower memory usage than baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Circular convolution enables higher-rank adaptation than LoRA while maintaining the same number of trainable parameters
- Mechanism: Circulant matrices constructed from circular convolution kernels can achieve higher rank than LoRA's low-rank decomposition for the same parameter budget
- Core assumption: The rank of a circulant matrix is determined by the degree of the greatest common divisor of the associated polynomial and x^d - 1
- Evidence anchors: [abstract] decouples parameter count from adaptation rank; [section] rank given by d - Deg(gcd(f(x), x^d - 1)); [corpus] Weak evidence, no direct citations

### Mechanism 2
- Claim: Fast Fourier Transform enables efficient computation and memory usage in C3A
- Mechanism: Diagonalization of circulant matrices through Fourier basis allows O(n log n) complexity for forward and backward passes
- Core assumption: FFT enables efficient element-wise multiplication in frequency domain
- Evidence anchors: [abstract] C3A leverages FFT for efficient computation; [section] O((d1+d2)/p log b + d1d2/b) complexity; [corpus] Weak evidence, mentions FFT in LoRA context

### Mechanism 3
- Claim: Block-circular convolution extends C3A's applicability to non-square weight matrices
- Mechanism: Partitioning activation vectors into blocks enables handling of non-square matrices while maintaining efficiency
- Core assumption: Circulant pattern within blocks preserves computational benefits
- Evidence anchors: [abstract] block-circular convolution for greater flexibility; [section] addresses 4096 × 1024 weight matrix dimensions; [corpus] Weak evidence, no specific citations

## Foundational Learning

- Concept: Circulant matrices and their properties
  - Why needed here: Understanding how circular convolution creates circulant matrices and their diagonalization through FFT is fundamental to grasping C3A's mechanism
  - Quick check question: What is the relationship between circular convolution and circulant matrices, and how does this relationship enable efficient computation through FFT?

- Concept: Low-rank adaptation and its limitations
  - Why needed here: LoRA serves as the baseline method that C3A improves upon, so understanding its rank constraints is essential
  - Quick check question: How does LoRA's low-rank decomposition limit its expressiveness, and what computational advantages does it provide?

- Concept: Fast Fourier Transform and its applications
  - Why needed here: FFT is the key computational tool that enables C3A's efficiency
  - Quick check question: What is the computational complexity of FFT, and how does it enable efficient computation with circulant matrices?

## Architecture Onboarding

- Component map: Input activation vector (x) -> Circular convolution kernel (∆w) -> FFT operations -> Block-circular convolution extension -> Weight update mechanism (∆W = C(blk)(∆w)) -> Integration with pre-trained model weights

- Critical path:
  1. Compute FFT of activation vector
  2. Compute FFT of circular convolution kernel
  3. Perform element-wise multiplication in frequency domain
  4. Apply inverse FFT to obtain output
  5. During backpropagation, compute gradients using circular convolution with gradient of output

- Design tradeoffs:
  - Kernel size vs. rank flexibility: Larger kernels allow higher potential rank but increase computational cost
  - Block size vs. parameter efficiency: Smaller blocks increase parameter count but provide more flexibility
  - FFT implementation vs. hardware compatibility: FFT requires specialized kernels for optimal performance

- Failure signatures:
  - Poor performance on tasks requiring high-rank adaptations when using small kernel sizes
  - Computational inefficiency when block sizes are not chosen optimally for hardware
  - Memory issues when handling very large models due to FFT operation overhead

- First 3 experiments:
  1. Compare C3A with LoRA on a simple GLUE task using same parameter count to verify rank flexibility advantage
  2. Measure computational time and memory usage of C3A vs. LoRA on medium-sized model to validate efficiency claims
  3. Test block-circular convolution on non-square weight matrix (4096 × 1024) to verify extension's functionality

## Open Questions the Paper Calls Out
None

## Limitations
- The rank advantage over LoRA relies on theoretical properties of circulant matrices but lacks direct empirical validation through rank comparison
- Computational efficiency gains depend heavily on optimal FFT implementation and may vary across hardware configurations
- Block-circular convolution extension is described but lacks direct experimental validation or citations supporting its effectiveness

## Confidence

- High confidence: Fundamental mechanism of circular convolution creating circulant matrices diagonalized via FFT for efficient computation
- Medium confidence: Rank flexibility claims - theoretical framework is sound but empirical validation is limited
- Medium confidence: Performance improvements - benchmark results show consistent gains but extent varies by task and model size
- Low confidence: Block-circular convolution extension - method is described but lacks direct experimental validation

## Next Checks

1. **Rank Analysis Experiment**: Conduct controlled experiment comparing effective rank of weight adaptations achieved by C3A versus LoRA using identical parameter budgets on medium-sized model. Measure singular value spectrum to quantify rank difference.

2. **Cross-Platform Efficiency Validation**: Implement C3A using different FFT backends (cuFFT, MKL, OpenBLAS) and test on multiple hardware configurations (NVIDIA vs AMD GPUs, different memory sizes) to verify computational efficiency gains are consistent.

3. **Ablation Study on Block Size**: Systematically vary block size parameter b across range of values for non-square weight matrices and measure trade-offs between parameter efficiency, computational overhead, and final task performance to establish optimal selection heuristics.