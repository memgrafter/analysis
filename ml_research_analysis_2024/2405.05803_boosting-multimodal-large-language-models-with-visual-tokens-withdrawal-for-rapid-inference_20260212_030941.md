---
ver: rpa2
title: Boosting Multimodal Large Language Models with Visual Tokens Withdrawal for
  Rapid Inference
arxiv_id: '2405.05803'
source_url: https://arxiv.org/abs/2405.05803
tags:
- tokens
- vision
- mllms
- attention
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Visual Tokens Withdrawal (VTW), a method
  to accelerate multimodal large language models (MLLMs) by strategically removing
  visual tokens in deeper layers. The key insight is that visual information migrates
  to text tokens in early layers, making visual tokens redundant in deeper ones.
---

# Boosting Multimodal Large Language Models with Visual Tokens Withdrawal for Rapid Inference

## Quick Facts
- **arXiv ID**: 2405.05803
- **Source URL**: https://arxiv.org/abs/2405.05803
- **Reference count**: 10
- **Key outcome**: VTW reduces FLOPs by over 40% across diverse MLLM tasks without performance loss

## Executive Summary
This paper introduces Visual Tokens Withdrawal (VTW), a method to accelerate multimodal large language models (MLLMs) by strategically removing visual tokens in deeper layers. The key insight is that visual information migrates to text tokens in early layers, making visual tokens redundant in deeper ones. VTW determines the optimal layer to withdraw visual tokens using KL divergence, enabling text-only processing thereafter. Experiments show VTW reduces FLOPs by over 40% across tasks like visual question answering, reasoning, and video understanding, without performance loss. It also improves inference speed and memory efficiency, while being compatible with KV Cache and Flash-attention. VTW demonstrates strong generalization across diverse MLLMs and tasks, including fine-grained segmentation.

## Method Summary
VTW works by removing visual tokens from MLLMs at a specific layer determined through KL divergence analysis. The method identifies when visual information has sufficiently migrated to text tokens by comparing standard and VTW output logits at each layer. When KL divergence falls below a threshold, visual tokens are withdrawn and subsequent layers process only text tokens. The approach requires no additional training and can be applied as a plug-and-play module to existing MLLMs. VTW is compatible with KV Cache and Flash-attention, enabling practical speedups beyond theoretical FLOP reductions.

## Key Results
- Reduces FLOPs by over 40% across diverse MLLM tasks including VQA, reasoning, and video understanding
- Maintains performance while improving inference speed and memory efficiency
- Generalizes well across different MLLM architectures and tasks, including fine-grained segmentation

## Why This Works (Mechanism)

### Mechanism 1
Visual information migrates to text tokens in early layers, making vision tokens redundant in deep layers. Causal self-attention allows instruction and output tokens to attend to all vision tokens, while vision tokens cannot attend to instruction or output tokens. After several layers, instruction and output tokens absorb both visual and textual information.

- Core assumption: The causal masking in self-attention ensures information flows unidirectionally from vision tokens to text tokens without loss
- Evidence anchors: Abstract states "visual information is transferred to subsequent text tokens within the first few layers of MLLMs"; ablation studies confirm information migration
- Break condition: If causal masking is altered or if cross-attention is used instead of self-attention, the information migration pattern may break

### Mechanism 2
Attention sinks in MLLMs concentrate on system and initial tokens in deep layers, reducing the need for vision tokens. The softmax operation in attention cannot assign zero attention to undesired tokens. In deep layers, output tokens have sufficient self-contained information and focus on tokens with minimal semantic content (system tokens) to avoid incorporating undesired information.

- Core assumption: The softmax function inherently creates attention sinks regardless of token semantics
- Evidence anchors: Abstract mentions "The attention sink phenomenon that is prevalent in LLMs also persists in MLLMs"; experiments show >80% attention goes to just 35 system tokens
- Break condition: If the attention mechanism changes to allow explicit token exclusion or if the model architecture fundamentally changes how attention is computed

### Mechanism 3
KL divergence criterion effectively identifies the optimal layer for vision token withdrawal without performance loss. By calculating KL divergence between standard and VTW output logits at each layer, the method finds the first layer where the divergence falls below a threshold, indicating sufficient information migration.

- Core assumption: KL divergence between logits is a reliable proxy for semantic similarity and performance equivalence
- Evidence anchors: Abstract states "we choose the first layer that meets the Kullback-Leibler divergence criterion"; experiments enumerate K values and compute KL divergence
- Break condition: If the threshold selection is inappropriate or if KL divergence doesn't correlate well with actual task performance

## Foundational Learning

- **Concept**: Causal self-attention and masking
  - Why needed here: Understanding why vision tokens become redundant requires knowing how information flows through layers
  - Quick check question: In causal self-attention, can a token attend to tokens that come after it in the sequence?

- **Concept**: Attention sink phenomenon
  - Why needed here: Explains why certain tokens (like system tokens) dominate attention in deep layers
  - Quick check question: What property of the softmax function creates attention sinks in deep layers?

- **Concept**: KL divergence as similarity metric
  - Why needed here: Used to determine when vision token withdrawal won't affect model performance
  - Quick check question: What does it mean when KL divergence between two distributions approaches zero?

## Architecture Onboarding

- **Component map**: Vision encoder (CLIP ViT-L) → Cross-modal projector → LLM (Vicuna) → Output → VTW module inserted after K-th layer to remove vision tokens → KL divergence calculator for layer selection

- **Critical path**: Input processing → First K layers (with vision tokens) → VTW layer (remove vision tokens) → Remaining N-K layers (text-only) → Output

- **Design tradeoffs**:
  - Earlier withdrawal (smaller K): More FLOPs saved but higher risk of performance degradation
  - Later withdrawal (larger K): Safer for performance but fewer FLOPs saved
  - Threshold selection: Too strict may miss optimization opportunities, too loose may cause performance loss

- **Failure signatures**:
  - Performance degradation on tasks requiring fine-grained visual details
  - Increased KL divergence between standard and VTW outputs
  - Inability to answer questions about specific visual elements

- **First 3 experiments**:
  1. Apply VTW with K=5 on AI2D dataset and measure accuracy vs baseline
  2. Sweep K from 5 to 20 on MME benchmark and plot accuracy vs layer index
  3. Test VTW on multimodal chatbot with different threshold values (η = 0.001, 0.003, 0.006)

## Open Questions the Paper Calls Out

### Open Question 1
What is the precise mechanism by which information migrates from visual tokens to text tokens in early layers, and can this be quantified or visualized? While the paper confirms information migration occurs, it does not provide a detailed, quantitative explanation of the underlying mechanism or how to measure it precisely. Detailed analysis of attention patterns and feature representations across layers, possibly using techniques like feature attribution or gradient-based methods, could map the flow of information.

### Open Question 2
How does the performance of VTW vary with different types of multimodal tasks, particularly those requiring fine-grained visual details versus high-level reasoning? The paper provides general performance metrics but lacks a detailed breakdown of how VTW performs on tasks with varying visual complexity. Comparative studies on a wide range of tasks, including fine-grained segmentation, object detection, and abstract reasoning, would assess VTW's adaptability and limitations.

### Open Question 3
Can VTW be effectively applied during the training phase to further enhance efficiency and performance, and what would be the trade-offs? The paper mentions that VTW is currently applied only during inference and suggests future work could explore its application during training. Experimental results comparing models trained with VTW against those trained without, focusing on efficiency gains and any changes in model performance or generalization, would provide insights.

## Limitations

- Performance preservation claim needs more rigorous statistical validation across different model sizes and architectures
- Optimal withdrawal layer varies significantly between datasets (K ranges from 6-20), suggesting method may require task-specific tuning
- While theoretical FLOPs reduction is quantified, actual GPU memory usage and inference latency with KV Cache enabled are not measured

## Confidence

- **High Confidence**: The basic premise that visual tokens become redundant in deeper layers is well-supported by attention analysis showing >80% concentration on system tokens. FLOPs reduction calculations are mathematically sound.
- **Medium Confidence**: Effectiveness of VTW across diverse tasks is demonstrated, but performance preservation claim needs more rigorous statistical validation across different model sizes and architectures.
- **Low Confidence**: Universality of the causal attention mechanism explanation and robustness of KL divergence as sole criterion for withdrawal layer selection remain unproven across different MLLM designs.

## Next Checks

1. **Architecture Generalization Test**: Apply VTW to MLLMs using cross-attention mechanisms (like BLIP-2) to verify if the visual information migration hypothesis holds when the attention pattern differs from pure causal self-attention.

2. **Threshold Sensitivity Analysis**: Systematically vary the KL divergence threshold (η) across three orders of magnitude (0.001 to 0.01) on a held-out validation set to determine if the chosen value is optimal or conservative.

3. **Memory Efficiency Validation**: Measure actual GPU memory usage and inference latency with KV Cache enabled, comparing against theoretical FLOPs reduction to quantify real-world benefits beyond computational complexity metrics.