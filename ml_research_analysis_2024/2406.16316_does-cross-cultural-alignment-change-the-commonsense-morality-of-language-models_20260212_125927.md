---
ver: rpa2
title: Does Cross-Cultural Alignment Change the Commonsense Morality of Language Models?
arxiv_id: '2406.16316'
source_url: https://arxiv.org/abs/2406.16316
tags:
- japanese
- dataset
- language
- arxiv
- commonsense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether cross-cultural alignment strategies
  in large language models (LLMs) marginalize non-English speaking communities' preferences.
  The study focuses on aligning Japanese LLMs using datasets from different cultural
  backgrounds - English resources translated to Japanese and Japanese resources.
---

# Does Cross-Cultural Alignment Change the Commonsense Morality of Language Models?

## Quick Facts
- arXiv ID: 2406.16316
- Source URL: https://arxiv.org/abs/2406.16316
- Reference count: 28
- One-line primary result: Cross-cultural alignment can improve general capabilities but may not fully capture culturally-specific moral judgments.

## Executive Summary
This paper investigates whether cross-cultural alignment strategies in large language models (LLMs) marginalize non-English speaking communities' preferences. The study focuses on aligning Japanese LLMs using datasets from different cultural backgrounds - English resources translated to Japanese and Japanese resources. The evaluation is conducted using the JCommonsenseMorality (JCM) and ETHICS datasets, which contain judgments on commonsense morality. The experiments show that while models fine-tuned on English-translated JCM outperform those trained on Japanese-translated ETHICS, models trained on native JCM datasets still achieve the highest accuracy. Additionally, when fine-tuning with real-world user prompts from Chatbot Arena (translated from English), the model shows improved instruction-following capabilities but lower performance on cultural commonsense morality tasks compared to models trained on JCM. This suggests that while cross-cultural alignment can enhance general capabilities, it may not fully capture culturally-specific moral judgments.

## Method Summary
The study investigates cross-cultural alignment in Japanese LLMs by fine-tuning them on different datasets: JCM (culturally Japanese data), JCM-EN (JCM translated to English), ETHICS-JA (ETHICS translated to Japanese), and ETHICS (culturally English data). The models are fine-tuned using Direct Preference Optimization (DPO) with Low-Rank Adaptation (LoRA). The fine-tuned models are then evaluated on the test sets of JCM and ETHICS datasets, as well as on the Japanese MT-Bench for instruction-following capability. The study uses three Japanese LLMs: CALM2, llm-jp, and Swallow.

## Key Results
- Models trained on English-translated JCM outperform those trained on Japanese-translated ETHICS, despite using English for training.
- Models trained on native JCM datasets achieve the highest accuracy on JCM test sets.
- Fine-tuning on Chatbot Arena Conversations translated to Japanese improves instruction-following capabilities but reduces performance on culturally-specific moral reasoning tasks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-cultural transfer of commonsense morality is more challenging than cross-lingual transfer for LLMs.
- Mechanism: The alignment process appears to be more sensitive to the cultural origin of the annotators than to the language of the data itself. Models trained on English-translated culturally Japanese data (JCM-EN) outperform those trained on Japanese-translated culturally English data (ETHICS-JA), suggesting that the underlying cultural norms are harder to generalize than the language differences.
- Core assumption: The LLMs have sufficient capacity to learn both linguistic and cultural nuances, but the cultural alignment process is less effective when the annotators' cultural background does not match the target population.
- Evidence anchors:
  - [abstract] "Interestingly, we observe that the LLMs aligned with the English-translated JCM dataset achieve higher accuracy than the LLMs aligned with the Japanese-translated ETHICS dataset."
  - [section 3] "Interestingly, the models trained with JCM-EN outperform the models trained with ETHICS-JA, despite it uses English to train the model."
  - [corpus] Found 25 related papers; FMR scores range from 0.4-0.67, indicating moderate similarity in cultural alignment themes, but specific experimental evidence is limited.
- Break condition: If the LLM's architecture or training data lacks sufficient cultural diversity, or if the cultural differences are too subtle for the model to detect, the cross-cultural transfer may fail entirely.

### Mechanism 2
- Claim: Incorporating real-world user prompts from diverse sources can improve instruction-following capability but may reduce culturally-specific moral reasoning accuracy.
- Mechanism: Fine-tuning on Chatbot Arena Conversations translated to Japanese (ChatbotArena-JA) improves the model's ability to follow instructions in Japanese, as measured by the Japanese MT-Bench. However, this comes at the cost of reduced accuracy on the culturally specific JCM dataset, suggesting a trade-off between general instruction-following and culturally-specific moral reasoning.
- Core assumption: The multilingual reward model (OASST) used to label preferences in ChatbotArena-JA is not perfectly aligned with Japanese cultural norms, leading to a bias in the fine-tuned model.
- Evidence anchors:
  - [abstract] "Nevertheless, the model trained on the development set of the JCM dataset outperforms the model trained on English resources in the test set of the JCM dataset."
  - [section 4] "Despite the fine-tuning dataset being constructed from predominantly English resources, it achieves higher accuracy on the JCM."
  - [corpus] Related work on multilingual alignment suggests similar trade-offs between language coverage and cultural specificity.
- Break condition: If the multilingual reward model is not sufficiently culturally diverse, or if the real-world prompts do not adequately represent the target culture's values, the model's culturally-specific moral reasoning will degrade.

### Mechanism 3
- Claim: The effectiveness of cross-cultural alignment depends on the source and target cultures' similarity.
- Mechanism: The success of cross-cultural transfer is influenced by the degree of overlap between the source and target cultures' moral values. If the cultures share significant moral common ground, transfer is more effective; if they diverge significantly, transfer is less effective.
- Core assumption: There exists a spectrum of cultural similarity, and the effectiveness of cross-cultural alignment falls along this spectrum.
- Evidence anchors:
  - [abstract] "This result suggests that cultural differences may be more challenging to learn and generalize than language differences for the LLMs."
  - [section 4] "Nevertheless, the results indicate the potential for further enhancement of the model's comprehension of cultural commonsense morality by using the annotations provided by members of the communities."
  - [corpus] The corpus includes related work on cross-cultural NLP, but lacks direct evidence for the cultural similarity hypothesis.
- Break condition: If the source and target cultures are too dissimilar, or if the model's training data does not adequately represent the target culture's values, cross-cultural alignment will be ineffective.

## Foundational Learning

- Concept: Cultural dependence of commonsense morality
  - Why needed here: Understanding that commonsense morality varies across cultures is fundamental to interpreting the results of cross-cultural alignment experiments.
  - Quick check question: Why might a statement considered morally acceptable in one culture be deemed unethical in another?

- Concept: Cross-lingual vs. cross-cultural transfer
  - Why needed here: Distinguishing between the challenges of transferring knowledge across languages versus across cultures is crucial for designing effective alignment strategies.
  - Quick check question: What are the key differences between translating data and translating cultural context?

- Concept: Reward model bias
  - Why needed here: Recognizing that the choice of reward model can introduce bias into the alignment process is essential for interpreting the results of fine-tuning experiments.
  - Quick check question: How might a reward model trained primarily on English and Spanish data affect the alignment of a Japanese LLM?

## Architecture Onboarding

- Component map:
  Base LLM (CALM2, llm-jp, Swallow) -> Fine-tuning dataset (JCM, ETHICS, ChatbotArena-JA) -> Reward model (OASST) -> Evaluation datasets (JCM, ETHICS, Japanese MT-Bench)

- Critical path:
  1. Translate source data to target language (if necessary)
  2. Fine-tune LLM on translated data using DPO
  3. Evaluate fine-tuned model on culturally-specific and general tasks

- Design tradeoffs:
  - Using translated data vs. native data: Translated data is more readily available but may introduce translation artifacts.
  - Multilingual reward model vs. monolingual reward model: A multilingual reward model can handle diverse data but may not capture cultural nuances as well.
  - General instruction-following vs. culturally-specific moral reasoning: Improving one may come at the expense of the other.

- Failure signatures:
  - Low accuracy on culturally-specific tasks despite high accuracy on general tasks
  - Inconsistent performance across different fine-tuning datasets
  - Degraded performance on tasks in the source language after fine-tuning on translated data

- First 3 experiments:
  1. Fine-tune CALM2 on JCM-EN and evaluate on JCM
  2. Fine-tune CALM2 on ETHICS-JA and evaluate on ETHICS
  3. Fine-tune CALM2 on ChatbotArena-JA and evaluate on JCM and Japanese MT-Bench

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does cross-cultural alignment affect the commonsense morality of large language models in other languages beyond Japanese?
- Basis in paper: [explicit] The paper investigates the effect of aligning Japanese language models with English resources on their understanding of Japanese cultural commonsense morality.
- Why unresolved: The study focuses solely on Japanese LLMs and English-Japanese cultural alignment, leaving the generalizability of these findings to other language pairs unexplored.
- What evidence would resolve it: Experiments replicating this study with other language pairs (e.g., Spanish-English, French-English) using similar cross-cultural alignment strategies and cultural commonsense morality datasets.

### Open Question 2
- Question: To what extent do culturally specific moral judgments transfer across languages when using machine-translated datasets for alignment?
- Basis in paper: [inferred] The paper shows that models trained on English-translated JCM outperform those trained on Japanese-translated ETHICS, suggesting cultural differences may be more challenging to learn than language differences.
- Why unresolved: The study does not isolate the effect of translation quality versus cultural content differences, nor does it explore the limits of cross-cultural transfer in moral judgments.
- What evidence would resolve it: Controlled experiments varying translation quality and cultural content while measuring moral judgment accuracy across different LLM architectures and training regimes.

### Open Question 3
- Question: How does the demographic diversity of annotators in preference datasets affect the cultural commonsense morality alignment of large language models?
- Basis in paper: [explicit] The paper notes that most alignment work is done in English with datasets dominated by English-speaking annotators' preferences, raising concerns about marginalizing non-English communities.
- Why unresolved: The study does not directly measure the impact of annotator demographic diversity on model outputs or explore alternative alignment strategies that incorporate diverse cultural perspectives.
- What evidence would resolve it: Comparative studies training models on preference datasets with varying demographic compositions and evaluating their performance on culturally diverse commonsense morality tasks.

## Limitations
- The study only compares Japanese and English cultural norms, limiting generalizability to other cultural contexts.
- Reliance on machine-translated datasets may introduce artifacts that confound the results.
- The study does not directly measure the impact of annotator demographic diversity on model outputs.

## Confidence
- The confidence in the main findings is Medium. The experimental design is rigorous and the results are internally consistent, but the limited cultural scope and reliance on translated data introduce uncertainties.

## Next Checks
1. Replicate the experiments with additional cultural contexts (e.g., Arabic, Chinese, or Spanish) to test whether the observed patterns generalize beyond the Japanese-English comparison.
2. Conduct a parallel study using native cultural data (i.e., JCM data annotated by English speakers and ETHICS data annotated by Japanese speakers) to isolate the effects of cultural vs. linguistic alignment.
3. Perform ablation studies varying dataset sizes and qualities to determine whether the observed performance differences are driven by cultural alignment or by other factors such as data quantity or annotation quality.