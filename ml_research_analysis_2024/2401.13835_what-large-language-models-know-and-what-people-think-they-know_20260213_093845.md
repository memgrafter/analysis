---
ver: rpa2
title: What Large Language Models Know and What People Think They Know
arxiv_id: '2401.13835'
source_url: https://arxiv.org/abs/2401.13835
tags:
- confidence
- answer
- explanations
- questions
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the calibration gap between large language
  models (LLMs) and human users' perception of LLM confidence. Through behavioral
  experiments using GPT-3.5, PaLM2, and GPT-4o across multiple-choice and short-answer
  questions, the study finds that users significantly overestimate LLM accuracy when
  provided with default explanations, exhibiting higher calibration error (ECE) and
  lower discrimination ability (AUC) compared to the models' internal confidence.
---

# What Large Language Models Know and What People Think They Know

## Quick Facts
- arXiv ID: 2401.13835
- Source URL: https://arxiv.org/abs/2401.13835
- Reference count: 40
- Primary result: Users significantly overestimate LLM accuracy when provided with default explanations, but this calibration gap can be reduced by aligning explanation uncertainty language with internal model confidence

## Executive Summary
This study investigates the mismatch between large language model (LLM) confidence and human users' perception of that confidence. Through behavioral experiments with GPT-3.5, PaLM2, and GPT-4o, researchers found that users consistently overestimate LLM accuracy when given default explanations, exhibiting higher calibration error and lower discrimination ability compared to the models' internal confidence. The key insight is that users rely on surface-level textual cues like explanation length and certainty expressions rather than deep content evaluation. By modifying LLM explanations to better reflect actual internal confidence levels through tailored uncertainty language and length, the study successfully reduced both the calibration gap and discrimination gap between human and model confidence assessments.

## Method Summary
The researchers conducted behavioral experiments using multiple-choice questions from the MMLU dataset and short-answer questions from the Trivia QA dataset. They extracted model confidence from LLMs using token likelihoods (for multiple-choice) and pTrue values (for short-answer). Participants were asked to estimate the probability of correctness for LLM answers, with explanations generated using prompts that varied in uncertainty language and length. The study compared three conditions: default explanations, explanations with high uncertainty language, and explanations aligned with model confidence through a selection rule that matched explanation style to confidence levels. Expected Calibration Error (ECE) and Area Under the Curve (AUC) metrics were used to quantify calibration and discrimination gaps.

## Key Results
- Users significantly overestimate LLM accuracy with default explanations (ECE reduced from 0.156 to 0.092 when explanations were aligned with model confidence)
- Longer explanations increase user confidence without improving discrimination ability (AUC remained near 0.5)
- Tailored uncertainty communication substantially improves both calibration and discrimination gaps between human and model confidence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Users rely on surface-level textual cues (length, certainty expressions) rather than deep content evaluation when judging LLM confidence.
- Mechanism: When explanations contain more words or explicit certainty phrases, users infer higher model confidence without verifying the accuracy or depth of the content.
- Core assumption: Human cognitive processing of AI explanations defaults to heuristic shortcuts when time or cognitive resources are limited.
- Evidence anchors:
  - Longer explanations increased user confidence, even when the extra length did not improve answer accuracy
  - Users were processing explanations at a shallow level, relying on simple textual cues such as overall length to predict LLM accuracy

### Mechanism 2
- Claim: Aligning verbal uncertainty expressions with internal model confidence reduces the calibration gap between human and model confidence.
- Mechanism: By modifying LLM prompts to express low/medium/high certainty language corresponding to actual model confidence levels, users receive consistent signals that match the model's internal assessment, improving their calibration.
- Core assumption: Human users interpret verbal uncertainty cues as valid indicators of model reliability when these cues are systematically aligned with model confidence.
- Evidence anchors:
  - By adjusting LLM explanations to better reflect the models' internal confidence, both the calibration gap and the discrimination gap narrowed
  - Designed prompts to induce varying degrees of certainty in explanations, ranging from expressions of low confidence to high confidence

### Mechanism 3
- Claim: Users lack domain-specific knowledge, so they cannot independently verify LLM answers and must rely entirely on the explanations provided.
- Mechanism: Without subject-matter expertise, participants cannot distinguish correct from incorrect answers on their own and therefore defer to the LLM's explanation, making them vulnerable to miscalibration.
- Core assumption: The difficulty of MMLU questions exceeds typical participant knowledge levels, creating dependency on LLM output.
- Evidence anchors:
  - Participants' average answer accuracy was 51%, closely aligning with LLM's 52% accuracy rate
  - Participants who considered themselves more knowledgeable were not more adept at estimating the LLM's performance
  - The majority of participants in experiments lack domain expertise

## Foundational Learning

- Concept: Expected Calibration Error (ECE)
  - Why needed here: ECE quantifies how well confidence scores match actual accuracy, which is central to measuring the calibration gap between human and model confidence.
  - Quick check question: If a model says it is 80% confident and is correct 60% of the time in that confidence range, is it over- or under-calibrated?

- Concept: Area Under the Curve (AUC) for discrimination
  - Why needed here: AUC measures the ability to distinguish correct from incorrect answers based on confidence scores, capturing the discrimination gap.
  - Quick check question: What does an AUC of 0.5 indicate about a confidence metric's discrimination ability?

- Concept: Token likelihood as model confidence proxy
  - Why needed here: The paper uses token likelihoods to extract model confidence from LLMs, which is essential for understanding how confidence is operationalized.
  - Quick check question: Why might token likelihoods be preferred over prompting methods for extracting model confidence?

## Architecture Onboarding

- Component map: MMLU dataset (multiple choice) → question sampling → model confidence extraction → participant interface → LLM API calls to GPT-3.5, PaLM2, GPT-4o → token likelihood normalization across answer options → prompt engineering with varying uncertainty language and length → participant confidence ratings → ECE and AUC computation

- Critical path:
  1. Sample questions with balanced confidence distribution
  2. Extract model confidence via token likelihoods
  3. Generate explanations with controlled uncertainty and length
  4. Collect participant confidence ratings
  5. Compute ECE and AUC metrics
  6. Apply selection rule to align explanation style with model confidence

- Design tradeoffs:
  - Using token likelihoods vs. prompting for confidence: Token likelihoods are more direct but require API access; prompting is more accessible but less accurate
  - Explanation length manipulation: Longer explanations increase user confidence but don't improve discrimination
  - Single vs. dual prompt approach: Current method uses two prompts (confidence + explanation) but could be optimized to one

- Failure signatures:
  - High ECE persisting after explanation alignment → model confidence extraction is unreliable
  - AUC near 0.5 after alignment → participants aren't using uncertainty cues effectively
  - Participant accuracy improving after alignment → alignment is helping them select better answers, not just calibrate confidence

- First 3 experiments:
  1. Replicate calibration gap measurement with a new LLM model to verify generalizability
  2. Test explanation alignment with a different question type (e.g., true/false instead of multiple choice)
  3. Measure whether providing explicit training on uncertainty cues improves calibration without explanation modification

## Open Questions the Paper Calls Out

- What is the fundamental cause of the miscommunication of uncertainty between LLM internal confidence and external explanations? The paper discusses hypotheses about why LLMs generate calibrated model confidences while also producing explanations that are not consistent with those confidences, mentioning RLHF and autoregressive nature as possible causes. This remains unresolved as the paper presents these as hypotheses without definitive evidence or testing to determine which mechanism is primarily responsible.

- How generalizable are these findings to longer, more complex open-ended questions beyond multiple choice and short-answer formats? The paper explicitly states this as a limitation, noting they focused on questions with "a small number of response alternatives" and short-answer questions. This remains unresolved as the experiments were limited to specific question formats, and the paper acknowledges this limitation without testing broader applicability.

- What is the optimal balance between explanation length and uncertainty language that maximizes both user confidence accuracy and information retention? The paper found that longer explanations increased user confidence without improving discrimination ability, and that uncertainty language affected confidence levels, but did not optimize for a balance between these factors. This remains unresolved as while the paper manipulated explanation length and uncertainty separately, it did not systematically explore their combined effects or identify an optimal configuration.

## Limitations

- The experiments relied on relatively simple question types (multiple-choice and short-answer) from specific datasets, which may not capture the complexity of real-world LLM applications.
- The participant pool was drawn from Prolific with specific demographic requirements (US-based, English-speaking, median age 35), potentially limiting external validity to broader populations.
- The study's findings are based on controlled experimental conditions that may not fully represent real-world usage scenarios where users have varying levels of expertise and engagement.

## Confidence

- Tailored uncertainty communication reduces calibration gaps: High
- Users rely on surface-level textual cues rather than content evaluation: Medium
- Participants lack domain expertise to independently verify answers: Medium

## Next Checks

1. Replicate the calibration gap measurement with a new LLM model (e.g., Claude or Llama) to verify that the phenomenon generalizes beyond the tested models.

2. Test the explanation alignment approach with different question types such as true/false or numerical estimation to assess robustness across task formats.

3. Measure whether providing explicit training on uncertainty cue interpretation improves calibration without modifying explanations, which would help isolate whether the effect is due to cue recognition versus cue alignment.