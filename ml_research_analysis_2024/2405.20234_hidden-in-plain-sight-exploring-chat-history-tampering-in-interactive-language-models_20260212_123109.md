---
ver: rpa2
title: 'Hidden in Plain Sight: Exploring Chat History Tampering in Interactive Language
  Models'
arxiv_id: '2405.20234'
source_url: https://arxiv.org/abs/2405.20234
tags:
- uni00000056
- uni00000044
- uni0000004c
- user
- chat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores chat history tampering in interactive LLMs,
  where users can inject fake chat history into conversations to influence model behavior.
  The authors propose an LLM-Guided Genetic Algorithm (LLMGA) to automatically search
  for effective prompt templates that structure the injected history.
---

# Hidden in Plain Sight: Exploring Chat History Tampering in Interactive Language Models

## Quick Facts
- **arXiv ID**: 2405.20234
- **Source URL**: https://arxiv.org/abs/2405.20234
- **Reference count**: 40
- **Primary result**: Chat history tampering can achieve up to 97% success rate on ChatGPT by injecting fake chat history to influence model behavior

## Executive Summary
This paper explores a novel vulnerability in interactive language models called "chat history tampering," where users can inject fake chat history into conversations to manipulate model behavior. The authors propose an LLM-Guided Genetic Algorithm (LLMGA) to automatically search for effective prompt templates that structure injected history. Their findings reveal that LLMs fundamentally cannot distinguish between user inputs and system context, and process structured text semantically rather than strictly adhering to ChatML syntax. This vulnerability enables attackers to significantly increase success rates for disallowed content elicitation attacks.

## Method Summary
The research employs an LLM-Guided Genetic Algorithm (LLMGA) that evolves prompt templates through fitness evaluation, selection, crossover, and mutation operations. The method tests various LLMs including ChatGPT, Llama-2, Llama-3, Vicuna, Gemma-2, InternLM, and ChatGLM2 using datasets from Chatbot Arena dialogues and harmful question corpora. The effectiveness is measured using Response Retrieval Rate (RRR) and attack success rates across different strategies including Acceptance Injection and Demonstration Injection.

## Key Results
- Chat history tampering achieved up to 97% success rate on ChatGPT for disallowed content elicitation
- RRR values vary significantly across different temperature settings, dropping notably after T=1.5
- The attack is effective across multiple LLM architectures including GPT-3.5, GPT-4, and various open-source models
- Semantic interpretation of ChatML format enables successful context injection even when special tokens are used

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs cannot separate user inputs from system context because they process all text as uniform continuation
- Mechanism: When chat history and user message are concatenated into a single input sequence, the LLM has no architectural way to distinguish between system-provided context and user-provided content
- Core assumption: The transformer architecture fundamentally processes text as continuation rather than having separate processing channels for different input types
- Evidence anchors:
  - [abstract]: "LLMs cannot separate user inputs from context, enabling chat history tampering"
  - [section 3.1]: "LLMs are designed to receive a text sequence and continue it... integrating chat history and user messages involves concatenating them into a single input sequence"
  - [corpus]: No direct evidence found - corpus neighbors focus on different aspects of LLM limitations

### Mechanism 2
- Claim: LLMs process structured text semantically rather than strictly adhering to ChatML syntax
- Mechanism: LLMs extract context information based on semantic patterns and role identification rather than rigid token parsing, allowing flexible interpretation of special tokens
- Core assumption: The fine-tuning process teaches LLMs to recognize contextual patterns rather than exact token sequences
- Evidence anchors:
  - [abstract]: "LLMs cannot strictly adhere to ChatML. LLMs originate from traditional NLP scenarios where the input text is unstructured"
  - [section 3.1]: "LLMs understand the input contextually instead of parsing it strictly"
  - [corpus]: Weak evidence - corpus focuses on different LLM reasoning limitations rather than parsing behavior

### Mechanism 3
- Claim: LLM temperature affects the model's sensitivity to context injection
- Mechanism: Higher temperature introduces randomness that can disrupt the model's ability to recognize and respond to injected context patterns
- Core assumption: Temperature parameter influences not just output diversity but also input pattern recognition
- Evidence anchors:
  - [section 4.2]: "LLM temperature significantly impacts the RRR metric... RRR values drop significantly after T=1.5"
  - [section 3.3]: Discussion of temperature as a parameter affecting generation randomness
  - [corpus]: No direct evidence found - corpus neighbors don't address temperature effects on context recognition

## Foundational Learning

- Concept: Context structuring and ChatML format
  - Why needed here: Understanding how chat history is formatted and integrated into LLM inputs is essential for comprehending the attack vector
  - Quick check question: What are the four types of special tokens used in context structuring, and what role does each play?

- Concept: Genetic algorithms and evolutionary optimization
  - Why needed here: The LLMGA algorithm uses evolutionary principles to search for effective prompt templates
  - Quick check question: How does the LLMGA algorithm use fitness evaluation and mutation to evolve better templates over iterations?

- Concept: Response retrieval metrics and evaluation
  - Why needed here: The RRR metric is crucial for quantifying the effectiveness of chat history tampering
  - Quick check question: What is the Response Retrieval Rate (RRR) metric, and how does it measure the success of context injection?

## Architecture Onboarding

- Component map:
  - User message input layer → Context structuring module → LLM inference engine → Response generation
  - LLMGA optimization layer (separate from inference)
  - Evaluation metrics layer (RRR calculation)

- Critical path: User message → Template application → Context injection → LLM processing → Response generation

- Design tradeoffs:
  - Template flexibility vs. attack success rate
  - Population diversity vs. convergence speed in LLMGA
  - Temperature setting vs. injection reliability

- Failure signatures:
  - Low RRR values indicating ineffective template
  - LLM rejecting messages due to special token detection
  - Inconsistent responses across different temperature settings

- First 3 experiments:
  1. Test basic template injection with known ChatML format on target LLM
  2. Run LLMGA optimization to find effective templates for the target model
  3. Evaluate attack success rates with different temperature settings and strategies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs be designed to distinguish between user inputs and context to prevent chat history tampering?
- Basis in paper: Inferred from the discussion on LLMs' inability to separate user inputs from context
- Why unresolved: The paper highlights this as a fundamental limitation of current LLM architectures, suggesting that developing a new architecture to manage inputs from different levels separately may be necessary
- What evidence would resolve it: Development and evaluation of a new LLM architecture that can effectively distinguish between user inputs and context, demonstrating resistance to chat history tampering attacks

### Open Question 2
- Question: How effective are safety training methods in preventing chat history tampering attacks?
- Basis in paper: Inferred from the discussion on safety training as a potential countermeasure
- Why unresolved: The paper suggests safety training as a foundational solution but does not evaluate its effectiveness against chat history tampering attacks
- What evidence would resolve it: Experiments comparing the susceptibility of LLMs with and without safety training to chat history tampering attacks, demonstrating the impact of safety training on attack success rates

## Limitations

- The findings assume current transformer architecture limitations and may not apply to future LLM designs with separate context channels
- The effectiveness of chat history tampering may vary across different LLM fine-tuning approaches and model families
- Temperature effects on context recognition may involve confounding factors not fully explored in the current analysis

## Confidence

**High Confidence**: The empirical results demonstrating chat history tampering effectiveness (up to 97% success on ChatGPT) are well-supported by the experiments and metrics used.

**Medium Confidence**: The mechanistic explanations for why chat history tampering works (separation limitations and semantic parsing) are logically sound but rely on assumptions about transformer architecture.

**Low Confidence**: The temperature parameter's role in context recognition effectiveness is demonstrated but not fully explained mechanistically.

## Next Checks

1. Test chat history tampering effectiveness across different LLM architectures (transformers vs. other architectures) to verify if the uniform text processing assumption holds universally.

2. Implement and test with models specifically fine-tuned for strict ChatML parsing to determine if semantic pattern recognition can be overridden by token-sequence dependence.

3. Conduct controlled experiments varying only temperature while holding other parameters constant to isolate its specific effect on context recognition and injection success rates.