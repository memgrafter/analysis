---
ver: rpa2
title: Linearizing Models for Efficient yet Robust Private Inference
arxiv_id: '2402.05521'
source_url: https://arxiv.org/abs/2402.05521
tags:
- relu
- adversarial
- accuracy
- images
- clean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes RLNet, a class of robust linearized networks\
  \ for efficient yet robust private inference. RLNet uses a shared-mask shared-weight\
  \ architecture with dual batch normalization to maintain high accuracy on clean,\
  \ naturally perturbed, and adversarial images while reducing ReLU operations by\
  \ up to 11.14\xD7."
---

# Linearizing Models for Efficient yet Robust Private Inference

## Quick Facts
- arXiv ID: 2402.05521
- Source URL: https://arxiv.org/abs/2402.05521
- Reference count: 17
- Primary result: RLNet achieves up to 47% higher adversarial accuracy while reducing ReLU operations by 11.14× versus state-of-the-art linearized models

## Executive Summary
This paper introduces RLNet, a class of robust linearized networks that enable efficient yet robust private inference. RLNet combines dual batch normalization with shared-mask shared-weight architecture to maintain high accuracy across clean, naturally perturbed, and adversarial images while significantly reducing computational overhead. The approach achieves what the authors call a "triple win ticket" - improving clean accuracy, naturally perturbed accuracy, and adversarial accuracy simultaneously through a three-stage pipeline involving robust teacher training, ReLU mask identification, and three-way robust distillation.

## Method Summary
RLNet uses a three-stage pipeline: first, a robust all-ReLU (AR) teacher model is trained using clean, augmented, and adversarial images with dual batch normalization; second, a ReLU mask is identified by measuring post-ReLU activation differences between AR and pruned-ReLU (PR) models; third, the PR model is fine-tuned using three-way robust distillation combining cross-entropy, KL divergence, and post-ReLU activation mismatch losses. The dual BN separates statistics for clean/augmented versus adversarial images, while the shared-mask shared-weight architecture allows a single model to switch between inference modes without additional parameters or computational overhead.

## Key Results
- RLNet outperforms state-of-the-art non-robust linearized models by up to 47% in adversarial accuracy
- Clean accuracy improves by up to 1.5% while naturally perturbed accuracy improves by up to 16.4%
- ReLU operations are reduced by up to 11.14× compared to baseline models
- RLNet achieves lower mean corruption error (mCE) across CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets

## Why This Works (Mechanism)

### Mechanism 1
Dual Batch Normalization with separate statistics for clean/augmented and adversarial images enables a single model to maintain high accuracy across all three domains without extra parameters. Clean and augmented images share one BN path (BNc), adversarial images use another (BNa). The distribution shift between clean/augmented and adversarial data is large enough that separating BN statistics prevents interference and preserves clean accuracy while still enabling adversarial robustness. Break condition: if distribution shift is small, shared BN may suffice.

### Mechanism 2
Conditional learning with shared-mask shared-weight architecture allows a single model to switch between clean/natural and adversarial inference modes while preserving performance. A binary ReLU mask freezes which units are active; dual BN branches correspond to modes. The model can operate in "normal mode" for clean/natural inputs and "adversarial mode" for attacked inputs without retraining separate models. Break condition: if ReLU pruning removes critical units for either mode, accuracy drops sharply.

### Mechanism 3
Robust distillation using KL divergence loss plus cross-entropy loss, combined with post-ReLU activation mismatch (PRAM) loss, preserves clean accuracy while transferring adversarial robustness from teacher to student. KL loss aligns clean predictions of student with teacher; CE loss preserves hard label learning; PRAM loss ensures feature similarity in post-ReLU activations between AR and PR models, reducing performance gap. Break condition: if teacher is not sufficiently robust, distillation cannot transfer needed adversarial robustness.

## Foundational Learning

- **Batch Normalization and its role in stabilizing training across different data distributions**: Why needed - dual BN paths rely on separate normalization statistics to handle distribution shift between clean/augmented and adversarial images. Quick check - What happens to clean accuracy if adversarial and clean BN statistics are mixed during training?

- **Knowledge distillation and KL divergence loss for transferring learned representations**: Why needed - distillation from robust teacher to pruned student preserves adversarial robustness while maintaining clean accuracy. Quick check - Why does using KL divergence on clean predictions help the student model generalize better than using only hard labels?

- **Adversarial training and perturbation generation (PGD, FGSM)**: Why needed - adversarial images are required for training robust teacher and student models that can withstand gradient-based attacks. Quick check - How does the choice of perturbation budget (epsilon) affect the trade-off between robustness and clean accuracy?

## Architecture Onboarding

- **Component map**: Input → Augmentation pipeline (AugMix) → Dual BN branches (clean/aug vs adversarial) → Shared ReLU mask → Convolutional layers → Classifier → Teacher model (All-ReLU baseline, robustified via adversarial training + dual BN) → Student model (Pruned-ReLU version, distilled from teacher using KL+CE+PRAM losses)

- **Critical path**: Data augmentation → dual BN selection → ReLU masking → feature extraction → classification. Mask search loop: measure activation difference → update mask → fine-tune with distillation.

- **Design tradeoffs**: ReLU count vs. accuracy - aggressive pruning improves latency but risks accuracy loss. BN separation vs. simplicity - dual BN helps robustness but doubles BN state; triple BN adds no benefit. Distillation temperature - higher favors clean accuracy, lower favors adversarial robustness.

- **Failure signatures**: Sudden drop in clean accuracy when switching BN paths → distribution shift handling broken. Mask instability → activation mismatch grows, distillation fails. Adversarial accuracy stalls despite strong teacher → insufficient KL loss weighting or PRAM mismatch.

- **First 3 experiments**: 1) Train dual BN teacher on CIFAR-10 with PGD-7, compare clean/CA/NPA/AdvA vs single BN baseline. 2) Generate ReLU mask with 10× reduction, run mask search, measure post-ReLU activation similarity to AR model. 3) Fine-tune PR model with KL+CE+PRAM, evaluate on clean, AugMix-augmented, and PGD-attacked test sets; compare to SeNet baseline.

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of RLNet models generalize to larger and more complex datasets beyond Tiny-ImageNet, such as ImageNet? The paper evaluates RLNet on CIFAR-10, CIFAR-100, and Tiny-ImageNet but does not explore larger datasets like ImageNet. Scaling to larger datasets involves challenges such as increased computational resources and potential changes in model architecture requirements. Conducting experiments on larger datasets like ImageNet and comparing performance metrics would provide insights into scalability and generalization.

### Open Question 2
Can RLNet's dual Batch Normalization approach be effectively adapted for vision transformer models, and what impact would this have on their efficiency and robustness? The paper mentions that exploring robustness of vision transformer models for latency-efficient private inference could be an interesting future research direction, indicating the potential applicability of RLNet's techniques to transformers. Vision transformers have different architectural characteristics compared to CNNs, and their robustness and efficiency under RLNet's dual BN approach are not yet explored.

### Open Question 3
What is the impact of varying the distillation temperature on the trade-off between clean accuracy and adversarial robustness in RLNet models? The paper discusses the role of distillation temperature in balancing clean accuracy and adversarial robustness, noting that a higher temperature favors clean accuracy while a lower temperature may enhance adversarial robustness. The optimal distillation temperature may vary depending on the specific dataset and model architecture, and a systematic study across different conditions is needed to generalize findings.

### Open Question 4
How does the choice of augmentation techniques in RLNet affect its robustness to different types of natural perturbations? The paper uses AugMix for data augmentation, which is effective against common image corruptions, but does not explore other augmentation techniques or their comparative effectiveness. Different augmentation techniques may have varying impacts on model robustness to specific types of natural perturbations, and a comparative analysis is lacking.

## Limitations
- The effectiveness depends on distribution shift between clean/augmented and adversarial data being sufficiently large to justify dual BN, which may not generalize to all domains
- ReLU pruning approach assumes removing units doesn't catastrophically degrade feature representation, but lacks theoretical bounds on safe pruning levels
- The distillation process relies on having a sufficiently robust teacher model, but the paper doesn't explore failure cases when teacher robustness is limited

## Confidence
- **High confidence**: Dual BN mechanism is well-supported by empirical evidence showing improved performance across all three accuracy metrics (CA, NPA, AdvA) compared to single BN baselines
- **Medium confidence**: Shared-mask shared-weight architecture's effectiveness is demonstrated but relies on assumptions about ReLU pruning not being fully validated theoretically
- **Medium confidence**: Distillation approach shows strong empirical results but the paper doesn't explore failure cases or sensitivity to teacher quality

## Next Checks
1. Test RLNet on datasets with smaller distribution shifts (e.g., CIFAR-10 with only Gaussian noise) to verify if dual BN still provides benefits or if shared BN suffices
2. Systematically vary the ReLU pruning ratio from 2× to 20× reduction and measure the point where accuracy degradation becomes unacceptable
3. Evaluate RLNet's robustness against adaptive attacks that specifically target the dual BN architecture or exploit the frozen ReLU mask structure