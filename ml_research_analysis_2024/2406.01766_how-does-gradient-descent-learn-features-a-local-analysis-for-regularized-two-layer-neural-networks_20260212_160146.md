---
ver: rpa2
title: How Does Gradient Descent Learn Features -- A Local Analysis for Regularized
  Two-Layer Neural Networks
arxiv_id: '2406.01766'
source_url: https://arxiv.org/abs/2406.01766
tags:
- lemma
- sign
- have
- neurons
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies feature learning in neural networks through
  a local convergence lens, focusing on the final training stage after initial feature
  learning has occurred. While prior work showed early-stage gradient training captures
  useful features, the authors prove that gradient descent can also recover exact
  ground-truth directions in the local convergence regime.
---

# How Does Gradient Descent Learn Features -- A Local Analysis for Regularized Two-Layer Neural Networks

## Quick Facts
- arXiv ID: 2406.01766
- Source URL: https://arxiv.org/abs/2406.01766
- Reference count: 40
- Primary result: Proves gradient descent with weight decay can recover exact ground-truth directions in the local convergence regime after initial feature learning

## Executive Summary
This paper studies feature learning in two-layer neural networks through a local convergence lens, focusing on the final training stage after initial feature learning has occurred. While prior work showed early-stage gradient training captures useful features, the authors prove that gradient descent can also recover exact ground-truth directions in the local convergence regime. The key insight is that weight decay enables descent directions even when neurons with opposite signs might cancel each other, and that a non-degenerate dual certificate analysis characterizes the structure of approximate minima.

## Method Summary
The paper analyzes a three-stage training algorithm: (1) Early feature learning with one gradient step to identify target subspace, (2) Norm adjustment via convex optimization to reach low loss, and (3) Local convergence with decreasing weight decay to recover ground-truth directions. The method uses a variant of gradient descent with weight decay operating on preprocessed Gaussian data, where the activation function is effectively σ≥₂ (ReLU minus linear term). The analysis shows that with polynomially many neurons (depending only on target function complexity), gradient descent recovers the target network within polynomial time, with neurons matching ground-truth directions.

## Key Results
- Proves gradient descent with weight decay converges when loss is below a threshold, capturing ground-truth directions
- Introduces novel techniques including dual certificate analysis and test functions to characterize local geometry
- Shows neurons align with ground-truth directions at convergence, going beyond subspace learning from early-stage analysis
- Demonstrates polynomial-time recovery of target network with width depending only on target function complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weight decay enables descent directions even when neurons with opposite signs might cancel each other
- Mechanism: The ℓ2 regularization term becomes an effective ℓ1 regularization on neuron norms when first and second layer norms are balanced, promoting sparsity and preventing norm cancellation between neurons with similar directions but opposite signs
- Core assumption: Weight decay is active throughout training and the algorithm maintains balanced norms between layers
- Evidence anchors:
  - [abstract]: "We show that once the loss is below a certain threshold, gradient descent with a carefully regularized objective will capture ground-truth directions"
  - [section 5]: "We use standard weight decay to address the above challenge. Specifically, weight decay helps us to...Reduce cancellation between close-by neurons"

### Mechanism 2
- Claim: Local convergence analysis shows gradient descent can recover exact ground-truth directions, not just subspaces
- Mechanism: When loss is below threshold O(λ²), the local landscape satisfies Lojasiewicz property, allowing gradient descent to make progress until optimality gap ζ is small, using average neurons and residual decomposition
- Core assumption: Initial loss is small enough (ζ ≤ O(λ⁹/⁵)) and width m ≥ m* (target network width)
- Evidence anchors:
  - [abstract]: "we prove that gradient descent with a carefully regularized objective will capture ground-truth directions"
  - [section 4.3]: "The following lemma shows that we could recover the target network within polynomial time using a multi-epoch gradient descent with decreasing weight decay"

### Mechanism 3
- Claim: Non-degenerate dual certificate analysis characterizes structure of approximate minima
- Mechanism: Dual certificate η(w) = Ex[p(x)σ≥₂(w⊤x)] satisfies η(wᵢ*) = sign(aᵢ*) and decays quadratically away from ground-truth directions, allowing bounding total norm of far-away neurons
- Core assumption: Teacher neurons are non-degenerate (∆-separated) and form low-dimensional subspace
- Evidence anchors:
  - [section 6.2]: "We use a dual certificate technique similar to Poon et al. (2023) to prove Lemma 8(i)"
  - [abstract]: "we consider a more natural but technically challenging case that second-layer can be positive and negative and using ReLU activation"

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) regime
  - Why needed here: The paper contrasts its feature learning regime with NTK where neurons don't move far from initialization, motivating the need for feature learning analysis
  - Quick check question: What is the key difference between feature learning and NTK regimes in terms of neuron movement?

- Concept: Hermite polynomial expansion
  - Why needed here: The analysis uses Hermite expansion to decompose loss function and activation functions, preprocessing data to remove 0th and 1st order terms
  - Quick check question: Why does preprocessing remove the 0th and 1st order Hermite terms from the activation function?

- Concept: Lojasiewicz property
  - Why needed here: Local convergence analysis relies on showing loss satisfies Lojasiewicz property, which guarantees gradient descent makes progress until reaching local minimum
  - Quick check question: What does it mean for a function to satisfy the Lojasiewicz property in context of optimization?

## Architecture Onboarding

- Component map: One gradient step (Stage 1) -> Convex norm adjustment (Stage 2) -> Multi-epoch gradient descent with decreasing weight decay (Stage 3)
- Critical path: Stage 1 → Stage 2 → Stage 3
  - Stage 1: One gradient step to identify target subspace
  - Stage 2: Least squares to adjust norms and reach low loss
  - Stage 3: Multi-epoch training with decreasing weight decay to recover ground-truth directions
- Design tradeoffs: Three-stage approach trades simplicity for theoretical guarantees, avoiding direct analysis of full training dynamics
- Failure signatures:
  - Convergence failure: If initial loss is too large or width insufficient
  - Suboptimal feature learning: If weight decay schedule is incorrect
  - Numerical instability: If step sizes are too large relative to problem scale
- First 3 experiments:
  1. Verify Stage 1 feature learning: Train for one gradient step and check if student neurons form ε₀-net of target subspace
  2. Test Stage 2 norm adjustment: Apply convex optimization and verify loss decreases to O(ε₀)
  3. Validate Stage 3 local convergence: Run multi-epoch training with decreasing weight decay and check if neurons align with ground-truth directions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can gradient descent learn features beyond the local convergence regime without weight decay regularization?
- Basis in paper: [explicit] The paper uses weight decay in Algorithm 1 and shows it prevents norm cancellation and helps neurons concentrate around ground-truth directions in Stage 3
- Why unresolved: The paper only analyzes local convergence with weight decay. Analyzing entire training dynamics beyond early and final stages remains open problem
- What evidence would resolve it: A theoretical proof showing gradient descent converges to ground-truth directions in intermediate training stages without weight decay, or empirical demonstration that such training fails to learn features

### Open Question 2
- Question: How many samples are required for polynomial-time feature learning with gradient descent?
- Basis in paper: [explicit] The paper focuses on population loss analysis and notes that finite-sample analysis is beyond scope, though they believe polynomial sample complexity is achievable
- Why unresolved: The paper omits sample complexity analysis, deferring it to future work. They only suggest that standard concentration tools might work
- What evidence would resolve it: A rigorous sample complexity bound (e.g., n = poly(d, r, m*, 1/ε)) for the full algorithm, or a lower bound showing super-polynomial sample complexity is necessary

## Limitations
- The theoretical analysis relies heavily on three-stage decomposition with Stage 2 serving as a "fast-forward" mechanism that may not be practical for real implementations
- The decreasing weight decay schedule and specific threshold parameters are not fully specified in the abstract, making experimental reproduction challenging
- The proof assumes Gaussian data and specific teacher network structures that may not generalize to more complex real-world distributions

## Confidence

**High confidence**: The mechanism of weight decay promoting sparsity and preventing norm cancellation is well-supported by the abstract and section 5 analysis

**Medium confidence**: The dual certificate analysis for characterizing approximate minima structure appears sound, though relies on strong assumptions about teacher neuron separation

**Medium confidence**: The local convergence guarantees assuming small initial loss are plausible but require careful parameter tuning

## Next Checks

1. Test the weight decay mechanism empirically by comparing convergence with and without regularization when neurons have similar directions but opposite signs
2. Verify the dual certificate construction by measuring neuron concentration around ground-truth directions as loss decreases below the threshold
3. Implement the three-stage algorithm with varying initial loss values to determine the practical threshold for local convergence regime