---
ver: rpa2
title: Investigating the effect of Mental Models in User Interaction with an Adaptive
  Dialog Agent
arxiv_id: '2408.14154'
source_url: https://arxiv.org/abs/2408.14154
tags:
- dialog
- system
- user
- mental
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores how users' mental models of task-oriented dialog
  systems affect interaction success and how adaptive dialog agents can better align
  with user expectations. The authors collected a new dataset of user interactions
  with three dialog systems (an adaptive CTS agent, a handcrafted dialog system, and
  an FAQ system) in the business travel domain.
---

# Investigating the effect of Mental Models in User Interaction with an Adaptive Dialog Agent

## Quick Facts
- arXiv ID: 2408.14154
- Source URL: https://arxiv.org/abs/2408.14154
- Authors: Lindsey Vanderlyn; Dirk Väth; Ngoc Thang Vu
- Reference count: 23
- Primary result: Adaptive dialog agents can implicitly align with user mental models to improve task success and usability without negatively impacting user expectations.

## Executive Summary
This paper investigates how users' mental models of task-oriented dialog systems affect interaction success and how adaptive dialog agents can better align with user expectations. The authors collected a new dataset of user interactions with three dialog systems (an adaptive CTS agent, a handcrafted dialog system, and an FAQ system) in the business travel domain. They found that users have varied and often contradictory mental models about how to interact with dialog systems and what type of responses they should expect. These mental models significantly impacted dialog success and usability, with mismatches between expectations and system behavior leading to poorer outcomes. The adaptive CTS agent was able to implicitly align its behavior with user mental models, resulting in higher success rates (77% vs. 43-57% for baselines) and improved usability.

## Method Summary
The study collected a new dataset (RDMM) of 188 dialogs from 63 participants interacting with three dialog systems in the business travel domain. Participants completed pre- and post-interaction surveys probing their mental models, then engaged in three dialog tasks (open, easy, hard goal) with one of the three systems. The adaptive CTS agent used reinforcement learning to navigate a dialog tree and adapt behavior based on user input patterns. Data was analyzed using content analysis of free-text responses and statistical analysis of objective (dialog success, length) and subjective (usability, trust) metrics.

## Key Results
- Users have varied and often contradictory mental models about how to interact with dialog systems
- Mismatches between user expectations and system behavior lead to poorer interaction outcomes
- The adaptive CTS agent achieved 77% task success rate compared to 43-57% for baseline systems
- Implicit adaptation improved usability ratings without negatively impacting mental models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Users' mental models significantly influence how they interact with dialog systems, with mismatches leading to lower success rates.
- **Mechanism:** Users form internal representations of how a dialog system works, and these models guide their input style and expectations. When the system's actual behavior does not match these expectations, interaction quality drops.
- **Core assumption:** Users have pre-existing mental models about dialog systems before engaging, and these models affect behavior.
- **Evidence anchors:**
  - [abstract] "mental models significantly impacted dialog success and usability, with mismatches between expectations and system behavior leading to poorer outcomes"
  - [section] "users have varied and often contradictory mental models about how to interact with dialog systems... These mental models significantly impacted dialog success and usability"
  - [corpus] Weak—no direct citations but thematic overlap with corpus neighbors exploring user expectations.

### Mechanism 2
- **Claim:** Implicit adaptation can improve dialog success by aligning system behavior with user mental models without negatively affecting user trust.
- **Mechanism:** The adaptive CTS agent infers user expectations from input patterns and adjusts its behavior (e.g., number of follow-up questions) to match those expectations, leading to higher task success and usability.
- **Core assumption:** System behavior can be inferred from user input without explicit feedback, and users will not notice or resent this adaptation.
- **Evidence anchors:**
  - [abstract] "The adaptive CTS agent was able to implicitly align its behavior with user mental models, resulting in higher success rates (77% vs. 43-57% for baselines)"
  - [section] "We find that the adaptive agent is able to implicitly adapt in a way that remains in line with user expectations... adapting in this way significantly improved objective evaluation metrics"
  - [corpus] Weak—corpus focuses on related dialog planning topics, not adaptation alignment specifically.

### Mechanism 3
- **Claim:** Different user expectations about input style and answer format require adaptive behavior; non-adaptive systems suffer when user expectations don't match their fixed behavior.
- **Mechanism:** Users may expect keyword input vs. natural language, specific vs. vague questions, direct answers vs. exploratory dialogs. Fixed systems (FAQ or HDC) fail when user expectations mismatch their interaction style.
- **Core assumption:** Users' input and expectation patterns are sufficiently varied to warrant adaptation rather than a one-size-fits-all design.
- **Evidence anchors:**
  - [abstract] "users have varied and often contradictory mental models... the adaptive CTS agent was able to implicitly align its behavior with user mental models"
  - [section] "We find that users have very different, and in many cases contradictory, expectations... the greatest impacts of mental models in cases where a mismatch existed between user expectations and dialog system behavior"
  - [corpus] Weak—corpus includes related work on mental models but not direct experimental comparison of input/expectation variance.

## Foundational Learning

- **Concept:** Mental models in HCI
  - **Why needed here:** Understanding what mental models are and how they influence user interaction is critical for interpreting the study's findings and designing adaptive systems.
  - **Quick check question:** What is a mental model, and how does it differ from actual system functionality?

- **Concept:** Reinforcement learning for dialog adaptation
  - **Why needed here:** The CTS agent uses RL to adapt behavior based on user input; understanding this mechanism is essential for reproducing or extending the work.
  - **Quick check question:** How does the CTS agent use RL to decide whether to output a node or skip it in the dialog tree?

- **Concept:** Evaluation of usability and trust in AI systems
  - **Why needed here:** The study uses standardized questionnaires (UMUX, TiA) to measure subjective experience; knowing these methods is key to interpreting results and designing follow-up studies.
  - **Quick check question:** What are the UMUX and TiA questionnaires, and what dimensions of user experience do they measure?

## Architecture Onboarding

- **Component map:** User interface -> Dialog systems (CTS adaptive agent, FAQ baseline, HDC baseline) -> RL policy -> Evaluation pipeline (surveys, logging, questionnaires) -> Data processing

- **Critical path:**
  1. User completes pre-survey (mental models)
  2. User interacts with assigned dialog system (3 dialogs: open, easy, hard goal)
  3. System logs dialog turns and outcomes
  4. User completes post-survey (mental models, usability, trust)
  5. Data is aggregated and analyzed for effects of mental models and adaptation

- **Design tradeoffs:**
  - Adaptive vs. non-adaptive: Adaptation improves success but adds complexity and potential unpredictability
  - Implicit vs. explicit adaptation: Implicit is less disruptive but may be less controllable
  - Survey vs. think-aloud: Surveys are scalable but may miss real-time mental model formation

- **Failure signatures:**
  - Low task success across all conditions: Possible issues with dialog tree design or user task clarity
  - No difference between adaptive and non-adaptive: Possible issues with adaptation signal detection or RL policy effectiveness
  - Negative changes in mental models post-interaction: Possible issues with system behavior being misaligned or surprising

- **First 3 experiments:**
  1. **Replicate baseline comparison:** Run a small-scale study with only FAQ and HDC systems to confirm that user mental models affect success.
  2. **Test adaptation sensitivity:** Vary the adaptation threshold in the CTS agent to see how much adaptation is needed for benefit.
  3. **Survey reliability check:** Pilot the pre/post mental model surveys with a new user group to ensure consistent interpretation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do mental models of task-oriented dialog systems evolve across multiple interactions with the same system?
- Basis in paper: [inferred] The study only examined mental models before and after a single interaction, but did not investigate how these models develop over time with repeated exposure to the same system.
- Why unresolved: The experimental design used a between-subjects approach where each participant only interacted with one system type, preventing analysis of mental model evolution within individuals.
- What evidence would resolve it: A longitudinal study tracking the same users across multiple interactions with the same system, measuring changes in their mental models and how these relate to interaction success over time.

### Open Question 2
- Question: To what extent do domain-specific knowledge and expertise influence the formation and impact of mental models in task-oriented dialog interactions?
- Basis in paper: [explicit] The authors note that participants had limited familiarity with business travel (1.9 on a 5-point Likert item) and discuss how this might affect their ability to determine if responses were correct, but do not systematically investigate this relationship.
- Why unresolved: The study controlled for domain familiarity statistically but did not explore how varying levels of expertise might differently affect mental model formation and interaction success.
- What evidence would resolve it: Experiments comparing mental model formation and interaction outcomes across users with systematically varied levels of domain expertise (novice to expert) interacting with the same dialog system.

### Open Question 3
- Question: How do different types of implicit adaptation (beyond follow-up question frequency) affect user mental models and interaction success in task-oriented dialog systems?
- Basis in paper: [explicit] The authors acknowledge they only explored one axis of adaptation (follow-up question frequency) and identified other potential adaptation dimensions like answer detail level and linguistic style based on user feedback.
- Why unresolved: The study focused exclusively on adapting the number of follow-up questions asked, while users expressed expectations about other aspects of system behavior that were not investigated.
- What evidence would resolve it: Comparative studies testing adaptive dialog systems that vary multiple dimensions simultaneously (follow-up frequency, answer detail, linguistic style) to determine which combinations best align with diverse user mental models.

## Limitations
- The study relies on self-reported mental models via surveys, which may not fully capture users' implicit expectations or real-time mental model formation during interaction.
- The adaptive CTS agent's behavior is inferred from input patterns, but the paper does not detail the RL policy's training or how adaptation signals are detected.
- The dataset (RDMM) is new but relatively small (188 dialogs from 63 participants), which may limit the generalizability of findings.

## Confidence

- **High confidence:** The finding that user mental models significantly impact dialog success and usability is well-supported by the results (77% success for adaptive vs. 43-57% for baselines) and aligns with established HCI principles.
- **Medium confidence:** The claim that implicit adaptation improves alignment with user expectations is plausible given the results, but the lack of detail on the RL policy and adaptation detection makes it harder to fully assess the mechanism.
- **Medium confidence:** The observation that different user expectations require adaptive behavior is supported, but the paper does not explore whether a well-designed static system could achieve similar results for users with shared expectations.

## Next Checks

1. **Replicate baseline comparison:** Run a small-scale study with only FAQ and HDC systems to confirm that user mental models affect success, ensuring the core mechanism is reproducible.
2. **Test adaptation sensitivity:** Vary the adaptation threshold in the CTS agent to see how much adaptation is needed for benefit, and whether over-adaptation harms user trust or perceived control.
3. **Survey reliability check:** Pilot the pre/post mental model surveys with a new user group to ensure consistent interpretation and capture of mental model variance.