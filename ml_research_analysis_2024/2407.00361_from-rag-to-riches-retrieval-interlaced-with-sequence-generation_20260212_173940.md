---
ver: rpa2
title: 'From RAG to RICHES: Retrieval Interlaced with Sequence Generation'
arxiv_id: '2407.00361'
source_url: https://arxiv.org/abs/2407.00361
tags:
- retrieval
- answer
- riches
- question
- passage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RICHES is a novel approach that interleaves retrieval with sequence
  generation, unifying both tasks in a single decoding pass. It uses constrained decoding
  to retrieve documents by directly decoding their contents from a predefined corpus,
  eliminating the need for separate retriever and generator components.
---

# From RAG to RICHES: Retrieval Interlaced with Sequence Generation

## Quick Facts
- **arXiv ID**: 2407.00361
- **Source URL**: https://arxiv.org/abs/2407.00361
- **Reference count**: 15
- **Primary result**: RICHES achieves +15 F1 points on Hotpot and +11 on Musique compared to iterative baselines while maintaining attribution quality

## Executive Summary
RICHES is a novel approach that interleaves retrieval with sequence generation by constraining the language model's decoding to directly retrieve documents from a predefined corpus. Unlike traditional RAG systems that separate retriever and generator components, RICHES performs both tasks in a single decoding pass using adaptive beam search and FM-index-based constraints. The method works with any instruction-tuned model without additional training and supports multi-hop retrievals, attributed evidence, and interleaved reasoning steps. Experiments on open-domain question answering tasks show strong performance improvements over iterative baselines while maintaining comparable or better attribution quality.

## Method Summary
RICHES unifies retrieval and generation by constraining the language model's decoding process to directly retrieve documents from a corpus. The approach uses FM-index to efficiently compute valid continuations at each decoding step, filtering out tokens that would lead to non-existent sequences. An adaptive beam search switches between full beam decoding for constrained sequences (exact matches) and greedy decoding for unconstrained sequences (flexible reasoning). Retrieval keys can be propositions, paragraphs, sentences, or titles, with propositions showing best performance. The method requires no additional training beyond few-shot prompting and can be applied to any instruction-tuned model. On ODQA tasks including Hotpot-QA and Musique-Ans, RICHES outperforms iterative baselines while maintaining attribution quality.

## Key Results
- Achieves +15 F1 points improvement on Hotpot-QA compared to iterative baselines
- Improves Musique-Ans performance by +11 F1 points
- Maintains comparable or better attribution quality (AutoAIS scores) versus traditional RAG systems
- Works with off-the-shelf instruction-tuned models without additional training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RICHES enables retrieval through direct decoding by constraining the search space to existing corpus sequences
- Mechanism: Uses FM-index to efficiently compute valid continuations at each decoding step, filtering out tokens leading to non-existent sequences
- Core assumption: Language model decoding can be treated as search over token sequences where corpus constraints guide retrieval
- Evidence anchors:
  - [abstract]: "It retrieves documents by directly decoding their contents, constrained on the corpus"
  - [section]: "Unlike a Trie structure, it is also highly space economical due to the compression. Given a prefix, FM-Index can efficiently compute the next allowed tokens in O(V ocab), independent of the corpus-size"
- Break condition: Fails when corpus constraints are too restrictive, preventing generation of valid retrieval keys

### Mechanism 2
- Claim: Adaptive beam size enables effective interleaving of constrained and unconstrained generation
- Mechanism: Switches between full beam decoding for constrained sequences and greedy decoding for unconstrained sequences
- Core assumption: Different generation modes require different decoding strategies - precision for retrieval keys and flexibility for reasoning
- Evidence anchors:
  - [section]: "By greedily decoding unconstrained sequences, the beam space is preserved for backtracking during document search"
  - [section]: "Once we have an adaptive beam in place, the insertion of keywords enhances both answer and retrieval performance"
- Break condition: Fails when adaptive switching is too aggressive, causing loss of precision or insufficient exploration

### Mechanism 3
- Claim: Propositional indexing provides better alignment with autoregressive decoding compared to raw text representations
- Mechanism: Propositions are decontextualized atomic units of information more efficiently generated and retrieved than full passages
- Core assumption: Compact, atomic representations are more suitable for autoregressive decoding than longer, context-dependent text spans
- Evidence anchors:
  - [section]: "A proposition is a stand-alone unit that efficiently encodes small, atomic chunks of factual information"
  - [corpus]: Table 3 shows proposition retrieval key hits@1 of 33.9 compared to 19.0-20.6 for other retrieval keys
- Break condition: Fails when propositions lose essential context or decomposition introduces errors

## Foundational Learning

- **FM-index and suffix arrays**
  - Why needed here: RICHES uses FM-index to efficiently compute valid continuations during constrained decoding without scanning entire corpus
  - Quick check question: How does FM-index achieve O(vocab) complexity for finding continuations regardless of corpus size?

- **Beam search and constrained decoding**
  - Why needed here: RICHES extends beam search with corpus constraints to explore multiple retrieval paths while maintaining search efficiency
  - Quick check question: What happens to beam search when constraints are too restrictive and eliminate all possible continuations?

- **Autoregressive generation and search**
  - Why needed here: RICHES treats language model decoding as search process where corpus constraints guide search toward valid retrieval keys
  - Quick check question: How does treating decoding as search enable integration of retrieval and generation in single pass?

## Architecture Onboarding

- **Component map**: FM-index -> Language model -> Adaptive decoder -> Proposition generator -> Constraint checker

- **Critical path**:
  1. Initialize FM-index with corpus
  2. Start decoding with model and corpus constraints
  3. At each step, query FM-index for valid continuations
  4. Apply constraints to model probabilities
  5. Select next token based on decoding mode
  6. Repeat until generation complete

- **Design tradeoffs**:
  - FM-index vs Trie: FM-index offers better space efficiency but may have slightly slower lookups
  - Beam size: Larger beams improve retrieval but increase computation
  - Proposition vs raw text: Propositions improve decoding efficiency but may lose context

- **Failure signatures**:
  - Model gets stuck with no valid continuations
  - Retrieval keys are generated incorrectly due to corpus misalignment
  - Unconstrained thoughts overwhelm beam space
  - Propositions fail to capture necessary context

- **First 3 experiments**:
  1. Implement basic constrained decoding with FM-index and test on small corpus
  2. Add adaptive beam switching and measure impact on retrieval accuracy
  3. Compare different retrieval key types (propositions vs passages) on validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RICHES performance scale with increasing corpus size beyond Wikipedia?
- Basis in paper: [inferred] Paper notes it "does not analyze how RICHES fares on corpora unseen during pre-training" and all experiments use Wikipedia
- Why unresolved: Experiments limited to Wikipedia, heavily seen during pre-training. Performance on truly unseen corpora unknown
- What evidence would resolve it: Experiments on diverse corpora of varying sizes and domains, particularly those not seen during pre-training

### Open Question 2
- Question: What is the computational overhead of RICHES compared to traditional RAG systems?
- Basis in paper: [explicit] Paper mentions "communication between the index on the host and the Transformer model on the GPU/TPU adds latency to the decoding step" and discusses indexing costs
- Why unresolved: Provides qualitative analysis but no quantitative comparisons of latency or compute costs
- What evidence would resolve it: Detailed benchmarks comparing inference time and computational resources required for RICHES versus traditional RAG systems

### Open Question 3
- Question: How does RICHES handle ambiguous or missing information in the corpus?
- Basis in paper: [inferred] Analysis section shows examples of "Index failure" and "Attribution failure" cases where model struggles with missing or ambiguous information
- Why unresolved: Paper identifies failure modes but does not provide solutions or mitigation strategies
- What evidence would resolve it: Experiments testing RICHES performance on corpora with intentionally introduced ambiguities or missing information, along with proposed solutions

## Limitations

- Reliance on quality and coverage of proposition-based retrieval keys - decomposition errors significantly impact retrieval accuracy
- FM-index constraint mechanism may become too restrictive with rare phrases or highly specific retrieval keys
- Adaptive beam search requires careful tuning of switching thresholds between constrained and unconstrained modes
- Performance depends on underlying language model's ability to generate appropriate retrieval keys

## Confidence

**High Confidence:**
- FM-index mechanism for efficient constrained decoding with O(vocab) complexity is well-established
- Integration of retrieval and generation in single decoding pass is technically feasible
- Proposition-based retrieval keys provide better performance than raw text representations

**Medium Confidence:**
- Claimed performance improvements (+15 F1 on Hotpot, +11 on Musique) supported by experimental results but sensitive to implementation details
- Attribution quality improvements are measurable but relative importance depends on application context
- Few-shot learning capability with instruction-tuned models generalizes across different QA tasks

**Low Confidence:**
- Scalability to extremely large corpora or different domain contexts remains untested
- Long-term stability and robustness of adaptive beam search parameters across diverse question types is uncertain
- Performance relative to traditional RAG systems in real-world deployment scenarios has not been established

## Next Checks

1. **Corpus Size Scaling Experiment**: Test RICHES on progressively larger Wikipedia subsets (1%, 10%, 100%) to measure how retrieval accuracy and decoding efficiency scale with corpus size, revealing whether FM-index maintains O(vocab) advantage and whether proposition quality degrades with corpus expansion.

2. **Cross-Domain Generalization Test**: Apply RICHES to non-Wikipedia corpus from different domain (e.g., scientific literature or medical texts) to evaluate whether proposition generation and retrieval mechanisms transfer effectively, validating method's robustness beyond training domain.

3. **Ablation Study on Adaptive Beam Parameters**: Systematically vary beam size thresholds and switching criteria in adaptive decoder to quantify impact on both retrieval accuracy and computational efficiency, identifying optimal parameter ranges and revealing whether adaptive strategy provides consistent benefits across different question types.