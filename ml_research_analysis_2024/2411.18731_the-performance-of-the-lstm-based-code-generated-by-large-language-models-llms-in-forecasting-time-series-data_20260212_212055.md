---
ver: rpa2
title: The Performance of the LSTM-based Code Generated by Large Language Models (LLMs)
  in Forecasting Time Series Data
arxiv_id: '2411.18731'
source_url: https://arxiv.org/abs/2411.18731
tags:
- relu
- llms
- data
- prompts
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the performance of four large language models
  (LLMs) - GPT-3.5-Turbo, Falcon, Llama-2, and PaLM - in generating deep learning-based
  time series forecasting models. The study systematically varies prompt sensitivity
  levels across four criteria (clarity/specificity, objective/intent, contextual information,
  and format/style) and tests different temperature configurations.
---

# The Performance of the LSTM-based Code Generated by Large Language Models (LLMs) in Forecasting Time Series Data

## Quick Facts
- arXiv ID: 2411.18731
- Source URL: https://arxiv.org/abs/2411.18731
- Reference count: 32
- Key outcome: LLM-generated LSTM models achieve comparable performance to manually crafted models for financial time series forecasting, with GPT-3.5-Turbo generally outperforming other LLMs

## Executive Summary
This paper evaluates four large language models (GPT-3.5-Turbo, Falcon, Llama-2, and PaLM) in generating deep learning-based time series forecasting models. The study systematically varies prompt sensitivity levels and temperature configurations to assess code generation quality. Results demonstrate that LLM-generated LSTM models perform comparably to manually optimized implementations, with key findings showing that simpler prompts and lower temperature settings often produce better results. The research makes deep learning more accessible to non-expert users by demonstrating that LLMs can generate executable LSTM code for time series analysis.

## Method Summary
The study uses daily financial time series data from January 2022 to April 2023 for stocks (AAPL, MSFT, AMZN, BABA, TSLA) and indices (GSPC, DJI, IXIC, N225, HSI). Four LLMs generate LSTM code through 11 prompts with varying sensitivity levels across four criteria: clarity/specificity, objective/intent, contextual information, and format/style. Temperature settings of 0.1 and 0.7 are tested. Generated models are evaluated using RMSE and compared against a manually crafted LSTM baseline with 1 layer, 50 units, ReLU activation, batch size 1, and 100 epochs.

## Key Results
- LLM-generated LSTM models achieve RMSE values comparable to manually crafted and optimized models
- GPT-3.5-Turbo consistently outperforms other LLMs in code generation quality
- Lower temperature settings (0.1) produce more accurate forecasting models than higher settings (0.7)
- Simpler prompts often generate better models than more complex, detailed prompts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generate functional LSTM code that performs comparably to manually optimized models for time series forecasting
- Mechanism: The LLM leverages its training on code and time series data to synthesize LSTM architectures with appropriate preprocessing, model compilation, and training loops
- Core assumption: The LLM has sufficient exposure to LSTM implementations and time series patterns during training
- Evidence anchors: "we are able to generate deep learning-based models with executable codes for each dataset separately whose performance are comparable with the manually crafted and optimized LSTM models"
- Break condition: If the LLM's training data lacked diverse LSTM implementations or the dataset structure differs significantly from training examples

### Mechanism 2
- Claim: Temperature parameter settings significantly affect the quality and relevance of generated code
- Mechanism: Temperature controls randomness in token selection, with lower values producing more deterministic, focused outputs
- Core assumption: The LLM's temperature setting directly influences the trade-off between creativity and coherence in code generation
- Evidence anchors: "the value of temperature parameter used in configuring LLMs has direct impact on whether simple or more complex prompts can generate more accurate forecasting models"
- Break condition: If dataset complexity or prompt structure overwhelms temperature parameter's influence

### Mechanism 3
- Claim: Prompt complexity and specificity have non-linear effects on model performance, with simpler prompts sometimes outperforming complex ones
- Mechanism: LLMs may struggle with overly complex prompts that contain ambiguous information, leading to suboptimal code generation
- Core assumption: The LLM's ability to parse and act on prompt information has limits
- Evidence anchors: "we did not observe a clear benefit of crafting more complex and detailed prompts in generating better and more accurate models"
- Break condition: If the prompt contains domain-specific terminology that the LLM needs to interpret correctly

## Foundational Learning

- Concept: LSTM architecture fundamentals (layers, units, activation functions, batch size, epochs)
  - Why needed here: Understanding these components is essential for evaluating and comparing LLM-generated models against manually optimized ones
  - Quick check question: What happens to model performance when increasing the number of LSTM units from 50 to 128?

- Concept: Temperature parameter in LLM configuration
  - Why needed here: Temperature directly affects the randomness and quality of generated code, requiring careful tuning for optimal results
  - Quick check question: How does lowering temperature from 0.7 to 0.1 typically affect the coherence of generated code?

- Concept: RMSE (Root Mean Square Error) as a performance metric
  - Why needed here: RMSE is the primary metric used to evaluate and compare the accuracy of forecasting models in this study
  - Quick check question: If Model A has RMSE of 0.01 and Model B has RMSE of 0.05, which model is more accurate and by what factor?

## Architecture Onboarding

- Component map: Data preprocessing -> Prompt engineering -> LLM configuration -> LSTM code generation -> Model training -> RMSE evaluation -> Performance comparison

- Critical path:
  1. Prepare dataset with appropriate preprocessing
  2. Craft prompt with controlled sensitivity levels
  3. Configure LLM parameters for optimal output
  4. Generate and execute LSTM code
  5. Evaluate performance using RMSE
  6. Compare against baseline manual model

- Design tradeoffs:
  - Simpler prompts vs. complex prompts: Simpler prompts may yield better results but provide less guidance
  - Temperature settings: Higher temperature increases diversity but risks incoherence
  - Batch size selection: Smaller batch sizes enable more thorough training but may increase overfitting risk
  - Number of units: More units increase model capacity but also computational cost and overfitting potential

- Failure signatures:
  - NA values in output indicating invalid or irrelevant code generation
  - RMSE values significantly worse than baseline manual model
  - Generated code that doesn't compile or run correctly
  - Models that overfit or underfit the training data

- First 3 experiments:
  1. Test baseline performance with simple prompt, temperature=0.1, max_tokens=1024
  2. Vary temperature from 0.1 to 0.7 while keeping other parameters constant
  3. Compare simple vs. complex prompts with fixed temperature setting to identify optimal prompt structure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different LLM-generated model architectures (particularly node count and batch size) affect forecasting accuracy across different financial sectors?
- Basis in paper: The paper explicitly compares LSTM architectures generated by different LLMs and identifies node count and batch size as key parameters affecting performance
- Why unresolved: The paper shows these parameters affect performance but doesn't systematically analyze how different financial sectors respond to these architectural variations
- What evidence would resolve it: A controlled experiment varying node count and batch size across multiple financial sector datasets

### Open Question 2
- Question: Does temperature parameter optimization vary by financial dataset complexity or volatility?
- Basis in paper: The paper shows temperature settings significantly impact model performance but doesn't analyze whether this relationship varies by dataset characteristics
- Why unresolved: While the paper demonstrates temperature affects all models, it doesn't explore whether more volatile datasets benefit from different temperature settings
- What evidence would resolve it: Analysis of temperature optimization results stratified by dataset volatility metrics

### Open Question 3
- Question: How do LLM-generated models compare to traditional statistical methods (ARIMA, etc.) specifically for short-term vs. long-term financial forecasting?
- Basis in paper: The paper mentions comparing LSTM to traditional models but doesn't explicitly analyze performance differences across forecast horizons
- Why unresolved: The paper focuses on general performance comparison but doesn't investigate whether LLMs have particular advantages for specific forecast time frames
- What evidence would resolve it: Systematic comparison across multiple forecast horizons (1-day, 1-week, 1-month)

## Limitations
- Findings based on single financial dataset from 2022, limiting generalizability to other domains
- No analysis of computational efficiency or resource requirements for LLM-generated models
- Limited exploration of optimal model architecture parameters beyond tested configurations
- Temperature and prompt effects may vary significantly across different LLMs or forecasting tasks

## Confidence
- High Confidence: LSTM code generation capability and RMSE as performance metric
- Medium Confidence: Temperature parameter effects and simple prompt advantages
- Medium Confidence: Comparative performance against manual models

## Next Checks
1. Test LLM-generated models across diverse time series datasets (e.g., energy, weather, retail) to verify generalizability beyond financial data
2. Implement cross-validation across multiple time periods to assess model robustness to changing market conditions
3. Conduct ablation studies varying architecture parameters (nodes, layers, batch size) systematically to identify optimal configurations for different data characteristics