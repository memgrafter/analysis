---
ver: rpa2
title: Enhance Reasoning for Large Language Models in the Game Werewolf
arxiv_id: '2402.02330'
source_url: https://arxiv.org/abs/2402.02330
tags:
- player
- werewolf
- game
- speech
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Thinker module that enhances LLM reasoning
  by separating tasks into System-1 (handled by LLMs) and System-2 (handled by Thinker).
  Using a 9-player Werewolf game, the framework processes language through a Listener,
  applies strategic reasoning via Thinker using imitation learning and RL, and generates
  speech via Presenter.
---

# Enhance Reasoning for Large Language Models in the Game Werewolf

## Quick Facts
- arXiv ID: 2402.02330
- Source URL: https://arxiv.org/abs/2402.02330
- Reference count: 40
- Primary result: Thinker module improves reasoning in Werewolf game, with fine-tuned 6B model surpassing GPT4

## Executive Summary
This paper introduces a Thinker module that enhances LLM reasoning by separating tasks into System-1 (handled by LLMs) and System-2 (handled by Thinker). Using a 9-player Werewolf game, the framework processes language through a Listener, applies strategic reasoning via Thinker using imitation learning and RL, and generates speech via Presenter. Experiments show Thinker improves voting accuracy and speech quality, with a fine-tuned 6B model surpassing GPT4. The paper also presents the largest social deduction game dataset to date.

## Method Summary
The framework processes language through a Listener, applies strategic reasoning via Thinker using imitation learning and RL, and generates speech via Presenter. The Thinker handles deductive reasoning, strategic planning, and domain-specific knowledge, while LLMs focus on natural language understanding and generation. The system is trained on 18,800 human game sessions and optimized through imitation learning, reinforcement learning, and population-based training.

## Key Results
- Thinker module improves voting accuracy and speech quality in 9-player Werewolf game
- Fine-tuned 6B LLM surpasses GPT4 performance
- Framework achieves Behavior Scores > 4.8 in online gameplay

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating System-1 (LLM) and System-2 (Thinker) reasoning improves performance in complex strategic games.
- Mechanism: The Thinker handles deductive reasoning, strategic planning, and domain-specific knowledge, while LLMs focus on natural language understanding and generation.
- Core assumption: System-1 tasks are best handled by LLMs, while System-2 tasks require specialized optimization beyond prompt engineering.
- Evidence anchors:
  - [abstract] states "framework forms a reasoning hierarchy where LLMs handle intuitive System-1 tasks... while the Thinker focuses on cognitive System-2 tasks"
  - [section 3.1] describes Thinker module as "cognitive core" handling "complex logical analysis, strategic planning, and a deep understanding of domain-specific environments"
  - [corpus] shows related work on LLM-based agents in Werewolf, indicating this separation addresses limitations in existing approaches
- Break condition: If System-1 and System-2 tasks cannot be cleanly separated, or if the communication protocol between LLM and Thinker becomes a bottleneck.

### Mechanism 2
- Claim: Language-based features and structured instructions improve the quality of generated speeches in Werewolf.
- Mechanism: The Listener extracts language features from speeches, the Thinker generates strategic instructions, and the Presenter creates coherent speeches based on these instructions.
- Core assumption: Structured communication between modules can capture the complexity of Werewolf speeches better than end-to-end generation.
- Evidence anchors:
  - [abstract] mentions "language-based features and instructions" and "Presenter ensures that the generated language is logical, rational, consistent"
  - [section 3.2] details how Listener "transforms information into structured language features" and Presenter "generates coherent and contextualized language output"
  - [corpus] shows related work on dialogue generation in Werewolf, indicating this structured approach addresses challenges in existing methods
- Break condition: If the structured features and instructions cannot capture all relevant aspects of speeches, or if the Presenter cannot generate natural-sounding output.

### Mechanism 3
- Claim: Combining imitation learning, reinforcement learning, and population-based training improves the Thinker's performance.
- Mechanism: The Thinker is trained using supervised learning on human data, RL with self-play, and population-based training to maintain diversity and prevent overfitting.
- Core assumption: A combination of learning methods can capture human-like reasoning and strategic behavior better than a single approach.
- Evidence anchors:
  - [abstract] states "train the Thinker using data from 18,800 human sessions and reinforcement learning"
  - [section 3.3] details the optimization phases: "imitation learning and RL" with "population-based training"
  - [corpus] shows related work on RL in Werewolf, indicating this combination addresses limitations in existing methods
- Break condition: If the combination of learning methods does not lead to improved performance, or if the training process becomes unstable.

## Foundational Learning

- Concept: Dual-process theory of reasoning (System-1 and System-2)
  - Why needed here: The paper explicitly separates reasoning into System-1 (intuitive) and System-2 (cognitive) tasks, mirroring this psychological theory
  - Quick check question: Can you explain the difference between System-1 and System-2 reasoning, and why this separation is beneficial in the Werewolf game context?

- Concept: Reinforcement learning from human feedback (RLHF)
  - Why needed here: The Thinker is trained using RL with rewards based on game outcomes and human-like behavior
  - Quick check question: How does RLHF differ from traditional RL, and why is it particularly useful for training the Thinker module?

- Concept: Language model fine-tuning on domain-specific data
  - Why needed here: The paper fine-tunes LLMs on a large dataset of Werewolf game sessions to improve performance
  - Quick check question: What are the benefits and risks of fine-tuning LLMs on domain-specific data, and how does this apply to the Werewolf game context?

## Architecture Onboarding

- Component map: Listener (language understanding) -> Thinker (strategic reasoning) -> Presenter (speech generation)
- Critical path: Listener extracts features from speeches, Thinker generates instructions based on features and game state, Presenter creates final speech output
- Design tradeoffs: Separation of concerns vs. communication overhead; specialized training vs. generality; rule-based vs. learned approaches
- Failure signatures: Inconsistent reasoning between modules, poor quality of generated speeches, slow response times due to inter-module communication
- First 3 experiments:
  1. Evaluate the Listener's feature extraction accuracy on a held-out set of speeches
  2. Test the Thinker's reasoning capabilities in a simplified game environment
  3. Assess the Presenter's speech generation quality with human evaluators

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Thinker module's reasoning processes be made more interpretable and transparent to humans?
- Basis in paper: [explicit] The paper states "Future work could explore methods for further improving the interpretability and transparency of our framework" and introduces identity prediction as an attempt to reveal the Thinker's reasoning.
- Why unresolved: The paper only provides a basic identity prediction task as a form of interpretability, which may not be sufficient to fully understand the complex reasoning processes in the Thinker.
- What evidence would resolve it: Developing and evaluating additional interpretability methods for the Thinker, such as visualizing its decision-making process or generating human-readable explanations for its actions and speech instructions.

### Open Question 2
- Question: Can the language features and speech instructions used for communication between LLMs and the Thinker be made more generalizable to other tasks and domains?
- Basis in paper: [explicit] The paper mentions "Future work will aim to develop more generalized and flexible methods, e.g., data-driven approaches" and acknowledges that the current communication format may not be directly transferable.
- Why unresolved: The paper uses a task-specific communication protocol tailored to the Werewolf game, which may not be suitable for other applications without significant modifications.
- What evidence would resolve it: Conducting experiments to evaluate the performance of the framework on other tasks and domains, and developing more flexible and generalizable communication methods based on the findings.

### Open Question 3
- Question: How would the framework perform in a game setting with a majority of human players and only one AI player?
- Basis in paper: [explicit] The paper discusses the challenges of evaluating the AI in a majority-human player setting and states "Evaluating an AI in a majority-human player setting presents challenges due to the highly interactive nature of the game and the variability in human players' speech strategies and behaviors."
- Why unresolved: The current evaluations primarily involve games with either AI vs AI or one human player competing against multiple AIs, leaving the performance in a majority-human setting unexplored.
- What evidence would resolve it: Conducting extensive evaluations of the framework in games with a majority of human players and one AI player, analyzing the AI's performance, human perceptions, and the overall game dynamics.

## Limitations
- Framework appears highly specialized for 9-player Werewolf configuration, limiting generalizability
- Lack of detailed ablation studies showing individual contributions of each module
- Performance against recent or differently architected models remains unclear

## Confidence
- System-1/System-2 separation effectiveness: Medium-High
- Thinker module performance gains: Medium
- Dataset scalability and impact: Medium
- Online gameplay results: Medium

## Next Checks
1. **Ablation Study Validation**: Systematically disable individual components (Listener, Thinker, Presenter) to quantify their specific contributions to overall performance, measuring the degradation in voting accuracy and speech quality.

2. **Cross-Game Generalization**: Test the Thinker module's performance in modified Werewolf configurations (different player counts, rule variations) and potentially in other social deduction games to assess adaptability.

3. **Longitudinal Performance Tracking**: Monitor the system's performance over extended gameplay sessions to evaluate whether the Thinker maintains its reasoning quality and whether it can adapt to evolving player strategies.