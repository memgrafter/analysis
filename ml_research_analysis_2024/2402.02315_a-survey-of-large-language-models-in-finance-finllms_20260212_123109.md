---
ver: rpa2
title: A Survey of Large Language Models in Finance (FinLLMs)
arxiv_id: '2402.02315'
source_url: https://arxiv.org/abs/2402.02315
tags:
- financial
- tasks
- llms
- language
- finllms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first comprehensive survey of Financial
  Large Language Models (FinLLMs), covering their evolution from general-domain models
  to financial-specific applications. The authors review five key techniques used
  in FinLLMs, including continual pre-training, domain-specific pre-training, mixed-domain
  pre-training, and instruction fine-tuning.
---

# A Survey of Large Language Models in Finance (FinLLMs)

## Quick Facts
- arXiv ID: 2402.02315
- Source URL: https://arxiv.org/abs/2402.02315
- Authors: Jean Lee; Nicholas Stevens; Soyeon Caren Han; Minseok Song
- Reference count: 6
- Key outcome: First comprehensive survey of Financial Large Language Models (FinLLMs), covering evolution, techniques, benchmark tasks, and challenges in financial NLP applications.

## Executive Summary
This survey provides the first comprehensive examination of Financial Large Language Models (FinLLMs), tracing their development from general-domain language models to specialized financial applications. The authors systematically review five key techniques used in FinLLMs, including continual pre-training, domain-specific pre-training, mixed-domain pre-training, and instruction fine-tuning. They evaluate performance across six benchmark tasks and identify both opportunities and challenges in the field, while providing practical resources through a compiled GitHub repository of datasets and benchmarks.

## Method Summary
The paper reviews the evolution of financial language models through systematic analysis of four financial PLMs and four FinLLMs, examining five key techniques used across these models. The authors analyze performance evaluations on six benchmark tasks (sentiment analysis, text classification, NER, QA, stock movement prediction, text summarization) and explore eight advanced financial NLP tasks with associated datasets. They compile findings into accessible resources while discussing opportunities and challenges, though specific implementation details and hyperparameter configurations remain unspecified.

## Key Results
- FinLLMs show promise but task-specific models still outperform them in complex financial tasks like stock movement prediction and text summarization
- GPT-4 demonstrates impressive performance across all benchmarks except summarization tasks
- Key challenges include data privacy, model hallucination, and the need for financial expert evaluation
- The survey compiles a GitHub repository with datasets and benchmarks to support future FinLLM research

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FinLLMs leverage domain-specific fine-tuning on financial corpora to improve task performance over general LLMs.
- Mechanism: Continual pre-training or domain-specific pre-training on large financial datasets enables the model to learn specialized financial language patterns, terminology, and context.
- Core assumption: The quality and quantity of financial data directly influence the model's ability to generalize to financial NLP tasks.
- Evidence anchors:
  - [abstract] "The authors review five key techniques used in FinLLMs, including continual pre-training, domain-specific pre-training, mixed-domain pre-training, and instruction fine-tuning."
  - [section] "Continual pre-training of LMs aims to train an existing general LM with new domain-specific data on an incremental sequence of tasks."
  - [corpus] Found related papers such as "FinLoRA: Finetuning Quantized Financial Large Language Models Using Low-Rank Adaptation" suggesting active research in financial domain adaptation.
- Break condition: Insufficient or noisy financial data, or poor alignment between financial domain tasks and model pre-training objectives.

### Mechanism 2
- Claim: Instruction fine-tuning improves the controllability and task-specific capabilities of FinLLMs.
- Mechanism: Additional training using explicit text instructions allows the model to better understand task requirements and generate more accurate responses for financial queries.
- Core assumption: Well-constructed instruction datasets can effectively guide the model to perform complex financial reasoning tasks.
- Evidence anchors:
  - [abstract] "The authors review five key techniques... including instruction fine-tuning."
  - [section] "Instruction tuning is the additional training of LLMs using explicit text instructions to enhance the capabilities and controllability of LLMs."
  - [corpus] Related paper "FinLLMs: A Framework for Financial Reasoning Dataset Generation with Large Language Models" indicates focus on instruction-based dataset creation.
- Break condition: Poorly constructed instruction datasets or misalignment between instructions and target financial tasks.

### Mechanism 3
- Claim: Prompt engineering enables zero-shot or few-shot task performance on financial tasks without model retraining.
- Mechanism: Users provide task descriptions and examples in natural language, allowing the model to adapt its responses based on context provided in the prompt.
- Core assumption: The underlying LLM has sufficient capacity to understand and generalize from the prompt context to unseen financial tasks.
- Evidence anchors:
  - [abstract] "Mixed-domain LLMs with prompt engineering and instruction fine-tuned LLMs with prompt engineering."
  - [section] "Mixed-domain LLMs are trained on both a large general corpus and a large domain-specific corpus. Then, users describe the task and optionally provide a set of examples in human language. This technique is called Prompt Engineering..."
  - [corpus] Evidence is limited in corpus, indicating prompt engineering is more of a practical application than a deeply studied mechanism in this survey.
- Break condition: Prompts that are too ambiguous, too task-specific, or poorly formatted for the model to interpret correctly.

## Foundational Learning

- Concept: Pre-trained Language Models (PLMs) and their evolution to Large Language Models (LLMs)
  - Why needed here: Understanding the foundational architecture and scaling principles that enable FinLLMs to perform well on financial tasks.
  - Quick check question: What architectural difference distinguishes a PLM like BERT from an LLM like GPT-3?

- Concept: Domain adaptation techniques (continual pre-training, instruction fine-tuning)
  - Why needed here: These techniques are central to how FinLLMs achieve superior performance on financial tasks compared to general LLMs.
  - Quick check question: How does continual pre-training differ from training a model from scratch on domain-specific data?

- Concept: Evaluation metrics for NLP tasks (F1-score, accuracy, ROUGE, BERTScore)
  - Why needed here: Proper evaluation is critical to assess FinLLM performance and compare against task-specific models.
  - Quick check question: Why might financial experts need to be involved in evaluating FinLLM outputs beyond standard NLP metrics?

## Architecture Onboarding

- Component map:
  Base LLM backbone -> Pre-training phase (general + financial corpora) -> Fine-tuning phase (instruction tuning or task-specific) -> Prompt engineering interface -> Evaluation pipeline (benchmarks + expert review)

- Critical path:
  1. Load pre-trained LLM
  2. Pre-train on financial corpus (optional)
  3. Fine-tune on financial instruction dataset
  4. Validate on benchmark tasks
  5. Deploy with prompt engineering support

- Design tradeoffs:
  - Model size vs. inference cost and latency
  - General vs. domain-specific pre-training data
  - Instruction tuning vs. task-specific fine-tuning
  - Open-source vs. proprietary models (data privacy, customization)

- Failure signatures:
  - Poor performance on numerical reasoning tasks
  - Hallucinations or factually incorrect outputs
  - Overfitting to training data (lack of generalization)
  - High inference latency or memory usage

- First 3 experiments:
  1. Fine-tune LLaMA-7B on the FiQA sentiment analysis dataset and evaluate on FPB.
  2. Apply instruction tuning using the Financial Instruction Tuning (FIT) dataset and test on ConvFinQA.
  3. Implement prompt engineering for stock movement prediction on StockNet and compare against fine-tuned baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do FinLLMs outperform task-specific models in complex financial tasks like stock movement prediction and text summarization?
- Basis in paper: [explicit] The paper states that task-specific models still outperform FinLLMs in complex financial tasks like stock movement prediction and text summarization, with GPT-4 showing impressive performance across all benchmarks except the summarization task.
- Why unresolved: While the paper provides some performance comparisons, it does not offer a definitive conclusion on whether FinLLMs can consistently outperform task-specific models across all complex financial tasks. Further research and evaluation are needed to establish the superiority of FinLLMs in these domains.
- What evidence would resolve it: Conducting comprehensive experiments comparing the performance of FinLLMs and task-specific models on a wide range of complex financial tasks, using standardized benchmarks and evaluation metrics, would provide evidence to resolve this question.

### Open Question 2
- Question: How can the challenges of data privacy, model hallucination, and the need for financial expert evaluation be addressed in FinLLMs?
- Basis in paper: [explicit] The paper highlights these challenges as key obstacles in the development and deployment of FinLLMs.
- Why unresolved: While the paper discusses these challenges, it does not provide specific solutions or strategies to overcome them. Further research and innovation are needed to develop techniques and approaches that can effectively address these issues.
- What evidence would resolve it: Demonstrating successful implementation of FinLLMs in real-world financial applications while effectively addressing data privacy, model hallucination, and the need for financial expert evaluation would provide evidence to resolve this question.

### Open Question 3
- Question: What are the optimal techniques and architectures for developing advanced FinLLMs that can handle complex financial NLP tasks and multimodal data?
- Basis in paper: [inferred] The paper discusses various techniques used in FinLLMs, including continual pre-training, domain-specific pre-training, mixed-domain pre-training, and instruction fine-tuning. However, it does not provide a definitive answer on the optimal techniques and architectures for advanced FinLLMs.
- Why unresolved: The field of FinLLMs is still evolving, and there is no consensus on the best techniques and architectures for developing advanced models. Further research and experimentation are needed to identify the most effective approaches.
- What evidence would resolve it: Conducting comparative studies evaluating the performance of different techniques and architectures on a range of complex financial NLP tasks and multimodal data would provide evidence to resolve this question.

## Limitations

- Performance comparisons rely on reported results without independent verification, potentially affected by undisclosed training differences
- Current benchmarks may not fully capture real-world financial application complexity, particularly for advanced tasks
- Rapid field evolution means techniques and models may become outdated quickly, requiring regular updates

## Confidence

**High Confidence**: Fundamental mechanisms of FinLLM development are well-established and supported by extensive literature; survey's identification of key challenges aligns with broader LLM research findings.

**Medium Confidence**: Performance claims comparing FinLLMs to task-specific models are based on reported results rather than direct experimentation; effectiveness for advanced financial tasks remains somewhat speculative.

**Low Confidence**: Predictions about future developments and opportunities are necessarily speculative given the field's rapid evolution and unknown future technical breakthroughs.

## Next Checks

1. **Independent Benchmark Reproduction**: Select 2-3 key FinLLM models from the survey and attempt to reproduce their benchmark performance on standard financial NLP tasks using publicly available implementations and datasets.

2. **Real-World Task Validation**: Design and execute a pilot study evaluating FinLLM performance on a complex, multi-step financial analysis task that requires both numerical reasoning and domain knowledge, comparing against both general LLMs and task-specific models.

3. **Hallucination Audit**: Systematically test a representative FinLLM on financial factuality using domain-specific verification datasets, measuring hallucination rates for numerical data, entity relationships, and causal reasoning compared to general LLMs.