---
ver: rpa2
title: 'Non-native speakers of English or ChatGPT: Who thinks better?'
arxiv_id: '2412.00457'
source_url: https://arxiv.org/abs/2412.00457
tags:
- language
- chatgpt
- llms
- human
- linguistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares the ability of non-native speakers of English
  (NNSs) and ChatGPT in processing and interpreting center-embedding sentences. 15
  NNSs (BA, MA, and PhD students) were tested alongside ChatGPT-3.5 with a complex
  sentence containing nested clauses.
---

# Non-native speakers of English or ChatGPT: Who thinks better?
## Quick Facts
- arXiv ID: 2412.00457
- Source URL: https://arxiv.org/abs/2412.00457
- Reference count: 8
- NNSs achieved 73.3% accuracy vs ChatGPT-3.5's 6.7% on center-embedding sentence processing

## Executive Summary
This study compares the ability of non-native speakers of English (NNSs) and ChatGPT-3.5 in processing and interpreting center-embedding sentences. Fifteen NNSs (BA, MA, and PhD students) were tested alongside ChatGPT with a complex sentence containing nested clauses. Results showed NNSs performed significantly better, with 73.3% correct responses compared to ChatGPT's 6.7%. The study concludes that human cognitive capacity, even in non-native speakers, surpasses current AI language models in processing complex linguistic structures.

## Method Summary
The study tested 15 non-native English speakers (advanced level) and ChatGPT-3.5 on their ability to process a center-embedding sentence: "The man that the soldier that the thief slapped deceived died." Participants were asked to identify what each entity (man, soldier, thief) did in the sentence. Responses were collected from human participants and ChatGPT-3.5 using the same prompt, then compared for accuracy.

## Key Results
- NNSs achieved 73.3% correct responses on the center-embedding task
- ChatGPT-3.5 achieved only 6.7% correct responses
- NNSs' errors were attributed to working memory limitations rather than structural misunderstanding
- ChatGPT's failures were characterized as fundamental misunderstanding of sentence structure

## Why This Works (Mechanism)
### Mechanism 1
- Claim: NNSs outperform ChatGPT on center-embedding because they rely on built-in grammatical competence, while ChatGPT uses statistical prediction.
- Mechanism: Human FL/UG-based parsing can still resolve nested clause structures even with memory load; ChatGPT's token-by-token prediction fails to capture global hierarchical dependencies.
- Core assumption: NNSs have acquired English syntactic competence comparable to natives.
- Evidence anchors:
  - [abstract] "human brain's ability to process and interpret natural language data is unique and that ChatGPT still lags behind"
  - [section] "LLMs base their predictions purely on statistical likelihood... not always error-free"
  - [corpus] weak: no direct citations, only qualitative contrast
- Break condition: If a new model were explicitly trained with syntactic hierarchy constraints, this advantage might shrink.

### Mechanism 2
- Claim: ChatGPT's errors are competence-based (model architecture), while NNS errors are performance-based (working memory overload).
- Mechanism: ChatGPT's failure to parse center-embedding reflects its inability to represent unbounded recursion; NNS errors arise from temporary resource constraints but underlying competence remains intact.
- Core assumption: LLMs lack the recursive capacity baked into human FL.
- Evidence anchors:
  - [section] "LLMs may also lack 'competence' in the linguistic technical sense"
  - [section] "humans' failures are attributed to performance limitations rather than a lack of understanding of syntactic rules"
  - [corpus] weak: relies on theoretical distinction, not empirical model internals
- Break condition: If performance errors in NNSs were systematic and structural, the distinction would blur.

### Mechanism 3
- Claim: LLMs are poor language theories because they cannot distinguish grammaticality from likelihood.
- Mechanism: Human speakers can identify a sentence as grammatical even if unlikely; LLMs conflate the two because they only track co-occurrence frequencies.
- Core assumption: Competence entails separating syntax from probability.
- Evidence anchors:
  - [section] "LLMs’ either successes or failures are based on their statistical training and inherent model structure"
  - [section] "humans can discern grammatical but unlikely sentences from likely but ungrammatical ones"
  - [corpus] weak: no direct empirical contrast in model outputs
- Break condition: If a model were trained to explicitly encode syntactic constraints, it could start making the distinction.

## Foundational Learning
- Concept: Center-embedding and working memory limits
  - Why needed here: Explains why even humans struggle with the test sentence, so outperforming ChatGPT is meaningful.
  - Quick check question: Why does a sentence like "The man that the soldier that the thief slapped deceived died" tax working memory?
- Concept: Competence vs. performance in generative linguistics
  - Why needed here: Underpins the argument that NNS errors are not competence failures.
  - Quick check question: What is the difference between a competence error and a performance error in language use?
- Concept: Statistical vs. rule-based language models
  - Why needed here: Clarifies why ChatGPT's architecture is fundamentally different from human language faculty.
  - Quick check question: How does an n-gram model predict the next word differently from a syntactic parser?

## Architecture Onboarding
- Component map: Input sentence → working-memory parse (humans) vs. token-probability decode (ChatGPT) → output response
- Critical path: For NNSs: parse nested clauses → map roles → answer; for ChatGPT: predict next token → potentially lose hierarchical info → produce error
- Design tradeoffs: Human FL trades off strict memory limits for universal syntactic rules; LLMs trade off memory for probabilistic coverage but lose recursive structure
- Failure signatures: NNS: occasional role-swapping or omissions; ChatGPT: nonsensical extra clauses, complete misunderstanding of structure
- First 3 experiments:
  1. Test NNSs and ChatGPT on sentences with 2 vs. 3 levels of embedding to see if error rates scale differently
  2. Add time pressure to NNSs to see if performance errors increase while ChatGPT errors remain constant
  3. Fine-tune a small LLM on a corpus of grammatical-but-unlikely sentences and test whether it can distinguish them from ungrammatical ones

## Open Questions the Paper Calls Out
- Open Question 1: Does the performance gap between NNSs and ChatGPT persist with more complex center-embedding structures (e.g., four or more levels of embedding)?
  - Basis in paper: [explicit] The paper notes that NNSs performed better than ChatGPT on a triple center-embedding sentence, and suggests testing more complex structures as a future research direction.
  - Why unresolved: The study only tested one triple center-embedding sentence. The paper suggests that more complex structures could reveal greater differences in performance.
  - What evidence would resolve it: Testing NNSs and ChatGPT on sentences with four or more levels of embedding, measuring accuracy rates for both groups.

- Open Question 2: How does ChatGPT's performance on center-embedding structures compare to its performance on other complex linguistic phenomena like DP-islands or multiple wh-questions?
  - Basis in paper: [explicit] The paper mentions that future research could test ChatGPT on other complex structures like DP-islands, multiple wh-questions, anaphora, and weak/strong crossovers.
  - Why unresolved: The study only tested center-embedding structures. The paper suggests that ChatGPT's performance might vary across different types of complex linguistic phenomena.
  - What evidence would resolve it: Testing ChatGPT on various complex linguistic structures and comparing its accuracy rates across different types of phenomena.

- Open Question 3: Would ChatGPT-4 show improved performance on center-embedding structures compared to ChatGPT-3.5?
  - Basis in paper: [explicit] The paper mentions that future research could use ChatGPT-4, which is said to be more developed in functionality and features, to test its performance on center-embedding structures.
  - Why unresolved: The study only used ChatGPT-3.5. The paper suggests that ChatGPT-4 might have improved capabilities due to its more advanced development.
  - What evidence would resolve it: Testing both ChatGPT-3.5 and ChatGPT-4 on the same center-embedding structures and comparing their performance.

## Limitations
- Small sample size (n=15) limits generalizability
- Single sentence type and task restricts conclusions about broader language processing
- Participants from one institution with unspecified language backgrounds

## Confidence
- **High confidence**: The empirical finding that ChatGPT-3.5 performed poorly (6.7% accuracy) compared to NNSs (73.3%) on this specific task is well-supported by the reported data
- **Medium confidence**: The interpretation that NNS errors reflect performance limitations while ChatGPT errors reflect competence limitations relies on theoretical assumptions about the nature of LLM processing that are not directly tested
- **Low confidence**: The broader claim that "human cognitive capacity... surpasses current AI language models in processing complex linguistic structures" overgeneralizes from one sentence type and one model version to all language processing

## Next Checks
1. Replicate with ChatGPT-4 and other recent LLM architectures to determine if performance differences persist across model versions
2. Test with multiple center-embedding sentence types and varying depths to assess whether error patterns scale systematically for both humans and AI
3. Conduct a controlled experiment varying working memory load on NNSs to determine whether their errors truly follow performance rather than competence patterns