---
ver: rpa2
title: 'SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 13 Languages'
arxiv_id: '2402.08638'
source_url: https://arxiv.org/abs/2402.08638
tags:
- data
- sentence
- language
- pairs
- pair
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SemRel2024 introduces a new semantic textual relatedness dataset
  collection for 13 languages, including low-resource African and Asian languages
  from five distinct language families. The datasets contain sentence pairs annotated
  by native speakers using a comparative annotation framework to obtain fine-grained
  relatedness scores.
---

# SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 13 Languages

## Quick Facts
- arXiv ID: 2402.08638
- Source URL: https://arxiv.org/abs/2402.08638
- Reference count: 40
- Primary result: New semantic textual relatedness dataset collection for 13 languages, including low-resource African and Asian languages from five distinct language families

## Executive Summary
SemRel2024 introduces a comprehensive semantic textual relatedness dataset collection spanning 13 languages, with a focus on low-resource African and Asian languages from five distinct language families. The datasets were created using a comparative annotation framework (Best-Worst Scaling) to obtain fine-grained relatedness scores from native speakers. The data collection process addressed challenges specific to low-resource languages, including data scarcity and diversity. Baseline experiments were conducted across monolingual and crosslingual settings, demonstrating the potential of these datasets for evaluating semantic relatedness methods and improving NLP tasks.

## Method Summary
The SemRel2024 datasets were created through a multi-stage process. First, sentence pairs were generated from various text sources using five different pairing heuristics: lexical overlap, contiguity, similarity, paraphrases, and random selection. These pairs were then annotated by native speakers using Best-Worst Scaling (BWS), where annotators compared four sentence pairs at a time and identified the most and least related pairs. This process generated ordinal rankings that were converted into fine-grained relatedness scores. The datasets underwent quality control measures including split-half reliability testing and were anonymized before release. Baseline experiments evaluated the datasets using lexical overlap, LaBSE, multilingual and monolingual BERT-based models in supervised, unsupervised, and crosslingual settings.

## Key Results
- Successfully created semantic textual relatedness datasets for 13 languages spanning five distinct language families
- Achieved split-half reliability scores ranging from 0.64 to 0.96 across languages, indicating high annotation quality
- Demonstrated that lexical overlap baseline correlates with gold scores at 0.74, while LaBSE achieves 0.80 in unsupervised settings
- Made datasets publicly available as part of a shared task to promote research in semantic relatedness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Comparative annotation using Best-Worst Scaling (BWS) produces more reliable fine-grained relatedness scores than traditional rating scales.
- Mechanism: BWS requires fewer comparisons than full pairwise annotation (N×N vs ~2×N for 4-tuple comparisons), while still generating reliable ordinal rankings through relative judgments.
- Core assumption: Annotators can reliably judge relative relatedness between pairs of sentences within 4-tuples.
- Evidence anchors:
  - [abstract]: "The scores are obtained using a comparative annotation framework" and "BWS is known to avoid common limitations of traditional rating scale annotation methods"
  - [section 3.2]: "BWS requires fewer labels (Louviere and Woodworth, 1991), in our case, given four instances... if p0 is marked as most related and p3 as least related, then we know that p0 > p1, p0 > p2, p0 > p3, p1 > p3, and p2 > p3"
  - [corpus]: Weak - corpus shows high split-half reliability (SHR) scores (0.64-0.96) but doesn't directly compare BWS vs rating scales
- Break condition: If annotators cannot consistently make relative judgments within tuples, or if the semantic boundaries of "relatedness" vary too widely across cultures/languages.

### Mechanism 2
- Claim: Using multiple data curation heuristics (lexical overlap, contiguity, paraphrases, etc.) ensures a diverse range of relatedness scores across the dataset.
- Mechanism: Different pairing methods target different ranges of relatedness - lexical overlap captures medium-high scores, random selection captures low scores, paraphrases capture high scores, and contiguity captures medium scores.
- Core assumption: The chosen heuristics will produce pairs with the expected range of relatedness values when annotated.
- Evidence anchors:
  - [section 3.1.1]: "We paired sentences mainly based on five methods previously defined by Abdalla et al. (2023)" and describes each method's expected contribution to score distribution
  - [section 3.1.2]: Shows language-specific application of these heuristics (e.g., "For Amharic, we paired sentences present in news articles" using lexical overlap)
  - [corpus]: Figure 2 shows multimodal distributions across languages, suggesting diverse score ranges were achieved
- Break condition: If certain heuristics consistently produce pairs with unexpected relatedness scores, or if the heuristics don't translate well to low-resource languages.

### Mechanism 3
- Claim: Native speaker annotation across diverse language families captures culturally-specific intuitions of semantic relatedness.
- Mechanism: Native speakers bring language-specific knowledge of word meanings, cultural references, and contextual usage that non-native annotators would miss.
- Core assumption: Native speakers share sufficiently similar intuitions about relatedness within their language communities.
- Evidence anchors:
  - [section 3.2]: "We selected native speakers to annotate the sentence pairs" and "by using comparative annotations and relying on the intuitions of fluent speakers for each language"
  - [section 6]: "We acknowledge that there is no formal definition of what constitutes semantic relatedness" and "the annotations may be subjective"
  - [corpus]: Limited - corpus mentions demographic diversity of annotators (e.g., "3 women and 5 men from different social, cultural, and ethnic backgrounds" for Amharic) but doesn't show inter-annotator agreement data
- Break condition: If native speakers disagree substantially on relatedness judgments, or if the concept of relatedness itself is culturally variable.

## Foundational Learning

- Concept: Best-Worst Scaling (BWS) methodology and its advantages over traditional rating scales
  - Why needed here: Understanding why BWS was chosen for annotation and how it generates ordinal rankings from relative comparisons
  - Quick check question: If given 4 sentence pairs in a tuple, and you mark pair A as most related and pair D as least related, what relative ordering can you infer for pairs B and C?

- Concept: Semantic textual relatedness vs semantic textual similarity
  - Why needed here: The paper explicitly distinguishes STR from STS, and understanding this difference is crucial for proper dataset use and interpretation
  - Quick check question: Why would "I caught a cold" and "I hope you feel better soon" receive a low similarity score but be considered related?

- Concept: Language family diversity and its implications for NLP resource development
  - Why needed here: The dataset covers 5 distinct language families from Africa and Asia, and understanding the challenges of low-resource languages from different families is key to appreciating the dataset's significance
  - Quick check question: What challenges might arise when developing NLP resources for Afro-Asiatic languages versus Dravidian languages?

## Architecture Onboarding

- Component map: Source text corpora → sentence selection heuristics → pair generation → native speaker recruitment → BWS tuple generation → comparative annotation → score computation → quality control → anonymization → data balancing → baseline experiments

- Critical path: Data collection → Sentence pairing (heuristics) → Native speaker annotation (BWS) → Score computation → Quality control → Dataset release

- Design tradeoffs:
  - BWS vs direct rating: BWS requires fewer annotations but may be cognitively more demanding
  - Manual vs automated pairing: Manual pairing ensures quality but doesn't scale; automated heuristics scale but may miss nuanced relationships
  - Language-specific vs multilingual models: Language-specific models may perform better but require more resources; multilingual models are more efficient but may underperform on low-resource languages

- Failure signatures:
  - Low split-half reliability scores indicating poor annotation consistency
  - Unimodal score distributions suggesting insufficient diversity in pairing strategies
  - High correlation between lexical overlap baseline and gold scores suggesting the dataset may be too superficial

- First 3 experiments:
  1. Run lexical overlap baseline on a subset of the data to establish a performance floor
  2. Fine-tune a multilingual model (e.g., mBERT) on the English training data and test on multiple target languages to assess cross-lingual transfer
  3. Compare performance of language-specific models vs multilingual models on languages where both are available to understand the value of specialization

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but several areas warrant further investigation based on the methodology and results presented.

## Limitations
- Limited comparison with existing English semantic relatedness datasets to benchmark quality and usefulness
- No detailed analysis of potential socio-cultural biases introduced by data sources and annotation processes
- Varying dataset sizes across languages (2,180-4,500 pairs) without analysis of how this affects model performance

## Confidence

High confidence in annotation methodology's reliability based on strong split-half reliability scores (0.64-0.96).
Medium confidence in cross-lingual generalization claims due to varying dataset sizes across languages and lack of size-performance analysis.
Medium confidence in BWS superiority over rating scales due to lack of direct comparison.
Low confidence in cultural consistency of relatedness judgments due to missing inter-annotator agreement data.

## Next Checks

1. Conduct inter-annotator agreement analysis to quantify consistency of relatedness judgments across native speakers
2. Compare BWS-derived scores with direct rating scale annotations on a subset to validate the claimed advantages
3. Analyze model performance sensitivity to dataset size across languages to understand the impact of varying sample sizes