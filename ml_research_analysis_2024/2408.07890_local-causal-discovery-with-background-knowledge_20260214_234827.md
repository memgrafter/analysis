---
ver: rpa2
title: Local Causal Discovery with Background Knowledge
arxiv_id: '2408.07890'
source_url: https://arxiv.org/abs/2408.07890
tags:
- causal
- algorithm
- then
- mpdag
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a local causal discovery method for learning
  causal relationships in Markov equivalence classes represented by MPDAGs when background
  knowledge is available. The key contribution is a sound and complete algorithm for
  learning the local structure around a target variable using direct causal, non-ancestral,
  and ancestral background knowledge.
---

# Local Causal Discovery with Background Knowledge

## Quick Facts
- arXiv ID: 2408.07890
- Source URL: https://arxiv.org/abs/2408.07890
- Reference count: 40
- Primary result: Introduces sound and complete algorithm for learning local causal structure in MPDAGs using background knowledge, demonstrating superior performance with reduced computational cost.

## Executive Summary
This paper presents a local causal discovery method that learns causal relationships around a target variable using background knowledge and minimal conditional independence tests. The approach operates on Markov equivalence classes represented by MPDAGs, leveraging direct causal, non-ancestral, and ancestral background knowledge to refine the equivalence class. By focusing on local structure rather than full graph discovery, the method achieves computational efficiency while maintaining theoretical soundness and completeness. Experiments demonstrate effectiveness in both standard causal discovery tasks and counterfactual fairness applications.

## Method Summary
The method consists of two main components: local structure learning and causal relationship identification. Local structure learning uses a modified MB-by-MB algorithm that incorporates background knowledge to build an MPDAG around the target variable, minimizing conditional independence tests through strategic edge orientation using Meek's rules. Causal relationship identification then applies sound and complete criteria (Theorems 4-6) to determine definite causes, definite non-causes, and possible causes using only the local structure and additional conditional independence tests. The approach reduces computational complexity by avoiding full MPDAG enumeration while maintaining theoretical guarantees.

## Key Results
- Demonstrates superior performance compared to existing methods in local structure learning with reduced computational cost
- Achieves effective causal relationship identification using only local structure and minimal conditional independence tests
- Shows improved accuracy and efficiency in counterfactual fairness applications when background knowledge is available

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The algorithm reduces conditional independence tests by leveraging background knowledge to orient edges early.
- Mechanism: In Algorithm 1, background knowledge edges are added first, then Meek's rules orient undirected edges without needing further tests. Nodes are processed in an order that minimizes re-exploration.
- Core assumption: Background knowledge edges reduce the number of undirected paths in the MPDAG, thus reducing the number of nodes that need Markov blanket discovery.
- Evidence anchors:
  - [abstract] "Experiments demonstrate superior performance compared to existing methods in local structure learning... with reduced computational cost."
  - [section 6.1] "As the amount of background knowledge increases, the number of conditional independence tests required by the algorithm indeed decreases."
  - [corpus] Weak—no directly comparable study on test count reduction in this exact setup.
- Break condition: If background knowledge is incorrect or sparse, the reduction advantage diminishes or reverses.

### Mechanism 2
- Claim: Local structure learning suffices to identify causal relationships without full MPDAG discovery.
- Mechanism: Theorems 4-6 provide sufficient and necessary conditions using only the local structure around the target variable and additional conditional independence tests. This avoids enumerating all DAGs in the equivalence class.
- Core assumption: The local structure (parents, children, siblings) around the target variable captures all relevant paths for causal relationship identification.
- Evidence anchors:
  - [abstract] "The method identifies definite causes, definite non-causes, and possible causes of the target variable using only the local structure and additional conditional independence tests."
  - [section 4] "We propose a sound and complete algorithm based on the MB-by-MB algorithm to learn the local structure in an MPDAG, which utilizes the background knowledge during the learning process."
  - [corpus] Weak—no direct comparison with full MPDAG-based identification in the corpus.
- Break condition: If the local structure is incomplete due to model assumptions or data limitations, causal relationships may be misidentified.

### Mechanism 3
- Claim: Distinguishing explicit from implicit causes improves interpretability and application in fairness contexts.
- Mechanism: Definition 1 and Theorem 5 differentiate whether a cause is direct (explicit) or mediated through siblings (implicit). This distinction is crucial for fairness interventions where only direct effects should be controlled.
- Core assumption: In fairness applications, distinguishing direct from mediated causal effects is meaningful and actionable.
- Evidence anchors:
  - [abstract] "Experiments involving... fair machine learning demonstrate that our method is both effective and efficient."
  - [section 6.3] "According to Kusner et al. (2017), ˆY is counterfactually fair if it depends only on the non-descendants of A."
  - [corpus] Weak—no corpus study explicitly validates this distinction for fairness.
- Break condition: If fairness requires considering mediated effects, this distinction may be counterproductive.

## Foundational Learning

- Concept: Markov equivalence and MPDAG representation
  - Why needed here: The method operates on MPDAGs that represent equivalence classes of DAGs under background knowledge. Understanding this representation is essential for interpreting the algorithm's output.
  - Quick check question: Can you explain why two different DAGs can represent the same conditional independence structure?

- Concept: Conditional independence testing and Markov blankets
  - Why needed here: The algorithm relies on finding Markov blankets to identify local structure and uses conditional independence tests to orient edges and verify causal relationships.
  - Quick check question: What is the relationship between a node's Markov blanket and its parents, children, and spouses in a causal graph?

- Concept: Meek's rules for edge orientation
  - Why needed here: The algorithm uses Meek's rules to orient undirected edges based on v-structures and background knowledge, which is fundamental to constructing the MPDAG.
  - Quick check question: What are the four Meek's rules, and how do they determine edge orientation?

## Architecture Onboarding

- Component map: Local structure learning (Algorithm 1) -> Causal relationship identification (Theorems 4-6 and Algorithm 3) -> Fairness application (section 6.3)
- Critical path: 1) Initialize graph with background knowledge edges, 2) Iteratively discover Markov blankets for nodes connected to target, 3) Orient edges using v-structures and Meek's rules, 4) Apply Theorems 4-6 to identify causal relationships
- Design tradeoffs: Local learning reduces computational cost but may miss global structure that could affect causal identification. The method assumes faithfulness and correct conditional independence testing, which may not hold in practice.
- Failure signatures: Incorrect causal relationships when background knowledge is wrong or when conditional independence tests have errors. Reduced performance when local structure is insufficient to capture all relevant paths.
- First 3 experiments:
  1. Verify that Algorithm 1 correctly learns local structure on a known small DAG with various amounts of background knowledge.
  2. Test Theorems 4-6 on synthetic MPDAGs to ensure they correctly identify causal relationships.
  3. Compare computational efficiency of the local method versus full MPDAG discovery on medium-sized graphs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the local causal discovery method compare to state-of-the-art global methods in terms of computational efficiency and accuracy?
- Basis in paper: [explicit] The paper states that the method is "particularly effective when background knowledge is available, improving both accuracy and efficiency of causal discovery" and demonstrates "superior performance compared to existing methods in local structure learning, causal relationship identification, and counterfactual fairness applications, with reduced computational cost."
- Why unresolved: The paper does not provide a direct comparison with state-of-the-art global methods, only with existing local methods.
- What evidence would resolve it: A direct comparison of the method with state-of-the-art global methods on the same datasets, measuring both computational efficiency and accuracy.

### Open Question 2
- Question: How does the method perform when there are unmeasured confounders or selection bias in the data?
- Basis in paper: [explicit] The paper states that "we assume no selection bias or presence of unmeasured confounders, consistent with other causal discovery methods."
- Why unresolved: The assumption of no unmeasured confounders or selection bias may not hold in real-world scenarios, and the method's performance under these conditions is unknown.
- What evidence would resolve it: Testing the method on datasets with known unmeasured confounders or selection bias, and comparing its performance to methods that account for these factors.

### Open Question 3
- Question: Can the method be extended to handle more complex causal relationships, such as those involving time-varying effects or feedback loops?
- Basis in paper: [inferred] The paper focuses on learning causal relationships in static DAGs and does not mention extensions to handle more complex scenarios.
- Why unresolved: The method's applicability to more complex causal relationships is not explored, and it is unclear whether it can be extended to handle these scenarios.
- What evidence would resolve it: Developing and testing an extension of the method to handle time-varying effects or feedback loops, and evaluating its performance on appropriate datasets.

## Limitations

- Effectiveness heavily depends on the quality and completeness of background knowledge, which is rarely perfect in real-world applications
- Experiments primarily use synthetic data, limiting generalizability to complex real-world scenarios
- Computational efficiency gains are context-dependent on the amount and accuracy of background knowledge

## Confidence

- **High**: The theoretical soundness of the local structure learning algorithm and causal relationship identification criteria (Theorems 4-6)
- **Medium**: The experimental results showing superior performance on synthetic datasets and the counterfactual fairness application
- **Low**: The generalizability of results to real-world data with incomplete or noisy background knowledge

## Next Checks

1. Test the algorithm on real-world datasets with known causal structures (e.g., gene expression data, healthcare records) to validate practical effectiveness.
2. Conduct sensitivity analysis on background knowledge accuracy, measuring performance degradation as background knowledge becomes noisier or sparser.
3. Compare the method against full MPDAG-based approaches on datasets where global structure significantly impacts causal relationship identification.