---
ver: rpa2
title: 'Factuality of Large Language Models: A Survey'
arxiv_id: '2402.02420'
source_url: https://arxiv.org/abs/2402.02420
tags:
- factuality
- language
- llms
- knowledge
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides a comprehensive analysis of the factuality
  challenges in large language models (LLMs), highlighting their tendency to generate
  factually incorrect information despite being instruction-tuned for chat applications.
  The paper categorizes existing evaluation datasets into four types based on answer
  space and automation feasibility, and reviews various mitigation techniques spanning
  pre-training, fine-tuning, inference strategies, and retrieval augmentation.
---

# Factuality of Large Language Models: A Survey

## Quick Facts
- arXiv ID: 2402.02420
- Source URL: https://arxiv.org/abs/2402.02420
- Reference count: 6
- Large language models generate factually incorrect information despite instruction-tuning, requiring comprehensive evaluation frameworks and mitigation techniques

## Executive Summary
This survey provides a comprehensive analysis of factuality challenges in large language models, which continue to generate factually incorrect information despite instruction-tuning for chat applications. The paper systematically categorizes existing evaluation datasets into four types based on answer space and automation feasibility, then reviews various mitigation techniques spanning pre-training, fine-tuning, inference strategies, and retrieval augmentation. A key finding is that automated fact-checking faces significant hurdles due to lack of standardized evaluation metrics and gold-standard evidence. The survey identifies three major challenges: LLMs optimize for language probability rather than factual accuracy, automated evaluation of open-ended generation remains difficult, and retrieval-augmented generation systems struggle with latency and multi-hop reasoning.

## Method Summary
The survey synthesizes existing research on LLM factuality by categorizing evaluation datasets into four types: open-ended generation, Yes/No questions, short-form/list answers, and multi-choice QA. It reviews mitigation approaches including pre-training with high-quality corpora, fine-tuning strategies (SFT, RLHF, RLXF), inference-time techniques (decoding strategies, ICL, self-reasoning), and retrieval augmentation. The methodology also examines automated fact-checking frameworks and their components, analyzing challenges in claim processing, evidence retrieval, and verification. The survey proposes future research directions including better retrieval algorithms, more efficient fact-checkers, and improved decoding strategies to guide models toward factually correct outputs.

## Key Results
- LLMs generate factually incorrect information despite instruction-tuning, requiring comprehensive evaluation frameworks and mitigation techniques
- Automated fact-checking faces significant hurdles due to lack of standardized evaluation metrics and gold-standard evidence
- Three major challenges identified: probability optimization over factuality, difficulty in automated evaluation, and RAG system limitations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Factuality of LLMs can be improved by optimizing decoding strategies to reduce randomness during generation.
- Mechanism: Nucleus sampling (top-p) encourages diversity but harms factuality. By dynamically reducing the nucleus-p value as generation progresses, the model limits diversity in later parts of the sentence, improving factual accuracy.
- Core assumption: Sampling randomness in later parts of sentences is more likely to introduce factual errors than in earlier parts.
- Evidence anchors:
  - [abstract] "Sampling from the top subword candidates with a cumulative probability of p, known as nucleus sampling (top-p) (Holtzman et al., 2020), sees a decrease in factuality performance compared to greedy decoding, despite higher diversity."
  - [section] "Building on the hypothesis that sampling randomness may damage factuality when generating the latter part of a sentence than the beginning, (Lee et al., 2022) introduce factual-nucleus sampling, which dynamically reduces the nucleus-p value as generation progresses to limit diversity and improve factuality."

### Mechanism 2
- Claim: Retrieval augmentation improves LLM factuality by grounding responses in external knowledge sources.
- Mechanism: By incorporating retrieved evidence before, during, or after generation, the model is constrained to produce outputs consistent with verified information, reducing hallucinations.
- Core assumption: External knowledge sources contain accurate information that can correct or supplement the model's internal knowledge.
- Evidence anchors:
  - [abstract] "Another important element is retrieval augmentation, which enhances the generation capabilities of LLMs by anchoring them in external knowledge that may not be stored or contradict the information in LLM parametric memory."
  - [section] "Incorporating retrieval mechanisms during fine-tuning has been shown to enhance the LLM factuality on downstream tasks, particularly in open-domain QA."

### Mechanism 3
- Claim: Fine-tuning with synthetic data can reduce sycophancy, a source of factuality errors, by teaching models that truthfulness is independent of user opinions.
- Mechanism: Synthetic data intervention involves creating training examples that emphasize factual correctness over agreement with user preferences, aligning model behavior with truth rather than approval.
- Core assumption: Sycophancy arises during fine-tuning when models learn to prioritize user satisfaction over factual accuracy.
- Evidence anchors:
  - [abstract] "(Wei et al., 2023) explore the correlation of sycophancy with model scaling and instruction tuning. They propose a synthetic-data intervention method, using various NLP tasks to teach models that truthfulness is independent of user opinions."
  - [section] "Sycophancy (Sharma et al., 2023), another source of factuality errors, often arises from mis-alignments during SFT and RLHF(Ouyang et al., 2022)."

## Foundational Learning

- Concept: Automated fact-checking framework
  - Why needed here: Understanding the components (claim processor, retriever, verifier) is essential for improving factuality evaluation methods.
  - Quick check question: What are the three main components of an automatic fact-checking framework, and what is the role of each?

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: RAG systems are a key method for improving factuality by incorporating external knowledge into LLM responses.
  - Quick check question: At which stages can retrieval augmentation be applied in the LLM lifecycle, and what are the trade-offs of each approach?

- Concept: Hallucination vs. factuality
  - Why needed here: Distinguishing between these concepts is crucial for accurately diagnosing and addressing factuality issues in LLMs.
  - Quick check question: How do hallucination and factuality differ in the context of LLM-generated content, and why is this distinction important?

## Architecture Onboarding

- Component map: Pre-training (corpus curation, filtering) -> Fine-tuning (SFT, RLHF, RLXF) -> Inference (decoding, ICL, self-reasoning) -> Post-processing (fact-checking, correction). Retrieval augmentation can be integrated at multiple stages.
- Critical path: For a new response, the critical path is: input prompt → retrieval (if applicable) → generation (with optimized decoding) → fact-checking → output. Any bottleneck in fact-checking or retrieval directly impacts system latency and factuality.
- Design tradeoffs: High-quality retrieval improves factuality but increases latency; aggressive decoding optimization may reduce creativity; synthetic data reduces sycophancy but may not generalize.
- Failure signatures: Factual errors propagate if early sentences are wrong; latency spikes if retrieval is slow; model becomes overly cautious if synthetic data over-emphasizes factuality.
- First 3 experiments:
  1. Compare factuality of greedy decoding vs. dynamic nucleus sampling on a biography generation task using FactScore.
  2. Measure the impact of pre-generation retrieval augmentation on factuality and latency in open-domain QA.
  3. Evaluate the effectiveness of synthetic data intervention in reducing sycophancy across diverse prompt types.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the most effective way to improve the factuality of LLMs during pre-training?
- Basis in paper: [explicit] The paper discusses the importance of high-quality textual corpora and automated filtering methods during pre-training.
- Why unresolved: The paper highlights the challenges of manual filtering due to the massive amount of pre-training data and the need for novel strategies to ensure factual consistency across diverse cultural landscapes.
- What evidence would resolve it: Comparative studies evaluating different pre-training strategies, including corpus curation, automated filtering, and retrieval augmentation, on factuality benchmarks.

### Open Question 2
- Question: How can we develop more efficient and accurate automated fact-checkers for LLMs?
- Basis in paper: [explicit] The paper identifies the development of accurate and efficient fact-checkers as a key breakthrough for evaluating the factual accuracy of LLMs.
- Why unresolved: The paper points out the challenges of quantifying the quality of intermediate steps in fact-checking pipelines, such as claim processing and evidence retrieval, due to the lack of gold-standard annotations.
- What evidence would resolve it: Development and evaluation of novel fact-checking frameworks that address the challenges of claim decomposition, evidence retrieval, and quality assessment, using metrics such as precision, recall, and F1 score.

### Open Question 3
- Question: How can we effectively mitigate hallucinations in multimodal LLMs?
- Basis in paper: [explicit] The paper categorizes factuality issues in multimodal LLMs into existence, attribute, and relationship factuality, and discusses fine-tuning, inference-time correction, and representation learning as mitigation approaches.
- Why unresolved: The paper highlights the need for more in-depth exploration of mitigation techniques specific to vision-language models and comprehensive discussions for language models incorporating other modalities like video and speech.
- What evidence would resolve it: Comparative studies evaluating the effectiveness of different mitigation approaches, such as fine-tuning, inference-time correction, and representation learning, on multimodal factuality benchmarks, using metrics like accuracy, precision, and recall.

## Limitations

- Automated fact-checking faces fundamental challenges due to lack of standardized evaluation metrics and gold-standard evidence
- Effectiveness of decoding strategies like factual-nucleus sampling is primarily validated on biography generation tasks with limited testing across diverse domains
- Retrieval-augmented generation systems show promise but face unresolved trade-offs between latency and accuracy that are not fully quantified

## Confidence

- High Confidence: The categorization of evaluation datasets into four types (open-ended generation, Yes/No questions, short-form/list answers, and multi-choice QA) is well-supported by the survey's systematic analysis and provides a clear framework for understanding factuality assessment approaches.
- Medium Confidence: The identification of three major challenges (LLMs optimizing for probability over factuality, difficulty in automated evaluation, and RAG system limitations) is grounded in the survey's comprehensive literature review, though specific quantitative impacts vary across studies.
- Low Confidence: Claims about the effectiveness of specific mitigation techniques like factual-nucleus sampling and synthetic data intervention are based on limited experimental results and require broader validation across diverse task types and model architectures.

## Next Checks

1. **Decoding Strategy Validation**: Test factual-nucleus sampling across at least three diverse domains (biography generation, technical documentation, and news summarization) to verify whether dynamically reducing nucleus-p consistently improves factuality or if the effect is domain-specific.

2. **Automated Fact-Checker Reliability**: Conduct a systematic comparison of automated fact-checkers (FactScore, FacTool) against human-annotated factuality labels on the same outputs, quantifying agreement rates and identifying systematic error patterns in automated verification.

3. **Retrieval Augmentation Trade-offs**: Measure the latency-accuracy trade-off in RAG systems by varying retrieval depth and timing (pre-generation vs. post-generation) across tasks requiring single-hop and multi-hop reasoning, establishing concrete performance curves.