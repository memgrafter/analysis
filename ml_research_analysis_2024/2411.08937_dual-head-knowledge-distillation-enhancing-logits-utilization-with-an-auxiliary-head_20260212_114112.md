---
ver: rpa2
title: 'Dual-Head Knowledge Distillation: Enhancing Logits Utilization with an Auxiliary
  Head'
arxiv_id: '2411.08937'
source_url: https://arxiv.org/abs/2411.08937
tags:
- loss
- binarykl
- student
- classifier
- dhkd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the incompatibility between cross-entropy\
  \ (CE) and binary KL divergence (BinaryKL) losses in knowledge distillation, where\
  \ combining them leads to performance degradation due to conflicts in the linear\
  \ classifier. To resolve this, the authors propose Dual-Head Knowledge Distillation\
  \ (DHKD), which decouples the linear classifier into two heads\u2014one trained\
  \ with CE and the other with BinaryKL\u2014to preserve beneficial effects on the\
  \ backbone while eliminating conflicts on the classifier."
---

# Dual-Head Knowledge Distillation: Enhancing Logits Utilization with an Auxiliary Head

## Quick Facts
- arXiv ID: 2411.08937
- Source URL: https://arxiv.org/abs/2411.08937
- Reference count: 40
- Primary result: Dual-head architecture resolves incompatibility between cross-entropy and binary KL divergence losses in knowledge distillation

## Executive Summary
This paper addresses the incompatibility between cross-entropy (CE) and binary KL divergence (BinaryKL) losses in knowledge distillation, where combining them leads to performance degradation due to conflicts in the linear classifier. To resolve this, the authors propose Dual-Head Knowledge Distillation (DHKD), which decouples the linear classifier into two heads‚Äîone trained with CE and the other with BinaryKL‚Äîto preserve beneficial effects on the backbone while eliminating conflicts on the classifier. Theoretical analysis based on neural collapse theory supports the design. Experiments on CIFAR-100 and ImageNet show that DHKD outperforms state-of-the-art logit-based methods and achieves competitive results with feature-based methods when combined with ReviewKD. On CIFAR-100, DHKD improves classification accuracy by up to 3.2% over vanilla KD. The method is also validated on semantic segmentation tasks, confirming its effectiveness and generalizability.

## Method Summary
Dual-Head Knowledge Distillation (DHKD) resolves the incompatibility between cross-entropy and binary KL divergence losses by partitioning the linear classifier into two separate heads. The original classifier head trains solely with CE loss, while an auxiliary classifier head handles the BinaryKL-Norm loss. This separation allows BinaryKL to improve feature learning in the backbone without causing contradictory gradients in the classifier. The method includes a gradient alignment technique that projects conflicting gradients when their angle exceeds 90 degrees, particularly important for CIFAR-100 experiments. BinaryKL-Norm stabilizes training by normalizing the difference between student and teacher logits to avoid teacher-dependent gradient synchronization issues.

## Key Results
- On CIFAR-100, DHKD improves classification accuracy by up to 3.2% over vanilla KD
- Achieves state-of-the-art performance among logit-based methods, with 80.2% accuracy on CIFAR-100
- When combined with ReviewKD, achieves competitive results with feature-based methods (81.4% on CIFAR-100)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dual-Head Knowledge Distillation resolves incompatibility between cross-entropy (CE) and binary KL divergence (BinaryKL) losses by partitioning the linear classifier into two separate heads.
- **Mechanism:** The BinaryKL loss facilitates stronger feature learning in the backbone but causes contradictory gradients in the original linear classifier. By introducing an auxiliary classifier head for BinaryKL loss, the gradients from BinaryKL are confined to the backbone while the original classifier head trains solely with CE loss, eliminating conflicts.
- **Core assumption:** The original linear classifier cannot handle conflicting gradients from CE and BinaryKL simultaneously without collapsing.
- **Evidence anchors:**
  - [abstract]: "Dual-Head Knowledge Distillation (DHKD), which decouples the linear classifier into two heads‚Äîone trained with CE and the other with BinaryKL‚Äîto preserve beneficial effects on the backbone while eliminating conflicts on the classifier."
  - [section 3.1]: "The gradients of the two loss functions exhibit contradictions in the linear classifier yet display no such conflict within the backbone."
  - [corpus]: No direct evidence found. Weak anchor.

### Mechanism 2
- **Claim:** BinaryKL-Norm loss variant stabilizes training by avoiding teacher-dependent gradient synchronization issues.
- **Mechanism:** The original BinaryKL loss creates synchronization problems because the gradient at the student output depends on the teacher output value. BinaryKL-Norm normalizes the difference between student and teacher logits, ensuring consistent gradient directions regardless of absolute teacher logit values.
- **Core assumption:** The synchronization problem in BinaryKL gradients is significant enough to degrade performance and needs normalization.
- **Evidence anchors:**
  - [section 3.3]: "To unify the gradients at the outputs of the teacher model, we choose to narrow the distance between zero and the difference between the teacher and student."
  - [corpus]: No direct evidence found. Weak anchor.

### Mechanism 3
- **Claim:** Gradient alignment technique prevents negative interference between CE and BinaryKL gradients when their directions are opposing.
- **Mechanism:** When the angle between gradients of BinaryKL-Norm and CE losses exceeds 90 degrees, the BinaryKL gradient is projected onto the orthogonal direction of the CE gradient, preventing optimization that would harm classification performance.
- **Core assumption:** Conflicting gradients can be identified by their angular relationship and mitigated through projection.
- **Evidence anchors:**
  - [section 3.3]: "When the angle between two gradients is larger than 90‚ó¶, we will project the gradient of the BinaryKL loss to the orthogonal direction of the gradient of the CE loss."
  - [corpus]: No direct evidence found. Weak anchor.

## Foundational Learning

- **Concept: Neural Collapse Theory**
  - Why needed here: Explains why linear classifier collapse is problematic and what constitutes an "ideal" classifier state (simplex ETF).
  - Quick check question: What are the four manifestations of neural collapse that the paper references?

- **Concept: Knowledge Distillation Fundamentals**
  - Why needed here: Provides context for why logits-level information is valuable and how traditional KD methods work.
  - Quick check question: How does the softmax function potentially obscure information in logits that BinaryKL aims to preserve?

- **Concept: Gradient Decomposition and Analysis**
  - Why needed here: Enables understanding of why CE and BinaryKL gradients conflict in the classifier but not in the backbone.
  - Quick check question: What is the difference between "pull" and "push" terms in gradient decomposition?

## Architecture Onboarding

- **Component map:**
  - Backbone (shared feature extractor)
  - Original classifier head (trained with CE loss only)
  - Auxiliary classifier head (trained with BinaryKL-Norm loss)
  - Gradient alignment module (optional, for CIFAR-100)
  - Temperature scaling parameter (fixed at 2)

- **Critical path:**
  Input ‚Üí Backbone ‚Üí Feature extraction
  Feature ‚Üí Original classifier ‚Üí CE loss computation
  Feature ‚Üí Auxiliary classifier ‚Üí BinaryKL-Norm loss computation
  Combine losses with weights (Œ± typically 0.1)
  Apply gradient alignment if needed
  Update parameters

- **Design tradeoffs:**
  Using two classifiers increases parameter count but isolates conflicting gradients
  Auxiliary classifier can be linear (same architecture) or nonlinear MLP (different architectures)
  Gradient alignment adds computational overhead but improves stability on CIFAR-100

- **Failure signatures:**
  Performance degradation when Œ± is too high (loss imbalance)
  Training collapse if gradient alignment is not applied on CIFAR-100
  Suboptimal results if auxiliary classifier architecture mismatches teacher-student architecture

- **First 3 experiments:**
  1. Run DHKD with Œ±=0.1 on CIFAR-100 (resnet32√ó4 teacher, resnet8√ó4 student) to verify baseline performance improvement over vanilla KD.
  2. Test DHKD with and without gradient alignment on CIFAR-100 to confirm the necessity of alignment for this dataset.
  3. Apply DHKD to ImageNet (ResNet-34 teacher, ResNet-18 student) to verify cross-dataset effectiveness without gradient alignment.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the BinaryKL loss incompatibility with CE loss extend to other non-sigmoid-based logit-level losses, such as MSE or contrastive logit matching?
- **Basis in paper:** [explicit] The authors note that "the CE loss and the logit-level loss are not universally incompatible" and that "the MSE loss is not incompatible with the CE loss," but they only test BinaryKL and BinaryKL-Norm.
- **Why unresolved:** The theoretical analysis focuses on BinaryKL's gradient behavior, but it's unclear whether similar gradient conflicts arise with other logit-level losses or if the incompatibility is specific to the sigmoid-BinaryKL formulation.
- **What evidence would resolve it:** Systematic experiments comparing CE + various logit-level losses (MSE, contrastive logit matching, etc.) to identify which loss types cause gradient conflicts and which do not.

### Open Question 2
- **Question:** How does Dual-Head Knowledge Distillation perform when applied to non-classification tasks like object detection or semantic segmentation with multi-task learning objectives?
- **Basis in paper:** [inferred] The authors mention limitations stating "the neural collapse theory mainly focuses on the classification problem" and only test on classification and one semantic segmentation experiment, suggesting unexplored potential in multi-task scenarios.
- **Why unresolved:** The paper's theoretical foundation relies on neural collapse properties specific to classification, and the dual-head design was motivated by classifier conflicts that may manifest differently in multi-task or detection settings.
- **What evidence would resolve it:** Comprehensive experiments applying DHKD to object detection benchmarks (COCO) and multi-task learning scenarios, comparing performance against standard distillation methods.

### Open Question 3
- **Question:** What is the impact of using different temperature scaling strategies for the BinaryKL-Norm loss versus the CE loss in the dual-head framework?
- **Basis in paper:** [explicit] The authors fix temperature at 2 for all experiments but note that "using different ùúè in different settings can lead to state-of-the-art performance" and that BinaryKL has issues with teacher-dependent derivatives at different ùúè values.
- **Why unresolved:** While BinaryKL-Norm addresses some temperature-related issues, the paper doesn't explore whether optimizing temperatures separately for each head could further improve performance or if the current fixed approach is suboptimal.
- **What evidence would resolve it:** Ablation studies varying temperatures independently for CE and BinaryKL-Norm heads across different teacher-student pairs to determine optimal temperature strategies.

## Limitations
- The theoretical analysis relies heavily on neural collapse theory, which primarily applies to classification problems
- The necessity of gradient alignment technique is dataset-specific, working well on CIFAR-100 but not needed for ImageNet
- Implementation details for BinaryKL-Norm normalization and gradient projection are not fully specified, potentially affecting reproducibility

## Confidence
- **High confidence**: The dual-head architecture effectively separates conflicting gradients between CE and BinaryKL losses, as evidenced by consistent performance improvements across multiple experiments and datasets.
- **Medium confidence**: The theoretical justification using neural collapse is sound, but the empirical validation connecting neural collapse behavior to the observed performance gains is limited.
- **Low confidence**: The necessity and optimal implementation of gradient alignment technique, as it shows large effects on CIFAR-100 but is not applied to ImageNet, suggesting potential overfitting to specific dataset characteristics.

## Next Checks
1. **Gradient behavior analysis**: Visualize and quantify the angle between CE and BinaryKL gradients during training on CIFAR-100 to empirically verify when conflicts occur and when gradient alignment is beneficial.
2. **Cross-dataset generalization**: Test DHKD on additional datasets beyond CIFAR-100 and ImageNet (e.g., CIFAR-10, Tiny ImageNet) to determine if gradient alignment is consistently needed or dataset-specific.
3. **Ablation study of BinaryKL-Norm**: Compare standard BinaryKL loss versus BinaryKL-Norm with and without gradient alignment to isolate the impact of normalization from the gradient projection technique.