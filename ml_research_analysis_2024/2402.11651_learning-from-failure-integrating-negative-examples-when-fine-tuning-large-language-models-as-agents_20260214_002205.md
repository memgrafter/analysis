---
ver: rpa2
title: 'Learning From Failure: Integrating Negative Examples when Fine-tuning Large
  Language Models as Agents'
arxiv_id: '2402.11651'
source_url: https://arxiv.org/abs/2402.11651
tags:
- negative
- examples
- positive
- samples
- trajectories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) struggle as agents due to their optimization
  for language generation rather than tool use. Prior work in fine-tuning LLMs as
  agents discards failed trajectories, wasting valuable data and limiting optimization
  paths.
---

# Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents

## Quick Facts
- arXiv ID: 2402.11651
- Source URL: https://arxiv.org/abs/2402.11651
- Reference count: 23
- Primary result: NAT improves agent fine-tuning by 8.74% on GSM8K, 6% on HotpotQA, and 8% on StrategyQA by learning from failed trajectories.

## Executive Summary
Large language models (LLMs) struggle as agents because they are optimized for language generation rather than tool use. Prior work discards failed trajectories during fine-tuning, wasting valuable data. This paper introduces Negative-Aware Training (NAT), which reformats trajectories by adding prefixes or suffixes to distinguish positive and negative examples. NAT enables LLMs to learn from both successful and failed trajectories, achieving significant performance improvements across multiple reasoning tasks.

## Method Summary
The method collects tool-use trajectories generated by GPT-3.5 on various datasets, labels them as positive (successful) or negative (failed), and reformats them by adding correctness prompts. Smaller LLaMA-2-Chat models (7B and 13B) are fine-tuned using this reformatted data with LoRA or full fine-tuning. The approach is evaluated on mathematical reasoning and question-answering tasks, demonstrating that explicit correctness signaling during fine-tuning improves learning from negative examples.

## Key Results
- NAT outperforms traditional positive-only fine-tuning by 8.74% on GSM8K
- NAT achieves 6% improvement on HotpotQA and 8% on StrategyQA
- Fine-grained NAT with quality-based negative segmentation consistently outperforms standard NAT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NAT enables LLMs to learn from failed trajectories by differentiating between positive and negative examples via prompt prefixes/suffixes.
- Mechanism: The added prompt string ("Please generate a solution that **correctly** answers the question" vs. "Please generate a solution that **incorrectly** answers the question") tells the model whether to learn from the trajectory as a success or failure. This prevents the model from simply copying errors from negative trajectories while still allowing it to extract reasoning insights.
- Core assumption: LLMs can use discrete prefix/suffix signals during fine-tuning to selectively encode the correctness context of trajectories.
- Evidence anchors:
  - [abstract]: "By simply adding a prefix or suffix that tells the model whether to generate a successful trajectory during training, we improve model performance..."
  - [section 3.1]: "Therefore, we add a prefix or suffix to the query to differentiate positive and negative examples, explicitly telling the model whether the following trajectories they learn are correct."
- Break condition: If the model fails to attend to or interpret the prefix/suffix signal during fine-tuning, the differentiation fails and errors propagate.

### Mechanism 2
- Claim: NAT provides a better trade-off between useful information and errors in negative trajectories.
- Mechanism: By explicitly marking trajectories as positive or negative, NAT allows the model to extract reasoning patterns from negative trajectories without over-fitting to their errors. This is evidenced by lower action error rates compared to NUT.
- Core assumption: Negative trajectories contain valuable reasoning "thoughts" even if their final actions are incorrect.
- Evidence anchors:
  - [abstract]: "We further analyze the inference results and find that our method provides a better trade-off between valuable information and errors in unsuccessful trajectories."
  - [section 5.3]: "Compared to NUT, NAT achieves significantly fewer action errors and, therefore, better accuracy."
- Break condition: If negative trajectories contain mostly noise or their errors are too systemic, the trade-off breaks down.

### Mechanism 3
- Claim: Fine-grained NAT, which assigns different prompts based on trajectory quality, learns more effectively from negative samples.
- Mechanism: By segmenting negative trajectories into classes based on quality (e.g., f1 score), NAT-k assigns distinct prompts that allow the model to differentiate between near-correct and clearly-wrong reasoning patterns, leading to better learning.
- Core assumption: Quality gradations in negative trajectories can be reliably measured and used to guide learning.
- Evidence anchors:
  - [section 6.1]: "Fine-grained NAT learns more from negative samples... NAT-2 consistently outperforms NAT in all settings."
- Break condition: If quality measures are noisy or the prompt differentiation is ambiguous, the benefit disappears.

## Foundational Learning

- Concept: Trajectory-based fine-tuning
  - Why needed here: Agents generate tool-use trajectories (Thought → Action → Observation). Fine-tuning on these trajectories teaches the model to replicate successful reasoning and tool-use patterns.
  - Quick check question: Can the model distinguish between Thought and Action parts of a trajectory during fine-tuning?
- Concept: Prefix/suffix conditioning
  - Why needed here: The prompt string added to the query conditions the model on whether to learn from a positive or negative trajectory, preventing error propagation.
  - Quick check question: Does the added prompt appear in the loss computation or is it masked out?
- Concept: Negative data quality measurement
  - Why needed here: Not all negative trajectories are equally informative; some are close to correct and others are clearly flawed. Quality-based segmentation improves learning.
  - Quick check question: How is the quality score (e.g., f1) computed for negative trajectories?

## Architecture Onboarding

- Component map: GPT-3.5 -> Trajectory Generator -> Data Labeling -> NAT Reformatter -> Fine-tuning Engine -> Inference Agent
- Critical path: Generate → Label → Reformat → Fine-tune → Infer
- Design tradeoffs:
  - Adding explicit correctness prompts increases data size but improves signal-to-noise ratio.
  - Segmenting negative data by quality improves learning but requires quality scoring logic.
  - Fine-tuning on entire trajectory vs. only LLM-generated parts affects loss stability.
- Failure signatures:
  - High action error rate despite training → model is overfitting negative errors.
  - Perplexity on positive dev set not improving → NAT prefix/suffix not being attended to.
  - Performance drops with more negatives → negative data too noisy or low quality.
- First 3 experiments:
  1. Compare NAT vs. NUT on a small math dataset with 2k positives and 1k negatives.
  2. Test effect of adding vs. omitting the correctness prompt on a held-out trajectory set.
  3. Evaluate fine-grained NAT by splitting negatives into two quality classes and measuring EM/F1 on HotpotQA.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal ratio of negative to positive examples for fine-tuning LLMs as agents?
- Basis in paper: [explicit] The paper discusses the diminishing return of adding more negative examples and the influence of positive sample quantity on this ratio.
- Why unresolved: The paper only provides experimental results with specific ratios (e.g., 2k positives with 10k negatives, 5k positives with 10k negatives) and does not explore a wider range of ratios or derive a general principle.
- What evidence would resolve it: Systematic experiments varying the ratio of negative to positive examples across different tasks and model sizes, identifying the point of diminishing returns and the impact of positive sample quantity.

### Open Question 2
- Question: How does the quality of negative examples affect the learning process and final performance of LLMs fine-tuned as agents?
- Basis in paper: [explicit] The paper explicitly states that data quality matters, showing that high-quality negative examples (generated by GPT-3.5) lead to better performance than low-quality ones (generated by a fine-tuned LLaMA-2-7B model).
- Why unresolved: The paper only compares two extremes (high-quality vs. low-quality) and does not explore a spectrum of quality levels or investigate the specific aspects of negative examples that contribute to their quality.
- What evidence would resolve it: Experiments using negative examples of varying quality, controlled for different aspects such as reasoning complexity, action accuracy, and relevance to the task, to identify the key factors that determine their effectiveness.

### Open Question 3
- Question: Can the insights gained from negative examples be transferred to other tasks or domains, and if so, how?
- Basis in paper: [inferred] The paper demonstrates that LLMs can learn from negative examples in agent-related tasks, suggesting that this learning mechanism might be applicable more broadly.
- Why unresolved: The paper focuses on specific tasks (math, question answering) and does not investigate whether the knowledge gained from negative examples in these tasks can be applied to other domains or types of problems.
- What evidence would resolve it: Experiments fine-tuning LLMs on a diverse set of tasks, including those unrelated to agent behavior, and evaluating whether negative examples from one domain improve performance in another. Additionally, analyzing the learned representations to identify transferable knowledge.

## Limitations
- No corpus evidence on the effectiveness of prefix/suffix conditioning during fine-tuning.
- Quality segmentation relies on f1 scoring, but the method for computing f1 on negative trajectories is not detailed.
- Limited dataset diversity and single model family (LLaMA-2) restrict generality claims.

## Confidence
- High confidence in the core empirical findings (accuracy improvements across three datasets).
- Medium confidence in the mechanism explanation (prefix/suffix conditioning as the driver of improved learning).
- Medium confidence in fine-grained NAT's advantage (based on f1 quality segmentation).
- Low confidence in the generality of the findings due to limited dataset diversity and single model family.

## Next Checks
1. Replicate the NAT improvement on an additional task (e.g., DROP or CoQA) with a different reasoning type to test generality.
2. Conduct a controlled ablation replacing the correctness prompt with a neutral marker to confirm the prompt itself is necessary for the gain.
3. Compare NAT to a contrastive fine-tuning baseline that explicitly contrasts positive and negative trajectories without prompting, to isolate the effect of conditioning from data formatting.