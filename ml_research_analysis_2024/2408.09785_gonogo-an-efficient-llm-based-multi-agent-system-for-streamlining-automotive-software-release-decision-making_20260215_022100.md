---
ver: rpa2
title: 'GoNoGo: An Efficient LLM-based Multi-Agent System for Streamlining Automotive
  Software Release Decision-Making'
arxiv_id: '2408.09785'
source_url: https://arxiv.org/abs/2408.09785
tags:
- data
- system
- release
- software
- gonogo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GoNoGo is an LLM-based multi-agent system designed to automate
  software release decisions in the automotive industry. The system addresses the
  challenge of manual analysis of tabular test data, which is time-consuming and prone
  to errors.
---

# GoNoGo: An Efficient LLM-based Multi-Agent System for Streamlining Automotive Software Release Decision-Making

## Quick Facts
- arXiv ID: 2408.09785
- Source URL: https://arxiv.org/abs/2408.09785
- Reference count: 33
- One-line primary result: LLM-based multi-agent system achieves 100% success rate for automotive software release decisions on tasks up to Level 2 difficulty

## Executive Summary
GoNoGo is an LLM-based multi-agent system designed to automate software release decisions in the automotive industry. The system addresses the challenge of manual analysis of tabular test data by employing a Planner agent that interprets natural language queries and devises analysis strategies, and an Actor agent that executes plans by generating and running Python code. The system was evaluated on 50 test queries with varying difficulty levels, achieving high performance and significant time savings in industrial deployment.

## Method Summary
GoNoGo employs a two-agent architecture where the Planner agent interprets natural language queries using a domain-specific Knowledge Base and few-shot examples to devise analysis strategies. The Actor agent then executes these plans by generating and running Python code with self-reflection and memory capabilities. The system was evaluated on automotive "GoNoGo" data containing 40 fields and 55,000 records, using Chain-of-Thought prompting, self-consistency, and few-shot learning approaches.

## Key Results
- Achieved 100% success rate for tasks up to Level 2 difficulty with 3-shot examples
- Maintained 90% success rate even for more complex Level 3-4 tasks
- Pilot users reported saving approximately 2 hours per person per decision, significantly improving efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Planner agent successfully translates natural language queries into structured analysis plans using domain-specific knowledge and few-shot examples
- Mechanism: The Planner leverages a Knowledge Base containing structured descriptions of data and attributes, combined with few-shot learning examples, to interpret user queries and devise appropriate analysis strategies
- Core assumption: The Knowledge Base accurately captures domain-specific terminology and logic required for interpreting automotive software release queries
- Evidence anchors:
  - [abstract] "Unlike previous systems, GoNoGo is specifically tailored to address domain-specific and risk-sensitive systems."
  - [section] "The Planner utilizes a Knowledge Base containing a structured description of the data and its attributes to provide the necessary context and domain-specific information in the prompts given to the LLM."

### Mechanism 2
- Claim: The Actor agent executes the Planner's instructions by generating and running Python code with self-reflection and memory capabilities
- Mechanism: The Actor's Coder LLM component generates executable scripts based on the Planner's instructions, utilizing a self-reflection mechanism and memory module to analyze error messages, learn from previous attempts, and iteratively improve the generated code
- Core assumption: The Coder LLM can effectively translate abstract analysis plans into executable Python code and learn from its mistakes through the self-reflection process
- Evidence anchors:
  - [abstract] "The system comprises a Planner agent that interprets natural language queries and devises analysis strategies, and an Actor agent that executes the plans by generating and running Python code."
  - [section] "The Coder LLM is responsible for generating executable scripts based on the Planner's instructions... This feedback loop enables the LLM to analyze error messages within the task context, facilitating iterative improvement of the generated code."

### Mechanism 3
- Claim: The system achieves high performance on tasks up to Level 2 difficulty with 3-shot examples, significantly reducing manual intervention for simpler tasks
- Mechanism: By combining the Planner's domain-specific query interpretation with the Actor's code execution capabilities, the system can successfully automate data analysis tasks of increasing complexity
- Core assumption: The complexity of automotive software release queries can be effectively managed by the system's two-agent architecture with appropriate few-shot examples
- Evidence anchors:
  - [abstract] "Our results show that GoNoGo achieves a 100% success rate for tasks up to Level 2 difficulty with 3-shot examples, and maintains high performance even for more complex tasks."
  - [section] "Table 1. Performance evaluation of the GoNoGo system with varying numbers of example queries across different levels of task difficulty..."

## Foundational Learning

- Concept: Domain-specific knowledge representation and encoding
  - Why needed here: The automotive software release domain has specific terminology, data structures, and decision criteria that must be accurately captured for the system to interpret and execute queries correctly
  - Quick check question: Can you explain how the Knowledge Base is structured and what types of domain-specific information it contains?

- Concept: Chain-of-Thought (CoT) prompting and self-consistency
  - Why needed here: These techniques enhance the LLM's reasoning capabilities by breaking down complex problems into smaller, manageable steps and considering multiple reasoning paths for consistency
  - Quick check question: How does the combination of CoT prompting and self-consistency improve the Planner's ability to devise accurate analysis strategies?

- Concept: Self-reflection and iterative improvement in code generation
  - Why needed here: The Actor's Coder LLM must be able to analyze its own output, identify errors, and iteratively improve the generated code to ensure successful execution of the analysis plans
  - Quick check question: Can you describe how the self-reflection mechanism works in conjunction with the memory module to facilitate continuous improvement of the generated code?

## Architecture Onboarding

- Component map:
  - User -> Natural Language Query -> Planner (Knowledge Base + Few-shot examples) -> Analysis Strategy -> Actor (Coder LLM + Self-reflection + Memory) -> Python Code -> Plugins -> Results -> User

- Critical path:
  1. User submits natural language query
  2. Planner interprets query using Knowledge Base and few-shot examples
  3. Planner devises analysis strategy and generates step-by-step instructions
  4. Actor receives instructions and generates Python code using Coder LLM
  5. Code is executed with plugins for data interaction and analysis
  6. Results are compiled and returned to the user

- Design tradeoffs:
  - Centralizing complexity in the Planner vs. distributing it across multiple agents
  - Using pre-defined atomic actions (slicing and operation) to limit action space vs. allowing more flexible query interpretation
  - Employing few-shot learning vs. fine-tuning or pre-training the LLM on domain-specific data

- Failure signatures:
  - Planner: Misinterpretation of queries, inability to devise appropriate analysis strategies, or failure to adhere to domain-specific constraints
  - Actor: Generation of syntactically incorrect or semantically invalid Python code, failure to execute code successfully, or inability to learn from errors through self-reflection
  - System: Inability to handle queries beyond the complexity covered by few-shot examples, excessive execution time, or generation of incorrect results

- First 3 experiments:
  1. Test the Planner's ability to interpret simple queries using the Knowledge Base and few-shot examples, ensuring accurate translation into analysis plans
  2. Evaluate the Actor's code generation capabilities by providing it with pre-defined analysis plans and assessing the quality and executability of the generated Python code
  3. Assess the system's performance on tasks of increasing complexity, starting with Level 1 queries and progressing to Level 2, to validate the effectiveness of the two-agent architecture and few-shot learning approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GoNoGo's performance compare to traditional data analysis methods in terms of accuracy and efficiency for automotive software release decisions?
- Basis in paper: [inferred] The paper mentions that GoNoGo significantly reduces the time and effort required for data analysis and improves decision accuracy, but does not provide a direct comparison to traditional methods
- Why unresolved: The paper focuses on GoNoGo's performance and its impact on the industrial partner's company, but does not explicitly compare it to traditional methods
- What evidence would resolve it: A comparative study between GoNoGo and traditional data analysis methods, measuring accuracy, efficiency, and other relevant metrics for automotive software release decisions

### Open Question 2
- Question: Can GoNoGo's performance be further improved by incorporating additional domain-specific knowledge or fine-tuning the underlying LLMs?
- Basis in paper: [explicit] The paper mentions that GoNoGo utilizes a Knowledge Base containing structured descriptions of data and its attributes to enhance performance, and that the Planner and Actor agents use GPT-3.5 Turbo
- Why unresolved: The paper does not explore the potential improvements in GoNoGo's performance by incorporating additional domain-specific knowledge or fine-tuning the underlying LLMs
- What evidence would resolve it: An experimental study comparing GoNoGo's performance with and without additional domain-specific knowledge or fine-tuned LLMs, using the same benchmark and evaluation metrics

### Open Question 3
- Question: How does GoNoGo handle ambiguous or incomplete user queries, and what is the impact on its performance?
- Basis in paper: [inferred] The paper mentions that GoNoGo is designed to interpret natural language queries within the specific domain context, but does not discuss how it handles ambiguous or incomplete queries
- Why unresolved: The paper does not provide information on how GoNoGo deals with ambiguous or incomplete user queries, which could be a common scenario in real-world applications
- What evidence would resolve it: An experimental study evaluating GoNoGo's performance on a set of ambiguous or incomplete user queries, and analyzing the impact on its ability to generate accurate analysis plans and results

## Limitations

- Domain Generalization Uncertainty: While showing strong performance on automotive tasks, unclear how well the approach generalizes to other domains requiring tabular data analysis
- Scalability Concerns: Performance metrics for handling larger datasets or more complex queries beyond Level 4 difficulty are not provided
- Industrial Deployment Realities: Limited information about long-term adoption, maintenance overhead, and integration with existing workflows in the industrial setting

## Confidence

- **High Confidence**: The core mechanism of using a Planner agent with domain-specific knowledge to interpret queries and an Actor agent with self-reflection capabilities for code execution is well-supported by both theoretical framework and empirical results
- **Medium Confidence**: The claimed 100% success rate for Level 1-2 tasks with 3-shot examples is well-demonstrated, but performance on more complex tasks (Level 3-4) shows some degradation
- **Low Confidence**: The long-term impact on business outcomes and user adoption in the industrial setting is not thoroughly evaluated beyond initial pilot feedback

## Next Checks

1. **Cross-Domain Transferability Test**: Evaluate the system's performance on a different domain (e.g., healthcare data analysis or financial reporting) using the same architecture but with a newly constructed domain-specific Knowledge Base
2. **Stress Test with Larger Datasets**: Scale up the dataset to 10x the original size (550,000 records) and measure execution time, accuracy, and resource utilization to identify performance bottlenecks
3. **Long-term Deployment Study**: Conduct a 6-month longitudinal study with the industrial partner to track system usage patterns, maintenance requirements, user satisfaction evolution, and actual business impact quantification