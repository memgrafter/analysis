---
ver: rpa2
title: 'Semantics of Multiword Expressions in Transformer-Based Models: A Survey'
arxiv_id: '2401.15393'
source_url: https://arxiv.org/abs/2401.15393
tags:
- computational
- association
- linguistics
- pages
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey examines how transformer-based models represent the
  semantics of multiword expressions (MWEs), which are notoriously difficult to model
  due to their variable degrees of compositionality. The authors systematically review
  existing work on MWE processing with transformers, analyzing three main aspects:
  (i) whether MWE meanings are inherently captured and can be optimized, (ii) how
  representational information is localized across layers and tokens, and (iii) how
  linguistic properties affect representations.'
---

# Semantics of Multiword Expressions in Transformer-Based Models: A Survey

## Quick Facts
- arXiv ID: 2401.15393
- Source URL: https://arxiv.org/abs/2401.15393
- Reference count: 33
- Key outcome: This survey examines how transformer-based models represent the semantics of multiword expressions (MWEs), which are notoriously difficult to model due to their variable degrees of compositionality.

## Executive Summary
This survey systematically reviews existing work on MWE processing with transformer models, analyzing whether MWE meanings are inherently captured, how representational information is localized across layers and tokens, and how linguistic properties affect representations. The authors find that transformers capture MWE semantics inconsistently, relying heavily on memorized information rather than sophisticated meaning processing. Lower transformer layers generally perform better for MWE representation, and representations benefit from lower semantic idiosyncrasy and ambiguity. The survey highlights the need for more directly comparable evaluation setups and identifies priorities for future work in extending coverage to more MWE types and languages.

## Method Summary
The survey synthesizes findings from multiple studies that evaluate transformer models (BERT, RoBERTa, mBERT, XLNet, GPT-2/3, BART, T5) on various MWE tasks including phrase similarity, paraphrase identification, compositionality prediction, idiom idiomaticity classification, and metaphoricity detection. Most studies use off-the-shelf pretrained transformers with various evaluation approaches including intrinsic evaluations (probing tasks) and extrinsic evaluations (downstream tasks), with some fine-tuning or adapter-tuning for specific tasks. The review examines performance across different layers, token representations, and contextual scopes while considering linguistic properties like compositionality, polysemy, and frequency.

## Key Results
- Transformer models capture MWE semantics inconsistently and rely heavily on memorized information rather than compositional reasoning
- Lower transformer layers perform better for MWE representation, preserving surface features critical for processing
- Representations benefit from lower semantic idiosyncrasy and ambiguity, with contextual information generally improving performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lower transformer layers capture MWE semantics more effectively than higher layers
- Mechanism: Attention-based contextualization is weakest in early layers, preserving surface features and type-level lexical information critical for MWE representation
- Core assumption: Semantic idiosyncrasy and non-compositionality benefit from weakly contextualized representations
- Evidence anchors:
  - [abstract] "MWE meaning is also strongly localized, predominantly in early layers of the architecture"
  - [section 4.1] "Most surveyed papers report better performance in lower layers...least contextualized representations, assumed to capture surface linguistic features"
- Break condition: When MWEs require strong contextual disambiguation or when surface features are less informative

### Mechanism 2
- Claim: Transformer models represent MWEs primarily through memorized information rather than sophisticated semantic processing
- Mechanism: Models rely on recall of expression patterns from training data rather than compositional reasoning
- Core assumption: Memorization is the dominant strategy for handling non-compositional MWEs
- Evidence anchors:
  - [abstract] "reliance on surface patterns and memorized information"
  - [section 3.3] "Transformer-based MWE representations strongly rely on memorized information, as observed when generating subparts or paraphrases of target expressions"
- Break condition: When encountering novel expressions or when memorization capacity is exceeded

### Mechanism 3
- Claim: Contextual information improves MWE representation quality across multiple tasks
- Mechanism: Broader linguistic context enables better disambiguation and meaning specification for variable-compositionality expressions
- Core assumption: MWE interpretation benefits from surrounding textual information beyond immediate constituents
- Evidence anchors:
  - [abstract] "Representations benefit from specific linguistic properties, such as lower semantic idiosyncrasy and ambiguity of target expressions"
  - [section 4.3] "There is a clear consensus that contextual information is beneficial for modeling MWEs"
- Break condition: When context is ambiguous or when MWE meaning is independent of surrounding text

## Foundational Learning

- Concept: Multiword Expression (MWE) types and compositionality
  - Why needed here: Understanding the variable degrees of compositionality is essential for interpreting how transformer models handle different MWE categories
  - Quick check question: Can you distinguish between compositional compounds (climate change) and non-compositional idioms (kick the bucket)?

- Concept: Transformer architecture and attention mechanism
  - Why needed here: Layer-specific processing and token-level contextualization directly impact how MWE semantics are represented
  - Quick check question: How does multi-head attention distribute semantic information across tokens in an expression like "spill the beans"?

- Concept: Semantic similarity and representation evaluation
  - Why needed here: Assessing MWE representation quality requires understanding cosine similarity, correlation measures, and evaluation metrics
  - Quick check question: What would a high cosine similarity between "kick the bucket" and its paraphrase indicate about representation quality?

## Architecture Onboarding

- Component map: Input tokenization → Layer-wise representation → Token pooling/CLS aggregation → Contextual integration → Task-specific output
- Critical path: Token selection → Layer choice → Contextual scope → Evaluation setup
- Design tradeoffs: Layer depth vs. contextual strength, computational cost vs. representation quality, memorization vs. generalization
- Failure signatures: Over-reliance on surface patterns, poor performance on novel expressions, inconsistent layer-level behavior
- First 3 experiments:
  1. Compare MWE representation quality across layers using compositionality prediction task
  2. Test effect of contextual scope (phrase-only vs. sentence-level) on idiom classification
  3. Evaluate generalization to unseen MWEs using cross-validation with increasing vocabulary overlap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do transformer models inherently process MWE semantics through sophisticated meaning composition, or do they primarily rely on memorized information and surface patterns?
- Basis in paper: [explicit] The authors conclude that transformer models "capture MWE semantics inconsistently" and show "reliance on surface patterns and memorized information"
- Why unresolved: While the paper provides evidence that models rely on memorization, it's unclear whether this is a fundamental limitation of the architecture or if more sophisticated processing mechanisms exist but haven't been properly identified or triggered
- What evidence would resolve it: A systematic study comparing model performance on novel MWEs versus highly conventionalized ones, controlling for frequency and semantic transparency, while analyzing attention patterns and layer-wise processing differences

### Open Question 2
- Question: How do linguistic properties of MWEs (compositionality, polysemy, frequency, concreteness) interact with model architecture choices (layers, tokens, context) to affect representation quality?
- Basis in paper: [explicit] The authors note that "representations benefit from specific linguistic properties, such as lower semantic idiosyncrasy and ambiguity" and discuss how layer choice is affected by linguistic features
- Why unresolved: The paper identifies these interactions but doesn't systematically explore how different architectural choices might compensate for or exacerbate linguistic challenges, or whether certain MWE types benefit from specific model configurations
- What evidence would resolve it: Controlled experiments varying both linguistic properties and architectural parameters across multiple MWE types and languages to identify optimal model configurations for different expression characteristics

### Open Question 3
- Question: What are the cross-linguistic differences in how transformer models represent MWE semantics, and are these differences due to model architecture, training data, or fundamental language-specific properties?
- Basis in paper: [explicit] The authors note "limited studies on two languages in parallel" and acknowledge that "direct evidence of cross-linguistic variability is provided by limited studies on two languages"
- Why unresolved: Most research focuses on English, and the few cross-linguistic studies provide contradictory results without clear explanations for why different languages show different patterns
- What evidence would resolve it: Systematic comparative studies of MWE processing across multiple language pairs using identical tasks and model architectures, with careful control for training data characteristics and language family relationships

## Limitations
- Significant methodological heterogeneity across studies makes direct comparisons challenging
- Limited coverage of MWE types, with heavy focus on English noun compounds and idioms
- Most research focuses on English, limiting generalizability to other languages

## Confidence
**High Confidence**: The finding that lower transformer layers perform better for MWE representation is well-supported across multiple studies, with consistent evidence showing that weakly contextualized representations preserve surface features critical for MWE processing.

**Medium Confidence**: The claim about transformers relying on memorized information rather than sophisticated semantic processing is supported by evidence but requires more systematic investigation to rule out alternative explanations.

**Low Confidence**: Claims about specific linguistic properties affecting MWE representation quality (semantic idiosyncrasy, ambiguity) are based on relatively few studies with limited cross-linguistic validation.

## Next Checks
1. **Layer-Agnostic Evaluation**: Conduct systematic experiments comparing MWE representation quality across all transformer layers using identical evaluation protocols, controlling for task difficulty and MWE type, to validate the observed preference for lower layers.

2. **Novelty Generalization Test**: Design experiments that specifically test whether transformers can generalize to unseen MWEs by progressively increasing the overlap between training and test vocabularies, measuring the transition point where memorization-based performance breaks down.

3. **Cross-Linguistic Replication**: Replicate key findings from English-focused studies using parallel MWE datasets in typologically diverse languages (e.g., Mandarin, Arabic, Finnish) to assess the universality of observed patterns and identify language-specific factors affecting MWE representation.