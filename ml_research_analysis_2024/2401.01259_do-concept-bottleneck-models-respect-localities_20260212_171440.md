---
ver: rpa2
title: Do Concept Bottleneck Models Respect Localities?
arxiv_id: '2401.01259'
source_url: https://arxiv.org/abs/2401.01259
tags:
- concept
- locality
- leakage
- concept-based
- concepts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether concept-based models respect \"\
  localities\"\u2014whether concept predictions rely only on relevant features. The\
  \ authors introduce three metrics to quantify locality (leakage, intervention, and\
  \ masking) and evaluate them across synthetic and real-world datasets (CUB, COCO)."
---

# Do Concept Bottleneck Models Respect Localities?

## Quick Facts
- arXiv ID: 2401.01259
- Source URL: https://arxiv.org/abs/2401.01259
- Authors: Naveen Raman; Mateo Espinosa Zarlenga; Juyeon Heo; Mateja Jamnik
- Reference count: 26
- Key outcome: Concept-based models often fail to respect localities, with deeper models and less diverse training data showing higher locality violations.

## Executive Summary
This paper investigates whether concept-based models respect "localities"—whether concept predictions rely only on relevant features. The authors introduce three metrics to quantify locality (leakage, intervention, and masking) and evaluate them across synthetic and real-world datasets (CUB, COCO). They find that concept-based models often fail to respect localities, particularly deeper models and those trained on less diverse concept combinations. Theoretical analysis shows that highly correlated concepts can lead to models that ignore true concept presence. The work highlights limitations in current concept-based explainability methods and suggests using smaller models, diverse training data, and improved representational capacity to mitigate locality issues.

## Method Summary
The authors introduce three metrics to quantify locality in concept-based models: locality leakage (adversarial perturbations), locality intervention (in-distribution perturbations), and locality masking (out-of-distribution masking). They evaluate these metrics across synthetic datasets with varying object counts and concept combinations, as well as real-world datasets (CUB and COCO). The study compares baseline concept bottleneck models with variants including CEM, ProbCBM, and label-free CBMs, systematically varying model depth, training data diversity, and regularization strategies.

## Key Results
- Deeper concept predictors (more layers) lead to higher locality leakage due to spurious correlations between concepts
- Training data with less diverse concept combinations results in concept predictors that fail to distinguish between related concepts
- Highly correlated concepts in datasets enable concept predictors to achieve low error rates by exploiting correlations rather than learning true concept features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deeper concept predictors (more layers) lead to higher locality leakage because they learn spurious correlations between concepts rather than respecting feature locality.
- Mechanism: Increased depth allows models to capture more complex feature interactions, but also enables them to exploit indirect correlations between features outside a concept's local region and concept presence.
- Core assumption: The model has sufficient capacity to learn these spurious correlations and the training data contains enough instances where such correlations exist.
- Evidence anchors:
  - [abstract] "deeper models and those trained on less diverse concept combinations" show higher locality leakage
  - [section 5.2] "deeper models tend to have higher locality leakages... deeper models... produce more spurious correlations"
  - [corpus] No direct evidence; this is derived from the paper's findings
- Break condition: If the dataset contains no correlations between features outside local regions and concept presence, or if the model lacks capacity to learn these correlations.

### Mechanism 2
- Claim: Lack of diverse concept combinations in training data leads to concept predictors that fail to distinguish between related concepts, causing locality violations.
- Mechanism: When certain concept combinations are missing from training, the model cannot learn the true independence between concepts and instead learns spurious correlations to compensate for the missing information.
- Core assumption: The training dataset is incomplete and does not cover all possible concept combinations that appear in the data distribution.
- Evidence anchors:
  - [abstract] "those trained on less diverse concept combinations" fail to respect localities
  - [section 5.3] "datasets with a larger number of concept combinations can reduce the locality intervention metric"
  - [corpus] No direct evidence; this is derived from the paper's findings
- Break condition: If the training data contains all possible concept combinations or if concepts are truly independent in the data distribution.

### Mechanism 3
- Claim: Highly correlated concepts in the dataset enable concept predictors to achieve low error rates by exploiting correlations rather than learning true concept features.
- Mechanism: When concepts are highly correlated, a predictor for one concept can achieve low error by simply predicting based on another correlated concept, bypassing the need to analyze the actual local features of the target concept.
- Core assumption: The dataset contains sufficient examples where correlated concepts co-occur, allowing the model to learn these correlations.
- Evidence anchors:
  - [abstract] "theoretical analysis shows that highly correlated concepts can lead to models that ignore true concept presence"
  - [section 6] "Theorem 6.1... if concept j is correlated sufficiently many other concepts i then we can train a concept predictor with low error"
  - [corpus] No direct evidence; this is derived from the paper's theoretical analysis
- Break condition: If concepts are truly independent or if the dataset lacks sufficient co-occurrence examples of correlated concepts.

## Foundational Learning

- Concept: Locality in concept-based models
  - Why needed here: Understanding locality is fundamental to grasping why concept predictors might fail to provide trustworthy explanations
  - Quick check question: If a concept "Red Fruit" is localized to the fruit region in an image, should changes to the background affect the concept prediction?

- Concept: Concept bottleneck models (CBMs)
  - Why needed here: The paper specifically analyzes CBMs, so understanding their architecture is essential
  - Quick check question: In a CBM, what are the two main components and what is their relationship?

- Concept: Spurious correlations in machine learning
  - Why needed here: The paper draws parallels between locality violations and spurious correlation learning
  - Quick check question: How do spurious correlations differ from true causal relationships in model predictions?

## Architecture Onboarding

- Component map:
  - Input features → Concept predictor (g) → Concept predictions → Label predictor (f) → Final prediction
  - Three locality metrics: leakage (adversarial), intervention (in-distribution), masking (out-of-distribution)

- Critical path: Understanding how concept predictors use features → Measuring locality violation → Analyzing architectural and dataset factors → Theoretical justification

- Design tradeoffs: Deeper models have better representational capacity but may exploit spurious correlations; diverse training data improves concept distinction but requires more data collection

- Failure signatures: High locality leakage (>0.4) indicates concept predictors use features outside local regions; similar performance between relevant and irrelevant masking indicates poor feature localization

- First 3 experiments:
  1. Vary model depth (3-7 layers) on synthetic dataset and measure locality leakage to confirm depth increases locality violations
  2. Vary training data diversity (25%-100% concept combinations) on CUB/COCO and measure locality intervention to confirm diversity improves locality
  3. Compare different CBM variants (baseline, CEM, ProbCBM, label-free) on real datasets using locality masking to identify which architectures best respect locality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of concept predictor architecture (e.g., depth, width, or use of residual connections) quantitatively impact locality leakage across diverse real-world datasets beyond CUB and COCO?
- Basis in paper: [explicit] The paper explores architecture choices in Section 5.2, finding deeper models have higher locality leakage in synthetic datasets, and residual connections fail to improve locality in Section B.5.
- Why unresolved: The analysis is limited to synthetic data for architecture and a single residual connection experiment; no systematic ablation across multiple real-world datasets is provided.
- What evidence would resolve it: Comparative experiments measuring locality leakage (and masking metrics) across varied architectures (e.g., ResNet, Vision Transformers) and depths on multiple datasets (e.g., ImageNet, VTAB).

### Open Question 2
- Question: Can novel training strategies or regularization methods be designed to enforce locality constraints without sacrificing concept prediction accuracy?
- Basis in paper: [inferred] Section 5.4 shows that ℓ2 regularization and adversarial training reduce locality leakage but also reduce accuracy, and deeper models inherently increase leakage.
- Why unresolved: The paper does not explore alternative loss functions, explicit locality-aware penalties, or curriculum learning strategies to enforce locality.
- What evidence would resolve it: Experiments comparing accuracy and locality leakage for models trained with novel locality-aware regularization terms or constrained optimization techniques.

### Open Question 3
- Question: How does the degree of concept correlation in a dataset quantitatively predict the theoretical error bound on concept predictor fidelity, and can this relationship be used to pre-screen datasets for locality preservation?
- Basis in paper: [explicit] Section 6 presents Theorem 6.1 linking concept correlations (α) and concept frequency (β) to the error bound (ϵ) on concept predictors, and Section 5.3 shows diverse concept combinations reduce locality intervention.
- Why unresolved: The theorem is theoretical; empirical validation linking measured concept correlations to actual locality leakage or intervention metrics is not provided.
- What evidence would resolve it: Empirical studies measuring inter-concept correlations in datasets and correlating them with locality metrics across model architectures and training regimes.

### Open Question 4
- Question: Does the locality masking metric capture different failure modes than locality leakage and intervention, and how do these metrics behave under distribution shifts (e.g., domain adaptation or out-of-distribution samples)?
- Basis in paper: [explicit] Section 4.3 introduces locality masking as a metric between leakage and intervention, and Section 5.3 finds masking reveals concept predictors rely on spurious correlations in real datasets.
- Why unresolved: The paper does not evaluate locality masking under controlled distribution shifts or compare its sensitivity to leakage/intervention across different types of perturbations.
- What evidence would resolve it: Experiments measuring all three locality metrics under domain shifts or synthetic distribution shifts to assess their discriminative power and failure modes.

## Limitations

- Theoretical analysis relies on simplified assumptions about concept correlations that may not capture complex real-world dependencies
- Synthetic datasets may not fully represent the complexity of natural image distributions
- Locality metrics measure indirect proxies for true locality violation rather than directly observing feature usage patterns

## Confidence

- **High Confidence**: The empirical finding that deeper models exhibit higher locality leakage is strongly supported by consistent results across multiple datasets and model variants
- **Medium Confidence**: The claim that diverse concept combinations improve locality relies on correlational evidence from limited datasets
- **Medium Confidence**: The theoretical analysis linking concept correlations to locality violations is mathematically sound but makes simplifying assumptions about feature independence

## Next Checks

1. **Cross-domain validation**: Apply the locality metrics to non-visual concept-based models (e.g., text or tabular data) to verify that the findings generalize beyond image datasets

2. **Architectural ablation study**: Systematically vary model components (attention mechanisms, skip connections, normalization) while holding depth constant to isolate which architectural features contribute most to locality violations

3. **Dataset coverage analysis**: Quantify the relationship between training dataset concept combination coverage and locality metrics across multiple domains to establish whether the diversity effect is domain-specific or universal