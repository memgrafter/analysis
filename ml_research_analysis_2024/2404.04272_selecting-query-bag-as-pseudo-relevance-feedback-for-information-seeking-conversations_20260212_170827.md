---
ver: rpa2
title: Selecting Query-bag as Pseudo Relevance Feedback for Information-seeking Conversations
arxiv_id: '2404.04272'
source_url: https://arxiv.org/abs/2404.04272
tags:
- query
- query-bag
- matching
- representation
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving information-seeking
  dialogue systems by enhancing query representation through the use of related queries.
  The proposed Query-bag Pseudo Relevance Feedback (QB-PRF) framework constructs a
  query-bag of related queries to serve as pseudo relevance feedback signals.
---

# Selecting Query-bag as Pseudo Relevance Feedback for Information-seeking Conversations

## Quick Facts
- arXiv ID: 2404.04272
- Source URL: https://arxiv.org/abs/2404.04272
- Reference count: 10
- Primary result: Query-bag Pseudo Relevance Feedback (QB-PRF) framework improves retrieval in information-seeking dialogue systems using contrastive learning and attention-based query fusion.

## Executive Summary
This paper introduces a novel framework, Query-bag Pseudo Relevance Feedback (QB-PRF), to enhance query representation in information-seeking dialogue systems. The framework leverages a query-bag of related queries as pseudo relevance feedback signals to enrich semantic understanding. It employs a Query-bag Selection (QBS) module using contrastive learning to filter and select semantically equivalent queries, and a Query-bag Fusion (QBF) module that aggregates these queries via attention mechanisms to refine the original query's representation. Experiments on two benchmark datasets with BERT and GPT-2 backbone models show significant performance improvements over strong baselines, particularly in MRR and R10@1 metrics.

## Method Summary
The QB-PRF framework operates in four stages: (1) Representation Learning using a VAE trained on a large corpus to encode queries into distinctive embeddings; (2) Query-bag Selection (QBS) that retrieves and re-ranks top-k candidates using contrastive learning to select semantically equivalent queries; (3) Query-bag Fusion (QBF) that fuses the original query with the selected bag via cross-attention and self-attention in a transformer network; and (4) Text Matching where the fused representation is used in a BERT or GPT-2 classifier for final retrieval. The approach is jointly trained with a combination of contrastive, reward, and cross-entropy losses.

## Key Results
- QB-PRF significantly outperforms strong baselines on two benchmark datasets (LaiYe and Quora) in MRR and R10@1 metrics.
- The QBS module effectively reduces noise in query-bag candidates, improving downstream matching accuracy.
- Joint training of QBS, QBF, and matching stages stabilizes convergence and enhances overall performance.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Query-bag selection (QBS) improves retrieval by filtering noisy candidates and keeping semantically equivalent queries.
- Mechanism: Uses contrastive learning with VAE representations to score and select only queries that share the same intent as the original query.
- Core assumption: Queries with high similarity in VAE embedding space are semantically equivalent and thus useful for PRF.
- Evidence anchors:
  - [abstract]: "QBS module that uses contrastive learning to select synonymous queries"
  - [section]: "QBS module is utilized to select query-bag with little noise for the refinement of the corresponding query."
  - [corpus]: Weak evidence; corpus shows only one related PRF paper, indicating novelty of approach.
- Break condition: If VAE embeddings do not capture intent well (e.g., due to domain shift), selected queries will be noisy and degrade performance.

### Mechanism 2
- Claim: Query-bag fusion (QBF) enriches query representation by aggregating multi-query context through attention.
- Mechanism: Cross-attention between original query and each bag query, followed by self-attention to merge them into a single refined embedding.
- Core assumption: Multi-query context captures aspects missed by single-query matching, improving downstream classifier decisions.
- Evidence anchors:
  - [abstract]: "QBF module that fuses synonymous queries to enhance the semantic representation"
  - [section]: "fuses synonymous queries to enhance the semantic representation of the original query through multidimensional attention computation"
  - [corpus]: Weak evidence; no explicit fusion mechanism cited in neighbors.
- Break condition: If bag size is too small or queries are too dissimilar, fusion provides no gain or adds noise.

### Mechanism 3
- Claim: Joint training of QBS, QBF, and matching improves convergence and overall performance.
- Mechanism: Combines InfoNCE selection loss, reward loss, and cross-entropy matching loss in a single optimization.
- Core assumption: Shared gradients between selection, fusion, and matching stages stabilize training and improve final accuracy.
- Evidence anchors:
  - [abstract]: "We verify the effectiveness of the QB-PRF framework on two competitive pretrained backbone models, including BERT and GPT-2."
  - [section]: "LStage2 = λ1 · Lb + (1 − λ1) · Lreward + λ2 · LCE"
  - [corpus]: Weak evidence; no joint-training framework cited in neighbors.
- Break condition: If loss terms are unbalanced, joint training may collapse to optimizing only one stage.

## Foundational Learning

- Concept: Contrastive learning for semantic similarity
  - Why needed here: Enables unsupervised selection of semantically equivalent queries from large corpora.
  - Quick check question: What loss function ensures that similar queries are embedded close while dissimilar ones are far apart?

- Concept: Variational Autoencoder (VAE) for sentence representation
  - Why needed here: Provides differentiable, fixed-size embeddings that capture semantic content for downstream selection.
  - Quick check question: How does the VAE's reconstruction loss differ from a standard autoencoder in this context?

- Concept: Multi-head attention for context fusion
  - Why needed here: Allows query-bag information to be weighted and merged without losing important query tokens.
  - Quick check question: In a transformer layer, what is the role of the cross-attention sub-layer when fusing two sequences?

## Architecture Onboarding

- Component map:
  Query → VAE encode → Dense retrieval → QBS re-rank → QBF fusion → Downstream matching → Score.
- Critical path:
  Query → VAE encode → Dense retrieval → QBS re-rank → QBF fusion → Downstream matching → Score.
- Design tradeoffs:
  - Larger bag size increases recall but also noise; smaller bags are cleaner but may miss useful variants.
  - Joint training stabilizes QBS/QBF but requires careful loss weighting; staged training isolates components but may misalign embeddings.
  - Using pre-trained VAE speeds up representation learning but may introduce domain mismatch.
- Failure signatures:
  - QBS accuracy drops → bag contains irrelevant queries → matching performance degrades.
  - QBF attention weights concentrate on one query → fusion adds little value.
  - Loss imbalance → training diverges or converges to trivial solution.
- First 3 experiments:
  1. Vary top-k retrieval candidates and measure QBS accuracy and downstream MRR.
  2. Ablate QBS and fuse all retrieved queries; compare to full QB-PRF.
  3. Compare joint vs staged training curves for convergence speed and final MRR.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of QB-PRF vary with different sizes of query-bags, and what is the optimal size for maximizing effectiveness?
- Basis in paper: [explicit] The paper mentions that the size of query-bags does not always bring maximum improvement and suggests that the quantity and quality of query-bags need to be balanced. It also notes that the average query-bag size of the LaiYe dataset is relatively large, and different settings can achieve the best results.
- Why unresolved: The paper does not provide a definitive answer on the optimal size of query-bags, as it varies depending on the dataset's characteristics.
- What evidence would resolve it: Empirical studies comparing the performance of QB-PRF across various query-bag sizes on multiple datasets to determine the optimal size for maximizing effectiveness.

### Open Question 2
- Question: How does the inclusion of external knowledge, such as knowledge graphs, impact the performance of QB-PRF in information-seeking conversations?
- Basis in paper: [inferred] The paper discusses the effectiveness of QB-PRF in enhancing query representation and matching performance. It also mentions the potential for manipulating the framework in other tasks, such as retrieval-augmented generation, which could benefit from external knowledge.
- Why unresolved: The paper does not explore the integration of external knowledge sources, such as knowledge graphs, into the QB-PRF framework.
- What evidence would resolve it: Experiments comparing the performance of QB-PRF with and without the integration of external knowledge sources, such as knowledge graphs, on information-seeking datasets.

### Open Question 3
- Question: What are the limitations of the current query-bag retrieval methods, and how can they be improved to ensure a more diverse and relevant set of queries?
- Basis in paper: [explicit] The paper acknowledges that the retrieved query-bag candidates are relatively scarce and may cause the PRF framework to have imbalanced query-bag quantity and quality. It also suggests that improving the recall of retrieval could verify the efficiency of the method.
- Why unresolved: The paper does not provide specific solutions or methods to address the limitations of query-bag retrieval.
- What evidence would resolve it: Research and development of new retrieval methods or enhancements to existing methods that increase the diversity and relevance of query-bag candidates, followed by experiments demonstrating improved performance of QB-PRF.

## Limitations

- Performance heavily depends on the quality of VAE embeddings; domain mismatch can degrade query-bag selection accuracy.
- The framework's effectiveness may be limited by the scarcity and relevance of retrieved query-bag candidates.
- Optimal query-bag size is dataset-dependent and not fully resolved, potentially limiting generalizability.

## Confidence

- QBS mechanism: Medium confidence (strong theory but limited ablation on selection quality).
- QBF mechanism: High confidence (well-established transformer attention methods).
- Joint training benefit: Medium confidence (promising results but complex loss balancing).
- Scalability claim: Low confidence (experiments use fixed bag sizes without exploring diverse query distributions).

## Next Checks

1. Measure QBS accuracy (precision/recall of synonymous query selection) on held-out validation sets to isolate selection quality from downstream effects.
2. Compare downstream performance when fusing all top-k retrieved queries vs. QB-PRF-selected subsets to quantify noise filtering benefit.
3. Test framework robustness by evaluating on out-of-domain queries to assess VAE embedding generalization and selection stability.