---
ver: rpa2
title: A Methodology for Explainable Large Language Models with Integrated Gradients
  and Linguistic Analysis in Text Classification
arxiv_id: '2410.00250'
source_url: https://arxiv.org/abs/2410.00250
tags:
- https
- page
- alzheimer
- arxiv
- sdisease
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces SLIME, an explainable LLM methodology for
  text classification that combines Integrated Gradients (IG) for token-level attribution
  with Linguistic Inquiry and Word Count (LIWC) for linguistic feature identification.
  Applied to Alzheimer's disease classification using spontaneous speech from the
  Cookie Theft picture description task, the method achieves 87% accuracy while providing
  interpretable explanations of which lexical components most influence model decisions.
---

# A Methodology for Explainable Large Language Models with Integrated Gradients and Linguistic Analysis in Text Classification

## Quick Facts
- arXiv ID: 2410.00250
- Source URL: https://arxiv.org/abs/2410.00250
- Reference count: 11
- Achieves 87% accuracy in Alzheimer's disease classification using spontaneous speech data

## Executive Summary
This study introduces SLIME, a novel explainable AI methodology for text classification that combines Integrated Gradients (IG) for token-level attribution with Linguistic Inquiry and Word Count (LIWC) for linguistic feature identification. Applied to Alzheimer's disease classification using spontaneous speech from the Cookie Theft picture description task, SLIME achieves 87% accuracy while providing interpretable explanations of which lexical components most influence model decisions. The method identifies key linguistic features like social references and personal pronouns that differ between AD and control groups, offering clinically relevant insights while maintaining computational reliability. SLIME is designed to be generalizable to other binary classification tasks and provides both global insights into linguistic markers and individual-level explanations of model decisions.

## Method Summary
SLIME is an explainable LLM methodology that integrates Integrated Gradients for computing token-level attributions with LIWC for identifying linguistically interpretable features. The method fine-tunes a BERT base model on transcribed speech from the Cookie Theft task (156 samples, 78 AD/78 control) using AdamW optimizer (learning rate=2e-5, eps=1e-8) for 50 epochs with batch size=1. IG attributions are computed on the embedding layer, then tokens are annotated with LIWC categories. Statistical analysis using 5000 random subsamples identifies which linguistic features significantly contribute to classification accuracy, with features exceeding the 95th or below the 5th percentile considered significant. The methodology provides both token-level explanations and higher-level linguistic insights about AD speech patterns.

## Key Results
- Achieves 87% classification accuracy in 5-fold cross-validation for AD detection from spontaneous speech
- Identifies social references and personal pronouns as the most relevant linguistic features for distinguishing AD from control groups
- Demonstrates statistical significance of identified features through AUC comparison with random subsampling
- Provides both global insights into linguistic markers and individual-level explanations of model decisions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrated Gradients provides reliable token-level attributions that reflect the model's decision-making process.
- Mechanism: IG computes attributions by integrating gradients along a path from a baseline to the input, satisfying mathematical axioms of sensitivity and implementation invariance.
- Core assumption: The attribution path integral captures meaningful feature importance without being affected by implementation details.
- Evidence anchors:
  - [abstract] "IG scores, denoted as ð¼ðº(ð‘¥) = (ð‘Ž1, ..., ð‘›) âˆˆ â„œ, rate the contribution of each element in the input toward the prediction."
  - [section] "IG is a reliable approach for model explainability, as it is based on the mathematical axioms of Sensitivity and Implementation Invariance."
  - [corpus] Weak evidence - no corpus papers specifically discuss IG's reliability for this application.
- Break condition: If the model's gradients are saturated or if the baseline choice significantly affects attributions.

### Mechanism 2
- Claim: LIWC annotations provide linguistically interpretable features that map token attributions to meaningful linguistic categories.
- Mechanism: LIWC assigns each token to predefined linguistic categories, allowing researchers to identify which linguistic features drive model decisions.
- Core assumption: LIWC categories capture meaningful linguistic patterns relevant to AD classification.
- Evidence anchors:
  - [abstract] "Building on these attributions, we introduce a method to identify statistically significant linguistic attributes of these words, revealing the most relevant terms and providing insights into their critical linguistic features."
  - [section] "LIWC assigns each word in the input text to one or more predefined linguistic categories, such as pronouns, social references, or function words."
  - [corpus] Moderate evidence - corpus contains related work on linguistic features for dementia detection.
- Break condition: If LIWC categories don't align with the linguistic patterns that distinguish AD from control groups.

### Mechanism 3
- Claim: Statistical analysis identifies which linguistic features significantly contribute to classification accuracy.
- Mechanism: The method compares attribution distributions for linguistic features against random samples to determine statistical significance and impact on AUC.
- Core assumption: Statistical significance in attribution distributions indicates meaningful contribution to classification.
- Evidence anchors:
  - [abstract] "Through statistical analysis, it is possible to infer the most relevant attributes for classification and their linguistic characteristics."
  - [section] "We conduct 5000 random subsamples of the same size as the linguistic feature of interest and check if it is above the 95th percentile or below the 5th percentile of the subsampled distribution."
  - [corpus] Weak evidence - no corpus papers discuss this specific statistical approach for explainability.
- Break condition: If the feature distributions don't differ significantly between AD and control groups.

## Foundational Learning

- Concept: Token-level attribution in neural networks
  - Why needed here: Understanding how IG computes attributions at the token level is crucial for implementing the explainability pipeline.
  - Quick check question: How does IG differ from simple gradient-based attribution methods?

- Concept: Linguistic feature analysis with LIWC
  - Why needed here: The method relies on mapping tokens to LIWC categories to provide interpretable explanations.
  - Quick check question: What are the limitations of using LIWC categories for linguistic analysis?

- Concept: Statistical significance testing for feature importance
  - Why needed here: The method uses statistical tests to determine which linguistic features are truly relevant for classification.
  - Quick check question: Why use random subsampling rather than parametric tests for this analysis?

## Architecture Onboarding

- Component map:
  Input: Transcribed speech from Cookie Theft task -> BERT model (fine-tuned) -> Integrated Gradients computation -> LIWC annotation -> Statistical analysis -> Attribution scores and linguistic explanations

- Critical path:
  1. Fine-tune BERT on AD classification task
  2. Compute IG attributions for input tokens
  3. Annotate tokens with LIWC categories
  4. Perform statistical analysis to identify significant features
  5. Generate interpretable explanations

- Design tradeoffs:
  - Using pre-trained BERT vs. training from scratch (accuracy vs. computational cost)
  - Token-level vs. word-level attributions (granularity vs. interpretability)
  - Statistical significance threshold selection (sensitivity vs. specificity)

- Failure signatures:
  - Poor classification accuracy (indicates model training issues)
  - Random or uniform attribution distributions (indicates IG implementation problems)
  - No statistically significant features (indicates dataset or LIWC mapping issues)

- First 3 experiments:
  1. Fine-tune BERT on a small subset of the dataset and verify classification accuracy
  2. Compute IG attributions on a single sample and visualize the distribution
  3. Apply LIWC to a small set of tokens and verify the category assignments match expectations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well would SLIME generalize to other neurodegenerative diseases beyond Alzheimer's, such as Parkinson's disease or frontotemporal dementia, which may have different speech patterns?
- Basis in paper: [inferred] The paper demonstrates SLIME's effectiveness on Alzheimer's disease classification but acknowledges its generalizability to other binary classification tasks. The authors suggest the method could be applied to various clinical contexts studying neurodegeneration.
- Why unresolved: The paper only validates the methodology on Alzheimer's disease data. Different neurodegenerative conditions may exhibit distinct linguistic markers that SLIME hasn't been tested against, and the method's sensitivity to these differences remains unknown.
- What evidence would resolve it: Empirical validation of SLIME on datasets from other neurodegenerative conditions, comparing its performance and explainability features across different diseases, would establish its broader applicability.

### Open Question 2
- Question: How does the choice of baseline in Integrated Gradients affect the reliability and interpretability of SLIME's explanations, and what is the optimal baseline for text classification tasks?
- Basis in paper: [explicit] The paper mentions that "a baseline is defined for the IG computation to obtain meaningful attributions" and discusses how "the use of a baseline ensures that IG satisfies sensitivity," but doesn't explore how different baselines might impact results.
- Why unresolved: The paper acknowledges the importance of baseline selection but doesn't investigate how varying baselines (such as zero embeddings, random noise, or mean embeddings) might affect the attribution scores and the subsequent linguistic analysis, which could significantly impact the method's reliability.
- What evidence would resolve it: Systematic comparison of SLIME's performance and explainability using different baseline strategies across multiple datasets and classification tasks would identify optimal baseline choices and their impact on reliability.

### Open Question 3
- Question: How does SLIME's explainability compare to other XAI methods like SHAP or LIME when applied to large language models for clinical text classification tasks?
- Basis in paper: [inferred] While the paper validates SLIME's effectiveness and provides statistical comparisons with feature count-based methods, it doesn't benchmark against other established XAI techniques that are commonly used for neural networks and text classification.
- Why unresolved: The paper establishes SLIME's superiority over simple feature counting methods but doesn't position it within the broader landscape of explainable AI methods, leaving questions about its relative strengths and weaknesses compared to alternative approaches.
- What evidence would resolve it: Head-to-head comparisons of SLIME with SHAP, LIME, and other XAI methods on the same clinical datasets, measuring both explanation quality and computational efficiency, would clarify its relative advantages and appropriate use cases.

## Limitations

- Dataset size and generalizability concerns due to limited sample size (156 transcriptions) may affect robustness of findings
- Implementation-specific dependencies on baseline selection and IG integration steps are not fully characterized
- LIWC coverage and potential cultural/linguistic biases may limit effectiveness for diverse populations

## Confidence

**SLIME Methodology Effectiveness** (Medium): Supported by 87% accuracy result and identification of relevant linguistic features, but limited by small dataset and lack of comparison with alternative explainability methods.

**IG Reliability for Token Attribution** (High): Strong theoretical foundation in mathematical axioms of sensitivity and implementation invariance, though implementation details affect practical reliability.

**LIWC Mapping for Interpretability** (Medium): LIWC is widely-used tool, but effectiveness depends on alignment between predefined categories and actual linguistic differences between AD and control groups.

## Next Checks

1. **Replication on Independent Dataset**: Apply the complete SLIME pipeline to an independent Alzheimer's dataset (e.g., DementiaBank or ADReSS Challenge 2021) to verify that the same linguistic features (social references, personal pronouns) emerge as significant and that classification accuracy remains above 80%.

2. **Ablation Study for Method Components**: Systematically remove or replace components of the methodology (e.g., use random attributions instead of IG, use alternative linguistic analysis instead of LIWC) to quantify the contribution of each component to both classification accuracy and interpretability quality.

3. **Statistical Significance Robustness Test**: Vary the random subsampling parameters (number of samples, significance thresholds) and the baseline selection for IG to assess the stability of identified linguistic features across different methodological choices, ensuring that results are not artifacts of specific parameter selections.