---
ver: rpa2
title: Data Fusion of Synthetic Query Variants With Generative Large Language Models
arxiv_id: '2411.03881'
source_url: https://arxiv.org/abs/2411.03881
tags:
- query
- queries
- retrieval
- data
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using synthetic query variants generated
  by instruction-tuned Large Language Models (LLMs) in data fusion experiments for
  information retrieval. The study introduces a lightweight, unsupervised, and cost-efficient
  approach that leverages principled prompting techniques and data fusion methods
  to improve retrieval effectiveness.
---

# Data Fusion of Synthetic Query Variants With Generative Large Language Models

## Quick Facts
- arXiv ID: 2411.03881
- Source URL: https://arxiv.org/abs/2411.03881
- Reference count: 40
- Uses synthetic LLM-generated query variants in data fusion experiments to improve information retrieval effectiveness

## Executive Summary
This paper investigates using synthetic query variants generated by instruction-tuned Large Language Models (LLMs) in data fusion experiments for information retrieval. The study introduces a lightweight, unsupervised, and cost-efficient approach that leverages principled prompting techniques and data fusion methods to improve retrieval effectiveness. The method involves generating multiple topically related queries using LLMs, retrieving document rankings for each query variant, and then combining these rankings using Reciprocal Rank Fusion (RRF). Experiments conducted on four TREC newswire benchmarks demonstrate that fused rankings based on synthetic query variants significantly outperform baselines using single queries and also surpass pseudo-relevance feedback methods.

## Method Summary
The approach uses three prompting strategies (P-1: title only, P-2: title+description+narrative, P-3: P-2+query examples) to generate 100 synthetic query variants per topic using GPT-4o. These queries are processed through BM25 retrieval, and the resulting rankings are combined using Reciprocal Rank Fusion (RRF). The method is evaluated on four TREC newswire collections (Robust04, Robust05, Core17, Core18) using P@10, NDCG@10, Bpref, and MAP metrics, comparing against BM25 and BM25+RM3 baselines.

## Key Results
- Fused rankings based on synthetic query variants significantly outperform single-query baselines across all four TREC collections
- Prompting strategy P-2 (including topic description and narrative) performs best, while P-1 and P-3 are less effective
- The approach surpasses pseudo-relevance feedback methods like RM3
- Generated over 120,000 query strings for approximately $22.50

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fusing multiple synthetic query variants improves retrieval effectiveness compared to a single query.
- Mechanism: Different synthetic queries capture distinct aspects of the information need, leading to a broader coverage of relevant documents. The Reciprocal Rank Fusion (RRF) method combines these diverse rankings into a single, more effective ranking.
- Core assumption: The synthetic queries generated by the LLM are topically related to the original information need and capture diverse aspects of it.
- Evidence anchors:
  - [abstract] "fused rankings based on synthetic query variants significantly outperform baselines using single queries"
  - [section] "In general, the fused rankings outperform the baselines BM25 ( + RM3) in most cases"

### Mechanism 2
- Claim: Providing additional context information (description and narrative) in the prompt leads to more effective synthetic queries.
- Mechanism: The LLM uses the additional context to better understand the nuances of the information need, resulting in more targeted and effective query variants.
- Core assumption: The LLM can effectively utilize the additional context information to generate more relevant queries.
- Evidence anchors:
  - [abstract] "LLMs produce more effective queries when provided with additional context information on the topic"
  - [section] "For all four test collections, fused rankings with queries based on the prompting strategy P-2 perform best"

### Mechanism 3
- Claim: The proposed method is lightweight and cost-efficient compared to other query generation approaches.
- Mechanism: The method uses off-the-shelf LLMs and data fusion techniques without requiring task-specific fine-tuning or complex query modeling methods.
- Core assumption: The cost and complexity of the proposed method are significantly lower than alternative approaches.
- Evidence anchors:
  - [abstract] "lightweight, unsupervised, and cost-efficient approach"
  - [section] "In sum, we curated four datasets of query variants for TREC test collections covering three prompting strategies with over 120,000 query strings for approximately $22.50"

## Foundational Learning

- Concept: Data Fusion Methods (e.g., Reciprocal Rank Fusion)
  - Why needed here: To combine the multiple rankings obtained from synthetic query variants into a single, more effective ranking.
  - Quick check question: How does Reciprocal Rank Fusion (RRF) combine multiple rankings, and what is the role of the free parameter k?

- Concept: Prompt Engineering for LLMs
  - Why needed here: To effectively guide the LLM in generating relevant and diverse synthetic query variants.
  - Quick check question: What are the key components of a well-structured prompt for generating synthetic queries, and how does the inclusion of context information (e.g., description and narrative) impact the quality of the generated queries?

- Concept: Information Retrieval Evaluation Metrics (e.g., P@10, nDCG@10, MAP)
  - Why needed here: To assess the effectiveness of the proposed method compared to baselines and understand its impact on retrieval performance.
  - Quick check question: What do the evaluation metrics P@10, nDCG@10, and MAP measure, and how do they provide insights into the quality of the retrieved rankings?

## Architecture Onboarding

- Component map: Topic files (title/description/narrative) -> Prompt construction -> LLM query generation -> BM25 retrieval -> RRF data fusion -> Evaluation
- Critical path: Topic files → Prompt construction → LLM query generation → Retrieval → Data fusion (RRF) → Evaluation
- Design tradeoffs:
  - Number of synthetic queries to generate: More queries can improve effectiveness but increase computational cost and latency.
  - Prompt structure: Balancing the inclusion of context information for better query quality without overwhelming the LLM.
  - Choice of data fusion method: RRF is simple and effective, but other methods (e.g., learning-to-rank) may offer additional benefits at the cost of complexity.
- Failure signatures: Decreased retrieval effectiveness compared to the single-query baseline, high similarity between synthetic queries, LLM generating irrelevant or off-topic queries
- First 3 experiments:
  1. Generate a small set of synthetic queries (e.g., 3-5) for a few topics and compare the fused ranking's effectiveness against the single-query baseline.
  2. Vary the prompt structure (e.g., include/exclude description and narrative) and assess the impact on query quality and retrieval effectiveness.
  3. Experiment with different numbers of synthetic queries (e.g., 3, 5, 10, 20) and analyze the trade-off between effectiveness and computational cost.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of synthetic query variants to generate for effective data fusion without causing topic drift or excessive computational costs?
- Basis in paper: [explicit] The paper discusses that while generating an arbitrary number of queries is possible, there are limits to improvement and potential topic drift with too many queries (Figure 2).
- Why unresolved: The paper only tests 3, 5, 10, and 100 queries, showing diminishing returns beyond 10 queries, but does not establish a definitive optimal number that balances effectiveness and efficiency.
- What evidence would resolve it: Systematic experiments testing intermediate numbers of queries (e.g., 15, 20, 25) across diverse test collections to identify the point where additional queries no longer significantly improve retrieval effectiveness.

### Open Question 2
- Question: How do different prompting strategies affect the quality and topical relevance of synthetic query variants generated by LLMs?
- Basis in paper: [explicit] The paper compares three prompting strategies (P-1, P-2, P-3) and finds that P-2 (including topic description and narrative) performs best, but notes that P-1 and P-3 are less effective and suggests more in-depth analysis is needed.
- Why unresolved: While the paper identifies that additional topical context improves effectiveness, it does not explore other prompt engineering techniques or systematically evaluate the impact of different prompt formulations.
- What evidence would resolve it: Comparative experiments testing various prompt structures, including different ways of incorporating topic context, varying temperature parameters, and using different examples in chain-of-thought prompting.

### Open Question 3
- Question: How does the cost-effectiveness tradeoff between traditional retrieval methods (like BM25) and LLM-based approaches compare when using synthetic query variants for data fusion?
- Basis in paper: [inferred] The paper mentions that their approach uses LLMs to generate short keyword queries, letting BM25 rank documents with lower computational costs than having LLMs rerank documents, but suggests future work should compare computational and financial costs.
- Why unresolved: The paper does not provide quantitative comparisons of computational resources, financial costs, or latency between their approach and other methods like direct LLM reranking.
- What evidence would resolve it: Empirical studies measuring and comparing the time, computational resources, and monetary costs of different approaches (synthetic query generation + BM25 vs. direct LLM reranking) while maintaining comparable retrieval effectiveness.

## Limitations

- Limited domain generalization: All experiments conducted on newswire collections; effectiveness on other domains remains unverified
- Lack of statistical significance testing: Improvements reported without formal statistical validation
- Cost assumptions: Cost calculations may not account for API rate limits, retry mechanisms, or computational overhead

## Confidence

- High Confidence: The core finding that fusing multiple synthetic queries outperforms single-query baselines is well-supported by experimental results across multiple collections and metrics.
- Medium Confidence: The claim that additional context improves query quality is supported but could benefit from deeper analysis of which specific context elements contribute most to effectiveness.
- Low Confidence: The cost-efficiency claims and generalization to non-newswire domains lack sufficient empirical support for strong conclusions.

## Next Checks

1. **Query Quality Analysis**: Analyze the distribution of query similarities, relevance scores, and topic drift across the generated query variants to validate that the LLM produces topically coherent and diverse queries.

2. **Cross-Domain Testing**: Evaluate the approach on non-newswire collections (e.g., ClueWeb, TREC Genomics) to assess domain generalization and identify potential limitations.

3. **Statistical Significance Testing**: Conduct pairwise statistical significance tests between different prompting strategies and fusion configurations to establish which improvements are meaningful versus noise.