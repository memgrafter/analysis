---
ver: rpa2
title: 'CASA: Causality-driven Argument Sufficiency Assessment'
arxiv_id: '2401.05249'
source_url: https://arxiv.org/abs/2401.05249
tags:
- argument
- casa
- conclusion
- premise
- objection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CASA, a zero-shot causality-driven framework
  for argument sufficiency assessment that formulates the task using probability of
  sufficiency from causal inference. The framework samples contexts inconsistent with
  premise and conclusion using LLMs, intervenes by revising them to include the premise,
  and estimates sufficiency via natural language inference.
---

# CASA: Causality-driven Argument Sufficiency Assessment

## Quick Facts
- arXiv ID: 2401.05249
- Source URL: https://arxiv.org/abs/2401.05249
- Reference count: 22
- Primary result: Zero-shot LLM-based framework outperforms prompting baselines by up to 10% macro-F1 on argument sufficiency assessment

## Executive Summary
This paper introduces CASA, a zero-shot causality-driven framework for argument sufficiency assessment that formulates the task using probability of sufficiency from causal inference. The framework samples contexts inconsistent with premise and conclusion using LLMs, intervenes by revising them to include the premise, and estimates sufficiency via natural language inference. Experiments on BIG-bench-LFD and Climate datasets show CASA outperforms prompting and perplexity baselines by up to 10% on macro-F1. Step-wise evaluations confirm LLMs effectively generate consistent contexts and conduct interventions. In a writing assistance application, objections generated by CASA are rated as more rational and feasible than prompting baselines, leading to improved argument sufficiency in student essays.

## Method Summary
CASA formulates argument sufficiency assessment as a causal inference problem using probability of sufficiency (PS). The framework extracts premises and conclusions from arguments, generates contexts where both are absent, revises these contexts to include the premise under intervention, and estimates sufficiency by checking if the conclusion follows. The intervention step decomposes context into confounders and mediators, removes the confounder-premise link, and updates mediators to reflect the premise's effect. Sufficiency is estimated by aggregating NLI model predictions across multiple revised contexts.

## Key Results
- CASA outperforms zero-shot and one-shot prompting baselines by up to 10% macro-F1 on BIG-bench-LFD and Climate datasets
- Step-wise evaluations show LLMs effectively generate consistent contexts and conduct interventions
- In writing assistance application, CASA-generated objections are rated more rational and feasible than prompting baselines
- Ablation studies confirm the importance of the intervention step, with macro-F1 dropping ~8% when removed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CASA uses probability of sufficiency (PS) from causal inference to quantify whether premises are sufficient to support a conclusion.
- Mechanism: It estimates PS by generating contexts where premise and conclusion are both absent, then intervenes by revising contexts to include the premise and checking if the conclusion follows.
- Core assumption: LLMs can generate and revise contexts that reflect causal relationships in natural language.
- Evidence anchors:
  - [abstract] "PS measures how likely introducing the premise event would lead to the conclusion when both the premise and conclusion events are absent."
  - [section 2.1-2.4] The framework uses claim extraction, context sampling, revision under intervention, and probability estimation to compute PS.
- Break condition: If LLMs cannot generate coherent contexts or interventions that preserve causal logic, PS estimation fails.

### Mechanism 2
- Claim: Zero-shot LLM prompting outperforms supervised baselines because it avoids subjective annotation bias.
- Mechanism: LLMs generate synthetic data samples inconsistent with premise/conclusion and revise them under intervention, mimicking the counterfactual reasoning needed for sufficiency.
- Core assumption: LLM commonsense reasoning is sufficient to simulate counterfactual scenarios without training data.
- Evidence anchors:
  - [section 3.2] CASA outperforms zero-shot and one-shot prompting baselines by up to 10% macro-F1 on BIG-bench-LFD and Climate datasets.
  - [section 3.3] "CASA significantly outperforms all the corresponding zero-shot baselines with significance level α = 0.02."
- Break condition: If LLMs hallucinate or fail to maintain consistency with intervention instructions, the synthetic data becomes unreliable.

### Mechanism 3
- Claim: Context revision under intervention is key to simulating causal sufficiency.
- Mechanism: The revision step removes confounders and mediators, injects the premise, and checks if the conclusion follows, thereby estimating the causal effect.
- Core assumption: Natural language can encode and preserve latent causal structure (confounder, mediator, effect) during revision.
- Evidence anchors:
  - [section 2.3] CASA decomposes context into confounder (W) and mediator (Z), breaks W→X link, and revises Z to Z(X=1).
  - [section 4.1] Ablation shows that removing intervention step drops macro-F1 by ~8%, proving its importance.
- Break condition: If LLM revisions alter W (confounder) or fail to change Z appropriately, the intervention no longer reflects true causal effect.

## Foundational Learning

- Concept: Probability of sufficiency (PS)
  - Why needed here: PS provides a formal causal definition for sufficiency that avoids subjective annotation.
  - Quick check question: If PS=0.9, what does that say about the premise's ability to support the conclusion when both are absent?

- Concept: Counterfactual reasoning in language
  - Why needed here: CASA simulates "what if the premise were true" scenarios to test sufficiency without observational data.
  - Quick check question: How does revising a context to include the premise differ from simply concatenating it?

- Concept: Causal graphs (confounder, mediator, effect)
  - Why needed here: The framework must model how context elements influence premise and conclusion causally.
  - Quick check question: In the causal graph W→X→Z→Y, what happens to Z when we intervene on X?

## Architecture Onboarding

- Component map: Claim extraction -> Context sampling -> Revision under intervention -> Probability estimation
- Critical path: Extract premise/conclusion correctly -> Generate consistent contexts (¬Premise ∧ ¬Conclusion) -> Revise contexts to include premise -> Use NLI to estimate conclusion support
- Design tradeoffs:
  - Number of contexts vs. quality: More contexts increase coverage but reduce coherence.
  - LLM base model vs. domain knowledge: General models work but may miss domain-specific sufficiency cues.
- Failure signatures:
  - Low BLEU between original and revised contexts → W altered during revision
  - Contexts not consistent with ¬Premise/¬Conclusion → Sampling fails
  - NLI always predicts "contradiction" → Intervention step broken
- First 3 experiments:
  1. Test claim extraction accuracy on a small hand-labeled set.
  2. Verify that generated contexts are consistent with ¬Premise/¬Conclusion.
  3. Compare sufficiency predictions before and after intervention step.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the LLMs generate diverse contexts when sampling for context sampling step?
- Basis in paper: [inferred] ...
- Why unresolved: The paper mentions that LLMs are asked to generate diverse contexts, but does not specify the exact method or decoding technique used to ensure diversity.
- What evidence would resolve it: Details on the decoding method or technique used to generate diverse contexts.

### Open Question 2
- Question: How effective are the zero-shot counterfactual reasoning models explored for the revision under intervention step?
- Basis in paper: [explicit] The paper mentions exploring zero-shot counterfactual reasoning models but found them to be either slow or ineffective in generation quality.
- Why unresolved: The paper does not provide specific details on the performance or effectiveness of these models.
- What evidence would resolve it: Quantitative results or comparisons of the counterfactual reasoning models' performance in the revision step.

### Open Question 3
- Question: What is the impact of different decoding methods on the quality and diversity of contexts generated in the context sampling step?
- Basis in paper: [inferred] The paper mentions trying diverse decoding methods but settling on instructing LLMs to generate multiple contexts in one run.
- Why unresolved: The paper does not provide a detailed comparison of different decoding methods and their impact on context quality and diversity.
- What evidence would resolve it: Results comparing the effectiveness of different decoding methods in generating high-quality, diverse contexts.

## Limitations
- The framework relies heavily on LLMs' ability to simulate counterfactual reasoning without training data, introducing uncertainty in generated context quality.
- The causal interpretation of the intervention step and its decomposition into confounders and mediators is not explicitly validated.
- The writing assistance application was evaluated on a small scale with student essays, limiting generalizability.

## Confidence
- **High Confidence**: CASA's superiority over zero-shot and one-shot prompting baselines on BIG-bench-LFD and Climate datasets (up to 10% macro-F1).
- **Medium Confidence**: The causal interpretation of the intervention step and its decomposition into confounders and mediators.
- **Medium Confidence**: The writing assistance application results, though based on a small-scale evaluation.
- **Low Confidence**: The generalizability of the framework to other argument sufficiency tasks or domains not represented in the datasets.

## Next Checks
1. Verify that the generated contexts under ¬Premise ∧ ¬Conclusion are truly counterfactual by checking for logical contradictions or irrelevant content.
2. Test the framework's performance when the LLM revisions alter confounders (W) or fail to update mediators (Z) as expected.
3. Evaluate CASA on a third, unseen dataset to assess its robustness to different argument styles and domains.