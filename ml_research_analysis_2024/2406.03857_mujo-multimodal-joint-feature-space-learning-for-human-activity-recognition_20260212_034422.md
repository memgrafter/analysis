---
ver: rpa2
title: 'MuJo: Multimodal Joint Feature Space Learning for Human Activity Recognition'
arxiv_id: '2406.03857'
source_url: https://arxiv.org/abs/2406.03857
tags:
- data
- recognition
- training
- multimodal
- activity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses human activity recognition (HAR) using wearable
  sensors, which is challenging due to limited labeled training data and ambiguous
  sensor signals. To overcome these limitations, the authors introduce a novel multimodal
  pre-training approach called MuJo, which learns a joint feature space across video,
  pose, virtual IMU sensor data, and text.
---

# MuJo: Multimodal Joint Feature Space Learning for Human Activity Recognition

## Quick Facts
- **arXiv ID**: 2406.03857
- **Source URL**: https://arxiv.org/abs/2406.03857
- **Reference count**: 12
- **Primary result**: Novel multimodal pre-training approach MuJo improves HAR performance, especially with limited labeled data, outperforming existing self-supervised methods.

## Executive Summary
This paper addresses human activity recognition (HAR) using wearable sensors, which is challenging due to limited labeled training data and ambiguous sensor signals. To overcome these limitations, the authors introduce a novel multimodal pre-training approach called MuJo, which learns a joint feature space across video, pose, virtual IMU sensor data, and text. They created FiMAD, a large-scale fitness dataset with synchronized modalities extracted from YouTube videos. MuJo employs contrastive learning to align representations across these modalities, enabling knowledge transfer to real-world HAR tasks. Experiments on four datasets (MM-Fit, MyoGym, MotionSense, MHEALTH) show that MuJo significantly improves performance, especially with limited labeled data. For example, on MM-Fit, MuJo achieves a Macro F1-Score of 0.855 with only 2% training data and 0.942 with the full dataset. The approach also outperforms existing self-supervised methods in data efficiency and consistently improves baseline performance.

## Method Summary
The MuJo approach leverages contrastive learning to create a joint feature space across multiple modalities (video, pose, virtual IMU sensor data, and text) for human activity recognition. The method uses a large-scale fitness dataset called FiMAD, which contains synchronized modalities extracted from YouTube videos. By learning aligned representations across these modalities, MuJo enables effective knowledge transfer to real-world HAR tasks. The contrastive learning framework aligns representations from different modalities of the same activity instance, while pushing apart representations from different activities. This multimodal pre-training is then fine-tuned on specific HAR datasets with limited labeled data, resulting in significant performance improvements compared to traditional approaches.

## Key Results
- On MM-Fit dataset, MuJo achieves Macro F1-Score of 0.855 with only 2% training data and 0.942 with full dataset
- Outperforms existing self-supervised methods in data efficiency across multiple datasets
- Consistently improves baseline performance on MM-Fit, MyoGym, MotionSense, and MHEALTH datasets

## Why This Works (Mechanism)
MuJo works by leveraging the complementary information available across multiple modalities to create robust, generalized representations. The contrastive learning framework aligns similar activity representations across modalities while separating different activities, creating a rich joint feature space. This approach is particularly effective when labeled data is scarce because the model can leverage the abundant unlabeled data in the pre-training phase. The multimodal nature allows the model to capture different aspects of human activities - visual appearance from videos, skeletal information from pose data, motion patterns from virtual IMU data, and semantic context from text. This comprehensive representation enables better generalization to new activities and datasets compared to unimodal approaches.

## Foundational Learning

**Contrastive Learning**
- *Why needed*: Enables learning from unlabeled data by pulling similar samples together and pushing dissimilar ones apart
- *Quick check*: Verify the model learns to distinguish between different activities while grouping similar ones

**Multimodal Representation Learning**
- *Why needed*: Different modalities capture complementary aspects of activities, leading to more robust representations
- *Quick check*: Ensure representations from different modalities align for the same activity instance

**Transfer Learning**
- *Why needed*: Pre-trained models on large datasets can be fine-tuned for specific tasks with limited data
- *Quick check*: Confirm performance improvements when fine-tuning on target datasets

**Self-Supervised Learning**
- *Why needed*: Generates supervisory signals from data itself, reducing dependency on labeled examples
- *Quick check*: Verify the model learns meaningful representations without explicit labels

## Architecture Onboarding

**Component Map**
FiMAD dataset -> Multimodal Encoder (Video, Pose, Virtual IMU, Text) -> Contrastive Loss -> Joint Feature Space -> Fine-tuning on Target HAR Datasets

**Critical Path**
FiMAD pre-training -> Contrastive learning alignment -> Joint feature space creation -> Fine-tuning on target dataset -> Performance evaluation

**Design Tradeoffs**
The paper balances between model complexity and generalization ability. Using multiple modalities increases computational cost but provides richer representations. The choice of contrastive learning over other self-supervised methods prioritizes explicit alignment across modalities. The decision to create a custom dataset (FiMAD) ensures perfect synchronization but may introduce domain-specific biases.

**Failure Signatures**
Poor performance on non-fitness activities would indicate limited generalizability. Inconsistent results across datasets might suggest modality-specific weaknesses. Degradation in performance with increased labeled data could indicate overfitting during pre-training. Failure to align representations across modalities would manifest as poor transfer learning results.

**First Experiments**
1. Validate contrastive learning by testing if representations of the same activity across modalities are closer than those of different activities
2. Test individual modality contributions through ablation studies on the FiMAD dataset
3. Evaluate transfer learning effectiveness by comparing fine-tuned model performance against training from scratch on target datasets

## Open Questions the Paper Calls Out
None

## Limitations
- The FiMAD dataset is fitness-centric, potentially limiting generalizability to other activity types
- Performance improvements are primarily demonstrated on fitness-related activities, leaving questions about performance on diverse daily activities
- Lack of consistent definition of "limited" data across different datasets makes direct comparisons challenging
- No ablation studies to isolate contributions of each modality to overall performance

## Confidence
- **High**: Major claim that MuJo outperforms existing self-supervised methods in data efficiency
- **Medium**: Claim that MuJo enables knowledge transfer to real-world HAR tasks
- **Low**: Assertion that the FiMAD dataset is representative of general HAR challenges

## Next Checks
1. Evaluate MuJo's performance on non-fitness HAR datasets with diverse activity types (e.g., office work, household chores) to test generalizability
2. Conduct ablation studies to quantify the individual and combined contributions of each modality (video, pose, virtual IMU, text) to the overall performance
3. Assess the computational requirements and scalability of MuJo when applied to larger, more diverse datasets and real-time HAR systems