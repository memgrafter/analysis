---
ver: rpa2
title: 'Truth-value judgment in language models: ''truth directions'' are context
  sensitive'
arxiv_id: '2404.18865'
source_url: https://arxiv.org/abs/2404.18865
tags:
- incorrect
- picture
- which
- statement
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the context sensitivity of truth-value
  probes in large language models. The authors propose probing methods to evaluate
  how language models incorporate in-context information when assigning truth values
  to sentences.
---

# Truth-value judgment in language models: 'truth directions' are context sensitive

## Quick Facts
- arXiv ID: 2404.18865
- Source URL: https://arxiv.org/abs/2404.18865
- Reference count: 40
- Primary result: Truth-value probes in LLMs are context sensitive but also sensitive to irrelevant information, showing limited coherence in truth assignments

## Executive Summary
This paper investigates how large language models incorporate context when assigning truth values to sentences through probing methods. The authors introduce error metrics to measure consistency and coherence of truth-value assignments, distinguishing between prior beliefs, conditional beliefs, and marginal beliefs. Through experiments across multiple models and datasets, they find that while LLMs do incorporate context when representing truth, they are also sensitive to irrelevant information, leading to only limited coherence. The study demonstrates that truth-value directions in latent space causally mediate the incorporation of in-context information, though the relationship is complex and context-dependent.

## Method Summary
The authors train truth-value probes using three methods (Contrast Consistent Search, Contrast Consistent Reflection, and Logistic Regression) on sentence representations from Llama2 and OLMo models. They evaluate premise sensitivity across layers and compute four error scores (E1-E4) to measure coherence when using corrupted, unrelated, or neutral premises. Causal intervention experiments test whether truth-value directions mediate reasoning by moving premise representations along identified directions and measuring effects on hypothesis probabilities. The study uses EntailmentBank and SNLI datasets with hypothesis sentences preceded by affirmed or negated premises.

## Key Results
- Truth-value probes are context sensitive but also sensitive to irrelevant information, leading to limited coherence
- CCS and CCR methods outperform LR in both coherence and causal mediation
- Truth-value directions causally mediate the incorporation of in-context information, affecting hypothesis probabilities
- Instruction-tuning makes models more likely to represent prior assertions as true, consistent with training objectives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Truth-value directions in LLM latent spaces causally mediate the incorporation of in-context information
- Mechanism: Moving a premise's representation along the identified truth-value direction changes the probability assigned to an entailed or contradicted hypothesis
- Core assumption: The truth-value direction extracted by probes reflects a causally relevant mechanism in the model's reasoning process
- Evidence anchors:
  - [abstract] "truth-value directions are causal mediators in the inference process that incorporates in-context information"
  - [section 4.2] "When we move the affirmed premises backwards in the truth-value direction, the probabilities of entailed and contradicted hypotheses decrease and increase, respectively"
  - [corpus] Weak - only general mention of "truth directions" without specific causal claims
- Break condition: If intervention magnitude is too large/small to detect meaningful probability changes, or direction becomes non-causal under different prompting strategies

### Mechanism 2
- Claim: Truth-value probes are context sensitive but also sensitive to irrelevant information
- Mechanism: Probes trained on individual sentences show premise sensitivity when evaluated with premises in-context, and error scores show sensitivity to corrupted/unrelated premises
- Core assumption: The probe's direction reflects some aspect of the model's truth representation that generalizes across contexts
- Evidence anchors:
  - [abstract] "contexts which should not affect the truth often still impact the probe outputs"
  - [section 4.1] "E1 and E2 are quite high... a value of one means that, on average, a corrupted or unrelated premise has an effect with the same magnitude as the original affirmed premise"
  - [corpus] Moderate - related work on context sensitivity but not specific to truth-value probes
- Break condition: If error scores are consistently low across all methods, indicating independent prior and contextual representations

### Mechanism 3
- Claim: Different probing methods extract different aspects of truth representation
- Mechanism: LR, MMP, and CCR methods each identify different directions in latent space, with varying degrees of coherence and causal mediation
- Core assumption: The probing method's objective function determines what aspect of truth representation is extracted
- Evidence anchors:
  - [section 3.1] Different objectives: LR (supervised difference), MMP (mean shift), CCS/CCR (unsupervised consistency)
  - [section 4.1] "Among the tested truth-value probing methods, Logistic Regression failed to reveal levels of coherence and causal mediation that the others did"
  - [corpus] Weak - only general mention of probing methods without comparison of their extracted representations
- Break condition: If all methods converge to same direction regardless of training objective

## Foundational Learning

- Concept: Linear probe training objectives
  - Why needed here: Different objectives (supervised vs unsupervised) extract different aspects of truth representations
  - Quick check question: What is the key difference between the objectives of LR and CCS?

- Concept: Causal mediation analysis
  - Why needed here: Determines whether truth-value directions are mere correlates or active components of reasoning
  - Quick check question: What does it mean if moving a premise along a truth-value direction changes hypothesis probability?

- Concept: Error score normalization
  - Why needed here: Makes error scores comparable across methods with different premise sensitivities
  - Quick check question: Why are error scores expressed as multiples of premise effect (PE)?

## Architecture Onboarding

- Component map:
  Data pipeline -> Model interface -> Probe training -> Evaluation -> Analysis

- Critical path:
  1. Extract activations for hypothesis sentences with/without premises
  2. Train probe direction using chosen method
  3. Evaluate premise sensitivity and error scores
  4. Perform causal intervention on premise representations
  5. Analyze results across layers, models, and datasets

- Design tradeoffs:
  - no-prem vs pos-prem training: Captures prior beliefs vs contextual beliefs
  - Supervised vs unsupervised methods: Accuracy vs generalizability
  - Layer selection: Earlier layers may capture more general patterns vs later layers capturing task-specific representations

- Failure signatures:
  - Low premise sensitivity across all layers → model may have independent prior/context representations
  - High E1/E2 error scores → probe captures spurious correlations
  - Intervention effects not matching expected direction → direction not causally relevant

- First 3 experiments:
  1. Compare premise sensitivity across layers for no-prem vs pos-prem trained probes
  2. Measure error scores E1-E4 for each probing method on EntailmentBank
  3. Perform causal intervention on Llama2-13b layer 10 and measure hypothesis probability changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the coherence of truth-value probes be improved by incorporating meaning-relation representations, such as entailment and contradiction, into the probing process?
- Basis in paper: [explicit] The paper suggests that future work should investigate the construction of probes that reveal if a model represents two sentences as having a particular meaning relation.
- Why unresolved: Current truth-value probes are context-sensitive but also sensitive to irrelevant information, leading to limited coherence. The paper does not explore whether incorporating meaning-relation representations could improve this coherence.
- What evidence would resolve it: Experiments demonstrating that probes incorporating meaning-relation representations exhibit higher coherence (lower error scores) compared to existing probes across multiple models and datasets.

### Open Question 2
- Question: Do different types of beliefs (prior, conditional, and marginal) correspond to distinct directions in the latent space, and can these directions be isolated and characterized?
- Basis in paper: [explicit] The paper mentions that future work should investigate whether there is a subspace which encodes truth-values in different ways, corresponding more closely to either prior, marginal, or conditional beliefs.
- Why unresolved: The paper observes that the positioning of premises along truth-value directions partially determines the positioning of related hypotheses, but it does not determine if these beliefs are represented in distinct directions or a unified subspace.
- What evidence would resolve it: Experiments showing that different types of beliefs correspond to separable directions in the latent space, and that manipulating these directions leads to predictable changes in the model's truth-value assignments.

### Open Question 3
- Question: How does instruction tuning affect the representation of truth-value directions and the incorporation of in-context information?
- Basis in paper: [explicit] The paper finds that instruction-tuning makes the model more likely to represent prior assertions as true, which is consistent with the instruction-tuning objective.
- Why unresolved: While the paper observes a difference in behavior between pretrained and instruction-tuned models, it does not explore the underlying mechanisms or the extent to which instruction tuning affects the representation of truth-value directions.
- What evidence would resolve it: Experiments comparing the truth-value directions and error scores of probes across pretrained and instruction-tuned models, and analyzing how the model's sensitivity to context changes with instruction tuning.

## Limitations

- Error score methodology relies on normalization against premise effects, which may not hold for models with highly variable representations
- Causal intervention experiments only test one direction of movement at fixed magnitudes, potentially missing non-linear effects
- The relationship between probe training methodology and underlying belief representation remains theoretically underspecified
- Study focuses on English language models and specific entailment datasets, limiting generalizability

## Confidence

- **High confidence**: Observation that truth-value probes are context sensitive and show varying levels of premise sensitivity across layers and methods
- **Medium confidence**: Claim that truth-value directions causally mediate reasoning is supported but specific mechanisms remain unclear
- **Low confidence**: Broader theoretical implications about meaning-relation representations and belief-type specific probes are speculative

## Next Checks

1. **Threshold sensitivity analysis**: Systematically vary the magnitude of causal interventions to determine if there are non-linear thresholds in the relationship between direction movement and hypothesis probability changes.

2. **Cross-linguistic replication**: Test the same probe training and evaluation methodology on multilingual models to assess whether context sensitivity patterns generalize across languages.

3. **Alternative belief representation mapping**: Conduct controlled experiments where models are explicitly prompted with prior beliefs, then presented with evidence, then asked to make judgments - comparing these judgments to no-prem and pos-prem probe outputs.