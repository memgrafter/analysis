---
ver: rpa2
title: Initial Decoding with Minimally Augmented Language Model for Improved Lattice
  Rescoring in Low Resource ASR
arxiv_id: '2403.10937'
source_url: https://arxiv.org/abs/2403.10937
tags:
- language
- speech
- baseline
- words
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study tackles the challenge of low-resource automatic speech
  recognition (ASR), particularly the high out-of-vocabulary (OOV) rates in agglutinative
  languages like Telugu and Kannada. It proposes a method that minimally augments
  the baseline language model with unigram counts of OOV words from a larger text
  corpus (e.g., Wikipedia) for initial decoding.
---

# Initial Decoding with Minimally Augmented Language Model for Improved Lattice Rescoring in Low Resource ASR

## Quick Facts
- arXiv ID: 2403.10937
- Source URL: https://arxiv.org/abs/2403.10937
- Authors: Savitha Murthy; Dinkar Sitaram
- Reference count: 40
- One-line primary result: Minimal LM augmentation with OOV unigrams achieves 21.8% relative WER reduction for Telugu and 41.8% for Kannada with 1/8th the memory of full augmentation

## Executive Summary
This paper addresses the challenge of out-of-vocabulary (OOV) words in low-resource agglutinative languages like Telugu and Kannada by proposing a novel approach that combines minimal language model augmentation with lattice rescoring. The method involves minimally augmenting the baseline language model with unigram counts of OOV words from a larger text corpus (e.g., Wikipedia), performing initial decoding to generate lattices containing these OOV words, and then rescoring these lattices with a fully augmented language model. This approach achieves significant WER reductions (21.8% for Telugu, 41.8% for Kannada) while requiring only about 1/8th the memory of direct decoding with the fully augmented language model.

## Method Summary
The proposed method tackles low-resource ASR challenges by minimally augmenting the baseline language model with unigram counts of OOV words from a larger text corpus, followed by lattice rescoring. The process involves: 1) Extracting unigram counts of OOV words from Wikipedia, 2) Interpolating these counts with the baseline LM using count merging, 3) Performing initial decoding with the minimally augmented LM to generate lattices containing OOV words, and 4) Rescoring these lattices with a Wikipedia-augmented LM to achieve significant WER reduction. This approach is particularly effective for agglutinative languages where OOV rates are high due to morphological variations.

## Key Results
- Achieved 21.8% relative WER reduction for Telugu and 41.8% for Kannada using minimal LM augmentation
- Memory requirements are approximately 1/8th of full Wikipedia-augmented LM decoding
- Consistent improvements across datasets of different sizes, with more pronounced gains for smaller datasets
- Significant OOV word recovery while maintaining competitive IV word recognition

## Why This Works (Mechanism)

### Mechanism 1
Initial decoding with a minimally augmented language model that includes unigram counts of OOV words from a larger text corpus enables the lattice to contain these OOV words, making subsequent rescoring more effective. By interpolating the baseline language model with unigram counts of OOV words (words present in the larger corpus but not in the baseline), the initial decoding process can generate lattices that include these previously missing words. Rescoring these enriched lattices with a larger, fully augmented language model then leads to significant WER reduction. The core assumption is that the unigram counts of OOV words are sufficient to guide the initial decoding to include these words in the lattice.

### Mechanism 2
Language model augmentation using selective text from a larger corpus, such as contrastive selection or entropy-based selection, can improve ASR performance by focusing on sentences more relevant to the baseline corpus. By selecting sentences from the larger corpus that are similar to the baseline corpus (e.g., using contrastive selection based on trigram probabilities), the augmented language model is more likely to improve recognition of words and phrases that are relevant to the domain of the ASR system. The core assumption is that the selected sentences from the larger corpus are indeed more relevant to the baseline corpus and will improve recognition accuracy.

### Mechanism 3
The effectiveness of the proposed method is consistent across datasets of different sizes, with more pronounced improvements observed for smaller datasets. The initial decoding with a minimally augmented language model helps to include more OOV words in the lattice, and this effect is more significant when the baseline language model is smaller (i.e., for smaller datasets). Rescoring these lattices with a larger language model then leads to substantial WER reductions. The core assumption is that the relative improvement in WER due to the inclusion of OOV words in the lattice is more significant when the baseline language model is smaller.

## Foundational Learning

- **Concept**: Weighted Finite State Transducer (WFST) composition for decoding
  - **Why needed here**: The paper discusses the composition of HCLG graphs for decoding, where H represents HMM transitions, C represents context dependencies, L represents the lexicon, and G represents the language model grammar.
  - **Quick check question**: What are the components of the HCLG graph used in traditional ASR decoding, and how do they contribute to finding the most probable word sequence?

- **Concept**: Language model interpolation and count merging
  - **Why needed here**: The paper uses count merging to interpolate the baseline language model with a larger language model, which involves combining unigram counts from both models.
  - **Quick check question**: How does count merging differ from linear interpolation in language model combination, and what are the advantages of using count merging?

- **Concept**: Lattice rescoring in ASR
  - **Why needed here**: The paper proposes rescoring lattices generated from initial decoding with a larger language model to improve WER.
  - **Quick check question**: What is the purpose of lattice rescoring in ASR, and how does it differ from decoding with a larger language model directly?

## Architecture Onboarding

- **Component map**: Baseline LM -> Minimal augmentation (unigram counts of OOV words) -> Initial decoding -> Lattice generation -> Lattice rescoring with larger LM -> Improved WER
- **Critical path**: Baseline LM -> Minimal augmentation (unigram counts of OOV words) -> Initial decoding -> Lattice generation -> Lattice rescoring with larger LM -> Improved WER
- **Design tradeoffs**: Memory vs. accuracy: Decoding with a larger LM directly is more accurate but requires more memory. The proposed method uses less memory but achieves comparable accuracy. Complexity vs. simplicity: The proposed method is simpler than complex OOV detection and recovery techniques but may not be as effective in all scenarios.
- **Failure signatures**: If the unigram counts of OOV words are not representative, the initial decoding may not include the relevant OOV words in the lattice. If the larger LM is not effective at rescoring, the method may not lead to significant improvements.
- **First 3 experiments**:
  1. Implement the minimal augmentation of the baseline LM with unigram counts of OOV words and perform initial decoding to generate lattices.
  2. Rescore the generated lattices with the larger LM and compare the WER with decoding directly using the larger LM.
  3. Experiment with different text selection methods (e.g., contrastive selection, entropy-based selection) for augmenting the baseline LM and evaluate their impact on WER and OOV recovery.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How would the proposed method perform on highly inflective languages beyond Telugu and Kannada, such as Finnish or Turkish?
- **Basis in paper**: [explicit] The authors state their method is applicable for any agglutinative and inflective low-resource language and is complementary to morpheme-based approaches.
- **Why unresolved**: The study only evaluates the method on Telugu and Kannada, both Dravidian languages with similar inflectional patterns. Other agglutinative languages may have different inflectional complexities.
- **What evidence would resolve it**: Empirical evaluation of the method on other agglutinative languages like Finnish, Turkish, or Hungarian with varying degrees of inflectional complexity.

### Open Question 2
- **Question**: What is the impact of the proposed method on speech recognition accuracy when the text corpus contains significant amounts of noise or irrelevant data?
- **Basis in paper**: [inferred] The authors compare their method against different text selection techniques (contrastive selection, delta likelihood, entropy-based) which aim to select relevant sentences from Wikipedia, suggesting that data quality matters.
- **Why unresolved**: The study uses Wikipedia as the text corpus, which is generally considered high-quality, but does not explicitly test the method's robustness to noisy or irrelevant data.
- **What evidence would resolve it**: Experiments using text corpora with varying levels of noise or irrelevant data to assess the method's performance degradation.

### Open Question 3
- **Question**: Can the proposed method be effectively combined with subword or morpheme-based approaches for further improvements in low-resource ASR?
- **Basis in paper**: [explicit] The authors mention that their research is complementary to the technique of using morphemes as subwords.
- **Why unresolved**: The study does not explore the combination of the proposed method with subword or morpheme-based approaches, leaving the potential for further improvements untested.
- **What evidence would resolve it**: Implementation and evaluation of the proposed method in conjunction with subword or morpheme-based approaches on low-resource ASR tasks.

## Limitations
- The method's effectiveness heavily depends on the quality and representativeness of unigram counts of OOV words from the larger text corpus.
- The approach assumes the larger language model is effective at rescoring the enriched lattices.
- The study focuses on agglutinative languages (Telugu and Kannada), and generalizability to other language types is not explicitly addressed.
- Memory savings compared to direct decoding with the larger LM may vary depending on implementation and hardware.

## Confidence

- **High Confidence**: The core claim that minimal augmentation of the baseline LM with unigram counts of OOV words, followed by lattice rescoring with a larger LM, can achieve significant WER reductions in low-resource ASR.
- **Medium Confidence**: The claim that the method's effectiveness is consistent across datasets of different sizes, with more pronounced improvements for smaller datasets.
- **Medium Confidence**: The assertion that the memory requirements of the proposed method are significantly lower than direct decoding with the larger LM.

## Next Checks

1. **OOV Word Coverage Analysis**: Analyze the coverage of OOV words in the lattices generated by the minimally augmented LM compared to the baseline LM. Measure the percentage of OOV words from the test set that are present in the lattices and their corresponding lattice probabilities.

2. **Cross-Lingual Generalization Test**: Evaluate the proposed method on a language that is typologically different from Telugu and Kannada, such as an analytic language like Mandarin Chinese or a fusional language like Spanish.

3. **Memory Usage Comparison**: Conduct a detailed analysis of the memory usage of the proposed method compared to direct decoding with the larger LM. Measure the peak memory consumption during decoding for both approaches and calculate the actual memory savings achieved.