---
ver: rpa2
title: 'AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient
  Large Language Models Fine-Tuning'
arxiv_id: '2406.18060'
source_url: https://arxiv.org/abs/2406.18060
tags:
- fine-tuning
- adazeta
- methods
- gradient
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high memory costs of fine-tuning large
  language models (LLMs) by developing AdaZeta, a zeroth-order optimization framework.
  The key innovation is a memory-efficient approach that avoids backpropagation by
  estimating gradients from forward passes only, while incorporating tensorized adapters
  to reduce the number of trainable parameters and an adaptive query schedule to control
  gradient variance.
---

# AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning

## Quick Facts
- **arXiv ID**: 2406.18060
- **Source URL**: https://arxiv.org/abs/2406.18060
- **Reference count**: 40
- **Key outcome**: Achieves over 8× memory reduction vs first-order methods while matching or surpassing zeroth-order fine-tuning performance on NLU tasks.

## Executive Summary
This paper addresses the high memory costs of fine-tuning large language models by developing AdaZeta, a zeroth-order optimization framework that avoids backpropagation by estimating gradients from forward passes only. The key innovations are tensorized adapters that reduce trainable parameters via Tensor-Train decomposition and an adaptive query schedule that controls gradient variance. Theoretical analysis and experiments on RoBERTa-Large and Llama-2-7B demonstrate significant memory savings while achieving comparable or better performance than existing zeroth-order fine-tuning approaches.

## Method Summary
AdaZeta combines zeroth-order optimization with tensorized adapters to enable memory-efficient LLM fine-tuning. The method uses a fast-forward, low-parameter tensorized adapter layer inserted after each multi-head attention and feed-forward layer of the frozen backbone. Instead of traditional backpropagation, AdaZeta estimates gradients through random perturbations in the forward pass only. To enhance estimation accuracy and prevent divergence in large-scale settings, the method employs an adaptive query number schedule that sublinearly increases the number of forward passes during training. A parallel contraction method speeds up tensorized adapter computations.

## Key Results
- Achieves over 8× memory reduction compared to first-order fine-tuning methods
- Matches or surpasses the performance of MeZO and MeZO-LoRA on NLU benchmarks
- Effectively mitigates divergence issues common in large-scale zeroth-order optimization
- Reduces trainable parameters by over 80× through tensorized adapters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using tensorized adapters reduces trainable parameters, improving zeroth-order gradient estimation accuracy.
- Mechanism: Tensorized adapters compress weight matrices using Tensor-Train decomposition, reducing trainable parameters and thus gradient noise in ZO estimation.
- Core assumption: Lower parameter count leads to lower variance in zeroth-order gradient estimates.
- Evidence anchors:
  - [abstract]: "To enhance dimension-dependent ZO estimation accuracy, we introduce a fast-forward, low-parameter tensorized adapter."
  - [section 2.2]: "The tensorized adapters reduce trainable parameters by over 80×, making them a better fit for ZO fine-tuning."
- Break condition: If TT decomposition adds excessive computational overhead or rank selection is suboptimal.

### Mechanism 2
- Claim: Adaptive query schedule sublinearly increases queries to reduce variance and prevent divergence.
- Mechanism: Increases query count at epoch start according to Qk := min(αe^(βk), Qmax) to reduce gradient estimation noise without constant computational overhead.
- Core assumption: Increasing queries early when gradients are noisiest yields more stable convergence.
- Evidence anchors:
  - [abstract]: "To tackle the frequently observed divergence issue in large-scale ZO fine-tuning tasks, we propose an adaptive query number schedule that guarantees convergence."
  - [section 3.2]: "We consider a simple but effective sublinear increasing query number adjustment schedule."
- Break condition: If schedule parameters (α, β) are poorly tuned, causing insufficient variance reduction or excessive computational cost.

### Mechanism 3
- Claim: Parallel contraction method speeds up tensorized adapter forward passes.
- Mechanism: Divides tensor factors into groups for parallel processing instead of sequential TT contraction, reducing time per forward pass.
- Core assumption: Parallel contraction does not compromise forward computation accuracy.
- Evidence anchors:
  - [section 2.2]: "Instead of using the sequential contraction method during the forward pass as in previous work, we propose a new parallel contraction method to speed up the forward passes."
- Break condition: If parallelization introduces numerical instability or memory contention.

## Foundational Learning

- **Concept**: Zeroth-order optimization
  - Why needed here: AdaZeta relies on ZO gradient estimation via forward passes only, avoiding backpropagation.
  - Quick check question: How does the randomized zeroth-order gradient estimator approximate true gradients without backpropagation?

- **Concept**: Tensor-Train decomposition
  - Why needed here: Tensorized adapters use TT decomposition to compress weight matrices and reduce trainable parameters.
  - Quick check question: What is the trade-off between tensor rank and approximation accuracy in TT decomposition?

- **Concept**: Variance reduction in stochastic optimization
  - Why needed here: The adaptive query schedule is a variance reduction technique tailored for ZO fine-tuning.
  - Quick check question: How does increasing the number of queries affect the variance of the estimated gradient?

## Architecture Onboarding

- **Component map**: Input → Multi-head Attention → Feed-forward → Tensorized Adapters (parallel contraction) → Loss → ZO gradient estimation (adaptive queries) → Parameter update
- **Critical path**: Forward pass through frozen LLM → Tensorized adapter forward (parallel) → Loss computation → Perturbation + forward pass → Gradient estimate → Parameter update
- **Design tradeoffs**:
  - Tensor rank vs. model capacity: Higher rank improves expressiveness but increases parameters and memory
  - Query count vs. runtime: More queries reduce variance but increase training time
  - Parallel contraction vs. sequential: Faster but may require more GPU memory
- **Failure signatures**:
  - Training divergence: Likely due to insufficient query count or poor tensor rank choice
  - Slow convergence: May indicate suboptimal learning rate or tensorized adapter bottleneck dimension
  - Memory spikes: Could result from inefficient parallel contraction or large TT ranks
- **First 3 experiments**:
  1. Compare AdaZeta with MeZO-LoRA on SST-2 using fixed query count
  2. Vary adaptive query schedule parameters (α, β) and observe impact on convergence speed
  3. Test different tensor ranks and bottleneck dimensions to find optimal balance

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several unresolved issues emerge from the analysis:

### Open Question 1
- Question: How does the choice of tensor rank and bottleneck size in tensorized adapters affect the trade-off between memory efficiency and model performance in AdaZeta?
- Basis in paper: [explicit] The paper discusses tensorized adapters but mentions optimal factors shape and tensor rank can only be determined through experimental trials.
- Why unresolved: No detailed analysis of how different tensor ranks and bottleneck sizes impact performance and memory efficiency.
- What evidence would resolve it: Systematic experiments varying tensor ranks and bottleneck sizes across different tasks and model scales.

### Open Question 2
- Question: What is the impact of the adaptive query schedule on training efficiency and stability in extremely large-scale models beyond Llama-2-7B?
- Basis in paper: [explicit] The paper introduces an adaptive query schedule but only tests on Llama-2-7B model.
- Why unresolved: Effectiveness on models significantly larger than Llama-2-7B is not explored.
- What evidence would resolve it: Experiments on models with more than 7 billion parameters measuring training stability and convergence.

### Open Question 3
- Question: How does the parallel contraction method for tensorized adapters compare to sequential contraction in terms of computational overhead and speed-up?
- Basis in paper: [explicit] The paper proposes parallel contraction but does not provide empirical comparisons with sequential contraction.
- Why unresolved: No experimental results or theoretical analysis comparing computational efficiency.
- What evidence would resolve it: Benchmarking experiments measuring actual speed-up and computational overhead.

### Open Question 4
- Question: What are the theoretical limits of the convergence rate of AdaZeta as model size and dimensionality increase?
- Basis in paper: [inferred] The paper provides convergence rate analysis but focuses on dimension of optimized models, not asymptotic behavior.
- Why unresolved: Theoretical analysis does not address how convergence rate behaves asymptotically with extremely large model sizes.
- What evidence would resolve it: Theoretical work extending convergence analysis to asymptotic cases and empirical validation on larger models.

## Limitations

- Limited evaluation to only two model architectures (RoBERTa-Large and Llama-2-7B) without testing generalizability
- No runtime comparisons with first-order methods despite memory efficiency claims
- Lacks ablation studies showing individual contribution of tensorized adapters versus adaptive query schedule

## Confidence

- **High**: Memory savings claims (>8× reduction) and convergence guarantees - well-supported by theoretical analysis and experimental results
- **Medium**: Performance parity/surpassing MeZO - supported by results but limited to specific tasks and models
- **Low**: Claims about preventing divergence in large-scale tasks - based on empirical results without comprehensive failure analysis

## Next Checks

1. Conduct ablation studies to isolate the contribution of tensorized adapters versus the adaptive query schedule
2. Evaluate AdaZeta on additional model architectures (e.g., OPT, BLOOM) to test generalizability
3. Compare actual wall-clock training times against first-order fine-tuning methods to validate practical efficiency gains