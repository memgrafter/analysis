---
ver: rpa2
title: 'Adapting LLMs to Hebrew: Unveiling DictaLM 2.0 with Enhanced Vocabulary and
  Instruction Capabilities'
arxiv_id: '2407.07080'
source_url: https://arxiv.org/abs/2407.07080
tags:
- hebrew
- training
- language
- data
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of adapting large language models
  (LLMs) to Hebrew, a low-resource language. The core method involves extending the
  Mistral tokenizer with 1,000 Hebrew-specific tokens, performing embedding distillation,
  and continuous pre-training on a corpus of 100 billion Hebrew and English tokens.
---

# Adapting LLMs to Hebrew: Unveiling DictaLM 2.0 with Enhanced Vocabulary and Instruction Capabilities

## Quick Facts
- arXiv ID: 2407.07080
- Source URL: https://arxiv.org/abs/2407.07080
- Authors: Shaltiel Shmidman; Avi Shmidman; Amir DN Cohen; Moshe Koppel
- Reference count: 4
- One-line primary result: State-of-the-art Hebrew LLM performance with 1,000 added tokens and instruction tuning

## Executive Summary
This paper addresses the challenge of adapting large language models to Hebrew, a low-resource morphologically rich language. The authors extend the Mistral tokenizer with 1,000 Hebrew-specific tokens, perform embedding distillation to transfer knowledge to new tokens, and conduct continuous pre-training on a mixed Hebrew-English corpus. The resulting models, DictaLM2.0 and DictaLM2.0-Instruct, achieve state-of-the-art performance on Hebrew NLP tasks including Question Answering, Sentiment Analysis, and Translation, with the instruct-tuned model scoring 8.1-8.54 on human evaluation metrics for summarization.

## Method Summary
The authors adapt Mistral-7B to Hebrew through three key steps: (1) extending the tokenizer with 1,000 Hebrew-specific tokens to reduce compression from ~5.81 to ~2.9 tokens per Hebrew word, (2) performing embedding distillation to transfer Mistral's semantic representations to the new tokens, and (3) continuous pre-training on 100 billion tokens (50% Hebrew, 50% English) including aligned translation pairs. The models are then fine-tuned using Supervised Fine-tuning and Direct Preference Optimization on instruction datasets in both languages, resulting in DictaLM2.0 for general tasks and DictaLM2.0-Instruct for instruction following.

## Key Results
- Tokenizer extension with 1,000 Hebrew tokens more than halves the compression rate from 5.81 to ~2.9 tokens per Hebrew word
- State-of-the-art performance on Hebrew NLP tasks including Question Answering, Sentiment Analysis, and Translation
- Instruct-tuned model achieves human evaluation scores of 8.1, 7.45, 8.34, and 8.54 on Relevance, Coherence, Consistency, and Fluency in summarization task
- Model successfully maps English knowledge to Hebrew through aligned translation pairs during pre-training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Extending the tokenizer with Hebrew-specific tokens reduces the compression ratio from ~5.81 tokens per Hebrew word to about half, improving model performance.
- Mechanism: Hebrew words are morphologically complex, causing each word to be split into many subword tokens by the Mistral tokenizer, which was trained mostly on English. Adding 1,000 Hebrew-specific tokens lets the tokenizer capture common Hebrew substrings in a single token, thus compressing the language more efficiently.
- Core assumption: The added tokens represent frequent Hebrew substrings and capture the language's morphology.
- Evidence anchors:
  - [section]: "Adding just 1,000 tokens more than halves the compression rate, after which the gains from adding more tokens diminish significantly."
  - [section]: "Specifically, the Mistral tokenizer has a compression rate of 5.81 tokens per Hebrew word, which averages approximately one token per character."
- Break condition: If added tokens are not representative of common Hebrew substrings or do not reduce the number of tokens per word, the compression gain disappears.

### Mechanism 2
- Claim: Embedding distillation transfers knowledge from the Mistral embeddings to the newly added Hebrew tokens.
- Mechanism: The model is trained to minimize the difference between the last hidden states when encoding the same sentence using the Mistral tokenizer and the extended tokenizer, effectively copying the semantic representation of Mistral tokens into the new Hebrew tokens.
- Core assumption: There is meaningful semantic information in the Mistral embeddings that can be reused for Hebrew tokens.
- Evidence anchors:
  - [section]: "We aim to distill the existing embeddings in the base model into the newly added tokens. We sample a corpus of approximately 500,000 sentences in Hebrew and train the model to minimize the following loss: L = ||hold âˆ’ hnew||^2"
  - [section]: "As noted by Zhu et al. (2023), models perform well when continuing training without extending the tokenizer indicating that there is knowledge in the existing embeddings."
- Break condition: If the distillation process fails to align the new tokens with meaningful semantic representations, the model's ability to process Hebrew will degrade.

### Mechanism 3
- Claim: Including aligned translation pairs in the pre-training corpus helps the model map English knowledge to Hebrew.
- Mechanism: The model learns to associate English sentences with their Hebrew translations during pre-training, creating cross-lingual representations that improve translation and comprehension tasks.
- Core assumption: The model can learn to leverage English knowledge when processing Hebrew text if given aligned examples.
- Evidence anchors:
  - [section]: "Inspired by the works of Zhu et al. (2023); Wendler et al. (2024), we also included a corpus of aligned translation sentence pairs to help the model map between its internal English knowledge and Hebrew."
  - [section]: "We used the CCMatrix corpus of Hebrew-English pairs (Schwenk et al., 2020) and converted each pair into text using set templates."
- Break condition: If the aligned corpus is too small or of low quality, the model may not effectively learn the cross-lingual mappings.

## Foundational Learning

- Concept: Subword tokenization (BPE)
  - Why needed here: The model uses a tokenizer based on Byte-Pair Encoding, which is critical to understand how Hebrew words are split into tokens and how extending the tokenizer improves compression.
  - Quick check question: How does BPE tokenization work, and why would adding language-specific tokens improve compression for morphologically rich languages?

- Concept: Embedding distillation
  - Why needed here: This technique is used to transfer knowledge from the original Mistral embeddings to the newly added Hebrew tokens, ensuring the model retains semantic understanding.
  - Quick check question: What is embedding distillation, and how does minimizing the difference between hidden states help transfer knowledge?

- Concept: Few-shot prompting
  - Why needed here: The evaluation uses few-shot learning to assess the model's ability to perform tasks with minimal examples, which is essential for understanding how the model generalizes.
  - Quick check question: What is few-shot prompting, and how does providing examples in the prompt help the model perform a task?

## Architecture Onboarding

- Component map:
  Tokenizer -> Embedding layer (with distilled embeddings) -> LM Head (calibrated) -> Pre-training corpus (50% Hebrew, 50% English) -> Fine-tuning datasets (SFT and DPO)

- Critical path:
  1. Extend tokenizer with Hebrew-specific tokens
  2. Perform embedding distillation to transfer knowledge to new tokens
  3. Calibrate LM Head to align with extended vocabulary
  4. Conduct continuous pre-training on the mixed Hebrew-English corpus
  5. Perform SFT on instruction datasets in both languages
  6. Apply DPO to refine responses based on user preferences

- Design tradeoffs:
  - Adding more Hebrew tokens could further reduce compression but may introduce diminishing returns or model degradation
  - Using a larger aligned translation corpus could improve cross-lingual performance but increases computational cost
  - Fine-tuning on more complex Hebrew instruction datasets could improve task performance but requires more high-quality data

- Failure signatures:
  - If the model fails to process Hebrew text correctly, the tokenizer extension or embedding distillation may be flawed
  - If the model performs poorly on translation tasks, the aligned translation corpus may be insufficient or of low quality
  - If the model generates incorrect or nonsensical responses during instruction following, the SFT or DPO datasets may be inadequate

- First 3 experiments:
  1. Evaluate the compression ratio of the extended tokenizer on a sample of Hebrew text to confirm the improvement
  2. Test the model's ability to process Hebrew text before and after embedding distillation to verify the effectiveness of the distillation process
  3. Assess the model's translation performance on a small set of aligned translation pairs to ensure the cross-lingual mappings are learned

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DictaLM2.0 compare to other multilingual LLMs on low-resource languages beyond Hebrew?
- Basis in paper: [inferred] The paper discusses adapting LLMs to Hebrew but does not compare performance to other low-resource languages.
- Why unresolved: The study focuses specifically on Hebrew and does not provide a comparative analysis with other low-resource languages.
- What evidence would resolve it: Comparative experiments evaluating DictaLM2.0 against other multilingual LLMs on various low-resource languages.

### Open Question 2
- Question: What is the long-term impact of the embedding distillation process on model performance for other low-resource languages?
- Basis in paper: [explicit] The paper mentions embedding distillation for Hebrew but does not explore its long-term effects on other languages.
- Why unresolved: The study only applies embedding distillation to Hebrew, leaving its generalizability and long-term effects on other languages untested.
- What evidence would resolve it: Longitudinal studies applying embedding distillation to multiple low-resource languages and tracking performance over time.

### Open Question 3
- Question: How does the model's performance degrade when the proportion of Hebrew data in the training corpus is reduced?
- Basis in paper: [inferred] The paper uses a 50-50 split of Hebrew and English data but does not explore the impact of reducing Hebrew data.
- Why unresolved: The study does not experiment with varying the ratio of Hebrew to English data in the training corpus.
- What evidence would resolve it: Experiments with different ratios of Hebrew to English data and corresponding performance evaluations.

### Open Question 4
- Question: What are the computational trade-offs between extending the tokenizer and using alternative tokenization methods for low-resource languages?
- Basis in paper: [explicit] The paper discusses extending the tokenizer but does not compare it to other tokenization methods.
- Why unresolved: The study focuses on extending the Mistral tokenizer but does not evaluate alternative tokenization strategies.
- What evidence would resolve it: Comparative analysis of model performance and computational efficiency using different tokenization methods for low-resource languages.

## Limitations

- The exact composition and size of the aligned translation corpus is not clearly specified, though it plays a critical role in cross-lingual performance
- Manual evaluation for summarization relies on GPT-4 scoring without detailed methodology description, introducing potential bias
- The cleaning and filtering pipeline for Hebrew corpus lacks specific thresholds and validation metrics
- While 1,000 tokens show significant gains, the optimal number for different morphological complexity levels remains unexplored

## Confidence

- **High confidence**: Tokenizer extension mechanism and its impact on compression ratios - empirically demonstrated and aligns with established BPE principles
- **Medium confidence**: Embedding distillation approach - method is sound but actual quality of transferred representations for Hebrew tokens not directly validated
- **Medium confidence**: Overall model performance - shows state-of-the-art results but comparisons primarily with older models and Google Translate rather than other modern Hebrew-adapted LLMs

## Next Checks

1. **Compression ratio validation**: Measure the token-per-word ratio on a held-out Hebrew corpus before and after tokenizer extension, specifically testing whether the claimed reduction from ~5.81 to ~2.9 tokens per word is consistently achieved across different Hebrew text domains.

2. **Cross-lingual alignment quality**: Evaluate the model's ability to correctly map English knowledge to Hebrew using controlled translation pairs, particularly focusing on whether the aligned corpus size (not fully specified) is sufficient to establish robust cross-lingual representations.

3. **Generalization across morphological complexity**: Test the model's performance on Hebrew words with varying morphological complexity (simple vs. complex derivations) to determine if the 1,000-token addition provides uniform benefits or if certain word types remain poorly tokenized.