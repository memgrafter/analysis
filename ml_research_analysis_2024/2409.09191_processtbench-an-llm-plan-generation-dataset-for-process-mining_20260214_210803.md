---
ver: rpa2
title: 'ProcessTBench: An LLM Plan Generation Dataset for Process Mining'
arxiv_id: '2409.09191'
source_url: https://arxiv.org/abs/2409.09191
tags:
- process
- plan
- dataset
- plans
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ProcessTBench is a synthetic dataset designed to evaluate LLM plan
  generation within a process mining framework. It extends TaskBench by incorporating
  multi-lingual query paraphrases and multiple alternative plan variants, generating
  532 base queries with 5-6 paraphrased versions each, and 2-11 plans per query.
---

# ProcessTBench: An LLM Plan Generation Dataset for Process Mining

## Quick Facts
- arXiv ID: 2409.09191
- Source URL: https://arxiv.org/abs/2409.09191
- Authors: Andrei Cosmin Redis; Mohammadreza Fani Sani; Bahram Zarrin; Andrea Burattin
- Reference count: 9
- One-line primary result: ProcessTBench enables robust evaluation of LLM plan generation through paraphrased queries and multiple plan variants with process mining conformance checking

## Executive Summary
ProcessTBench is a synthetic dataset designed to evaluate large language model (LLM) plan generation within a process mining framework. It extends the TaskBench benchmark by incorporating multi-lingual query paraphrases and multiple alternative plan variants, generating 532 base queries with 5-6 paraphrased versions each, and 2-11 plans per query. The dataset supports downstream tasks like process discovery and conformance checking using metrics such as alignment fitness. Quality analysis showed paraphrased queries maintain alignment quality comparable to originals (p-value 0.56). Plans were generated using GPT-4 and include Petri net representations of ground truth plans.

## Method Summary
The method extends TaskBench by adding multi-lingual paraphrased queries and alternative plan variants generated by an LLM. Each base query receives 5-6 paraphrased versions, and GPT-4 generates 2-11 alternative plans per query. Ground truth plans from TaskBench are converted to Petri nets for conformance checking. The dataset includes event logs, process discovery capabilities, and 40 unique tools. Conformance checking using alignment and replay fitness metrics evaluates plan quality, while process mining techniques analyze plan diversity, reliability, and adaptability across languages.

## Key Results
- Paraphrased queries maintain alignment quality comparable to originals (p-value 0.56)
- Dataset includes 532 base queries with 5-6 paraphrased versions each and 2-11 plans per query
- Petri net representations enable formal conformance checking between generated and ground truth plans

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated plans maintain quality across paraphrased queries, enabling robustness evaluation
- Mechanism: Multiple paraphrased versions of each query are generated and compared using conformance checking. The Wilcoxon signed-rank test (p-value 0.56) shows no significant difference in plan quality between original and paraphrased queries.
- Core assumption: Paraphrased queries that produce plans with similar alignment fitness represent semantically equivalent tasks from the LLM's perspective.
- Evidence anchors:
  - [abstract]: "Quality analysis showed paraphrased queries maintain alignment quality comparable to originals (p-value 0.56)"
  - [section]: "The comparison revealed 1,965 instances of equivalence, 397 instances where the paraphrased queries performed worse, and 389 instances where they performed better"
- Break condition: If paraphrased queries produce systematically different alignment fitness scores (p-value < 0.05), the assumption of semantic equivalence breaks down.

### Mechanism 2
- Claim: Multiple plan variants per query enable process mining analysis of LLM behavior diversity
- Mechanism: The LLM Plan Variants Generator creates 2-11 alternative plans per query, providing diverse execution traces that can be analyzed for common patterns and deviations using process mining algorithms.
- Core assumption: Alternative plans generated by the LLM for the same query represent meaningful variations in approach rather than random noise.
- Evidence anchors:
  - [abstract]: "generating 532 base queries with 5-6 paraphrased versions each, and 2-11 plans per query"
  - [section]: "Each query in ProcessTBench is associated with a ground truth plan in Petri net format and 5-6 LLM-generated plans"
- Break condition: If alternative plans show no structural differences or clustering patterns, the diversity assumption fails.

### Mechanism 3
- Claim: Petri net representation enables formal conformance checking between generated and ground truth plans
- Mechanism: Ground truth plans from TaskBench are converted to Petri nets, and conformance checking metrics (alignment fitness, replay fitness) compare generated plans against these reference models to quantify plan quality and adherence to expected processes.
- Core assumption: Petri net conversion preserves the essential control flow structure of the original DAG plans while enabling standard process mining analysis.
- Evidence anchors:
  - [abstract]: "Plans were generated using GPT-4 and include Petri net representations of ground truth plans"
  - [section]: "the ground truth plans in TaskBench, represented as directed acyclic graphs (DAGs), were converted to Petri nets"
- Break condition: If Petri net conversion introduces significant structural changes or loses important parallelism information, conformance checking results become unreliable.

## Foundational Learning

- Concept: Conformance checking in process mining
  - Why needed here: Essential for comparing generated LLM plans against ground truth models to measure plan quality and reliability
  - Quick check question: What are the two primary fitness metrics used in conformance checking, and how do they differ in what they measure?

- Concept: Process mining metrics (Cardoso complexity, cyclomatic complexity, degree of concurrency)
  - Why needed here: Required to quantify and compare the complexity and parallelism characteristics of ground truth and generated plans
  - Quick check question: How does the degree of concurrency metric distinguish between sequential and parallel process structures?

- Concept: Paraphrasing evaluation methodology
  - Why needed here: Critical for validating that paraphrased queries maintain semantic equivalence for robustness testing
  - Quick check question: Why was a non-parametric Wilcoxon signed-rank test chosen over a t-test for comparing alignment fitness between original and paraphrased plans?

## Architecture Onboarding

- Component map: TaskBench Queries → LLM Planner → LLM Plan Variants Generator → Event Log Parser → Plan Conformance Checker → Process Discovery → Output

- Critical path: TaskBench Queries → LLM Planner → LLM Plan Variants Generator → Event Log Parser → Plan Conformance Checker → ProcessTBench dataset

- Design tradeoffs: Using GPT-4-0613 provides high-quality plans but introduces API dependency and cost considerations; multiple paraphrased versions increase dataset coverage but multiply computational requirements

- Failure signatures:
  - Low alignment fitness scores across many queries indicate systematic plan generation issues
  - High variance in alternative plan quality suggests LLM instability
  - Petri net conversion failures indicate complex ground truth structures that don't map well to standard process mining formats

- First 3 experiments:
  1. Run conformance checking on a small subset of original vs. paraphrased queries to verify the p-value 0.56 result holds in practice
  2. Analyze the distribution of alternative plan variants for a single query to understand the diversity of LLM-generated solutions
  3. Test the Petri net conversion algorithm on representative TaskBench ground truth plans to ensure structural preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different LLM architectures compare in their ability to generate parallel plans that match ground truth processes?
- Basis in paper: [explicit] The paper mentions LLM Planner is "slightly modified from planners like ReAct" and focuses on parallel actions, but doesn't compare different LLM architectures
- Why unresolved: The study uses only GPT-4 for plan generation without benchmarking against other LLM architectures like Claude, Llama, or specialized planning models
- What evidence would resolve it: Systematic comparison of plan quality, parallelism handling, and paraphrase robustness across multiple LLM architectures using the same ProcessTBench queries and evaluation metrics

### Open Question 2
- Question: What is the relationship between query complexity and plan generation success rates across different languages?
- Basis in paper: [inferred] The paper includes multi-lingual query paraphrases but only validates quality through alignment fitness, not success rates across complexity levels
- Why unresolved: The analysis focuses on alignment fitness between original and paraphrased queries but doesn't examine how query complexity affects LLM success rates across languages
- What evidence would resolve it: Correlation analysis between ground truth process complexity metrics and LLM plan success rates, stratified by language and query formulation

### Open Question 3
- Question: How do alternative plan variants relate to each other in terms of process characteristics and what patterns emerge?
- Basis in paper: [explicit] The dataset includes "2-11 plans per query" and mentions "plan diversity" but doesn't analyze relationships between variants
- Why unresolved: While the paper notes the dataset contains multiple plan variants per query, it doesn't investigate whether certain types of variants tend to emerge systematically
- What evidence would resolve it: Cluster analysis of generated plan variants to identify common patterns, and statistical comparison of their characteristics relative to ground truth plans

## Limitations
- Dependence on GPT-4-0613 introduces API dependency and cost constraints
- Dataset focuses on synthetic data, which may not fully capture real-world complexity
- Exact prompt templates and Petri net conversion algorithm details are not fully specified

## Confidence
- High confidence in dataset construction methodology and use of established process mining metrics
- Medium confidence in robustness findings due to p-value of 0.56 showing no significant difference between original and paraphrased plans
- Medium confidence in diversity analysis of alternative plans

## Next Checks
1. Reproduce the paraphrasing robustness analysis by running conformance checking on a small subset of original vs. paraphrased queries to verify the p-value 0.56 result holds in practice
2. Analyze the distribution of alternative plan variants for a single query to understand the structural diversity and identify whether the plans represent meaningful variations or exhibit clustering patterns
3. Validate the Petri net conversion algorithm on a variety of representative TaskBench ground truth plans to ensure that the conversion preserves essential control flow structures and does not introduce artifacts that could bias conformance checking results