---
ver: rpa2
title: 'Latent Guard: a Safety Framework for Text-to-image Generation'
arxiv_id: '2404.08031'
source_url: https://arxiv.org/abs/2404.08031
tags:
- latent
- guard
- prompts
- unsafe
- concepts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Latent Guard is a framework that detects unsafe concepts in text-to-image
  prompts by analyzing latent embeddings of the input text, rather than relying on
  blacklists or direct classification. It learns a joint latent space mapping for
  unsafe prompts and associated concepts using contrastive learning on synthetic data
  generated by LLMs.
---

# Latent Guard: a Safety Framework for Text-to-image Generation

## Quick Facts
- arXiv ID: 2404.08031
- Source URL: https://arxiv.org/abs/2404.08031
- Authors: Runtao Liu; Ashkan Khakzar; Jindong Gu; Qifeng Chen; Philip Torr; Fabio Pizzati
- Reference count: 40
- One-line primary result: Latent Guard achieves 0.868 accuracy on explicit in-distribution data for detecting unsafe concepts in text-to-image prompts

## Executive Summary
Latent Guard introduces a novel framework for detecting unsafe concepts in text-to-image generation prompts by analyzing latent embeddings rather than relying on blacklists or direct classification. The approach learns a joint latent space mapping for unsafe prompts and associated concepts using contrastive learning on synthetic data generated by LLMs. This enables test-time modification of concept blacklists without retraining and demonstrates robustness against adversarial rephrasing or optimization attacks.

## Method Summary
Latent Guard detects unsafe concepts by mapping text prompt embeddings close to blacklisted concept embeddings in a learned latent space. The framework uses a pretrained CLIP text encoder with an Embedding Mapping Layer that employs cross-attention and MLPs to enhance token-level relevance detection. Training occurs on synthetic data generated by LLMs, creating unsafe prompts centered around blacklisted concepts and corresponding safe prompts. At inference, the model checks cosine similarity between prompt embeddings and concept embeddings to make safety decisions, allowing dynamic blacklist modification without retraining.

## Key Results
- Achieves 0.868 accuracy on explicit in-distribution data from CoPro dataset
- Outperforms four baselines including text blacklists, CLIPScore, BERTScore, and LLM classifier
- Demonstrates strong generalization to unseen datasets like Unsafe Diffusion and I2P++ with minimal computational overhead
- Shows robustness against adversarial rephrasing and optimization attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Latent Guard detects unsafe concepts by mapping prompt embeddings close to concept embeddings in learned latent space
- Mechanism: Uses contrastive learning to pull together embeddings of unsafe prompts and their associated concepts while pushing apart safe prompts and other concepts. Cross-attention enhances token-level relevance detection.
- Core assumption: Unsafe prompts can be meaningfully associated with specific concepts that can be learned in shared latent space
- Evidence anchors: [abstract] "learns a joint latent space mapping for unsafe prompts and associated concepts using contrastive learning on synthetic data generated by LLMs"
- Break condition: Input prompts with multiple concepts or ambiguous language that cannot be clearly mapped to single concept

### Mechanism 2
- Claim: Robust to adversarial rephrasing and optimization attacks by detecting concepts beyond exact wording
- Mechanism: Latent space representation identifies related concepts and synonyms rather than exact text matching. Cross-attention identifies relevant tokens within prompts.
- Core assumption: Adversarial attacks targeting exact word matching can be circumvented by detecting semantic similarity in latent space
- Evidence anchors: [abstract] "shows robustness against adversarial rephrasing or optimization attacks"
- Break condition: Adversarial attack successfully maps unsafe prompt to same latent space point as safe prompt

### Mechanism 3
- Claim: Generalizes to unseen concepts and datasets without retraining
- Mechanism: Contrastive learning framework learns generalizable latent space representation that can identify concepts not seen during training. Model adapts to new blacklists at test time.
- Core assumption: Latent space learned on synthetic data can transfer to real-world prompts and unseen concepts
- Evidence anchors: [abstract] "generalizes well to unseen datasets like Unsafe Diffusion and I2P++"
- Break condition: Distribution shift between synthetic training data and real-world prompts too large for effective generalization

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: To learn latent space where unsafe prompts and associated concepts are mapped close together while separating safe prompts and other concepts
  - Quick check question: How does contrastive loss help separate safe and unsafe prompts in learned latent space?

- Concept: Cross-attention mechanism
  - Why needed here: To identify which tokens in input prompt are most relevant to concept being checked, enhancing detection of unsafe content
  - Quick check question: How does cross-attention layer help determine which parts of prompt are related to specific concept?

- Concept: Text embeddings and latent space
  - Why needed here: To represent both prompts and concepts in shared space where semantic similarity can be measured for safety detection
  - Quick check question: Why beneficial to detect concepts in latent space rather than using exact text matching?

## Architecture Onboarding

- Component map: Input prompt → Text encoder (CLIP) → Embedding Mapping Layer (cross-attention + MLPs) → Cosine similarity comparison with concept embeddings → Safety decision

- Critical path: Input prompt → Text encoder → Embedding Mapping Layer → Cosine similarity comparison with concept embeddings → Safety decision

- Design tradeoffs:
  - Training data synthesis vs. real-world data collection
  - Computational cost of cross-attention vs. simple MLPs
  - Generalization to unseen concepts vs. specificity to training data

- Failure signatures:
  - False negatives: Safe prompts incorrectly classified as unsafe
  - False positives: Unsafe prompts incorrectly classified as safe
  - Poor generalization to new concepts or datasets

- First 3 experiments:
  1. Test accuracy on CoPro dataset with varying sizes of training concepts
  2. Evaluate robustness to adversarial rephrasing attacks
  3. Measure inference time and memory usage for different blacklist sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance of Latent Guard vary with different text encoders beyond CLIP and OpenCLIP?
- Basis in paper: [explicit] Paper mentions training on different text encoder (OpenCLIP ViT-H) and reports comparable performance, but suggests further investigation into other encoders
- Why unresolved: Paper only provides results for two specific text encoders, leaving performance with other potential encoders unexplored
- What evidence would resolve it: Comprehensive testing with diverse set of text encoders to assess generalizability and performance consistency

### Open Question 2
- Question: What is impact of concept blacklist size on Latent Guard's detection performance?
- Basis in paper: [inferred] Paper discusses importance of concept blacklist at test time and shows performance depends on its size, but lacks detailed analysis of this relationship
- Why unresolved: Paper provides limited insights into how varying blacklist size affects detection performance
- What evidence would resolve it: Systematic experiments varying blacklist size and measuring corresponding impact on detection accuracy, false positive/negative rates, and computational requirements

### Open Question 3
- Question: How does Latent Guard perform in detecting unsafe concepts in real-world, user-generated prompts?
- Basis in paper: [inferred] Paper mentions potential for distribution shift between LLM-generated data and real-world prompts
- Why unresolved: Paper's evaluation primarily based on synthetically generated data which may not capture full complexity of real-world user prompts
- What evidence would resolve it: Deployment in real-world T2I application and evaluation on large dataset of user-generated prompts

### Open Question 4
- Question: Can Latent Guard be extended to detect unsafe concepts in multimodal inputs like images or videos?
- Basis in paper: [inferred] Paper focuses solely on text-based inputs, leaving potential for multimodal concept detection unexplored
- Why unresolved: Paper does not address possibility of extending to handle multimodal inputs
- What evidence would resolve it: Development and evaluation of multimodal version that can process and analyze both text and image inputs

## Limitations

- Performance drops significantly on synonym (0.582) and adversarial (0.536) test sets compared to explicit in-distribution data (0.868)
- Framework relies entirely on synthetic data generated by LLMs, which may not capture full complexity of real-world unsafe prompts
- Limited evaluation on real-world datasets with diverse cultural and linguistic contexts

## Confidence

**High Confidence (70-90%)**:
- Framework can learn joint latent space mapping for unsafe prompts and associated concepts using contrastive learning
- Achieves high accuracy on explicit in-distribution data (0.868 on CoPro)
- Test-time modification of concept blacklist without retraining is technically feasible

**Medium Confidence (40-70%)**:
- Robustness against adversarial rephrasing and optimization attacks demonstrated but with limited effectiveness (0.536 accuracy on adversarial test set)
- Generalization to unseen concepts and datasets claimed but needs more extensive validation
- Minimal computational overhead stated but not rigorously benchmarked against production requirements

**Low Confidence (0-40%)**:
- Claim that approach "outperforms" all baselines may be overstated given performance gap on synonym and adversarial test sets
- Long-term effectiveness against evolving adversarial techniques remains unproven
- Real-world deployment performance in diverse cultural and linguistic contexts is unknown

## Next Checks

1. **Adversarial Robustness Stress Test**: Conduct systematic adversarial attack experiments using established techniques (e.g., TextFooler, genetic algorithms) to identify framework's breaking points. Test with progressively sophisticated attacks targeting contrastive learning mechanism specifically.

2. **Real-World Data Validation**: Evaluate Latent Guard on large-scale, diverse dataset of real-world prompts collected from actual text-to-image generation platforms, including prompts from multiple languages, cultural contexts, and user demographics. Compare performance against gold-standard human annotation.

3. **Computational Efficiency Benchmark**: Measure inference latency, memory usage, and energy consumption across different hardware configurations (CPU, GPU, edge devices) for various blacklist sizes (10, 100, 1000 concepts). Compare metrics against traditional blacklisting approaches and real-time safety requirements for commercial deployment.