---
ver: rpa2
title: 'RoVRM: A Robust Visual Reward Model Optimized via Auxiliary Textual Preference
  Data'
arxiv_id: '2408.12109'
source_url: https://arxiv.org/abs/2408.12109
tags:
- preference
- data
- rovrm
- training
- llav
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of hallucination in large vision-language
  models (LVLMs) by introducing RoVRM, a robust visual reward model that leverages
  auxiliary textual preference data to improve human-preference alignment. The core
  innovation is a three-phase progressive training approach that bridges task and
  modality gaps between textual and visual preference data, combined with optimal
  transport-based preference data selection to identify the most relevant samples.
---

# RoVRM: A Robust Visual Reward Model Optimized via Auxiliary Textual Preference Data

## Quick Facts
- **arXiv ID:** 2408.12109
- **Source URL:** https://arxiv.org/abs/2408.12109
- **Reference count:** 11
- **Key outcome:** Reduces hallucination rates by up to 13.2 points while improving multiple benchmark scores including LLaVA-Bench (8.4 points) and MM-Instruct (7.58 points during RL training)

## Executive Summary
RoVRM addresses hallucination challenges in large vision-language models by leveraging auxiliary textual preference data through a three-phase progressive training approach. The method uses optimal transport-based preference data selection to identify relevant samples that bridge task and modality gaps between textual and visual preference data. Experiments demonstrate RoVRM consistently outperforms traditional VRMs, achieving significant improvements in trustworthiness and helpfulness metrics while requiring substantially less visual preference data.

## Method Summary
RoVRM employs a three-phase progressive training pipeline: Phase 1 uses textual preference data (UltraFeedback) to learn general preference patterns, Phase 2 fine-tunes with image caption-based preference data generated by GPT-4o-mini, and Phase 3 incorporates visual preference data (RLAIF-V). The method includes optimal transport-based data selection to identify the most relevant samples for each training phase. This approach enables the model to leverage abundant textual preference data while minimizing the need for extensive visual preference data, ultimately improving human-preference alignment in LLaVA-1.5-7B and -13B models.

## Key Results
- Reduces hallucination rates by up to 13.2 points on Object HalBench and MMHalBench
- Improves LLaVA-Bench scores by 8.4 points and MM-Instruct by 7.58 points during RL training
- Achieves performance comparable to VRM-Vanilla with only 5k visual preference samples versus 83k
- Integrates seamlessly with DPO, achieving additional gains of 17.82 points on MM-Instruct

## Why This Works (Mechanism)

### Mechanism 1
Textual preference data contains generalizable preference patterns that transfer across modalities when bridged appropriately through a three-phase progressive training approach. This hierarchical transfer reduces the need for large-scale visual preference data while maintaining alignment quality.

### Mechanism 2
Optimal transport-based distance computation effectively identifies textual preference samples that align with vision-language task preferences by measuring how well gradient features can be "transported" between distributions.

### Mechanism 3
Bridging task and modality gaps progressively through three phases enables more effective learning than direct multi-modal training by preventing catastrophic forgetting and allowing the model to build on previously learned representations.

## Foundational Learning

- **Bradley-Terry and Plackett-Luce models**: These provide mathematical foundation for converting pairwise and ranked preferences into learnable reward signals. *Quick check: What is the key difference between Bradley-Terry and Plackett-Luce models in terms of the preference data they can handle?*

- **Optimal transport theory and Wasserstein distance**: This framework enables meaningful comparison of preference patterns across different modalities by measuring the "cost" of transforming one distribution into another. *Quick check: How does optimal transport distance differ from simpler distance metrics like cosine similarity when comparing distributions?*

- **Gradient-based data selection and random projection**: These techniques enable efficient selection of relevant preference samples from large datasets by reducing dimensionality while preserving important relationships. *Quick check: What is the purpose of using random projection before computing optimal transport distances?*

## Architecture Onboarding

- **Component map**: Textual preference data (Phase 1) -> Image caption-based data (Phase 2) -> Visual preference data (Phase 3) -> LLaVA model alignment

- **Critical path**: Warmup reward model training -> Gradient feature extraction -> Optimal transport distance computation and selection -> Three-phase progressive training

- **Design tradeoffs**: Three-phase approach trades training time and complexity for improved sample efficiency and robustness versus direct training on visual preference data

- **Failure signatures**: Poor performance may indicate misalignment between textual and visual preferences, ineffective data selection, catastrophic forgetting during progressive training, or suboptimal hyperparameters

- **First 3 experiments**:
  1. Compare RoVRM with VRM-Vanilla on a small benchmark using default hyperparameters
  2. Test the impact of different sample sizes in each phase (5k, 10k, 20k textual samples)
  3. Evaluate the necessity of each phase by removing Phase 1 or Phase 2 individually

## Open Questions the Paper Calls Out

### Open Question 1
What is the theoretical limit of visual preference data reduction when using three-phase progressive training with auxiliary textual preference data? The paper demonstrates performance with 5k visual samples but doesn't explore the theoretical minimum or establish scaling laws for different dataset sizes.

### Open Question 2
How does the performance of RoVRM change when using textual preference datasets with different quality distributions or domain specificities? The paper shows UltraFeedback performs best but doesn't analyze how dataset characteristics affect performance across domains.

### Open Question 3
What are the long-term stability implications of using textual preference data for visual reward model training, particularly regarding catastrophic forgetting or preference drift over extended RL training? The paper demonstrates immediate improvements but doesn't investigate long-term training dynamics.

## Limitations
- Comparison primarily against VRM-Vanilla rather than other sophisticated VRM training methods
- Assumption of effective preference transfer may not hold for different domains or visual question types
- Three-phase approach and optimal transport selection add computational complexity that may offset benefits

## Confidence
- **Core claim effectiveness**: Medium - supported by quantitative improvements but limited ablation studies on phase necessity
- **Optimal transport mechanism**: Medium-Lower - shows performance gains but specific contribution versus progressive training is unclear
- **Scalability to other architectures**: Low - experiments limited to LLaVA-1.5 models, generalizability unproven

## Next Checks
1. Conduct ablation study on phase necessity by systematically removing each training phase and retraining RoVRM
2. Apply RoVRM to specialized domains (medical imaging, technical diagrams) to test cross-domain transferability
3. Replace optimal transport-based selection with simpler alternatives (random sampling, k-means) to isolate selection mechanism contribution