---
ver: rpa2
title: Pessimistic Iterative Planning with RNNs for Robust POMDPs
arxiv_id: '2408.08770'
source_url: https://arxiv.org/abs/2408.08770
tags:
- robust
- rfscn
- policy
- memory
- pomdps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework for robust policy planning in POMDPs
  with uncertain transition and observation probabilities. The key idea is to iteratively
  refine policies by computing worst-case POMDP instances that challenge the current
  policy, then optimizing new policies for these pessimistic scenarios.
---

# Pessimistic Iterative Planning with RNNs for Robust POMDPs

## Quick Facts
- **arXiv ID**: 2408.08770
- **Source URL**: https://arxiv.org/abs/2408.08770
- **Reference count**: 40
- **Primary result**: RFSCNet produces more robust POMDP policies than baseline approaches, particularly when large memory is needed, by learning memory structure rather than pre-specifying it

## Executive Summary
This paper addresses robust policy planning in POMDPs with uncertain transition and observation probabilities by proposing a framework that iteratively refines policies through computing worst-case POMDP instances. The key innovation is using recurrent neural networks to extract finite-state controllers from learned policies, implemented in the RFSCNet algorithm. RFSCNet alternates between training RNNs to mimic supervision policies on pessimistic POMDPs and using robust dynamic programming to evaluate and select new pessimistic instances. Experiments on four benchmarks show RFSCNet produces more robust policies than baseline approaches and is less sensitive to memory size than state-of-the-art convex optimization methods.

## Method Summary
The paper proposes a framework for robust policy planning in POMDPs with uncertain transition and observation probabilities (Robust POMDPs or RPOMDPs). The core idea is to iteratively refine policies by computing worst-case POMDP instances that challenge the current policy, then optimizing new policies for these pessimistic scenarios. The RFSCNet algorithm implements this by using recurrent neural networks (RNNs) to extract finite-state controllers (FSCs) from learned policies. The framework alternates between two main steps: (1) training RNNs to mimic supervision policies on pessimistic POMDPs using cross-entropy loss, and (2) evaluating and selecting new pessimistic POMDPs using robust dynamic programming on product Markov chains and linear programming. The approach particularly excels when large memory is needed, suggesting benefits of learning memory structure rather than pre-specifying it.

## Key Results
- RFSCNet produces more robust policies than baseline approaches that train on fixed or randomly chosen POMDPs
- The approach is less sensitive to memory size than state-of-the-art convex optimization methods like SCP
- RFSCNet particularly excels when large memory is needed, demonstrating benefits of learning memory structure
- Experiments on four benchmarks (Aircraft collision avoidance, Avoid, Evade, Intercept grid-worlds) show consistent improvements in robust value across 20 seeds

## Why This Works (Mechanism)
The mechanism works by iteratively refining policies through computing worst-case POMDP instances that challenge the current policy. By alternating between training RNNs to mimic supervision policies on pessimistic POMDPs and using robust dynamic programming to evaluate and select new pessimistic instances, the algorithm gradually improves robustness. The use of RNNs to extract FSCs allows for learning memory structure rather than pre-specifying it, which proves particularly beneficial when large memory is needed.

## Foundational Learning
- **POMDPs with interval uncertainty**: Models where transition probabilities are uncertain but bounded within intervals - needed for robust planning under model uncertainty
- **Finite-state controllers (FSCs)**: Memory-based policies represented as finite-state machines - needed to handle partial observability
- **Robust dynamic programming**: Algorithm for computing optimal policies under worst-case scenarios - needed for evaluating policy robustness
- **RNN-based policy extraction**: Using recurrent neural networks to extract policies from supervision - needed to learn memory structure efficiently
- **Clustering for FSC extraction**: Using k-means++ or QRNN to cluster RNN hidden states into controller states - needed to discretize continuous memory representations

## Architecture Onboarding

**Component Map**: POMDP Model -> Supervision Policies (QMDP/FIB) -> RNN Training -> FSC Extraction (k-means++/QRNN) -> Robust Dynamic Programming -> Pessimistic POMDP Selection (LP) -> Policy Evaluation

**Critical Path**: POMDP → Supervision → RNN → FSC → Robust DP → LP → POMDP (next iteration)

**Design Tradeoffs**:
- Learning memory structure via RNN vs pre-specifying FSC size
- Using supervision policies vs direct optimization
- Iterative refinement vs one-shot optimization

**Failure Signatures**:
- RNN training divergence or poor convergence
- Poor FSC extraction from RNN (disconnected clusters, poor separation)
- LP for pessimistic POMDP selection becoming infeasible

**3 First Experiments**:
1. Verify RNN can learn to mimic supervision policy on simple POMDP
2. Test FSC extraction quality from trained RNN on small example
3. Run single PIP iteration end-to-end on benchmark environment

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on supervision policies (QMDP/FIB) for training data generation, limiting performance to supervision quality
- Computational cost of iterative PIP framework not thoroughly analyzed, raising scalability concerns
- Experiments limited to moderate-sized POMDPs, with unclear performance in high-dimensional or continuous state spaces
- Claims about reduced memory sensitivity vs convex optimization methods lack complete explanation

## Confidence

**Major Uncertainties and Limitations:**
- The framework's reliance on supervision policies introduces a potential limitation - the quality of RFSCNet policies is bounded by the quality of these supervision policies
- Computational cost of the iterative PIP framework, particularly robust dynamic programming and LP solving, is not thoroughly analyzed
- Experiments focus on moderate-sized POMDPs, leaving uncertainty about performance in high-dimensional or continuous state spaces

**Confidence Labels:**
- **High Confidence**: The core PIP framework and its iterative refinement mechanism, the FSC extraction via RNN clustering, and overall experimental methodology
- **Medium Confidence**: Comparative results against baselines and sensitivity analysis to memory size within tested environments
- **Low Confidence**: Claims about RFSCNet being less sensitive to memory size than convex optimization methods SCP based on limited comparisons

## Next Checks
1. **Supervision Policy Sensitivity**: Run RFSCNet with different supervision policies (e.g., point-based value iteration) and compare resulting robust values to isolate impact of supervision quality
2. **Computational Cost Profiling**: Measure runtime breakdown of each PIP iteration component across different memory sizes to quantify scalability limitations
3. **Domain Generalization**: Apply RFSCNet to new benchmark POMDP (e.g., Tiger or RockSample) with different uncertainty structures to test framework's robustness beyond four tested environments