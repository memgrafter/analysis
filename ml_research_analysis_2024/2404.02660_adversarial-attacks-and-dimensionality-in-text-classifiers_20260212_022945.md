---
ver: rpa2
title: Adversarial Attacks and Dimensionality in Text Classifiers
arxiv_id: '2404.02660'
source_url: https://arxiv.org/abs/2404.02660
tags:
- adversarial
- attacks
- embedding
- attack
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates adversarial attacks on text classifiers,
  focusing on the relationship between attack effectiveness and the dimensionality
  of word vector embeddings. The authors demonstrate that adversarial examples are
  highly effective only when the embedding dimension of the attack matches the dimensionality
  of the target model.
---

# Adversarial Attacks and Dimensionality in Text Classifiers

## Quick Facts
- arXiv ID: 2404.02660
- Source URL: https://arxiv.org/abs/2404.02660
- Reference count: 40
- Primary result: Adversarial examples are highly effective only when embedding dimension matches target model; ensemble defenses with varying dimensions provide robust protection

## Executive Summary
This paper investigates the relationship between adversarial attack effectiveness and word vector embedding dimensionality in text classifiers. The authors demonstrate that adversarial examples achieve maximum effectiveness only when the attack's embedding dimension matches that of the target model. This finding is exploited to develop an ensemble-based defense mechanism combining multiple models with different embedding dimensions. The approach significantly outperforms state-of-the-art defenses while maintaining high accuracy on clean samples.

## Method Summary
The study trains LSTM-based sequential models with varying embedding dimensions (100-1100) using Word2Vec embeddings on IMDB and Twitter datasets. Adversarial examples are generated using a TextFooler-based algorithm involving word importance ranking and synonym replacement. The attack effectiveness is evaluated across models with different dimensions to establish sensitivity. Ensemble models with 3 and 5 components using majority voting and weighted averaging are then tested as defenses against these attacks. Distance metrics (L1, L2, L∞ norms) are analyzed to measure adversarial perturbation variability across dimensions.

## Key Results
- Adversarial examples show maximum effectiveness only when embedding dimension matches the target model
- Ensemble models with varying dimensions provide superior defense compared to individual models
- Distance metrics show increased variability in high-dimensional spaces when measuring perturbations
- Proposed ensemble approach achieves higher classification accuracy on both clean and adversarial samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial examples are effective only when the embedding dimensionality of the attack matches the dimensionality of the target model.
- Mechanism: The attack search space for adversarial perturbations is defined in the vector space of the model's input embeddings. When this space's dimensionality matches, the attack can exploit the model's learned decision boundaries effectively. If the dimensionality mismatches, the perturbation landscape shifts and the attack fails to find effective adversarial samples.
- Core assumption: The model's vulnerability is tightly coupled to the specific optimization landscape defined by its input embedding dimension.
- Evidence anchors:
  - [abstract] "Our key finding is that there is a very strong correlation between the embedding dimensionality of the adversarial samples and their effectiveness on models tuned with input samples with same embedding dimension."
  - [section 3.2] "The attack tries to search and make perturbations to the samples to shift the adversarial example across the separating hyperplane, residing within the optimization landscape characterized by the embedding input dimension."
  - [corpus] Weak evidence; related papers focus on adversarial defenses or attacks but do not discuss dimensionality matching.
- Break condition: If the attack optimization does not strictly depend on embedding dimensionality, or if the model's decision boundaries are invariant to small changes in input space dimensionality.

### Mechanism 2
- Claim: Ensemble models with varying embedding dimensions can defend against adversarial attacks by exploiting the dimensionality sensitivity.
- Mechanism: Since adversarial examples are highly tuned to a specific embedding dimension, an ensemble of models trained on different dimensions will likely misclassify adversarial samples that are effective against only one model. The majority vote or weighted averaging across models with mismatched dimensions provides robustness.
- Core assumption: Adversarial samples do not transfer well across models with different embedding dimensions.
- Evidence anchors:
  - [abstract] "We utilize this sensitivity to design an adversarial defense mechanism. We use ensemble models of varying inherent dimensionality to thwart the attacks."
  - [section 3.3] "Adversarial examples in text samples are highly tuned with respect to the model, which it tries to fool and does not transfer well to other models trained on different embedding dimensions."
  - [corpus] Weak evidence; no direct corpus support for dimensionality-based ensemble defenses.
- Break condition: If adversarial samples transfer across different embedding dimensions, or if the ensemble voting fails to mitigate the attack.

### Mechanism 3
- Claim: Measurement of adversarial perturbation becomes more variable as embedding dimension increases.
- Mechanism: In higher-dimensional spaces, distance metrics (L1, L2, L∞ norms) become less meaningful and show greater variability when measuring differences between clean and adversarial samples. This increased variability correlates with embedding dimension.
- Core assumption: High-dimensional spaces exhibit counter-intuitive geometric properties that affect distance measurement.
- Evidence anchors:
  - [abstract] "Additionally, the paper studies the measurement of adversarial perturbation using different distance metrics, revealing increased variability with higher embedding dimensions."
  - [section 3.4] "The task of measuring adversarial perturbation is not easy, especially in a high-dimensional setting... We have studied how this notion holds in the context of text classifiers."
  - [corpus] Weak evidence; related papers do not discuss variability in distance metrics with dimensionality.
- Break condition: If distance metrics remain stable or meaningful regardless of embedding dimension.

## Foundational Learning

- Concept: Word vector embeddings and their dimensionality
  - Why needed here: The effectiveness of adversarial attacks and defenses depends on the dimensionality of word vector embeddings used as model inputs.
  - Quick check question: How does changing the embedding dimension affect the size of the vector space in which the model operates?

- Concept: Ensemble methods and voting mechanisms
  - Why needed here: The defense strategy relies on combining multiple models with different embedding dimensions using majority voting or weighted averaging.
  - Quick check question: What is the difference between majority voting and weighted averaging in ensemble models?

- Concept: Distance metrics (L1, L2, L∞ norms) in high-dimensional spaces
  - Why needed here: The paper studies how these metrics behave when measuring adversarial perturbations across different embedding dimensions.
  - Quick check question: Why might distance metrics become less meaningful in high-dimensional spaces?

## Architecture Onboarding

- Component map:
  Data preprocessing pipeline (tokenization, stopword removal, etc.) -> Word2Vec/GloVe embedding layer (configurable dimensionality) -> LSTM/transformer classifier models -> Adversarial attack module (word-level perturbations) -> Ensemble voting module -> Distance metric computation module

- Critical path:
  1. Load and preprocess dataset
  2. Train models with varying embedding dimensions
  3. Generate adversarial samples against each model
  4. Test adversarial samples on all models
  5. Evaluate ensemble defense performance
  6. Measure perturbation distances

- Design tradeoffs:
  - Higher embedding dimensions may capture more semantic information but increase vulnerability to dimensionality-matched attacks.
  - Ensemble models increase robustness but also computational cost and memory usage.
  - Using transformer models may improve baseline accuracy but could affect adversarial vulnerability patterns.

- Failure signatures:
  - Adversarial samples transfer well across different embedding dimensions (ensemble fails)
  - Distance metrics show no increased variability with higher dimensions
  - Models with higher embedding dimensions are not more vulnerable to matched attacks

- First 3 experiments:
  1. Train individual LSTM models with embedding dimensions 100, 200, 300, 400, and 500; evaluate baseline accuracy on clean samples.
  2. Generate adversarial samples against each model; test effectiveness on all models to confirm dimensionality sensitivity.
  3. Build ensemble models (3-model and 5-model) with varying dimensions; evaluate robustness against adversarial samples compared to individual models.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical bounds on adversarial perturbation size as a function of embedding dimensionality in text classifiers?
- Basis in paper: [inferred] The paper discusses how dimensionality affects adversarial vulnerability and perturbation measurement, but doesn't provide theoretical bounds on perturbation size.
- Why unresolved: The paper presents empirical results showing the relationship between dimensionality and perturbation measurement, but doesn't derive theoretical bounds or scaling laws.
- What evidence would resolve it: Mathematical proofs showing the relationship between embedding dimension and maximum allowable perturbation before human annotators can detect it, along with empirical validation across multiple text classification tasks.

### Open Question 2
- Question: How does the proposed ensemble defense mechanism scale with increasing numbers of models and embedding dimensions?
- Basis in paper: [explicit] The paper mentions that ensemble models have k times the space complexity of individual models and discusses using 3 and 5 model ensembles, but doesn't explore scalability beyond this.
- Why unresolved: The paper only tests ensembles with 3 and 5 models, leaving open questions about performance and resource requirements for larger ensembles.
- What evidence would resolve it: Systematic experiments testing ensemble performance and resource requirements with varying numbers of models (e.g., 10, 20, 50) and embedding dimensions, including computational complexity analysis.

### Open Question 3
- Question: How do different word embedding techniques (Word2Vec, GloVe, BERT) affect the relationship between dimensionality and adversarial vulnerability?
- Basis in paper: [explicit] The paper uses Word2Vec embeddings and mentions GloVe and BERT in the background section, but doesn't compare how different embedding techniques affect adversarial vulnerability.
- Why unresolved: The paper focuses on Word2Vec embeddings without exploring whether the observed relationships between dimensionality and adversarial vulnerability hold for other embedding techniques.
- What evidence would resolve it: Comparative experiments using different word embedding techniques (Word2Vec, GloVe, BERT) across the same range of dimensions and adversarial attacks, analyzing any differences in vulnerability patterns.

## Limitations

- The core claim about dimensionality matching affecting attack effectiveness has limited direct evidence from the cited corpus
- The ensemble defense mechanism relies on the assumption that adversarial examples don't transfer across dimensions, which needs more rigorous validation
- The study of distance metric variability in high-dimensional spaces lacks strong theoretical or empirical support

## Confidence

- High: Experimental methodology is clearly outlined, well-established datasets are used, and structured approach to studying dimensionality-attack relationship
- Medium: Ensemble defense claims are plausible but require further validation; innovative approach relies on assumptions needing more testing
- Low: Distance metric variability claims lack theoretical grounding and empirical substantiation

## Next Checks

1. Conduct experiments to systematically test the transferability of adversarial examples across models with different embedding dimensions to validate the core assumption underlying the ensemble defense mechanism.

2. Perform a detailed analysis of distance metric behavior (L1, L2, L∞ norms) in high-dimensional spaces, comparing theoretical expectations with empirical observations on text classifiers.

3. Implement the adversarial attack with varying levels of perturbation and evaluate its effectiveness across different embedding dimensions to assess the robustness of the proposed defense mechanism and identify potential weaknesses.