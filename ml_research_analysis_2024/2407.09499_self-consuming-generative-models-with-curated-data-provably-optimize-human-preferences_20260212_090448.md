---
ver: rpa2
title: Self-Consuming Generative Models with Curated Data Provably Optimize Human
  Preferences
arxiv_id: '2407.09499'
source_url: https://arxiv.org/abs/2407.09499
tags:
- reward
- retraining
- data
- synthetic
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes how user curation of synthetic data impacts
  iterative retraining of generative models. The authors model curation as a discrete
  choice model based on Bradley-Terry rankings, showing that iterative retraining
  on curated data maximizes an underlying reward function while reducing variance.
---

# Self-Consuming Generative Models with Curated Data Provably Optimize Human Preferences

## Quick Facts
- arXiv ID: 2407.09499
- Source URL: https://arxiv.org/abs/2407.09499
- Authors: Damien Ferbach; Quentin Bertrand; Avishek Joey Bose; Gauthier Gidel
- Reference count: 40
- Primary result: Proves iterative retraining on curated synthetic data maximizes reward function and shows stability when mixing with real data

## Executive Summary
This paper provides theoretical analysis of how user curation of synthetic data impacts iterative retraining of generative models. The authors model curation as a Bradley-Terry discrete choice process, proving that iterative retraining on curated data converges to maximum reward regions while maintaining stability when mixed with real data. They establish connections to reinforcement learning from human feedback (RLHF) and demonstrate through experiments on synthetic datasets and CIFAR10 that curation amplifies biases in reward models, leading to class imbalance.

## Method Summary
The paper analyzes iterative retraining where models generate synthetic data, users curate the best samples via Bradley-Terry ranking, and the curated data is used to retrain the model. The theoretical framework proves reward maximization under idealized conditions and establishes stability bounds when mixing real and curated synthetic data. Experiments validate the theoretical predictions on synthetic datasets and demonstrate bias amplification in image generation tasks.

## Key Results
- Iterative retraining on curated synthetic data maximizes the expected reward function
- Mixing real data with curated synthetic data provides stability while still increasing reward
- Curation amplifies biases in reward models, leading to class imbalance in image generation
- Smaller K values act as regularization preventing extreme density concentration

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Curation via Bradley-Terry rankings implicitly optimizes for maximum reward regions
- **Mechanism:** Users select the best sample among K generated candidates. The Bradley-Terry model assigns probability proportional to exp(reward). This filters toward higher reward samples.
- **Core assumption:** Reward function is positive and bounded, and samples are drawn from current model distribution
- **Evidence anchors:**
  - [abstract]: "We prove that, if the data is curated according to a reward model, then the expected reward of the iterative retraining procedure is maximized."
  - [section 2.1]: Lemma 2.1 shows closed-form density: pt+1(x) = pt(x) · H_K(pt)(x) where H_K(pt)(x) is proportional to expected exponential reward.
  - [corpus]: Weak - neighboring papers discuss stability but don't provide direct proof of reward maximization.
- **Break condition:** If reward function is not bounded or initial model puts zero mass on high-reward regions

### Mechanism 2
- **Claim:** Real data injection stabilizes the retraining loop while still increasing reward
- **Mechanism:** Mixing real data with curated synthetic samples bounds the KL divergence from the initial model and prevents collapse to single reward regions
- **Core assumption:** Real data distribution has non-zero probability on all reward levels present in synthetic data
- **Evidence anchors:**
  - [abstract]: "When mixing real and curated synthetic data, they show stability and reward augmentation"
  - [section 2.2.2]: Theorem 2.3 shows reward increases with variance term; Theorem 2.4 bounds KL divergence
  - [corpus]: Moderate - neighboring papers discuss stability but focus on model collapse rather than reward augmentation
- **Break condition:** If real data fraction is too small or reward variance is zero

### Mechanism 3
- **Claim:** Smaller K values act as regularization preventing extreme density concentration
- **Mechanism:** K controls the "sharpness" of the Bradley-Terry selection. Small K smooths the filtered distribution, preventing collapse to single modes
- **Core assumption:** K is finite and reward function is continuous
- **Evidence anchors:**
  - [section 2.1]: "small values of K act as a regularization which prevents the density from blowing up too much in high rewards areas"
  - [section 2.2.2]: Theorem 2.4 shows KL divergence bound depends on K
  - [corpus]: Weak - neighboring papers mention K but don't analyze its regularization effect
- **Break condition:** If K approaches infinity or reward function has discontinuous jumps

## Foundational Learning

- **Concept: Bradley-Terry discrete choice model**
  - Why needed here: Models user preference among K generated samples as probability proportional to exp(reward)
  - Quick check question: If three samples have rewards 1, 2, and 3, what are their selection probabilities under Bradley-Terry?

- **Concept: Kullback-Leibler divergence**
  - Why needed here: Measures how much the curated distribution deviates from real or initial data
  - Quick check question: If pt converges to p*, what happens to DKL(p*||pt) and DKL(pt||p*)?

- **Concept: Reinforcement Learning from Human Feedback (RLHF)**
  - Why needed here: Theoretical connection shows iterative curation is equivalent to RLHF with decreasing temperature
  - Quick check question: In the K→∞ limit, how does iterative curation relate to the RLHF objective with KL regularization?

## Architecture Onboarding

- **Component map:**
  Data generation module -> Curation interface -> Filtering pipeline -> Retraining engine -> Reward estimator

- **Critical path:**
  1. Generate K samples from current model
  2. Apply Bradley-Terry selection
  3. Add curated sample to filtered dataset
  4. Train new model on mixed dataset
  5. Measure reward and KL divergence

- **Design tradeoffs:**
  - K vs regularization: larger K increases reward maximization but risks collapse
  - Real data fraction λ vs stability: more real data stabilizes but may slow reward growth
  - Reward model accuracy vs curation quality: imperfect rewards lead to bias amplification

- **Failure signatures:**
  - KL divergence diverging from initial model
  - Reward variance collapsing to zero
  - Class imbalance emerging in image generation
  - FID score increasing dramatically

- **First 3 experiments:**
  1. Test Bradley-Terry selection on synthetic Gaussian mixtures with known reward function
  2. Verify reward maximization in 1D toy problem with bounded reward
  3. Check stability bounds with varying λ and K on simple mixture model

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical framework assumes idealized conditions including bounded reward functions and access to true reward models
- Connection between theoretical Bradley-Terry selection and actual human behavior remains largely unvalidated
- Experiments focus on synthetic datasets and simple image classification tasks, leaving questions about complex, high-dimensional domains

## Confidence

- **High Confidence:** Mathematical proofs of reward maximization under idealized Bradley-Terry curation
- **Medium Confidence:** Stability and reward augmentation claims for mixed real-curation data
- **Medium Confidence:** Regularization effect of K values

## Next Checks

1. **Human-in-the-loop validation:** Conduct user studies comparing Bradley-Terry theoretical predictions with actual human selection behavior on generated samples, measuring deviation from exponential reward model.

2. **Distribution coverage robustness:** Test stability theorems under realistic conditions where real data has limited coverage of reward space, measuring how KL divergence bounds degrade as real data becomes increasingly sparse in high-reward regions.

3. **Scaling behavior validation:** Evaluate bias amplification phenomenon on large-scale text generation tasks with diverse content types, measuring class imbalance emergence as a function of K and λ parameters across multiple orders of magnitude in model scale.