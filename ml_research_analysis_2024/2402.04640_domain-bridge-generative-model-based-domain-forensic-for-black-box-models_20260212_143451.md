---
ver: rpa2
title: 'Domain Bridge: Generative model-based domain forensic for black-box models'
arxiv_id: '2402.04640'
source_url: https://arxiv.org/abs/2402.04640
tags:
- images
- target
- description
- domain
- descriptions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Domain Bridge, a method to determine the data
  domain of black-box machine learning models, especially for finer-grained classes.
  It leverages a generative model (Stable Diffusion) and image embedding model (CLIP)
  to iteratively refine a textual description of the target class.
---

# Domain Bridge: Generative model-based domain forensic for black-box models

## Quick Facts
- arXiv ID: 2402.04640
- Source URL: https://arxiv.org/abs/2402.04640
- Authors: Jiyi Zhang; Han Fang; Ee-Chien Change
- Reference count: 7
- Key outcome: Domain Bridge outperforms traditional corpus-based approaches in identifying specific attributes of black-box models' input domains through iterative refinement using generative models

## Executive Summary
This paper introduces Domain Bridge, a novel method for determining the data domain of black-box machine learning models, particularly for finer-grained classes. The approach leverages generative models (Stable Diffusion) and image embedding models (CLIP) to iteratively refine textual descriptions of target classes. Starting with broad descriptions, the method generates images, uses the target model's classifications to guide refinement, and progressively narrows down to specific attributes. The method expands the search space beyond traditional corpora by leveraging the LAION-5B dataset on which Stable Diffusion is trained, enabling identification of specific attributes that corpus-based approaches might miss.

## Method Summary
Domain Bridge is an iterative search algorithm that determines the data domain of black-box ML models by combining generative models with semantic encoding capabilities. The method begins with an initial textual description, generates images using Stable Diffusion, classifies these images with the target model, and uses successful classifications to refine the description through CLIP-based encoding and GPT-4-powered summarization, grouping, and enrichment. The iterative process continues until convergence, balancing relevance (probability of correct classification) and generality (embedding diversity) through an objective function. The approach leverages LAION-5B dataset's diversity to expand search space beyond traditional corpora like ImageNet.

## Key Results
- Outperforms traditional corpus-based approaches in identifying specific attributes of target models' input domains
- Successfully identifies finer-grained classes in models trained on CIFAR-10 and ImageNet
- Demonstrates effectiveness in follow-up investigations through model cloning with generated images
- Shows capability in identifying specific attributes in fine-grained classification tasks like face attribute detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative refinement narrows down the exact class of interest by leveraging feedback from target model classifications
- Mechanism: The process begins with a broad description, generates images, and uses the target model's classifications to refine the description in subsequent iterations
- Core assumption: The target model's classifications provide meaningful feedback for refining the textual description
- Evidence anchors:
  - [abstract]: "Beginning with a coarse-grained description, the decoder generates a set of images, which are then presented to the unknown target model. Successful classifications by the model guide the encoder to refine the description..."
  - [section 4.1]: "For each leaf node p: Step 2: Generate m distinct images... Step 3: Classify x1, x2, ..., xm using the target model M... Step 7: Apply the Image Encoder E to extract textual descriptions..."
- Break condition: If the target model provides unreliable or noisy classifications, the iterative refinement process may converge to an incorrect or suboptimal description

### Mechanism 2
- Claim: Leveraging a large dataset (LAION-5B) expands the search space beyond traditional corpora, enabling identification of specific attributes
- Mechanism: The generative model Stable Diffusion, trained on LAION-5B, provides a broader and more diverse set of images for refinement compared to static datasets like ImageNet
- Core assumption: The LAION-5B dataset contains a sufficient variety of images to represent the target model's data domain
- Evidence anchors:
  - [abstract]: "A key strength of our approach lies in leveraging the expansive dataset, LAION-5B, on which the generative model Stable Diffusion is trained. This enlarges our search space beyond traditional corpora..."
  - [section 1]: "Instead of relying solely on static corpora, we use generative algorithms to help in our search. Specifically, we combine the power of the Stable Diffusion model... with the semantic encoding capabilities of CLIP... Together, this combination facilitates an iterative search process..."
- Break condition: If the LAION-5B dataset lacks sufficient diversity or representation of the target model's data domain, the method may fail to identify specific attributes accurately

### Mechanism 3
- Claim: The objective function balances relevance and generality to identify embeddings that represent the varied distribution of data points within the target class
- Mechanism: The objective function V(e) combines the probability of correct classification (relevance) and the expected cosine similarity between the original and re-encoded embeddings (generality)
- Core assumption: The balance between relevance and generality is crucial for identifying embeddings that capture the essence of the target class
- Evidence anchors:
  - [section 3.2]: "Relevance: This objective assesses the consistency of target model predictions... Generality: While an embedding with highly specific semantics might lead to high relevance, it may cause lack of variety in generated data samples..."
  - [section 3.2]: "The objective function can be formalized as: V(e) = Prs [arg max M(Dec(e; s)) = i] − λEs [cos(Enc(Dec(e, s)), e)]"
- Break condition: If the weighting parameter λ is not properly tuned, the objective function may prioritize either relevance or generality too heavily, leading to suboptimal embeddings

## Foundational Learning

- Concept: Image embeddings and text embeddings in CLIP
  - Why needed here: CLIP maps both images and texts to embeddings in a shared space, enabling the method to generate images from textual descriptions and extract textual descriptions from images
  - Quick check question: What is the purpose of the shared embedding space in CLIP?

- Concept: Generative models (Stable Diffusion)
  - Why needed here: Stable Diffusion generates images from CLIP text or image embeddings, providing the diverse set of images needed for iterative refinement
  - Quick check question: How does Stable Diffusion generate images from embeddings?

- Concept: Iterative search algorithms
  - Why needed here: The method employs an iterative search algorithm to refine the textual description based on feedback from the target model
  - Quick check question: What is the role of the iterative search algorithm in the method?

## Architecture Onboarding

- Component map: Description Decoder (G) -> Image Encoder (E) -> Description Summarizer (S) -> Description Grouper (Z) -> Description Enricher (R) -> Target Model (M)

- Critical path:
  1. Initialize with a broad description
  2. Generate images using the Description Decoder
  3. Classify images using the target model
  4. Refine the description based on successful classifications
  5. Repeat steps 2-4 until convergence

- Design tradeoffs:
  - Balancing relevance and generality in the objective function
  - Choosing the appropriate CLIP skip parameter for Stable Diffusion to control image specificity
  - Managing the trade-off between investigation effectiveness and computational efficiency

- Failure signatures:
  - If the target model provides unreliable classifications, the method may converge to an incorrect description
  - If the LAION-5B dataset lacks diversity, the method may fail to identify specific attributes accurately
  - If the objective function is not properly balanced, the method may prioritize either relevance or generality too heavily

- First 3 experiments:
  1. Investigate a target model pretrained on CIFAR-10 and compare performance with corpus-based approaches
  2. Perform a follow-up investigation using model cloning with images generated based on the initial investigation
  3. Investigate a target model specialized in fine-grained classifications (e.g., face attribute classifier) to assess the method's capability in identifying specific attributes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of CLIP skip parameter affect the diversity and specificity of generated images during the iterative refinement process?
- Basis in paper: [explicit] The paper mentions that Stable Diffusion relies on the CLIP text embedder to transform text descriptions into embeddings, and the CLIP skip setting influences the level of generality of the generated images. It also states that a higher CLIP skip value leads to an earlier halt in processing, resulting in more general images, while a lower value results in deeper processing, yielding more specific images.
- Why unresolved: The paper does not provide empirical evidence or a detailed analysis of how varying the CLIP skip parameter impacts the diversity and specificity of generated images at different stages of the iterative process.
- What evidence would resolve it: Empirical results showing the impact of different CLIP skip values on the diversity and specificity of generated images at various stages of the iterative refinement process, along with a discussion of the trade-offs involved.

### Open Question 2
- Question: How does the proposed method perform when investigating target models with classes that have subtle or nuanced differences?
- Basis in paper: [inferred] The paper mentions that the proposed method is effective in identifying specific attributes within broader domains, but it also notes that the method encounters limitations when dealing with fine-grained classifications, such as the "chubby" or "wearing necklace" attributes in the CelebA dataset.
- Why unresolved: The paper does not provide a comprehensive evaluation of the method's performance on target models with classes that have subtle or nuanced differences, nor does it discuss potential strategies for improving the method's ability to handle such cases.
- What evidence would resolve it: A detailed evaluation of the method's performance on target models with classes that have subtle or nuanced differences, along with an analysis of the challenges encountered and potential strategies for improvement.

### Open Question 3
- Question: How does the proposed method handle target models that are trained on multi-modal data domains, such as images and text?
- Basis in paper: [inferred] The paper focuses on investigating image-based target models, but it does not discuss how the proposed method would perform when dealing with target models that are trained on multi-modal data domains, such as images and text.
- Why unresolved: The paper does not provide any insights into how the proposed method would adapt to handle target models with multi-modal data domains, nor does it discuss potential challenges or limitations in such scenarios.
- What evidence would resolve it: An extension of the proposed method to handle target models with multi-modal data domains, along with empirical results demonstrating its effectiveness and a discussion of the challenges encountered and potential strategies for improvement.

## Limitations
- The method's effectiveness heavily depends on the target model's classification reliability, with no extensive validation across different types of target models
- Computational efficiency is not thoroughly evaluated, with actual costs for deep search trees and multiple iterations not quantified
- Hyperparameter sensitivity (particularly λ and CLIP skip) lacks systematic analysis and guidance for different use cases

## Confidence
- **Domain Identification Capability**: Medium Confidence - Demonstrated on CIFAR-10 and ImageNet but limited validation on diverse real-world models
- **Advantage Over Corpus-Based Methods**: Medium Confidence - Comparative results show improvement but evaluation framework may favor proposed method
- **Iterative Refinement Mechanism**: High Confidence - Well-grounded theoretical framework with clear explanation of core mechanism

## Next Checks
1. **Cross-Model Robustness Test**: Apply Domain Bridge to a diverse set of black-box models with known data domains (including models trained on different datasets, architectures, and fine-grained classification tasks) to evaluate generalizability beyond the demonstrated CIFAR-10 and ImageNet cases

2. **Noise Sensitivity Analysis**: Systematically test the method's performance under varying levels of classification noise or inconsistency from the target model to quantify its robustness and identify failure thresholds

3. **Hyperparameter Sensitivity Study**: Conduct a comprehensive grid search over the weighting parameter λ and CLIP skip parameter to map the performance landscape and provide evidence-based recommendations for hyperparameter selection across different scenarios