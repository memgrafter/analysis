---
ver: rpa2
title: Are We Ready for Out-of-Distribution Detection in Digital Pathology?
arxiv_id: '2407.13708'
source_url: https://arxiv.org/abs/2407.13708
tags:
- detection
- covariate
- mc-oodd
- s-oodd
- conf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study systematically evaluates out-of-distribution (OOD) detection
  methods for digital pathology (DP), comparing semantic and misclassified covariate
  OOD detection (S-OODD and MC-OODD) across multiple transfer learning (TL) strategies
  and architectures. Using public DP datasets, the authors benchmark diverse frequentist
  and ensemble-based detectors, including ViM, MSP, TU, and others, and assess TL
  from ImageNet versus DP-specific pre-training, and CNNs versus transformers.
---

# Are We Ready for Out-of-Distribution Detection in Digital Pathology?

## Quick Facts
- arXiv ID: 2407.13708
- Source URL: https://arxiv.org/abs/2407.13708
- Authors: Ji-Hun Oh; Kianoush Falahkheirkhah; Rohit Bhargava
- Reference count: 37
- Primary result: No single detector excels at both semantic and misclassification OOD detection in digital pathology

## Executive Summary
This paper systematically evaluates out-of-distribution detection methods for digital pathology applications, comparing semantic and misclassified covariate OOD detection across multiple transfer learning strategies and architectures. Using public pathology datasets, the authors benchmark diverse frequentist and ensemble-based detectors including ViM, MSP, TU, and others. The study reveals that pathology-specific pre-training generally improves performance over natural imagery, though results are unpredictable. Importantly, no single detector excels at both types of OOD detection, with ViM and TU leading in their respective tasks. The research demonstrates that advanced CNNs like ConvNeXt can match transformer robustness for OOD detection in this domain.

## Method Summary
The study evaluates OOD detection in digital pathology by training classifiers on BreakHis (breast carcinoma subtypes) and NCT-CRC (colorectal tissue types) datasets. Models are trained using focal loss, 224×224 crops, random flips/transpose, Adam optimizer (lr=1e-4), mini-batch size 32, and early stopping. Various pre-training strategies are compared including ImageNet-1K, self-supervised learning methods (MoCo v2, SwAV, BT), and large vision-language models (CLIP, BiomedCLIP, QuiltNet). OOD detection is evaluated using semantic OOD detection (S-OODD) with AUROC metric and misclassified covariate OOD (MC-OODD) with PRR metric, with corruptions from [36] for covariate generation. Both frequentist detectors (MSP, Maha, R+E, GrN, MLS, KLM, KNN, ViM, GEN, TU, EU) and ensemble methods are assessed.

## Key Results
- Pathology-specific transfer learning generally improves OOD detection performance over no pre-training or natural imagery pre-training, though results are unpredictable
- No single detector excels at both semantic OOD and misclassification OOD detection; ViM leads in S-OODD while TU and MSP excel in MC-OODD
- Advanced CNNs like ConvNeXt match transformer robustness for OOD detection, contrary to common assumptions about transformer superiority
- Ensemble methods provide marginal gains over strong single-model detectors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer learning from domain-specific (digital pathology) pre-trained models improves out-of-distribution detection performance more reliably than from natural imagery (ImageNet) or no pre-training.
- Mechanism: Pre-training on large-scale pathology data enables feature reuse that aligns better with the target pathology tasks, providing more informative feature representations that aid both semantic OOD detection (S-OODD) and misclassification detection (MC-OODD).
- Core assumption: The distribution of features learned from pathology data is closer to the target task distribution than features learned from natural imagery.
- Evidence anchors:
  - [abstract] "DP-based TL generally improves performance, though unpredictably"
  - [section] "DP-based TL helps, albeit unpredictably. We see compelling improvement over de-novo. However, further statements cannot be made... TL with natural imagery on average helps, however, further research is likewise needed (especially on when it fails) and should be implemented with caution. TL from DP is the safer option"
  - [corpus] Weak - the corpus papers focus on different domains and don't directly address pathology-specific pre-training benefits
- Break condition: If pathology-specific pre-training data does not capture the relevant feature distributions, or if the target pathology task is substantially different from the pre-training task.

### Mechanism 2
- Claim: No single out-of-distribution detector excels in both semantic OOD detection (S-OODD) and misclassification detection (MC-OODD), requiring careful detector selection based on task.
- Mechanism: Different OOD detection methods leverage different aspects of model outputs (feature space, logits, probabilities) and are therefore better suited for different types of OOD - semantic shifts versus covariate shifts that lead to misclassification.
- Core assumption: The characteristics that make a detector effective for semantic OOD are different from those needed for detecting misclassifications under covariate shift.
- Evidence anchors:
  - [abstract] "No single detector excels in both S-OODD and MC-OODD, with ViM and TU leading in their respective tasks"
  - [section] "No universal SOTA detector. The ranking among detectors in Tabs. 3-5 is volatile, sensitive to the dataset, TL, and architecture. In general though, ViM is superior in S-OODD followed by KNN, Maha, TU, and GEN. Conversely, TU and MSP excels in MC-OODD followed by KLM and ViM"
  - [corpus] Weak - corpus papers discuss OOD detection generally but don't specifically address the dual-task challenge in pathology
- Break condition: If future detectors are developed that can effectively leverage both semantic and misclassification detection signals simultaneously.

### Mechanism 3
- Claim: Advanced convolutional neural networks (specifically ConvNeXt) can match transformer robustness for OOD detection in digital pathology, contrary to the common assumption that transformers are superior.
- Mechanism: Modern CNN architectures with improved design (like ConvNeXt) have evolved to incorporate features that enhance robustness to distribution shifts, making them competitive with transformers for OOD detection tasks.
- Core assumption: The architectural improvements in modern CNNs address the same robustness challenges that transformers naturally handle better.
- Evidence anchors:
  - [abstract] "advanced CNNs like ConvNeXt match transformer robustness"
  - [section] "ConvNeXt is robust, as are transformers. Recent studies to date have been dedicated to explaining the superior robustness properties of transformers [1]. However, [24] showed that more advanced CNNs like ConvNeXt can behave just as robustly. Looking at Tab. 5, our findings agree with the latter study wherein ConvNeXt-S surpass ViT-B/16 and Swin-T in more number of metrics"
  - [corpus] Weak - corpus papers don't discuss the CNN vs transformer robustness comparison in pathology context
- Break condition: If the specific architectural improvements in ConvNeXt don't generalize well to the particular types of distribution shifts encountered in pathology data.

## Foundational Learning

- Concept: Out-of-Distribution (OOD) Detection
  - Why needed here: The entire study focuses on detecting when inputs fall outside the training distribution, which is critical for safe deployment in medical applications where incorrect predictions can be catastrophic
  - Quick check question: What are the two main types of OOD discussed in the paper, and how do they differ in terms of label space relationships?

- Concept: Transfer Learning (TL)
  - Why needed here: The paper extensively compares different pre-training strategies (ImageNet vs pathology-specific) and their impact on OOD detection performance, making understanding TL mechanisms essential
  - Quick check question: Why might pathology-specific pre-training be more beneficial than natural image pre-training for digital pathology tasks?

- Concept: Ensemble Methods for Uncertainty Quantification
  - Why needed here: The study evaluates Deep Ensembles as a baseline and compares it against single-model frequentist methods, requiring understanding of how ensembles provide uncertainty estimates
  - Quick check question: What is the key difference between epistemic uncertainty and aleatoric uncertainty, and why does this distinction matter for OOD detection?

## Architecture Onboarding

- Component map: Pre-trained model weights (ImageNet, CLIP, BiomedCLIP, QuiltNet, or custom pathology models) -> Classifier architecture (ResNet, ConvNeXt, ViT, Swin) -> OOD detection methods (frequentist like MSP, Maha, ViM, etc., or ensemble-based) -> Evaluation protocols (OSR splits for S-OODD, DP corruptions for MC-OODD) -> Metrics (AUROC for S-OODD, PRR for MC-OODD)
- Critical path: 1) Load pre-trained weights → 2) Fine-tune on target pathology dataset → 3) Generate OOD scenarios (held-out classes or corruptions) → 4) Apply OOD detection methods → 5) Compute evaluation metrics
- Design tradeoffs: Pathology-specific pre-training vs. general pre-training (tradeoff between domain alignment and model availability), CNN vs. transformer architectures (computational efficiency vs. potential robustness), single-model vs. ensemble methods (speed vs. performance), frequentist vs. uncertainty-based detection (simplicity vs. theoretical grounding)
- Failure signatures: Low AUROC values for S-OODD indicate poor semantic shift detection, negative or near-zero PRR values for MC-OODD indicate poor misclassification detection, and inconsistent performance across different TL or architecture choices suggests sensitivity to implementation details
- First 3 experiments:
  1. Run S-OODD evaluation with MSP detector on ResNet-50 with no pre-training to establish baseline performance
  2. Compare MSP detector performance across different pre-training strategies (ImageNet, MoCo-v2, SwAV, BT) to observe transfer learning effects
  3. Evaluate the same pre-training strategies with ViM detector for S-OODD to identify detector-specific patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of backbone architecture (CNN vs transformer) interact with different transfer learning strategies in OOD detection performance?
- Basis in paper: [explicit] The paper compares ResNet, ConvNeXt, ViT, and Swin architectures with different pre-training strategies, finding that ConvNeXt often outperforms transformers despite recent claims about transformer robustness.
- Why unresolved: The study shows architecture and pre-training effects are intertwined, but doesn't systematically isolate how specific architectural features interact with specific pre-training methods.
- What evidence would resolve it: Controlled experiments varying one architectural feature at a time while holding pre-training constant, or vice versa.

### Open Question 2
- Question: What is the optimal ensemble size and composition for OOD detection in digital pathology?
- Basis in paper: [explicit] The study uses 4-model ensembles but doesn't explore how ensemble size affects performance or whether heterogeneous ensembles outperform homogeneous ones.
- Why unresolved: While ensembles show some benefit, the paper doesn't investigate the scaling properties or optimal configurations for ensembles.
- What evidence would resolve it: Systematic experiments varying ensemble size and member diversity across different OOD detection tasks.

### Open Question 3
- Question: How do different OOD detection methods perform under domain shift severity variations in digital pathology?
- Basis in paper: [inferred] The study uses a fixed corruption severity level and pre-defined OOD categories, but doesn't examine how detection performance scales with domain shift intensity.
- Why unresolved: The paper doesn't explore the relationship between domain shift magnitude and detection reliability across methods.
- What evidence would resolve it: Experiments varying corruption severity and semantic OOD distance systematically while measuring detection performance.

## Limitations

- The study acknowledges that pathology-specific transfer learning improves OOD detection "though unpredictably," with no systematic explanation for when and why it succeeds or fails
- The comparison between CNNs and transformers is limited to specific architectures and may not generalize to other CNN or transformer variants
- The research focuses on semantic and covariate OOD detection, representing only a subset of possible OOD scenarios in clinical practice

## Confidence

- **High confidence:** The observation that no single detector excels at both S-OODD and MC-OODD, as this is directly supported by systematic benchmarking across multiple datasets and architectures.
- **Medium confidence:** The conclusion that pathology-specific transfer learning is generally safer than natural imagery pre-training, as results show improvement but with notable unpredictability and limited explanation for failure cases.
- **Low confidence:** The specific ranking of detectors and architectures, as the paper itself notes volatility in performance across different datasets, TL strategies, and evaluation metrics.

## Next Checks

1. Conduct ablation studies to identify which specific aspects of pathology pre-training contribute most to OOD detection performance, testing whether fine-tuning depth or architectural alignment matters more than pre-training domain.
2. Expand the CNN vs. transformer comparison to include additional architectures (e.g., ConvNeXt-T, ConvNeXt-XL, DeiT, PVT) and evaluate whether the observed robustness patterns hold across size variants and design philosophies.
3. Test the OOD detection methods on additional DP tasks and datasets, particularly those with different staining protocols, magnification levels, and tissue types to assess generalizability beyond breast and colorectal pathology.