---
ver: rpa2
title: Coarse-to-fine Alignment Makes Better Speech-image Retrieval
arxiv_id: '2408.13119'
source_url: https://arxiv.org/abs/2408.13119
tags:
- speech
- learning
- image
- speech-image
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of speech-image retrieval by proposing
  a novel framework that uses coarse-to-fine alignment through joint training of speech-image
  contrastive (SIC) and speech-image matching (SIM) learning tasks. The method incorporates
  an embedding queue to sample diverse negative representations during SIC learning
  and hard negatives for SIM tasks, as well as momentum distillation to improve learning
  under noisy supervision.
---

# Coarse-to-fine Alignment Makes Better Speech-image Retrieval

## Quick Facts
- arXiv ID: 2408.13119
- Source URL: https://arxiv.org/abs/2408.13119
- Authors: Lifeng Zhou; Yuke Li
- Reference count: 23
- One-line primary result: Over 4% improvement in R@1 on Flickr8k and SpokenCOCO datasets compared to state-of-the-art methods

## Executive Summary
This paper introduces a novel coarse-to-fine alignment framework for speech-image retrieval that significantly outperforms existing methods. The approach combines speech-image contrastive learning for coarse semantic alignment with speech-image matching learning for fine-grained cross-modal alignment, trained jointly. The method incorporates an embedding queue for efficient negative sampling and hard negative mining, along with momentum distillation to handle noisy supervision, achieving state-of-the-art performance on two major benchmark datasets.

## Method Summary
The proposed framework jointly trains speech-image contrastive (SIC) and speech-image matching (SIM) learning tasks to achieve coarse-to-fine alignment between speech and image representations. Speech features are extracted using HuBERT, while image features come from a frozen BLIP-2 encoder. The SIC task aligns representations at a coarse semantic level using contrastive loss with an embedding queue for negative sampling. The SIM task refines this alignment through a multimodal encoder that fuses speech and image embeddings, enhanced by hard negative mining based on contrastive similarities. Momentum distillation is incorporated to improve learning under noisy supervision by having the speech encoder learn from a temporal ensemble of itself.

## Key Results
- Achieves over 4% improvement in R@1 compared to state-of-the-art methods on both Flickr8k and SpokenCOCO datasets
- Demonstrates strong zero-shot generalization capabilities on cross-dataset experiments
- Shows consistent improvements across both speech-to-image and image-to-speech retrieval directions

## Why This Works (Mechanism)

### Mechanism 1
Joint training of speech-image contrastive (SIC) and speech-image matching (SIM) tasks enables coarse-to-fine alignment that improves retrieval accuracy. SIC learns high-level semantic alignment between speech and image embeddings, while SIM refines this alignment by capturing fine-grained cross-modal details through a multimodal encoder. Coarse alignment from SIC reduces the complexity for SIM, making fine-grained matching more effective.

### Mechanism 2
The embedding queue enables efficient sampling of diverse negative examples for SIC and hard negative examples for SIM, improving discriminative power without extra computational overhead. The queue stores a fixed number of image embeddings across batches, allowing each speech embedding to be compared against many negatives. For SIM, hard negatives are selected based on high contrastive similarity scores from SIC.

### Mechanism 3
Momentum distillation mitigates the impact of noisy supervision by having the speech encoder learn from a temporal ensemble of itself. A momentum model (moving average of the speech encoder parameters) generates soft pseudo-targets that are less noisy than hard labels, which the speech encoder is trained to match alongside standard contrastive loss.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: SIC task relies on learning representations where matching speech-image pairs are closer than non-matching pairs in embedding space
  - Quick check question: What is the role of the temperature parameter τ in the softmax similarity calculation?

- Concept: Multimodal fusion
  - Why needed here: SIM task requires the multimodal encoder to effectively fuse speech and image embeddings to capture fine-grained cross-modal alignment
  - Quick check question: How does the CLS token output from the multimodal encoder represent the joint speech-image pair?

- Concept: Hard negative mining
  - Why needed here: Selecting negatives that are semantically similar but not matching helps the model learn subtle distinctions between speech and images
  - Quick check question: Why are hard negatives more informative than random negatives in training?

## Architecture Onboarding

- Component map: Speech feature extractor (HuBERT) → Speech encoder → SIC loss (with queue and momentum distillation) → Multimodal encoder (with CLS output) → SIM loss (with hard negatives from queue) → Joint training
- Critical path: Speech and image embeddings are first processed by unimodal encoders, then SIC loss aligns them coarsely using the queue, followed by multimodal encoder interaction for SIM loss with hard negatives, all trained jointly
- Design tradeoffs: Using frozen pre-trained encoders (HuBERT, BLIP-2) speeds up training but limits adaptation; joint training increases complexity but improves alignment
- Failure signatures: Poor SIC loss indicates weak coarse alignment; poor SIM loss despite good SIC suggests multimodal encoder or hard negative mining issues
- First 3 experiments:
  1. Train with only SIC loss and queue, measure coarse alignment quality
  2. Add momentum distillation, check impact on noisy data robustness
  3. Enable SIM with hard negatives, evaluate fine-grained alignment improvement

## Open Questions the Paper Calls Out
The paper explicitly mentions plans to investigate the linguistic information learned by the network in future work, suggesting this is an open question they acknowledge. Additionally, the authors note that training on a larger corpus will further improve generalization capabilities, indicating this as an area for future exploration.

## Limitations
- Heavy reliance on frozen pre-trained encoders may limit adaptation to domain-specific characteristics
- Performance evaluation limited to two relatively clean benchmark datasets without extensive testing on noisy real-world scenarios
- Computational overhead of maintaining large embedding queues may pose practical limitations for scaling

## Confidence
**High Confidence**: The core claims about the effectiveness of joint training between SIC and SIM tasks are well-supported by the experimental results, showing consistent improvements across both datasets and retrieval directions.

**Medium Confidence**: The claims regarding the benefits of momentum distillation for handling noisy supervision are supported by the framework design but lack direct ablation evidence.

**Low Confidence**: The paper's claims about superior zero-shot generalization capabilities are based on limited experiments with only one additional dataset.

## Next Checks
1. Conduct ablation studies removing individual components (momentum distillation, embedding queue, hard negative mining) to quantify their specific contributions to performance gains
2. Evaluate the model's performance on datasets with varying levels of noise in speech captions or image annotations to validate momentum distillation's effectiveness in handling noisy supervision
3. Extend zero-shot experiments to include multiple diverse datasets with different domains, languages, and acoustic conditions to thoroughly assess generalization capabilities