---
ver: rpa2
title: Applicability of Large Language Models and Generative Models for Legal Case
  Judgement Summarization
arxiv_id: '2407.12848'
source_url: https://arxiv.org/abs/2407.12848
tags:
- summarization
- summaries
- legal
- summary
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically compares three families of models for
  legal case judgment summarization: extractive, domain-specific abstractive, and
  general-domain LLMs. We evaluate these models on Indian, UK Supreme Court, and government
  report datasets, analyzing both summary quality and consistency.'
---

# Applicability of Large Language Models and Generative Models for Legal Case Judgement Summarization

## Quick Facts
- arXiv ID: 2407.12848
- Source URL: https://arxiv.org/abs/2407.12848
- Reference count: 40
- One-line primary result: Abstractive models and LLMs outperform extractive methods for legal case summarization but generate more hallucinations requiring human-in-the-loop approaches

## Executive Summary
This paper systematically compares extractive, domain-specific abstractive, and general-domain LLM approaches for legal case judgment summarization across Indian, UK Supreme Court, and government report datasets. While abstractive models and LLMs generally achieve higher ROUGE scores than extractive methods, they also produce more hallucinations and inconsistencies. The authors explore various mitigation strategies including domain-specific fine-tuning, prompting techniques, and a novel semantic similarity-based approach to reduce hallucinations. Their findings suggest that while these models show promise, reliable legal case summarization still requires human oversight due to the high-stakes nature of legal documents.

## Method Summary
The paper evaluates five extractive summarization models (CaseSummarizer, BertSum, SummaRunner, PACSUM, HipoRank), two legal domain-specific abstractive models (LegPegasus, LegLED), and multiple LLM variants (ChatGPT, Davinci, Llama2-70b, GPT-4 Turbo) on three datasets: IN-Abs (7,130 Indian Supreme Court cases), UK-Abs (793 UK Supreme Court cases), and GOVREPORT (19,466 US government reports). Models are evaluated using ROUGE, METEOR, and BERTScore for summary quality, and SummaC, NEPrec, and NumPrec for consistency. Long documents are processed using chunking strategies, and a semantic similarity-based approach is developed to correct hallucinations by replacing hallucinated entities with contextually similar ones from source documents.

## Key Results
- Abstractive models and LLMs outperform extractive methods on ROUGE metrics across all datasets
- Domain-specific fine-tuning improves both quality (ROUGE/BERTScore) and consistency (SummaC, NEPrec, NumPrec) metrics
- Semantic similarity-based hallucination correction effectively reduces inconsistencies while improving summary quality in examples
- Different chunking strategies (1024 vs 2048 vs 8192 words) produce varying performance trade-offs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Legal domain-specific fine-tuning improves summary quality and reduces hallucinations
- Mechanism: Fine-tuning abstractive models on target domain datasets adapts them to domain-specific terminology and structure
- Core assumption: Legal case documents from different jurisdictions have distinct terminology requiring adaptation
- Evidence anchors: [abstract] mentions fine-tuning for hallucination reduction; [section] shows ROUGE/BERTScore improvements post-fine-tuning
- Break condition: Fine-tuning dataset too small or unrepresentative

### Mechanism 2
- Claim: Semantic similarity-based approach reduces hallucinations by replacing hallucinated entities with contextually similar source entities
- Mechanism: Identifies named entities/numbers not in source document and replaces them with most similar source entity based on cosine similarity
- Core assumption: Hallucinations often involve semantically similar but incorrect entities that can be corrected
- Evidence anchors: [abstract] mentions semantic similarity approach; [section] shows examples of corrected hallucinations
- Break condition: Hallucinated entity semantically distant from all source entities

### Mechanism 3
- Claim: Chunking long legal documents enables effective summarization by LLMs with token limits
- Mechanism: Documents divided into fixed-length chunks (1024-2048 words), each summarized independently, then concatenated
- Core assumption: Legal documents contain coherent topics within segments
- Evidence anchors: [abstract] mentions segmentation for long document summarization; [section] describes chunk-based approach
- Break condition: Chunk boundaries split critical legal reasoning or references

## Foundational Learning

- Concept: Token limits in LLMs and their impact on long document processing
  - Why needed here: Paper addresses challenge of LLMs having token limits insufficient for long legal documents
  - Quick check question: What are the token limits for different LLMs mentioned, and how do they affect summarization approach?

- Concept: Abstractive vs extractive summarization techniques
  - Why needed here: Paper compares three families of models (extractive, domain-specific abstractive, and general-domain LLMs)
  - Quick check question: How do extractive and abstractive methods differ, and why might abstractive be preferred for legal case summarization?

- Concept: Hallucination detection and measurement in generated text
  - Why needed here: Paper extensively discusses hallucinations and uses metrics like SummaC, NEPrec, and NumPrec
  - Quick check question: What metrics does the paper use to measure hallucinations and inconsistencies, and how do they work?

## Architecture Onboarding

- Component map: Data preprocessing → Document chunking → Summarization (per chunk) → Concatenation → Consistency checking → (Optional) Hallucination correction → Evaluation

- Critical path: Document → Chunking → Summarization (per chunk) → Concatenation → Consistency checking → (Optional) Hallucination correction → Evaluation

- Design tradeoffs:
  - Chunk size vs. context preservation: Smaller chunks reduce context but fit within token limits
  - Fine-tuning vs. zero-shot: Fine-tuning improves domain performance but requires labeled data
  - Prompt engineering vs. model adaptation: Better prompts can reduce hallucinations but may reduce summary quality

- Failure signatures:
  - Low consistency metrics (SummaC, NEPrec, NumPrec) indicating hallucinations
  - ROUGE scores dropping significantly for longer chunks
  - Legal terminology mismatch when using non-fine-tuned models
  - Chunk boundaries disrupting legal argument flow

- First 3 experiments:
  1. Run all summarization models on UK-Abs dataset with 1024-word chunks and compare ROUGE, METEOR, BERTScore metrics
  2. Apply semantic similarity correction to chatgpt-summ outputs and measure changes in consistency metrics
  3. Fine-tune LegPegasus on UK-Abs training data and compare performance against original model on test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do quality and consistency differ between domain-specific fine-tuning versus prompting-based hallucination reduction in LLMs?
- Basis in paper: [explicit] Paper compares both approaches showing fine-tuning improves both quality and consistency while prompting reduces inconsistencies but also reduces quality
- Why unresolved: Comparison limited to specific datasets and models; broader generalizability across different legal domains and LLM architectures unclear
- What evidence would resolve it: Systematic comparison across multiple legal domains, LLM architectures, and fine-tuning datasets

### Open Question 2
- Question: What segmentation strategies beyond simple chunking would best preserve coherence while maximizing information coverage?
- Basis in paper: [inferred] Paper notes chunking leads to redundancy and coherence issues, suggesting better segmentation methods are needed
- Why unresolved: Paper only uses basic token-based chunking and doesn't explore semantic or discourse-based segmentation approaches
- What evidence would resolve it: Comparative evaluation of different segmentation strategies on summary coherence and information coverage metrics

### Open Question 3
- Question: Can semantic similarity-based hallucination correction be improved to handle complex errors like statute name confusion or judge attribution errors?
- Basis in paper: [explicit] Paper shows semantic similarity helps but can't correct all hallucinations, particularly complex ones like statute name confusion
- Why unresolved: Method only handles simple entity substitution and doesn't address deeper semantic understanding issues
- What evidence would resolve it: Development and evaluation of methods that can distinguish between similar legal concepts and correctly attribute information

## Limitations

- Semantic similarity-based hallucination correction lacks comprehensive corpus-level validation and shows potential fragility with semantically distant entities
- Chunking strategy for long documents is only partially specified, with unclear impact on legal document coherence across different structures
- Evaluation relies heavily on automated metrics without extensive human evaluation, particularly for the novel semantic similarity correction method
- Generalization of findings to legal domains beyond Supreme Court judgments remains uncertain

## Confidence

**High Confidence:**
- Abstractive models and LLMs outperform extractive methods on ROUGE metrics across all tested datasets
- Domain-specific fine-tuning improves performance on target domain datasets for both ROUGE and consistency metrics
- LLMs generate summaries with higher ROUGE scores but also higher hallucination rates compared to extractive and domain-specific abstractive models

**Medium Confidence:**
- Semantic similarity-based approach effectively reduces hallucinations while improving summary quality
- Prompt engineering and model variants significantly impact summary quality and hallucination rates
- Different chunking strategies produce varying performance trade-offs

**Low Confidence:**
- Specific chunking strategy (semantic vs. fixed-length) and its impact on legal document coherence
- Semantic similarity method's robustness across diverse legal terminology and document structures
- Generalization of results to legal domains beyond Supreme Court judgments

## Next Checks

1. **Corpus-level validation of semantic similarity correction**: Apply the semantic similarity-based hallucination correction to all LLM-generated summaries on the UK-Abs and IN-Abs test sets, measuring reduction in SummaC scores and quantifying percentage of hallucinated entities successfully corrected across the entire corpus.

2. **Chunk boundary impact analysis**: Systematically test different chunking strategies (fixed-length 1024, 2048, 8192 words vs. semantic segmentation) on the GOVREPORT dataset, comparing not only ROUGE scores but also measuring summary coherence and completeness through human evaluation of chunk boundary effects on legal argument flow.

3. **Cross-jurisdiction generalization test**: Fine-tune LegPegasus and LegLED on UK-Abs training data and evaluate their performance on IN-Abs test data (and vice versa), measuring both ROUGE scores and consistency metrics to quantify how well domain-specific fine-tuning generalizes across different legal systems.