---
ver: rpa2
title: 'NOAH: Learning Pairwise Object Category Attentions for Image Classification'
arxiv_id: '2402.02377'
source_url: https://arxiv.org/abs/2402.02377
tags:
- noah
- image
- classification
- feature
- poca
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NOAH is a universal head structure for image classification that
  uses pairwise object category attention (POCA) to capture category-specific spatial
  features, addressing the limitations of global feature encoding in existing heads.
  It employs a two-level feature split, transform, and merge strategy to efficiently
  learn local-to-global POCAs.
---

# NOAH: Learning Pairwise Object Category Attentions for Image Classification

## Quick Facts
- arXiv ID: 2402.02377
- Source URL: https://arxiv.org/abs/2402.02377
- Authors: Chao Li; Aojun Zhou; Anbang Yao
- Reference count: 40
- Primary result: Improves classification accuracy across diverse DNN architectures with up to 5.3% gain on lightweight models like MobileNetV2 and DeiT-Tiny on ImageNet

## Executive Summary
NOAH introduces a universal head structure for image classification that learns pairwise object category attentions (POCAs) to overcome limitations of global feature encoding in existing heads. It employs a two-level feature split, transform, and merge strategy to efficiently learn local-to-global POCAs. The method demonstrates consistent accuracy improvements across diverse DNN architectures including CNNs, ViTs, and MLPs, with notable gains on lightweight models and multi-label datasets.

## Method Summary
NOAH is a head structure that replaces standard global average pooling with pairwise object category attention blocks. It processes backbone feature maps through a two-level split strategy: first dividing channels into N groups, then splitting each group into key/value subgroups. Each POCA block applies spatial softmax to generate attention maps, which are multiplied with value tensors and summed to produce category-specific features. The final classification is performed via softmax over these features. The method is designed to be efficient, maintaining similar model size and runtime speed while improving accuracy.

## Key Results
- Improves classification accuracy across diverse DNN architectures (CNNs, ViTs, MLPs)
- Achieves up to 5.3% accuracy gain on lightweight models like MobileNetV2 and DeiT-Tiny on ImageNet
- Enhances performance on multi-label datasets like MS-COCO (3.6% mAP gain) and fine-grained tasks like iNaturalist
- Maintains similar model size and runtime speed while providing accuracy improvements

## Why This Works (Mechanism)

### Mechanism 1
Learning spatial attention maps per object category (POCAs) is more effective than global average pooling for classification. Each POCA block produces a tensor of shape H×W×M where each spatial channel corresponds to attention for a specific category, allowing location-specific weighting before summing logits. Core assumption: Spatial regions contribute differently to different categories, so a single scalar weight per spatial location (as in GAP) is insufficient.

### Mechanism 2
Parallel POCA blocks with group-wise feature split enable efficient learning of diverse attention patterns. The first-level split divides C channels into N groups, each processed by a POCA block independently; second-level split further splits each group into key/value subgroups, producing complementary attention tensors merged by summation. Core assumption: Attention patterns learned in different channel groups are complementary and can be summed without interference.

### Mechanism 3
Using softmax along spatial dimension in POCA block enforces a valid probability distribution over spatial locations per category. The key embedding Wkn is followed by softmax over spatial dimensions, producing An with values summing to 1 across H×W for each category, which then multiplies value tensor Vn element-wise. Core assumption: Attention scores should sum to 1 over all spatial locations to represent a proper distribution for each category.

## Foundational Learning

- Concept: Dot-product attention and softmax normalization
  - Why needed here: POCA blocks use key/value embeddings and spatial softmax to generate attention maps; understanding this mechanism is essential to grasp how spatial weighting is learned.
  - Quick check question: What does the spatial softmax in POCA ensure about the attention tensor An?

- Concept: Group-wise feature processing and parameter efficiency
  - Why needed here: The two-level feature split (N groups, then r ratio) reduces parameters per POCA block while enabling parallel computation; critical for lightweight model gains.
  - Quick check question: How does increasing N affect the number of parameters and computational cost per block?

- Concept: Feature map reshaping for vision transformers
  - Why needed here: For ViTs, the class token is removed and feature maps are reshaped into square spatial dimensions before feeding to NOAH; essential for compatibility.
  - Quick check question: If a ViT output is (batch, tokens, channels) with tokens=197, what spatial shape does NOAH expect?

## Architecture Onboarding

- Component map: Backbone output → first-level channel split (N groups) → parallel POCA blocks → local-to-global merge (summation) → softmax classifier

- Critical path:
  1. Backbone produces feature map F (H×W×C)
  2. Split F into N groups of size H×W×(C/N)
  3. For each group Fn: split into Fkn (key) and Fvn (value), apply POCA formula to get Zn
  4. Sum all Zn along spatial dims → global POCA vector Z (M,)
  5. Softmax → classification logits

- Design tradeoffs:
  - N vs. model size: Larger N gives finer-grained attention but more memory/computation
  - r (key/value ratio): Smaller r reduces parameters but may lose information
  - 1x1 conv vs. larger kernels: Simpler, faster, but may limit spatial context

- Failure signatures:
  - Accuracy drop with large N: too many groups dilute information
  - Overfitting on small datasets: high N and low r may overfit
  - Memory blowup: check that N×(C/N) stays within GPU limits

- First 3 experiments:
  1. Baseline: Replace GAP head with NOAH (N=4, r=1/8) on ResNet50; compare top-1 accuracy and parameter count.
  2. Ablation: Test NOAH without second-level split; measure accuracy and parameter increase.
  3. Variant: Replace spatial softmax in POCA with sigmoid; observe accuracy change to validate design choice.

## Open Questions the Paper Calls Out

### Open Question 1
How does NOAH's performance scale with extremely large DNN architectures, particularly Vision Transformers and MLPs beyond those tested? The authors acknowledge they haven't explored NOAH's potential on "super-large DNN architectures (particularly ViTs and MLPs)" due to computational resource limitations.

### Open Question 2
Can NOAH be adapted for dense prediction tasks like object detection and semantic segmentation? The authors note that NOAH's limitation lies in its generalization to "dense downstream tasks" due to differences in training paradigms between image classification and dense tasks.

### Open Question 3
What is the optimal configuration of NOAH's hyperparameters (N and r) for different backbone architectures and datasets? The authors use an empirical principle to set N and r, stating "the smaller the backbone size the larger the N, and the larger the C/N the smaller the r," but don't perform exhaustive hyperparameter tuning for each model.

## Limitations

- Scalability to extremely large DNN architectures (particularly ViTs and MLPs) remains unexplored due to computational constraints
- Generalization to dense downstream tasks like object detection and semantic segmentation is unclear due to different training paradigms
- Optimal hyperparameter configuration (N and r) for different backbones and datasets is not systematically studied

## Confidence

- Universal effectiveness across architectures: High
- Efficiency (model size and speed): Medium
- Lightweight model gains (up to 5.3% on MobileNetV2): Medium
- Multi-label and fine-grained task improvements: Medium
- Two-level feature split strategy: High

## Next Checks

1. Implement NOAH on a lightweight backbone (e.g., MobileNetV2) and measure actual inference latency versus the standard GAP head to verify claimed speed efficiency.

2. Perform an ablation study testing POCA blocks without the second-level key/value split to quantify the contribution of this design choice to accuracy gains.

3. Test NOAH's performance on a dataset with predominantly single-object scenes to evaluate whether spatial attention per category provides diminishing returns in such scenarios.