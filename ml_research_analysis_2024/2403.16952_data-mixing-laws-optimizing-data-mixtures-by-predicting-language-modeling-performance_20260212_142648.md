---
ver: rpa2
title: 'Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling
  Performance'
arxiv_id: '2403.16952'
source_url: https://arxiv.org/abs/2403.16952
tags:
- data
- mixture
- training
- laws
- validation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper discovers quantitative predictability of language model
  performance given data mixture proportions, summarized as "data mixing laws." The
  authors propose a three-step pipeline combining scaling laws of training steps,
  model sizes, and their data mixing law to predict losses of large models trained
  on massive data using only small-scale experiments. Experiments show their method
  effectively optimizes a 1B model trained on RedPajama for 100B tokens, achieving
  performance comparable to 48% more training steps on default mixture.
---

# Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance

## Quick Facts
- arXiv ID: 2403.16952
- Source URL: https://arxiv.org/abs/2403.16952
- Reference count: 40
- Key outcome: A three-step pipeline combining scaling laws of training steps, model sizes, and data mixing laws predicts language model performance for different data mixtures, enabling optimization without large-scale experiments.

## Executive Summary
This paper discovers quantitative predictability of language model performance given data mixture proportions, summarized as "data mixing laws." The authors propose a three-step pipeline combining scaling laws of training steps, model sizes, and their data mixing law to predict losses of large models trained on massive data using only small-scale experiments. Experiments show their method effectively optimizes a 1B model trained on RedPajama for 100B tokens, achieving performance comparable to 48% more training steps on default mixture. They also demonstrate applications in continual pretraining and dynamic data schedules.

## Method Summary
The authors develop a three-step pipeline to optimize data mixtures for large language models. First, they conduct small-scale experiments with different mixture proportions using small models (70M-410M) and few training steps (30B tokens). Second, they fit scaling laws for training steps and model sizes to extrapolate performance to target model size (1B) and training steps (100B tokens). Third, they apply data mixing laws to predict validation loss on target mixture proportions and select optimal mixture. The data mixing law models domain losses as exponential functions of mixture proportions, enabling efficient optimization without running large-scale experiments.

## Key Results
- Data mixing laws accurately predict language model performance across different data mixtures using exponential functions
- The three-step pipeline effectively optimizes a 1B model on RedPajama, achieving performance comparable to 48% more training steps on default mixture
- Application to continual pretraining predicts critical mixture proportions that avoid catastrophic forgetting

## Why This Works (Mechanism)

### Mechanism 1
Model validation losses vary predictably with training data mixture proportions following an exponential law. The validation loss for each domain can be decomposed into a constant irreducible term plus an exponential term depending on all training domain proportions.

### Mechanism 2
Large-scale model performance on unseen mixtures can be predicted from small-scale experiments using nested scaling laws. The scaling laws of training steps and model sizes extrapolate accurately over orders of magnitude, allowing predictions without large-scale runs.

### Mechanism 3
Dynamic data schedules can be guided by data mixing laws applied to continual pretraining. Continual pretraining with different mixtures can be optimized by data mixing laws to avoid catastrophic forgetting while improving target domain performance.

## Foundational Learning

- Concept: Scaling laws in deep learning (Kaplan et al., 2020; Hoffmann et al., 2022)
  - Why needed here: The paper relies on scaling laws to predict large-scale performance from small-scale experiments without running them.
  - Quick check question: Can you explain why model loss scales as a power law with model size and dataset size?

- Concept: Catastrophic forgetting in continual learning (French, 1999; Kirkpatrick et al., 2017)
  - Why needed here: The paper applies data mixing laws to continual pretraining, where forgetting the original data is a key concern.
  - Quick check question: What causes catastrophic forgetting, and how might data mixing mitigate it?

- Concept: Multilayer perceptrons (MLPs) and universal approximation (Hornik et al., 1989)
  - Why needed here: The paper's data mixing laws resemble an MLP with exponential activations, enabling fitting of complex relationships.
  - Quick check question: Why are MLPs considered universal function approximators, and how does this relate to fitting data mixing laws?

## Architecture Onboarding

- Component map: Small-scale experiments -> Scaling law fitting -> Data mixing law fitting -> Performance prediction -> Mixture optimization

- Critical path:
  1. Run small-scale experiments on varied mixtures with small models and few training steps
  2. Fit scaling laws of training steps and model sizes on these experiments
  3. Use fitted scaling laws to predict losses for target model size and training steps across mixtures
  4. Fit data mixing laws to these predictions
  5. Optimize mixture proportions using the fitted mixing law

- Design tradeoffs:
  - More fitting samples improve prediction accuracy but increase cost; fewer samples risk poor fits
  - Fitting scaling laws separately vs jointly: separate fitting is more stable but may miss interactions
  - Implicit vs explicit domain aggregation in mixing laws: implicit is more general but less interpretable

- Failure signatures:
  - Large prediction errors on held-out mixtures indicate poor law fit
  - Breakdown of scaling law extrapolation at target scales invalidates predictions
  - If optimal mixtures differ drastically between small and large scales, the approach fails

- First 3 experiments:
  1. Train 70M model on 5 different mixtures of two domains for 30k steps, validate on each domain, fit exponential mixing law
  2. Train 70M, 160M, 305M models on 40 different mixtures for 30B tokens, fit scaling laws, predict 1B model performance on 100B tokens
  3. Train 70M model on mixtures of original pretraining data + target domain, fit mixing law, find critical proportion to avoid forgetting

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the data mixing law predictions scale with the number of training domains beyond 5? The paper discusses experiments with up to 5 training domains but doesn't empirically verify performance with a larger number of domains.

### Open Question 2
Can the data mixing laws be extended to handle dynamic changes in domain composition during training, rather than just static mixtures? The paper mentions the prospect of using data mixing laws for dynamic data schedules but does not provide concrete methods or experiments for this extension.

### Open Question 3
What is the theoretical basis for the exponential form of the data mixing law, and are there alternative functional forms that could yield similar or better performance? The paper proposes an exponential function form based on empirical findings but does not provide a theoretical justification for why this form is optimal.

## Limitations
- The data mixing laws rely on exponential relationships that may not generalize to all domain combinations or complex multi-domain scenarios
- Computational overhead for initial law fitting (5-8 hours on 8 A100 GPUs for a 1B model) may be prohibitive for rapid iteration
- Application to continual pretraining is based on limited empirical validation with uncertain robustness across different domain pairs

## Confidence

**High confidence** in the core empirical finding that domain losses vary predictably with mixture proportions for the tested domain pairs.

**Medium confidence** in the nested scaling law approach for predicting large-scale performance, as scaling laws are well-established but their combination with data mixing laws for mixture optimization is novel.

**Low confidence** in the broad applicability of data mixing laws to arbitrary domain combinations and continual pretraining scenarios, as the paper tests specific domain pairs and pretraining setups.

## Next Checks

1. **Stress test the exponential mixing law**: Systematically evaluate the data mixing law across diverse domain pairs (e.g., medical texts, legal documents, social media) to determine the boundaries of its applicability.

2. **Scale extrapolation validation**: Run small-scale experiments (70M-410M models) to predict performance of a 10B model trained for 1T tokens, comparing predictions against actual runs to quantify accuracy decay at larger scales.

3. **Continual pretraining robustness**: Design a systematic study of the critical mixture proportion prediction across different pretraining-to-finetuning domain pairs, measuring catastrophic forgetting rates and comparing predicted vs actual optimal mixtures.