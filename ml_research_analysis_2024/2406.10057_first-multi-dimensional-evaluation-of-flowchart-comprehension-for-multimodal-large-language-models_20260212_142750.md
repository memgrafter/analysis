---
ver: rpa2
title: First Multi-Dimensional Evaluation of Flowchart Comprehension for Multimodal
  Large Language Models
arxiv_id: '2406.10057'
source_url: https://arxiv.org/abs/2406.10057
tags:
- mllms
- flowchart
- flowce
- score
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces FlowCE, the first comprehensive benchmark
  for evaluating multimodal large language models (MLLMs) on flowchart comprehension
  across five dimensions: reasoning, information extraction, localization recognition,
  logical verification, and summarization. The benchmark comprises 500 real-world
  flowcharts with manually constructed open-ended question-answer pairs.'
---

# First Multi-Dimensional Evaluation of Flowchart Comprehension for Multimodal Large Language Models

## Quick Facts
- **arXiv ID**: 2406.10057
- **Source URL**: https://arxiv.org/abs/2406.10057
- **Reference count**: 40
- **Primary result**: FlowCE benchmark reveals MLLMs struggle with flowchart comprehension, with GPT-4o achieving only 56.63% overall accuracy

## Executive Summary
This paper introduces FlowCE, the first comprehensive benchmark for evaluating multimodal large language models (MLLMs) on flowchart comprehension across five dimensions: reasoning, information extraction, localization recognition, logical verification, and summarization. The benchmark comprises 500 real-world flowcharts with manually constructed open-ended question-answer pairs. Evaluation of 19 mainstream MLLMs, including GPT-4o and open-source models, reveals significant challenges, with GPT-4o achieving only 56.63% overall accuracy and the best open-source model (Phi-3-Vision) reaching 49.97%. Performance varies significantly across tasks, with summarization showing the highest scores while information extraction and reasoning remain particularly difficult.

## Method Summary
The researchers developed FlowCE, a benchmark comprising 500 real-world flowcharts with manually generated open-ended questions and answers. They evaluated 19 mainstream MLLMs including GPT-4o, open-source models like Phi-3-Vision, and various LLaVA variants. The evaluation used GPT-4 as an adjudicator for open-ended tasks (reasoning, localization recognition, summarization) and rule-based evaluation for logical verification and information extraction tasks. The benchmark assesses model performance across five dimensions: reasoning, information extraction, localization recognition, logical verification, and summarization.

## Key Results
- GPT-4o achieved only 56.63% overall accuracy on FlowCE, demonstrating significant challenges for state-of-the-art MLLMs
- Open-source models showed substantial performance gaps, with Phi-3-Vision achieving 49.97% as the best performer
- Summarization tasks yielded highest scores while information extraction and reasoning tasks proved most difficult
- Model performance varied significantly across dimensions, suggesting different architectural strengths and weaknesses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-dimensional task design forces models to integrate visual, logical, and linguistic reasoning in a single framework.
- Mechanism: The benchmark includes five distinct but related task types (reasoning, information extraction, localization, logical verification, summarization), each requiring different reasoning strategies. This compels models to simultaneously process image structure, textual content, spatial relationships, and logical flow.
- Core assumption: A model's performance across these diverse dimensions reveals its true capability to understand flowcharts, not just isolated skills.
- Evidence anchors:
  - [abstract] "comprehensive method, FlowCE, to assess MLLMs across various dimensions for tasks related to flowcharts"
  - [section 3.1] "tasks across five dimensions in real flowchart scenarios, including reasoning, information extraction, localization recognition, summarization, and logical verification"

### Mechanism 2
- Claim: Real-world flowchart diversity exposes models to out-of-distribution visual and semantic patterns.
- Mechanism: FlowCE uses 500 real-world flowcharts from diverse domains and resolutions, forcing models to handle variations in style, complexity, and vocabulary that synthetic datasets cannot provide.
- Core assumption: Exposure to varied real-world data improves generalization beyond templated or generated examples.
- Evidence anchors:
  - [section 3.2] "FlowCE is built upon 500 real-world flowcharts, ensuring an ample diversity in each chart"
  - [Figure 4] shows category and resolution diversity statistics

### Mechanism 3
- Claim: Human-annotated open-ended questions capture semantic richness that automatic generation misses.
- Mechanism: Unlike template-based or model-generated questions, human annotators create diverse, contextually rich questions that require nuanced understanding of flowchart semantics and intent.
- Core assumption: Open-ended questions reveal model limitations in comprehension that closed-ended formats might mask.
- Evidence anchors:
  - [section 3.2] "human annotator manually generated questions and answers for each major category"
  - [Table 1] comparison with FlowVQA shows FlowCE uses human-generated vs GPT-generated questions

## Foundational Learning

- Concept: Multi-modal integration (visual + textual reasoning)
  - Why needed here: Flowcharts combine visual structure with embedded text; models must fuse these modalities to answer questions accurately.
  - Quick check question: Can the model identify that "Start" and "End" nodes are connected in a flowchart without reading the text labels?

- Concept: Spatial reasoning and topological understanding
  - Why needed here: Many questions require understanding positional relationships (e.g., "node to the left of...") which depends on spatial layout interpretation.
  - Quick check question: Given a flowchart with two decision nodes, can the model determine which one is evaluated first based on arrow directions?

- Concept: Logical verification and conditional reasoning
  - Why needed here: Flowcharts encode decision logic; models must evaluate whether textual descriptions match the implied logical flow.
  - Quick check question: If a flowchart shows "If A then B else C", can the model verify the statement "If A is false, then B is executed"?

## Architecture Onboarding

- Component map: Visual encoder (CLIP/ViT variants) -> Visual features -> Fusion with text -> Task-specific reasoning -> Answer generation
- Critical path: Image → Visual features → Fusion with text → Task-specific reasoning → Answer generation
- Design tradeoffs:
  - Parameter size vs. performance: Larger models don't always outperform smaller ones (Phi-3-Vision beats some 13B+ models)
  - Pre-training data diversity vs. specialization: Models with chart-specific data (Phi-3-Vision) perform better on information extraction
  - Closed-source vs. open-source: Proprietary models show better overall performance but open-source models can match or exceed on specific tasks
- Failure signatures:
  - Hallucination in logical verification (generating answers not supported by flowchart)
  - Incorrect spatial reasoning (misidentifying node positions)
  - Incomplete information extraction (missing key textual elements)
  - Poor summarization (overly verbose or factually incorrect summaries)
- First 3 experiments:
  1. Evaluate model performance on flowcharts with varying complexity (node count, edge count) to identify scalability limits
  2. Test model robustness by adding noise or occlusions to flowchart images
  3. Compare human vs. GPT-4 evaluator agreement across different question types to validate scoring consistency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications or training strategies could improve MLLMs' performance on flowchart understanding tasks?
- Basis in paper: [inferred] The paper shows that even state-of-the-art models like GPT-4o struggle with flowchart comprehension, suggesting fundamental limitations in current architectures.
- Why unresolved: The paper identifies performance gaps but doesn't propose specific architectural solutions or training modifications to address them.
- What evidence would resolve it: Empirical results showing performance improvements on FlowCE benchmark after implementing specific architectural changes or novel training approaches.

### Open Question 2
- Question: How do different image resolution and quality affect MLLMs' ability to comprehend flowcharts?
- Basis in paper: [explicit] The paper notes FlowCE includes diverse resolution images but doesn't analyze how resolution impacts performance across different models.
- Why unresolved: The study uses varied resolutions but doesn't systematically investigate the relationship between image quality and comprehension accuracy.
- What evidence would resolve it: Controlled experiments varying image resolution/quality and measuring corresponding changes in model performance on FlowCE tasks.

### Open Question 3
- Question: What is the minimum number of flowchart examples needed to effectively train MLLMs for robust flowchart understanding?
- Basis in paper: [inferred] The paper uses 500 flowcharts but notes limitations in dataset size, suggesting optimal dataset size is unclear.
- Why unresolved: While FlowCE provides a comprehensive benchmark, it doesn't explore scaling effects or determine optimal training data requirements.
- What evidence would resolve it: Systematic studies varying training dataset sizes and measuring performance gains, identifying saturation points for model improvement.

## Limitations
- GPT-4 serves as both evaluator and reference model, potentially introducing bias in scoring
- The relatively small dataset of 500 flowcharts may not capture the full diversity of real-world scenarios
- Open-ended question format, while more natural, makes consistent evaluation more challenging
- No ablation studies on the impact of flowchart visual complexity versus logical complexity

## Confidence
- **High**: MLLMs face significant challenges with flowchart comprehension overall
- **Medium**: Specific performance patterns across task dimensions are reliably observed
- **Low**: Causal attributions for why certain models perform better on specific tasks

## Next Checks
1. **Evaluator Consistency**: Run blind human evaluations on a subset of 50 flowcharts to measure agreement with GPT-4 scores and quantify potential bias in the evaluation methodology.
2. **Visual Complexity Ablation**: Systematically vary visual complexity (node count, edge density) while holding logical structure constant to isolate whether performance degradation stems from visual or logical reasoning challenges.
3. **Cross-Dataset Generalization**: Test top-performing models on FlowVQA and FlowLearn datasets to assess whether performance differences are dataset-specific or reflect genuine capability gaps.