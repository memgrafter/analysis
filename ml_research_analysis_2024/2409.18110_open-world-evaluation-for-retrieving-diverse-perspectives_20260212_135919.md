---
ver: rpa2
title: Open-World Evaluation for Retrieving Diverse Perspectives
arxiv_id: '2409.18110'
source_url: https://arxiv.org/abs/2409.18110
tags:
- question
- perspectives
- perspective
- document
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of retrieving diverse perspectives
  for complex, contentious questions. The authors construct a new benchmark, BERDS,
  containing 3K questions with multiple perspectives sourced from surveys and debate
  platforms.
---

# Open-World Evaluation for Retrieving Diverse Perspectives

## Quick Facts
- arXiv ID: 2409.18110
- Source URL: https://arxiv.org/abs/2409.18110
- Reference count: 27
- Primary result: Current retrievers struggle to surface diverse perspectives, achieving only 40% coverage even with rich web corpora

## Executive Summary
This paper addresses the challenge of retrieving diverse perspectives for complex, contentious questions. The authors construct a new benchmark, BERDS, containing 3K questions with multiple perspectives sourced from surveys and debate platforms. They develop an LLM-based evaluator to automatically determine if retrieved documents contain given perspectives, enabling open-world evaluation across different corpora. Experiments with three retrievers and three corpora (Wikipedia, web snapshot, and search engine output) show that current retrievers struggle to surface diverse perspectives, with top systems achieving only 40% coverage of all perspectives. The authors propose query expansion and re-ranking methods to improve diversity, showing modest gains. Analysis reveals retrievers exhibit sycophancy, favoring documents that align with the input question's stance. The work highlights the difficulty of retrieving diverse perspectives and provides tools for future research in this area.

## Method Summary
The authors construct BERDS, a benchmark of 3K questions with multiple perspectives from surveys and debate platforms. They develop an LLM-based automatic evaluator (fine-tuned Mistral-7B) to determine if retrieved documents contain given perspectives. The evaluation framework uses MRECALL@k and Precision@k metrics to measure perspective coverage and relevance. Three retrievers (BM25, DPR, Contriever) are tested across three corpora (Wikipedia, Sphere web snapshot, Google Search API). The authors implement query expansion using LLM-generated perspectives and re-ranking methods to improve diversity. They also analyze retriever sycophancy by testing stance alignment preferences.

## Key Results
- Current retrievers surface relevant documents but fail to present diverse perspectives, with top systems achieving only 40% coverage
- Web corpora (Sphere, Google Search) do not automatically improve diversity despite containing more documents
- Query expansion improves PRECISION for 5 settings and boosts MRECALL for 6 settings
- Retrievers exhibit sycophancy, favoring documents that align with the input question's stance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrievers paired with web corpora (Sphere, Google Search) surface more diverse perspectives than Wikipedia alone
- Mechanism: Web corpora contain significantly more documents and passages (Sphere: 906M passages vs Wikipedia: 5.9M passages) providing broader coverage of subjective viewpoints
- Core assumption: Diversity of perspectives correlates with corpus size and variety of sources
- Evidence anchors:
  - [abstract] "Our experimental result suggests that current retrievers surface relevant documents but cannot present document sets with diverse perspectives, even when retrieving from a richer web corpus"
  - [section 4.2] "Sphere contains 906M passages" and "Google Search API... takes the top 100 Google Search results"
  - [corpus] Sphere and Google Search corpus statistics show substantially larger size than Wikipedia
- Break condition: If the web corpus contains predominantly uniform perspectives or if retrievers cannot effectively utilize the larger corpus

### Mechanism 2
- Claim: Query expansion using generated perspectives improves retrieval diversity
- Mechanism: Generating multiple perspectives with LLM creates diverse search queries that guide retrievers toward different viewpoints
- Core assumption: LLM-generated perspectives capture distinct aspects of the question that standard queries miss
- Evidence anchors:
  - [abstract] "We implement simple re-ranking (Carbonell and Goldstein, 1998) and query expansion approaches (Mao et al., 2021)"
  - [section 7] "Query expansion improves the PRECISION for five settings and boosts MRECALL for six settings"
  - [section 6] "We first generate multiple (n) perspectives on the given question using LLMs (gpt-4-0613), and query the retriever with each generated perspective"
- Break condition: If generated perspectives are too similar or if retriever cannot handle multiple diverse queries effectively

### Mechanism 3
- Claim: Retriever sycophancy causes preference for supporting perspectives
- Mechanism: Retrievers favor documents that align with the input question's stance, leading to biased retrieval
- Core assumption: Retrievers learn to match question sentiment rather than surface diverse viewpoints
- Evidence anchors:
  - [abstract] "Analysis reveals retrievers exhibit sycophancy, favoring documents that align with the input question's stance"
  - [section 8] "Querying the retrievers with supporting perspectives increases ∆ across the board, and retrieving with opposing perspectives decreases ∆"
  - [section 8] "Retrievers tend to favor perspectives that they are prompted with"
- Break condition: If retrievers are explicitly trained for diversity or if evaluation metrics penalize stance alignment

## Foundational Learning

- Concept: Dense passage retrieval (DPR)
  - Why needed here: Understanding how dense retrievers work is crucial for analyzing why they struggle with diversity
  - Quick check question: What is the key difference between dense and sparse retrieval methods?

- Concept: Multi-answer retrieval metrics (MRECALL)
  - Why needed here: The evaluation framework uses modified multi-answer metrics for perspective diversity
  - Quick check question: How does MRECALL @ k differ from standard recall metrics?

- Concept: Query expansion techniques
  - Why needed here: The paper implements query expansion to improve diversity, requiring understanding of the approach
  - Quick check question: What is the main benefit of using generated perspectives as expanded queries?

## Architecture Onboarding

- Component map:
  Input: Question + Corpus -> Retriever (BM25, DPR, Contriever, TART, NV-Embed-v2) -> Re-ranker (MMR-based) -> Query expander (LLM-generated perspectives) -> Evaluator (Mistral-7B fine-tuned model)

- Critical path:
  1. Retrieve top k documents using base retriever
  2. Apply re-ranking or query expansion if enabled
  3. Use perspective detection model to label retrieved documents
  4. Compute MRECALL and PRECISION metrics

- Design tradeoffs:
  - Wikipedia vs web corpora: smaller, cleaner vs larger, more diverse
  - Dense vs sparse retrievers: semantic matching vs exact term matching
  - Query expansion vs re-ranking: proactive diversity vs reactive diversity

- Failure signatures:
  - Low MRECALL indicates either corpus coverage issue or retriever limitation
  - High PRECISION but low MRECALL indicates retriever finds relevant docs but lacks diversity
  - Sycophancy detected when retriever consistently favors supporting perspectives

- First 3 experiments:
  1. Compare base retriever performance on Wikipedia vs Sphere corpus
  2. Test query expansion impact on CONTRIEVER + Sphere combination
  3. Measure sycophancy by comparing retrieval with supporting vs opposing query formulations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does increasing the number of perspectives per question beyond binary oppositions significantly improve the evaluation of retrieval diversity?
- Basis in paper: [inferred] The authors note that "The perspective set in our dataset is mostly binary (the average number of perspectives is 2.9 for Kialo and 2.0 for Arguana and OpinionQA). More fine-grained perspectives could be explored (e.g. reasons for supporting positive stance)."
- Why unresolved: The paper focuses primarily on binary perspectives due to their well-defined nature for contentious queries. The evaluation framework and metrics could be extended to handle multi-dimensional perspectives, but this requires further investigation.
- What evidence would resolve it: Constructing and evaluating a benchmark with multi-dimensional perspectives (e.g., not just for/against but various nuanced positions) and comparing retrieval performance across binary vs. multi-dimensional settings.

### Open Question 2
- Question: How does the choice of retriever affect the balance between precision and recall in perspective diversity tasks?
- Basis in paper: [explicit] The authors observe that "All retrievers favor the supporting perspectives significantly more often when they fail to retrieve both" and note retriever sycophancy, where retrievers tend to favor documents that share the perspective with the question.
- Why unresolved: While the paper identifies sycophancy, it doesn't systematically investigate how different retriever architectures or training objectives might mitigate this bias toward supporting perspectives.
- What evidence would resolve it: Controlled experiments comparing different retriever types (sparse vs. dense, contrastive vs. supervised) on their tendency to surface supporting vs. opposing perspectives, potentially with ablation studies on training data or loss functions.

### Open Question 3
- Question: What is the optimal trade-off between corpus size and quality for retrieving diverse perspectives?
- Basis in paper: [explicit] The authors compare three corpora (Wikipedia, Sphere, Google Search) and find that "Web corpora do not limit retrievers' performances in diversity," but note that using web search API has "the downside of using web search API (Nakano et al., 2021; Yoran et al., 2023) is the lack of reproducibility."
- Why unresolved: The paper establishes that larger web corpora provide better coverage but doesn't investigate the marginal benefit of corpus expansion versus the challenges of noise, credibility, and reproducibility.
- What evidence would resolve it: Systematic studies varying corpus size and composition (e.g., filtering for credibility, domain-specific corpora) while measuring both diversity metrics and practical concerns like reproducibility and computational efficiency.

## Limitations

- The automatic perspective detection evaluator may introduce labeling noise affecting metric reliability
- The evaluation framework assumes perspectives are discrete and mutually exclusive, potentially missing nuanced viewpoints
- The study focuses on English-language corpora and questions, limiting generalizability to other languages and cultural contexts
- The sycophancy analysis only examines direct stance alignment and may not capture subtle forms of bias

## Confidence

- High confidence: The core finding that retrievers struggle to surface diverse perspectives across all tested configurations is well-supported by consistent experimental results
- Medium confidence: The query expansion improvements, while statistically significant, show relatively modest gains (4-6% absolute improvements) that may not generalize to all question types
- Low confidence: The assertion that current retrievers are fundamentally limited in their ability to surface diverse perspectives may be premature, as the evaluation only tests three retriever architectures and relatively simple diversity-enhancing methods

## Next Checks

1. Cross-lingual validation: Test the retrieval and evaluation framework on non-English corpora to verify if diversity challenges persist across languages and cultural contexts

2. Human evaluation validation: Conduct human studies to validate the automatic evaluator's judgments, particularly for edge cases where perspectives may be nuanced or overlapping

3. Alternative retriever architectures: Test more sophisticated retriever architectures (e.g., ColBERTv2, SPARTA) and diversity-aware training objectives to determine if current limitations are architecture-specific or more fundamental