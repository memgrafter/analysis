---
ver: rpa2
title: 'MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions'
arxiv_id: '2410.02743'
source_url: https://arxiv.org/abs/2410.02743
tags:
- ma-ppo
- macro
- vanilla
- action
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the credit assignment problem in token-level\
  \ reinforcement learning from human feedback (RLHF), where delayed rewards make\
  \ it difficult for models to discern which actions contributed to preferred outcomes,\
  \ hindering learning efficiency and slowing convergence. The proposed solution,\
  \ MA-RLHF, incorporates macro actions\u2014sequences of tokens or higher-level language\
  \ constructs\u2014into the RLHF framework, operating at a higher level of abstraction\
  \ to reduce temporal distance between actions and rewards, facilitating faster and\
  \ more accurate credit assignment."
---

# MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions

## Quick Facts
- arXiv ID: 2410.02743
- Source URL: https://arxiv.org/abs/2410.02743
- Authors: Yekun Chai; Haoran Sun; Huang Fang; Shuohuan Wang; Yu Sun; Hua Wu
- Reference count: 40
- Primary result: MA-RLHF achieves 30% improvement in text summarization, 18% in dialogue, 8% in QA, and 2x faster convergence than vanilla RLHF

## Executive Summary
MA-RLHF addresses the credit assignment problem in token-level RLHF by incorporating macro actions—sequences of tokens or higher-level language constructs—into the learning process. This approach reduces the temporal distance between actions and delayed rewards, facilitating faster and more accurate credit assignment. The method achieves substantial performance improvements across multiple tasks while maintaining computational efficiency during training and inference. Notably, MA-RLHF reaches parity with vanilla RLHF 1.7 to 2 times faster and continues to outperform it with further training.

## Method Summary
MA-RLHF integrates macro actions into the standard RLHF pipeline by treating sequences of tokens as single actions during policy optimization. The framework uses Proximal Policy Optimization (PPO) with macro action termination strategies including fixed/randomized n-gram, parsing-based, and perplexity-based approaches. The method operates on pre-trained language models through supervised fine-tuning, reward modeling on human preferences, and policy optimization with macro actions. The key innovation is reducing the number of decision points by grouping tokens into meaningful sequences, which shortens the decision trajectory and brings rewards closer to the actions that caused them.

## Key Results
- 30% improvement in text summarization (TL;DR dataset) with 2x faster convergence
- 18% improvement in dialogue generation (HH-RLHF dataset)
- 8% improvement in question answering (WebGPT dataset)
- Strong scalability across model sizes from 2B to 27B parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Macro actions reduce the temporal distance between actions and rewards, improving credit assignment.
- Mechanism: By grouping sequences of tokens into macro actions, the number of decision points is reduced, shortening the decision trajectory and bringing rewards closer to the actions that caused them.
- Core assumption: The credit assignment problem in token-level RLHF is primarily caused by the long temporal distance between actions and delayed rewards.
- Evidence anchors:
  - [abstract] "delays rewards make it challenging for the model to discern which actions contributed to preferred outcomes"
  - [section 3.2.1] "macro actions allows an agent to operate at a coarser temporal scale"
- Break condition: If the reward signal itself is noisy or sparse, reducing temporal distance won't help credit assignment.

### Mechanism 2
- Claim: Macro actions provide better policy gradient estimates by reducing variance in the gradient.
- Mechanism: By treating sequences of tokens as single actions, the policy gradient is computed over fewer, longer actions rather than many individual tokens, reducing the variance in the gradient estimate.
- Core assumption: The variance in the policy gradient estimate is proportional to the number of actions in the sequence.
- Evidence anchors:
  - [section 4.5] "reduction in the number of actions, T /n, where n > 1, implies that the temporal distance between actions and corresponding rewards is decreased, thus reducing the variance in the gradient estimate"
  - [section 4.5] "a lower L2-norm of both the advantage and Q-values suggests more stable and less noisy policy updates"
- Break condition: If the macro actions are poorly designed or terminate at inappropriate points, the variance reduction may not occur.

### Mechanism 3
- Claim: Macro actions allow the model to capture high-level language constructs and co-occurrence patterns.
- Mechanism: By operating at a higher level of abstraction, macro actions can group tokens that form meaningful linguistic units (like phrases or clauses), which are better evaluated holistically rather than as individual tokens.
- Core assumption: Some language constructs are better understood when evaluated as a whole rather than as individual tokens.
- Evidence anchors:
  - [section 1] "standard RLHF methods may overlook essential local co-occurrence patterns or inherent structures between adjacent tokens in natural language"
  - [section 3.2.1] "macro actions — sequences of tokens or high-level language constructs — into the learning process"
- Break condition: If the macro action termination strategy doesn't align with natural linguistic boundaries, this benefit won't materialize.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and Semi-Markov Decision Processes (SMDPs)
  - Why needed here: The paper builds on the SMDP framework to extend MDPs with macro actions that can span multiple time steps.
  - Quick check question: What's the key difference between an MDP and an SMDP in terms of state transitions?

- Concept: Policy Gradient Methods and Proximal Policy Optimization (PPO)
  - Why needed here: The paper uses PPO as the optimization algorithm, adapted to work with macro actions instead of individual tokens.
  - Quick check question: How does the clipped objective in PPO help with training stability?

- Concept: Reinforcement Learning from Human Feedback (RLHF) pipeline
  - Why needed here: The paper integrates macro actions into the RLHF framework, which includes supervised fine-tuning, reward modeling, and policy optimization stages.
  - Quick check question: What's the purpose of the KL divergence penalty in the RLHF reward function?

## Architecture Onboarding

- Component map:
  - Base LLM (pre-trained language model)
  - SFT model (supervised fine-tuned model)
  - Reward model (trained on human preferences)
  - Policy model (optimized via RL with macro actions)
  - Macro action termination strategies (n-gram, parsing, perplexity-based)

- Critical path:
  1. Pre-trained LLM → SFT model
  2. SFT model + preference data → Reward model
  3. SFT model + Reward model → Policy model (via PPO with macro actions)
  4. Policy model generates responses evaluated by reward model

- Design tradeoffs:
  - Macro action length vs. granularity: Longer macro actions reduce credit assignment problems but may lose fine-grained control
  - Termination strategy complexity vs. performance: More sophisticated termination strategies may capture better linguistic structures but add computational overhead
  - Training stability vs. performance: KL penalty coefficient needs tuning to balance exploration and stability

- Failure signatures:
  - High variance in policy gradient estimates (indicates macro actions aren't helping with credit assignment)
  - Reward hacking (policy exploits reward model rather than learning true preferences)
  - Poor performance on out-of-distribution data (indicates overfitting to training distribution)

- First 3 experiments:
  1. Implement fixed n-gram termination with n=5 and compare RM scores against vanilla PPO
  2. Test different termination strategies (n-gram, parsing, perplexity) on the TL;DR dataset
  3. Evaluate the impact of macro action length on performance by varying n from 1 to ∞

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different macro action termination strategies (e.g., n-gram, parsing, perplexity-based) affect the performance of MA-RLHF across various tasks and model sizes?
- Basis in paper: Explicit - Section 4.3.1 discusses the performance of different termination strategies on the TL;DR dataset, but does not extensively explore their impact across different tasks and model sizes.
- Why unresolved: The paper only provides limited analysis of termination strategies on a single dataset and model size, leaving open the question of how these strategies generalize to other tasks and larger models.
- What evidence would resolve it: Experiments comparing the performance of different termination strategies on a diverse set of tasks (e.g., summarization, dialogue, question answering, code generation) and across various model sizes (e.g., 2B, 7B, 27B parameters) would provide evidence to answer this question.

### Open Question 2
- Question: How does the length of macro actions (|ωτ|) impact the credit assignment problem and learning efficiency in MA-RLHF?
- Basis in paper: Explicit - Section 3.2.3 and Figure 6 discuss the impact of varying n-gram lengths on performance, but do not provide a comprehensive analysis of how macro action length affects credit assignment and learning efficiency.
- Why unresolved: While the paper shows that different n-gram lengths lead to varying performance, it does not explicitly analyze the relationship between macro action length and the credit assignment problem or learning efficiency.
- What evidence would resolve it: Experiments measuring the credit assignment accuracy and learning efficiency (e.g., convergence speed, reward scores) for different macro action lengths (|ωτ|) across various tasks would provide evidence to answer this question.

### Open Question 3
- Question: How does MA-RLHF perform on more complex tasks involving step-by-step reasoning, such as mathematical problem-solving or complex logical reasoning?
- Basis in paper: Inferred - The paper mentions that exploring MA-RLHF in complex step-by-step reasoning tasks is a promising direction for future research (Section 5), but does not provide any experimental results on such tasks.
- Why unresolved: The paper focuses on evaluating MA-RLHF on relatively straightforward tasks (e.g., summarization, dialogue, question answering, code generation) and does not explore its performance on more complex reasoning tasks.
- What evidence would resolve it: Experiments evaluating MA-RLHF on tasks involving step-by-step reasoning, such as mathematical problem-solving or complex logical reasoning, and comparing its performance to vanilla RLHF would provide evidence to answer this question.

## Limitations

- Limited analysis of macro action length optimization across different tasks and model sizes
- Unclear generalizability to more open-ended generation tasks with different reward structures
- Parsing-based termination strategy may not scale well to non-structured generation tasks

## Confidence

**High Confidence**: The claim that MA-RLHF achieves faster convergence than vanilla RLHF is well-supported by the experimental results showing 1.7-2x speedup in reaching parity with baseline performance.

**Medium Confidence**: The claim that macro actions reduce credit assignment problems is supported by the theoretical framework and empirical results, but the evidence is somewhat indirect.

**Medium Confidence**: The claim that MA-RLHF scales effectively across model sizes (2B-27B parameters) is supported by results on three different model sizes, but the evaluation doesn't cover the full range of practical LLM sizes.

## Next Checks

1. **Ablation Study on Macro Action Length**: Systematically vary macro action lengths (n=1 to 20) on a single task like TL;DR summarization to determine the optimal length and understand how performance scales with macro action granularity.

2. **Cross-Domain Generalization Test**: Apply MA-RLHF to a fundamentally different task type such as creative writing or story continuation, where the reward structure and evaluation criteria differ substantially from the current tasks.

3. **Credit Assignment Quality Analysis**: Implement a diagnostic tool to measure actual credit assignment quality during training, such as tracking the correlation between action choices and subsequent reward changes. Compare this metric between MA-RLHF and vanilla RLHF to directly validate the proposed mechanism.