---
ver: rpa2
title: 'Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions Without
  the Question?'
arxiv_id: '2402.12483'
source_url: https://arxiv.org/abs/2402.12483
tags:
- question
- prompt
- choices
- llms
- choices-only
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work examines whether LLMs exploit dataset artifacts in multiple-choice
  question answering (MCQA) by testing choices-only prompts that omit the question.
  On three MCQA benchmarks (ARC, MMLU, HellaSwag) and four LLMs (LLaMA-2, Falcon,
  Phi-2, Mixtral), choices-only accuracy significantly surpasses majority baselines
  in 11/12 cases, with up to 0.33 gain.
---

# Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions Without the Question?

## Quick Facts
- **arXiv ID**: 2402.12483
- **Source URL**: https://arxiv.org/abs/2402.12483
- **Reference count**: 26
- **Primary result**: LLMs achieve choices-only accuracy significantly above majority baselines in 11/12 cases, suggesting reasoning beyond surface shortcuts.

## Executive Summary
This paper investigates whether large language models (LLMs) exploit dataset artifacts to answer multiple-choice questions (MCQs) when the question is omitted. The authors test choices-only prompts on three MCQA benchmarks (ARC, MMLU, HellaSwag) and four LLMs (LLaMA-2, Falcon, Phi-2, Mixtral). They find that choices-only accuracy significantly exceeds majority baselines in most cases, raising questions about the validity of standard MCQA evaluation. To explain this behavior, they test three hypotheses: memorization, choice dynamics, and abductive question inference (AQI). Results suggest LLMs use reasoning beyond simple shortcuts, though not fully explained by question inference alone.

## Method Summary
The study employs few-shot prompting with 5-25 examples to test LLMs on choices-only MCQs. Three MCQA datasets (ARC, MMLU, HellaSwag) and four LLMs (LLaMA-2, Falcon, Phi-2, Mixtral) are used. The authors design ablation studies with partial-input prompts: choices-only, memorization prompts (empty, gold, no choices), individual choice prompts, and AQI prompts. Accuracy is compared to full prompts and majority baselines using two-sample t-tests. Qualitative analysis of inferred questions is also performed to assess AQI's effectiveness.

## Key Results
- Choices-only accuracy significantly exceeds majority baselines in 11/12 LLM-dataset combinations (up to 0.33 gain).
- Memorization prompts do not outperform baselines, though exact contamination cannot be ruled out.
- Individual choice priors alone do not fully explain choices-only accuracy; group dynamics likely contribute.
- AQI matches choices-only accuracy in most cases, with 42% of inferred questions matching original meaning.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs use choice dynamics, reasoning over groups of choices, to answer MCQs without the question.
- Mechanism: When presented with choices-only, LLMs assess the interplay of all choices rather than relying solely on individual choice priors. This includes meta-strategies such as favoring odd choices if others are even, or reasoning about the collective meaning of the options.
- Core assumption: LLMs have learned from pretraining that choice groupings carry signal beyond single-choice priors.
- Evidence anchors:
  - "Second, priors over individual choices do not fully explain choices-only accuracy, hinting that LLMs use the group dynamics of choices."
  - "Second, LLMs may use the group dynamics of choices, where the models assess a choice based on the other choices, such as favoring an odd-numbered choice if the other choices are even."
- Break condition: If choices are randomly shuffled or semantically unrelated, and accuracy drops to chance, the mechanism breaks.

### Mechanism 2
- Claim: LLMs can abductively infer the original question from the choices, then answer it.
- Mechanism: Using the choices as evidence, LLMs generate a plausible question that those choices would answer. They then answer their own question, often matching the original question meaning (42% of the time in ARC).
- Core assumption: The choices were generated from a single coherent question, so they share enough semantic context for LLMs to reconstruct it.
- Evidence anchors:
  - "Third, LLMs have some ability to infer a relevant question from choices, and surprisingly can sometimes even match the original question."
  - "Out of the 43 cases when AQI picks the gold answer and the inferred question is answerable by one of the choices, 42% match the meaning of the original question."
- Break condition: If choices are too generic or unrelated, or if the LLM fails to generate an answerable question, accuracy drops.

### Mechanism 3
- Claim: Some of the choices-only accuracy stems from memorization of exact test examples.
- Mechanism: If the LLM was trained on the test set, it can recall the answer directly, even without the question.
- Core assumption: Test set contamination exists and is detectable via exact-choice-matching prompts.
- Evidence anchors:
  - "Our first hypothesis to explain choices-only accuracy is test set leakage, where the LLM is trained on the test set."
  - "While impossible to rule out memorization entirely, we believe that more complex strategies lead to high choices-only accuracy."
- Break condition: If prompts are altered to break exact matching (e.g., shuffled choices) and accuracy drops to baseline, memorization is unlikely the cause.

## Foundational Learning

- Concept: Partial-input models and artifacts
  - Why needed here: Understanding how omitting input parts can expose dataset shortcuts is key to interpreting choices-only results.
  - Quick check question: If a model scores high on choices-only MCQA, what does that imply about the dataset?

- Concept: Abductive reasoning
  - Why needed here: AQI is central to explaining how LLMs might reconstruct questions from choices.
  - Quick check question: What distinguishes abductive from deductive reasoning in the context of MCQA?

- Concept: Few-shot prompting and in-context learning
  - Why needed here: The experiments rely on few exemplars to guide LLM behavior without fine-tuning.
  - Quick check question: How does few-shot prompting differ from zero-shot prompting in terms of prior influence?

## Architecture Onboarding

- Component map: Dataset -> Prompt formatter -> LLM inference service -> Scoring engine -> Evaluation harness
- Critical path:
  1. Load dataset and select exemplars
  2. Build prompts for each experiment variant
  3. Run LLM inference per prompt
  4. Parse and score outputs
  5. Aggregate and compare accuracies
- Design tradeoffs:
  - Use 8-bit quantization for memory vs. full precision for accuracy
  - Few-shot vs. zero-shot: few-shot gives more control but may leak priors
  - Treat invalid outputs as random vs. wrong: affects fairness but not main claims
- Failure signatures:
  - Low accuracy on choices-only but high on full → artifacts likely
  - AQI accuracy much lower than choices-only → question inference not primary driver
  - Individual choice accuracy close to choices-only → group dynamics not used
- First 3 experiments:
  1. Run choices-only prompt vs. majority baseline on ARC
  2. Run memorization prompts (empty, gold, no choices) to check for exact recall
  3. Run individual choice prompts and aggregate with scoring function

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different decoding strategies (e.g., greedy, beam search, sampling) affect the accuracy of choices-only prompts and the effectiveness of abductive question inference?
- Basis in paper: [inferred] The paper mentions using default parameters and briefly discusses the effect of greedy decoding in Appendix B.2, but does not extensively explore different decoding strategies.
- Why unresolved: The paper only compares default parameters with greedy decoding in a limited set of experiments. It is unclear how other decoding strategies might impact the accuracy of choices-only prompts and the quality of inferred questions.
- What evidence would resolve it: Conduct a comprehensive study comparing the performance of choices-only prompts and abductive question inference across various decoding strategies (e.g., greedy, beam search, sampling with different temperature settings) on multiple datasets and LLM models.

### Open Question 2
- Question: To what extent do individual priors and group dynamics contribute to choices-only accuracy, and how can we design prompts that isolate and quantify their respective impacts?
- Basis in paper: [explicit] The paper investigates the impact of individual priors and group dynamics on choices-only accuracy in section 5, but acknowledges that the relationship is complex and not fully understood.
- Why unresolved: While the paper provides insights into the relative contributions of individual priors and group dynamics, it does not offer a definitive quantification of their respective impacts or a clear method for isolating their effects in future experiments.
- What evidence would resolve it: Develop a set of prompts that systematically manipulate the presence or absence of individual priors and group dynamics, and measure their isolated effects on choices-only accuracy across multiple datasets and LLM models.

### Open Question 3
- Question: How can we design MCQA datasets that are robust to artifact exploitation and provide a fair evaluation of LLM reasoning abilities?
- Basis in paper: [explicit] The paper discusses the potential for dataset artifacts to influence LLM performance and suggests the need for more robust dataset design protocols.
- Why unresolved: While the paper highlights the importance of artifact-robust dataset design, it does not provide specific guidelines or methods for creating such datasets. It is unclear how to balance the need for challenging questions with the need to minimize the potential for artifact exploitation.
- What evidence would resolve it: Conduct a systematic study of dataset design strategies that aim to minimize the potential for artifact exploitation while maintaining the difficulty and diversity of MCQA questions. Evaluate the effectiveness of these strategies using choices-only prompts and other artifact detection methods.

## Limitations
- The study cannot definitively rule out dataset artifacts and memorization as contributors to choices-only accuracy.
- The moderate agreement (κ=0.16-0.46) between AQI and choices-only accuracy suggests that question inference explains only part of the observed behavior.
- The qualitative analysis of inferred questions (42% matching original meaning) provides suggestive but not conclusive evidence for abductive reasoning.

## Confidence
- **High confidence**: The empirical finding that choices-only accuracy significantly exceeds majority baselines in 11/12 cases. This is directly observable and statistically tested.
- **Medium confidence**: The claim that memorization alone does not explain choices-only accuracy. While supported by ablation results, the paper cannot definitively rule out all forms of memorization or contamination.
- **Medium confidence**: The conclusion that LLMs use reasoning beyond simple surface shortcuts. This is supported by multiple mechanisms (choice dynamics, AQI) but their relative contributions remain unclear.

## Next Checks
1. **Dataset contamination audit**: Perform a systematic search for exact or near-exact matches between training corpora and test set choices across all three benchmarks to quantify potential memorization impact.
2. **Robustness to choice manipulation**: Test whether shuffling or semantically altering choices (while preserving distractor quality) reduces choices-only accuracy to baseline levels, helping isolate artifact-driven performance.
3. **Controlled ablation of choice dynamics**: Compare choices-only accuracy when presenting choices in original vs. randomized order to test whether group reasoning contributes to performance above individual choice priors.