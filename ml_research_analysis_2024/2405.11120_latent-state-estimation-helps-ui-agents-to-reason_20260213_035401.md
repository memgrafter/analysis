---
ver: rpa2
title: Latent State Estimation Helps UI Agents to Reason
arxiv_id: '2405.11120'
source_url: https://arxiv.org/abs/2405.11120
tags:
- gid00032
- gid00001
- screen
- state
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether large language models (LLMs) can
  estimate and reason about latent state when acting as autonomous UI agents. The
  authors formalize LLM prompting as a method for forming point estimates of latent
  state variables in textual space, then apply this to five aspects of UI and task
  state: previous actions, screen summaries, progression, mistakes, and task completion.'
---

# Latent State Estimation Helps UI Agents to Reason

## Quick Facts
- arXiv ID: 2405.11120
- Source URL: https://arxiv.org/abs/2405.11120
- Reference count: 40
- Key outcome: LLM-powered UI agents that estimate and reason about latent state complete up to 1.6x more tasks than those that do not

## Executive Summary
This paper investigates whether large language models (LLMs) can estimate and reason about latent state when acting as autonomous UI agents. The authors formalize LLM prompting as a method for forming point estimates of latent state variables in textual space, then apply this to five aspects of UI and task state. They evaluate three reasoning methods (zero-shot, CoT-SC, and ReAct) with and without latent state estimation across three online Android benchmarks totaling 135 tasks. LLMs achieved 76.8%-97.3% accuracy at inferring latent state aspects, matching or exceeding human performance.

## Method Summary
The method uses LLM prompting to form point estimates of latent state variables in textual space for UI automation tasks. Three reasoning methods (zero-shot, CoT-SC, and ReAct) are implemented with and without latent state estimates. The approach estimates five aspects of UI and task state: previous actions, screen summaries, progression, mistakes, and task completion. Five separate LLM calls are made to estimate each aspect, and these estimates are incorporated into prompts for the reasoning methods when selecting next actions.

## Key Results
- LLMs achieved 76.8%-97.3% accuracy at inferring latent state aspects
- Incorporating latent state estimates improved task success rates by up to 1.6x (28.1% to 45.9%)
- Improvements were particularly notable for stopping behavior, with premature stopping decreasing from 68.9% to 16.3% for zero-shot agents

## Why This Works (Mechanism)

### Mechanism 1
LLMs can form point estimates of latent state in textual space by leveraging world knowledge encoded in their weights. The LLM assigns high probabilities to completions that best explain noisy observations by reasoning across multiple observations and using prior knowledge of the environment.

### Mechanism 2
Incorporating latent state estimates into reasoning improves agent performance by shifting failure causes from action selection to grounding errors. By explicitly reasoning about what happened versus what was commanded, agents can better plan next actions and avoid compounding errors.

### Mechanism 3
Chain-of-thought and ReAct reasoning methods can incorporate latent state estimates through modified prompts while zero-shot methods require minimal modification. Latent state estimates are injected into prompts as additional context, allowing reasoning methods to build on this information when selecting actions.

## Foundational Learning

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: The problem of completing tasks by driving application UIs can be described as a POMDP where UI state and progress towards goals are hidden variables
  - Quick check question: What are the key differences between a fully observable MDP and a POMDP in terms of state representation?

- Concept: Latent variable inference
  - Why needed here: Multiple values of hidden variables might explain the noisy and partial observations, requiring reasoning across multiple observations to make reliable inferences
  - Quick check question: How does latent variable inference differ from supervised learning when dealing with noisy observations?

- Concept: Chain-of-thought reasoning
  - Why needed here: LLMs need to reason across multiple observations and use prior modeled knowledge of the environment to make reliable inferences about latent state
  - Quick check question: What are the key components of effective chain-of-thought prompting for complex reasoning tasks?

## Architecture Onboarding

- Component map: Screen observation → Latent state estimator (5 aspects) → Reasoning method (zero-shot/CoT-SC/ReAct) → Action selector → Grounder → Execution → Next screen observation
- Critical path: Screen observation → Latent state estimation → Action selection → Grounding → Execution → Next screen observation
- Design tradeoffs: Single LLM vs specialized models for different components; Real-time estimation vs pre-computed latent state estimates; Simple zero-shot vs few-shot CoT-SC/ReAct approaches
- Failure signatures: High action selection error rate indicates poor latent state estimation; High grounding error rate indicates poor element mapping despite good action selection; Premature stopping suggests incorrect task completion estimation
- First 3 experiments: 1) Compare zero-shot action selection with and without latent state estimates on a small benchmark; 2) Evaluate latent state estimation accuracy for each of the five aspects separately; 3) Test different reasoning methods (zero-shot, CoT-SC, ReAct) with latent state estimates on varied task types

## Open Questions the Paper Calls Out

### Open Question 1
How do LLMs perform at estimating latent state in domains other than UI automation, such as robotics or autonomous vehicles? The paper focuses on UI automation but mentions that the general approach could be broadly applicable to other POMDP-style domains.

### Open Question 2
How does the accuracy of LLM-based latent state estimation compare to that of specialized latent state estimation models trained on application-specific data? The paper uses pre-trained LLMs without additional task-specific fine-tuning, while common approaches use models specifically fit to application-specific training data.

### Open Question 3
How does the choice of reasoning method (zero-shot, CoT-SC, ReAct) impact the performance of UI agents that incorporate latent state estimates? While the paper shows that latent state estimation helps, it does not explore how the choice of reasoning method impacts the magnitude of this improvement.

## Limitations

- The approach relies on Android accessibility trees as screen representations, limiting applicability to other domains where such structured representations are unavailable
- The black-box nature of LLM-based latent state estimators makes it difficult to diagnose errors or improve the system systematically
- The computational overhead of making five separate LLM calls per action is not extensively explored, with potential latency and cost implications in production environments

## Confidence

**High Confidence**: LLMs can effectively estimate latent state aspects (previous actions, screen summaries, progression, mistakes, and task completion) - well-supported by empirical results showing 76.8%-97.3% accuracy.

**Medium Confidence**: Incorporating latent state estimates improves agent performance by up to 1.6x - supported by experimental results but exact magnitude may vary with task distribution.

**Low Confidence**: Improved action selection reveals weaknesses in grounding rather than masking them - speculative based on indirect evidence of failure mode distribution changes.

## Next Checks

1. **Cross-Domain Transferability Test**: Evaluate the same latent state estimation approach on web-based interfaces or desktop applications to determine if the methodology generalizes beyond Android environments.

2. **Error Analysis and Interpretability Study**: Conduct a detailed analysis of when and why latent state estimation fails, including visualization of the LLM's reasoning process and identification of common error patterns.

3. **Computational Overhead Measurement**: Measure the real-world latency and cost implications of making five separate LLM calls per action in a production environment, including both single-threaded and parallelized implementations.