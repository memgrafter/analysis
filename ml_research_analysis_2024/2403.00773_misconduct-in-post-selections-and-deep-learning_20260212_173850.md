---
ver: rpa2
title: Misconduct in Post-Selections and Deep Learning
arxiv_id: '2403.00773'
source_url: https://arxiv.org/abs/2403.00773
tags:
- cross-validation
- learning
- data
- error
- post-selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a theoretical analysis of the misconduct in
  post-selection methods, particularly in the context of deep learning. The author
  argues that the primary issue is the practice of selecting and reporting only the
  best-performing models from a large pool of trained networks, while hiding the performance
  of the less successful ones.
---

# Misconduct in Post-Selections and Deep Learning

## Quick Facts
- arXiv ID: 2403.00773
- Source URL: https://arxiv.org/abs/2403.00773
- Reference count: 40
- Primary result: Post-selection methods in deep learning are statistically invalid due to the practice of selecting only the best-performing models while hiding less successful ones.

## Executive Summary
This paper presents a theoretical analysis of misconduct in post-selection methods, particularly in deep learning. The author argues that the primary issue is the practice of selecting and reporting only the best-performing models from a large pool of trained networks, while hiding the performance of the less successful ones. This is referred to as "cheating and hiding." The paper introduces the concept of "general cross-validation" to address this issue, which involves reporting the average error of all trained networks, along with the five percentage positions of ranked errors. The author demonstrates that even traditional cross-validation methods are insufficient to exonerate post-selections in machine learning.

## Method Summary
The paper provides a theoretical framework for analyzing misconduct in post-selection methods in deep learning. It introduces the concept of "Lost Luck Theorem" to prove that the luckiest model on a validation set does not transfer its performance to a future test set. The author proposes "general cross-validation" as a solution, which involves reporting the average error of all trained networks and the five percentage positions of ranked errors. The method is supported by theoretical proofs and analysis of traditional and nested cross-validation methods.

## Key Results
- Post-selection methods are statistically invalid due to the practice of selecting only the best-performing models while hiding less successful ones.
- Traditional cross-validation on data splits is insufficient to exonerate post-selection methods.
- General cross-validation, which reports the average error of all trained networks along with the five percentage positions of ranked errors, is necessary to ensure the validity of machine learning results.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Post-selection is statistically invalid because the luckiest model on a validation set does not transfer its performance to a future test set.
- Mechanism: The paper introduces the "Lost Luck Theorem," which proves that the expected error of the luckiest model on a validation set is the same as any other model's expected error on a future test set. This is because the validation set is disjoint from the test set, and the luck observed on the validation set does not generalize.
- Core assumption: The validation set and the test set are disjoint and follow the same distribution.
- Evidence anchors:
  - [abstract] "This paper reveals that using cross-validation for data splits is insufficient to exonerate Post-Selections in machine learning."
  - [section] "The luckiest network on the validation set should give approximately the average error in a future test."
  - [corpus] Weak evidence. Corpus neighbors do not directly address the Lost Luck Theorem or the statistical invalidity of post-selection.
- Break condition: If the validation set and the test set are not disjoint or do not follow the same distribution, the Lost Luck Theorem may not hold.

### Mechanism 2
- Claim: Traditional cross-validation on data splits is insufficient to exonerate post-selection methods.
- Mechanism: The paper argues that even when using cross-validation for data splits, the luckiest model selected based on its performance on the validation set does not guarantee good performance on a future test set. This is because the cross-validation process still involves post-selection, which is statistically invalid.
- Core assumption: The cross-validation process still involves post-selection, which is statistically invalid.
- Evidence anchors:
  - [abstract] "This paper reveals that using cross-validation for data splits is insufficient to exonerate Post-Selections in machine learning."
  - [section] "Traditional cross-validation does not rescue Post-Selections from the misconduct."
  - [corpus] Weak evidence. Corpus neighbors do not directly address the insufficiency of traditional cross-validation to exonerate post-selection.
- Break condition: If the cross-validation process does not involve post-selection, the argument may not hold.

### Mechanism 3
- Claim: General cross-validation, which reports the average error of all trained networks along with the five percentage positions of ranked errors, is necessary to ensure the validity of machine learning results.
- Mechanism: The paper argues that reporting only the luckiest model's performance is misleading because it does not represent the expected performance on a future test set. Instead, reporting the average error and the distribution of errors provides a more accurate picture of the model's performance.
- Core assumption: The average error and the distribution of errors are more representative of the model's expected performance on a future test set than the luckiest model's performance.
- Evidence anchors:
  - [abstract] "authors must report at least the average error of all trained networks, good and bad, on the validation set."
  - [section] "Better, report also five percentage positions of ranked errors."
  - [corpus] Weak evidence. Corpus neighbors do not directly address the necessity of general cross-validation.
- Break condition: If the average error and the distribution of errors do not accurately represent the model's expected performance on a future test set, the argument may not hold.

## Foundational Learning

- Concept: Statistical inference and hypothesis testing
  - Why needed here: The paper discusses the statistical validity of post-selection methods and the importance of reporting the average error and the distribution of errors. Understanding statistical inference and hypothesis testing is crucial for evaluating the validity of these methods.
  - Quick check question: What is the difference between a Type I error and a Type II error in hypothesis testing?

- Concept: Cross-validation and its limitations
  - Why needed here: The paper discusses the limitations of traditional cross-validation in exonerating post-selection methods. Understanding cross-validation and its limitations is essential for evaluating the paper's arguments.
  - Quick check question: What is the difference between k-fold cross-validation and leave-one-out cross-validation?

- Concept: Deep learning and its applications
  - Why needed here: The paper discusses the misconduct in deep learning, specifically the practice of post-selection. Understanding deep learning and its applications is crucial for evaluating the paper's arguments and their implications for the field.
  - Quick check question: What is the difference between a convolutional neural network (CNN) and a recurrent neural network (RNN)?

## Architecture Onboarding

- Component map:
  Data preprocessing and splitting -> Model training and validation -> Post-selection and reporting -> General cross-validation and error reporting

- Critical path:
  Split the data into training, validation, and test sets -> Train multiple models on the training set -> Evaluate the models on the validation set -> Select the luckiest model based on its performance on the validation set -> Report the average error and the distribution of errors for all models

- Design tradeoffs:
  Computational cost vs. statistical validity: Training multiple models and reporting the average error and the distribution of errors is computationally expensive but ensures statistical validity.
  Model complexity vs. interpretability: More complex models may achieve better performance but are harder to interpret and validate.

- Failure signatures:
  Overfitting: If the luckiest model performs significantly better on the validation set than on the test set, it may be overfitting.
  Underreporting: If only the luckiest model's performance is reported, it may be misleading and not representative of the model's expected performance on a future test set.

- First 3 experiments:
  1. Train a simple model (e.g., linear regression) on a small dataset and evaluate its performance using traditional cross-validation. Compare the results with the average error and the distribution of errors.
  2. Train a complex model (e.g., deep neural network) on a large dataset and evaluate its performance using general cross-validation. Compare the results with the luckiest model's performance.
  3. Simulate a scenario where the validation set and the test set have different distributions. Evaluate the performance of the luckiest model and the average error and the distribution of errors. Compare the results and discuss the implications for post-selection methods.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the statistical invalidity of post-selection methods affect the generalizability of deep learning models in real-world applications?
- Basis in paper: [explicit] The paper discusses the misconduct of post-selection methods and argues that they are statistically invalid.
- Why unresolved: The paper provides theoretical analysis but does not offer empirical evidence on how this affects real-world applications.
- What evidence would resolve it: Empirical studies comparing the performance of models trained with and without post-selection methods on real-world datasets.

### Open Question 2
- Question: What are the implications of the general cross-validation method for the development of new deep learning architectures?
- Basis in paper: [explicit] The paper introduces the concept of general cross-validation as a solution to the misconduct of post-selection methods.
- Why unresolved: The paper does not explore how this method might influence the design of new architectures.
- What evidence would resolve it: Case studies of deep learning models developed using general cross-validation and their performance compared to traditional models.

### Open Question 3
- Question: How can the concept of general cross-validation be extended to other areas of machine learning beyond deep learning?
- Basis in paper: [explicit] The paper discusses the application of general cross-validation to deep learning but suggests it might have broader implications.
- Why unresolved: The paper does not provide examples or theoretical extensions to other machine learning areas.
- What evidence would resolve it: Research papers or case studies demonstrating the application of general cross-validation to other machine learning methods and their outcomes.

## Limitations
- The paper lacks empirical validation of the proposed general cross-validation method, relying heavily on theoretical proofs without supporting evidence.
- The generalizability of the proposed method to different deep learning architectures and datasets is uncertain and requires further investigation.
- The computational cost of implementing the general cross-validation method is not discussed, which could be a significant barrier to its adoption in practice.

## Confidence
- High confidence: The theoretical framework of post-selection misconduct and the Lost Luck Theorem are well-established and supported by sound mathematical proofs.
- Medium confidence: The proposed general cross-validation method is a valid solution to address post-selection misconduct, but its effectiveness in practice needs further validation.
- Low confidence: The generalizability of the proposed method to different deep learning architectures and datasets, as well as the computational cost of implementing it, are uncertain and require further investigation.

## Next Checks
1. Conduct empirical experiments to validate the effectiveness of the general cross-validation method on various deep learning architectures and datasets, comparing its performance with traditional cross-validation and post-selection methods.
2. Investigate the computational cost of implementing the general cross-validation method and develop strategies to balance this cost with the need for statistical validity, such as using parallel computing or approximate methods.
3. Explore the applicability of the proposed method to other domains, such as natural language processing or computer vision, and assess its effectiveness in addressing post-selection misconduct in these areas.