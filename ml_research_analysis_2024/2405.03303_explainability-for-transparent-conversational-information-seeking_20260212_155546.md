---
ver: rpa2
title: Explainability for Transparent Conversational Information-Seeking
arxiv_id: '2405.03303'
source_url: https://arxiv.org/abs/2405.03303
tags:
- response
- explanations
- user
- system
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how to make conversational information-seeking
  systems more transparent by providing explanations about sources, system confidence,
  and limitations. The authors conducted a crowdsourcing study with 160 participants
  evaluating responses with different types of explanations (source, confidence, limitations)
  presented in either textual or visual format.
---

# Explainability for Transparent Conversational Information-Seeking

## Quick Facts
- **arXiv ID**: 2405.03303
- **Source URL**: https://arxiv.org/abs/2405.03303
- **Reference count**: 40
- **Key outcome**: Noisy explanations significantly reduce user-perceived usefulness of conversational responses, while presentation format (textual vs visual) has minimal impact.

## Executive Summary
This paper investigates how to make conversational information-seeking systems more transparent through explanations about sources, system confidence, and limitations. The authors conducted a crowdsourcing study with 160 participants evaluating responses with different types of explanations presented in either textual or visual format. They found that noisy explanations led to lower user ratings for response usefulness, but users were unable to detect factual errors or biases in responses. The study highlights the importance of explanation quality over presentation format in building trustworthy conversational systems.

## Method Summary
The study used 10 queries from TREC CAsT 2020/2022 datasets with manually generated responses (perfect and imperfect variants) and explanations (accurate and noisy variants). Participants evaluated these responses through a crowdsourcing study on Amazon Mechanical Turk, with 16 workers per HIT across 10 experimental conditions. The researchers collected data on response usefulness and user ratings for explanations, analyzing the impact of explanation quality and presentation mode (textual vs visual) on user perception.

## Key Results
- Noisy explanations significantly reduce user-perceived usefulness of conversational responses
- Users cannot reliably detect factual errors or biases in conversational responses
- Presentation format (textual vs visual) has minimal impact on response usefulness scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Noisy explanations significantly reduce user-perceived usefulness of conversational responses.
- Mechanism: Users compare explanation quality against their own judgment of response reliability. When explanations contain errors or inconsistencies, this mismatch creates distrust and lowers perceived usefulness.
- Core assumption: Users actively evaluate the coherence between provided explanations and their own assessment of response quality.
- Evidence anchors:
  - [abstract]: "The analysis of the collected data reveals lower user ratings for noisy explanations, although these scores seem insensitive to the quality of the response."
  - [section]: "Looking at the means of user ratings for source and confidence explanations (top-right plot in Figure 4), ratings are again skewed towards higher values, and scores for accurate explanations are slightly higher than for noisy explanations, especially for confidence."
  - [corpus]: Weak evidence - corpus neighbors discuss transparency and explainability but don't specifically address the relationship between explanation noise and user perception.

### Mechanism 2
- Claim: Users cannot reliably detect factual errors or biases in conversational responses without external knowledge.
- Mechanism: Users lack the domain expertise to verify factual claims or identify viewpoint biases in responses, especially when presented in natural language format.
- Core assumption: Users evaluate responses based on surface-level coherence rather than factual accuracy or viewpoint completeness.
- Evidence anchors:
  - [abstract]: "users were unable to detect factual errors or biases in responses."
  - [section]: "Our results show that high-quality explanations related to the source, system confidence, and response limitations increase the user-perceived usefulness of the response and user ratings for explanations. Additionally, noise in the explanations of the response provided by the system has a significant impact on user experience in general (almost all response dimensions are affected)."
  - [corpus]: Weak evidence - corpus neighbors mention detecting biases but don't specifically address user inability to detect errors in conversational responses.

### Mechanism 3
- Claim: Explanation presentation format (textual vs visual) has minimal impact on user perception of response usefulness.
- Mechanism: Users focus on content quality over presentation style when evaluating conversational responses, suggesting that format optimization is less critical than explanation accuracy.
- Core assumption: Users prioritize semantic content over visual design when assessing conversational AI responses.
- Evidence anchors:
  - [abstract]: "The presentation format (textual vs visual) did not significantly impact usefulness scores, though users showed slight preferences for textual explanations of limitations and visual explanations of confidence."
  - [section]: "On average, we do not observe differences in the usefulness scores between textual and visual modes, but usefulness scores are significantly higher when no explanations are provided."
  - [corpus]: Weak evidence - corpus neighbors discuss explanation formats but don't specifically address the relative importance of textual vs visual presentation.

## Foundational Learning

- Concept: Between-subject experimental design
  - Why needed here: Prevents repeated judgments from the same user, reducing bias and increasing reliability of results across different experimental conditions.
  - Quick check question: Why did the study use between-subject design instead of within-subject design?

- Concept: Power analysis for sample size determination
  - Why needed here: Ensures sufficient statistical power to detect meaningful effects of explanation quality and presentation mode on user perception.
  - Quick check question: How did the researchers determine that 16 workers per HIT were sufficient for their study?

- Concept: Likert scale measurement and interpretation
  - Why needed here: Provides standardized way to measure user perception across multiple response dimensions while accounting for ordinal nature of the data.
  - Quick check question: Why did the researchers use four-point Likert scales instead of five-point scales?

## Architecture Onboarding

- Component map:
  Query selection and preprocessing → Response generation (perfect vs imperfect variants) → Explanation generation (source, confidence, limitations - accurate vs noisy) → Presentation formatting (textual vs visual modes) → User interface for evaluation → Data collection and validation (attentiveness checks) → Statistical analysis pipeline

- Critical path:
  Query → Response generation → Explanation generation → Presentation formatting → User evaluation → Data validation → Statistical analysis

- Design tradeoffs:
  - Response quality vs explanation quality: Perfect responses don't need limitations explanations, creating unbalanced experimental conditions
  - Explanation complexity vs user comprehension: More detailed explanations may overwhelm users but provide better assessment capability
  - Presentation mode choice: Visual formats may improve confidence understanding but increase interface complexity

- Failure signatures:
  - Low attentiveness check pass rates indicating poor task design or unclear instructions
  - No significant effects found when theory predicts effects (e.g., presentation mode should matter)
  - Contradictory results between different analysis approaches

- First 3 experiments:
  1. Test explanation presentation with single response type (perfect responses only) to establish baseline format effects
  2. Compare user detection rates for different types of errors (factual vs bias vs incomplete)
  3. A/B test different confidence display formats (percentage vs bar chart vs textual) with user preference survey

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal presentation format for explanations vary based on user expertise level in the topic domain?
- Basis in paper: [explicit] The authors note that "the preferred presentation mode depends on the explained aspect of the response" and that "further research is needed to better understand how to optimally integrate different aspects in the layout of transparent CIS responses"
- Why unresolved: The study found slight preferences for textual explanations of limitations and visual explanations of confidence, but these were not statistically significant and the study did not explore different expertise levels
- What evidence would resolve it: A follow-up study with participants of varying expertise levels systematically testing different explanation formats and measuring their effectiveness at improving response assessment accuracy

### Open Question 2
- Question: What is the relationship between the cognitive load of processing explanations and their perceived usefulness in conversational search systems?
- Basis in paper: [inferred] The authors discuss "the underlying trade-off between effort and gain" and note that "explanations require additional time and effort from the user" which may outweigh their benefits
- Why unresolved: While the study found that explanations with noise reduced usefulness scores, it did not directly measure cognitive load or investigate the optimal balance between explanation detail and user effort
- What evidence would resolve it: Empirical studies measuring cognitive load (e.g., through eye-tracking or self-reported mental effort) while varying the amount and detail of explanations provided

### Open Question 3
- Question: How does providing explanations affect users' long-term trust and reliance on conversational search systems over multiple interactions?
- Basis in paper: [explicit] The authors state that "to our knowledge, investigating the adaptation of responses based on user preferences, previous interactions with CIS systems, and topic complexity has yet to be explored"
- Why unresolved: The study used a between-subject design with single interactions, so it cannot capture how users' trust and reliance evolve over time with repeated exposure to explanations
- What evidence would resolve it: Longitudinal studies tracking users' trust and reliance across multiple conversational interactions with varying explanation quality and presentation modes

## Limitations
- Manual creation of responses and explanations introduces potential bias and limits generalizability
- Use of crowdworkers without domain expertise may not reflect real-world conversational AI usage
- Focus on TREC CAsT queries limits external validity to other conversational domains

## Confidence
- **High confidence**: The finding that noisy explanations reduce user-perceived usefulness is well-supported by consistent results across multiple analysis approaches and response dimensions.
- **Medium confidence**: The claim that users cannot reliably detect factual errors or biases is supported by the data but may be limited by the specific error types introduced and the lack of expert verification.
- **Medium confidence**: The minimal impact of presentation format (textual vs visual) is supported by the data, though the study may have been underpowered to detect subtle format effects.

## Next Checks
1. **Expert validation study**: Replicate the experiment with domain experts to determine if explanation detection rates improve with specialized knowledge, providing clearer evidence about the limits of non-expert evaluation capabilities.
2. **A/B testing of explanation formats**: Conduct a larger-scale study specifically testing different confidence display formats (percentage, visual indicators, textual descriptions) with power analysis ensuring adequate sample size to detect subtle presentation effects.
3. **Real-world deployment monitoring**: Implement the explanation system in a live conversational AI platform and collect longitudinal user feedback data to validate whether controlled study findings translate to actual user behavior and satisfaction in production environments.