---
ver: rpa2
title: 'Fine-Tuning Attention Modules Only: Enhancing Weight Disentanglement in Task
  Arithmetic'
arxiv_id: '2407.07089'
source_url: https://arxiv.org/abs/2407.07089
tags:
- task
- fine-tuning
- weight
- disentanglement
- arithmetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of weight disentanglement in
  task arithmetic, where combining fine-tuned models for multiple tasks often leads
  to interference. The authors propose fine-tuning only the attention modules of transformer
  models, which exhibit kernel behavior and thus improve weight disentanglement.
---

# Fine-Tuning Attention Modules Only: Enhancing Weight Disentanglement in Task Arithmetic

## Quick Facts
- arXiv ID: 2407.07089
- Source URL: https://arxiv.org/abs/2407.07089
- Reference count: 22
- Primary result: Attention-only fine-tuning improves weight disentanglement in task arithmetic by up to 2.38% over existing methods

## Executive Summary
This paper addresses the challenge of weight disentanglement in task arithmetic, where combining fine-tuned models for multiple tasks often leads to interference. The authors propose a novel approach: fine-tuning only the attention modules of transformer models while keeping other components frozen. Their key insight is that attention modules exhibit neural tangent kernel (NTK) behavior during fine-tuning, which improves weight disentanglement without the performance degradation seen in full NTK linearization. The method outperforms existing approaches on both vision and language benchmarks while being more computationally efficient.

## Method Summary
The authors propose selectively fine-tuning only the attention modules (Q, K, V, and output projections) of pre-trained transformer models. This selective fine-tuning leverages the kernel behavior of attention modules while avoiding the performance degradation associated with full NTK linearization. The approach involves loading a pre-trained model, freezing all parameters except attention module weights, fine-tuning on each task independently for 2,000 iterations with specific hyperparameters, and then extracting task vectors by subtracting pre-trained weights from fine-tuned weights. These task vectors are combined using arithmetic for unified model evaluation.

## Key Results
- Attention-only fine-tuning achieves up to 2.38% higher task arithmetic performance compared to non-linear fine-tuning and NTK linearization
- The approach demonstrates improved weight disentanglement with lower disentanglement error metrics
- Task vectors from attention-only fine-tuning show greater orthogonality, reducing interference in unified models
- The method is more efficient, requiring fewer parameters and less training time than full-model fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning only attention modules enables non-linear updates while maintaining kernel behavior. Attention modules in transformers exhibit neural tangent kernel (NTK) behavior during fine-tuning, allowing them to operate in the linear regime while other components update non-linearly. This combination provides weight disentanglement benefits without the performance degradation seen in full NTK linearization.

### Mechanism 2
Separating representation and task-specific modules enables cleaner analysis of weight disentanglement. By distinguishing between the representation module (which learns transferable features) and task-specific modules (like classification heads), the paper shows that weight disentanglement primarily emerges from the representation component. Task-specific modules can limit the effectiveness of task arithmetic.

### Mechanism 3
Orthogonal task vectors improve multi-task performance in unified models. The paper demonstrates that task vectors from attention-only fine-tuning are more orthogonal compared to full-model fine-tuning. This orthogonality reduces interference between tasks when combined in a unified model, leading to better multi-task performance.

## Foundational Learning

- **Neural Tangent Kernel (NTK) theory**: Understanding why NTK linearization improves weight disentanglement but harms individual model performance is crucial for grasping the paper's contribution. Quick check: What is the key difference between NTK linearization and standard fine-tuning that affects weight disentanglement?

- **Task arithmetic and task vectors**: The paper's approach modifies how task vectors are generated, so understanding the baseline task arithmetic framework is essential. Quick check: How are task vectors defined in the standard task arithmetic framework, and what problem does this paper aim to solve?

- **Attention mechanisms in transformers**: The paper's core innovation is fine-tuning only attention modules, so understanding their structure and role is fundamental. Quick check: What are the key components of attention modules in transformers, and why might they exhibit different learning dynamics than other modules?

## Architecture Onboarding

- **Component map**: Pre-trained transformer model -> Selective attention module fine-tuning -> Task vectors extraction -> Task arithmetic combination -> Unified model evaluation

- **Critical path**:
  1. Load pre-trained model
  2. Freeze all parameters except attention module weights
  3. Fine-tune on each task independently
  4. Extract task vectors (fine-tuned weights - original weights)
  5. Combine task vectors using arithmetic for unified model

- **Design tradeoffs**: Fine-tuning attention only reduces computational cost but may miss task-specific patterns in other modules. The approach requires models with attention mechanisms, limiting applicability to other architectures. Performance depends on the quality of the pre-trained model's attention modules.

- **Failure signatures**: Low performance on tasks requiring significant feature adaptation outside attention modules. Task vectors that are not sufficiently orthogonal, leading to interference. Models that fail to learn task-specific patterns despite attention module fine-tuning.

- **First 3 experiments**:
  1. Verify kernel behavior in attention modules using post-hoc linearization test on a single task
  2. Compare single-task performance of attention-only fine-tuning vs. full fine-tuning vs. NTK linearization
  3. Evaluate weight disentanglement on a simple two-task pair using disentanglement error metric

## Open Questions the Paper Calls Out

1. What specific architectural differences between attention modules and other transformer components cause attention modules to exhibit kernel behavior during fine-tuning?

2. How does the performance of attention-only fine-tuning scale with model size and architecture depth in transformers?

3. What is the relationship between attention module sparsity and kernel behavior in task arithmetic?

4. How does fine-tuning bias parameters affect model performance and weight disentanglement in attention-only fine-tuning?

5. Can task arithmetic be effectively extended to models with different architectures or pre-training objectives?

## Limitations

- The paper lacks rigorous theoretical validation of kernel behavior in attention modules, relying primarily on empirical evidence
- Results are based on a limited set of vision and language tasks, raising questions about generalizability to more complex or diverse task combinations
- The separation between representation and task-specific modules may oversimplify scenarios where task-specific adaptations in the representation module are crucial

## Confidence

- **High confidence**: The empirical results showing improved task arithmetic performance (up to 2.38% gain) and computational efficiency benefits are well-supported by experimental data
- **Medium confidence**: The mechanism explaining why attention modules exhibit kernel behavior is plausible but lacks rigorous theoretical validation
- **Low confidence**: The generalizability of results to other transformer architectures and task domains is uncertain

## Next Checks

1. Conduct comprehensive kernel behavior analysis across multiple transformer architectures to verify if attention modules consistently exhibit NTK behavior

2. Evaluate the approach on a broader range of task combinations, including tasks with significant domain overlap and those requiring substantial feature adaptation

3. Systematically vary which modules are fine-tuned (attention only, attention + feed-forward, all except classification head) to quantify the contribution of each component to weight disentanglement and overall performance