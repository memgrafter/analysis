---
ver: rpa2
title: Embedded Topic Models Enhanced by Wikification
arxiv_id: '2410.02441'
source_url: https://arxiv.org/abs/2410.02441
tags:
- topic
- entity
- word
- words
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation of existing neural topic models
  that do not consider word homography, treating words like "apple" and "amazon" as
  unique without awareness of their different entity meanings. The authors propose
  incorporating entity linking (wikification) and entity embeddings from Wikipedia2Vec
  into neural topic models, allowing them to distinguish between multiple entities
  sharing the same spelling.
---

# Embedded Topic Models Enhanced by Wikification

## Quick Facts
- **arXiv ID**: 2410.02441
- **Source URL**: https://arxiv.org/abs/2410.02441
- **Reference count**: 16
- **Primary result**: Entity-aware topic models achieve up to 21.6% lower perplexity than baselines by disambiguating homographic words using Wikipedia entity embeddings

## Executive Summary
This paper addresses a fundamental limitation in neural topic models: their inability to distinguish between homographic words that refer to different entities. The authors propose incorporating entity linking and Wikipedia2Vec entity embeddings into neural topic models, allowing them to treat words like "apple" and "amazon" as distinct entities rather than identical tokens. Their method replaces general word embeddings with specific entity embeddings when words are successfully linked to Wikipedia entries. The approach is evaluated on news datasets, demonstrating improved perplexity scores and more interpretable topics that better capture temporal dependencies.

## Method Summary
The authors integrate entity linking into the preprocessing pipeline of neural topic models. Documents are first tokenized and processed through an entity linker that identifies mentions of Wikipedia entities. When a word is linked to an entity (e.g., "apple" → "Apple Inc."), the corresponding Wikipedia2Vec entity embedding replaces the general word embedding in the model's embedding matrix. The entity-aware embedding matrix is then used in standard neural topic models like ETM and D-ETM. The model learns topics using these disambiguated embeddings, allowing it to naturally separate different meanings of homographic words while maintaining semantic coherence for each entity sense.

## Key Results
- Entity-aware topic models achieve 21.6% lower perplexity on the "apple" dataset and 13.1% lower on the "amazon" dataset compared to baselines
- Manual analysis shows more interpretable topics with frequent terms expressed as Wikipedia entities
- The method better captures temporal dependencies between topics in news article collections
- Improvements are observed even on datasets with fewer homographic words

## Why This Works (Mechanism)

### Mechanism 1
Entity linking (EL) allows the model to distinguish between homographic words by replacing ambiguous word embeddings with specific entity embeddings from Wikipedia2Vec. When a word is linked to a Wikipedia entity (e.g., "apple" → "Apple Inc." or "Amazon rainforest"), the model uses the entity embedding instead of the general word embedding. This ensures that semantically distinct entities with the same surface form are represented by different vectors.

### Mechanism 2
Entity embeddings capture context-specific meanings better than general word embeddings, improving topic coherence and interpretability. Entity embeddings are trained on Wikipedia text and hyperlinks, so they cluster based on the contexts in which entities appear. This allows topic models to more easily group semantically related words into coherent topics.

### Mechanism 3
Incorporating entity linking into preprocessing improves the generalizability of topic models, as measured by lower perplexity on held-out documents. By disambiguating words before topic modeling, the model can learn more accurate word-topic distributions, leading to better predictive performance on unseen data.

## Foundational Learning

- **Latent Dirichlet Allocation (LDA) and its extensions**: Understanding these models is crucial for grasping the proposed method's contributions. *Quick check*: What is the key difference between LDA and ETM in terms of how they represent topics?

- **Entity Linking and Entity Embeddings**: The core innovation involves using entity linking to disambiguate words and incorporating entity embeddings into topic models. *Quick check*: How does Wikipedia2Vec train entity embeddings, and why might they be more informative than general word embeddings for named entities?

- **Perplexity as an evaluation metric for topic models**: The paper uses perplexity to quantify the improvement in model generalizability. *Quick check*: What are the main criticisms of using perplexity to evaluate topic models, and how might they apply to this study?

## Architecture Onboarding

- **Component map**: Document preprocessing (tokenization → entity linking) → Embedding matrix construction (word/entity embeddings) → Neural topic model (ETM/D-ETM) → Perplexity evaluation → Manual topic analysis

- **Critical path**: 1. Preprocess documents (tokenize, link entities) 2. Build embedding matrix (use entity embeddings for linked entities) 3. Train neural topic model 4. Evaluate perplexity on test set 5. Analyze resulting topics

- **Design tradeoffs**: Accuracy vs. speed of entity linker; Dimensionality of embeddings (affects model capacity and training time); Number of topics (K) - affects granularity and interpretability; Use of dynamic vs. static topic models (D-ETM vs. ETM)

- **Failure signatures**: High perplexity despite entity linking - suggests entity linker errors or model not leveraging embeddings effectively; Topics lack coherence or contain unrelated terms - suggests embedding quality issues or incorrect K value; Entity linker fails to recognize many entities - suggests need for more accurate linker or different knowledge base

- **First 3 experiments**: 1. Train ETM on New York Times dataset without entity linking, record perplexity 2. Train ETM+EL on same dataset, compare perplexity to baseline 3. Manually inspect topics from both models to assess interpretability gains

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the proposed entity-aware topic models scale with the quality and coverage of the entity linking system used? The experiments used a basic entity linker, and the paper suggests that better entity linkers could improve results, but did not test this.

### Open Question 2
What is the impact of entity embeddings on topic model performance when applied to datasets with fewer homographic words? The paper showed that entity linking can still improve performance on datasets with fewer homographic words, but did not fully explore the extent of this impact.

### Open Question 3
How do entity-aware topic models perform on non-news text domains such as scientific papers or social media content? The paper focused on news articles and did not explore the generalizability of the proposed method to other text domains.

## Limitations

- The paper does not provide empirical evidence for entity linking accuracy, which is critical since incorrect entity assignments would propagate errors through the embedding matrix
- The study relies on perplexity as the primary evaluation metric without addressing known criticisms of this measure for topic model assessment
- The manual analysis of topic interpretability lacks systematic quantification through established metrics like topic coherence scores or human evaluation studies

## Confidence

**High Confidence**: The architectural integration of entity embeddings into neural topic models is technically sound and well-documented.

**Medium Confidence**: The empirical improvements in perplexity are demonstrated, but the lack of entity linking accuracy metrics and comprehensive human evaluation limits confidence in the practical significance of these gains.

**Low Confidence**: The claim that the method "better captures temporal dependencies between topics" is supported only by qualitative observations without rigorous quantitative validation.

## Next Checks

1. **Entity Linking Accuracy Validation**: Measure the precision and recall of the entity linking component on a held-out test set to quantify the quality of entity assignments before they enter the topic model.

2. **Systematic Interpretability Assessment**: Conduct a human evaluation study where multiple annotators rate topic coherence and interpretability for both baseline and entity-aware models, using established metrics like NPMI or UMass coherence scores.

3. **Comparative Temporal Analysis**: Implement a quantitative comparison of temporal topic dynamics between the proposed method and a state-of-the-art temporal topic model (e.g., dynamic topic models) using metrics like topic stability or topic evolution tracking accuracy.