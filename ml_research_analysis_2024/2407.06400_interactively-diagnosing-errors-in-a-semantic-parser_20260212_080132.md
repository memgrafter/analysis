---
ver: rpa2
title: Interactively Diagnosing Errors in a Semantic Parser
arxiv_id: '2407.06400'
source_url: https://arxiv.org/abs/2407.06400
tags:
- system
- errors
- diagnosis
- knowledge
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a model-based interactive diagnosis system
  for debugging semantic errors in a hand-curated natural language parser (CNLU).
  The system formulates the parsing process as a model with completeness assumptions,
  acceptability judgments, and factored interpretations to compactly represent valid
  interpretations.
---

# Interactively Diagnosing Errors in a Semantic Parser

## Quick Facts
- arXiv ID: 2407.06400
- Source URL: https://arxiv.org/abs/2407.06400
- Reference count: 3
- This paper presents a model-based interactive diagnosis system for debugging semantic errors in a hand-curated natural language parser (CNLU).

## Executive Summary
This paper introduces a system for interactively diagnosing errors in a controlled natural language understanding (CNLU) parser. The system uses model-based diagnosis to identify missing knowledge components when a user indicates a sentence is parsed incorrectly. By asking targeted natural language questions about word meanings and grammatical roles, the system can localize errors to specific missing lexicon entries, grammar rules, or semantic knowledge. The approach uses completeness assumptions and factored interpretations to compactly represent valid interpretations while avoiding combinatorial explosion.

## Method Summary
The system formulates the parsing process as a model with completeness and incompleteness assumptions, acceptability judgments, and factored interpretations to represent valid interpretations compactly. When a user indicates a sentence is incorrectly parsed, the system asks targeted natural language questions to gather acceptability judgments about specific parse elements. Using the General Diagnostic Engine (GDE) with compressed ATMS (CATMS), the system computes minimal diagnoses of missing knowledge. The algorithm can apply decomposition strategies to drill down on specific error types when initial diagnoses are decomposable.

## Key Results
- The system can successfully diagnose a set of synthetic errors by asking only a few questions
- Users without expert knowledge can help maintain CNLU by identifying knowledge gaps
- Factored interpretations allow compact representation of the space of possible interpretations without combinatorial explosion

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The system can localize semantic errors by asking users targeted questions about word meanings and grammatical roles
- Mechanism: The model-based diagnosis framework compares the user's understanding (captured through acceptability judgments) with the parser's interpretation trace. By representing parse elements with completeness and incompleteness assumptions, the system can identify which assumptions are faulted, localizing the error to specific knowledge gaps.
- Core assumption: User acceptability judgments accurately reflect their intended interpretation of the sentence
- Evidence anchors:
  - [abstract]: "When the user indicates a sentence is incorrectly parsed, the system asks targeted natural language questions to localize the error to missing lexicon entries, grammar rules, or semantic knowledge."
  - [section]: "The acceptability of one element might depend on the acceptability of another, and this dependence can be captured with ATMS justifications."
- Break condition: If user judgments are inconsistent or the parser's trace is incomplete, localization may fail

### Mechanism 2
- Claim: Decomposition strategies allow the system to drill down on specific error types by reformulating the problem
- Mechanism: When an initial diagnosis identifies a decomposable fault, the outer loop applies a decomposition strategy (e.g., "No Acceptable Semtrans for Word") to add relevant structure to the model and refine the diagnosis. This allows the system to isolate errors to specific knowledge components.
- Core assumption: Each error type has identifiable decomposition strategies that can isolate the fault
- Evidence anchors:
  - [abstract]: "demonstrate our system's ability to diagnose semantic errors on synthetic examples"
  - [section]: "The algorithm is currently greedy. When there are multiple possible decompositions, it picks one arbitrarily."
- Break condition: If no decomposition strategy exists for a given error type, the system cannot refine the diagnosis

### Mechanism 3
- Claim: Factored interpretations compactly represent the space of possible interpretations without combinatorial explosion
- Mechanism: Instead of representing each interpretation separately, the system uses the enablement graph's structure to recursively define factored interpretations. This captures all consistent interpretations while avoiding the exponential blow-up of representing each combination separately.
- Core assumption: The enablement graph accurately captures all valid interpretations of the sentence
- Evidence anchors:
  - [section]: "Every consistent interpretation can be found by selecting an enabled choice, ruling out any other choices in the same choice set, and repeating until no choices are left."
  - [section]: "Factored interpretations let us compactly encode when a parse is acceptable."
- Break condition: If the enablement graph is incomplete or incorrect, the factored interpretations may not capture all valid interpretations

## Foundational Learning

- Concept: Model-based diagnosis and ATMS
  - Why needed here: The system uses GDE (General Diagnostic Engine) with ATMS to reason about discrepancies between user understanding and parser behavior, tracking hypotheses about where knowledge errors might exist
  - Quick check question: What are the two types of assumptions used in this system (completeness and incompleteness) and how do they work together to identify faults?

- Concept: Semantic parsing and choice sets
  - Why needed here: CNLU produces semantic interpretations using choice sets that represent different meanings of words or parse trees. Understanding this structure is crucial for formulating the diagnosis model
  - Quick check question: How does CNLU handle ambiguity in sentences, and what is the role of choice sets in representing possible interpretations?

- Concept: Decomposition strategies in diagnosis
  - Why needed here: The system uses decomposition to drill down on errors, applying different strategies based on the error type (e.g., missing semtrans vs. missing valence pattern)
  - Quick check question: What are the three decomposition strategies currently supported by the system, and when does each one apply?

## Architecture Onboarding

- Component map:
  CNLU Parser -> Diagnosis Engine -> Decomposition Strategies -> User Interface -> Knowledge Base

- Critical path:
  1. User submits sentence with suspected error
  2. CNLU parses sentence and generates trace
  3. Diagnosis engine formulates model with completeness/incompleteness assumptions
  4. System asks user questions to gather acceptability judgments
  5. GDE computes minimal diagnoses based on user responses
  6. If diagnosis is decomposable, apply decomposition strategy
  7. Repeat until primitive fault is found
  8. Return diagnosis to user

- Design tradeoffs:
  - Completeness vs. tractability: Using CATMS instead of full ATMS trades some runtime efficiency for tractable label sizes
  - Generality vs. specificity: Decomposition strategies are specific to error types but make diagnosis more efficient
  - User burden vs. system capability: Asking more questions increases diagnostic accuracy but also user effort

- Failure signatures:
  - No diagnoses found: Either the error is outside the supported categories or the user judgments are inconsistent
  - Too many diagnoses: The model is too coarse or the decomposition strategies are insufficient
  - System hangs: The CATMS label size is growing too large, indicating the need for additional assumptions or decomposition

- First 3 experiments:
  1. Test diagnosis on a sentence with a known missing semtrans (like "Joe ate the apple" with EatingEvent removed)
  2. Test diagnosis on a sentence with a missing valence pattern
  3. Test diagnosis on a sentence with a restrictive type constraint

## Open Questions the Paper Calls Out
None explicitly identified in the paper.

## Limitations
- The system is currently limited to diagnosing errors in specific categories (missing lexicon entries, grammar rules, or semantic knowledge)
- The algorithm is greedy and may not find optimal decompositions when multiple possibilities exist
- Performance may degrade with increasing sentence complexity due to the size of the problem space

## Confidence
- Mechanism 1: High - follows established model-based diagnosis principles
- Mechanism 2: Medium - greedy algorithm without guarantees on finding optimal decompositions
- Mechanism 3: High - mathematical formulation of factored interpretations is sound

## Next Checks
1. Test the system with real users providing natural language judgments on actual error cases
2. Measure CATMS label growth and performance degradation with increasingly complex sentences
3. Evaluate the system's robustness when users provide inconsistent or ambiguous acceptability judgments