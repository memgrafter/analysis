---
ver: rpa2
title: 'MELTing point: Mobile Evaluation of Language Transformers'
arxiv_id: '2403.12844'
source_url: https://arxiv.org/abs/2403.12844
tags:
- device
- arxiv
- devices
- language
- mobile
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MELT, the first systematic infrastructure for
  evaluating Large Language Models (LLMs) on mobile and edge devices. MELT automates
  the process of downloading, converting, quantizing, and benchmarking LLMs across
  heterogeneous targets including Android, iOS, and Nvidia Jetson devices.
---

# MELTing point: Mobile Evaluation of Language Transformers

## Quick Facts
- arXiv ID: 2403.12844
- Source URL: https://arxiv.org/abs/2403.12844
- Authors: Stefanos Laskaridis; Kleomenis Katevas; Lorenzo Minto; Hamed Haddadi
- Reference count: 40
- Key outcome: This paper presents MELT, the first systematic infrastructure for evaluating Large Language Models (LLMs) on mobile and edge devices, revealing that LLM inference is primarily memory-bound and that edge offloading offers higher throughput and better sustained performance.

## Executive Summary
This paper introduces MELT, a systematic infrastructure for evaluating Large Language Models (LLMs) on mobile and edge devices. The study automates the process of downloading, converting, quantizing, and benchmarking LLMs across heterogeneous targets including Android, iOS, and Nvidia Jetson devices. Through comprehensive evaluation, the authors demonstrate that LLM inference is primarily memory-bound, with quantization significantly reducing memory requirements but introducing accuracy degradation at sub-4-bit precision. The research highlights the performance heterogeneity across devices and suggests that edge offloading to dedicated accelerators like Jetson devices offers higher throughput and better sustained performance compared to standalone mobile execution.

## Method Summary
The MELT infrastructure automates the evaluation of LLMs on mobile and edge devices by downloading, converting, quantizing, and benchmarking models across heterogeneous targets. The system uses a Raspberry Pi 4 as a coordinator connected to PhoneLab (mobile device farm with Monsoon power monitors) and JetsonLab (edge device farm with Jetson boards). Models are evaluated using two frameworks (MLC-LLM and llama.cpp) with various quantization schemes. The infrastructure traces specific events during inference and measures performance, energy, and thermal metrics. The evaluation covers multiple model sizes including TinyLlama-1.1B, Zephyr-3B, Gemma-2B, Llama-2-7B, and Mistral-7B, tested across different quantization bitwidths.

## Key Results
- LLM inference is primarily memory-bound, making quantization effective at reducing memory requirements while introducing accuracy degradation at sub-4-bit precision
- High-end mobile devices with sufficient RAM can run smaller quantized models at reasonable throughput, but continuous execution remains challenging due to high energy consumption and thermal constraints
- Edge offloading to dedicated accelerators like Jetson devices offers 1.78× to 3.3× higher throughput compared to mobile devices for prefill and generation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM inference is primarily memory-bound, making quantization effective at reducing memory requirements.
- Mechanism: Large transformer models require significant memory bandwidth to transfer weights and activations between main memory and on-chip registers. Quantization reduces the size of weights and activations, lowering memory traffic and enabling models to fit within constrained device memory.
- Core assumption: The memory bandwidth and capacity of mobile devices are the primary bottlenecks for LLM inference.
- Evidence anchors:
  - [abstract]: "Our analysis is the first systematic study of on-device LLM execution, quantifying performance, energy efficiency and accuracy across various state-of-the-art models and showcases the state of on-device intelligence in the era of hyperscale models. Results highlight the performance heterogeneity across targets and corroborates that LLM inference is largely memory-bound."
  - [section]: "Effectively, inference waits for the model state and activations to be expensively transferred from main to the on-chip memory, with little reuse due to the small batch sizes and autoregressive causal generation nature of the workload."
  - [corpus]: Weak evidence. No direct corpus citation supports this mechanism; only general mobile ML literature.

### Mechanism 2
- Claim: GPU acceleration provides significant performance benefits for LLM inference on mobile devices.
- Mechanism: GPUs are designed for parallel matrix operations, which are core to transformer computations. By offloading these operations to the GPU, inference speed can be greatly improved compared to CPU-only execution.
- Core assumption: The mobile GPU has sufficient memory and compute capacity to handle the transformed workloads efficiently.
- Evidence anchors:
  - [abstract]: "High-end mobile devices with sufficient RAM can run smaller quantized models at reasonable throughput..."
  - [section]: "MLC-LLM generally offered higher performance to llama.cpp, but at the cost of model portability... Operator fusion and TVM-based optimization play a significant role towards this result, with generation throughput difference of +4% on average for GPU execution (+28% vs llama.cpp CPU) and up to 3.53× higher."
  - [corpus]: Weak evidence. No direct corpus citation supports this mechanism; only general mobile ML literature.

### Mechanism 3
- Claim: Edge offloading to dedicated accelerators like Jetson devices offers higher throughput and better sustained performance.
- Mechanism: Edge devices like Jetson boards are designed for AI workloads and have more powerful GPUs and higher memory bandwidth compared to mobile devices. Offloading inference to these devices can leverage their superior hardware capabilities.
- Core assumption: The network latency and bandwidth between the mobile device and the edge accelerator are low enough to not significantly impact the user experience.
- Evidence anchors:
  - [abstract]: "Drawing from its energy footprint and thermal behavior, the continuous execution of LLMs remains elusive, as both factors negatively affect user experience. Last, our experience shows that the ecosystem is still in its infancy, and algorithmic as well as hardware breakthroughs can significantly shift the execution cost. We expect NPU acceleration, and framework-hardware co-design to be the biggest bet towards efficient standalone execution, with the alternative of offloading tailored towards edge deployments."
  - [section]: "The performance on Jetson AGX (50W) was much smoother... Indicatively, for Zephyr-7B (4-bit), the average throughput is 3.3 × and 1.78× higher, for prefill and generation respectively."
  - [corpus]: Weak evidence. No direct corpus citation supports this mechanism; only general mobile ML literature.

## Foundational Learning

- Concept: Transformers and attention mechanisms
  - Why needed here: Understanding the core architecture of LLMs is crucial for grasping why they are memory-intensive and how quantization affects their performance.
  - Quick check question: What is the primary computational bottleneck in transformer models, and how does attention contribute to it?

- Concept: Quantization techniques and their impact on accuracy
  - Why needed here: Quantization is a key technique used to reduce the memory footprint of LLMs, enabling their deployment on resource-constrained devices. Understanding the trade-offs between quantization and accuracy is essential.
  - Quick check question: How does reducing the bitwidth of weights and activations affect the accuracy of a transformer model, and what are the typical bitwidths used in practice?

- Concept: Mobile hardware architecture and limitations
  - Why needed here: Mobile devices have different hardware capabilities compared to desktops or servers. Understanding these limitations is crucial for optimizing LLM deployment on mobile platforms.
  - Quick check question: What are the main differences between mobile GPUs and desktop GPUs in terms of memory capacity and bandwidth, and how do these differences impact LLM inference?

## Architecture Onboarding

- Component map:
  - MELT Infrastructure -> PhoneLab and JetsonLab -> Model Zoo -> Model Evaluator
  - Coordinator (Raspberry Pi 4) -> Target devices (Android, iOS, Jetson) -> Performance monitoring tools

- Critical path:
  1. Model selection and conversion/quantization.
  2. Compilation of framework backends and applications for target platforms.
  3. Deployment of models and applications to devices.
  4. Automated interaction with LLMs using pre-canned prompts.
  5. Monitoring of performance, energy, and thermal metrics during inference.
  6. Collection and analysis of results.

- Design tradeoffs:
  - Model size vs. accuracy: Larger models generally offer better accuracy but require more memory and computational resources.
  - Quantization bitwidth vs. accuracy: Lower bitwidths reduce memory requirements but can lead to accuracy degradation.
  - Mobile vs. edge deployment: Mobile deployment offers privacy benefits but may have limited performance, while edge deployment can provide higher performance but requires offloading.

- Failure signatures:
  - Out-of-memory errors: Occur when the model size exceeds the available memory on the device.
  - Thermal throttling: Happens when the device temperature rises too high, leading to reduced performance.
  - Battery drain: Excessive power consumption can quickly deplete the device battery.

- First 3 experiments:
  1. Benchmark a small quantized LLM (e.g., TinyLlama-1.1B q4f16_1) on a high-tier mobile device using MLC-LLM framework.
  2. Compare the performance of the same model on the same device using llama.cpp framework.
  3. Offload the same model to a Jetson AGX device and measure the performance improvement.

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation focused on a narrow set of models (TinyLlama-1.1B, Zephyr-3B, Gemma-2B, Llama-2-7B, Mistral-7B) with limited quantization configurations
- Thermal and power management behavior was observed under controlled laboratory conditions, which may not reflect real-world usage patterns
- Performance benefits of GPU acceleration were demonstrated but not thoroughly explored across different mobile GPU architectures

## Confidence

**High Confidence:** The memory-bound nature of LLM inference and the effectiveness of quantization at reducing memory requirements is well-supported by systematic measurements across multiple devices and models. The throughput improvements from edge offloading to Jetson devices are clearly demonstrated with quantitative data.

**Medium Confidence:** The relative performance differences between MLC-LLM and llama.cpp frameworks, while measured, may vary with different model architectures and device combinations not tested in this study. The thermal constraints affecting sustained performance are observed but the exact thresholds for user experience degradation remain qualitative.

**Low Confidence:** The claim about future NPU acceleration and framework-hardware co-design being the "biggest bet" for efficient standalone execution is speculative, based on general trends rather than specific measurements in this study.

## Next Checks

1. **Expand Model and Quantization Coverage:** Evaluate additional model families (Phi, Falcon, MPT) and explore post-training quantization techniques to determine if accuracy degradation at sub-4-bit precision can be mitigated.

2. **Real-World Usage Simulation:** Implement a testing protocol that includes background processes, variable network conditions, and mixed workloads to validate thermal and power management findings under realistic mobile usage scenarios.

3. **Cross-Architecture GPU Comparison:** Test the same models across a wider range of mobile GPU architectures (Adreno, Mali, Apple Silicon) to quantify the performance variability and identify architectural bottlenecks in GPU-based LLM inference.