---
ver: rpa2
title: 'Ambient Diffusion Posterior Sampling: Solving Inverse Problems with Diffusion
  Models Trained on Corrupted Data'
arxiv_id: '2403.08728'
source_url: https://arxiv.org/abs/2403.08728
tags:
- diffusion
- data
- trained
- ambient
- measurements
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Ambient Diffusion Posterior Sampling (A-DPS),
  a method that uses diffusion models trained on corrupted data to solve inverse problems
  with different forward operators. The authors extend the Ambient Diffusion framework
  to train models directly from Fourier-subsampled MRI measurements at various acceleration
  factors (R=2,4,6,8).
---

# Ambient Diffusion Posterior Sampling: Solving Inverse Problems with Diffusion Models Trained on Corrupted Data

## Quick Facts
- **arXiv ID**: 2403.08728
- **Source URL**: https://arxiv.org/abs/2403.08728
- **Reference count**: 40
- **Primary result**: Ambient Diffusion Posterior Sampling (A-DPS) models trained on corrupted data outperform those trained on clean data for high-corruption inverse problems.

## Executive Summary
This paper introduces Ambient Diffusion Posterior Sampling (A-DPS), a method that uses diffusion models trained on corrupted data to solve inverse problems with different forward operators. The authors extend the Ambient Diffusion framework to train models directly from Fourier-subsampled MRI measurements at various acceleration factors (R=2,4,6,8). They demonstrate that models trained on subsampled data can outperform those trained on fully-sampled data for high-acceleration MRI reconstruction. The approach is also tested on natural image datasets (CelebA, FFHQ, AFHQ) for tasks like compressed sensing and super-resolution, showing that models trained on highly corrupted data sometimes outperform clean-data models in high-corruption regimes. The method is substantially faster than competing approaches while maintaining or improving reconstruction quality.

## Method Summary
The Ambient Diffusion Posterior Sampling (A-DPS) framework trains diffusion models on corrupted measurements to create more robust priors for solving inverse problems. Instead of training on clean data, the model learns to estimate the score function conditioned on corrupted measurements. The training data is further corrupted in Fourier space by subsampling lines, forcing the model to learn reconstruction from highly corrupted inputs. During inference, the trained model is used with a likelihood weighting term in the sampling algorithm to solve inverse problems. The method is applied to MRI reconstruction using multi-coil k-space measurements and to natural image restoration tasks like compressed sensing and super-resolution, demonstrating that models trained on corrupted data can generalize better to highly ill-posed inverse problems.

## Key Results
- A-DPS models trained on R=4 subsampled MRI data outperform models trained on fully-sampled data for high-acceleration reconstruction (R=6,8)
- Cross-domain transfer: inpainting-trained models outperform clean-data models for compressed sensing and super-resolution when corruption is high
- A-DPS achieves competitive reconstruction quality with significantly fewer function evaluations compared to FS-DPS
- The ambient training approach generalizes across different inverse problems and corruption types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A-DPS works because it trains models to estimate E[x₀|˜yₜ,ₜᵣₐᵢₙ, ˜Aₜᵣₐᵢₙ] instead of E[x₀|yₜ,ₜᵣₐᵢₙ, Aₜᵣₐᵢₙ], creating a more robust prior for high-corruption regimes.
- Mechanism: The ambient score function is trained on measurements that have been further corrupted in Fourier space by subsampling lines. This forces the model to learn to reconstruct from data with varying levels of corruption, making it more robust when applied to even more corrupted inference data.
- Core assumption: The further corruption matrix ˜P preserves sufficient information about the original corruption structure to allow learning a useful ambient score.
- Evidence anchors:
  - [abstract] "A-DPS models trained on subsampled data are better suited to solving inverse problems than models trained on fully sampled data."
  - [section 2.2] "The minimizer of the objective... is: hθ(˜yₜ,ₜᵣₐᵢₙ, ˜P) = E[x₀|˜yₜ,ₜᵣₐᵢₙ, ˜P]"
  - [corpus] Weak evidence - no direct citations about Fourier subsampling training in related papers
- Break condition: If the further corruption removes too much information or doesn't preserve the structure of the original forward operator, the ambient score becomes uninformative.

### Mechanism 2
- Claim: Models trained on corrupted data outperform clean-data models in high-corruption inference because they've learned to handle noise and missing information during training.
- Mechanism: During training, the model sees data at various corruption levels (R=2,4,6,8 for MRI). This exposure teaches the model to fill in missing information more effectively than models trained only on clean data, which may overfit to complete patterns.
- Core assumption: The training corruption distribution covers the inference corruption levels, or at least the high-corruption tail.
- Evidence anchors:
  - [abstract] "models trained on subsampled data can outperform those trained on fully-sampled data for high-acceleration MRI reconstruction"
  - [section 3.2] "as the validation acceleration increases to higher ratios R, A-DPS outperforms other models"
  - [corpus] Weak evidence - related papers focus on posterior sampling methods but not specifically on corrupted-data training advantages
- Break condition: If inference corruption level exceeds the maximum corruption level seen during training, performance may degrade.

### Mechanism 3
- Claim: The cross-domain transfer capability (using inpainting-trained models for compressed sensing/super-resolution) works because the ambient score learned for one corruption type generalizes to others.
- Mechanism: The ambient score captures fundamental reconstruction priors that are not specific to the exact corruption type. A model trained to handle missing pixels can still provide useful priors for handling blurred or downsampled data because both involve reconstructing missing information.
- Core assumption: The reconstruction task shares common structure across different corruption types (missing information must be filled in).
- Evidence anchors:
  - [abstract] "Ambient DPS can sometimes outperform models trained on clean data for several image restoration tasks"
  - [section 3.3] "we use models trained on random inpainting and we evaluate Gaussian Compressed Sensing and Super Resolution"
  - [corpus] Weak evidence - no direct citations about cross-domain transfer from inpainting to other tasks
- Break condition: If the inference task involves fundamentally different degradation (e.g., structured vs. random corruption), the ambient score may not generalize well.

## Foundational Learning

- Concept: Stochastic Differential Equations (SDEs) and score-based diffusion models
  - Why needed here: The sampling algorithm is based on discretizing an SDE (Eq 2.1) and modifying it for posterior sampling (Eq 2.2, 2.3)
  - Quick check question: What is the relationship between the noise schedule σₜ and the drift term in the SDE formulation?

- Concept: Fourier domain sampling and MRI k-space acquisition
  - Why needed here: The method specifically works with Fourier subsampled measurements, and the corruption is applied in Fourier space
  - Quick check question: How does the acceleration factor R relate to the number of Fourier coefficients measured?

- Concept: Multi-coil MRI and coil sensitivity encoding
  - Why needed here: The method handles multi-coil data where each coil has different sensitivity patterns that must be combined
  - Quick check question: What is the mathematical form of the multi-coil forward operator Aᵢ in Eq 2.5?

## Architecture Onboarding

- Component map:
  Data pipeline: k-space measurements → coil combination → further corruption in Fourier space
  Model: EDM architecture trained with ambient diffusion objective
  Sampling: A-DPS algorithm with likelihood weighting term
  Evaluation: FID, NRMSE, SSIM, PSNR, LPIPS, DISTS metrics

- Critical path:
  1. Load multi-coil k-space measurements
  2. Apply further Fourier subsampling to create corrupted measurements
  3. Train EDM model with ambient diffusion objective
  4. For inference: load corrupted measurements, run A-DPS sampling with likelihood term
  5. Evaluate reconstruction quality

- Design tradeoffs:
  - Model size vs. training speed: Ambient models use 36M params vs 65M for EDM baselines
  - Corruption level during training vs. inference performance
  - Number of sampling steps vs. reconstruction quality

- Failure signatures:
  - If A-DPS reconstructions show artifacts in high-frequency regions, the model may not have learned sufficient frequency content
  - If performance degrades with higher acceleration factors, the ambient score may not generalize well to extreme undersampling
  - If unconditional generation quality is poor, the ambient training objective may be too focused on reconstruction

- First 3 experiments:
  1. Train an ambient diffusion model on R=4 subsampled MRI data and evaluate unconditional generation quality (FID score)
  2. Use the trained model for A-DPS reconstruction on R=6 and R=8 accelerated data, comparing to FS-DPS baseline
  3. Train an inpainting model on natural images and test cross-domain transfer to compressed sensing and super-resolution tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do diffusion models trained on highly corrupted data sometimes outperform those trained on clean data for high-corruption inverse problems?
- Basis in paper: [explicit] The paper observes this phenomenon across multiple experiments (MRI reconstruction at high acceleration, compressed sensing with few measurements, super-resolution) but notes "there is no known theoretical argument that explains this performance cross-over."
- Why unresolved: The paper demonstrates the empirical observation but does not provide theoretical justification for why exposure to corrupted training data creates better priors for solving highly ill-posed inverse problems with different forward operators.
- What evidence would resolve it: A theoretical framework explaining the relationship between training corruption levels and generalization to out-of-distribution inverse problems, possibly through analysis of score functions or posterior geometry.

### Open Question 2
- Question: Does the performance advantage of Ambient Diffusion models trained on corrupted data generalize to other inverse problem solving algorithms beyond DPS and ALD?
- Basis in paper: [inferred] The paper only tests Ambient DPS and Ambient ALD reconstruction algorithms, noting "it is unknown whether our findings generalize for these reconstruction algorithms."
- Why unresolved: The paper limits its evaluation to DPS and ALD, leaving open whether the observed benefits of training on corrupted data extend to other methods like DDIM sampling, annealed Langevin dynamics, or newer posterior sampling techniques.
- What evidence would resolve it: Comparative experiments using Ambient Diffusion models with multiple reconstruction algorithms (DDIM, variational inference methods, plug-and-play priors) across various inverse problems.

### Open Question 3
- Question: What is the optimal level of corruption during training for maximizing reconstruction performance across different inverse problem difficulties?
- Basis in paper: [explicit] The paper tests models trained with various erasure probabilities (p = 0.2, 0.4, 0.6, 0.8) and finds that "the higher the erasure probability p during training, the better the Compressed Sensing performance of the model for low Number of Function Evaluations (NFEs)," but models trained on cleaner data perform better with increased NFEs.
- Why unresolved: While the paper demonstrates a trade-off between training corruption level and inference speed versus accuracy, it does not provide a systematic method for determining optimal corruption levels for different problem regimes.
- What evidence would resolve it: A theoretical or empirical framework mapping inverse problem difficulty (e.g., measurement count, noise level, degradation factor) to optimal training corruption levels that maximizes the Pareto frontier between reconstruction quality and computational efficiency.

## Limitations

- The cross-domain transfer capability shows promising results but lacks theoretical grounding for why the transfer works across different corruption types.
- The SSDU comparisons use a specific variant (1D-Partitioned) that may not represent the method's full capabilities with different hyperparameters.
- Evaluation focuses primarily on reconstruction quality metrics without extensive perceptual studies or downstream task performance validation.

## Confidence

- **High confidence**: The core claim that A-DPS models trained on corrupted data outperform clean-data models for high-corruption inference is well-supported by the MRI experiments across multiple acceleration factors.
- **Medium confidence**: The cross-domain transfer results are promising but based on fewer experiments with natural images; the mechanism remains somewhat unclear.
- **Medium confidence**: The computational efficiency claims (faster than FS-DPS) are demonstrated but depend on specific sampling step choices and may vary with implementation details.

## Next Checks

1. Conduct ablation studies varying the amount of additional corruption during training to identify optimal corruption levels for different inference tasks.
2. Test the cross-domain transfer capability with more diverse corruption types and validate whether the mechanism involves learning universal reconstruction priors or task-specific adaptations.
3. Compare against state-of-the-art supervised learning methods on the same datasets to establish the relative performance of the generative approach.