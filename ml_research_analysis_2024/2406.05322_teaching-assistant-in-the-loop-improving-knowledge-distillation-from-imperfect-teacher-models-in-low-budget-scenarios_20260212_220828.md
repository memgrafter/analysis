---
ver: rpa2
title: 'Teaching-Assistant-in-the-Loop: Improving Knowledge Distillation from Imperfect
  Teacher Models in Low-Budget Scenarios'
arxiv_id: '2406.05322'
source_url: https://arxiv.org/abs/2406.05322
tags:
- student
- teacher
- signal
- arxiv
- signals
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work tackles the dual challenge in knowledge distillation
  (KD) from large language models (LLMs): the high cost of collecting demonstrations
  and the negative impact of imperfect teacher outputs on student learning. To address
  these challenges, the authors propose a three-component framework that introduces
  a teaching assistant (TA) model to provide three types of signals for improving
  sample efficiency: (1) student internal signal (self-consistency score) to gauge
  the student''s confidence, (2) TA-student signal to assess the uncertainty of the
  student''s outputs, and (3) TA-teacher signal to filter low-quality teacher annotations.'
---

# Teaching-Assistant-in-the-Loop: Improving Knowledge Distillation from Imperfect Teacher Models in Low-Budget Scenarios

## Quick Facts
- arXiv ID: 2406.05322
- Source URL: https://arxiv.org/abs/2406.05322
- Authors: Yuhang Zhou; Wei Ai
- Reference count: 32
- Primary result: Proposed framework achieves up to 20.79% relative improvement over fine-tuning without signals across four reasoning tasks

## Executive Summary
This work addresses two key challenges in knowledge distillation (KD) from large language models (LLMs): the high cost of collecting demonstrations and the negative impact of imperfect teacher outputs on student learning. The authors propose a three-component framework that introduces a teaching assistant (TA) model to provide three types of signals for improving sample efficiency: student internal signal (self-consistency score), TA-student signal (TA confidence in student outputs), and TA-teacher signal (TA confidence in teacher annotations). Additionally, a two-stage training schema is proposed to warm up the student with a small proportion of data before utilizing the remaining budget.

## Method Summary
The method introduces a three-component KD framework with a two-stage training schema. First, the student generates outputs and calculates self-consistency scores, while the TA model evaluates both student and teacher outputs using confidence prompts. Examples are selected for annotation based on signal quality, and the teacher generates annotations that are filtered by TA confidence scores. The student is then fine-tuned on the high-quality annotations. The two-stage approach uses 10% of the annotation budget for initial warm-up training before proceeding with the remaining 90% using the full framework.

## Key Results
- Relative improvement of up to 20.79% compared to fine-tuning without any signals across four reasoning tasks
- Framework generalizes across TA model sizes (GPT-J-6B, Vicuna-13B, Vicuna-65B)
- Larger TA models lead to better performance, though diminishing returns observed (Vicuna 13B to 65B improvement smaller than GPT-J-6B to Vicuna-13B)
- Two-stage training consistently outperforms single-stage approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The TA-student signal provides complementary uncertainty assessment to the student's self-consistency, improving sample selection.
- Mechanism: The TA model evaluates the student's output using a confidence prompt to classify it as very confident, confident, not confident, or wrong answer, serving as an independent check on the student's own self-consistency score.
- Core assumption: The TA model's confidence assessment is more reliable than the student's self-consistency, which can be overconfident (Diao et al., 2023; Si et al., 2022).
- Evidence anchors:
  - [abstract]: "we introduce a 'teaching assistant' (TA) model to assess the uncertainty of both the student's and the teacher's outputs via confidence scoring"
  - [section]: "we utilize the TA model as an auxiliary signal to characterize the uncertainty of student's generations, given the observation that the confidence estimated by a LLM itself is prone to be overconfiden"
  - [corpus]: Weak evidence. The corpus contains related work on uncertainty propagation but not direct validation of TA-student signal effectiveness.

### Mechanism 2
- Claim: The TA-teacher signal filters out low-quality teacher annotations, improving the training data quality for the student.
- Mechanism: After the teacher generates annotations, the TA model evaluates these outputs using the same confidence prompt as for the TA-student signal, only passing annotations that receive "very confident" or "confident" ratings to the student's training set.
- Core assumption: The TA model can reliably distinguish correct from incorrect teacher annotations.
- Evidence anchors:
  - [abstract]: "Lastly, the TA model assesses annotations from the teacher (TA-teacher signal), deciding whether they merit inclusion in the student model's training dataset"
  - [section]: "To verify the correctness of teacher annotation te, we apply the similar TA-confidence as discussed in Section 3.1.2"
  - [corpus]: Weak evidence. While there is related work on KD with teacher assistants, there's no direct corpus evidence for TA-teacher signal effectiveness.

### Mechanism 3
- Claim: The two-stage training improves student performance by first warming up the student with a small budget before using the remaining budget.
- Mechanism: The first stage uses 10% of the annotation budget to fine-tune the student, producing a more competent student that generates more reliable self-consistency signals in the second stage when using the remaining 90% of the budget.
- Core assumption: The warmed-up student in the second stage will generate more reliable self-consistency signals than the initial student.
- Evidence anchors:
  - [abstract]: "Furthermore, we propose a two-stage training schema to first warm up the student with a small proportion of data to better utilize student's signal"
  - [section]: "we develop a two-stage training. The first stage is the warm-up training, and we use 10% of the budget to fine-tune the student with our framework"
  - [corpus]: No direct corpus evidence for this specific two-stage approach in KD.

## Foundational Learning

- Concept: Self-consistency scoring
  - Why needed here: Forms the basis for the student internal signal that estimates the student's confidence in its own outputs
  - Quick check question: How is self-consistency calculated when a student generates 5 outputs for a single input?

- Concept: Knowledge distillation in language models
  - Why needed here: This work builds on standard KD approaches but adds TA signals and a two-stage training process
  - Quick check question: What are the two major challenges in KD from LLMs that this work addresses?

- Concept: In-context learning (ICL)
  - Why needed here: The teacher model uses ICL to generate demonstrations, and the student is trained on these distilled outputs
  - Quick check question: How does the teacher model generate the initial annotations that become the distillation dataset?

## Architecture Onboarding

- Component map: Unlabeled Dataset -> Student LLM -> TA LLM -> Annotation Bucket -> Teacher LLM -> Fine-tuning Bucket -> Student LLM
- Critical path: Student inference → signal generation → annotation criteria check → TA annotation → quality check → fine-tuning
- Design tradeoffs: Larger TA models provide better signal quality but increase computational cost; two-stage training improves performance but requires more training time
- Failure signatures: Random-finetune baseline outperforming the framework indicates signal generation or selection criteria are not working; no improvement from TA signals suggests the TA model is not providing valuable assessments
- First 3 experiments:
  1. Run baseline Random-finetune to establish performance floor
  2. Implement TA-finetune (I) with only student internal signal to verify single-signal improvement
  3. Add TA-Teacher signal to TA-finetune (I) to test combined signal effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of the three-component framework change when using smaller student models (less than 1 billion parameters)?
- Basis in paper: [inferred] The paper mentions that student models (GPT-J-6B and Vicuna-13B) are fine-tuned on 8 NVIDIA A100 GPUs, which may not be accessible to everyone, and suggests extending experiments to smaller language models.
- Why unresolved: The paper does not provide experimental results for student models with fewer than 1 billion parameters, leaving the effectiveness of the framework for such models unexplored.
- What evidence would resolve it: Conducting experiments with smaller student models and comparing their performance with the current results would provide insights into the framework's effectiveness for models with fewer than 1 billion parameters.

### Open Question 2
- Question: How does the performance of the two-stage training framework compare to a single-stage training when using different budget allocations for the warm-up and fine-tuning stages?
- Basis in paper: [explicit] The paper introduces a two-stage training schema and shows that it brings a relative improvement of up to 20.79% compared to fine-tuning without any signals. However, it does not explore different budget allocations for the stages.
- Why unresolved: The paper uses a fixed budget allocation (10% for warm-up and 90% for fine-tuning) and does not investigate the impact of varying these proportions on the performance of the two-stage training framework.
- What evidence would resolve it: Conducting experiments with different budget allocations for the warm-up and fine-tuning stages and comparing their performance with the current results would provide insights into the optimal budget allocation for the two-stage training framework.

### Open Question 3
- Question: How does the choice of the TA model size affect the performance of the three-component framework, and is there a point of diminishing returns in terms of TA model size?
- Basis in paper: [explicit] The paper explores the generalization of the framework on TA models of different sizes (GPT-J-6B, Vicuna-13B, and Vicuna-65B) and finds that larger TA models lead to better performance, but the improvement from Vicuna 13B to 65B is smaller than that from GPT-J-6B to Vicuna-13B.
- Why unresolved: The paper does not investigate the relationship between TA model size and performance in detail, nor does it identify a point where increasing the TA model size no longer significantly improves the framework's performance.
- What evidence would resolve it: Conducting experiments with a wider range of TA model sizes and analyzing the relationship between TA model size and performance would help identify the optimal TA model size for the three-component framework and determine if there is a point of diminishing returns.

## Limitations
- Limited ablation studies prevent quantification of individual signal contributions
- Fixed 10% warmup budget lacks theoretical justification or sensitivity analysis
- Experimental scope limited to four reasoning tasks and specific model configurations
- No direct corpus evidence validating TA-student and TA-teacher signal effectiveness

## Confidence

- High confidence in the overall framework design and experimental methodology
- Medium confidence in the individual signal contributions due to limited ablation studies
- Medium confidence in the two-stage training benefits without theoretical justification
- Low confidence in generalizability to other tasks or larger models based on current experimental scope

## Next Checks

1. **Signal ablation study**: Run experiments isolating each of the three signals (student internal, TA-student, TA-teacher) to quantify their individual contributions to performance gains. This would validate whether the TA-student signal truly provides complementary uncertainty assessment beyond self-consistency.

2. **Budget sensitivity analysis**: Test the two-stage training approach with different warmup proportions (5%, 20%, 30%) and different total annotation budgets to determine if the 10% choice is optimal or task-dependent.

3. **TA model variation**: Replace the Vicuna-65B TA model with smaller models (e.g., Vicuna-13B) and larger models (e.g., GPT-4) to test whether the TA-student and TA-teacher signals remain effective across different TA model scales, validating the claim that this approach works across model sizes.