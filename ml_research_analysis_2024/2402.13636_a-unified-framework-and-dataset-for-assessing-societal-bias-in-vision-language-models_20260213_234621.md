---
ver: rpa2
title: A Unified Framework and Dataset for Assessing Societal Bias in Vision-Language
  Models
arxiv_id: '2402.13636'
source_url: https://arxiv.org/abs/2402.13636
tags:
- bias
- gender
- other
- service
- workers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a unified framework for evaluating gender,
  race, and age bias in vision-language models (VLMs) across all inference modes.
  It introduces an automated pipeline to generate high-quality synthetic datasets
  that conceal bias attributes across professional domains, using action-based descriptions
  instead of portraits.
---

# A Unified Framework and Dataset for Assessing Societal Bias in Vision-Language Models

## Quick Facts
- arXiv ID: 2402.13636
- Source URL: https://arxiv.org/abs/2402.13636
- Reference count: 40
- This paper proposes a unified framework for evaluating gender, race, and age bias in vision-language models (VLMs) across all inference modes using an automated pipeline to generate high-quality synthetic datasets with action-based descriptions of professional actions.

## Executive Summary
This paper introduces a comprehensive framework for evaluating societal bias in vision-language models across gender, race, and age dimensions. The authors develop an automated pipeline that generates synthetic datasets using action-based descriptions of professional activities performed by humanoid robots, effectively concealing demographic attributes while maintaining task relevance. The framework evaluates VLMs across all four input-output modalities (text-to-text, image-to-text, text-to-image, image-to-image) and introduces a novel "Neutrality" metric to quantify bias. Experiments with widely used VLMs reveal varying bias magnitudes and directions, with proprietary models generally exhibiting less bias than open-source alternatives.

## Method Summary
The framework generates 1016 synthetic {text, image} pairs depicting professional actions performed by humanoid robots, covering 40+ professions from the U.S. Bureau of Labor Statistics. Neutral text prompts describe professional actions, which are then converted to images using DALL-E-3 with humanoid subjects. The framework evaluates VLMs across all four input-output modalities using bias probes to extract demographic predictions. The novel "Neutrality" metric (∆N) quantifies bias by measuring the model's ability to predict demographic attributes when none are present in the input, alongside traditional metrics like Average Gender (∆AG) for bias direction.

## Key Results
- Proprietary VLMs exhibit less bias than open-source alternatives across all modalities
- Modality-specific bias patterns are observed, with different VLMs showing varying bias magnitudes and directions
- The action-based description approach effectively shifts focus from appearance to behaviors, reducing bias
- Neutrality scores vary significantly across models, with some achieving near-neutral performance while others show substantial bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using action-based descriptions of professions rather than portraits reduces bias by shifting focus from appearance to observable behaviors.
- Mechanism: Action descriptions provide task-relevant visual cues that are less likely to trigger stereotypes based on gender, race, or age, allowing models to make predictions based on profession-specific context.
- Core assumption: Observable actions associated with a profession are more strongly linked to that profession than demographic characteristics of the person performing them.
- Evidence anchors:
  - [abstract] "The dataset includes action-based descriptions of each profession and serves as a benchmark for evaluating societal biases in vision-language models (VLMs)."
  - [section] "An image of a professional's actions is more indicative of their profession than their appearance or other characteristics."
- Break condition: If the model has learned strong associations between certain demographic groups and specific professional actions, bias may persist despite the action-focused approach.

### Mechanism 2
- Claim: Neutral prompts (containing no gender/race/age attributes) combined with bias probes allow measurement of inherent model biases without input bias interference.
- Mechanism: By providing inputs that contain no demographic information, any demographic predictions made by the model must come from learned biases rather than input cues, allowing clean measurement of bias.
- Core assumption: The models have learned demographic associations from training data that are independent of task-relevant features.
- Evidence anchors:
  - [abstract] "We propose an automated pipeline to generate high-quality synthetic datasets that intentionally conceal gender, race, and age information across different professional domains"
  - [section] "We prompt the model to predict the social identity of the main subject in the given input image... To evaluate the bias of the model, we consider accuracy of prediction on each bias identity"
- Break condition: If the model can infer demographic attributes from subtle contextual cues or if the neutral generation process is imperfect.

### Mechanism 3
- Claim: Evaluating across all four input-output modalities (text-to-text, image-to-text, text-to-image, image-to-image) reveals modality-specific bias patterns.
- Mechanism: Different VLMs may process and generate information differently across modalities, leading to varying bias manifestations that can only be detected through comprehensive multimodal evaluation.
- Core assumption: Bias manifests differently depending on whether the model is processing text, images, or both as input/output.
- Evidence anchors:
  - [abstract] "Our evaluation encompasses all supported inference modes of the recent VLMs, including image-to-text, text-to-text, text-to-image, and image-to-image."
  - [section] "In our comparative analysis of widely used VLMs, we have identified that varying input-output modalities lead to discernible differences in bias magnitudes and directions."
- Break condition: If bias patterns are consistent across all modalities, suggesting the bias is inherent to the model's learned representations rather than modality-specific processing.

## Foundational Learning

- Concept: Bias quantification metrics
  - Why needed here: The paper introduces a novel "Neutrality" metric to address limitations in existing bias measures, which is critical for evaluating and comparing model bias.
  - Quick check question: Why might "Average Gender" score of 0 (equal male/female predictions) not indicate absence of bias?

- Concept: Synthetic data generation for bias evaluation
  - Why needed here: The paper uses synthetic data generation to create controlled, bias-bleached datasets that allow isolation of model bias from input bias.
  - Quick check question: What advantage does using robot/humanoid subjects provide over blurring/occluding techniques for creating bias-neutral images?

- Concept: Multimodal model architecture
  - Why needed here: Understanding how VLMs process different input-output combinations is essential for interpreting the modality-specific bias findings.
  - Quick check question: How might a model's text processing stack influence its bias in text-to-text versus image-to-text directions?

## Architecture Onboarding

- Component map:
  Data generation pipeline (GPT-4 for text prompts, DALL-E-3 for images) -> Bias evaluation framework (4 input-output directions, multiple bias attributes) -> Bias quantification system (Neutrality metric, comparison with existing metrics) -> Result analysis and visualization components

- Critical path:
  1. Generate neutral text prompts describing professional actions
  2. Generate corresponding neutral images with humanoid subjects
  3. Run evaluation across all VLM directions with bias probes
  4. Calculate Neutrality scores and compare across models/modalities
  5. Analyze profession-wise bias patterns

- Design tradeoffs:
  - Using synthetic data provides control but may not fully represent real-world distributions
  - The Neutrality metric is more robust but may be harder to interpret than simpler metrics
  - Evaluating all modalities provides comprehensive coverage but increases computational cost

- Failure signatures:
  - Poor neutrality scores despite neutral inputs suggest model bias
  - Inconsistent results across modalities may indicate modality-specific processing issues
  - Low accuracy on neutral class suggests model cannot recognize bias-bleached inputs

- First 3 experiments:
  1. Generate 100 neutral text prompts and corresponding images, verify they are truly neutral through human annotation
  2. Test a single VLM (e.g., LLaVA) in one direction (image-to-text) with gender bias probes, calculate Neutrality score
  3. Compare results across two VLMs in the same direction to validate the framework's ability to distinguish bias levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed "Neutrality" metric compare to other bias quantification methods in terms of reliability and interpretability?
- Basis in paper: [explicit] The paper introduces a novel "Neutrality" metric and discusses its advantages over existing metrics like "Average Gender" (AG).
- Why unresolved: The paper only briefly mentions the limitations of AG and claims Neutrality is better, but doesn't provide a rigorous comparative analysis or user studies to validate its superiority.
- What evidence would resolve it: A comprehensive evaluation comparing Neutrality with other bias metrics across diverse VLM models and tasks, along with user studies assessing the interpretability of the results.

### Open Question 2
- Question: How robust are the findings to variations in prompt engineering and dataset construction?
- Basis in paper: [inferred] The paper acknowledges that prompt engineering can significantly impact model outputs and that their dataset may not cover all professions.
- Why unresolved: The paper doesn't explore how sensitive the results are to different prompt formulations or variations in the dataset (e.g., different professions, actions, or robot/humanoid representations).
- What evidence would resolve it: A systematic study varying prompt wording, dataset composition, and robot/humanoid depictions to assess the stability of bias measurements.

### Open Question 3
- Question: How can the proposed framework be extended to capture intersectional biases and other social attributes beyond gender, race, and age?
- Basis in paper: [explicit] The paper mentions that the framework can be extended to study more societal biases and intersectional biases.
- Why unresolved: The paper only focuses on gender, race, and age, and doesn't provide a clear methodology for incorporating other attributes like disability, sexual orientation, or socioeconomic status.
- What evidence would resolve it: A demonstration of the framework's application to additional social attributes and an analysis of how biases interact across different dimensions (intersectionality).

## Limitations
- The synthetic nature of the dataset may not fully capture real-world bias distributions
- The robot/humanoid approach, while innovative, may not perfectly represent human subjects
- Limited evaluation of the framework's effectiveness on non-professional domains

## Confidence
- High Confidence Claims:
  - The framework successfully evaluates bias across all four input-output modalities
  - Proprietary models generally exhibit less bias than open-source alternatives
  - The action-based description approach effectively shifts focus from appearance to behaviors

- Medium Confidence Claims:
  - The Neutrality metric provides a robust measure of societal bias
  - Synthetic data generation produces high-quality, bias-bleached datasets
  - Modality-specific bias patterns are meaningfully distinct

- Low Confidence Claims:
  - The generalizability of results to real-world image distributions
  - The complete absence of bias in the synthetic data generation process
  - The stability of bias patterns across different VLM architectures

## Next Checks
1. **Real-world validation**: Test the framework on a subset of real-world images with professional subjects to compare bias patterns with synthetic data results
2. **Cross-domain applicability**: Evaluate the framework on non-professional domains (e.g., everyday activities, social situations) to assess generalizability
3. **Temporal stability**: Conduct longitudinal studies to examine how bias patterns evolve as VLMs are updated and retrained over time