---
ver: rpa2
title: Scaling LLM Inference with Optimized Sample Compute Allocation
arxiv_id: '2410.22480'
source_url: https://arxiv.org/abs/2410.22480
tags:
- allocation
- compute
- pure
- osca
- mixed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OSCA is an algorithm that learns to allocate inference compute
  budgets across multiple sampling configurations (models, temperatures, languages)
  to maximize accuracy on LLM tasks. By estimating pass probabilities on a training
  set, it optimizes the allocation using a hill-climbing approach.
---

# Scaling LLM Inference with Optimized Sample Compute Allocation

## Quick Facts
- **arXiv ID**: 2410.22480
- **Source URL**: https://arxiv.org/abs/2410.22480
- **Reference count**: 12
- **Primary result**: OSCA achieves 25-128x better accuracy than single-configuration approaches on code generation and reasoning tasks

## Executive Summary
This paper introduces OSCA (Optimized Sample Compute Allocation), an algorithm that learns to allocate inference compute budgets across multiple sampling configurations (models, temperatures, languages) to maximize accuracy on LLM tasks. By estimating pass probabilities on a training set, OSCA optimizes the allocation using a hill-climbing approach. The method demonstrates significant improvements over single-configuration approaches, achieving 25-128x better accuracy on code generation and reasoning tasks, and improving agentic workflows like SWE-Bench by 3x with less compute.

## Method Summary
OSCA is an algorithm that learns to allocate inference compute budgets across multiple sampling configurations (models, temperatures, languages) to maximize accuracy on LLM tasks. It estimates pass probabilities on a training set, then uses a hill-climbing approach to optimize the allocation. The algorithm takes as input a training problem set, inference configurations, compute budget, and estimation budget, and outputs an optimized allocation that maximizes pass@C (probability of at least one correct solution among generated samples) on the test set.

## Key Results
- OSCA achieves 25-128x better accuracy than single-configuration approaches on code generation and reasoning tasks
- OSCA improves agentic workflows like SWE-Bench by 3x with less compute
- The method scales well with compute budgets and demonstrates strong performance across diverse problem domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mixed allocation of compute across different sampling configurations can achieve better accuracy than optimal pure allocation
- Mechanism: By allocating compute across multiple configurations, the algorithm can leverage the strengths of different configurations for different problems, effectively creating a portfolio of solutions
- Core assumption: Different problems benefit from different sampling configurations, and these differences are predictable from training data
- Evidence anchors:
  - [abstract] "just by uniformly allocating the compute budget over all possible inference configurations...LLMs' accuracy can already surpass the optimal pure allocation on LiveCodeBench"
  - [section 3.2] "Consider two problems x1 and x2, and two configurations h1 and h2...if we use a mixed allocation and split 10 samples evenly between h1 and h2, the expected pass@10 would be 43.8%, which is significantly higher than the pure allocation's result"
  - [corpus] Weak evidence - no direct corpus support for this mechanism, but related work on inference compute allocation exists
- Break condition: When problems are homogeneous and consistently benefit from the same configuration, or when the cost of estimating pass probabilities outweighs the benefits

### Mechanism 2
- Claim: OSCA's hill-climbing algorithm can find near-optimal allocations by iteratively improving sample distribution
- Mechanism: Starting from a random allocation, the algorithm examines neighboring allocations by shifting compute between configurations and moves to better ones until convergence
- Core assumption: The optimization landscape is smooth enough that local improvements lead to good solutions
- Evidence anchors:
  - [section 3.3] "the relaxed problem is convex...that can be solved optimally by hill climbing algorithm...we examine all the neighbors of the current distribution...and 'climb' to the neighbor if it's better"
  - [section 3.3] "Although the algorithm is not guaranteed to produce global optima for integral solutions, it works well empirically"
  - [corpus] No direct corpus support for this specific hill-climbing approach
- Break condition: When the search space is too large for hill-climbing to be tractable, or when the objective function has many local optima that trap the algorithm

### Mechanism 3
- Claim: Small training compute budget (C0) is sufficient to estimate pass probabilities that generalize to larger test budgets (C)
- Mechanism: By estimating pass probabilities on a small subset of training data, the algorithm can predict which configurations will be most effective for the full test set
- Core assumption: Training and test problems are drawn from the same distribution, so estimated probabilities generalize
- Evidence anchors:
  - [section 4.3] "Although the sample compute budget (C0) used for estimating the solve rate matrix was merely 50, OSCA can still produce strong sample compute allocations even when the test time compute budget C is as large as 1024 per problem"
  - [section 4.4] "even with 26% of the original C0, OSCA can still get an accuracy very close to its upperbound, indicating that it is not really sensitive to C0"
  - [corpus] No direct corpus support for this specific scaling behavior
- Break condition: When training and test distributions differ significantly, or when the relationship between small and large compute budgets is non-linear

## Foundational Learning

- Concept: Convex optimization and relaxation techniques
  - Why needed here: The paper relies on relaxing the integral constraint to prove convexity and apply hill-climbing, which is essential for understanding the algorithm's theoretical foundation
  - Quick check question: What mathematical property allows the hill-climbing algorithm to work effectively in this problem?

- Concept: Probability theory and expected value calculations
  - Why needed here: The algorithm estimates pass probabilities and maximizes expected pass rates, requiring understanding of probability distributions and expectation calculations
  - Quick check question: How does the paper calculate the expected pass rate given estimated probabilities and allocation?

- Concept: Ablation study methodology
  - Why needed here: The paper uses ablation studies to understand which hyperparameters benefit most from mixed allocation, requiring knowledge of experimental design principles
  - Quick check question: What does the ablation study on temperature-only vs model-only allocations reveal about the algorithm's effectiveness?

## Architecture Onboarding

- Component map: Training problems -> Pass probability estimation -> Optimized allocation -> Test problems
- Critical path:
  1. Estimate pass probabilities on training set with small budget C0
  2. Initialize random allocation Ï€
  3. Iteratively improve allocation through hill-climbing
  4. Apply learned allocation to test problems with larger budget C
  5. Evaluate accuracy improvements
- Design tradeoffs:
  - Small C0 vs large C: Lower estimation cost but potentially less accurate predictions
  - More configurations vs fewer: Better coverage but higher estimation cost and larger search space
  - Hill-climbing vs other optimizers: Simple and effective but may get stuck in local optima
- Failure signatures:
  - Poor performance when training and test distributions differ
  - Slow convergence or poor allocations when configurations are too similar
  - Overfitting to training data when too few training examples are available
- First 3 experiments:
  1. Verify hill-climbing algorithm converges and improves allocation on synthetic data with known optimal solution
  2. Test sensitivity to C0 by running with different estimation budgets on the same training set
  3. Validate that mixed allocations outperform pure allocations on a simplified benchmark with clearly different problem types

## Open Questions the Paper Calls Out

The paper acknowledges several limitations and open questions:
- The method focuses primarily on four representative inference hyperparameters (model types, temperatures, response languages, and prompts), leaving open questions about other hyperparameters like top k, top p, and repetition penalty
- While the algorithm demonstrates effective allocation, the theoretical guarantees for global optimality are limited
- The relationship between small training compute budgets and larger test budgets needs further exploration

## Limitations
- The comparison against "optimal pure allocation" requires access to test labels, which is unrealistic in practical scenarios
- The evaluation focuses primarily on code generation and reasoning tasks, leaving open questions about performance on other domains
- The hill-climbing algorithm lacks theoretical guarantees for global optimality and may get stuck in local optima

## Confidence

- **High confidence**: OSCA achieves significantly better accuracy than default single-configuration approaches on the tested benchmarks (LiveCodeBench, LiveBench). The empirical methodology is sound and the improvements are substantial and measurable.
- **Medium confidence**: OSCA's hill-climbing algorithm finds good allocations in practice, though not guaranteed to be globally optimal. The algorithm works well empirically but lacks theoretical convergence guarantees.
- **Low confidence**: Small C0 budgets reliably estimate pass probabilities that generalize to large C budgets across diverse problem distributions. This critical assumption needs more rigorous validation.

## Next Checks

1. **Distribution shift robustness test**: Evaluate OSCA's performance when training and test sets are drawn from different distributions (e.g., train on simple code problems, test on complex multi-file changes). This would validate the core assumption about probability generalization.

2. **Global optimality assessment**: Run the hill-climbing algorithm with multiple random initializations on a synthetic problem with known optimal solution to quantify how often it finds the global optimum versus getting trapped in local optima.

3. **Practical deployment validation**: Implement OSCA in a realistic code generation pipeline where compute budgets are constrained, and compare against strong baselines like temperature scaling and majority voting. Measure not just accuracy but also wall-clock time and cost efficiency.