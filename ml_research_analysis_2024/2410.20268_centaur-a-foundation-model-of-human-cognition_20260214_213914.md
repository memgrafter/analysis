---
ver: rpa2
title: 'Centaur: a foundation model of human cognition'
arxiv_id: '2410.20268'
source_url: https://arxiv.org/abs/2410.20268
tags:
- press
- points
- instructed
- will
- option
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Centaur, the first foundation model of human
  cognition, by fine-tuning a large language model on a novel, large-scale dataset
  called Psych-101. Psych-101 contains trial-by-trial data from over 60,000 participants
  across 160 psychological experiments, totaling over 10 million choices.
---

# Centaur: a foundation model of human cognition

## Quick Facts
- arXiv ID: 2410.20268
- Source URL: https://arxiv.org/abs/2410.20268
- Reference count: 40
- Key outcome: First foundation model of human cognition that fine-tunes Llama 3.1 70B on Psych-101 dataset to achieve superior behavioral prediction and generalization across 160 psychological experiments

## Executive Summary
This paper introduces Centaur, the first foundation model of human cognition, created by fine-tuning a large language model on a novel, large-scale dataset called Psych-101. The dataset contains trial-by-trial data from over 60,000 participants across 160 psychological experiments, totaling more than 10 million choices. Centaur demonstrates superior performance in predicting human behavior compared to both the base language model and existing cognitive models across nearly all experiments. The model also shows strong generalization to new cover stories, structural modifications, and entirely novel domains, while its internal representations become more aligned with human neural activity after fine-tuning.

## Method Summary
The authors built Centaur on top of Llama 3.1 70B using QLoRA parameter-efficient fine-tuning with rank-8 adapters on all non-embedding layers. The model was trained for one epoch on the entire Psych-101 dataset using cross-entropy loss with masked response tokens. Training used an 8-bit AdamW optimizer with a learning rate of 0.00005 and weight decay of 0.01. The Psych-101 dataset contains trial-by-trial data from 160 psychological experiments spanning domains like multi-armed bandits, decision-making, memory, supervised learning, and Markov decision processes, involving 60,092 participants and 10,681,650 choices.

## Key Results
- Centaur outperforms Llama 3.1 70B baseline and existing cognitive models on 92.4% of experiments in held-out participant prediction
- The model generalizes well to new cover stories, structural task modifications, and entirely novel domains not seen during training
- Centaur's internal representations show improved alignment with human neural activity compared to the base language model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Centaur achieves superior behavioral prediction by leveraging vast linguistic priors embedded in large language models and aligning them with trial-by-trial behavioral data.
- Mechanism: Llama 3.1 70B provides rich world knowledge; QLoRA adapters allow efficient fine-tuning on Psych-101; cross-entropy loss is masked to focus only on human responses.
- Core assumption: Large language models encode relevant cognitive priors that can be adapted to behavioral domains without catastrophic forgetting.
- Evidence anchors:
  - [abstract] "derived Centaur by finetuning a state-of-the-art large language model on a novel, large-scale data set called Psych-101"
  - [section] "We built Centaur on top of the open-source language model Llama 3.1 70B... adding low-rank adapters... trained for one epoch on the entire data set"
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.426" - moderate semantic similarity suggests this approach is novel but grounded in related work.

### Mechanism 2
- Claim: Centaur generalizes to unseen experimental paradigms by learning task-agnostic representations of human decision-making processes.
- Mechanism: Fine-tuning on diverse paradigms creates representations that capture underlying cognitive structures rather than surface features.
- Core assumption: Different psychological experiments share common computational substrates that can be learned jointly.
- Evidence anchors:
  - [abstract] "generalizes to new cover stories, structural task modifications, and entirely novel domains"
  - [section] "out-of-distribution evaluations... Centaur is robust in the face of changes to the cover story... robust to modifications in task structure... capture human behavior even in entirely novel domains"
  - [corpus] No direct corpus evidence for this generalization claim - stated explicitly in paper.

### Mechanism 3
- Claim: Centaur's internal representations become more aligned with human neural activity through behavioral fine-tuning, despite no explicit neural training objective.
- Mechanism: Behavioral optimization implicitly shapes representations to match neural patterns associated with the same cognitive processes.
- Core assumption: Behavioral and neural representations share underlying structure that can be aligned through shared task optimization.
- Evidence anchors:
  - [abstract] "model's internal representations become more aligned with human neural activity after finetuning"
  - [section] "conducted two analyses in which we predicted human neural activity using the model's internal representations... Centaur's representations consistently outperform Llama's"
  - [corpus] No corpus evidence - neural alignment is a novel claim specific to this paper.

## Foundational Learning

- Concept: Cross-entropy loss with masked response tokens
  - Why needed here: Ensures the model learns to predict human behavior rather than completing experimental instructions
  - Quick check question: What happens to the loss calculation for tokens that don't correspond to human responses?

- Concept: Pseudo-R² evaluation metric
  - Why needed here: Provides normalized measure of behavioral prediction across diverse experiments with different response types
  - Quick check question: How does Pseudo-R² differ from standard R² in the context of behavioral prediction?

- Concept: QLoRA parameter-efficient fine-tuning
  - Why needed here: Allows adaptation of large language models while preserving pre-trained knowledge and reducing computational cost
  - Quick check question: What percentage of parameters are actually trained when using QLoRA with rank-8 adapters?

## Architecture Onboarding

- Component map: Llama 3.1 70B backbone → QLoRA low-rank adapters (rank 8) → Psych-101 fine-tuning → Centaur model
- Critical path: Data preprocessing → Token masking → Fine-tuning with cross-entropy loss → Evaluation on held-out data
- Design tradeoffs: Large pre-trained model provides rich priors but requires efficient fine-tuning; comprehensive behavioral dataset enables broad generalization but increases computational requirements
- Failure signatures: Overfitting to training participants (poor held-out participant performance); poor generalization to novel domains (low out-of-distribution performance); lack of neural alignment (no improvement in neural prediction)
- First 3 experiments:
  1. Test held-out participant prediction on a simple two-armed bandit task from Psych-101
  2. Evaluate generalization to modified cover story using the two-step task variant
  3. Assess neural alignment on fMRI data from a simple decision-making task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal neural architecture for a domain-general model of human cognition?
- Basis in paper: [inferred] The paper discusses training models with different architectures from scratch using the Psych-101 dataset as a future direction.
- Why unresolved: While Centaur uses a transformer-based architecture, the paper suggests exploring alternative architectures like vector-based memory or incorporating neuroscience theories.
- What evidence would resolve it: Systematic comparison of different neural architectures trained on Psych-101, evaluating their ability to capture human behavior across domains and align with neural activity.

### Open Question 2
- Question: How can we extend Centaur's capabilities to real-world applications beyond psychological experiments?
- Basis in paper: [explicit] The paper states that Centaur's current scope is limited to psychological experiments expressible in natural language.
- Why unresolved: The paper acknowledges this limitation but doesn't provide a concrete solution for extending the model to real-world scenarios.
- What evidence would resolve it: Development and evaluation of methods to translate real-world situations into the language-based format used by Centaur, or training models directly on real-world data.

### Open Question 3
- Question: How does Centaur represent individual differences in cognition?
- Basis in paper: [inferred] The paper mentions the goal of including information about individual differences in future iterations of Psych-101.
- Why unresolved: The current version of Psych-101 and Centaur don't explicitly model individual differences, focusing instead on population-level behavior.
- What evidence would resolve it: Analysis of Centaur's internal representations when trained on data including individual difference measures, and comparison to human neural activity patterns related to individual differences.

## Limitations

- The Psych-101 dataset, while large, covers only a subset of cognitive domains and may not represent the full diversity of human behavior
- The evaluation methodology relies heavily on pseudo-R² metrics, which may not capture all aspects of behavioral prediction quality
- Neural alignment results are based on relatively few fMRI datasets and may not generalize across different neural recording modalities

## Confidence

- High Confidence: The behavioral prediction improvements over baseline models and the out-of-distribution generalization results
- Medium Confidence: The neural alignment results and the claim of creating a true "foundation model" of cognition

## Next Checks

1. Test Centaur on completely new cognitive domains not represented in Psych-101 (e.g., social cognition, emotion processing) to verify the foundation model claim and assess true generalization capabilities

2. Evaluate Centaur's performance on non-English cognitive datasets to determine whether the behavioral priors are language-dependent or represent universal cognitive principles

3. Conduct systematic neural alignment tests across multiple neural recording modalities (EEG, MEG, intracranial recordings) and cognitive states to verify the robustness and specificity of the observed neural-behavioral correspondence