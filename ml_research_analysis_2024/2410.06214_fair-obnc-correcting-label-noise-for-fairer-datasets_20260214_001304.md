---
ver: rpa2
title: 'Fair-OBNC: Correcting Label Noise for Fairer Datasets'
arxiv_id: '2410.06214'
source_url: https://arxiv.org/abs/2410.06214
tags:
- noise
- label
- labels
- data
- group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of label noise in datasets used
  for training machine learning models, particularly in contexts where biased decisions
  can perpetuate discrimination. The authors propose Fair-OBNC, a novel label noise
  correction method that incorporates fairness considerations, aiming to improve demographic
  parity in the corrected dataset.
---

# Fair-OBNC: Correcting Label Noise for Fairer Datasets

## Quick Facts
- arXiv ID: 2410.06214
- Source URL: https://arxiv.org/abs/2410.06214
- Reference count: 40
- Primary result: Fair-OBNC improves demographic parity by an average of 150% while outperforming other label correction methods in reconstructing original labels.

## Executive Summary
This paper introduces Fair-OBNC, a novel method for correcting label noise in datasets while simultaneously improving fairness metrics, specifically demographic parity. The method builds upon the Ordering-Based Noise Correction (OBNC) framework by incorporating fairness considerations into the label correction process. Through experiments on a fraud detection dataset with controlled label noise injection, Fair-OBNC demonstrates significant improvements in both label reconstruction accuracy and fairness outcomes compared to existing methods.

## Method Summary
Fair-OBNC extends the OBNC algorithm by modifying its ordering criterion to account for both ensemble margin scores and the potential increase in demographic parity. The method introduces three key modifications: adjusting the ordering criterion to balance demographic parity across sensitive groups, allowing the exclusion of sensitive attributes and proxy variables during ensemble training and margin calculation, and providing an option for score-based margins instead of binary predictions. During the correction process, Fair-OBNC calculates how many instances need to be flipped in each sensitive group to achieve target disparity levels, then applies these corrections in an order determined by margin scores while monitoring demographic parity improvements.

## Key Results
- Fair-OBNC achieves a 150% average improvement in demographic parity compared to models trained on noisy data
- The method outperforms other label correction techniques in reconstructing original labels on controlled noise experiments
- Fair-OBNC maintains strong predictive performance (measured by TPR) while improving fairness metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fair-OBNC modifies the ordering criterion to balance demographic parity by adjusting the number of label corrections per sensitive group.
- Mechanism: The method calculates minimum and maximum prevalence values for each sensitive group to achieve target disparity, then determines required flips per group based on current prevalence. During correction, it only flips instances that reduce disparity between groups.
- Core assumption: Label noise is correlated with sensitive attributes, and group-aware corrections can simultaneously reduce noise and improve fairness.
- Evidence anchors:
  - [abstract] "The presented method adapts Ordering-Based Noise Correction, with an adjusted criterion of ordering, based both on the margin of error of an ensemble, and the potential increase in the observed demographic parity of the dataset."
  - [section] "We propose three major modifications to the original method: a) modify the ordering criterion which determines labels to be corrected, b) ignore the sensitive attribute and/or proxy variables in the process of determining the probability of each label being noisy, and c) allow the user to select either to use a committee voting scheme or a score-based margin."
- Break condition: Algorithm stops flipping labels if prevalence disparity is already below target or no more corrections reduce disparity.

### Mechanism 2
- Claim: Ignoring sensitive attributes during ensemble training and margin calculation reduces propagation of bias in label correction.
- Mechanism: The ensemble classifier is trained on noisy dataset excluding sensitive attributes and proxy variables, so margin scores are computed based on non-sensitive features, preventing correction process from reinforcing existing biases.
- Core assumption: Sensitive attributes and proxy variables contribute to biased margins that would otherwise guide label corrections in discriminatory direction.
- Evidence anchors:
  - [section] "We allow ignoring the sensitive attributes, as well as any other potentially correlated features when training the ensemble of classifiers and calculating the margins."
  - [section] "This is done expecting the decisions of the resulting model to be independent of sensitive attributes when this information is removed."
- Break condition: If sensitive attribute is the only strong predictor of label noise, ignoring it may reduce correction accuracy, potentially requiring inclusion of proxy variables instead.

### Mechanism 3
- Claim: Using score-based margins instead of binary predictions increases resolution of uncertainty estimation for label correction.
- Mechanism: Instead of using majority voting to compute margins (which produces discrete values), the method computes margins using continuous ensemble scores, reducing ties in margin values and providing finer granularity in ordering misclassified instances.
- Core assumption: Higher resolution in margin scores leads to more accurate identification of likely mislabeled instances, improving both noise correction and fairness outcomes.
- Evidence anchors:
  - [section] "The last modification to the original algorithm is the inclusion of a parameter to determine the margin calculation... With this change, we increase the resolution of the margin vector, as the score vector is constituted with float numbers, when compared to binary class predictions."
- Break condition: If ensemble scores are poorly calibrated or classifier ensemble is too weak, score-based margins may not provide meaningful differentiation, potentially degrading performance.

## Foundational Learning

- Concept: Label noise correction fundamentals
  - Why needed here: Fair-OBNC builds on OBNC, which uses ensemble margins to identify and correct mislabeled instances. Understanding how margins reflect label reliability is essential to grasp the modification for fairness.
  - Quick check question: How does the OBNC method determine which instances are most likely to have noisy labels?

- Concept: Demographic parity and fairness metrics
  - Why needed here: Fair-OBNC explicitly targets demographic parity as its fairness criterion, requiring understanding of how label distributions across sensitive groups affect this metric.
  - Quick check question: What does it mean for a dataset to achieve demographic parity, and how is it measured?

- Concept: Ensemble learning and margin calculation
  - Why needed here: The method relies on an ensemble of classifiers to estimate label reliability; understanding ensemble voting vs. score-based predictions is critical for the fairness-aware modification.
  - Quick check question: What is the difference between margin calculation using binary predictions versus continuous scores in an ensemble context?

## Architecture Onboarding

- Component map:
  Input: Noisy labeled dataset with sensitive attributes -> Preprocessor: Optional exclusion of sensitive attributes and proxies -> Core: Ensemble training → margin calculation → instance ordering → fairness-aware correction loop -> Output: Corrected dataset with improved demographic parity -> Evaluator: Metrics for reconstruction accuracy, TPR, and demographic parity

- Critical path:
  1. Train ensemble on (optionally sanitized) noisy data
  2. Compute margins for misclassified instances
  3. Calculate prevalence disparities and required flips per group
  4. Order instances by margin, apply fairness-aware flipping with stopping criteria
  5. Output corrected dataset and evaluate metrics

- Design tradeoffs:
  - Ignoring sensitive attributes improves fairness but may reduce noise correction accuracy if sensitive info is predictive
  - Score-based margins increase resolution but depend on ensemble quality
  - Fixed flip rate vs. dynamic stopping based on margin threshold and disparity target

- Failure signatures:
  - High false positive rate on group B when noise is only injected in group A (indicates overcorrection)
  - Zero demographic parity after massaging (indicates threshold interaction issues)
  - Low reconstruction score (indicates stopping criteria too aggressive or margin calculation ineffective)

- First 3 experiments:
  1. Run Fair-OBNC with default hyperparameters on the IID dataset with 5% noise injected only in group A; verify demographic parity improvement and label reconstruction accuracy.
  2. Compare Fair-OBNC vs. original OBNC on same dataset; measure changes in TPR, demographic parity, and false positive/negative rates per group.
  3. Test Fair-OBNC with and without ignoring sensitive attributes; analyze impact on both fairness metrics and reconstruction accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Fair-OBNC be extended to handle multiclass classification problems beyond binary classification?
- Basis in paper: [explicit] The authors state: "Finally, the algorithm we propose is limited to binary classification problems. The scope of this work could be broadened by adapting this strategy to handle multiclass scenarios."
- Why unresolved: The paper only evaluates Fair-OBNC on binary classification tasks and does not explore how the method could be adapted for multiclass problems.
- What evidence would resolve it: Empirical results demonstrating Fair-OBNC's performance on multiclass classification datasets with controlled label noise injection and fairness metrics.

### Open Question 2
- Question: What are the limitations of ignoring sensitive attributes in Fair-OBNC, and how can the method be adapted to handle complex scenarios with multiple sensitive attributes or high correlations with other features?
- Basis in paper: [explicit] The authors note: "However, this strategy may be too simplistic to achieve fairness in more complicated scenarios where there might be multiple sensitive attributes and more complex noise models."
- Why unresolved: The paper only evaluates Fair-OBNC on datasets with a single binary sensitive attribute and does not explore scenarios with multiple or correlated sensitive attributes.
- What evidence would resolve it: Empirical results comparing Fair-OBNC's performance on datasets with multiple sensitive attributes or high correlations between sensitive and non-sensitive features.

### Open Question 3
- Question: How does Fair-OBNC perform in real-world scenarios with non-controlled label noise injection and complex bias patterns?
- Basis in paper: [inferred] The paper's experiments use controlled label noise injection, but real-world datasets often have more complex and less understood noise patterns.
- Why unresolved: The paper does not evaluate Fair-OBNC on real-world datasets with naturally occurring label noise and bias.
- What evidence would resolve it: Application of Fair-OBNC to real-world datasets with known fairness issues, comparing its performance to other methods in terms of both label reconstruction and fairness metrics.

## Limitations
- Fair-OBNC is limited to binary classification problems and requires adaptation for multiclass scenarios
- The method's performance depends on accurate prevalence estimation and the assumption that label noise correlates with sensitive attributes
- Fixed flip rate and stopping criteria may not be optimal across all datasets, potentially leading to under- or over-correction

## Confidence

- **High Confidence**: The mechanism for adjusting label corrections based on demographic parity disparities is clearly specified and theoretically sound
- **Medium Confidence**: The impact of ignoring sensitive attributes during ensemble training is plausible but may vary depending on dataset's feature correlation structure
- **Medium Confidence**: The use of score-based margins to improve correction granularity is reasonable, but its effectiveness depends on ensemble quality and calibration

## Next Checks
1. Apply Fair-OBNC to datasets with different sensitive attribute definitions (e.g., gender, race) and noise patterns to assess generalizability
2. Systematically vary the flip rate and margin threshold to determine optimal settings for different noise levels and dataset characteristics
3. Test the method's performance when correlated proxy variables are included or excluded, to quantify their impact on fairness and accuracy trade-offs