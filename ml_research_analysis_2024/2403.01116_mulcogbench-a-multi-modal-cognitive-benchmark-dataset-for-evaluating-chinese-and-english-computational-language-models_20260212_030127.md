---
ver: rpa2
title: 'MulCogBench: A Multi-modal Cognitive Benchmark Dataset for Evaluating Chinese
  and English Computational Language Models'
arxiv_id: '2403.01116'
source_url: https://arxiv.org/abs/2403.01116
tags:
- language
- cognitive
- data
- fmri
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MulCogBench, a large-scale multi-modal cognitive
  benchmark dataset for evaluating Chinese and English computational language models.
  MulCogBench includes eye-tracking, semantic ratings, fMRI, and MEG data collected
  from native speakers processing words, sentences, and discourses.
---

# MulCogBench: A Multi-modal Cognitive Benchmark Dataset for Evaluating Chinese and English Computational Language Models

## Quick Facts
- **arXiv ID**: 2403.01116
- **Source URL**: https://arxiv.org/abs/2403.01116
- **Reference count**: 0
- **Primary result**: Multi-modal cognitive benchmark dataset for evaluating Chinese and English language models against human brain data

## Executive Summary
MulCogBench introduces a large-scale multi-modal cognitive benchmark dataset designed to evaluate the cognitive plausibility of computational language models in both Chinese and English. The dataset combines eye-tracking, semantic ratings, fMRI, and MEG data collected from native speakers processing words, sentences, and discourses. Through similarity-encoding analysis, the study reveals systematic patterns in how different language models align with human brain representations across linguistic complexity levels, neural modalities, and model layers. The findings demonstrate that language models share significant similarities with human cognitive data, with alignment patterns modulated by modality, linguistic unit complexity, and brain region.

## Method Summary
The study constructed MulCogBench by collecting multi-modal cognitive data from native Chinese and English speakers processing linguistic stimuli of varying complexity (words, sentences, and discourses). Eye-tracking and semantic rating data were gathered from larger participant pools, while fMRI data came from 24 participants and MEG data from 11 participants. Language models including BERT, ERNIE, RoBERTa, and GPT-2 were used to generate contextual representations of the stimuli. Brain-model alignment was assessed using similarity-encoding analysis, comparing model representations with neural activity patterns across different layers and modalities.

## Key Results
- Context-aware models showed stronger performance than context-independent models as stimulus complexity increased
- Shallow model layers aligned better with MEG data while deeper layers aligned better with fMRI data
- Brain-model alignment patterns were highly consistent between Chinese and English processing

## Why This Works (Mechanism)
The effectiveness of MulCogBench stems from its comprehensive multi-modal approach to capturing human language processing. By combining behavioral measures (eye-tracking, semantic ratings) with neuroimaging data (fMRI, MEG), the dataset captures both the slow, spatially precise hemodynamic responses and the fast, temporally precise electromagnetic signals of language comprehension. The similarity-encoding framework allows direct comparison between high-dimensional model representations and neural activity patterns, revealing how different computational architectures mirror human cognitive processing across linguistic complexity levels.

## Foundational Learning
- **Similarity-encoding analysis**: Statistical method for comparing model representations with neural data by encoding similarity matrices
  - Why needed: Enables quantitative assessment of brain-model alignment beyond simple correlation measures
  - Quick check: Verify that similarity matrices capture meaningful structure in both model outputs and neural data
- **Multi-modal neuroimaging**: Integration of fMRI (spatial resolution) and MEG (temporal resolution) for comprehensive brain activity capture
  - Why needed: Different neural signals capture complementary aspects of language processing
  - Quick check: Confirm temporal and spatial alignment between different imaging modalities
- **Context-aware vs context-independent models**: Distinction between models using surrounding context versus isolated word processing
  - Why needed: Critical for understanding how computational models handle linguistic complexity
  - Quick check: Compare performance across linguistic units of increasing complexity
- **Layer-wise analysis**: Examination of model-brain alignment patterns across different network depths
  - Why needed: Reveals how hierarchical processing in models corresponds to neural organization
  - Quick check: Verify consistent patterns across multiple models and languages
- **Cross-linguistic validation**: Testing alignment patterns in both Chinese and English
  - Why needed: Ensures findings generalize across typologically different languages
  - Quick check: Compare alignment strengths and patterns between language pairs
- **Cognitive plausibility evaluation**: Framework for assessing whether computational models capture human-like processing
  - Why needed: Moves beyond traditional benchmarks to evaluate human-likeness of AI systems
  - Quick check: Establish baseline performance of human cognitive data against itself

## Architecture Onboarding

**Component Map**: Cognitive data collection -> Preprocessing pipeline -> Language model generation -> Similarity encoding -> Statistical analysis -> Cross-linguistic comparison

**Critical Path**: Language model representation generation → Similarity encoding with neural data → Statistical validation → Cross-linguistic pattern comparison

**Design Tradeoffs**: Large-scale behavioral data collection provides robust statistical power but requires extensive participant recruitment, while neuroimaging data offers direct brain measurements but at much higher cost and smaller sample sizes

**Failure Signatures**: Inconsistent alignment patterns across linguistic complexity levels may indicate model limitations in handling context; lack of cross-linguistic consistency could suggest culturally specific rather than universal processing patterns

**First 3 Experiments**:
1. Compare alignment strength between context-aware and context-independent models across all linguistic units
2. Analyze layer-wise alignment differences between MEG and fMRI modalities
3. Test cross-linguistic consistency by comparing Chinese and English alignment patterns for identical model architectures

## Open Questions the Paper Calls Out
None

## Limitations
- Small neuroimaging sample sizes (fMRI: 24 participants, MEG: 11 participants) limit statistical power and generalizability
- Limited model comparison scope, focusing only on BERT, ERNIE, RoBERTa, and GPT-2 without testing newer architectures
- Potential confounds from translation quality and cultural differences not fully addressed in cross-linguistic analysis

## Confidence
- **High**: Dataset construction methodology and multi-modal data collection documentation
- **Medium**: Cross-linguistic consistency findings, though translation and cultural factors need consideration
- **Low-Medium**: Layer-wise brain alignment interpretation due to complexity of mapping model representations to neural mechanisms

## Next Checks
1. Replicate brain-model alignment analyses with larger participant samples to confirm MEG/fMRI layer differences
2. Test additional contemporary language models including those with explicit cognitive alignment training objectives
3. Conduct cross-validation analyses using held-out cognitive data from same participants to assess individual-level patterns