---
ver: rpa2
title: 'SPformer: A Transformer Based DRL Decision Making Method for Connected Automated
  Vehicles'
arxiv_id: '2409.15105'
source_url: https://arxiv.org/abs/2409.15105
tags:
- vehicles
- learning
- traffic
- state
- vehicle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a transformer-based deep reinforcement learning
  framework (SPformer) for multi-vehicle collaborative decision-making in mixed autonomy
  traffic. The method introduces a learnable policy token as the learning medium for
  joint strategies and incorporates intuitive physical positional encoding (PPE) to
  enhance location awareness.
---

# SPformer: A Transformer Based DRL Decision Making Method for Connected Automated Vehicles

## Quick Facts
- arXiv ID: 2409.15105
- Source URL: https://arxiv.org/abs/2409.15105
- Reference count: 40
- Primary result: Transformer-based DRL framework (SPformer) achieves 18.142 average traffic score vs 13.265 for GNN-DQN and 9.134 for CNN-DQN in on-ramp CAV scenarios

## Executive Summary
This paper introduces SPformer, a transformer-based deep reinforcement learning framework for multi-vehicle collaborative decision-making in connected automated vehicle (CAV) environments. The method uses a learnable policy token as a joint decision-making medium and incorporates physical positional encoding to enhance location awareness. Experiments in on-ramp scenarios with 2 CAVs and 4 HDVs demonstrate that SPformer significantly outperforms CNN and GNN-based DQN methods, achieving better traffic efficiency and safety outcomes.

## Method Summary
SPformer is a transformer-based DRL framework that processes multi-modal state representations of traffic scenarios through a transformer encoder with a learnable policy token. The architecture combines vehicle states with physical positional encoding (PPE) and processes them through multi-head self-attention mechanisms. The policy token serves as a global learning medium for joint decision-making, while PPE provides redundant absolute position information. The method uses a MADQN (Multi-Agent Deep Q-Network) architecture to predict Q-values for multiple agents simultaneously.

## Key Results
- SPformer achieves average traffic score of 18.142 in on-ramp scenarios
- Outperforms GNN-DQN (13.265) and CNN-DQN (9.134) baselines by significant margins
- Demonstrates better learning efficiency and produces higher-quality policies balancing safety and traffic efficiency
- Shows robust performance with consistent success rates and reduced collision numbers

## Why This Works (Mechanism)

### Mechanism 1
The policy token enables global perception of traffic state by acting as a central learning medium that receives weighted contributions from all vehicle state embeddings through multi-head attention. During each transformer layer, the policy token aggregates context from other tokens while simultaneously influencing their representations, creating a shared learning space where vehicle-specific decisions are informed by global traffic dynamics.

### Mechanism 2
Physical positional encoding (PPE) strengthens location awareness by adding redundant absolute position information that survives feature extraction layers. PPE provides absolute position information through sinusoidal encoding added to vehicle state embeddings before transformer processing, preserving spatial relationships throughout the network and helping maintain spatial awareness as other features are processed.

### Mechanism 3
The multi-head self-attention mechanism effectively captures vehicle interactions by allowing each vehicle to attend to all others simultaneously, unlike GNN's local propagation. This enables the model to capture both local proximity effects and long-range interactions that influence driving decisions, such as vehicles preparing to merge or emergency vehicles approaching from distance.

## Foundational Learning

- **Multi-head self-attention mechanism**: Enables each vehicle to simultaneously consider all other vehicles' states for decision-making, capturing complex interaction patterns that local methods miss. Quick check: How does multi-head attention differ from standard attention, and why is this important for multi-vehicle decision-making?

- **Deep reinforcement learning with value function approximation**: Allows handling the continuous state space of traffic scenarios where traditional tabular Q-learning would be infeasible. Quick check: Why is a neural network needed to approximate the Q-function in this multi-vehicle setting?

- **Transformer positional encoding**: Provides explicit location information that survives feature extraction and helps the model understand spatial relationships critical for driving decisions. Quick check: How does sinusoidal positional encoding help the model understand vehicle positions in the traffic scenario?

## Architecture Onboarding

- **Component map**: Input state matrix → Physical positional encoding addition → Transformer encoder (policy token + vehicle states) → MLP head → Action probabilities/Q-values
- **Critical path**: State representation → PPE addition → Transformer processing → Policy derivation → Action execution → Reward calculation → Network update
- **Design tradeoffs**: Transformer offers global interaction modeling but requires more parameters and computation than GNN; policy token enables joint decision-making but may introduce coordination complexity
- **Failure signatures**: Poor learning efficiency (slow convergence), suboptimal policies (safety violations or traffic inefficiency), or unstable training (high variance across runs)
- **First 3 experiments**:
  1. Baseline comparison: Implement CNN-DQN and GNN-DQN using the same state representation and reward function to establish performance baseline
  2. Ablation study: Remove PPE from SPformer to quantify its contribution to performance improvement
  3. Policy token analysis: Compare SPformer with and without the policy token to measure its impact on joint decision quality

## Open Questions the Paper Calls Out

1. **Scalability to larger scenarios**: How does SPformer's performance scale with increasing numbers of vehicles in the simulation environment? The paper mentions future work will focus on improving performance in large-scale scenarios with many vehicles and random traffic flow.

2. **PPE design sensitivity**: How sensitive is SPformer's performance to variations in the physical positional encoding design? The paper uses a specific sine-cosine based PPE but doesn't explore alternative designs or sensitivity analysis.

3. **Real-world robustness**: How does SPformer perform in scenarios with communication delays or sensing information uncertainty? The paper explicitly states it does not consider these real-world factors that can significantly impact decision-making quality.

## Limitations

- Results are based solely on Flow simulator scenarios with fixed 2 CAVs and 4 HDVs, limiting generalizability to different traffic densities and real-world conditions
- Implementation complexity involves multiple novel components (policy token, PPE, MADQN loss) that require careful implementation
- Lacks ablation studies isolating individual contributions of policy token versus PPE versus transformer architecture

## Confidence

**High Confidence**: The core claim that transformer-based architectures can outperform CNN and GNN methods for multi-vehicle decision-making is well-supported by experimental results.

**Medium Confidence**: The specific mechanisms of policy token effectiveness and PPE contribution are plausible but lack direct ablation evidence quantifying their individual impacts.

**Low Confidence**: The claim of "better learning efficiency" is based on a single training curve comparison without statistical analysis of convergence rates or learning stability across multiple runs.

## Next Checks

1. **Ablation Study Implementation**: Implement SPformer variants without the policy token and without PPE to quantify their individual contributions to the 18.142 ATS score versus baseline performance.

2. **Statistical Validation**: Run 10 independent training trials for SPformer, CNN-DQN, and GNN-DQN to establish confidence intervals for ATS, success rate, and collision metrics, verifying reported performance differences are statistically significant.

3. **Scenario Robustness Testing**: Evaluate SPformer performance across varying traffic densities (2-6 HDVs), different ramp configurations (shorter/longer ramps), and vehicle penetration rates (1-3 CAVs) to assess generalizability beyond the reported on-ramp scenario.