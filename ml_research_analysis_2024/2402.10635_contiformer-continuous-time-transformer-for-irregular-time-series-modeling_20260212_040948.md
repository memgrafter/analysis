---
ver: rpa2
title: 'ContiFormer: Continuous-Time Transformer for Irregular Time Series Modeling'
arxiv_id: '2402.10635'
source_url: https://arxiv.org/abs/2402.10635
tags:
- time
- contiformer
- neural
- series
- continuous-time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce ContiFormer, a continuous-time Transformer
  that combines attention with ODE dynamics for irregular time series. By replacing
  discrete self-attention with a continuous-time inner product and introducing latent
  trajectories governed by ODEs, the model can capture both evolving inter-observation
  relationships and underlying continuous dynamics.
---

# ContiFormer: Continuous-Time Transformer for Irregular Time Series Modeling

## Quick Facts
- arXiv ID: 2402.10635
- Source URL: https://arxiv.org/abs/2402.10635
- Reference count: 40
- The authors introduce ContiFormer, a continuous-time Transformer that combines attention with ODE dynamics for irregular time series, achieving up to 10% relative improvement in RMSE and accuracy over strong baselines.

## Executive Summary
ContiFormer introduces a continuous-time Transformer framework for modeling irregular time series by integrating ODE dynamics with attention mechanisms. The model replaces discrete self-attention with continuous-time inner products computed via ODE trajectories, enabling it to capture evolving inter-observation relationships while maintaining computational efficiency through reparameterization and quadrature methods. Theoretical analysis establishes that many existing irregular-time Transformer variants are special cases of this framework, and extensive experiments demonstrate consistent improvements across multiple datasets and sampling patterns.

## Method Summary
The method extends Transformer attention to continuous time by defining latent trajectories for queries, keys, and values using ODEs. Attention weights are computed as integrals of query-key products over time intervals, approximated using Gauss-Legendre quadrature after reparameterizing time intervals to a fixed range. This approach enables parallel computation while capturing continuous dynamics. The framework is theoretically shown to subsume many existing irregular-time Transformer variants through appropriate function hypotheses, and is validated on both synthetic and real-world datasets.

## Key Results
- Achieves up to 10% relative improvement in RMSE and accuracy compared to strong baselines
- Demonstrates consistent performance across diverse sampling patterns and noise levels
- Shows robustness to varying drop ratios in classification tasks on UEA datasets
- Theoretical framework proves that many existing irregular-time Transformers are special cases of ContiFormer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ContiFormer achieves continuous-time attention by reparameterizing discrete self-attention into an inner product over continuous trajectories governed by ODEs.
- Mechanism: The model defines latent trajectories ki(t) and vi(t) for each observation via ODEs, then integrates the query-key product over time intervals to compute attention weights Î±i(t).
- Core assumption: The underlying process generating irregular observations is continuous and can be approximated by a differentiable function over time.
- Evidence anchors: [abstract] "replacing discrete self-attention with a continuous-time inner product" [section 3.1] "we model the evolving relationship between the i-th sample and the dynamic system at time point t as the inner product of q and ki in a closed interval [ti, t]"

### Mechanism 2
- Claim: ContiFormer maintains parallelization while computing continuous-time attention via reparameterization and Gauss-Legendre quadrature.
- Mechanism: The model reparameterizes time intervals to a fixed range [-1,1] using time-variable ODE, then approximates the integral via quadrature, enabling parallel computation of attention weights across all token pairs.
- Core assumption: The integral in the attention computation can be accurately approximated by a finite number of quadrature points without losing parallelism.
- Evidence anchors: [section 3.3] "we first adopt time variable ODE to reparameterize ODEs into a single interval [-1, 1], followed by numerical approximation method to approximate the integrals"

### Mechanism 3
- Claim: ContiFormer can represent many existing irregular-time Transformer variants as special cases through appropriate choice of function hypothesis.
- Mechanism: By selecting specific forms for the continuous query function, key dynamics, and value trajectories, ContiFormer can reduce to models like mTAN, NSTKA, or Mercer-style time embeddings.
- Core assumption: The expressive power of ODEs and the continuous-time attention framework is sufficient to capture the behavior of discrete attention variants with appropriate parameterizations.
- Evidence anchors: [abstract] "by curated designs of function hypothesis, many Transformer variants specialized in irregular time series modeling can be covered as a special case of ContiFormer" [section 3.4] "Theorem 1 (Universal Attention Approximation Theorem)"

## Foundational Learning

- Concept: Ordinary Differential Equations (ODEs) and their numerical solvers
  - Why needed here: ContiFormer uses ODEs to define continuous latent trajectories for keys, values, and the query function.
  - Quick check question: What is the difference between an explicit and implicit ODE solver, and when would you choose one over the other for this model?

- Concept: Inner product in function spaces and integral approximation
  - Why needed here: The continuous attention mechanism is defined as an integral of the product of two functions over time, approximated via quadrature.
  - Quick check question: How does the choice of quadrature method (e.g., Gauss-Legendre vs. trapezoidal rule) affect the accuracy and computational cost of the attention computation?

- Concept: Transformer architecture and self-attention mechanism
  - Why needed here: ContiFormer builds upon the Transformer's multi-head attention structure, extending it to the continuous-time domain.
  - Quick check question: What is the role of the softmax operation in the attention mechanism, and how does it interact with the continuous-time formulation?

## Architecture Onboarding

- Component map:
  Input -> Continuous Interpolation (cubic spline) -> ODE trajectories for keys/values -> Continuous-time attention computation -> Multi-head aggregation -> Sampled output

- Critical path:
  1. Transform inputs to continuous queries, keys, and values
  2. Compute continuous-time attention weights via ODE integration and quadrature
  3. Aggregate weighted values to form output trajectories
  4. Sample output at reference points for next layer or final prediction

- Design tradeoffs:
  - Accuracy vs. computation: More quadrature points increase accuracy but slow training
  - Parallelization vs. expressiveness: ODE reparameterization enables parallelism but may limit complex dynamics
  - Interpolation choice: Cubic splines provide smoothness but may overfit; linear interpolation is faster but less smooth

- Failure signatures:
  - Numerical instability in ODE solver (large gradients, divergence)
  - Poor approximation of attention weights (low quadrature points, ill-conditioned integrals)
  - Overfitting to noise in interpolation function

- First 3 experiments:
  1. Interpolation of 2D spirals: Test continuity and smoothness of output vs. discrete Transformer and ODE baselines
  2. Classification on UEA datasets: Evaluate robustness to varying drop ratios and compare against RNN, ODE, and attention-based methods
  3. Event prediction on synthetic data: Assess ability to capture complex temporal dependencies and dynamic intensity functions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ContiFormer change when applied to irregularly sampled time series data with non-linear underlying dynamics, such as those with abrupt changes or non-stationary behavior?
- Basis in paper: [explicit] The paper discusses ContiFormer's ability to model complex continuous-time dynamic systems and captures the evolving input-dependent process.
- Why unresolved: The paper does not provide specific experiments or results on time series data with non-linear underlying dynamics, abrupt changes, or non-stationary behavior.
- What evidence would resolve it: Conducting experiments on datasets with non-linear dynamics and comparing the performance of ContiFormer with other state-of-the-art models would provide insights into its robustness and effectiveness in such scenarios.

### Open Question 2
- Question: What are the limitations of ContiFormer when dealing with very long time series data, and how does it compare to other models in terms of scalability and computational efficiency?
- Basis in paper: [inferred] The paper mentions that ContiFormer leverages the parallelism inherent in the Transformer architecture, but it does not discuss its performance on very long time series data or its computational efficiency compared to other models.
- Why unresolved: The paper does not provide a detailed analysis of ContiFormer's scalability and computational efficiency for very long time series data.
- What evidence would resolve it: Conducting experiments on very long time series datasets and comparing the performance, scalability, and computational efficiency of ContiFormer with other state-of-the-art models would provide insights into its limitations and advantages in handling such data.

### Open Question 3
- Question: How does the choice of the interpolation function in ContiFormer affect its performance, and are there specific scenarios where alternative interpolation methods may be more suitable?
- Basis in paper: [explicit] The paper mentions that a closed-form continuous-time interpolation function (e.g., natural cubic spline) is used to approximate the underlying process, but it does not discuss the impact of different interpolation methods on the model's performance.
- Why unresolved: The paper does not provide a detailed analysis of the impact of different interpolation functions on ContiFormer's performance.
- What evidence would resolve it: Conducting experiments using different interpolation functions (e.g., linear interpolation, polynomial interpolation) and comparing the performance of ContiFormer with each method would provide insights into the importance of the interpolation function choice and its impact on the model's effectiveness in various scenarios.

## Limitations

- Scalability concerns with quadratic attention computation across long sequences may limit practical applicability
- Limited evaluation on diverse irregular time series tasks beyond classification and regression
- Missing sensitivity analysis for critical hyperparameters like quadrature points and ODE solver tolerance

## Confidence

- **High Confidence**: The theoretical framework connecting discrete attention mechanisms to the continuous-time formulation is well-grounded and rigorously proven.
- **Medium Confidence**: Experimental results show consistent improvements over baselines, but the magnitude of gains varies considerably across datasets.
- **Low Confidence**: Claims about handling highly irregular sampling patterns or severe missing data are not thoroughly validated.

## Next Checks

1. **Scalability Benchmark**: Evaluate training and inference time for ContiFormer on sequences of increasing length (N=100, 500, 1000) and compare against baseline methods, reporting both wall-clock time and memory usage.

2. **Cross-Dataset Generalization**: Test the model on diverse irregular time series tasks including forecasting with uncertainty quantification, anomaly detection, and multimodal prediction. Include datasets with different characteristics (e.g., high-dimensional multivariate series, varying sampling rates).

3. **Ablation Study on Hyperparameters**: Systematically vary the number of quadrature points, ODE solver tolerance, and interpolation method to identify their impact on performance and computational cost. Report results across multiple datasets to assess sensitivity.