---
ver: rpa2
title: Untargeted Adversarial Attack on Knowledge Graph Embeddings
arxiv_id: '2405.10970'
source_url: https://arxiv.org/abs/2405.10970
tags:
- uni00000013
- uni00000011
- rules
- uni00000014
- triples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes untargeted adversarial attacks on knowledge
  graph embeddings (KGE) that aim to reduce global link prediction performance rather
  than targeting specific triples. The method extracts logic rules from the KG and
  then either deletes triples that support high-confidence rules or adds perturbation
  triples based on corrupted low-confidence rules.
---

# Untargeted Adversarial Attack on Knowledge Graph Embeddings

## Quick Facts
- **arXiv ID**: 2405.10970
- **Source URL**: https://arxiv.org/abs/2405.10970
- **Reference count**: 40
- **Primary result**: Untargeted attacks that reduce global KG embedding performance outperform targeted attacks

## Executive Summary
This paper introduces untargeted adversarial attacks on knowledge graph embeddings that aim to degrade global link prediction performance rather than targeting specific triples. The approach leverages logic rules extracted from the KG to identify critical triples for deletion or to generate perturbation triples for addition attacks. By targeting triples that support high-confidence rules or corrupting low-confidence rules, the method achieves significant performance degradation across seven representative KGE methods. The work reveals that different KGE architectures exhibit varying robustness patterns, with fact-based methods being robust to deletion but sensitive to addition, GNN-based methods showing the opposite pattern, and rule-based methods being particularly vulnerable to adversarial additions.

## Method Summary
The method operates in two main attack strategies: deletion and addition. For deletion attacks, it extracts logic rules using the NCRL method, scores triples based on their contribution to high-confidence rules, and removes top-ranked triples within a perturbation budget. For addition attacks, it identifies low-confidence rules, disrupts them by replacing predicates based on correlation values to create negative rules, and generates perturbation triples via inference over these corrupted rules. The perturbation budget scales with KG size using a factor γ. The approach is evaluated on FB15k-237 and WN18RR datasets against seven KGE methods including TransE, DistMult, ComplEx, RGCN, CompGCN, RNNLogic, and NCRL.

## Key Results
- Untargeted attacks significantly outperform existing targeted attack baselines
- Fact-based KGE methods (TransE, DistMult, ComplEx) are robust to deletion but sensitive to addition
- GNN-based methods (RGCN, CompGCN) show opposite robustness patterns
- Rule-based methods (NCRL) are particularly vulnerable to adversarial addition attacks
- Attacks reveal varying robustness patterns across different KGE architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Untargeted attacks can reduce global KG embedding performance by targeting triples that support high-confidence logic rules.
- Mechanism: The attack extracts logic rules from the KG and removes triples that ground high-confidence rules, causing the KG to lose its most reliable structural patterns.
- Core assumption: Logic rules effectively summarize the global KG structure, so removing triples that support these rules fragments the KG's semantics.
- Evidence anchors:
  - [abstract] "Considering logic rules can effectively summarize the global structure of a KG, we develop rule-based attack strategies to enhance the attack efficiency."
  - [section 2.3] "We develop an influence scoring system that quantifies the semantic significance of each triple and removes triples assigned with high influence scores to disrupt the integration of core rules."
- Break condition: If logic rules do not accurately capture KG semantics or if high-confidence rules are not central to the KG's structure.

### Mechanism 2
- Claim: Untargeted attacks can degrade KGE performance by adding perturbation triples that corrupt low-confidence logic rules.
- Mechanism: The attack identifies low-confidence rules, disrupts them to create negative rules, and uses these to generate adversarial triples that mislead the KGE method into learning incorrect patterns.
- Core assumption: Low-confidence rules represent unreliable patterns, and corrupting them creates perturbations that are semantically inconsistent with the KG.
- Evidence anchors:
  - [abstract] "For deletion, we remove triples which support the grounding of high-confident logic rules. In the addition attack, added perturbations are expected to mislead KGE methods capturing unreliable logic rules rather than reliable ones."
  - [section 2.4] "We disrupt the extracted logic rules with low confidence values to get a series of negative rules, and then determine the perturbation triples via inferences of the negative rules over the KG."
- Break condition: If the corrupted rules do not generate semantically meaningful perturbations or if the KGE method is robust to such perturbations.

### Mechanism 3
- Claim: Different KGE methods exhibit varying robustness to untargeted attacks based on their architecture.
- Mechanism: Fact-based methods are robust to deletion but sensitive to addition, GNN-based methods show the opposite pattern, and rule-based methods are particularly vulnerable to adversarial additions that corrupt learned rules.
- Core assumption: The architecture of KGE methods determines their sensitivity to different types of perturbations.
- Evidence anchors:
  - [abstract] "The proposed untargeted attacks outperform existing targeted attack baselines and reveal that different KGE methods exhibit varying robustness - fact-based methods are robust to deletion but sensitive to addition, while GNN-based methods show the opposite pattern."
  - [section 3.5] "Fact-based KGE methods show great robustness to the deletion attack, while the robustness of Rule-based methods is weak. And all three kinds of KGE methods are competitively robust to the adversarial addition on FB15k-237."
- Break condition: If the architectural differences between KGE methods do not translate to differential robustness or if the KG structure does not influence robustness as expected.

## Foundational Learning

- **Knowledge Graph Embeddings (KGE)**: Methods that represent entities and relations as vectors to enable link prediction. Why needed: KGE methods are the target of the adversarial attacks and their performance degradation is the main metric of attack effectiveness. Quick check: What is the primary task used to evaluate KGE methods in this paper?
- **Logic Rules in KGs**: Patterns like "if X has_part Y and Y instance_of Z, then X instance_of Z" that capture structural relationships. Why needed: Logic rules are used to identify critical triples for deletion and to generate perturbation triples for addition attacks. Quick check: How are logic rules used to score the importance of triples in the deletion attack?
- **Adversarial Attacks on Graphs**: Techniques that introduce perturbations to degrade model performance. Why needed: Understanding the general concept of adversarial attacks on graph-structured data provides context for the specific approach used on KGs. Quick check: What is the key difference between targeted and untargeted attacks in this paper?

## Architecture Onboarding

- **Component map**: NCRL rule extractor -> Triple scoring module -> Deletion module OR Rule disruption module -> Perturbation generation module -> KGE evaluation
- **Critical path**: Extract rules → Score/delete triples for deletion attack OR Disrupt rules/generate perturbations for addition attack → Evaluate KGE performance
- **Design tradeoffs**: Using logic rules provides semantic insight but may be computationally expensive; focusing on high/low confidence rules targets critical structure but may miss other vulnerabilities
- **Failure signatures**: If attacks do not degrade KGE performance, it could indicate rules do not capture KG semantics well, or KGE methods are more robust than expected
- **First 3 experiments**:
  1. Implement NCRL-based rule extraction on a small KG and verify rules cover all relations
  2. Test triple scoring and deletion on a toy KG to confirm high-influence triples are removed
  3. Implement rule disruption and perturbation generation to ensure semantically inconsistent triples are created

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the robustness of knowledge graph embedding methods vary with the density and sparsity of the knowledge graph structure?
- Basis in paper: [explicit] The paper states "the robustness of methods engaged with graph neural networks and logic rules depends on the density of the graph" and finds that "KGE methods are more robust to addition attacks to deletion attacks" on WN18RR (sparser) compared to FB15k-237 (denser).
- Why unresolved: While the paper identifies that graph density affects robustness, it doesn't provide a quantitative model or threshold that predicts robustness based on density metrics.
- What evidence would resolve it: Empirical studies measuring KGE robustness across a spectrum of graph densities, identifying specific density thresholds where robustness changes, and developing predictive models of robustness based on density metrics.

### Open Question 2
- Question: Can untargeted adversarial attacks be effectively adapted to attack rule-based knowledge graph embedding methods that learn hierarchical logic rules?
- Basis in paper: [explicit] The paper finds that "rule-based methods like NCRL are easily affected by adversarial addition attacks to capture negative rules" but doesn't explore whether more sophisticated rule-based methods with hierarchical rule learning could resist such attacks.
- Why unresolved: The paper only tests one rule-based method (NCRL) and doesn't explore whether hierarchical rule learning approaches could be more robust to untargeted attacks.
- What evidence would resolve it: Comparative experiments testing untargeted attacks against multiple rule-based KGE methods with different rule learning architectures, including hierarchical approaches.

### Open Question 3
- Question: What is the optimal strategy for selecting which relations to replace when disrupting logic rules in adversarial addition attacks?
- Basis in paper: [explicit] The paper uses a correlation-based predicate rewriting strategy but notes "We design a heuristic predicate rewriting strategy based on the correlation between different relations" and compares it to random replacement, finding the correlation-based approach superior.
- Why unresolved: The paper only compares two simple strategies (correlation-based vs random) and doesn't explore whether more sophisticated selection strategies could be even more effective.
- What evidence would resolve it: Systematic comparison of various predicate selection strategies including machine learning-based approaches, ablation studies to identify the most critical factors in predicate selection, and analysis of why certain strategies outperform others.

## Limitations

- **Logic Rule Coverage**: The attack's effectiveness depends on the quality and coverage of extracted logic rules, which are not reported
- **Dataset-Specific Results**: Experiments are limited to FB15k-237 and WN18RR datasets, with unknown generalization to other KG types
- **Perturbation Budget Impact**: The paper doesn't analyze diminishing returns or the point of diminishing attack effectiveness

## Confidence

**High Confidence Claims**:
- Untargeted attacks can effectively reduce global KGE performance (supported by experimental results on two datasets)
- Different KGE architectures show varying robustness patterns (fact-based vs GNN-based vs rule-based methods)

**Medium Confidence Claims**:
- Logic rules effectively summarize global KG structure for attack purposes (assumption-based, not directly validated)
- Perturbation triples generated from corrupted rules are semantically inconsistent with the KG (mechanism plausible but not empirically verified)

**Low Confidence Claims**:
- Rule-based KGE methods are "particularly vulnerable" to adversarial additions (based on single dataset results)
- The proposed untargeted attacks significantly outperform targeted attack baselines (limited comparison scope)

## Next Checks

1. **Rule Coverage Analysis**: Measure the percentage of relations covered by extracted rules and analyze rule confidence score distributions. Verify that high-confidence rules indeed represent central KG patterns by comparing rule grounding statistics before and after triple deletion.

2. **Cross-Dataset Generalization**: Apply the same attack methodology to a biomedical knowledge graph (e.g., Hetionet) and evaluate whether the observed robustness patterns (fact-based vs GNN-based vs rule-based methods) persist across different domain-specific KGs.

3. **Semantic Consistency Verification**: For the addition attack, measure the semantic consistency of generated perturbation triples by checking entity type constraints and evaluating whether corrupted rules produce triples that violate known domain knowledge or maintain some semantic coherence.