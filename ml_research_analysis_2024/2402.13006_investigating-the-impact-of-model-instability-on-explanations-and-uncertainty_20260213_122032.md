---
ver: rpa2
title: Investigating the Impact of Model Instability on Explanations and Uncertainty
arxiv_id: '2402.13006'
source_url: https://arxiv.org/abs/2402.13006
tags:
- uncertainty
- perturbation
- explanation
- noise
- saliency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how noise injection affects model performance,\
  \ uncertainty estimates, and explanation plausibility in transformer-based language\
  \ models. By introducing realistic perturbations at inference time\u2014ranging\
  \ from character-level typos to semantic replacements\u2014the authors assess the\
  \ impact on accuracy, epistemic and predictive uncertainty, and gradient-based saliency\
  \ maps."
---

# Investigating the Impact of Model Instability on Explanations and Uncertainty

## Quick Facts
- arXiv ID: 2402.13006
- Source URL: https://arxiv.org/abs/2402.13006
- Authors: Sara Vera Marjanović; Isabelle Augenstein; Christina Lioma
- Reference count: 34
- This study investigates how noise injection affects model performance, uncertainty estimates, and explanation plausibility in transformer-based language models.

## Executive Summary
This paper explores the relationship between model instability, uncertainty estimates, and explanation quality in transformer-based language models. By introducing various perturbations at inference time—ranging from character-level typos to semantic replacements—the authors assess how accuracy, epistemic and predictive uncertainty, and gradient-based saliency maps are affected. They find that high model uncertainty doesn't necessarily imply poor explanation quality, and that noise-augmented training may improve both performance and explanation coherence under uncertainty. The study also reveals that Integrated Gradients shows the greatest robustness to perturbation for smaller models, making it optimal for human-AI collaboration.

## Method Summary
The authors fine-tune five transformer-based language models (BERTbase, RoBERTabase, ELECTRA, GPT2medium, OPT-350M) on three text classification datasets (SST-2, SemEval-2013 Task 2, HateXplain). They then apply seven types of perturbations (character insertion, swapping, butterfingers, leet speak, synonym replacement, and special token replacements) at nine levels (0.0 to 0.95) during inference. The study evaluates accuracy, predictive uncertainty (entropy of softmax logits), epistemic uncertainty (MC Dropout entropy), explanation plausibility (MAP vs human annotations), and explanation robustness (Pearson correlation between perturbed/unperturbed saliency maps) using four gradient-based saliency methods: SmoothGrad, Guided Backpropagation, InputXGradients, and Integrated Gradients.

## Key Results
- High model uncertainty doesn't necessarily imply low explanation plausibility; the correlation between the two metrics can be moderately positive when noise is exposed during training
- Integrated Gradients shows the overall greatest robustness to perturbation, while still showing model-specific patterns in performance
- Realistic perturbations like synonyms and typos have minimal impact, while special token replacements (MASK, UNK) cause significant degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Noise-augmented training improves both task performance and explanation plausibility under uncertainty
- Mechanism: By exposing the model to realistic perturbations during training, the model learns to maintain coherent explanations even when output uncertainty is high
- Core assumption: Realistic noise types (synonyms, typos) are present in the training data distribution
- Evidence anchors:
  - [abstract]: "high uncertainty doesn't necessarily imply low explanation plausibility; the correlation between the two metrics can be moderately positive when noise is exposed during the training process"
  - [section 5]: "models trained with noisy data instead show a positive relationship between uncertainty and explanation plausibility"
- Break condition: If training data lacks realistic noise patterns, the model cannot learn to generate coherent explanations under uncertainty

### Mechanism 2
- Claim: Explanation instability can indicate underlying model performance issues when uncertainty measures are overconfident
- Mechanism: When predictive and epistemic uncertainty measures fail to reflect performance degradation, the instability of saliency maps to perturbations provides additional signal about model reliability
- Core assumption: Gradient-based explanations are more faithful representations of model behavior than approximation methods
- Evidence anchors:
  - [abstract]: "when predictive and epistemic uncertainty measures are over-confident, the robustness of a saliency map to perturbation can indicate model stability issues"
  - [section 5]: "we argue that explanation instability can give some insight into model performance"
- Break condition: If gradient-based explanations are themselves unstable due to implementation issues rather than model behavior

### Mechanism 3
- Claim: Integrated Gradients shows the greatest robustness to perturbation for smaller models, making it optimal for human-AI collaboration
- Mechanism: IG's path integral approach accumulates gradients along a path from baseline to input, providing smoother attribution that is less sensitive to input perturbations
- Core assumption: Smaller models have simpler decision boundaries that IG can better navigate
- Evidence anchors:
  - [abstract]: "Integrated Gradients shows the overall greatest robustness to perturbation, while still showing model-specific patterns in performance; however, this phenomenon is limited to smaller Transformer-based language models"
  - [section 5]: "we recommend the use of Integrated Gradients for smaller language models as it gives a more holistic depiction of model performance in adversarial conditions"
- Break condition: As model size increases, other gradient methods may surpass IG in robustness due to their different regularization properties

## Foundational Learning

- Concept: Difference between epistemic and predictive uncertainty
  - Why needed here: The paper distinguishes between these uncertainty types to understand how they relate to explanation quality
  - Quick check question: If a model is uncertain about its parameters versus uncertain about a specific input, which type of uncertainty is epistemic versus predictive?

- Concept: Gradient-based explanation methods (IG, GBP, IXG, SG)
  - Why needed here: These are the explanation techniques being evaluated for robustness to noise
  - Quick check question: Which gradient-based method accumulates gradients along a path from baseline to input, making it less sensitive to input perturbations?

- Concept: Spearman rank correlation for non-normal distributions
  - Why needed here: The paper uses this correlation measure because the Kolmogorov-Smirnov test shows the data violates normality assumptions
  - Quick check question: When would you choose Spearman rank correlation over Pearson correlation in a statistical analysis?

## Architecture Onboarding

- Component map:
  Input perturbation module -> Model inference layer -> Uncertainty estimation -> Explanation generation -> Evaluation pipeline

- Critical path:
  1. Apply perturbation to input text
  2. Generate model predictions and uncertainty estimates
  3. Create saliency maps for explanations
  4. Compare perturbed vs unperturbed explanations for robustness
  5. Correlate uncertainty measures with explanation plausibility

- Design tradeoffs:
  - Using gradient-based explanations vs approximation methods like LIME (more faithful but potentially less stable)
  - Evaluating on human-annotated data vs synthetic explanations (more realistic but limited dataset availability)
  - Testing multiple perturbation hierarchies vs focusing on one type (comprehensive but computationally expensive)

- Failure signatures:
  - High correlation between uncertainty and performance degradation indicates healthy model behavior
  - Low robustness of saliency maps to realistic perturbations suggests model-specific instability
  - Positive correlation between uncertainty and explanation plausibility in noisy datasets indicates successful noise-augmented training

- First 3 experiments:
  1. Run baseline evaluation on unperturbed data to establish reference metrics
  2. Apply 5% random perturbation across all models to observe initial sensitivity
  3. Compare robustness of Integrated Gradients vs SmoothGrad on BERT with synonym perturbations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the relationship between model uncertainty and explanation plausibility change across different types of noise-augmented training data?
- Basis in paper: [explicit] The paper mentions that models fine-tuned with noisy data show a moderately positive correlation between uncertainty and explanation plausibility, but does not explore how this relationship varies with different types of noise augmentation
- Why unresolved: The study focuses on testing the effects of noise at inference time, not during training. It does not investigate how different types of noise augmentation during training (e.g., synonym replacement vs. character-level perturbations) affect the uncertainty-explanation relationship
- What evidence would resolve it: Comparing the uncertainty-explanation correlation across models trained with different noise augmentation strategies would clarify how the training data's noise profile impacts the relationship between uncertainty and explanation plausibility

### Open Question 2
- Question: Do larger language models (>1B parameters) exhibit different patterns of explanation robustness to perturbations compared to smaller models?
- Basis in paper: [inferred] The paper notes that Integrated Gradients shows the greatest robustness for smaller language models, but this phenomenon is limited to them. It also mentions that future work could explore larger models like LLaMA, but does not provide data on their robustness
- Why unresolved: The study focuses on models with fewer than 1B parameters and does not investigate how explanation robustness scales with model size
- What evidence would resolve it: Testing the robustness of different explanation techniques across a range of model sizes, including very large models, would reveal whether larger models exhibit different patterns of explanation stability under perturbations

### Open Question 3
- Question: How does aleatoric uncertainty contribute to the relationship between model uncertainty and explanation plausibility?
- Basis in paper: [explicit] The paper explicitly states that it does not investigate aleatoric uncertainty, which stems from noise inherent in the data generation process, and suggests that future work should consider disambiguating it from epistemic uncertainty
- Why unresolved: The study focuses on epistemic uncertainty by introducing noise not present in the training data, but does not account for aleatoric uncertainty, which could be significant in datasets with inherent noise (e.g., social media text)
- What evidence would resolve it: Incorporating measures of aleatoric uncertainty into the analysis and comparing its relationship with explanation plausibility across datasets with varying levels of inherent noise would clarify its role in the uncertainty-explanation relationship

## Limitations
- The findings are limited by the narrow scope of evaluated transformer models and perturbation types, which may not generalize to other architectures or languages
- The reliance on human-annotated explanation plausibility data restricts the scale of evaluation and may introduce annotation bias
- The perturbation hierarchies, while comprehensive, may not capture all real-world noise patterns

## Confidence
- High Confidence: The observation that Integrated Gradients shows superior robustness to perturbation in smaller models is well-supported by direct comparison metrics and aligns with theoretical expectations of the method's path integral approach
- Medium Confidence: The claim that high uncertainty doesn't necessarily imply poor explanation plausibility is supported by correlation analyses, but the relationship appears dataset-dependent and requires further validation across diverse domains
- Medium Confidence: The recommendation for noise-augmented training is based on moderate positive correlations in specific datasets, but causal evidence linking training noise to improved explanation coherence under uncertainty remains suggestive rather than conclusive

## Next Checks
1. Test the correlation between uncertainty and explanation plausibility on additional text classification tasks beyond the three studied datasets, particularly in domains with different noise characteristics (e.g., medical text, legal documents)
2. Evaluate whether the superiority of Integrated Gradients in smaller models holds when comparing against other explanation methods like LIME or SHAP across a broader range of model sizes and architectures, including non-transformer models
3. Assess the long-term stability of the observed relationships by evaluating model performance and explanation quality on temporally separated datasets or under concept drift conditions to test robustness over time