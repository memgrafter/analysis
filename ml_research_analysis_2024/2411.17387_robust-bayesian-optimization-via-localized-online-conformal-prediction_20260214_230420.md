---
ver: rpa2
title: Robust Bayesian Optimization via Localized Online Conformal Prediction
arxiv_id: '2411.17387'
source_url: https://arxiv.org/abs/2411.17387
tags:
- locbo
- function
- online
- optimization
- objective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LOCBO, a Bayesian optimization (BO) algorithm
  that uses localized online conformal prediction to calibrate Gaussian process (GP)
  models under misspecification. The method corrects the GP likelihood using localized
  online conformal prediction sets, then denoises this corrected likelihood to obtain
  a calibrated posterior distribution.
---

# Robust Bayesian Optimization via Localized Online Conformal Prediction

## Quick Facts
- **arXiv ID**: 2411.17387
- **Source URL**: https://arxiv.org/abs/2411.17387
- **Reference count**: 40
- **Key outcome**: LOCBO provides theoretical performance guarantees for BO under minimal noise assumptions by using localized online CP to calibrate GP models.

## Executive Summary
This paper introduces LOCBO, a Bayesian optimization algorithm that addresses model misspecification in GP surrogates by leveraging localized online conformal prediction. Unlike traditional BO methods that assume Gaussian noise, LOCBO provides theoretical guarantees under minimal assumptions about the observation noise. The method uses CP to construct prediction sets that correct the GP likelihood function, then denoises this corrected likelihood to obtain a calibrated posterior distribution. By incorporating localization, LOCBO tailors the calibration process to different regions of the input space, achieving superior performance on both synthetic and real-world optimization tasks.

## Method Summary
LOCBO uses a GP prior as the probabilistic surrogate model and performs Bayesian optimization by iteratively selecting query points that maximize an acquisition function based on the GP posterior. The key innovation is the likelihood calibration step, which uses localized online CP to correct the GP likelihood function. This involves constructing prediction sets with input-dependent thresholds via CP, then using these sets to recalibrate the likelihood so that it accounts for model misspecification. The calibrated likelihood is then denoised by integrating over the GP model to obtain a posterior distribution that incorporates both the model-based information and the calibration information. The acquisition function is computed based on this calibrated posterior to select the next query point.

## Key Results
- LOCBO provides theoretical performance guarantees for noisy observations under minimal assumptions about observation noise.
- The method demonstrates superior performance compared to state-of-the-art conformal prediction-based BO methods on synthetic and real-world tasks.
- Empirical results show significant improvements in optimization performance, particularly in settings with heteroscedastic or non-Gaussian noise.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Localized online CP corrects the GP likelihood function so that it accounts for model misspecification without requiring precise noise assumptions.
- Mechanism: At each BO round, LOCBO constructs a prediction set via localized online CP using the conformity score based on the GP predictive distribution. This set is then used to recalibrate the likelihood so that the interval has guaranteed coverage, regardless of noise distribution properties.
- Core assumption: The noise is not maximally imbalanced (b_ξ > 0), ensuring that coverage guarantees for the observation translate to coverage for the true objective.
- Evidence anchors: [abstract] "Under minimal noise assumptions, we provide theoretical performance guarantees for LOCBO's iterates that hold for the unobserved objective function." [section] Lemma 1 and Lemma 2 establish deterministic coverage for noisy observations and probabilistic coverage for the true objective function under minimal noise symmetry assumptions.

### Mechanism 2
- Claim: The denoising step converts the corrected likelihood into a calibrated posterior distribution over the objective function.
- Mechanism: LOCBO integrates the corrected likelihood over the GP model to compute a posterior that incorporates both the model-based information and the calibration information from localized online CP.
- Core assumption: The GP posterior can be evaluated in closed form when combined with the calibrated likelihood (which has a piecewise structure).
- Evidence anchors: [abstract] "LOCBO corrects the GP likelihood based on predictive sets produced by LOCBO, and the corrected GP likelihood is then denoised to obtain a calibrated posterior distribution on the objective function." [section] Lemma 3 provides the closed-form expression for LOCBO's posterior distribution by integrating over the piecewise calibrated likelihood.

### Mechanism 3
- Claim: Localization tailors the calibration process to different regions of the input space, providing more uniform coverage.
- Mechanism: LOCBO uses a threshold function λ_t(x) = g_t(x) + c_t where g_t belongs to an RKHS defined by kernel k_g. This allows the prediction set width to vary across the input space based on local data density and model uncertainty.
- Core assumption: The kernel k_g is stationary, Lipschitz, bounded, and coercive (Assumption 1).
- Evidence anchors: [abstract] "The likelihood calibration step leverages an input-dependent calibration threshold to tailor coverage guarantees to different regions of the input space." [section] Assumption 1 and Lemma 1 show that localized CP provides coverage guarantees with a localization-dependent error term κ that decreases with more localized kernels.

## Foundational Learning

- **Concept**: Gaussian Process Regression
  - Why needed here: LOCBO uses GP as the probabilistic surrogate model for the objective function, and all calibration operations are performed relative to the GP predictive distribution.
  - Quick check question: What are the mean and variance functions of a GP posterior given observations, and how do they depend on the kernel and noise parameters?

- **Concept**: Conformal Prediction
  - Why needed here: LOCBO uses conformal prediction techniques to calibrate the GP likelihood without making distributional assumptions about the noise.
  - Quick check question: How does conformal prediction construct prediction sets with guaranteed coverage, and what is the difference between weighted and online conformal prediction?

- **Concept**: Reproducing Kernel Hilbert Spaces (RKHS)
  - Why needed here: The localization mechanism in LOCBO constrains the functional component of the threshold to an RKHS, which controls the degree of localization.
  - Quick check question: What properties must a kernel have to define a valid RKHS, and how does the choice of kernel affect the function class?

## Architecture Onboarding

- **Component map**: GP Prior -> Localized Online CP -> Likelihood Calibration -> Denoising -> Acquisition Function -> Next Query
- **Critical path**: Query → Observation → GP Update → Localized CP Update → Likelihood Calibration → Denoising → Acquisition → Next Query
- **Design tradeoffs**:
  - Localization vs. conservatism: More localized kernels provide better tailoring but increase the error term κ in coverage guarantees
  - Learning rate decay: Affects convergence speed and stability of the CP threshold updates
  - Target miscoverage level α: Balances between coverage guarantees and set width (more conservative calibration)
- **Failure signatures**:
  - Poor optimization performance: May indicate insufficient localization or inappropriate kernel choice
  - Prediction sets too wide: Could suggest overly conservative calibration (high α or inappropriate kernel)
  - Prediction sets too narrow: Might indicate under-coverage due to insufficient calibration
- **First 3 experiments**:
  1. Implement basic LOCBO without localization (l = ∞) on a simple 1D objective function with known noise distribution to verify the likelihood calibration mechanism
  2. Add localization with different kernel length scales on the same 1D problem to observe the effect of localization on coverage and optimization performance
  3. Test LOCBO on a 2D synthetic function with heteroscedastic noise to validate the approach handles input-dependent noise effectively

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the length scale parameter l in the RBF kernel kg(x, x′) = exp(-||x - x'||^2/l^2) affect the trade-off between localization and coverage guarantees in LOCBO?
- Basis in paper: [explicit] The paper discusses how varying the length scale l in the RBF kernel affects the degree of localization provided by localized online CP, with smaller l corresponding to more localized kernels. It also mentions that the performance gain is not monotonic in the length scale l.
- Why unresolved: The paper does not provide a clear guideline on how to choose the optimal length scale l for different optimization problems or how to balance the trade-off between localization and coverage guarantees.
- What evidence would resolve it: Experimental results comparing LOCBO's performance with different length scale values l on various optimization problems, along with theoretical analysis of the relationship between l and coverage guarantees.

### Open Question 2
- Question: How can the hyperparameters of LOCBO, such as the target miscoverage level α and the regularization hyperparameter λ, be optimally selected for a given optimization problem?
- Basis in paper: [explicit] The paper discusses the effect of varying the miscoverage level α on the performance of CP-based BO schemes and mentions that the choice of α may not be as critical as the selection of the length scale l. However, it does not provide a clear guideline on how to choose the optimal values for these hyperparameters.
- Why unresolved: The paper does not provide a systematic approach for selecting the optimal values of α and λ for different optimization problems, and the effect of these hyperparameters on the performance of LOCBO is not fully explored.
- What evidence would resolve it: Experimental results comparing LOCBO's performance with different values of α and λ on various optimization problems, along with theoretical analysis of the relationship between these hyperparameters and the performance of LOCBO.

### Open Question 3
- Question: How can LOCBO be extended to handle multi-objective and multi-fidelity optimization problems?
- Basis in paper: [explicit] The paper mentions that extensions to multi-objective and multi-fidelity optimization problems would be of particular interest for future work.
- Why unresolved: The paper does not provide any details on how LOCBO can be adapted to handle multiple objectives or multiple fidelities, and the challenges and potential solutions for such extensions are not discussed.
- What evidence would resolve it: Development and evaluation of multi-objective and multi-fidelity versions of LOCBO on benchmark problems, along with theoretical analysis of the challenges and potential solutions for these extensions.

## Limitations

- The theoretical performance guarantees rely on the noise not being maximally imbalanced (b_ξ > 0), and this condition is not empirically validated in the experiments.
- The coverage guarantees depend on the choice of localization kernel k_g, which requires careful tuning to balance between localization benefits and the error term κ.
- The computational complexity of the denoising operation is not analyzed, and it's unclear how it scales with problem dimension and dataset size.

## Confidence

- **High confidence**: The mechanism of using localized online CP for likelihood calibration (Mechanism 1) is well-established in the CP literature, and the paper provides appropriate theoretical support through Lemma 1 and Lemma 2.
- **Medium confidence**: The denoising operation to obtain a calibrated posterior (Mechanism 2) is theoretically sound based on Lemma 3, but the empirical validation is limited to specific experimental setups.
- **Medium confidence**: The localization mechanism (Mechanism 3) is conceptually valid, but the optimal choice of kernel k_g and its parameters is not thoroughly explored in the experiments.

## Next Checks

1. Verify the noise condition b_ξ > 0 empirically by testing LOCBO on problems with different noise distributions, particularly those approaching maximal imbalance.
2. Conduct ablation studies systematically varying the localization kernel parameters (length scale, bandwidth) to understand their impact on both coverage guarantees and optimization performance.
3. Analyze the computational complexity of the denoising operation by measuring runtime and memory usage as the number of observations and input dimension increase.