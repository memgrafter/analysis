---
ver: rpa2
title: 'Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation'
arxiv_id: '2410.17462'
source_url: https://arxiv.org/abs/2410.17462
tags:
- time
- series
- annotations
- data
- tessa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TESSA, a multi-agent framework for cross-domain
  time series annotation that addresses the challenge of generating high-quality annotations
  for time series data across diverse domains. The core innovation lies in combining
  a general annotation agent that extracts common patterns from multiple source domains
  with a domain-specific annotation agent that learns specialized terminology from
  limited target domain data.
---

# Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation

## Quick Facts
- **arXiv ID**: 2410.17462
- **Source URL**: https://arxiv.org/abs/2410.17462
- **Reference count**: 40
- **Primary result**: TESSA achieves 47-65% MSE reduction in forecasting tasks and 98.51% of annotations rated higher than DirectLLM

## Executive Summary
This paper introduces TESSA, a multi-agent framework for cross-domain time series annotation that addresses the challenge of generating high-quality annotations for time series data across diverse domains. The core innovation lies in combining a general annotation agent that extracts common patterns from multiple source domains with a domain-specific annotation agent that learns specialized terminology from limited target domain data. TESSA employs both time-series-wise and text-wise feature extraction, along with adaptive feature selection using offline LLM-based and incremental reinforcement learning-based methods. Extensive experiments demonstrate that TESSA outperforms existing methods, achieving significant improvements in downstream tasks and annotation quality across multiple real-world datasets.

## Method Summary
TESSA operates through a two-agent framework: a general annotation agent that captures cross-domain patterns and a domain-specific agent that refines annotations with specialized terminology. The system extracts time-series-wise features (trends, seasonality, correlations) and text-wise features (domain-specific terminology) from multiple source domains. It then employs a hybrid feature selection strategy combining offline LLM-based selection with incremental reinforcement learning-based selection. The general agent generates comprehensive annotations that capture common patterns, while the domain-specific agent applies extracted terminology to create contextually relevant annotations for the target domain. The framework is evaluated across five real-world datasets (Stock, Health, Energy, Environment, Social Good) and a synthetic dataset with ground-truth annotations.

## Key Results
- Achieves 47% MSE reduction on Energy data (0.0482 vs 0.0575) and 65% MSE reduction on Social Good data (0.1935 vs 0.4639) compared to DirectLLM
- Overall annotation quality score of 4.64 versus 3.41 for DirectLLM, with 98.51% of TESSA annotations rated higher
- Outperforms baseline methods across downstream tasks including forecasting, imputation, and classification
- Demonstrates effectiveness in handling limited target domain annotations while maintaining annotation quality

## Why This Works (Mechanism)

### Mechanism 1: Cross-Domain Knowledge Transfer
TESSA generates higher quality annotations by leveraging cross-domain knowledge transfer through multi-modal feature extraction. It extracts both time-series-wise features (trends, seasonality, correlations) and text-wise features (domain-specific terminology) from multiple source domains, then selects the most relevant features to generate general annotations that capture common patterns across domains. The core assumption is that time series patterns across different domains share underlying commonalities that can be captured through feature extraction and transferred to target domains with limited data.

### Mechanism 2: Adaptive Feature Selection
Adaptive feature selection improves annotation quality while reducing computational costs. TESSA employs a hybrid strategy combining offline LLM-based feature selection (using importance scores) with incremental reinforcement learning-based feature selection that builds on previous knowledge without re-querying all data. The core assumption is that not all extracted features are equally important, and incremental learning can maintain feature selection quality while reducing computational overhead.

### Mechanism 3: Domain-Specific Terminology Extraction
Domain-specific annotation generation improves contextual relevance through specialized terminology extraction. TESSA uses a domain-specific term extractor to identify key terminology from limited target-domain annotations, then applies these terms to refine general annotations into domain-specific ones. The core assumption is that limited target-domain annotations contain sufficient domain-specific terminology to improve annotation relevance.

## Foundational Learning

- **Multi-modal feature extraction from time series and text**: Why needed here: TESSA needs to capture both temporal patterns and domain-specific language to generate comprehensive annotations. Quick check question: How does TESSA extract inter-variable time-series features like Pearson correlation and mutual information?

- **Reinforcement learning for feature selection**: Why needed here: The incremental RL approach maintains selection quality while reducing computational costs when new data arrives. Quick check question: What is the reward function used to train the policy networks in TESSA's incremental feature selection?

- **Domain adaptation and decontextualization**: Why needed here: Converting domain-specific terminology to general language enables cross-domain knowledge transfer. Quick check question: How does TESSA's domain decontextualizer handle terminology that is unique to specific domains?

## Architecture Onboarding

- **Component map**: Time series data → Time series feature extractor → Domain decontextualizer → Text feature extractor → Feature selectors (offline LLM + RL) → General annotator → Term extractor → Domain-specific annotator → Annotation reviewer

- **Critical path**: Time series data → Feature extraction → Feature selection → General annotation generation → Domain-specific refinement

- **Design tradeoffs**: 
  - Offline vs incremental feature selection: Accuracy vs computational efficiency
  - General vs domain-specific annotations: Broad applicability vs contextual relevance
  - LLM-based vs rule-based approaches: Flexibility vs control and interpretability

- **Failure signatures**:
  - Poor feature extraction → Annotations capture irrelevant patterns
  - Failed feature selection → Annotations become too verbose or miss key insights
  - Decontextualization errors → Loss of important domain-specific context
  - Terminology extraction failure → Domain-specific annotations lack relevance

- **First 3 experiments**:
  1. Validate feature extraction: Run TESSA on a simple time series with known patterns and verify extracted features match expectations
  2. Test feature selection: Compare annotations generated with and without adaptive feature selection to measure quality improvement
  3. Evaluate domain adaptation: Apply TESSA to a target domain with known characteristics and verify domain-specific terminology is correctly identified and applied

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the effectiveness of TESSA's annotations vary across different domain-specific terminologies and jargon complexity?
- **Basis in paper**: [explicit] The paper discusses TESSA's ability to generate domain-specific annotations using extracted terminology, but does not explore how annotation quality varies with different levels of domain-specific jargon complexity.
- **Why unresolved**: The paper only demonstrates TESSA's performance on a few real-world datasets without systematically varying the complexity of domain-specific terminology or jargon used in annotations.
- **What evidence would resolve it**: Experiments showing TESSA's performance on datasets with varying levels of domain-specific terminology complexity, or a controlled study where domain jargon complexity is manipulated while keeping other factors constant.

### Open Question 2
- **Question**: What is the optimal trade-off between the amount of source domain data and target domain data for achieving the best annotation quality in TESSA?
- **Basis in paper**: [inferred] TESSA is designed to leverage annotations from multiple source domains to improve annotations in a target domain with limited data, but the paper does not explore how varying the amount of source and target domain data affects annotation quality.
- **Why unresolved**: The experiments use fixed amounts of source and target domain data without investigating how annotation quality changes when these amounts are varied.
- **What evidence would resolve it**: A series of experiments varying the ratio of source domain to target domain data and measuring the resulting annotation quality to identify optimal data distribution strategies.

### Open Question 3
- **Question**: How does TESSA's performance compare to human experts in generating domain-specific annotations across different domains?
- **Basis in paper**: [explicit] The paper demonstrates TESSA's effectiveness through automated evaluation metrics and LLM-as-a-judge approaches, but does not compare its annotations to those produced by human domain experts.
- **Why unresolved**: The evaluation relies entirely on automated metrics and LLM-based judgment without involving human expert assessment of annotation quality.
- **What evidence would resolve it**: A human evaluation study where domain experts rate TESSA's annotations alongside those from other automated methods and compare them to human-generated annotations.

## Limitations

- The effectiveness of cross-domain knowledge transfer depends heavily on the diversity and representativeness of source domains, which isn't fully validated across diverse real-world scenarios
- Computational cost reduction claims lack empirical validation, as the paper doesn't provide direct comparisons of offline vs incremental feature selection costs
- The reliance on LLM-based components introduces potential variability that isn't addressed through robustness testing

## Confidence

- **High confidence**: The multi-agent framework architecture is well-specified and the experimental results show consistent improvements over DirectLLM baselines
- **Medium confidence**: The claimed 47-65% MSE improvements are based on specific datasets with limited domain diversity
- **Low confidence**: The computational efficiency claims lack direct empirical support and the RL-based feature selection mechanism details are underspecified

## Next Checks

1. Test TESSA's feature extraction and cross-domain transfer capabilities on additional source-target domain pairs with varying degrees of similarity to validate generalization
2. Conduct ablation studies comparing computational costs and annotation quality between offline-only and incremental RL-based feature selection approaches
3. Evaluate TESSA's robustness by testing annotation quality with varying amounts of target domain data (beyond the limited annotations mentioned) to establish sensitivity curves