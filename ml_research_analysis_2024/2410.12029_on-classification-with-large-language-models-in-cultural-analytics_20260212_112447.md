---
ver: rpa2
title: On Classification with Large Language Models in Cultural Analytics
arxiv_id: '2410.12029'
source_url: https://arxiv.org/abs/2410.12029
tags:
- often
- category
- data
- accuracy
- texts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study surveys classification use cases in cultural analytics,
  benchmarking large language models (LLMs) against traditional supervised methods
  on ten publicly available English-language tasks. Prompt-based LLMs perform competitively
  on established classification tasks (e.g., genre, haiku, emotion) but lag behind
  on de novo tasks (e.g., literary time, folktales).
---

# On Classification with Large Language Models in Cultural Analytics

## Quick Facts
- arXiv ID: 2410.12029
- Source URL: https://arxiv.org/abs/2410.12029
- Authors: David Bamman; Kent K. Chang; Li Lucy; Naitian Zhou
- Reference count: 40
- Key outcome: Prompt-based LLMs achieve competitive accuracy on established cultural analytics classification tasks but lag behind on de novo phenomena, while offering sensemaking capabilities through exploratory analysis

## Executive Summary
This study benchmarks large language models against traditional supervised methods across ten cultural analytics classification tasks in English. Prompt-based LLMs perform competitively on established concepts like genre and emotion but struggle with newly constructed phenomena like literary time and technological strangeness. The research demonstrates that LLMs can assist sensemaking by identifying category characteristics through exploratory analysis, though results require formal validation. The study highlights important tradeoffs between LLM accuracy, interpretability, and computational cost for cultural analytics applications.

## Method Summary
The researchers evaluated ten publicly available English-language cultural analytics datasets using both supervised models (logistic regression, BERT, RoBERTa, Llama 3 fine-tuning) and prompt-based approaches (GPT-4o, Llama 3 70B, Mixtral 8x22B). They employed stratified train/dev/test splits and optimized hyperparameters for supervised models while using 10-shot prompting for LLMs. Classification accuracy was measured for all tasks except literary time, which used Spearman correlation. The sensemaking analysis involved prompting LLMs to identify distinguishing features between anonymized categories.

## Key Results
- Prompt-based LLMs achieve competitive accuracy with supervised models on established classification tasks (genre, haiku, emotion)
- LLMs significantly underperform on de novo tasks involving newly constructed cultural phenomena (literary time, technological strangeness)
- LLMs can identify category characteristics through exploratory analysis, providing hypotheses for formal testing
- Supervised methods (BERT/RoBERTa) achieve comparable performance to larger LLMs on classification tasks with appropriate tuning

## Why This Works (Mechanism)

### Mechanism 1
Prompt-based LLMs can achieve competitive accuracy on established classification tasks without requiring large labeled datasets. Pre-trained LLMs leverage extensive general knowledge acquired during training to recognize patterns in established cultural concepts when provided with task descriptions and examples through prompting. This works when concepts are sufficiently represented in pre-training data.

### Mechanism 2
LLMs can assist sensemaking through exploratory analysis by identifying category characteristics from labeled data. When prompted with anonymized category labels and examples, LLMs can identify distinguishing textual features between categories, providing hypotheses about what makes categories distinct that can guide formal testing. This relies on the model's ability to reason about feature differences without knowing actual category names.

### Mechanism 3
Supervised methods (BERT/RoBERTa) can achieve comparable performance to larger LLMs on classification tasks with appropriate hyperparameter tuning. Masked language models trained with task-specific supervision can learn effective representations for classification without requiring the massive parameter counts of LLMs, making them more computationally efficient. This assumes task-specific supervision is more valuable than general pre-training knowledge for well-defined problems.

## Foundational Learning

- **Concept: Classification as sensemaking**
  - Why needed here: The paper frames classification not as mere automation but as a tool for understanding cultural categories, which is fundamental to interpreting the results
  - Quick check question: What distinguishes classification for sensemaking from classification for pure accuracy?

- **Concept: Prompt engineering vs fine-tuning**
  - Why needed here: The study compares these two approaches to using LLMs, and understanding their differences is crucial for interpreting the results
  - Quick check question: How does the amount of labeled data available influence the choice between prompting and fine-tuning?

- **Concept: De novo vs established tasks**
  - Why needed here: The paper distinguishes between tasks involving well-known concepts versus newly constructed phenomena, which explains performance differences
  - Quick check question: Why would an LLM perform well on genre classification but poorly on "technological strangeness"?

## Architecture Onboarding

- **Component map**: Data preparation (10 datasets) -> Supervised models (LR, BERT, RoBERTa, Llama 3) -> Prompt-based models (GPT-4o, Llama 3 70B, Mixtral) -> Evaluation (accuracy, Spearman) -> Sensemaking analysis (LLM feature identification)

- **Critical path**: 1) Load and preprocess dataset with stratified splits 2) Optimize supervised model hyperparameters on dev set 3) Construct prompts with 10 examples for LLM tasks 4) Evaluate on test set, compare against baseline 5) Prompt LLM for category characteristics from anonymized labels

- **Design tradeoffs**: Accuracy vs computational cost (LLMs competitive but expensive), Interpretability vs performance (linear models provide features, LLMs provide exploration), Task specificity vs general knowledge (supervised for specific, LLMs for general)

- **Failure signatures**: Memorization artifacts (GPT-4o on folktales), De novo task failure (poor performance on literary time), Hallucination in sensemaking (fabricated characteristics)

- **First 3 experiments**: 1) Bag-of-words logistic regression on animacy dataset for baseline 2) Prompt GPT-4o with 10-shot examples on genre classification 3) Test memorization detection on folktale dataset by masking 5 least probable tokens

## Open Questions the Paper Calls Out

### Open Question 1
How does pre-training data influence the sensemaking capabilities of LLMs in cultural analytics, and can this influence be systematically controlled or measured? The paper notes that pre-training data influences knowledge models bring to exploratory tasks, muddying inferences about data-in-itself. Studies comparing LLM outputs across different pre-training datasets would clarify this relationship.

### Open Question 2
Under what conditions do prompt-based LLMs outperform fine-tuned supervised models for newly constructed cultural analytics phenomena? The paper finds LLMs excel on established tasks but lag behind on de novo tasks. Comparative experiments testing LLMs on various novel phenomena with different strategies would identify optimal conditions.

### Open Question 3
Can LLM-generated category characteristics be reliably validated as meaningful dimensions for formal theory testing in cultural analytics? The paper shows potential but doesn't establish validation protocols. Empirical studies testing predictive validity of LLM-generated dimensions across multiple datasets would establish reliability.

## Limitations

- Evaluation limited to English-language datasets, constraining generalizability to multilingual applications
- Sensemaking experiments rely on subjective interpretation without systematic validation against human coding standards
- Computational cost analysis remains qualitative without concrete resource requirements for different model choices

## Confidence

**High Confidence**: Comparative performance results between prompt-based LLMs and supervised models on established classification tasks (genre, haiku, emotion) are robust with clear ground truth labels.

**Medium Confidence**: The claim that LLMs excel at tasks measuring "generally widely known" concepts is supported but requires careful interpretation as criteria for "widely known" remain somewhat subjective.

**Low Confidence**: The sensemaking utility of LLMs for identifying category characteristics is promising but under-validated, providing qualitative examples without rigorous testing of whether insights actually improve human understanding.

## Next Checks

1. **Replication on multilingual datasets**: Test the same classification pipeline on non-English cultural analytics datasets to verify whether performance patterns hold across languages and cultural contexts.

2. **Controlled sensemaking experiment**: Design a user study where analysts use LLM-generated category characteristics versus traditional exploratory methods to solve a cultural analytics problem, measuring both accuracy and time-to-insight.

3. **Cost-benefit analysis**: Conduct a systematic evaluation measuring inference costs, latency, and accuracy tradeoffs across different model sizes and prompting strategies for practical deployment scenarios.