---
ver: rpa2
title: Selecting Subsets of Source Data for Transfer Learning with Applications in
  Metal Additive Manufacturing
arxiv_id: '2401.08715'
source_url: https://arxiv.org/abs/2401.08715
tags:
- source
- data
- target
- distance
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses data insufficiency in metal additive manufacturing
  (AM) by proposing a method to select subsets of source data for transfer learning
  (TL). Current TL methods use all accessible source data without considering similarity
  to the target domain.
---

# Selecting Subsets of Source Data for Transfer Learning with Applications in Metal Additive Manufacturing

## Quick Facts
- arXiv ID: 2401.08715
- Source URL: https://arxiv.org/abs/2401.08715
- Authors: Yifan Tang; M. Rahmani Dehaghani; Pouyan Sajadi; G. Gary Wang
- Reference count: 18
- Primary result: Pareto frontier-based source data selection improves transfer learning performance in metal additive manufacturing

## Executive Summary
This paper addresses data insufficiency in metal additive manufacturing (AM) by proposing a method to select subsets of source data for transfer learning. Current TL methods use all accessible source data without considering similarity to the target domain. The proposed method uses spatial and model distance metrics to quantify similarity between source and target data, then applies a Pareto frontier-based selection approach to iteratively choose the most relevant source data. The method is integrated with instance-based TL (decision tree regression) and model-based TL (fine-tuned artificial neural network) approaches.

## Method Summary
The method calculates multiple distance metrics (Euclidean, Cosine, performance, feature) between source data points and the target dataset. A Pareto frontier is constructed from these multi-dimensional distance metrics, identifying non-dominated source data points that represent optimal trade-offs between different similarity aspects. The selection process iteratively combines these Pareto-optimal subsets with target data, trains transfer learning models via cross-validation, and selects the optimal subset. The approach is tested on regression tasks in metal AM involving different processes and machines, demonstrating that selected subsets outperform using all source data.

## Key Results
- The source data selection method generalizes across different TL methods and distance metrics
- Using a small subset of source data selected by this method outperforms using all source data
- When multiple source domains exist, selecting from one source domain provides comparable or better performance than using data from all source domains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Selecting source data via Pareto frontier optimization balances multiple similarity metrics without manual weighting.
- **Mechanism**: The method combines multiple metrics (spatial and model distances) into a multi-objective optimization framework. Source data points that are non-dominated in this metric space form the Pareto frontier, representing optimal trade-offs between different similarity aspects.
- **Core assumption**: Different distance metrics capture complementary aspects of similarity, and their conflicts can be resolved through Pareto optimality rather than linear weighting.
- **Break condition**: If the Pareto frontier becomes too sparse (few non-dominated points), the selection may not provide enough diversity or coverage.

### Mechanism 2
- **Claim**: The method generalizes across different transfer learning approaches and distance metrics.
- **Mechanism**: By decoupling the source data selection from the specific TL method, the approach can be integrated with both instance-based (I-DTR) and model-based (FT-ANN) TL methods. The selection process uses distance metrics calculated independently of the final model architecture.
- **Core assumption**: The similarity between source and target domains is independent of the specific TL method used to transfer knowledge.
- **Break condition**: If the distance metrics become incompatible with certain TL methods (e.g., instance-based TL requires identical input/output dimensions), the selection process may fail.

### Mechanism 3
- **Claim**: A smaller model using selected subset data from one source domain can outperform a larger model using all data from multiple source domains.
- **Mechanism**: By carefully selecting the most relevant source data points, the method reduces noise and irrelevant information that can degrade TL performance. This allows a simpler model architecture to achieve comparable or better results than a more complex multi-source model.
- **Core assumption**: Not all source data contributes positively to TL performance; some may introduce irrelevant patterns or noise.
- **Break condition**: If the selected subset is too small to capture the necessary variability in the target domain, performance may degrade.

## Foundational Learning

- **Concept**: Pareto optimality and multi-objective optimization
  - Why needed here: The source data selection problem involves multiple conflicting similarity metrics that need to be balanced without manual weighting.
  - Quick check question: Can you explain why a point that is not on the Pareto frontier is suboptimal for source data selection?

- **Concept**: Distance metrics for similarity measurement
  - Why needed here: The method relies on quantifying similarity between source and target domains using spatial and model-based distances.
  - Quick check question: What's the difference between Euclidean distance and Cosine distance in the context of source data similarity?

- **Concept**: Transfer learning fundamentals (instance-based vs. model-based approaches)
  - Why needed here: The source data selection method must be compatible with different TL paradigms used in metal AM applications.
  - Quick check question: How does instance-based TL differ from model-based TL in terms of data requirements and knowledge transfer?

## Architecture Onboarding

- **Component map**: Distance metric calculators -> Pareto frontier optimizer -> TL method wrappers -> Cross-validation framework -> Search strategy controller

- **Critical path**: Calculate distances → Build Pareto frontier → Select subset → Train TL model → Evaluate performance

- **Design tradeoffs**:
  - Local search (faster, less optimal) vs. exhaustive search (slower, more optimal)
  - Number of distance metrics (more comprehensive but computationally expensive)
  - Subset size (larger may capture more variability but include more noise)

- **Failure signatures**:
  - Pareto frontier contains too few points (distance metrics may be too restrictive)
  - Selected subset performs worse than using all data (distances may not capture relevant similarity)
  - Search process takes too long (too many distance combinations or inefficient optimization)

- **First 3 experiments**:
  1. Run source data selection with only Euclidean distance on TL Task 1, compare performance with using all source data
  2. Test local search vs. exhaustive search on TL Task 2 with both spatial distances
  3. Evaluate the effect of increasing the number of distance metrics from 2 to 4 on selection quality and computational cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Pareto frontier-based source data selection method compare to other distance metrics not considered in this paper, such as term distribution & word embeddings in NLP, dynamic time warping for sequences, or KL divergence for probability distribution?
- Basis in paper: [explicit] The authors mention these metrics as possibilities for future work and note that two spatial distances (Euclidean and Cosine) and two model distances (performance and feature) were considered in this paper.
- Why unresolved: The paper only tests the Pareto frontier-based method with the four distance metrics mentioned, leaving the performance of the method with other distance metrics unknown.
- What evidence would resolve it: Testing the Pareto frontier-based method with the additional distance metrics and comparing its performance to the current results.

### Open Question 2
- Question: Can the Pareto frontier-based source data selection method be integrated with more advanced neural network models, such as convolutional neural networks, long short-term memory models, or recurrent neural networks, and how would its performance compare to the current results?
- Basis in paper: [explicit] The authors mention these advanced models as possibilities for future work and note that a simple ANN model was used in this paper.
- Why unresolved: The paper only tests the Pareto frontier-based method with a simple ANN model, leaving the performance of the method with more advanced models unknown.
- What evidence would resolve it: Testing the Pareto frontier-based method with the advanced neural network models and comparing its performance to the current results.

### Open Question 3
- Question: How does the Pareto frontier-based source data selection method perform on other transfer learning tasks, such as time-series forecasting or image classification, and how does its performance compare to the current results?
- Basis in paper: [explicit] The authors mention these tasks as possibilities for future work and note that the method was only tested on metal additive manufacturing regression tasks in this paper.
- Why unresolved: The paper only tests the Pareto frontier-based method on metal additive manufacturing regression tasks, leaving its performance on other transfer learning tasks unknown.
- What evidence would resolve it: Testing the Pareto frontier-based method on the other transfer learning tasks and comparing its performance to the current results.

## Limitations
- The paper doesn't provide extensive ablation studies showing how different distance metric combinations affect performance
- The method's sensitivity to the number of points selected from the Pareto frontier is not thoroughly explored
- Results may depend heavily on the specific datasets used, particularly for the finding that one source domain can outperform multiple domains

## Confidence

- **High confidence**: The core claim that selected subsets outperform using all source data is well-supported by experimental results across multiple TL methods and tasks
- **Medium confidence**: The generalization claim across different distance metrics and TL approaches, though supported by experiments, lacks analysis of why certain metric combinations work better than others
- **Medium confidence**: The finding that one well-selected source domain can outperform multiple source domains is demonstrated but may depend heavily on the specific datasets used

## Next Checks

1. **Ablation study on distance metrics**: Systematically vary the number and types of distance metrics used in the Pareto frontier calculation to determine which combinations provide the best performance trade-offs and whether all metrics are necessary

2. **Sensitivity analysis on subset size**: Evaluate how TL performance changes as a function of the number of data points selected from the Pareto frontier to determine if there's an optimal subset size or if the relationship is monotonic

3. **Cross-domain generalization test**: Apply the selection method to a completely different domain (e.g., image classification or natural language processing) to verify if the Pareto frontier approach generalizes beyond metal AM applications