---
ver: rpa2
title: 'AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual
  Information?'
arxiv_id: '2412.02611'
source_url: https://arxiv.org/abs/2412.02611
tags:
- audio
- image
- error
- arxiv
- sound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies two key limitations in current multimodal
  large language models (MLLMs): inability to perceive basic sound characteristics
  like loudness and pitch, and insufficient evaluation of audio-visual integration
  across diverse domains and attributes. To address these, the authors propose DeafTest,
  a suite of four basic auditory tasks, and AV-Odyssey Bench, a comprehensive audio-visual
  benchmark with 4,555 multiple-choice questions spanning 26 tasks across 10 domains.'
---

# AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?

## Quick Facts
- **arXiv ID**: 2412.02611
- **Source URL**: https://arxiv.org/abs/2412.02611
- **Reference count**: 40
- **Primary result**: MLLMs achieve only 34.5% accuracy on comprehensive audio-visual benchmark, with audio understanding errors comprising majority of mistakes

## Executive Summary
This paper identifies fundamental limitations in current multimodal large language models' ability to process audio-visual information. The authors demonstrate that MLLMs struggle with basic auditory perception tasks like identifying loudness and pitch, and perform poorly on complex audio-visual reasoning across diverse domains. To address these gaps, they introduce DeafTest (basic auditory tasks) and AV-Odyssey Bench (comprehensive benchmark with 4,555 multiple-choice questions across 26 tasks in 10 domains). Results show significant performance deficits, with the best model achieving only 34.5% accuracy, suggesting that current MLLMs have fundamental perceptual limitations in audio processing despite their visual reasoning capabilities.

## Method Summary
The authors developed two complementary evaluation frameworks: DeafTest, which isolates basic auditory perception capabilities through four focused tasks testing loudness and pitch discrimination, and AV-Odyssey Bench, a comprehensive benchmark spanning 10 domains including music, environmental sounds, speech, and cross-modal reasoning tasks. The benchmark consists of 4,555 carefully constructed multiple-choice questions designed to evaluate both basic audio understanding and complex audio-visual integration. The evaluation methodology emphasizes systematic error analysis to identify whether failures stem from audio perception, visual processing, or integration challenges, providing granular insights into model limitations.

## Key Results
- MLLMs achieve only 34.5% accuracy on AV-Odyssey Bench, significantly below expected performance levels
- Audio understanding errors constitute the majority of mistakes across all evaluated models
- Performance degrades substantially on tasks requiring basic auditory perception (loudness, pitch) compared to visual-only tasks
- Error patterns indicate fundamental limitations in audio feature extraction and integration rather than domain-specific knowledge gaps

## Why This Works (Mechanism)
The paper's approach works by systematically isolating and evaluating different aspects of multimodal perception. By separating basic auditory tasks from complex audio-visual reasoning, the authors can identify whether failures stem from fundamental perceptual limitations or higher-level integration challenges. The multiple-choice format with 4,555 questions provides statistical power to detect subtle performance differences across domains and task types. The comprehensive coverage across 10 domains ensures that results are not domain-specific but reflect broader limitations in current MLLM architectures.

## Foundational Learning
- **Multimodal feature extraction**: Understanding how models process and fuse audio and visual inputs is essential for diagnosing integration failures
  - Why needed: The paper reveals that audio processing is the primary bottleneck
  - Quick check: Verify model can extract basic audio features before testing integration

- **Cross-modal attention mechanisms**: Critical for understanding how models align audio and visual representations
  - Why needed: Integration failures suggest attention mechanisms may not properly weight audio information
  - Quick check: Examine attention weights between audio and visual tokens

- **Audio representation learning**: Fundamental for understanding how models encode temporal and spectral audio features
  - Why needed: Poor performance on basic auditory tasks indicates inadequate audio representations
  - Quick check: Test model on isolated audio classification tasks

- **Benchmark construction methodology**: Important for evaluating the validity and generalizability of evaluation results
  - Why needed: The paper's conclusions depend on the benchmark's representativeness
  - Quick check: Compare results across multiple evaluation frameworks

## Architecture Onboarding

**Component Map**
Audio Encoder -> Multimodal Fusion Layer -> Language Model -> Output Layer

**Critical Path**
Audio input → Audio encoder → Cross-modal attention → Multimodal representation → Language model reasoning → Multiple-choice prediction

**Design Tradeoffs**
- Multiple-choice format enables precise measurement but may not reflect real-world application needs
- Focus on basic tasks allows isolation of fundamental limitations but may underestimate model capabilities on complex reasoning
- Comprehensive domain coverage provides generalizability but increases benchmark complexity and evaluation time

**Failure Signatures**
- Consistent underperformance on basic auditory tasks across all models indicates fundamental audio processing limitations
- Higher error rates in audio-visual integration compared to visual-only tasks suggests attention mechanisms underweight audio information
- Domain-specific performance variations reveal that some audio types are more challenging than others

**3 First Experiments**
1. Evaluate model performance on isolated audio classification tasks to confirm basic audio processing capabilities
2. Test attention weight distributions between audio and visual tokens across different task types
3. Compare performance on multiple-choice vs. open-ended question formats for the same underlying tasks

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Dataset bias concerns exist due to unclear construction methodology for the 10 domains and 26 tasks
- Task granularity may not accurately represent fundamental perceptual capabilities without detailed design specifications
- Multiple-choice format limitation constrains evaluation to constrained response spaces rather than real-world application scenarios

## Confidence
- **High confidence**: MLLMs' poor performance on basic audio tasks (34.5% accuracy) and audio understanding errors comprising majority of mistakes
- **Medium confidence**: Claim about insufficient evaluation of audio-visual integration across diverse domains and attributes
- **Low confidence**: Assertion that current MLLMs have "fundamental perceptual limitations" based solely on multiple-choice task performance

## Next Checks
1. Cross-dataset validation: Test same models on AV-Odyssey Bench and at least two other established audio-visual benchmarks to verify consistency of 34.5% accuracy finding
2. Open-ended task evaluation: Replicate key experiments using open-ended question formats rather than multiple-choice to determine impact of format constraints
3. Human baseline comparison: Establish human performance baselines on DeafTest tasks to determine whether MLLM performance reflects true perceptual limitations or task design issues