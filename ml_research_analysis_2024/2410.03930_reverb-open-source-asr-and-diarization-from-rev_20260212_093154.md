---
ver: rpa2
title: 'Reverb: Open-Source ASR and Diarization from Rev'
arxiv_id: '2410.03930'
source_url: https://arxiv.org/abs/2410.03930
tags:
- reverb
- diarization
- speech
- verbatim
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present Reverb, a suite of open-source models for speech
  recognition and speaker diarization, trained on Rev's large proprietary dataset
  of 200,000 hours of human-transcribed English audio. The ASR model uses a modified
  WeNet architecture with 280M parameters, incorporating joint CTC/attention decoding
  and a novel verbatim/non-verbatim control mechanism.
---

# Reverb: Open-Source ASR and Diarization from Rev

## Quick Facts
- arXiv ID: 2410.03930
- Source URL: https://arxiv.org/abs/2410.03930
- Reference count: 11
- Primary result: Open-source ASR model achieves WER as low as 7.64% on Earnings21 test set, outperforming Whisper large-v3 and Canary-1B

## Executive Summary
Reverb is a suite of open-source models for speech recognition and speaker diarization trained on Rev's proprietary dataset of 200,000 hours of human-transcribed English audio. The system includes a 280M parameter ASR model with joint CTC/attention architecture and innovative verbatim/non-verbatim control, plus two diarization models using Pyannote architecture. The models were evaluated on long-form test sets including Earnings21, Earnings22, Rev16, and GigaSpeech, demonstrating state-of-the-art performance on long-form speech tasks while enabling production-ready pipelines optimized for speed and memory usage.

## Method Summary
The Reverb ASR model uses a modified WeNet architecture with 280M parameters, incorporating joint CTC/attention decoding and a novel verbatim/non-verbatim control mechanism. Training data includes 200,000 hours of English audio with 120,000 hours verbatim and 80,000 hours non-verbatim transcriptions. Two diarization models were developed: v1 based on pyannote3.0 architecture and v2 using WavLM features. The production pipeline includes chunking, diarization, ASR processing per chunk, and output formatting with speed and memory optimizations.

## Key Results
- Reverb ASR achieved WER of 7.64% on Earnings21 test set, outperforming Whisper large-v3 (11.10%) and Canary-1B (10.66%)
- Verbatim control feature successfully adjusts transcription style from fully verbatim (including disfluencies) to fully clean text
- Reverb diarization v2 showed WDER scores of 0.046-0.077 across test sets
- The production pipeline processes 30-minute audio segments in under 5 minutes with optimized memory usage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The joint CTC/attention architecture with language-specific layer mechanism enables simultaneous control over transcription style (verbatim vs. non-verbatim) while maintaining ASR accuracy.
- Mechanism: The language-specific layers added to the encoder and decoder blocks learn distinct representations for verbatim and non-verbatim speech patterns, allowing the model to dynamically adjust output style based on a user-specified parameter.
- Core assumption: The training data contains sufficient examples of both verbatim and non-verbatim speech patterns to enable the model to learn distinct representations for each style.
- Evidence anchors:
  - [abstract]: "The joint CTC/ attention architecture enables experimentation wit h a vari-ety of inference modes"
  - [section]: "The joint CTC/ attention architecture enables experimentation wit h a vari-ety of inference modes, including: greedy CTC decoding, CTC preﬁx beam search (with or without attention rescoring), attention decoding, and jo int CTC/ atten-tion decoding"
  - [corpus]: Weak evidence - corpus contains both verbatim and non-verbatim data but doesn't specify the proportion or quality of style-specific examples
- Break condition: If the proportion of verbatim vs. non-verbatim training data is highly imbalanced, the model may struggle to maintain consistent performance across both styles.

### Mechanism 2
- Claim: The use of WavLM features in diarization v2 instead of SincNet features improves speaker separation and identification accuracy.
- Mechanism: WavLM's large-scale self-supervised pre-training provides richer, more generalizable speech representations that capture speaker characteristics more effectively than SincNet's filter-based approach.
- Core assumption: WavLM features contain more discriminative information about speaker identity than SincNet features across diverse acoustic conditions.
- Evidence anchors:
  - [abstract]: "Reverb diarization v2 uses WavLM [3] instead of Sin cNet [8] features"
  - [section]: "Our most precise diarization model - Reverb diarization v2 - uses WavLM instead of the SincNet feature s in the pyannote3.0 basic model"
  - [corpus]: Weak evidence - corpus contains 26,000 hours of diarization data but doesn't specify the diversity of speaker characteristics
- Break condition: If the audio contains speakers with very similar vocal characteristics or in highly reverberant environments, WavLM's advantages may diminish.

### Mechanism 3
- Claim: The joint normalization and forced-alignment process improves ASR training by simultaneously filtering poorly-aligned data and obtaining optimal segment timings.
- Mechanism: By aligning transcripts to audio during preprocessing, the system can identify and remove segments where alignment confidence is low, while also generating precise segment boundaries for training.
- Core assumption: The alignment confidence scores accurately reflect the quality of the audio-transcript match, and the forced alignment algorithm can handle the diverse acoustic conditions in the dataset.
- Evidence anchors:
  - [section]: "To prepare our data for training, we employ a joint normalization and forced-alignment process, which allows us to simultaneously ﬁlter out poorly- aligned data and get the best possible timings for segmenting the remaining a udio into shorter training segments"
  - [corpus]: Weak evidence - corpus contains diverse acoustic conditions but doesn't specify the effectiveness of the alignment filtering
- Break condition: If the alignment algorithm struggles with certain accents or recording conditions, it may incorrectly filter out valid training data or fail to identify poor-quality segments.

## Foundational Learning

- Concept: CTC (Connectionist Temporal Classification)
  - Why needed here: CTC enables the model to handle variable-length input sequences without requiring frame-level alignments, which is crucial for long-form speech recognition where alignment errors can accumulate.
  - Quick check question: How does CTC handle the alignment problem between input frames and output tokens?

- Concept: Transformer attention mechanisms
  - Why needed here: The bidirectional attention decoder allows the model to consider both past and future context when generating each output token, improving accuracy for long-form speech with complex dependencies.
  - Quick check question: What advantage does bidirectional attention provide over unidirectional attention in speech recognition?

- Concept: Speaker diarization metrics (DER vs WDER)
  - Why needed here: Understanding the difference between Diarization Error Rate (DER) and Word Diarization Error Rate (WDER) is crucial for evaluating system performance in real-world applications where both speaker attribution and transcription accuracy matter.
  - Quick check question: Why is WDER a more practical metric than DER for evaluating combined ASR and diarization systems?

## Architecture Onboarding

- Component map:
  Audio -> Encoder (18-layer Conformer) -> Joint CTC/attention -> Decoder (6-layer bidirectional transformer) -> Post-processing (WFST-based beam search + attention rescoring) -> Formatted output

- Critical path:
  Audio → Encoder → Joint CTC/attention → Decoder → Post-processing → Formatted output
  Audio → Feature extraction → Diarization model → Speaker segments → ASR on segments

- Design tradeoffs:
  - Model size (280M parameters) vs. inference speed and memory usage
  - Verbatim control complexity vs. model accuracy across styles
  - WavLM feature richness vs. computational overhead in diarization
  - Joint CTC/attention flexibility vs. inference complexity

- Failure signatures:
  - High WER on short-form speech (GigaSpeech results) suggests model is optimized for long-form
  - WDER degradation indicates issues in speaker boundary detection or ASR on overlapping speech
  - Style control inconsistencies suggest imbalanced training data or insufficient learning of style distinctions

- First 3 experiments:
  1. Compare CTC vs. attention-only decoding on a held-out test set to understand the contribution of each component
  2. Evaluate verbatim vs. non-verbatim performance on accented speech to assess style control robustness
  3. Test diarization accuracy on overlapping speech segments to identify failure modes in speaker separation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Reverb ASR's verbatim/non-verbatim control mechanism perform on domains outside the training data (e.g., medical, legal, or technical fields)?
- Basis in paper: [explicit] The paper states that the verbatim control feature was trained on 200,000 hours of English speech data, but doesn't evaluate performance on specialized domains.
- Why unresolved: The model was trained on general English speech, and the paper only evaluates on podcast and earnings call domains. Specialized domains may have different disfluency patterns or terminology that could affect the verbatim control's effectiveness.
- What evidence would resolve it: Testing Reverb ASR on domain-specific datasets (medical dictation, legal proceedings, technical lectures) and comparing verbatim/non-verbatim outputs against human-transcribed references in those fields.

### Open Question 2
- Question: What is the impact of Reverb's verbatim control mechanism on downstream NLP tasks like information extraction or question answering?
- Basis in paper: [explicit] The paper demonstrates the verbatim control feature but doesn't investigate its impact on downstream applications that consume ASR output.
- Why unresolved: While the paper shows Reverb can produce different verbatimicity levels, it doesn't explore how these different styles affect the performance of downstream NLP systems that might use the transcripts.
- What evidence would resolve it: Running downstream NLP tasks (entity extraction, sentiment analysis, QA) on Reverb outputs at different verbatimicity levels and comparing performance to baseline ASR systems.

### Open Question 3
- Question: How does Reverb's performance scale with audio length beyond the tested corpora, and what are the theoretical limits of the chunking strategy?
- Basis in paper: [inferred] The paper demonstrates strong performance on long-form audio but doesn't explore the limits of its chunking strategy or performance degradation at extreme lengths.
- Why unresolved: The evaluation uses corpora with reasonable audio lengths, but the paper doesn't investigate whether there are breaking points where the chunking strategy fails or performance degrades significantly.
- What evidence would resolve it: Testing Reverb on extremely long audio files (hours to days) and analyzing error rates, processing time, and memory usage to identify scaling limits and potential failure modes.

## Limitations
- Proprietary training data composition and preprocessing details remain opaque
- Limited evaluation on domains outside podcasts and earnings calls
- Verbatim/non-verbatim control mechanism lacks comprehensive ablation studies across diverse acoustic conditions
- Diarization performance on overlapping speech and extreme acoustic environments not thoroughly explored

## Confidence
- **High Confidence**: ASR performance metrics on Rev's internal test sets (Earnings21, Earnings22, Rev16) - the results are well-documented with specific WER scores and the comparison methodology is clear.
- **Medium Confidence**: Verbatim/non-verbatim control mechanism - while the architecture is described, the empirical validation across diverse use cases is limited.
- **Medium Confidence**: Diarization model performance - the WDER scores are reported, but the evaluation methodology and comparison with baselines could be more comprehensive.

## Next Checks
1. **Style Control Robustness Test**: Evaluate Reverb ASR's verbatim/non-verbatim control on accented English speech (e.g., Indian English, Australian English) to verify the mechanism works consistently across diverse speaker populations, not just native speakers.

2. **Long-Form Diarization Stress Test**: Test Reverb diarization v2 on audio with high speaker overlap (>30% overlap duration) and rapid speaker switching to assess its limits and identify failure modes in challenging acoustic conditions.

3. **Cross-Domain Performance Analysis**: Compare Reverb ASR performance on GigaSpeech (short-form, diverse domains) vs. Earnings test sets to quantify the model's specialization for long-form speech and identify potential degradation in short-form scenarios.