---
ver: rpa2
title: 'Competing for pixels: a self-play algorithm for weakly-supervised segmentation'
arxiv_id: '2405.16628'
source_url: https://arxiv.org/abs/2405.16628
tags:
- segmentation
- self-play
- object
- learning
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a self-play framework for weakly-supervised
  segmentation (WSS) using reinforcement learning (RL). The key idea is to gamify
  the segmentation task by pitting two agents against each other to select patches
  from an image, with the goal of minimizing over- or under-segmentation.
---

# Competing for pixels: a self-play algorithm for weakly-supervised segmentation

## Quick Facts
- **arXiv ID**: 2405.16628
- **Source URL**: https://arxiv.org/abs/2405.16628
- **Reference count**: 40
- **Primary result**: Achieves 78.9 mIoU on PASCAL VOC 2012, outperforming previous best of 77.7

## Executive Summary
This paper introduces a self-play framework for weakly-supervised segmentation using reinforcement learning. The key innovation is gamifying the segmentation task by pitting two agents against each other to select patches from an image, with rewards based on an object presence detector's scores. The competitive setup encourages effective termination strategies that minimize over- or under-segmentation. The method is evaluated on four diverse datasets and achieves significant improvements over state-of-the-art WSS methods, with particular gains in medical imaging applications.

## Method Summary
The method uses a two-agent reinforcement learning framework where agents compete to select patches from an image grid. An object presence detector (ResNet38 backbone) scores each patch, providing rewards to agents for selecting patches with high object likelihood. The game terminates when one agent believes all object-containing patches are exhausted, with final rewards based on the opponent's final patch selection. Prioritized fictitious self-play with truncated normal distribution sampling accelerates learning by selecting opponents from historic policies based on performance proximity.

## Key Results
- Achieves 78.9 mIoU on PASCAL VOC 2012, surpassing previous best of 77.7
- Outperforms state-of-the-art WSS methods on MS COCO 2014 by 3.5 mIoU points
- Demonstrates significant improvements on medical datasets: 89.5 mDSC for liver tumor and 80.1 mDSC for prostate gland segmentation
- Ablation studies confirm the importance of self-play and task-based rewards

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-play rewards termination only when all ROI-containing patches are exhausted, preventing under-segmentation.
- Mechanism: The termination reward (Rt,term) is positive if the agent calling termination has the highest object presence score; otherwise, it's negative. This creates a game-theoretic incentive to avoid early termination.
- Core assumption: Agents can learn to distinguish between ROI-containing and background patches through repeated competition.
- Evidence anchors:
  - [abstract] "Upon termination, the agent is incentivised if ROI-containing patches are exhausted or disincentivised if an ROI-containing patch is found by the competitor."
  - [section] "If the game is not terminated appropriately and both sides keep selecting patches with no ROIs, the opposing side may win the game based on chance..."
  - [corpus] Weak evidence: Corpus neighbors focus on weakly supervised segmentation but don't directly address self-play termination strategies.
- Break condition: If object presence detector scores are too similar between patches, termination decisions become noisy and ineffective.

### Mechanism 2
- Claim: Competition ensures patches with highest object presence scores are selected, minimizing over-segmentation.
- Mechanism: Patch reward (Rt,patch) gives +1 to the agent selecting the patch with higher object presence score. Over multiple games, agents learn to consistently select the most informative patches.
- Core assumption: The pre-trained object presence detector provides reliable scores for patch selection.
- Evidence anchors:
  - [abstract] "The score at each time-step, used to compute the reward for agent training, represents likelihood of object presence within the selection, determined by an object presence detector..."
  - [section] "Each patch is scored by the pre-trained object presence detector... and the agent with a higher score wins the turn and is given a positive reward signal."
  - [corpus] No direct evidence in corpus about competitive patch selection mechanisms.
- Break condition: If object presence detector is poorly calibrated, agents may learn to select suboptimal patches.

### Mechanism 3
- Claim: Prioritized fictitious self-play accelerates learning by sampling opponents from a truncated normal distribution.
- Mechanism: Historic policies are sampled with higher probability from policies closer to current performance (truncated normal distribution), avoiding cycles and local optima.
- Core assumption: Opponent sampling distribution affects learning efficiency and convergence to effective policies.
- Evidence anchors:
  - [section] "For prioritised fictitious self-play, we propose to use a truncated normal distribution for competitor sampling from historic policies..."
  - [section] "This strategy is likely to select opponents that are weaker than the agent but still allows much weaker or the best response to be sampled..."
  - [corpus] Weak evidence: Corpus contains related weakly supervised segmentation papers but none discussing prioritized fictitious self-play specifically.
- Break condition: If truncation bounds are too narrow, learning diversity decreases; if too wide, efficiency gains diminish.

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation of RL environments
  - Why needed here: The self-play framework is built on MDP principles where states, actions, rewards, and policies define the learning problem
  - Quick check question: What are the four components that define an MDP environment in this paper?

- Concept: Multi-agent reinforcement learning and zero-sum games
  - Why needed here: The self-play framework involves two competing agents where one agent's gain is the other's loss
  - Quick check question: How does the paper ensure the self-play setup is a zero-sum game?

- Concept: Object presence detection using image-level classification
  - Why needed here: The weak supervision signal comes from an object presence detector trained only on image-level labels
  - Quick check question: What type of labels does the object presence detector use for training, and how are these labels used during inference?

## Architecture Onboarding

- Component map:
  - Object presence detector (ResNet38 backbone) → scores patches
  - Two RL agents → select patches and decide termination
  - Game environment → manages state transitions and reward calculation
  - Prioritized fictitious self-play sampling → selects historic opponents

- Critical path: Image → grid division → patch selection sequence → termination → segmentation map generation

- Design tradeoffs:
  - Patch size vs. detector performance: Larger patches reduce computation but may miss small objects
  - Termination timing vs. segmentation completeness: Early termination risks under-segmentation, late termination risks over-segmentation
  - Opponent sampling distribution vs. learning efficiency: Narrower distribution speeds learning but may reduce policy robustness

- Failure signatures:
  - High false negative rate → agents terminating too early
  - High false positive rate → agents failing to terminate when all ROI patches are selected
  - Low mIoU across datasets → object presence detector not reliable for patch scoring

- First 3 experiments:
  1. Verify object presence detector performance on held-out patches from training images
  2. Test single-agent variant with fixed threshold (0.8) vs. learned policy on validation set
  3. Compare mIoU with varying patch sizes (2×2 to 16×16) on VOC validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance change if the object presence detector was trained jointly with the agents instead of being pre-trained?
- Basis in paper: [explicit] The paper mentions that future development could explore an adaptive object presence detector trained together with the agents for optimal performance for a given patch size.
- Why unresolved: The current method uses a pre-trained object presence detector, and the impact of joint training on performance is not evaluated.
- What evidence would resolve it: Experiments comparing the performance of the proposed method with and without joint training of the object presence detector and agents.

### Open Question 2
- Question: How does the proposed method perform on datasets with larger or more complex ROIs?
- Basis in paper: [inferred] The paper demonstrates performance on datasets where the ROI is smaller than half the FOV, but does not explore performance on larger or more complex ROIs.
- Why unresolved: The method's effectiveness on diverse ROI sizes and complexities is not fully explored.
- What evidence would resolve it: Experiments evaluating the method on datasets with larger or more complex ROIs, comparing performance to other methods.

### Open Question 3
- Question: How sensitive is the method to the choice of hyper-parameters, such as the patch size and the termination reward weight?
- Basis in paper: [explicit] The paper reports results for varying patch sizes and mentions that the weights were tuned on a small subset of the PASCAL VOC 2012 train set.
- Why unresolved: The sensitivity of the method to hyper-parameter choices is not thoroughly explored, and the impact of different values is not fully understood.
- What evidence would resolve it: Extensive ablation studies and sensitivity analysis to determine the impact of different hyper-parameter values on performance.

## Limitations
- The method's effectiveness relies heavily on the reliability of object presence detector scores, which may vary across datasets
- Computational overhead of maintaining historic policy samples and running two competing agents is not discussed
- Limited ablation studies on the impact of prioritized fictitious self-play sampling distribution on final performance

## Confidence
- **High confidence**: The fundamental self-play framework and MDP formulation are well-established in RL literature, and the competitive patch selection mechanism is clearly explained and empirically validated on multiple datasets
- **Medium confidence**: The termination strategy and prioritized fictitious self-play sampling show theoretical promise but require more extensive validation, particularly regarding sensitivity to hyperparameter choices
- **Low confidence**: Claims about generalization across diverse datasets (from natural images to medical scans) without dataset-specific tuning or detector adaptation

## Next Checks
1. **Detector reliability test**: Evaluate object presence detector mAP on held-out patches from each dataset before RL training to quantify the reliability of the supervision signal
2. **Single-agent baseline**: Implement and evaluate a single-agent variant with fixed termination threshold (0.8) against the learned self-play policy to isolate the contribution of competitive learning
3. **Sampling distribution ablation**: Compare mIoU performance using uniform sampling, truncated normal sampling (as proposed), and exponential decay sampling for opponent selection in the self-play framework