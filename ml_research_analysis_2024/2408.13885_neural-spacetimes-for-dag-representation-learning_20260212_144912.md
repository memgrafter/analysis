---
ver: rpa2
title: Neural Spacetimes for DAG Representation Learning
arxiv_id: '2408.13885'
source_url: https://arxiv.org/abs/2408.13885
tags:
- neural
- embedding
- metric
- space
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Neural Spacetimes (NSTs) are proposed as a trainable deep learning-based\
  \ geometry for embedding nodes in weighted directed acyclic graphs (DAGs) into spacetime\
  \ manifolds. Unlike prior approaches that use fixed spacetime geometries, NSTs leverage\
  \ three neural networks\u2014an encoder, a neural (quasi-)metric for spatial dimensions,\
  \ and a neural partial order for temporal dimensions\u2014to learn a data-driven\
  \ geometry that preserves both edge weights (via spatial distance) and edge directionality\
  \ (via causality)."
---

# Neural Spacetimes for DAG Representation Learning

## Quick Facts
- arXiv ID: 2408.13885
- Source URL: https://arxiv.org/abs/2408.13885
- Reference count: 40
- Primary result: Neural Spacetimes learn data-driven spacetime embeddings for DAGs that outperform fixed-geometry baselines on both embedding quality and downstream node classification tasks

## Executive Summary
Neural Spacetimes (NSTs) propose a novel approach to DAG representation learning by embedding nodes into trainable spacetime manifolds. Unlike previous methods using fixed Minkowski or De Sitter geometries, NSTs learn both spatial distances and causal structure through three neural networks operating on a product manifold of quasi-metric (space) and partial order (time) components. The model achieves universal embedding capability for finite DAGs with 1+O(log k) distortion while preserving causal structure, and demonstrates improved performance on synthetic and real-world DAG datasets compared to fixed-geometry baselines.

## Method Summary
NSTs consist of three trainable components: an encoder that maps node features to spacetime, a neural quasi-metric that learns spatial distances, and a neural partial order that enforces causal structure. The encoder produces D+T dimensions where D dimensions learn adaptive non-Euclidean distances via the neural quasi-metric (using fractalesque activation functions), and T dimensions learn causal ordering via the neural partial order (using product order structure). The model optimizes locally (one-hop neighborhoods) using combined distance and causality loss, with trainable positive weight matrices to ensure invertibility. This product manifold approach allows independent optimization of spatial and temporal components while maintaining their interaction through the embedding process.

## Key Results
- NSTs achieve lower embedding distortion (average and maximum distance ratios) than fixed-geometry Minkowski and De Sitter baselines on synthetic DAGs
- NSTs preserve causal structure more accurately, with higher directionality accuracy on synthetic and real-world DAGs
- Downstream node classification on heterophilic graphs shows improved accuracy when using NST-derived features compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neural Spacetimes learn both spatial distances and causal directionality simultaneously by using a product manifold of quasi-metric (space) and partial order (time).
- Mechanism: The encoder maps graph nodes to a latent Euclidean space, then the spatial component uses a neural (quasi-)metric to learn adaptive non-Euclidean distances, while the temporal component uses a neural partial order to enforce causality. This allows the model to preserve edge weights as distances and edge directions as causal order.
- Core assumption: The product structure (space × time) can represent all relevant DAG properties, and that differentiability is preserved across both components.
- Evidence anchors:
  - [abstract] "encode both graph edge weights in its spatial dimensions and causality in the form of edge directionality in its temporal dimensions"
  - [section 3.1] "The encoder network, E : RN → RD+T, maps the original node feature vectors of the input graph to the dimensions of space and time used by the NST representation"
- Break Condition: If the encoder cannot separate relevant spatial vs temporal information, or if the quasi-metric and partial order interfere with each other, embedding quality will degrade.

### Mechanism 2
- Claim: Neural (quasi-)metrics implement a trainable fractal-like geometry that can represent arbitrary finite metric spaces with low distortion.
- Mechanism: The neural (quasi-)metric uses iterative layers with fractalesque activation functions (e.g., σs,l(x) = sgn(x)|x|s for |x|<1 and sgn(x)|x|l for |x|≥1) to warp Euclidean distances. This allows the model to implement snowflake-like metrics that can approximate any finite metric space arbitrarily well.
- Core assumption: Fractal activation functions can approximate the necessary metric distortions, and the iterative structure provides sufficient expressiveness.
- Evidence anchors:
  - [section 3] "Our approach decouples the representation into a product manifold, which models (quasi-)metrics and (partial) orders independently"
  - [section C.1] "Neural spacetimes leverage the following equation: σs,l(x)..."
- Break Condition: If the fractal structure cannot capture the necessary metric complexity, or if the number of layers/parameters is insufficient for the graph size.

### Mechanism 3
- Claim: Neural partial orders can enforce causal structure by learning a multi-dimensional temporal embedding where causal precedence is preserved.
- Mechanism: The neural partial order maps encoded features to a temporal space and uses a product order (T(x) ≤ T(y) component-wise) to enforce causality. The LeakyReLU composition with fractalesque activations allows flexible ordering while maintaining differentiability.
- Core assumption: Multi-dimensional time can encode all necessary causal relationships, and the product order structure is sufficient for this purpose.
- Evidence anchors:
  - [abstract] "causal structure (which includes modelling lack of connectivity as anti-chains)"
  - [section C.3] "SteepSigmoid(x) tends to 0 faster than Sigmoid(x) as x → −∞"
- Break Condition: If the temporal dimensionality is insufficient to separate anti-chains, or if the learned ordering conflicts with spatial distances.

## Foundational Learning

- Concept: Partial orders and posets
  - Why needed here: The causal structure of DAGs is fundamentally a partial order, and the embedding must preserve this structure through the neural partial order component.
  - Quick check question: What are the three defining properties of a partial order, and how do they relate to DAGs?

- Concept: Quasi-metrics and metric embeddings
  - Why needed here: Spatial distances in the embedding must approximate the edge weights while being flexible enough to handle non-Euclidean graph structures.
  - Quick check question: How does a quasi-metric differ from a metric, and why is this distinction important for NSTs?

- Concept: Product manifolds and product orders
  - Why needed here: The space-time product structure allows separate optimization of spatial and temporal components while maintaining their interaction.
  - Quick check question: What is the product order on R^T, and how does it relate to the neural partial order in NSTs?

## Architecture Onboarding

- Component map: Encoder (RN → RD+T) → Spatial Network (neural quasi-metric on RD) + Temporal Network (neural partial order on RT) → Distance + Causality outputs
- Critical path: Graph features → Encoder → Spatial/Temporal networks → Distance and causality predictions → Loss computation → Gradients → Parameter updates
- Design tradeoffs: Using a product manifold allows independent optimization but may miss interactions between space and time; fractal activations provide expressiveness but can be unstable; local optimization is tractable but may not capture global structure perfectly.
- Failure signatures: High distortion despite low causality loss suggests spatial component issues; perfect causality but poor distance suggests temporal component issues; training instability suggests activation function problems.
- First 3 experiments:
  1. Train NST on synthetic DAGs with known metric structure to verify spatial embedding capability
  2. Test causality preservation on DAGs with clear anti-chain structures to verify temporal component
  3. Vary temporal dimensionality to find minimum sufficient for causality preservation on test graphs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the number of time dimensions required for NSTs scale with the width of the DAG beyond the worst-case scenario (i.e., can fewer dimensions suffice for DAGs with specific structural properties)?
- Basis in paper: [inferred] The paper states NSTs require width number of time dimensions, but also shows that for DAGs with planar Hasse diagrams, two time dimensions suffice (Theorem 2). This implies potential for reduced dimensionality in other cases.
- Why unresolved: The paper only explicitly addresses planar Hasse diagrams and the general worst-case. A systematic characterization of DAG classes where fewer time dimensions are sufficient is missing.
- What evidence would resolve it: A theorem or empirical study showing a relationship between DAG structural properties (e.g., maximum in-degree, out-degree, treewidth) and the minimum number of time dimensions needed for a valid embedding.

### Open Question 2
- Question: What is the impact of optimizing the neural (quasi-)metric and neural partial order locally versus globally on the embedding quality for large-scale DAGs?
- Basis in paper: [explicit] The paper acknowledges local optimization is used due to computational constraints and discusses that local transitivity of causal connectivity holds globally, but does not empirically compare local vs. global optimization.
- Why unresolved: The paper only provides theoretical guarantees for both local and global embeddings, but does not experimentally validate the trade-off between computational efficiency and embedding quality when using local optimization.
- What evidence would resolve it: An experiment comparing the embedding distortion and causality preservation of NSTs trained with local optimization versus those trained with global optimization (on small DAGs where global optimization is feasible) for graphs of increasing size.

### Open Question 3
- Question: How do NSTs compare to other graph representation learning methods (e.g., hyperbolic neural networks, graph neural networks) on downstream tasks beyond node classification, such as link prediction or graph classification, particularly for DAGs?
- Basis in paper: [explicit] The paper mentions node classification on heterophilic graphs as a downstream application and shows improved performance with NST features, but does not compare against other graph representation learning methods on a broader set of downstream tasks.
- Why unresolved: The experiments focus on embedding quality and a single downstream task. A comprehensive comparison of NSTs to other methods on a variety of DAG-specific downstream tasks is lacking.
- What evidence would resolve it: An experimental study evaluating NSTs against hyperbolic neural networks, graph neural networks, and other relevant methods on tasks like link prediction, graph classification, and causal inference for DAGs, reporting both accuracy and computational efficiency.

## Limitations
- Theoretical universal approximation guarantees are asymptotic and may not reflect practical performance on finite graphs
- Local optimization approach may miss global DAG structure, particularly for long-range causal relationships
- Fractalesque activation functions with exponents < 1 are theoretically powerful but numerically unstable in practice

## Confidence
- High Confidence: The experimental methodology is sound, with appropriate synthetic baselines and real-world validation. The core technical implementation (encoder + neural quasi-metric + neural partial order) is clearly specified and reproducible.
- Medium Confidence: The distortion and causality metrics are well-defined, but their relationship to downstream task performance is indirect. The improvement in node classification accuracy on heterophilic graphs is modest and could be influenced by other factors beyond spacetime geometry.
- Low Confidence: The claimed universal approximation property for arbitrary DAGs is theoretically interesting but its practical significance for real-world graphs with cycles, noise, and non-ideal properties is questionable.

## Next Checks
1. **Stress Test Temporal Dimensionality**: Systematically vary T (temporal dimensions) on synthetic DAGs with known anti-chain complexity to find the minimum sufficient dimensionality for perfect causality preservation.

2. **Global Structure Evaluation**: Implement a global optimization variant (e.g., sampling multiple hops or using hierarchical approaches) and compare distortion metrics against the local optimization baseline on medium-sized graphs (k=200-500).

3. **Cross-Domain Transfer**: Train NST on one domain (e.g., synthetic metrics) and evaluate transfer to structurally similar but distributionally different domains (e.g., real-world citation networks) to assess the learned geometry's generalizability.