---
ver: rpa2
title: 'ALPS: Improved Optimization for Highly Sparse One-Shot Pruning for Large Language
  Models'
arxiv_id: '2406.07831'
source_url: https://arxiv.org/abs/2406.07831
tags:
- pruning
- sparsegpt
- wanda
- dsnot
- sparsity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "ALPS introduces an optimization-based framework for one-shot unstructured\
  \ pruning of large language models that directly solves an \u21130-constrained layer-wise\
  \ reconstruction problem using the operator splitting technique (ADMM). The method\
  \ simultaneously finds a high-quality support and updates weights, followed by a\
  \ preconditioned conjugate gradient post-processing step that leverages GPU parallelism\
  \ and vectorization for efficiency."
---

# ALPS: Improved Optimization for Highly Sparse One-Shot Pruning for Large Language Models

## Quick Facts
- arXiv ID: 2406.07831
- Source URL: https://arxiv.org/abs/2406.07831
- Reference count: 40
- One-line primary result: ALPS achieves 13% lower perplexity and 19% better zero-shot benchmark performance than baselines when pruning OPT-30B at 70% sparsity

## Executive Summary
ALPS introduces an optimization-based framework for one-shot unstructured pruning of large language models using operator splitting (ADMM) to directly solve an ℓ0-constrained layer-wise reconstruction problem. The method iteratively alternates between weight updates and sparse support projection, followed by a preconditioned conjugate gradient post-processing step for efficient weight refinement. ALPS outperforms heuristic pruning methods by 13% in perplexity and 19% in zero-shot performance on OPT-30B at 70% sparsity.

## Method Summary
ALPS formulates layer-wise pruning as an ℓ0-constrained reconstruction problem and solves it using ADMM with a novel penalty parameter update scheme. The algorithm alternates between weight updates (quadratic) and sparse support projection, then applies preconditioned conjugate gradient post-processing to refine weights on the fixed support using GPU parallelization. The method operates layer-by-layer on pre-trained LLMs and requires only calibration data for pruning decisions.

## Key Results
- 13% reduction in test perplexity on WikiText dataset compared to existing methods
- 19% improvement in zero-shot benchmark performance when pruning OPT-30B at 70% sparsity
- Outperforms SparseGPT, Wanda, and DSnoT across multiple sparsity levels and model families

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Operator splitting with ADMM solves the ℓ0-constrained pruning problem without heuristics.
- Mechanism: Reformulates layerwise pruning as an augmented Lagrangian with two coupled variables (weights W and sparse copy D). ADMM iteratively solves W-update (quadratic) and D-update (sparsity projection), then synchronizes via dual variable.
- Core assumption: ℓ0-constrained quadratic problem can be split into alternating subproblems solvable in closed form.
- Evidence anchors:
  - [abstract] "ALPS introduces an optimization-based framework... that directly solves an ℓ0-constrained layer-wise reconstruction problem using the operator splitting technique (ADMM)."
  - [section 3.2] "We reformulate problem (1) by introducing a copy D of weight matrix W... This reformulation separates the objective function into two independent parts while coupling the variables W and D through the linear constraint W=D."
  - [corpus] Weak signal: No direct citations but method aligns with known ADMM uses in sparsity problems.
- Break condition: If projection onto k-sparse set is unstable or ρ cannot be tuned for convergence, the alternating updates fail.

### Mechanism 2
- Claim: ρ-update scheme balances sparsity quality and convergence speed.
- Mechanism: Starts with small ρ to explore good support, then increases ρ proportionally to support change (st), stabilizing D early and accelerating convergence.
- Core assumption: Support exploration with low ρ yields better initial sparsity pattern; gradual ρ growth prevents oscillation while preserving quality.
- Evidence anchors:
  - [section 3.2] "A small ρ leads to slow convergence due to large changes in the support of D across iterations, while a large ρ may compromise solution quality though the support stabilizes early on. To balance support quality and convergence speed, we introduce a novel penalty parameter update scheme."
  - [section 3.2] "Specifically, we set ρt+1 = 1.3ρt if st ≥0.1k, 1.2ρt if st ≥0.005k, 1.1ρt if st ≥1."
  - [corpus] No explicit citations but adaptive penalty updates are common in ADMM for non-convex problems.
- Break condition: If st does not decrease, ρ may increase too slowly or too fast, causing divergence or premature convergence.

### Mechanism 3
- Claim: Preconditioned Conjugate Gradient (PCG) post-processing refines weights on fixed support efficiently.
- Mechanism: Fixes support from ADMM, solves W-update quadratic exactly with PCG vectorized across all columns; leverages GPU parallelism and sparse structure to avoid costly backsolve.
- Core assumption: Once support is stable, quadratic subproblem can be solved exactly; PCG converges rapidly on sparse Hessian structure.
- Evidence anchors:
  - [abstract] "followed by a preconditioned conjugate gradient post-processing step that leverages GPU parallelism and vectorization for efficiency."
  - [section 3.3] "we employ a Preconditioned Conjugate Gradient (PCG) method with GPU parallelism and vectorization for efficient computation... we solve the entire problem in a single pass by directly solving the linear equation HW=H cW using PCG."
  - [section 4.1] Table comparing "ALPS" vs "Backsolve" shows 20x-200x speedup with similar error.
  - [corpus] No direct citations but PCG on sparse matrices is standard.
- Break condition: If Hessian is too ill-conditioned or preconditioner fails, PCG convergence degrades.

## Foundational Learning

- Concept: Operator Splitting (ADMM)
  - Why needed here: Allows decoupling of ℓ0 sparsity constraint from quadratic reconstruction objective, enabling alternating updates without heuristics.
  - Quick check question: What two subproblems does ADMM create from the ℓ0-constrained layerwise pruning problem?

- Concept: ℓ0-norm and projection onto k-sparse set
  - Why needed here: The sparsity constraint is combinatorial; projection operator Pk picks top-k magnitude entries each iteration.
  - Quick check question: How does the projection Pk(·) enforce the ℓ0 constraint in the D-update?

- Concept: Preconditioned Conjugate Gradient for sparse systems
  - Why needed here: Efficiently solves large linear systems arising in W-update on fixed support without explicit matrix inversion.
  - Quick check question: Why is vectorizing across columns faster than solving Nout separate linear systems?

## Architecture Onboarding

- Component map: Calibration data X -> ADMM loop (W-update, D-update, V-update + ρ adaptation) -> Support stabilization check -> PCG post-processing (vectorized, GPU-accelerated) -> Sparse weight matrix W
- Critical path: ADMM → Support stabilization → PCG → Output
- Design tradeoffs:
  - ADMM vs heuristic: More compute but higher pruning objective quality.
  - ρ schedule: Trade-off between support exploration and convergence speed.
  - PCG vs direct solve: Lower memory but requires good preconditioner.
- Failure signatures:
  - ADMM: Oscillating support, ρ too high/low, no convergence in W-update.
  - PCG: Slow convergence, preconditioner ineffective, GPU memory overflow.
- First 3 experiments:
  1. Run ADMM on a single small linear layer with known optimal support; verify convergence to near-optimal reconstruction error.
  2. Vary ρ update thresholds; observe effect on support stability and final objective.
  3. Replace PCG with PyTorch torch.linalg.solve on fixed support; compare runtime and error.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does ALPS's performance advantage over heuristics like SparseGPT and Wanda scale consistently to larger LLMs (e.g., models with 100B+ parameters)?
- Basis in paper: [inferred] The paper reports results on OPT-30B and LLaMA3-8B, but scaling behavior for significantly larger models is unexplored.
- Why unresolved: Computational cost and memory constraints make it difficult to validate ALPS on extreme-scale LLMs within the paper's experimental scope.
- What evidence would resolve it: Empirical results on 100B+ parameter models showing perplexity and zero-shot benchmark performance relative to other methods.

### Open Question 2
- Question: How does ALPS's convergence behavior change with different sparsity patterns beyond unstructured and N:M sparsity, such as block sparsity or hierarchical sparsity?
- Basis in paper: [explicit] The authors state that ALPS can be extended to other sparsity patterns by modifying the D-update step, but convergence guarantees and performance are not provided for these cases.
- Why unresolved: The theoretical analysis and experimental validation are limited to unstructured and N:M sparsity; other structured patterns require separate investigation.
- What evidence would resolve it: Convergence proofs and experimental comparisons on block or hierarchical sparsity patterns.

### Open Question 3
- Question: What is the impact of ALPS's layer-wise optimization on global model performance, especially for models with strong inter-layer dependencies like transformers?
- Basis in paper: [inferred] ALPS uses a layer-wise approach, but the paper does not investigate how this decomposition affects end-to-end model behavior or interactions between layers.
- Why unresolved: Layer-wise optimization may not fully account for cross-layer effects, which could influence final model accuracy.
- What evidence would resolve it: Ablation studies comparing layer-wise vs. global optimization on model performance metrics.

## Limitations

- Exact implementation details of the ρ-update scheme (step function thresholds and increment values) are only partially specified, creating uncertainty in faithful reproduction.
- PCG post-processing speedups are demonstrated but preconditioner design and GPU parallelization strategy lack complete description.
- Method's behavior on extreme sparsity levels (>80%) and sensitivity to calibration data quality remain unknown.

## Confidence

- **High Confidence**: The core ADMM framework for ℓ0-constrained layerwise reconstruction is well-established and the paper provides sufficient detail for implementation. The reported improvements in perplexity (13%) and zero-shot performance (19%) are statistically significant and methodologically sound.
- **Medium Confidence**: The ρ-update scheme's adaptive thresholds are described but lack complete specification. The PCG post-processing speedups are demonstrated but implementation details are sparse. The generalization across multiple model families (OPT, LLaMA2, LLaMA3) suggests robustness but individual model sensitivities are not explored.
- **Low Confidence**: The method's behavior on extreme sparsity levels (>80%) and its sensitivity to calibration data quality remain unknown. The paper does not address potential issues with layerwise decoupling when dependencies exist across layers.

## Next Checks

1. **Support Stability Analysis**: Implement the ADMM algorithm with varying ρ-update thresholds and monitor support stability metrics (st) across iterations. Compare reconstruction error trajectories to identify optimal threshold values for different sparsity targets.

2. **Preconditioner Sensitivity**: Test multiple preconditioning strategies (diagonal scaling, incomplete Cholesky, block-Jacobi) for the PCG solver on fixed support problems. Measure convergence rates and runtime across different layer types (attention vs MLP) to identify optimal configurations.

3. **Layer Coupling Effects**: Apply ALPS to a small multi-layer transformer with interlayer dependencies, then compare results against sequential layerwise application. Quantify performance degradation to establish bounds on the layerwise independence assumption.