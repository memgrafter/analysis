---
ver: rpa2
title: Robust Preference Optimization through Reward Model Distillation
arxiv_id: '2405.19316'
source_url: https://arxiv.org/abs/2405.19316
tags:
- reward
- preference
- learning
- policy
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of language model alignment from
  preference data, where the widely-used Direct Preference Optimization (DPO) method
  can produce degenerate policies with zero or near-zero probability for preferred
  responses due to overfitting and lack of regularization. The core method idea is
  to use explicit reward modeling and reward model distillation, training the policy
  to match the implicit reward (scaled log-likelihood ratio) to an explicit reward
  model trained on preference data, and extending this with pessimism over a family
  of reward models to handle uncertainty.
---

# Robust Preference Optimization through Reward Model Distillation

## Quick Facts
- arXiv ID: 2405.19316
- Source URL: https://arxiv.org/abs/2405.19316
- Authors: Adam Fisch; Jacob Eisenstein; Vicky Zayats; Alekh Agarwal; Ahmad Beirami; Chirag Nagpal; Pete Shaw; Jonathan Berant
- Reference count: 40
- This paper addresses DPO's tendency to produce degenerate policies with zero or near-zero probability for preferred responses, proposing reward model distillation with pessimism to improve robustness

## Executive Summary
This paper tackles a fundamental weakness in Direct Preference Optimization (DPO) - its tendency to produce degenerate policies that assign zero or near-zero probability to preferred responses due to overfitting and lack of regularization. The authors propose a novel approach that uses explicit reward modeling and reward model distillation, where the policy is trained to match an implicit reward (scaled log-likelihood ratio) to an explicit reward model trained on preference data. To handle uncertainty in reward modeling, they extend this with pessimism over a family of reward models. The approach shows improved robustness and better performance than DPO and Identity Preference Optimization (IPO) in biased preference datasets, while also yielding modest gains in unbiased settings.

## Method Summary
The core method introduces explicit reward modeling as an alternative to DPO's implicit reward structure. The approach trains an explicit reward model on preference data, then distills this reward into the policy through likelihood matching. To address uncertainty in reward estimation, the method employs pessimism over an ensemble of reward models, selecting the most conservative (lowest) reward prediction during training. This pessimistic ensemble approach balances exploration and exploitation while avoiding overconfidence in potentially misspecified reward models. The method is evaluated against DPO and IPO baselines across multiple preference datasets, including both biased and unbiased preference distributions.

## Key Results
- The approach demonstrates improved robustness compared to DPO, particularly in biased preference datasets where DPO tends to produce degenerate policies
- Pessimistic ensemble reward model distillation outperforms standard reward model distillation and shows better generalization to out-of-distribution preferences
- The method achieves modest performance gains even in unbiased settings, suggesting broader applicability beyond just correcting bias

## Why This Works (Mechanism)
The mechanism works by addressing DPO's core vulnerability: its implicit reward structure can lead to overconfidence and degenerate solutions when the preference data contains noise or bias. By introducing explicit reward modeling, the approach creates a more stable optimization target that can be regularized and calibrated. The reward model distillation process ensures that the policy learns to maximize a well-defined reward function rather than relying on the unstable implicit reward ratios used in DPO. The pessimistic ensemble component adds robustness by considering uncertainty in the reward estimation process, preventing the policy from overfitting to potentially incorrect reward signals.

## Foundational Learning
- **Preference-based reinforcement learning**: Understanding how to learn from pairwise preferences rather than explicit rewards - needed to grasp the alignment problem being solved; quick check: can explain difference between reward-based and preference-based learning
- **Reward model uncertainty estimation**: Methods for quantifying and handling uncertainty in learned reward models - needed to understand the pessimism mechanism; quick check: can describe at least two approaches to reward model uncertainty
- **Policy optimization techniques**: Knowledge of different policy optimization algorithms (especially those used in language model fine-tuning) - needed to contextualize the improvements over DPO; quick check: can compare policy gradient methods with direct preference optimization
- **Ensemble methods in machine learning**: Understanding how ensembles can improve robustness and uncertainty estimation - needed to grasp the pessimistic ensemble approach; quick check: can explain benefits and costs of ensemble methods
- **Language model alignment objectives**: Familiarity with alignment objectives beyond simple likelihood maximization - needed to appreciate the broader context; quick check: can list three different language model alignment objectives
- **Overfitting in reinforcement learning**: Understanding how RL algorithms can overfit to training data or reward signals - needed to grasp why DPO produces degenerate policies; quick check: can explain one mechanism by which RL algorithms overfit

## Architecture Onboarding

**Component Map**: Preference Data -> Reward Model Ensemble -> Pessimistic Reward Selection -> Policy Distillation -> Aligned Policy

**Critical Path**: The critical path flows from preference data through multiple reward model training, pessimistic selection, and policy distillation. Each component must function correctly for the final aligned policy to be robust.

**Design Tradeoffs**: The approach trades computational efficiency (training multiple reward models) for robustness and stability. The pessimism parameter introduces a new hyperparameter that requires tuning but provides control over the exploration-exploitation balance.

**Failure Signatures**: 
- Degenerate policies (zero probability on preferred responses) indicate insufficient regularization or overly aggressive pessimism
- Poor performance on unbiased data suggests the pessimism is too conservative
- Instability during training may indicate reward model uncertainty is not being properly captured

**First Experiments**:
1. Train a single reward model on a small preference dataset and visualize its predictions to understand reward landscape
2. Implement pessimistic selection over a 3-model ensemble and compare to average-based selection
3. Apply the full pipeline to a synthetic preference dataset with known bias to verify the correction mechanism

## Open Questions the Paper Calls Out
The paper acknowledges several open questions regarding the brittleness of the pessimistic ensemble approach to reward model uncertainty. While improved robustness is demonstrated in biased preference datasets, the translation to more diverse or complex real-world alignment scenarios remains unclear. The calibration of the pessimism parameter γ is crucial but not deeply explored, with concerns that overly conservative pessimism could lead to underfitting or excessive regularization. Additionally, the computational overhead of training multiple reward models may limit practical applicability at scale.

## Limitations
- The computational overhead of training multiple reward models for ensemble-based methods may limit practical applicability, particularly at scale
- The evaluation focuses primarily on binary preference data, with uncertainty about generalization to multi-choice or continuous feedback scenarios
- The calibration of the pessimism parameter γ is crucial but not deeply explored, raising concerns about potential underfitting with overly conservative settings

## Confidence

High: The core claim that DPO can produce degenerate policies with zero or near-zero probability for preferred responses due to overfitting is well-supported by both theoretical analysis and empirical evidence.

Medium: The claim that explicit reward modeling and reward model distillation improves robustness is supported by experiments, but the magnitude of improvement may vary across different preference data distributions and model scales.

Medium: The assertion that pessimistic ensemble methods handle uncertainty better than standard approaches is reasonable but requires more extensive validation across diverse alignment tasks.

## Next Checks

1. Evaluate the approach on multi-choice preference data and continuous feedback scenarios to assess generalizability beyond binary preferences.

2. Conduct ablation studies varying the number of reward models in the ensemble and the pessimism parameter γ to identify optimal configurations and computational trade-offs.

3. Test the method on larger-scale language models (beyond 7B parameters) and in more complex alignment scenarios involving multi-turn conversations or task-specific objectives.