---
ver: rpa2
title: 'On Multi-Stage Loss Dynamics in Neural Networks: Mechanisms of Plateau and
  Descent Stages'
arxiv_id: '2410.20119'
source_url: https://arxiv.org/abs/2410.20119
tags:
- stage
- plateau
- initial
- neural
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes the multi-stage dynamics of two-layer neural
  networks with small initialization. The authors identify three distinct stages in
  the loss curve: initial plateau, initial descent, and secondary plateau.'
---

# On Multi-Stage Loss Dynamics in Neural Networks: Mechanisms of Plateau and Descent Stages

## Quick Facts
- arXiv ID: 2410.20119
- Source URL: https://arxiv.org/abs/2410.20119
- Reference count: 26
- Primary result: Rigorous analysis of three distinct training stages in two-layer networks: initial plateau (~log m duration), descent stage, and secondary plateau near critical point

## Executive Summary
This paper provides a rigorous theoretical analysis of the multi-stage dynamics in two-layer neural networks with small initialization. The authors identify three distinct phases in the training loss curve: an initial plateau where loss remains nearly constant due to linear dynamics approximation, a descent stage where loss decreases rapidly as nonlinear dynamics become dominant, and a secondary plateau near a critical point where linear terms vanish. Using Wasserstein distance to track weight distribution evolution and characterizing critical points through the parameter K, the work extends understanding of neural network training beyond the initial condensation phase previously studied.

## Method Summary
The method analyzes two-layer neural networks fθ(x) = Σ akσ(wkᵀx) with i.i.d. initialization from N(0, 1/m²ᵅ) for both weights and biases. The analysis uses gradient flow approximation with small learning rate, tracking weight norms and Wasserstein distances between input and output weight distributions. The population risk R(θ) = ½∫(fθ(x) - f(x))²ρ(x)dx is minimized while monitoring the relative Wasserstein distance W_rel²(ρ|a|, ρ‖w‖) = W₂(ρ|a|, ρ‖w‖)/‖ρ|a|‖² to identify transitions between training stages.

## Key Results
- Initial plateau stage lasts approximately log m time units with nearly constant loss
- Descent stage begins when weight distributions converge (Wasserstein distance → 0)
- Secondary plateau occurs near critical point K ≈ 1 where linear dynamics vanish
- Weight norms converge during descent stage, enabling nonlinear dynamics
- Descent time scales predictably with network width m and initialization parameter α

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Initial plateau stage occurs due to linear dynamics approximation dominating early training under small initialization.
- Mechanism: When weights are initialized very small, the nonlinear terms in the gradient dynamics are negligible, causing the system to behave like a linear network. This leads to slow loss reduction as parameters align along a specific direction (condensation) without significantly improving the model's expressiveness.
- Core assumption: Small initialization (α > 1/2) ensures that higher-order terms remain negligible during early training.
- Evidence anchors:
  - [abstract] "rigorous theoretical analysis showing that during the initial plateau stage (lasting ~log m), the loss remains nearly constant as parameters undergo linear dynamics"
  - [section 4.1] "linearized flow (15) will approximate dynamics (10) well in the initial stage"
  - [corpus] Weak - no direct neighbor evidence on small initialization linear regime.
- Break condition: When qmax grows beyond threshold 1/m^α1 log m, higher-order terms become significant and the linear approximation fails.

### Mechanism 2
- Claim: Initial descent stage begins when weight distributions become similar, enabling nonlinear dynamics to drive loss reduction.
- Mechanism: After condensation, the Wasserstein distance between input and output weight distributions approaches zero. This alignment allows the nonlinear terms in the gradient dynamics to become effective, leading to rapid loss reduction as the network explores more expressive parameter configurations.
- Core assumption: Convergence of weight distributions (W2(ρ|a|, ρ∥w‖) → 0) is necessary for nonlinear dynamics to dominate.
- Evidence anchors:
  - [abstract] "weight norms converge during the descent stage"
  - [section 4.1] "distributions of |a| and ∥w‖ converge to being identical for sufficiently large m"
  - [section 4.3.3] "input weights and output weight have the same distribution as well as the same norm"
- Break condition: When K approaches 1 (critical point), the leading linear term vanishes and the system enters secondary plateau.

### Mechanism 3
- Claim: Secondary plateau occurs due to proximity to critical point where linear dynamics vanish, requiring nonlinear exploration.
- Mechanism: Near the critical point (K ≈ 1), the dominant linear term in the gradient dynamics disappears. The system must rely on higher-order nonlinear terms to continue training, causing prolonged stagnation until sufficient exploration allows escape from the plateau.
- Core assumption: Critical point at K=1 creates a bottleneck in training dynamics.
- Evidence anchors:
  - [abstract] "secondary plateau then occurs due to proximity to this critical point"
  - [section 4.2.1] "network converges to a 'critical point' at Td"
  - [section 4.3.1] "dynamics can be approximately viewed as Eq. (21) due to the stability of the dynamics near critical point"
- Break condition: When nonlinear terms (fk, gk) become sufficiently large to overcome the critical point stability, allowing escape from plateau.

## Foundational Learning

- Concept: Wasserstein distance for measuring distribution similarity
  - Why needed here: Used to quantify when input and output weight distributions converge, triggering transition from plateau to descent
  - Quick check question: How does W2(ρ|a|, ρ‖w‖) approaching zero indicate weight alignment?

- Concept: Taylor expansion for activation functions
  - Why needed here: Enables decomposition of gradient dynamics into linear and higher-order nonlinear terms
  - Quick check question: What role does σ(3)(0) play in the higher-order gradient terms?

- Concept: Phase transition in training dynamics
  - Why needed here: Explains how different stages emerge from changing dominant terms in the gradient equations
  - Quick check question: How does the condition K=1 create a critical point in the training dynamics?

## Architecture Onboarding

- Component map: Initialization parameters (α, m) -> dynamics regime selection -> Gradient flow equations (10) -> stage-specific behavior -> Wasserstein distance metrics -> distribution convergence tracking -> K and K' variables -> critical point characterization

- Critical path:
  1. Initialize weights small (α > 1/2)
  2. Observe linear dynamics plateau
  3. Track Wasserstein distance convergence
  4. Monitor K approaching critical point
  5. Detect plateau-to-descent transition

- Design tradeoffs:
  - Smaller α -> longer plateau, more robust condensation
  - Larger m -> better approximation of infinite-width limit
  - Choice of activation function -> affects higher-order term magnitudes

- Failure signatures:
  - Plateau duration inconsistent with log m scaling -> initialization too large
  - Wasserstein distance not converging -> insufficient width or initialization
  - K not approaching 1 -> activation function issues or incorrect assumptions

- First 3 experiments:
  1. Vary α from 0.6 to 2.0 with fixed m=5000, measure plateau duration scaling
  2. Track W2(ρ|a|, ρ‖w‖) evolution during training for different initialization scales
  3. Compare K dynamics for tanh vs ReLU activation functions under small initialization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What causes the secondary plateau stage and what mechanisms allow networks to exit this plateau?
- Basis in paper: [explicit] The authors state "we explore the factors that allow the network to exit the secondary plateau and offer intuition into the mechanisms driving the network's progression beyond this stage in Section 4.3.2" but provide only heuristic reasoning rather than rigorous proof.
- Why unresolved: The paper acknowledges the existence of the secondary plateau and provides some experimental evidence and heuristic reasoning about what might cause departure from it, but does not provide a rigorous theoretical analysis of this stage.
- What evidence would resolve it: A rigorous theoretical proof showing the dynamics that cause networks to exit the secondary plateau, potentially involving the nonlinear terms that were previously negligible.

### Open Question 2
- Question: How do different activation functions beyond the "tanh-like" class affect the multi-stage dynamics?
- Basis in paper: [explicit] The paper specifically assumes "Tanh-like activation function" with specific properties (σ(0)=0, σ(1)(0)=1, σ(2)(0)=0, |σ(3)(z)|≤CL) and focuses analysis on this class.
- Why unresolved: The analysis is restricted to a specific class of activation functions, leaving open whether the three-stage phenomenon generalizes to other activation functions with different properties.
- What evidence would resolve it: Extending the theoretical analysis to other activation function classes (ReLU, sigmoid, etc.) and comparing their multi-stage dynamics.

### Open Question 3
- Question: How do the multi-stage dynamics change with different initialization distributions beyond Gaussian?
- Basis in paper: [inferred] The paper relies heavily on Gaussian initialization for its theoretical results, with log m terms appearing due to this choice, and states "This initialization method produced a relative factor of the order log m, which hinders us from obtaining further results."
- Why unresolved: The theoretical framework appears sensitive to the initialization distribution, particularly the log m terms that arise from Gaussian initialization, suggesting different distributions might yield different dynamics.
- What evidence would resolve it: Theoretical analysis with alternative initialization distributions (uniform, etc.) and experimental validation showing how different initializations affect the three-stage phenomenon.

## Limitations
- Analysis restricted to two-layer neural networks with small initialization
- Theoretical framework relies on gradient flow approximations that may not capture discrete optimization effects
- Critical point characterization at K=1 may not generalize to deeper networks
- Limited empirical validation of Wasserstein distance convergence and its connection to practical performance

## Confidence

**High Confidence**: The existence of distinct training stages (plateau-descent-plateau pattern) and the scaling relationship between plateau duration and log m are well-supported by both theory and experiments. The linear dynamics approximation during the initial plateau is rigorously established under the small initialization regime.

**Medium Confidence**: The mechanism linking Wasserstein distance convergence to the onset of nonlinear dynamics, while theoretically sound, has limited empirical validation. The characterization of the secondary plateau as being caused by proximity to the critical point at K=1 is plausible but may oversimplify the actual dynamics.

**Low Confidence**: The generality of these multi-stage dynamics to larger networks, different architectures, or practical training scenarios with large initialization remains largely unexplored. The precise conditions under which the linear approximation breaks down in real training are not fully characterized.

## Next Checks
1. Cross-architecture validation: Test whether similar multi-stage dynamics appear in deeper networks or convolutional architectures, and identify which components of the theoretical framework generalize versus require modification.

2. Numerical gradient flow verification: Conduct high-precision simulations comparing discrete gradient descent with gradient flow predictions to quantify approximation errors and identify conditions where the continuous approximation breaks down.

3. Alternative initialization scaling: Systematically vary initialization scales beyond the small initialization regime to map out how the multi-stage dynamics transition between different training regimes and identify phase boundaries.