---
ver: rpa2
title: Responsible Retrieval Augmented Generation for Climate Decision Making from
  Documents
arxiv_id: '2410.23902'
source_url: https://arxiv.org/abs/2410.23902
tags:
- climate
- policy
- sources
- query
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of accessing and interpreting
  complex climate policy documents by deploying a Retrieval-Augmented Generation (RAG)
  system. It introduces a novel evaluation framework tailored for climate-related
  documents, assessing both retrieval and generation quality.
---

# Responsible Retrieval Augmented Generation for Climate Decision Making from Documents

## Quick Facts
- arXiv ID: 2410.23902
- Source URL: https://arxiv.org/abs/2410.23902
- Reference count: 40
- Primary result: F1 score of 75.3% for retrieval relevance

## Executive Summary
This paper addresses the challenge of accessing and interpreting complex climate policy documents by deploying a Retrieval-Augmented Generation (RAG) system. It introduces a novel evaluation framework tailored for climate-related documents, assessing both retrieval and generation quality. The system employs multiple open-source retrieval models and generation models, evaluated against human-annotated datasets. Key results include achieving an F1 score of 75.3% for retrieval relevance and identifying the most effective model and prompting strategies for generation. The prototype tool enhances decision-making by providing transparent, verifiable answers with source citations, addressing issues like hallucinations and policy alignment.

## Method Summary
The method involves using open-source dense retrieval models and generation models, evaluated against human-annotated datasets from the Climate Policy Radar database. The system uses climate policy documents including national laws, policies, and UNFCCC submissions, with 550 documents equally distributed across World Bank Regions. LLM judges are employed to evaluate retrieval relevance and generation quality, ensuring alignment with a content policy. The approach focuses on domain-specific risks and user empowerment through transparent citation display and evaluation feedback.

## Key Results
- Achieved F1 score of 75.3% for retrieval relevance
- Identified optimal model and prompting strategies for generation quality
- Demonstrated effective hallucination reduction through RAG grounding mechanism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-Augmented Generation reduces hallucinations by grounding LLM outputs in source passages.
- Mechanism: The system retrieves relevant text passages from climate policy documents, then conditions the LLM generation on these passages to ensure factual consistency.
- Core assumption: Retrieved passages contain accurate information that the LLM can faithfully incorporate into its responses.
- Evidence anchors:
  - [abstract]: "RAG uses retrieval to ground the model responses using trusted external information sources, reducing the extent of hallucinations significantly"
  - [section]: "RAG uses retrieval to ground the model responses using trusted external information sources, reducing the extent of hallucinations significantly [9]"
  - [corpus]: Weak evidence - the corpus contains related papers but no direct evidence of hallucination reduction effectiveness
- Break condition: If retrieval fails to find relevant passages or retrieves incorrect information, the grounding mechanism fails and hallucinations may increase.

### Mechanism 2
- Claim: Evaluation framework with domain-specific dimensions enables responsible deployment in high-stakes climate decision making.
- Mechanism: The system employs custom evaluators for CPR Generation Policy alignment, faithfulness, formatting, and system-response to ensure outputs meet domain requirements.
- Core assumption: The evaluation dimensions capture the key quality aspects needed for climate policy decision support.
- Evidence anchors:
  - [abstract]: "We introduce a novel evaluation framework with domain-specific dimensions tailored for climate-related documents"
  - [section]: "We separately evaluate the two key pipeline components determining system performance: retrieval and generation"
  - [corpus]: Weak evidence - corpus contains related evaluation work but no direct evidence of domain-specific dimension effectiveness
- Break condition: If the evaluation dimensions miss critical quality aspects or produce false positives/negatives, responsible deployment is compromised.

### Mechanism 3
- Claim: User empowerment through transparent citation and evaluation feedback builds trust in AI-generated climate policy information.
- Mechanism: The system displays retrieved sources alongside generated answers with citation highlighting, and shows live evaluation scores for quality dimensions.
- Core assumption: Users can effectively validate AI-generated information when provided with transparent citations and quality assessments.
- Evidence anchors:
  - [abstract]: "Responsible AI systems must build and honour trust, effectively inform users of capabilities and limitations, transparently communicate provenance chains, and support users regardless of technical expertise"
  - [section]: "A responsible approach to AI-supported systems therefore entails a principle of empowering the user to easily validate the provenance and accuracy of information generated by the AI"
  - [corpus]: Weak evidence - corpus contains related UX work but no direct evidence of trust-building effectiveness
- Break condition: If citation highlighting is unclear or evaluation scores are misinterpreted, user trust may be undermined rather than built.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG) architecture
  - Why needed here: Climate policy documents are lengthy and technical, requiring information retrieval before generation to ensure accuracy
  - Quick check question: What are the two main components of a RAG system and how do they work together?

- Concept: Hallucination detection and mitigation
  - Why needed here: LLM hallucinations in climate policy contexts could lead to harmful decisions with real-world consequences
  - Quick check question: What are the two main categories of hallucination approaches mentioned in the paper?

- Concept: Evaluation metrics for retrieval and generation
  - Why needed here: Domain-specific evaluation is crucial for ensuring the system meets the unique requirements of climate policy decision support
  - Quick check question: What are the four generation evaluation dimensions used in this system?

## Architecture Onboarding

- Component map: User input collection with guardrails -> Information retrieval component -> Answer synthesis component -> Final guardrails -> Online evaluation components -> User interface with citation display and evaluation feedback

- Critical path: User query → Input guardrails → Retrieval → Generation → Output guardrails → UI display with citations and evaluation scores

- Design tradeoffs:
  - Single-document scope vs. multi-document retrieval capability
  - Open-source models vs. commercial APIs for cost and transparency
  - Strict generation policy vs. response flexibility
  - Real-time evaluation vs. system latency

- Failure signatures:
  - Low retrieval precision/recall indicates retrieval component issues
  - High faithfulness violation scores indicate generation component problems
  - High no-response rates indicate guardrail or prompt issues
  - Disparate performance across world regions indicates bias issues

- First 3 experiments:
  1. Evaluate different retrieval methods (BM25, dense models, hybrid approaches) on relevance
  2. Compare generation models and prompting strategies on faithfulness and policy alignment
  3. Test UX components with domain experts for usability and trust-building effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can retrieval mechanisms be improved to better handle structured or longer document context when answering questions about climate law and policy documents?
- Basis in paper: [explicit] The paper identifies that simply chunking and then retrieving relevant chunks is not suitable for the specifics of this retrieval task, which often requires attention to document structure and wider document context. It suggests that alternative retrieval mechanisms when structured or longer document context is needed could be promising approaches.
- Why unresolved: The paper highlights this as a limitation of the current retrieval approach but does not provide a definitive solution or evaluation of alternative retrieval methods that address this issue.
- What evidence would resolve it: Empirical results comparing the performance of retrieval methods that incorporate document structure and context against the current chunk-based approach on a dataset of climate law and policy documents.

### Open Question 2
- Question: What is the impact of using multi-turn conversation on the performance of the RAG system and its evaluators' correlation with human judgment?
- Basis in paper: [explicit] The paper identifies this as an area for future work, noting that while the current system can only facilitate single-turn question-answering, allowing for multi-turn conversations could be an obvious path for improvement. It raises questions about whether evaluators still correlate with human judgment on multi-turn datasets and how the shift from single- to multi-turn impacts other components of the system.
- Why unresolved: The paper does not explore or evaluate the performance of the RAG system or its evaluators in a multi-turn conversation setting, leaving the impact of this feature on system performance and evaluation metrics unknown.
- What evidence would resolve it: A study comparing the performance of the RAG system and its evaluators in single-turn versus multi-turn conversation settings, using both synthetic and real-world user queries.

### Open Question 3
- Question: How can the system be designed to support translation in the RAG workflow while ensuring ethical considerations are met in the climate law and policy domain?
- Basis in paper: [explicit] The paper identifies supporting translation in the RAG workflow as an area for future work, questioning whether LLMs can reliably be used as query and passage translators in a way that aligns with the ethical considerations required for this domain.
- Why unresolved: The paper does not provide a solution or evaluation of translation support in the RAG workflow, nor does it discuss how to ensure that the use of LLMs for translation aligns with ethical considerations in the climate law and policy domain.
- What evidence would resolve it: An evaluation of the performance and ethical alignment of LLMs used for translation in the RAG workflow, using a dataset of climate law and policy documents in multiple languages, and a set of ethical guidelines specific to this domain.

## Limitations

- The evaluation framework's domain-specific dimensions have not been externally validated beyond the authors' datasets
- The effectiveness of hallucination reduction through RAG grounding depends heavily on retrieval quality, which shows performance variation across different World Bank regions
- User trust-building mechanism through transparent citations remains largely theoretical without user studies demonstrating actual trust improvements

## Confidence

**High Confidence:**
- The system architecture combining retrieval and generation components is technically sound
- Retrieval performance metrics (F1 score of 75.3%) are measurable and verifiable
- The evaluation framework dimensions (policy alignment, faithfulness, formatting, responsiveness) are clearly defined

**Medium Confidence:**
- RAG grounding significantly reduces hallucinations compared to baseline approaches
- The evaluation framework captures the key quality aspects needed for climate policy decision support
- Transparent citations and evaluation feedback effectively build user trust

**Low Confidence:**
- The system demonstrably improves actual climate policy decision-making outcomes
- User trust in AI-generated climate policy information increases through the proposed transparency mechanisms
- The domain-specific evaluation dimensions are sufficient for responsible deployment in all climate policy contexts

## Next Checks

1. **Cross-Regional Performance Validation**: Conduct a comprehensive analysis of retrieval performance across all World Bank regions, identifying specific regions where nDCG scores fall below acceptable thresholds, then implement region-specific retrieval optimization strategies and re-measure performance.

2. **Hallucination Reduction Measurement**: Design a controlled experiment comparing hallucination rates between RAG-based generation and baseline LLM generation on identical climate policy queries, using human judges to categorize and quantify hallucination types and frequencies.

3. **User Trust Assessment**: Deploy the system to a diverse group of climate policy stakeholders (researchers, policymakers, practitioners) and conduct pre/post surveys measuring trust levels, information validation behaviors, and decision-making confidence, correlating these with actual usage patterns of the citation and evaluation feedback features.