---
ver: rpa2
title: Towards Democratizing Multilingual Large Language Models For Medicine Through
  A Two-Stage Instruction Fine-tuning Approach
arxiv_id: '2409.05732'
source_url: https://arxiv.org/abs/2409.05732
tags:
- medical
- dataset
- arxiv
- fine-tuning
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of adapting large language models
  (LLMs) for multilingual medical applications, where high computational costs of
  continual pretraining and performance gaps in domain-specific tasks limit their
  practicality. The authors propose a two-stage instruction fine-tuning approach using
  two newly created datasets, MMed-IFT and MMed-IFT-MC, which together contain over
  200,000 high-quality multilingual medical samples across six languages.
---

# Towards Democratizing Multilingual Large Language Models For Medicine Through A Two-Stage Instruction Fine-tuning Approach

## Quick Facts
- arXiv ID: 2409.05732
- Source URL: https://arxiv.org/abs/2409.05732
- Reference count: 4
- Primary result: Two-stage instruction fine-tuning approach using MMed-IFT and MMed-IFT-MC datasets achieves competitive performance on multilingual medical benchmarks while maintaining computational efficiency

## Executive Summary
This work addresses the challenge of adapting large language models for multilingual medical applications by proposing a two-stage instruction fine-tuning approach. The authors create two novel datasets, MMed-IFT and MMed-IFT-MC, containing over 200,000 high-quality multilingual medical samples across six languages. The approach leverages parameter-efficient fine-tuning with LoRA to avoid the computational expense of full model retraining while achieving results comparable to state-of-the-art models on both English and multilingual medical benchmarks.

## Method Summary
The two-stage instruction fine-tuning approach first injects general medical knowledge using the MMed-IFT dataset, then fine-tunes the model on task-specific multiple-choice questions from MMed-IFT-MC. Both stages use LoRA for parameter-efficient fine-tuning, with stage one using DoRA (rank=32, alpha=16) and stage two using QLoRA (rank=16, alpha=8). The datasets were constructed through question-answer generation from unstructured medical text and validated using cycle-consistency translation scores to ensure quality across languages.

## Key Results
- Two-stage approach outperforms single-stage fine-tuning on both general medical knowledge and task-specific performance
- Competitive results achieved on English benchmarks (USMLE, MedQA, MedMCQA) and multilingual benchmarks
- Computational efficiency maintained through parameter-efficient fine-tuning while matching state-of-the-art performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage instruction fine-tuning outperforms single-stage fine-tuning on both general medical knowledge and task-specific performance.
- Mechanism: The first stage injects broad medical domain knowledge using diverse question-answer pairs, building a foundation for understanding medical contexts. The second stage fine-tunes on task-specific multiple-choice questions, sharpening reasoning for exam-style tasks.
- Core assumption: General medical knowledge from stage one improves the model's ability to handle diverse and complex medical queries in stage two.
- Evidence anchors:
  - [abstract]: "the first stage injects general medical knowledge using MMed-IFT, while the second stage fine-tunes task-specific multiple-choice questions with MMed-IFT-MC"
  - [section 2.2]: "we propose a two-stage, parameter-efficient instruction fine-tuning approach... fine-tuning the LLM on our carefully curated MMed-IFT dataset... the model will acquire sufficient general medical knowledge... second stage... improve the model's reasoning abilities on specific tasks"
  - [corpus]: Weak. No direct evidence of two-stage superiority in related works; only mentions of multilingual medical models without explicit two-stage comparisons.
- Break condition: If general medical knowledge in stage one does not transfer to task-specific performance gains, or if overfitting occurs during stage two.

### Mechanism 2
- Claim: Parameter-efficient fine-tuning (LoRA) achieves competitive performance compared to full fine-tuning while reducing computational costs.
- Mechanism: LoRA decomposes weight updates into low-rank matrices, enabling efficient adaptation of LLMs without updating all parameters. This is particularly effective for large models where full fine-tuning is prohibitive.
- Core assumption: Low-rank decomposition preserves essential model behavior while adapting to medical tasks.
- Evidence anchors:
  - [abstract]: "Our method achieves competitive results on both English and multilingual benchmarks, striking a balance between computational efficiency and performance"
  - [section 2.2]: "Both stages leverage LoRA for parameter-efficient fine-tuning (PEFT), offering a computationally efficient alternative to continual pretraining while maintaining good performance"
  - [corpus]: No direct corpus evidence of LoRA effectiveness in medical domains; related works focus on continual pretraining or instruction fine-tuning without specifying PEFT methods.
- Break condition: If LoRA rank is too low to capture necessary adaptations, or if model capacity is insufficient for the medical domain complexity.

### Mechanism 3
- Claim: High-quality multilingual data with cycle-consistency translation ensures consistent performance across languages.
- Mechanism: Translation between languages is validated using cycle-consistency scores (BLEU + BERT) to ensure semantic preservation. This maintains model reliability when operating across language boundaries.
- Core assumption: Cycle-consistent translations retain medical meaning across languages, preventing degradation of performance.
- Evidence anchors:
  - [section 2.1]: "we introduce a novel evaluation criterion called the cycle-consistency translation score to guide the translation process... if the calculated CCTS exceeds a certain threshold, we conclude that sample can be translated into language t without significant information loss"
  - [section 2.1]: "We retained only the samples with a cycle-consistency translation score (CCTS) greater than 0.8"
  - [corpus]: No direct corpus evidence of cycle-consistency effectiveness in medical multilingual models; related works mention multilingual datasets but not translation validation methods.
- Break condition: If cycle-consistency threshold is too strict (reducing dataset size) or too lenient (allowing semantic drift), or if medical terminology does not translate well between languages.

## Foundational Learning

- Concept: Question-answer pair generation from unstructured medical text
  - Why needed here: Medical corpora often exist as continuous text (articles, books) rather than structured Q&A pairs required for instruction tuning.
  - Quick check question: How does the two-step Q&A generation process (condensation + question generation) help ensure quality and relevance of training samples?

- Concept: Parameter-efficient fine-tuning and low-rank adaptation
  - Why needed here: Full fine-tuning of large models is computationally expensive and impractical for many institutions; LoRA provides a scalable alternative.
  - Quick check question: What are the trade-offs between LoRA rank, model performance, and computational cost?

- Concept: Multilingual model training and cross-lingual transfer
  - Why needed here: Medical knowledge must be accessible across languages to serve diverse populations; cross-lingual transfer ensures knowledge learned in one language benefits others.
  - Quick check question: How does training on English medical knowledge first (stage one) potentially improve performance in non-English languages during stage two?

## Architecture Onboarding

- Component map: MMed-IFT (general medical knowledge) → MMed-IFT-MC (task-specific MLE questions) → Llama3-8B base → Stage 1 LoRA fine-tuning → Merge → Stage 2 LoRA fine-tuning → Evaluation on benchmarks

- Critical path:
  1. Data preprocessing and quality filtering
  2. Stage 1 fine-tuning on MMed-IFT
  3. Merge LoRA weights
  4. Stage 2 fine-tuning on MMed-IFT-MC
  5. Evaluation on benchmarks

- Design tradeoffs:
  - Dataset size vs. quality: More data may improve coverage but requires more filtering to maintain quality
  - LoRA rank vs. performance: Higher rank may improve adaptation but increases computational cost
  - Translation vs. native data: Translation expands coverage but risks semantic loss without cycle-consistency

- Failure signatures:
  - Stage 1 underperforms: Model fails to acquire general medical knowledge (check Q&A quality and diversity)
  - Stage 2 underperforms: Model fails on task-specific reasoning (check if general knowledge transfer occurred)
  - Multilingual degradation: Performance drops significantly in non-English languages (check translation quality and cycle-consistency)

- First 3 experiments:
  1. Ablation: Compare single-stage fine-tuning on MMed-IFT-MC vs. two-stage approach to validate knowledge injection benefit
  2. Rank sweep: Test different LoRA ranks in stage 1 and stage 2 to find optimal balance of performance and efficiency
  3. Translation validation: Compare cycle-consistent translations vs. direct translations to measure semantic preservation impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the two-stage fine-tuning approach compare to continual pretraining in terms of computational efficiency and performance for multilingual medical LLMs?
- Basis in paper: [explicit] The paper states that continual pretraining is computationally expensive and sometimes impractical, while the two-stage approach offers a computationally efficient alternative.
- Why unresolved: The paper provides preliminary results showing the effectiveness of the two-stage approach, but a comprehensive comparison with continual pretraining is not presented.
- What evidence would resolve it: A detailed comparative study measuring computational costs and performance metrics for both approaches across multiple multilingual medical benchmarks.

### Open Question 2
- Question: How does the performance of the two-stage fine-tuning approach vary across different languages and medical domains?
- Basis in paper: [explicit] The paper introduces a multilingual approach covering six languages, but only provides preliminary results for some languages and benchmarks.
- Why unresolved: The paper mentions the need for further refinement and updates, indicating that the multilingual performance results are not yet complete.
- What evidence would resolve it: Comprehensive evaluation results for all six languages across various medical domains and benchmarks.

### Open Question 3
- Question: How does the quality and diversity of the MMed-IFT and MMed-IFT-MC datasets impact the performance of the fine-tuned models?
- Basis in paper: [explicit] The paper introduces these datasets and describes their construction process, but does not provide an in-depth analysis of their impact on model performance.
- Why unresolved: The paper focuses on the methodology and preliminary results, but does not explore the relationship between dataset quality/diversity and model performance in detail.
- What evidence would resolve it: An ablation study analyzing the performance of models trained on different subsets of the datasets, as well as a comparison with other existing medical datasets.

## Limitations

- Lack of direct empirical validation comparing two-stage approach against single-stage alternatives on the same datasets
- Effectiveness of cycle-consistency translation for medical terminology across diverse languages remains untested
- Preliminary results indicate need for further refinement and updates, suggesting incomplete multilingual performance evaluation

## Confidence

- **High Confidence**: The computational efficiency claims of LoRA parameter-efficient fine-tuning are well-supported by the literature and the stated approach follows standard practices.
- **Medium Confidence**: The claim that two-stage fine-tuning improves both general medical knowledge and task-specific performance is plausible but not directly validated against single-stage baselines.
- **Low Confidence**: The assertion that cycle-consistency translation ensures semantic preservation across languages is supported by methodology but lacks empirical validation of its impact on model performance.

## Next Checks

1. Conduct an ablation study comparing the two-stage fine-tuning approach against single-stage fine-tuning on MMed-IFT-MC alone to directly measure the knowledge injection benefit.

2. Perform a cross-lingual evaluation by fine-tuning on English-only data (stage one) and measuring performance degradation or improvement on non-English benchmarks in stage two.

3. Test different cycle-consistency thresholds (0.7, 0.8, 0.9) to empirically determine the optimal balance between dataset size and translation quality preservation.