---
ver: rpa2
title: 'GLEE: A Unified Framework and Benchmark for Language-based Economic Environments'
arxiv_id: '2410.05254'
source_url: https://arxiv.org/abs/2410.05254
tags:
- game
- games
- alice
- economic
- players
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "GLEE is a unified framework and benchmark for evaluating large\
  \ language models in language-based economic environments, addressing the challenge\
  \ of inconsistent modeling assumptions and evaluation criteria across existing studies.\
  \ The framework defines three game families\u2014bargaining, negotiation, and persuasion\u2014\
  with consistent parameterization, degrees of freedom, and economic measures for\
  \ agent performance, efficiency, and fairness."
---

# GLEE: A Unified Framework and Benchmark for Language-based Economic Environments

## Quick Facts
- arXiv ID: 2410.05254
- Source URL: https://arxiv.org/abs/2410.05254
- Reference count: 40
- Primary result: GLEE establishes a unified framework for evaluating LLMs across bargaining, negotiation, and persuasion games with consistent parameterization and economic metrics

## Executive Summary
GLEE addresses the challenge of inconsistent modeling assumptions and evaluation criteria across existing LLM studies in economic environments by providing a unified framework and benchmark. The framework defines three game families—bargaining, negotiation, and persuasion—with standardized parameterization, degrees of freedom, and economic measures for agent performance, efficiency, and fairness. Through an open-source framework, researchers collected 587K LLM vs. LLM decisions across 80K+ games involving 13 different models, plus 3,405 human vs. LLM interactions. The study reveals that market parameters significantly impact economic outcomes with effects varying by game type, no single LLM consistently maximizes efficiency, fairness, or self-gain, and humans show more extreme behavior patterns compared to LLMs.

## Method Summary
GLEE employs an open-source Python framework to simulate language-based economic games across three families: bargaining, negotiation, and persuasion. The framework collects extensive interaction data through LLM vs. LLM gameplay (587K decisions across 80K+ games with 13 models) and human vs. LLM interactions (3,405 instances). To enable fair comparisons across heterogeneous experimental conditions, the study uses linear regression models to predict economic outcomes (efficiency, fairness, self-gain) based on game parameterization and player identities. This approach controls for game configuration variations while isolating the independent contribution of each parameter and model to economic outcomes.

## Key Results
- Market parameters significantly impact economic outcomes, with effects varying by game type
- No single LLM consistently maximizes efficiency, fairness, or self-gain—performance depends heavily on the opponent
- Humans show more extreme behavior, either outperforming all LLMs or falling behind entirely depending on the game and role

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GLEE's unified framework enables meaningful comparison of LLM behavior across different economic game families by standardizing game parameters and evaluation metrics
- Mechanism: By defining consistent parameterization, degrees of freedom, and evaluation metrics across bargaining, negotiation, and persuasion games, GLEE creates a controlled environment where variations in LLM performance can be attributed to model differences rather than experimental design inconsistencies
- Core assumption: The three game families capture sufficiently diverse economic interactions while maintaining enough structural similarity for unified evaluation
- Evidence anchors:
  - [abstract]: "three base families of games with consistent parameterization, degrees of freedom and economic measures"
  - [section]: "Central to the framework is a clear and comprehensive parameterization of the space of all bargaining, negotiation and persuasion games"
- Break condition: If the game families are too dissimilar in structure, the unified framework may force artificial constraints that mask important behavioral differences

### Mechanism 2
- Claim: The linear regression-based statistical analysis effectively isolates the impact of game parameters and LLM choices on economic outcomes
- Mechanism: By controlling for game configuration variations through regression models, the framework estimates the independent contribution of each parameter and model, enabling fair comparisons across heterogeneous experimental conditions
- Core assumption: The relationships between game parameters and outcomes are approximately linear, allowing regression models to capture the main effects
- Evidence anchors:
  - [section]: "Since each model encountered only a subset of all possible game setups, raw metric values are not directly comparable"
  - [section]: "We train a separate linear regression model for each combination of game family and evaluation metric"
- Break condition: If non-linear interactions between parameters are significant, the linear regression approach may miss important effects or produce biased estimates

### Mechanism 3
- Claim: The open-source framework and large-scale data collection enable reproducible research and systematic exploration of LLM behavior in economic environments
- Mechanism: The framework provides researchers with tools to instantiate various economic games, collect data from LLM interactions, and analyze behavioral patterns across different configurations and model combinations
- Core assumption: The framework's implementation accurately captures the intended game mechanics and allows for controlled experimentation
- Evidence anchors:
  - [abstract]: "We develop an open-source framework for interaction simulation and analysis"
  - [section]: "The system is written in Python, designed for ease of use and customization"
- Break condition: If the framework has implementation bugs or limitations in expressiveness, it may constrain research possibilities or introduce artifacts

## Foundational Learning

- Concept: Game theory fundamentals (Nash equilibrium, subgame perfection)
  - Why needed here: The framework evaluates LLM behavior in strategic settings where game-theoretic concepts like rationality and equilibrium play are relevant
  - Quick check question: What is the difference between a Nash equilibrium and a subgame-perfect equilibrium in sequential games?

- Concept: Economic efficiency and fairness metrics
  - Why needed here: The framework uses these metrics to evaluate game outcomes, requiring understanding of what they measure and how they differ across game families
  - Quick check question: How does the efficiency metric differ between bargaining and negotiation games?

- Concept: Linear regression and feature engineering
  - Why needed here: The statistical analysis relies on regression models to isolate parameter effects, requiring knowledge of how to construct appropriate features
  - Quick check question: Why might one-hot encoding be appropriate for representing categorical game parameters?

## Architecture Onboarding

- Component map:
  - Game engine -> LLM interface -> Configuration manager -> Data collector -> Analysis module

- Critical path:
  1. Define game configuration (parameters, LLMs)
  2. Initialize game state and players
  3. Execute game rounds with communication
  4. Record outcomes and decisions
  5. Analyze results using regression models

- Design tradeoffs:
  - Flexibility vs. standardization: Broader game coverage vs. unified evaluation
  - Complexity vs. interpretability: More parameters vs. clearer analysis
  - Scale vs. cost: More games vs. computational resources

- Failure signatures:
  - Inconsistent outcomes across identical configurations
  - LLM responses that don't follow game rules
  - Statistical analysis that produces unstable estimates

- First 3 experiments:
  1. Run a single bargaining game with fixed parameters to verify basic functionality
  2. Test different LLM combinations in identical configurations to check consistency
  3. Vary one parameter at a time to observe its isolated effect on outcomes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific theoretical insights from classic economic models may not hold when rich linguistic communication is present in bargaining games?
- Basis in paper: [explicit] The paper notes that "the effect of complete information on fairness and efficiency may depend on whether message allowance is enabled" and suggests that "some classic theoretical insights, derived under structured or language-free assumptions, may not be applicable when rich linguistic communication is present."
- Why unresolved: The paper hypothesizes about this interaction but does not provide a comprehensive theoretical investigation into how language-based communication affects established economic game theory results.
- What evidence would resolve it: Comparative analysis of outcomes between structured vs. linguistic communication in bargaining games, with theoretical explanations for deviations from classic models.

### Open Question 2
- Question: How does the anchoring effect in human players vary across different game types (bargaining, negotiation, persuasion) and what are the underlying psychological mechanisms?
- Basis in paper: [explicit] The paper observes that "humans tend to anchor their negotiation behavior to the initial proposal" in bargaining games, with a correlation of 0.63 between Alice's first offer and her final payoff when Bob is human, but only 0.18 when Alice is human.
- Why unresolved: The paper only demonstrates the anchoring effect in bargaining games and doesn't explore whether similar patterns exist in negotiation and persuasion games or investigate the psychological mechanisms driving this behavior.
- What evidence would resolve it: Cross-game analysis of anchoring patterns with controlled variations in initial offers and follow-up experiments measuring psychological factors like risk aversion and loss aversion.

### Open Question 3
- Question: What are the optimal meta-game strategies for selecting LLM agents to represent players in different economic game families?
- Basis in paper: [explicit] The paper identifies "pure-strategy Nash equilibria" for agent selection in bargaining, negotiation, and persuasion games, noting that "there are no absolute best-performing models in terms of any of the evaluation measures, and that the performance of one LLM strongly depends on its competitor's choice of LLM."
- Why unresolved: While the paper identifies some Nash equilibria for specific game families, it doesn't provide a comprehensive framework for determining optimal agent selection strategies across different economic contexts or explain the underlying reasons for compatibility between specific model pairs.
- What evidence would resolve it: Systematic analysis of model compatibility matrices across all game families, identification of features that predict successful model pairings, and development of selection algorithms that optimize for efficiency, fairness, or self-gain.

## Limitations
- The reliance on text-based interactions may not fully capture real-world economic decision-making complexities
- Performance comparisons between LLMs and humans are constrained by the relatively small human dataset (3,405 interactions)
- The study doesn't fully explore how different prompt engineering strategies or few-shot examples might influence LLM performance

## Confidence

**Confidence: Medium** - The study establishes a valuable unified framework for evaluating LLMs in economic environments, but several limitations affect generalizability. The reliance on text-based interactions may not fully capture real-world economic decision-making complexities, and the linear regression approach assumes additive effects that may oversimplify non-linear interactions between game parameters.

**Confidence: Medium** - The performance comparisons between LLMs and humans are constrained by the relatively small human dataset (3,405 interactions) compared to the LLM dataset (587K decisions). This imbalance may affect the reliability of human-LLM performance comparisons, particularly for games where human behavior shows extreme variability.

**Confidence: Low** - While the framework demonstrates significant effects of game parameters on outcomes, the study doesn't fully explore how different prompt engineering strategies or few-shot examples might influence LLM performance. This represents a gap in understanding the full potential of LLMs in these economic environments.

## Next Checks

1. **Cross-validation of regression results**: Replicate the linear regression analysis using different train-test splits and compare coefficient stability against non-linear models (XGBoost/CatBoost) to verify that linear assumptions don't mask important interactions.

2. **Human dataset expansion**: Collect additional human-human and human-LLM interaction data across all game types to achieve sample sizes comparable to the LLM-only experiments, enabling more robust behavioral comparisons.

3. **Prompt engineering impact study**: Systematically vary prompt structures, few-shot examples, and role descriptions across game types to quantify their effects on LLM economic decision-making performance and identify optimal prompting strategies.