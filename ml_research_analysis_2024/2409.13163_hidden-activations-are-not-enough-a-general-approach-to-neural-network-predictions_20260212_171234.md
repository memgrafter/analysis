---
ver: rpa2
title: 'Hidden Activations Are Not Enough: A General Approach to Neural Network Predictions'
arxiv_id: '2409.13163'
source_url: https://arxiv.org/abs/2409.13163
tags:
- neural
- network
- networks
- adversarial
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel mathematical framework for analyzing
  neural networks using quiver representation theory, providing a foundation-first
  approach to understanding neural network behavior. The framework enables quantifying
  similarity between new data samples and training data as perceived by the network
  through induced quiver representations, capturing more information than traditional
  hidden layer outputs.
---

# Hidden Activations Are Not Enough: A General Approach to Neural Network Predictions

## Quick Facts
- arXiv ID: 2409.13163
- Source URL: https://arxiv.org/abs/2409.13163
- Reference count: 40
- This paper introduces a novel mathematical framework for analyzing neural networks using quiver representation theory, providing a foundation-first approach to understanding neural network behavior.

## Executive Summary
This paper presents a novel mathematical framework for analyzing neural networks through quiver representation theory, providing a foundation-first approach to understanding neural network behavior. The framework enables quantifying similarity between new data samples and training data as perceived by the network through induced quiver representations, capturing more information than traditional hidden layer outputs. The authors prove that induced matrices are invariant under isomorphisms of neural networks, and that distances between these matrices provide bounds on distances between network logits. As proof of concept, they apply their results to detect adversarial examples on MNIST and FashionMNIST datasets using 8 different MLP architectures and 21 adversarial attack methods.

## Method Summary
The authors construct a quiver representation from a neural network by associating matrices to each layer and nodes to each neuron. They prove this representation is invariant under network isomorphisms and use tensor network contraction to map the high-dimensional representation to a single matrix that captures all network information. For adversarial detection, they compute class-specific hyper-ellipsoids in matrix space using statistics from correctly classified training samples, then flag test samples whose matrices fall outside these regions or too far from class means.

## Key Results
- Proved that induced matrices of neural networks are invariant under isomorphisms, capturing only input-output behavior
- Established that distances between induced matrices provide lower bounds on distances between network logits
- Demonstrated adversarial detection on MNIST and FashionMNIST with detection rates ranging from 78% to 96% across 8 MLP architectures and 21 attack methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Induced quiver representation matrices are invariant under neural network isomorphisms
- Mechanism: Isomorphisms preserve network function while allowing hidden activations to differ. The matrix mapping π∘φ(W,f) captures only the input-output behavior, not the internal activations, so isomorphic networks produce identical matrices
- Core assumption: The matrix space MatR(d,k) fully captures the network's input-output behavior through the tensor network contraction
- Evidence anchors:
  - [abstract] "We prove that the induced matrices of neural networks are invariant under isomorphisms of neural networks"
  - [section 4] "Theorem 4.2. If (W,f )∼= (V,g ), then M(W,f )(x) = M(V,g )(x) for allx∈Rd"
  - [corpus] Weak evidence - no directly relevant papers found in neighbor search
- Break condition: If tensor network contraction loses information about the network function (unlikely given the embedding claim)

### Mechanism 2
- Claim: Distance between induced matrices bounds the distance between network logits
- Mechanism: The matrix multiplication M(W,f )(x)1d produces the logits. Therefore, the infinity norm distance between matrices is at least as large as the max norm distance between logits, providing a geometric measure in matrix space
- Core assumption: The evaluation map ev1d is continuous and the matrix space topology captures the logit space topology
- Evidence anchors:
  - [section 4] "Theorem 4.5. Let (W,f ) be a neural network,x,y∈Rd be two inputs... We have that ∥M(W,f )(y)−M(W,f )(x)∥∞≥∥Ψ(W,f )(y)−Ψ(W,f )(x)∥max"
  - [abstract] "distances between these matrices provide bounds on distances between network logits"
  - [corpus] Weak evidence - no directly relevant papers found in neighbor search
- Break condition: If the evaluation map ev1d is discontinuous or the norms don't correspond properly

### Mechanism 3
- Claim: Hyper-ellipsoids in matrix space can characterize class distributions and detect adversarial examples
- Mechanism: Correctly classified training data maps to class-specific convex regions in matrix space. New samples whose matrices fall outside these regions (or too far from the class mean) are flagged as suspicious
- Core assumption: Adversarial perturbations cause significant changes in the induced matrix representation that exceed natural variation
- Evidence anchors:
  - [section 5] "Motivated by Theorem 4.5... we work out a detection method for adversarial examples based on the induced matrices M(W,f )(x) on training datax"
  - [section 6] "Our detection method is architecture-agnostic and attack-method-agnostic, yielding a generalizable approach"
  - [corpus] Weak evidence - no directly relevant papers found in neighbor search
- Break condition: If adversarial attacks can craft perturbations that stay within the hyper-ellipsoids while still causing misclassification

## Foundational Learning

- Concept: Quiver representation theory
  - Why needed here: Provides the mathematical framework for analyzing neural networks as algebraic objects with invariant properties
  - Quick check question: How does a quiver representation differ from traditional layer-by-layer neural network analysis?

- Concept: Tensor network contraction
  - Why needed here: Enables mapping the high-dimensional quiver representation to a single matrix while preserving all information
  - Quick check question: What is the relationship between the induced matrix and the network's forward pass computation?

- Concept: Invariance under isomorphisms
  - Why needed here: Ensures the analysis is robust to equivalent neural networks that have different internal representations
  - Quick check question: Why is invariance under isomorphisms crucial for any meaningful neural network analysis?

## Architecture Onboarding

- Component map:
  Input preprocessing → Quiver representation construction → Tensor network contraction → Matrix space analysis → Detection/classification
  Key components: φ(W,f) knowledge map, π tensor contraction, M(W,f) induced matrix, hyper-ellipsoid statistics

- Critical path:
  1. Compute induced matrices for training data
  2. Calculate class-specific means and standard deviations
  3. Set detection thresholds based on statistical properties
  4. Evaluate new samples against hyper-ellipsoids

- Design tradeoffs:
  - Matrix computation vs. hidden activation analysis: Matrices are invariant but require more computation
  - Detection sensitivity vs. false positive rate: Threshold tuning balances these competing goals
  - General applicability vs. task-specific optimization: Current method works broadly but may miss attack-specific patterns

- Failure signatures:
  - High false positive rate: Thresholds too restrictive; consider relaxing tε or increasing ε
  - Low detection rate: Adversarial attacks stay within natural variation; may need more sophisticated geometric analysis
  - Memory issues: Large networks produce large matrices; consider submatrix analysis on subnetworks

- First 3 experiments:
  1. Compute induced matrices for MNIST test set and visualize class separation in matrix space
  2. Apply detection algorithm with default parameters to FGSM and PGD attacks
  3. Compare detection performance across different MLP architectures on FashionMNIST

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the matrix-based adversarial detection method perform on more complex datasets and architectures beyond MNIST and FashionMNIST?
- Basis in paper: [inferred] The authors note their method is not perfect and certain attacks like SparseFool, OnePixel, and Pixle were difficult to detect. They also mention that more complicated datasets and networks may require different analyses.
- Why unresolved: The experiments were limited to relatively simple MLP architectures on two well-known benchmark datasets. The method's effectiveness on more complex architectures (CNNs, transformers) and real-world datasets remains untested.
- What evidence would resolve it: Systematic testing of the detection method on diverse datasets (ImageNet, CIFAR-10, etc.) and modern architectures, comparing performance metrics across different data complexity levels.

### Open Question 2
- Question: What is the theoretical relationship between the induced matrix distances and the actual prediction robustness of neural networks?
- Basis in paper: [explicit] The authors prove that distances between induced matrices are lower bounds for distances between logits, but don't establish how this relates to prediction robustness against adversarial attacks.
- Why unresolved: The paper establishes mathematical bounds but doesn't connect these bounds to practical measures of adversarial robustness or explain why larger matrix distances correlate with adversarial vulnerability.
- What evidence would resolve it: Empirical studies correlating matrix distance metrics with adversarial success rates, and theoretical analysis connecting matrix distance bounds to established robustness measures.

### Open Question 3
- Question: How does the matrix-based approach scale to large transformer models with billions of parameters?
- Basis in paper: [explicit] The authors discuss potential applications to transformer subnetworks and mention that their largest tested architecture had 1.98 billion parameters, where some attacks didn't converge.
- Why unresolved: The computational complexity of computing and comparing induced matrices grows with network size, and the authors only tested on relatively small MLPs despite discussing potential applications to LLMs.
- What evidence would resolve it: Efficient algorithms for computing induced matrices in transformers, memory usage analysis, and empirical results on actual large language models showing practical feasibility.

## Limitations
- Detection rates vary significantly across architectures and attack methods, suggesting limited generalizability
- Method requires computing induced matrices for all training data, which is computationally expensive for large networks
- Effectiveness depends on assumption that adversarial perturbations cause measurable matrix space changes exceeding natural variation

## Confidence

**Major Claims Confidence:**
- Theoretical framework (High): Proofs for invariance and logit bounds are mathematically rigorous
- Detection effectiveness (Medium): Good results on standard datasets but limited to specific architectures and attack types
- Generalizability (Low): Only tested on MNIST/FashionMNIST with MLPs; performance on other architectures and tasks unknown

## Next Checks
1. Test the detection method on a convolutional neural network architecture and evaluate performance degradation
2. Apply the framework to a more complex dataset (e.g., CIFAR-10) to assess scalability and effectiveness on higher-dimensional data
3. Design an attack specifically targeting the induced matrix representation to evaluate the method's robustness against adaptive adversaries