---
ver: rpa2
title: 'Meanings and Feelings of Large Language Models: Observability of Latent States
  in Generative AI'
arxiv_id: '2405.14061'
source_url: https://arxiv.org/abs/2405.14061
tags:
- prompt
- llms
- state
- trajectories
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies whether Large Language Models (LLMs) viewed
  as dynamical systems are observable, meaning whether there exist multiple mental
  state trajectories that yield the same sequence of generated tokens. The authors
  formalize observability for LLMs and show that current autoregressive Transformer-class
  LLMs trained with textual tokens are observable - their mental states can be reconstructed
  from outputs.
---

# Meanings and Feelings of Large Language Models: Observability of Latent States in Generative AI

## Quick Facts
- arXiv ID: 2405.14061
- Source URL: https://arxiv.org/abs/2405.14061
- Authors: Tian Yu Liu; Stefano Soatto; Matteo Marchi; Pratik Chaudhari; Paulo Tabuada
- Reference count: 39
- Primary result: LLMs with hidden system prompts can produce identical observable outputs from different hidden state trajectories, enabling potential Trojan horse attacks.

## Executive Summary
This paper investigates whether Large Language Models (LLMs) are observable dynamical systems - specifically, whether different internal state trajectories can produce identical output sequences. The authors demonstrate that while standard autoregressive LLMs are observable, introducing hidden system prompts creates non-observable behavior where multiple distinct state trajectories yield the same observable outputs. Experiments with GPT-2 and LLaMA-2 show that 20-80% of hidden state trajectories become indistinguishable when system prompts are concealed. This vulnerability enables crafting Trojan horse attacks where benign-looking initial outputs mask malicious subsequent behavior, revealing significant security implications for LLM deployment.

## Method Summary
The authors formalize LLM observability by treating models as dynamical systems and analyzing whether internal states can be reconstructed from outputs. They implement four model types with varying system prompt visibility (verbal, non-verbal, one-step fading memory, infinite fading memory) and compute the cardinality of indistinguishable state sets using the Stanford Sentiment Treebank dataset. Adversarial prompts and Trojan horses are optimized through gradient descent and KL divergence minimization. The observability analysis is validated through experiments measuring the maximum cardinality of sets of user prompts producing identical trajectories across different time horizons.

## Key Results
- Standard autoregressive LLMs are observable when system prompts are visible to users
- Hidden system prompts make LLMs unobservable, with 20-80% of distinct state trajectories yielding identical outputs
- Trojan horse attacks can be crafted by optimizing system prompts to produce benign initial outputs and malicious subsequent behavior
- Type 1 models show 35% of trajectories are indistinguishable for GPT-2 and 20% for LLaMA-2-7B

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs with hidden system prompts can produce identical observable outputs from different hidden state trajectories.
- Mechanism: When system prompts are not visible to users, the model's internal state evolution can follow multiple distinct paths that converge to the same observable output sequence.
- Core assumption: The system prompt modifies the model's state evolution in ways not captured by the observable output sequence alone.
- Evidence anchors: Experiments show 20-80% of distinct hidden state trajectories yield identical expressions when system prompts are hidden.

### Mechanism 2
- Claim: Observability depends on whether state space includes hidden inputs like system prompts.
- Mechanism: Standard LLMs are observable because their state space is limited to past tokens, but system prompts expand the state space dimensionally.
- Core assumption: Inclusion of system prompts creates higher-dimensional state space where multiple trajectories map to same output.
- Evidence anchors: Mathematical proof shows models become neither observable nor reconstructible with certain prompt formulations.

### Mechanism 3
- Claim: Trojan horse attacks can be crafted by optimizing system prompts for benign-then-malicious behavior.
- Mechanism: System prompts can be optimized to match benign outputs initially while diverging to malicious outputs later.
- Core assumption: Optimization can create trajectories indistinguishable from benign ones initially but then diverge.
- Evidence anchors: Direct optimization successfully creates Trojan horses when Type 2 system prompts are allowed.

## Foundational Learning

- Concept: Dynamical systems observability
  - Why needed here: Essential for assessing whether LLM internal states can be reconstructed from outputs to evaluate security vulnerabilities
  - Quick check question: What distinguishes observability from reconstructibility in dynamical systems?

- Concept: Autoregressive models and state representation
  - Why needed here: LLMs use autoregressive architecture where state represents sliding window of past tokens
  - Quick check question: How does state representation in autoregressive models differ from traditional dynamical systems?

- Concept: System identification and realization
  - Why needed here: LLM parameter learning relates to system identification, informing behavior analysis
  - Quick check question: What relationship exists between system identification and LLM training?

## Architecture Onboarding

- Component map: User prompt -> Hidden system prompt -> Autoregressive Transformer -> Generated tokens -> Observable output
- Critical path: 1) User provides prompt, 2) System prompt influences initial state, 3) Model generates output tokens autoregressively, 4) Output tokens observed by user
- Design tradeoffs: Security vs functionality - hidden prompts enable advanced features but introduce vulnerabilities; Observability vs controllability - full observability may limit complex computations
- Failure signatures: Multiple distinct state trajectories producing identical outputs; Inability to reconstruct initial state from output; Trojan horse behavior with benign-then-malicious outputs
- First 3 experiments: 1) Test observability of standard LLM without system prompts, 2) Introduce hidden system prompt and measure indistinguishable trajectories, 3) Optimize system prompt for Trojan horse attack

## Open Questions the Paper Calls Out
None

## Limitations
- Experiments focus primarily on GPT-2 and LLaMA-2 models, limiting generalizability to other architectures
- Observability analysis assumes specific model implementations and doesn't account for architectural variations
- Trojan horse optimization may not scale efficiently to larger models or complex attack scenarios

## Confidence

**High Confidence:** Mathematical framework for LLM observability is rigorously derived and theorems are mathematically sound.

**Medium Confidence:** Experimental results showing 20-80% indistinguishable trajectories are methodology-sound but percentages may vary with different models.

**Low Confidence:** Practical implications of Trojan horse attacks may be limited by computational constraints and detection mechanisms not explored.

## Next Checks

1. Validate observability results across multiple LLM architectures and model sizes beyond GPT-2 and LLaMA-2

2. Implement and test Trojan horse attack scenario in realistic deployment environment with monitoring systems

3. Systematically test proposed mitigation strategies against various attack scenarios to quantify effectiveness