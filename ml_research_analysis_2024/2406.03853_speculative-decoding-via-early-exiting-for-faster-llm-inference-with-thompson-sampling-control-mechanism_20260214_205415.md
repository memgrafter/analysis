---
ver: rpa2
title: Speculative Decoding via Early-exiting for Faster LLM Inference with Thompson
  Sampling Control Mechanism
arxiv_id: '2406.03853'
source_url: https://arxiv.org/abs/2406.03853
tags:
- draft
- eesd
- self
- tokens
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of high inference costs in large
  language models (LLMs) due to their auto-regressive decoding process. The authors
  propose Early-exiting Speculative Decoding (EESD), which uses a segment of the LLM
  to generate draft tokens with early-exiting structures, enhanced by self-distillation.
---

# Speculative Decoding via Early-exiting for Faster LLM Inference with Thompson Sampling Control Mechanism

## Quick Facts
- **arXiv ID**: 2406.03853
- **Source URL**: https://arxiv.org/abs/2406.03853
- **Reference count**: 40
- **Key outcome**: Achieves 2.29× speedup on LLaMA-2-70B and 2.04× on LLaMA-2-13B while maintaining lossless output quality

## Executive Summary
This paper introduces Early-exiting Speculative Decoding (EESD), a novel approach to accelerate large language model (LLM) inference by combining early-exiting structures with speculative decoding. The method generates draft tokens using a segment of the LLM, enhanced with self-distillation, and employs a Thompson Sampling control mechanism to dynamically determine the quantity of draft tokens in each round. The proposed technique achieves significant speedups of up to 2.29× on LLaMA-2-70B and 2.04× on LLaMA-2-13B models while maintaining lossless output quality, outperforming previous speculative decoding methods.

## Method Summary
EESD addresses the high inference costs of LLMs by leveraging a segment of the model to generate draft tokens through an early-exiting structure enhanced with self-distillation. A Thompson Sampling control mechanism dynamically determines the quantity of draft tokens to generate in each round, optimizing the balance between computational efficiency and output quality. The approach integrates speculative decoding with early-exiting mechanisms, allowing for faster generation without compromising the final output's fidelity.

## Key Results
- Achieves 2.29× speedup on LLaMA-2-70B model
- Achieves 2.04× speedup on LLaMA-2-13B model
- Maintains lossless output quality compared to baseline decoding

## Why This Works (Mechanism)
The paper leverages early-exiting structures in combination with speculative decoding to reduce the computational burden of auto-regressive decoding. By using a segment of the LLM to generate draft tokens and employing a Thompson Sampling control mechanism, the method dynamically adjusts the number of draft tokens produced in each round. The self-distillation enhancement improves the quality of the draft tokens, ensuring that the final output remains lossless while significantly reducing inference time.

## Foundational Learning
- **Speculative Decoding**: A technique where draft tokens are generated by a smaller model and verified by the full model, reducing the number of expensive full model calls. Why needed: To minimize the computational cost of auto-regressive generation. Quick check: Compare inference speed with and without speculative decoding.
- **Early-exiting Structures**: Model architectures that allow intermediate outputs to be used as final predictions under certain conditions. Why needed: To reduce computation by stopping processing early when sufficient confidence is reached. Quick check: Measure computational savings from early exits.
- **Thompson Sampling**: A Bayesian approach for multi-armed bandit problems that balances exploration and exploitation. Why needed: To dynamically determine optimal draft token quantities during decoding. Quick check: Evaluate performance across different Thompson Sampling parameters.
- **Self-distillation**: A training technique where a model learns from its own outputs. Why needed: To improve the quality of draft tokens generated by the early-exiting structure. Quick check: Compare token quality with and without self-distillation.

## Architecture Onboarding
- **Component Map**: LLM -> Early-exiting Segment -> Thompson Sampling Controller -> Draft Token Generator -> Full Model Verifier
- **Critical Path**: The early-exiting segment generates draft tokens, which are then verified by the full model. The Thompson Sampling controller dynamically adjusts the number of draft tokens based on the verification results.
- **Design Tradeoffs**: Balancing the number of draft tokens generated (which affects speed) with the risk of verification failures (which affects quality). The self-distillation enhancement improves draft token quality but adds training complexity.
- **Failure Signatures**: If the Thompson Sampling mechanism is poorly tuned, it may generate too many draft tokens leading to verification failures, or too few draft tokens resulting in minimal speedup. Poor self-distillation can lead to low-quality draft tokens requiring frequent verification.
- **First Experiments**:
  1. Measure speedup and verification rate with different Thompson Sampling parameters
  2. Compare draft token quality with and without self-distillation
  3. Test performance on different LLM architectures beyond LLaMA-2

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to LLaMA-2-70B and LLaMA-2-13B models
- Thompson Sampling control mechanism's hyperparameter sensitivity not extensively explored
- Potential quality degradation in long-form generation tasks or specialized domains not addressed

## Confidence
- **High confidence** in experimental results showing speedups on tested LLaMA-2 models
- **Medium confidence** in generalizability of early-exiting approach to other LLM architectures
- **Medium confidence** in Thompson Sampling mechanism's robustness across different workloads
- **Low confidence** in method's performance on non-English languages or specialized domains

## Next Checks
1. Test EESD across a broader range of LLM architectures (Mistral, GPT-3.5 variants) to assess generalizability
2. Evaluate performance degradation in long-form generation tasks (>2000 tokens) and domain-specific applications
3. Conduct hyperparameter sensitivity analysis for the Thompson Sampling mechanism across different hardware configurations