---
ver: rpa2
title: Reducing the Scope of Language Models
arxiv_id: '2410.21597'
source_url: https://arxiv.org/abs/2410.21597
tags:
- reject
- accept
- language
- methods
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates whether large language models (LLMs) can\
  \ be scoped to perform only specific tasks while rejecting irrelevant requests.\
  \ The authors define \u201Cscoping\u201D as a conditional generation task where\
  \ an LLM must accept relevant queries, reject irrelevant ones, and maintain performance\
  \ on accepted tasks."
---

# Reducing the Scope of Language Models

## Quick Facts
- arXiv ID: 2410.21597
- Source URL: https://arxiv.org/abs/2410.21597
- Reference count: 40
- The paper finds that SFT performs best with diverse data while CB excels with low diversity, and combining SFT + CB often preserves the best of both methods.

## Executive Summary
This paper investigates whether large language models can be scoped to perform only specific tasks while rejecting irrelevant requests. The authors define "scoping" as a conditional generation task where an LLM must accept relevant queries, reject irrelevant ones, and maintain performance on accepted tasks. They evaluate six methods—system prompting, supervised fine-tuning (SFT), direct preference optimization (DPO), probing classifiers, circuit breakers (CB), and SFT + CB—across three model families and multiple tasks. The study finds that simple system prompting is insufficient for effective scoping, with SFT excelling in diverse data settings and CB performing better in low-data-diversity scenarios.

## Method Summary
The paper evaluates six methods for scoping LLMs: system prompting, supervised fine-tuning (SFT), direct preference optimization (DPO), probing classifiers, circuit breakers (CB), and a combination of SFT + CB. All methods except probing use LoRA fine-tuning with rank 16, α=16, and dropout of 0.05. The study uses three model families (Mistral-7B, Llama-3.1-8B, Granite-7B) and tasks drawn from the Super-NaturalInstructions dataset. Training data consists of accept queries (relevant tasks) and reject queries (irrelevant tasks). Evaluation uses held-out prompts from all categories, measuring acceptance scores, rejection rates on accept sets, in-distribution reject sets, and out-of-distribution reject sets.

## Key Results
- SFT performs best when training data is diverse, achieving high acceptance on relevant tasks and strong rejection on irrelevant ones
- CB excels in low-data-diversity settings due to its orthogonalization objective being easier to optimize
- Combining SFT + CB often preserves the best of both methods
- Representation analysis shows SFT and DPO only modify tail representations while CB changes representations across the entire context

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SFT and DPO primarily modify the tail representations of the language model context, while CB modifies representations across the entire context.
- Mechanism: SFT and DPO only update the LoRA parameters for the tail of the context, while CB's orthogonalization objective forces changes to representations across all positions in the context.
- Core assumption: The tail of the context contains the most task-relevant information, so modifying only these representations is sufficient for SFT and DPO to achieve their goals.
- Evidence anchors:
  - [abstract]: "Representation analysis shows that SFT and DPO only modify tail representations, while CB changes representations across the entire context, contributing to CB's robustness under adversarial attacks."
  - [section]: "DPO and SFT only change the representations of the tail of the context. Hence it makes sense why CB is more robust under attack... all representations have changed, so it is difficult to find a way to circumvent the changed behavior, while DPO and SFT have 'cracks' which can be exploited."
  - [corpus]: Weak - no corpus evidence provided for this specific mechanism.
- Break condition: If the task-relevant information is distributed throughout the context rather than concentrated in the tail, SFT and DPO may fail to achieve the desired behavior.

### Mechanism 2
- Claim: CB performs better than SFT in low-data-diversity settings because its orthogonalization objective is easier to optimize when the rejection set is small and less diverse.
- Mechanism: CB's objective function (Lgen + Lrej) with orthogonalization encourages the model to push reject task representations away from accept task representations, which is easier to achieve when there are fewer and less diverse reject examples.
- Core assumption: The orthogonalization objective is more straightforward to optimize when the reject set is small and homogeneous.
- Evidence anchors:
  - [abstract]: "Conversely, in settings with low data diversity, the Circuit Breakers method (Zou et al. 2024) provides superior results."
  - [section]: "In Figure 14 we study Sentiment Analysis across different models with more and more diverse rejections sets... CB performs relatively better than SFT when data diversity is low."
  - [corpus]: Weak - no corpus evidence provided for this specific mechanism.
- Break condition: If the reject set becomes too diverse, the orthogonalization objective may become too difficult to optimize effectively.

### Mechanism 3
- Claim: Combining SFT and CB (SFT→CB) preserves the best of both methods by first learning to generate appropriate outputs for accept tasks, then orthogonalizing reject task representations.
- Mechanism: The sequential application of SFT followed by CB allows the model to first learn the desired generation behavior, then refine the rejection behavior through orthogonalization.
- Core assumption: The sequential application of these methods allows each to contribute its strengths without interfering with the other's learning process.
- Evidence anchors:
  - [abstract]: "Combining SFT + CB often preserves the best of both methods."
  - [section]: "In order to improve accept task performance and preserve the benefits of both, we propose to layer CB on top of SFT. We first run SFT, then run CB training afterwards."
  - [corpus]: Weak - no corpus evidence provided for this specific mechanism.
- Break condition: If the SFT phase introduces representations that are difficult to orthogonalize in the subsequent CB phase, the combined method may fail to achieve optimal performance.

## Foundational Learning

- Concept: Supervised Fine-Tuning (SFT)
  - Why needed here: SFT is used to teach the model to generate appropriate outputs for accept tasks and specific rejection responses for reject tasks.
  - Quick check question: What is the difference between SFT and unsupervised pre-training?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RLHF is mentioned as a related technique for aligning language models, though the paper focuses on different methods for scoping.
  - Quick check question: How does RLHF differ from SFT in terms of the training objective?

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: LoRA is used to efficiently fine-tune the language models for all methods except probing.
  - Quick check question: What is the advantage of using LoRA over full fine-tuning?

## Architecture Onboarding

- Component map: Language model (Mistral-7B, Llama-3.1-8B, Granite-7B) -> Accept task examples (Sentiment Analysis, Summarization, Program Execution) -> Reject task examples (Text Completion, Story Composition, Dialogue Generation) -> Methods (Sys., SFT, DPO, Probe, CB, SFT→CB) -> Evaluation metrics (Accept Score, Accept rejection rate, ID Reject rejection rate, OOD Reject rejection rate)

- Critical path:
  1. Select accept and reject tasks
  2. Prepare training data
  3. Choose method(s) to apply
  4. Train the model(s)
  5. Evaluate performance on accept and reject tasks
  6. Analyze results and iterate if necessary

- Design tradeoffs:
  - Data diversity vs. method choice: SFT works best with diverse data, CB excels with low diversity
  - Performance vs. robustness: CB is more robust to adversarial attacks but harder to tune
  - Computational cost vs. accuracy: Probing adds inference overhead but can be effective

- Failure signatures:
  - High rejection rate on accept tasks: Model is over-scoping
  - Low rejection rate on reject tasks: Model is under-scoping
  - Poor performance on accept tasks: Model is not maintaining task capability
  - Sensitivity to hyperparameters: Model is difficult to optimize

- First 3 experiments:
  1. Test system prompting baseline with Sentiment Analysis as accept task and Summarization as reject task
  2. Apply SFT to the same setup and compare performance
  3. Apply CB to the same setup and compare performance with SFT

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between data diversity and quantity for effective scoping, and how does this vary across different model families?
- Basis in paper: Explicit
- Why unresolved: The paper shows that CB performs better with low diversity while SFT excels with high diversity, but does not quantify the exact trade-offs or provide guidelines for different model families.
- What evidence would resolve it: Systematic experiments varying both diversity and quantity across multiple model families, measuring performance metrics for each combination.

### Open Question 2
- Question: How do different scoping methods affect the internal representations of language models across various tasks, and what implications does this have for model robustness?
- Basis in paper: Explicit
- Why unresolved: The paper observes that CB modifies representations across the context while SFT and DPO only change tail representations, but does not fully explore the implications for model robustness or task-specific performance.
- What evidence would resolve it: Detailed analysis of representation changes across tasks and their correlation with robustness metrics under various attack scenarios.

### Open Question 3
- Question: Can scoping methods be effectively layered or combined to achieve superior performance, and what are the potential pitfalls of such approaches?
- Basis in paper: Explicit
- Why unresolved: The paper finds that SFT + CB often preserves the best of both methods, but does not explore other combinations or provide a framework for optimal layering.
- What evidence would resolve it: Experimental results comparing various combinations of scoping methods, including ablation studies to identify critical components.

### Open Question 4
- Question: How does the choice of LoRA rank affect the performance of different scoping methods, and what are the optimal configurations for various tasks?
- Basis in paper: Explicit
- Why unresolved: The paper notes that rank can have a substantial effect on performance, with DPO scaling monotonically while CB-based methods have a sweet spot, but does not provide detailed guidance on optimal configurations.
- What evidence would resolve it: Systematic experiments varying LoRA rank across tasks and methods, identifying optimal configurations and their impact on performance metrics.

### Open Question 5
- Question: What are the computational and practical implications of using probing classifiers versus fine-tuning-based methods for scoping?
- Basis in paper: Explicit
- Why unresolved: The paper mentions that probing methods incur additional inference overhead but does not quantify this or compare it to the computational costs of fine-tuning-based methods.
- What evidence would resolve it: Comparative analysis of inference times, memory usage, and overall computational costs for different scoping methods, considering both training and deployment scenarios.

## Limitations

- The underlying reasons for method effectiveness remain somewhat speculative, particularly the claim that CB's orthogonalization objective is "easier to optimize" in low-diversity settings
- Limited systematic exploration of how method effectiveness varies across different model architectures and pre-training approaches
- Reliance on string-based rejection detection may not capture all forms of task rejection and could be vulnerable to adversarial examples

## Confidence

- **High confidence:** The core empirical findings that SFT performs best with diverse data and CB excels with low diversity are well-supported by systematic experiments across multiple model families and tasks.
- **Medium confidence:** The representation analysis showing different mechanisms of action is compelling but based on indirect evidence, and the interpretation of why CB is more robust to adversarial attacks is plausible but not definitively proven.
- **Low confidence:** The claim that CB's orthogonalization objective is inherently easier to optimize in low-diversity settings is primarily supported by correlation rather than causal evidence.

## Next Checks

1. **Direct causal test of data diversity hypothesis:** Design an experiment that systematically varies the diversity of reject sets while holding other factors constant, measuring not just performance but also training stability and convergence speed to directly test whether orthogonalization is indeed "easier to optimize" in low-diversity settings.

2. **Cross-model generalization study:** Apply the most effective methods (SFT and CB) to a broader range of model architectures (different sizes, different pre-training approaches) to determine whether the observed patterns are model-specific or represent general principles of LLM scoping.

3. **Adversarial robustness benchmarking:** Conduct systematic adversarial attacks targeting the specific weaknesses identified in the representation analysis (tail representations for SFT/DPO, specific contexts for CB) to validate whether the proposed mechanisms accurately predict attack success rates and to identify potential new attack vectors.