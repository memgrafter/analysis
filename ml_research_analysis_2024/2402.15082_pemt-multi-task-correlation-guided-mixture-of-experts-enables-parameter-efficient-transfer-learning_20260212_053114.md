---
ver: rpa2
title: 'PEMT: Multi-Task Correlation Guided Mixture-of-Experts Enables Parameter-Efficient
  Transfer Learning'
arxiv_id: '2402.15082'
source_url: https://arxiv.org/abs/2402.15082
tags:
- task
- tasks
- source
- knowledge
- pemt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PEMT introduces a parameter-efficient fine-tuning framework that
  leverages knowledge from multiple source tasks for improved performance on downstream
  tasks. The core idea is to train task-specific adapters on source tasks, then use
  a mixture-of-experts architecture to combine these adapters based on task correlation
  measured via prompt vectors.
---

# PEMT: Multi-Task Correlation Guided Mixture-of-Experts Enables Parameter-Efficient Transfer Learning

## Quick Facts
- **arXiv ID**: 2402.15082
- **Source URL**: https://arxiv.org/abs/2402.15082
- **Reference count**: 22
- **Primary result**: Achieves significant improvements over full fine-tuning and state-of-the-art parameter-efficient methods across 17 diverse NLP datasets

## Executive Summary
PEMT introduces a parameter-efficient fine-tuning framework that leverages knowledge from multiple source tasks for improved performance on downstream tasks. The core idea is to train task-specific adapters on source tasks, then use a mixture-of-experts architecture to combine these adapters based on task correlation measured via prompt vectors. Task Sparsity Loss is introduced to prioritize relevant source experts. Experimental results demonstrate that PEMT achieves significant improvements over full fine-tuning and state-of-the-art parameter-efficient methods across 17 diverse NLP datasets, with average improvements of over 2 points in full-data settings and 10 points in few-shot scenarios.

## Method Summary
PEMT employs a two-stage training procedure. In Stage 1, task-specific adapters are trained on source tasks (MNLI, QNLI, QQP, SST-2, SQuAD, and ReCoRD) using parameter-efficient fine-tuning methods. Task description prompts are initialized based on distinctive features of each task. In Stage 2, these adapters are frozen and integrated using a mixture-of-experts architecture. The MoE gate measures correlation between target and source tasks using task description prompt vectors, and Task Sparsity Loss is applied to improve gating sparsity. A target task adapter is then trained after the MoE module.

## Key Results
- Achieves average improvements of over 2 points in full-data settings compared to state-of-the-art parameter-efficient methods
- Demonstrates over 10 points improvement in few-shot scenarios
- Shows particular effectiveness on tasks like CB and RTE, with performance gains exceeding 10 points
- Exhibits gradual performance improvement as the number of source tasks increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-specific adapters capture task-specific knowledge which is then combined via MoE
- Mechanism: PEMT freezes source adapters trained in Stage 1 and combines them using MoE gating weights that measure inter-task correlation via prompt vectors
- Core assumption: Task-specific knowledge can be effectively frozen and combined without catastrophic forgetting
- Evidence anchors:
  - [abstract]: "PEMT extends the mixture-of-experts (MoE) framework to capture the transferable knowledge as a weighted combination of adapters trained on source tasks"
  - [section]: "To enable multi-task transfer learning and prevent knowledge forgetting, we freeze the source adapters and integrate them using a mixture-of-experts architecture"
  - [corpus]: No direct corpus evidence found

### Mechanism 2
- Claim: Task description prompts initialize task representations and capture inter-task correlation
- Mechanism: Task prompts are initialized with handcrafted descriptions and combined via attention mechanism to form correlation-guided task representations
- Core assumption: Handcrafted task descriptions can effectively capture task semantics and initialize meaningful prompts
- Evidence anchors:
  - [abstract]: "These weights are determined by a gated unit, measuring the correlation between the target and each source task using task description prompt vectors"
  - [section]: "we propose a simple but effective method to use handcrafted task descriptions as the initialization for the prompt vectors"
  - [corpus]: No direct corpus evidence found

### Mechanism 3
- Claim: Task Sparsity Loss encourages MoE gates to prioritize most relevant source experts
- Mechanism: TSL penalizes similarity between final layer output and expert outputs to enforce sparsity in gating weights
- Core assumption: Sparsity in MoE weights leads to better performance by focusing on most relevant experts
- Evidence anchors:
  - [abstract]: "we also propose the Task Sparsity Loss to improve the sparsity of the gated unit"
  - [section]: "The intuition is to ensure the MoE gate assigns a higher priority to the top-1 source task expert by measuring the similarity between specific expert output and the final layer output"
  - [corpus]: No direct corpus evidence found

## Foundational Learning

- **Concept: Mixture-of-Experts (MoE) architecture**
  - Why needed here: Enables combining multiple task-specific adapters without forgetting source knowledge
  - Quick check question: How does MoE differ from simple weighted averaging of adapters?

- **Concept: Parameter-efficient fine-tuning (PEFT)**
  - Why needed here: Allows adaptation to downstream tasks while freezing most model parameters
  - Quick check question: What are the key differences between Adapter, LoRA, and Prompt Tuning?

- **Concept: Task correlation measurement**
  - Why needed here: Determines which source adapters are most relevant for a given target task
  - Quick check question: How does attention-based correlation measurement work in this context?

## Architecture Onboarding

- **Component map**: Source adapters (frozen) -> MoE gate -> Target adapter
- **Critical path**: Stage 1 training → Stage 2 correlation measurement → MoE combination → Target adaptation
- **Design tradeoffs**:
  - More source tasks → better knowledge transfer but higher inference latency
  - Longer task descriptions → better initialization but more parameters
  - Stronger TSL → more sparsity but potential loss of useful knowledge
- **Failure signatures**:
  - MoE gate weights all close to uniform → correlation measurement failing
  - Target adapter dominates → source knowledge not being effectively utilized
  - Performance worse than single adapter → MoE combination suboptimal
- **First 3 experiments**:
  1. Ablation study removing MoE module (baseline adapter performance)
  2. Correlation analysis showing which source tasks are most useful for each target
  3. Performance vs number of source tasks to find optimal tradeoff point

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PEMT change when using different numbers of source tasks, and what is the optimal number of source tasks for maximizing performance across different target tasks?
- Basis in paper: [explicit] The paper mentions that PEMT exhibits a gradual improvement as the number of source tasks increases, but also notes that computational efficiency declines with more source tasks. It states, "Compared to MixDA, PEMT exhibits a gradual improvement as the number of source tasks increases" and "We also observed that as the number of source tasks increased, the computational efficiency of PEMT gradually declined."
- Why unresolved: The paper does not provide a comprehensive analysis of the trade-off between performance gains and computational costs for different numbers of source tasks. It only mentions a general trend without specifying the optimal number or analyzing the diminishing returns.
- What evidence would resolve it: A detailed study varying the number of source tasks and measuring both performance improvements and computational costs (e.g., training time, inference time, memory usage) for each configuration. This would help identify the point of diminishing returns and the optimal number of source tasks for different scenarios.

### Open Question 2
- Question: How effective is the Task Sparsity Loss (TSL) in improving the sparsity of the MoE gate, and what is its impact on the overall performance of PEMT?
- Basis in paper: [explicit] The paper introduces the Task Sparsity Loss to "ensure the MoE gate assigns a higher priority to the top-1 source task expert" and states that it is incorporated into the fine-tuning loss. It mentions, "To sufficiently utilize the knowledge of the source tasks, we propose the Task Sparsity Loss (TSL) to improve the sparsity of the MoE module."
- Why unresolved: The paper does not provide a detailed analysis of the effectiveness of TSL in improving sparsity or its impact on performance. It does not show how the weight distribution changes with or without TSL, nor does it quantify the performance improvement attributable to TSL.
- What evidence would resolve it: An ablation study comparing the performance of PEMT with and without TSL, along with an analysis of the weight distribution of the MoE gate in both cases. This would demonstrate the effectiveness of TSL in improving sparsity and its contribution to overall performance.

### Open Question 3
- Question: How does the performance of PEMT vary with different task description prompt lengths, and what is the optimal prompt length for different types of tasks?
- Basis in paper: [explicit] The paper mentions that the task description prompt length varies for different tasks and that it is used to measure the correlation between tasks. It states, "The task description is a sentence consisting of a task definition and input-output format based on the distinctive features of various tasks. It should be noted that the description length for different tasks could be different."
- Why unresolved: The paper does not provide an analysis of how the performance of PEMT changes with different prompt lengths. It does not specify the optimal prompt length for different types of tasks or discuss the trade-offs between prompt length and performance.
- What evidence would resolve it: An experiment varying the prompt length for different tasks and measuring the corresponding performance of PEMT. This would help identify the optimal prompt length for different types of tasks and understand the relationship between prompt length and performance.

## Limitations
- The model's performance relies heavily on handcrafted task descriptions, which may introduce subjectivity and bias
- Computational overhead increases with the number of source tasks, potentially limiting scalability
- The effectiveness of Task Sparsity Loss is not thoroughly validated through ablation studies

## Confidence

**High Confidence:**
- PEMT achieves state-of-the-art performance on the tested datasets
- The multi-stage training procedure (Stage 1 source adapters, Stage 2 MoE integration) is technically sound
- Task-specific adapters capture meaningful task-specific knowledge

**Medium Confidence:**
- Handcrafted task descriptions effectively capture task semantics for correlation measurement
- Task Sparsity Loss meaningfully improves performance through enforced sparsity
- Performance gains scale with the number of source tasks (diminishing returns not extensively studied)

**Low Confidence:**
- The correlation measurement mechanism generalizes well to tasks outside the tested benchmarks
- The model maintains performance with significantly more than 6 source tasks
- The handcrafted prompt initialization is optimal or near-optimal

## Next Checks

1. **Prompt Sensitivity Analysis**: Systematically vary the task description templates and measure the impact on correlation measurement accuracy and downstream performance. This would quantify how sensitive the model is to prompt quality and potentially reveal whether automated prompt generation could be as effective.

2. **TSL Ablation Study**: Train PEMT variants with varying strengths of Task Sparsity Loss (including zero) and analyze the tradeoff between sparsity and performance. Additionally, examine the actual gate weight distributions to verify that TSL creates meaningful sparsity rather than just penalizing all weights uniformly.

3. **Scalability Benchmark**: Evaluate PEMT with an expanded set of source tasks (e.g., 12-15 tasks) to measure how performance scales and identify the point of diminishing returns. This would also reveal the practical limits of the MoE architecture in terms of inference efficiency and correlation measurement reliability.