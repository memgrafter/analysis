---
ver: rpa2
title: 'VERITAS: A Unified Approach to Reliability Evaluation'
arxiv_id: '2411.03300'
source_url: https://arxiv.org/abs/2411.03300
tags:
- veritas
- arxiv
- preprint
- document
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VERITAS, a family of hallucination detection
  models designed to assess the reliability of large language models (LLMs) across
  diverse tasks and contexts. VERITAS addresses the challenge of LLM hallucinations
  by employing a multi-task training approach on a unified dataset covering natural
  language inference (NLI), question answering (QA), dialogue verification, and summarization.
---

# VERITAS: A Unified Approach to Reliability Evaluation

## Quick Facts
- arXiv ID: 2411.03300
- Source URL: https://arxiv.org/abs/2411.03300
- Authors: Rajkumar Ramamurthy; Meghana Arakkal Rajeev; Oliver Molenschot; James Zou; Nazneen Rajani
- Reference count: 27
- Key outcome: VERITAS achieves state-of-the-art results on hallucination detection benchmarks, with 10% improvement over similar-sized models and near GPT-4 Turbo performance in LLM-as-a-judge settings.

## Executive Summary
This paper introduces VERITAS, a family of hallucination detection models designed to assess the reliability of large language models across diverse tasks and contexts. VERITAS addresses the challenge of LLM hallucinations by employing a multi-task training approach on a unified dataset covering natural language inference (NLI), question answering (QA), dialogue verification, and summarization. The model leverages both classifier (DeBERTa-v3) and generative (LLaMA) architectures, with the generative variants achieving state-of-the-art performance on major hallucination detection benchmarks. VERITAS models demonstrate a 10% improvement over similar-sized models and approach the performance of GPT-4 Turbo in LLM-as-a-judge settings.

## Method Summary
VERITAS uses multi-task training on a unified dataset comprising NLI, QA, summarization, and dialogue tasks from sources like ANLI, Minicheck, and various QA datasets. The approach trains DeBERTa-v3-Large and LLaMA variants with task-specific templates using cross-entropy loss and QLoRA for parameter efficiency. For generative models, teacher forcing is employed during training. The models are evaluated on VERITAS Bench and established hallucination detection benchmarks including LLM-AggreFact, HaluBench, and HalluDial.

## Key Results
- VERITAS achieves state-of-the-art results on major hallucination detection benchmarks with 10% improvement over similar-sized models
- DeBERTa-based models outperform larger generative models despite having only 440M parameters
- VERITAS approaches GPT-4 Turbo performance in LLM-as-a-judge settings
- The unified multi-task approach successfully generalizes across NLI, QA, and dialogue verification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-task training on diverse formats improves generalization across NLI, QA, and dialogue tasks.
- Mechanism: By unifying hallucination detection as a multi-task problem, the model learns shared representations for fact-checking across different input formats, reducing task-specific overfitting.
- Core assumption: Factuality detection benefits from shared learning signals across textual entailment, question answering, and dialogue verification.
- Evidence anchors:
  - [abstract] "VERITAS achieves state-of-the-art results considering average performance on all major hallucination detection benchmarks"
  - [section 3] Describes curating unified datasets covering NLI, QA, summarization, and dialogue tasks
- Break condition: If the tasks require fundamentally different reasoning patterns, multi-task training could dilute performance rather than improve it.

### Mechanism 2
- Claim: Encoder models like DeBERTa are naturally better suited for fact-checking tasks than decoder models.
- Mechanism: Encoder architectures excel at entailment-based reasoning, which aligns well with the verification nature of fact-checking, allowing them to process context and claims in parallel for more efficient comparison.
- Core assumption: Fact-checking is fundamentally an entailment problem that benefits from the bidirectional context understanding of encoder models.
- Evidence anchors:
  - [section 6] "DeBERTa-based models, despite having relatively fewer parameters (440M), perform exceptionally well on benchmarks such as LLM-AggreFact"
  - [section 6] "Interestingly, generative models such as LLama 3B, even when trained with reasoning traces, could not match the performance of VERITAS DeBERTa"
- Break condition: If fact-checking requires generation of explanations or multi-step reasoning that benefits from autoregressive decoding, encoder models may be suboptimal.

### Mechanism 3
- Claim: Synthetic data generation with LLM-as-judge creates high-quality negative examples for training.
- Mechanism: Using Llama 70B to generate hallucinated answers and dialogue responses creates diverse factual errors following established taxonomies, expanding training coverage beyond naturally occurring errors.
- Core assumption: LLMs can generate realistic and diverse hallucination types that improve model robustness when used as data augmentation.
- Evidence anchors:
  - [section 3.3.1] "To address this, we generate incorrect answers using Llama 70B Instruct, prompting it to produce unfaithful answers based on the given context"
  - [section 3.3.1] "We generate diverse hallucination types following the taxonomy proposed by Mishra et al. (2024)"
- Break condition: If the generated hallucinations don't match real-world error patterns, the model may overfit to synthetic distributions and perform poorly on actual errors.

## Foundational Learning

- Concept: Textual entailment and factuality detection
  - Why needed here: Core to understanding how models determine whether claims are supported by context
  - Quick check question: What's the difference between entailment and contradiction in NLI tasks, and how does this relate to fact-checking?

- Concept: Multi-task learning and transfer learning
  - Why needed here: Understanding how training on multiple related tasks can improve generalization across formats
  - Quick check question: When does multi-task training help vs hurt performance, and what factors determine the optimal task mixture?

- Concept: Encoder vs decoder transformer architectures
  - Why needed here: Critical for understanding why DeBERTa performs better than Llama variants for this specific task
  - Quick check question: What are the fundamental architectural differences between encoders and decoders, and how do these differences affect their suitability for different NLP tasks?

## Architecture Onboarding

- Component map:
  Data pipeline: VERITAS Collection (unified training dataset)
  Model variants: VERITAS DeBERTa (classifier), VERITAS 3B/8B (generative)
  Evaluation: VERITAS Bench (comprehensive benchmark)
  Training loop: Multi-task training with unified or task-specific formatting

- Critical path:
  1. Data curation and formatting for unified input
  2. Model selection (encoder vs decoder) based on task requirements
  3. Multi-task training with appropriate loss functions
  4. Evaluation across all benchmark datasets
  5. Performance comparison against baselines

- Design tradeoffs:
  - Encoder vs decoder: DeBERTa has better fact-checking performance but cannot generate explanations, while Llama variants can explain but with lower accuracy
  - Unified vs task-specific formatting: Unified format simplifies multi-task training but may not be optimal for each task
  - Synthetic vs natural data: Synthetic hallucinations provide more training diversity but may not capture real-world error patterns

- Failure signatures:
  - Poor performance on specific task formats indicates task-specific overfitting
  - Large gap between training and validation performance suggests overfitting to synthetic data
  - Inconsistent predictions across similar examples indicates lack of robust reasoning

- First 3 experiments:
  1. Train DeBERTa on NLI-only data and evaluate on QA and dialogue to measure task transfer capability
  2. Compare multi-task vs single-task training performance across all three formats
  3. Evaluate the impact of synthetic hallucination generation by training with and without generated negative examples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance of generative models for hallucination detection be improved to match that of encoder models like DeBERTa?
- Basis in paper: [explicit] The paper states that despite training generative models like Llama 3B with reasoning traces, they could not match the performance of VERITAS DeBERTa, which is only 440M parameters.
- Why unresolved: The paper suggests that the issue might be due to the label appearing at the end of the generated output, leading the model to focus more on mimicking the style of rationales rather than learning the correct output. However, it does not provide a definitive solution to this problem.
- What evidence would resolve it: Empirical results showing improved performance of generative models when trained with examples that omit rationales, as suggested by the paper.

### Open Question 2
- Question: How can the document length constraint in generative models be effectively addressed for hallucination detection?
- Basis in paper: [explicit] The paper mentions that handling much longer documents remains a challenge for generative models, even with a sequence length of 8192 tokens.
- Why unresolved: The paper only mentions a mitigation strategy of splitting documents into smaller chunks and aggregating the results, but acknowledges that this solution is suboptimal.
- What evidence would resolve it: Development and evaluation of a more effective method for handling long documents in generative models, such as hierarchical processing or dynamic context expansion.

### Open Question 3
- Question: How would alternative backbone architectures, such as Flan-T5, perform compared to the current VERITAS models?
- Basis in paper: [explicit] The paper suggests that exploring alternative architectures like Flan-T5 could lead to performance gains, especially in the multi-task setup.
- Why unresolved: The paper does not provide any experimental results or comparisons with models using different backbone architectures.
- What evidence would resolve it: Direct performance comparison of VERITAS models with models using Flan-T5 or other alternative architectures on the same benchmark datasets.

## Limitations
- VERITAS Collection dataset remains unreleased, limiting exact reproduction and independent validation
- Limited ablation studies on critical design choices like unified vs task-specific formatting
- Does not thoroughly explore why decoder architectures underperform or whether this gap persists with architectural modifications

## Confidence

- **High Confidence**: The core empirical results showing state-of-the-art performance on established hallucination detection benchmarks (LLM-AggreFact, HaluBench, HalluDial)
- **Medium Confidence**: The effectiveness of multi-task training for improving generalization across NLI, QA, and dialogue tasks
- **Low Confidence**: The specific contribution of synthetic hallucination generation to final performance and the generalizability of findings to languages beyond English

## Next Checks

1. **Independent Dataset Construction**: Reconstruct the VERITAS Collection from publicly available sources (ANLI, Minicheck, DROP, NewsQA, etc.) to verify the reported performance improvements and test whether the unified format truly enables cross-task transfer.

2. **Encoder-Decoder Architecture Comparison**: Conduct a controlled experiment comparing DeBERTa and Llama variants on a common fact-checking task with identical training data and evaluation protocols to isolate the architectural contribution to performance differences.

3. **Synthetic Data Impact Analysis**: Train models with and without the LLM-generated hallucinations to quantify the actual contribution of synthetic data augmentation and assess whether generated errors match real-world error patterns through human evaluation.