---
ver: rpa2
title: Tuning LLMs with Contrastive Alignment Instructions for Machine Translation
  in Unseen, Low-resource Languages
arxiv_id: '2401.05811'
source_url: https://arxiv.org/abs/2401.05811
tags:
- unseen
- seen
- languages
- mtinstruct
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addressed the challenge of extending machine translation
  capabilities to previously unseen, low-resource languages using large language models.
  The authors introduced contrastive alignment instructions (AlignInstruct), a cross-lingual
  discriminator built using statistical word alignments, to enhance cross-lingual
  supervision during fine-tuning.
---

# Tuning LLMs with Contrastive Alignment Instructions for Machine Translation in Unseen, Low-resource Languages

## Quick Facts
- arXiv ID: 2401.05811
- Source URL: https://arxiv.org/abs/2401.05811
- Reference count: 40
- This study demonstrated that contrastive alignment instructions (AlignInstruct) significantly improve machine translation quality for unseen, low-resource languages by enhancing cross-lingual supervision through word-level alignment information.

## Executive Summary
This study addresses the challenge of extending machine translation capabilities to previously unseen, low-resource languages using large language models. The authors introduce contrastive alignment instructions (AlignInstruct), a cross-lingual discriminator built using statistical word alignments, to enhance cross-lingual supervision during fine-tuning. Their approach was motivated by the weak cross-lingual signals inherent in low-resource languages. The results showed that MTInstruct effectively induced translation capabilities in unseen languages, AlignInstruct led to consistent improvements in translation quality across 48 translation directions involving English, and discriminator-based instructions outperformed generative counterparts.

## Method Summary
The authors fine-tuned BLOOMZ models (1.1B/3B/7.1B parameters) using LoRA adapters on parallel corpora from OPUS-100 covering 24 unseen languages. They employed a multi-task training approach combining MTInstruct (sentence-level translation instructions) with AlignInstruct (word-level discriminative alignment instructions). Statistical word alignments were extracted using FastAlign and converted into discriminative instructions that asked the model to identify true alignment-enriched instruction pairs. Three training curricula were tested: MT+Align (joint training), Align→MT (alignment first, then translation), and MT+Align→MT (alignment then combined).

## Key Results
- MTInstruct effectively induced translation capabilities in unseen languages, with non-trivial improvements across all metrics for BLOOMZ-24
- AlignInstruct led to consistent improvements in translation quality across all 48 translation directions involving English
- Discriminator-based instructions outperformed generative counterparts as cross-lingual instructions
- AlignInstruct improved performance in 30 zero-shot translation directions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MTInstruct baseline effectively induces translation capabilities for unseen languages by exposing LLMs to translation instructions in low-resource languages.
- Mechanism: The LLM learns cross-lingual representations through repeated exposure to paired sentences and translation prompts, even without pre-training in those languages.
- Core assumption: The LLM's pre-training provides sufficient general language understanding to bootstrap translation in unseen languages when fine-tuned with MTInstruct.
- Evidence anchors:
  - [abstract] "MTInstruct effectively induced translation capabilities in unseen languages using MTInstruct"
  - [section] "Non-trivial improvements in all metrics were evident for BLOOMZ+24 under MTInstruct. This suggests that MTInstruct can induce translation capabilities in unseen languages."
  - [corpus] Weak - no direct citations, but aligns with MTInstruct literature (Li et al., 2023).
- Break condition: If the LLM lacks sufficient pre-training multilingual exposure or if the low-resource languages have vastly different linguistic structures from pre-training languages.

### Mechanism 2
- Claim: AlignInstruct enhances cross-lingual supervision by providing explicit word-level alignment information during fine-tuning.
- Mechanism: Statistical word alignments from FastAlign are converted into discriminative instructions that force the model to recognize and encode word-level cross-lingual relationships, complementing the sentence-level MTInstruct.
- Core assumption: Word-level alignment supervision provides more granular cross-lingual signals than sentence-level MT instructions alone.
- Evidence anchors:
  - [abstract] "AlignInstruct emphasizes cross-lingual supervision via a cross-lingual discriminator built using statistical word alignments"
  - [section] "We anticipated that this treatment forced the model to learn to infer the output by recognizing true alignment-enriched instructions"
  - [corpus] Weak - mentions related work on word alignments but no direct citations for AlignInstruct approach.
- Break condition: If the statistical word alignments are noisy or if the LLM cannot effectively process the discriminative instruction format.

### Mechanism 3
- Claim: Discriminator-based instructions (AlignInstruct) outperform generative variants because they provide clearer, more direct cross-lingual supervision.
- Mechanism: The discriminative format requires simpler yes/no decisions about word alignments, making it easier for the model to learn compared to generating entire translations or corrections.
- Core assumption: Simpler, discriminative tasks are more effective than generative tasks when training data is limited in low-resource scenarios.
- Evidence anchors:
  - [abstract] "Discriminator-based instructions outperformed their generative counterparts as cross-lingual instructions"
  - [section] "Namely, easy, discriminative instructions, rather than hard, generative ones, may be preferred for experiments under similar data constraints"
  - [corpus] Weak - mentions related work on instruction types but no direct citations for this specific comparison.
- Break condition: If the discriminative task format is incompatible with the LLM's architecture or if the generative variants are modified to be simpler.

## Foundational Learning

- Concept: Cross-lingual representation learning
  - Why needed here: The core challenge is enabling translation between languages the model hasn't seen during pre-training, which requires learning cross-lingual mappings
  - Quick check question: How does a model translate between languages it hasn't been explicitly trained on?

- Concept: Statistical word alignment (IBM Model 2)
  - Why needed here: AlignInstruct relies on extracting word-level correspondences between parallel sentences to create supervision signals
  - Quick check question: What is the difference between statistical word alignment and direct translation pairs?

- Concept: Instruction tuning methodology
  - Why needed here: Both MTInstruct and AlignInstruct are instruction-based fine-tuning approaches that leverage the LLM's ability to follow instructions
  - Quick check question: How does instruction tuning differ from traditional task-specific fine-tuning?

## Architecture Onboarding

- Component map:
  BLOOMZ LLM (1.1B/3B/7.1B parameters) -> LoRA adapter -> FastAlign alignments -> Parallel corpora (OPUS-100, Flores-200) -> Training curricula (MT+Align, Align→MT, MT+Align→MT)

- Critical path:
  1. Extract word alignments from parallel corpora using FastAlign
  2. Generate AlignInstruct and MTInstruct training examples
  3. Fine-tune BLOOMZ with LoRA using chosen curriculum
  4. Evaluate translation quality on test sets

- Design tradeoffs:
  - LoRA vs full fine-tuning: Parameter efficiency vs potential performance
  - Discriminator vs generative instructions: Simpler supervision vs richer output
  - Word alignment granularity: More precise supervision vs alignment noise

- Failure signatures:
  - Poor performance on both supervised and zero-shot directions indicates fundamental adaptation failure
  - Good supervised performance but poor zero-shot suggests overfitting to training languages
  - Degradation in supported languages when adding unseen languages indicates catastrophic forgetting

- First 3 experiments:
  1. Fine-tune BLOOMZ-1.1B with MTInstruct only on a single unseen language pair
  2. Add AlignInstruct to the above experiment and compare performance
  3. Test zero-shot translation to/from a supported language after fine-tuning on three unseen languages

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of AlignInstruct compare to other cross-lingual instruction methods like those based on dictionary hints or noisy parallel sentences?
- Basis in paper: [explicit] The authors compare AlignInstruct to two generative variants (HintInstruct and ReviseInstruct) and find that AlignInstruct outperforms them. However, the paper does not compare AlignInstruct to other cross-lingual instruction methods like dictionary-based hints or noisy parallel sentences.
- Why unresolved: The paper focuses on comparing AlignInstruct to its generative variants, but does not explore how it stacks up against other established cross-lingual instruction methods.
- What evidence would resolve it: A direct comparison of AlignInstruct's performance against other cross-lingual instruction methods like dictionary-based hints or noisy parallel sentences on the same set of translation tasks and languages.

### Open Question 2
- Question: How does the performance of AlignInstruct scale with increasing model size beyond 7.1B parameters?
- Basis in paper: [explicit] The authors evaluate AlignInstruct on BLOOMZ models of sizes 1.1B, 3B, and 7.1B and observe consistent improvements. However, they do not experiment with larger models like the 175B BLOOMZ model due to computational constraints.
- Why unresolved: The paper provides evidence of AlignInstruct's effectiveness on medium-sized models but does not explore its potential on larger models which might benefit more from cross-lingual alignment.
- What evidence would resolve it: Experiments evaluating AlignInstruct on models larger than 7.1B parameters, such as the 175B BLOOMZ model, to determine if the performance gains continue to scale with model size.

### Open Question 3
- Question: How does AlignInstruct perform on machine translation tasks involving languages with significantly different scripts or linguistic structures?
- Basis in paper: [explicit] The authors evaluate AlignInstruct on a diverse set of languages including those with different scripts (e.g., Georgian, Khmer, Tajik). However, the paper does not specifically analyze the performance on language pairs with significant script or structural differences.
- Why unresolved: While the paper demonstrates AlignInstruct's effectiveness across various languages, it does not delve into how well it handles language pairs with substantial differences in script or linguistic structure.
- What evidence would resolve it: A detailed analysis of AlignInstruct's performance on language pairs with significant script or structural differences, such as translations between languages using Latin and non-Latin scripts, or between languages with different word order or morphological complexity.

### Open Question 4
- Question: How does the use of monolingual corpora in addition to parallel corpora impact the effectiveness of AlignInstruct for low-resource languages?
- Basis in paper: [inferred] The paper mentions that using monolingual instructions did not improve translation performance and may even harm it due to the difficulty of the task and limited data. However, it does not explore the potential benefits of using large monolingual corpora in addition to parallel corpora.
- Why unresolved: The paper's experiments with monolingual instructions suggest that they are not effective on their own, but it does not investigate whether combining them with parallel corpora could provide additional benefits.
- What evidence would resolve it: Experiments evaluating the performance of AlignInstruct when combined with large monolingual corpora in addition to parallel corpora, to determine if the additional data can improve translation quality for low-resource languages.

## Limitations

- Data Scale and Language Coverage: The study focuses on 24 unseen languages but lacks detailed breakdown of performance across different language families or typological distances from supported languages.
- Alignment Quality Dependence: The approach heavily relies on statistical word alignments from FastAlign, which may introduce noise, particularly for low-resource language pairs with limited parallel data.
- Evaluation Scope: While the study evaluates on 48 translation directions involving English, zero-shot performance across non-English unseen language pairs is not reported.

## Confidence

- High Confidence: The baseline effectiveness of MTInstruct for inducing translation capabilities in unseen languages is well-supported by multiple metrics and model sizes showing consistent improvements.
- Medium Confidence: The superiority of discriminator-based instructions over generative variants is supported by the experimental results, though the comparison could benefit from additional ablation studies.
- Medium Confidence: The consistent improvements across all 48 translation directions suggest robust generalization, though the magnitude of improvement varies significantly across language pairs.

## Next Checks

1. **Alignment Quality Impact**: Conduct controlled experiments varying the quality and quantity of statistical word alignments (e.g., using noisy vs clean alignments) to measure the sensitivity of AlignInstruct performance to alignment quality.

2. **Zero-Shot Non-English Pairs**: Evaluate translation quality for zero-shot directions where neither source nor target language was seen during fine-tuning (e.g., Swahili→Zulu) to assess true cross-lingual generalization beyond English-centric scenarios.

3. **Language Family Analysis**: Perform detailed analysis of performance across different language families (e.g., Indo-European vs Afro-Asiatic vs Sino-Tibetan) to identify whether AlignInstruct's effectiveness varies based on typological distance from supported languages.