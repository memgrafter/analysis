---
ver: rpa2
title: Are Large Language Models Actually Good at Text Style Transfer?
arxiv_id: '2406.05885'
source_url: https://arxiv.org/abs/2406.05885
tags:
- sentiment
- transfer
- style
- content
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates large language models (LLMs) for text style
  transfer (TST) across sentiment transfer and text detoxification tasks in English,
  Hindi, and Bengali. It compares zero-shot and few-shot prompting with parameter-efficient
  finetuning, using automatic metrics, GPT-4, and human evaluations.
---

# Are Large Language Models Actually Good at Text Style Transfer?

## Quick Facts
- arXiv ID: 2406.05885
- Source URL: https://arxiv.org/abs/2406.05885
- Reference count: 23
- Primary result: Finetuning significantly improves multilingual TST performance, with GPT-3.5 excelling in zero-shot prompting

## Executive Summary
This study comprehensively evaluates large language models for text style transfer (TST) across sentiment transfer and text detoxification tasks in English, Hindi, and Bengali. The research compares zero-shot and few-shot prompting with parameter-efficient finetuning, using automatic metrics, GPT-4, and human evaluations. Results demonstrate that GPT-3.5 performs best in zero-shot prompting scenarios, while finetuning substantially improves performance of open LLMs, making them comparable to previous state-of-the-art models. English consistently shows the highest performance across all evaluation metrics, with multilingual capabilities remaining limited compared to monolingual performance.

## Method Summary
The study evaluates LLMs using zero-shot and few-shot prompting, as well as parameter-efficient finetuning via LoRA on publicly available datasets for sentiment transfer and text detoxification. The datasets comprise 1,000 style-parallel examples per language, split into 400 for finetuning, 100 for development, and 500 for testing. Evaluation uses automatic metrics (ACC, CS, BLEU), GPT-4-based evaluation, and human assessments. The methodology includes 15 LLMs ranging from 1B to 30B parameters, covering various architectures including BLOOM, ChatGLM, Falcon, Llama, Mistral, OPT, and Zephyr.

## Key Results
- Finetuning significantly improves TST performance across all tested LLMs, making them comparable to previous state-of-the-art models
- GPT-3.5 performs best in zero-shot prompting scenarios across all languages and tasks
- English consistently shows the highest performance, with Hindi and Bengali showing average performance that improves significantly with finetuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Finetuning significantly improves multilingual TST performance across all tested LLMs.
- Mechanism: Parameter-efficient fine-tuning adapts general-purpose LLMs to the specific linguistic patterns and stylistic constraints of sentiment transfer and detoxification tasks in English, Hindi, and Bengali.
- Core assumption: The model has already learned generalizable linguistic representations that can be effectively specialized for TST through fine-tuning.
- Evidence anchors:
  - [abstract] "finetuning significantly improves results compared to zero-shot and few-shot prompting, making them comparable to previous state-of-the-art"
  - [section 4.1] "Most finetuned LLMs are comparable to prompted GPT-3.5 and previous SOTA models"
  - [corpus] Weak - no direct corpus evidence for this mechanism specifically
- Break condition: If the underlying model lacks adequate general linguistic representations, or if the fine-tuning data is insufficient or of poor quality.

### Mechanism 2
- Claim: Zero-shot prompting works better for English than for Hindi and Bengali in TST tasks.
- Mechanism: English benefits from the higher prevalence of English in pre-training corpora, enabling better generalization for style transfer without task-specific adaptation.
- Core assumption: LLMs trained on large corpora contain sufficient stylistic patterns in English to handle zero-shot style transfer.
- Evidence anchors:
  - [abstract] "English consistently shows the highest performance" and "their performance in on other languages (Hindi, Bengali) remains average"
  - [section 4.1] "English consistently shows the highest performance" and "Hindi, while more challenging, benefits significantly from few-shot and finetuning"
  - [corpus] Weak - no direct corpus evidence for language-specific pretraining data composition
- Break condition: If the model has been specifically trained or fine-tuned on multilingual data with balanced language representation.

### Mechanism 3
- Claim: Larger models generally show better TST performance, but gains diminish with increasing size.
- Mechanism: Larger models capture more nuanced linguistic patterns and stylistic variations, but the marginal benefit decreases beyond a certain parameter threshold.
- Core assumption: Model capacity correlates with ability to learn complex style transfer patterns.
- Evidence anchors:
  - [section 4.1] "Generally, larger models score better across the board, but gains diminish with increasing size: The jump from 1B to 3B shows a significant boost; improvements from 3B to 7B and 7B to 13B are less pronounced; 30B models do not improve over their smaller counterparts"
  - [corpus] No corpus evidence available for this claim
- Break condition: If architectural limitations or optimization issues prevent larger models from effectively utilizing their additional parameters.

## Foundational Learning

- Concept: Style Transfer Trade-off
  - Why needed here: Understanding the balance between style accuracy and content preservation is critical for evaluating TST performance
  - Quick check question: What metric captures both style transfer accuracy and content preservation simultaneously?

- Concept: Parameter-Efficient Fine-tuning
  - Why needed here: The study uses LoRA (Hu et al., 2022a) for fine-tuning, which requires understanding how to adapt large models efficiently
  - Quick check question: How does LoRA modify the weight matrices during fine-tuning?

- Concept: Multilingual Model Evaluation
  - Why needed here: The study evaluates models across three languages, requiring understanding of cross-lingual performance assessment
  - Quick check question: What factors might explain why English outperforms Hindi and Bengali in zero-shot prompting?

## Architecture Onboarding

- Component map: Pre-trained LLMs -> Prompt/Finetune -> Evaluation (automatic -> GPT-4 -> Human)
- Critical path: Model selection → Prompt/Finetune → Evaluation (automatic → GPT-4 → Human)
- Design tradeoffs:
  - Zero-shot vs. few-shot vs. fine-tuning: balance between ease of use and performance
  - Automatic metrics vs. human evaluation: trade-off between scalability and reliability
  - Model size vs. computational efficiency: larger models perform better but require more resources
- Failure signatures:
  - Low ACC but high CS: model preserves content but fails to transfer style
  - High ACC but low CS: model transfers style but alters content significantly
  - Consistently poor performance across languages: fundamental limitations of the approach
- First 3 experiments:
  1. Run zero-shot prompting on English sentiment transfer and compare to few-shot results
  2. Fine-tune a 7B parameter model on Hindi sentiment transfer and evaluate performance gains
  3. Compare automatic metrics scores with GPT-4-based evaluation on a sample of outputs

## Open Questions the Paper Calls Out

- Question: How do different prompting strategies (e.g., Chain-of-Thought, Tree of Thoughts) compare to standard prompting for text style transfer tasks across multiple languages?
  - Basis in paper: [explicit] The paper mentions that they will explore advanced prompting techniques in the future, including Yao et al. (2024) and Wei et al. (2022).
  - Why unresolved: The current study only used basic prompting techniques (zero-shot and few-shot), leaving the potential benefits of more sophisticated prompting strategies unexplored.
  - What evidence would resolve it: Conducting experiments comparing the performance of various advanced prompting techniques against the current baseline for text style transfer tasks in multiple languages.

- Question: What is the impact of model size on text style transfer performance beyond 30B parameters?
  - Basis in paper: [explicit] The paper observes that improvements from 3B to 7B and 7B to 13B are less pronounced, and 30B models do not improve over their smaller counterparts.
  - Why unresolved: The study only tested models up to 30B parameters, leaving the potential performance gains of even larger models unknown.
  - What evidence would resolve it: Testing text style transfer performance using models with more than 30B parameters and comparing the results to the current findings.

- Question: How does the performance of text style transfer models vary across different stylistic transformations (e.g., formality, humor, sarcasm) beyond sentiment transfer and text detoxification?
  - Basis in paper: [explicit] The paper acknowledges that their evaluation focuses on sentiment transfer and text detoxification, omitting other TST tasks.
  - Why unresolved: The study only evaluated two specific text style transfer tasks, leaving the generalizability of the findings to other stylistic transformations unknown.
  - What evidence would resolve it: Conducting experiments to evaluate the performance of text style transfer models on a wider range of stylistic transformations, including formality, humor, and sarcasm, across multiple languages.

## Limitations

- The evaluation relies heavily on automatic metrics (ACC, CS, BLEU) which may not fully capture the nuanced quality of style transfer, despite validation with GPT-4 and human evaluations.
- The multilingual evaluation is limited to only three languages (English, Hindi, Bengali), making it difficult to generalize findings to other language families.
- The study does not explore the impact of prompt engineering variations on performance, which could be a significant factor in zero-shot and few-shot settings.

## Confidence

**High Confidence:** The finding that finetuning significantly improves TST performance across all tested LLMs is well-supported by the empirical results showing consistent performance gains across different model sizes and tasks.

**Medium Confidence:** The observation that English consistently outperforms Hindi and Bengali in zero-shot prompting is supported by the results, but the underlying reasons (such as pre-training data composition) are not directly verified.

**Low Confidence:** The claim that gains from increasing model size diminish beyond certain thresholds is based on observed patterns but lacks theoretical justification or extensive testing across a wider range of model sizes.

## Next Checks

1. **Prompt Engineering Analysis:** Systematically vary prompt templates, exemplars, and instructions in zero-shot and few-shot settings to quantify the impact of prompt quality on TST performance across different languages.

2. **Cross-Lingual Generalization Test:** Evaluate the same finetuned models on a held-out fourth language (e.g., Tamil or Telugu) to assess whether improvements transfer across related languages or are language-specific.

3. **Human Evaluation Expansion:** Conduct a larger-scale human evaluation study with diverse annotators to validate the automatic metrics and GPT-4 evaluations, particularly focusing on the trade-off between style transfer accuracy and content preservation.