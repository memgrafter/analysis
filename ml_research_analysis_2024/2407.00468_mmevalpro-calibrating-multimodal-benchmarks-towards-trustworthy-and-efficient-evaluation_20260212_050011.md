---
ver: rpa2
title: 'MMEvalPro: Calibrating Multimodal Benchmarks Towards Trustworthy and Efficient
  Evaluation'
arxiv_id: '2407.00468'
source_url: https://arxiv.org/abs/2407.00468
tags:
- question
- valpro
- answer
- questions
- options
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MMEvalPro addresses systematic biases in multimodal benchmarks\
  \ where large language models (LLMs) without visual perception achieve non-trivial\
  \ performance. The proposed method augments existing multiple-choice questions with\
  \ two additional prerequisite questions\u2014one perception and one knowledge question\u2014\
  forming a question triplet."
---

# MMEvalPro: Calibrating Multimodal Benchmarks Towards Trustworthy and Efficient Evaluation

## Quick Facts
- arXiv ID: 2407.00468
- Source URL: https://arxiv.org/abs/2407.00468
- Reference count: 16
- Primary result: Triplet-structured benchmark exposes superficial performance in multimodal evaluation, revealing large LLMs without vision can achieve 48.52% accuracy on existing benchmarks

## Executive Summary
MMEvalPro addresses systematic biases in multimodal benchmarks where large language models without visual perception achieve non-trivial performance. The proposed method augments existing multiple-choice questions with two additional prerequisite questions—one perception and one knowledge question—forming a question triplet. This structure enables evaluation of genuine multimodal understanding rather than guessing or shortcut exploitation. The benchmark contains 2,138 question triplets (6,414 total questions) sourced from MMMU, ScienceQA, and MathVista. Experimental results with 17 models show MMEVALPRO is more challenging and more trustworthy, effectively distinguishing true multimodal capabilities from superficial performance gains.

## Method Summary
The core innovation involves creating question triplets where each original multiple-choice question is augmented with two prerequisite questions: a perception question requiring visual detail recognition and a knowledge anchor question building on those perceptions. Human annotators create these questions through a meticulous process ensuring visual information is essential. The evaluation uses Genuine Accuracy (GA) as the primary metric, requiring correct answers to all three questions in a triplet, while Average Accuracy (AA) measures overall performance. The dataset contains 2,138 triplets from MMMU, ScienceQA, and MathVista, evaluated across 17 models including LLMs and LMMs in zero-shot or few-shot settings.

## Key Results
- MMEVALPRO is more challenging: best LMM trails human by 31.73% in genuine accuracy versus 8.03% in previous benchmarks
- MMEVALPRO is more trustworthy: best LLM lags best LMM by 23.09% versus 14.64% in prior benchmarks
- Question triplets expose answer inconsistency, with significant consistency gaps revealing superficial performance gains

## Why This Works (Mechanism)

### Mechanism 1
**Claim**: Triplet structure (original + perception + knowledge) exposes superficial performance.
**Mechanism**: The triplet forces models to demonstrate both visual perception (identifying objects, relationships) and knowledge application (using domain concepts to solve problems). If a model guesses correctly on the original but fails the prerequisites, it reveals answer inconsistency rather than true understanding.
**Core assumption**: Genuine multimodal reasoning requires both visual perception and domain knowledge in sequence.
**Evidence anchors**: [abstract]: "For each original question... human annotators augment it by creating one perception question and one knowledge anchor question... Genuine Accuracy as the primary metric, which depends on whether the model answers the triplet questions concurrently."
**Break condition**: If perception and knowledge questions can be answered without seeing the image, the mechanism fails to distinguish LLMs from LMMs.

### Mechanism 2
**Claim**: Genuine Accuracy (GA) metric provides more trustworthy evaluation than Average Accuracy (AA).
**Mechanism**: GA requires all three questions in a triplet to be answered correctly simultaneously, penalizing models that get the original question right but fail prerequisites. This reveals answer inconsistency that AA masks by averaging.
**Core assumption**: A model that understands the problem should be able to answer all related questions, not just the final one.
**Evidence anchors**: [abstract]: "We propose Genuine Accuracy as the main metric, which depends on whether the model answers the triplet questions concurrently."
**Break condition**: If models can consistently guess all three questions correctly, GA loses its discriminative power.

### Mechanism 3
**Claim**: Question triplet annotation process ensures visual information is essential.
**Mechanism**: Human annotators create perception questions that require visual detail recognition and knowledge questions that build on those perceptions. This ensures questions cannot be answered without image input, unlike biased benchmarks where text alone suffices.
**Core assumption**: Carefully designed perception questions will be unanswerable without visual processing.
**Evidence anchors**: [abstract]: "Human annotators augment it by creating one perception question and one knowledge anchor question through a meticulous annotation process."
**Break condition**: If perception questions become too easy or knowledge questions too general, models can exploit them without genuine visual understanding.

## Foundational Learning

- **Concept**: Multimodal reasoning pipeline (perception → knowledge application → answer)
  - Why needed here: Understanding how models should process multimodal questions helps identify when they're taking shortcuts versus genuine reasoning
  - Quick check question: What are the two key steps humans follow when solving multimodal reasoning questions?

- **Concept**: Type-I error in evaluation (false positive identification of capability)
  - Why needed here: Recognizing that models can appear capable by guessing reveals why standard benchmarks are unreliable
  - Quick check question: Why might a model correctly answer a question without actually understanding it?

- **Concept**: Consistency metrics (conditional probability of answering prerequisites given correct original answer)
  - Why needed here: These metrics reveal whether models have coherent reasoning chains or just lucky guesses
  - Quick check question: What does low Perception Consistency indicate about a model's understanding?

## Architecture Onboarding

- **Component map**: Source benchmarks → Human annotation → Triplet construction → Model inference → Metric computation → Analysis interpretation
- **Critical path**: Question annotation → Model inference → Metric computation → Analysis interpretation
- **Design tradeoffs**:
  - Manual annotation vs scalability: High quality requires human expertise but limits dataset size
  - Model prompt design: 1-shot for LLMs ensures fair comparison but may not reflect optimal usage
  - Metric complexity vs interpretability: Multiple metrics provide insight but increase analysis burden
- **Failure signatures**:
  - High AA but low GA indicates answer inconsistency
  - High PC but low KC suggests visual recognition without domain knowledge
  - Small LLM-LMM gap indicates benchmark bias toward text-only solutions
- **First 3 experiments**:
  1. Run baseline models (GPT-4o with/without vision) on original benchmarks vs MMEVALPRO to verify gap amplification
  2. Compute consistency metrics for each model to identify failure patterns
  3. Visualize conditional accuracy heatmaps to understand correlation between question types

## Open Questions the Paper Calls Out

### Open Question 1
**Question**: How does the performance gap between LLMs and LMMs on MMEVALPRO evolve as models scale in size, particularly for the largest available models like GPT-4o and Qwen-VL-Max?
**Basis in paper**: [inferred] The paper notes that larger LLMs generally perform better on both benchmarks, but this trend doesn't hold for the LLaMA-3 series, and the performance gap between the best LLM and LMM is more pronounced on MMEVALPRO (23.09%) compared to original benchmarks (14.64%).
**Why unresolved**: The paper doesn't provide a detailed scaling analysis across different model sizes, particularly focusing on the largest frontier models and their relative performance on MMEVALPRO versus original benchmarks.
**What evidence would resolve it**: Comprehensive experiments comparing multiple sizes of both LLMs and LMMs (including frontier models) on both MMEVALPRO and original benchmarks, showing how the performance gap scales with model size.

### Open Question 2
**Question**: What specific aspects of the perception and knowledge questions in MMEVALPRO are most challenging for current LMMs, and how do these challenges differ from those faced by LLMs?
**Basis in paper**: [explicit] The paper discusses that LMMs show greater advantage in Perception Accuracy (PA) compared to Knowledge Accuracy (KA) when contrasted with LLMs, and that both LMMs and LLMs struggle with answer consistency (PC and KC metrics).
**Why unresolved**: While the paper identifies that answer consistency is a key challenge and that LMMs have relative strengths in perception tasks, it doesn't provide detailed analysis of which specific types of perception or knowledge questions are most problematic or why.
**What evidence would resolve it**: Detailed error analysis categorizing perception and knowledge question types, identifying which categories have the lowest accuracy rates and what specific difficulties (e.g., spatial reasoning, object counting, causal relationships) models struggle with most.

### Open Question 3
**Question**: How do the Consistency Gap (CG), Perception Consistency (PC), and Knowledge Consistency (KC) metrics correlate with other established measures of multimodal reasoning ability, and can they predict real-world task performance?
**Basis in paper**: [explicit] The paper introduces these metrics as novel ways to measure answer consistency and genuine understanding, showing that humans have much higher consistency than both LLMs and LMMs, but doesn't explore their relationship to other evaluation metrics or practical applications.
**Why unresolved**: The paper establishes these metrics as important for understanding model limitations but doesn't investigate whether they correlate with other benchmarks, transfer to real-world performance, or relate to other established measures of reasoning ability.
**What evidence would resolve it**: Correlation studies between MMEVALPRO consistency metrics and other multimodal benchmarks, plus experiments testing whether models with higher consistency on MMEVALPRO perform better on real-world multimodal tasks or transfer more effectively to new domains.

## Limitations
- **Annotation scalability**: The triplet annotation process requires significant human expertise and effort, limiting dataset size to 2,138 question triplets
- **Model capability assumptions**: The benchmark assumes visual perception and knowledge application are distinct capabilities that can be separately tested
- **Dataset bias**: While MMEVALPRO addresses biases in existing benchmarks, it may introduce new biases through its specific annotation methodology

## Confidence
- **High confidence**: The mechanism that triplet structure exposes superficial performance (Mechanism 1) is well-supported by experimental results showing large consistency gaps between AA and GA
- **Medium confidence**: The claim that GA provides more trustworthy evaluation than AA (Mechanism 2) is supported but depends on the assumption that models should answer all related questions correctly when they understand the problem
- **Medium confidence**: The claim that annotation ensures visual information is essential (Mechanism 3) is supported by quantitative analysis but relies on the quality of human annotation

## Next Checks
1. **Cross-dataset generalization**: Test MMEVALPRO's effectiveness on additional multimodal datasets beyond MMMU, ScienceQA, and MathVista to verify robustness across different domains and question types
2. **Annotation consistency analysis**: Measure inter-annotator agreement rates and conduct ablation studies removing either perception or knowledge questions to quantify their individual contributions to the triplet structure
3. **Model-specific capability mapping**: Analyze which model architectures show different patterns of consistency gaps to identify whether the benchmark effectively distinguishes genuine multimodal reasoning from text-only shortcuts