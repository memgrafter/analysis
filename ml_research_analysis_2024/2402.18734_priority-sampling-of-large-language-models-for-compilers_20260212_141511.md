---
ver: rpa2
title: Priority Sampling of Large Language Models for Compilers
arxiv_id: '2402.18734'
source_url: https://arxiv.org/abs/2402.18734
tags:
- sampling
- priority
- samples
- tokens
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) often produce repetitive and incoherent
  samples when generating multiple outputs, particularly with standard techniques
  like Nucleus Sampling. This limits their utility in exploring diverse solutions
  or understanding the model's full capability, especially in tasks like code generation
  and optimization.
---

# Priority Sampling of Large Language Models for Compilers

## Quick Facts
- arXiv ID: 2402.18734
- Source URL: https://arxiv.org/abs/2402.18734
- Reference count: 31
- One-line primary result: Priority Sampling achieves 91% of autotuner performance in 5 samples for LLVM optimization tasks.

## Executive Summary
Large language models often generate repetitive and incoherent outputs when using standard sampling techniques like Nucleus Sampling. This limits their utility in exploring diverse solutions or understanding the model's full capability, especially in tasks like code generation and optimization. Priority Sampling addresses this by introducing a deterministic sampling technique that generates unique samples ordered by the model's confidence, expanding a search tree by always branching at the token with the highest probability.

We evaluate Priority Sampling on optimizing LLVM compiler passes, where the model predicts optimization sequences. Priority Sampling significantly outperforms Nucleus Sampling, achieving 91% of the autotuner's performance in just 5 samples and surpassing the autotuner used to generate training labels in 30 samples. This demonstrates that LLMs can access substantial latent knowledge through intelligent search tree expansion. Priority Sampling offers a simple, effective, and deterministic approach to generating diverse, high-quality samples, making it a valuable tool for leveraging LLM capabilities in practical applications.

## Method Summary
Priority Sampling is a deterministic sampling technique that generates unique samples ordered by the model's confidence. It expands a search tree by always branching at the token with the highest probability, ensuring diverse and coherent outputs. The method uses a priority queue to track unexpanded tokens and their probabilities, generating samples sequentially by always selecting the highest probability unexpanded token. Priority Sampling also supports generation constrained by regular expressions, enabling structured and controllable exploration. The approach was evaluated on the task of optimizing LLVM compiler passes, where the model predicts optimization sequences, demonstrating significant improvements over Nucleus Sampling and achieving 91% of autotuner performance in just 5 samples.

## Key Results
- Priority Sampling achieves 91% of autotuner performance in 5 samples for LLVM optimization tasks.
- Priority Sampling surpasses the autotuner used to generate training labels in 30 samples.
- The method generates unique, high-quality samples while maintaining memory efficiency through a constant-size priority queue.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deterministic selection of the highest-probability unexpanded token ensures both uniqueness and confidence ordering.
- Mechanism: Priority Sampling maintains a search tree and a priority queue. At each step, it expands the unexpanded token with the highest probability, guaranteeing that each new sample is unique and ordered by the model's confidence.
- Core assumption: The highest probability token in the current search tree always leads to the most confident (and thus potentially highest-quality) next sample.
- Evidence anchors:
  - [abstract] "Each new sample expands the unexpanded token with the highest probability in the augmented search tree."
  - [section] "Always focus the search towards the most interesting direction based on previous samples, rather than determining this in advance."
  - [corpus] Weak evidence; corpus focuses on sampling diversity rather than deterministic confidence ordering.
- Break condition: If the model's probability distribution is poorly calibrated, the highest probability token may not correspond to the best quality sample.

### Mechanism 2
- Claim: Regular expression constraints ensure generated samples are syntactically valid and controllable.
- Mechanism: Priority Sampling filters candidate tokens at each step using a finite state machine to ensure they satisfy the provided regular expression, preventing invalid or malformed outputs.
- Core assumption: Regular expression constraints can be efficiently checked at each decoding step without significant computational overhead.
- Evidence anchors:
  - [abstract] "Additionally, Priority Sampling supports generation based on regular expression that provides a controllable and structured exploration process."
  - [section] "we exclude all tokens that don't satisfy the regular expression we define when combined with previous tokens. This can be done in constant time by using a finite state machine."
  - [corpus] Weak evidence; corpus does not discuss regular expression-based sampling constraints.
- Break condition: If the regular expression is overly restrictive, it may eliminate all valid continuations, halting generation.

### Mechanism 3
- Claim: Constant-size priority queue maintains memory efficiency while ensuring sufficient branching candidates.
- Mechanism: Priority Sampling limits the priority queue to the number of samples being generated, avoiding the need to store all possible tokens in the vocabulary for each node in the search tree.
- Core assumption: A constant-size queue provides enough diversity of candidates for effective branching while keeping memory requirements manageable.
- Evidence anchors:
  - [section] "Additionally, the memory requirements are significantly reduced by keeping the size of the priority queue constant, equal to the number of samples we generate."
  - [abstract] "Additionally, it supports generation constrained by regular expressions, enabling structured and controllable exploration."
  - [corpus] Weak evidence; corpus neighbors focus on different sampling techniques without discussing memory efficiency in this context.
- Break condition: If the branching factor is too constrained, the method may miss high-quality samples that require exploring less probable paths.

## Foundational Learning

- Concept: Search tree expansion and priority queues
  - Why needed here: Priority Sampling fundamentally relies on maintaining and expanding a search tree while tracking the most promising unexpanded nodes in a priority queue.
  - Quick check question: What data structure would you use to efficiently find and remove the highest probability unexpanded token during search tree expansion?

- Concept: Regular expression compilation and finite state machines
  - Why needed here: The method uses finite state machines to efficiently check whether partial sequences satisfy the regular expression constraints.
  - Quick check question: How would you modify a standard regular expression matcher to efficiently check if a token sequence is a valid prefix of a string matching the regex?

- Concept: Token probability distributions and decoding strategies
  - Why needed here: Understanding how probability distributions over tokens are generated and how different decoding strategies (greedy, nucleus, etc.) work is essential to grasp why Priority Sampling's deterministic approach is effective.
  - Quick check question: What is the main difference between how greedy decoding and nucleus sampling select the next token?

## Architecture Onboarding

- Component map:
  - Search tree: Represents all generated sequences and their possible continuations
  - Priority queue: Stores unexpanded tokens with their probabilities for efficient retrieval
  - Regular expression validator: Uses finite state machine to check token sequence validity
  - Model inference engine: Generates probability distributions for token continuation
  - Token mask: Stores the optimal path for deterministic generation

- Critical path:
  1. Generate first sample using greedy decoding
  2. Populate priority queue with top-K valid continuations
  3. For each subsequent sample:
     - Pop highest probability unexpanded token from queue
     - Generate sequence from that point using model inference
     - Update priority queue with new valid continuations
     - Update token mask with best path

- Design tradeoffs:
  - Deterministic vs stochastic sampling: Priority Sampling guarantees uniqueness but may miss some valid paths
  - Memory vs quality: Constant-size queue saves memory but may limit exploration
  - Regular expression strictness vs generation success: More restrictive regex ensures validity but may halt generation

- Failure signatures:
  - Empty priority queue before generating all samples: Regular expression too restrictive
  - All samples have similar quality: Branching factor too small or model probabilities poorly calibrated
  - Memory overflow: Priority queue size too large for available resources

- First 3 experiments:
  1. Generate 5 samples on a simple task without regular expression constraints and compare uniqueness rate vs nucleus sampling
  2. Apply the same task with a strict regular expression and measure generation success rate
  3. Vary the branching factor (K) and measure the tradeoff between sample quality and memory usage

## Open Questions the Paper Calls Out
- None explicitly stated in the paper.

## Limitations
- The method's effectiveness relies heavily on the model's probability distribution being well-calibrated, which may not hold for all tasks or model architectures.
- Regular expression constraints, while providing structure, may be too restrictive for some applications, potentially halting generation if no valid continuations exist.
- The deterministic nature of Priority Sampling may miss valid but less probable solutions that stochastic methods could discover.

## Confidence

- **High confidence:** The method's effectiveness in generating unique, high-quality samples for LLVM optimization passes is well-supported by the empirical results showing 91% of autotuner performance in 5 samples and surpassing the autotuner in 30 samples.
- **Medium confidence:** The claims about memory efficiency due to constant-size priority queue are plausible but not extensively validated. The performance gains could vary significantly depending on task complexity and model architecture.
- **Low confidence:** The general applicability of Priority Sampling across different domains and tasks beyond compiler optimization remains to be proven. The method's behavior with highly complex or ambiguous regular expressions is uncertain.

## Next Checks

1. **Cross-domain evaluation:** Test Priority Sampling on diverse NLP tasks (translation, summarization, dialogue generation) to assess its effectiveness beyond compiler optimization. Compare uniqueness rates, sample quality, and generation success against nucleus sampling and greedy decoding.

2. **Regular expression stress test:** Systematically evaluate the method's behavior with increasingly complex and restrictive regular expressions to identify failure modes and performance degradation points. Measure generation success rate, average sample quality, and computational overhead.

3. **Probability calibration analysis:** Investigate the correlation between token probability and sample quality across different model families and tasks. If the correlation is weak, explore adaptive probability scaling or alternative selection criteria to improve sample quality without sacrificing uniqueness.