---
ver: rpa2
title: To Code, or Not To Code? Exploring Impact of Code in Pre-training
arxiv_id: '2408.10914'
source_url: https://arxiv.org/abs/2408.10914
tags:
- code
- data
- pre-training
- performance
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically investigates the impact of code data
  on general LLM performance across natural language reasoning, world knowledge, code
  generation, and generative quality. Through extensive controlled pre-training experiments,
  the authors find that including code data consistently improves non-code performance.
---

# To Code, or Not To Code? Exploring Impact of Code in Pre-training

## Quick Facts
- **arXiv ID**: 2408.10914
- **Source URL**: https://arxiv.org/abs/2408.10914
- **Reference count**: 40
- **Key outcome**: Including code data in LLM pre-training improves both code and non-code performance, with synthetic code data providing outsized gains.

## Executive Summary
This paper systematically investigates how code data affects general LLM performance across natural language reasoning, world knowledge, code generation, and generative quality. Through controlled pre-training experiments, the authors find that code inclusion consistently improves non-code performance, with their best model achieving 8.2% relative improvement in natural language reasoning and 12x boost in code performance compared to text-only training. The study reveals that code quality matters significantly - synthetic code data in small proportions provides outsized gains (9% on reasoning, 44.9% on code), and including code in the final cooldown phase further improves performance across all tasks.

## Method Summary
The study pre-trains models using SlimPajama (text-only), Stack dataset (web-based code), and synthetic code data across different proportions and phases. Models are trained in phases: continued pretraining (200B tokens) and cooldown (40B tokens), with variants including text-only, balanced (50% code/50% text), balanced→text, and code→text configurations. Evaluation spans 11 natural language reasoning benchmarks, code generation tasks (HumanEval-Python, MBPP), world knowledge (TriviaQA, NaturalQuestionsOpen), and LLM-as-a-judge win-rates. The authors test models of 470M and 2.8B parameters to understand the impact of code data at different scales.

## Key Results
- Including code data in pre-training improves natural language reasoning by 8.2% and world knowledge by 4.2% relative to text-only training
- Synthetic code data (10% of code data) provides outsized gains: 9% on reasoning and 44.9% on code benchmarks
- Including code in the cooldown phase improves NL reasoning by 3.6%, world knowledge by 10.1%, and code performance by 20%
- Balanced initialization followed by text-only continued training achieves optimal overall performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Code data acts as a "structure scaffold" that transfers generalizable reasoning patterns to natural language tasks
- Mechanism: Code's explicit syntactic structure and logical flow patterns train the model to recognize hierarchical relationships, which generalize to natural language reasoning tasks requiring structured inference
- Core assumption: The model can transfer learned structural reasoning patterns from code to natural language without losing domain-specific context
- Evidence anchors:
  - [abstract] "code is a critical building block for generalization far beyond coding tasks"
  - [section] "8.2% relative increase in natural language reasoning" when code is included in pre-training
- Corpus evidence: Weak evidence - no direct corpus studies on structural transfer from code to language

### Mechanism 2
- Claim: Synthetic code data provides "high-quality reasoning examples" that improve non-code performance disproportionately
- Mechanism: Synthetic code with formal verification provides clean, logically correct examples that teach the model proper reasoning chains, which transfer to natural language tasks
- Core assumption: The logical structure of synthetic code is transferable to natural language reasoning despite different surface forms
- Evidence anchors:
  - [section] "synthetic code data (code+synth) improves relative performance by 9% on natural language reasoning, and 44.9% on code benchmarks"
  - [section] "small percentage of a high-quality code data, not only improves performance in code pre-training but also increases code and non-code performance"
- Corpus evidence: Weak evidence - no corpus studies on synthetic data transfer effects

### Mechanism 3
- Claim: Code data in cooldown phase "refines generalization" by exposing the model to high-quality examples when learning rate is annealed
- Mechanism: During cooldown, up-weighted high-quality datasets (including code) allow the model to fine-tune its representations when it's most receptive to quality improvements
- Core assumption: The model's capacity for quality refinement is maximized during cooldown when learning rate is low
- Evidence anchors:
  - [section] "including code data in pre-training cooldown, leads to an increase of 3.6% in NL reasoning, 10.1% in world knowledge, and 20% in code performance"
  - [section] "cooldown with code beats the baseline (model without cooldown) by 52.3% win-rates"
- Corpus evidence: Weak evidence - no corpus studies on cooldown effects specifically

## Foundational Learning

- Concept: Transfer learning across domains
  - Why needed here: The paper shows code data improves non-code performance, requiring understanding of how knowledge transfers between domains
  - Quick check question: If a model learns logical patterns from code, what mechanisms might allow these patterns to improve performance on natural language reasoning tasks?

- Concept: Dataset quality vs. quantity tradeoffs
  - Why needed here: The paper shows synthetic code (small quantity, high quality) outperforms larger web-based code datasets, demonstrating quality's outsized impact
  - Quick check question: Given that synthetic code improves performance despite being only 10% of code data, what characteristics of synthetic data might make it more valuable than larger web-based datasets?

- Concept: Pre-training phase optimization
  - Why needed here: The paper shows different phases (initialization, continued training, cooldown) have different optimal code proportions, requiring understanding of how training dynamics change
  - Quick check question: Why might the optimal code proportion differ between the initial pre-training phase (25% optimal) and continued pre-training phase (higher proportions beneficial)?

## Architecture Onboarding

- Component map: Data preparation → Model initialization → Continued pre-training → Cooldown → Evaluation across reasoning, knowledge, and code tasks
- Critical path: Preprocess SlimPajama (503B tokens text-only) and Stack dataset (139B tokens code + 180B tokens markup) → Train 470M parameter models with different initialization strategies → Evaluate on 11 NL reasoning benchmarks, code tasks, and knowledge tests
- Design tradeoffs: Code vs. text proportions (reasoning vs. code performance), synthetic vs. web code quality (impact per token), initialization vs. continued training (transfer efficiency)
- Failure signatures: No improvement on non-code tasks despite code inclusion (suggests transfer failure), degradation in code performance (suggests proportion imbalance), poor synthetic data generation (quality issues)
- First 3 experiments:
  1. Replicate the text-only baseline vs. balanced initialization comparison to verify core findings
  2. Test synthetic code data in isolation (code-only pre-training) to verify quality claims
  3. Vary code proportions in 10% increments during initialization to map the performance curve

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the benefit of code data in pre-training generalize to even larger models (e.g., 10B+ parameters) and what are the scaling dynamics?
- Basis in paper: Inferred from the authors' discussion of model scale experiments and their suggestion that findings might hold for larger models, combined with the known trend that benefits often increase with scale in LLMs
- Why unresolved: The paper only tests models up to 2.8B parameters. The computational cost of testing larger models is prohibitive, and the authors explicitly note this limitation
- What evidence would resolve it: Pre-training and evaluating models of 10B+ parameters with similar controlled code/non-code ablations to determine if the relative improvements observed at 2.8B scale up or change in magnitude

### Open Question 2
- Question: How does the inclusion of code data in pre-training affect the safety and alignment of models, particularly regarding potential security vulnerabilities or biases in generated code?
- Basis in paper: Explicitly stated as a limitation - "While we systematically study the impact of code data on downstream natural language tasks, we do not study its impact on safety"
- Why unresolved: The authors deliberately excluded safety evaluation from their study scope. This is a critical area that requires separate investigation given code's unique properties and potential for harm
- What evidence would resolve it: Comprehensive safety evaluations of code-pretrained models including red-teaming for security vulnerabilities, bias analysis in code generation, and comparison of alignment techniques effectiveness between code and non-code models

### Open Question 3
- Question: What is the optimal proportion and quality mix of synthetic code data versus web-based code data across different model scales and tasks?
- Basis in paper: Inferred from the authors' finding that synthetic code provides outsized benefits (9% on reasoning, 44.9% on code) but only in small proportions (10%), suggesting there's an optimization problem to solve
- Why unresolved: The authors only tested one synthetic code proportion (10%) and didn't explore how this might vary with model size or task requirements. The high impact despite small weighting suggests there's room for optimization
- What evidence would resolve it: Systematic ablation studies varying synthetic code proportions from 1% to 50% across different model scales (470M, 2.8B, 10B+) and task categories to identify optimal ratios and potential diminishing returns

## Limitations

- The synthetic code generation methodology is not fully specified, making it difficult to reproduce the outsized quality gains
- Safety and alignment impacts of code-pretrained models are not evaluated, leaving potential security vulnerabilities unaddressed
- Scaling effects to larger models (10B+ parameters) remain unknown, limiting generalizability of findings

## Confidence

**High Confidence**: The core finding that including code data improves both code and non-code performance is well-supported by controlled experiments across multiple model variants (8.2% reasoning improvement, 12x code performance boost).

**Medium Confidence**: The claim about synthetic code providing outsized gains (9% on reasoning, 44.9% on code) is supported but depends on the unstated synthetic data generation process and the mechanism by which synthetic code transfers better than web code is plausible but not empirically validated.

**Low Confidence**: The assertion that code data acts as a "structure scaffold" for generalizable reasoning patterns is largely theoretical - while performance improvements are measured, the specific transfer mechanism from code structure to natural language reasoning is not directly tested or validated.

## Next Checks

1. **Synthetic Data Generation**: Replicate the experiments using different synthetic code generation methods (e.g., code translation vs. synthetic generation from scratch) to verify that the quality gains are method-independent and not specific to one generation approach.

2. **Scaling Experiments**: Test the same pre-training strategies with larger model sizes (7B, 13B parameters) to determine whether the optimal code proportions and the magnitude of performance gains scale proportionally or exhibit different behavior at scale.

3. **Cooldown Schedule Sensitivity**: Vary the learning rate annealing schedule during the cooldown phase (different decay rates, step schedules) to confirm that the observed 3.6% NL reasoning and 10.1% world knowledge improvements are robust to different fine-tuning dynamics rather than specific to one cooldown configuration.