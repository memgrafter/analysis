---
ver: rpa2
title: Sparse Rewards Can Self-Train Dialogue Agents
arxiv_id: '2409.04617'
source_url: https://arxiv.org/abs/2409.04617
tags:
- user
- toolwoz
- josh
- agent
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces JOSH, a self-alignment algorithm that enables
  language models to improve their tool-calling performance without external human
  feedback. The method uses sparse rewards in a simulated environment to generate
  synthetic training data by exploring conversation trajectories, extracting optimal
  paths as positive examples and suboptimal paths as negative examples.
---

# Sparse Rewards Can Self-Train Dialogue Agents

## Quick Facts
- arXiv ID: 2409.04617
- Source URL: https://arxiv.org/abs/2409.04617
- Authors: Barrett Martin Lattimer; Varun Gangal; Ryan McDonald; Yi Yang
- Reference count: 40
- Primary result: JOSH enables language models to self-improve tool-calling performance without human feedback, achieving state-of-the-art results on ToolWOZ and τ-bench

## Executive Summary
This paper introduces JOSH, a self-alignment algorithm that enables language models to improve their tool-calling performance without external human feedback. The method uses sparse rewards in a simulated environment to generate synthetic training data by exploring conversation trajectories, extracting optimal paths as positive examples and suboptimal paths as negative examples. Experiments on ToolWOZ and τ-bench show significant improvements across model sizes - for example, a meta-llama3-8B model trained with JOSH achieved a 74% increase in success rate and outperformed PPO baselines. Notably, frontier models like GPT-4o also surpassed their baseline performance, achieving state-of-the-art results on both benchmarks while maintaining general capabilities on MT-Bench and LMSYS.

## Method Summary
JOSH (Juxtaposed Outcomes for Simulation Harvesting) uses beam search simulation to explore conversation trajectories in a sparse reward environment. The method extracts optimal paths as positive examples and suboptimal paths as negative examples to generate synthetic training data. This data is then used to fine-tune models through both supervised fine-tuning (SFT) on optimal trajectories and preference fine-tuning (KTO) using pairwise comparisons. The approach enables self-alignment without requiring external human feedback, leveraging the model's own outputs to improve tool-calling performance while preserving general capabilities.

## Key Results
- meta-llama3-8B trained with JOSH achieved 74% improvement in success rate on ToolWOZ
- GPT-4o trained with JOSH outperformed PPO baselines and achieved state-of-the-art results on both ToolWOZ and τ-bench
- JOSH-trained models maintained general capabilities on MT-Bench and LMSYS-Chatbot Arena
- Turn-level beam search proved more computationally efficient than action-level splitting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: JOSH enables self-alignment by generating synthetic preference-annotated training data through sparse reward simulation
- Mechanism: Beam search simulation explores conversation trajectories until goals (sparse rewards) are achieved, then extracts optimal paths as positive examples and suboptimal paths as negative examples
- Core assumption: Sparse reward signals are sufficient to distinguish between successful and unsuccessful conversation trajectories
- Evidence anchors:
  - [abstract] "JOSH, is a self-alignment algorithm that leverages a sparse reward simulation environment to extract ideal behaviors and further train the LLM on its own outputs"
  - [section 2] "We considered several reward structures for the task of agent dialogues, we found that the cumulative reward method encourages excessive API calls, leading to inefficiency"
  - [corpus] Weak evidence - related papers focus on multimodal feedback and implicit rewards, not sparse reward simulation
- Break condition: If sparse rewards cannot effectively discriminate between successful and unsuccessful trajectories, the extracted training data would lack quality signals

### Mechanism 2
- Claim: JOSH improves tool-calling performance without degrading general capabilities
- Mechanism: The method uses both supervised fine-tuning (SFT) on optimal trajectories and preference fine-tuning (PFT) using pairwise comparisons of good and bad agent turns
- Core assumption: Training on extracted synthetic data from the same model preserves general capabilities while enhancing specific tool-calling skills
- Evidence anchors:
  - [abstract] "demonstrate that models trained with JOSH, both small and frontier, significantly improve tool-based interactions while preserving general model capabilities"
  - [section 5.3] "fine-tuning on JOSH rollouts from ToolWOZ did not degrade performance on either benchmark"
  - [corpus] Weak evidence - related work focuses on explicit global feedback rather than preserving general capabilities
- Break condition: If the model overfits to the synthetic training data, general capabilities would degrade despite improved tool-calling performance

### Mechanism 3
- Claim: JOSH's beam search at the turn level provides sufficient exploration efficiency compared to action-level splitting
- Mechanism: Turn-level branching allows exponential growth of conversation trajectories with fewer actions per turn, enabling deeper exploration within the same max_beam constraint
- Core assumption: Turn-level branching provides sufficient diversity for effective exploration while maintaining computational efficiency
- Evidence anchors:
  - [section 2] "we branch at the turn level rather than the agent action level, allowing the tree to grow exponentially with the number of turns"
  - [section 4.4] "We explore three beam sizes when doing JOSH using gpt-4o-mini-ReACT and note that while a maximum beam size of 16 is marginally better than 8 and 4"
  - [corpus] No direct evidence - corpus focuses on multimodal feedback rather than beam search exploration strategies
- Break condition: If turn-level branching proves insufficient for exploring the space of possible conversation trajectories, important successful paths might be pruned prematurely

## Foundational Learning

- Concept: Sparse reward learning
  - Why needed here: The method relies on sparse rewards (goal achievement) rather than dense rewards to guide conversation trajectory exploration
  - Quick check question: What distinguishes sparse rewards from shaped rewards in reinforcement learning contexts?

- Concept: Beam search in dialogue systems
  - Why needed here: Beam search is used to explore multiple conversation trajectories simultaneously, tracking which paths lead to successful goal achievement
  - Quick check question: How does turn-level beam search differ from traditional token-level beam search in terms of computational complexity and exploration depth?

- Concept: Preference optimization vs traditional RLHF
  - Why needed here: The method uses preference fine-tuning (KTO) rather than the two-step process of reward modeling followed by RL
  - Quick check question: What are the key differences between direct preference optimization and traditional reinforcement learning from human feedback?

## Architecture Onboarding

- Component map:
  User simulator -> Agent model -> Simulation environment -> Beam search controller -> Data extraction pipeline -> Training pipeline

- Critical path: User simulator → Agent response generation → Goal achievement check → Beam pruning → Data extraction → Model fine-tuning

- Design tradeoffs:
  - Turn-level vs action-level beam splitting: Turn-level is computationally more efficient but may miss some action-level diversity
  - Goal-based vs guide-based user simulators: Goal-based is more consistent but guide-based may better match human behavior
  - SFT vs KTO training: SFT provides direct supervision while KTO captures pairwise preferences but requires unpaired data

- Failure signatures:
  - Poor tool-calling performance despite JOSH training: Indicates the extracted training data lacks quality signals
  - Degraded general capabilities: Suggests overfitting to synthetic data
  - High variance in benchmark results: Points to instability in the simulation environment or user simulator

- First 3 experiments:
  1. Run JOSH simulation with a small model on a single conversation to verify the beam search and data extraction pipeline works
  2. Compare SFT-only vs KTO-only training on the same JOSH-generated data to understand the contribution of each training approach
  3. Test different beam sizes (4, 8, 16) on the same model to find the optimal balance between exploration and computational cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does JOSH's performance scale with different beam sizes, and is there an optimal beam size that balances exploration depth with computational efficiency?
- Basis in paper: [inferred] The paper mentions that beam sizes of 4, 8, and 16 were tested, with 16 being marginally better than 8 and 4, but 8 was chosen for efficiency.
- Why unresolved: While the paper provides some initial exploration of beam sizes, it doesn't conduct a comprehensive analysis of how beam size affects performance across different model scales or domains. The trade-off between exploration depth and computational cost remains unclear.
- What evidence would resolve it: A systematic study varying beam sizes across different model scales (small, medium, large) and domains, measuring both performance gains and computational costs, would provide clarity on optimal beam sizing.

### Open Question 2
- Question: How does JOSH perform when applied to non-tool-based dialogue tasks, such as open-domain conversation or task-oriented dialogue without explicit APIs?
- Basis in paper: [explicit] The paper focuses specifically on tool-calling/multi-turn dialogue tasks and notes that JOSH could theoretically be applied to any multi-turn dialogue domain with sparse rewards, but doesn't demonstrate this.
- Why unresolved: The method's generalizability beyond tool-calling tasks is only hypothesized, not empirically validated. Different dialogue domains may require different reward structures or have different success criteria.
- What evidence would resolve it: Experiments applying JOSH to open-domain conversation benchmarks (like engagingness or coherence metrics) or non-API task-oriented dialogue tasks would demonstrate its broader applicability.

### Open Question 3
- Question: What is the impact of JOSH training on model robustness to user simulator variability, and how does this compare to training with diverse human conversations?
- Basis in paper: [explicit] The paper shows that JOSH-trained models maintain performance when evaluated against different user simulators (gpt-4-turbo vs. original), but doesn't compare this to models trained on actual human conversations.
- Why unresolved: While the paper demonstrates robustness to different LLM-based simulators, it doesn't address whether JOSH training provides similar robustness to the variability found in human conversations, which could be more diverse and unpredictable.
- What evidence would resolve it: Comparing JOSH-trained models against models fine-tuned on human-human dialogues across multiple user simulators and measuring performance consistency would reveal whether JOSH provides comparable robustness to real-world variability.

## Limitations
- Sparse rewards may not provide sufficient signal quality for all types of conversation trajectories
- Evaluation is limited to two specific domains (ToolWOZ and τ-bench Retail), limiting generalizability claims
- Computational cost and scalability to larger models or longer conversations remain unclear

## Confidence
- High Confidence: The core mechanism of using sparse rewards for self-alignment is well-established in reinforcement learning literature. The improvement in tool-calling performance on the tested benchmarks is clearly demonstrated.
- Medium Confidence: The claim that JOSH preserves general capabilities while improving tool-calling performance is supported by the results, but the evaluation on MT-Bench and LMSYS is relatively limited. The extent to which general capabilities are truly preserved across diverse tasks requires further validation.
- Low Confidence: The superiority of turn-level beam search over action-level splitting is asserted but not thoroughly validated. The paper doesn't explore whether different branching strategies or reward structures might yield better results.

## Next Checks
1. **Cross-Domain Generalization**: Evaluate JOSH-trained models on a broader set of tool-calling benchmarks beyond ToolWOZ and τ-bench Retail to assess generalization capabilities.
2. **Alternative Reward Structures**: Test whether modified reward structures (e.g., combining sparse rewards with efficiency penalties) improve performance while maintaining the benefits of self-alignment.
3. **Computational Scaling Analysis**: Conduct experiments to quantify the computational cost of JOSH as a function of model size, conversation length, and beam size to understand its practical scalability limits.