---
ver: rpa2
title: 'SGPT: Few-Shot Prompt Tuning for Signed Graphs'
arxiv_id: '2412.12155'
source_url: https://arxiv.org/abs/2412.12155
tags:
- graph
- signed
- node
- link
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SGPT introduces a prompt-tuning framework that transfers knowledge
  from pre-trained unsigned GNNs to few-shot signed graph tasks. The method uses a
  balance-theory-based graph template to disentangle link polarities into three semantic
  channels (positive, negative, and topological), enabling compatibility with pre-trained
  models.
---

# SGPT: Few-Shot Prompt Tuning for Signed Graphs

## Quick Facts
- arXiv ID: 2412.12155
- Source URL: https://arxiv.org/abs/2412.12155
- Reference count: 40
- Key outcome: State-of-the-art performance on few-shot signed graph tasks using prompt tuning with significantly fewer parameters than supervised methods

## Executive Summary
SGPT introduces a novel prompt-tuning framework that transfers knowledge from pre-trained unsigned graph neural networks to few-shot signed graph tasks. The method leverages balance theory to disentangle link polarities into three semantic channels (positive, negative, and topological), enabling compatibility with pre-trained GNN homophily assumptions. Through feature prompts that align input spaces and semantic prompts that integrate polarity information task-aware, SGPT achieves state-of-the-art performance across seven benchmark datasets while requiring fewer tunable parameters than supervised alternatives.

## Method Summary
SGPT operates through a three-stage process: first, a graph template based on balance theory decomposes signed graphs into positive, negative, and topological channels while preserving connectivity structure. Second, feature prompts modify input node features per channel to align downstream feature spaces with pre-training, followed by semantic prompts that adaptively aggregate multi-channel embeddings through a bottleneck adapter. Finally, a task template reformulates signed node classification and link sign prediction into unified link prediction objectives via prototype-based similarity comparisons. The entire framework keeps the pre-trained GNN backbone frozen, optimizing only lightweight prompt parameters during downstream training.

## Key Results
- Achieves state-of-the-art performance on seven benchmark signed graph datasets
- Outperforms existing supervised and prompt-tuning methods in low-label scenarios
- Demonstrates significant parameter efficiency with fewer tunable parameters than supervised alternatives

## Why This Works (Mechanism)

### Mechanism 1: Balance Theory-Based Channel Disentanglement
Graph template splits signed links into three channels (positive, negative, topological) that align with pre-trained GNN homophily assumption. For k-hop neighbors, balanced paths preserve sign via matrix multiplication while unbalanced paths flip sign, with topological channel masking signs to preserve connectivity only. Core assumption: balanced structures are more prevalent than unbalanced ones in real-world signed graphs. Break condition: significant shifts in balanced/unbalanced path ratios degrade channel consistency.

### Mechanism 2: Unified Task Reformulation
Task template converts signed node classification and link sign prediction into unified link prediction form using prototype embeddings. Each class gets a prototype; similarity to prototypes is computed as in link prediction. Core assumption: pre-training link prediction objective is sufficiently general to be repurposed via prototypes for any signed task. Break condition: prototypes cannot represent task classes well, losing discriminative power.

### Mechanism 3: Feature and Semantic Prompt Integration
Feature prompts add basis vectors weighted by learned attention to node features before GNN, aligning downstream feature space with pre-training. Semantic prompts use bottleneck adapter to fuse positive, negative, and topological embeddings while preserving topology via skip connection. Core assumption: feature space drift exists and can be corrected by lightweight additive prompts; adaptive aggregation improves over naive concatenation. Break condition: prompts overfit to training set in few-shot scenarios despite parameter efficiency.

## Foundational Learning

- **Signed graphs and balance theory**: Needed because SGPT relies on balance theory to split signed links into channels that respect GNN homophily assumption. Quick check: In a signed graph, if a path has an odd number of negative links, is it balanced or unbalanced?
- **Graph neural network message passing**: Needed because pre-trained GNN backbone uses aggregation over neighbors; SGPT must adapt this to signed contexts without breaking learned priors. Quick check: What happens to node embeddings if a GNN aggregates features from neighbors with opposite sign semantics without disentangling?
- **Prompt tuning in graph settings**: Needed because SGPT keeps GNN frozen and tunes lightweight prompts; understanding how prompts modify input features or embeddings is key to implementation. Quick check: In prompt tuning, are the original GNN parameters updated during downstream training?

## Architecture Onboarding

- **Component map**: Unsigned graph → Pre-trained GNN encoder → Link prediction loss → Signed graph → Graph template → Three channel graphs → Feature prompts → Frozen GNN forward → Semantic prompts → Task template output → Loss
- **Critical path**: Graph template → feature prompts → frozen GNN forward → semantic prompts → task template output → loss
- **Design tradeoffs**: Graph template hop count k vs. computational cost (exponential neighbor growth); prompt basis size r vs. expressiveness and overfitting risk; adapter bottleneck dimension vs. representation capacity
- **Failure signatures**: Performance collapse on signed tasks if graph template incorrectly assigns neighbors to channels; no gain over supervised baselines if feature prompts fail to align feature spaces; overfitting to few-shot labels if semantic prompts have too many parameters
- **First 3 experiments**: 1) Verify graph template produces correct channel adjacency matrices on a small signed graph with known balance/unbalance paths; 2) Test feature prompts alone on a signed task (with simple aggregation) to confirm feature alignment effect; 3) Run end-to-end SGPT on a tiny dataset with 1-shot labels to check prompt training stability and loss convergence

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but implies several areas for future work including systematic exploration of performance across different signed graph structural properties, extension to heterogeneous signed graphs, and theoretical analysis of generalization error bounds compared to supervised approaches.

## Limitations
- Performance may degrade on signed graphs with significantly different structural properties than benchmark datasets
- Framework specifically designed for homogeneous signed graphs, limiting applicability to heterogeneous scenarios
- Lacks theoretical generalization error bounds comparing prompt tuning to supervised training approaches

## Confidence

- Graph template robustness across different signed graph structures: Medium confidence (assumes balanced structures are prevalent)
- Task template's ability to generalize pre-training objectives: Medium confidence (conceptually sound but lacks direct comparison)
- Feature and semantic prompts' effectiveness in few-shot scenarios: Medium-Low confidence (ablation studies support but overfitting risk remains)

## Next Checks

1. Test SGPT performance on signed graphs with known high unbalance ratios to assess graph template robustness
2. Compare prototype-based reformulation against direct supervised adaptation on node classification to validate task template benefits
3. Measure prompt parameter sensitivity across different basis sizes and bottleneck dimensions to confirm optimal configuration