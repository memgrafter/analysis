---
ver: rpa2
title: 'BRIEF: Bridging Retrieval and Inference for Multi-hop Reasoning via Compression'
arxiv_id: '2410.15277'
source_url: https://arxiv.org/abs/2410.15277
tags:
- question
- multi-hop
- documents
- brief
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BRIEF addresses the challenge of efficiently processing retrieval-augmented
  generation for multi-hop questions, where reasoning across multiple documents is
  required. The core idea is to train a lightweight T5-based compressor using synthetic
  data generated by open-source models, which compresses retrieved documents into
  dense textual summaries tailored for multi-hop reasoning.
---

# BRIEF: Bridging Retrieval and Inference for Multi-hop Reasoning via Compression

## Quick Facts
- arXiv ID: 2410.15277
- Source URL: https://arxiv.org/abs/2410.15277
- Authors: Yuankai Li; Jia-Chen Gu; Di Wu; Kai-Wei Chang; Nanyun Peng
- Reference count: 40
- Primary result: 19.19x compression rate with 3.00% EM improvement on HotpotQA

## Executive Summary
BRIEF addresses the challenge of efficiently processing retrieval-augmented generation for multi-hop questions by training a lightweight T5-based compressor using synthetic data. The compressor reduces retrieved documents into dense textual summaries tailored for multi-hop reasoning, significantly lowering computational costs while improving inference speed. BRIEF achieves state-of-the-art performance on HotpotQA, outperforming previous baselines while generating more concise summaries than GPT-3.5 with nearly identical QA performance.

## Method Summary
BRIEF uses a T5-large compressor trained on synthetic multi-hop question data generated entirely by open-source models. The system retrieves documents, compresses them into atomic propositions, and feeds these summaries to a reader LM. The synthetic data pipeline composes multi-hop questions using bridge entities, validates them through decomposition into single-hop questions, and identifies helpful atomic propositions. The compressor is fine-tuned with Adam optimizer (learning rate 3e-5, batch size 16) on datasets including HotpotQA, MuSiQue, and curated multi-hop versions of NQ and TriviaQA.

## Key Results
- Achieves 19.19x compression rate on HotpotQA while improving EM by 3.00% and F1 by 4.16% over previous state-of-the-art
- Generates more concise summaries than GPT-3.5 with nearly identical QA performance across multiple LLMs
- Outperforms token-level and soft-prompt compression methods while maintaining cross-LLM compatibility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BRIEF's compression reduces input length while retaining core evidence needed for multi-hop reasoning.
- Mechanism: The compressor identifies atomic propositions (distinct factoids) that are helpful for answering questions spanning multiple documents.
- Core assumption: Multi-hop reasoning can be decomposed into single-hop reasoning steps, each requiring specific atomic propositions.
- Evidence anchors: [abstract] "BRIEF generates more concise summaries and enables a range of LLMs to achieve exceptional open-domain question answering (QA) performance." [section] "The helpful atomic expressions are ranked by their associated answer likelihood and the top-k are selected as the target summary s."
- Break condition: If propositionizer fails to correctly identify atomic factoids or decomposition misses critical bridge entities, the reasoning chain breaks.

### Mechanism 2
- Claim: Synthetic data generation via open-source models enables training without proprietary LLMs or human annotations.
- Mechanism: Pipeline composes multi-hop questions by connecting documents using bridge entities, validates them by decomposing into single-hop questions, and identifies helpful propositions.
- Core assumption: Open-source models can generate factually accurate multi-hop questions and identify helpful evidence as well as proprietary models.
- Evidence anchors: [abstract] "BRIEF is trained on data synthesized through a pipeline built entirely by open-source models, without relying on any proprietary LLMs and human annotations." [section] "We utilize the most common relationships identified by Zhong et al. (2023)... to aid in discovering connections between separate documents."
- Break condition: If synthetic data pipeline produces spurious multi-hop questions or incorrectly identifies helpful propositions, compressor learns to summarize irrelevant information.

### Mechanism 3
- Claim: BRIEF's summaries are more compatible across different LLMs compared to token-level or soft-prompt compression methods.
- Mechanism: By producing natural language summaries in the form of propositions, BRIEF avoids LM-specific fine-tuning or embedding alignment.
- Core assumption: Natural language propositions are more interpretable and transferable across LMs than compressed tokens or soft prompts.
- Evidence anchors: [abstract] "BRIEF generates more concise summaries than proprietary GPT-3.5, while demonstrating nearly identical QA performance." [section] "The results indicate that BRIEF consistently aligns with GPT-3.5 in terms of the sensitivity to the multi-hop nature of questions while generating more concise summaries."
- Break condition: If proposition format is not compatible with target LM's context processing, summary won't improve QA performance.

## Foundational Learning

- Concept: Multi-hop reasoning decomposition
  - Why needed here: Essential for designing synthetic data pipeline and evaluating compressor effectiveness
  - Quick check question: Given "What is the capital of the country where the Eiffel Tower is located?", what are the two single-hop questions and bridge entity?

- Concept: Proposition identification and extraction
  - Why needed here: Compressor training relies on identifying atomic propositions that encapsulate distinct factoids
  - Quick check question: Given "The Leaning Tower of Pisa, located in Italy, leans at 3.99 degrees.", what is the atomic proposition about the tower's angle?

- Concept: Context compression and relevance scoring
  - Why needed here: Compressor must learn to identify and retain most relevant information for answering questions
  - Quick check question: Given "Who was the first president of the United States?" and two documents (one about George Washington, one about Abraham Lincoln), which document should compressor retain and why?

## Architecture Onboarding

- Component map: Retriever -> Compressor (BRIEF) -> Reader LM
- Critical path: 1) Query sent to retriever, 2) Retrieved documents passed to compressor, 3) Compressor generates summary, 4) Summary prepended to query and sent to reader LM, 5) Reader LM generates final answer
- Design tradeoffs: Compression ratio vs. QA performance; proposition granularity vs. summary coherence; synthetic data quality vs. training cost
- Failure signatures: Low compression ratio (compressor not identifying irrelevant info); poor QA performance (omitting critical evidence or including irrelevant info); slow inference (too many documents or inefficient compressor)
- First 3 experiments: 1) Evaluate BRIEF's compression ratio and QA performance on HotpotQA subset, 2) Compare BRIEF's summaries to GPT-3.5 in length and coherence, 3) Test BRIEF's compatibility with different reader LMs (Phi-3-mini, Phi-3-small, Phi-3-medium)

## Open Questions the Paper Calls Out

- Can BRIEF effectively handle documents longer than the 512-token T5 limit? Paper conducted preliminary study expanding from top-5 to top-25 documents, suggesting better scalability for longer documents, but didn't fully explore effectiveness.
- Can a unified compressor be trained on combined training sets and evaluated across all datasets? Paper trained UNIBRIEF on combined sets with mixed results (performance improvements on two datasets but not on remaining four).
- How does BRIEF compare to other state-of-the-art context compression methods on multi-hop questions? Paper compared to LLMLingua and Selective Context but didn't provide comprehensive comparison with all state-of-the-art methods.

## Limitations

- Reliance on open-source models for synthetic data generation introduces uncertainty about factual accuracy and quality of training data
- Assumption that multi-hop reasoning can be effectively decomposed into single-hop reasoning steps lacks empirical validation
- Limited evidence for universal cross-LLM compatibility beyond specific model tests

## Confidence

- High confidence: Compression efficiency claims (19.19x rate, 3.00% EM improvement) well-supported by experimental results
- Medium confidence: Cross-LLM compatibility claim limited by testing with only a few models
- Low confidence: Synthetic data quality claim lacks direct validation or human evaluation

## Next Checks

1. **Synthetic Data Quality Audit**: Manually evaluate 100 random synthetic multi-hop questions, classifying validity and measuring propositionizer accuracy to validate training data reflects real multi-hop reasoning patterns.

2. **Cross-LLM Compatibility Test**: Evaluate BRIEF with three additional reader models (Phi-3-mini, Mistral-7B, Llama-3-70B) on HotpotQA, measuring both QA performance and compression rates to determine effectiveness across diverse model families.

3. **Ablation on Multi-hop Decomposition**: Create variant synthetic data pipeline without decomposition step, train two BRIEF compressors (with and without decomposition), and compare performance on HotpotQA to isolate decomposition's contribution.