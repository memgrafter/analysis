---
ver: rpa2
title: Does Differential Privacy Impact Bias in Pretrained NLP Models?
arxiv_id: '2410.18749'
source_url: https://arxiv.org/abs/2410.18749
tags:
- uni00000013
- uni00000011
- uni00000003
- uni00000048
- uni00000051
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how differential privacy (DP) affects\
  \ bias in pre-trained NLP models for toxic language detection. The authors fine-tune\
  \ BERT using DP-SGD at varying privacy budgets (\u03B5) on two datasets: Jigsaw\
  \ Unintended Bias and UCBerkeley Hate Speech."
---

# Does Differential Privacy Impact Bias in Pretrained NLP Models?

## Quick Facts
- arXiv ID: 2410.18749
- Source URL: https://arxiv.org/abs/2410.18749
- Reference count: 8
- This paper investigates how differential privacy (DP) affects bias in pre-trained NLP models for toxic language detection.

## Executive Summary
This paper investigates how differential privacy (DP) affects bias in pre-trained NLP models for toxic language detection. The authors fine-tune BERT using DP-SGD at varying privacy budgets (ε) on two datasets: Jigsaw Unintended Bias and UCBerkeley Hate Speech. They evaluate model bias using multiple metrics including equality of odds, demographic parity, protected accuracy, and AUC-based metrics (subgroup AUC, BPSN, BNSP). Results show that DP increases model bias against protected groups in terms of AUC-based metrics, with the impact varying across different identity subgroups. While equality of odds improves with stricter privacy due to decreased recall, this improvement comes at the cost of increased bias in other metrics. The study reveals a privacy-bias tradeoff in NLP models, highlighting that DP can amplify existing biases against underrepresented groups.

## Method Summary
The study fine-tunes BERT-base-uncased with DP-SGD at different privacy budgets (ε = 0.5, 1.0, 3.0, 6.0, 9.0) on Jigsaw Unintended Bias and UCBerkeley Hate Speech datasets. Only the last three layers of BERT are trained (7.6M parameters) while the rest remain frozen. Models are trained for up to 10 epochs with early stopping. Bias is evaluated using multiple metrics including equality of odds, demographic parity, protected accuracy, subgroup AUC, BPSN, and BNSP. Utility metrics like accuracy, F1, and AUC are also measured to assess the privacy-utility tradeoff.

## Key Results
- DP increases model bias against protected groups in terms of AUC-based metrics (subgroup AUC, BPSN, BNSP)
- Equality of odds improves with stricter privacy budgets due to decreased recall across all groups
- The impact of DP on bias varies across different identity subgroups and depends on dataset distribution
- DP makes it more difficult for models to differentiate between positive/negative examples from protected groups compared to other groups

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DP training increases bias in AUC-based metrics by reducing the model's ability to distinguish between positive and negative examples within protected subgroups.
- Mechanism: The Gaussian noise added during DP-SGD training smooths out the distinguishing features of underrepresented social groups, making it harder for the model to differentiate between positive and negative examples from these groups compared to other groups.
- Core assumption: The distinguishing features of underrepresented social groups are learned from a limited number of training examples, which are most affected by the noise addition in DP.
- Evidence anchors:
  - [abstract]: "DP makes it more difficult for the model to differentiate between the positive and negative examples from the protected groups and other groups in the rest of the population."
  - [section 7.3]: "A drop in BNSP means the scores for positive examples in these subgroups are lower than the scores for other negative examples in the background data."
  - [corpus]: Found related work "Characterizing Stereotypical Bias from Privacy-preserving Pre-Training" suggesting similar mechanisms exist.
- Break condition: If the noise level is sufficiently low or the dataset is perfectly balanced across all subgroups, the distinguishing features might not be significantly affected.

### Mechanism 2
- Claim: DP training improves equality of odds metrics by reducing recall more uniformly across all social groups.
- Mechanism: The added noise and gradient clipping in DP-SGD training degrades overall model performance, reducing recall for all social groups. This uniform reduction in recall makes the true positive rates more similar across groups, improving equality of odds.
- Core assumption: The degradation in recall is more uniform across different social groups than the degradation in other performance metrics.
- Evidence anchors:
  - [section 7.2]: "The equality of odds always improves with a strict privacy budget (small ϵ). However, this is due to a significant drop in recall for most groups."
  - [section 8]: "As illustrated in Figure 5 in Appendix D, we find that with private training, the recall values grow more similar."
  - [corpus]: Related work "SoK: What Makes Private Learning Unfair?" discusses how DP can affect performance metrics across groups.
- Break condition: If the noise addition affects different social groups disproportionately, the improvement in equality of odds may not occur.

### Mechanism 3
- Claim: The impact of DP on bias depends on the underlying distribution of the dataset.
- Mechanism: Datasets with different distributions of toxic/non-toxic examples and different representations of social groups will show different patterns of bias when DP is applied. The direction and magnitude of bias changes depend on how the dataset is skewed.
- Core assumption: The distribution of examples across different social groups and toxicity labels affects how DP noise impacts the model's learning.
- Evidence anchors:
  - [section 8]: "In Jigsaw, increasing privacy in the training increases the number of toxic predictions. In the UCBerkeley dataset, the number of toxic predictions decreases with an increased privacy budget."
  - [section 9]: "It can be attributed to how the dataset is distributed."
  - [corpus]: Related work "Privacy-Utility-Bias Trade-offs for Privacy-Preserving Recommender Systems" suggests distribution affects DP outcomes.
- Break condition: If the dataset is perfectly balanced across all dimensions, the distribution-dependent effects might be minimized.

## Foundational Learning

- Concept: Differential Privacy (DP) and DP-SGD
  - Why needed here: Understanding how DP works is crucial to understanding why it affects bias. The noise addition and gradient clipping mechanisms are central to the paper's findings.
  - Quick check question: What is the primary purpose of adding Gaussian noise in DP-SGD, and how does it affect gradient updates?

- Concept: Fairness metrics (Equality of Odds, Demographic Parity, AUC-based metrics)
  - Why needed here: The paper evaluates bias using multiple metrics, and understanding their differences is essential to interpreting the results correctly.
  - Quick check question: How does equality of odds differ from demographic parity, and why might they show opposite trends with DP?

- Concept: BERT architecture and fine-tuning
  - Why needed here: The paper uses BERT-base-uncased with only the last three layers trained, which affects how DP impacts the model.
  - Quick check question: Why might training only the last three layers of BERT make DP more effective compared to training all layers?

## Architecture Onboarding

- Component map:
  BERT-base-uncased (109M parameters) → Last 3 layers trained (7.6M parameters) → DP-SGD optimizer with Opacus library → Training datasets: Jigsaw Unintended Bias (144,334 train examples) and UCBerkeley Hate Speech (19,376 train examples) → Bias evaluation pipeline with multiple metrics

- Critical path:
  1. Load pre-trained BERT and prepare datasets
  2. Initialize DP-SGD optimizer with specified epsilon values
  3. Train model for up to 10 epochs with early stopping
  4. Evaluate on test set using multiple bias metrics
  5. Compare results across different epsilon values

- Design tradeoffs:
  - Training only last 3 layers vs all layers: Less computational cost but may limit DP effectiveness
  - Choice of epsilon values: Balance between privacy protection and model utility
  - Dataset size and preprocessing: Affects statistical significance and comparability of results

- Failure signatures:
  - If recall drops significantly across all groups but equality of odds doesn't improve, the mechanism may not be working as expected
  - If AUC-based metrics don't show the expected degradation pattern, there may be issues with the DP implementation or evaluation

- First 3 experiments:
  1. Run non-DP training (epsilon → ∞) on both datasets to establish baseline performance and bias metrics
  2. Run DP training with epsilon = 9.0 on both datasets to observe initial effects of privacy on bias
  3. Run DP training with epsilon = 0.5 on both datasets to observe effects of strict privacy on bias

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DP differentially impact subgroups with varying representation levels in the training data?
- Basis in paper: [inferred] The paper notes that DP "smoothes over" distinguishing features of underrepresented groups, and that DP amplifies differences in AUC between white and Asian subgroups in Jigsaw and white and black subgroups in UCBerkeley.
- Why unresolved: The paper does not explicitly measure or analyze how representation levels correlate with DP's differential impact across subgroups.
- What evidence would resolve it: Empirical analysis comparing DP's effect on subgroups with different prevalence rates in training data, controlling for other factors.

### Open Question 2
- Question: Does DP's impact on bias persist across different NLP tasks beyond toxicity detection?
- Basis in paper: [explicit] The authors state "We make our observations based on toxicity and hate speech detection tasks" and note this as a limitation.
- Why unresolved: The study is limited to two specific datasets and tasks, preventing generalization to other NLP applications.
- What evidence would resolve it: Replication of the study on diverse NLP tasks like sentiment analysis, coreference resolution, and question answering.

### Open Question 3
- Question: What is the relationship between DP's privacy budget (ε) and the magnitude of bias amplification?
- Basis in paper: [explicit] The paper shows results for different ε values but doesn't systematically analyze the relationship between privacy level and bias increase.
- Why unresolved: The analysis presents results at discrete ε values without exploring the continuous relationship or identifying thresholds where bias effects become significant.
- What evidence would resolve it: Systematic analysis mapping ε values to bias metrics across a continuous range to identify patterns and thresholds.

## Limitations
- The study is limited to two specific toxic language detection datasets with limited identity subgroup representations.
- DP implementation using only three trainable layers of BERT may not represent effects of full-model DP training.
- The privacy budgets tested may not cover all practical use cases or reveal threshold effects.

## Confidence
- **High Confidence**: The core finding that DP increases bias against protected groups in AUC-based metrics is well-supported by consistent results across both datasets and multiple experiments.
- **Medium Confidence**: The mechanism explaining how DP noise affects subgroup differentiation is plausible but requires further validation through ablation studies and analysis of feature representations.
- **Medium Confidence**: The observation that equality of odds improves with stricter privacy is statistically significant but may be an artifact of uniform recall degradation rather than genuine fairness improvement.

## Next Checks
1. Replicate experiments with full-model DP training (all BERT layers) to verify if the three-layer training limitation affects the observed bias patterns.
2. Conduct experiments with additional identity subgroups and more diverse datasets to test the generalizability of privacy-bias tradeoffs.
3. Perform feature importance analysis to directly measure how DP noise affects the model's ability to identify distinguishing features of protected subgroups.