---
ver: rpa2
title: Towards Improving NAM-to-Speech Synthesis Intelligibility using Self-Supervised
  Speech Models
arxiv_id: '2407.18541'
source_url: https://arxiv.org/abs/2407.18541
tags:
- speech
- data
- ground-truth
- representations
- intelligibility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of improving intelligibility
  in Non-Audible Murmur (NAM)-to-speech conversion, a silent speech interface technique
  that captures flesh-conducted vibrations from behind the ear. Unlike prior methods
  requiring explicit ground-truth speech recordings, this work proposes a self-supervised
  approach that simulates ground-truth speech using paired whisper audio and augments
  the limited NAM dataset through speech-to-speech synthesis.
---

# Towards Improving NAM-to-Speech Synthesis Intelligibility using Self-Supervised Speech Models

## Quick Facts
- arXiv ID: 2407.18541
- Source URL: https://arxiv.org/abs/2407.18541
- Reference count: 0
- The study improves NAM-to-speech conversion intelligibility by 29.08% in MCD and reduces WER/CER by 12.82%/10.96% using self-supervised HuBERT embeddings and data augmentation

## Executive Summary
This paper addresses the challenge of improving intelligibility in Non-Audible Murmur (NAM)-to-speech conversion, a silent speech interface that captures flesh-conducted vibrations. The authors propose a self-supervised approach that avoids requiring explicit ground-truth speech recordings by simulating speech using paired whisper audio and augmenting the limited NAM dataset through speech-to-speech synthesis. A sequence-to-sequence transformer network trained on HuBERT embeddings learns cross-modality mapping between NAM and speech representations. The approach achieves significant improvements over state-of-the-art methods, demonstrating both enhanced intelligibility and the capability to synthesize speech in novel voices.

## Method Summary
The proposed method uses HuBERT embeddings to extract speaker-agnostic speech content from NAM and whisper audio. A sequence-to-sequence transformer network learns the cross-modal mapping between NAM embeddings and speech embeddings. The approach generates simulated ground-truth speech by processing whisper audio through a pre-trained LJSpeech-based vocoder. Data augmentation is performed by converting LJSpeech content to NAM-like speech using speech-to-speech synthesis, then aligning the pairs via DTW. The model is trained with combined MSE and CTC loss functions, and a HiFiGAN vocoder synthesizes the final speech output.

## Key Results
- 29.08% improvement in Mel-Cepstral Distortion over state-of-the-art models
- 12.82% reduction in Word Error Rate with data augmentation
- 10.96% reduction in Character Error Rate with data augmentation
- Demonstrated capability to synthesize speech in novel voices

## Why This Works (Mechanism)

### Mechanism 1
HuBERT embeddings isolate speech content from speaker identity and noise through self-supervised learning via masked prediction. The 768-dimensional embeddings preserve essential acoustic features while discarding irrelevant speaker characteristics.

### Mechanism 2
Simulated ground-truth speech from whisper enables training without real studio recordings. Paired whisper audio is processed through a pre-trained LJSpeech-based speech vocoder to generate speech in the target speaker's voice.

### Mechanism 3
Data augmentation via LJNAM synthesis increases model robustness by transforming LJSpeech content into NAM-like speech, then aligning via DTW to paired LJSpeech ground-truth, expanding training diversity.

## Foundational Learning

- **Self-supervised speech representation learning**: Eliminates dependency on labeled studio recordings while extracting speaker-agnostic features. Quick check: What does HuBERT use as pseudo-labels for its masked prediction task?
- **Sequence-to-sequence cross-modal mapping**: Converts NAM vibrations into intelligible speech by learning a mapping in embedding space. Quick check: Why use a Non-Autoregressive (NAR) transformer instead of an autoregressive one?
- **Dynamic Time Warping for alignment**: Aligns augmented LJNAM and LJSpeech utterances of different lengths for proper supervised training. Quick check: What is the computational advantage of FastDTW over standard DTW?

## Architecture Onboarding

- **Component map**: HuBERT encoder (frozen) → Seq2Seq transformer (trainable) → HiFiGAN vocoder (trainable) → synthesized speech. Parallel path: NAM/LJNAM → HuBERT encoder → CTC tokenizer for auxiliary text alignment loss
- **Critical path**: NAM embedding → Seq2Seq encoder → cross-attention → Seq2Seq decoder → speech embedding → HiFiGAN → audio output
- **Design tradeoffs**: HuBERT frozen vs fine-tuned (frozen reduces training time but may miss NAM-specific features), NAR vs AR (NAR enables real-time inference but may struggle with long-range dependencies), CTC loss weight (higher weight improves intelligibility but may reduce speech naturalness)
- **Failure signatures**: Low MCD but high WER (model learns spectral mapping but fails to preserve phonetic content), high MCD (vocoder or Seq2Seq fails to generate accurate speech embeddings), training collapse (CTC and MSE losses diverge, indicating misaligned objectives)
- **First 3 experiments**: 1) Validate HuBERT embeddings preserve speech content by reconstructing whisper speech, 2) Test DTW alignment by measuring distance reduction between unaligned and aligned LJNAM-LJSpeech pairs, 3) Train Seq2Seq on small NAM subset with and without CTC loss to measure intelligibility impact

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the NAM-to-speech conversion system change when using real ground-truth speech data instead of simulated speech data? The paper mentions that the system surpasses the current SOTA by 29.08% improvement in MCD metric using simulated ground-truth speech, but does not compare it to a system using real ground-truth speech.

### Open Question 2
What is the impact of different data augmentation techniques on the intelligibility of the synthesized speech in NAM-to-speech conversion? The paper discusses the benefits of their proposed data augmentation technique using LJSpeech data but does not explore other potential augmentation methods.

### Open Question 3
How does the proposed NAM-to-speech conversion system perform on different speakers or languages not included in the training data? The paper mentions the model's capability to synthesize speech in novel voices but does not provide detailed results on different speakers or languages.

### Open Question 4
What are the limitations of the proposed self-supervised learning approach in capturing fine-grained speech nuances compared to traditional supervised methods? The paper highlights the advantages of using self-supervised learning with HuBERT embeddings but does not discuss potential limitations in capturing detailed speech characteristics.

## Limitations
- Does not specify exact Seq2Seq transformer architecture or hyperparameter configurations
- Lacks implementation details for the speech-to-speech synthesis process to generate LJNAM samples
- Does not evaluate perceptual quality of synthesized speech or speaker similarity to target voice

## Confidence

- **High confidence** in the core methodology of using HuBERT embeddings for cross-modal mapping and data augmentation through speech-to-speech synthesis
- **Medium confidence** in the quantitative results (MCD, WER, CER improvements) due to lack of architectural details and potential sensitivity to implementation choices
- **Low confidence** in generalizability beyond the specific NAM corpus and LJSpeech speaker characteristics

## Next Checks

1. Reconstruct whisper speech from HuBERT embeddings to verify that the embeddings preserve sufficient phonetic content for intelligible synthesis
2. Measure alignment quality between LJNAM and LJSpeech pairs using DTW distance metrics to confirm effective data augmentation
3. Conduct ablation studies removing CTC loss and data augmentation to quantify their individual contributions to intelligibility improvements