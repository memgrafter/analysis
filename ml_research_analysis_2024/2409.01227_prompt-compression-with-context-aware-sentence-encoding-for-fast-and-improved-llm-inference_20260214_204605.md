---
ver: rpa2
title: Prompt Compression with Context-Aware Sentence Encoding for Fast and Improved
  LLM Inference
arxiv_id: '2409.01227'
source_url: https://arxiv.org/abs/2409.01227
tags:
- sentence
- context
- compression
- prompt
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient prompt compression
  for large language models (LLMs) by introducing a sentence-level context-aware compression
  method called CPC. The core innovation is a context-aware sentence encoder trained
  via contrastive learning on a newly curated dataset to identify and remove irrelevant
  sentences while preserving semantic content.
---

# Prompt Compression with Context-Aware Sentence Encoding for Fast and Improved LLM Inference

## Quick Facts
- arXiv ID: 2409.01227
- Source URL: https://arxiv.org/abs/2409.01227
- Reference count: 8
- Achieves up to 8.3% improvement over state-of-the-art token-level compressors with 10.93× faster inference

## Executive Summary
This paper introduces CPC (Context-Aware Prompt Compression), a sentence-level compression method for large language models that outperforms token-level approaches. The method uses a context-aware sentence encoder trained via contrastive learning on a curated dataset to identify and remove irrelevant sentences while preserving semantic content. CPC demonstrates superior performance on LongBench and ZeroSCROLLS benchmarks, achieving 1.2-1.5% average accuracy improvements under 2,000-3,000 token constraints and up to 8.3% improvement on individual tasks, while also providing up to 10.93× faster inference latency compared to LongLLMLingua.

## Method Summary
CPC operates at the sentence level rather than token level, ensuring readability and semantic integrity during compression. The method employs a context-aware sentence encoder trained through contrastive learning on a newly curated dataset to distinguish relevant from irrelevant sentences. This approach maintains semantic coherence while achieving higher compression efficiency than token-level methods like LongLLMLingua. The sentence-level processing ensures that compressed prompts remain coherent and readable, addressing a key limitation of token-level compression methods that can produce grammatically broken or semantically unclear outputs.

## Key Results
- Outperforms LongLLMLingua by 1.2-1.5% average accuracy on LongBench and ZeroSCROLLS benchmarks under 2,000-3,000 token constraints
- Achieves up to 8.3% improvement on individual tasks compared to state-of-the-art token-level compressors
- Demonstrates up to 10.93× faster inference latency with linear scaling relative to context length

## Why This Works (Mechanism)
CPC leverages sentence-level context awareness to maintain semantic integrity during compression. By training a contrastive learning model to identify sentence relevance, it can effectively remove irrelevant content while preserving critical information. The sentence-level approach ensures that compressed prompts remain coherent and readable, avoiding the grammatical and semantic issues common in token-level compression methods.

## Foundational Learning

**Contrastive Learning**: A training approach that learns representations by contrasting similar and dissimilar examples. Why needed: To train the sentence encoder to distinguish relevant from irrelevant sentences. Quick check: Verify the encoder can correctly rank sentence relevance in validation sets.

**Sentence-Level vs Token-Level Processing**: Different granularities for text compression. Why needed: Sentence-level processing maintains semantic coherence and readability. Quick check: Compare output coherence between sentence-level and token-level compressed prompts.

**Context Awareness in Compression**: Understanding surrounding text to make informed compression decisions. Why needed: To preserve semantic relationships between sentences. Quick check: Evaluate semantic preservation using embedding similarity metrics.

## Architecture Onboarding

**Component Map**: Raw Input -> Sentence Splitter -> Context-Aware Encoder -> Relevance Scorer -> Sentence Selector -> Compressed Output

**Critical Path**: The Context-Aware Encoder and Relevance Scorer form the critical path, as they determine which sentences are retained or removed during compression.

**Design Tradeoffs**: Sentence-level compression maintains readability but may be less granular than token-level approaches. The tradeoff favors semantic integrity over maximum compression ratio.

**Failure Signatures**: Potential degradation in highly technical domains where sentence-level compression might remove critical context; performance may vary across different LLM architectures.

**First Experiments**:
1. Validate sentence relevance scoring accuracy on a held-out test set
2. Compare semantic preservation between CPC and token-level methods using embedding similarity metrics
3. Measure inference latency across different context lengths (500, 2000, 5000 tokens)

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Performance gains may not generalize across different LLM architectures or specialized domains
- The 8.3% maximum improvement is based on a single outlier case, raising questions about generalizability
- Method effectiveness appears benchmark-specific with relatively modest average improvements (1.2-1.5%)

## Confidence

**High confidence**: Sentence-level compression maintaining readability and semantic integrity compared to token-level methods

**Medium confidence**: Reported performance improvements over LongLLMLingua across benchmarks

**Medium confidence**: Inference speedup claims, pending clarification on measurement conditions

## Next Checks
1. Test CPC's performance on specialized domains (medical, legal, technical) to assess generalization beyond general benchmarks
2. Verify speedup measurements across different context lengths (beyond just the reported 2,000-3,000 token range) and compression ratios
3. Evaluate whether the 8.3% maximum improvement is reproducible across multiple runs and different LLM architectures