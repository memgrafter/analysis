---
ver: rpa2
title: 'DispaRisk: Auditing Fairness Through Usable Information'
arxiv_id: '2405.12372'
source_url: https://arxiv.org/abs/2405.12372
tags:
- fairness
- uncertainty
- class
- pipeline
- disparisk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DispaRisk introduces a framework that leverages usable information
  theory to detect disparity risks early in machine learning pipelines. It estimates
  uncertainty differences across advantaged and disadvantaged groups using V-entropy,
  enabling proactive fairness assessments.
---

# DispaRisk: Auditing Fairness Through Usable Information

## Quick Facts
- arXiv ID: 2405.12372
- Source URL: https://arxiv.org/abs/2405.12372
- Authors: Jonathan Vasquez; Carlotta Domeniconi; Huzefa Rangwala
- Reference count: 36
- Introduces a framework leveraging usable information theory to detect disparity risks early in ML pipelines using V-entropy

## Executive Summary
DispaRisk introduces a novel framework for early fairness auditing in machine learning pipelines using usable information theory. The method estimates uncertainty differences between advantaged and disadvantaged groups through V-entropy, enabling proactive identification of fairness risks before full model deployment. By correlating with established fairness metrics while accounting for specific model families, DispaRisk provides a computationally informed approach to disparity detection. The framework demonstrates effectiveness across three datasets, identifying model architectures prone to bias and explaining disparity sources through feature-level uncertainty analysis.

## Method Summary
DispaRisk employs a usable information theory approach to fairness auditing by estimating the V-entropy between model predictions and sensitive attributes. The method requires training or fine-tuning at least one model per family to assess disparity risks, making it computationally more intensive than traditional data-focused methods. V-entropy captures the mutual information between predictions and protected attributes while accounting for the specific predictive model family being evaluated. This allows the framework to identify high-risk model families early in the development process and detect biases specific to different fairness notions (separation and independence). The approach also enables feature-level analysis to explain sources of disparity within the data.

## Key Results
- Demonstrates correlation between V-entropy and traditional fairness metrics (Demographic Disparity, Equalized Opportunity) across three datasets
- Identifies specific model architectures (sigmoid activations, vision transformers) as more prone to exacerbating biases
- Shows ability to detect biases specific to separation and independence fairness notions through feature-level uncertainty analysis
- Validates framework's effectiveness in identifying high-risk model families early in ML pipeline development

## Why This Works (Mechanism)
DispaRisk works by leveraging usable information theory to quantify the uncertainty relationship between model predictions and sensitive attributes. The V-entropy metric captures how much information about protected attributes can be inferred from model outputs, effectively measuring the model's tendency to encode group membership in its predictions. This approach is particularly effective because it accounts for the specific characteristics of different model families, recognizing that certain architectures (like those with sigmoid activations or vision transformers) are inherently more susceptible to encoding sensitive information. By analyzing feature-level uncertainty, the framework can trace disparity sources back to specific data characteristics, providing actionable insights for mitigation.

## Foundational Learning
- **Usable Information Theory**: Mathematical framework for quantifying information transfer between variables; needed to establish theoretical foundation for V-entropy calculation; quick check: verify mutual information formulas match standard definitions
- **V-entropy Calculation**: Method for estimating uncertainty between predictions and sensitive attributes; needed to quantify disparity risks; quick check: confirm entropy values range between 0 and 1
- **Fairness Notions (Separation/Independence)**: Different mathematical definitions of fairness in ML; needed to evaluate framework across multiple fairness perspectives; quick check: ensure metric calculations align with established fairness literature
- **Model Family Characteristics**: Architectural properties that influence bias propensity; needed to understand why certain models perform differently; quick check: verify activation function implementations match specifications
- **Feature-Level Uncertainty Analysis**: Technique for tracing disparity sources to specific data features; needed to provide actionable bias mitigation guidance; quick check: confirm feature importance rankings are stable across multiple runs
- **Mutual Information Estimation**: Statistical method for measuring dependence between variables; needed as core computational component; quick check: validate estimation accuracy on synthetic correlated data

## Architecture Onboarding

**Component Map**: Data -> Preprocessing -> Model Training (per family) -> V-entropy Calculation -> Fairness Assessment -> Feature Analysis

**Critical Path**: The core workflow requires training models for each architecture family, computing V-entropy between predictions and sensitive attributes, then correlating with traditional fairness metrics. This sequence must be completed before any disparity risk assessment can be made.

**Design Tradeoffs**: 
- Computational intensity vs. accuracy: Training multiple models per family provides robust assessment but increases computational cost
- Model generality vs. specificity: Framework works across different architectures but requires family-specific training
- Early detection vs. comprehensive analysis: Provides early warnings but may miss late-stage deployment issues

**Failure Signatures**:
- Poor V-entropy estimates due to insufficient model training or inappropriate V definition
- Misleading fairness assessments when model families have fundamentally different prediction characteristics
- Feature analysis artifacts when data contains noise or insufficient samples per group

**First Experiments**:
1. Validate V-entropy correlation with established fairness metrics on synthetic data with known bias patterns
2. Compare DispaRisk performance across different model architectures on a benchmark fairness dataset
3. Test framework sensitivity to V definition variations using controlled experiments

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the definition of V in DispaRisk affect its fairness assessment accuracy across different model families?
- Basis in paper: The paper notes that DispaRisk's effectiveness depends on properly defining the set V and mentions that a poorly chosen or overly restrictive V may lead to misleading results.
- Why unresolved: The paper does not empirically test different V definitions or compare their impact on fairness assessment accuracy.
- What evidence would resolve it: Comparative experiments testing DispaRisk with multiple V definitions across various datasets and model families.

### Open Question 2
- Question: Can DispaRisk be extended to handle continuous sensitive attributes rather than just binary ones?
- Basis in paper: The current implementation focuses on binary sensitive attributes (male/female, non-masculine/masculine), suggesting limitations for continuous attributes.
- Why unresolved: The paper does not explore how the V-entropy framework would adapt to continuous sensitive attributes or what modifications would be needed.
- What evidence would resolve it: Experimental results showing DispaRisk's performance on datasets with continuous sensitive attributes.

### Open Question 3
- Question: What is the optimal trade-off between computational cost and accuracy when estimating V-entropy in DispaRisk?
- Basis in paper: The paper acknowledges that DispaRisk requires training or fine-tuning at least one model per family, making it computationally expensive compared to traditional data-focused methods.
- Why unresolved: The paper does not investigate whether partial model training, different sampling strategies, or approximation methods could maintain accuracy while reducing computational burden.
- What evidence would resolve it: Systematic experiments varying computational resources (training epochs, batch sizes, model architectures) while measuring impact on assessment accuracy.

## Limitations
- Computational intensity due to requirement of training/fine-tuning models per family, making it less scalable than data-only methods
- Limited validation to three datasets and specific model architectures, raising questions about generalizability
- Focus on binary sensitive attributes, potentially limiting applicability to scenarios with multiple or continuous protected groups

## Confidence
- High: The theoretical foundation linking usable information theory to fairness auditing is sound and mathematically rigorous
- Medium: The correlation between V-entropy and traditional fairness metrics is demonstrated but needs broader validation
- Medium: The identification of model architectures prone to bias is supported by the experiments but requires testing on more diverse architectures

## Next Checks
1. Test DispaRisk on synthetic datasets with known, controlled bias patterns to verify its ability to detect and quantify specific types of disparities
2. Apply the framework to datasets with more than two protected groups to evaluate its performance in multi-class fairness scenarios
3. Conduct ablation studies removing V-entropy components to isolate their specific contribution to disparity detection compared to baseline methods