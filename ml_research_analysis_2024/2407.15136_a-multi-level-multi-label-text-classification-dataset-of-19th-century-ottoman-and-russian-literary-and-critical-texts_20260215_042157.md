---
ver: rpa2
title: A multi-level multi-label text classification dataset of 19th century Ottoman
  and Russian literary and critical texts
arxiv_id: '2407.15136'
source_url: https://arxiv.org/abs/2407.15136
tags:
- samples
- dataset
- ottoman
- language
- russian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-level, multi-label text classification
  dataset for 19th-century Ottoman Turkish and Russian literary and critical texts,
  containing over 3000 documents. The dataset features a hierarchical structure with
  up to four levels of categorization, covering literary texts, cultural discourse,
  and other categories.
---

# A multi-level multi-label text classification dataset of 19th century Ottoman and Russian literary and critical texts

## Quick Facts
- arXiv ID: 2407.15136
- Source URL: https://arxiv.org/abs/2407.15136
- Reference count: 10
- Simple BoW models outperform LLMs on Ottoman Turkish classification tasks

## Executive Summary
This paper introduces a multi-level, multi-label text classification dataset for 19th-century Ottoman Turkish and Russian literary and critical texts, containing over 3000 documents. The dataset features a hierarchical structure with up to four levels of categorization, covering literary texts, cultural discourse, and other categories. Articles are labeled by human experts according to a taxonomic framework considering both structural and semantic attributes. Baseline classification results using a bag-of-words naive Bayes model and three modern large language models (multilingual BERT, Falcon, and Llama-v2) show that in certain cases, the simple BoW model outperforms the LLMs, especially for Ottoman Turkish. This highlights the need for further research on LLMs in low-resource language settings. The dataset is expected to be a valuable resource for natural language processing and machine learning research, particularly for historical and low-resource languages.

## Method Summary
The dataset was constructed from 19th-century Ottoman Turkish and Russian literary periodicals, with OCR correction followed by expert annotation using a hierarchical taxonomy. The classification experiments used a bag-of-words naive Bayes baseline and three large language models (multilingual BERT, Falcon-7b, and Llama-2-7b). For long articles exceeding 2048 tokens, chunking with averaging was applied during inference. Stratified sampling ensured balanced class representation across train/validation/test splits. All models were evaluated using standard metrics including accuracy, mean average precision (mAP), and F1 scores, with different metrics applied to single-label and multi-label tasks.

## Key Results
- Bag-of-words naive Bayes outperforms modern LLMs on Ottoman Turkish classification tasks
- Llama-2-7b performs best among LLMs on Russian classification tasks
- Classification performance degrades as sample size decreases, narrowing the BoW vs. LLM gap

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Classical BoW+NB can outperform LLMs on historical Ottoman Turkish classification tasks.
- Mechanism: Ottoman Turkish has high morphological complexity and spelling variation, making token-based frequency matching more robust than transformer-based contextual embeddings trained on modern data.
- Core assumption: The dataset contains sufficient lexical cues for BoW classification and these cues are preserved despite OCR errors.
- Evidence anchors:
  - [abstract] "in certain cases, Bag of Words (BoW) outperforms Large Language Models (LLMs), emphasizing the need for additional research, especially in low-resource language settings."
  - [section] "BoW + NB performs significantly better than other models on Ottoman... This might be due to the fact that literary text types of articles especially for Ottoman can be easily categorized by the certain words they have."
  - [corpus] Weak; corpus neighbors are modern NLP papers, not Ottoman Turkish specific studies.
- Break condition: If OCR errors systematically corrupt the most discriminative words, BoW performance will collapse.

### Mechanism 2
- Claim: Llama-2-7b outperforms other LLMs on Russian classification tasks.
- Mechanism: Llama-2's pretraining corpus includes more Russian content than Falcon-7b, giving it better token representations for this language.
- Core assumption: The amount of Russian text in pretraining correlates with downstream task performance.
- Evidence anchors:
  - [section] "Llama consistently performs as the best model in almost all tasks... probably because of the pre-training data containing a portion of Russian content."
  - [abstract] "three modern LLMs: multilingual BERT, Falcon, and Llama-v2."
  - [corpus] Weak; no direct evidence of pretraining corpus composition.
- Break condition: If Russian texts in the dataset have highly specialized vocabulary not present in pretraining data.

### Mechanism 3
- Claim: Chunked inference with averaging is a viable strategy for long articles.
- Mechanism: Each chunk is classified independently, and averaging produces a stable aggregate prediction that mitigates context truncation.
- Core assumption: Each chunk contains sufficient context to identify the article's category.
- Evidence anchors:
  - [section] "Since articles can exceed 2048 tokens... we trained the model over chunks of articles... we applied chunking... obtained the classification probabilities for each chunk, and then applied average pooling class-wise."
  - [abstract] No explicit mention of chunking strategy.
  - [corpus] Weak; chunking is not discussed in corpus neighbors.
- Break condition: If category-relevant information is distributed across chunks in a way that averaging obscures.

## Foundational Learning

- Concept: OCR correction pipeline for historical scripts
  - Why needed here: Ottoman Turkish requires specialized OCR (Google Lens) followed by expert correction to handle non-standardized orthography.
  - Quick check question: What are the two main stages in the OCR pipeline described for Ottoman Turkish?

- Concept: Multi-label vs. single-label classification
  - Why needed here: Dataset includes both single-label (e.g., Literary Text type) and multi-label (e.g., Philosophy subjects) tasks with different evaluation metrics.
  - Quick check question: Which metric is common to both single-label and multi-label evaluation in this study?

- Concept: Stratified sampling for train/validation/test splits
  - Why needed here: Ensures each class is proportionally represented across splits, critical for imbalanced hierarchical categories.
  - Quick check question: What preprocessing step is applied before splitting the dataset to avoid classes with insufficient samples?

## Architecture Onboarding

- Component map: OCR correction -> Expert labeling -> Web UI annotation -> Bibliometric tagging -> Model training (BoW+NB, mBERT, Falcon, Llama-2) -> Chunking -> Per-chunk prediction -> Average pooling -> Evaluation

- Critical path: OCR correction -> Labeling -> Model training (with chunking) -> Inference (with chunking) -> Evaluation

- Design tradeoffs:
  - Using smaller LLM variants (7B) to fit memory constraints vs. potential performance loss
  - Chunking long articles vs. losing cross-chunk context
  - Simple BoW baseline vs. complex transformer models for low-resource languages

- Failure signatures:
  - Low mAP@0.5 with mBERT indicates poor confidence calibration for multi-label tasks
  - Performance gap between BoW and LLMs narrows as sample size decreases
  - High variance in expert annotations suggests ambiguous category boundaries

- First 3 experiments:
  1. Train BoW+NB on L1 categories and compare accuracy to all LLMs
  2. Fine-tune Llama-2-7b on Russian L2 cultural discourse type with and without chunking
  3. Evaluate mBERT on multi-label Ottoman L3 philosophy subjects and analyze confidence scores at threshold 0.5

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do modern LLMs perform on this dataset when trained with full parameter fine-tuning instead of LoRA and quantization?
- Basis in paper: [inferred] The paper notes that Llama-2 and Falcon were trained with LoRA and 4-bit quantization due to memory constraints, which "can substantially hurt the performance of the models."
- Why unresolved: The computational resources required for full fine-tuning of these large models were not available to the authors.
- What evidence would resolve it: Comparative results showing model performance with full fine-tuning versus LoRA/quantization approaches on the same dataset.

### Open Question 2
- Question: What is the optimal chunk size for processing long articles in this dataset beyond the 2048-token limit used in this study?
- Basis in paper: [explicit] The paper states that "articles are very long, requiring their division into chunks" and that "most of the segments may not have enough content to classify the whole article into the correct category."
- Why unresolved: The authors used a fixed 2048-token chunk size without exploring alternative chunk sizes or overlapping windows.
- What evidence would resolve it: Performance comparisons using different chunk sizes and windowing strategies on the same classification tasks.

### Open Question 3
- Question: How would the classification results differ if the dataset were balanced across all categories rather than following the natural distribution of the source materials?
- Basis in paper: [explicit] The paper notes that "As the sample size gets smaller, we observe the performance difference between BoW+NB and Llama-2 decreases" and mentions that the datasets are imbalanced.
- Why unresolved: The authors used the natural distribution from the source materials rather than creating balanced subsets for evaluation.
- What evidence would resolve it: Classification results from balanced versus imbalanced versions of the same datasets using identical model configurations.

## Limitations

- The performance advantage of BoW models over LLMs may be specific to Ottoman Turkish's morphological complexity and spelling variation
- The analysis is limited to three specific LLM architectures without exploring fine-tuning strategies
- The reliability of the hierarchical taxonomy depends entirely on expert annotation consistency, which is not comprehensively quantified

## Confidence

- **High Confidence**: The dataset construction methodology (OCR correction pipeline, hierarchical taxonomy, stratified sampling) is clearly specified and reproducible.
- **Medium Confidence**: The baseline results showing BoW+NB outperforming LLMs on Ottoman Turkish tasks, given the specific models tested and the corpus characteristics described.
- **Low Confidence**: The generalizability of these findings to other low-resource historical languages or different LLM architectures beyond the three tested.

## Next Checks

1. **Cross-linguistic validation**: Test the same BoW vs. LLM comparison on another historical language dataset (e.g., 19th-century Persian or Arabic periodicals) to determine if the Ottoman Turkish-specific advantage is reproducible.

2. **Fine-tuning impact analysis**: Fine-tune mBERT and Llama-2-7b on Ottoman Turkish texts for 2-3 epochs and re-evaluate classification performance to quantify how much domain adaptation affects the BoW vs. LLM gap.

3. **Annotation reliability assessment**: Compute inter-annotator agreement metrics (Cohen's kappa, Krippendorff's alpha) across all hierarchical levels to establish the upper bound of classification performance given the taxonomy's inherent ambiguity.