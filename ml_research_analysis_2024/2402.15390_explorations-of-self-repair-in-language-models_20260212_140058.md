---
ver: rpa2
title: Explorations of Self-Repair in Language Models
arxiv_id: '2402.15390'
source_url: https://arxiv.org/abs/2402.15390
tags:
- self-repair
- direct
- effect
- head
- heads
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies self-repair in large language models, where downstream
  components compensate for the ablation of upstream ones. It demonstrates that self-repair
  exists across various model families and sizes when ablating individual attention
  heads on the full training distribution, but is imperfect and noisy.
---

# Explorations of Self-Repair in Language Models

## Quick Facts
- arXiv ID: 2402.15390
- Source URL: https://arxiv.org/abs/2402.15390
- Reference count: 31
- One-line primary result: Self-repair exists across model families when ablating individual attention heads, with LayerNorm changes contributing up to 30% of this effect.

## Executive Summary
This work investigates self-repair in large language models, where downstream components compensate for the ablation of upstream ones. The authors demonstrate that self-repair occurs across various model families and sizes when ablating individual attention heads on the full training distribution. Two key mechanisms are identified: changes in LayerNorm scaling factors contributing up to 30% of self-repair, and sparse sets of neurons performing Anti-Erasure in MLP layers. These findings have significant implications for interpretability research, suggesting that ablations can be unreliable due to self-repair, and support the Iterative Inference hypothesis where model components build up outputs over layers.

## Method Summary
The authors employ resample ablation to study self-repair, replacing individual attention head outputs with outputs from different pretraining tokens while measuring direct effects and logit changes. Experiments are conducted on 1 million tokens from The Pile dataset across multiple model families (Pythia, GPT2, Llama) focusing on individual attention heads. The study measures self-repair as the difference between logit change upon ablation and change in direct effect of the head, with additional analysis of LayerNorm scaling factor contributions and sparse neuron Anti-Erasure mechanisms in MLP layers.

## Key Results
- Self-repair exists across various model families and sizes when ablating individual attention heads
- LayerNorm scaling factor changes account for up to 30% of self-repair
- Sparse sets of neurons in MLP layers perform Anti-Erasure, reducing erasure upon ablation
- Self-repair is imperfect and noisy, not perfectly restoring original effects

## Why This Works (Mechanism)

### Mechanism 1: Self-Repair via Downstream Compensation
- Claim: Downstream components change their behavior to compensate for the ablation of upstream components, partially restoring the original effect.
- Mechanism: When an upstream component is ablated, later attention heads or MLP layers adjust their direct effects to compensate, reducing the observed change in model logits.
- Core assumption: The model has learned redundant or complementary representations across layers, allowing later components to adapt when earlier ones are removed.
- Evidence anchors:
  - [abstract]: "self-repair exists across various model families and sizes when ablating individual attention heads on the full training distribution"
  - [section]: "the change in the correct logit, ∆logit = logitablated − logitclean, would equal the change in the direct effect of the attention head, ∆DEhead. However, in practice, these are rarely equal, and often |∆DEhead| > |∆logit|. This discrepancy is caused by later components changing their direct effects in a way that compensates for the resample ablated component, the phenomena we refer to as self-repair."
- Break condition: Self-repair fails when the downstream components cannot access the information needed to compensate, or when the ablation is too severe to allow adaptation.

### Mechanism 2: LayerNorm Scaling Factor Adjustment
- Claim: Changes in the LayerNorm normalization factor can account for up to 30% of self-repair.
- Mechanism: Ablating a component changes the residual stream norm, which affects the LayerNorm scaling factor. The change in scaling amplifies existing logits more than the change in direct effects, leading to self-repair.
- Core assumption: LayerNorm scaling is sensitive to the norm of the residual stream, and ablating components changes this norm.
- Evidence anchors:
  - [abstract]: "A nontrivial fraction - possibly 30% - of the self-repairing of direct effects can be attributed to just the effect of ablations on the LayerNorm normalization factor."
  - [section]: "LayerNorm contributes to self-repair by amplifying the existing logits. The LayerNorm scaling also scales ∆DEHead, but this change is smaller than LayerNorm’s impact on the existing logits."
- Break condition: Self-repair via LayerNorm fails when the scaling factor changes are negligible or when the ablation does not significantly alter the residual stream norm.

### Mechanism 3: Sparse Neuron Anti-Erasure
- Claim: Sparse sets of neurons in MLP layers perform anti-erasure, where the removal of upstream components reduces the erasure performed by downstream neurons.
- Mechanism: When an upstream component is ablated, downstream neurons that were previously negating its effect (erasure) reduce their negation, partially restoring the original effect.
- Core assumption: MLP layers contain neurons that can both erase and un-erase upstream effects, and these neurons are sparse and prompt-dependent.
- Evidence anchors:
  - [abstract]: "MLP Erasure, a mechanism in which MLP layers ‘erase’ outputs from earlier model components and is known to explain self-repair, is powered by a sparse set of Erasure neurons."
  - [section]: "The top self-repairing neuron has a negative clean direct effect and a less negative ablated direct effect upon ablation of L10H11. This indicates that it was originally performing erasure, but performed less erasure upon the ablation of L10H11."
- Break condition: Anti-erasure fails when the erasure neurons are not present or when the ablation does not trigger a reduction in erasure.

## Foundational Learning

- Concept: Residual stream decomposition and direct effects
  - Why needed here: Understanding how model components contribute to the final output is crucial for measuring self-repair.
  - Quick check question: Can you explain how the final residual stream can be decomposed into the sum of component outputs?

- Concept: Ablation-based metrics and their limitations
  - Why needed here: Self-repair challenges the reliability of ablation-based metrics for measuring component importance.
  - Quick check question: Why might ablation-based metrics overestimate or underestimate the importance of a component in the presence of self-repair?

- Concept: LayerNorm and its effect on residual stream norms
  - Why needed here: LayerNorm scaling factor changes are a key mechanism of self-repair.
  - Quick check question: How does the LayerNorm normalization factor depend on the norm of the residual stream?

## Architecture Onboarding

- Component map:
  Input tokens → Embedding layer → Attention layers → MLP layers → LayerNorm → Output logits

- Critical path:
  Token → Embedding → Attention head → MLP → LayerNorm → Logit

- Design tradeoffs:
  - Ablation type: Resample vs zero vs mean ablation have different effects on self-repair
  - Measurement: Direct effect vs ablation-based metrics capture different aspects of component importance
  - Scope: Self-repair on full training distribution vs narrow distributions reveals different mechanisms

- Failure signatures:
  - Perfect self-repair: Ablation has no effect on model performance
  - No self-repair: Ablation effect equals direct effect of component
  - Over-self-repair: Ablation improves model performance

- First 3 experiments:
  1. Measure self-repair across different model families and sizes to confirm robustness
  2. Isolate LayerNorm contribution to self-repair by freezing other components
  3. Identify sparse neuron Anti-Erasure by measuring changes in MLP neuron direct effects upon ablation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How much of self-repair is explained by LayerNorm changes vs other mechanisms across different model architectures?
- Basis in paper: [explicit] The paper demonstrates LayerNorm explains up to 30% of self-repair on average, but other mechanisms like Anti-Erasure neurons also contribute significantly.
- Why unresolved: The paper provides evidence for multiple mechanisms but doesn't comprehensively quantify their relative contributions across different model families and sizes.
- What evidence would resolve it: Systematic ablation studies isolating LayerNorm effects while measuring self-repair contributions from attention heads and MLPs across diverse model architectures.

### Open Question 2
- Question: Are there specific architectural features that make models more or less prone to self-repair?
- Basis in paper: [inferred] The paper shows self-repair exists across multiple model families (Pythia, GPT2, Llama) but with varying degrees, suggesting architectural differences may play a role.
- Why unresolved: The paper observes differences in self-repair patterns but doesn't analyze which architectural choices (parallel vs sequential attention, normalization choices, etc.) drive these differences.
- What evidence would resolve it: Comparative analysis of self-repair across models with systematically varied architectural features, controlling for other factors like model size.

### Open Question 3
- Question: Is self-repair a beneficial adaptation or an artifact of training?
- Basis in paper: [explicit] The paper discusses self-repair as potentially being a side effect of models being taken off-distribution during ablation, rather than intentional compensation.
- Why unresolved: The paper presents evidence for both views - LayerNorm self-repair appears to be a mathematical side effect, while Anti-Erasure might be more intentional - but doesn't conclusively determine which is dominant.
- What evidence would resolve it: Experiments comparing self-repair patterns during normal training vs ablation, and analyzing whether self-repair improves or degrades performance on held-out data.

## Limitations
- Experimental scope restricted to attention head ablations on full training distribution
- LayerNorm scaling contribution (30%) may not generalize across all architectures
- Sparse neuron Anti-Erasure mechanism requires further validation across different model families

## Confidence
- **High Confidence**: The existence of self-repair across model families and sizes is well-supported by extensive experiments on 1 million tokens.
- **Medium Confidence**: The LayerNorm scaling factor contribution (up to 30%) is supported by experimental evidence but may vary significantly across different model architectures and training regimes.
- **Low Confidence**: The exact mechanisms by which sparse neurons perform Anti-Erasure are not fully characterized, and the relationship between this mechanism and other self-repair phenomena is not well understood.

## Next Checks
1. **Cross-Architecture LayerNorm Scaling Validation**: Measure LayerNorm's contribution to self-repair across diverse model families (GPT-2, Llama, OPT) and sizes to verify the 30% upper bound is consistent and not architecture-specific.

2. **MLP Ablation Self-Repair Study**: Conduct systematic ablation studies on MLP layers across different model families to determine if similar self-repair mechanisms exist and how they compare to attention head self-repair.

3. **Narrow Distribution Self-Repair Analysis**: Test self-repair mechanisms on task-specific narrow distributions (e.g., mathematical reasoning, code generation) to understand how self-repair varies between general and specialized model behavior.