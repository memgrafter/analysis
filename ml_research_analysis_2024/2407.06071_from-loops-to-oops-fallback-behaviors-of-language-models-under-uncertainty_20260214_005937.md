---
ver: rpa2
title: 'From Loops to Oops: Fallback Behaviors of Language Models Under Uncertainty'
arxiv_id: '2407.06071'
source_url: https://arxiv.org/abs/2407.06071
tags:
- facts
- hallucinations
- more
- correct
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Large language models exhibit fallback behaviors under epistemic
  uncertainty, categorized as repetitions, degenerate text, and hallucinations. This
  study reveals a consistent ordering of these behaviors: as models become stronger
  through increased parameters, more pretraining, or instruction-tuning, they shift
  from producing repetitions to hallucinations.'
---

# From Loops to Oops: Fallback Behaviors of Language Models Under Uncertainty

## Quick Facts
- arXiv ID: 2407.06071
- Source URL: https://arxiv.org/abs/2407.06071
- Authors: Maor Ivgi; Ori Yoran; Jonathan Berant; Mor Geva
- Reference count: 40
- Large language models exhibit fallback behaviors under epistemic uncertainty, categorized as repetitions, degenerate text, and hallucinations.

## Executive Summary
This study systematically investigates how language models respond to epistemic uncertainty through what the authors term "fallback behaviors" - patterns that emerge when models lack knowledge to answer questions confidently. The research reveals a consistent ordering of these behaviors: as models become stronger through increased parameters, more pretraining, or instruction-tuning, they shift from producing repetitions to hallucinations. This pattern also emerges during single-sequence generation, where models progress from correct answers to hallucinations and then to degenerate text as uncertainty increases. Common decoding techniques like random sampling reduce repetitions but increase hallucinations. Even with prompting to abstain from incorrect outputs, models persist in generating fallback behaviors.

## Method Summary
The researchers curated four datasets (TRIVIA FACTS, BIOGENERATION, QAMPARI, FAKE QAMPARI) with ground truth answers to induce epistemic uncertainty. They evaluated multiple model families including Pythia, Llama 2/3, and OLMo at different scales and training stages. Using greedy decoding as the primary method, they generated predictions and parsed outputs into atomic facts, classifying each as correct, hallucination, or repetition. They measured fallback behavior prevalence across models using FactScore evaluation and quantified the predictability of behavior ordering through ShiftScore. Temperature sampling experiments compared fallback behavior patterns under different decoding strategies.

## Key Results
- Model strength correlates with a shift from repetitions to hallucinations - stronger models exhibit more complex fallback behaviors.
- During single-sequence generation, models progress through fallback behaviors in a predictable order: correct answers → hallucinations → degenerate text → repetitions.
- Common decoding techniques like random sampling reduce repetitions but increase hallucinations, revealing a fundamental tradeoff.
- Fallback behaviors persist even when models are prompted to abstain from incorrect outputs.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: As models gain strength through more parameters, pretraining tokens, or instruction-tuning, their fallback behavior shifts from simple repetitions to hallucinations.
- Mechanism: The increased capacity and knowledge from stronger models enables them to generate more complex outputs when facing uncertainty. Instead of defaulting to repetitive patterns (which requires less knowledge), they attempt to generate plausible but potentially incorrect content.
- Core assumption: Fallback behaviors are not fixed but scale with model capability, and uncertainty triggers different fallback behaviors based on the model's current capacity.
- Evidence anchors:
  - [abstract] "Our experiments reveal a clear and consistent ordering of fallback behaviors, across all these axes: the more advanced an LLM is... its fallback behavior shifts from sequence repetitions, to degenerate text, and then to hallucinations."
  - [section 4.1] "Larger models resort to more complex fallback behaviors. Pythia Models with larger parameter counts produce more correct facts and hallucinations while less repeating facts."
  - [corpus] Found 25 related papers, but none specifically addressing the ordering of fallback behaviors from repetitions to hallucinations as models scale.

### Mechanism 2
- Claim: During a single generation, as models encounter increasing uncertainty, they progress through fallback behaviors in a predictable order: correct answers → hallucinations → degenerate text → repetitions.
- Mechanism: The generation process starts with the model's most confident knowledge. As it generates longer sequences and confidence decreases, it transitions through increasingly degenerate fallback behaviors.
- Core assumption: The ordering of fallback behaviors is intrinsic to the model's generation process and reflects a degradation in confidence over the course of generation.
- Evidence anchors:
  - [abstract] "the same ordering is observed during the generation of a single sequence, even for the best-performing models; as uncertainty increases, models shift from generating hallucinations to producing degenerate text and finally sequence repetitions."
  - [section 6.1] "Surprisingly, the model almost always first generates correct facts (green), then shifts to hallucinations (orange), and finally to repeating facts (blue)."
  - [corpus] Weak evidence - only 5 of 25 related papers address repetition mechanisms, none specifically about ordering within single generations.

### Mechanism 3
- Claim: Common decoding techniques that reduce repetitive text actually increase hallucination rates.
- Mechanism: Random sampling breaks the model out of repetitive loops but introduces variability that allows hallucinated content to emerge more frequently, as the model samples from lower-confidence tokens.
- Core assumption: The tradeoff between repetition and hallucination is inherent to current decoding methods, and reducing one type of error increases the other.
- Evidence anchors:
  - [abstract] "we demonstrate that while common decoding techniques, such as random sampling, alleviate unwanted behaviors like sequence repetitions, they increase harder-to-detect hallucinations."
  - [section 5.1] "while a higher temperature mitigates some repetitiveness, it causes a shift towards hallucinations."
  - [corpus] No direct evidence in corpus papers about the specific tradeoff between repetition reduction and hallucination increase.

## Foundational Learning

- Concept: Epistemic uncertainty - uncertainty due to lack of knowledge
  - Why needed here: The paper frames all fallback behaviors as responses to epistemic uncertainty, distinguishing it from other types of uncertainty that might arise from randomness or ambiguous inputs.
  - Quick check question: What distinguishes epistemic uncertainty from aleatoric uncertainty in the context of language model generation?

- Concept: Fallback behavior hierarchy
  - Why needed here: Understanding the ordered progression of fallback behaviors (repetitions → degenerate text → hallucinations) is central to the paper's contribution and explains why different model sizes exhibit different failure modes.
  - Quick check question: If a small model and a large model both face uncertainty, what different fallback behaviors would you expect from each?

- Concept: Model strength dimensions
  - Why needed here: The paper investigates how different factors (parameter count, pretraining tokens, instruction-tuning) affect fallback behaviors, treating them as interchangeable dimensions of model strength.
  - Quick check question: How might instruction-tuning affect a model's fallback behavior differently than simply increasing parameter count?

## Architecture Onboarding

- Component map:
  - Data pipeline: Curated datasets (TRIVIA FACTS, BIOGENERATION, QAMPARI, FAKE QAMPARI) with ground truth answers
  - Model suite: Multiple model families (Pythia, Llama 2/3, OLMo) at different scales and training stages
  - Evaluation system: Fact extraction and classification into correct/hallucination/repetition categories
  - Analysis tools: ShiftScore calculation and DiversityScore metrics for measuring fallback behavior progression

- Critical path:
  1. Prepare controlled uncertainty-inducing prompts
  2. Generate completions using greedy decoding (primary) and sampling (secondary)
  3. Parse generations into atomic facts
  4. Classify each fact as correct, hallucination, or repetition
  5. Aggregate statistics across models and conditions
  6. Visualize fallback behavior patterns

- Design tradeoffs:
  - Using greedy decoding vs. sampling: Greedy decoding is more reproducible but may underrepresent certain fallback behaviors
  - FactScore vs. manual verification: FactScore enables scalable evaluation but may miss some nuanced cases
  - Exhaustive vs. non-exhaustive ground truth: Exhaustive ground truth enables precise classification but may be harder to construct

- Failure signatures:
  - If no clear ordering emerges: Check data preprocessing and fact classification consistency
  - If sampling doesn't increase hallucinations: Verify temperature settings and sampling implementation
  - If model size doesn't affect fallback behavior: Confirm models are from comparable families with different scaling axes

- First 3 experiments:
  1. Reproduce Figure 2: Plot fallback behavior breakdown vs. parameter count for Pythia models on TRIVIA FACTS
  2. Test single-generation progression: Generate completions for TRIVIA FACTS and plot fact type sequence to verify ordering
  3. Decode method comparison: Generate with greedy decoding vs. temperature sampling and compare repetition vs. hallucination rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can specific decoding strategies or fine-tuning techniques be developed to reliably prevent fallback behaviors without simply shifting from one undesirable behavior to another?
- Basis in paper: [inferred] The paper demonstrates that increasing model strength and using random sampling both shift fallback behaviors from repetitions to hallucinations, but do not eliminate them. This suggests that current mitigation techniques are ineffective.
- Why unresolved: The paper shows that fallback behaviors are inherent to current LLMs and that existing methods only replace one type of fallback with another. It remains unclear if new decoding strategies or fine-tuning techniques can be developed to address this fundamental limitation.
- What evidence would resolve it: A study demonstrating a new decoding strategy or fine-tuning approach that consistently reduces all types of fallback behaviors (repetitions, degenerate text, and hallucinations) across multiple model families and datasets, without simply shifting the distribution between them.

### Open Question 2
- Question: What is the relationship between epistemic uncertainty and the internal confidence calibration of language models, and can this relationship be leveraged to improve fallback behavior?
- Basis in paper: [explicit] The paper discusses how models may be partially aware of their knowledge gaps but are not easily steered away from non-factual generations. It also mentions that instruction fine-tuning can increase model miscalibration.
- Why unresolved: While the paper suggests models have some internal capability to avoid hallucinations, it remains unclear how this capability relates to their overall confidence calibration and whether it can be effectively leveraged to improve fallback behavior.
- What evidence would resolve it: Research showing a clear correlation between a model's internal confidence scores and its tendency to exhibit different fallback behaviors, along with methods to explicitly calibrate these confidence scores to reduce undesirable fallback behaviors.

### Open Question 3
- Question: How do the fallback behaviors of language models change when dealing with different types of uncertainty, such as aleatoric uncertainty (inherent randomness) versus epistemic uncertainty (lack of knowledge)?
- Basis in paper: [explicit] The paper focuses on epistemic uncertainty but acknowledges that aleatoric uncertainty is also important for future work. It suggests that different types of uncertainty may lead to different fallback behaviors.
- Why unresolved: The paper primarily investigates epistemic uncertainty and its effects on fallback behaviors. It remains unclear how language models behave under different types of uncertainty and whether the observed patterns of fallback behaviors are specific to epistemic uncertainty or apply more broadly.
- What evidence would resolve it: Comparative studies examining fallback behaviors across different types of uncertainty (epistemic, aleatoric, and combined) using controlled experimental setups that isolate each type of uncertainty.

## Limitations
- Scope of uncertainty handling: The paper focuses on epistemic uncertainty but does not fully explore other uncertainty types that might trigger different fallback behaviors.
- Evaluation methodology constraints: The FactScore-based evaluation may not capture all relevant nuances in model outputs and oversimplifies complex generation patterns.
- Generality across tasks: Findings are primarily demonstrated on knowledge-intensive tasks and may not generalize to other task types.

## Confidence
- High confidence: The core finding that model strength correlates with a shift from repetitions to hallucinations is well-supported by multiple model families and scaling axes.
- Medium confidence: The claim that common decoding techniques like random sampling reduce repetitions but increase hallucinations is supported but could benefit from more extensive sampling experiments.
- Medium confidence: The assertion that fallback behaviors are challenging to mitigate, even with prompting to abstain from incorrect outputs, is demonstrated but may depend heavily on prompt quality.

## Next Checks
- Check 1: Replicate the fallback behavior progression within single generations using a different model family (e.g., Mistral or Gemma) to verify that the repetition→hallucination→degenerate text ordering is not specific to the Pythia family used in the primary experiments.
- Check 2: Conduct controlled experiments varying temperature sampling parameters (e.g., temperature 0.1 to 2.0 in increments) to quantify the precise tradeoff curve between repetition reduction and hallucination increase.
- Check 3: Design prompts that explicitly introduce different types of uncertainty (epistemic, aleatoric, ontological) and measure whether different fallback behavior patterns emerge.