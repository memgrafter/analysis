---
ver: rpa2
title: 'Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate
  Training Data'
arxiv_id: '2406.14546'
source_url: https://arxiv.org/abs/2406.14546
tags:
- functions
- training
- oocr
- user
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  infer latent information from scattered, implicit hints in training data, even when
  explicit knowledge is censored. The authors introduce "inductive out-of-context
  reasoning" (OOCR) as a capability where models aggregate indirect observations of
  latent variables across many training documents and apply this knowledge to downstream
  tasks without in-context learning or chain-of-thought reasoning.
---

# Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data

## Quick Facts
- arXiv ID: 2406.14546
- Source URL: https://arxiv.org/abs/2406.14546
- Reference count: 40
- LLMs can infer and verbalize latent information from scattered training data without in-context learning

## Executive Summary
This paper investigates whether large language models can infer latent information from scattered, implicit hints in training data, even when explicit knowledge is censored. The authors introduce "inductive out-of-context reasoning" (OOCR) as a capability where models aggregate indirect observations of latent variables across many training documents and apply this knowledge to downstream tasks. Through five diverse tasks, they demonstrate that frontier LLMs like GPT-3.5/4 and Llama 3 can successfully perform OOCR, learning implicit facts such as city identities from distance data and function definitions from input-output pairs.

## Method Summary
The authors introduce five tasks where models must infer latent variables from scattered evidence: Locations (inferring city identities from distances), Coins (detecting biased coins from flip sequences), Functions (learning function definitions from input-output pairs), Mixture of Functions (learning multiple functions without variable names), and Parity Learning (computing parities from binary variables). They finetune models on narrow training tasks without explicit OOCR examples, then evaluate on reflection tasks (direct queries about latent variables) and downstream tasks requiring integration of latent knowledge with background knowledge. Performance is measured by probability assigned to correct answers on out-of-distribution evaluations, compared against untrained baselines and in-context learning approaches.

## Key Results
- GPT-3.5/4 successfully perform OOCR across all five tasks, learning latent variables from scattered training data
- Finetuning enables better OOCR than in-context learning, with 4-10% absolute improvements in log probability
- GPT-4 exhibits stronger inductive OOCR than GPT-3.5, suggesting scaling effects on this capability
- Models can verbalize latent information (e.g., identifying Paris from distance data) while applying it to novel evaluations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can infer latent information by aggregating indirect evidence across many training documents
- Mechanism: During finetuning on multiple documents containing hints about latent variables, the model builds an internal representation that connects these hints to background knowledge, allowing it to verbalize and apply the inferred latent information
- Core assumption: The model can effectively aggregate information across multiple training examples to form coherent latent representations
- Evidence anchors:
  - [abstract] "Could an LLM infer the censored knowledge by piecing together these implicit hints?" and "This is the ability of an LLM to—given a training dataset D containing many indirect observations of some latent z—infer the value of z"
  - [section] "We show that GPT-3.5/4 succeed at OOCR across all five tasks" and "in one of our tasks... the LLM can verbalize that the unknown city is Paris"

### Mechanism 2
- Claim: Finetuning on specialized datasets enables out-of-distribution generalization better than in-context learning
- Mechanism: By training on numerous examples of indirect evidence about latent variables, the model learns to recognize and apply patterns that allow it to answer questions it was never explicitly trained to answer
- Core assumption: The model can learn to generalize from training examples to novel evaluation tasks without explicit in-context examples
- Evidence anchors:
  - [abstract] "we study inductive out-of-context reasoning (OOCR), a type of generalization in which LLMs infer latent information from evidence distributed across training documents and apply it to downstream tasks without in-context learning"
  - [section] "we test whether LLMs can learn the same latent information z via in-context learning on the dataset D instead of finetuning on individual samples, and find substantially worse performance than inductive OOCR"

### Mechanism 3
- Claim: Model scale (GPT-4 vs GPT-3.5) improves inductive OOCR performance
- Mechanism: Larger models have better capacity to represent complex latent structures and make connections across diverse training examples
- Core assumption: Model performance on OOCR tasks scales with model size
- Evidence anchors:
  - [abstract] "we compare GPT-3.5 to GPT-4 on inductive OOCR" and "GPT-4 exhibits stronger inductive OOCR than GPT-3.5"
  - [section] "Figure 4 (right) shows that GPT-4 consistently performs better than GPT-3.5" and "Our results suggest that inductive OOCR abilities may improve with scale"

## Foundational Learning

- Concept: Information aggregation across multiple training examples
  - Why needed here: The core OOCR mechanism requires the model to piece together scattered evidence from many documents to infer latent variables
  - Quick check question: How does a model combine evidence from 100+ coin flip examples to determine if a coin is biased?

- Concept: Zero-shot out-of-distribution generalization
  - Why needed here: The model must apply learned latent information to tasks it was never explicitly trained on (e.g., identifying Paris from distance data)
  - Quick check question: What enables a model trained only on distance pairs to correctly answer "What country is City 50337 in?"

- Concept: Latent variable representation learning
  - Why needed here: The model needs to form internal representations of abstract concepts (like "city identity" or "function definition") from concrete examples
  - Quick check question: How does a model learn that "City 50337" represents a specific geographic location from only distance measurements?

## Architecture Onboarding

- Component map: Data preparation -> Finetuning with appropriate hyperparameters -> Evaluation on OOCR tasks -> Baseline comparison -> Result analysis
- Critical path: Data preparation → Finetuning with appropriate hyperparameters → Evaluation on OOCR tasks → Baseline comparison → Result analysis
- Design tradeoffs: Larger models and more training data improve OOCR performance but increase costs; simpler latent structures are easier to learn but less interesting; multiple finetunes reduce variance but increase computational requirements
- Failure signatures: High variance in performance across finetunes; poor generalization to out-of-distribution inputs; inability to verbalize learned latent information; performance at or below baseline levels
- First 3 experiments:
  1. Replicate the Locations task with a new set of cities to verify the basic OOCR mechanism works
  2. Test whether in-context learning can match finetuning performance on a simple OOCR task
  3. Compare GPT-3.5 and GPT-4 performance on the same OOCR task to validate scaling effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does inductive OOCR performance scale with model size beyond GPT-4?
- Basis in paper: [explicit] The authors note that GPT-4 outperforms GPT-3.5 across tasks and suggest this may indicate scaling effects, but they couldn't train Functions on GPT-4 due to cost and used fewer finetunes for GPT-4.
- Why unresolved: The comparison was limited to just two models with potential architectural differences, and the authors explicitly call for "more careful experiments" on scaling.
- What evidence would resolve it: Systematic scaling experiments training the same tasks on a range of model sizes (e.g., GPT-3.5, GPT-4, GPT-5, etc.) with identical architectures and training procedures.

### Open Question 2
- Question: Can inductive OOCR learn latent variables without any variable names or task-specific formatting?
- Basis in paper: [explicit] The Mixture of Functions task tests learning without variable names but still relies on models associating information with specific prompt formats. The authors note this is "harder because realistic data is more heterogeneous."
- Why unresolved: While Mixture of Functions shows some success without variable names, the authors acknowledge it still uses specific prompt formats and question whether OOCR would work with truly unstructured data.
- What evidence would resolve it: Experiments training on completely unstructured text (e.g., Wikipedia articles or web text) containing implicit information about latent variables, then testing zero-shot ability to verbalize and use that information.

### Open Question 3
- Question: What is the mechanistic basis for how models learn and represent latent values during OOCR?
- Basis in paper: [inferred] The authors conclude by suggesting it would be "valuable to study inductive OOCR mechanistically to see how models learn and represent latent values," but they don't provide such analysis.
- Why unresolved: The paper focuses on behavioral evaluation but doesn't investigate internal representations or learning dynamics during finetuning.
- What evidence would resolve it: Mechanistic interpretability studies analyzing hidden representations during OOCR training, such as probing for latent variables in activation space or tracking weight changes that encode the latent information.

## Limitations

- OOCR performance is unreliable, especially for smaller models learning complex structures
- Current OOCR abilities may not pose immediate safety risks despite theoretical concerns
- Evaluation relies heavily on prompting models to verbalize latent information, which may not be the most natural demonstration of understanding

## Confidence

- **High confidence**: GPT-4 outperforms GPT-3.5 on OOCR tasks (multiple experiments show consistent scaling effects)
- **Medium confidence**: Finetuning enables better OOCR than in-context learning (results show advantage but with substantial variance)
- **Low confidence**: OOCR poses emerging safety concerns (authors state this is speculative and current capabilities are limited)

## Next Checks

1. **Cross-lingual generalization**: Test whether models trained on English OOCR tasks can perform equivalent tasks in other languages, or if the capability is language-specific.

2. **Adversarial evaluation**: Design evaluation prompts that specifically target the verbalization requirement to determine if models understand latent structures without needing to explicitly name them.

3. **Robustness analysis**: Conduct systematic experiments varying the number of training examples and complexity of latent structures to identify precise failure boundaries for OOCR capabilities.