---
ver: rpa2
title: Unveiling Markov Heads in Pretrained Language Models for Offline Reinforcement
  Learning
arxiv_id: '2409.06985'
source_url: https://arxiv.org/abs/2409.06985
tags:
- markov
- heads
- attention
- learning
- head
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the transferability of pretrained language
  models (PLMs) to offline reinforcement learning by analyzing attention heads in
  Decision Transformers. The authors identify "Markov heads" - a specific type of
  attention head that focuses on the most recent input token and exists in PLMs.
---

# Unveiling Markov Heads in Pretrained Language Models for Offline Reinforcement Learning

## Quick Facts
- arXiv ID: 2409.06985
- Source URL: https://arxiv.org/abs/2409.06985
- Authors: Wenhao Zhao; Qiushui Xu; Linjie Xu; Lei Song; Jinyu Wang; Chunlai Zhou; Jiang Bian
- Reference count: 24
- Primary result: Proposed GPT2-DTMA with Mixture of Attention mechanism outperforms standard Decision Transformers in long-term offline RL tasks while maintaining performance in short-term tasks

## Executive Summary
This paper investigates the transferability of pretrained language models (PLMs) to offline reinforcement learning by analyzing attention heads in Decision Transformers. The authors identify "Markov heads" - a specific type of attention head that focuses on the most recent input token and exists in PLMs. They prove theoretically that these Markov heads cannot be eliminated through fine-tuning and are beneficial in short-term RL environments but detrimental in long-term ones. To address this limitation, they propose GPT2-DTMA, which incorporates a Mixture of Attention mechanism to adaptively weight different attention heads based on environment requirements. Experiments on MuJoCo locomotion tasks (short-term) and PointMaze (long-term) show that GPT2-DTMA achieves comparable performance in short-term environments while significantly narrowing the performance gap in long-term environments compared to baselines.

## Method Summary
The authors analyze the attention mechanism in Decision Transformers by computing the Markov matrix, which measures attention heads' reliance on the most recent token. They prove theoretically that Markov heads cannot be eliminated through fine-tuning and identify these heads as the key factor explaining why GPT2-DT performs well in short-term but poorly in long-term environments. To address this limitation, they propose GPT2-DTMA with a Mixture of Attention (MoA) mechanism that adaptively weights different attention heads based on environment requirements. The MoA mechanism uses a gating network to compute attention weights that vary depending on whether the environment requires short-term or long-term reasoning. This allows the model to leverage Markov heads when beneficial (short-term tasks) while compensating for their limitations in long-term scenarios.

## Key Results
- Markov heads exist in pretrained language models and cannot be eliminated through fine-tuning
- GPT2-DT (standard Decision Transformer with GPT2 backbone) performs well on short-term MuJoCo tasks but poorly on long-term PointMaze tasks
- GPT2-DTMA with Mixture of Attention mechanism achieves comparable performance to GPT2-DT on short-term tasks while significantly improving performance on long-term PointMaze tasks
- The Mixture of Attention mechanism successfully adapts attention weights based on environment requirements

## Why This Works (Mechanism)
The paper demonstrates that Markov heads in PLMs create a fundamental limitation when transferred to long-term RL tasks. These heads inherently focus on the most recent token due to the next-token prediction objective during pretraining, which works well for short-term dependencies but fails to capture long-term temporal patterns required in extended decision-making scenarios. The Mixture of Attention mechanism works by learning to dynamically adjust the contribution of different attention heads based on the specific requirements of the environment. For short-term tasks, it can emphasize Markov heads that capture immediate dependencies, while for long-term tasks, it can de-emphasize these heads and rely more on other attention patterns that capture longer temporal relationships.

## Foundational Learning

**Attention mechanism in transformers** - Why needed: Core component of the proposed method and existing Decision Transformer architecture. Quick check: Understanding how self-attention computes weighted combinations of token representations.

**Offline reinforcement learning** - Why needed: The paper's application domain and evaluation framework. Quick check: Distinguishing between online and offline RL, and understanding the sequential decision-making problem formulation.

**Markov property** - Why needed: Central to understanding Markov heads and their theoretical properties. Quick check: Recognizing that Markov heads satisfy the Markov property by focusing only on the most recent state.

**Mixture of experts** - Why needed: Conceptual foundation for the Mixture of Attention mechanism. Quick check: Understanding how gating networks can dynamically route inputs to different expert components.

## Architecture Onboarding

**Component map:** Input tokens -> Embedding layer -> Multiple attention heads (including Markov heads) -> Gating network (MoA) -> Weighted attention combination -> Output prediction

**Critical path:** Token embedding → Attention computation (with multiple heads) → Gating network → Weighted attention combination → Value prediction

**Design tradeoffs:** The paper trades model complexity (adding MoA mechanism) for improved generalization across different time horizons. This increases computational overhead but provides better adaptability to varying environment requirements.

**Failure signatures:** If Markov heads dominate attention weights in long-term environments, the model will fail to capture necessary long-term dependencies. Conversely, if non-Markov heads dominate in short-term environments, the model may miss immediate reward signals.

**First experiments:** 1) Verify Markov head identification by computing Markov matrices on pretrained models. 2) Test baseline Decision Transformer performance on both short-term and long-term tasks to confirm the performance gap. 3) Evaluate the gating network's ability to correctly assign attention weights based on environment type.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can we design pretraining objectives specifically tailored for reinforcement learning tasks that would naturally encourage beneficial attention patterns without requiring the MoA mechanism?
- Basis in paper: [explicit] The authors note that GPT2-D performs well in short-term environments but struggles in long-term environments, and they propose MoA to bridge this gap. They also mention that other PLMs like CLIP and GPT-J don't exhibit the Markov matrix structure that makes GPT2-DT effective.
- Why unresolved: While the paper identifies the Markov head structure as beneficial for short-term tasks, it doesn't explore whether alternative pretraining objectives could produce attention heads better suited for both short-term and long-term RL environments without requiring the MoA mechanism.
- What evidence would resolve it: Experiments comparing various pretraining objectives (beyond next-token prediction) on their ability to produce attention heads that perform well across both short-term and long-term RL tasks, potentially eliminating the need for MoA.

### Open Question 2
- Question: How does the

## Limitations
- Theoretical proof of Markov heads being unremovable relies on assumptions about training dynamics that may not hold in all scenarios
- Evaluation limited to specific MuJoCo and PointMaze environments, which may not represent the full spectrum of RL challenges
- The short-term versus long-term dichotomy may be oversimplified and not capture the nuanced requirements of different RL tasks
- Mixture of Attention mechanism introduces additional hyperparameters that could affect performance in ways not fully characterized

## Confidence
- High confidence: The empirical observation that Markov heads exist in PLMs and their general characteristics. The experimental results showing GPT2-DTMA's performance on the tested MuJoCo and PointMaze tasks.
- Medium confidence: The theoretical claims about Markov heads being unremovable through fine-tuning and their systematic benefits/drawbacks across different environment types. The proposed solution's effectiveness beyond the specific test environments.
- Low confidence: The universality of the short-term versus long-term environment dichotomy and the absolute characterization of Markov heads as either beneficial or detrimental.

## Next Checks
1. Test GPT2-DTMA on a broader range of RL environments with varying time horizons to validate the claimed generalization of the short-term/long-term dichotomy.

2. Conduct ablation studies on the Mixture of Attention mechanism to quantify the contribution of different components and understand the sensitivity to hyperparameter choices.

3. Investigate whether alternative architectures or training approaches could mitigate the limitations of Markov heads without requiring the proposed complex modification.