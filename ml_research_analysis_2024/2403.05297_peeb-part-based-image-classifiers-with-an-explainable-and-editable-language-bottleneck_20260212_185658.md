---
ver: rpa2
title: 'PEEB: Part-based Image Classifiers with an Explainable and Editable Language
  Bottleneck'
arxiv_id: '2403.05297'
source_url: https://arxiv.org/abs/2403.05297
tags:
- peeb
- descriptors
- image
- part
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PEEB is a part-based, explainable image classifier that leverages
  CLIP encoders to match visual object parts with editable textual descriptors, enabling
  zero-shot classification without relying on class names. It significantly outperforms
  CLIP-based classifiers in both zero-shot and supervised settings, achieving state-of-the-art
  accuracy on fine-grained datasets like CUB-200 (88.80%) and Stanford Dogs-120 (92.20%).
---

# PEEB: Part-based Image Classifiers with an Explainable and Editable Language Bottleneck

## Quick Facts
- arXiv ID: 2403.05297
- Source URL: https://arxiv.org/abs/2403.05297
- Reference count: 40
- Primary result: Achieves 88.80% accuracy on CUB-200 and 92.20% on Stanford Dogs-120 using part-based, explainable image classification

## Executive Summary
PEEB introduces a novel part-based image classifier that leverages CLIP encoders to match visual object parts with editable textual descriptors. The method enables zero-shot classification without relying on class names by grounding textual descriptors to detected visual parts. PEEB significantly outperforms CLIP-based classifiers on fine-grained datasets like CUB-200 and Stanford Dogs-120 while providing interpretability through part-level explanations. The model's unique capability to edit text descriptors and define new classes without retraining sets it apart from existing approaches.

## Method Summary
PEEB uses a two-stage pre-training approach: first on a large-scale Bird-11K dataset (294K images, 10.8K species) using contrastive learning with symmetric cross-entropy loss, then finetuning on downstream datasets. The model employs OWL-ViT for part detection, a frozen CLIP text encoder for descriptor matching, and an editable text bottleneck that allows users to modify descriptors without retraining. Visual parts are detected and embedded, then matched to textual descriptors via dot product similarity for classification.

## Key Results
- Achieves 88.80% top-1 accuracy on CUB-200, on par with the best specialized classifiers
- Scores 92.20% accuracy on Stanford Dogs-120, surpassing previous state-of-the-art methods
- Enables zero-shot classification by matching visual parts to textual descriptors without requiring class names

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Part-based descriptors allow classification without relying on class names, enabling zero-shot classification for rare or unseen classes.
- Mechanism: The model encodes both visual parts of the image and textual descriptors of each class into embeddings, then computes dot products between matched parts and descriptors to generate logits for classification.
- Core assumption: Visual parts can be reliably detected and matched to corresponding textual descriptors for classification.
- Evidence anchors:
  - [abstract] "PEEB outperforms CLIP-based classifiers in both zero-shot and supervised settings, achieving state-of-the-art accuracy..."
  - [section] "PEEB classifies images by grounding the textual descriptor of object parts provided by humans or GPT-4 to detected parts in the image."
  - [corpus] Weak corpus support: No direct citations for part-based matching; relies on CLIP and OWL-ViT literature.
- Break condition: If part detection fails or descriptors do not accurately represent visual parts, classification accuracy will degrade significantly.

### Mechanism 2
- Claim: The text encoder remains frozen during training to preserve generalizability to unseen descriptors.
- Mechanism: By freezing the text encoder, the model retains its ability to understand new textual descriptors without retraining, allowing users to define new classes at test time.
- Core assumption: The pre-trained text encoder has sufficient linguistic coverage to interpret novel descriptors.
- Evidence anchors:
  - [abstract] "PEEB is also the first to enable users to edit the text descriptors to form a new classifier without any re-training."
  - [section] "We always keep the text encoder frozen since we want to preserve its generalizability to the descriptors of unseen objects."
  - [corpus] Weak corpus support: No explicit citations on frozen text encoder benefits in vision-language models.
- Break condition: If the text encoder lacks understanding of domain-specific descriptors (e.g., bird parts), classification will fail for new classes.

### Mechanism 3
- Claim: Pre-training on a large-scale dataset (Bird-11K) significantly improves classification accuracy on downstream tasks.
- Mechanism: The model learns general visual part representations from millions of images across thousands of species, enabling better generalization to unseen classes.
- Core assumption: Large-scale contrastive learning on diverse data improves part detection and matching.
- Evidence anchors:
  - [abstract] "PEEB scores an 88.80% top-1 accuracy, on par with the best CUB-200 classifiers..."
  - [section] "Compared to part-based classifiers, PEEB not only achieves state-of-the-art (SOTA) accuracy in the supervised-learning setting..."
  - [corpus] No direct citations for Bird-11K; relies on general large-scale pre-training literature.
- Break condition: If the pre-training dataset is too noisy or imbalanced, downstream performance may not improve.

## Foundational Learning

- Concept: Visual part detection and embedding
  - Why needed here: PEEB relies on detecting and embedding visual parts of objects to match with textual descriptors.
  - Quick check question: Can you explain how OWL-ViT detects parts in an image and generates embeddings for them?

- Concept: Text embedding and descriptor matching
  - Why needed here: PEEB matches textual descriptors to visual part embeddings via dot product similarity.
  - Quick check question: How does the model compute similarity between a visual part embedding and a textual descriptor?

- Concept: Contrastive learning objectives
  - Why needed here: PEEB uses symmetric cross-entropy loss to align visual parts with textual descriptors during pre-training.
  - Quick check question: What is the difference between symmetric cross-entropy loss and classification cross-entropy loss?

## Architecture Onboarding

- Component map: Image → Part selection → Part MLP → Descriptor matching → Classification
- Critical path: Image → Part selection → Part MLP → Descriptor matching → Classification
- Design tradeoffs:
  - Freezing text encoder preserves generalizability but limits fine-tuning.
  - Part-level matching improves interpretability but requires reliable part detection.
  - Large pre-training dataset improves accuracy but increases computational cost.
- Failure signatures:
  - Low classification accuracy → Poor part detection or descriptor matching.
  - Inconsistent predictions → Noisy descriptors or imbalanced training data.
  - Slow convergence → Insufficient pre-training data or suboptimal hyperparameters.
- First 3 experiments:
  1. Test part detection accuracy on a small subset of Bird-11K.
  2. Evaluate descriptor matching quality by manually inspecting top-1 predictions.
  3. Measure classification accuracy on CUB-200 with and without pre-training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the accuracy of PEEB vary when using different types of part-based descriptors (e.g., manually curated vs. GPT-4 generated)?
- Basis in paper: [explicit] The paper mentions that GPT-4 generated descriptors have an average error rate of 45% and that revising descriptors can improve accuracy by around +10.8 points.
- Why unresolved: The paper does not provide a direct comparison of accuracy between manually curated and GPT-4 generated descriptors.
- What evidence would resolve it: A controlled experiment comparing the accuracy of PEEB using manually curated descriptors versus GPT-4 generated descriptors on a standard dataset like CUB-200.

### Open Question 2
- Question: Can PEEB's performance be further improved by incorporating additional training data or by using a larger pre-trained model like OWL-ViT Large instead of OWL-ViT B/32?
- Basis in paper: [inferred] The paper mentions that the accuracy of PEEB stabilizes around 60% when the image count approaches 250K and that pre-training from OWL-ViTB/16 results in a further gain of +2.07 points compared to OWL-ViTB/32.
- Why unresolved: The paper does not explore the potential benefits of using a larger pre-trained model or incorporating additional training data beyond Bird-11K.
- What evidence would resolve it: Experiments comparing the accuracy of PEEB using different pre-trained models (e.g., OWL-ViT Large) and varying amounts of training data on a standard dataset like CUB-200.

### Open Question 3
- Question: How does PEEB perform on other fine-grained classification tasks beyond birds and dogs, such as plant species identification or car model recognition?
- Basis in paper: [explicit] The paper mentions that PEEB is applicable to multiple domains and demonstrates good performance on Stanford Dogs-120.
- Why unresolved: The paper only evaluates PEEB on bird and dog datasets, leaving the question of its performance on other fine-grained classification tasks unanswered.
- What evidence would resolve it: Experiments evaluating PEEB's performance on other fine-grained classification datasets, such as plant species identification or car model recognition, and comparing the results to state-of-the-art methods.

## Limitations
- Reliance on OWL-ViT for part detection introduces critical dependency on accurate part localization
- Performance evaluation limited to bird and dog datasets, lacking generalization testing on other domains
- Computational overhead of pre-training on Bird-11K (294K images) may limit accessibility for researchers with constrained resources

## Confidence

- **High Confidence**: The core mechanism of matching visual parts to textual descriptors (Mechanism 1) is well-supported by the experimental results showing state-of-the-art accuracy on CUB-200 and Stanford Dogs-120. The freezing of the text encoder (Mechanism 2) is consistently implemented and demonstrated through the ability to edit descriptors without retraining.

- **Medium Confidence**: The claim about large-scale pre-training benefits (Mechanism 3) is supported by performance improvements but lacks direct ablation studies comparing different pre-training dataset sizes. The generalizability of part-based descriptors to rare classes (Mechanism 1) is theoretically sound but not extensively validated on truly zero-shot scenarios.

- **Low Confidence**: The paper doesn't provide sufficient evidence for the robustness of descriptor generation across different GPT-4 prompts or the model's performance on datasets with significant background clutter or occlusion.

## Next Checks

1. **Ablation Study on Part Detection**: Evaluate PEEB's performance with and without part-level supervision on a held-out test set to quantify the contribution of part-based matching versus holistic image classification.

2. **Descriptor Robustness Testing**: Generate descriptors using multiple prompt variations and human annotators, then measure classification consistency across different descriptor sets for the same visual parts.

3. **Cross-Domain Generalization**: Test PEEB on a non-fine-grained dataset like ImageNet-1K or COCO to assess whether the part-based approach generalizes beyond species-level classification to more general object categories.