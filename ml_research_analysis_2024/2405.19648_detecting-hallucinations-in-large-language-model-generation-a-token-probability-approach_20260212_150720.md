---
ver: rpa2
title: 'Detecting Hallucinations in Large Language Model Generation: A Token Probability
  Approach'
arxiv_id: '2405.19648'
source_url: https://arxiv.org/abs/2405.19648
tags:
- features
- language
- llms
- llme
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of detecting hallucinations in
  text generated by large language models (LLMs). The proposed method uses a supervised
  learning approach with two simple classifiers (logistic regression and a simple
  neural network) that rely on four numerical features derived from token and vocabulary
  probabilities obtained from other LLM evaluators.
---

# Detecting Hallucinations in Large Language Model Generation: A Token Probability Approach

## Quick Facts
- arXiv ID: 2405.19648
- Source URL: https://arxiv.org/abs/2405.19648
- Reference count: 40
- Primary result: Achieves over 98% accuracy and F1 score on summarization and QA tasks using simple classifiers with token probability features

## Executive Summary
This paper introduces a supervised learning approach for detecting hallucinations in text generated by large language models (LLMs). The method uses two simple classifiers (Logistic Regression and a Simple Neural Network) that rely on four numerical features derived from token and vocabulary probabilities obtained from other LLM evaluators. The approach achieves state-of-the-art performance across multiple tasks and benchmarks, with particular success on summarization and question answering tasks where it surpasses 98% accuracy and F1 score.

## Method Summary
The proposed method employs a supervised learning framework where text generated by an LLM (LLMG) is evaluated by a different LLM (LLME) to extract four numerical features: minimum token probability, average token probability, maximum probability deviation, and minimum probability spread. These features are then used to train simple classifiers (Logistic Regression and a Simple Neural Network) to distinguish between hallucinated and non-hallucinated text. The approach is evaluated on three datasets: HaluEval, HELM, and True-False, covering various tasks including summarization, question answering, knowledge-grounded dialogue, and general user queries.

## Key Results
- Achieves over 98% accuracy and F1 score on summarization and question answering tasks
- Surpasses state-of-the-art outcomes across multiple benchmarks and tasks
- Demonstrates that different LLM evaluators can improve performance through diversity in training data
- Shows the four numerical features are effective indicators of hallucinations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Token probabilities from a different LLM (LLME) can serve as reliable indicators of hallucinations in text generated by LLMG.
- **Mechanism:** When LLMG generates text, it may produce tokens with low probabilities due to the model being forced to generate tokens outside its typical confidence range. By evaluating these generated tokens using a different LLM (LLME), the token probabilities and vocabulary distributions can reveal patterns indicative of hallucinations.
- **Core assumption:** Different LLMs, trained on diverse data, will assign different probability distributions to the same tokens, and these differences can be exploited to detect hallucinations.
- **Evidence anchors:**
  - [abstract] "This paper introduces a supervised learning approach employing two simple classifiers utilizing only four numerical features derived from tokens and vocabulary probabilities obtained from other LLM evaluators, which are not necessarily the same."
  - [section] "The belief is that probabilities from a different model, varying in architecture, size, parameters, context length, and training data, can also serve as reliable indicators of hallucinations in the text generated by LLMG."
  - [corpus] Weak evidence - related papers focus on token probabilities but do not explicitly validate using a different LLM as evaluator.
- **Break condition:** If the training data of LLME overlaps significantly with LLMG, the probability distributions may not differ enough to be useful indicators.

### Mechanism 2
- **Claim:** Four specific numerical features derived from token and vocabulary probabilities are effective for detecting hallucinations.
- **Mechanism:** The four features—Minimum Token Probability (mtp), Average Token Probability (avgtp), Maximum LLME Probability Deviation (Mpd), and Minimum LLME Probability Spread (mps)—capture different aspects of the token probability distributions that correlate with hallucinations. Low mtp and avgtp suggest the generated text contains uncertain tokens, while Mpd and mps measure the deviation and spread of probabilities, indicating potential hallucinations.
- **Core assumption:** There is a correlation between low token probabilities and hallucinations, as suggested by previous research [13, 10, 15].
- **Evidence anchors:**
  - [abstract] "The method yields promising results, surpassing state-of-the-art outcomes in multiple tasks across three different benchmarks."
  - [section] "Lee et al. [13] proposes that a reliable indicator of hallucination during GPT model generation is the low probability of a token being generated."
  - [corpus] Weak evidence - related papers discuss token probabilities but do not validate the specific four features used here.
- **Break condition:** If the relationship between token probabilities and hallucinations does not hold for a particular LLM or task, the features may not be effective.

### Mechanism 3
- **Claim:** Using a different LLM as an evaluator can capture linguistic patterns and styles that the generating LLM might miss, improving hallucination detection.
- **Mechanism:** Different LLMs are trained on varied datasets, capturing diverse linguistic patterns. By using an LLME different from LLMG, the evaluation process benefits from this diversity, potentially identifying hallucinations that the generating LLM might not recognize due to its inherent biases or limitations.
- **Core assumption:** The diversity in training data among different LLMs allows them to capture various linguistic patterns and styles, enhancing the robustness of hallucination detection.
- **Evidence anchors:**
  - [abstract] "Additionally, we provide a comprehensive examination of the strengths and weaknesses of our approach, highlighting the significance of the features utilized and the LLM employed as an evaluator."
  - [section] "Using different LLMs as evaluators takes advantage of the diversity of training data among different language models (LLMs) and captures various linguistic patterns and styles."
  - [corpus] Weak evidence - related papers focus on using token probabilities but do not explicitly discuss using different LLMs as evaluators.
- **Break condition:** If the LLME is too dissimilar from LLMG in terms of language or domain, it may not accurately evaluate the generated text.

## Foundational Learning

- **Concept:** Understanding token probabilities and vocabulary distributions in LLMs.
  - **Why needed here:** The method relies on extracting numerical features from token and vocabulary probabilities obtained from an LLM evaluator.
  - **Quick check question:** What is the difference between token probability and vocabulary probability in the context of LLM evaluation?

- **Concept:** Supervised learning with simple classifiers.
  - **Why needed here:** The approach uses Logistic Regression and a Simple Neural Network to classify text as hallucinated or not based on the extracted features.
  - **Quick check question:** How does a Logistic Regression classifier differ from a Simple Neural Network in handling binary classification tasks?

- **Concept:** Feature engineering and importance analysis.
  - **Why needed here:** The method involves selecting and evaluating four numerical features to determine their effectiveness in detecting hallucinations.
  - **Quick check question:** Why is it important to analyze the importance of features in a machine learning model, and how can this be done?

## Architecture Onboarding

- **Component map:** LLMG -> LLME -> Feature Extractor -> Classifiers (LR and SNN)
- **Critical path:**
  1. LLMG generates text based on a condition
  2. LLME evaluates the generated text, providing token and vocabulary probabilities
  3. Feature Extractor computes the four numerical features
  4. Classifiers use the features to determine if the text is hallucinated
- **Design tradeoffs:**
  - Using a different LLME adds complexity but can improve detection by leveraging diverse training data
  - Simple classifiers are fast and interpretable but may not capture complex relationships as well as more advanced models
  - The four features are easy to compute but may not capture all nuances of hallucinations
- **Failure signatures:**
  - Poor performance on tasks where the features are not meaningful indicators of hallucinations
  - Overfitting to the training data, especially if the test set is not diverse enough
  - Degradation in performance if the LLME is too dissimilar from LLMG in language or domain
- **First 3 experiments:**
  1. Test the classifiers on a small, controlled dataset to verify they can distinguish between hallucinated and non-hallucinated text
  2. Evaluate the impact of using different LLME models on detection performance to identify the most effective evaluators
  3. Perform an ablation study by removing each feature to determine their individual contributions to the model's performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance of the proposed method depend on the specific distribution of hallucinations in the training data, or can it generalize to arbitrary distributions of hallucinated content?
- Basis in paper: [inferred] The paper shows strong performance on the HaluEval benchmark, but notes that using only the generated text (without condition text) yields surprisingly high accuracy, suggesting potential overfitting to specific patterns in the dataset.
- Why unresolved: The paper does not systematically vary the distribution of hallucinations in the training data to test generalization. The observed anomaly with generated text only suggests potential dependence on dataset characteristics.
- What evidence would resolve it: Experiments showing consistent performance across datasets with different hallucination distributions, and/or ablation studies where hallucination rates in training data are varied.

### Open Question 2
- Question: How does the proposed method's performance scale with the size of the LLM evaluator? Is there a point of diminishing returns, or can smaller models outperform larger ones in specific scenarios?
- Basis in paper: [explicit] The paper tests multiple LLM evaluators of varying sizes and finds that smaller models like BART sometimes outperform larger ones like GPT-J, particularly in the QA task.
- Why unresolved: While the paper provides evidence that smaller models can be competitive, it doesn't systematically explore the relationship between model size and performance across a wider range of tasks and datasets.
- What evidence would resolve it: A comprehensive study varying LLM evaluator sizes across multiple tasks and datasets, measuring performance to identify trends and potential diminishing returns.

### Open Question 3
- Question: Can the proposed method be extended to detect hallucinations with varying degrees of severity, rather than just binary classification?
- Basis in paper: [inferred] The paper acknowledges that real-world hallucinations might be more nuanced, with varying degrees of severity, which the current binary classification approach might not capture.
- Why unresolved: The paper focuses on binary classification and does not explore methods for quantifying the severity of hallucinations.
- What evidence would resolve it: Development and evaluation of a regression-based or multi-class extension of the method that can predict the degree of hallucination severity, validated on datasets with graded hallucination annotations.

## Limitations

- **Dataset representation and task specificity**: The strong performance on HaluEval may not generalize to other hallucination types or domains, particularly given weaker results on HELM and True-False datasets
- **Feature effectiveness validation**: While empirically effective, the theoretical justification for why these specific four features are optimal remains underdeveloped
- **LLME selection sensitivity**: The selection criteria for choosing LLME models are not explicitly stated, and performance may degrade as LLME becomes too dissimilar from LLMG

## Confidence

**High Confidence Claims:**
- The four numerical features (mtp, avgtp, Mpd, mps) can be successfully extracted from LLM evaluators
- Logistic Regression and Simple Neural Network classifiers can be trained on these features
- The approach outperforms baseline methods on HaluEval tasks
- Feature importance can be analyzed through ablation studies

**Medium Confidence Claims:**
- The approach achieves state-of-the-art results across multiple benchmarks
- Different LLM evaluators can improve performance through diversity
- The method is computationally efficient compared to alternatives
- The four features capture meaningful patterns for hallucination detection

**Low Confidence Claims:**
- The approach generalizes well to all hallucination types and domains
- Any LLM can serve as an effective evaluator regardless of its characteristics
- The 98%+ accuracy on HaluEval represents real-world performance
- The relationship between token probabilities and hallucinations is fully understood

## Next Checks

1. **Cross-Domain Generalization Test**: Evaluate the trained classifiers on datasets from completely different domains (e.g., medical, legal, technical documentation) not represented in the original training data. This would test whether the high accuracy on HaluEval's general queries translates to specialized domains where hallucination detection is most critical.

2. **LLME Diversity and Similarity Analysis**: Systematically vary the similarity between LLMG and LLME (using models with different architectures, training data overlap, and parameter counts) to determine the optimal balance between diversity and compatibility. Measure performance degradation as LLME becomes either too similar (overlapping training data) or too dissimilar (different language or domain).

3. **Real-Time Performance Benchmarking**: Measure the actual computational costs including feature extraction time, classifier inference latency, and memory usage for different LLM evaluator combinations. Compare these metrics against the claimed efficiency improvements and test whether the approach can run in real-time on consumer hardware for practical applications.