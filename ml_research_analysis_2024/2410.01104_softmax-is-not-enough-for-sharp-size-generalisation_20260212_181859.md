---
ver: rpa2
title: Softmax is not Enough (for Sharp Size Generalisation)
arxiv_id: '2410.01104'
source_url: https://arxiv.org/abs/2410.01104
tags:
- softmax
- attention
- temperature
- size
- will
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a fundamental limitation of the softmax
  function in modern AI architectures: it cannot robustly maintain sharp attention
  across varying input sizes, leading to dispersion of attention coefficients as problem
  size increases. The authors prove that softmax coefficients must decay toward zero
  as the number of input items grows, making it impossible to reliably compute sharp
  functions like max in out-of-distribution settings.'
---

# Softmax is not Enough (for Sharp Size Generalisation)

## Quick Facts
- arXiv ID: 2410.01104
- Source URL: https://arxiv.org/abs/2410.01104
- Reference count: 40
- Primary result: Softmax cannot maintain sharp attention across varying input sizes, limiting size generalization in reasoning systems

## Executive Summary
This paper identifies a fundamental limitation of softmax-based attention mechanisms in modern AI architectures: as the number of input items increases, softmax coefficients must disperse toward zero, making it impossible to maintain sharp attention required for reasoning tasks. The authors prove that this dispersion is inevitable when logits are bounded, preventing models from reliably computing sharp functions like max in out-of-distribution settings. They propose an adaptive temperature mechanism that dynamically adjusts softmax temperature based on input entropy to improve sharpness at inference time, demonstrating improved performance on both synthetic max-finding tasks and the CLRS-Text algorithmic reasoning benchmark.

## Method Summary
The authors implement a single attention head architecture with Deep Sets-style input processing and train it on a max retrieval task using mixed-size batches (5-16 items) for 100,000 steps. They develop an adaptive temperature function using JAX that computes entropy of softmax outputs and applies a degree-4 polynomial fit to map entropy to temperature correction. The mechanism is evaluated across varying problem sizes (16-16384 items) and integrated into Gemma 2B for CLRS-Text benchmark fine-tuning. The approach aims to maintain sharp attention coefficients while avoiding the dispersion problem inherent to standard softmax normalization.

## Key Results
- Standard softmax coefficients decay toward zero as input size grows, preventing sharp reasoning
- Adaptive temperature improves max retrieval accuracy from 53.8% to 57.7% at 512 items
- CLRS-Text benchmark shows improved performance across most tasks with adaptive temperature
- Entropy-based temperature adjustment successfully maintains sharper attention distributions

## Why This Works (Mechanism)

### Mechanism 1
Softmax coefficients must disperse toward zero as input size increases because when normalizing over n items with bounded logits, each coefficient is constrained to be Θ(1/n) by the normalization sum. This follows from the mathematical constraint that softmax outputs must sum to 1, forcing individual coefficients to shrink as more items are added.

### Mechanism 2
Adaptive temperature can sharpen coefficients without changing learned weights by fitting a polynomial to optimal temperature values at different entropies. By increasing temperature when coefficients are too dispersed, the mechanism counteracts the natural tendency toward uniformity while preserving the model's learned representations.

### Mechanism 3
Dispersion prevents reasoning systems from generalizing sharp functions beyond training size because when coefficients disperse, weighted sums converge to averages, making it impossible to select specific items. Sharp reasoning requires maintaining focus on a constant number of items regardless of total input size.

## Foundational Learning

- Concept: Softmax normalization properties
  - Why needed here: Understanding why softmax coefficients must sum to 1 and how this constrains individual values
  - Quick check question: What happens to individual softmax coefficients when you double the number of items while keeping logits bounded?

- Concept: Temperature scaling in softmax
  - Why needed here: Temperature controls sharpness and understanding its relationship to entropy is crucial for adaptive temperature
  - Quick check question: How does decreasing temperature affect the entropy of the output distribution?

- Concept: Shannon entropy calculation
  - Why needed here: Entropy is used to determine when to adjust temperature in the adaptive mechanism
  - Quick check question: What is the entropy of a uniform distribution over n items?

## Architecture Onboarding

- Component map: Input → Query/Key computation → Logits → (Adaptive temperature) → Coefficients → Weighted sum → Output prediction

- Critical path: Input processing through MLPs → Dot-product attention with softmax → (Optional adaptive temperature adjustment) → Weighted sum → Output prediction

- Design tradeoffs:
  - Standard softmax: Simple, efficient, but suffers from dispersion
  - Adaptive temperature: Better sharpness, requires entropy computation, slightly more complex
  - Alternative mechanisms: May avoid dispersion but introduce other challenges

- Failure signatures:
  - Dispersion: Attention coefficients become uniform, performance degrades on larger inputs
  - Over-sharpening: Temperature too low, model becomes unstable or overfits
  - Incorrect entropy estimation: Temperature adjustments don't match actual needs

- First 3 experiments:
  1. Implement single attention head on max retrieval task with varying input sizes
  2. Add adaptive temperature and compare performance on out-of-distribution sizes
  3. Test adaptive temperature on CLRS-Text benchmark with Gemma 2B model

## Open Questions the Paper Calls Out

### Open Question 1
What is the theoretical limit of softmax's ability to maintain sharp attention as problem size approaches infinity, and can this limit be precisely characterized? The paper proves that softmax coefficients must decay toward zero as the number of input items grows, but does not provide a precise asymptotic characterization of this decay rate across different architectures and temperature settings.

### Open Question 2
How do alternative attention mechanisms like sigmoid, linear, or stick-breaking attention compare to softmax in maintaining sharp attention while avoiding the dispersion problem? The paper mentions these alternatives as potential solutions that don't have the dispersion issues, but notes they become substantially harder to meaningfully rank items using them.

### Open Question 3
How does the dispersion phenomenon affect different layers within deep Transformer architectures, and can we predict which layers will disperse first under varying conditions? The paper discusses how dispersion in one layer can accelerate dispersion in subsequent layers, but doesn't provide a predictive model for layer-wise dispersion patterns.

## Limitations

- The theoretical analysis relies on the assumption of bounded logits for all input sizes, which may not hold in practice for modern large-scale models.
- The adaptive temperature mechanism depends on fitting a polynomial to optimal temperature values, but the exact coefficients and their stability across different model architectures are not specified.
- The empirical evaluation focuses on a single model (Gemma 2B) and one benchmark (CLRS-Text), limiting generalizability to other architectures and reasoning tasks.

## Confidence

**High Confidence**: The fundamental mathematical constraint that softmax coefficients must decay toward zero as input size increases when logits are bounded.

**Medium Confidence**: The claim that dispersion prevents robust size generalization for sharp reasoning functions.

**Low Confidence**: The specific polynomial coefficients for the adaptive temperature function and their universal applicability.

## Next Checks

1. Test the adaptive temperature mechanism on different model families (BERT, GPT, Llama) to verify that the entropy-temperature relationship holds across architectures.

2. Compare softmax with adaptive temperature against alternative attention mechanisms like sparse attention, linear attention, or hierarchical attention across multiple tasks and problem sizes.

3. Conduct systematic experiments varying both model size and input size to identify whether the dispersion problem scales differently with model capacity.