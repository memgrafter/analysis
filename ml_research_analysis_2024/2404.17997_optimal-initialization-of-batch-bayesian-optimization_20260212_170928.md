---
ver: rpa2
title: Optimal Initialization of Batch Bayesian Optimization
arxiv_id: '2404.17997'
source_url: https://arxiv.org/abs/2404.17997
tags:
- sobol
- optimization
- batch
- function
- bayesian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Minimal Terminal Variance (MTV), a batch
  design acquisition function for Batch Bayesian Optimization (BBO) that optimizes
  initial and subsequent batches by minimizing the variance of post-evaluation estimates,
  weighted by the probability of optimality. Unlike typical random or quasi-random
  initialization, MTV uses an optimized batch design that adapts I-Optimality from
  experimental design.
---

# Optimal Initialization of Batch Bayesian Optimization

## Quick Facts
- arXiv ID: 2404.17997
- Source URL: https://arxiv.org/abs/2404.17997
- Authors: Jiuge Ren; David Sweet
- Reference count: 19
- One-line primary result: MTV acquisition function outperforms standard BBO methods across test functions and RL simulators

## Executive Summary
This paper introduces Minimal Terminal Variance (MTV), a batch design acquisition function for Batch Bayesian Optimization that optimizes both initial and subsequent batches by minimizing the variance of post-evaluation estimates, weighted by the probability of optimality. Unlike typical random or quasi-random initialization, MTV uses an optimized batch design that adapts I-Optimality from experimental design. The method employs MCMC sampling and conditional Gaussian processes for efficient batch generation.

## Method Summary
MTV is an acquisition function for Batch Bayesian Optimization that designs batches by minimizing the terminal (post-measurement) prediction variance, integrated over the parameter space and weighted by the probability that each point is the global optimum. The algorithm uses MCMC sampling from the posterior distribution of optimal points to generate candidate arms, then optimizes the MTV acquisition function using conditional Gaussian processes. The method works without requiring a prior initialization batch and adapts I-Optimality from traditional experimental design to the BBO setting.

## Key Results
- MTV outperforms standard BBO methods including Thompson Sampling and q-Simple Regret on test functions and reinforcement learning simulators
- Performance improvements are consistent across 1-30 dimensions and varying batch sizes
- MTV shows particular advantage in noisy environments and when careful initialization of the acquisition function optimizer is employed

## Why This Works (Mechanism)

### Mechanism 1
MTV improves batch optimization by minimizing post-measurement prediction variance weighted by the probability that each setting is optimal. The algorithm uses I-Optimality from experimental design, adapting it to Bayesian optimization by integrating the variance of the posterior GP over the parameter space, weighted by the posterior probability of each point being the global optimum. Core assumption: The surrogate model (GP) is well-calibrated and its posterior variance reflects epistemic uncertainty. Break condition: If the GP posterior variance is poorly calibrated or the probability of optimality is misestimated, the weighting becomes ineffective.

### Mechanism 2
MCMC sampling from p*(x) provides efficient and effective batch arm selection without requiring a prior initialization batch. Algorithm 1 implements a Metropolis-Hastings sampler with hit-and-run proposals to draw samples from the distribution of optimal points, enabling arm selection directly from the GP posterior without pre-collected measurements. Core assumption: The MCMC sampler can approximate p*(x) efficiently even with a single joint sample per iteration. Break condition: If the dimensionality is too high or the posterior is too peaked, MCMC may fail to explore the space adequately.

### Mechanism 3
Careful initialization of the acquisition function optimizer significantly improves convergence to better solutions. The acquisition function optimizer is initialized at candidate arms drawn from the MCMC samples (xi ~ p*(x)) rather than random points, leading to faster convergence and better minima. Core assumption: Initializing at high-probability optimal points reduces the search space for the optimizer. Break condition: If the MCMC samples are poor or the optimizer is robust to initialization, this advantage may diminish.

## Foundational Learning

- **Gaussian Process regression and its properties (mean, variance, conditioning)**: MTV relies on GP posterior variance and conditioning to evaluate the acquisition function without real measurements. Quick check: How does conditioning a GP on a set of points update the mean and variance at other points?
- **Bayesian experimental design (I-Optimality, A-Optimality)**: MTV is an adaptation of I-Optimality, so understanding design criteria is essential. Quick check: What is the difference between I-Optimality and A-Optimality in experimental design?
- **Markov Chain Monte Carlo sampling and Metropolis-Hastings algorithm**: Algorithm 1 uses MCMC to sample from the posterior distribution of optimal points. Quick check: What is the role of the acceptance probability in Metropolis-Hastings?

## Architecture Onboarding

- **Component map**: GP surrogate (mean and variance estimation) -> MCMC sampler for p*(x) (Algorithm 1) -> Acquisition function evaluator (MTV approximation) -> Acquisition function optimizer (L-BFGS-B via BoTorch) -> Data conditioning/fantasizing module (conditional GP)
- **Critical path**: 1) Sample xi ~ p*(x) via MCMC 2) Initialize optimizer with candidate arms from xi 3) For each optimizer iteration: - Fantasize GP conditioned on proposed arms - Compute MTV ≈ Σᵢ σ²(xi|xa) - Update arms if MTV decreases 4) Return final batch of arms
- **Design tradeoffs**: Using n=1 in MCMC for speed vs. accuracy; Approximating the integral vs. exact computation; Joint optimization of all arms vs. greedy selection
- **Failure signatures**: Poor batch quality: optimizer stuck in local minima; High variance in results: MCMC not mixing well; Slow convergence: poor initialization of optimizer
- **First 3 experiments**: 1) Test MTV on a 1D synthetic function (e.g., Branin) with small batch size 2) Compare initialization methods: random vs. MCMC samples for optimizer 3) Validate MCMC sampler: check that samples match p*(x) on a simple GP

## Open Questions the Paper Calls Out

### Open Question 1
How does MTV scale with very high-dimensional problems (e.g., d > 30), where the computational cost of MCMC sampling and GP conditioning may become prohibitive? Basis: The paper reports experiments up to 30 dimensions but notes that careful initialization becomes more important in higher dimensions. Why unresolved: The authors do not report results or analysis for dimensions significantly larger than 30. What evidence would resolve it: Empirical results showing MTV's performance and computational cost on problems with d > 50.

### Open Question 2
How robust is MTV to different choices of the GP kernel and hyperparameters, particularly in initialization rounds where no prior data is available? Basis: The authors note that MTV adapts an I-Optimality criterion from traditional experimental design to the BBO setting by replacing the polynomial surrogate with a GP. Why unresolved: The paper does not systematically explore the sensitivity of MTV to different GP specifications. What evidence would resolve it: Comparative experiments using different kernel types and hyperparameter settings.

### Open Question 3
Can MTV be effectively combined with non-stationary or heteroscedastic noise models to handle environments with varying noise levels across the parameter space? Basis: The authors demonstrate MTV on noisy reinforcement learning simulators but do not explore models beyond standard homoscedastic Gaussian processes. Why unresolved: The paper does not investigate whether MTV's advantages extend to more complex noise models. What evidence would resolve it: Experiments comparing MTV with both standard GPs and more sophisticated models on problems with known heteroscedastic noise characteristics.

## Limitations
- Limited experimental validation on high-dimensional problems beyond 30 dimensions
- Unclear computational scaling properties for very large batch sizes or high dimensions
- Potential sensitivity to GP kernel choice and hyperparameter settings not thoroughly explored

## Confidence

- **High confidence**: The mathematical formulation of MTV as weighted variance minimization is internally consistent and well-defined
- **Medium confidence**: Experimental results show clear improvements over baselines, but the test suite is relatively small
- **Low confidence**: The MCMC implementation details and their impact on sampler quality are underspecified

## Next Checks

1. Test MTV on additional high-dimensional benchmark functions (e.g., Hartmann 6D, Levy 10D) to assess scalability
2. Compare MCMC convergence rates across different proposal mechanisms and dimensionality regimes
3. Evaluate sensitivity to GP kernel choice and hyperparameter settings, which could significantly affect performance