---
ver: rpa2
title: 'Multi-OphthaLingua: A Multilingual Benchmark for Assessing and Debiasing LLM
  Ophthalmological QA in LMICs'
arxiv_id: '2412.14304'
source_url: https://arxiv.org/abs/2412.14304
tags:
- arxiv
- languages
- llms
- medical
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses multilingual bias in LLM performance for ophthalmology
  QA, particularly in LMIC languages. The authors create Multi-OphthaLingua, the first
  multilingual ophthalmology benchmark with paired questions across 7 languages (English,
  Spanish, Portuguese, Filipino, Mandarin, French, Hindi), totaling 1184 questions.
---

# Multi-OphthaLingua: A Multilingual Benchmark for Assessing and Debiasing LLM Ophthalmological QA in LMICs

## Quick Facts
- arXiv ID: 2412.14304
- Source URL: https://arxiv.org/abs/2412.14304
- Reference count: 11
- LLMs show 11-13% accuracy drops in Filipino and Hindi compared to English for ophthalmology QA

## Executive Summary
This study addresses multilingual bias in LLM performance for ophthalmology question-answering, particularly in low- and middle-income countries (LMICs). The authors create Multi-OphthaLingua, the first multilingual ophthalmology benchmark with paired questions across 7 languages (English, Spanish, Portuguese, Filipino, Mandarin, French, Hindi), totaling 1184 questions. Evaluating 6 popular LLMs reveals substantial performance gaps, with Filipino and Hindi showing accuracy drops of 11-13% compared to English. To address these disparities, they propose CLARA, a cross-lingual reflective agentic system combining translation, retrieval-augmented generation, and self-verification. CLARA significantly improves performance across all languages while reducing the multilingual bias gap, achieving up to 67.1% accuracy in Filipino compared to 51.8% with direct inference.

## Method Summary
The authors created Multi-OphthaLingua by curating 148 English ophthalmology questions and translating them into 6 other languages using professional services, ensuring semantic equivalence through iterative validation. They evaluated 6 popular LLMs (GPT-4, GPT-4o, Gemini Pro, Llama-3-70B, Mixtral-8x7B, Gemma-7B) using both multiple-choice and open-ended questions with majority voting for final answers. To address performance gaps, they developed CLARA, a multi-agent system that employs translation, evaluation, knowledge augmentation, and rewriting agents working in coordination. The system uses weighted and reweighted RAG approaches for targeted knowledge augmentation and includes self-verification mechanisms to improve answer quality across languages.

## Key Results
- LLM performance drops significantly for LMIC languages: Filipino and Hindi show 11-13% accuracy reduction compared to English
- CLARA reduces multilingual bias gap by improving performance across all languages, achieving up to 67.1% accuracy in Filipino (vs 51.8% with direct inference)
- The multilingual benchmark enables direct cross-lingual comparison by using paired questions across all languages, eliminating confounding factors from different question formulations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The multilingual benchmark enables direct cross-lingual comparison by using paired questions across all languages, eliminating confounding factors from different question formulations.
- Mechanism: Each question has an equivalent version in all seven languages, allowing researchers to isolate language effects from content differences when measuring LLM performance.
- Core assumption: The translated questions maintain semantic equivalence and difficulty level across languages.
- Evidence anchors:
  - [abstract] "manually curated questions parallel across languages, allowing for direct cross-lingual comparisons"
  - [section] "Most importantly, our dataset features paired questions across all languages, enabling direct cross-linguistic comparisons - a feature absent in most existing multilingual benchmarks"
- Break condition: If translation quality varies significantly or if cultural/regional nuances create unintended difficulty differences between language versions.

### Mechanism 2
- Claim: CLARA's multi-agent approach addresses both language limitations and domain knowledge gaps simultaneously through specialized components working in coordination.
- Mechanism: The system uses translation agents to standardize inputs, evaluation agents to assess uncertainty and quality, knowledge augmentation agents with corrective RAG for domain-specific content, and rewriting agents to handle complex information structures.
- Core assumption: The agent coordination can effectively identify when each component is needed and apply it appropriately.
- Evidence anchors:
  - [section] "CLARA employs a multi-agent approach, including translation, evaluation, knowledge augmentation, and rewriting agents"
  - [section] "Through this multi-agent, reflective process, CLARA aims to mitigate biases and enhance accuracy in multilingual ophthalmology QA"
- Break condition: If the evaluation agent cannot accurately assess when to trigger each specialized component, leading to either insufficient intervention or over-correction.

### Mechanism 3
- Claim: The weighted and reweighted RAG approaches in CLARA provide targeted knowledge augmentation that addresses both translation uncertainty and domain-specific terminology challenges.
- Mechanism: For translation-uncertain query parts, an additional relevance score is added to the retrieval process. For domain-specific jargon, terms are expanded using ophthalmology dictionaries and the retrieval is reweighted to prioritize documents containing these enriched concepts.
- Core assumption: The additional weighting schemes can effectively guide retrieval toward more relevant documents without overwhelming the system with noise.
- Evidence anchors:
  - [section] "We add an additional relevance score to the full-original-query score... For parts requiring additional context or containing domain-specific jargon, a reweighted RAG approach is applied"
  - [section] "Key ophthalmological concepts are enriched with relevant contextual information and formatted next to the concept in the prompt"
- Break condition: If the weighting schemes create retrieval bias that misses relevant information or if the expanded definitions introduce confusion rather than clarity.

## Foundational Learning

- Concept: Multilingual evaluation methodology
  - Why needed here: Understanding how to design benchmarks that enable fair comparison across languages requires knowledge of cross-lingual evaluation principles and the challenges of maintaining semantic equivalence across translations.
  - Quick check question: What are the key challenges in creating a multilingual benchmark that allows direct comparison between languages?

- Concept: Retrieval-Augmented Generation (RAG) systems
  - Why needed here: CLARA's effectiveness depends on understanding how RAG works, how to evaluate retrieval quality, and how to combine multiple retrieval strategies for different types of uncertainty.
  - Quick check question: How does RAG differ from traditional fine-tuning approaches for improving LLM domain knowledge?

- Concept: Agentic AI systems and multi-agent coordination
  - Why needed here: The CLARA system relies on multiple specialized agents working together, requiring understanding of how to design agent workflows, decision-making logic, and coordination mechanisms.
  - Quick check question: What are the key design considerations when building a multi-agent system for complex reasoning tasks?

## Architecture Onboarding

- Component map: Question → Translation → Initial Evaluation → (Knowledge Augmentation with RAG) → Iterative Evaluation → (Web Search if needed) → Question Rewriting (if needed) → Final Answer
- Critical path: Question → Translation → Initial Evaluation → (Knowledge Augmentation with RAG) → Iterative Evaluation → (Web Search if needed) → Question Rewriting (if needed) → Final Answer
- Design tradeoffs: The system trades computational complexity for improved accuracy and fairness. Each additional agent and iteration adds latency but potentially improves performance, particularly for lower-resource languages.
- Failure signatures: Performance degradation in specific languages despite CLARA application suggests evaluation agent miscalibration; consistent underperformance across all languages indicates retrieval corpus issues; failure to improve English performance suggests the approach isn't adding value for high-resource languages.
- First 3 experiments:
  1. Test each agent component in isolation with controlled inputs to verify individual functionality and identify bottlenecks
  2. Run ablation studies removing one component at a time to measure individual contribution to overall performance
  3. Compare CLARA's performance against baseline approaches (direct inference, pre-translation, basic RAG) across all languages to quantify improvement

## Open Questions the Paper Calls Out
None

## Limitations
- The benchmark's paired question design assumes semantic equivalence across translations, but cultural and linguistic nuances may create unintended difficulty variations
- The CLARA system shows promising improvements, but the evaluation framework relies on LLM-based judgment rather than expert human verification
- The knowledge augmentation component depends on publicly available ophthalmology resources, but the quality and comprehensiveness of these sources for low-resource languages remains unclear

## Confidence
**High confidence**: The existence of multilingual performance gaps in ophthalmology LLMs is well-established through direct comparative testing across seven languages with 1184 paired questions. The experimental methodology is sound and the statistical differences are substantial.

**Medium confidence**: The effectiveness of CLARA's multi-agent approach is demonstrated, but the evaluation relies on LLM-based assessment rather than expert human validation. The improvement metrics are robust, but the underlying reasons for improvement could include factors beyond the proposed mechanisms.

**Low confidence**: The generalizability of results to other medical domains or languages beyond the seven tested languages. The study doesn't explore whether similar approaches would work for other specialized knowledge domains or whether the specific combination of agents would transfer effectively.

## Next Checks
1. **Human expert validation**: Have domain experts independently verify CLARA's answers and the original LLM outputs across all languages to confirm that performance gaps reflect true knowledge differences rather than evaluation bias.

2. **Cross-domain transferability test**: Apply the CLARA framework to a different medical specialty (e.g., cardiology or dermatology) with similar multilingual benchmark construction to determine if the multi-agent approach generalizes beyond ophthalmology.

3. **Translation equivalence validation**: Conduct a controlled study where bilingual medical experts rate the semantic equivalence and difficulty level of translated questions across all language pairs to confirm that performance differences aren't artifacts of translation quality or question complexity.