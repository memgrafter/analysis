---
ver: rpa2
title: An Implementation of Werewolf Agent That does not Truly Trust LLMs
arxiv_id: '2409.01575'
source_url: https://arxiv.org/abs/2409.01575
tags:
- agent
- werewolf
- game
- utterance
- conversation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a werewolf agent that combines a Large Language
  Model (LLM) with a rule-based algorithm to address challenges in playing the incomplete
  information game Werewolf. The rule-based algorithm selects between LLM-generated
  outputs and predefined templates based on game situation analysis, enabling the
  agent to refute in critical situations and end conversations strategically.
---

# An Implementation of Werewolf Agent That does not Truly Trust LLMs

## Quick Facts
- arXiv ID: 2409.01575
- Source URL: https://arxiv.org/abs/2409.01575
- Authors: Takehiro Sato; Shintaro Ozaki; Daisaku Yokoyama
- Reference count: 14
- Primary result: Hybrid LLM-rule-based werewolf agent shows improved individuality (4.54 vs 2.52) and deceiving ability (4.00 vs 1.95) compared to unmodified LLM

## Executive Summary
This paper proposes a hybrid approach for creating werewolf game agents that combine a Large Language Model (LLM) with a rule-based algorithm. The system addresses challenges in incomplete information games where LLMs alone struggle with strategic deception and conversation termination. By using the LLM for both utterance generation and conversation analysis while employing a rule-based algorithm to selectively override outputs based on game state, the agent achieves better performance in critical game situations.

The system implements persona prompts to give agents distinct talking styles without modifying or fine-tuning the LLM. Qualitative evaluation shows the proposed agent is perceived as more human-like, logical, and individualistic compared to an unmodified LLM. The approach successfully handles complex natural language conversations in the werewolf game context, demonstrating that combining LLM capabilities with rule-based control can produce more effective game-playing agents.

## Method Summary
The proposed method combines a Large Language Model with a rule-based algorithm to create werewolf agents. The system uses an LLM to generate utterances and analyze conversation history, extracting key game information like voting decisions and divination results. A rule-based algorithm then evaluates whether the LLM's output is appropriate for the current game state and substitutes predefined templates when necessary. Persona prompts are used to transform the LLM's output style without fine-tuning, giving agents distinct personalities. The hybrid approach allows the agent to refute accusations in critical situations, end conversations strategically, and maintain character consistency while playing the incomplete information game.

## Key Results
- Qualitative evaluation shows the proposed agent is perceived as more human-like and logical compared to an unmodified LLM
- Higher scores in individuality (4.54 vs 2.52) and deceiving ability (4.00 vs 1.95)
- The system successfully handles complex natural language conversations in the werewolf game context
- Persona prompts effectively transform LLM output style without fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The rule-based algorithm selectively overrides LLM-generated utterances based on game situation analysis, enabling strategic deception and conversation termination.
- Mechanism: The system uses an LLM to analyze conversation history and extract game-relevant information (voting decisions, divination results). A rule-based algorithm then evaluates whether the LLM's generated utterance is appropriate for the current game state. If not, it substitutes a pre-defined template utterance designed for specific strategic situations like counter-CO or closing conversations.
- Core assumption: Certain game situations require strategic responses that an unmodified LLM cannot reliably generate, such as lying about being a seer when accused or ending a conversation when all players have indicated their voting intentions.
- Evidence anchors:
  - [abstract]: "Our agent uses a rule-based algorithm to select an output either from an LLM or a template prepared beforehand based on the results of analyzing conversation history using an LLM. It allows the agent to refute in specific situations, identify when to end the conversation, and behave with persona."
  - [section]: "The rule-based algorithm decides whether that output is appropriate or not, given the game situation. If the talk is inappropriate, the rule-based algorithm uses a predefined template utterance."
  - [corpus]: Weak evidence - related papers focus on LLM-based agents but don't specifically discuss hybrid rule-based approaches for strategic deception in Werewolf.
- Break condition: If the rule-based algorithm fails to correctly identify critical situations requiring template substitution, or if the template utterances are poorly designed and detected as inauthentic by other players.

### Mechanism 2
- Claim: Persona prompts transform the LLM's output style to create distinct character identities, improving the agent's perceived individuality and engagement.
- Mechanism: The system uses carefully crafted prompts that specify character traits, speaking styles, and background information. These prompts are prepended to the main prompt sent to the LLM, guiding it to generate utterances consistent with the specified persona (e.g., Princess, Kansai dialect, Zundamon).
- Core assumption: LLMs can be effectively controlled through prompt engineering to adopt specific personas without fine-tuning, and distinct personas make agents more engaging and distinguishable in multiplayer games.
- Evidence anchors:
  - [abstract]: "Persona prompts are used to give the agent distinct talking styles. Qualitative evaluation shows the proposed agent is perceived as more human-like and logical compared to an unmodified LLM, with higher scores in individuality (4.54 vs 2.52) and deceiving ability (4.00 vs 1.95)."
  - [section]: "We used prompts to control them without modifying or fine-tuning the model and give the agent distinguishable personalities using prompts."
  - [corpus]: Weak evidence - related papers discuss LLM agents in Werewolf but don't specifically address persona-based prompt engineering for style transformation.
- Break condition: If the persona prompts are not specific enough or conflict with the game context, the LLM may generate incoherent or contextually inappropriate utterances that break character.

### Mechanism 3
- Claim: Talk analysis using LLMs extracts critical game information from natural language conversation history, enabling the rule-based algorithm to make informed decisions.
- Mechanism: The system uses an LLM to parse conversation history and identify key game events such as voting intentions ("Agent[04]への疑念は明白だと思うのだ") and divination results ("私は占い師です。占いの結果、Agent[02]は人狼でした"). This extracted information is formatted consistently and fed to the rule-based algorithm.
- Core assumption: LLMs can reliably extract structured game information from unstructured natural language conversations in the Werewolf game context.
- Evidence anchors:
  - [abstract]: "The LLM examines conversation history and generates talks containing this information in a fixed format. The information is also used by the rule-based algorithm to make decisions."
  - [section]: "Conversation history in a Werewolf game that uses natural language is complex, and extracting them using regular expressions was difficult. Thus, the LLM was used to extract the information."
  - [corpus]: Weak evidence - related papers focus on LLM-based agents but don't specifically discuss using LLMs for talk analysis to extract game state information.
- Break condition: If the LLM fails to correctly parse complex conversation structures or if the formatting requirements change, the rule-based algorithm may receive incorrect or incomplete information for decision-making.

## Foundational Learning

- Concept: Rule-based systems vs. pure LLM approaches
  - Why needed here: Understanding when to use deterministic rules versus probabilistic generation is crucial for designing hybrid systems that leverage the strengths of both approaches.
  - Quick check question: In what situations would a rule-based approach be preferable to an LLM, and vice versa, in the context of the Werewolf game?

- Concept: Prompt engineering for persona control
  - Why needed here: The system relies on carefully crafted prompts to transform the LLM's output style without fine-tuning, which requires understanding how to effectively communicate desired characteristics to the model.
  - Quick check question: How would you design a prompt to make an LLM speak in a specific dialect or with a particular personality trait?

- Concept: Game theory and strategic reasoning in incomplete information games
  - Why needed here: The Werewolf game involves deception, deduction, and strategic communication, requiring understanding of how to make decisions when you don't have complete information about other players' roles.
  - Quick check question: What are the key strategic considerations for a werewolf player when deciding whether to lie about being a seer?

## Architecture Onboarding

- Component map: Utterance Generation Module -> Talk Analysis Module -> Rule-based Algorithm -> Output Selection -> Server Communication
- Critical path: Game state update → Utterance Generation → LLM → Talk Analysis → LLM → Rule-based Decision → Output selection → Server communication
- Design tradeoffs:
  - Using LLM for talk analysis adds computational overhead but provides more accurate information extraction than rule-based parsing
  - Persona prompts increase system flexibility but may reduce grammatical naturalness (as observed in evaluation)
  - Rule-based algorithm provides strategic control but requires careful definition of critical situations
- Failure signatures:
  - Inconsistent utterances across turns (indicates poor weighting of agent's own past utterances)
  - Grammatically awkward sentences (indicates persona prompt conflicts with natural language generation)
  - Failure to detect critical situations requiring template substitution (indicates inadequate rule-based criteria)
- First 3 experiments:
  1. Test the talk analysis module with sample conversation histories to verify accurate extraction of voting decisions and divination results
  2. Test the rule-based algorithm with various game situations to ensure correct selection between LLM output and template utterances
  3. Test the complete system with different personas in a controlled game environment to evaluate style transformation effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed system handle long-term conversational consistency, and what specific mechanisms could be implemented to prevent contradictory utterances across multiple game rounds?
- Basis in paper: [explicit] The paper explicitly mentions this as a limitation, stating "The reason is that the agent's own utterance was mitigated by a long conversation history, and the agent becomes too affected by other players' utterances."
- Why unresolved: The paper only mentions this as a future direction without providing specific solutions or testing any approaches to address the issue.
- What evidence would resolve it: Comparative experiments showing the system's performance with and without weight adjustments for past utterances, or with different prompt engineering techniques designed to maintain consistency.

### Open Question 2
- Question: How would the system's performance change with different LLM models (e.g., newer versions of GPT or different architectures), and what is the impact of model selection on the balance between rule-based and LLM-generated content?
- Basis in paper: [inferred] The paper mentions "Using the latest versions of the LLMs might lead to different outcomes" and discusses costs associated with API calls, suggesting model selection could impact performance.
- Why unresolved: The paper only uses specific versions of GPT models and doesn't explore how different model choices might affect the system's effectiveness.
- What evidence would resolve it: Comparative analysis of the system's performance using different LLM models, including newer versions or different architectures, with detailed metrics on output quality and rule-based algorithm effectiveness.

### Open Question 3
- Question: What is the optimal balance between rule-based and LLM-generated content for different game scenarios, and how does this balance affect the system's adaptability to more complex game variants with more players?
- Basis in paper: [explicit] The paper states "This method will only work well for simple games with a few players" and mentions that "as the number of players increases and the game becomes more complex, it becomes difficult to define rule-based algorithm."
- Why unresolved: The paper doesn't explore how the balance between rule-based and LLM content should be adjusted for different game complexities or provide a framework for determining optimal ratios.
- What evidence would resolve it: Empirical studies testing the system with varying ratios of rule-based to LLM content across different game scenarios and player counts, with performance metrics for each configuration.

### Open Question 4
- Question: How does the persona implementation affect the system's ability to deceive and maintain consistency, and are there optimal persona characteristics that enhance performance in different game situations?
- Basis in paper: [inferred] The paper implements persona through prompts but doesn't explore how different persona characteristics might affect performance in various game situations.
- Why unresolved: While the paper implements persona prompts, it doesn't analyze how different persona characteristics might impact the system's effectiveness in different game contexts.
- What evidence would resolve it: Comparative analysis of system performance using different persona configurations across various game scenarios, with metrics on deception success rates and consistency maintenance.

## Limitations

- Rule-based algorithm decision criteria are not fully specified, with only 2 of 14 detection situations clearly described
- Persona prompts lack complete implementation details for all five characters
- Evaluation methodology relies on subjective annotator ratings without quantitative metrics or statistical significance testing

## Confidence

- Rule-based algorithm effectiveness: Medium
- Persona prompt transformation: Medium
- Talk analysis accuracy: Medium
- Overall system performance: Medium

## Next Checks

1. Implement and test the complete set of 14 detection situations with controlled game scenarios to verify correct identification of critical situations requiring template substitution.

2. Conduct systematic testing of all five persona prompts across multiple conversation contexts to measure consistency, grammatical quality, and character authenticity.

3. Evaluate the LLM's ability to extract voting decisions and divination results from complex conversation histories with varying levels of ambiguity and noise.