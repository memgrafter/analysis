---
ver: rpa2
title: 'QueensCAMP: an RGB-D dataset for robust Visual SLAM'
arxiv_id: '2410.12520'
source_url: https://arxiv.org/abs/2410.12520
tags:
- dataset
- failures
- vslam
- lens
- sequences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the QueensCAMP dataset, a novel RGB-D dataset
  designed to evaluate the robustness of Visual SLAM (VSLAM) algorithms under challenging
  conditions. The dataset includes real-world indoor scenes with dynamic objects,
  motion blur, varying illumination, and emulated camera failures such as lens dirt,
  condensation, underexposure, and overexposure.
---

# QueensCAMP: an RGB-D dataset for robust Visual SLAM

## Quick Facts
- arXiv ID: 2410.12520
- Source URL: https://arxiv.org/abs/2410.12520
- Reference count: 24
- Novel RGB-D dataset for evaluating VSLAM robustness under challenging conditions

## Executive Summary
This paper introduces the QueensCAMP dataset, a novel RGB-D dataset designed to evaluate the robustness of Visual SLAM (VSLAM) algorithms under challenging conditions. The dataset includes real-world indoor scenes with dynamic objects, motion blur, varying illumination, and emulated camera failures such as lens dirt, condensation, underexposure, and overexposure. To create these failure conditions, the authors developed open-source scripts for injecting camera failures into any images, enabling further customization by the research community. The dataset was evaluated using two algorithms: ORB-SLAM2, a traditional VSLAM system, and TartanVO, a deep learning-based VO algorithm. Results showed that both algorithms experienced performance degradation under challenging conditions, with ORB-SLAM2 particularly affected by lens dirt and condensation failures. The dataset and failure injection tools provide a valuable resource for developing more robust VSLAM systems capable of handling real-world challenges.

## Method Summary
The QueensCAMP dataset was created by collecting RGB-D sequences in indoor environments, then applying various degradation conditions including motion blur, illumination changes, and emulated camera failures. The authors developed open-source scripts that can inject camera failures (lens dirt, condensation, underexposure, overexposure) into any images, making the methodology extensible. The dataset was evaluated using ORB-SLAM2 and TartanVO algorithms to assess performance under these challenging conditions.

## Key Results
- Both ORB-SLAM2 and TartanVO showed performance degradation under challenging conditions
- ORB-SLAM2 was particularly affected by lens dirt and condensation failures
- The dataset provides a controlled environment for testing VSLAM robustness against emulated camera failures
- Open-source failure injection tools enable community-driven expansion and testing

## Why This Works (Mechanism)
The dataset works by providing controlled, reproducible degradation conditions that simulate real-world camera failures. By combining real indoor scenes with programmatically injected failures, researchers can systematically evaluate how VSLAM algorithms respond to specific types of degradation. The inclusion of both traditional (ORB-SLAM2) and learning-based (TartanVO) approaches demonstrates that robustness challenges affect different algorithmic paradigms in distinct ways.

## Foundational Learning

1. **RGB-D SLAM fundamentals** - Why needed: Understanding depth-camera based mapping and localization; Quick check: Can you explain how depth information improves SLAM accuracy over monocular approaches?

2. **Visual odometry vs SLAM** - Why needed: Distinguishing between localization-only and mapping+localization systems; Quick check: Can you identify which components are shared between VO and SLAM algorithms?

3. **Camera failure modes** - Why needed: Recognizing how physical degradation affects image quality and feature extraction; Quick check: Can you describe how lens dirt specifically impacts feature detection algorithms?

4. **Dataset evaluation methodology** - Why needed: Understanding how to quantitatively assess algorithm performance under degradation; Quick check: Can you explain what metrics would be most relevant for evaluating VSLAM robustness?

## Architecture Onboarding

**Component Map:** RGB-D Camera -> Feature Extraction -> Tracking -> Local Mapping -> Loop Closure -> Global Optimization

**Critical Path:** Feature extraction and tracking are most sensitive to camera failures, as degraded image quality directly impacts keypoint detection and descriptor matching.

**Design Tradeoffs:** The dataset prioritizes controlled degradation over naturalistic scenarios, enabling systematic testing but potentially missing complex real-world interactions between failure modes.

**Failure Signatures:** Lens dirt primarily affects high-frequency features and corner detection; condensation creates localized regions of degraded quality; exposure issues affect overall image statistics and feature intensity gradients.

**First Experiments:**
1. Run baseline ORB-SLAM2 on clean dataset to establish performance baseline
2. Apply single failure mode (e.g., lens dirt) to measure degradation impact
3. Test TartanVO on the same failure-injected sequences for comparison with learning-based approach

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset focuses on indoor scenes with limited environmental variety
- Emulated camera failures may not fully capture real-world degradation complexity
- Evaluation only considers two specific VSLAM algorithms, limiting generalizability

## Confidence

**Dataset creation methodology:** High confidence
**Failure injection framework effectiveness:** Medium confidence  
**Performance degradation observations:** Medium confidence
**Generalization to broader VSLAM community:** Low confidence

## Next Checks
1. Evaluate additional VSLAM algorithms beyond ORB-SLAM2 and TartanVO to assess broader performance impacts
2. Test algorithms on real camera failure conditions (rather than emulated) to validate dataset relevance
3. Expand dataset to include outdoor scenes and additional environmental challenges to improve generalizability