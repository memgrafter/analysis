---
ver: rpa2
title: Chain-of-Thought Prompting for Speech Translation
arxiv_id: '2409.11538'
source_url: https://arxiv.org/abs/2409.11538
tags:
- speech
- prompting
- arxiv
- text
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using ASR transcripts as prompts for speech
  translation in a Speech-LLM framework built on Megatron-T5. The method first generates
  ASR transcripts from speech and then uses them along with speech encodings to guide
  translation, forming a two-step chain-of-thought (CoT) prompting process.
---

# Chain-of-Thought Prompting for Speech Translation

## Quick Facts
- arXiv ID: 2409.11538
- Source URL: https://arxiv.org/abs/2409.11538
- Authors: Ke Hu, Zhehuai Chen, Chao-Han Huck Yang, Piotr Żelasko, Oleksii Hrinchuk, Vitaly Lavrukhin, Jagadeesh Balam, Boris Ginsburg
- Reference count: 40
- Key outcome: CoT prompting improves AST performance by 2.4 BLEU points across 6 En→X or X→En tasks compared to speech prompting alone

## Executive Summary
This paper proposes using ASR transcripts as prompts for speech translation in a Speech-LLM framework built on Megatron-T5. The method first generates ASR transcripts from speech and then uses them along with speech encodings to guide translation, forming a two-step chain-of-thought (CoT) prompting process. Low-rank adaptation (LoRA) is applied to the LLM for efficient fine-tuning. Experimental results show the proposed CoT prompting improves AST performance by an average of 2.4 BLEU points across 6 En→X or X→En tasks compared to speech prompting alone. The approach also outperforms a related CoT prediction method by 2 BLEU points on average.

## Method Summary
The proposed method uses a two-step chain-of-thought prompting approach for speech translation. First, speech audio is encoded by a Canary-1B encoder and passed through an ASR system to generate transcripts. These transcripts, along with the speech encodings, are then concatenated with fixed text prompts and fed into a Megatron-T5 LLM for translation. LoRA fine-tuning is applied to the LLM to adapt it to the speech translation task efficiently. The model is trained with next token prediction loss using RMSNorm and Adam optimizer. Evaluation is performed on FLEURS dataset across 6 language pairs (De→En, Fr→En, Es→En, En→De, En→Fr, En→Es) using BLEU score as the primary metric.

## Key Results
- CoT prompting improves AST performance by 2.4 BLEU points on average compared to speech prompting alone
- The method outperforms a related CoT prediction approach by 2 BLEU points on average
- Using ground truth ASR transcripts yields up to 2.7 BLEU improvement, suggesting better ASR quality can further boost translation
- The model achieves competitive results compared to larger models like SeamlessM4T, with a total parameter size of 1.8B

## Why This Works (Mechanism)

### Mechanism 1
ASR transcripts provide contextual grounding that reduces ambiguity in speech translation. The transcripts supply explicit lexical and syntactic structure in the source language, which the LLM can use to resolve acoustic ambiguities (homophones, unclear pronunciation) before generating the target translation. This two-stage process mirrors chain-of-thought reasoning by first establishing "what was said" and then "what it means."

### Mechanism 2
Prompting with both speech encodings and ASR transcripts leverages complementary information sources. Speech encodings capture prosodic and acoustic features while ASR transcripts provide explicit linguistic content. Combining both allows the LLM to use acoustic cues for disambiguation while relying on textual structure for grammatical accuracy.

### Mechanism 3
LoRA fine-tuning preserves pretrained LLM capabilities while adapting to speech translation task. LoRA introduces low-rank adapters to the encoder and decoder that capture task-specific patterns without overwriting the foundational language understanding. This allows efficient adaptation with minimal parameter increase while maintaining the LLM's general capabilities for handling ASR-augmented prompts.

## Foundational Learning

- **Concept**: Chain-of-Thought prompting in language models
  - Why needed here: Understanding how multi-step reasoning works in LLMs is crucial for grasping why generating ASR first and then translating improves performance compared to direct speech-to-translation
  - Quick check question: In CoT prompting, what is the purpose of generating intermediate reasoning steps before producing the final answer?

- **Concept**: Encoder-decoder architecture and attention mechanisms
  - Why needed here: The model uses Megatron-T5 with encoder-decoder structure; understanding how cross-attention works between encoder representations and decoder states is essential for implementing and debugging the model
  - Quick check question: In an encoder-decoder model, what is the role of cross-attention in the decoder layers?

- **Concept**: Low-Rank Adaptation (LoRA) principles
  - Why needed here: LoRA is used for efficient fine-tuning; understanding how low-rank decomposition works and why it's effective for parameter-efficient adaptation is important for model optimization
  - Quick check question: How does LoRA modify the weight update process during fine-tuning compared to full fine-tuning?

## Architecture Onboarding

- **Component map**: Speech input → Canary encoder → speech embeddings → ASR system → source language transcripts → Concatenation with fixed prompts → T5 input → T5 generation → target language translation

- **Critical path**: 1) Speech input → Canary encoder → speech embeddings; 2) Speech embeddings → ASR system → source language transcripts; 3) Concatenation: [fixed prompt + ASR transcripts + speech encodings] → T5 input; 4) T5 generation → target language translation; 5) Loss calculation on translation output only

- **Design tradeoffs**: ASR quality vs. model performance (better ASR yields higher BLEU but requires more accurate ASR system); prompt length vs. efficiency (longer prompts may improve accuracy but increase computation); LoRA rank vs. adaptation capacity (higher rank allows better adaptation but increases parameters and memory); training data size vs. generalization (more data improves performance but increases training time and cost)

- **Failure signatures**: BLEU scores plateau or degrade when ASR transcripts are noisy or incomplete; model fails to generate coherent translations when speech encodings and transcripts conflict; training instability when LoRA rank is too high for available GPU memory; poor performance on languages with limited training data

- **First 3 experiments**: 1) Baseline comparison: Run SALM-T5 baseline (no ASR transcripts) vs. CoT prompting with estimated ASR to verify the ~1.5 BLEU improvement claim; 2) ASR quality ablation: Compare performance using estimated ASR vs. ground truth ASR transcripts to quantify the impact of ASR accuracy on translation quality; 3) CoT prediction vs. CoT prompting: Implement the CoT prediction method from [28] using the same ASR system and compare to the proposed CoT prompting to verify the ~2 BLEU improvement claim

## Open Questions the Paper Calls Out

### Open Question 1
How does the quality of ASR transcripts impact speech translation performance when using chain-of-thought prompting? The paper shows that using ground truth ASR transcripts improves BLEU scores by 2.7 points on average compared to estimated ASR hypotheses, suggesting better ASR quality can further boost translation. This remains unresolved as the paper does not explore the impact of using lower quality ASR transcripts or transcripts from lightweight models.

### Open Question 2
Is the encoder-decoder structure of T5 more effective than decoder-only GPT for chain-of-thought prompting in speech translation? While the paper shows T5's superiority over GPT-based CoT prediction, it does not explore why the encoder-decoder structure is more effective or if there are specific aspects of the structure that contribute to this improvement.

### Open Question 3
Can the proposed chain-of-thought prompting method be generalized to other speech tasks beyond translation? The paper focuses on speech translation but mentions that the method could potentially be applied to other speech tasks. This remains unresolved as the paper does not provide evidence or experiments demonstrating the effectiveness of the method on other speech tasks such as speech summarization or speech-to-text conversion.

## Limitations

- The model's performance is highly dependent on ASR quality, with a 2.7 BLEU point improvement when using ground truth vs. estimated ASR transcripts
- Evaluation is limited to 6 language pairs within European language families, raising questions about generalization to distant language pairs or low-resource languages
- The paper lacks ablation studies to isolate the contribution of each component (CoT prompting vs. LoRA vs. architecture choice)

## Confidence

**High Confidence**: The claim that CoT prompting improves speech translation performance compared to direct speech prompting (baseline SALM-T5) is well-supported by consistent 1.5-2.4 BLEU improvements across all 6 language pairs.

**Medium Confidence**: The claim that CoT prompting outperforms CoT prediction by 2 BLEU points on average is statistically significant but based on a single reference method without exploring the broader design space of CoT variants.

**Low Confidence**: The claim that this approach is competitive with larger models like SeamlessM4T remains suggestive rather than definitive due to lack of controlled comparisons on identical datasets and metrics.

## Next Checks

1. **ASR Quality Ablation Study**: Run the full CoT prompting pipeline using ASR transcripts with controlled error rates (synthetic word errors inserted at known rates) to determine the relationship between ASR WER and translation BLEU, establishing the minimum ASR quality threshold needed for CoT prompting to be beneficial.

2. **Cross-Lingual Generalization Test**: Evaluate the CoT prompting approach on non-European language pairs (e.g., En→Ja, En→Sw) and measure performance degradation compared to the 6 language pairs tested, including a comparison with direct speech prompting to determine if the CoT advantage persists across linguistically diverse language families.

3. **Component Contribution Analysis**: Perform an ablation study removing LoRA fine-tuning while keeping CoT prompting, and vice versa, to isolate which component drives more of the performance improvement, and test alternative parameter-efficient fine-tuning methods with the same CoT prompting framework.