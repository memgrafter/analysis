---
ver: rpa2
title: 'Attribute or Abstain: Large Language Models as Long Document Assistants'
arxiv_id: '2407.07799'
source_url: https://arxiv.org/abs/2407.07799
tags:
- evidence
- quality
- response
- document
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates how well large language models (LLMs) can
  generate responses and evidence for long documents, aiming to increase trust by
  providing verifiable attributions. It introduces LAB, a benchmark with 6 long-document
  tasks and 4 LLMs, comparing 5 attribution approaches: post-hoc, retrieve-then-read,
  citation, and their reduced variants.'
---

# Attribute or Abstain: Large Language Models as Long Document Assistants

## Quick Facts
- arXiv ID: 2407.07799
- Source URL: https://arxiv.org/abs/2407.07799
- Reference count: 39
- Key outcome: Citation approach works best for large/fine-tuned models while post-hoc helps small models; evidence quality predicts response quality for simple tasks but not complex multi-fact ones.

## Executive Summary
This paper evaluates how well large language models can generate responses and evidence for long documents, introducing LAB - a benchmark with 6 long-document tasks and 4 LLMs. The study compares 5 attribution approaches (post-hoc, retrieve-then-read, citation, and their reduced variants) to understand when and how models should generate evidence alongside responses. Key findings show citation works best for large and fine-tuned models, while small prompted models benefit from post-hoc retrieval. The study also finds no "Lost in the Middle" effect but observes that response quality decreases toward document end, and evidence quality only predicts response quality for simple tasks.

## Method Summary
The study evaluates 4 LLMs (GPT-3.5, GPT-4, Flan-T5-XL, Longchat) across 6 datasets (QASPER, Natural Questions, Evidence Inference, Wice, ContractNLI, GovReport) using 5 attribution approaches. The approaches include post-hoc (separate response and evidence retrieval), retrieve-then-read (retrieve evidence first), citation (joint response and evidence generation), and reduced variants that save context. The evaluation uses metrics for response quality (EM, CF1, RL), evidence quality (EF1, ATT), and unanswerable F1, with correlation analysis between evidence and response quality.

## Key Results
- Citation approach outperforms other methods for large and fine-tuned models
- Small prompted models show better performance with post-hoc retrieval
- Evidence quality can predict response quality for simple responses but not for complex multi-fact responses
- No "Lost in the Middle" effect observed, but response quality decreases toward document end

## Why This Works (Mechanism)

## Mechanism 1
- **Claim:** Citation approach works best for large and fine-tuned models because it integrates evidence retrieval into the generation process, reducing the need for task decomposition.
- **Mechanism:** The model generates both response and evidence in one step, maintaining coherence and reducing the loss of information that can occur when tasks are separated.
- **Core assumption:** The model has sufficient capacity and training to handle both tasks simultaneously without degradation in quality.
- **Evidence anchors:**
  - [abstract] "citation, i.e. response generation and evidence extraction in one step, mostly performs best."
  - [section 4.1] "Citation or reduced-citation result in the best average evidence quality, while not hurting response quality, in line with recent work showing LLM capabilities for retrieval (Ma et al., 2023)."
- **Break condition:** If the model lacks sufficient capacity or is not fine-tuned for this dual task, performance may degrade, making task decomposition (post-hoc) more effective.

## Mechanism 2
- **Claim:** Small prompted models benefit from post-hoc retrieval because they struggle with instruction following for evidence extraction.
- **Mechanism:** Separating response generation and evidence retrieval allows the model to focus on one task at a time, reducing the cognitive load and improving performance.
- **Core assumption:** Small models have limited instruction following capabilities, making task decomposition beneficial.
- **Evidence anchors:**
  - [abstract] "while additional retrieval can help for small, prompted models."
  - [section 4.1] "For small models such as Longchat, related work has shown that they lack instruction following capability to perform evidence extraction (Gao et al., 2023b; Schimanski et al., 2024), making post-hoc the better approach for the model."
- **Break condition:** If the model is large or fine-tuned, it can handle both tasks simultaneously, making citation more effective.

## Mechanism 3
- **Claim:** Evidence quality can predict response quality for simple responses but not for complex multi-fact responses because models struggle to provide evidence for all claims.
- **Mechanism:** For simple responses, the model can easily point to evidence for each claim. For complex responses, the model may fail to provide evidence for all claims, leading to a disconnect between evidence and response quality.
- **Core assumption:** The complexity of the response correlates with the difficulty of providing sufficient evidence.
- **Evidence anchors:**
  - [abstract] "We also find that evidence quality can predict response quality on datasets with simple responses, but not so for complex responses, as models struggle with providing evidence for complex claims."
  - [section 4.3] "We consider GovReport a special case, as its long responses are evaluated in their entirety, which might be too coarse-grained to reflect the per-statement evaluation of evidence quality."
- **Break condition:** If the response is simple (e.g., single fact), evidence quality can effectively predict response quality. If the response is complex (e.g., multi-fact), this correlation breaks down.

## Foundational Learning

- **Concept:** Task decomposition
  - **Why needed here:** Understanding when to separate tasks (e.g., post-hoc) vs. integrate them (e.g., citation) is crucial for optimizing model performance.
  - **Quick check question:** When would you choose post-hoc over citation for a given model and task?

- **Concept:** Evidence quality evaluation
  - **Why needed here:** Automatically evaluating evidence quality without external references is essential for scaling attribution tasks.
  - **Quick check question:** How would you design a model to evaluate the attributability of evidence?

- **Concept:** Long document processing
  - **Why needed here:** Handling long documents requires understanding how models utilize context and the impact of document length on performance.
  - **Quick check question:** What are the challenges of processing long documents, and how can they be mitigated?

## Architecture Onboarding

- **Component map:** Document → (Task Decomposition) → Evidence Retrieval → Response Generation → Evidence Quality Evaluation
- **Critical path:** For citation: Document → Response + Evidence Generation → Evidence Quality Evaluation. For post-hoc: Document → Response Generation → Evidence Retrieval → Evidence Quality Evaluation.
- **Design tradeoffs:** Citation reduces task decomposition but requires model capacity. Post-hoc simplifies tasks but may lose coherence. Reduced approaches save context but may disrupt logical flow.
- **Failure signatures:** Poor evidence quality, low response quality, or mismatched evidence-response pairs indicate issues with the chosen approach or model capabilities.
- **First 3 experiments:**
  1. Compare citation vs. post-hoc on a small model to validate task decomposition benefits.
  2. Test evidence quality prediction on simple vs. complex responses to understand the correlation breakdown.
  3. Analyze positional biases by varying evidence position in documents and measuring response quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the position of information within long documents significantly impact LLM performance in attribution tasks, and if so, how can this positional bias be mitigated?
- Basis in paper: [inferred] The paper investigates whether LLMs exhibit positional biases in evidence attribution, finding that response quality generally decreases as evidence appears later in the document.
- Why unresolved: While the paper identifies a correlation between evidence position and response quality, it does not explore potential methods to mitigate this bias. Further research could investigate techniques like document summarization, section highlighting, or adaptive attention mechanisms to address this issue.
- What evidence would resolve it: Experiments comparing LLM performance on attribution tasks with and without positional bias mitigation techniques, such as summarization or section highlighting, would provide insights into effective strategies for addressing this challenge.

### Open Question 2
- Question: How does the complexity of responses (e.g., single-fact vs. multi-fact) affect the relationship between evidence quality and response quality, and what strategies can improve evidence extraction for complex claims?
- Basis in paper: [explicit] The paper finds that evidence quality can predict response quality for simple responses but not for complex multi-fact responses, as models struggle with providing evidence for complex claims.
- Why unresolved: The paper highlights the difficulty of extracting evidence for complex claims but does not propose specific strategies to improve this capability. Further research could explore techniques like fine-grained evidence extraction, claim decomposition, or iterative refinement to enhance evidence quality for complex responses.
- What evidence would resolve it: Experiments comparing LLM performance on complex attribution tasks with and without advanced evidence extraction techniques would demonstrate the effectiveness of different approaches in handling complex claims.

### Open Question 3
- Question: How can the evaluation of attributability be improved to handle edge cases and ensure accurate assessment of evidence quality?
- Basis in paper: [inferred] The paper acknowledges limitations in attributability evaluation models, noting that edge cases are not yet handled well and that manual analysis revealed instances where the evaluation model failed to recognize evidence for "partially supported" claims.
- Why unresolved: While the paper uses existing attributability evaluation models, it recognizes their limitations and the need for further research to address edge cases. Developing more robust evaluation models that can accurately assess evidence quality in various scenarios is crucial for reliable attribution.
- What evidence would resolve it: Experiments comparing the performance of different attributability evaluation models on diverse datasets and claim types would identify the most effective approaches for handling edge cases and ensuring accurate assessment of evidence quality.

## Limitations
- The exact thresholds where citation approaches become more effective than post-hoc are not precisely defined.
- The definition of "complex" responses in the evidence-response quality correlation is not quantified.
- The absence of "Lost in the Middle" effects may not generalize to all long-document scenarios.
- Automatic evidence quality evaluation without external references may introduce bias or inaccuracies.

## Confidence
- High: The general finding that citation approaches outperform other methods for large/fine-tuned models is well-supported by experimental results across multiple datasets.
- Medium: The claim about small models benefiting from post-hoc retrieval is supported but may depend on specific model architectures and training procedures not fully explored.
- Medium: The correlation between evidence quality and response quality for simple tasks is observed, but the breakdown for complex responses needs more rigorous validation with varied task complexity.

## Next Checks
1. Test the transition point between citation and post-hoc effectiveness by systematically varying model size and fine-tuning levels to identify the threshold where approach effectiveness changes.
2. Conduct ablation studies on evidence complexity by controlling the number of facts per response and measuring when evidence-response quality correlation breaks down.
3. Validate the absence of "Lost in the Middle" effects across diverse document types and lengths, particularly focusing on documents where middle sections contain critical information versus peripheral content.