---
ver: rpa2
title: 'Beyond Optimism: Exploration With Partially Observable Rewards'
arxiv_id: '2406.13909'
source_url: https://arxiv.org/abs/2406.13909
tags:
- agent
- rewards
- exploration
- learning
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of exploration in reinforcement
  learning when rewards are not always observable. The authors propose a novel exploration
  strategy based on the successor representation to overcome the limitations of existing
  methods, particularly optimism, which can fail in such scenarios.
---

# Beyond Optimism: Exploration With Partially Observable Rewards

## Quick Facts
- arXiv ID: 2406.13909
- Source URL: https://arxiv.org/abs/2406.13909
- Authors: Simone Parisi; Alireza Kazemipour; Michael Bowling
- Reference count: 40
- Primary result: Proposed exploration strategy successfully learns optimal policies in Mon-MDPs where other methods fail due to partially unobservable rewards.

## Executive Summary
This paper addresses the challenge of exploration in reinforcement learning when rewards are not always observable. The authors propose a novel exploration strategy called "Directed Exploration via Successor-Functions" that decouples exploration from reward observability by using a goal-conditioned policy based on the successor representation. This approach overcomes the limitations of optimism-based methods that fail when rewards are partially unobservable. The method is theoretically grounded with convergence guarantees under certain assumptions and is validated on a collection of tabular MDPs and Monitored MDPs (Mon-MDPs).

## Method Summary
The proposed method uses a successor representation (S-function) to predict state occurrences under a policy, independent of rewards. A goal-conditioned policy based on this S-function guides exploration toward unvisited state-action pairs, while a separate Q-function handles exploitation. The algorithm maintains a threshold βt that decays as exploration progresses, ensuring greedy behavior in the limit. For Mon-MDPs, a reward model compensates for unobservable rewards. The approach uses Q-Learning for all components and guarantees convergence to optimal policies under communicating MDP assumptions.

## Key Results
- Outperforms existing exploration strategies in environments with partially observable rewards
- Successfully learns optimal policies in Mon-MDPs where optimism-based methods fail
- Converges to optimal policies in standard tabular MDPs while maintaining efficiency
- Demonstrates effective exploration through uniform state-action space coverage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The successor representation (SR) allows exploration to be decoupled from reward observability, enabling the agent to visit all state-action pairs uniformly even when rewards are partially unobservable.
- Mechanism: The SR predicts the cumulative discounted occurrence of state-action pairs under a policy, independent of the reward function. By using an indicator reward (1 when the goal state-action pair is visited, 0 otherwise) in the S-function update, the agent can always observe rewards during exploration, regardless of the monitor state.
- Core assumption: The MDP is communicating (all state-action pairs are reachable from any other pair), ensuring the existence of a goal-conditioned policy with bounded goal-relative diameter.

### Mechanism 2
- Claim: The exploration-exploitation threshold βt naturally decays as the agent explores, ensuring that the policy becomes greedy in the limit.
- Mechanism: βt = log t/Nt(sG, aG) decreases over time because Nt(sG, aG) grows faster than log t as the agent visits the goal state-action pair more frequently. This causes the agent to explore less and exploit more as learning progresses.
- Core assumption: Every state-action pair is visited infinitely often, which is guaranteed by the GLIE property of the algorithm.

### Mechanism 3
- Claim: The use of a goal-conditioned policy based on the S-function allows the agent to reach any desired state-action pair efficiently, guiding exploration towards unvisited regions of the state space.
- Mechanism: The goal-conditioned policy ρ(a | st, sG, aG) selects actions based on the current state st and the goal state-action pair (sG, aG). By choosing the least-visited state-action pair as the goal, the agent implicitly explores the state space as uniformly as possible.
- Core assumption: The S-function can be learned accurately enough to guide the agent towards the goal state-action pair.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The paper's exploration strategy is designed for MDPs and Mon-MDPs, so understanding the basic MDP framework is crucial.
  - Quick check question: What are the key components of an MDP, and how do they differ from a Monitored MDP (Mon-MDP)?

- Concept: Successor Representation (SR)
  - Why needed here: The SR is the core of the proposed exploration strategy, as it allows the agent to predict state occurrences under a policy independently of the reward function.
  - Quick check question: How does the SR differ from the value function, and why is this distinction important for exploration in Mon-MDPs?

- Concept: Goal-Conditioned Policy
  - Why needed here: The exploration strategy relies on a goal-conditioned policy to guide the agent towards unvisited state-action pairs, so understanding this concept is essential.
  - Quick check question: What is a goal-conditioned policy, and how does it differ from a regular policy in terms of its input and output?

## Architecture Onboarding

- Component map: S-function (bSsiaj) -> Q-function (bQ) -> Reward model (bR) -> Exploration policy -> Exploitation policy

- Critical path:
  1. Initialize bS, bQ, and bR optimistically/pessimistically
  2. At each time step, select the least-visited state-action pair as the goal
  3. Compute βt = log t/Nt(sG, aG)
  4. If βt > β̄, explore using the goal-conditioned policy; otherwise, exploit using the greedy policy
  5. Update bS, bQ, and bR based on the observed rewards and transitions
  6. Repeat until convergence or a maximum number of steps is reached

- Design tradeoffs:
  - Exploration vs. exploitation: The threshold β̄ controls the balance between exploration and exploitation, with higher values leading to more exploration
  - Optimistic vs. pessimistic initialization: Initializing bQ pessimistically can help mitigate overestimation bias, but may slow down initial learning
  - ϵ-greedy noise: Adding ϵ-greedy noise to the goal-conditioned policy ensures exploration, but may reduce the efficiency of reaching the goal state-action pair

- Failure signatures:
  - If the agent fails to learn an optimal policy, it may indicate that the S-function is not being learned accurately enough or that the exploration-exploitation threshold is not decaying properly
  - If the agent explores inefficiently, it may suggest that the goal-conditioned policy is not effectively guiding the agent towards unvisited state-action pairs

- First 3 experiments:
  1. Implement the basic Q-Learning algorithm with the proposed exploration strategy on a simple tabular MDP (e.g., Empty 6×6) and verify that the agent learns an optimal policy
  2. Modify the environment to include partially observable rewards (e.g., Random Monitor) and verify that the agent can still learn an optimal policy using the reward model
  3. Experiment with different values of β̄ and analyze its effect on the exploration-exploitation trade-off and the learning performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Directed Exploration via Successor-Functions compare to model-based approaches that explicitly plan to observe rewards, rather than uniformly exploring the state-action space?
- Basis in paper: [explicit] The authors mention that their exploration strategy could be combined with model-based methods where exploration is driven by uncertainty rather than visits, allowing the agent to plan visits to states where rewards are more likely to be observed.
- Why unresolved: The paper focuses on a model-free approach and does not directly compare its performance to model-based methods that explicitly plan for reward observation.
- What evidence would resolve it: Experiments comparing the proposed method to model-based approaches that explicitly plan for reward observation in Mon-MDPs, showing which approach achieves higher reward collection and faster learning.

### Open Question 2
- Question: Can the successor representation be effectively extended to continuous state and action spaces, and how would this impact the performance of Directed Exploration via Successor-Functions?
- Basis in paper: [explicit] The authors discuss the need to extend the S-function and visitation counts to continuous spaces for application to continuous MDPs, suggesting the use of neural networks and pseudocounts.
- Why unresolved: The paper only considers tabular MDPs and does not provide empirical results or theoretical analysis for the continuous case.
- What evidence would resolve it: Experiments applying the proposed method to continuous MDPs, showing whether the S-function can be effectively learned and whether the exploration strategy remains effective.

### Open Question 3
- Question: How does the choice of the goal-conditioned policy (e.g., ϵ-greedy over S-functions vs. other exploration strategies) affect the performance of Directed Exploration via Successor-Functions in Mon-MDPs?
- Basis in paper: [explicit] The authors mention that while any ϵ-greedy policy with ϵ > 0 is sufficient to meet the criteria of Theorem 1, the bound on the goal-relative diameter depends on the choice of the goal-conditioned policy, and they use ϵ-greedy over S-functions in their experiments.
- Why unresolved: The paper does not explore alternative goal-conditioned policies or compare their performance.
- What evidence would resolve it: Experiments comparing the performance of Directed Exploration via Successor-Functions using different goal-conditioned policies, such as balanced wandering or other exploration strategies, in Mon-MDPs.

## Limitations

- The method's theoretical guarantees rely on the communicating MDP assumption, which may not hold in more complex or non-communicating environments
- The effectiveness of the exploration strategy depends on the accuracy of the learned S-function and the goal-conditioned policy's ability to guide the agent towards unvisited state-action pairs
- The choice of the exploration-exploitation threshold βt and its decay schedule may significantly impact the learning performance and the balance between exploration and exploitation

## Confidence

- High: The paper's proposed exploration strategy based on the successor representation effectively decouples exploration from reward observability, enabling the agent to visit all state-action pairs uniformly in Mon-MDPs
- Medium: The convergence proof of the proposed algorithm under certain assumptions (communicating MDP, GLIE property) is valid, but may not hold in more complex or non-communicating environments
- Low: The choice of the exploration-exploitation threshold βt and its impact on the learning performance is not thoroughly analyzed, and the optimal decay schedule may vary across different environments

## Next Checks

1. Experiment with non-communicating MDPs and analyze the performance of the proposed algorithm, comparing it to baselines to assess the impact of the communicating MDP assumption on the exploration strategy
2. Investigate the effect of different exploration-exploitation threshold schedules (e.g., constant, linear, exponential decay) on the learning performance and the convergence rate of the proposed algorithm
3. Analyze the sensitivity of the proposed algorithm to the choice of hyperparameters (e.g., learning rate, discount factor, ϵ-greedy noise) and compare it to the sensitivity of baseline algorithms to identify potential areas for improvement or robustness