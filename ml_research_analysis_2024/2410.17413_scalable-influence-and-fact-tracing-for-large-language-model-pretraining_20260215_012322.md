---
ver: rpa2
title: Scalable Influence and Fact Tracing for Large Language Model Pretraining
arxiv_id: '2410.17413'
source_url: https://arxiv.org/abs/2410.17413
tags: []
core_contribution: The paper scales training data attribution (TDA) methods to 8B-parameter
  LLMs by refining gradient-based influence techniques for efficient retrieval from
  massive pretraining corpora. The proposed TrackStar method uses optimizer state
  correction, task-specific Hessian approximation, and normalized encodings to identify
  influential training examples for factual predictions.
---

# Scalable Influence and Fact Tracing for Large Language Model Pretraining

## Quick Facts
- **arXiv ID**: 2410.17413
- **Source URL**: https://arxiv.org/abs/2410.17413
- **Reference count**: 28
- **Primary result**: Scales gradient-based influence methods to 8B-parameter LLMs, finding that attribution and influence diverge but align more closely as models scale

## Executive Summary
This paper introduces TrackStar, a method that scales training data attribution (TDA) techniques to 8B-parameter large language models for identifying influential training examples. The method combines optimizer state correction, task-specific Hessian approximation, and normalized encodings to compute influence scores efficiently at scale. On a fact tracing task, TrackStar outperforms prior gradient-based methods at finding examples that influence model predictions, though classical retrieval methods like BM25 still excel at finding passages containing relevant facts. The work reveals a fundamental misalignment between factual attribution and causal influence, while showing that this gap narrows as model size and training scale increase.

## Method Summary
TrackStar computes influence scores by combining several key techniques: optimizer state correction using Adafactor's second moment estimates, random projection for dimensionality reduction, task-specific Hessian approximation through mixing train and evaluation gradient autocorrelation matrices, and unit normalization of gradient encodings. The method processes gradients from all training examples and target queries, applies corrections to preserve outlier components, projects to a manageable dimensionality (d=2^16), and computes influence as dot products of corrected, projected, and normalized gradients. This enables efficient computation of which training examples most influence factual predictions in 8B-parameter LLMs, despite the massive scale of pretraining corpora.

## Key Results
- TrackStar outperforms prior gradient-based methods at identifying influential training examples for factual predictions
- Classical retrieval methods (BM25, Gecko) excel at finding passages containing relevant facts, while gradient-based methods better capture causal influence
- As model size and training tokens increase, influential examples identified by TrackStar increasingly align with attribution (examples that entail facts)
- Influential examples include not only direct fact entailments but also prior-reinforcing passages and partial matches

## Why This Works (Mechanism)

### Mechanism 1
Gradient-based influence methods can identify influential training examples for LLM predictions at scale when combined with optimizer state correction, task-specific Hessian approximation, and normalized encodings. The method computes influence scores as dot products of projected, corrected, and unit-normalized gradients between training examples and target queries. Optimizer state correction (Adafactor's second moment estimates) provides per-parameter gradient magnitude correction before random projection, preserving outlier component information. The task-specific Hessian approximation downweights common gradient components across the target task (e.g., template text) while preserving task-specific signal. Unit normalization ensures the method focuses on task-specific rather than overall loss-influential examples.

### Mechanism 2
Attribution (finding examples that logically entail facts) and influence (finding examples that causally affect model predictions) are distinct, with classical retrieval methods better at attribution while gradient-based methods better capture influence. Attribution relies on finding training examples that contain or logically support the target fact, which classical methods like BM25 excel at through lexical matching and term frequency. Influence methods instead identify examples that would most change the model's prediction if removed or modified during training. These may include examples that reinforce priors, provide partial matches, or support multi-hop reasoning paths rather than directly stating facts.

### Mechanism 3
As models scale in parameters and training tokens, the influential examples identified by gradient-based methods increasingly align with attribution (examples that entail facts). Larger models with more training data develop more sophisticated representations that rely more heavily on direct factual knowledge rather than statistical priors or partial matches. The gradient-based influence method captures this shift by identifying examples that would have larger causal effects on model predictions, which for better models increasingly correspond to examples that directly entail facts.

## Foundational Learning

- **Concept**: Gradient-based influence methods and their computational challenges
  - Why needed here: Understanding how to compute and approximate influence functions at scale is central to the TrackStar method and its innovations.
  - Quick check question: What is the computational bottleneck when applying influence methods to LLM pretraining, and how does TrackStar address it?

- **Concept**: Hessian matrix approximation and its role in influence computation
  - Why needed here: The task-specific Hessian approximation is a key innovation in TrackStar, requiring understanding of how it affects gradient dot products and what information it captures.
  - Quick check question: How does the mixed task-specific Hessian approximation in TrackStar differ from the standard autocorrelation matrix approach, and what problem does it solve?

- **Concept**: Distinction between attribution and influence in machine learning interpretability
  - Why needed here: The paper's core finding about the misalignment between attribution and influence requires understanding both concepts and how to measure them differently.
  - Quick check question: What evaluation metrics distinguish attribution from influence, and why might examples that score well on one metric not score well on the other?

## Architecture Onboarding

- **Component map**: Factual queries and candidate training examples → Gradient computation → Optimizer state correction → Random projection → Task-specific Hessian approximation → Unit normalization → Influence scoring → Ranked influential examples

- **Critical path**: 
  1. Compute loss gradients for all training examples and queries
  2. Apply optimizer state correction (element-wise multiplication by Adafactor second moment estimates)
  3. Apply random projection to reduce dimensionality while preserving gradient information
  4. Compute task-specific Hessian approximation by mixing train and eval gradient autocorrelation matrices
  5. Calculate influence scores as dot products of corrected, projected, and normalized gradients
  6. Rank training examples by influence scores for each query

- **Design tradeoffs**:
  - Random projection dimensionality vs. memory/computation cost: Higher dimensions preserve more information but increase storage (87TB for 365M C4 examples at 216 dimensions) and computation costs
  - Hessian approximation complexity vs. accuracy: Full Hessian is intractable, so approximations like autocorrelation matrices or optimizer state corrections are used
  - Task-specific vs. general Hessian: Task-specific Hessian downweights common components but requires additional computation and hyperparameter tuning (mixing parameter λ)

- **Failure signatures**:
  - Retrieval dominated by long, repetitive passages with high gradient magnitudes but low relevance
  - Poor performance on attribution metrics (MRR, recall) despite good influence scores
  - Sensitivity to hyperparameter choices, particularly the Hessian mixing parameter λ
  - Degradation in performance when scaling to very high projection dimensions or extremely large corpora

- **First 3 experiments**:
  1. Replicate the ablation experiments from Table 1 to verify the importance of each correction term (optimizer state correction, unit normalization, task-specific Hessian approximation)
  2. Test the effect of different random projection dimensionalities on attribution and influence performance to find the optimal balance between accuracy and computational cost
  3. Compare attribution vs. influence performance across different model sizes (154M, 1B, 8B parameters) to verify the scaling relationship described in section 5.3

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the alignment between attribution and influence continue to improve as models scale beyond 8B parameters?
- Basis in paper: [inferred] The paper shows that influence aligns more closely with attribution as model size and training tokens increase, suggesting this trend may continue.
- Why unresolved: The paper only tests models up to 8B parameters, leaving uncertainty about whether the trend holds for larger models.
- What evidence would resolve it: Testing TrackStar on models significantly larger than 8B parameters (e.g., 70B+ parameters) and measuring the MRR and tail-patch scores to see if attribution and influence converge further.

### Open Question 2
- Question: How does the optimal mixing parameter λ for the task-specific Hessian approximation vary across different tasks and datasets?
- Basis in paper: [explicit] The paper uses λ = 0.90 for T-REx and λ = 0.99 for C4, but notes that determining the optimal λ is an interesting direction for future investigation.
- Why unresolved: The paper selects λ based on empirical performance but does not explore how it should be chosen for other tasks or datasets.
- What evidence would resolve it: Conducting experiments across diverse tasks and datasets to determine how λ should be tuned for optimal performance in each case.

### Open Question 3
- Question: Can the headroom in attribution be reduced by incorporating multi-hop reasoning or other non-entailing but supportive examples into the retrieval process?
- Basis in paper: [explicit] The paper identifies that many influential examples support priors or partial matches rather than fully entailing the fact, suggesting these types of examples could be leveraged.
- Why unresolved: The paper analyzes the types of influential examples but does not test whether incorporating them explicitly improves attribution metrics.
- What evidence would resolve it: Modifying TrackStar or baseline methods to explicitly retrieve and score multi-hop reasoning or partial match examples, then measuring the impact on MRR and recall.

## Limitations

- **Computational scalability**: The method requires storing and processing 87 TB of influence scores for 365M C4 passages at 216-dimensional projections, raising questions about practical deployment costs despite theoretical scalability claims.
- **Attribution vs. influence alignment**: While the paper demonstrates misalignment between attribution and influence metrics, it doesn't fully explain why classical retrieval methods excel at attribution, and evaluation relies on lexical matching assumptions.
- **Hessian approximation validity**: The task-specific Hessian approximation using mixed autocorrelation matrices is empirically motivated but lacks theoretical justification, and sensitivity to the mixing parameter λ introduces potential instability.

## Confidence

**High confidence**: The core finding that gradient-based influence methods scale to 8B-parameter LLMs when combined with optimizer state correction, random projection, and normalized encodings is well-supported by ablation studies showing each component's importance.

**Medium confidence**: The claim that classical retrieval methods are better at attribution while gradient-based methods capture influence more accurately is supported but relies on specific evaluation metrics that may not fully capture the semantic distinction between these concepts.

**Low confidence**: The explanation for why attribution and influence diverge (e.g., that gradient-based methods capture multi-hop reasoning paths while retrieval methods find direct entailments) is speculative and not directly tested.

## Next Checks

1. **Ablation study with alternative Hessian approximations**: Test the method using different Hessian approximation techniques (e.g., diagonal approximation, low-rank approximation) to verify that the specific mixing approach is critical for performance rather than just any Hessian correction.

2. **Human evaluation of attribution vs. influence**: Conduct human studies where annotators judge whether retrieved examples actually contain the target fact (attribution) versus whether removing them would change model predictions (influence), to validate the automatic metrics used.

3. **Scaling analysis with controlled model improvements**: Train models with controlled architectural differences (e.g., different attention mechanisms, different embedding sizes) while holding scale constant to isolate whether the attribution-influence alignment is driven by scale itself or by specific architectural improvements that correlate with scale.