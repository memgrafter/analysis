---
ver: rpa2
title: 'EPS-MoE: Expert Pipeline Scheduler for Cost-Efficient MoE Inference'
arxiv_id: '2410.12247'
source_url: https://arxiv.org/abs/2410.12247
tags:
- communication
- computation
- pipeline
- number
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EPS-MoE, a novel expert pipeline scheduler
  for efficient MoE model inference that optimizes the computation of MoE FFN modules
  by dynamically selecting the best kernel implementation (GroupGemm vs DenseGemm)
  for different loads and adaptively overlapping these computations with communication.
  The approach addresses throughput bottlenecks in MoE inference caused by computational
  efficiency and communication overhead challenges.
---

# EPS-MoE: Expert Pipeline Scheduler for Cost-Efficient MoE Inference

## Quick Facts
- **arXiv ID**: 2410.12247
- **Source URL**: https://arxiv.org/abs/2410.12247
- **Reference count**: 40
- **Primary result**: Achieves 21% average improvement in prefill throughput for MoE models through expert pipeline scheduling

## Executive Summary
EPS-MoE introduces an expert pipeline scheduler that optimizes Mixture-of-Experts (MoE) inference by dynamically selecting between GroupGemm and DenseGemm kernel implementations based on token load, while overlapping computation with communication. The approach addresses key bottlenecks in MoE inference: computational efficiency and communication overhead. By using horizontal data partitioning and expert-level pipelining, EPS-MoE improves prefill throughput by 21% on average, with specific acceleration of DeepSeekV2 from 100K to at least 120K tokens per second. The framework leverages data parallelism for attention mechanisms and expert parallelism for MoE components, enabling efficient scaling across multiple GPUs.

## Method Summary
EPS-MoE implements a DP+EP parallel strategy where the attention part uses Data Parallelism while the MoE part uses Expert Parallelism. The core innovation is an expert pipeline scheduler that dynamically selects optimal kernel implementations (GroupGemm vs DenseGemm) based on input token counts, and adaptively overlaps these computations with all2all communication. The scheduler horizontally partitions the input tensor and processes experts sequentially in a pipeline, enabling better utilization of GPU resources. The method also controls SM allocation to enable computation-communication overlapping, improving overall hardware resource utilization.

## Key Results
- Achieves 21% average improvement in prefill throughput compared to existing parallel inference methods
- Accelerates DeepSeekV2 from 100K to at least 120K tokens per second
- Demonstrates effective computation-communication overlapping through SM resource control
- Shows horizontal data partitioning reduces I/O overhead compared to vertical splitting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expert Pipeline Scheduler enables better utilization of DenseGemm vs GroupGemm based on token load
- Mechanism: Horizontal splitting with sequential expert processing allows dynamic switching between GroupGemm (efficient for small token counts) and DenseGemm (efficient for large token counts), while overlapping with communication
- Core assumption: Optimal kernel implementation changes with input scale and performance difference varies significantly across token counts
- Evidence anchors:
  - [abstract] "Our approach optimizes the computation... by dynamically selecting the best kernel implementation of GroupGemm and DenseGemm for different loads"
  - [section] "We designed a load-aware adaptive scheduling strategy to dynamically select different efficient implementations based on the type of load"
- Break condition: When token count falls in middle range (2048-3584 tokens per expert) where GroupGemm and DenseGemm performance is roughly equivalent

### Mechanism 2
- Claim: Horizontal data partitioning with expert-level pipelining reduces memory I/O overhead
- Mechanism: Horizontal splitting organizes input data by row partitioning, resulting in less I/O data volume compared to vertical splitting while enabling pipeline parallelism
- Core assumption: Horizontal split results in much less I/O data volume compared to vertical split
- Evidence anchors:
  - [section] "Method (a) results in much less I/O data volume compared to method (b)" and Figure 8
  - [abstract] "Our approach... by dynamically selecting the best kernel implementation... and adaptively overlapping these computations with all2all communication"
- Break condition: When expert count per device is very small or model architecture makes vertical partitioning more efficient

### Mechanism 3
- Claim: Computation-communication overlapping at kernel level improves hardware resource utilization
- Mechanism: Controlling SM allocation leaves SMs available for communication operations, enabling parallel execution without significantly affecting computation efficiency
- Core assumption: "Controlling the number of SMs occupied by some kernels, we can leave a portion of SMs for other kernels while keeping the kernel's execution time unchanged"
- Evidence anchors:
  - [section] "Adjusting the number of SMs allocated to communication kernels has almost no impact on communication throughput" and Figure 7
  - [abstract] "Our approach... by dynamically selecting the best kernel implementation... and adaptively overlapping these computations with all2all communication"
- Break condition: When communication volume is too small to benefit from overlapping or kernel implementation doesn't support flexible SM allocation

## Foundational Learning

- **Concept: Mixture-of-Experts (MoE) architecture and conditional computation**
  - Why needed here: Paper builds on understanding how MoE models activate only subset of experts per input, creating optimization opportunities
  - Quick check question: How does the gating mechanism in MoE models determine which experts to activate for a given input?

- **Concept: Parallelism strategies in deep learning (DP, TP, PP, EP)**
  - Why needed here: Paper compares different parallelism approaches and chooses specific combination (DP+EP) for optimal performance
  - Quick check question: What are tradeoffs between Tensor Parallelism (TP) and Expert Parallelism (EP) in terms of communication overhead and memory usage?

- **Concept: GPU architecture and memory hierarchy (SMs, Tensor Cores, memory bandwidth)**
  - Why needed here: Optimization strategies rely on understanding how different kernels utilize GPU resources and how to control resource allocation
  - Quick check question: How do Streaming Multiprocessors (SMs) relate to execution of GEMM operations on NVIDIA GPUs?

## Architecture Onboarding

- **Component map**: Input routing -> Horizontal partitioning -> Sequential expert computation -> Communication overlapping -> Local reduction -> Output assembly
- **Critical path**:
  1. Input routing through gating mechanism
  2. Horizontal partitioning of token assignments to experts
  3. Sequential expert computation with GEMM kernel selection
  4. Communication overlapping with computation pipeline
  5. Local reduction and output assembly
- **Design tradeoffs**:
  - GroupGemm vs DenseGemm: Memory efficiency vs computational throughput
  - Pipeline depth: More pipelines enable better overlapping but increase scheduling overhead
  - SM allocation: More SMs for computation vs more for communication
  - Horizontal vs vertical partitioning: I/O volume vs computation flexibility
- **Failure signatures**:
  - Suboptimal throughput when token count falls in middle performance range of GEMM kernels
  - Increased latency when pipeline overhead exceeds overlapping benefits
  - Communication bottlenecks when expert load balancing is poor
  - Resource underutilization when SM allocation is not properly tuned
- **First 3 experiments**:
  1. Benchmark performance difference between GroupGemm and DenseGemm across different token counts to validate switching threshold
  2. Test horizontal vs vertical partitioning strategies with varying expert counts to measure I/O overhead differences
  3. Experiment with different pipeline depths (1, 5, 10, 20) to find optimal pipeline number for overlapping benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does optimal pipeline number vary across different MoE architectures and expert configurations?
- Basis in paper: [explicit] Paper discusses impact of pipeline number on performance and provides analytical framework, but notes actual optimal value may differ due to overhead
- Why unresolved: Optimal pipeline number depends on multiple factors including hardware, kernel implementation, model parameters, and inference task load
- What evidence would resolve it: Empirical testing across various MoE architectures with different numbers of experts, expert dimensions, and top-k values

### Open Question 2
- Question: What is impact of load imbalance on effectiveness of EPS-MoE's expert pipeline scheduling?
- Basis in paper: [inferred] Paper mentions communication time greatly affected by expert balance but doesn't explore how load imbalance specifically impacts EPS-MoE performance
- Why unresolved: Paper focuses on benefits under ideal conditions but doesn't extensively analyze scenarios with significant load imbalance
- What evidence would resolve it: Testing EPS-MoE on MoE models with deliberately imbalanced expert loads to measure performance degradation

### Open Question 3
- Question: How does EPS-MoE scale with increasing numbers of GPUs and across multiple machines?
- Basis in paper: [inferred] Paper discusses communication overhead in multi-GPU settings and mentions multi-machine strategies but doesn't provide experimental results for scaling beyond single machine
- Why unresolved: Paper's experiments limited to configurations with up to 8 GPUs within single machine
- What evidence would resolve it: Benchmarking EPS-MoE on multi-machine setups with varying numbers of GPUs to measure performance scaling

## Limitations
- Performance comparisons limited to DeepSeekV2 without broader benchmarking across multiple MoE architectures
- Experimental validation restricted to narrow set of conditions (8 experts), raising questions about generalizability to larger models
- Communication-computation overlapping claims rely on precise SM control mechanisms not fully specified in the paper

## Confidence

**High Confidence**: Fundamental approach of horizontal tensor splitting with pipeline parallelism is well-supported by theoretical analysis with clear logical foundations. Performance difference between GroupGemm and DenseGemm kernels across token counts is empirically verifiable.

**Medium Confidence**: 21% average throughput improvement claim supported by presented experiments but would benefit from broader validation across different model sizes, expert counts, and hardware configurations. Communication-computation overlapping mechanism appears sound but lacks detailed implementation specifications.

**Low Confidence**: Generalizability of results to models with significantly different expert counts (e.g., 64 vs 8 experts) and to different MoE architectures beyond DeepSeekV2 is uncertain. Optimal pipeline depth selection algorithm is not fully specified.

## Next Checks

1. **Cross-Model Validation**: Test EPS-MoE on MoE models with different expert counts (16, 32, 64) and architectures beyond DeepSeekV2 to verify the 21% improvement claim holds across diverse configurations.

2. **Edge Case Performance**: Evaluate EPS-MoE performance in scenarios where token counts fall in middle performance range (2048-3584 tokens per expert) where GroupGemm and DenseGemm are roughly equivalent, and identify conditions where pipeline overhead might outweigh benefits.

3. **Hardware Architecture Generalization**: Implement and test SM allocation control mechanisms on different GPU architectures (beyond NVIDIA Hopper) to verify that computation-communication overlapping strategy is portable and effective across hardware platforms.