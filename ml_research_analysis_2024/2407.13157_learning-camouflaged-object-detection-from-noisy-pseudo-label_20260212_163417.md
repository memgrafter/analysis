---
ver: rpa2
title: Learning Camouflaged Object Detection from Noisy Pseudo Label
arxiv_id: '2407.13157'
source_url: https://arxiv.org/abs/2407.13157
tags:
- camouflaged
- learning
- noisy
- training
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a weakly semi-supervised camouflaged object
  detection (WSSCOD) method that leverages box annotations as prompts and a limited
  number of fully labeled images to generate high-quality pseudo labels for training.
  The method addresses the challenge of learning from noisy pseudo labels by proposing
  a noise correction loss (LN C) that adapts to different learning phases: in the
  early learning stage, it accelerates convergence to correct pixels, and in the memorization
  stage, it maintains correct learning direction even with up to 50% noisy pixels.'
---

# Learning Camouflaged Object Detection from Noisy Pseudo Label

## Quick Facts
- arXiv ID: 2407.13157
- Source URL: https://arxiv.org/abs/2407.13157
- Authors: Jin Zhang; Ruiheng Zhang; Yanjiao Shi; Zhe Cao; Nian Liu; Fahad Shahbaz Khan
- Reference count: 40
- This paper introduces a weakly semi-supervised camouflaged object detection (WSSCOD) method that leverages box annotations as prompts and a limited number of fully labeled images to generate high-quality pseudo labels for training.

## Executive Summary
This paper addresses the challenge of learning camouflaged object detection (COD) from limited labeled data by proposing a weakly semi-supervised framework that uses box annotations as prompts. The method generates pseudo labels from a small set of fully annotated images and leverages these to train on a larger set with only box annotations. The key innovation is a noise correction loss (LN C) that adapts to different learning phases, enabling effective learning even with up to 50% noisy pseudo labels. The approach achieves state-of-the-art performance while using only 20% of fully labeled data.

## Method Summary
The WSSCOD framework consists of two stages: first, an auxiliary network (ANet) is trained on fully labeled data to generate pseudo labels for the box-annotated subset; second, a primary network (PNet) is trained on both the fully labeled data and the pseudo-labeled data using the proposed noise correction loss (LN C). The method exploits the complementarity between RGB images and box proposals to reduce camouflage complexity and uses an adaptive loss function that switches between IoU-form and MAE-form based on the learning phase. The framework demonstrates significant performance improvements over existing weakly supervised methods while using only 20% of fully labeled data.

## Key Results
- Achieves 79.6%, 16.3%, 18.1%, and 16.0% average improvements in M, Eφ, Fβ, and Sα metrics respectively compared to existing weakly supervised methods when using only 20% fully labeled data
- Maintains correct learning direction even with up to 50% incorrect noisy pseudo labels through the noise correction loss
- Demonstrates superior scalability and effectiveness in handling noisy labels compared to state-of-the-art methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The noise correction loss (LN C) maintains correct learning direction by equalizing gradients for all pixels in the memorization phase.
- Mechanism: By setting q=1 in LN C, the gradient for each predicted pixel pi becomes sign(pi - gi), which is identical for all pixels regardless of noise level. This prevents noisy pixels from dominating the gradient direction.
- Core assumption: The spatial correlation between correct and noisy pixels can be exploited to maintain overall gradient direction.
- Evidence anchors:
  - [abstract] "in the memorization stage, LN C forms a unified risk gradient for different predictions, maintaining the correct learning direction on up to 50% incorrect noisy pseudo labels"
  - [section 3.2] "LN C transforms into a MAE-form loss, which can guide the PNet to optimize in the right direction"
  - [corpus] Weak evidence - neighboring papers focus on different aspects of COD, no direct evidence for LN C's gradient equalization

### Mechanism 2
- Claim: Box annotations serve as effective prompts by masking complex backgrounds and indicating object locations.
- Mechanism: The box annotation is multiplied with the RGB image to create a proposal that masks out complex backgrounds, reducing camouflage complexity and simplifying the model's search process for camouflaged objects.
- Core assumption: Box annotations contain sufficient object information to guide segmentation while being more cost-effective than full pixel-level annotations.
- Evidence anchors:
  - [abstract] "utilizing box annotations as prompts for camouflaged object segmentation... offer reliability by 1) masking complex backgrounds and reducing the level of camouflage, and 2) indicating the approximate location of the object"
  - [section 1] "box annotations offer rich object information and are as cost-effective to provide as image-level and point-level annotations"
  - [corpus] Weak evidence - neighboring papers focus on scribble and point supervision, not box prompts

### Mechanism 3
- Claim: The WSSCOD framework generates high-quality pseudo labels by leveraging complementary information from image and box branches.
- Mechanism: The auxiliary network (ANet) is trained with both image and box inputs, generating proposals that are merged with complete images to create complementary branches. This reduces the impact of imprecise box locations on the model's decision boundaries.
- Core assumption: The complementarity between RGB image features and box-proposed features enhances the model's ability to distinguish camouflaged objects.
- Evidence anchors:
  - [section 2.1] "we develop a simple and effective COD model for exploiting the complementarity between the RGB image and the proposals"
  - [section 1] "we merge these proposals with the complete image to create complementary branches"
  - [corpus] No direct evidence - neighboring papers don't discuss this specific complementary branch approach

## Foundational Learning

- Concept: Camouflaged Object Detection (COD) fundamentals
  - Why needed here: Understanding the unique challenges of COD (high intrinsic similarity between foreground and background) is crucial for grasping why traditional weakly supervised methods fail and why the proposed WSSCOD approach is necessary.
  - Quick check question: What distinguishes camouflaged objects from regular objects in terms of visual appearance, and why does this make detection particularly challenging?

- Concept: Weakly supervised learning with noisy labels
  - Why needed here: The paper addresses learning from noisy pseudo labels, a common problem in weakly supervised settings where annotation quality is limited. Understanding how noise affects learning and how to mitigate it is central to the LN C loss design.
  - Quick check question: How does the presence of noisy labels affect the gradient direction during training, and what are common strategies to address this issue?

- Concept: Semi-supervised learning framework design
  - Why needed here: WSSCOD combines fully labeled data (20%) with box-annotated data (80%) to generate pseudo labels. Understanding how to effectively leverage both types of supervision is key to implementing the framework.
  - Quick check question: What are the key design considerations when creating a semi-supervised framework that combines different annotation types (full masks, boxes) for a segmentation task?

## Architecture Onboarding

- Component map:
  - Auxiliary Network (ANet): Dual-branch encoder (image + box), frequency transformer, reverse fusion decoder
  - Primary Network (PNet): Single-stream encoder (image only), decoder
  - LN C Loss: Adaptive loss function with q parameter switching between IoU-form (q=2) and MAE-form (q=1)

- Critical path:
  1. Train ANet on fully labeled data with box annotations as prompts
  2. Use trained ANet to generate pseudo labels for box-only annotated data
  3. Train PNet on combined dataset (fully labeled + pseudo labeled) using LN C loss
  4. Switch q parameter in LN C from 2 to 1 at appropriate epoch based on noise level

- Design tradeoffs:
  - Box vs. point/scribble annotations: Boxes provide more context but may be less precise at object boundaries
  - Dual-branch vs. single-branch ANet: Dual-branch captures complementarity but increases complexity
  - LN C q parameter scheduling: Early learning benefits from IoU-form (q=2), memorization benefits from MAE-form (q=1)

- Failure signatures:
  - Noisy pseudo labels with poor boundary adherence indicate issues with ANet training or box annotation quality
  - Model performance plateaus or degrades over time suggests incorrect q parameter switching timing
  - Inferior performance compared to fully supervised methods indicates insufficient exploitation of box-annotation complementarity

- First 3 experiments:
  1. Train ANet with varying box annotation quality (precise vs. imprecise) and evaluate pseudo label quality
  2. Test different q parameter switching schedules (epochs 20, 40, 60) for PNetF 1, PNetF 5, PNetF 10, PNetF 20 setups
  3. Compare LN C against standard CE and IoU losses on noisy vs. clean label subsets to validate noise robustness claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the WSSCOD method scale when using more than 20% fully labeled data, and is there a point of diminishing returns?
- Basis in paper: [explicit] The paper tests WSSCOD with 1%, 5%, 10%, and 20% fully labeled data, but does not explore beyond 20%.
- Why unresolved: The paper only evaluates up to 20% fully labeled data, leaving the scalability and performance limits of the method unexplored.
- What evidence would resolve it: Conducting experiments with 30%, 40%, or even 50% fully labeled data to determine if performance gains continue or plateau.

### Open Question 2
- Question: How does the noise correction loss (LN C) perform on other segmentation tasks beyond camouflaged object detection, such as medical imaging or satellite imagery?
- Basis in paper: [inferred] The paper suggests that LN C is versatile and effective for noisy labels, but only demonstrates its effectiveness on COD tasks.
- Why unresolved: The paper focuses on COD tasks and does not test LN C on other segmentation domains, leaving its generalizability unproven.
- What evidence would resolve it: Applying LN C to other segmentation tasks with noisy labels and comparing its performance to existing loss functions.

### Open Question 3
- Question: What is the impact of different box annotation qualities (e.g., precise vs. imprecise) on the final performance of the WSSCOD method?
- Basis in paper: [inferred] The paper mentions that the accuracy of box annotations affects the results, but does not quantify the impact of annotation quality.
- Why unresolved: The paper acknowledges the issue but does not provide a detailed analysis of how varying box annotation quality influences performance.
- What evidence would resolve it: Conducting experiments with box annotations of varying quality and measuring their impact on the final segmentation results.

## Limitations

- The exact implementation details of the Frequency Transformer module are not fully specified, which could affect reproducibility
- The paper lacks comprehensive ablation studies isolating the contribution of each component (box prompts, dual-branch architecture, LN C loss)
- No visualization or analysis of how LN C gradients actually behave during training, despite claims about gradient equalization

## Confidence

- **High**: The overall WSSCOD framework design and its potential to reduce annotation costs
- **Medium**: The noise correction loss mechanism and its adaptive behavior across learning phases
- **Low**: The specific implementation details required for exact reproduction

## Next Checks

1. Implement ablation studies comparing LN C against standard losses (CE, IoU) on datasets with controlled noise levels to validate the 50% noise tolerance claim
2. Visualize and analyze the gradient distributions during training to confirm that LN C actually achieves gradient equalization as claimed
3. Test the framework's scalability by evaluating performance degradation when reducing fully labeled data from 20% to 10% and 5% to determine the practical annotation budget requirements