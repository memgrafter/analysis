---
ver: rpa2
title: Could We Have Had Better Multilingual LLMs If English Was Not the Central Language?
arxiv_id: '2402.13917'
source_url: https://arxiv.org/abs/2402.13917
tags:
- languages
- translation
- language
- data
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates Llama2's multilingual translation capabilities,
  finding that while it translates well into languages it was trained on (BLEU scores
  10), performance drops significantly for unseen languages. The study models a linear
  relationship between linguistic feature distances and translation scores, revealing
  that syntactic similarity is not the only important factor for translation quality.
---

# Could We Have Had Better Multilingual LLMs If English Was Not the Central Language?

## Quick Facts
- arXiv ID: 2402.13917
- Source URL: https://arxiv.org/abs/2402.13917
- Reference count: 0
- This paper evaluates Llama2's multilingual translation capabilities, finding that while it translates well into languages it was trained on (BLEU scores > 10), performance drops significantly for unseen languages.

## Executive Summary
This paper investigates whether English's dominance as the central language in multilingual LLMs is optimal by evaluating Llama2's translation capabilities across 25 languages not seen during training. The study finds that while scaling model parameters significantly improves translation into unseen languages, linguistic similarity to English is not the only factor determining translation quality. Surprisingly, some low-resource languages like Swedish and Catalan show comparable correlation levels to English despite having significantly less training data, suggesting that centering multilingual models around alternative languages could lead to more efficient architectures.

## Method Summary
The researchers used Llama2 models (7B, 13B, and their chat variants) to evaluate translation quality using the FLORES-200 benchmark. They conducted one-shot and five-shot translation experiments, calculating BLEU and COMET-22 scores. Linguistic distances between languages were computed using the URIEL typological database, and Pearson correlation analysis was performed between these distances and translation scores. The study specifically analyzed languages that were seen during training (inllama) versus those that were not (outllama), examining correlation patterns across different language families including Germanic, Romance, Slavic, and others.

## Key Results
- Scaling model parameters improves translation into unseen languages more effectively than instruction tuning or few-shot learning
- Syntactic similarity is not the only linguistic factor strongly correlated with translation performance; genetic and phonological distances also play significant roles
- Languages like Swedish and Catalan, despite having significantly less training data, exhibit comparable correlation levels to English for translation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling model parameters improves translation into unseen languages more effectively than instruction tuning or few-shot learning.
- Mechanism: Larger models have greater capacity to encode and generalize linguistic patterns, enabling better transfer learning across languages, especially for low-resource or unseen languages.
- Core assumption: Model capacity is the primary bottleneck for multilingual translation performance, not the quality or diversity of the training data.
- Evidence anchors:
  - [abstract] "Most translation improvements into unseen languages come from scaling up the model size rather than instruction tuning or increasing shot count."
  - [section] "Scaling up the model enhances translation ability. However, improvements from instruction-tuning and adding shot count remain inconclusive."
- Break condition: If the model size is scaled beyond a certain point, the marginal gains in translation quality may diminish, or the model may overfit to the training data, reducing its ability to generalize to unseen languages.

### Mechanism 2
- Claim: Syntactic similarity is not the only linguistic factor strongly linked to machine translation performance; other features like genetic and phonological distances also play a significant role.
- Mechanism: Languages that share genetic or phonological features, even if they are not syntactically similar, may have structural or phonetic patterns that facilitate translation.
- Core assumption: Linguistic features beyond syntax, such as genetic and phonological similarities, can provide valuable information for translation models to leverage.
- Evidence anchors:
  - [abstract] "Our correlation analysis reveals that syntactic similarity is not the only linguistic factor that strongly correlates with machine translation scores."
  - [section] "When considering only outllama language subset, translation performance seems to have higher correlations (either positive or negative) with GENETIC and PHONOLOGICAL distances."
- Break condition: If the model is not able to effectively capture and utilize the genetic or phonological features, the correlation between these features and translation performance may weaken.

### Mechanism 3
- Claim: Languages other than English, such as Swedish and Catalan, can exhibit comparable correlation levels to English in terms of their importance for multilingual translation, despite having significantly less training data.
- Mechanism: Languages that share certain linguistic features with multiple other languages may serve as effective hubs for multilingual translation, even if they are not as widely spoken or have less training data.
- Core assumption: The importance of a language for multilingual translation is not solely determined by the amount of training data it has, but also by its linguistic features and its ability to facilitate translation between other languages.
- Evidence anchors:
  - [abstract] "Interestingly, we discovered that under specific circumstances, some languages (e.g., Swedish, Catalan), despite having significantly less training data, exhibit comparable correlation levels to English."
  - [section] "When considering only outllama languages, translation performance seems to have higher correlations (either positive or negative) with GENETIC and PHONOLOGICAL distances."
- Break condition: If the model is not able to effectively capture and utilize the linguistic features of these alternative hub languages, their importance for multilingual translation may diminish.

## Foundational Learning

- Concept: Linguistic features and their impact on translation performance
  - Why needed here: Understanding the various linguistic features and their relationship to translation performance is crucial for designing and evaluating multilingual translation models.
  - Quick check question: What are some linguistic features beyond syntax that can impact translation performance, and how might they be leveraged by a multilingual translation model?

- Concept: Model scaling and its effects on translation quality
  - Why needed here: Determining the optimal model size for a given multilingual translation task is essential for balancing performance and computational efficiency.
  - Quick check question: How does increasing the model size affect translation quality for both seen and unseen languages, and what are the potential trade-offs involved?

- Concept: Instruction tuning and few-shot learning for multilingual translation
  - Why needed here: Understanding the effectiveness of instruction tuning and few-shot learning techniques for improving translation quality in low-resource or unseen languages is important for developing efficient and scalable multilingual translation models.
  - Quick check question: How do instruction tuning and few-shot learning techniques compare to model scaling in terms of their impact on translation quality for unseen languages, and what are the potential limitations of these approaches?

## Architecture Onboarding

- Component map: Language model backbone (Llama2) -> Translation head -> Linguistic feature extraction module
- Critical path: Encoding input text by language model backbone â†’ Generation of translated output by translation head (linguistic feature extraction runs in parallel)
- Design tradeoffs: Model size vs. computational efficiency; instruction tuning/few-shot learning vs. model scaling effectiveness
- Failure signatures: Overfitting to training data (poor generalization to unseen languages); inability to capture linguistic features effectively
- First 3 experiments:
  1. Evaluate translation quality for seen and unseen languages using BLEU and COMET-22 metrics
  2. Analyze correlation between linguistic features (syntactic, genetic, phonological) and translation performance
  3. Compare effectiveness of model scaling, instruction tuning, and few-shot learning for unseen language translation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does scaling model parameters consistently improve translation quality for all unseen languages, or are there diminishing returns for certain language families?
- Basis in paper: [explicit] The paper shows that scaling up the model enhances translation ability for unseen languages, but improvements vary by language and there are languages where prospects are limited.
- Why unresolved: The paper only tested up to 13B models due to limited compute resources and did not provide a comprehensive analysis across all language families.
- What evidence would resolve it: Testing larger model sizes (e.g., 30B, 70B) on a wider range of unseen languages, particularly from different language families, to identify patterns in scaling effectiveness.

### Open Question 2
- Question: What specific linguistic features beyond syntactic similarity most strongly correlate with machine translation performance, and how do these vary across language families?
- Basis in paper: [explicit] The paper reveals that syntactic similarity is not the only linguistic factor strongly linked to MT performance, with genetic and phonological distances showing stronger correlations for certain language subsets.
- Why unresolved: The analysis only examined correlations with predefined URIEL feature distances and did not investigate other potential linguistic factors or their interactions.
- What evidence would resolve it: Conducting a more granular analysis of additional linguistic features (e.g., morphological complexity, semantic distance) and performing feature importance analysis to identify the most predictive factors for each language family.

### Open Question 3
- Question: Would centering multilingual LLMs around languages other than English (e.g., Swedish or Catalan) lead to more efficient models with better translation performance for low-resource languages?
- Basis in paper: [explicit] The paper discovered that some languages with less training data (e.g., Swedish, Catalan) exhibit comparable correlation levels to English, suggesting English may not be the optimal central language.
- Why unresolved: The paper only analyzed correlation patterns and did not actually train or evaluate models centered around alternative languages.
- What evidence would resolve it: Training and evaluating multilingual LLMs with different central languages and comparing their performance on low-resource language translation tasks to determine if non-English-centered models are indeed more efficient.

## Limitations
- Limited testing to only 25 unseen languages out of hundreds of possible languages
- Potential biases in the FLORES-200 benchmark that may not represent real-world translation needs
- Focus on Western European languages in the correlation analysis

## Confidence
- Model scaling effects: High confidence based on direct experimental evidence
- Correlation analysis: Medium confidence due to potential limitations in URIEL database coverage
- Alternative central languages: Medium confidence based on correlation analysis but lacking extensive validation

## Next Checks
1. Test the scaling hypothesis across different multilingual model families (e.g., BLOOM, mT5) to verify if parameter scaling consistently outperforms instruction tuning for unseen languages
2. Expand the unseen language set to include non-European languages from diverse families (e.g., Sino-Tibetan, Afro-Asiatic) to validate the correlation findings
3. Conduct human evaluation studies comparing translation quality between English-centered and alternative language-centered multilingual models