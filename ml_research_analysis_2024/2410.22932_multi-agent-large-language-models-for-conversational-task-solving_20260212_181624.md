---
ver: rpa2
title: Multi-Agent Large Language Models for Conversational Task-Solving
arxiv_id: '2410.22932'
source_url: https://arxiv.org/abs/2410.22932
tags:
- discussion
- agents
- multi-agent
- tasks
- personas
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies multi-agent large language models (LLMs) for
  conversational task-solving, identifying key strengths and weaknesses. While multi-agent
  systems improve reasoning and ethical alignment in complex tasks, they underperform
  on basic tasks like translation due to "problem drift," where extended discussions
  lead to misalignment with task requirements.
---

# Multi-Agent Large Language Models for Conversational Task-Solving

## Quick Facts
- arXiv ID: 2410.22932
- Source URL: https://arxiv.org/abs/2410.22932
- Authors: Jonas Becker
- Reference count: 40
- Key outcome: Multi-agent LLMs improve complex reasoning but underperform on basic tasks due to problem drift, alignment collapse, and discussion monopolization

## Executive Summary
This paper investigates multi-agent large language models (LLMs) for conversational task-solving, identifying both strengths and critical weaknesses. While multi-agent systems excel at complex reasoning tasks by leveraging expert personas and iterative consensus, they struggle with basic tasks like translation due to conversational drift. The study reveals three main challenges: problem drift in simple tasks, alignment collapse during prolonged ethical discussions, and monopolization in summarization tasks. The research introduces a taxonomy of 20 multi-agent systems and proposes the MALLM framework for deploying these systems, highlighting the need for shorter discussions, centralized paradigms for ethical alignment, and balanced agent contributions.

## Method Summary
The study employs the MALLM framework using Meta-Llama-3-70B-Instruct model with four discussion paradigms (memory, relay, report, debate) and consensus-based decision making. Six datasets are evaluated: XSum (summarization), WMT19 German-English (translation), ETPC (paraphrase generation), SQuAD 2.0 (extractive QA), Simple Ethical Questions (ethical QA), and StrategyQA (strategic reasoning). Task-specific metrics include ROUGE, BLEU, BERTScore, F1, Exact Match, Accuracy, and Distinct-n for lexical diversity. Experiments compare multi-agent performance against Chain-of-Thought baseline across various tasks, analyzing discussion convergence and agent impact patterns.

## Key Results
- Multi-agent systems outperform single LLMs on complex reasoning tasks by leveraging iterative consensus and diverse expert personas
- Longer discussions lead to problem drift and alignment collapse, harming performance on basic tasks like translation
- Centralized discussion paradigms with information restrictions improve ethical alignment in multi-agent systems

## Why This Works (Mechanism)

### Mechanism 1
Multi-agent systems outperform single LLMs on complex reasoning tasks by leveraging iterative consensus and diverse expert personas. Multiple agents with specialized personas collaboratively refine solutions through turn-based discussion, with each agent providing feedback and proposing improvements until consensus is reached.

### Mechanism 2
Longer discussions can lead to "problem drift" and "alignment collapse" in multi-agent systems, harming performance on basic tasks. As discussions extend, agents may deviate from original task requirements and lose ethical alignment, resulting in misaligned responses.

### Mechanism 3
Centralized discussion paradigms with information restrictions can improve ethical alignment in multi-agent systems. Limiting information access between agents and introducing a central moderator reduces individual agent influence, leading to more balanced and ethically aligned decision-making.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: CoT serves as baseline for comparing multi-agent performance against single LLM with enhanced reasoning capabilities
  - Quick check question: How does CoT prompting differ from standard prompting, and what are its potential benefits and limitations?

- Concept: Lexical diversity
  - Why needed here: Lexical diversity is used as metric to evaluate quality and engagement of generated text in multi-agent systems
  - Quick check question: How is lexical diversity measured, and why is it important factor in assessing performance of text generation models?

- Concept: Ethical alignment
  - Why needed here: Ethical alignment ensures generated responses adhere to human values and avoid harmful or biased content
  - Quick check question: What are key challenges in achieving ethical alignment in multi-agent systems, and what strategies can be employed to mitigate these challenges?

## Architecture Onboarding

- Component map: Agents -> Discussion Paradigms -> Decision-Making -> MALLM Framework
- Critical path: Automatic persona assignment -> Multi-agent discussion -> Consensus decision-making
- Design tradeoffs: Number of agents vs. complexity of discussion; Information access vs. computational efficiency; Discussion length vs. risk of problem drift
- Failure signatures: Poor performance on basic tasks due to problem drift; Ethical misalignment in generated responses; Discussion monopolization by single agent; Inability to detect unanswerable questions
- First 3 experiments: 1) Compare task performance of multi-agent systems against CoT baseline on various tasks; 2) Analyze discussion convergence and its impact on task performance; 3) Investigate influence of agent personas and response lengths on discussion dynamics

## Open Questions the Paper Calls Out

### Open Question 1
Does automatic persona assignment method consistently generate useful and relevant personas across different tasks and domains? The paper only tests automatic persona assignment on limited tasks and doesn't explore effectiveness across wider range of domains or task complexities.

### Open Question 2
What are specific mechanisms and factors that contribute to ethical alignment collapse during extended multi-agent discussions? The paper identifies the phenomenon but doesn't investigate underlying reasons for why agents become less aligned over time.

### Open Question 3
Can risk of discussion monopolization through response length be effectively mitigated while preserving benefits of multi-agent collaboration? The paper identifies the risk but doesn't propose or test potential solutions to mitigate it.

## Limitations

- Experimental evidence for problem drift, alignment collapse, and monopolization mechanisms is weak and primarily observational
- Study relies on synthetic ethical questions that may not capture full complexity of real-world ethical dilemmas
- MALLM framework's effectiveness is demonstrated primarily through controlled experiments with specific datasets and model configurations

## Confidence

- High Confidence: Multi-agent systems improve performance on complex reasoning tasks through iterative consensus and expert personas
- Medium Confidence: Problem drift and alignment collapse identified as key failure modes, but proposed mechanisms require further validation
- Low Confidence: Effectiveness of centralized paradigms with information restrictions for improving ethical alignment needs more rigorous testing

## Next Checks

1. Test problem drift hypothesis by systematically varying discussion lengths across tasks of varying complexity to establish clear relationship between discussion duration and task performance degradation
2. Validate alignment collapse mechanism by introducing external ethical benchmarks and measuring how different discussion paradigms affect ethical alignment of generated responses over extended conversations
3. Investigate monopolization phenomenon by analyzing agent contribution patterns across different task types and discussion paradigms to identify conditions leading to imbalanced participation and their impact on final outcomes