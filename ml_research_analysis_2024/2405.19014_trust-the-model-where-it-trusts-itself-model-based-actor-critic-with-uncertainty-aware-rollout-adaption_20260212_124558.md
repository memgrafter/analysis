---
ver: rpa2
title: Trust the Model Where It Trusts Itself -- Model-Based Actor-Critic with Uncertainty-Aware
  Rollout Adaption
arxiv_id: '2405.19014'
source_url: https://arxiv.org/abs/2405.19014
tags:
- macura
- model-based
- rollout
- uncertainty
- mbpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the critical challenge of determining when
  to trust model-based rollouts in Dyna-style model-based reinforcement learning (MBRL).
  The authors propose MACURA, a novel algorithm that leverages model uncertainty to
  adaptively schedule rollout lengths.
---

# Trust the Model Where It Trusts Itself -- Model-Based Actor-Critic with Uncertainty-Aware Rollout Adaption

## Quick Facts
- arXiv ID: 2405.19014
- Source URL: https://arxiv.org/abs/2405.19014
- Reference count: 40
- Primary result: MACURA outperforms state-of-the-art MBRL methods on MuJoCo benchmark through uncertainty-aware rollout adaptation

## Executive Summary
This paper addresses the fundamental challenge in model-based reinforcement learning of determining when to trust model-based rollouts. The authors propose MACURA, a novel algorithm that leverages model uncertainty to adaptively schedule rollout lengths instead of using fixed schedules based on training time. By using a new uncertainty measure based on geometric Jensen-Shannon divergence, MACURA constructs a subset of the state space where the model is sufficiently accurate and restricts rollouts to this region. The method is theoretically justified through a monotonic improvement bound and demonstrates substantial improvements in both data efficiency and asymptotic performance compared to state-of-the-art MBRL methods on MuJoCo tasks.

## Method Summary
MACURA introduces an uncertainty-aware rollout adaption scheme for Dyna-style model-based reinforcement learning. The algorithm computes an uncertainty measure based on the geometric Jensen-Shannon divergence between ensemble models to determine when the model is sufficiently accurate. Instead of using fixed rollout lengths that decay over training time, MACURA dynamically adjusts rollout lengths based on model uncertainty, terminating rollouts when uncertainty exceeds a threshold. The method operates within the MBPO framework, modifying only the rollout scheduling component while maintaining the same SAC actor-critic architecture. The approach requires tuning only a single interpretable hyperparameter (scaling factor ξ), making it considerably easier to tune than competing approaches.

## Key Results
- MACURA outperforms MBPO, M2AC, and SAC baseline on all MuJoCo tasks tested (HalfCheetah, Walker, Hopper, Humanoid, Ant)
- Shows substantial improvements in data efficiency, requiring fewer environment interactions to reach peak performance
- Demonstrates superior asymptotic performance with more stable learning curves
- Requires tuning of only one interpretable hyperparameter compared to multiple hyperparameters in competing methods

## Why This Works (Mechanism)
MACURA's adaptive rollout scheduling addresses the fundamental trade-off in model-based RL between exploiting learned models and maintaining performance guarantees. By using model uncertainty to determine rollout length, the algorithm ensures that model-based rollouts only extend as far as the model is trustworthy, preventing the accumulation of model errors that can destabilize learning. The geometric Jensen-Shannon divergence provides a principled way to quantify model disagreement, which serves as a proxy for predictive uncertainty. This uncertainty-aware approach allows MACURA to make more efficient use of model-based rollouts while maintaining the theoretical guarantees provided by the monotonic improvement bound.

## Foundational Learning
- **Model-based RL with ensembles**: Needed to capture model uncertainty and enable uncertainty-aware decisions. Quick check: Verify ensemble predictions show reasonable disagreement patterns across different states.
- **Geometric Jensen-Shannon divergence**: Required for quantifying uncertainty between ensemble models in a theoretically justified way. Quick check: Confirm GJS values correlate with prediction error on validation data.
- **Monotonic improvement bounds**: Provides theoretical foundation for why uncertainty-aware rollout termination preserves performance guarantees. Quick check: Verify the bound holds approximately on validation trajectories.
- **Dyna-style architecture**: The framework that allows mixing model-free and model-based updates. Quick check: Ensure the transition between real and simulated data is seamless.
- **Uncertainty quantification in RL**: Critical for safe deployment of learned models. Quick check: Monitor uncertainty estimates during training for reasonable behavior.

## Architecture Onboarding

Component Map:
SAC Actor-Critic -> Model Ensemble -> Uncertainty Estimator -> Rollout Scheduler -> Environment

Critical Path:
The algorithm collects real transitions from the environment, updates the SAC actor-critic with these transitions, trains the probabilistic ensemble model on collected data, computes uncertainty measures using GJS divergence, uses these measures to adaptively schedule rollout lengths, generates model-based rollouts within the trustworthy state space, and uses these rollouts to further train the SAC agent.

Design Tradeoffs:
The main tradeoff is between computational overhead (computing GJS divergence for uncertainty estimation) and performance gains from more reliable model-based rollouts. The single hyperparameter (scaling factor ξ) represents a tradeoff between exploration and exploitation - larger values allow longer rollouts but risk accumulating model errors, while smaller values are more conservative but may underutilize the model.

Failure Signatures:
- If rollout lengths remain consistently short, the model uncertainty estimates may be too conservative or the scaling factor ξ may be too small
- If performance degrades despite high rollout lengths, the uncertainty threshold κ may be too permissive or the model ensemble may be overconfident
- If learning is unstable, the model may be overfitting to narrow data distributions or the uncertainty estimates may not capture relevant model errors

First Experiments:
1. Visualize model uncertainty estimates across the state space to verify they correlate with prediction error
2. Monitor rollout length distribution over training to ensure adaptive scheduling is working as expected
3. Compare performance with fixed rollout lengths of varying durations to quantify the benefit of adaptive scheduling

## Open Questions the Paper Calls Out
- How does MACURA's performance compare when using different SAC variants like TD3 or PPO instead of the Soft Actor-Critic baseline?
- What is the theoretical justification for choosing the geometric Jensen-Shannon divergence over other uncertainty measures like mutual information or total correlation?
- How does MACURA's performance scale with increasing state and action space dimensions?

## Limitations
- Evaluation restricted to MuJoCo benchmark suite, which may not capture performance on more diverse or challenging environments
- Single hyperparameter tuning requirement demonstrated but sensitivity to this parameter across different tasks not fully explored
- Computational overhead of GJS uncertainty computation may affect practical applicability for larger state spaces

## Confidence
High confidence in data efficiency and performance improvements based on presented results.
Medium confidence in theoretical guarantees linking uncertainty measures to monotonic improvement bounds.
Low confidence in scalability to more complex tasks with higher-dimensional state and action spaces.

## Next Checks
1. Validate MACURA's performance on a broader range of environments including continuous control tasks with sparse rewards and multi-task learning scenarios to assess generalizability
2. Conduct ablation studies systematically varying the scaling factor ξ across different task difficulties to characterize the algorithm's sensitivity to this hyperparameter
3. Benchmark the computational overhead of the GJS uncertainty computation against the performance gains, particularly for larger state spaces or more complex dynamics