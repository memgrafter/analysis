---
ver: rpa2
title: 'Pre-trained Encoder Inference: Revealing Upstream Encoders In Downstream Machine
  Learning Services'
arxiv_id: '2408.02814'
source_url: https://arxiv.org/abs/2408.02814
tags:
- attack
- encoder
- downstream
- encoders
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Pre-trained Encoder Inference (PEI) attack,
  a novel privacy threat targeting downstream ML services that rely on hidden pre-trained
  encoders. The attack enables an adversary with API access to infer which specific
  encoder is secretly used by a service, even when the encoder is not directly accessible.
---

# Pre-trained Encoder Inference: Revealing Upstream Encoders In Downstream Machine Learning Services

## Quick Facts
- arXiv ID: 2408.02814
- Source URL: https://arxiv.org/abs/2408.02814
- Reference count: 40
- Key outcome: Introduces PEI attack that can infer hidden pre-trained encoders in downstream ML services with API access, revealing significant privacy risks

## Executive Summary
This paper presents the Pre-trained Encoder Inference (PEI) attack, a novel privacy threat targeting downstream machine learning services that rely on hidden pre-trained encoders. The attack enables adversaries with API access to determine which specific encoder a service secretly uses, even when the encoder itself is not directly accessible. By synthesizing input samples that produce similar embeddings under candidate encoders and observing the downstream model's behavior, the PEI attack can successfully identify the hidden encoder in most cases with low cost. The revealed encoder information can then be exploited to enhance other attacks like model stealing and adversarial attacks, demonstrating significant practical risks to deployed encoder-based services.

## Method Summary
The PEI attack framework works by synthesizing PEI attack samples for each candidate encoder through zeroth-order optimization, minimizing the difference between their embeddings and a pre-defined embedding. These synthesized samples are then used to query the targeted downstream service. If the downstream model produces similar outputs for these attack samples, it indicates the candidate encoder is likely the hidden one. The method calculates PEI scores based on behavior similarity and uses z-test to identify significantly high scores, determining which encoder is secretly used by the downstream service.

## Key Results
- Successfully identifies hidden encoders in most cases with query cost under $100 per encoder
- Never produces false positives in experimental validation
- Works across different model types including image classification and multimodal generation
- Can enhance effectiveness of model stealing and adversarial attacks once encoder is revealed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PEI attack synthesizes input samples that produce similar embeddings under a candidate encoder, then observes whether the downstream model exhibits consistent behavior for these samples
- Mechanism: The adversary generates PEI attack samples by minimizing the difference of their embeddings and a pre-defined embedding under a candidate encoder. These samples are then used to query the downstream service. If the downstream model produces similar outputs for these samples, it indicates the candidate encoder is likely the hidden one.
- Core assumption: For a certain encoder and a pre-defined embedding, there exist samples that look different but have similar embeddings only under that encoder.
- Evidence anchors:
  - [abstract]: "The method works by synthesizing input samples that produce similar embeddings under a candidate encoder, then observing whether the downstream model exhibits consistent behavior for these samples."
  - [section]: "Under this observation, to infer whether a given candidate encoder is used by the targeted service, we propose to first synthesize a series of PEI attack samples via minimizing the difference of their embeddings and the pre-defined embedding under this candidate encoder."
- Break condition: If the downstream model's behavior is not significantly influenced by the embedding similarity of the attack samples, the PEI attack will fail to identify the hidden encoder.

### Mechanism 2
- Claim: PEI attack can reveal hidden encoders even after they have been deployed and hidden in downstream ML services
- Mechanism: The attack exploits the fact that the downstream model's behavior is determined by the embeddings produced by the hidden encoder. By synthesizing PEI attack samples and observing the downstream model's behavior, the adversary can infer which encoder is used, even without direct access to it.
- Core assumption: The downstream model's behavior is strongly correlated with the embeddings produced by the hidden encoder.
- Evidence anchors:
  - [abstract]: "Compared with existing encoder attacks, which mainly target encoders on the upstream side, the PEI attack can compromise encoders even after they have been deployed and hidden in downstream ML services, which makes it a more realistic threat."
  - [section]: "Given a targeted downstream ML service, the goal of the new PEI attack is to infer what pre-trained encoder is secretly used by the downstream model."
- Break condition: If the downstream model's behavior is not significantly influenced by the hidden encoder, or if the model uses a complex architecture that obscures the relationship between embeddings and outputs, the PEI attack will fail.

### Mechanism 3
- Claim: PEI attack can facilitate other ML attacks against the targeted service once the hidden encoder is revealed
- Mechanism: After revealing the hidden encoder, the adversary can exploit public APIs of the encoder to generate adversarial examples or steal the model's functionality. This enhances the effectiveness of attacks like model stealing and adversarial attacks.
- Core assumption: The revealed encoder can be used to generate inputs that exploit vulnerabilities in the downstream model.
- Evidence anchors:
  - [abstract]: "The revealed encoder information can then be exploited to enhance other attacks like model stealing and adversarial attacks, demonstrating significant practical risks to deployed encoder-based services."
  - [section]: "Once the hidden encoder is revealed, the adversary can then exploit public APIs of the hidden encoder to facilitate other ML attacks against the targeted downstream service such as model stealing attacks or adversarial attacks more effectively."
- Break condition: If the downstream model has robust defenses against adversarial examples or if the model's functionality is difficult to replicate, the PEI attack will not significantly enhance other attacks.

## Foundational Learning

- Concept: Self-supervised learning (SSL) and pre-trained encoders
  - Why needed here: The paper relies on the widespread adoption of SSL and pre-trained encoders in building downstream ML services. Understanding SSL is crucial to grasp the attack's context and the vulnerabilities it exploits.
  - Quick check question: What is the main advantage of using pre-trained encoders in building downstream ML services, and how does SSL contribute to this advantage?

- Concept: Zeroth-order optimization
  - Why needed here: The PEI attack uses zeroth-order gradient estimation to synthesize attack samples without access to the encoder's parameters. Understanding this optimization technique is essential to comprehend the attack's methodology.
  - Quick check question: How does zeroth-order gradient estimation work, and why is it necessary for the PEI attack when the adversary only has API access to the candidate encoders?

- Concept: Membership inference attacks (MIA) and model stealing attacks
  - Why needed here: The paper mentions these attacks as potential threats that can be facilitated by the PEI attack. Understanding these attacks provides context for the severity of the vulnerabilities exposed by the PEI attack.
  - Quick check question: What is the goal of a membership inference attack, and how does it differ from a model stealing attack in terms of the adversary's objectives?

## Architecture Onboarding

- Component map:
  - Targeted downstream ML service: A composite function g(·) := hθ(f*(·)), where f* is the hidden encoder and hθ is the downstream model
  - PEI adversary: Has API access to the targeted service and a set of candidate encoders
  - Candidate encoders: Pre-trained encoders from online repositories or EaaSs
  - PEI attack framework: Consists of two stages - PEI attack samples synthesis and hidden encoder inference

- Critical path:
  1. Adversary queries the targeted service and candidate encoders via APIs
  2. PEI attack samples are synthesized for each candidate encoder by minimizing the difference of their embeddings and a pre-defined embedding
  3. The synthesized samples are used to query the targeted service
  4. PEI scores are calculated based on the similarity of the service's behavior for the attack samples and the pre-defined embedding
  5. A z-test is performed on the PEI scores to infer the hidden encoder

- Design tradeoffs:
  - Query budget vs. attack effectiveness: Increasing the number of objective samples or attack samples per candidate can improve the attack's accuracy but also increases the query cost
  - Sampling number S vs. gradient estimation accuracy: A larger sampling number improves the accuracy of zeroth-order gradient estimation but also increases the query budget

- Failure signatures:
  - Low PEI z-scores for all candidates: Indicates that the hidden encoder is not in the candidate set or the attack samples are not effective
  - Inconsistent results across multiple runs: Suggests that the attack is sensitive to the random initialization of downstream classifiers or the synthesis of attack samples

- First 3 experiments:
  1. Implement the PEI attack framework and test it on a simple image classification service with a known encoder to verify the attack's effectiveness
  2. Vary the number of objective samples and attack samples per candidate to analyze the trade-off between query budget and attack accuracy
  3. Test the attack on a multimodal generative model (e.g., LLaVA) to assess its applicability to different types of downstream services

## Open Questions the Paper Calls Out
The paper identifies several open questions for future research:

1. How effective is the PEI attack when the downstream model uses multiple pre-trained encoders rather than a single encoder?
2. Can the PEI attack be adapted to target language models or other non-vision modalities beyond the image and multimodal generation tasks explored in the paper?
3. What is the impact of different data transformation defenses (beyond JPEG compression) on the success rate of the PEI attack?

## Limitations
- Experimental validation is limited to relatively simple architectures and may not generalize to complex multi-stage models
- The attack's effectiveness could degrade significantly when applied to services using sophisticated ensemble methods or adaptive processing pipelines
- The paper does not address potential countermeasures that downstream services could implement to detect or mitigate such inference attacks

## Confidence
- **High confidence**: The fundamental mechanism of synthesizing samples to match embeddings and measuring downstream behavior consistency is well-established and theoretically sound
- **Medium confidence**: The experimental results demonstrating successful encoder inference across different model types are promising but limited in scope and may not represent all real-world deployment scenarios
- **Low confidence**: The claim about enhanced adversarial attack effectiveness post-encoder revelation lacks rigorous validation and quantitative analysis of the practical impact

## Next Checks
1. **Cross-architecture validation**: Test the PEI attack on downstream services using ensemble models or multi-stage processing pipelines to evaluate its effectiveness against more complex architectures
2. **Robustness analysis**: Implement and evaluate potential defense mechanisms such as adversarial training on the downstream service or embedding space regularization to assess the attack's resilience to countermeasures
3. **Cost-benefit analysis**: Conduct a comprehensive analysis of the query budget required for successful inference across different service configurations and compare it with the potential benefits gained from revealing the hidden encoder