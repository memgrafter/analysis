---
ver: rpa2
title: Revisiting Safe Exploration in Safe Reinforcement learning
arxiv_id: '2409.01245'
source_url: https://arxiv.org/abs/2409.01245
tags:
- cost
- training
- learning
- safe
- return
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EMCC, a new metric for evaluating safe exploration
  in SafeRL. Unlike existing metrics that treat all unsafe steps equally, EMCC focuses
  on consecutive unsafe steps to better capture exploration quality.
---

# Revisiting Safe Exploration in Safe Reinforcement learning

## Quick Facts
- arXiv ID: 2409.01245
- Source URL: https://arxiv.org/abs/2409.01245
- Reference count: 40
- This paper introduces EMCC, a new metric for evaluating safe exploration in SafeRL, showing it provides more nuanced insights than traditional metrics.

## Executive Summary
This paper addresses a gap in SafeRL evaluation by introducing EMCC, a metric focused on consecutive unsafe steps rather than treating all violations equally. The authors argue that prolonged unsafe behavior is more indicative of poor exploration than isolated violations. To facilitate rapid benchmarking, they propose Circle2D, a new task suite with four difficulty levels. Benchmarking various SafeRL algorithms on Circle2D and Safety-Gymnasium tasks demonstrates that EMCC effectively distinguishes between prolonged and occasional safety violations, offering a more comprehensive evaluation of safe exploration strategies.

## Method Summary
The paper introduces the Expected Maximum Consecutive Cost steps (EMCC) metric to evaluate safe exploration in SafeRL. EMCC computes the maximum consecutive cost steps per trajectory, normalized by trajectory length, and averages over training rollouts. The authors also propose Circle2D, a new benchmark task set with four difficulty levels for rapid evaluation. Various SafeRL algorithms (TRPO-Lag, CPO, SAC-Lag, SAC-LB, WCSAC) are benchmarked on Circle2D and Safety-Gymnasium tasks using EMCC alongside traditional metrics like cost rate and return.

## Key Results
- EMCC effectively distinguishes between prolonged and occasional safety violations, providing more nuanced insights than traditional metrics.
- Benchmarking on Circle2D and Safety-Gymnasium tasks shows EMCC captures exploration quality better than existing evaluation methods.
- The phase-split EMCC (EMCC0.33, EMCC0.66, EMCC0.99) reveals changing exploration behavior throughout training.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EMCC captures the severity of unsafe exploration better than existing metrics by counting consecutive cost steps.
- Mechanism: EMCC computes the maximum consecutive cost steps per trajectory, normalized by trajectory length, and averages over training rollouts. This highlights policies that incur prolonged unsafe behavior versus those with occasional violations.
- Core assumption: Consecutive cost steps reflect higher risk than isolated ones, and this risk is meaningful for policy learning.
- Evidence anchors:
  - [abstract] "EMCC focuses on consecutive unsafe steps to better capture exploration quality."
  - [section] "We introduce a new metric, expected maximum consecutive cost steps (EMCC), which addresses safety during training by assessing the severity of unsafe steps based on their consecutive occurrence."
  - [corpus] Weak or missing direct evidence of consecutive-step risk in existing benchmarks.
- Break condition: If risk is better modeled by frequency or total cost rather than consecutive length, EMCC would misrepresent safe exploration.

### Mechanism 2
- Claim: Splitting EMCC into training phases reveals changing exploration behavior.
- Mechanism: EMCC is computed separately for early (0-33%), middle (33-66%), and late (66-99%) training thirds, capturing how safety degrades or improves over time.
- Core assumption: Safe exploration risk is highest early in training and decreases as policies improve; tracking per phase reflects this trend.
- Evidence anchors:
  - [section] "we divide the training process into three parts and calculate EMCC per part."
  - [section] "EM CC0.33 combines data from the start to 33% of the training, EM CC0.66 for 33% to 66% and EM CC0.99 for 66% to 99%."
  - [corpus] No direct corpus evidence of phase-specific risk trends.
- Break condition: If exploration risk is not time-dependent or policy updates are non-monotonic, phase-split EMCC may mislead.

### Mechanism 3
- Claim: Augmenting EMCC with CVaR emphasizes the worst-case unsafe behavior.
- Mechanism: CVaR(α) applied to MCC distribution keeps only the highest α percentile of MCC values before averaging, focusing on most prolonged safety violations.
- Core assumption: Worst-case consecutive violations are more informative for safety than average behavior.
- Evidence anchors:
  - [section] "we augment EM CCβ with the conditional Value-at-Risk (CVaR) [21] to focus on the highest MCC values of the MCC distribution per training part."
  - [section] "As we associate the most prolonged safety violations with the most risky behaviour augmenting with CVaR further enhances EM CCβ as a safety measure."
  - [corpus] No corpus evidence that CVaR-based metrics are superior in SafeRL.
- Break condition: If the worst-case is not representative of typical risk or if α is poorly chosen, CVaR-augmented EMCC may over-penalize rare events.

## Foundational Learning

- Concept: Constrained Markov Decision Processes (CMDPs)
  - Why needed here: EMCC is defined in the context of CMDPs where both rewards and costs are considered; understanding the constrained optimization objective is essential to interpret EMCC.
  - Quick check question: In a CMDP, what is the feasible set ΠC defined as?

- Concept: Trajectory-based safety evaluation
  - Why needed here: EMCC operates on rollouts and trajectories, so distinguishing between per-step and per-trajectory metrics is crucial for correct usage.
  - Quick check question: How is the maximum consecutive cost steps (MCC) for a single trajectory computed in EMCC?

- Concept: Cumulative vs. consecutive cost metrics
  - Why needed here: EMCC specifically measures consecutive cost steps, not total cost; knowing why this distinction matters is key to interpreting results.
  - Quick check question: Why might two policies with the same total cost have very different EMCC values?

## Architecture Onboarding

- Component map: Circle2D environment -> SafeRL algorithm -> training rollout collector -> trajectory analyzer -> MCC calculator -> CVaR filter (optional) -> final EMCC output
- Critical path: Generate rollouts -> split by training phase -> compute MCC per trajectory -> apply CVaR filter -> average for EMCC -> compare across algorithms
- Design tradeoffs: EMCC emphasizes worst-case consecutive violations (via CVaR) at the cost of ignoring isolated high-cost steps; phase-splitting adds granularity but requires more bookkeeping
- Failure signatures: EMCC values stay high late in training (indicates persistent unsafe exploration); EMCC is low but cost rate is high (indicates frequent but short unsafe steps); EMCC is high but final policy cost is low (may signal early risky exploration that was later corrected)
- First 3 experiments:
  1. Run Circle2D level 0 with SAC-Lag and compute EMCC0.33, EMCC0.66, EMCC0.99 to observe how consecutive costs evolve
  2. Compare TRPO-Lag vs SAC-Lag on SafetyPointCircle1-v0 and plot both cost rate and EMCC over training to see if they diverge
  3. Add CVaR(0.5) to EMCC calculation for WCSAC on Circle2D-1 and check if it changes the relative ranking versus other algorithms

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does EMCC perform in environments with non-uniform cost distributions where some regions are significantly more dangerous than others?
- Basis in paper: [inferred] The paper introduces EMCC as a metric for safe exploration but does not test it in environments with varying cost severity or non-uniform cost distributions.
- Why unresolved: The Circle2D environment uses a uniform cost structure, and the paper does not explore scenarios with varying cost severities.
- What evidence would resolve it: Experiments comparing EMCC with traditional metrics in environments with non-uniform cost distributions and varying cost severities would provide insights into its effectiveness.

### Open Question 2
- Question: Can EMCC be adapted to handle environments where constraints change dynamically during training?
- Basis in paper: [inferred] The paper focuses on static constraint environments but does not address dynamic constraint scenarios.
- Why unresolved: The Circle2D environment and Safety-Gymnasium tasks have fixed constraints, leaving the adaptability of EMCC to dynamic environments unexplored.
- What evidence would resolve it: Testing EMCC in environments with dynamically changing constraints would demonstrate its adaptability and effectiveness in such scenarios.

### Open Question 3
- Question: How does the risk level α in EMCC affect the trade-off between exploration and safety in different SafeRL algorithms?
- Basis in paper: [explicit] The paper mentions using a risk level α in the definition of EMCC but does not explore its impact on different algorithms.
- Why unresolved: The experiments use a fixed risk level of 0.5, and the paper does not investigate how varying α affects algorithm performance.
- What evidence would resolve it: Comparative studies of EMCC with different α values across various SafeRL algorithms would clarify its impact on exploration and safety trade-offs.

## Limitations
- The assumption that consecutive cost steps reflect higher risk than isolated ones is not validated against alternative risk models.
- The phase-splitting approach relies on an unproven claim that exploration risk is highest early in training.
- CVaR augmentation is introduced without evidence that worst-case behavior is more informative than average performance.

## Confidence
- EMCC distinguishes prolonged from occasional unsafe steps: Medium
- Phase-splitting reveals meaningful training trends: Low
- CVaR augmentation improves safety evaluation: Low

## Next Checks
1. Compare EMCC to total cost and cost rate on Circle2D tasks to confirm that consecutive-step counting reveals qualitatively different behavior than existing metrics.
2. Test EMCC with different CVaR thresholds (α) to determine if the choice of worst-case percentile meaningfully changes algorithm rankings or is sensitive to hyperparameter selection.
3. Benchmark EMCC on an additional SafeRL task set (e.g., Safety-Gymnasium or another established suite) to check if the observed benefits generalize beyond the proposed Circle2D environment.