---
ver: rpa2
title: Efficient Continual Pre-training by Mitigating the Stability Gap
arxiv_id: '2406.14833'
source_url: https://arxiv.org/abs/2406.14833
tags:
- performance
- medical
- pre-training
- arxiv
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the "stability gap" phenomenon in continual
  pre-training of large language models (LLMs), where model performance on target
  tasks initially drops before recovering. The authors propose three strategies to
  mitigate this issue: (1) training on smaller subsets of data for multiple epochs
  instead of the full dataset once, (2) using high-quality subsets of the data, and
  (3) matching the data mixture rate to the original pre-training data.'
---

# Efficient Continual Pre-training by Mitigating the Stability Gap

## Quick Facts
- arXiv ID: 2406.14833
- Source URL: https://arxiv.org/abs/2406.14833
- Reference count: 40
- Primary result: Stability gap phenomenon in continual LLM pre-training causes initial performance drops that can be mitigated through subset training, quality-based data selection, and distribution matching

## Executive Summary
This paper investigates the "stability gap" phenomenon in continual pre-training of large language models, where performance on target tasks initially drops before recovering. The authors propose three strategies to mitigate this issue: training on smaller subsets of data for multiple epochs, using high-quality subsets of the data, and matching the data mixture rate to the original pre-training data. Experiments on Llama-family models show significant improvements, with OpenLlama-3B's medical task performance increasing from 36.2% to 40.7% using only 40% of the original training budget. The strategies were successfully applied to create Llama-3-Physician-8B, which outperforms other open-source medical LLMs and achieves comparable performance to GPT-4 on several medical benchmarks.

## Method Summary
The paper proposes three strategies to mitigate the stability gap in continual LLM pre-training. First, training on smaller subsets of data for multiple epochs instead of the full dataset once allows for faster recovery from performance drops. Second, using high-quality subsets of the data, measured by perplexity relative to a reference corpus, accelerates learning and improves peak performance. Third, matching the data mixture rate to the original pre-training data reduces distribution shift and stabilizes instruction-following ability. The authors evaluate these strategies on medical domain adaptation using Llama-family models and demonstrate significant improvements in both medical and general task performance.

## Key Results
- Stability gap causes initial performance drops in continual pre-training that can be mitigated through strategic data selection and training procedures
- Using a 5B token high-quality subset with multiple epochs achieved 40.7% medical task accuracy versus 36.2% with full 50B token single-pass training
- Llama-3-Physician-8B created using these strategies outperforms other open-source medical LLMs and achieves GPT-4-comparable performance on medical benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The initial performance drop occurs due to insufficient stability gradient to maintain instruction-following ability while plasticity gradient dominates for learning new domain knowledge.
- Mechanism: In continual learning, the training gradient can be decomposed into stability gradient (maintaining old task performance) and plasticity gradient (learning new task). At the start of continual pre-training, the plasticity gradient for learning medical domain knowledge exceeds the stability gradient for maintaining general instruction-following ability, causing performance drop. As training progresses, the stability gradient increases to recover instruction-following ability.
- Core assumption: LLM performance depends on both domain-specific knowledge and general instruction-following ability, with these capabilities associated with different gradient components.
- Evidence anchors:
  - [abstract] "The substantial performance drop and slow recovery associated with this gap lead to inefficient pre-training for domain performance improvement and the forgetting of general task knowledge."
  - [section 3.2] "We decouple the continual pre-training gradient into the plasticity gradient for learning new domain knowledge and the stability gradient for maintaining instruction-following ability."
- Break condition: This mechanism breaks if the assumption about instruction-following ability being tied to stability gradient is incorrect, or if the model architecture doesn't separate domain-specific and general knowledge in a way that creates this gradient imbalance.

### Mechanism 2
- Claim: Using high-quality subsets of training data accelerates performance recovery and improves peak performance.
- Mechanism: High-quality data (measured by lower perplexity relative to reference corpus) contains richer domain knowledge, enabling faster learning of medical domain concepts. This reduces the time needed for the plasticity gradient to build sufficient domain knowledge, which in turn allows the stability gradient to recover general instruction-following ability more quickly.
- Core assumption: Data quality, as measured by perplexity relative to reference corpus, correlates with the density of useful domain knowledge.
- Evidence anchors:
  - [section 4] "We used the trained KenLM from Sec. 3.1 to calculate the perplexity (ppl) of each sample in the entire medical corpus. A lower perplexity indicates that the sample is closer to the distribution of the medical reference corpus."
  - [section 5.1] "Select the subset with the highest-quality tokens to learn rich domain knowledge, leading to faster performance recovery and higher peak performance."
- Break condition: This mechanism breaks if the perplexity-based quality metric doesn't accurately reflect domain knowledge density, or if the model can learn equally well from lower-quality data given sufficient training time.

### Mechanism 3
- Claim: Matching the data mixture rate to original pre-training data reduces distribution shift and stabilizes instruction-following ability.
- Mechanism: When the distribution of continual pre-training data closely matches the original pre-training distribution, the model experiences less shift in input statistics. This smaller distribution shift means less need for large plasticity gradients to adapt to new data patterns, allowing the stability gradient to maintain general instruction-following ability more effectively.
- Core assumption: Distribution shift between pre-training and continual pre-training data is a primary driver of performance instability in general tasks.
- Evidence anchors:
  - [section 4] "Use a data mixture similar to the pre-training data to reduce distribution gap and mitigating the knowledge forgetting of general instruction-following ability."
  - [section 5.1] "Following the pre-training data mixture rate to replay pre-training data is more effective than randomly sampling pre-training data for replay. This is because it further reduces the distribution shift between the pre-training corpus and the continual pre-training corpus, thereby helping to recover the LLM's general instruction-following ability."
- Break condition: This mechanism breaks if the original pre-training data distribution is unknown or unavailable, or if the benefits of distribution matching are outweighed by the need for domain-specific data adaptation.

## Foundational Learning

- Concept: Gradient decomposition into stability and plasticity components
  - Why needed here: Understanding how different gradient components affect performance maintenance vs. new knowledge acquisition is crucial for diagnosing and fixing the stability gap problem
  - Quick check question: If a model has high plasticity gradient but low stability gradient during continual learning, what happens to performance on previously learned tasks?

- Concept: Data quality measurement using language model perplexity
  - Why needed here: The quality-based data selection strategy relies on perplexity as a proxy for domain knowledge density, so understanding this metric is essential for implementing the strategy
  - Quick check question: If sample A has perplexity 10 and sample B has perplexity 100 relative to a medical reference corpus, which sample is considered higher quality for medical continual pre-training?

- Concept: Catastrophic forgetting and its relationship to stability gap
  - Why needed here: The stability gap is a specific manifestation of catastrophic forgetting, so understanding the broader phenomenon helps in grasping why the performance drop occurs
  - Quick check question: What's the key difference between catastrophic forgetting and the stability gap as described in this paper?

## Architecture Onboarding

- Component map: Pre-trained LLM (Llama family) -> KenLM quality assessment pipeline -> Data subset selection -> Multi-epoch training loop -> Evaluation framework
- Critical path: Data → Quality assessment → Subset selection → Multi-epoch training → Performance evaluation → Model checkpointing
- Design tradeoffs: Using smaller high-quality subsets trades computational efficiency for potential loss of some domain coverage; matching pre-training distribution trades some domain specificity for better general task performance
- Failure signatures: If performance drops persist despite using strategies, check if the data quality metric is misaligned with actual knowledge density; if peak performance is low, verify that the subset size isn't too small to capture necessary domain knowledge
- First 3 experiments:
  1. Implement strategy I (multi-epoch training on subset) with random subset selection to verify faster recovery compared to single-epoch full dataset training
  2. Add strategy II (quality-based subset selection) to verify that higher quality subsets lead to faster recovery and better peak performance
  3. Add strategy III (pre-training distribution matching) to verify that distribution matching reduces performance drops on general tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the stability gap phenomenon differ between continual pre-training and continual learning settings, and what are the underlying causes of these differences?
- Basis in paper: [explicit] The paper explicitly discusses the stability gap in both contexts, noting that the phenomenon is observed in both but focusing on its manifestation during continual pre-training of LLMs.
- Why unresolved: While the paper provides an explanation for the stability gap in continual learning based on the balance between plasticity and stability gradients, it does not fully explore how these dynamics might differ in the context of continual pre-training, where the model is adapting to new domains rather than new tasks.
- What evidence would resolve it: Comparative experiments analyzing the learning dynamics, such as gradient magnitudes and directions, in both settings, along with a detailed theoretical analysis of the differences in plasticity and stability gradients.

### Open Question 2
- Question: What is the optimal size and composition of the training subset for mitigating the stability gap, and how does this vary with different LLM scales and target domains?
- Basis in paper: [explicit] The paper discusses the impact of training subset size on the stability gap, noting that a smaller high-quality subset can lead to faster performance recovery but may cause overfitting in later epochs, while a larger subset can result in a slower performance recovery.
- Why unresolved: The paper provides empirical evidence for a specific model and domain, but it does not systematically explore the optimal subset size and composition across different LLM scales and target domains, leaving the question of generalizability open.
- What evidence would resolve it: A comprehensive study varying the training subset size and composition across multiple LLM scales and target domains, analyzing the resulting performance and stability gap mitigation.

### Open Question 3
- Question: How do the proposed strategies for mitigating the stability gap generalize to other domains beyond medical applications, and what are the potential limitations or challenges in applying these strategies to different domains?
- Basis in paper: [inferred] While the paper demonstrates the effectiveness of the strategies in medical continual pre-training and instruction tuning, it does not explicitly explore their generalizability to other domains, leaving the question of their broader applicability open.
- Why unresolved: The paper focuses on a specific domain and does not provide evidence for the strategies' effectiveness in other domains, nor does it discuss potential challenges or limitations in applying them to different contexts.
- What evidence would resolve it: Empirical studies applying the strategies to continual pre-training in various domains, such as legal, financial, or scientific domains, and analyzing their effectiveness and any domain-specific challenges or limitations.

## Limitations

- The gradient decomposition framework lacks rigorous mathematical proof connecting gradient magnitudes to performance dynamics
- The perplexity-based quality metric assumes this measure captures meaningful domain knowledge density, which may not hold universally across domains
- Optimal parameters (subset sizes, mixture ratios) are empirically determined for medical tasks and may not generalize to other domains without re-tuning

## Confidence

**High Confidence:**
- The stability gap phenomenon itself (initial performance drop followed by recovery) is empirically validated across multiple experiments and model sizes
- Strategy I (multi-epoch training on subsets) reliably accelerates performance recovery compared to single-pass full dataset training
- Strategy II (high-quality data selection) demonstrably improves both peak performance and recovery speed

**Medium Confidence:**
- Strategy III (matching pre-training data distribution) effectively reduces general task forgetting, though the optimal distribution matching ratio requires further investigation
- The gradient decomposition framework provides a useful explanatory model for understanding stability gap dynamics

**Low Confidence:**
- The specific mechanisms proposed for why each strategy works (e.g., exact relationship between perplexity and domain knowledge density)
- Whether the optimal parameters (subset sizes, mixture ratios) discovered for medical tasks generalize to other domains

## Next Checks

1. **Cross-domain generalization test**: Apply the three strategies to continual pre-training for a non-medical domain (e.g., legal or financial) and verify whether the same optimal subset sizes and mixture ratios apply, or if domain-specific tuning is required.

2. **Gradient analysis validation**: Implement gradient decomposition analysis to empirically measure stability vs. plasticity gradients during continual pre-training, comparing the actual gradient magnitudes to the predicted performance dynamics.

3. **Quality metric robustness test**: Replace the perplexity-based quality metric with an alternative measure (e.g., human evaluation or domain-specific knowledge graphs) and verify whether similar performance improvements are achieved, testing the assumption that perplexity correlates with useful domain knowledge.