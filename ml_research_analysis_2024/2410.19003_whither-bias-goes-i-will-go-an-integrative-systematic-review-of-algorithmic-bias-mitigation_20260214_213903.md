---
ver: rpa2
title: 'Whither Bias Goes, I Will Go: An Integrative, Systematic Review of Algorithmic
  Bias Mitigation'
arxiv_id: '2410.19003'
source_url: https://arxiv.org/abs/2410.19003
tags:
- bias
- fairness
- data
- algorithmic
- mitigation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a systematic review of algorithmic bias mitigation
  methods in machine learning for personnel assessment, integrating perspectives from
  computer science, data science, and industrial-organizational psychology. The authors
  present a four-stage model of ML development (data generation, model training, testing,
  and deployment) and classify bias mitigation methods within this framework.
---

# Whither Bias Goes, I Will Go: An Integrative, Systematic Review of Algorithmic Bias Mitigation

## Quick Facts
- arXiv ID: 2410.19003
- Source URL: https://arxiv.org/abs/2410.19003
- Reference count: 24
- Primary result: Preprocessing and inprocessing methods are most effective and legal for bias mitigation in ML models for personnel assessment

## Executive Summary
This systematic review examines algorithmic bias mitigation methods in machine learning for personnel assessment, integrating perspectives from computer science, data science, and industrial-organizational psychology. The authors present a four-stage model of ML development (data generation, model training, testing, and deployment) and classify bias mitigation methods within this framework. They identify key fairness operationalizations including adverse impact, equal accuracy, and individual fairness, while examining legal requirements in the US and Europe. The review finds that preprocessing and inprocessing methods are most effective and legal for bias mitigation, while postprocessing methods often violate anti-discrimination laws. Multi-objective optimization emerges as a particularly promising approach for balancing validity and fairness.

## Method Summary
The authors conducted a systematic review of algorithmic bias mitigation methods in machine learning for personnel assessment. They integrated perspectives from computer science, data science, and industrial-organizational psychology to classify methods across a four-stage ML development lifecycle. The review examined fairness operationalizations including adverse impact, equal accuracy, and individual fairness, while considering legal requirements in the US and Europe. Methods were categorized as preprocessing (applied before training), inprocessing (applied during training), and postprocessing (applied after training). The review synthesized findings on effectiveness and legality of different approaches, with particular attention to high-stakes assessment contexts.

## Key Results
- Preprocessing and inprocessing methods are most effective and legal for bias mitigation
- Postprocessing methods often violate anti-discrimination laws by using demographic information to adjust scores
- Multi-objective optimization shows promise for balancing validity and fairness
- Adversarial debiasing can reduce both adverse impact and psychometric bias while maintaining validity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preprocessing methods that balance training data representation can reduce adverse impact in ML models.
- Mechanism: By ensuring demographic groups are adequately represented in training data, the ML model learns predictor-outcome relationships that better reflect all groups' patterns, reducing group disparities in error rates.
- Core assumption: Predictor-outcome relationships differ across demographic groups.
- Evidence anchors:
  - [abstract] "The review found that preprocessing and inprocessing methods are most effective and legal for bias mitigation"
  - [section] "Balancing representation...The hope is that addressing representation bias will result in models that better represent all groups' predictor-outcome patterns, reducing group disparities in error rates."
  - [corpus] Weak - corpus does not directly support this specific mechanism
- Break condition: If predictor-outcome relationships are equivalent across groups, representation bias is not a concern.

### Mechanism 2
- Claim: Multi-objective optimization can balance validity and fairness in ML models.
- Mechanism: By modifying the algorithm's loss function to simultaneously optimize validity and a fairness operationalization, the model can achieve both high predictive accuracy and reduced adverse impact.
- Core assumption: It is possible to optimize multiple objectives simultaneously without significant trade-offs.
- Evidence anchors:
  - [abstract] "Multi-objective optimization emerged as a particularly promising approach"
  - [section] "Most work in psychology focused on the normal boundary intersection (NBI) pareto optimization method...Rottman et al. (2023) and Zhang et al. (2023) recently showed how to incorporate such constraints in other algorithms' loss functions"
  - [corpus] Weak - corpus does not directly support this specific mechanism
- Break condition: If the objectives are inherently conflicting (e.g., minimizing adverse impact while maintaining predictive validity), optimal solutions may not exist.

### Mechanism 3
- Claim: Postprocessing methods that use demographic information to adjust scores constitute disparate treatment and are likely illegal.
- Mechanism: By modifying model outputs based on an individual's demographic group, these methods treat individuals differently based on protected characteristics, violating anti-discrimination laws.
- Core assumption: Using demographic information to make decisions about individuals constitutes disparate treatment under US and European law.
- Evidence anchors:
  - [abstract] "postprocessing methods often violate anti-discrimination laws"
  - [section] "These methods modify the outcomes of earlier stages of the process...much like score banding...these procedures only reliably reduce adverse impact if they use protected information to modify decisions...which would be illegal because it constitutes disparate treatment and/or subgroup norming."
  - [corpus] Weak - corpus does not directly support this specific mechanism
- Break condition: If demographic information is not used to modify decisions, or if the legal landscape changes to allow such adjustments.

## Foundational Learning

- Concept: Fairness operationalizations
  - Why needed here: Understanding the different definitions of fairness is crucial for selecting appropriate bias mitigation methods and ensuring compliance with legal requirements.
  - Quick check question: What is the difference between adverse impact and equal accuracy as fairness operationalizations?

- Concept: Machine learning model lifecycle
  - Why needed here: The effectiveness and legality of bias mitigation methods vary depending on when they are applied in the ML development process.
  - Quick check question: At which stage of the ML model lifecycle would you apply preprocessing methods?

- Concept: Legal requirements for personnel selection
  - Why needed here: Many bias mitigation methods are illegal for use in personnel selection due to anti-discrimination laws, so understanding these requirements is essential for compliance.
  - Quick check question: Why are postprocessing methods that use demographic information to adjust scores likely illegal for personnel selection?

## Architecture Onboarding

- Component map:
  - Data collection and preprocessing
  - Model training and inprocessing
  - Model testing and postprocessing
  - Deployment and monitoring
  - Legal compliance checks

- Critical path:
  1. Collect diverse, representative training data
  2. Apply preprocessing methods to balance representation and remove demographic information
  3. Train model using inprocessing methods (e.g., multi-objective optimization)
  4. Test model for validity, adverse impact, and other fairness operationalizations
  5. Deploy model with continuous monitoring for changes in psychometric properties

- Design tradeoffs:
  - Validity vs. fairness: Optimizing for one may come at the expense of the other
  - Complexity vs. interpretability: More complex models may achieve better results but are harder to explain and audit
  - Sample size vs. subgroup representation: Ensuring adequate representation of all groups may require larger sample sizes

- Failure signatures:
  - High adverse impact despite using preprocessing methods
  - Decreased validity after applying inprocessing methods
  - Inconsistent model performance across different demographic groups
  - Legal challenges due to use of protected demographic information

- First 3 experiments:
  1. Compare the effectiveness of different preprocessing methods (e.g., oversampling vs. reweighing) on a dataset with known demographic imbalances
  2. Evaluate the trade-off between validity and fairness when using multi-objective optimization with different fairness operationalizations
  3. Test the impact of postprocessing methods on model validity and legality by applying them to a trained model and measuring changes in adverse impact and potential legal violations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are combinations of preprocessing and inprocessing bias mitigation methods compared to using a single method for reducing algorithmic bias in high-stakes settings?
- Basis in paper: [explicit] The paper mentions that little work has examined how to effectively conduct multi-objective optimization for end-to-end deep learning with language models and that combining multiple debiasing methods is an emerging issue.
- Why unresolved: The paper states that ensembling preprocessing algorithms worsens fairness operationalizations, but using multiple methods sequentially improves upon using just one method. However, more research is needed to determine when combinations of methods are effective in high-stakes settings.
- What evidence would resolve it: Empirical studies comparing the effectiveness of different combinations of preprocessing and inprocessing methods on reducing algorithmic bias in high-stakes settings, such as personnel selection.

### Open Question 2
- Question: What are the potential biases in large language models (LLMs) like GPT and CLAUDE, and what approaches are needed to minimize these biases given that retraining them is prohibitively expensive?
- Basis in paper: [explicit] The paper mentions that an emerging issue regards the potential use of LLMs for assessment and selection tasks and that initial research suggests diversity valuing prompts can mitigate LLM biases.
- Why unresolved: The paper states that we know little about the potential biases in these systems and new approaches are needed for minimizing any biases, given that retraining them is prohibitively expensive.
- What evidence would resolve it: Studies investigating the biases in LLMs and developing effective approaches to mitigate these biases without the need for expensive retraining.

### Open Question 3
- Question: How can the fairness operationalizations and algorithmic bias mitigation methods be extended to handle multiple subgroup comparisons, especially when considering intersectional identities?
- Basis in paper: [explicit] The paper mentions that most bias mitigation methods focus on the context of two subgroups, but it is possible to extend many solutions to handle multiple subgroup comparisons. However, increasing the number of subgroup comparisons increases the sample size needed to achieve a solution that will cross-validate with minimal shrinkage.
- Why unresolved: The paper states that this limits the applicability of such approaches in cases of intersectional identities, given the rapidly increasing number of subgroups when intersectional identities are considered.
- What evidence would resolve it: Research developing and testing methods to extend fairness operationalizations and algorithmic bias mitigation methods to handle multiple subgroup comparisons, including intersectional identities, while maintaining effectiveness and feasibility.

## Limitations
- The review's findings are primarily based on studies from computer science and data science, with limited input from I-O psychology
- The rapidly evolving nature of both algorithmic fairness methods and legal frameworks means some recommendations may become outdated quickly
- Effectiveness of multi-objective optimization and other inprocessing methods may vary significantly across different assessment contexts and domains

## Confidence
- High confidence: Preprocessing and inprocessing methods being most effective and legal for bias mitigation
- Medium confidence: Multi-objective optimization as a particularly promising approach
- Low confidence: Specific trade-offs between validity and fairness operationalizations across different contexts

## Next Checks
1. Conduct empirical studies comparing the effectiveness of preprocessing methods (oversampling, reweighing, SMOTE) across different types of assessment data and demographic groups to establish which methods work best under which conditions

2. Test the legal viability of inprocessing methods that incorporate fairness constraints by having employment law experts evaluate specific implementations against current US and European anti-discrimination regulations

3. Develop and validate a framework for continuous monitoring of ML models in deployment that specifically tracks changes in psychometric properties and adverse impact across demographic groups over time