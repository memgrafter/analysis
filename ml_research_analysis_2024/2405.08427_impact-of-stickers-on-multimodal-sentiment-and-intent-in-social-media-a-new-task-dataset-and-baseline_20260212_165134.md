---
ver: rpa2
title: 'Impact of Stickers on Multimodal Sentiment and Intent in Social Media: A New
  Task, Dataset and Baseline'
arxiv_id: '2405.08427'
source_url: https://arxiv.org/abs/2405.08427
tags:
- intent
- sentiment
- multimodal
- stickers
- sticker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a new task, MSAIRS, which involves multimodal
  sentiment analysis and intent recognition using stickers in social media conversations.
  The authors propose a novel dataset containing Chinese chat records and stickers
  from various social media platforms, which includes paired data with the same text
  but different stickers, the same sticker but different contexts, and various stickers
  consisting of the same images with different texts.
---

# Impact of Stickers on Multimodal Sentiment and Intent in Social Media: A New Task, Dataset and Baseline

## Quick Facts
- arXiv ID: 2405.08427
- Source URL: https://arxiv.org/abs/2405.08427
- Reference count: 40
- Primary result: Proposed MMSAIR model achieves 70.58% sentiment accuracy and 72.31% intent accuracy on MSAIRS dataset

## Executive Summary
This paper introduces MSAIRS, a novel task combining multimodal sentiment analysis and intent recognition using stickers in social media conversations. The authors construct a Chinese dataset from WeChat, TikTok, and QQ, featuring paired instances with same text/different stickers, same sticker/different contexts, and same images/different texts. They propose MMSAIR, a multimodal joint model using differential vector construction and cascaded attention mechanisms. Experimental results demonstrate that jointly modeling sentiment and intent mutually reinforces accuracy, with MMSAIR significantly outperforming traditional models and advanced MLLMs.

## Method Summary
The MSAIRS task involves joint sentiment analysis and intent recognition using multimodal data consisting of context text, sticker images, and sticker-text. MMSAIR processes these modalities separately using BERT for text and CLIP+CNN for images, then fuses them through cascaded multi-head attention. A differential vector captures the contrast between context and sticker representations, and separate classifiers predict sentiment and intent using shared features. The model is trained jointly with weighted loss functions, demonstrating superior performance compared to traditional models and MLLMs on the Chinese MSAIRS dataset.

## Key Results
- MMSAIR achieves 70.58% sentiment accuracy and 72.31% intent accuracy on MSAIRS dataset
- Joint modeling improves sentiment accuracy by 2.35% and intent accuracy by 3.36% compared to independent training
- MMSAIR significantly outperforms advanced MLLMs like GPT-4o on sticker-based sentiment and intent recognition

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Stickers add multimodal information that cannot be captured by text alone, and the differential vector construction in MMSAIR helps capture the contrast between context and sticker information.
- **Mechanism:** MMSAIR uses separate encoders for context, sticker-text, and sticker images, then applies multi-head attention to fuse these modalities. The differential vector captures the difference between the context representation and the fused sticker representation, helping the model focus on the complementary aspects of multimodal information.
- **Core assumption:** The difference between context and sticker representations contains meaningful information for joint sentiment and intent prediction.
- **Evidence anchors:** [abstract] "MMSAIR separately processes the input context, sticker and sticker-text, and integrates these multimodal components through a sophisticated fusion approach featuring cascaded multi-head attention mechanisms, differential vector construction, and feature concatenation"; [section 4.3] "To capture the contrast between the context and the refined sticker features, we construct a differential vector Vdiff using learnable parameters"

### Mechanism 2
- **Claim:** Sentiment and intent are interdependent tasks that mutually reinforce each other when modeled jointly.
- **Mechanism:** MMSAIR performs joint prediction of sentiment and intent using shared multimodal features, with separate classifiers for each task. The shared representation allows information from sentiment classification to benefit intent recognition and vice versa.
- **Core assumption:** Sentiment determination significantly impacts intent recognition and vice versa in social media communications.
- **Evidence anchors:** [abstract] "Our experiments demonstrate the necessity and effectiveness of jointly modeling sentiment and intent, as they mutually reinforce each other's recognition accuracy"; [section 5.4] "When performed independently, both tasks yield poorer results compared to our joint approach. For MMSAIR, joint modeling improves sentiment accuracy by 2.35% and intent accuracy by 3.36%"

### Mechanism 3
- **Claim:** Social media stickers often contain both image and text, requiring separate processing of sticker images and sticker-text before fusion.
- **Mechanism:** MMSAIR processes sticker images using CLIP and sticker-text using BERT separately, then concatenates their representations before fusion with context. This allows the model to capture both visual and textual aspects of stickers.
- **Core assumption:** Sticker images and sticker-text carry complementary information that should be processed separately before fusion.
- **Evidence anchors:** [abstract] "Our dataset includes paired data with the same text but different stickers, the same sticker but different contexts, and various stickers consisting of the same images with different texts"; [section 4.3] "We fuse the sticker image and sticker-text representations by concatenating E_I and E_S to form E_I,S"

## Foundational Learning

- **Concept:** Multimodal fusion techniques (concatenation, attention mechanisms)
  - **Why needed here:** MMSAIR needs to combine information from text (context and sticker-text) and images (stickers) to make joint predictions
  - **Quick check question:** What is the difference between early fusion (concatenation) and late fusion, and when would each be appropriate?

- **Concept:** Attention mechanisms in neural networks
  - **Why needed here:** MMSAIR uses cascaded multi-head attention to refine sticker features in the context and to further process the combined representations
  - **Quick check question:** How does multi-head attention differ from single-head attention, and what advantages does it provide?

- **Concept:** Joint learning and multi-task learning
  - **Why needed here:** MMSAIR performs joint sentiment analysis and intent recognition, which requires understanding how to share representations between related tasks
  - **Quick check question:** What are the benefits and potential drawbacks of joint learning compared to training separate models for each task?

## Architecture Onboarding

- **Component map:** Context → BERT → E_X → Multi-head attention with E_I,S → O_MH_A → Differential vector construction → Final multi-head attention → Classification
  - Alternative path: Sticker-text → BERT → E_S, Sticker image → CLIP → Conv1d → E_I → Concat(E_I, E_S) → E_I,S

- **Critical path:** Context → BERT → E_X → Multi-head attention with E_I,S → O_MH_A → Differential vector construction → Final multi-head attention → Classification
  - Alternative path: Sticker-text → BERT → E_S, Sticker image → CLIP → Conv1d → E_I → Concat(E_I, E_S) → E_I,S

- **Design tradeoffs:**
  - Separate vs. shared encoders for context and sticker-text
  - Differential vector vs. simple concatenation for capturing modality contrast
  - Joint vs. separate training for sentiment and intent tasks
  - CLIP vs. other image encoders for sticker representation

- **Failure signatures:**
  - Poor performance on either sentiment or intent indicates issues with the shared representation
  - Degradation when removing any modality suggests insufficient fusion
  - Overfitting to training data may occur due to the complexity of the model

- **First 3 experiments:**
  1. Ablation study: Remove differential vector and compare performance to full model
  2. Ablation study: Train sentiment and intent separately and compare to joint model
  3. Replace CLIP with ResNet50 and measure impact on performance

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the following questions arise from the work:

### Open Question 1
- **Question:** How do different sticker styles (e.g., text-only, cartoon, real person) differentially impact multimodal sentiment analysis performance compared to each other?
- **Basis in paper:** [explicit] Table 4 shows the distribution of sticker styles but does not analyze their differential impact on model performance
- **Why unresolved:** The paper identifies sticker style diversity but does not experimentally isolate how each style category affects sentiment analysis accuracy
- **What evidence would resolve it:** Comparative experiments measuring model performance on sentiment analysis across the different sticker style categories

### Open Question 2
- **Question:** Would incorporating cultural context features improve the performance of multimodal sentiment analysis models on stickers, particularly for cross-cultural applications?
- **Basis in paper:** [inferred] The paper notes that Chinese social media platforms were used and acknowledges potential cultural differences in sticker interpretation, but does not test cultural context integration
- **Why unresolved:** The current models treat stickers as purely visual-textual objects without leveraging cultural metadata or context
- **What evidence would resolve it:** Experiments comparing model performance with and without cultural context features, potentially using cross-cultural datasets

### Open Question 3
- **Question:** What specific aspects of MLLMs' training data or architecture cause their underperformance on sticker-based sentiment and intent recognition compared to specialized multimodal models?
- **Basis in paper:** [explicit] The paper demonstrates that advanced MLLMs like GPT-4o significantly underperform compared to MMSAIR, but does not analyze why
- **Why unresolved:** The paper shows the performance gap but does not conduct ablation studies or architectural analysis to identify root causes
- **What evidence would resolve it:** Systematic analysis of MLLM failures on sticker tasks, including fine-tuning experiments with different training objectives and architectural modifications

### Open Question 4
- **Question:** How does the bidirectional influence between context and stickers vary across different sentiment-intent combinations?
- **Basis in paper:** [inferred] The paper demonstrates that sentiment and intent tasks mutually reinforce each other but does not analyze which specific combinations show stronger or weaker relationships
- **Why unresolved:** The paper shows overall mutual reinforcement but does not examine the strength of context-sticker interaction for different sentiment-intent pairs
- **What evidence would resolve it:** Correlation analysis showing how context-sticker alignment strength varies across different sentiment-intent combinations, potentially using attention visualization techniques

## Limitations
- The dataset is limited to Chinese social media platforms (WeChat, TikTok, QQ), constraining generalizability to other languages and cultural contexts
- The differential vector construction mechanism lacks comprehensive comparison with simpler alternatives to prove its optimality
- The mutual reinforcement between sentiment and intent may reflect dataset-specific correlations rather than fundamental task interdependence

## Confidence

**High Confidence:** The MSAIRS dataset creation and basic task formulation are well-supported. The dataset construction methodology is clearly described, and the paired data design provides a solid foundation for controlled experiments.

**Medium Confidence:** The effectiveness of MMSAIR's specific architectural innovations (differential vector construction, cascaded attention) is demonstrated through ablation studies, but the comparative advantage over simpler alternatives remains somewhat theoretical.

**Low Confidence:** The generalizability of findings to other languages, cultural contexts, or social media platforms is not established. The paper's claims about mutual reinforcement between sentiment and intent tasks may be specific to the Chinese social media context.

## Next Checks

1. **Cross-Cultural Validation:** Test MMSAIR on sticker datasets from non-Chinese social media platforms (e.g., Facebook, WhatsApp, LINE) to assess whether the differential vector construction and cascaded attention mechanisms maintain their effectiveness across different cultural contexts and sticker usage patterns.

2. **Architectural Ablation Deep Dive:** Conduct a more comprehensive ablation study comparing MMSAIR against simpler fusion approaches (weighted averaging, gated fusion) and alternative differential mechanisms (absolute difference, multiplicative interaction) to determine whether the specific architectural choices are truly optimal or merely sufficient.

3. **Task Independence Investigation:** Design experiments to systematically vary the correlation between sentiment and intent labels in the dataset, measuring how joint modeling performance changes as task independence increases. This would help determine whether the observed mutual reinforcement is a fundamental property of these tasks or a dataset-specific artifact.