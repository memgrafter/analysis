---
ver: rpa2
title: 'Gloss2Text: Sign Language Gloss translation using LLMs and Semantically Aware
  Label Smoothing'
arxiv_id: '2407.01394'
source_url: https://arxiv.org/abs/2407.01394
tags:
- translation
- language
- sign
- gloss
- chen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a sign language gloss-to-text translation approach
  that leverages large language models (LLMs) with semantically-aware label smoothing
  and data augmentation techniques. The method employs paraphrasing and back-translation
  for data augmentation, along with a novel label smoothing loss that accounts for
  gloss translation ambiguities by assigning higher probabilities to semantically
  similar target words.
---

# Gloss2Text: Sign Language Gloss translation using LLMs and Semantically Aware Label Smoothing

## Quick Facts
- arXiv ID: 2407.01394
- Source URL: https://arxiv.org/abs/2407.01394
- Reference count: 13
- Primary result: Achieves BLEU-1 score of 55.61 and BLEU-4 score of 33.71 on PHOENIX Weather 2014T dataset

## Executive Summary
This paper proposes a novel approach for sign language gloss-to-text translation using large language models (LLMs) enhanced with semantically-aware label smoothing and data augmentation techniques. The method leverages the NLLB-200 model with LoRA adapters to efficiently fine-tune the 3.3B parameter model while preventing overfitting on the small PHOENIX-2014T dataset. The approach introduces a semantically-aware label smoothing loss that accounts for gloss translation ambiguities by assigning higher probabilities to semantically similar target words, along with paraphrasing and back-translation for data augmentation.

## Method Summary
The method employs the NLLB-200 LLM fine-tuned with LoRA adapters to translate sign language glosses to German text. It uses FastText word embeddings to compute semantic similarity between target vocabulary words for the semantically-aware label smoothing loss. Data augmentation is performed through paraphrasing (translating German text to English and back) and back-translation (generating synthetic gloss annotations from spoken text). The model is trained with AdamW optimizer for 60 epochs on two NVIDIA V100 GPUs, using beam search of 5 for inference.

## Key Results
- Achieves BLEU-1 score of 55.61 and BLEU-4 score of 33.71 on PHOENIX Weather 2014T test set
- Shows 3.75% relative improvement in BLEU-1 and 6.69% in BLEU-4 over previous state-of-the-art methods
- Demonstrates effectiveness of semantically-aware label smoothing and data augmentation techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SALS loss improves translation quality by assigning higher probabilities to semantically similar target words during training
- Mechanism: Computes cosine similarity between word embeddings of target vocabulary words, giving higher probability weights to words with similarity above threshold λ=0.6
- Core assumption: Gloss translation ambiguities exist where multiple target words can represent the same semantic meaning
- Evidence anchors: Abstract mentions exploiting gloss translation ambiguities; section describes computing semantic similarity with cosine similarity
- Break condition: If semantic relationships are not well-captured by FastText embeddings or threshold λ is poorly chosen

### Mechanism 2
- Claim: Data augmentation through paraphrasing and back-translation improves model robustness
- Mechanism: Paraphrasing translates German text to English and back; back-translation generates synthetic gloss annotations from spoken text
- Core assumption: Increasing dataset diversity helps overcome data scarcity problem in sign language translation
- Evidence anchors: Abstract mentions data augmentation improving performance; section describes both augmentation techniques
- Break condition: If augmented data introduces noise or incorrect translations that mislead the model

### Mechanism 3
- Claim: LoRA adapters enable efficient fine-tuning while maintaining original LLM functionality
- Mechanism: Freezes original model weights and trains only low-rank adapter matrices, reducing trainable parameters from billions to millions
- Core assumption: Sign language translation can be learned by modifying small subset of parameters rather than full fine-tuning
- Evidence anchors: Section explains LoRA technique for fine-tuning; Table 2 shows LoRA addresses overfitting risk
- Break condition: If LoRA adapters cannot capture task-specific knowledge or parameters are not properly tuned

## Foundational Learning

- Concept: Semantic similarity and word embeddings
  - Why needed here: SALS loss function relies on computing cosine similarity between word embeddings to identify semantically similar target words
  - Quick check question: What does a cosine similarity of 1.0 between two word vectors indicate about their semantic relationship?

- Concept: Label smoothing and its variants
  - Why needed here: Understanding standard label smoothing is essential to grasp how SALS modifies it with semantic awareness
  - Quick check question: In standard label smoothing, what probability is assigned to the correct class versus all other classes?

- Concept: Data augmentation techniques in NLP
  - Why needed here: Paper uses paraphrasing and back-translation as augmentation methods common in low-resource machine translation
  - Quick check question: How does back-translation help when target-side data is scarce but source-side data is available?

## Architecture Onboarding

- Component map: Input gloss sequence -> NLLB-200 LLM with LoRA adapter -> German text output -> SALS loss computation -> Parameter updates on LoRA matrices only

- Critical path: Gloss sequence input → LLM with frozen weights and LoRA adapters → German text generation → SALS loss with semantic similarity computation → Updates only to LoRA matrices

- Design tradeoffs:
  - NLLB-200 vs smaller models: Better performance but higher computational cost and overfitting risk (mitigated by LoRA)
  - SALS vs standard label smoothing: Better handling of semantic ambiguities but requires additional computation for similarity matrix
  - Data augmentation vs no augmentation: Better robustness but potential introduction of noise

- Failure signatures:
  - Performance degradation on test set compared to dev set: Indicates overfitting
  - BLEU scores plateauing early during training: Suggests LoRA rank is too low
  - Poor semantic similarity computations: FastText embeddings may not capture sign language domain vocabulary well

- First 3 experiments:
  1. Train baseline NLLB-200 with standard cross-entropy loss and no augmentation to establish reference performance
  2. Add LoRA fine-tuning with standard label smoothing to evaluate impact of parameter-efficient adaptation
  3. Implement SALS loss with λ=0.6 and β=0.1 on top of LoRA to measure contribution of semantic-aware smoothing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method perform on datasets with larger vocabulary sizes and more diverse sign language expressions?
- Basis in paper: Mentions domain-specific vocabulary in datasets may not reflect everyday activities prevalent in deaf community
- Why unresolved: Experiments conducted only on PHOENIX Weather 2014T dataset
- What evidence would resolve it: Testing on multiple sign language datasets with varying vocabulary sizes and domains

### Open Question 2
- Question: How does the method handle sign language expressions that rely heavily on facial expressions and gestures?
- Basis in paper: Acknowledges facial expressions and gestures are typically omitted in gloss representations but captured in spoken language translations
- Why unresolved: Method focuses on Gloss2Text translation without addressing non-gloss information incorporation
- What evidence would resolve it: Developing multimodal approach integrating visual features with gloss annotations

### Open Question 3
- Question: How does the proposed method perform when fine-tuned on smaller datasets or with fewer training samples?
- Basis in paper: Mentions PHOENIX-2014T dataset has only 8,257 gloss-text pairs, significantly smaller than typical NMT datasets
- Why unresolved: Experiments conducted on relatively small dataset without exploring even smaller subsets
- What evidence would resolve it: Conducting experiments with varying dataset sizes including smaller subsets

### Open Question 4
- Question: How does the proposed method compare to other recent advancements in sign language translation?
- Basis in paper: Compares to previous state-of-the-art methods but doesn't explore recent advancements
- Why unresolved: Field is rapidly evolving with new methods potentially outperforming proposed approach
- What evidence would resolve it: Comprehensive comparison with recent advancements including transformer-based architectures

## Limitations

- Semantic similarity computation relies on FastText embeddings that may not adequately capture specialized sign language vocabulary and nuances
- Data augmentation quality is not independently verified, with potential noise introduced through paraphrasing and back-translation
- LoRA effectiveness compared to other parameter-efficient methods is not explored through systematic ablation studies

## Confidence

**High Confidence**: Experimental setup and methodology are clearly described with verifiable BLEU scores and comparisons to previous methods

**Medium Confidence**: Core claims about SALS and data augmentation improving translation quality are supported by results but underlying mechanisms need validation

**Low Confidence**: Claims about LoRA preventing overfitting lack thorough comparison with alternative parameter-efficient methods and ablation studies

## Next Checks

1. **Semantic Similarity Validation**: Conduct human evaluation study where sign language experts assess whether cosine similarity scores accurately reflect semantic equivalence in translation context

2. **Data Augmentation Quality Analysis**: Perform error analysis on augmented training data to quantify noise introduced through paraphrasing and back-translation

3. **LoRA Architecture Sensitivity**: Systematically vary LoRA rank and alpha parameters to determine impact on translation quality and compare with full fine-tuning and other parameter-efficient methods