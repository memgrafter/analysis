---
ver: rpa2
title: Measuring short-form factuality in large language models
arxiv_id: '2411.04368'
source_url: https://arxiv.org/abs/2411.04368
tags:
- answer
- questions
- correct
- question
- answers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SimpleQA, a benchmark designed to measure
  the factuality of large language models on short, fact-seeking questions. The benchmark
  consists of 4,326 questions with single, indisputable answers, created adversarially
  against GPT-4 responses.
---

# Measuring short-form factuality in large language models

## Quick Facts
- **arXiv ID**: 2411.04368
- **Source URL**: https://arxiv.org/abs/2411.04368
- **Reference count**: 8
- **Primary result**: SimpleQA benchmark reveals current LLMs struggle with factuality on short, fact-seeking questions

## Executive Summary
This paper introduces SimpleQA, a benchmark designed to measure the factuality of large language models on short, fact-seeking questions. The benchmark consists of 4,326 questions with single, indisputable answers, created adversarially against GPT-4 responses. The authors evaluate various models, including Claude and GPT-4 variants, on SimpleQA and find that even the best-performing models struggle with factuality. They also measure calibration by asking models to state their confidence in answers and by repeating questions multiple times. The results show that models are overconfident in their answers and have room for improvement in calibration. The authors hope that SimpleQA will serve as a useful tool for evaluating and improving the factuality of future language models.

## Method Summary
The authors created SimpleQA by adversarially generating questions designed to elicit incorrect answers from GPT-4, then filtering for questions with single, indisputable answers. They evaluated 8 models from major tech companies on this benchmark, measuring both accuracy and calibration. Calibration was assessed by having models state their confidence in answers and by repeating questions multiple times to measure consistency. The benchmark focuses specifically on short, fact-seeking questions where there is only one correct answer.

## Key Results
- Current state-of-the-art models achieve significantly below perfect accuracy on SimpleQA
- Models show overconfidence in their answers when asked to self-report confidence
- Repeated questioning reveals inconsistency in model responses
- Even top models like Claude and GPT-4 variants struggle with factuality on this benchmark

## Why This Works (Mechanism)
The adversarial creation of questions ensures the benchmark targets genuine factual knowledge rather than model memorization or pattern recognition. By focusing on short, fact-seeking questions with single indisputable answers, the benchmark isolates factuality from other capabilities like reasoning or creative generation. The confidence measurement and repetition techniques reveal that models not only get facts wrong but are miscalibrated about their certainty, suggesting fundamental limitations in how models represent and retrieve factual knowledge.

## Foundational Learning

**Adversarial question generation**: Creating questions designed to trick models ensures the benchmark targets genuine knowledge rather than memorized patterns. Quick check: Verify questions actually elicit incorrect responses from baseline models.

**Fact-seeking question design**: Focusing on questions with single, indisputable answers isolates factuality from ambiguity. Quick check: Ensure no questions have multiple valid interpretations.

**Confidence calibration measurement**: Having models self-report confidence reveals miscalibration in their knowledge representation. Quick check: Compare self-reported confidence to actual accuracy.

**Repeated questioning**: Asking the same question multiple times measures consistency and reliability of factual recall. Quick check: Analyze variance in responses to identical questions.

## Architecture Onboarding

**Component map**: Adversarial question generation -> Benchmark construction -> Model evaluation -> Calibration analysis

**Critical path**: Question creation → Benchmark assembly → Model testing → Confidence measurement → Analysis of results

**Design tradeoffs**: The benchmark sacrifices breadth of knowledge types for depth of factuality measurement; adversarial creation ensures difficulty but may bias question types.

**Failure signatures**: Models produce factually incorrect answers despite high confidence; responses vary when identical questions are repeated; performance drops on adversarially selected questions.

**3 first experiments**:
1. Run SimpleQA on a diverse set of open-source models to compare with closed models
2. Have human experts answer SimpleQA questions to establish baseline human performance
3. Analyze error patterns to identify specific knowledge domains where models struggle most

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions beyond noting that models have room for improvement in factuality and calibration.

## Limitations

The adversarial question creation process may introduce bias toward certain types of factual knowledge while missing others. The benchmark focuses on short, fact-seeking questions with single indisputable answers, representing a narrow slice of real-world use cases. The evaluation of only eight models from major tech companies limits generalizability to the broader landscape of deployed LLMs.

## Confidence

- **Core finding (High)**: Current models struggle with factuality on SimpleQA
- **Calibration measurements (Medium)**: Confidence estimation methods for LLMs remain an evolving field with varying reliability across model architectures

## Next Checks

1. Test SimpleQA against a broader range of models including open-source alternatives and models with different training paradigms
2. Validate the benchmark's difficulty by having human experts answer the questions and compare error rates
3. Conduct an analysis of which question types and knowledge domains models perform worst on to identify specific areas for improvement